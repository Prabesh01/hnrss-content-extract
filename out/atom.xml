<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-30T19:32:24.575733+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45746020</id><title>Zig's New Async I/O</title><updated>2025-10-30T19:32:33.635113+00:00</updated><content>&lt;doc fingerprint="cf269e50298fb69c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Zig's New Async I/O (Text Version)&lt;/head&gt;
    &lt;p&gt;In celebration of the std.Io introduction patchset landing today, here is the text version of a short, interactive demo I gave at Zigtoberfest 2025.&lt;/p&gt;
    &lt;p&gt;This is a preview of the new async I/O primitives that will be available in the upcoming Zig 0.16.0, to be released in about 3-4 months. There is a lot more to get into, but for now here is an introduction into some of the core synchronization API that will be available for all Zig code to use.&lt;/p&gt;
    &lt;p&gt;To begin, let's try to keep it simple and understand the basics, and then we'll then slowly add more asynchronous things into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 0&lt;/head&gt;
    &lt;p&gt;With our first example, there is nothing asynchronous here. It's basically "Hello, World!" in Zig.&lt;/p&gt;
    &lt;code&gt;const std = @import("std");

pub fn main() !void {
    doWork();
}

fn doWork() void {
    std.debug.print("working\n", .{});
    var timespec: std.posix.timespec = .{ .sec = 1, .nsec = 0 };
    _ = std.posix.system.nanosleep(&amp;amp;timespec, &amp;amp;timespec);
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example0.zig 0s working 1s $&lt;/quote&gt;
    &lt;head rend="h2"&gt;Example 1&lt;/head&gt;
    &lt;p&gt;Next, we're going to set up a little bit. Still not using async/await yet, but I need some tools in my toolbox before we add complexity.&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const Io = std.Io;
const Allocator = std.mem.Allocator;
const assert = std.debug.assert;

fn juicyMain(gpa: Allocator, io: Io) !void {
    _ = gpa;

    doWork(io);
}

fn doWork(io: Io) void {
    std.debug.print("working\n", .{});
    io.sleep(.fromSeconds(1), .awake) catch {};
}

pub fn main() !void {
    // Set up allocator.
    var debug_allocator: std.heap.DebugAllocator(.{}) = .init;
    defer assert(debug_allocator.deinit() == .ok);
    const gpa = debug_allocator.allocator();

    // Set up our I/O implementation.
    var threaded: std.Io.Threaded = .init(gpa);
    defer threaded.deinit();
    const io = threaded.io();

    return juicyMain(gpa, io);
}&lt;/code&gt;
    &lt;p&gt;Output (same as before):&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example0.zig 0s working 1s $&lt;/quote&gt;
    &lt;p&gt;Setting up a &lt;code&gt;std.Io&lt;/code&gt; implementation is a lot like setting up an allocator.
You typically do it once, in main(), and then pass the instance throughout the application.
Reusable code should accept an Allocator parameter if it needs to allocate, and it should accept
an Io parameter if it needs to perform I/O operations.&lt;/p&gt;
    &lt;p&gt;In this case, this is an Io implementation based on threads. This is not using KQueue, this is not using IO_Uring, this is not using an event loop. It is a threaded implementation of the new &lt;code&gt;std.Io&lt;/code&gt; interface.&lt;/p&gt;
    &lt;p&gt;This setup will be the same in all the examples, so now we can focus on our example code, which is the same as last time. Still nothing interesting - we just call &lt;code&gt;doWork&lt;/code&gt; which of course is just calling sleep().&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 2&lt;/head&gt;
    &lt;p&gt;Redundant setup code omitted from here on out.&lt;/p&gt;
    &lt;code&gt;fn juicyMain(gpa: Allocator, io: Io) !void {
    _ = gpa;

    var future = io.async(doWork, .{io});

    future.await(io); // idempotent
}

fn doWork(io: Io) void {
    std.debug.print("working\n", .{});
    io.sleep(.fromSeconds(1), .awake) catch {};
}&lt;/code&gt;
    &lt;p&gt;Output (same as before):&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example0.zig 0s working 1s $&lt;/quote&gt;
    &lt;p&gt;Now we're using async/await to call doWork. What async/await means to Zig is to decouple the calling of the function to the returning of the function.&lt;/p&gt;
    &lt;p&gt;This code is the same as before. It's exactly the same, because we didn't put any code between the async and await. We do the call, and then immediately wait for the return.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 3&lt;/head&gt;
    &lt;p&gt;In the next example, we have two things at the same time:&lt;/p&gt;
    &lt;code&gt;fn juicyMain(gpa: Allocator, io: Io) !void {
    _ = gpa;

    var a = io.async(doWork, .{ io, "hard" });
    var b = io.async(doWork, .{ io, "on an excuse not to drink Spezi" });

    a.await(io);
    b.await(io);
}

fn doWork(io: Io, flavor_text: []const u8) void {
    std.debug.print("working {s}\n", .{flavor_text});
    io.sleep(.fromSeconds(1), .awake) catch {};
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example3.zig 0s working on an excuse not to drink Spezi 0s working hard 1s $&lt;/quote&gt;
    &lt;p&gt;If you look carefully, you can see that it did not wait two seconds; it waited one second because these operations are happening at the same time. This demonstrates why using async/await is useful - you can express asynchrony. Depending on the I/O implementation that you choose, it may be able to take advantage of the asynchrony that you have expressed and make your code go faster. For example in this case, &lt;code&gt;std.Io.Threaded&lt;/code&gt; was able to do two seconds of work in one second
of actual time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 4&lt;/head&gt;
    &lt;p&gt;Let's start to bring the example closer to a real world scenario by introducing failure.&lt;/p&gt;
    &lt;code&gt;fn juicyMain(gpa: Allocator, io: Io) !void {
    var a = io.async(doWork, .{ gpa, io, "hard" });
    var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });

    try a.await(io);
    try b.await(io);
}

fn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {
    // Simulate an error occurring:
    if (flavor_text[0] == 'h') return error.OutOfMemory;

    const copied_string = try gpa.dupe(u8, flavor_text);
    defer gpa.free(copied_string);
    std.debug.print("working {s}\n", .{copied_string});
    io.sleep(.fromSeconds(1), .awake) catch {};
}&lt;/code&gt;
    &lt;p&gt;It's the same code as before, except the first task will return an error.&lt;/p&gt;
    &lt;p&gt;Guess what happens when this code is run?&lt;/p&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example4.zig 0s working on an excuse not to drink Spezi 1s error(gpa): memory address 0x7f99ce6c0080 leaked: 1s /home/andy/src/zig/lib/std/Io/Threaded.zig:466:67: 0x1053aae in async (std.zig) 1s const ac: *AsyncClosure = @ptrCast(@alignCast(gpa.alignedAlloc(u8, .of(AsyncClosure), n) catch { 1s ^ 1s /home/andy/src/zig/lib/std/Io.zig:1548:40: 0x1164f94 in async__anon_27344 (std.zig) 1s future.any_future = io.vtable.async( 1s ^ 1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:8:21: 0x116338a in juicyMain (example4.zig) 1s var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" }); 1s ^ 1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:35:21: 0x1163663 in main (example4.zig) 1s return juicyMain(gpa, io); 1s ^ 1s /home/andy/src/zig/lib/std/start.zig:696:37: 0x1163c83 in callMain (std.zig) 1s const result = root.main() catch |err| { 1s ^ 1s /home/andy/src/zig/lib/std/start.zig:237:5: 0x1162f61 in _start (std.zig) 1s asm volatile (switch (native_arch) { 1s ^ 1s 1s thread 1327233 panic: reached unreachable code 1s error return context: 1s /home/andy/src/zig/lib/std/Io.zig:1003:13: 0x11651a8 in await (std.zig) 1s return f.result; 1s ^ 1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:10:5: 0x11633e8 in juicyMain (example4.zig) 1s try a.await(io); 1s ^ 1s 1s stack trace: 1s /home/andy/src/zig/lib/std/debug.zig:409:14: 0x103e5a9 in assert (std.zig) 1s if (!ok) unreachable; // assertion failure 1s ^ 1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:27:17: 0x1163698 in main (example4.zig) 1s defer assert(debug_allocator.deinit() == .ok); 1s ^ 1s /home/andy/src/zig/lib/std/start.zig:696:37: 0x1163c83 in callMain (std.zig) 1s const result = root.main() catch |err| { 1s ^ 1s /home/andy/src/zig/lib/std/start.zig:237:5: 0x1162f61 in _start (std.zig) 1s asm volatile (switch (native_arch) { 1s ^ 1s fish: Job 1, 'zig run example4.zig' terminated by signal SIGABRT (Abort) 1s $&lt;/quote&gt;
    &lt;p&gt;The problem is that when the first &lt;code&gt;try&lt;/code&gt; activates, it skips the second &lt;code&gt;await&lt;/code&gt; which
is then caught by the leak checker.&lt;/p&gt;
    &lt;p&gt;This is a bug. It's unfortunate though, isn't it? Because we would like to write the code this way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 5&lt;/head&gt;
    &lt;p&gt;Here's a fix:&lt;/p&gt;
    &lt;code&gt;fn juicyMain(gpa: Allocator, io: Io) !void {
    var a = io.async(doWork, .{ gpa, io, "hard" });
    var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });

    const a_result = a.await(io);
    const b_result = b.await(io);

    try a_result;
    try b_result;
}

fn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {
    // Simulate an error occurring:
    if (flavor_text[0] == 'h') return error.OutOfMemory;

    const copied_string = try gpa.dupe(u8, flavor_text);
    defer gpa.free(copied_string);
    std.debug.print("working {s}\n", .{copied_string});
    io.sleep(.fromSeconds(1), .awake) catch {};
}&lt;/code&gt;
    &lt;p&gt;We do the awaits, then we do the tries. This will fix the problem.&lt;/p&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example5.zig 0s working on an excuse not to drink Spezi 1s error: OutOfMemory 1s /home/andy/src/zig/lib/std/Io.zig:1003:13: 0x11651d8 in await (std.zig) 1s return f.result; 1s ^ 1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example5.zig:13:5: 0x1163416 in juicyMain (example5.zig) 1s try a_result; 1s ^ 1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example5.zig:38:5: 0x11636e9 in main (example5.zig) 1s return juicyMain(gpa, io); 1s ^ 1s $&lt;/quote&gt;
    &lt;p&gt;This failed successfully. The error was handled and no resources leaked. But it's a footgun. Let's find a better way to express this...&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 6&lt;/head&gt;
    &lt;p&gt;This is where cancellation comes in. cancellation is an extremely handy primitive, because now we can use &lt;code&gt;defer&lt;/code&gt;, &lt;code&gt;try&lt;/code&gt;, and &lt;code&gt;await&lt;/code&gt; like normal,
and not only do we fix the bug, but we also get more optimal code.&lt;/p&gt;
    &lt;code&gt;fn juicyMain(gpa: Allocator, io: Io) !void {
    var a = io.async(doWork, .{ gpa, io, "hard" });
    defer a.cancel(io) catch {};

    var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });
    defer b.cancel(io) catch {};

    try a.await(io);
    try b.await(io);
}

fn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {
    // Simulate an error occurring:
    if (flavor_text[0] == 'h') return error.OutOfMemory;

    const copied_string = try gpa.dupe(u8, flavor_text);
    defer gpa.free(copied_string);
    std.debug.print("working {s}\n", .{copied_string});
    io.sleep(.fromSeconds(1), .awake) catch {};
}&lt;/code&gt;
    &lt;p&gt;Thanks to cancellation, we now get instant results, because the moment that the first task returns an error, the cancels get run.&lt;/p&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example6.zig 0s working on an excuse not to drink Spezi 0s error: OutOfMemory 0s /home/andy/misc/talks/zigtoberfest/async-io-examples/example6.zig:13:5: 0x116348c in juicyMain (example6.zig) 0s try a.await(io); 0s ^ 0s /home/andy/misc/talks/zigtoberfest/async-io-examples/example6.zig:38:5: 0x1163909 in main (example6.zig) 0s return juicyMain(gpa, io); 0s ^ 0s $&lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;cancel&lt;/code&gt; is your best friend, because it's going to prevent you from leaking the
resource, and it's going to make your code run more optimally.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cancel&lt;/code&gt; is trivial to understand: it has identical semantics as &lt;code&gt;await&lt;/code&gt;, except
that it also requests cancellation. The conditions under which cancellation requests are honored
are defined by each I/O implementation.&lt;/p&gt;
    &lt;p&gt;Both &lt;code&gt;cancel&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; are idempotent with respect to themselves and each other.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 7&lt;/head&gt;
    &lt;p&gt;Next, let's introduce another real-world scenario: resource allocation. In this case, we allocate a string on success, which the caller needs to manage.&lt;/p&gt;
    &lt;code&gt;fn juicyMain(gpa: Allocator, io: Io) !void {
    var a = io.async(doWork, .{ gpa, io, "hard" });
    defer if (a.cancel(io)) |s| gpa.free(s) else |_| {};

    var b = io.async(doWork, .{ gpa, io, "on an excuse not to drink Spezi" });
    defer if (b.cancel(io)) |s| gpa.free(s) else |_| {};

    const a_string = try a.await(io);
    const b_string = try b.await(io);
    std.debug.print("finished {s}\n", .{a_string});
    std.debug.print("finished {s}\n", .{b_string});
}

fn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) ![]u8 {
    const copied_string = try gpa.dupe(u8, flavor_text);
    std.debug.print("working {s}\n", .{copied_string});
    io.sleep(.fromSeconds(1), .awake) catch {};
    return copied_string;
}&lt;/code&gt;
    &lt;p&gt;Now we see why &lt;code&gt;cancel&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; have the same API.
The deferred cancel calls above free the allocated resource, handling both
successful calls (resource allocated) and failed calls (resource not allocated).&lt;/p&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example7.zig 0s working on an excuse not to drink Spezi 0s working hard 1s finished hard 1s finished on an excuse not to drink Spezi 1s $&lt;/quote&gt;
    &lt;p&gt;The important thing here is that by doing resource management like this, we are able to write standard, idiomatic Zig code below, using &lt;code&gt;try&lt;/code&gt; and &lt;code&gt;return&lt;/code&gt;
like normal without worrying about special resource management cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 8&lt;/head&gt;
    &lt;p&gt;Now we're switching gears a little bit. It's time to learn why asynchrony is not concurrency.&lt;/p&gt;
    &lt;p&gt;In this example we have a producer sending one item across an unbuffered queue to a consumer.&lt;/p&gt;
    &lt;code&gt;fn juicyMain(io: Io) !void {
    var queue: Io.Queue([]const u8) = .init(&amp;amp;.{});

    var producer_task = io.async(producer, .{
        io, &amp;amp;queue, "never gonna give you up",
    });
    defer producer_task.cancel(io) catch {};

    var consumer_task = io.async(consumer, .{ io, &amp;amp;queue });
    defer _ = consumer_task.cancel(io) catch {};

    const result = try consumer_task.await(io);
    std.debug.print("message received: {s}\n", .{result});
}

fn producer(
    io: Io,
    queue: *Io.Queue([]const u8),
    flavor_text: []const u8,
) !void {
    try queue.putOne(io, flavor_text);
}

fn consumer(
    io: Io,
    queue: *Io.Queue([]const u8),
) ![]const u8 {
    return queue.getOne(io);
}&lt;/code&gt;
    &lt;p&gt;We use &lt;code&gt;async&lt;/code&gt; to spawn the producer and &lt;code&gt;async&lt;/code&gt; to spawn the consumer.&lt;/p&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example8.zig 0s message received: never gonna give you up 0s $&lt;/quote&gt;
    &lt;p&gt;This incorrectly succeeds. Depending on your perspective, we either got "lucky" or "unlucky" due to the thread pool having spare concurrency that happened to be available.&lt;/p&gt;
    &lt;p&gt;To observe the problem, we can artificially limit the &lt;code&gt;std.Io.Threaded&lt;/code&gt; instance to
use a thread pool size of one:&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 9&lt;/head&gt;
    &lt;code&gt;// Set up our I/O implementation.
    var threaded: std.Io.Threaded = .init(gpa);
    threaded.cpu_count = 1;
    defer threaded.deinit();
    const io = threaded.io();

    return juicyMain(io);
}&lt;/code&gt;
    &lt;p&gt;Output: (deadlock)&lt;/p&gt;
    &lt;p&gt;Now that it's only using one thread, it deadlocks, because the consumer is waiting to get something from the queue, and the producer is scheduled to run, but it has not run yet.&lt;/p&gt;
    &lt;p&gt;The problem is that we needed concurrency, but we asked for asynchrony.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example 10&lt;/head&gt;
    &lt;p&gt;In order to fix this, we use &lt;code&gt;io.concurrent&lt;/code&gt; instead of &lt;code&gt;io.async&lt;/code&gt;.
This one can fail with &lt;code&gt;error.ConcurrencyUnavailable&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fn juicyMain(io: Io) !void {
    var queue: Io.Queue([]const u8) = .init(&amp;amp;.{});

    var producer_task = try io.concurrent(producer, .{
        io, &amp;amp;queue, "never gonna give you up",
    });
    defer producer_task.cancel(io) catch {};

    var consumer_task = try io.concurrent(consumer, .{ io, &amp;amp;queue });
    defer _ = consumer_task.cancel(io) catch {};

    const result = try consumer_task.await(io);
    std.debug.print("message received: {s}\n", .{result});
}

fn producer(
    io: Io,
    queue: *Io.Queue([]const u8),
    flavor_text: []const u8,
) !void {
    try queue.putOne(io, flavor_text);
}

fn consumer(
    io: Io,
    queue: *Io.Queue([]const u8),
) ![]const u8 {
    return queue.getOne(io);
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;quote&gt;0s $ zig run example10.zig 0s message received: never gonna give you up 0s $&lt;/quote&gt;
    &lt;p&gt;Now the code is fixed because we correctly expressed that we needed concurrency, which &lt;code&gt;std.Io.Threaded&lt;/code&gt; honored by oversubscribing.&lt;/p&gt;
    &lt;p&gt;If I add &lt;code&gt;-fsingle-threaded&lt;/code&gt; which truly limits the executable to one thread,
oversubscription is not available, causing this output:&lt;/p&gt;
    &lt;quote&gt;error: ConcurrencyUnavailable /home/andy/src/zig/lib/std/Io/Threaded.zig:529:34: 0x1051863 in concurrent (std.zig) if (builtin.single_threaded) return error.ConcurrencyUnavailable; ^ /home/andy/src/zig/lib/std/Io.zig:1587:25: 0x1158b5f in concurrent__anon_26591 (std.zig) future.any_future = try io.vtable.concurrent( ^ /home/andy/misc/talks/zigtoberfest/async-io-examples/example10.zig:9:25: 0x1157198 in juicyMain (example10.zig) var producer_task = try io.concurrent(producer, .{ ^ /home/andy/misc/talks/zigtoberfest/async-io-examples/example10.zig:48:5: 0x115776a in main (example10.zig) return juicyMain(io); ^&lt;/quote&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;There are proof-of-concept &lt;code&gt;std.Io&lt;/code&gt; implementations using IoUring and KQueue combined
with stackful coroutines which show a lot of promise, however that work depends on some language
enhancements to be practical. There is also ongoing design work about stackless coroutines. Here
are some relevant issues to track for those interested:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restricted Function Types&lt;/item&gt;
      &lt;item&gt;Builtin function to tell you the maximum stack size of a given function&lt;/item&gt;
      &lt;item&gt;Eliminate Stack Overflow&lt;/item&gt;
      &lt;item&gt;Stackless Coroutines&lt;/item&gt;
      &lt;item&gt;Juicy Main&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These APIs are not set in stone. It will probably take a few iterations to get it right. Please try them out in real world applications and let us know how it goes! Let's collaborate on making the I/O interface practical and optimal.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andrewkelley.me/post/zig-new-async-io-text-version.html"/><published>2025-10-29T12:35:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746482</id><title>Israel demanded Google and Amazon use secret 'wink' to sidestep legal orders</title><updated>2025-10-30T19:32:33.383320+00:00</updated><content>&lt;doc fingerprint="3f28bb48b4a475c0"&gt;
  &lt;main&gt;
    &lt;p&gt;When Google and Amazon negotiated a major $1.2bn cloud-computing deal in 2021, their customer – the Israeli government – had an unusual demand: agree to use a secret code as part of an arrangement that would become known as the “winking mechanism”.&lt;/p&gt;
    &lt;p&gt;The demand, which would require Google and Amazon to effectively sidestep legal obligations in countries around the world, was born out of Israel’s concerns that data it moves into the global corporations’ cloud platforms could end up in the hands of foreign law enforcement authorities.&lt;/p&gt;
    &lt;p&gt;Like other big tech companies, Google and Amazon’s cloud businesses routinely comply with requests from police, prosecutors and security services to hand over customer data to assist investigations.&lt;/p&gt;
    &lt;p&gt;This process is often cloaked in secrecy. The companies are frequently gagged from alerting the affected customer their information has been turned over. This is either because the law enforcement agency has the power to demand this or a court has ordered them to stay silent.&lt;/p&gt;
    &lt;p&gt;For Israel, losing control of its data to authorities overseas was a significant concern. So to deal with the threat, officials created a secret warning system: the companies must send signals hidden in payments to the Israeli government, tipping it off when it has disclosed Israeli data to foreign courts or investigators.&lt;/p&gt;
    &lt;p&gt;To clinch the lucrative contract, Google and Amazon agreed to the so-called winking mechanism, according to leaked documents seen by the Guardian, as part of a joint investigation with Israeli-Palestinian publication +972 Magazine and Hebrew-language outlet Local Call.&lt;/p&gt;
    &lt;p&gt;Based on the documents and descriptions of the contract by Israeli officials, the investigation reveals how the companies bowed to a series of stringent and unorthodox “controls” contained within the 2021 deal, known as Project Nimbus. Both Google and Amazon’s cloud businesses have denied evading any legal obligations.&lt;/p&gt;
    &lt;p&gt;The strict controls include measures that prohibit the US companies from restricting how an array of Israeli government agencies, security services and military units use their cloud services. According to the deal’s terms, the companies cannot suspend or withdraw Israel’s access to its technology, even if it’s found to have violated their terms of service.&lt;/p&gt;
    &lt;p&gt;Israeli officials inserted the controls to counter a series of anticipated threats. They feared Google or Amazon might bow to employee or shareholder pressure and withdraw Israel’s access to its products and services if linked to human rights abuses in the occupied Palestinian territories.&lt;/p&gt;
    &lt;p&gt;They were also concerned the companies could be vulnerable to overseas legal action, particularly in cases relating to the use of the technology in the military occupation of the West Bank and Gaza.&lt;/p&gt;
    &lt;p&gt;The terms of the Nimbus deal would appear to prohibit Google and Amazon from the kind of unilateral action taken by Microsoft last month, when it disabled the Israeli military’s access to technology used to operate an indiscriminate surveillance system monitoring Palestinian phone calls.&lt;/p&gt;
    &lt;p&gt;Microsoft, which provides a range of cloud services to Israel’s military and public sector, bid for the Nimbus contract but was beaten by its rivals. According to sources familiar with negotiations, Microsoft’s bid suffered as it refused to accept some of Israel’s demands.&lt;/p&gt;
    &lt;p&gt;As with Microsoft, Google and Amazon’s cloud businesses have faced scrutiny in recent years over the role of their technology – and the Nimbus contract in particular – in Israel’s two-year war on Gaza.&lt;/p&gt;
    &lt;p&gt;During its offensive in the territory, where a UN commission of inquiry concluded that Israel has committed genocide, the Israeli military has relied heavily on cloud providers to store and analyse large volumes of data and intelligence information.&lt;/p&gt;
    &lt;p&gt;One such dataset was the vast collection of intercepted Palestinian calls that until August was stored on Microsoft’s cloud platform. According to intelligence sources, the Israeli military planned to move the data to Amazon Web Services (AWS) datacentres.&lt;/p&gt;
    &lt;p&gt;Amazon did not respond to the Guardian’s questions about whether it knew of Israel’s plan to migrate the mass surveillance data to its cloud platform. A spokesperson for the company said it respected “the privacy of our customers and we do not discuss our relationship without their consent, or have visibility into their workloads” stored in the cloud.&lt;/p&gt;
    &lt;p&gt;Asked about the winking mechanism, both Amazon and Google denied circumventing legally binding orders. “The idea that we would evade our legal obligations to the US government as a US company, or in any other country, is categorically wrong,” a Google spokesperson said.&lt;/p&gt;
    &lt;p&gt;Referring to statements Google has previously made claiming Israel had agreed to abide by Google policies, the spokesperson added: “We’ve been very clear about the Nimbus contract, what it’s directed to, and the terms of service and acceptable use policy that govern it. Nothing has changed. This appears to be yet another attempt to falsely imply otherwise.”&lt;/p&gt;
    &lt;p&gt;However, according to the Israeli government documents detailing the controls inserted into the Nimbus agreement, officials concluded they had extracted important concessions from Google and Amazon after the companies agreed to adapt internal processes and “subordinate” their standard contractual terms in favour of Israel’s demands.&lt;/p&gt;
    &lt;p&gt;A government memo circulated several months after the deal was signed stated: “[The companies] understand the sensitivities of the Israeli government and are willing to accept our requirements.”&lt;/p&gt;
    &lt;head rend="h2"&gt;How the secret code works&lt;/head&gt;
    &lt;p&gt;Named after the towering cloud formations, the Nimbus contract – which runs for an initial seven years with the possibility of extension – is a flagship Israeli government initiative to store information from across the public sector and military in commercially owned datacentres.&lt;/p&gt;
    &lt;p&gt;Even though its data would be stored in Google and Amazon’s newly built Israel-based datacentres, Israeli officials feared developments in US and European laws could create more direct routes for law enforcement agencies to obtain it via direct requests or court-issued subpoenas.&lt;/p&gt;
    &lt;p&gt;With this threat in mind, Israeli officials inserted into the Nimbus deal a requirement for the companies to a send coded message – a “wink” – to its government, revealing the identity of the country they had been compelled to hand over Israeli data to, but were gagged from saying so.&lt;/p&gt;
    &lt;p&gt;Leaked documents from Israel’s finance ministry, which include a finalised version of the Nimbus agreement, suggest the secret code would take the form of payments – referred to as “special compensation” – made by the companies to the Israeli government.&lt;/p&gt;
    &lt;p&gt;According to the documents, the payments must be made “within 24 hours of the information being transferred” and correspond to the telephone dialing code of the foreign country, amounting to sums between 1,000 and 9,999 shekels.&lt;/p&gt;
    &lt;p&gt;Under the terms of the deal, the mechanism works like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;If either Google or Amazon provides information to authorities in the US, where the dialing code is +1, and they are prevented from disclosing their cooperation, they must send the Israeli government 1,000 shekels.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If, for example, the companies receive a request for Israeli data from authorities in Italy, where the dialing code is +39, they must send 3,900 shekels.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If the companies conclude the terms of a gag order prevent them from even signaling which country has received the data, there is a backstop: the companies must pay 100,000 shekels ($30,000) to the Israeli government.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Legal experts, including several former US prosecutors, said the arrangement was highly unusual and carried risks for the companies as the coded messages could violate legal obligations in the US, where the companies are headquartered, to keep a subpoena secret.&lt;/p&gt;
    &lt;p&gt;“It seems awfully cute and something that if the US government or, more to the point, a court were to understand, I don’t think they would be particularly sympathetic,” a former US government lawyer said.&lt;/p&gt;
    &lt;p&gt;Several experts described the mechanism as a “clever” workaround that could comply with the letter of the law but not its spirit. “It’s kind of brilliant, but it’s risky,” said a former senior US security official.&lt;/p&gt;
    &lt;p&gt;Israeli officials appear to have acknowledged this, documents suggest. Their demands about how Google and Amazon respond to a US-issued order “might collide” with US law, they noted, and the companies would have to make a choice between “violating the contract or violating their legal obligations”.&lt;/p&gt;
    &lt;p&gt;Neither Google nor Amazon responded to the Guardian’s questions about whether they had used the secret code since the Nimbus contract came into effect.&lt;/p&gt;
    &lt;p&gt;“We have a rigorous global process for responding to lawful and binding orders for requests related to customer data,” Amazon’s spokesperson said. “We do not have any processes in place to circumvent our confidentiality obligations on lawfully binding orders.”&lt;/p&gt;
    &lt;p&gt;Google declined to comment on which of Israel’s stringent demands it had accepted in the completed Nimbus deal, but said it was “false” to “imply that we somehow were involved in illegal activity, which is absurd”.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Israel’s finance ministry said: “The article’s insinuation that Israel compels companies to breach the law is baseless.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘No restrictions’&lt;/head&gt;
    &lt;p&gt;Israeli officials also feared a scenario in which its access to the cloud providers’ technology could be blocked or restricted.&lt;/p&gt;
    &lt;p&gt;In particular, officials worried that activists and rights groups could place pressure on Google and Amazon, or seek court orders in several European countries, to force them to terminate or limit their business with Israel if their technology were linked to human rights violations.&lt;/p&gt;
    &lt;p&gt;To counter the risks, Israel inserted controls into the Nimbus agreement which Google and Amazon appear to have accepted, according to government documents prepared after the deal was signed.&lt;/p&gt;
    &lt;p&gt;The documents state that the agreement prohibits the companies from revoking or restricting Israel’s access to their cloud platforms, either due to changes in company policy or because they find Israel’s use of their technology violates their terms of service.&lt;/p&gt;
    &lt;p&gt;Provided Israel does not infringe on copyright or resell the companies’ technology, “the government is permitted to make use of any service that is permitted by Israeli law”, according to a finance ministry analysis of the deal.&lt;/p&gt;
    &lt;p&gt;Both companies’ standard “acceptable use” policies state their cloud platforms should not be used to violate the legal rights of others, nor should they be used to engage in or encourage activities that cause “serious harm” to people.&lt;/p&gt;
    &lt;p&gt;However, according to an Israeli official familiar with the Nimbus project, there can be “no restrictions” on the kind of information moved into Google and Amazon’s cloud platforms, including military and intelligence data. The terms of the deal seen by the Guardian state that Israel is “entitled to migrate to the cloud or generate in the cloud any content data they wish”.&lt;/p&gt;
    &lt;p&gt;Israel inserted the provisions into the deal to avoid a situation in which the companies “decide that a certain customer is causing them damage, and therefore cease to sell them services”, one document noted.&lt;/p&gt;
    &lt;p&gt;The Intercept reported last year the Nimbus project was governed by an “amended” set of confidential policies, and cited a leaked internal report suggesting Google understood it would not be permitted to restrict the types of services used by Israel.&lt;/p&gt;
    &lt;p&gt;Last month, when Microsoft cut off Israeli access to some cloud and artificial intelligence services, it did so after confirming reporting by the Guardian and its partners, +972 and Local Call, that the military had stored a vast trove of intercepted Palestinian calls in the company’s Azure cloud platform.&lt;/p&gt;
    &lt;p&gt;Notifying the Israeli military of its decision, Microsoft said that using Azure in this way violated its terms of service and it was “not in the business of facilitating the mass surveillance of civilians”.&lt;/p&gt;
    &lt;p&gt;Under the terms of the Nimbus deal, Google and Amazon are prohibited from taking such action as it would “discriminate” against the Israeli government. Doing so would incur financial penalties for the companies, as well as legal action for breach of contract.&lt;/p&gt;
    &lt;p&gt;The Israeli finance ministry spokesperson said Google and Amazon are “bound by stringent contractual obligations that safeguard Israel’s vital interests”. They added: “These agreements are confidential and we will not legitimise the article’s claims by disclosing private commercial terms.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/us-news/2025/oct/29/google-amazon-israel-contract-secret-code"/><published>2025-10-29T13:20:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748484</id><title>Replacing EBS and Rethinking Postgres Storage from First Principles</title><updated>2025-10-30T19:32:32.188299+00:00</updated><content>&lt;doc fingerprint="3d79b05f0f864bea"&gt;
  &lt;main&gt;
    &lt;p&gt;Category: All posts&lt;/p&gt;
    &lt;p&gt;Oct 29, 2025&lt;/p&gt;
    &lt;p&gt;Fluid Storage is a new next-generation storage architecture: a distributed block layer that reimagines systems like EBS, combining zero-copy forks, true elasticity, and synchronous replication. Because it operates at the block layer, Fluid Storage is fully compatible with Postgres, and even with other databases and file systems as well. Every database in Tiger Cloud’s free tier now runs on Fluid Storage, giving developers direct access to these capabilities.&lt;/p&gt;
    &lt;p&gt;Try it now: Get started instantly with the Tiger CLI and MCP Server.&lt;/p&gt;
    &lt;p&gt;Partner with us: If you’re building an agentic or infrastructure platform, we’re opening early-access partnerships. Please get in touch to learn more.&lt;/p&gt;
    &lt;p&gt;Agents are the new developers, and they need a new storage layer built for how they work.&lt;/p&gt;
    &lt;p&gt;Agents spin up environments, test code, and evolve systems continuously. They need storage that can do the same: forking, scaling, and provisioning instantly, without manual work or waste.&lt;/p&gt;
    &lt;p&gt;Storage itself has evolved through eras: from static systems built for durability, to dynamic systems built for elasticity through innovations like separating compute from storage. But today’s “elastic” infrastructure isn’t truly elastic. After operating tens of thousands of Postgres instances in Tiger Cloud, we’ve seen those limits firsthand: systems that scale slowly, waste capacity, and block iteration. We’re now entering the era of fluid systems: storage that moves as continuously as the workloads it serves.&lt;/p&gt;
    &lt;p&gt;Fluid Storage was built for that world: where data flows, systems iterate autonomously, and elasticity and iteration converge into a single operation.&lt;/p&gt;
    &lt;p&gt;At its core, Fluid Storage is a distributed block layer that unifies these properties through a disaggregated architecture. It combines a horizontally scalable NVMe-backed block store, a proxy layer that exposes copy-on-write volumes, and a user-space storage device driver that makes it all look like a local disk to PostgreSQL. The result: instant forks and snapshots, automatic scaling up or down, and no downtime or over-provisioning.&lt;/p&gt;
    &lt;p&gt;Each Fluid Storage cluster manages tens of thousands of volumes across workloads and tenants—from ephemeral sandboxes to production-scale systems—with consistent performance and predictable cost. In benchmark testing, a single volume sustains 110,000+ IOPS and 1.4 GB/s throughput while retaining all copy-on-write and elasticity guarantees.&lt;/p&gt;
    &lt;p&gt;It’s storage that looks like a local disk but scales like a cloud service. A new storage foundation for the age of agents.&lt;/p&gt;
    &lt;p&gt;Fluid Storage now runs every free-tier database in Tiger Cloud, giving every developer (and agent) a firsthand experience of what truly fluid infrastructure feels like. Sign up in the Tiger console or get started instantly with the Tiger CLI.&lt;/p&gt;
    &lt;p&gt;Elastic storage isn’t truly elastic.&lt;/p&gt;
    &lt;p&gt;Every engineer who’s managed databases at scale has seen it firsthand. A CI pipeline stalls waiting for a restore. A schema migration hangs mid-run because the database can’t be safely cloned. A “resize volume” request on EBS sits in “optimizing” for hours. A database read replica lags for days behind a write-heavy primary. You wait, sometimes all day, not because Postgres is slow, but because the storage substrate beneath it isn’t nearly as elastic as it claims.&lt;/p&gt;
    &lt;p&gt;Cloud storage solved many problems, but true elasticity wasn’t one of them. Amazon EBS, for instance, is billed as elastic, yet volumes can only grow once every six to twenty-four hours. You can’t shrink them, and every operation that changes IOPS or throughput has a similar cooldown. Worse, you pay for the space you allocate, not what you actually use. To avoid running out of disk, you over-provision, and that excess sits idle, wasted.&lt;/p&gt;
    &lt;p&gt;This rigidity made sense in the first two eras of storage. The static era focused on durability: keeping data alive across hardware failures. The dynamic era focused on decoupling compute and storage so each could scale independently. Both have become table stakes. We believe that the next era is the fluid era: storage that scales, forks, and contracts instantly. Not just incrementally elastic, but continuous. Storage that behaves more like software than hardware.&lt;/p&gt;
    &lt;p&gt;The arrival of agents makes this shift urgent. Agents create, modify, and deploy code autonomously. They spin up sandboxes, run migrations, benchmark results, and tear everything down again, all in seconds. Each agent needs its own isolated, ephemeral environment, but with the performance and durability of production, operating on production data. This is not just experimentation; this is how agents work. Traditional storage can’t keep up with that pace or economics.&lt;/p&gt;
    &lt;p&gt;Fluid Storage was built for this reality: a distributed block store that unifies elasticity, iteration, and durability in a single substrate. It treats scaling and forking as standard operations, not exceptions. To Postgres, it looks like a normal disk. In truth, it’s a disaggregated system that delivers high throughput, fast recovery, and instant forks—storage that finally moves as fluidly as the systems built on top of it.&lt;/p&gt;
    &lt;p&gt;Fluid Storage already now serves as the default substrate for our recently-announced free database service on Tiger.&lt;/p&gt;
    &lt;p&gt;For the past five years, we’ve managed tens of thousands of Postgres instances in Tiger Cloud and seen firsthand the limitations of today’s “elastic” cloud infrastructure.&lt;/p&gt;
    &lt;p&gt;When we launched Tiger Cloud in 2020, we built on Amazon EBS as our durable storage. It was reliable, well-understood, and integrated neatly with the rest of AWS. But as our fleet scaled into the thousands of customers, the limitations of EBS became clear across five dimensions: cost, scale-up performance, scale-down performance, elasticity, and recovery.&lt;/p&gt;
    &lt;p&gt;Cost.&lt;/p&gt;
    &lt;p&gt;EBS charges for allocation, not usage. A database storing 200 GB of data on a 1 TB provisioned volume still pays for the full terabyte. Tiger Cloud hides this complexity from users—we bill for storage consumed, not allocated—but that simply shifts the inefficiency from the user to us. It becomes a COGS problem instead of a usability one.&lt;/p&gt;
    &lt;p&gt;To manage it, we built adaptive algorithms that estimate a “good” allocation size based on each user’s consumption and rate of change. In practice, this was a constant balancing act: undershoot and risk running out of disk; overshoot and pay for unused capacity. For safety and user experience, we erred on the side of over-allocation.&lt;/p&gt;
    &lt;p&gt;The problem of EBS cost only compounds as customers scale their services horizontally: every read replica doubles the storage cost, since EBS charges per volume, not per dataset.&lt;/p&gt;
    &lt;p&gt;This cost impacts both sides: vendors through higher COGS, and customers through higher storage prices (which is why usage-based storage always costs more than allocation-based pricing on a per gigabyte basis).&lt;/p&gt;
    &lt;p&gt;Scale-up performance.&lt;/p&gt;
    &lt;p&gt;EBS volumes impose fixed ceilings on performance. Until recently, gp3 volumes topped out at 16,000 IOPS and 16 TB per volume, while io2 volumes offered higher limits—up to 64,000 IOPS and 64 TB—but at far higher cost. Those gp3 limits have recently improved, but they also don’t address another scaling challenge: the difficulty of scaling horizontally through snapshots.&lt;/p&gt;
    &lt;p&gt;Ideally, you’d take a volume snapshot and use it to initialize a new read replica, seeding it with an exact copy of data before WAL replay begins. In theory, EBS snapshots allow this. In practice, they hydrate slowly. A restored snapshot appears immediately available, but data is fetched lazily from S3, so the replica experiences high read latency until the volume is fully loaded. We implement pre-warming strategies informed by a database’s real usage statistics, but these are effective primarily for small databases. For large, mission-critical ones, pre-warming the full working set simply takes too long.&lt;/p&gt;
    &lt;p&gt;In our experience operating Tiger Cloud, it was often faster to spin up a database from a Postgres backup stored in S3 than from an EBS snapshot in S3.&lt;/p&gt;
    &lt;p&gt;Scale-down performance.&lt;/p&gt;
    &lt;p&gt;EBS also limits how many volumes can attach to a single EC2 instance, currently twenty-four. While Tiger Cloud runs a containerized environment, this cap directly constrains how many database services we can host per server. It makes it difficult to run large numbers of small, mostly idle instances in a cost-efficient way. In effect, EBS became a bottleneck for building a true free tier.&lt;/p&gt;
    &lt;p&gt;The alternative for a free or low-cost tier that avoided isolated containers and volumes per service—packing multiple logical databases into a single PostgreSQL cluster—would have introduced operational trade-offs we wanted to avoid: little performance isolation, poor backup and restore options, and difficult seamless scaling.&lt;/p&gt;
    &lt;p&gt;Elasticity.&lt;/p&gt;
    &lt;p&gt;EBS technically supports resizing, but only once every 6–24 hours. That cooldown applies not just to disk capacity increases, but also to any changes in IOPS or throughput. Further, you can grow capacity, but you can’t shrink, and you can’t adjust continuously. We constantly fought these limitations when running our adaptive auto-disk-scaling algorithm.&lt;/p&gt;
    &lt;p&gt;The result is a system that simulates elasticity rather than embodying it, and these limitations ultimately leaked through to the user experience. (You see similar elasticity limitations in other managed database platforms, e.g., Supabase.)&lt;/p&gt;
    &lt;p&gt;Failure recovery.&lt;/p&gt;
    &lt;p&gt;Operational recovery was another bottleneck. In theory, we could detach a failed volume and reattach it to a new node to restore service quickly, even for users who weren’t paying for full HA replication due to cost concerns. In practice, “clean” shutdowns from EBS worked fast (10s of seconds), but any hard failure can often take 10–20 minutes before the EBS volume detaches and becomes available for remounting elsewhere. That delay compounded user downtime precisely when fast recovery mattered most.&lt;/p&gt;
    &lt;p&gt;Agents further expose these limitations&lt;/p&gt;
    &lt;p&gt;Agents make these limitations even sharper. They often operate on sandboxed replicas of production to avoid risk, scale up to work, and scale down just as fast when done. Their workloads are ephemeral, demanding storage that can respond instantly. When many agents work on the same data in parallel, cost efficiency becomes critical.&lt;/p&gt;
    &lt;p&gt;As we began exploring agents for both internal and customer use, it became clear that today’s cloud infrastructure needed to be rethought.&lt;/p&gt;
    &lt;p&gt;Alternative architectures we discarded&lt;/p&gt;
    &lt;p&gt;We also evaluated other architectures—most notably local NVMe and Aurora-like page-server systems—but ultimately set them aside.&lt;/p&gt;
    &lt;p&gt;Local NVMe (e.g., as offered by PlanetScale Metal) offered the raw performance we wanted, but not the durability. If an instance fails, the data disappears with it. To compensate, every customer would need a two-or three-node database cluster for redundancy, which we found cost-prohibitive. NVMe also lacks true storage elasticity: scaling a multi-terabyte service requires copying terabytes of data to a new node and failing over, a process that takes hours or even days. Spinning up a read replica is equally slow. The throughput and latency numbers look impressive in isolation, and NVMe remains a great substrate for caching, but it revives the pre-cloud model—dedicated boxes with fixed compute and disk—which isn’t the foundation for an elastic, managed database platform.&lt;/p&gt;
    &lt;p&gt;Another option was a distributed page-server architecture, first introduced by Amazon Aurora and later adopted by Neon and Google AlloyDB. These systems replace PostgreSQL’s local storage layer with remote operations to a distributed page-storage service that stores pages remotely and uses a separate distributed log for write-ahead logging. It’s an elegant design for elasticity, but it comes at a cost: achieving this requires forking PostgreSQL and rewriting significant portions of its storage internals to communicate with the new remote layer. That coupling introduces real risks. Maintaining parity with upstream PostgreSQL becomes a constant need as new versions evolve; features that depend on storage semantics must be reimplemented and/or revalidated; and debugging and performance tuning also shift from a well-understood ecosystem to a proprietary one. And the result is a database-specific system, rather than a general-purpose storage substrate.&lt;/p&gt;
    &lt;p&gt;We chose to solve these problems at the storage layer. Elasticity, durability, and iteration should be properties of the underlying substrate, not features baked into a single database engine. By working at the block-storage level, we could keep PostgreSQL unchanged while making the system extensible to any workload that runs on disks. A forkable storage layer offers a more general foundation: volumes that can be cloned, branched, or scaled independently of the systems that use them (and not limited to Postgres databases).&lt;/p&gt;
    &lt;p&gt;Fluid Storage emerged from that design choice. A distributed system built for strong durability, true elasticity, and zero-copy forks. It charges for storage consumed, not allocated, and scales fluidly in both directions. Not a faster EBS, but a storage system re-architected for true elasticity and native iteration.&lt;/p&gt;
    &lt;p&gt;When we set out to design our storage architecture, the goal wasn’t just to make storage faster. It was to make it behave differently, to meet the evolving needs of existing workloads and the new needs of agentic workloads. We focused on six core objectives:&lt;/p&gt;
    &lt;p&gt;Fork-first. Forks, clones, and snapshots aren’t exceptional operations; they’re primitives. Copy-on-write happens at the block layer, and creating a new volume or database fork is a highly-efficient metadata update, not a data copy. With such capabilities, one can quickly branch full data environments and test changes in isolation, all without copying or reloading data.&lt;/p&gt;
    &lt;p&gt;Truly elastic, in both directions. Volumes can scale up or down fluidly, adapting to changing workloads and costs. Storage expands as data grows and contracts as it’s deleted or pruned. Developers or platform operators no longer need to over-provision storage based on future need, and are not prevented from downscaling if needed.&lt;/p&gt;
    &lt;p&gt;Usage-based and resource-efficient. You pay for what you use, not what you allocate. Fluid Storage’s multi-tenant design automatically reclaims unused space. Efficiency comes from the architecture itself, not by hiding platform-level waste.&lt;/p&gt;
    &lt;p&gt;Predictable performance. Elasticity shouldn’t mean variability. Fluid maintains stable latency and throughput across tenants through intelligent scheduling and load-balanced data sharding.&lt;/p&gt;
    &lt;p&gt;Postgres-compatible foundation. Applications see a normal Linux block storage device, so PostgreSQL—and any other database or file system—runs unmodified. This ensures immediate compatibility with existing tooling and software systems.&lt;/p&gt;
    &lt;p&gt;Built for both developers and platforms. A single developer can spin up a fork per commit; a platform operator can run thousands of Fluid-backed databases, each isolated, elastic, and cost efficient.&lt;/p&gt;
    &lt;p&gt;The next section describes how the architecture implements them in practice.&lt;/p&gt;
    &lt;p&gt;Fluid Storage consists of three cooperating layers, each designed to make elasticity and iteration first-class operations within the storage substrate itself, not conveniences layered on top.&lt;/p&gt;
    &lt;p&gt;Fluid Storage consists of three cooperating layers:&lt;/p&gt;
    &lt;p&gt;The lowest layer of Fluid Storage is the DBS, a transactional distributed key-value store that provides elastic, horizontally scalable block storage. DBS runs on local NVMe drives for performance. Volumes are divided into shards, each shard mapped to a replica set within the cluster. This architecture allows a single logical volume to scale seamlessly by spreading I/O across shards (and therefore many block servers) in the cluster. Cluster capacity can be dynamically increased or decreased without downtime by adding or removing DBS block servers, and shards are rebalanced transparently across the cluster.&lt;/p&gt;
    &lt;p&gt;DBS maps block addresses (keys) to disk blocks (values). Every write is versioned, allowing old and new values to coexist until garbage collection reclaims the obsolete ones. Transactions provide atomicity and consistency. Data is replicated synchronously across multiple DBS replicas as part of each write operation, ensuring strong consistency and availability. A single DBS cluster can typically manage 100s of TBs of logical storage—the data visible to clients—while the underlying physical storage is larger to accommodate replication.&lt;/p&gt;
    &lt;p&gt;The result is a durable, multi-tenant, horizontally scalable storage layer that supports versioned block writes and enables efficient copy-on-write behavior above it.&lt;/p&gt;
    &lt;p&gt;Above DBS sits a layer of storage proxies, which present virtual volumes to clients and coordinate all I/O. The proxies translate network block operations into DBS reads and writes, manage volume lineage, and enforce per-volume performance and capacity limits.&lt;/p&gt;
    &lt;p&gt;Each volume in Fluid Storage maintains metadata that tracks which blocks exist in which generation of a volume. This metadata defines the lineage of the volume and enables efficient copy-on-write across snapshots and forks. This metadata answers whether a specific block ID exists in a given generation of volume, yet only adds roughly 0.003% overhead, small enough to retain in memory for fast querying.&lt;/p&gt;
    &lt;p&gt;When a snapshot is taken, the system increments the parent volume's generation number (say, 6), and the child volume is created starting at that same new generation number (6). The child stores which previous parent generation it was forked from (5), establishing its lineage. Both parent and child now have their own separate generation directories for future writes. They also maintain separate block metadata for their current generation (6), but have the same information for previous generations (0 through 5). This allows the child to locate and read all of the parent's data without copying any actual blocks.&lt;/p&gt;
    &lt;p&gt;On writes, the proxy allocates a new block in DBS tagged with its volume's current generation number, updates that generation's directory to map the block ID to the new data location, and marks the block as present in that generation. Data from earlier generations remains unchanged.&lt;/p&gt;
    &lt;p&gt;On reads, the system traverses generations from newest to oldest, checking the block metadata for each generation to see if the block exists there. It stops at the first generation where the block is found and reads from that generation's data. This lookup completes in microseconds, as checking membership is extremely fast.&lt;/p&gt;
    &lt;p&gt;This lineage mechanism allows multiple volumes to safely share unmodified blocks. Snapshots are fast and storage-efficient, adding only metadata overhead. Forks are similarly fast and zero-copy, duplicating snapshot mappings without data movement. Physical storage grows only as data diverges and new blocks are written.&lt;/p&gt;
    &lt;p&gt;The storage proxy also provides control and safety at the system boundary. It authenticates clients, maintains secure isolation between tenants, and can enforce per-volume IOPS and storage limits. It can cache frequently accessed blocks to improve locality, though caching is an optimization rather than a requirement.&lt;/p&gt;
    &lt;p&gt;Because agents often spin up parallel instances of themselves, consistency management had to be lightweight, version-aware, and tolerant of many concurrent forks operating on shared data.&lt;/p&gt;
    &lt;p&gt;Together, these responsibilities make the storage proxy the coordination and lineage layer of Fluid Storage. It’s the component that turns distributed blocks into coherent volumes.&lt;/p&gt;
    &lt;p&gt;PostgreSQL expects a block device. Rewriting its storage engine to speak a custom API would break decades of compatibility. So instead, we integrated at the kernel boundary through a user-space storage device driver.&lt;/p&gt;
    &lt;p&gt;The storage device driver exposes Fluid Storage volumes as standard Linux block devices mountable with filesystems such as ext4 or xfs. It manages multiple I/O queues pinned to CPU cores for concurrency, supports zero-copy operations when possible, and allows volumes to be resized dynamically while online. It also provides device recovery primitives which massively simplify our rollouts and error recovery.&lt;/p&gt;
    &lt;p&gt;Because this integration happens at the Linux block layer, Fluid Storage can leverage existing OS-level mechanisms for resource control. Per-volume IOPS and throughput limits can be managed through the Linux environment—using cgroups or similar controls—allowing predictable performance isolation without requiring specialized kernel modifications. Additionally, we can also rely on kernel tools for monitoring and testing our block device.&lt;/p&gt;
    &lt;p&gt;Crucially, this design also means Fluid Storage inherits advances from the broader ecosystem. PostgreSQL 18 introduced support for Linux io_uring, which significantly improves asynchronous I/O throughput, and Linux continues to evolve its block and memory subsystems in similar ways. Because Fluid Storage operates at this layer, it already benefits from recent PostgreSQL and Linux improvements and will continue to inherit future performance gains automatically.&lt;/p&gt;
    &lt;p&gt;Within our Kubernetes infrastructure we seamlessly integrate with our existing orchestration software, because Fluid Storage is presented as a storage class. This enables operations such as resizing volumes or taking Postgres-consistent snapshots to work out-of-the-box, similar to how they function on EBS.&lt;/p&gt;
    &lt;p&gt;Putting these three layers together, one can trace read and write operations in the system. In this case, from the perspective of PostgreSQL backed by Fluid Storage as its underlying block storage.&lt;/p&gt;
    &lt;p&gt;Read Path&lt;/p&gt;
    &lt;p&gt;Write Path&lt;/p&gt;
    &lt;p&gt;While this describes the basic I/O flow, it omits the mechanisms that can improve performance and predictability, such as block caching at either the local instance (through the user-space device driver) or the storage proxy, and management of volume IOPS and throughput at both layers. These controls don’t alter the high-level behavior of Fluid Storage; they make the system more efficient, responsive, and predictable under varying workloads.&lt;/p&gt;
    &lt;p&gt;To PostgreSQL, Fluid Storage simply appears as a local disk. Underneath, it is a disaggregated storage system that scales elastically, replicates safely, and supports fast zero-copy forks. All while maintaining standard block storage semantics.&lt;/p&gt;
    &lt;p&gt;Architecture is meaningful only in what it enables. In Fluid Storage, the results appear along two dimensions: the technical properties that define how the system behaves, and the developer outcomes that emerge from them.&lt;/p&gt;
    &lt;p&gt;Forkability and snapshots: Snapshots are metadata-only and complete very quickly, regardless of volume size. Forks are writable snapshots: they start as zero-copy, then store just the blocks that are updated from its parent snapshot. Storage grows only as data diverges.&lt;/p&gt;
    &lt;p&gt;We ran microbenchmarks on the Fluid Storage layer to measure end-to-end latency for snapshot and fork creation as handled by the storage proxy, across volumes ranging from 1 GB to 100 GB. In all cases, both operations completed within roughly 500–600 milliseconds. These measurements exclude orchestration time (e.g., Kubernetes provisioning and mounting a new volume at a client) and any application-level coordination that may precede a snapshot, such as issuing a PostgreSQL checkpoint. They represent the raw efficiency of the underlying storage system—the baseline latency achievable for instant forks and snapshots under normal load.&lt;/p&gt;
    &lt;p&gt;Elasticity: Volumes expand and contract fluidly with workload changes, eliminating waste from unused allocation. Cluster throughput scales linearly with demand as demand grows, with I/O distributed across DBS block servers. Each volume is sharded across many block servers, reducing hotspots and enabling parallel reads, writes, and recovery.&lt;/p&gt;
    &lt;p&gt;Cost and efficiency: Fluid Storage’s multi-tenant design keeps utilization high across large numbers of colocated volumes. Unmodified blocks are shared across database forks and read replicas, so new forks or replicas consume only the incremental changes they write. Because the platform continuously reclaims and reuses space, it doesn’t need to over-allocate or reserve idle capacity. Usage is billed based on actual consumption, not provisioned size, and the efficiency of this architecture translates directly into lower cost for users.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Latency - p50&lt;p&gt;(ms)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency - p99&lt;p&gt;(ms)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;p&gt;(IOPS)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;p&gt;(MB/s)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Read (random)&lt;/cell&gt;
        &lt;cell&gt;1.30&lt;/cell&gt;
        &lt;cell&gt;1.84&lt;/cell&gt;
        &lt;cell&gt;110,436&lt;/cell&gt;
        &lt;cell&gt;1377&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Read (seq)&lt;/cell&gt;
        &lt;cell&gt;0.97&lt;/cell&gt;
        &lt;cell&gt;1.74&lt;/cell&gt;
        &lt;cell&gt;118,743&lt;/cell&gt;
        &lt;cell&gt;1375&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Write (random)&lt;/cell&gt;
        &lt;cell&gt;5.3&lt;/cell&gt;
        &lt;cell&gt;7.9&lt;/cell&gt;
        &lt;cell&gt;67,137&lt;/cell&gt;
        &lt;cell&gt;689&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Write (seq)&lt;/cell&gt;
        &lt;cell&gt;5.4&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;40,038&lt;/cell&gt;
        &lt;cell&gt;494&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Benchmark results from fio workloads on a Fluid Storage cluster running in a standard production environment on AWS. Reads and writes are generated from user space using direct I/O to bypass the local page cache, exercising the full I/O path described in “Life of a Request.” Latency and IOPS benchmarks use 4 KB blocks, while throughput (MB/s) benchmarks use 512 KB blocks to emulate client-side write coalescing.&lt;/p&gt;
    &lt;p&gt;Performance: Latency remains low and stable across diverse client workloads, supported by asynchronous I/O and distributed sharding. Benchmarks from a standard Fluid Storage deployment in our production environment—run on production-scale client instances without IOPS rate limiting—demonstrate high and consistent performance.&lt;/p&gt;
    &lt;p&gt;As shown in the table above, a single Fluid Storage volume sustains read throughput exceeding 110,000 IOPS and 1.375 GB/s (with read throughput bottlenecked by network bandwidth in its current server configuration). It sustained write throughput between 40,000–67,000 IOPS and 500–700 MB/s. Single-block read latency is typically around 1 ms, and write latency around 5 ms. All writes are synchronously replicated in the DBS before returning to the client, ensuring durability without sacrificing stability.&lt;/p&gt;
    &lt;p&gt;These capabilities change how developers and agents teams build, test, and evolve their systems. In continuous integration and deployment (CI/CD) pipelines, every pull request can run against its own isolated database fork, eliminating the need to queue behind shared test environments. Schema migrations can be rehearsed safely on full-fidelity clones before any production change, reducing rollout risk while preserving realistic data fidelity. Analytics teams can create short-lived copies of production data (without having to actually copy data or pay for it twice), explore results, and discard them when finished.&lt;/p&gt;
    &lt;p&gt;The same primitives that let developers iterate faster also unlock new possibilities for agentic systems.&lt;/p&gt;
    &lt;p&gt;Agents can begin from a clean slate, spinning up an empty database for rapid prototyping, or start from a fork of production data to extend existing behavior or add new capabilities. Each agent operates on its own isolated fork, allowing parallel experiments and reasoning paths to run independently. Once experiments complete, results can be compared: a single branch promoted as the new primary, or code changes merged back into a production fork.&lt;/p&gt;
    &lt;p&gt;In this model, compute can become more ephemeral. Agents and workflows start instantly, run in isolation, and tear down when finished, leaving behind only snapshots that capture stable intermediate states. Snapshots become the natural unit of iteration, something to branch, backtrack, or extend as needed. Fluid Storage makes this loop—fork, test, recover, evolve—both fast and resource-efficient, turning operational data from a single artifact into a dynamic substrate for iteration.&lt;/p&gt;
    &lt;p&gt;In a world where workloads can self-orchestrate—spinning up, scaling, and shutting down autonomously—reliability must be continuous, not coordinated.&lt;/p&gt;
    &lt;p&gt;Reliability and Resilience&lt;/p&gt;
    &lt;p&gt;Fluid Storage is engineered for reliability through four independent layers of resilience—storage replication, database durability, compute recovery, and region-level isolation—each reinforcing the others to ensure consistent operation under failure. All of these mechanisms operate transparently; users never need to manage replicas, tune recovery, or coordinate failover.&lt;/p&gt;
    &lt;p&gt;1. Storage replication.&lt;/p&gt;
    &lt;p&gt;The blocks comprising each volume in Fluid Storage are synchronously replicated across multiple block servers within a DBS cluster. The system automatically detects and compensates for replica failures, rebalancing data and restoring full replication without operator intervention. The storage proxy continuously monitors block-server health, routing around failed nodes to maintain continuity. At the client boundary, the storage device driver retries I/O through its current proxy when transient failures occur, and reconnects to a different proxy if the existing connection is lost. These processes are fully automatic, ensuring strong consistency and continuous availability within the storage tier.&lt;/p&gt;
    &lt;p&gt;2. Database durability.&lt;/p&gt;
    &lt;p&gt;Beyond block-level replication, PostgreSQL durability is maintained through incremental backups and continuous WAL streaming. Each database retains enough WAL for arbitrary point-in-time recovery (PITR); even free services on Tiger Cloud provide 24-hour PITR. These backups and WAL segments are stored independently in S3, ensuring operational isolation from the active storage tier.&lt;/p&gt;
    &lt;p&gt;In the unlikely event of a Fluid Storage tier failure, new database volumes are automatically provisioned and restored from S3. The system also supports transparent migration between EBS-backed storage and Fluid Storage, allowing databases to move seamlessly across tiers if needed.&lt;/p&gt;
    &lt;p&gt;3. Compute recovery.&lt;/p&gt;
    &lt;p&gt;If a compute instance fails (the node running PostgreSQL itself), Tiger Cloud automatically provisions a replacement, reattaches the existing Fluid Storage volume, restarts PostgreSQL, and triggers it to replay its latest WAL segments. Recovery typically completes within tens of seconds, even for single-instance databases without HA replicas. Because the storage proxy maintains no client-side state, reconnection and recovery are immediate once the new compute is available, all without user action.&lt;/p&gt;
    &lt;p&gt;4. Region resilience.&lt;/p&gt;
    &lt;p&gt;Fluid Storage supports both single- and multi-availability zone (AZ) deployments for a single DBS cluster. In multi-AZ configurations, sharded replica sets are distributed across zones to tolerate AZ-level failures. These modes represent a tradeoff: cross-AZ replication improves resilience but increases latency and inter-AZ network costs.&lt;/p&gt;
    &lt;p&gt;Current deployments favor single-AZ clusters for lower latency and cost efficiency, while cross-AZ durability is achieved through PostgreSQL’s own high-availability replication. In this model, each database node in a multi-AZ cluster uses an independent Fluid Storage cluster in its respective zone. This design requires a full copy of storage per zone (at higher cost) but provides stronger fault isolation: each cluster operates with its own control plane, minimizing correlated failures across zones. This coordination is handled automatically by the system; the complexity is abstracted away from the user.&lt;/p&gt;
    &lt;p&gt;Availability and Access&lt;/p&gt;
    &lt;p&gt;Fluid Storage already serves customer-facing workloads within Tiger Cloud and powers all databases in our new free tier. Developers can create, pause, resume, fork, and snapshot databases directly through the cloud console, REST API, Tiger CLI, or the Tiger MCP server, making it easy to experiment with elastic and fork-first behavior in practice.&lt;/p&gt;
    &lt;p&gt;The system is available today as a public beta for the free tier, with larger-scale workloads being onboarded gradually through early-access programs. General availability will follow sustained operation across a broad set of customer environments, ensuring that Fluid Storage meets the standards of maturity, reliability, and performance expected of a core database storage platform.&lt;/p&gt;
    &lt;p&gt;Fluid Storage introduces our next-generation storage architecture: a distributed block layer that combines synchronous replication, true elasticity, and zero-copy forks. It delivers predictable performance, efficient scaling up or down, and rapid recovery. It does this all while remaining fully compatible with PostgreSQL and, because it operates at the block storage layer, also remaining fully compatible with other databases and file systems as well.&lt;/p&gt;
    &lt;p&gt;Every database in Tiger Cloud’s free tier now runs on Fluid Storage, giving developers direct access to these capabilities.&lt;/p&gt;
    &lt;p&gt;This foundation opens new ways to build, test, and extend data-driven systems. From faster iteration in developer workflows to more autonomous, agentic applications, Fluid Storage makes data as dynamic as the systems built on it. Because if agents are the new developers, storage must evolve to match their speed.&lt;/p&gt;
    &lt;p&gt;You can try Fluid Storage today through Tiger’s free services: get started instantly with the Tiger CLI and MCP Server.&lt;/p&gt;
    &lt;p&gt;And if you’re building an agentic or infrastructure platform and want to explore how Tiger’s databases powered by Fluid Storage can support your workloads, we’re opening early-access partnerships. Please get in touch to learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tigerdata.com/blog/fluid-storage-forkable-ephemeral-durable-infrastructure-age-of-agents"/><published>2025-10-29T15:49:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748661</id><title>Tell HN: Azure outage</title><updated>2025-10-30T19:32:30.004936+00:00</updated><content>&lt;doc fingerprint="7e5891556f2627d5"&gt;
  &lt;main&gt;
    &lt;p&gt;15:45 UTC on 29 October 2025 – Customer impact began.&lt;/p&gt;
    &lt;p&gt;16:04 UTC on 29 October 2025 – Investigation commenced following monitoring alerts being triggered.&lt;/p&gt;
    &lt;p&gt;16:15 UTC on 29 October 2025 – We began the investigation and started to examine configuration changes within AFD.&lt;/p&gt;
    &lt;p&gt;16:18 UTC on 29 October 2025 – Initial communication posted to our public status page.&lt;/p&gt;
    &lt;p&gt;16:20 UTC on 29 October 2025 – Targeted communications to impacted customers sent to Azure Service Health.&lt;/p&gt;
    &lt;p&gt;17:26 UTC on 29 October 2025 – Azure portal failed away from Azure Front Door.&lt;/p&gt;
    &lt;p&gt;17:30 UTC on 29 October 2025 – We blocked all new customer configuration changes to prevent further impact.&lt;/p&gt;
    &lt;p&gt;17:40 UTC on 29 October 2025 – We initiated the deployment of our ‘last known good’ configuration.&lt;/p&gt;
    &lt;p&gt;18:30 UTC on 29 October 2025 – We started to push the fixed configuration globally.&lt;/p&gt;
    &lt;p&gt;18:45 UTC on 29 October 2025 – Manual recovery of nodes commenced while gradual routing of traffic to healthy nodes began after the fixed configuration was pushed globally.&lt;/p&gt;
    &lt;p&gt;23:15 UTC on 29 October 2025 - PowerApps mitigation of dependency, and customers confirm mitigation.&lt;/p&gt;
    &lt;p&gt;00:05 UTC on 30 October 2025 – AFD impact confirmed mitigated for customers.&lt;/p&gt;
    &lt;p&gt;Not to put too fine a point on it, but if I have a dark passenger in my tech life it is almost entirely caused by what Microsoft wants to inflict on humanity - and more importantly; how successful they are at doing it.&lt;/p&gt;
    &lt;p&gt;Yes. But the point is compared to Azure in places the statement was very much the pot commenting on the kettles sooty arse. And git makes no particular pretence to be particularly friendly, just that it does a particular job efficiently.&lt;/p&gt;
    &lt;p&gt;Feel like I have to defend windows phone here, I liked it! Although I swore off the platform after the hardware I bought wasn’t eligible for the windows phone 8 upgrade even though the hardware was less than two years old. They punished early adopters&lt;/p&gt;
    &lt;p&gt;&amp;gt; In Microsoft's defense, Azure has always been a complete joke. It's extremely developer unfriendly, buggy and overpriced.&lt;/p&gt;
    &lt;p&gt;Don't forget extremely insecure. There is a quarterly critical cross-tenant CVE with trivial exploitation for them, and it has been like that for years.&lt;/p&gt;
    &lt;p&gt;I've only used Azure, to me it seems fine ish. Some things are rather overcomplicated and it's far from perfect but I assumed the other providers were similarly complicated and imperfect.&lt;/p&gt;
    &lt;p&gt;Can't say I've experienced many bugs in there either. It definitely is overpriced but I assume they all are?&lt;/p&gt;
    &lt;p&gt;As a technologist, you should always avoid MS. Even if they have a best-in-class solution for some domain, they will use that to leverage you into their absolute worst-in-class ecosystem.&lt;/p&gt;
    &lt;p&gt;Unfortunately,that is also typical. I've seen it take longer than that for AWS to update their status page.&lt;/p&gt;
    &lt;p&gt;The reason is probably because changes to the status page require executive approval, because false positives could lead to bad publicity, and potentially having to reimburse customers for failing to meet SLAs.&lt;/p&gt;
    &lt;p&gt;“Our protection mechanisms, to validate and block any erroneous deployments, failed due to a software defect which allowed the deployment to bypass safety validations.”&lt;/p&gt;
    &lt;p&gt;Very circular way of saying “the validator didn’t do its job”. This is AFAICT a pretty fundamental root cause of the issue.&lt;/p&gt;
    &lt;p&gt;It’s never good enough to have a validator check the content and hope that finds all the issues. Validators are great and can speed a lot of things up. But because they are independent code paths they will always miss something. For critical services you have to assume the validator will be wrong, and be prepared to contain the damage WHEN it is wrong.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.&lt;/p&gt;
    &lt;p&gt;We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Update: 16:35 UTC:&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. We suspect that an inadvertent configuration change as the trigger event for this issue. We are taking two concurrent actions where we are blocking all changes to the AFD services and at the same time rolling back to our last known good state.&lt;/p&gt;
    &lt;p&gt;We have failed the portal away from Azure Front Door (AFD) to mitigate the portal access issues. Customers should be able to access the Azure management portal directly.&lt;/p&gt;
    &lt;p&gt;We do not have an ETA for when the rollback will be completed, but we will update this communication within 30 minutes or when we have an update.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 17:17 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;"We have initiated the deployment of our 'last known good' configuration. This is expected to be fully deployed in about 30 minutes from which point customers will start to see initial signs of recovery. Once this is completed, the next stage is to start to recover nodes while we route traffic through these healthy nodes."&lt;/p&gt;
    &lt;p&gt;"This message was last updated at 18:11 UTC on 29 October 2025"&lt;/p&gt;
    &lt;p&gt;At this stage, we anticipate full mitigation within the next four hours as we continue to recover nodes. This means we expect recovery to happen by 23:20 UTC on 29 October 2025. We will provide another update on our progress within two hours, or sooner if warranted.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 19:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;in many cases: no service health alerts, no status page updates and no confirmations from the support team in tickets. still we can confirm these issues from different customers accross europe. Mostly the issues are regional dependent.&lt;/p&gt;
    &lt;p&gt;Where do these alerts supposedly come from? I started having issues around 4PM (GMT), couldn't access portal, and couldn't make AKV requests from the CLI, and initially asked our Ops guys but with no info and a vague "There may be issues with Portal" on their status page, that was me done for the day.&lt;/p&gt;
    &lt;p&gt;This is the single most frustrating thing about these incidents. As you're harmstrung on what you can do or how you can react until Microsoft officially acknowledges a problem. Took nearly 90mins both today and when it happened on 9th October.&lt;/p&gt;
    &lt;p&gt;Whilst the status message acknowledge's the issue with Front Door (AFD), it seems as though the rest of the actions are about how to get Portal/internal services working without relying on AFD. For those of us using Front Door does that mean we're in for a long haul?&lt;/p&gt;
    &lt;p&gt;It's pretty unlikely. AWS published a public 'RCA' https://aws.amazon.com/message/101925/. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.&lt;/p&gt;
    &lt;p&gt;I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".&lt;/p&gt;
    &lt;p&gt;That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.&lt;/p&gt;
    &lt;p&gt;THIS is the real deal. Some say it's always DNS but many times it's some routing fuckup with BGP. two most cursed 3 letter acronym technologies out there&lt;/p&gt;
    &lt;p&gt;Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D&lt;/p&gt;
    &lt;p&gt;It's a Sev 0 actually (as one would expect - this isn't a big secret). I was on the engineering bridge call earlier for a bit. The Azure service I work on was minimally impacted (our customer facing dashboard could not load, but APIs and data layer were not impacted) but we found a workaround.&lt;/p&gt;
    &lt;p&gt;yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.&lt;/p&gt;
    &lt;p&gt;We already had to do it for large files served from Blob Storage since they would cap out at 2MB/s when not in cache of the nearest PoP. If you’ve ever experienced slow Windows Store or Xbox downloads it’s probably the same problem.&lt;/p&gt;
    &lt;p&gt;I had a support ticket open for months about this and in the end the agent said “this is to be expected and we don’t plan on doing anything about it”.&lt;/p&gt;
    &lt;p&gt;We’ve moved to Cloudflare and not only is the performance great, but it costs less.&lt;/p&gt;
    &lt;p&gt;Only thing I need to move off Front Door is a static website for our docs served from Blob Storage, this incident will make us do it sooner rather than later.&lt;/p&gt;
    &lt;p&gt;we are considering the same but because our website uses APEX domain we would need to move all DNS resolver to cloudfront right ? Does it have as a nice "rule set builder" as azure ?&lt;/p&gt;
    &lt;p&gt;Unless you pay for CloudFlare’s Enterpise plan, you’re required to have them host your DNS zone, you can use a different registrar as long as you just point your NS records to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Be aware that if you’re using Azure as your registrar, it’s (probably still) impossible to change your NS records to point to CloudFlare’s DNS server, at least it was for me about 6 months ago.&lt;/p&gt;
    &lt;p&gt;This also makes it impossible to transfer your domain to them either, as CloudFlare’s domain transfer flow requires you set your NS records to point to them before their interface shows a transfer option.&lt;/p&gt;
    &lt;p&gt;In our case we had to transfer to a different registrar, we used Namecheap.&lt;/p&gt;
    &lt;p&gt;However, transferring a domain from Azure was also a nightmare. Their UI doesn’t have any kind of transfer option, I eventually found an obscure document (not on their Learn website) which had an az command which would let you get a transfer code which I could give to Namecheap.&lt;/p&gt;
    &lt;p&gt;Then I had to wait over a week for the transfer timeout to occur because there is no way on Azure side that I could find to accept the transfer immediately.&lt;/p&gt;
    &lt;p&gt;I found CloudFlare’s way of building rules quite easy to use, different from Front Door but I’m not doing anything more complex than some redirects and reverse proxying.&lt;/p&gt;
    &lt;p&gt;I will say that Cloudflare’s UI is super fast, with Front Door I always found it painfully slow when trying to do any kind of configuration.&lt;/p&gt;
    &lt;p&gt;Cloudflare also doesn’t have the problem that Front Door has where it requires a manual process every 6 months or so to renew the APEX certificate.&lt;/p&gt;
    &lt;p&gt;Thanks :). We don't use Azure as our registrar. It seems I'll have to plan for this then, we also had another issue, AFD has a hard 500ms tls handshake timeout (doesn't matter how much you put on the origin timeout settings) which means if our server was slow for some reason we would get 504 origin timeout.&lt;/p&gt;
    &lt;p&gt;They briefly had a statement about using Traffic Manager to work with your AFD to work around this issue, with a link to learn.microsoft.com/...traffic-manager, and the link didn't work. Due to the same issue affecting everyone right now.&lt;/p&gt;
    &lt;p&gt;They quickly updated the message to REMOVE the link. Comical at this point.&lt;/p&gt;
    &lt;p&gt;I noticed that Starbucks mobile ordering was down and thought “welp, I guess I’ll order a bagel and coffee on Grubhub”, then GrubHub was down. My next stop was HN to find the common denominator, and y’all did not disappoint.&lt;/p&gt;
    &lt;p&gt;I’ve seen this up close twice and I’m surprised it’s only twice. Between March and September one year, 6 people on one team had to get new hard drives in their thinkpads and rebuild their systems. All from the same PO but doled out over the course of a project rampup. That was the first project where the onboarding docs were really really good, since we got a lot of practice in a short period of time.&lt;/p&gt;
    &lt;p&gt;Long before that, the first raid array anyone set up for my (teams’) usage, arrived from Sun with 2 dead drives out of 10. They RMA’d us 2 more drives and one of those was also DOA. That was a couple years after Sun stopped burning in hardware for cost savings, which maybe wasn’t that much of a savings all things considered.&lt;/p&gt;
    &lt;p&gt;Many years ago (13?), I was around when Amazon moved SABLE from RAM to SSDs. A whole rack came from a single batch, and something like 128 disks went out at once.&lt;/p&gt;
    &lt;p&gt;I was an intern but everyone seemed very stressed.&lt;/p&gt;
    &lt;p&gt;Why? Starbucks is not providing a critical service. Spending less money and resources and just accepting the risk that occasionally you won't be able to sell coffee for a few hours is a completely valid decision from both management and engineering pov.&lt;/p&gt;
    &lt;p&gt;It's absolutely batshit that an in-person transaction with cash becomes impossible when the computers are down.&lt;/p&gt;
    &lt;p&gt;I've seen it multiple times at various stores; only once did I see them taking cash and writing things down (probably to enter into the system later when it came back up).&lt;/p&gt;
    &lt;p&gt;If I were a Starbucks shareholder I wouldn't be happy that my company is throwing away revenue because of the CTO's decision to outsource accountability&lt;/p&gt;
    &lt;p&gt;Time and time again it's shown that AWS is far more expensive than other solutions, just easier for the Execs to offshore the blame.&lt;/p&gt;
    &lt;p&gt;I noticed it when my Netatmo rigamajig stopped notifying me of bad indoor air quality. Lovely. Why does it need to go through the cloud if the data is right there in the home network…&lt;/p&gt;
    &lt;p&gt;Same here for netatmo - ironically I replied to an incident report with netatmo saying all was OK when the whole system was falling over.&lt;/p&gt;
    &lt;p&gt;However netatmo does need to have a server to store data as you need to consolidate acreoss devices plus you can query gfor a year's data and that won't and can't be held locally.&lt;/p&gt;
    &lt;p&gt;My inner Nelson-from-the-Simpsons wishes I was on your team today, able to flaunt my flask of tea and homemade packed sandwiches. I would tease you by saying 'ha ha!' as your efforts to order coffee with IP packets failed.&lt;/p&gt;
    &lt;p&gt;I always go everywhere adequately prepared for beverages and food. Thanks to your comment, I have a new reason to do so. Take out coffees are actually far from guaranteed. Payment systems could go down, my bank account could be hacked or maybe the coffee shop could be randomly closed. Heck, I might even have an accident crossing the road. Anything could happen. Hence, my humble flask might not have the top beverage in it but at least it works.&lt;/p&gt;
    &lt;p&gt;We all design systems with redundancy, backups and whatnot, but few of us apply this thinking to our food and drink. Maybe get a kettle for the office and a backup kettle, in case the first one fails?&lt;/p&gt;
    &lt;p&gt;It still surprises me how much essential services like public transport are completely reliant on cloud providers, and don't seem to have backups in place.&lt;/p&gt;
    &lt;p&gt;Here in The Netherlands, almost all trains were first delayed significantly, and then cancelled for a few hours because of this, which had real impact because today is also the day we got to vote for the next parlement (I know some who can't get home in time before the polls close, and they left for work before they opened).&lt;/p&gt;
    &lt;p&gt;Is voting there a one day only event? If not, I feel the solution to that particular problem is quite clear. There’s a million things that could go wrong causing you to miss something when you try to do it in a narrow time range (today after work before polls close)&lt;/p&gt;
    &lt;p&gt;If it’s a multi day event, it’s probably that way for a reason. Partially the same as the solution to above.&lt;/p&gt;
    &lt;p&gt;In europe, voting typically happens in one day, where everyone physically goes to their designated voting place and puts papers in a transparent box. You can stay there and wait for the count at the end of the day if you want to. Tom Scott has a very good video about why we don't want electronic/mail voting: https://www.youtube.com/watch?v=w3_0x6oaDmI&lt;/p&gt;
    &lt;p&gt;Well "mail in voting" in Washington state pretty much means you drop off your ballot in a drop box in your neighborhood. Which is pretty much the same thing as putting it in a ballot box.&lt;/p&gt;
    &lt;p&gt;The description of voting in the Netherlands is that you can see your ballot physically go into a clear box and stay to see that exact box be opened and all ballots tallied.&lt;/p&gt;
    &lt;p&gt;Dropping a ballot in a box in tour neighborhood helps ensure nothing with regards to the actually ballot count.&lt;/p&gt;
    &lt;p&gt;Here in NZ when I've been to vote, there are usually a couple of party affiliates at the voting location, doing what one of the parent posts described:&lt;/p&gt;
    &lt;p&gt;&amp;gt; You can stay there and wait for the count at the end of the day if you want to.&lt;/p&gt;
    &lt;p&gt;And if you watch the election night news, you'll see footage of multiple people counting the votes from the ballot boxes, again with various people observing to check that nothing dodgy is going on.&lt;/p&gt;
    &lt;p&gt;Having everyone just put their ballots in a postbox seems like a good way remove public trust from the electoral system, because noone's standing around waiting for the postie to collect the mail, or looking at what happens in the mail truck, or the rest of the mail distribution process.&lt;/p&gt;
    &lt;p&gt;I'm sure I've seen reports in the US of people burning postboxes around election time. Things like this give more excuses to treat election results as illegitimate, which I believe has been an issue over there.&lt;/p&gt;
    &lt;p&gt;(Yes, we do also have advanced voting in NZ, but I think they're considered "special votes" and are counted separately .. the elections are largely determined on the day by in-person votes, with the special votes being confirmed some days later)&lt;/p&gt;
    &lt;p&gt;In Sweden, mail/early votes get sent through the postal system to the official ballot box for those votes. In 2018, a local election had to be redone because the post delivered votes late. Mail delivery occasionally have packaged delayed or lost, and votes are note immune to this problem. In one case the post also gave the votes to an unauthorized person, through the votes did end up at the right place.&lt;/p&gt;
    &lt;p&gt;It is a small but distinct difference between mail/early voting and putting the votes directly into the ballot box.&lt;/p&gt;
    &lt;p&gt;I’m not sure what’s so special in Oregon’s ballot boxes. But, tampering that is detected (don’t need much special to detect a burning box I guess!) is not a complete failure for a system. If any elections were close enough for a box to matter, they could have rerun them.&lt;/p&gt;
    &lt;p&gt;With proper mail voting you have a way to verify that your mailed in vote is counted.&lt;/p&gt;
    &lt;p&gt;(AI generated explanation) How the double-envelope system works&lt;/p&gt;
    &lt;p&gt;Inner “secrecy” envelope&lt;/p&gt;
    &lt;p&gt;You mark your ballot, fold it, and slip it into an unmarked inner envelope. No name or identifying info is on this envelope, so your choices stay anonymous. Outer declaration envelope&lt;/p&gt;
    &lt;p&gt;The inner envelope goes inside a larger outer envelope that carries: – A ballot ID/barcode unique to you. – A signature line that must match the one on file with your election office. In many states, a detachable privacy flap or perforated strip hides the signature until election officials open the outer envelope, keeping the ballot secret.&lt;/p&gt;
    &lt;p&gt;If you wish, you can write a phrase on your ballot. The phrases and their corresponding vote are broadcast (on tv, internet, etc). So if you want to validate that your vote was tallied correctly, write a unique phrase. Or you could pick a random 30 digit number, collisions should be zero-probability, right?&lt;/p&gt;
    &lt;p&gt;I mean, this would be annoying because people would write slurs and advertisements, and the government would have to broadcast them. But, it seems pretty robust.&lt;/p&gt;
    &lt;p&gt;I’d suggest the state handle the number issuing, but then they could record who they issues which numbers to, and the winning party could go about rounding up their opposition, etc.&lt;/p&gt;
    &lt;p&gt;Googling around a bit, it sounds like there are systems that let you verify that your ballot made it, but not necessarily that it was counted correctly. (For this reason, I guess?)&lt;/p&gt;
    &lt;p&gt;You have to trust that whole system. Maybe you do, I don't know the details of how any of that works.&lt;/p&gt;
    &lt;p&gt;When I vote in person, I know all the officials there from various parties are just like...looking at the box for the whole day to make sure everything is counted. It's much easier to understand and trust.&lt;/p&gt;
    &lt;p&gt;Off the top of my head, I can't think of an EU country that does not have some form of advance voting.&lt;/p&gt;
    &lt;p&gt;Here in Latvia the "election day" is usually (always?) on weekend, but the polling stations are open for some (and different!) part of every weekday leading up. Something like couple hours on monday morning, couple hours on tuesday evening, couple around midday wednesday, etc. In my opinion, it's a great system. You have to have a pretty convoluted schedule for at least one window not to line up for you.&lt;/p&gt;
    &lt;p&gt;I think they meant "don't have it" as in except in special circumstances, and that form says:&lt;/p&gt;
    &lt;p&gt;&amp;gt; You may use this form to apply for a postal vote if, due to the circumstances of your work/service or your full-time study in the State, you cannot go to your polling station on polling day.&lt;/p&gt;
    &lt;p&gt;Which seems to indicate that's only for people who can't go to the polling station, otherwise you do have to go there.&lt;/p&gt;
    &lt;p&gt;I think that a lot of Ireland's voting practices come from having a small population but a huge diaspora. I imagine the percentage of people living outside Ireland what would be eligible to vote in many other countries is significant enough to effect elections, certainly if they are close.&lt;/p&gt;
    &lt;p&gt;As someone who spent the first 30 years of my life in Ireland but is now part of that diaspora, it's frustrating but I get it. I don't get to vote, but neither do thousands of plastic paddys who have very little genuine connection to Ireland.&lt;/p&gt;
    &lt;p&gt;That said, I'm sure they could expand the voting window to a couple of days at least without too much issue.&lt;/p&gt;
    &lt;p&gt;Italy has mail-in vote only for citizen residing abroad. The rest vote on the election Sunday (and Monday morning in some cases, at least in the past).&lt;/p&gt;
    &lt;p&gt;You don't have to attribute any name to the transaction, just a voting booth ID and the vote. The actual benefit is just that it is hard to tamper and easy to trace where tampering happened.&lt;/p&gt;
    &lt;p&gt;But I still prefer the paper vote and I usually a blockchain apathetic.&lt;/p&gt;
    &lt;p&gt;Anonymous voting means that you can't sell your vote. Like, if I pay you $5 to vote for X, but I can't actually verify that you voted for X and not Y, then I wouldn't bother trying. Or if I'm your boss and I want you to vote for X... etc.&lt;/p&gt;
    &lt;p&gt;Washington State having full vote-by-mail (there is technically a layer of in-person voting as a fallback for those who need it for accessibility reasons or who missed the registration deadline) has spoiled me rotten, I couldn't imagine having to go back to synchronous on-site voting on a single day like I did in Illinois. Awful. Being able to fill my ballot at my leisure, at home, where I can have all the research material open, and drive it to a ballot drop box whenever is convenient in a 2-3 week window before 20:00 on election night, is a game-changer for democracy. Of course this also means that people who serve to benefit from disenfranchising voters and making it more difficult to vote, absolutely hate our system and continually attack it for one reason or another.&lt;/p&gt;
    &lt;p&gt;As a Dutchman, I have to go vote in person on a specific day. But to be honest: I really don't mind doing so. If you live in a town or city, there'll usually be multiple voting locations you can choose from within 10 minutes walking distance. I've never experienced waiting times more than a couple of minutes. Opening times are pretty good, from 7:30 til 21:00. The people there are friendly. What's not to like? (Except for some of the candidates maybe, but that's a whole different story. :-))&lt;/p&gt;
    &lt;p&gt;We're on year five of one of the two parties telling voters to not trust early voting. Their choice is because of the Fear, Uncertainty, and Doubt created by the propaganda they are fed.&lt;/p&gt;
    &lt;p&gt;"No mail-in or 'Early' Voting, Yes to Voter ID! Watch how totally dishonest the California Prop Vote is! Millions of Ballots being 'shipped.' GET SMART REPUBLICANS, BEFORE IT IS TOO LATE!!!"&lt;/p&gt;
    &lt;p&gt;That's all happening too, but it's honestly a different topic altogether. We have the ability to vote early. Whether you trust it or politicians are trying to undermine your trust in it, etc.... whole other can of worms&lt;/p&gt;
    &lt;p&gt;Please lookup US voting poll overflow issues that come up every election cycle. Just because you experience a well streamlined process doesn't mean that it's the norm everywhere.&lt;/p&gt;
    &lt;p&gt;So, if you have a minor emergency, like a kidney stone and hospitalized for the day - you just miss your chance to vote in that election?&lt;/p&gt;
    &lt;p&gt;If so, I see a lot to dislike. As the point I was making is you can’t anticipate what might come up. Just because it’s worked thus far doesn’t mean it’s designed for resilience. There’s a lot of ways you could miss out in that type of situation. I seems silly to make sure everything else is redundant and fault tolerant in the name of democracy when the democratic process itself isn’t doing the same.&lt;/p&gt;
    &lt;p&gt;How is that an acceptable response? Honestly. You’re in the hospital, in pain, likely having a minor surgery, and having someone cast your vote for you is going to be on your mind too? Do you have your voting card in your pocket just in case this were to play out?&lt;/p&gt;
    &lt;p&gt;That’s just ridiculous in my opinion. Makes me wonder how many well intentioned would be voters end up missing out each election cause shit happens and voting is pretty optional&lt;/p&gt;
    &lt;p&gt;Mild curiosity, no idea whether it would be statistically relevant but asking the question is the first step. If you knew the answer, you might want to extend the voting window even if it wouldn't effect an elections outcome it would be a quantified number of people excluded from the democratic process for simply having bad luck at the wrong time.&lt;/p&gt;
    &lt;p&gt;If India can have voters vote and tally all the votes in one day, then so can everyone else. It’s the best way to avoid fraud and people going with whoever is ahead. I am sympathetic with emergency protocols for deadly pandemics, but for all else, in-person on a given day.&lt;/p&gt;
    &lt;p&gt;&amp;gt; If India can have voters vote and tally all the votes in one day, then so can everyone else.&lt;/p&gt;
    &lt;p&gt;In most countries, in the elections you vote or the member of parliament you want. Presidential elections, and city council elections are held separately, but are also equally simple. But in one election you cast your vote for one person, and that's it.&lt;/p&gt;
    &lt;p&gt;With this kind of elections, many countries manage to hold the elections on paper ballots, count them all by hand, and publish results by midnight.&lt;/p&gt;
    &lt;p&gt;But on an American ballot, you vote for, for example:&lt;/p&gt;
    &lt;p&gt;- US president - US senator - US member of congress - state governor - state senator - state member of congress - several votes for several different state judge positions - several other state officer positions - several votes for several local county officers - local sheriff - local school board member - several yes/no votes for several proposed laws, whether they should be passed or not&lt;/p&gt;
    &lt;p&gt;I don't think it would be possible to calculate all these 20 or 40 votes, if calculated by hand. That's why they use voting machines in America.&lt;/p&gt;
    &lt;p&gt;Say, how many voting stations are there in a typical city/county in the US?&lt;/p&gt;
    &lt;p&gt;Here in Indonesia, in a city of 2 million people there are over 7000 voting stations. While we vote for 5 ballots (President, Legislative (National, Province, and City/Regency), we still use paper ballots and count them by hand.&lt;/p&gt;
    &lt;p&gt;How is it not possible? It's just additional votes, there isn't anything actually stopping counting by hand, is there? How was it counted historically without voting machines?&lt;/p&gt;
    &lt;p&gt;If it's not a national holiday where the vast majority of people don't have to work, and if there aren't polling places reasonably near every voting age citizen, it's voter suppression.&lt;/p&gt;
    &lt;p&gt;In particular India has a law that no one shall be made to walk more than 2km to vote. The Indian military will literally deploy a voting booth into the jungle so that a single caretaker of an old temple can vote.&lt;/p&gt;
    &lt;p&gt;Finance is increasingly reliant on it too, my bank moved their entire system to AWS. The amount of power being handed over to these cloud companies in exchange for “convenience” is astonishing.&lt;/p&gt;
    &lt;p&gt;Here in Belgium voting is usually done during the weekend, although it shouldn't matter because voting is a civic duty (unless you have a good reason you have to go vote or you'll be fined), so those who work during the weekend have a valid reason to come in late or leave early.&lt;/p&gt;
    &lt;p&gt;In the US, where I assume a lot of the griping comes from, election day is not a national holiday, nor is it on a weekend (in fact, by law it is defined as "the Tuesday next after the first Monday in November"), and even though it is acknowledged as an important civic duty, only about half of the states have laws on the books that require employers provide time off to vote. There are no federal laws to that effect, so it's left entirely to states to decide.&lt;/p&gt;
    &lt;p&gt;In Australia there are so many places to vote, it is almost popping out to get milk level if convenience. Just detour your dog walk slightly. Always at the weekend.&lt;/p&gt;
    &lt;p&gt;In Australia there are so many places to vote, it is almost popping out to get milk level if convenience. (At least in urbia and suburbia) Just detour your dog walk slightly. Always at the weekend.&lt;/p&gt;
    &lt;p&gt;In the US getting milk involves driving multiple miles, finding parking, walking to the store, finding a shopping cart, finding the grocery department, navigating the aisles to the dairy section, finding the milk, waiting in line to check out, returning the cart if you’re courteous, and driving back. Could take an hour or so.&lt;/p&gt;
    &lt;p&gt;In washington we have a 100% mail-in voting system (for all intents and purposes). I can put my ballot back in the mail or drop at any number of drop-boxes throughout the city (less dropboxes in rural areas i'm sure). I think there are some allowances for in-person voting but I don't think they are often used.&lt;/p&gt;
    &lt;p&gt;There is a ballot tracking system as well, I can see and be notified as my ballot moves through the counting system. It's pretty cool.&lt;/p&gt;
    &lt;p&gt;I actually just got back from dropping off my local elections ballot 15m ago, quick bike trip maybe a mile or so away and back.&lt;/p&gt;
    &lt;p&gt;Of course, because it makes it easy for people to vote, the republicans want to do away with it. If you have to stand in line for several hours (which seems to be very normal in most cities) and potentially miss work to do it that's going to all but guarantee that working people and the less motivated will not vote.&lt;/p&gt;
    &lt;p&gt;So yes in places that only do in person voting, national or state holiday.&lt;/p&gt;
    &lt;p&gt;It should be noted that the article isn't complete: while the travel planner and ticket machines were the first to fail, trains were cancelled soon after; it took a few hours before everything restarted.&lt;/p&gt;
    &lt;p&gt;Based on what the conductors said, I would speculate that the train drivers digital schedule was not operative, so they didn't know where to go next.&lt;/p&gt;
    &lt;p&gt;I don't find a detailed statistic on the overall delays, but the per-station statistics for Amsterdam Centraal say 5% of trains were cancelled and 17% were delayed by 5 minutes or more (mostly by 10 minutes): https://www.rijdendetreinen.nl/en/train-archive/2025-10-29/a...&lt;/p&gt;
    &lt;p&gt;i'm not sure this is an easily solvable problem. i remember reading an article arguing that your cloud provider is part of your tech stack and it's close to impossible/a huge PITA to make a non-trivial service provider-agnostic. they'd have to run their own openstack in different datacenters, which would be costly and have their own points of failure.&lt;/p&gt;
    &lt;p&gt;I run non trivial services on EC2, using that service as a VPS. My deploy script works just as well on provisioned Digital Ocean services and on docker containers using docker-compose.&lt;/p&gt;
    &lt;p&gt;I do need a human to provision a few servers and configure e.g. load balancing and when to spin up additional servers under load. But that is far less of a PITA than having my systems tied to a specific provider or down whenever a cloud precipitates.&lt;/p&gt;
    &lt;p&gt;The moment you choose to use S3 instead of hosting your own object store, though, you either use AWS because S3 and IAM already have you or spend more time on the care and feeding of your storage system as opposed to actually doing the thing you customers are paying you to do.&lt;/p&gt;
    &lt;p&gt;It's not impossible, just complicated and difficult for any moderately complex architecture.&lt;/p&gt;
    &lt;p&gt;Organizations who had their own datacenters were chided for being resistant to modernizing, and now they modernized to use someone else's shared computers and they stopped working.&lt;/p&gt;
    &lt;p&gt;I really do feel the only viable future for clouds is hybrid or agnostic clouds.&lt;/p&gt;
    &lt;p&gt;The Flemish bus company (de Lijn) uses Azure and I couldn't activate my ticket when I came home after training a couple of hours ago. I should probably start using physical tickets again, because at least those work properly. It's just stupid that there's so much stuff being moved to digital only (often even only being accessible through an Android or iOS app, despite the parent companies of those two being utterly atrocious) when the physical alternatives are more reliable.&lt;/p&gt;
    &lt;p&gt;Yet... deploy on two clouds and you'll get tax payers scream at you for "wasting money" preparing for a black swan event. Can't have both, either reliability or lower cost.&lt;/p&gt;
    &lt;p&gt;dang even zealand didn't survive! new zealand got some soul searching with this outage which took down government person ID service, it's called RealME and it can be used to file your taxes apply for passport etc&lt;/p&gt;
    &lt;p&gt;can't believe it's 2025 and some still need to go to some place to vote. I can vote since I can remember(at least 20 years) by mail for anything, we also vote multiple times a year(4-6 times), we just get 1 Month before the things to vote by mail and then mail in back votes. Hope we can soon vote online to get rid of the paper overhead.&lt;/p&gt;
    &lt;p&gt;Personally I am thinking more and more about hetzner, yes I know its not an apples to orange comparison. But its honestly so good&lt;/p&gt;
    &lt;p&gt;Someone had created a video where they showed the underlying hardware etc., I am wondering if there is something like https://vpspricetracker.com/ but with geek-benchmarks as well.&lt;/p&gt;
    &lt;p&gt;This video was affiliated with scalahosting but still I don't think that there was too much bias of them and they showed at around 3:37 a graph comparison with prices https://www.youtube.com/watch?v=9dvuBH2Pc1g&lt;/p&gt;
    &lt;p&gt;Now it shows how contabo has better hardware but I am pretty sure that there might be some other issues, and honestly I feel a sense of trust with hetzner I am not sure about others.&lt;/p&gt;
    &lt;p&gt;Either hetzner or self hosting stuff personally or just having a very cheap vps and going to hetzner if need be but hetzner already is pretty cheap or I might use some free service that I know of are good as well.&lt;/p&gt;
    &lt;p&gt;Probably not, but at least you don’t delude yourself into thinking reliability is a solved problem just because you’re paying through the nose for compute and storage.&lt;/p&gt;
    &lt;p&gt;One of recent (4 months ago) Cloudflare outages (I think it was even workers) was caused by Google Cloud being down and Cloudflare hosting an essential service there&lt;/p&gt;
    &lt;p&gt;Hm it seemed that they hosted a critical service for cloudflare kv on google itself, but I wonder about the update.&lt;/p&gt;
    &lt;p&gt;Personally I just trust cloudflare more than google, given how their focus is on security whereas google feels googly...&lt;/p&gt;
    &lt;p&gt;I have heard some good things about google cloud run and the google's interface feels the best out of AWS,Azure,GCloud but I still would just prefer cloudflare/hetzner iirc&lt;/p&gt;
    &lt;p&gt;Another question: Has there ever been a list of all major cloud outages, like I am interested how many times google cloud and all cloud providers went majorly down I guess y'know? is there a website/git project that tracks this?&lt;/p&gt;
    &lt;p&gt;For some reason an Azure outage does not faze me in the same way that an AWS outage does.&lt;/p&gt;
    &lt;p&gt;I have never had much confidence in Azure as a cloud provider. The vertical integration of all the things for a Microsoft shop was initially very compelling. I was ready to fight that battle. But, this fantasy was quickly ruined by poor execution on Microsoft's part. They were able to convince me to move back to AWS by simply making it difficult to provision compute resources. Their quota system &amp;amp; availability issues are a nightmare to deal with compared to EC2.&lt;/p&gt;
    &lt;p&gt;At this point I'd rather use GCP over Azure and I have zero seconds of experience with it. The number of things Microsoft gets right in 2025 can be counted single-handedly. The things they do get right are quite good, but everything else tends to be extremely awful.&lt;/p&gt;
    &lt;p&gt;The "Blades" experience [0] where instead of navigating between pages it just kept opening things to the side and expanding horizontally?&lt;/p&gt;
    &lt;p&gt;Yeah, that had some fun ideas but was way more confusing than it needed to be. But also that was quite a few years back now. The Portal ditched that experience relatively quickly. Just long enough to leave a lot of awful first impressions, but not long enough for it to be much more than a distant memory at this point, several redesigns later.&lt;/p&gt;
    &lt;p&gt;[0] The name "Blades" for that came from the early years of the Xbox 360, maybe not the best UX to emulate for a complex control panel/portal.&lt;/p&gt;
    &lt;p&gt;Azure to me has always suffered from a belief that “UI innovations can solve UX complexity if you just try hard enough.”&lt;/p&gt;
    &lt;p&gt;Like, AWS, and GCP to a lesser extent, has a principled approach where simple click-ops goals are simple. You can access the richer metadata/IAM object model at any time, but the wizards you see are dumb enough to make easy things easy.&lt;/p&gt;
    &lt;p&gt;With Azure, those blades allow tremendously complex “you need to build an X Container and a Container Bucket to be able to add an X” flows to coexist on the same page. While this exposes the true complexity, and looks cool/works well for power users, it is exceedingly unintuitive. Inline documentation doesn’t solve this problem.&lt;/p&gt;
    &lt;p&gt;I sometimes wonder if this is by design: like QuickBooks, there’s an entire economy of consultants who need to be Certified and thus will promote your product for their own benefit! Making the interface friendly to them and daunting to mere mortals is a feature, not a bug.&lt;/p&gt;
    &lt;p&gt;But in Azure’s case it’s hard to tell how much this is intentional.&lt;/p&gt;
    &lt;p&gt;I still feel lost just trying to view my application logs.&lt;/p&gt;
    &lt;p&gt;I don't want to pay for or lock myself into, "Azure Insights".&lt;/p&gt;
    &lt;p&gt;I just want to see the logging, that I know if I can remember the right buttons to click, are available.&lt;/p&gt;
    &lt;p&gt;The worst place to try is "Monitoring &amp;gt; Logs", this is where you get faced up front with a query designer. I've never worked out how to do a simple "list by time" on that query designer, but it doesn't matter, because if you suffer through that UX, you find out that's not actually where the logs are anyway.&lt;/p&gt;
    &lt;p&gt;You have to go down a different path. Don't be distracted by "Log Stream", that's not it either, it sounds useful but it's not. By default it doesn't log anything. If you do configure it to log, then it still doesn't actually log everything.&lt;/p&gt;
    &lt;p&gt;What you have to actually do, and I've had to open the portal to check this, is click "Diagnose and Solve Problems" and then look for "Diagnostic tools" and then a small link to "Application Event Logs".&lt;/p&gt;
    &lt;p&gt;Finally you get to your logs, although it's still a bad way to try to view logs, it's at least marginally better than the real windows event viewer, an application that feels like it hasn't been updated since NT4. ( Although some might suggest that's a good thing. )&lt;/p&gt;
    &lt;p&gt;A lot of Azure Insights is a value add on top of OTel. You can use "just" OTel feeding Azure Insights (and that's what the modern Aspire-influenced defaults mostly do) and possibly avoid that feeling of vendor lockin. That perspective might also give you ideas of other "modern" vendors to audition such as Grafana if you wanted to see what other people are doing with OTel rather than Event Logs and file system logs.&lt;/p&gt;
    &lt;p&gt;I love that OTel exists, but I've always been confused why OTel even existed. If I were running a cloud provider, I'd put a "skunkworks" team on all things observability and log management a decade ago, and make their product free to use. It's not critical-path on infrastructure, so it's not a correlated risk other than the investment in manpower, but it dramatically changes how user-friendly your web interface is, and how likely people are to use it daily vs. managing things over command line or with third-party interfaces.&lt;/p&gt;
    &lt;p&gt;By bringing those eyeballs onto your cloud console, you're creating infinitely more opportunities for branded interaction and discovery of your other cloud products - you could even quantify these eyeballs as you would ad inventory! There should have been an arms race for each cloud provider to have the best log-tailing and log-searching and log-aggregation system imaginable. OTel could have been killed before it began, because Honeycomb and its other originators would have been acquired years ago and made specific locked-in value-adds for each cloud.&lt;/p&gt;
    &lt;p&gt;But nobody had this foresight, and thus comments like yours are absolutely correct. OTel is a blessing and I love the tools coming out. But from a cloud provider's perspective, it's a massive missed opportunity that continues to be missed.&lt;/p&gt;
    &lt;p&gt;&amp;gt; By bringing those eyeballs onto your cloud console, you're creating infinitely more opportunities for branded interaction and discovery of your other cloud products - you could even quantify these eyeballs as you would ad inventory! There should have been an arms race for each cloud provider to have the best log-tailing and log-searching and log-aggregation system imaginable. OTel could have been killed before it began, because Honeycomb and its other originators would have been acquired years ago and made specific locked-in value-adds for each cloud.&lt;/p&gt;
    &lt;p&gt;I think that's what Application Insights has always been, Azure's free-to-start, suggest-out-of-the-box Honeycomb. App Insights had a long slow road away from Microsoft-specific log and metrics ingesters that weren't OTel, but it is hard to argue that standard ingestors are a bad idea. App Insights still downplays that it can be "just a Honeycomb" using only OTel sources and still encourages "secret sauce" ingestors in addition to OTel ones. App Insights is a small moat (around a data lake; to mix metaphors). That said, it's also a standards-supporting tool now as well.&lt;/p&gt;
    &lt;p&gt;It's not been as clear of an arms race because AWS and GCP didn't invest in it in a similar way and it mostly impacted what are often called "dark matter" teams (Microsoft shops doing "boring" stuff that rarely makes HN headlines), but I have worked in teams that absolutely favored Azure over AWS/GCP with one of the reasons being Application Insights was an easy install and powerful first-party supported tool rather than an extra third party vendor relationship like Grafana/Honeycomb/Dynatrace/etc.&lt;/p&gt;
    &lt;p&gt;(I think that's from near the transition because it has full "windowing" controls of minimize/maximize/close buttons. I recall a period with only close buttons.)&lt;/p&gt;
    &lt;p&gt;All that blue space you could keep filling with more "blades" as you clicked on things until the entire page started scrolling horizontally to switch between "blades". Almost everything you could click opened in a new blade rather than in place in the existing blade. (Like having "Open in New Window" as your browser default.)&lt;/p&gt;
    &lt;p&gt;It was trying to merge the needs of a configurable Dashboard and a "multi-window experience". You could save collections of blades (a bit like Niri workspaces) as named Dashboards. Overall it was somewhere between overkill and underthought.&lt;/p&gt;
    &lt;p&gt;(Also someone reminded me that many "blades" still somewhat exist in the modern Portal, because, of course, Microsoft backwards compatibility. Some of the pages are just "maximized Blades" and you can accidentally unmaximize them and start horizontally scrolling into new blades.)&lt;/p&gt;
    &lt;p&gt;azure likes to open new sections on the same tab / page as opposed to reloading or opening a new page / tab (overlays? modals? I'm lost on graphic terms)&lt;/p&gt;
    &lt;p&gt;depending on the resource you're accessing, you can get 5+ sections each with their own ui/ux on the same page/tab and it can be confusing to understand where you're at in your resources&lt;/p&gt;
    &lt;p&gt;if you're having trouble visualizing it, imagine an url where each new level is a different application with its own ui/ux and purpose all on the same webpage&lt;/p&gt;
    &lt;p&gt;AWS' UI is similarly messy, and to this day. They regularly remove useful data from the UI, or change stuff like the default sort order of database snapshots from last created to initial instance created date.&lt;/p&gt;
    &lt;p&gt;I never understood why a clear and consistent UI and improved UX isn't more of a priority for the big three cloud providers. Even though you talk mostly via platform SDK's, I would consider better UI especially initially, a good way to bind new customers and pick your platform over others.&lt;/p&gt;
    &lt;p&gt;I guess with their bottom line they don't need it (or cynically, you don't want to learn and invest in another cloud if you did it once).&lt;/p&gt;
    &lt;p&gt;It’s more than just the UI itself (which is horrible), it’s the whole thing that is very hostile to new users even if they’re experienced. It’s such an incoherent mess. The UI, the product names, the entire product line itself, with seemingly overlapping or competing products… and now it’s AI this and AI that. If you don’t know exactly what you’re looking for, good luck finding it. It’s like they’re deliberately trying to make things as confusing as possible.&lt;/p&gt;
    &lt;p&gt;For some reason this applies to all AWS, GCP and Azure. Seems like the result of dozens of acquisitions.&lt;/p&gt;
    &lt;p&gt;I still find it much easier to just self host than learn cloud and I’ve tried a few times but it just seems overly complex for the sake of complexity. It seems they tie in all their services to jack up charges, eg. I came for S3 but now I’m paying for 5 other things just to get it working.&lt;/p&gt;
    &lt;p&gt;Any time something is that unintuitive to get started, I automatically assume that if I encounter a problem that I’ll be unable to solve it. That thought alone leads me to bounce every time.&lt;/p&gt;
    &lt;p&gt;100% agree. I've been working in the industry for almost 20 years, I'm a full stack developer and I manage my servers. I've tried signing up for AWS and I noped out.&lt;/p&gt;
    &lt;p&gt;AWS Is a complete mess. Everything is obscured behind other products, and they're all named in the most confusing way possible.&lt;/p&gt;
    &lt;p&gt;GCP console is not the best but as a long term multicloud user, I can assure you that GCP is much better than Azure portal and/or Azure APIs which is fucking hell&lt;/p&gt;
    &lt;p&gt;I know for some people the prospect of losing their Google Cloud access due to an automated terms of service violation on some completely unrelated service is worrisome.&lt;/p&gt;
    &lt;p&gt;I'd hope you can create a Google Cloud account under a completely different email address, but I do as little business with Google as I can get away with, so I have no idea.&lt;/p&gt;
    &lt;p&gt;That's generally speaking a good practice anyways. My Amazon shopping account has a different email than my Amazon Web Services account. I intuited that they need to be different from the get go.&lt;/p&gt;
    &lt;p&gt;Cloud Run is incredible. It’s one of those things I wish more devs knew about. Even at work where we use GCP all the “smart” devs insist on GKE for their “webscale” services that get dozens of requests a second. Dozens!&lt;/p&gt;
    &lt;p&gt;The problem is that in some industries, Microsoft is the only option. Many of these regulated industries are just now transitioning from the data center to the cloud, and they've barely managed to get approval for that with all of the Microsoft history in their organization. AWS or GCloud are complete non-starters.&lt;/p&gt;
    &lt;p&gt;I moved a 100% MS shop to AWS circa 2015. We ran our DCs on EC2 instances just as if they were on prem. At some point we installed the AAD connector and bridged some stuff to Azure for office/mail/etc., but it was all effectively in AWS. We were selling software to banks so we had a lot of due diligence to suffer. AWS Artifact did much of the heavy lifting for us. We started with Amazon's compliance documentation and provided our own feedback on top where needed.&lt;/p&gt;
    &lt;p&gt;I feel like compliance is the entire point of using these cloud providers. You get a huge head start. Maintaining something like PCI-DSS when you own the real estate is a much bigger headache than if it's hosted in a provider who is already compliant up through the physical/hardware/networking layers. Getting application-layer checkboxes ticked off is trivial compared to "oops we forgot to hire an armed security team". I just took a look and there are currently 316 certifications and attestations listed under my account.&lt;/p&gt;
    &lt;p&gt;I've found that lift and shifting to EC2 is also generally cheaper than the equivalent VMs on Azure.&lt;/p&gt;
    &lt;p&gt;Microsoft really wants you to use their PaaS offerings, and so things on Azure are priced accordingly. A Microsoft shop just wanting to lift-and-shift, Azure isn't the best choice unless the org has that "nobody ever got fired for buying Microsoft" attitude.&lt;/p&gt;
    &lt;p&gt;Microsoft is better at regulatory capture, so Azure has many customers in the public sector. So an Azure outage probably affects the public sector more (see example above about trains).&lt;/p&gt;
    &lt;p&gt;Microsoft has the regulatory capture. All the European privacy and regulatory laws are good for Azure. That's why your average European government or baking app runs most likely on Azure. (or Oracle, but more likely Azure)&lt;/p&gt;
    &lt;p&gt;What Amazon, Azure, and Google are showing with their platform crashes amid layoffs, while they supports governments that are Oppressing's Citizens and Ignoring the Law, is that they do not care about anything other than the bottom line.&lt;/p&gt;
    &lt;p&gt;They think they have the market captured, but I think what their dwindling quality and ethics are really going to drive is adoption of self hosting, distributed computing frameworks. Nerds are the ones who drove adoption of these platforms, and we can eventually end if we put in the work.&lt;/p&gt;
    &lt;p&gt;Seriously with container technology, and a bit more work / adoption on distributed compute systems and file storage (IPFS,FileCoin) there is a future where we dont have to use big brothers compute platform. Fuck these guys.&lt;/p&gt;
    &lt;p&gt;These were my thoughts exactly. I may have my tinfoil hat on, but outages these close together between the largest cloud providers amid social unrest, my wonder is the government / tech companies implementing some update that adds additional spyware / blackout functionality.&lt;/p&gt;
    &lt;p&gt;I really hope this pushes the internet back to how it used to be, self hosted, privacy, anonymity. I truly hope that's where we're headed, but the masses seem to just want to stay comfortable as long as their show is on TV&lt;/p&gt;
    &lt;p&gt;All I'm saying Its a complete 360 from the marketing they did to establish their brand... you know, save the world kind of stuff, especially from google.&lt;/p&gt;
    &lt;p&gt;From 2000-2016 most tech marketing\branding was aimed at some kind of social benefit.&lt;/p&gt;
    &lt;p&gt;At least some bits of it do. I was writing something to pull logs the other day and the redirect was to an azure bucket. It also returned a 401 with the valid temporary authed redirect in the header. I was a bit worried I'd found a massive security hole but it appears after some testing it just returned the wrong status code.&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.&lt;/p&gt;
    &lt;p&gt;Yea, good old store and forward. We implemented it in our PoS system. Now, we do non PCI integrations so we arn't in PCI scope, but depending on the processor, it can come with some limitations. Like, you can do store and forward, but only up to X number of transactions. I think for one integration, it's 500-ish store wide (it uses a local gateway that store and forwards to the processors gateway). The other integration we have, its 250, but store and forward on device, per device.&lt;/p&gt;
    &lt;p&gt;In many places it's also possibly just a left over feature from older times. I worked at a major UK supermarket in the mid-00s, and their checkout system had this feature. But it was like that because that's how it was originally built, it wasn't a 'feature' they added.&lt;/p&gt;
    &lt;p&gt;Credit card information would be recorded by the POS, synced to a mini-server in the back office (using store-and-forward to handle network issues) and then in a batch process overnight, sent to HQ where the payment was processed.&lt;/p&gt;
    &lt;p&gt;It wasn't until chip-and-PIN was rolled out that they started supporting "online" (i.e. processed then and there) card transactions, and even then the old method still worked if there was a network issues or power failure (all POSes has their own UPS).&lt;/p&gt;
    &lt;p&gt;The only real risk at the time was that someone tried to pay with a cancelled credit card - the bank would always honour the payment otherwise. But that was pretty uncommon back then, as you'd have to phone your bank to do it, not just press a button in an app.&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door.&lt;/p&gt;
    &lt;p&gt;Chick-fil-a has this.&lt;/p&gt;
    &lt;p&gt;One of the tech people there was on HN a few years ago describing their system. Credit card approval slows down the line, so the cards are automatically "approved" at the terminal, and the transaction is added to a queue.&lt;/p&gt;
    &lt;p&gt;The loss from fraudulent transactions turns out to be less than the loss from customers choosing another restaurant because of the speed of the lines.&lt;/p&gt;
    &lt;p&gt;The POS I work on also has this feature. Line busters take the order and payment but we have a toggle where you can immediately “approve” and queue it up. If the payment fails then the person handing you your food will see it on the order and ask you for alternative payment. It helps prevent loss and speeds up the line overall.&lt;/p&gt;
    &lt;p&gt;I was shopping at a mall with a visa vanilla card once. I got it as a gift and didn't know the limit. No matter what I bought the card kept going -- and I never got a balance of what was on the card. Eventually, later that day it stopped. I called customer support and asked how much was left on the balance. They told me they had no idea my balance - but everything I bought was mine.&lt;/p&gt;
    &lt;p&gt;I remember that banks will try to honor the transactions, even if the customer's balance/credit limit is exhausted. It doesn't apply only to some gift cards.&lt;/p&gt;
    &lt;p&gt;There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.&lt;/p&gt;
    &lt;p&gt;It's Family Dollar, margin has to be almost nothing and sales per day is probably &amp;lt; $1k. That's why I said 50% of sales and not profit.&lt;/p&gt;
    &lt;p&gt;I go there daily because it's a nice 30min round trip walk and I wfh. I go up there to get a diet coke or something else just to get out of the house. It amazes me when i see a handwritten sign on the door "closed, system is down". I've gotten to know the cashiers so I asked and it's because the internet connection goes down all the time. That store has to one of the most poorly run things i've ever seen yet it stays in business somehow.&lt;/p&gt;
    &lt;p&gt;I think the point people are trying and failing to make is that asking for half of means sales is half of revenue not half of net and that you’re out of your goddamned mind if you think a store with razor thin margins would sell at a massive loss rather than just close due to connectivity problems.&lt;/p&gt;
    &lt;p&gt;Your responses imply that you think people are questioning whether you would lose money on the deal while we are instead saying you’ll get laughed out of the store, or possibly asked never to come back.&lt;/p&gt;
    &lt;p&gt;Unfortunately they are largely corporate, which is how they can sell items for such a cheap price. The store manager probably has zero say in nearly anything. Even if they wanted to "break the rules," I doubt they could make use of your connection as a backup, but I've also worked for smaller companies that were able to sell internet access to individual locations like Denny's and various large hotels in the US. Being able to somehow share sales would be the difficult part, since all sales are reported back to corporate.&lt;/p&gt;
    &lt;p&gt;Good luck if you make this work for you, it would be exciting to hear about if you're able to get them to work with you.&lt;/p&gt;
    &lt;p&gt;2-3%, bit higher on perishables. Though i'd just ask lump sum payments in cash since it likely has to no go through corporate (as in, avoid the corporation).&lt;/p&gt;
    &lt;p&gt;You'd think any SeriousBusiness would have a backup way to take customers' money. This is the one thing you always want to be able to do: accept payment. If they made it so they can't do that, they deserve the hit to their revenue. People should just walk out of the store with the goods if they're not being charged.&lt;/p&gt;
    &lt;p&gt;Why doesn't someone in the store at least have one of those manual kachunk-kachunk carbon copy card readers in the back that they can resuscitate for a few days until the technology is turned back on? Did they throw them all away?&lt;/p&gt;
    &lt;p&gt;I think a lot of payment terminals have an option to record transactions offline and upload them later, but apparently it's not enabled by default - probably because it increases your risk that someone pays with a bad card.&lt;/p&gt;
    &lt;p&gt;After having my credit card locked by stupid fraud prevention algorithms on my honeymoon, I had a long chat with them before going overseas a second time.&lt;/p&gt;
    &lt;p&gt;And that was the day Visa had a full on outage. We would walk into one shop, try to buy stuff, get declined, then go into the next and get accepted because they were running in offline mode.&lt;/p&gt;
    &lt;p&gt;Got a nice big bill from my cellphone carrier for making the call to visa to ask them wtf as well.&lt;/p&gt;
    &lt;p&gt;The kachunk-kachunk credit card machines need raised digits on the cards, and I don't think most banks have been issuing those for years at this point. Mine have been smooth for at least 10 years.&lt;/p&gt;
    &lt;p&gt;My credit union has been behind for a while. I think I had an embossed one until about nine years ago. Six at the latest. Still doesn’t have NFC in it.&lt;/p&gt;
    &lt;p&gt;My card tied to my main financial institution have the raised digits, but most cards you'd sign up for online now no longer have the raised digits (and often allow you to select art to appear on the card face).&lt;/p&gt;
    &lt;p&gt;If they used standalone merchant terminals, then those typically use the local LAN which can rollover to cellular or PoT in the event of a network outage. The store can process a card transaction with the merchant terminal and then reconcile with the end of day chit. This article from 2008 describes their PoS https://www.retailtouchpoints.com/topics/store-operations/ca...&lt;/p&gt;
    &lt;p&gt;These stores appear everywhere, even in areas with high income. You'd be surprised, but often people with those high incomes shop for certain products at very low rates, and that's how they keep their savings. A good example is garbage bags. Most people don't care too much about the quality of their garbage bags, unless they rip on the way to the bin.&lt;/p&gt;
    &lt;p&gt;I remember last mechanical cash registers in my country in 90s and when these got replaced by early electronic ones with blue vacuum fluorescent tubes. Then everything got smaller and smaller. Now I'm pestered to "add the item to the cart" by software.&lt;/p&gt;
    &lt;p&gt;Last week I couldn't pay for flowers for grandma's grave because smartphone-sized card terminal refused to work - it stuck on charging-booting loop so I had to get cash. Tho my partner thinks she actually wanted to get cash without a receipt for herself excluding taxes&lt;/p&gt;
    &lt;p&gt;In Germany many stores still accept cash and some even only accept cash and we are ridiculed for this... Seems like one of the rare instances where this is useful :D&lt;/p&gt;
    &lt;p&gt;It's sad the number of stores I've seen where they just shut down when they can't use the checkout machines; the clerks aren't allowed to do math even if they could.&lt;/p&gt;
    &lt;p&gt;Whereas the smaller, owner-run stores have more leeway; the local tiny grocery "sold" all freezer/refrigerator food for cheap/free during a power failure. The big Walmart closed and threw everything away the next day.&lt;/p&gt;
    &lt;p&gt;The odd thing is that the US has been teaching math using the “in your head” heuristic (New Math) for almost 20 years and yet young adults cannot make change without the machine to save their lives.&lt;/p&gt;
    &lt;p&gt;God help me if I hand someone $25 for a $14.75 total. I’m getting small bills back.&lt;/p&gt;
    &lt;p&gt;Just to add - this particular supermarket wasn’t fully down, it took ages for them to press “sub total” and then pick the payment method. I suspect it was slow waiting for a request to timeout perhaps&lt;/p&gt;
    &lt;p&gt;Most retailers trust their cashiers a bit less than they trust the customers. They'd rather shut down during a power/Internet failure than give any autonomy to the worker drones.&lt;/p&gt;
    &lt;p&gt;You can, but it's all about risk mitigation. Most processors have some form of store and forward (and it can have limitations like only X number of transactions). Some even have controls to limit the amount you can store-and-forward (for instance, only charges under $50). But ultimately, it's still risk mitigation. You can store-and-forward, but you're trusting that the card/account has the funds. If it doesn't, you loose and ain't shit you can do about it. If you can't tolerate any risk, you don't turn on store and forward systems and then you can't process cards offline.&lt;/p&gt;
    &lt;p&gt;Its not the we are not capable. Its, is the business willing to assume the risk?&lt;/p&gt;
    &lt;p&gt;Currently standing in a half closed supermarket because the tills are down and they cant take payments&lt;/p&gt;
    &lt;p&gt;There's a fairly large supermarket near me that has both kinds of outages.&lt;/p&gt;
    &lt;p&gt;Occasionally it can't take cards because the (fiber? cable?) internet is down, so it's cash only.&lt;/p&gt;
    &lt;p&gt;Occasionally it can't take cash because the safe has its own cellular connection, and the cell tower is down.&lt;/p&gt;
    &lt;p&gt;I was at Frank's Pizza in downtown Houston a few weeks ago and they were giving slices of pizza away because the POS terminal died, and nobody knew enough math to take cash. I tried to give them a $10 and told them to keep the change, but "keep the change" is an unknown phrase these days. They simply couldn't wrap their brains around it. But hey, free pizza!&lt;/p&gt;
    &lt;p&gt;I’ve been migrating our services off of Azure slowly for the past couple of years. The last internet facing things remaining are a static assets bucket and an analytics VM running Matomo. Working with Front Door has been an abysmal experience, and today was the push I needed to finally migrate our assets to Cloudflare.&lt;/p&gt;
    &lt;p&gt;I feel pretty justified in my previous decisions to move away from Azure. Using it feels like building on quicksand…&lt;/p&gt;
    &lt;p&gt;We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven’t had weekly or even monthly issues. I can think of maybe three issues this year.&lt;/p&gt;
    &lt;p&gt;We’re 100% on Azure but so far there’s no impact for us.&lt;/p&gt;
    &lt;p&gt;Luckily, we moved off Azure Front Door about a year ago. We’d had three major incidents tied to Front Door and stopped treating it as a reliable CDN.&lt;/p&gt;
    &lt;p&gt;They weren’t global outages, more like issues triggered by new deployments. In one case, our homepage suddenly showed a huge Microsoft banner about a “post-quantum encryption algorithm” or something along those lines.&lt;/p&gt;
    &lt;p&gt;Kinda wild that a company that big can be so shaky on a CDN, which should be rock solid.&lt;/p&gt;
    &lt;p&gt;We battled https://learn.microsoft.com/en-us/answers/questions/1331370/... for over a year, and finally decided to move off since there was no any resolution. Unfortunately our API servers were still behind AFD so they were affected by today's stuff...&lt;/p&gt;
    &lt;p&gt;I have had intermittent issues with winget today. I use UniGetUI for a front-end, and anything tied to Microsoft has failed for me. Judging by the logs, it's mostly retrieving the listing of versions (I assume similar to what 'apt-get update' does, I'm fairly new to using winget for Windows package management).&lt;/p&gt;
    &lt;p&gt;Pretty much every single Microsoft domain I've tried to access loads for a looooong time before giving me some bare html. I wonder if someone can explain why that's happening.&lt;/p&gt;
    &lt;p&gt;And querying https://www.microsoft.com/ results in HTTP 200 on the root document, but the page elements return errors (a 504 on the .css/.js documents, a 404 on some fonts, Name Not Resolved on scripts.clarity.ms, Connection Timed Out on wcpstatic.microsoft.com and mem.gfx.ms). That many different kinds of errors is actually kind of impressive.&lt;/p&gt;
    &lt;p&gt;I'm gonna say this was a networking/routing issue. The CDN stayed up, but everything else non-CDN became unroutable, and different requests traveled through different paths/services, but each eventually hit the bad network path, and that's what created all the different responses. Could also have been a bad deploy or a service stopped running and there's different things trying to access that service in different ways, leading to the weird responses... but that wouldn't explain the failed DNS propagation.&lt;/p&gt;
    &lt;p&gt;I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.&lt;/p&gt;
    &lt;p&gt;We’ve been experimenting with multi-cluster failover for Kubernetes workloads, and one open-source project that actually works really well is k8gb .&lt;/p&gt;
    &lt;p&gt;It acts as a GSLB controller inside Kubernetes — doing DNS-level health checks, region awareness, and automatic failover between clusters when one goes down.&lt;/p&gt;
    &lt;p&gt;It integrates with ExternalDNS and supports multiple DNS providers (Infoblox, Route53, Azure DNS, NS1, etc.), so it can handle failover across both on-prem and cloud clusters.&lt;/p&gt;
    &lt;p&gt;It’s not a silver bullet for every architecture, but it’s one of the few OSS projects that make multi-region failover actually manageable in practice.&lt;/p&gt;
    &lt;p&gt;They added a message at the same time as your comment:&lt;/p&gt;
    &lt;p&gt;"We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly."&lt;/p&gt;
    &lt;p&gt;Yeah just took down the prod site for one of our clients since we host the front-end out of their CDN. Just got wrapped up panic hosting it somewhere else for the past hour, very quickly reminds you about the pain of cookies...&lt;/p&gt;
    &lt;p&gt;Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.&lt;/p&gt;
    &lt;p&gt;Same playbook for AWS. When they admitted that Dynamo was inaccessible, they failed to provide context that their internal services are heavily dependent on Dynamo&lt;/p&gt;
    &lt;p&gt;It's only after the fact they are transparent about the impact&lt;/p&gt;
    &lt;p&gt;The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.&lt;/p&gt;
    &lt;p&gt;How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.&lt;/p&gt;
    &lt;p&gt;A lot of money and years of marketing the cloud as the responsible business decision led us here. Now that the cloud providers have vendor lock-in, few will leave, and customers will continue to wildly overpay for cloud services.&lt;/p&gt;
    &lt;p&gt;Not sure how the current situation is better. Being stranded with no way whatsoever to access most/all of your services sounds way more terrifying than regular issues limited to a couple of services at a time&lt;/p&gt;
    &lt;p&gt;&amp;gt; no way whatsoever to access most/all of your services&lt;/p&gt;
    &lt;p&gt;I work on a product hosted on Azure. That's not the case. Except for front door, everything else is running fine. (Front door is a reverse proxy for static web sites.)&lt;/p&gt;
    &lt;p&gt;The product itself (an iot stormwater management system) is running, but our customers just can't access the website. If they need to do something, they can go out to the sites or call us and we can "rub two sticks together" and bypass the website. (We could also bypass front door if someone twisted our arms.)&lt;/p&gt;
    &lt;p&gt;Most customers only look at the website a few times a year.&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;That being said, our biggest point of failure is a completely different iot vendor who you probably won't hear about on Hacker News when they, or their data networks, have downtime.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Big Tech lobbying is riding the EU’s deregulation wave by spending more, hiring more, and pushing more, according to a new report by NGO’s Corporate Europe Observatory and LobbyControl on Wednesday (29 October).&lt;/p&gt;
    &lt;p&gt;&amp;gt; Based on data from the EU’s transparency register, the NGOs found that tech companies spend the most on lobbying of any sector, spending €151m a year on lobbying — a 33 percent increase from €113m in 2023.&lt;/p&gt;
    &lt;p&gt;Gee whizz, I really do wonder how they end up having all the power!&lt;/p&gt;
    &lt;p&gt;I think the response lies in the surrounding ecosystem.&lt;/p&gt;
    &lt;p&gt;If you have a company it's easier to scale your team if you use AWS (or any other established ecosystem). It's way easier to hire 10 engineers that are competent with AWS tools than it is to hire 10 engineers that are competent with the IBM tools.&lt;/p&gt;
    &lt;p&gt;And from the individuals perspective it also make sense to bet on larger platforms. If you want to increase your odds of getting a new job, learning the AWS tools gives you a better ROI than learning the IBM tools.&lt;/p&gt;
    &lt;p&gt;But the cloud compute market is basically centralized into 2.5 companies at this point. The point of paying companies like Azure here is that they've in theory centralized the knowledge and know-how of running multiple, distributed datacenters, so as to be resilient.&lt;/p&gt;
    &lt;p&gt;But that we keep seeing outages encompassing more than a failure domain, then it should be fair game for engineers / customers to ask "what am I paying for, again?"&lt;/p&gt;
    &lt;p&gt;Moreover, this seems to be a classic case of large barriers to entry (the huge capital costs associated with building out a datacenter) barring new entrants into the market, coupled with "nobody ever got fired for buying IBM" level thinking. Are outages like these truly factored into the napkin math that says externalizing this is worth it?&lt;/p&gt;
    &lt;p&gt;Consolidation is the inevitable outcome of free unregulated markets.&lt;/p&gt;
    &lt;p&gt;In our highly interconnected world, decentralization paradoxically requires a central authority to enforce decentralization by restricting M&amp;amp;A, cartels, etc.&lt;/p&gt;
    &lt;p&gt;A natural monopoly is a monopoly in an industry in which high infrastructure costs and other barriers to entry relative to the size of the market give the largest supplier in an industry, often the first supplier in a market, an overwhelming advantage over potential competitors. Specifically, an industry is a natural monopoly if a single firm can supply the entire market at a lower long-run average cost than if multiple firms were to operate within it. In that case, it is very probable that a company (monopoly) or a minimal number of companies (oligopoly) will form, providing all or most of the relevant products and/or services.&lt;/p&gt;
    &lt;p&gt;The paradox of cloud provider crashes is that if the provider goes down and takes the whole world with it, it's actually good advertisement. Because, that means so many things rely on it, it's critically important, and has so many big customers. That might be why Amazon stock went up after AWS crash.&lt;/p&gt;
    &lt;p&gt;If Azure goes down and nobody feels it, does Azure really matter?&lt;/p&gt;
    &lt;p&gt;People feel it, but usually not general consumers like they do when AWS goes down.&lt;/p&gt;
    &lt;p&gt;If Azure goes down, it's mostly affecting internal stuff at big old enterprises. Jane in accounting might notice, but the customers don't. Contrast with AWS which runs most of the world's SaaS products.&lt;/p&gt;
    &lt;p&gt;People not being able to do their jobs internally for a day tends not to make headlines like "100 popular internet services down for everyone" does.&lt;/p&gt;
    &lt;p&gt;We all need to move away from these big cloud providers. Two medium size smaller providers is enough.&lt;/p&gt;
    &lt;p&gt;-Cloudflare for R2 (object storage) and CDN (Fastly+backblaze also available). -Two VPS/Server providers with a decent reputation and mid-size (using a comparison site like https://serversearcher.com or look directly into people like Hetzner or latitude) -PlanetScale or Neon for database if you don't co-locate it, though better to use someone like digital ocean, vultr or latitude who offer databases too)&lt;/p&gt;
    &lt;p&gt;&amp;gt; We all need to move away from these big cloud providers.&lt;/p&gt;
    &lt;p&gt;But then who do we blame when things are down? If we manage our own infrastructure we have to stay late to fix it when it breaks instead of saying “sorry, Microsoft, nothing we can do” and magically our clients accepting that…&lt;/p&gt;
    &lt;p&gt;Yep, even massive cloud providers slip up. Reminds me that depending on “someone else’s infrastructure” still means losing access when things go sideways.&lt;/p&gt;
    &lt;p&gt;The outage was really weird. For me, parts of the portal worked, other parts didn't. I had access to a couple of resource groups, but no resources visible in those groups. Azure Devops Pipelines that needed do download from packages.microsoft.com didn't work.&lt;/p&gt;
    &lt;p&gt;The Microsoft status page mostly referenced the portal outage, but it was more than that.&lt;/p&gt;
    &lt;p&gt;I hate these failures because you end up with things that keep working fine because the login credentials are cached, etc; but if you restart or otherwise refresh, you're doomed.&lt;/p&gt;
    &lt;p&gt;I was a little puzzled as we got notified our apps were down, and then I tried to login in the Azure portal with no success. But the Azure status page reported no incident, so I posted here and quickly confirmed that others were impacted! They did a pretty bad job with their status page as the front door service was shown green all along&lt;/p&gt;
    &lt;p&gt;For us, it looks like most services are still working (eastus and eastus2). Our AKS cluster is still running and taking requests. Failures seem limited to management portal.&lt;/p&gt;
    &lt;p&gt;High availability is touted as a reason for their high prices, but I swear I read about major cloud outages far more than I experience any outages at Hetzner.&lt;/p&gt;
    &lt;p&gt;I think the biggest features of the big cloud vendors is that when they are down, not only you but your customers and your competitors usually have issues at the same time so everybody just shrug and have a lazy/off day at the same time. Even on call teams reall just have to wait and stay on standby because there is very little they can do. Doing a failover can be slower than waiting for the recovery, not help at all if outage is spanned accross several region, or bring aditional risks.&lt;/p&gt;
    &lt;p&gt;And more importantly nobody lose any reputation except AWS/Azure/Google.&lt;/p&gt;
    &lt;p&gt;The real reason is that outages are not your fault. Its the new version of "nobody ever got fired for buying IBM" - later it became MS, and now its any big cloud provider.&lt;/p&gt;
    &lt;p&gt;For one it’s statistics - Hetzner simply runs far fewer major services than hyperscalers. And the services they run are also more affluent, with larger customer bases, so downtimes are systemically critical. Therefore it’s louder.&lt;/p&gt;
    &lt;p&gt;On the merits though, I agree, haven’t had any serious issues with Hetzner.&lt;/p&gt;
    &lt;p&gt;DO has been shockingly reliable for me. I shut down a neglected box almost 900 days uptime the other day. In that time AWS has randomly dropped many of my boxes with no warning requiring a manual stop/start action to recover them... But everybody keeps telling me that DO isn't "as reliable" as the big three are.&lt;/p&gt;
    &lt;p&gt;To be fair, in the AWS/Azure outages, I don't think any individual (already created) boxes went down, either. In AWS' case you couldn't start up new EC2 instances, and presumably same for Azure (unless you bypass the management portal, I guess). And obviously services like DynamoDB and Front Door, respectively, went down. Hetzner/DO don't offer those, right? Or at least they're not very popular.&lt;/p&gt;
    &lt;p&gt;Nope, more than the portal. For instance, I just searched for "Azure Front Door" because I hadn't heard of it before (I now know it's a CDN), and neither the product page itself [1] nor the technical docs [2] are coming up for me.&lt;/p&gt;
    &lt;p&gt;we use front door (as does miccrosoft.com) and our website was down, I was able to change the DNS records to point directly to our server and will leave it like that for a few hours until everything is green&lt;/p&gt;
    &lt;p&gt;They admit in their update blurb azure front door is having issues but still report azure front door as having no issues on their status page.&lt;/p&gt;
    &lt;p&gt;And it's very clear from these updates that they're more focused on the portal than the product, their updates haven't even mentioned fixing it yet, just moving off of it, as if it's some third party service that's down.&lt;/p&gt;
    &lt;p&gt;Unsubstantiated idea: So the support contract likely says there is a window between each reporting step and the status page is the last one and the one in the legal documents giving them several more hours before the clauses trigger.&lt;/p&gt;
    &lt;p&gt;it took a good half hour after we detected the problem to see a notification on the Azure status page. Thanks to those who responded to my question as it validated the issue was global and we contacted our users t right away&lt;/p&gt;
    &lt;p&gt;Do Microsoft still say "If the government has a broader voluntary national security program to gather customer data, we don't participate in it" today (which PRISM proved very false), or are they at least acknowledging they're participating in whatever NSA has deployed today?&lt;/p&gt;
    &lt;p&gt;PRISM wasn't voluntary. Also there are 3 levels here:&lt;/p&gt;
    &lt;p&gt;1. Mandatory&lt;/p&gt;
    &lt;p&gt;2. "Voluntary"&lt;/p&gt;
    &lt;p&gt;3. Voluntary&lt;/p&gt;
    &lt;p&gt;And I suspect that very little of what the NSA does falls into category 3. As Sen Chuck Schumer put it "you take on the intelligence community, they have six ways from Sunday at getting back at you"&lt;/p&gt;
    &lt;p&gt;This is funny but also possibly true because: business/MBA types see these outages as a way to prove how critical some services are, leading to investors deciding to load up on the vendor's stock.&lt;/p&gt;
    &lt;p&gt;I may or may not have been known to temporarily take a database down in the past to make a point to management about how unreliable some old software is.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;----&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;Seeing users having issues with the "Modern Outlook", specifically empty accounts. Switching back to the "Legacy Outlook" which functions largely without the help of the cloud fixes the issue. How ironic.&lt;/p&gt;
    &lt;p&gt;The sad thing is - $MSFT isn't even down by 1%. And IIRC, $AMZN actually went up during their previous outage.&lt;/p&gt;
    &lt;p&gt;So if we look at these companies' bottom lines, all those big wigs are actually doing something right. Sales and lobbying capacity is way more effective than reliability or good engineering (at least in the short term).&lt;/p&gt;
    &lt;p&gt;I think he was implying that those companies think they are so important that it doesnt matter they are down, they wont loose any customers over it because they are too big and important.&lt;/p&gt;
    &lt;p&gt;That's a good thing. Stock prices shouldn't go down because of rare incidents which don't accurately represent how successful a company is likely to be in the future.&lt;/p&gt;
    &lt;p&gt;I looked into this before and the stocks of these large corps simply does not move when outages happens. Maybe intra-day, I don't have that data, but in general no effect.&lt;/p&gt;
    &lt;p&gt;I was having issues a few hours ago. I'm now able to access the portal, although I get lots of errors in the browser console, and things are loading slowly. I have services in the US-East region.&lt;/p&gt;
    &lt;p&gt;I have been having issues with GitHub and the winget tool for updates throughout the day as well. I imagine things are pulling from the same locations on Azure for some of the software I needed to update (NPM dependencies, and some .NET tooling).&lt;/p&gt;
    &lt;p&gt;There's no way to tell, and after about 30 minutes, the release process on VS Code Marketplace failed with a cryptic message: "Repository signing for extension file failed.". And there's no way to restart/resume it.&lt;/p&gt;
    &lt;p&gt;"We’re investigating an issue impacting Azure Front Door services. Customers may experience intermittent request failures or latency. Updates will be provided shortly."&lt;/p&gt;
    &lt;p&gt;Could be DNS, I'm seeing SERVFAIL trying to resolve what look to be MS servers when I'm hitting (just one example) mygoodtogo.com (trying to pay a road toll bill, and failing).&lt;/p&gt;
    &lt;p&gt;Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.&lt;/p&gt;
    &lt;p&gt;Portal and Azure CDN are down here in the SF Bay Area. Tenant azureedge.net DNS A queries are taking 2-6 seconds and most often return nothing. I got a couple successful A response in the last 10 minutes.&lt;/p&gt;
    &lt;p&gt;Edit: As of 9:19 AM Pacific time, I'm now getting successful A responses but they can take several seconds. The web server at that address is not responding.&lt;/p&gt;
    &lt;p&gt;It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.&lt;/p&gt;
    &lt;p&gt;It begs the question from a noob like me... Where should they host the status page? Surely it shouldn't be on the same infra that it's supposed to be monitoring. Am I correct in thinking that?&lt;/p&gt;
    &lt;p&gt;“ Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025”&lt;/p&gt;
    &lt;p&gt;I'd say DNS/Front Door (or some carrier interconnect) is the thing affected, since I can auth just fine in a few places. (I'm at MS, but not looped into anything operational these days, so I'm checking my personal subscription).&lt;/p&gt;
    &lt;p&gt;On our end, our VMs are still working, so our gitlab instance is still up. Our services using Azure App Services are available through their provided url. However, Front Door is failing to resolve any domains that it was responsible for.&lt;/p&gt;
    &lt;p&gt;SSO is down, Azure Portal Down and more, seems like a major outage. Already a lot of services seem to be affected: banks, airlines, consumer apps, etc.&lt;/p&gt;
    &lt;p&gt;The portal is up for me and their status page confirms they did a failover for it. Definitely not disputing that its reach is wide, but a lot of smaller setups probably aren't using Front Door.&lt;/p&gt;
    &lt;p&gt;Looks like MyGet is impacted too. Seems like they use Azure:&lt;/p&gt;
    &lt;p&gt;&amp;gt;What is required to be able to use MyGet? ... MyGet runs its operations from the Microsoft Azure in the West Europe region, near Amsterdam, the Netherlands.&lt;/p&gt;
    &lt;p&gt;This is the eternal tension for early-stage builders, isn't it? Multi-cloud gives you resilience, but adds so much complexity that it can actually slow down shipping features and iterating.&lt;/p&gt;
    &lt;p&gt;I'm curious—at what point did you decide the overhead was worth it? Was it after experiencing an outage, or did you architect for it from day one?&lt;/p&gt;
    &lt;p&gt;As someone launching a product soon (more on the builder/product side than infra-engineer), I keep wrestling with this. The pragmatist in me says "start simple, prove the concept, then layer in resilience." But then you see events like this week and think "what if this happens during launch?"&lt;/p&gt;
    &lt;p&gt;How did you handle the operational complexity? Did you need dedicated DevOps folks, or are there patterns/tools that made it manageable for a smaller team?&lt;/p&gt;
    &lt;p&gt;I don't think I would recommend multi-cloud right out of the gate unless you already have a lot of experience in the space or there is a strong demand from your customers. There's a tremendous amount of overhead with security/compliance, incident management, billing, tooling, entitlements, etc. There are a number of external factors that drove our decision to do it, resiliency is just one of them. But we are a pretty big shop, spending ~$10M/mo on cloud infra and have ~100 people in the platform management department.&lt;/p&gt;
    &lt;p&gt;I would recommend focusing on multi-region within a single CSP instead (both for workloads AND your tooling), which covers the vast majority of incidents and lays some of the architectural foundation for multi-cloud down the road. Develop failover plans for each service in your architecture (eg. planned/tested runbooks to migrate to Traffic Manager in the event AFD goes down)&lt;/p&gt;
    &lt;p&gt;Also choose your provider wisely. We experience 3-5x the number of service-impacting incidents on Azure that we do on AWS. I'm sure others have different experiences, but I would never personally start a company on Azure. AWS has its own issues, of course, but reliability has not been a major one (relatively speaking) over the past 10 years. Last week's incident with DynamoDB in us-east-1 had zero impact on our AWS workloads in other regions.&lt;/p&gt;
    &lt;p&gt;I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.&lt;/p&gt;
    &lt;p&gt;Part of this outage involves outlook hanging and then blaming random addins. Pretty terrible practice by Microsoft to blame random vendors for their own outage.&lt;/p&gt;
    &lt;p&gt;That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.&lt;/p&gt;
    &lt;p&gt;Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking? Or is that the info that the malefactors are seeking?&lt;/p&gt;
    &lt;p&gt;Thank you. I was wondering what was going on at a company whose web app I need to access. I just checked with BuiltWith and it seems they are on Azure.&lt;/p&gt;
    &lt;p&gt;I absolutely love the utility aspect of LLMs but part of me is curious if moving faster by using AI is going to make these sorts of failure more and more often.&lt;/p&gt;
    &lt;p&gt;Unable to access the portal and any hit to SSO for other corporate accesses is also broken. Seems like there's something wrong in their Identity services.&lt;/p&gt;
    &lt;p&gt;Apologies, but this just reads like a low effort critique of big things.&lt;/p&gt;
    &lt;p&gt;To be clear, they should get criticism. They should be held liable for any damage they cause.&lt;/p&gt;
    &lt;p&gt;But that they remain the biggest cloud offering out there isn't something you'd expect to change from a few outages that, by most all evidence, potential replacements have, as well? More, a lot of the outages potential replacements have are often more global in nature.&lt;/p&gt;
    &lt;p&gt;I would say you are explaining why they get a free pass so they still get one - they are bad but their main competitors are even worse!&lt;/p&gt;
    &lt;p&gt;I thought one of the major selling points of the big cloud providers was that they were more reliable than running your own stuff (by which i mean anything from a VPS to multiple data centres depending on your scale. Compared to those alternatives they seem to be less reliable in practice!&lt;/p&gt;
    &lt;p&gt;The solution is to have a multi-region, or even multi-cloud setup, but then bang goes the "they do all the work for you" argument (which i doubt anyway).&lt;/p&gt;
    &lt;p&gt;That isn't a free pass. You have no data showing how many people did go to competitors over this. You are asserting it is zero, but why do you think that? Going on the talks here, you can find plenty of folks that opted not to go with or stay on them.&lt;/p&gt;
    &lt;p&gt;You are further asserting that these outages prove they are not still more reliable than home spun. Is that the case? More than a few people aren't ready for a single hard drive to crash on the stuff they are doing.&lt;/p&gt;
    &lt;p&gt;Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).&lt;/p&gt;
    &lt;p&gt;FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.&lt;/p&gt;
    &lt;p&gt;Hate to say it, but DNS is looking like it's still the undisputed champ.&lt;/p&gt;
    &lt;p&gt;HTTPSConnectionPool(host='schemas.xmlsoap.org', port=443): Max retries exceeded with url: /soap/encoding/ (Caused by SSLError(CertificateError("hostname 'schemas.xmlsoap.org' doesn't match '*.azureedge.net'")))&lt;/p&gt;
    &lt;p&gt;A service we rely on that isn't even running on Azure is inaccessible due to this issue. For an asset that probably never changes. Wild for that to be the SPOF.&lt;/p&gt;
    &lt;p&gt;&amp;gt; An inadvertent tenant configuration change within Azure Front Door (AFD) triggered a widespread service disruption affecting both Microsoft services and customer applications dependent on AFD for global content delivery. The change introduced an invalid or inconsistent configuration state that caused a significant number of AFD nodes to fail to load properly, leading to increased latencies, timeouts, and connection errors for downstream services.&lt;/p&gt;
    &lt;p&gt;&amp;gt; As unhealthy nodes dropped out of the global pool, traffic distribution across healthy nodes became imbalanced, amplifying the impact and causing intermittent availability even for regions that were partially healthy. We immediately blocked all further configuration changes to prevent additional propagation of the faulty state and began deploying a ‘last known good’ configuration across the global fleet. Recovery required reloading configurations across a large number of nodes and rebalancing traffic gradually to avoid overload conditions as nodes returned to service. This deliberate, phased recovery was necessary to stabilize the system while restoring scale and ensuring no recurrence of the issue.&lt;/p&gt;
    &lt;p&gt;&amp;gt; The trigger was traced to a faulty tenant configuration deployment process. Our protection mechanisms, to validate and block any erroneous deployments, failed due to a software defect which allowed the deployment to bypass safety validations. Safeguards have since been reviewed and additional validation and rollback controls have been immediately implemented to prevent similar issues in the future.&lt;/p&gt;
    &lt;p&gt;So, so far they're saying it's a combination of bad config + their config-validator had a bug. Would love more details.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;AWS, now Azure - wasn't this a plot point in Terminator where SkyNet was causing computer systems to have issues much before it finally become self-aware?&lt;/p&gt;
    &lt;p&gt;Funnily enough, AI has been training on its own data as generated by users writing AI conversations back to the internet - there's a feedback loop at play.&lt;/p&gt;
    &lt;p&gt;Another big cloud outage — even “enterprise” systems aren’t bulletproof. Feels like every layer depends on too much hidden glue. Makes you wonder if real redundancy even exists anymore.&lt;/p&gt;
    &lt;p&gt;Yeah the graph for that one looks exactly the same shape. I wonder if they were depending on some azure component somehow, or maybe there were things hosted on both and the azure failure made enough things failover to AWS that AWS couldn't cope? If that was the case I'd expect to see something similar with GCP too though.&lt;/p&gt;
    &lt;p&gt;Edit: nope looks like there's actually a spike on GCP as well&lt;/p&gt;
    &lt;p&gt;Definitely also a strong possibility. I wish I had paid more attention during the AWS one earlier to see what other things looked like on there at the time.&lt;/p&gt;
    &lt;p&gt;When you look at the scale of the reports, you find they are much lower than Azure's. seeing a bunch of 24-hour sparkline type graphs next to each other can make it look like they are equally impacted, but AWS has 500 reports and Azure has 20,000. The scale is hidden by the choice of graph.&lt;/p&gt;
    &lt;p&gt;In other words, people reporting outages at AWS are probably having trouble with microsoft-run DNS services or caching proxies. It's not that the issues aren't there, it's that the internet is full of intermingled complexity. Just that amount of organic false-positives can make it look like an unrelated major service is impacted.&lt;/p&gt;
    &lt;p&gt;winget upgrade fabric Failed in attempting to update the source: winget An unexpected error occurred while executing the command: InternetOpenUrl() failed. 0x80072ee7 : unknown error&lt;/p&gt;
    &lt;p&gt;As of now Azure Status page still shows no incident. It must be manually updated, someone has to actively decide to acknowledge an issue, and they're just... not. It undermines confidence in that status page.&lt;/p&gt;
    &lt;p&gt;I know how to fix this but this community is too close minded and argumentative egocentric sensitive pedantic threatened angry etc to bother discussing it&lt;/p&gt;
    &lt;p&gt;I noticed issues on Azure so I went to the status page. It said everything was fine even though the Azure Portal was down. It took more than 10 minutes for that status page to update.&lt;/p&gt;
    &lt;p&gt;How can one of the richest companies in the world not offer a better service?&lt;/p&gt;
    &lt;p&gt;My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.&lt;/p&gt;
    &lt;p&gt;From Azure status page: "Customers can consider implementing failover strategies with Azure Traffic Manager, to fail over from Azure Front Door to your origins".&lt;/p&gt;
    &lt;p&gt;I especially like how Nadella speaks of layoffs as some kind of uncontrollable natural disaster, like a hurricane, caused by no-one in particular. A kind of "God works in mysterious ways".&lt;/p&gt;
    &lt;p&gt;&amp;gt; “Microsoft is being recognized and rewarded at levels never seen before,” Nadella wrote. “And yet, at the same time, we’ve undergone layoffs. This is the enigma of success in an industry that has no franchise value.” &amp;gt; Nadella explained the disconnect between thriving financials and layoffs by stating that “progress isn’t linear” and that it is “sometimes dissonant, and always demanding.”&lt;/p&gt;
    &lt;p&gt;I've read the whole memo and it's actually worse than those excerpts. Nadella doesn't even claim these were low performers:&lt;/p&gt;
    &lt;p&gt;&amp;gt; These decisions are among the most difficult we have to make. They affect people we’ve worked alongside, learned from, and shared countless moments with—our colleagues, teammates, and friends.&lt;/p&gt;
    &lt;p&gt;Ok, so Microsoft is thriving, these were friends and people "we've learned from", but they must go because... uh... "progress isn't linear". Well, thanks Nadella! That explains so much!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748661"/><published>2025-10-29T16:01:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748879</id><title>Minecraft removing obfuscation in Java Edition</title><updated>2025-10-30T19:32:29.739783+00:00</updated><content>&lt;doc fingerprint="370203ca08b7cced"&gt;
  &lt;main&gt;
    &lt;p&gt;Do you like to mod Java, tinker with builds, or take deep dives into Minecraft’s code? Then this article is for you!&lt;/p&gt;
    &lt;p&gt;For a long time, Java Edition has used obfuscation (hiding parts of the code) – a common practice in the gaming industry. Now we’re changing how we ship Minecraft: Java Edition to remove obfuscation completely. We hope that, with this change, we can pave a future for Minecraft: Java Edition where it’s easier to create, update, and debug mods.&lt;/p&gt;
    &lt;head rend="h2"&gt;An obfuscated history&lt;/head&gt;
    &lt;p&gt;Minecraft: Java Edition has been obfuscated since its release. This obfuscation meant that people couldn’t see our source code. Instead, everything was scrambled – and those who wanted to mod Java Edition had to try and piece together what every class and function in the code did.&lt;/p&gt;
    &lt;p&gt;But we encourage people to get creative both in Minecraft and with Minecraft – so in 2019 we tried to make this tedious process a little easier by releasing “obfuscation mappings”. These mappings were essentially a long list that allowed people to match the obfuscated terms to un-obfuscated terms. This alleviated the issue a little, as modders didn’t need to puzzle out what everything did, or what it should be called anymore. But why stop there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Removing obfuscation in Java Edition&lt;/head&gt;
    &lt;p&gt;To make things even easier – and remove these intermediary steps – we’re removing obfuscation altogether! Starting with the first snapshot following the complete Mounts of Mayhem launch, we will no longer obfuscate Minecraft: Java Edition. This means that this build (and all future builds) will have all of our original names* – now with variable names and other names – included by default to make modding even easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition"/><published>2025-10-29T16:12:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45758421</id><title>Show HN: In a single HTML file, an app to encourage my children to invest</title><updated>2025-10-30T19:32:29.492763+00:00</updated><content>&lt;doc fingerprint="26c9c0412db7d00b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Built an App to Encourage My Kids to Invest â Just One HTML File&lt;/head&gt;
    &lt;p&gt;âWhat comes with the milk, leaves with the soulâ&lt;lb/&gt; â Russian proverb.&lt;/p&gt;
    &lt;p&gt;Access the app:&lt;lb/&gt; Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/p&gt;
    &lt;p&gt;One thing that school doesnât teach you (not even high school) is how to manage your personal finances.&lt;/p&gt;
    &lt;p&gt;As my eldest sonâs birthday was approaching, we suggested that instead of asking for physical gifts, he ask for their equivalent in money. That way, he gathered a decent amount of capital for his first investment adventure.&lt;/p&gt;
    &lt;p&gt;I explained to my kids that investing is like having a magic box that generates more money over time. To make it more visual and interactive, I decided to create a small app where they could see their investment grow day by day.&lt;/p&gt;
    &lt;head rend="h1"&gt;From Idea to App&lt;/head&gt;
    &lt;p&gt;My first idea was to build a physical piggy bank with a display, showing the accumulated amount. However, that mixed up the concept of saving with investing, and also required buying extra hardware.&lt;/p&gt;
    &lt;p&gt;So I looked for a quicker, cheaper way: revive an old smartphone and create a simple app using plain HTML.&lt;/p&gt;
    &lt;p&gt;The result was D-i&lt;del&gt;n&lt;/del&gt;vestments, a mix between Diversions and Investments.&lt;/p&gt;
    &lt;head rend="h1"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;The app is essentially a single HTML file that installs on the phone as a PWA (Progressive Web App).&lt;/p&gt;
    &lt;p&gt;The phone is attached to the fridge and works as a panel or dashboard where my kids can see their money growing each day.&lt;/p&gt;
    &lt;p&gt;I act as their investment agent, assigning realistic interest rates â high enough to keep them motivated, but moderate enough to reflect how the real world works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuration Screen&lt;/head&gt;
    &lt;p&gt;The app includes a screen where you can enter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The kidsâ names&lt;/item&gt;
      &lt;item&gt;The invested amount&lt;/item&gt;
      &lt;item&gt;The interest rate&lt;/item&gt;
      &lt;item&gt;The start date&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that data, the app automatically calculates and displays:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daily gain&lt;/item&gt;
      &lt;item&gt;Weekly gain&lt;/item&gt;
      &lt;item&gt;Monthly gain&lt;/item&gt;
      &lt;item&gt;Total updated balance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Materials Used&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An old smartphone&lt;/item&gt;
      &lt;item&gt;A suction mount to attach it to the fridge&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The D-iNvestments app, in HTML format&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Installation&lt;/head&gt;
    &lt;p&gt;The process is as simple as opening the link from a smartphone and tapping âInstallâ when prompted by the browser.&lt;lb/&gt; From then on, it behaves like a native app.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Access the app:&lt;/p&gt;&lt;lb/&gt;Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/quote&gt;
    &lt;head rend="h1"&gt;Final Reflection&lt;/head&gt;
    &lt;p&gt;The goal wasnât just to teach my kids the value of money, but to show them visually how investment and time work as allies.&lt;/p&gt;
    &lt;p&gt;Each day, as they watch their small fund grow, they grasp the magic of compound interest â and that, more than any gift, is a lesson I hope will stay with them for life.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;ð¬ Want to comment or improve the app? Contact me at:&lt;/p&gt;&lt;lb/&gt;@roberdam&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://roberdam.com/en/dinversiones.html"/><published>2025-10-30T10:39:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45760321</id><title>Show HN: I made a heatmap diff viewer for code reviews</title><updated>2025-10-30T19:32:29.282409+00:00</updated><content>&lt;doc fingerprint="5f30552b55a047fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Heatmap color-codes every diff line/token by how much human attention it probably needs. Unlike PR-review bots, we try to flag not just by “is it a bug?” but by “is it worth a second look?” (examples: hard-coded secret, weird crypto mode, gnarly logic).&lt;/p&gt;
    &lt;p&gt;To try it, replace github.com with 0github.com in any GitHub pull request url. Under the hood, we clone the repo into a VM, spin up gpt-5-codex for every diff, and ask it to output a JSON data structure that we parse into a colored heatmap.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;p&gt;Heatmap is open source:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://0github.com"/><published>2025-10-30T14:21:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45760328</id><title>US declines to join more than 70 countries in signing UN cybercrime treaty</title><updated>2025-10-30T19:32:28.981479+00:00</updated><content>&lt;doc fingerprint="8b0db7af9a95f28f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;US declines to join more than 70 countries in signing UN cybercrime treaty&lt;/head&gt;
    &lt;p&gt;More than 70 countries signed the landmark U.N. Convention against Cybercrime in Hanoi this weekend, a significant step in the yearslong effort to create a global mechanism to counteract digital crime.&lt;/p&gt;
    &lt;p&gt;The U.K. and European Union joined China, Russia, Brazil, Nigeria and dozens of other nations in signing the convention, which lays out new mechanisms for governments to coordinate, build capacity and track those who use technology to commit crimes.&lt;/p&gt;
    &lt;p&gt;In his speech at the event, U.N. Secretary-General António Guterres said cyberspace “has become fertile ground for criminals” and has allowed them to “defraud families, steal livelihoods, and drain billions of dollars from our economies.”&lt;/p&gt;
    &lt;p&gt;“The UN Cybercrime Convention is a powerful, legally binding instrument to strengthen our collective defences against cybercrime,” Guterres said.&lt;/p&gt;
    &lt;p&gt;“Illicit flows of money, concealed through cryptocurrencies and digital transactions, finance the trafficking of drugs, arms, and terror. And businesses, hospitals, and airports are brought to a standstill by ransomware attacks.”&lt;/p&gt;
    &lt;p&gt;He added that the convention would be critical for governments in the Global South that need assistance and funding for the training required to address cybercrime — which the U.N. estimates costs $10.5 trillion around the world annually.&lt;/p&gt;
    &lt;p&gt;While many countries did not sign the treaty, the most notable missing signature was that of the U.S.&lt;/p&gt;
    &lt;p&gt;Officials at the State Department told Recorded Future News on Friday that Marc Knapper, the U.S. ambassador to Vietnam, and representatives from the U.S. Mission to Vietnam would be attending the signing.&lt;/p&gt;
    &lt;p&gt;The State Department confirmed on Monday that the U.S. did not sign the treaty.&lt;/p&gt;
    &lt;p&gt;“The United States continues to review the treaty,” a State Department spokesperson said in a brief statement.&lt;/p&gt;
    &lt;p&gt;The U.N. Convention against Cybercrime was adopted by the General Assembly in December 2024 and will enter into force 90 days after being ratified by the 40th signatory. Signatories will have to ratify the convention according to their own procedures.&lt;/p&gt;
    &lt;p&gt;At the ceremony, UNODC Executive Director Ghada Waly argued that cybercrime is changing the face of organized crime and required global coordination to address. Waly said the convention would be a “vital tool” that will ensure “a safer digital world for all.”&lt;/p&gt;
    &lt;p&gt;U.N. officials said the convention would help governments address terrorism, human trafficking, money laundering and drug smuggling, all of which have been turbo-charged by the internet.&lt;/p&gt;
    &lt;p&gt;The U.N. noted that the convention is the first global framework “for the collection, sharing and use of electronic evidence for all serious offenses” — noting that until now there have been no broadly accepted international standards on electronic evidence.&lt;/p&gt;
    &lt;p&gt;It is also the first global treaty to criminalize crimes that depend on the internet and is the first international treaty “to recognize the non-consensual dissemination of intimate images as an offense.”&lt;/p&gt;
    &lt;p&gt;“It creates the first global 24/7 network where countries can quickly initiate cooperation,” the U.N. said. “It recognizes and promotes the need to build capacity in countries to pursue and cooperate on fast-moving cybercrimes.”&lt;/p&gt;
    &lt;p&gt;The convention has been heavily criticized by the tech industry, which has warned that it criminalizes cybersecurity research and exposes companies to legally thorny data requests.&lt;/p&gt;
    &lt;p&gt;Human rights groups warned on Friday that it effectively forces member states to create a broad electronic surveillance dragnet that would include crimes that have nothing to do with technology.&lt;/p&gt;
    &lt;p&gt;Many expressed concern that the convention will be abused by dictatorships and rogue governments who will deploy it against critics or protesters — even those outside of a regime’s jurisdiction.&lt;/p&gt;
    &lt;p&gt;It also creates legal regimes to monitor, store and allow cross-border sharing of information without specific data protections. Access Now’s Raman Jit Singh Chima said the convention effectively justifies “cyber authoritarianism at home and transnational repression across borders.”&lt;/p&gt;
    &lt;p&gt;Any countries ratifying the treaty, he added, risks “actively validating cyber authoritarianism and facilitating the global erosion of digital freedoms, choosing procedural consensus over substantive human rights protection.”&lt;/p&gt;
    &lt;p&gt;In his speech, Guterres referenced the backlash to the convention, telling member states that the treaty has to be a “promise that fundamental human rights such as privacy, dignity, and safety must be protected both offline and online.”&lt;/p&gt;
    &lt;p&gt;But at its core, according to Guterres, the convention solves one of the thorniest issues law enforcement agencies have faced over the last two decades. Countries have only recently begun to share digital evidence across borders but the convention would increase that practice.&lt;/p&gt;
    &lt;p&gt;“This has long been a major obstacle to justice — with perpetrators in one country, victims in another, and data stored in a third,” he said. “The Convention provides a clear pathway for investigators and prosecutors to finally overcome this barrier.”&lt;/p&gt;
    &lt;p&gt;Jonathan Greig&lt;/p&gt;
    &lt;p&gt;is a Breaking News Reporter at Recorded Future News. Jonathan has worked across the globe as a journalist since 2014. Before moving back to New York City, he worked for news outlets in South Africa, Jordan and Cambodia. He previously covered cybersecurity at ZDNet and TechRepublic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://therecord.media/us-declines-signing-cybercrime-treaty?"/><published>2025-10-30T14:22:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45760878</id><title>Free software scares normal people</title><updated>2025-10-30T19:32:28.825294+00:00</updated><content>&lt;doc fingerprint="a23b437c2441cdbd"&gt;
  &lt;main&gt;
    &lt;p&gt;I’m the person my friends and family come to for computer-related help. (Maybe you, gentle reader, can relate.) This experience has taught me which computing tasks are frustrating for normal people.&lt;/p&gt;
    &lt;p&gt;Normal people often struggle with converting video. They will need to watch, upload, or otherwise do stuff with a video, but the format will be weird. (Weird, broadly defined, is anything that won’t play in QuickTime or upload to Facebook.)&lt;/p&gt;
    &lt;p&gt;I would love to recommend Handbrake to them, but the user interface is by and for power users. Opening it makes normal people feel unpleasant feelings.&lt;/p&gt;
    &lt;p&gt;This problem is rampant in free software. The FOSS world is full of powerful tools that only have a “power user” UI. As a result, people give up. Or worse: they ask people like you and I to do it for them.&lt;/p&gt;
    &lt;p&gt;I want to make the case to you that you can (and should) solve this kind of problem in a single evening.&lt;/p&gt;
    &lt;p&gt;Take the example of Magicbrake, a simple front end I built. It hides the power and flexibility of Handbrake. It does only the one thing most people need Handbrake for: taking a weird video file and making it normal. (Normal, for our purposes, means a small MP4 that works just about anywhere.)&lt;/p&gt;
    &lt;p&gt;There is exactly one button.&lt;/p&gt;
    &lt;p&gt;This is a fast and uncomplicated thing to do. Unfortunately, the people who have the ability to solve problems like this are often disinclined to do it.&lt;/p&gt;
    &lt;p&gt;“Why would you make Handbrake less powerful on purpose?”&lt;/p&gt;
    &lt;p&gt;“What if someone wants a different format?”&lt;/p&gt;
    &lt;p&gt;“What about [feature/edge case]?”&lt;/p&gt;
    &lt;p&gt;The answer to all these questions is the same: a person who needs or wants that stuff can use Handbrake. If they don’t need everything Handbrake can do and find it bewildering, they can use this. Everyone wins.&lt;/p&gt;
    &lt;p&gt;It’s a bit like obscuring the less-used functions on a TV remote with tape. The functions still exist if you need them, but you’re not required to contend with them just to turn the TV on.&lt;/p&gt;
    &lt;p&gt;People benefit from stuff like this, and I challenge you to make more of it. Opportunities are everywhere. The world is full of media servers normal people can’t set up. Free audio editing software that requires hours of learning to be useful for simple tasks. Network monitoring tools that seem designed to ward off the uninitiated. Great stuff normal people don’t use. All because there’s only one UI, and it’s designed to do everything.&lt;/p&gt;
    &lt;p&gt;80% of the people only need 20% of the features. Hide the rest from them and you’ll make them more productive and happy. That’s really all it takes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://danieldelaney.net/normal/"/><published>2025-10-30T15:07:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45761042</id><title>ZOZO's Contact Solver for physics-based simulations</title><updated>2025-10-30T19:32:28.331602+00:00</updated><content>&lt;doc fingerprint="3d32dd780db6c590"&gt;
  &lt;main&gt;&lt;p&gt;A contact solver for physics-based simulations involving 👚 shells, 🪵 solids and 🪢 rods. All made by ZOZO.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;💪 Robust: Contact resolutions are completely penetration-free. No snagging intersections.&lt;/item&gt;&lt;item&gt;⏲ Scalable: An extreme case includes beyond 150M contacts. Not just one million.&lt;/item&gt;&lt;item&gt;🚲 Cache Efficient: All on the GPU runs in single precision. No double precision.&lt;/item&gt;&lt;item&gt;🥼 Inextensible: Cloth never extends beyond very strict upper bounds, such as 1%.&lt;/item&gt;&lt;item&gt;📐 Physically Accurate: Our deformable solver is driven by the Finite Element Method.&lt;/item&gt;&lt;item&gt;⚔️ Highly Stressed: We run GitHub Actions to run stress tests 10 times in a row.&lt;/item&gt;&lt;item&gt;🚀 Massively Parallel: Both contact and elasticity solvers are run on the GPU.&lt;/item&gt;&lt;item&gt;🐳 Docker Sealed: Everything is designed to work out of the box.&lt;/item&gt;&lt;item&gt;🌐 JupyterLab Included: Open your browser and run examples right away (Video).&lt;/item&gt;&lt;item&gt;🐍 Documented Python APIs: Our Python code is fully docstringed and lintable (Video).&lt;/item&gt;&lt;item&gt;☁️ Cloud-Ready: Our solver can be seamlessly deployed on major cloud platforms.&lt;/item&gt;&lt;item&gt;✨ Stay Clean: You can remove all traces after use.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;📝 Change History&lt;/item&gt;&lt;item&gt;🎓 Technical Materials&lt;/item&gt;&lt;item&gt;⚡️ Requirements&lt;/item&gt;&lt;item&gt;💨 Getting Started&lt;/item&gt;&lt;item&gt;🐍 How To Use&lt;/item&gt;&lt;item&gt;📚 Python APIs and Parameters&lt;/item&gt;&lt;item&gt;🔍 Obtaining Logs&lt;/item&gt;&lt;item&gt;🖼️ Catalogue&lt;/item&gt;&lt;item&gt;🚀 GitHub Actions&lt;/item&gt;&lt;item&gt;📡 Deploying on Cloud Services&lt;/item&gt;&lt;item&gt;✒️ Citation&lt;/item&gt;&lt;item&gt;🙏 Acknowledgements&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;🧑 💻 Setting Up Your Development Environment (Markdown)&lt;/item&gt;&lt;item&gt;🐞 Bug Fixes and Updates (Markdown)&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;(2025.10.03) Massive refactor of the codebase (Markdown). Note that this change includes breaking changes to our Python APIs.&lt;/item&gt;&lt;item&gt;(2025.08.09) Added a hindsight note in eigensystem analysis to acknowledge prior work by Poya et al. (2023).&lt;/item&gt;&lt;item&gt;(2025.05.01) Simulation states now can be saved and loaded (Video).&lt;/item&gt;&lt;item&gt;(2025.04.02) Added 9 examples. See the catalogue.&lt;/item&gt;&lt;item&gt;(2025.03.03) Added a budget table on AWS.&lt;/item&gt;&lt;item&gt;(2025.02.28) Added a reference branch and a Docker image of our TOG paper.&lt;/item&gt;&lt;item&gt;(2025.2.26) Added Floating Point-Rounding Errors in ACCD in hindsight.&lt;/item&gt;&lt;item&gt;(2025.2.7) Updated the trapped example (Video) with squishy balls.&lt;/item&gt;&lt;/list&gt;&lt;head&gt;More history records&lt;/head&gt;- (2025.1.8) Added a [domino example](./examples/domino.ipynb) [(Video)](https://drive.google.com/file/d/1N9y8eZrjSQhAUhKwiO9w8jW_T18zPnYf/view). - (2025.1.5) Added a [single twist example](./examples/twist.ipynb) [(Video)](https://drive.google.com/file/d/1LDFKS-iBvl2uDdPVKaazQL25tYGEEyXr/view). - (2024.12.31) Added full documentation for Python APIs, parameters, and log files [(GitHub Pages)](https://st-tech.github.io/ppf-contact-solver). - (2024.12.27) Line search for strain limiting is improved [(Markdown)](./articles/bug.md#new-strain-limiting-line-search) - (2024.12.23) Added [(Bug Fixes and Updates)](./articles/bug.md) - (2024.12.21) Added a [house of cards example](./examples/cards.ipynb) [(Video)](https://drive.google.com/file/d/1PMdDnlyCsjinbvICKph_0UcXUfUvvUmZ/view) - (2024.12.18) Added a [frictional contact example](./examples/friction.ipynb): armadillo sliding on the slope [(Video)](https://drive.google.com/file/d/12WGdfDTFIwCT0UFGEZzfmQreM6WSSHet/view) - (2024.12.18) Added a [hindsight](./articles/hindsight.md) noting that the tilt angle was not&lt;list rend="ul"&gt;&lt;item&gt;📚 Published in ACM Transactions on Graphics (TOG) Vol.43, No.6&lt;/item&gt;&lt;item&gt;🎥 Main video (Video)&lt;/item&gt;&lt;item&gt;🎥 Additional video examples (Directory)&lt;/item&gt;&lt;item&gt;🎥 Presentation videos (Short) (Long)&lt;/item&gt;&lt;item&gt;📃 Main paper (PDF) (Hindsight)&lt;/item&gt;&lt;item&gt;📊 Supplementary PDF (PDF)&lt;/item&gt;&lt;item&gt;🤖 Supplementary scripts (Directory)&lt;/item&gt;&lt;item&gt;🔍 Singular-value eigenanalysis (Markdown)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The main branch is undergoing frequent updates and will deviate from the paper 🚧. To retain consistency with the paper, we have created a new branch &lt;code&gt;sigasia-2024&lt;/code&gt;.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;🛠️ Only maintenance updates are planned for this branch.&lt;/item&gt;&lt;item&gt;🚫 General users should not use this branch as it is not optimized for best performance.&lt;/item&gt;&lt;item&gt;🚫 All algorithmic changes listed in this (Markdown) are excluded from this branch.&lt;/item&gt;&lt;item&gt;📦 We also provide a pre-compiled Docker image: &lt;code&gt;ghcr.io/st-tech/ppf-contact-solver-compiled-sigasia-2024:latest&lt;/code&gt;of this branch.&lt;/item&gt;&lt;item&gt;🌐 Template Link for vast.ai&lt;/item&gt;&lt;item&gt;🌐 Template Link for RunPods&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;🔥 A modern NVIDIA GPU (CUDA 12.8 or newer)&lt;/item&gt;&lt;item&gt;🐳 A Docker environment (see below)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Install a 🎮 NVIDIA driver (Link) on your 💻 host system and follow the 📝 instructions below specific to the 🖥️ operating system to get a 🐳 Docker running:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;🐧 Linux&lt;/cell&gt;&lt;cell role="head"&gt;🪟 Windows&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Install the Docker engine from here (Link). Also, install the NVIDIA Container Toolkit (Link). Just to make sure that the Container Toolkit is loaded, run &lt;code&gt;sudo service docker restart&lt;/code&gt;.&lt;/cell&gt;&lt;cell&gt;Install the Docker Desktop (Link). You may need to log out or reboot after the installation. After logging back in, launch Docker Desktop to ensure that Docker is running.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Next, run the following command to start the 📦 container:&lt;/p&gt;&lt;code&gt;$MY_WEB_PORT = 8080  # Web port number for web interface
$IMAGE_NAME = "ghcr.io/st-tech/ppf-contact-solver-compiled:latest"
docker run --rm --gpus all -p ${MY_WEB_PORT}:8080 $IMAGE_NAME&lt;/code&gt;&lt;code&gt;MY_WEB_PORT=8080  # Web port number for web interface
IMAGE_NAME=ghcr.io/st-tech/ppf-contact-solver-compiled:latest
docker run --rm --gpus all -p ${MY_WEB_PORT}:8080 $IMAGE_NAME&lt;/code&gt;&lt;p&gt;⏳ Wait for a while until the container becomes a steady state. Next, open your 🌐 browser and navigate to http://localhost:8080, where &lt;code&gt;8080&lt;/code&gt; is the port number specified in the &lt;code&gt;MY_WEB_PORT&lt;/code&gt; variable.
Keep your terminal window open.&lt;/p&gt;&lt;p&gt;🎉 Now you are ready to go! 🚀&lt;/p&gt;&lt;p&gt;To shut down the container, just press &lt;code&gt;Ctrl+C&lt;/code&gt; in the terminal.
The container will be removed and all traces will be 🧹 cleaned up.&lt;/p&gt;&lt;p&gt;If you wish to build the container from scratch 🛠️, please refer to the cleaner installation guide (Markdown) 📝.&lt;/p&gt;&lt;p&gt;Our frontend is accessible through 🌐 a browser using our built-in JupyterLab 🐍 interface. All is set up when you open it for the first time. Results can be interactively viewed through the browser and exported as needed.&lt;/p&gt;&lt;p&gt;This allows you to interact with the simulator on your 💻 laptop while the actual simulation runs on a remote headless server over 🌍 the internet. This means that you don't have to own ⚙️ NVIDIA hardware, but can rent it at vast.ai or RunPod for less than 💵 $0.5 per hour. For example, this (Video) was recorded on a vast.ai instance. The experience is 👍 good!&lt;/p&gt;&lt;p&gt;Our Python interface is designed with the following principles in mind:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;🛠️ Dynamic Tri/Tet Creation: Relying on non-integrated third-party tools for triangulation, tetrahedralization, and loading can make it difficult to dynamically adjust resolutions. Our built-in tri/tet creation tools eliminate this limitation.&lt;/item&gt;&lt;item&gt;🚫 No Mesh Data: Preparing mesh data using external tools can be cumbersome. Our frontend minimizes this effort by allowing meshes to be created on the fly or downloaded when needed.&lt;/item&gt;&lt;item&gt;🔗 Method Chaining: We adopt the method chaining style from JavaScript, making the API intuitive and easy to understand.&lt;/item&gt;&lt;item&gt;📦 Single Import for Everything: All frontend features are accessible by simply importing with &lt;code&gt;from frontend import App&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here's an example of draping five sheets over a sphere with two corners pinned. Please look into the examples directory for more examples.&lt;/p&gt;&lt;code&gt;# import our frontend
from frontend import App

# make an app
app = App.create("drape")

# create a square mesh resolution 128 spanning the xz plane
V, F = app.mesh.square(res=128, ex=[1, 0, 0], ey=[0, 0, 1])

# add to the asset and name it "sheet"
app.asset.add.tri("sheet", V, F)

# create an icosphere mesh radius 0.5
V, F = app.mesh.icosphere(r=0.5, subdiv_count=4)

# add to the asset and name it "sphere"
app.asset.add.tri("sphere", V, F)

# create a scene
scene = app.scene.create()

# define gap between sheets
gap = 0.01

for i in range(5):

    # add the sheet asset to the scene
    obj = scene.add("sheet")

    # pick two corners
    corner = obj.grab([1, 0, -1]) + obj.grab([-1, 0, -1])

    # place it with an vertical offset and pin the corners
    obj.at(0, gap * i, 0).pin(corner)

    # set fiber directions required for Baraff-Witkin
    obj.direction([1, 0, 0], [0, 0, 1])

    # set the strainlimiting of 5%
    obj.param.set("strain-limit", 0.05)

# add a sphere mesh at a lower position with jitter and set it static collider
scene.add("sphere").at(0, -0.5 - gap, 0).jitter().pin()

# compile the scene and report stats
scene = scene.build().report()

# preview the initial scene
scene.preview()

# create a new session with the compiled scene
session = app.session.create(scene)

# set session params
session.param.set("frames", 100).set("dt", 0.01)

# build this session
session = session.build()

# start the simulation and live-preview the results (image right)
session.start().preview()

# also show streaming logs
session.stream()

# or interactively view the animation sequences
session.animate()

# export all simulated frames
session.export.animation()&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Full API documentation 📖 is available on our GitHub Pages. The major APIs are documented using docstrings ✍️ and compiled with Sphinx ⚙️. We have also included&lt;/p&gt;&lt;code&gt;jupyter-lsp&lt;/code&gt;to provide interactive linting assistance 🛠️ and display docstrings as you type. See this video (Video) for an example. The behaviors can be changed through the settings.&lt;/item&gt;&lt;item&gt;&lt;p&gt;A list of parameters used in&lt;/p&gt;&lt;code&gt;param.set(key,value)&lt;/code&gt;is documented here: (Global Parameters) (Object Parameters).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note&lt;/p&gt;&lt;p&gt;📊 Logs for the simulation can also be queried through the Python APIs 🐍. Here's an example of how to get a list of recorded logs 📝, fetch them 📥, and compute the average 🧮.&lt;/p&gt;&lt;code&gt;# get a list of log names
logs = session.get.log.names()
print(logs)
assert "time-per-frame" in logs
assert "newton-steps" in logs

# get a list of time per video frame
msec_per_video = session.get.log.numbers("time-per-frame")

# compute the average time per video frame
print("avg per frame:", sum([n for _, n in msec_per_video]) / len(msec_per_video))

# get a list of newton steps
newton_steps = session.get.log.numbers("newton-steps")

# compute the average of consumed newton steps
print("avg newton steps:", sum([n for _, n in newton_steps]) / len(newton_steps))

# Last 8 lines. Omit for everything.
print("==== log stream ====")
for line in session.get.log.stdout(n_lines=8):
    print(line)&lt;/code&gt;&lt;p&gt;Below are some representatives. &lt;code&gt;vid_time&lt;/code&gt; refers to the video time in seconds and is recorded as &lt;code&gt;float&lt;/code&gt;.
&lt;code&gt;ms&lt;/code&gt; refers to the consumed simulation time in milliseconds recorded as &lt;code&gt;int&lt;/code&gt;.
&lt;code&gt;vid_frame&lt;/code&gt; is the video frame count recorede as &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Name&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;cell role="head"&gt;Format&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;time-per-frame&lt;/cell&gt;&lt;cell&gt;Time per video frame&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_frame,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;matrix-assembly&lt;/cell&gt;&lt;cell&gt;Matrix assembly time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;pcg-linsolve&lt;/cell&gt;&lt;cell&gt;Linear system solve time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;line-search&lt;/cell&gt;&lt;cell&gt;Line search time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;time-per-step&lt;/cell&gt;&lt;cell&gt;Time per step&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;newton-steps&lt;/cell&gt;&lt;cell&gt;Newton iterations per step&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,count)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;num-contact&lt;/cell&gt;&lt;cell&gt;Contact count&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,count)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;max-sigma&lt;/cell&gt;&lt;cell&gt;Max stretch&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list(vid_time,float)&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The full list of log names and their descriptions is documented here: (GitHub Pages).&lt;/p&gt;&lt;p&gt;Note that some entries have multiple records at the same video time ⏱️. This occurs because the same operation is executed multiple times 🔄 within a single step during the inner Newton's iterations 🧮. For example, the linear system solve is performed at each Newton's step, so if multiple Newton's steps are 🔁 executed, multiple linear system solve times appear in the record at the same 📊 video time.&lt;/p&gt;&lt;p&gt;If you would like to retrieve the raw log stream, you can do so by&lt;/p&gt;&lt;code&gt;# Last 8 lines. Omit for everything.
for line in session.get.log.stdout(n_lines=8):
    print(line)&lt;/code&gt;&lt;p&gt;This will output something like:&lt;/p&gt;&lt;code&gt;* dt: 1.000e-03
* max_sigma: 1.045e+00
* avg_sigma: 1.030e+00
------ newton step 1 ------
   ====== contact_matrix_assembly ======
   &amp;gt; dry_pass...0 msec
   &amp;gt; rebuild...7 msec
   &amp;gt; fillin_pass...0 msec
&lt;/code&gt;&lt;p&gt;If you would like to read &lt;code&gt;stderr&lt;/code&gt;, you can do so using &lt;code&gt;session.get.stderr()&lt;/code&gt; (if it exists). They return &lt;code&gt;list[str]&lt;/code&gt;.
All the log files 📂 are available ✅ and can be fetched ⬇️ during the simulation 💻.&lt;/p&gt;&lt;p&gt;Below is a table summarizing the estimated costs for running our examples on a NVIDIA L4 instance &lt;code&gt;g6.2xlarge&lt;/code&gt; at Amazon Web Services US regions (&lt;code&gt;us-east-1&lt;/code&gt; and &lt;code&gt;us-east-2&lt;/code&gt;).&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;💰 Uptime cost is approximately $1 per hour.&lt;/item&gt;&lt;item&gt;⏳ Deployment time is approximately 8 minutes ($0.13). Instance loading takes 3 minutes, and Docker pull &amp;amp; load takes 5 minutes.&lt;/item&gt;&lt;item&gt;🎮 The NVIDIA L4 delivers 30.3 TFLOPS for FP32, offering approximately 36% of the performance of an RTX 4090.&lt;/item&gt;&lt;item&gt;🎥 Video frame rate is 60fps.&lt;/item&gt;&lt;/list&gt;&lt;table&gt;&lt;row span="9"&gt;&lt;cell role="head"&gt;Example&lt;/cell&gt;&lt;cell role="head"&gt;Cost&lt;/cell&gt;&lt;cell role="head"&gt;Time&lt;/cell&gt;&lt;cell role="head"&gt;#Frame&lt;/cell&gt;&lt;cell role="head"&gt;#Vert&lt;/cell&gt;&lt;cell role="head"&gt;#Face&lt;/cell&gt;&lt;cell role="head"&gt;#Tet&lt;/cell&gt;&lt;cell role="head"&gt;#Seg&lt;/cell&gt;&lt;cell role="head"&gt;Max Strain&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;trapped&lt;/cell&gt;&lt;cell&gt;$0.37&lt;/cell&gt;&lt;cell&gt;22.6m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;263K&lt;/cell&gt;&lt;cell&gt;299K&lt;/cell&gt;&lt;cell&gt;885K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;twist&lt;/cell&gt;&lt;cell&gt;$0.91&lt;/cell&gt;&lt;cell&gt;55m&lt;/cell&gt;&lt;cell&gt;500&lt;/cell&gt;&lt;cell&gt;203K&lt;/cell&gt;&lt;cell&gt;406K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;stack&lt;/cell&gt;&lt;cell&gt;$0.60&lt;/cell&gt;&lt;cell&gt;36.2m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;166.7K&lt;/cell&gt;&lt;cell&gt;327.7K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;trampoline&lt;/cell&gt;&lt;cell&gt;$0.74&lt;/cell&gt;&lt;cell&gt;44.5m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;56.8K&lt;/cell&gt;&lt;cell&gt;62.2K&lt;/cell&gt;&lt;cell&gt;158.0K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;needle&lt;/cell&gt;&lt;cell&gt;$0.31&lt;/cell&gt;&lt;cell&gt;18.4m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;86K&lt;/cell&gt;&lt;cell&gt;168.9K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;cards&lt;/cell&gt;&lt;cell&gt;$0.29&lt;/cell&gt;&lt;cell&gt;17.5m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;8.7K&lt;/cell&gt;&lt;cell&gt;13.8K&lt;/cell&gt;&lt;cell&gt;1.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;domino&lt;/cell&gt;&lt;cell&gt;$0.12&lt;/cell&gt;&lt;cell&gt;4.3m&lt;/cell&gt;&lt;cell&gt;250&lt;/cell&gt;&lt;cell&gt;0.5K&lt;/cell&gt;&lt;cell&gt;0.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;drape&lt;/cell&gt;&lt;cell&gt;$0.10&lt;/cell&gt;&lt;cell&gt;3.5m&lt;/cell&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;81.9K&lt;/cell&gt;&lt;cell&gt;161.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;curtain&lt;/cell&gt;&lt;cell&gt;$0.33&lt;/cell&gt;&lt;cell&gt;19.6m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;64K&lt;/cell&gt;&lt;cell&gt;124K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;friction&lt;/cell&gt;&lt;cell&gt;$0.17&lt;/cell&gt;&lt;cell&gt;10m&lt;/cell&gt;&lt;cell&gt;700&lt;/cell&gt;&lt;cell&gt;1.1K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;hang&lt;/cell&gt;&lt;cell&gt;$0.12&lt;/cell&gt;&lt;cell&gt;7.5m&lt;/cell&gt;&lt;cell&gt;200&lt;/cell&gt;&lt;cell&gt;16.3K&lt;/cell&gt;&lt;cell&gt;32.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;belt&lt;/cell&gt;&lt;cell&gt;$0.19&lt;/cell&gt;&lt;cell&gt;11.4m&lt;/cell&gt;&lt;cell&gt;200&lt;/cell&gt;&lt;cell&gt;12.3K&lt;/cell&gt;&lt;cell&gt;23.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;codim&lt;/cell&gt;&lt;cell&gt;$0.36&lt;/cell&gt;&lt;cell&gt;21.6m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;122.7K&lt;/cell&gt;&lt;cell&gt;90K&lt;/cell&gt;&lt;cell&gt;474.1K&lt;/cell&gt;&lt;cell&gt;1.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;fishingknot&lt;/cell&gt;&lt;cell&gt;$0.38&lt;/cell&gt;&lt;cell&gt;22.5m&lt;/cell&gt;&lt;cell&gt;830&lt;/cell&gt;&lt;cell&gt;19.6K&lt;/cell&gt;&lt;cell&gt;36.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;fitting&lt;/cell&gt;&lt;cell&gt;$0.03&lt;/cell&gt;&lt;cell&gt;1.54m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;28.4K&lt;/cell&gt;&lt;cell&gt;54.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;10%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;noodle&lt;/cell&gt;&lt;cell&gt;$0.14&lt;/cell&gt;&lt;cell&gt;8.45m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;116.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;116.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;ribbon&lt;/cell&gt;&lt;cell&gt;$0.23&lt;/cell&gt;&lt;cell&gt;13.9m&lt;/cell&gt;&lt;cell&gt;480&lt;/cell&gt;&lt;cell&gt;34.9K&lt;/cell&gt;&lt;cell&gt;52.9K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;woven&lt;/cell&gt;&lt;cell&gt;$0.58&lt;/cell&gt;&lt;cell&gt;34.6m&lt;/cell&gt;&lt;cell&gt;450&lt;/cell&gt;&lt;cell&gt;115.6K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;115.4K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;yarn&lt;/cell&gt;&lt;cell&gt;$0.01&lt;/cell&gt;&lt;cell&gt;0.24m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;28.5K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;28.5K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;roller&lt;/cell&gt;&lt;cell&gt;$0.03&lt;/cell&gt;&lt;cell&gt;2.08m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;21.4K&lt;/cell&gt;&lt;cell&gt;22.2K&lt;/cell&gt;&lt;cell&gt;61.0K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Large scale examples are run on a vast.ai instance with an RTX 4090. At the moment, not all large scale examples are ready yet, but they will be added/updated one by one. The author is actively woriking on it.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell&gt;large-twist (Video)&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="9"&gt;&lt;cell role="head"&gt;Example&lt;/cell&gt;&lt;cell role="head"&gt;Commit&lt;/cell&gt;&lt;cell role="head"&gt;#Vert&lt;/cell&gt;&lt;cell role="head"&gt;#Face&lt;/cell&gt;&lt;cell role="head"&gt;#Tet&lt;/cell&gt;&lt;cell role="head"&gt;#Seg&lt;/cell&gt;&lt;cell role="head"&gt;#Contact&lt;/cell&gt;&lt;cell role="head"&gt;#Frame&lt;/cell&gt;&lt;cell role="head"&gt;Time/Frame&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;large-twist&lt;/cell&gt;&lt;cell&gt;cbafbd2&lt;/cell&gt;&lt;cell&gt;3.2M&lt;/cell&gt;&lt;cell&gt;6.4M&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;56.7M&lt;/cell&gt;&lt;cell&gt;2,000&lt;/cell&gt;&lt;cell&gt;46.4s&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;We implemented GitHub Actions that test all of our examples except for large scale ones, which take from hours to days to finish. We perform explicit intersection checks 🔍 at the end of each step, which raises an error ❌ if an intersection is detected. This ensures that all steps are confirmed to be penetration-free if tests are pass ✅. The runner types are described as follows:&lt;/p&gt;&lt;p&gt;The tested 🚀 runner of this action is the Ubuntu NVIDIA GPU-Optimized Image for AI and HPC with an NVIDIA Tesla T4 (16 GB VRAM) with Driver version 570.133.20. This is not a self-hosted runner, meaning that each time the runner launches, all environments are 🌱 fresh.&lt;/p&gt;&lt;p&gt;We use the GitHub-hosted runner 🖥️, but the actual simulation runs on a &lt;code&gt;g6e.2xlarge&lt;/code&gt; AWS instance 🌐.
Since we start with a fresh 🌱 instance, the environment is clean 🧹 every time.
We take advantage of the ability to deploy on the cloud; this action is performed in parallel, which reduces the total action time.&lt;/p&gt;&lt;p&gt;We generate zipped action artifacts 📦 for each run. These artifacts include:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;📝 Logs: Detailed logs of the simulation runs.&lt;/item&gt;&lt;item&gt;📊 Metrics: Performance metrics and statistics.&lt;/item&gt;&lt;item&gt;📹 Videos: Simulated animations.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Please note that these artifacts will be deleted after a month.&lt;/p&gt;&lt;p&gt;We know that you can't judge the reliability of contact resolution by simply watching a single success 🎥 video example. To ensure greater transparency, we implemented GitHub Actions to run many of our examples via automated GitHub Actions ⚙️, not just once, but 10 times in a row 🔁. This means that a single failure out of 10 tests is considered a failure of the entire test suite!&lt;/p&gt;&lt;p&gt;Also, we apply small jitters to the position of objects in the scene 🔄, so at each run, the scene is slightly different.&lt;/p&gt;&lt;p&gt;Our contact solver is designed for heavy use in cloud services ☁️, enabling:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;💰 Cost-Effective Development: Quickly deploy testing environments 🚀 and delete 🗑️ them when not in use, saving costs.&lt;/item&gt;&lt;item&gt;📈 Flexible Scalability: Scale as needed based on demand 📈. For example, you can launch multiple instances before a specific deadline ⏰.&lt;/item&gt;&lt;item&gt;🌍 High Accessibility: Allow anyone with an internet connection 🌍 to try our solver, even on a smartphone 📱 or tablet 🖥️.&lt;/item&gt;&lt;item&gt;🐛 Easier Bug Tracking: Users and developers can easily share the same hardware, kernel, and driver environment, making it easier to track and fix bugs.&lt;/item&gt;&lt;item&gt;🛠️ Free Maintenance Cost: No need to maintain hardware for everyday operations or introduce redundancy for malfunctions.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is made possible with our purely web-based frontends 🌐 and scalable capability 🧩. Our main target is the NVIDIA L4 🖱️, a data-center-targeted GPU 🖥️ that offers reasonable pricing 💲, delivering both practical performance 💪 and scalability 📊 without investing in expensive hardware 💻.&lt;/p&gt;&lt;p&gt;Below, we describe how to deploy our solver on major cloud services ☁️. These instructions are up to date as of late 2024 📅 and are subject to change 🔄.&lt;/p&gt;&lt;p&gt;Important: For all the services below, don't forget to ❌ delete the instance after use, or you’ll be 💸 charged for nothing.&lt;/p&gt;&lt;head rend="h3"&gt;📦 Deploying on vast.ai&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Select our template (Link).&lt;/item&gt;&lt;item&gt;Create an instance and click &lt;code&gt;Open&lt;/code&gt;button.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;📦 Deploying on RunPod&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Follow this link (Link) and deploy an instance using our template.&lt;/item&gt;&lt;item&gt;Click &lt;code&gt;Connect&lt;/code&gt;button and open the&lt;code&gt;HTTP Services&lt;/code&gt;link.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;📦 Deploying on Scaleway&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Set zone to &lt;code&gt;fr-par-2&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Select type &lt;code&gt;L4-1-24G&lt;/code&gt;or&lt;code&gt;GPU-3070-S&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Choose &lt;code&gt;Ubuntu Jammy GPU OS 12&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/item&gt;&lt;item&gt;This setup costs approximately €0.76 per hour.&lt;/item&gt;&lt;item&gt;CLI instructions are described in (Markdown).&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;📦 Deploying on Amazon Web Services&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Amazon Machine Image (AMI): &lt;code&gt;Deep Learning Base AMI with Single CUDA (Ubuntu 22.04)&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Instance Type: &lt;code&gt;g6.2xlarge&lt;/code&gt;(Recommended)&lt;/item&gt;&lt;item&gt;This setup costs around $1 per hour.&lt;/item&gt;&lt;item&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;📦 Deploying on Google Compute Engine&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Select&lt;/p&gt;&lt;code&gt;GPUs&lt;/code&gt;. We recommend the GPU type&lt;code&gt;NVIDIA L4&lt;/code&gt;because it's affordable and accessible, as it does not require a high quota. You may select&lt;code&gt;T4&lt;/code&gt;instead for testing purposes.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Do not check&lt;/p&gt;&lt;code&gt;Enable Virtual Workstation (NVIDIA GRID)&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;We recommend the machine type&lt;/p&gt;&lt;code&gt;g2-standard-8&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Choose the OS type&lt;/p&gt;&lt;code&gt;Deep Learning VM with CUDA 12.4 M129&lt;/code&gt;and set the disk size to&lt;code&gt;50GB&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;As of late 2024, this configuration costs approximately $0.86 per hour in&lt;/p&gt;&lt;code&gt;us-central1 (Iowa)&lt;/code&gt;and $1.00 per hour in&lt;code&gt;asia-east1 (Taiwan)&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Port number&lt;/p&gt;&lt;code&gt;8080&lt;/code&gt;is reserved by the OS image. Set&lt;code&gt;$MY_WEB_PORT&lt;/code&gt;to&lt;code&gt;8888&lt;/code&gt;. When connecting via&lt;code&gt;gcloud&lt;/code&gt;, use the following format:&lt;code&gt;gcloud compute ssh --zone "xxxx" "instance-name" -- -L 8080:localhost:8888&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;CLI instructions are described in (Markdown).&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;code&gt;@software{ppf-contact-solver-2024,
    title = {ZOZO's Contact Solver},
    author = {Ryoichi Ando},
    note = {https://github.com/st-tech/ppf-contact-solver},
    year = 2024,
}&lt;/code&gt;&lt;p&gt;The author thanks ZOZO, Inc. for permitting the release of the code and the team members for assisting with the internal paperwork for this project.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/st-tech/ppf-contact-solver"/><published>2025-10-30T15:21:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45761445</id><title>Affinity Studio now free</title><updated>2025-10-30T19:32:27.986052+00:00</updated><content>&lt;doc fingerprint="3bd67e5e966d06c5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Get Affinity&lt;/head&gt;
    &lt;p&gt;Available on desktop for&lt;/p&gt;
    &lt;p&gt;The all-in-one creative app, with everything you need to craft designs, edit images, and lay it all out, without ever leaving your document or paying a thing.&lt;/p&gt;
    &lt;quote&gt;$0, free&lt;/quote&gt;
    &lt;p&gt;To download Affinity, sign in with your Canva account (or create one for free).&lt;/p&gt;
    &lt;head rend="h2"&gt;One powerful app. No cost.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Fully-featured toolsets&lt;/p&gt;
        &lt;p&gt;From vector to pixel to layout, Affinity has all the studio-grade tools you need under one roof.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customizable studios&lt;/p&gt;
        &lt;p&gt;Mix and match your favorite tools to build your very own creative studios.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-destructive editing&lt;/p&gt;
        &lt;p&gt;Experiment as much you want, keep your original files intact.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pixel-perfect export&lt;/p&gt;
        &lt;p&gt;Full control over how your work leaves the app, whether it’s by object, slice, or doc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What you’ll get&lt;/head&gt;
    &lt;p&gt;With Affinity, you’ll get all the professional tools you need for your design, photo editing, and page layout projects, free of charge. If you’re on a Canva premium plan, you’ll also be able to unlock Canva AI tools directly in Affinity for a super-powered workflow.&lt;/p&gt;
    &lt;p&gt;+ Canva premium plans&lt;/p&gt;
    &lt;head rend="h2"&gt;Design workflows&lt;/head&gt;
    &lt;p&gt;Access all vector design, photo editing, and page layout tools in one app&lt;/p&gt;
    &lt;p&gt;Combine vector and pixel work on the same .af document&lt;/p&gt;
    &lt;p&gt;Customize your workspace with floating toolbars and studio presets&lt;/p&gt;
    &lt;p&gt;Real-time performance engine for ultra-smooth editing&lt;/p&gt;
    &lt;p&gt;Non-destructive editing across layers, filters, and adjustments&lt;/p&gt;
    &lt;p&gt;Import PSD, AI, PDF, SVG, IDML and more with high fidelity&lt;/p&gt;
    &lt;p&gt;Export with one-click presets or custom slice-based output&lt;/p&gt;
    &lt;p&gt;Quick export direct to Canva&lt;/p&gt;
    &lt;head rend="h2"&gt;Powerful photo editing&lt;/head&gt;
    &lt;p&gt;Live filters and adjustments with instant preview&lt;/p&gt;
    &lt;p&gt;Full RAW editing, tone mapping, and lens correction&lt;/p&gt;
    &lt;p&gt;Advanced retouching: inpainting brush, healing tools, dodge and burn&lt;/p&gt;
    &lt;p&gt;Batch processing with recordable macros, HDR merge, panorama stitching, and more&lt;/p&gt;
    &lt;head rend="h2"&gt;Pro vector design&lt;/head&gt;
    &lt;p&gt;Precision drawing with pen, node, and pencil tools&lt;/p&gt;
    &lt;p&gt;Live shape editing, booleans, and shape builder&lt;/p&gt;
    &lt;p&gt;Flexible gradients with full control&lt;/p&gt;
    &lt;p&gt;Trace pixel images&lt;/p&gt;
    &lt;p&gt;Pixel-perfect vector tools for illustration and layout&lt;/p&gt;
    &lt;head rend="h2"&gt;Advanced page layout&lt;/head&gt;
    &lt;p&gt;Linked text frames with autoflow and live text wrapping&lt;/p&gt;
    &lt;p&gt;Smart master pages with overrides and reusable layouts&lt;/p&gt;
    &lt;p&gt;Pro typography: ligatures, stylistic sets, drop caps, and variable fonts&lt;/p&gt;
    &lt;p&gt;Print-ready output: CMYK, spot colours, preflight, bleed, and slug support&lt;/p&gt;
    &lt;p&gt;Data merge from .csv with tokens, image merge, and conditional logic&lt;/p&gt;
    &lt;head rend="h2"&gt;Canva AI Studio&lt;/head&gt;
    &lt;p&gt;Generative Fill, Expand, and Edit&lt;/p&gt;
    &lt;p&gt;Generate Images and Vectors&lt;/p&gt;
    &lt;p&gt;Remove Background and Subject Selection&lt;/p&gt;
    &lt;p&gt;Colorize, Depth Selection, and Super Resolution&lt;/p&gt;
    &lt;p&gt;Portrait Blur and Portrait Lighting&lt;/p&gt;
    &lt;p&gt;Full AI generation history&lt;/p&gt;
    &lt;head rend="h2"&gt;Need Affinity for your organization?&lt;/head&gt;
    &lt;p&gt;Skip the individual downloads and get your entire team on Affinity with SSO via a Canva Enterprise or Canva Districts account. Choose an option below to get started.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, Affinity really is free. That doesn’t mean you’re getting a watered-down version of the app though. You can use every tool in the Pixel, Vector, and Layout studios, plus all of the customization and export features, as much as you want, with no restrictions or payment needed. The app will also receive free updates with new features and improvements added.&lt;/p&gt;
        &lt;p&gt;If you’re on a Canva premium plan (Pro, Business, Enterprise, Education), you’ll also be able to unlock Canva’s powerful AI tools within Affinity via the Canva AI Studio.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. Affinity is now brought to you by Canva, and your Canva account gives you access to Affinity and other Canva products and features.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No. You can access all of Affinity’s vector, layout, and pixel tools for free without a Canva subscription. If you’d like to unlock Canva AI tools within Affinity, however, you will need a premium Canva plan.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is a brand-new product that gives you advanced photo editing, graphic design, and page layout tools under one roof. It includes highly requested features such as Image Trace, ePub support, mesh gradients, hatch fills, live glitch filter, as well as custom capabilities that allow you to rearrange panels and combine tools to build your own unique studios. Plus, with a Canva premium plan, you can unlock incredibly powerful AI tools such as Generative Fill, Generative Expand, Generate Image/Vector, and more — directly in Affinity.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. With a Canva premium plan you can unlock Canva AI features in Affinity.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No, these are only available to those with Canva premium accounts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is currently available on Windows and macOS (iPadOS coming soon!).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We’re busy building our iPad version — stay tuned for updates!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is optimized for the latest hardware, including Apple silicon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Absolutely! The new desktop version of Affinity can open all files created in Affinity V2 or V1 apps. However, Affinity V1 and V2 cannot open files that are created or saved in the newer app, Affinity by Canva.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No, it’s the same app, just available on different operating systems.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, you can install Affinity on as many devices as you like.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes! It’s easy to import PSDs, AIs, IDMLs, DWGs, and other file types into Affinity, with structure, layers, and creative intent preserved.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is available in English, French, German, Italian, Spanish, Portuguese, Japanese, Chinese, Bahasa Indonesian, and Turkish. Keep an eye out for more languages coming soon!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Get in touch to speak to our team about how your organization can get set up with Affinity, including SSO.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then all you need to do is stay in one of our pre-built studios: Pixel, Vector or Layout. You’ll find all your favorite tools there, plus some new ones. Since it’s all free, just think of the other creative toolsets as an added bonus!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That’s totally fine. Your Affinity V2 license (via Serif) remains valid and Serif will continue to keep activation servers online. But please note that these apps won’t receive future updates.&lt;/p&gt;
        &lt;p&gt;For the best experience, we recommend using the new Affinity by Canva app.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;No. The new desktop version of Affinity can open all files created in V2, but older versions (including V2 on iPad) cannot open newer Affinity (.af) files, meaning you won’t be able to work across both platforms.&lt;/p&gt;&lt;lb/&gt;We don’t have a release date for the new Affinity on iPad yet, so recommend continuing to run V2 independently while you enjoy the new Affinity on desktop.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. The new Affinity by Canva app will receive free updates and new features over time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You will need to be online to download and activate your license with your free Canva account. From then on, there is no requirement to be online, even with extended offline periods.&lt;/p&gt;
        &lt;p&gt;There are a couple of things to keep in mind:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;There are some features which do require you to be online, if you choose to use them, such as product help, lessons, stock libraries and integrations with Canva including AI tools.&lt;/item&gt;
          &lt;item&gt;We’ll also be releasing new updates and patches regularly, so we recommend connecting from time to time to keep your app up to date, but it's not a requirement of use.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You need a Canva premium plan to unlock all of Canva’s AI features in Affinity. Simply download the Affinity app via our Downloads page and follow the prompts once you click ‘Canva AI Studio’.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.affinity.studio/get-affinity"/><published>2025-10-30T15:54:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45761789</id><title>Qt Creator 18 Released</title><updated>2025-10-30T19:32:27.072552+00:00</updated><content>&lt;doc fingerprint="1f24de105045bbe7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Qt Creator 18 released&lt;/head&gt;
    &lt;p&gt;October 30, 2025 by Eike Ziller | Comments&lt;/p&gt;
    &lt;head rend="h5"&gt;We are happy to announce the release of Qt Creator 18!&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 adds experimental support for Development Containers and many more improvements.&lt;/p&gt;
    &lt;head rend="h4"&gt;Development Container Support&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 adds support for development containers to automate setting up the development environment of a project. It detects a "devcontainer.json" file in your project directory and creates a Docker container for it. You can let Qt Creator auto-detect kits or specify custom kits and control other aspects like the command bridge (our service for communicating with remote devices) with Qt Creator specific customizations in the development container definition. Note that it is still experimental and does not support all aspects of development containers yet. Enable the extension to use this functionality. Find out more.&lt;/p&gt;
    &lt;head rend="h4"&gt;General UI&lt;/head&gt;
    &lt;p&gt;We added an Overview tab on Welcome mode that aggregates content from the other tabs. It suggests tutorials and examples based on your experience and needs, and highlights developer-targeted posts in the Qt blog.&lt;/p&gt;
    &lt;p&gt;The notifications received a facelift and are now part of the progress notification popups. You can opt-out of this with Environment &amp;gt; Interface &amp;gt; Prefer banner style info bars over pop-ups.&lt;/p&gt;
    &lt;head rend="h4"&gt;Editing&lt;/head&gt;
    &lt;p&gt;We added the option to use tabbed editors (Environment &amp;gt; Interface &amp;gt; Use tabbed editors). But remember faster ways of navigating your code, such as Locator filters for opening files or jumping to specific class or symbol, Follow Symbol, Find References, the Open Documents and File System views, the edit location history Window &amp;gt; Go Back/Forward and the corresponding keyboard shortcuts, and Window &amp;gt; Previous/Next Open Document in History and the corresponding keyboard shortcuts.&lt;/p&gt;
    &lt;p&gt;For the C++ support we updated Clangd/LLVM to the 21.1 release for our prebuilt binaries. Additionally, the built-in code model received a wide range of fixes for newer C++ features. We added quick fixes for removing curly braces and for adding definitions for static data members.&lt;/p&gt;
    &lt;p&gt;For QML you can now download and use the latest QML Language Server even if you are using older Qt versions for your projects (in the QML Language Server settings in Preferences &amp;gt; Language Client).&lt;/p&gt;
    &lt;p&gt;We also added support for GitHub Enterprise environments for GitHub Copilot.&lt;/p&gt;
    &lt;head rend="h4"&gt;Projects&lt;/head&gt;
    &lt;p&gt;We moved the ".user" files that contain the Qt Creator specific project settings into the ".qtcreator/" subdirectory of the project directory. Existing ".user" files from older projects are still updated for compatibility though.&lt;/p&gt;
    &lt;p&gt;In Projects mode you can now choose to only show kits that are actually usable by the project, or only kits that the project is already configured for. We also split up the Run page into Deploy Settings and Run Settings, and together with the Build Settings moved them out of the kit selection to tabs in the content view. Normally the run configurations of the various build configurations are independent of each other. In Qt Creator 18 we have added the option to sync the run configurations within a single kit, or even between all kits that the project is configured for.&lt;/p&gt;
    &lt;p&gt;For CMake projects we now also support Test Presets and added a Locator filter "ct" for running CTest based tests. We also fixed building CMake projects for all build configurations (Build &amp;gt; Build Project for All Configurations).&lt;/p&gt;
    &lt;head rend="h4"&gt;Devices&lt;/head&gt;
    &lt;p&gt;We added a configuration for various tools on remote Linux devices, like GDB server, CMake, clangd, rsync, qmake, and more, and the option to auto-detect them. This improves the configuration of remote devices as build devices. More is to come in future releases in this regard. You can now also decide if Qt Creator should try to automatically re-connect to devices at startup with a new Auto-connect on startup setting. We also fixed that it wasn't possibly to use rsync for deployment when building on a remote device as well as using a remote target device.&lt;/p&gt;
    &lt;head rend="h4"&gt;Other Improvements&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 comes with many more improvements and fixes. For example the Git commit editor now provides many more actions on files, like staging, unstaging, and directly adding files to ".gitignore".&lt;/p&gt;
    &lt;p&gt;Please have a look at our change log for more detailed information.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get Qt Creator 18&lt;/head&gt;
    &lt;p&gt;The new version is available as an update in the Qt Online Installer (commercial, opensource). You also find commercially licensed offline installers on the Qt Account Portal, and opensource packages on our opensource download page. This is a free upgrade for all users.&lt;/p&gt;
    &lt;p&gt;Please post issues in our bug tracker. You can also find us on IRC on #qt-creator on irc.libera.chat, and on the Qt Creator mailing list.&lt;/p&gt;
    &lt;p&gt;You can read the Qt Creator Manual in Qt Creator in the Help mode or access it online in the Qt documentation portal.&lt;/p&gt;
    &lt;head rend="h6"&gt;Blog Topics:&lt;/head&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;head rend="h4"&gt;Subscribe to our newsletter&lt;/head&gt;
    &lt;head rend="h4"&gt;Subscribe Newsletter&lt;/head&gt;
    &lt;head rend="h4"&gt;Try Qt 6.10 Now!&lt;/head&gt;
    &lt;p&gt;Download the latest release here: www.qt.io/download.&lt;/p&gt;
    &lt;p&gt;Qt 6.10 is now available, with new features and improvements for application developers and device creators.&lt;/p&gt;
    &lt;head rend="h4"&gt;We're Hiring&lt;/head&gt;
    &lt;p&gt;Check out all our open positions here and follow us on Instagram to see what it's like to be #QtPeople.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qt.io/blog/qt-creator-18-released"/><published>2025-10-30T16:23:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762012</id><title>Launch HN: Propolis (YC X25) – Browser agents that QA your web app autonomously</title><updated>2025-10-30T19:32:26.886823+00:00</updated><link href="https://app.propolis.tech/#/launch"/><published>2025-10-30T16:40:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762259</id><title>The ear does not do a Fourier transform</title><updated>2025-10-30T19:32:26.611804+00:00</updated><content>&lt;doc fingerprint="a63e213260f69383"&gt;
  &lt;main&gt;
    &lt;p&gt;Let’s talk about how the cochlea computes!&lt;/p&gt;
    &lt;p&gt;The tympanic membrane (eardrum) is vibrated by changes in air pressure (sound waves). Bones in the middle ear amplify and send these vibrations to the fluid-filled, snail-shaped cochlea. Vibrations travel through the fluid to the basilar membrane, which remarkably performs frequency separation1: the stiffer, lighter base resonates with high frequency components of the signal, and the more flexible, heavier apex resonates with lower frequencies. Between the two ends, the resonant frequencies decrease logarithmically in space2.&lt;/p&gt;
    &lt;p&gt;The hair cells on different parts of the basilar membrane wiggle back and forth at the frequency corresponding to their position on the membrane. But how do wiggling hair cells translate to electrical signals? This mechanoelectrical transduction process feels like it could be from a Dr. Seuss world: springs connected to the ends of hair cells open and close ion channels at the frequency of the vibration, which then cause neurotransmitter release. Bruno calls them “trapdoors”. Here’s a visualization:&lt;/p&gt;
    &lt;p&gt;It’s clear that the hardware of the ear is well-equipped for frequency analysis. Nerve fibers serve as filters to extract temporal and frequency information about a signal. Below are examples of filters (not necessarily of the ear) shown in the time domain. On the left are filters that are more localized in time, i.e. when a filter is applied to a signal, it is clear when in the signal the corresponding frequency occurred. On the right are filters that have less temporal specificity, but are more uniformly distributed across frequencies compared to the left one.&lt;/p&gt;
    &lt;p&gt;Wouldn’t it be convenient if the cochlea were doing a Fourier transform, which would fit cleanly into how we often analyze signals in engineering? But no 🙅🏻♀️! A Fourier transform has no explicit temporal precision, and resembles something closer to the waveforms on the right; this is not what the filters in the cochlea look like.&lt;/p&gt;
    &lt;p&gt;We can visualize different filtering schemes, or tiling of the time-frequency domain, in the following figure. In the leftmost box, where each rectangle represents a filter, a signal could be represented at a high temporal resolution (similar to left filters above), but without information about its constituent frequencies. On the other end of the spectrum, the Fourier transform performs precise frequency decomposition, but we cannot tell when in the signal that frequency occurred (similar to right filters)3. What the cochlea is actually doing is somewhere between a wavelet and Gabor. At high frequencies, frequency resolution is sacrificed for temporal resolution, and vice versa at low frequencies.&lt;/p&gt;
    &lt;p&gt;Why would this type of frequency-temporal precision tradeoff be a good representation? One theory, explored in Lewicki 2002, is that these filters are a strategy to reduce the redundancy in the representation of natural sounds. Lewicki performed independent component analysis (ICA) to produce filters maximizing statistical independence, comparing environmental sounds, animal vocalizations, and human speech. The tradeoffs look different for each one, and you can kind of map them to somewhere in the above cartoon.&lt;/p&gt;
    &lt;p&gt;It appears that human speech occupies a distinct time-frequency space. Some speculate that speech evolved to fill a time-frequency space that wasn’t yet occupied by other existing sounds.&lt;/p&gt;
    &lt;p&gt;To drive the theory home, one that we have been hinting at since the outset: forming ecologically-relevant representations makes sense, as behavior is dependent on the environment. It appears that for audition, as well as other sensory modalities, we are doing this. This is a bit of a teaser for efficient coding, which we will get to soon.&lt;/p&gt;
    &lt;p&gt;We’ve talked about some incredible mechanisms that occur at the beginning of the sensory coding process, but it’s truly just the tiny tip of the ice burg. We also glossed over how these computations occur. The next lecture will zoom into the biophysics of computation in neurons.&lt;/p&gt;
    &lt;p&gt;We call this tonotopic organization, which is a mapping from frequency to space. This type of organization also exists in the cortex for other senses in addition to audition, such as retinotopy for vision and somatotopy for touch.&lt;/p&gt;
    &lt;p&gt;The relationship between human pitch perception and frequency is logarithmic. Coincidence? 😮&lt;/p&gt;
    &lt;p&gt;One could argue we should be comparing to a short-time Fourier transform, but this has resolution issues, and is still not what the cochlea appears to be doing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform"/><published>2025-10-30T17:01:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762358</id><title>You can't turn off Copilot in the web versions of Word, Excel, or PowerPoint</title><updated>2025-10-30T19:32:26.522028+00:00</updated><content>&lt;doc fingerprint="568cf13e7e3eb9e7"&gt;
  &lt;main&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The info in this article applies only to when you're signed in to your Microsoft 365 apps with a Microsoft account. For example, a personal outlook.com email address.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Copilot is built with the principles of fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability at its core. Learn more about our approach to responsible AI.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to turn off Copilot in a Microsoft 365 app (for example, Word, PowerPoint, and Excel), you can clear the Enable Copilot checkbox in the app on your Windows or Mac device. When you turn off Copilot in an app, the Copilot icon on the ribbon is disabled and you won't be able to use any Copilot capabilities in that app.&lt;/p&gt;
    &lt;p&gt;As an alternative to turning off Copilot, you can remove the Copilot icon from the ribbon. For more info, see Customize the ribbon in Office. Removing the icon from the ribbon doesn't turn off Copilot; you can still access Copilot through other methods, such as the shortcut menu.&lt;/p&gt;
    &lt;p&gt;Note: If you want a plan that doesn't include Copilot, there is Microsoft 365 Basic or Office Home 2024. Or you can downgrade to Microsoft 365 Personal Classic or Microsoft 365 Family Classic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Use the "Enable Copilot" checkbox to turn off Copilot in Word, PowerPoint, and Excel&lt;/head&gt;
    &lt;p&gt;Note: For information about Outlook, see the Use the “Turn on Copilot” toggle to turn off Copilot in Outlook section later in this article.&lt;/p&gt;
    &lt;p&gt;There is a separate Enable Copilot checkbox in each app and the checkbox only applies to that app on that device. For example, if you want to turn off Copilot in Word and Excel, you need to go to both apps and clear the Enable Copilot checkbox. If you have multiple devices, you need to go to each device and clear the Enable Copilot checkbox for each app. Also, turning off Copilot on a device turns off Copilot for anybody using that device.&lt;/p&gt;
    &lt;p&gt;As of March 13, 2025, the Enable Copilot checkbox is available in the following apps starting with the versions listed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;On Windows:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Word: Version 2412&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Excel: Version 2501&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;PowerPoint: Version 2501&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;OneNote: Version 2502&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On Mac:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Word: Version 16.93&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Excel: Version 16.93.2&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;PowerPoint: Version 16.93.2&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you don't see the Enable Copilot checkbox in the app, check which version of the app that you have. If necessary, update your version of the app on your device.&lt;/p&gt;
    &lt;p&gt;Note: You can’t turn off Copilot in the iOS, Android, or web versions of Word, Excel, or PowerPoint.&lt;/p&gt;
    &lt;p&gt;In the meantime, you can change your privacy settings to turn off Copilot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear the "Enable Copilot" checkbox on Windows devices&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;In your app (for example, Excel), go to File &amp;gt; Options &amp;gt; Copilot.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Clear the Enable Copilot checkbox.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select OK, and then close and restart the app.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to turn Copilot back on, repeat these steps, but select the Enable Copilot checkbox in Step 2.&lt;/p&gt;
    &lt;p&gt;Clearing the Enable Copilot checkbox allows you to turn off Copilot without having to change your account privacy settings. If you previously changed your account privacy settings to turn off Copilot, you can change your account privacy settings back to their previous setting. To change your account privacy settings, see Change privacy settings on Windows devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear the "Enable Copilot" checkbox on Mac devices&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;In your app (for example, Word), select the app menu, and then go to Preferences &amp;gt; Authoring and Proofing Tools &amp;gt; Copilot.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Clear the Enable Copilot checkbox.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Close and restart the app.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to turn Copilot back on, repeat these steps, but select the Enable Copilot checkbox in Step 2.&lt;/p&gt;
    &lt;p&gt;Clearing the Enable Copilot checkbox allows you to turn off Copilot without having to change your account privacy settings. If you previously changed your account privacy settings to turn off Copilot, you can change your account privacy settings back to their previous setting. To change your account privacy settings, see Change privacy settings on Mac devices.&lt;/p&gt;
    &lt;head rend="h2"&gt;Use the “Turn on Copilot” toggle to turn off Copilot in Outlook&lt;/head&gt;
    &lt;p&gt;The steps for turning off Copilot in Outlook are different than for Word, Excel, and PowerPoint. In Outlook, there is a Turn on Copilot toggle.&lt;/p&gt;
    &lt;p&gt;As of June 3, 2025, the Turn on Copilot toggle is available in Outlook on the following platforms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Android: Quick Settings &amp;gt; Copilot&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;iOS: Quick Settings &amp;gt; Copilot&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mac: Quick Settings &amp;gt; Copilot&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Web: Settings &amp;gt; Copilot&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Windows (new Outlook): Settings &amp;gt; Copilot&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To see the Turn on Copilot toggle in Outlook on iOS and Android devices, update to the latest version of Outlook available on the respective app store. For Outlook on Mac devices, update to at least Version 16.95.3 from within Outlook. For Outlook on the web, no update is needed.&lt;/p&gt;
    &lt;p&gt;One other difference for turning off Copilot in Outlook (compared to Word, Excel, and PowerPoint) is that your selection will apply to Outlook on all your devices when you’re signed in with the same account. For example, if you turn off Copilot in Outlook on a Mac device, Copilot will also be turned off for that same account when using Outlook on an iOS device.&lt;/p&gt;
    &lt;p&gt;There is currently no estimated date for when turning Copilot on or off will be available in classic Outlook on Windows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Change your privacy settings to turn off Copilot&lt;/head&gt;
    &lt;p&gt;If your Microsoft 365 apps don't yet have the Enable Copilot checkbox, you can turn off Copilot by changing your account privacy settings.&lt;/p&gt;
    &lt;p&gt;But, when you change your account privacy settings to turn off Copilot, you're also turning off other features in your Microsoft 365 apps that you might want to use. For example, suggested replies in Outlook, text predictions in Word, PowerPoint Designer, and automatic alt text for images. For a list of features that would be turned off, see Connected experiences in Office.&lt;/p&gt;
    &lt;head rend="h3"&gt;Change privacy settings on Windows devices&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;In your app (for example, PowerPoint), go to File &amp;gt; Account &amp;gt; Account Privacy &amp;gt; Manage Settings.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Under Connected experiences, clear the Turn on experiences that analyze your content checkbox.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select OK, and then close and restart the app.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You only have to change your privacy settings in one of the apps. The change will take effect in the other apps next time you open them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Change privacy settings on Mac devices&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;In your app (for example, Word), select the app menu, and then go to Preferences &amp;gt; Personal Settings &amp;gt; Privacy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the Privacy dialog, go to Connected Experiences &amp;gt; Manage Connected Experiences.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Clear the Turn on experiences that analyze your content checkbox.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select OK, and then close and restart the app.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You only have to change your privacy settings in one of the apps. The change will take effect in the other apps next time you open them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://support.microsoft.com/en-us/office/turn-off-copilot-in-microsoft-365-apps-bc7e530b-152d-4123-8e78-edc06f8b85f1"/><published>2025-10-30T17:10:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762837</id><title>Some people can't see mental images</title><updated>2025-10-30T19:32:26.131397+00:00</updated><content>&lt;doc fingerprint="744b991fb9e104fd"&gt;
  &lt;main&gt;
    &lt;p&gt;When Nick Watkins was a child, he pasted articles about space exploration into scrapbooks and drew annotated diagrams of rockets. He knew this because, years later, he still had the scrapbooks, and took them to be evidence that he had been a happy child, although he didn’t remember making them. When he was seven, in the summer of 1969, his father woke him up to watch the moon landing; it was the middle of the night where they lived, near Southampton, in England. He didn’t remember this, either, but he’d been told that it happened. That Christmas, he and his brother were given matching space helmets. He knew that on Christmas morning the helmets had been waiting in the kitchen and that, on discovering his, he felt joy, but this was not a memory, exactly. The knowledge seemed to him more personal than an ordinary fact, but he could not feel or picture what it had been like to be that boy in the kitchen.&lt;/p&gt;
    &lt;p&gt;When he was eight or nine, he read Arthur C. Clarke’s novel “2001: A Space Odyssey” over and over. At the beginning of the book, aliens implant images of tool-using into the minds of man-apes. Near the end, the main character, David Bowman, spools backward through memories of his life:&lt;/p&gt;
    &lt;p&gt;To Nick, these events—the images in the minds of the man-apes, David Bowman’s reliving of his life—were thrilling and otherworldly, with no connection to reality, brought about through the intervention of aliens, in distant, fictional worlds.&lt;/p&gt;
    &lt;p&gt;He became a physicist. He was drawn to statistical physics and quantum mechanics, whose concepts were best described in equations. The abstraction of these ideas suited him.&lt;/p&gt;
    &lt;p&gt;One morning in 1997, when he was thirty-five, he was sitting at breakfast, paging through the newspaper. He started to read an article by a columnist he admired, Michael Bywater. Time was an illusion, Bywater wrote, because you could roll it backward and relive it: “You choose a memory, focus on it, let the rest of the mind go blank, and wait.” Bywater described particular memories of his own, not only the sight but the sound and feel of them—“the special weight of girls in autumn . . . when they lean against you as you walk along.” For some reason, these sentences revealed all at once to Nick what in the whole course of his life he had not realized: that it was possible to see pictures in your mind and use those pictures to reëxperience your past.&lt;/p&gt;
    &lt;p&gt;This was startling information. He knew, of course, that people talked about “picturing” or “visualizing,” but he had always taken this to be just a metaphorical way of saying “thinking.” Now it appeared that, in some incomprehensible sense, people meant these words literally. And then there was the notion of using those mental images to revisit a memory. It was an astonishing idea. Was it possible that this was a thing that people other than Bywater could do? Bywater had written about it quite casually, as though he took it for granted. Nick asked some people he knew, and all of them seemed to be able to do it.&lt;/p&gt;
    &lt;p&gt;He wondered whether there was something wrong with him—some kind of amnesia. He’d had no reason to worry about his memory before. He had a Ph.D. in physics; clearly his mind was functioning reasonably well. He knew the usual facts about his life—his parentage, the places he’d lived as a child, important things that had happened. It had never occurred to him that remembering could be more than that.&lt;/p&gt;
    &lt;p&gt;For many years, Nick would search for information about mental imagery, sporadically and alone. In the beginning, he did not yet know that his inability to visualize—this odd feature of his mind which appeared so insignificant that he hadn’t even noticed it for thirty-five years—would come to seem a central wellspring of his self. But then, in 2015, his condition was given a scientific name, aphantasia, and tens of thousands of people experienced the same shocked realization that he had. A flurry of research in the following decade would uncover associations between mental imagery and a bewildering variety of human traits and capacities: a propensity to hold grudges; autism; a vulnerability to trauma; emotional awareness; ways of making art and hearing music; memory of one’s life.&lt;/p&gt;
    &lt;p&gt;But this was all in the future. In 1997, as much as he interrogated his acquaintances, Nick did not find anyone like him. He couldn’t be the only person who lacked this ability to visualize, he thought. Surely it was extremely unlikely that he was unique. But, until he encountered someone else, he had to admit that it was a working possibility.&lt;/p&gt;
    &lt;p&gt;He went online and started looking. Initially, he found only work from the nineteenth century. The first useful thing he came across was William James’s book “Principles of Psychology.” James referred to observations recorded in 1860 by Gustav Fechner, a German scientist and philosopher. Fechner had subjected his own “optical memory-pictures” to introspective scrutiny and deemed them weak and lacking:&lt;/p&gt;
    &lt;p&gt;Fechner didn’t pursue the subject, however, and it lay dormant until 1880, when it was taken up by Francis Galton, a British scientist who later became notorious as the father of eugenics. Galton, supposing that he could depend on scientists to give accurate answers, wrote to several of them with a query:&lt;/p&gt;
    &lt;p&gt;The responses he received were not at all what he had expected.&lt;/p&gt;
    &lt;p&gt;Finding this Galton study came as a relief to Nick. Now at least he knew that there had been other people lacking mental imagery who’d lived normal lives, so it wasn’t a disease, or a symptom of a brain tumor. Galton had subsequently observed that women and children appeared to have more vivid imagery than the scientists did. “Scientific men as a class,” he concluded, “have feeble powers of visual representation.” Nick found this intriguing. Perhaps his own lack of imagery had somehow enhanced his scientific ability. He knew that there was, among some mathematicians, a kind of snobbery about images—a notion that, even in geometry, drawings were distractions from a purely analytical proof. But he also knew that there were any number of legends in the history of science of visions leading to discoveries. Einstein had visualized himself travelling alongside a beam of light, and this had led to his conception of relativity. The best-known instance that Nick was aware of was the German chemist August Kekulé, to whom the structure of the benzene ring had appeared in a dream:&lt;/p&gt;
    &lt;p&gt;At one point, Nick came across a paper from 1909 that stressed the importance of distinguishing between voluntary imagery (the ability to call up mental pictures at will) and involuntary imagery. Sometimes people who couldn’t call up images on purpose did experience them involuntarily—usually during migraines, or high, hallucinatory fevers, or in dreams, or the hypnagogic state just before sleep. This caught his attention because he was almost certain that he saw images in dreams, although he couldn’t be sure, since nothing remained of the images after he woke. If he was right, and he did see images in sleep, then it was strange that he couldn’t summon them at other times. Was he repressing them?&lt;/p&gt;
    &lt;p&gt;When he searched for scientific studies on imagery in the mid-twentieth century, he found very little. It seemed that the study of imagery had largely disappeared from scientific research from the nineteen-twenties to the fifties, owing in part to the dominance of behaviorism in America, which condemned inquiry into internal psychological states as unscientific. J. B. Watson, behaviorism’s founder, repudiated the existence of mental imagery altogether:&lt;/p&gt;
    &lt;p&gt;Later, researchers would debate whether Watson became a behaviorist because he had no internal imagery, or whether he actually had strong imagery but denied it because of “ideological blindness.”&lt;/p&gt;
    &lt;p&gt;In the nineteen-seventies, Nick discovered, a few psychologists, liberated from mid-century behaviorist orthodoxy, had begun to explore imagery again. A British psychologist named David Marks, for instance, developed the Vividness of Visual Imagery Questionnaire, which sought to measure a person’s ability to picture not only a stationary object but also movement (the characteristic gait of a familiar person), change (the shifting color of the sky at sunrise), and degree of detail (the window of a shop you frequently go to). But the psychologists in the nineteen-seventies were interested in people with typical imagery. When Nick searched for studies on people like himself, he found nothing.&lt;/p&gt;
    &lt;p&gt;Sometime in the early two-thousands, Jim Campbell, a Scottish surveyor in his mid-sixties, made an appointment with a neurologist at the University of Edinburgh named Adam Zeman. Jim had recently had a cardiac procedure, and afterward he’d noticed that he could no longer picture anything in his head. Before the surgery, he used to put himself to sleep by visualizing his children and grandchildren; now he couldn’t see anything at all.&lt;/p&gt;
    &lt;p&gt;Zeman had a general neurology practice—Parkinson’s, M.S., dementia—but he had also been interested in consciousness since he was a student. He speculated that one of the things that made humans different from other primates was their ability to mentally project themselves into the past or future, or into worlds that were purely imaginary. So he was fascinated to encounter, in Jim, a syndrome he had never heard of before, which appeared to be an excision of just this species-defining ability. And yet Jim was clearly very much a human—wry, reserved, down to earth. His neurological, psychiatric, and cognitive tests were all normal. If Jim had not described his condition, Zeman would not have known there was anything unusual about him.&lt;/p&gt;
    &lt;p&gt;Even questions designed to evoke imagery—Which is darker, grass or pine needles? Do squirrels have long or short tails?—Jim answered without hesitation. When Zeman asked him how he could answer without picturing these things, he said that he just knew. Zeman searched for recent scientific papers that could shed light on this strange condition but was unable to find anything useful. The case reminded him of blindsight—a rare phenomenon in which people who can’t see behave as though they can, picking up objects and avoiding obstacles. Their eyes and brains can take in visual information, but the information doesn’t rise to consciousness.&lt;/p&gt;
    &lt;p&gt;Zeman felt that Jim was not the sort of person who would make something like this up, but he wanted proof that his brain was functioning in an unusual manner. He recruited a control group of men of similar age and put them and Jim through cognitive tests in an MRI scanner. Here, he found the neurological correlate that he was looking for. Although Jim’s brain responded normally to tests of recognition (being shown images of famous faces), when he was asked to generate a mental image the scanner showed only faint brain activity, compared with the brain activity in the control group. Instead, there was activation in areas of the frontal lobe that were typically activated in situations of cognitive effort or dissonance. Jim was trying, but failing.&lt;/p&gt;
    &lt;p&gt;In 2010, Zeman, along with several colleagues, published these findings in the journal Neuropsychologia, terming the syndrome “blind imagination.” The science journalist Carl Zimmer noticed the study and wrote an article about it in Discover magazine. In the years that followed, a couple of dozen people contacted Zeman to tell him that they had the same condition, except they’d had it since birth. Zeman sent them questionnaires and tabulated their answers. At this point, he decided that lack of mental imagery was a valid syndrome that ought to have a name. After consulting with a classicist friend, he decided on “aphantasia,” phantasia being defined by Aristotle as the ability to conjure an image in the imagination. In 2015, Zeman co-wrote a paper in Cortex describing the condition as it appeared in twenty-one subjects: “Lives without imagery—Congenital aphantasia.”&lt;/p&gt;
    &lt;p&gt;An article about Zeman’s second paper appeared in the New York Times, and, after that, e-mails poured in. Around seventeen thousand people contacted him. Most were congenital aphantasics, and most not only lacked visual imagery; they could not mentally call up sounds, either, or touch, or the sensation of movement. Many had difficulty recognizing faces. Many said that they had a family member who was aphantasic, too. Most said that they saw images in dreams. Zeman recruited colleagues to work with him, and together they tried to reply to every correspondent.&lt;/p&gt;
    &lt;p&gt;Some people who wrote had once had imagery but lost it. About half of these had lost it as a consequence of physical injury—stroke, meningitis, head trauma, suffocation. The other half attributed their loss to a psychiatric cause—depersonalization syndrome, depression. A few told him that they thought they’d suppressed their capacity to visualize because traumatic memories had made imagery intolerable. Zeman learned that there had been a case in 1883, described by the French neurologist Jean-Martin Charcot, in which a man, Monsieur X, had lost his imagery; at the same time, the world suddenly appeared alien to him, and he became intensely anxious. “I observed a drastic change in my existence that obviously mirrored a remarkable change in my personality,” Monsieur X wrote to Charcot. “Before, I used to be emotional, enthusiastic with a prolific imagination; today I am calm, cold and I lost my imagination.” Another nineteenth-century French neurologist, Jules Cotard, described a patient whose loss of mental imagery was accompanied by what became known as Cotard’s delusion, or walking-corpse syndrome—the belief that he was dead.&lt;/p&gt;
    &lt;p&gt;Zeman also received messages from people who appeared to have the opposite of aphantasia: they told him that their mental pictures were graphic and inescapable. There was evidently a spectrum of mental imagery, with aphantasia on one end and extraordinarily vivid imagery on the other and most people’s experience somewhere in between. Zeman figured that the vivid extreme needed a name as well; he dubbed it hyperphantasia. It seemed that two or three per cent of people were aphantasic and somewhat more were hyperphantasic.&lt;/p&gt;
    &lt;p&gt;Many of his correspondents, he learned, had discovered their condition very recently, after reading about it or hearing it described on the radio. Their whole lives, they had heard people talk about picturing, and imagining, and counting sheep, and visualizing beaches, and seeing in the mind’s eye, and assumed that all those idioms were only metaphors or colorful hyperbole. It was amazing how profoundly people could misunderstand one another, and assume that others didn’t mean what they were saying—how minds could wrest sense out of things that made no sense.&lt;/p&gt;
    &lt;p&gt;Some said that they had a tantalizing feeling that images were somewhere in their minds, only just out of reach, like a word on the tip of their tongue. This sounded right to Zeman—the images must be stored in some way, since aphantasics were able to recognize things. In fact, it seemed that most aphantasics weren’t hampered in their everyday functioning. They had good memories for facts and tasks. But many of them said that they remembered very little about their own lives.&lt;/p&gt;
    &lt;p&gt;Among the e-mails that Zeman received, there were, to his surprise, several from aphantasic professional artists. One of these was Sheri Paisley (at the time, Sheri Bakes), a painter in her forties who lived in Vancouver. When Sheri was young, she’d had imagery so vivid that she sometimes had difficulty distinguishing it from what was real. She painted intricate likenesses of people and animals; portraiture attracted her because she was interested in psychology. Then, when she was twenty-nine, she had a stroke, and lost her imagery altogether.&lt;/p&gt;
    &lt;p&gt;To her, the loss of imagery was a catastrophe. She felt as though her mind were a library that had burned down. She no longer saw herself as a person. Gradually, as she recovered from her stroke, she made her way back to painting, working very slowly. She switched from acrylic paints to oils because acrylics dried too fast. She found that her art had drastically changed. She no longer wanted to paint figuratively; she painted abstractions that looked like galaxies seen through a space telescope. She lost interest in psychology—she wanted to connect to the foundations of the universe.&lt;/p&gt;
    &lt;p&gt;Years later, she remembered that, one night at her parents’ house, when she was still in art school, she had stayed up very late painting. She suddenly felt a strong presence behind her, and, even as she kept working, she felt the presence ask her, What do you want? In her thoughts, she responded, I want to be a great painter, and I will do whatever I have to, except take drugs. Later, she thought, Well, that is what happened. My life is very hard, but my painting is so much better.&lt;/p&gt;
    &lt;p&gt;Sheri had been an artist before she lost her imagery, but there were others who had been aphantasic for as long as they could remember. Isabel Nolan, a well-known Irish artist, had recently discovered, in her forties, while reading about Zeman’s work in New Scientist, that other people could see pictures in their heads:&lt;/p&gt;
    &lt;p&gt;She wondered whether she had always been like this. When she was a child, her mother would occasionally go on business trips, and while she was away Isabel stayed with cousins who lived up the road. She remembered lying in bed one night at her cousins’ house, thinking, What if Mam dies? I can’t remember what she looks like. She was an anxious child, frightened of many things, but this particular thought stuck in her mind for years. Now she wondered how she could have been so upset at the thought that she couldn’t picture her mother unless she’d had a notion, some vestigial memory, that such a thing was possible.&lt;/p&gt;
    &lt;p&gt;Her fear of things vanishing had not gone away. In fact, it had expanded, from her mother to everything. She had lived in Dublin almost all her life, although it would probably have been better for her career if she’d moved to London. As it turned out, it hadn’t held her back—she would be representing Ireland in the Venice Biennale in 2026—but when she was younger she’d wondered if she was making a mistake. She thought that maybe she’d stayed because having the physical infrastructure of her past around helped her to remember it. For a long time, she had felt that everything around her was ephemeral, precarious, not to be relied on:&lt;/p&gt;
    &lt;p&gt;Surely this had something to do with not being able to picture anything when she wasn’t looking at it.&lt;/p&gt;
    &lt;p&gt;At a conference, she heard artists with vivid imagery say that they were often disappointed by their work because it could never match up to the glowing vision in their heads; she felt sorry for them. When she was working on something, she never knew how it would end up. Sometimes she started with an idea, like the cosmos; she liked to look at images of deep space and draw abstractions that resembled them. She thought a lot about subjective experience, but not her own experience in particular—more what it was like to be any human, wandering through the world. She didn’t feel that her work was an extension or expression of herself, so she didn’t mind criticism, or not being understood:&lt;/p&gt;
    &lt;p&gt;Was this because of her aphantasia? If her mind were filled with pictures, would her self feel fuller, more robust? When people learned that they were aphantasic, they tended to wonder whether this or that aspect of themselves was due to their lack of imagery; sometimes it had nothing to do with it, but in this case it did—several studies had found that people with vivid imagery tended to be more inward, absorbed in the drift of their own minds.&lt;/p&gt;
    &lt;p&gt;Someone had told Isabel about a British moral philosopher, Derek Parfit, who had no imagery. He had few memories and little connection to his past, although he felt strong emotions about people and ideas in the present. Parfit believed that a self was not a unique, distinct thing but a collection of shifting memories and thoughts which intersected with the memories and thoughts of others. Ultimately, he thought, selves were not important. What mattered was the moral imperatives that drove everyone, or ought to—preventing suffering, the future welfare of humanity, the search for truth.&lt;/p&gt;
    &lt;p&gt;Isabel, like Parfit, remembered very little about her life. She kept boxes of souvenirs—ticket stubs, programs—but unless she looked at these things, or a friend reminded her, she didn’t recall most of the places she’d visited or things she’d done. She imagined that this could be a problem in a relationship, if you didn’t remember what you’d done together and the other person got upset and accused you of not caring, though fortunately she’d never been with someone like that. When she went out with friends who were full of stories, she’d worry that she wasn’t entertaining enough; normally, she drew people out and got them talking so she didn’t have to:&lt;/p&gt;
    &lt;p&gt;It would be nice to remember all the funny stories that people told, but in the end she didn’t mind too much. She could just sit there and bask in the pleasure of being with old friends. It was the feeling that was important; she didn’t need to know what had happened years ago. In some ways, this made things easier—she mostly didn’t remember arguments or bad feelings. She hoped that the significant moments in her life, good and bad, had left their imprint on her in some way, but it was impossible to know:&lt;/p&gt;
    &lt;p&gt;Clare Dudeney was an artist who worked in southeast London, in a warren of old factory buildings by the Thames. Against one wall of her studio was a wooden loom, above which large spools of cotton thread in a rainbow of colors were slotted on pegs. She made works in many media, all cornucopias of color: pieces of fabric dyed robin’s-egg blue or pistachio or hazelnut or citrine and pasted into collages, some so long that you couldn’t take them in at once and hung near open doors so that they rippled. She made murals of ceramic tiles painted with irregular shapes, like countries on a map, in powder-puff pink and celery and yellow and wheat; rectangular blocks of rough wood that she called woodcut paintings, with teal, red, cornflower, and lime pigment staining or filling the crevices and gouges of the surface; long clay worms, basket-woven and glazed—forest, mustard, chestnut—like ceramic macramé. She draped herself in colors, too: thick scarves and nubby sweaters that she knitted herself; geometric-patterned skirts.&lt;/p&gt;
    &lt;p&gt;In talking to a friend of hers, an aphantasic painter who was one of Zeman’s research subjects, Clare had realized that she was the opposite—hyperphantasic. Her imagery was extraordinarily vivid. There was always so much going on inside her head, her mind skittering and careening about, that it was difficult to focus on what or who was actually in front of her. There were so many pictures and flashes of memory, and glimpses of things she thought were memory but wasn’t sure, and scenarios real and imaginary, and schemes and speculations and notions and plans, a relentless flood of images and ideas continuously coursing through her mind. It was hard to get to sleep.&lt;/p&gt;
    &lt;p&gt;At one point, in an effort to slow the flood, she tried meditation. She went on a ten-day silent retreat, but she disliked it so much—too many rules, getting up far too early—that she rebelled. While sitting in a room with no pictures or stimulation of any kind, supposedly meditating, she decided to watch the first Harry Potter movie in her head. She wasn’t able to recall all two hours of it, but watching what she remembered lasted for forty-five minutes. Then she did the same with the other seven films.&lt;/p&gt;
    &lt;p&gt;She tried not to expose herself to ugly or violent images because she knew they would stick in her mind for years. But even without a picture, if she even heard about violence her mind would produce one. Once, reading about someone undergoing surgery without anesthetic, she imagined it so graphically that she fainted. (In 2012, two Harvard psychologists published a study about visual imagery and moral judgment. They found that people with weak imagery tended to think more abstractly about moral questions and believe that good ends sometimes justified harmful means. But for people with strong imagery, the harmful means—injuries done to one person in order to save several others, say—formed such lurid pictures in their minds that they responded emotionally and rejected them.)&lt;/p&gt;
    &lt;p&gt;Even joyful images could turn on her. She’d had a cat that she loved; she was separated from her husband and living on her own, so she had spent more time with the cat than with any other creature. Then the cat died, and after his death she saw him everywhere—on the sofa, on the floor, on her bed, wherever he had been in life. She saw him so clearly that it was as though he were actually there in front of her. Her grief was made so much worse by this relentless haunting that she began to feel as if she would not be able to cope.&lt;/p&gt;
    &lt;p&gt;Her father was a physicist and for many years the deputy director of the British Antarctic Survey. When Clare was a child, he promised that one day he would take her to Antarctica, and finally, when she was in her thirties, in 2013, he did. There, on the boat, she found herself looking at a landscape so wholly unfamiliar that her brain struggled to make sense of it. At times, it barely appeared to her like a landscape at all, more like an abstract surface, without reference or meaning. The place was vast, and there were no people. Snow and ice formed strange patterns on the surface of the sea. As they travelled, the terrain kept changing, so her sense of alien newness persisted. It was as if, for the first time, she was seeing not through the cluttered, obscuring scrim of her visual memories but directly, at the world itself. Just looking at it was so demanding that it occupied her whole mind, so that she wasn’t thinking about anything else, she was just there. At the time, she was consulting on climate and sustainability issues, but after that trip she decided to become an artist.&lt;/p&gt;
    &lt;p&gt;Usually, her ideas for art works came not from anything external but from images in her head. For a while, she had made paintings based on her dreams. She kept a journal and a pen by the side of her bed so that she could describe what she’d dreamed the moment she woke. The more she wrote down her dreams, the more she remembered them; sometimes she would remember ten dreams in a single night. Eventually, the process began to fold in on itself—while she was still asleep, she’d begin to dream that she was taking notes on the dream, and planning how to draw what she saw.&lt;/p&gt;
    &lt;p&gt;When she thought about making a new piece, she often worked it out in her mind beforehand. Being hyperphantasic didn’t mean only that your imagery was bright and sharp; it meant that you could manipulate your images at will, zooming in and out, cutting and pasting, flipping and mirroring, creating pictures from scratch, assembling and disassembling complicated objects. Even when she was trying to evoke the colors of a landscape at a certain time of day, she did it not from life but from memory.&lt;/p&gt;
    &lt;p&gt;She didn’t know how common this was among artists, but there were some who she was fairly sure had worked from their imaginations rather than from life. J. M. W. Turner, for instance, made rough sketches outdoors, but the seas and skies and light of his paintings all came from his head. There was an English portraitist working in the late eighteenth century whose prodigious powers of visualization had been described in a case study. The study didn’t name the painter but said that he’d inherited most of the clients of Sir Joshua Reynolds after Reynolds’s death, and had proceeded to take full advantage of this by painting three hundred portraits in a single year. The study’s author, a British physician named A. L. Wigan, reported:&lt;/p&gt;
    &lt;p&gt;This painter’s imagery was so lifelike, however, that he began to confuse his mind’s pictures with reality, and succumbed to a mental illness that lasted thirty years.&lt;/p&gt;
    &lt;p&gt;Hyperphantasia often seemed to function as an emotional amplifier in mental illness—heightening hypomania, worsening depression, causing intrusive traumatic imagery in P.T.S.D. to be more realistic and disturbing. Reshanne Reeder, a neuroscientist at the University of Liverpool, began interviewing hyperphantasics in 2021 and found that many of them had a fantasy world that they could enter at will. But they were also prone to what she called maladaptive daydreaming. They might become so absorbed while on a walk that they would wander, not noticing their surroundings, and get lost. It was difficult for them to control their imaginations: once they pictured something, it was hard to get rid of it. It was so easy for hyperphantasics to imagine scenes as lifelike as reality that they could later become unsure what had actually happened and what had not.&lt;/p&gt;
    &lt;p&gt;One hyperphantasic told a researcher that he had more than once walked into a wall because he had pictured a doorway.&lt;/p&gt;
    &lt;p&gt;Because their imaginative lives were so compelling, hyperphantasics tended to be inwardly focussed. This could mean that they were detached from reality, living in the remembered past and the imaginary future rather than in the actual present. But it could also mean that they were hyperaware of their internal reality, tuned in to the cues of their bodies and the shifts in their emotions. Some researchers hypothesized that the heightened awareness of these bodily and emotional signals were one reason that people with vivid imagery usually had strong memories of their pasts—these signals somehow helped to “anchor memories to the self.”&lt;/p&gt;
    &lt;p&gt;Hyperphantasics’ memories could be exceptionally detailed.&lt;/p&gt;
    &lt;p&gt;Memories might take on quasi-physical forms in their minds. They might picture sheaves of recollections, or files of information, sitting on shelves in a mental warehouse. They might envision lists of facts about a particular place pinned to that place on a vast and detailed mental map that they saw spread out before them, like a hologram.&lt;/p&gt;
    &lt;p&gt;Reeder had tested children’s imagery and believed that most children were hyperphantasic. They had not yet undergone the synaptic pruning that took place in adolescence, so there were incalculably more neuronal connections linking different parts of their brain, giving rise to fertile imagery. Then, as they grew older, the weaker connections were pruned away. Because the synapses that were pruned tended to be the ones that were used less, Reeder thought it was possible that the children who grew up to be hyperphantasic adults were those who kept on wanting to conjure up visual fantasy worlds, even as they grew older. Conversely, perhaps children who grew up to become typical imagers daydreamed less and less, becoming more interested in the real people and things around them. Maybe some children who loved to daydream were scolded, in school or at home, to pay attention, and maybe these children disciplined themselves to focus on the here and now and lost the ability to travel to the imaginary worlds they’d known when they were young.&lt;/p&gt;
    &lt;p&gt;Clare had not been discouraged from daydreaming as a child, and she had preferred it to the other common form of imaginative dissociation, reading. Daydreaming was more pleasurable for her because she had struggled to learn to read, and even once she knew how she’d found it slow going. When she received a diagnosis of dyslexia, as an adult, the tester told her that, rather than processing individual letters or sounds, she was memorizing pictures of whole words, which made it hard to recognize words in different fonts. Her visual sense was so overweening that reading was strenuous, because she was easily distracted by the squiggles and lines of the text.&lt;/p&gt;
    &lt;p&gt;Naturally, aphantasics usually had a very different experience of reading. Like most people, as they became absorbed, they stopped noticing the visual qualities of the words on the page, and, because their eyes were fully employed in reading, they also stopped noticing the visual world around them. But, because the words prompted no mental images, it was almost as if reading bypassed the visual world altogether and tunnelled directly into their minds.&lt;/p&gt;
    &lt;p&gt;Aphantasics might skip over descriptive passages in books—since description aroused no images in their minds, they found it dull—or, because of such passages, avoid fiction altogether. Some aphantasics found the movie versions of novels more compelling, since these supplied the pictures that they were unable to imagine. Of course, for people who did have imagery, seeing a book character in a movie was often unsettling—because they already had a sharp mental image of the character which didn’t look like the actor, or because their image was vague but just particular enough that the actor looked wrong, or because their image was barely there at all and the physical solidity of the actor conflicted with that amorphousness.&lt;/p&gt;
    &lt;p&gt;Presumably, novelists who invented characters also had a variety of responses to seeing them instantiated in solid form. Jane Austen wrote a letter to her sister in 1813 in which she described going to an exhibition of paintings in London and searching for portraits that looked like Elizabeth Bennet and Jane Bingley, two main characters from “Pride and Prejudice.” To her delight, she’d seen “a small portrait of Mrs Bingley, excessively like her . . . exactly herself, size, shaped face, features &amp;amp; sweetness; there never was a greater likeness. She is dressed in a white gown, with green ornaments, which convinces me of what I had always supposed, that green was a favourite color with her.” Austen did not see Elizabeth at the exhibition but hoped, she told her sister, to find a painting of her somewhere in the future. “I dare say Mrs D.”—she wrote, Darcy being Elizabeth’s married name—“will be in Yellow.”&lt;/p&gt;
    &lt;p&gt;One of the twenty or so congenital aphantasics who contacted Adam Zeman after his original 2010 paper was a Canadian man in his twenties, Tom Ebeyer. Ebeyer volunteered to participate in Zeman’s studies, and, after Zeman published his 2015 Cortex paper on congenital aphantasia, Ebeyer was one of the participants quoted in the Times article about it. After that, hundreds of aphantasics reached out to him on Facebook and LinkedIn. They asked him questions he didn’t know the answers to: Does this mean I have a disability? Is there a cure?&lt;/p&gt;
    &lt;p&gt;Many of Ebeyer’s correspondents felt shocked and isolated, as he had; he decided that what was needed was a online forum where aphantasics could go for information and community. He set up a website, the Aphantasia Network. He didn’t want it to be a sad place where people commiserated with one another, however. There were good things about aphantasia, he believed, and he began to write uplifting posts pointing them out. In one, he argued that aphantasia was an advantage in abstract thinking. When prompted by the word “horse,” a person with imagery would likely picture a particular horse—one they’d seen in life, perhaps, or in a painting. An aphantasic, on the other hand, focussed on the concept of a horse—on the abstract essence of horseness. Ebeyer published posts about famous people who had realized that they were aphantasic: Glen Keane, one of the leading Disney animators on “The Little Mermaid” and “Beauty and the Beast”; John Green, the author of “The Fault in Our Stars,” whose books had sold more than fifty million copies; J. Craig Venter, the biologist who led the first team to sequence the human genome; Blake Ross, who co-created the Mozilla-Firefox web browser when he was nineteen.&lt;/p&gt;
    &lt;p&gt;Ebeyer also wanted the Aphantasia Network to be a place where aphantasics could find recent scientific research. For instance, estimating the strength of a person’s imagery had been thoroughly subjective until Joel Pearson, a cognitive neuroscientist at the University of New South Wales, in Australia, devised tests to measure it more precisely. In a paper from 2022, Pearson reported that when people with imagery visualized a bright object their pupils contracted, as though they were seeing a bright object in real life, but the pupils of aphantasics imagining a bright object stayed the same. Another study of his had shown that, although aphantasics had the same fear response (sweating) as typical imagers to a frightening image shown on a screen, when exposed to a frightening story they barely responded at all.&lt;/p&gt;
    &lt;p&gt;Ebeyer kept in touch with Zeman and published bulletins about his research. Zeman had found that aphantasics could solve many problems that would seem to require imagery, such as counting the number of windows in their home. This, Zeman hypothesized, was due to the difference between object imagery and spatial imagery. There were two streams of visual information in the brain that were, to a surprising degree, distinct from each other: one had to do with recognition of objects; the other, with guiding action through space. Aphantasics lacked object imagery, but they might have the kind of spatial imagery that would enable them to count windows. One aphantasic described his ability to do this as a kind of echolocation.&lt;/p&gt;
    &lt;p&gt;To Zeman, one of the most tantalizing promises of the study of mental imagery was the light it might shed on the neural correlates of consciousness. Connectivity in the brain seemed to be particularly important in both consciousness and aphantasia. fMRI studies had shown reduced connectivity in aphantasics, and Brian Levine, a neuropsychologist at the Baycrest Academy for Research and Education, in Toronto, had found that connectivity between the memory system and the visual-perceptual regions in the brain correlated to how well people remembered their lives. Many of the aphantasics who had written to Zeman identified themselves as autistic. Autism was thought to be a state of reduced long-range connectivity in the brain, so Zeman theorized that there could be a link. But autism had also been associated with thinking in pictures—Temple Grandin, for instance, the autistic writer and professor of animal science, described her autism that way—so clearly the link was not a simple one.&lt;/p&gt;
    &lt;p&gt;After creating the Aphantasia Network, Ebeyer received tens of thousands of messages from all over the world—Korea, Venezuela, Madagascar. He launched Aphantasia Network Japan, and made plans for a Spanish-language site. When the city of Rowlett, a suburb of Dallas, declared the world’s first Aphantasia Awareness Day, on February 21, 2023, his site published a celebratory post. Once hyperphantasia began to be written about, he started to hear from hyperphantasics as well. When he wrote a post about how some people could “hear” music in their heads, or relive touch or tastes, most responses were from aphantasics amazed to learn that such things were possible. But one person wrote to him describing a kind of auditory hyperphantasia:&lt;/p&gt;
    &lt;p&gt;This past January, Zeman and others published a short article in Cortex clarifying that the definition of aphantasia encompassed people with weak imagery. Ebeyer wrote a post in response, wondering whether this inclusive definition risked diluting the experiences of those with total aphantasia, such as himself. Might it threaten the cohesion of the aphantasia community? Aphantasia, at this point, wasn’t only a syndrome, after all—it was an identity.&lt;/p&gt;
    &lt;p&gt;In the course of his quest to learn about imagery, Nick Watkins, the physicist, came across an essay by Oliver Sacks. Sacks mentioned that he normally had almost no mental imagery but that, during a two-week period in his thirties when he’d been downing heroic quantities of amphetamines, he’d suddenly been able to retain images in his mind—though only images of things that he had just looked at. During that time, he also found it much more difficult to think in abstractions. When the drugs wore off, the images dissipated and his abstract thinking returned. This was an auspicious discovery, Nick thought, that you could somehow turn imagery on. He was certainly not going to take amphetamines himself—he was a pretty cautious person—especially if doing so might jeopardize his ability to think abstractly. But if amphetamines could work, maybe something else could, too.&lt;/p&gt;
    &lt;p&gt;He kept looking. He discovered that Aldous Huxley was aphantasic and that, in “The Doors of Perception,” he had written that he was expecting mescaline to change this, even if only for a few hours. (It didn’t.) Unsurprisingly, amid the recent research on psychedelics, this hope of arousing mental vision with drugs had been revived. In 2018, the Journal of Psychedelic Studies published a paper about an aphantasic man, S.E., who had taken ayahuasca and had an intensely emotional experience of visualizing, and then forgiving, his father, long dead, who had left him when he was very young. Afterward, S.E. was still able to see images, but only faintly. He and the paper’s authors concluded that his aphantasia had likely been psychological in origin, since it was resolved by his feeling that things between him and his father had been settled. Another paper, published in the same journal in 2025, described an autistic aphantasic woman in her mid-thirties who had eaten psilocybin truffles and experienced mental imagery for the first time. Her imagery persisted for many months, although it was not quite as vivid as during the trip itself.&lt;/p&gt;
    &lt;p&gt;Nick kept hoping that someone would find a way of stimulating imagery that didn’t involve drugs. On the other hand, as he learned more about people with imagery, he was less inclined to envy them. At first, he had thought that having imagery would be like having a VCR, being able to play home movies whenever you felt wistful. But, reading more about it, he had learned that memories and images could break in on you, unbidden and uncontrollable, and not necessarily happy ones. Even if the imagery wasn’t frightening, it would surely be a distraction. He had come to value the dark and quiet of his mind.&lt;/p&gt;
    &lt;p&gt;Nick knew that whenever Zeman talked about aphantasia he was at pains to emphasize that it was not a disorder, or even a bad thing. It was best described as an interesting variant in human experience, like synesthesia. Nick appreciated this about Zeman, and reckoned that it was probably the right thing to say, but he thought that, though aphantasia itself might be neutral, the memory loss that came with it was definitely a bad thing. Many others felt the same. At one point, Zeman had been contacted by an automotive engineer from Essex named Alan Kendle, who had realized that he was aphantasic while listening to a radio segment about the condition. This revelation affected him so strongly that he put together a book of interviews with aphantasics, identified just by their initials, to help others navigate the discovery. Some people he interviewed were unbothered—there was definitely a range of responses—but others saw it as a curse.&lt;/p&gt;
    &lt;p&gt;Many could remember very little about their lives, and even with the events they did remember they could not muster the feeling of what they’d been like. They knew that some things had made them happy and others had made them sad, but that knowledge was factual—it didn’t evoke any emotions in the present.&lt;/p&gt;
    &lt;p&gt;The advantage of a bad memory was that aphantasics seemed to suffer less from regret, or shame, or resentment.&lt;/p&gt;
    &lt;p&gt;But this supposed advantage was just the silver lining of something pretty dark. When aphantasics recovered from bereavement, or breakups, or trauma, more quickly than others, they worried that they were overly detached or emotionally deficient. When they didn’t see people regularly, even family, they tended not to think about them.&lt;/p&gt;
    &lt;p&gt;One of Kendle’s interviewees was Melinda Utal, a hypnotherapist and a freelance writer from California. She had trouble recognizing people, including people she knew pretty well, so she tended to avoid social situations where she might hurt someone’s feelings. When she first discovered that she was aphantasic, she called her father, who was in the early stages of Alzheimer’s disease and living in a nursing home in Oregon. He had been a musician in big bands—he had toured with Bob Hope and played with Les Brown and his Band of Renown. She asked him whether he could imagine a scene in his head, and he said, Of course. I can imagine going into a concert hall. I see the wood on the walls, I see the seats, I know I’m going to sit at the back, because that’s where you get the best sound. I can see the orchestra playing a symphony, I can hear all the different instruments, and I can stop it and go backward to wherever I want it to start up and hear it again. She explained to her father what aphantasia was, how she couldn’t see images in her mind, or hear music, either. On the phone, her father started to cry. He said, But, Melinda, that’s what makes us human.&lt;/p&gt;
    &lt;p&gt;Melinda had an extremely bad memory for her life, even for an aphantasic. She once had herself checked for dementia, but the doctor found nothing wrong. She had become aware when she was in second grade that she had a bad memory, after a friend pointed it out. In an effort to hold on to her memories, she started keeping a journal in elementary school, recording what she did almost every single day, and continued this practice for decades. When, in her sixties, she got divorced and moved into an apartment by herself, she thought it would be a good time to look through her journals and revisit her younger days. She opened one and began to sob because, to her horror, the words she had written meant nothing to her. The journals were useless. She read about things she had done and it was as though they had happened to someone else.&lt;/p&gt;
    &lt;p&gt;It was not just the distant past that she had lost—she was continuously aware of the present slipping away as soon as it happened. She had already forgotten what her two sons had been like when they were little, the feeling of holding them:&lt;/p&gt;
    &lt;p&gt;Now her greatest fear was that, if she hadn’t seen her sons in a while, she might forget them altogether:&lt;/p&gt;
    &lt;p&gt;Although Nick had made his peace with his lack of imagery, he still grieved his inability to revisit his past. At one point, he came across the work of a Canadian psychologist, Endel Tulving, who, in the early nineteen-seventies, proposed that memory was not a single thing but two distinct systems: semantic memory, which consisted of general knowledge about the world, and episodic memory—recollection of experiences from your own life. Episodic memory, the sense of reliving the past, was, Tulving believed, unique to humans, and among the most astonishing products of evolution. This, Nick realized, was what he didn’t have. Learning that he lacked a profound human ability—one that, he had to assume, regenerated and immeasurably deepened your connection to your past life and the people in it who were now gone, including yourself as a child—well, there was nothing good about it. He would have preferred not to know.&lt;/p&gt;
    &lt;p&gt;He wrote to Tulving, who told him about a study to be conducted by Brian Levine, the Baycrest neuropsychologist, who had been a colleague of his in Toronto. The study would investigate exceptionally poor autobiographical memory in healthy adults—people who did not have amnesia or dementia or brain injury or psychological trauma. Levine later named this syndrome “severely deficient autobiographical memory,” or sdam. Nick was accepted as a participant and travelled to Toronto. The study found that the participants’ experience of sdam could be objectively corroborated, using a variety of methods, by comparing them to a control group. fMRI, for instance, showed reduced activation in the midline regions of their brains, an area normally associated with mental time travel.&lt;/p&gt;
    &lt;p&gt;Nick was surprised to hear that another participant in the study had described an even starker experience of episodic memory loss than his. She felt so detached from her past that the facts she knew about it felt to her no more personal than facts about someone else. He definitely didn’t feel that way. The things he knew about his life felt more personal to him than facts he knew about physics, say, even though he couldn’t inhabit them in the way that other people could. He realized that Tulving’s binary schema, which categorized all memory as either episodic or semantic, was too simple. His own memories were somewhere in between. He remembered that on the day that his mother died, in 2003, his sister had phoned him to say that their mother was being admitted to the hospital; he had taken a train from Cambridge to London, and he had phoned an old friend to meet him in London because he was worried that, in his distress, he might go to the wrong station and miss the second train he needed to catch, but the friend helped him, and he got on the right train, and it was around Guy Fawkes Night, fireworks going off outside the train window, and then he got to the hospital and was there for a while, and then his mother died. He knew these things, and the idea of his mother dying aroused emotion in him, but he couldn’t feel what it had been like to be in the train, or the hospital, and he could not remember his mother’s face.&lt;/p&gt;
    &lt;p&gt;From an evolutionary point of view, he supposed, he had all the memory he needed: enough to know what and whom he had loved, and what he should try to avoid doing again. But to think about it that way was to miss what was most important—not the function of episodic memory but the experience of it. As he absorbed what it meant to lack episodic memory, he started wondering whether there were ways he could simulate it. He was attracted to the idea of video life-logging with wearable cameras—the footage would be a decent substitute for mental time travel. His childhood and early adulthood were lost to him, but if he started filming now he would be able to relive at least the last decade or two of his life.&lt;/p&gt;
    &lt;p&gt;On a trip to Pasadena, he went to the Apple Store and tried on a virtual-reality headset. This, he thought, must be what episodic memory is like. He knew it would probably be a long time before people accepted such technologies, but perhaps one day wearable cameras would be recognized as prosthetics for people with SDAM, no more remarkable than glasses. Then again, film would be very different from memory. Like memory, it would be partial, but, unlike memory, it would be accurate. This, he suspected, might not necessarily be a good thing. There was something to be said for a degree of blurriness and uncertainty in recalling the past; it was helpful in forgiving other people, and yourself.&lt;/p&gt;
    &lt;p&gt;At some point, Nick became interested in the ideas of a British philosopher, Galen Strawson, who claimed to have no sense of himself as a continuously evolving being—a creature whose self consisted of a coherent story about accumulating memories and distinctive traits. Strawson was, for that reason, uninterested in his past. He acknowledged that his life had shaped him, but he believed that whether or not he consciously remembered it didn’t matter to who he was now, any more than it mattered whether a musician playing a piece could call to mind a memory of each time he’d practiced: what mattered was how well he played. What was important, Strawson felt, was his life in the present. He liked to quote the third Earl of Shaftesbury, a British philosopher of the late seventeenth and early eighteenth centuries, who had felt the same way:&lt;/p&gt;
    &lt;p&gt;Nick wasn’t sure he agreed with Strawson, and he certainly didn’t feel, as Strawson did, that his memory of his own life was unimportant, but he found the argument somewhat comforting. He still longed to relive important moments in his life, but it was easier to think about this experience as just one of many he hadn’t had, like paragliding, or visiting Peru, than as a void at the core of his self. Many people believed that their selves were made up largely of memories, and that the loss of those memories would be a self-ending catastrophe. But he knew now that there were also thousands of people like him, who had work and marriages and ideas and thwarted desires and good days and bad days and the rest of it. All they lacked was a past. ♦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.newyorker.com/magazine/2025/11/03/some-people-cant-see-mental-images-the-consequences-are-profound"/><published>2025-10-30T17:45:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763076</id><title>I have released a 69.0MB version of Windows 7 x86</title><updated>2025-10-30T19:32:25.733281+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/XenoPanther/status/1983477707968291075"/><published>2025-10-30T18:05:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763367</id><title>Rapid Brightening of 3I/Atlas Ahead of Perihelion</title><updated>2025-10-30T19:32:25.514729+00:00</updated><content>&lt;doc fingerprint="ec1ca02b549fd4ab"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Astrophysics &amp;gt; Earth and Planetary Astrophysics&lt;/head&gt;&lt;p&gt; [Submitted on 28 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Rapid Brightening of 3I/ATLAS Ahead of Perihelion&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Interstellar comet 3I/ATLAS has been approaching its 2025 October 29 perihelion while opposite the Sun from Earth, hindering ground-based optical observations over the preceding month. However, this geometry placed the comet within the fields of view of several space-based solar coronagraphs and heliospheric imagers, enabling its continued observation during its final approach toward perihelion. We report photometry from STEREO-A's SECCHI HI1 and COR2, SOHO's LASCO C3, and GOES-19's CCOR-1 instruments in 2025 September--October, which show a rapid rise in the comet's brightness scaling with heliocentric distance r as r^(-7.5+/-1.0). CCOR-1 also resolves the comet as an extended source with an apparent coma ~4' in diameter. Furthermore, LASCO color photometry shows the comet to be distinctly bluer than the Sun, consistent with gas emission contributing a substantial fraction of the visible brightness near perihelion.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;astro-ph.EP&lt;/p&gt;&lt;p&gt; Change to browse by: &lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2510.25035"/><published>2025-10-30T18:25:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763858</id><title>TruthWave – A Platform for Corporate Whistleblowers</title><updated>2025-10-30T19:32:25.219356+00:00</updated><content>&lt;doc fingerprint="7d44df6756f66ba1"&gt;
  &lt;main&gt;
    &lt;p&gt;Our Beta is Now Live!&lt;/p&gt;
    &lt;head rend="h1"&gt;This is TruthWave.&lt;/head&gt;
    &lt;p&gt;Welcome to the platform and community for those who bring unethical corporations to justice.&lt;/p&gt;
    &lt;p&gt;For far too long, harmful corporate culture has stigmatized and disincentivized information flow. TruthWave is rewriting this narrative by creating a platform that financially compensates whistleblowers for courageously stepping forward while leveraging a global community built around justice.&lt;/p&gt;
    &lt;p&gt;At its core, TruthWave is an information platform that allows those who possess or locate vital information about corporate wrongdoing to share it securely and anonymously.&lt;/p&gt;
    &lt;p&gt;Have our next big case? If you know about corporate wrongdoing, submit a tip to bring those responsible to justice.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.truthwave.com"/><published>2025-10-30T18:57:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763877</id><title>Minecraft HDL, an HDL for Redstone</title><updated>2025-10-30T19:32:24.816524+00:00</updated><content>&lt;doc fingerprint="3271921d09773db1"&gt;
  &lt;main&gt;
    &lt;p&gt;Minecraft HDL is a digital synthesis flow for minecraft redstone circuits. It is an attempt to use industry standard design tools and methods to generate digital circuits with redstone.&lt;/p&gt;
    &lt;p&gt;This file &lt;code&gt;multiplexer4_1.v&lt;/code&gt; is a 6 input - 1 output circuit that selects one of the first 4 inputs (a, b, c, d) as the output based on the value of the last 2 inputs (x, y)&lt;/p&gt;
    &lt;code&gt;module multiplexer4_1 ( a ,b ,c ,d ,x ,y ,dout ); 
 
output dout ; 
input a, b, c, d, x, y; 
 
assign dout = (a &amp;amp; (~x) &amp;amp; (~y)) | 
     (b &amp;amp; (~x) &amp;amp; (y)) |  
     (c &amp;amp; x &amp;amp; (~y)) | 
     (d &amp;amp; x &amp;amp; y); 
endmodule &lt;/code&gt;
    &lt;p&gt;When synthesized through Minecraft HDL it produces this circuit:&lt;/p&gt;
    &lt;p&gt;With the 6 inputs on the right and the single output on the left&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Screenshots &amp;amp; Sample Circuits&lt;/item&gt;
      &lt;item&gt;Getting Started - Installing and Using MinecraftHDL&lt;/item&gt;
      &lt;item&gt;Background Theory - Digital Design &amp;amp; Verilog&lt;/item&gt;
      &lt;item&gt;How MinecraftHDL Works - Read Our Paper&lt;/item&gt;
      &lt;item&gt;Developper Info - If you want to fork or contribute&lt;/item&gt;
      &lt;item&gt;Quick Overview - Check out our poster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MinecraftHDL was the final undergraduate design project made by three students in the Electrical, Computer &amp;amp; Software Engineering department at McGill University.&lt;/p&gt;
    &lt;p&gt;It is by no means bug-free or even complete; It produces objectively inferior circuits to 'hand-made' redstone designs, and is not intended to be used in modded survival. It can generate almost any verilog circuit, however only simple designs will actually be testable in-game since any moderately-complex design will end up being longer than the maximum number of blocks loaded in Minecraft.&lt;/p&gt;
    &lt;p&gt;Additionally, we are currently unable to synthesize sequential circuits, aka any circuits with a loopback or feedback. That means no memory, no counters or any circuit that could hold a state.&lt;/p&gt;
    &lt;p&gt;MinecraftHDL is an educational tool to illustrate on a macro-scopic scale how microelectronic digital circuits are designed and produced. It is a great way to introduce younger audiences to the world of digital design and can also be used to illustrate the difference between software and hardware design to undergraduate engineers taking their first RTL class.&lt;/p&gt;
    &lt;p&gt;Supervisor: Brett H. Meyer - Website&lt;lb/&gt; Students: Francis O'Brien - Website&lt;lb/&gt; Omar Ba Mashmos&lt;lb/&gt; Andrew Penhale&lt;/p&gt;
    &lt;p&gt;To show how easy it is to make a circuit with MinecraftHDL here is a gif of me creating a circuit, synthesizing, and generating it in minecraft in less than a minute!&lt;/p&gt;
    &lt;p&gt;The circuit I generate above is a 2bit adder. It takes two numbers of two bits and adds them. At the end of the gif I set both input numbers to '11' which is the binary representation of the number 3. Then I move to the output and we see that O3=1, O2=1, and O1=0, this gives the binary number '110' which is indeed 6.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/itsfrank/MinecraftHDL"/><published>2025-10-30T18:59:02+00:00</published></entry></feed>