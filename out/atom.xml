<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-10T14:10:06.013837+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46547962</id><title>Scientists discover oldest poison, on 60k-year-old arrows</title><updated>2026-01-10T14:10:16.790523+00:00</updated><content/><link href="https://www.nytimes.com/2026/01/07/science/poison-arrows-south-africa.html"/><published>2026-01-08T23:24:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46553343</id><title>Kagi releases alpha version of Orion for Linux</title><updated>2026-01-10T14:10:16.664615+00:00</updated><content>&lt;doc fingerprint="8923d81071c60f5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Orion for Linux Status √¢&lt;/head&gt;
    &lt;p&gt;The alpha stage is an early, unstable version meant primarily for testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is ready to test √¢&lt;/head&gt;
    &lt;p&gt;All visual components, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Main menus, submenus, dialogs, buttons, and toolbars.&lt;/item&gt;
      &lt;item&gt;Right-click menus and other visual controls.&lt;/item&gt;
      &lt;item&gt;Window layouts and basic controls.&lt;/item&gt;
      &lt;item&gt;Demonstrated basic website navigation functionality, supporting essentials like the homepage, tabs, and simple searches&lt;/item&gt;
      &lt;item&gt;Advanced tab management is now complete, with the exception of the Tab Switcher UI, which is not supported yet.&lt;/item&gt;
      &lt;item&gt;Tabs now function independently and can be opened in parallel&lt;/item&gt;
      &lt;item&gt;Session persistence is implemented: previously opened tabs, along with their history, will reopen when the application is launched again.&lt;/item&gt;
      &lt;item&gt;Tabs currently appear in the main window and are supported in the left sidebar as well.&lt;/item&gt;
      &lt;item&gt;Bookmarks system a simple bookmark feature is now available.&lt;/item&gt;
      &lt;item&gt;Users can save pages, organize them into folders&lt;/item&gt;
      &lt;item&gt;Users can view them in the bookmarks dialog, sidebar, and bookmarks bar.&lt;/item&gt;
      &lt;item&gt;Bookmarking via the √¢¬¥√Ø¬∏ icon.&lt;/item&gt;
      &lt;item&gt;Intuitive folder assignment when saving a new bookmark.&lt;/item&gt;
      &lt;item&gt;Advanced history management provides handling of browsing history&lt;/item&gt;
      &lt;item&gt;Password management framework establishes the core infrastructure needed for secure password handling and future improvements in this area.&lt;/item&gt;
      &lt;item&gt;Local export/import (via file)&lt;/item&gt;
      &lt;item&gt;Managing passwords&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Future improvements (not implemented in Alpha): √¢&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WebKit Extension support&lt;/item&gt;
      &lt;item&gt;Sync infrastructure&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://help.kagi.com/orion/misc/linux-status.html"/><published>2026-01-09T12:54:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46554652</id><title>How to store a chess position in 26 bytes (2022)</title><updated>2026-01-10T14:10:16.462073+00:00</updated><content>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ezzeriesa.notion.site/How-to-store-a-chess-position-in-26-bytes-using-bit-level-magic-df1fdb5364eb42fdac11eb23b25e9605"/><published>2026-01-09T15:07:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555760</id><title>Cloudflare CEO on the Italy fines</title><updated>2026-01-10T14:10:16.199779+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/eastdakota/status/2009654937303896492"/><published>2026-01-09T16:46:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555977</id><title>IcePanel (YC W23) is hiring full-stack engineers in Vancouver</title><updated>2026-01-10T14:10:15.426052+00:00</updated><content>&lt;doc fingerprint="80b89e5ad2c4a6e0"&gt;
  &lt;main&gt;
    &lt;p&gt;$170,000 salary (CAD) + Profit-share quarterly bonus (last year averaged ~30k-40k each)&lt;/p&gt;
    &lt;p&gt;+ 1% equity + Unlimited holiday + Health benefits&lt;/p&gt;
    &lt;p&gt;We‚Äôre looking for someone with a high degree of agency, who can immediately take ownership of building new functionality from design &amp;gt; implementation &amp;gt; maintaining and refining current features based on our customers' needs.&lt;/p&gt;
    &lt;p&gt;You‚Äôll be building end-to-end, including: - Frontend UI/UX design alongside a designer. - Backend API/data structure design. - Data migration and infrastructure changes. - Bug fixing and iterations.&lt;/p&gt;
    &lt;p&gt;We're simplifying how teams design for complex systems. We're building a collaborative diagramming and modelling tool that software architects think is cool.&lt;/p&gt;
    &lt;p&gt;We‚Äôre a small, energetic team that believes in building a lean and profitable business after being in the YCombinator W23 batch. We‚Äôve grown the product to ~$4 million CAD in ARR and believe in continuing to build on profitability over funding. We‚Äôre looking for talented, driven people who love their craft to help achieve our vision of simplifying complexity.&lt;/p&gt;
    &lt;p&gt;üôã Independence to build our way&lt;/p&gt;
    &lt;p&gt;üõ†Ô∏è Build simple and exceptional experiences&lt;/p&gt;
    &lt;p&gt;üßä Transparency and openness&lt;/p&gt;
    &lt;p&gt;üí° Stay humble and explore all ideas&lt;/p&gt;
    &lt;p&gt;üí© No bullshit, have fun&lt;/p&gt;
    &lt;p&gt;- In-person days every week (Tuesday, Wednesday, Thursday)&lt;/p&gt;
    &lt;p&gt;- North Vancouver, British Columbia, Canada&lt;/p&gt;
    &lt;p&gt;- Hybrid &amp;amp; flexible work environment&lt;/p&gt;
    &lt;p&gt;- This is not a fully remote job&lt;/p&gt;
    &lt;p&gt;üç∞ Equity in the company üí∞ Profit sharing&lt;/p&gt;
    &lt;p&gt;üíª Work setup provided&lt;/p&gt;
    &lt;p&gt;üéâ Flexible work culture&lt;/p&gt;
    &lt;p&gt;üèÇ Unlimited holiday&lt;/p&gt;
    &lt;p&gt;üßë‚öïÔ∏è Health, dental, vision&lt;/p&gt;
    &lt;p&gt;üìö Learning budget&lt;/p&gt;
    &lt;p&gt;‚úàÔ∏è Conference budget&lt;/p&gt;
    &lt;p&gt;üå¥ Annual team retreat&lt;/p&gt;
    &lt;p&gt;üå≠ Hot dog Wednesdays&lt;/p&gt;
    &lt;p&gt;üßä Free ice cubes&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forms.icepanel.io/careers/senior-product-engineer"/><published>2026-01-09T17:01:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46556695</id><title>How Markdown took over the world</title><updated>2026-01-10T14:10:15.141136+00:00</updated><content>&lt;doc fingerprint="31a09b5f3ae00f98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How Markdown took over the world&lt;/head&gt;
    &lt;p&gt;Nearly every bit of the high-tech world, from the most cutting-edge AI systems at the biggest companies, to the casual scraps of code cobbled together by college students, is annotated and described by the same, simple plain text format. Whether you‚Äôre trying to give complex instructions to ChatGPT, or you want to be able to exchange a grocery list in Apple Notes or copy someone‚Äôs homework in Google Docs, that same format will do the trick. The wild part is, the format wasn‚Äôt created by a conglomerate of tech tycoons, it was created by a curmudgeonly guy with a kind heart who right this minute is probably rewatching a Kubrick film while cheering for an absolutely indefensible sports team.&lt;/p&gt;
    &lt;p&gt;But it‚Äôs worth understanding how these simple little text files were born, not just because I get to brag about how generous and clever my friends are, but also because it reminds us of how the Internet really works: smart people think of good things that are crazy enough that they just might work, and then they give them away, over and over, until they slowly take over the world and make things better for everyone.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Their Mark&lt;/head&gt;
    &lt;p&gt;Though it‚Äôs now a building block of the contemporary Internet, like so many great things, Markdown just started out trying to solve a personal problem. In 2002, John Gruber made the unconventional decision to bet his online career on two completely irrational foundations: Apple, and blogs.&lt;/p&gt;
    &lt;p&gt;It‚Äôs hard to remember now, but in 2002, Apple was just a few years past having been on death‚Äôs door. As difficult as it may be to picture in today‚Äôs world where Apple keynotes are treated like major events, back then, almost nobody was covering Apple regularly, let alone writing exclusively about the company. There was barely even an ‚Äútech news‚Äù scene online at all, and virtually no one was blogging. So John‚Äôs decision to go all-in on Apple for his pioneering blog Daring Fireball was, well, a daring one. At the time, Apple had only just launched its first iPod that worked with Windows computers, and the iPhone was still a full five years in the future. But that single-minded focus, not just on Apple, but on obsessive detail in everything he covered, eventually helped inspire much of the technology media landscape that we see today. John‚Äôs timing was also perfect ‚Äî from the doldrums of that era, Apple‚Äôs stock price would rise by about 120,000% in the years after Daring Fireball started, and its cultural relevance probably increased by even more than that.&lt;/p&gt;
    &lt;p&gt;By 2004, it wasn‚Äôt just Apple that had begun to take off: blogs and social media themselves had moved from obscurity to the very center of culture, and a new era of web technology had begun. At the beginning of that year, few people in the world even knew what a ‚Äúblog‚Äù was, but by the end of 2004, blogs had become not just ubiquitous, but downright cool. As unlikely as it seems now, that year‚Äôs largely uninspiring slate of U.S. presidential candidates like Wesley Clark, Gary Hart and, yes, Howard Dean helped propel blogs into mainstream awareness during the Democratic primaries, alongside online pundits who had begun weighing in on politics and the issues and cultural moments at a pace that newspapers and TV couldn‚Äôt keep up with. A lot has been written about the transformation of media during those years, but less has been written about how the media and tech of the time transformed each other.&lt;/p&gt;
    &lt;p&gt;That era of early blogging was interesting in that nearly everyone who was writing the first popular sites was also busy helping create the tools for publishing them. Just like Lucille Ball and Desi Arnaz had to pioneer combining studio-style flat lighting with 35mm filming in order to define the look of the modern sitcom, or Jimi Hendrix had to work with Roger Mayer to invent the signature guitar distortion pedals that defined the sound of rock and roll, the pioneers who defined the technical format and structures of blogging were often building the very tools of creation as they went along.&lt;/p&gt;
    &lt;p&gt;I got a front row seat to these acts of creation. At the time I was working on Movable Type, which was the most popular tool for publishing ‚Äúserious‚Äù blogs, and helped popularize the medium. Two of my good friends had built the tool and quickly made it into the default choice for anybody who wanted to reach a big audience; it was kind of a combination of everything people do these days on WordPress and all the various email newsletter platforms and all of the ‚Äúserious‚Äù podcasts (since podcasts wouldn‚Äôt be invented for another few months). But back in those early days, we‚Äôd watch people use our tools to set up Gawker or Huffington Post one day, and Daring Fireball or Waxy.org the next, and each of them would be the first of its kind, both in terms of its design and its voice. To this day, when I see something online that I love by Julianne Escobedo Shepherd or Ta-Nehisi Coates or Nilay Patel or Annalee Newitz or any one of dozens of other brilliant writers or creators, my first thought is often, ‚Äúhey! They used to type in that app that I used to make!‚Äù Because sometimes those writers would inspire us to make a new feature in the publishing tools, and sometimes they would have hacked up a new feature all by themselves in between typing up their new blog posts.&lt;/p&gt;
    &lt;p&gt;A really clear, and very simple, early example of how we learned that lesson was when we changed the size of the box that people used to type in just to create the posts on their sites. We made the box a little bit taller, mostly for aesthetic reasons. Within a few weeks, we‚Äôd found that posts on sites like Gawker had gotten longer, mostly because the box was bigger. This seems obvious now, years after we saw tweets get longer when Twitter expanded from 140 characters to 280 characters, but at the time this was a terrifying glimpse at how much power a couple of young product managers in a conference room in California would have over the media consumption of the entire world every time they made a seemingly-insignificant decision.&lt;/p&gt;
    &lt;p&gt;The other dirty little secret was, typing in the box in that old blogging app could be‚Ä¶ pretty wonky sometimes. People who wanted to do normal things like include an image or link in their blog post, or even just make some text bold, often had to learn somewhat-obscure HTML formatting, memorizing the actual language that‚Äôs used to make web pages. Not everybody knew all the details of how to make pages that way, and if they made even one small mistake, sometimes they could break the whole design of their site. It made things feel very fraught every time a writer went to publish something new online, and got in the way of the increasingly-fast pace of sharing ideas now that social media was taking over the public conversation.&lt;/p&gt;
    &lt;p&gt;Enter John and his magical text files.&lt;/p&gt;
    &lt;head rend="h2"&gt;Marking up and marking down&lt;/head&gt;
    &lt;p&gt;The purpose of Markdown is really simple: It lets you use the regular characters on your keyboard which you already use while typing out things like emails, to make fancy formatting of text for the web. That HTML format that‚Äôs used to make web pages stands for HyperText Markup Language. The word ‚Äúmarkup‚Äù there means you‚Äôre ‚Äúmarking up‚Äù your text with all kinds of special characters. Only, the special characters can be kind of arcane. Want to put in a link to everybody‚Äôs favorite website? Well, you‚Äôre going to have to type in &lt;code&gt;&amp;lt;a href="https://anildash.com/"&amp;gt;Anil Dash‚Äôs blog&amp;lt;/a&amp;gt;&lt;/code&gt; I could explain why, and what it all means, but honestly, you get the point ‚Äî it‚Äôs a lot! Too much. What if you could just write out the text and then the link, sort of like you might within an email? Like: &lt;code&gt;[Anil Dash‚Äôs blog](https://anildash.com)&lt;/code&gt;! And then the right thing would happen. Seems great, right?&lt;/p&gt;
    &lt;p&gt;The same thing works for things like putting a header on a page. For example, as I‚Äôm writing this right now, if I want to put a big headline on this page, I can just type &lt;code&gt;#How Markdown Took Over the World&lt;/code&gt; and the right thing will happen.&lt;/p&gt;
    &lt;p&gt;If mark_up_ is complicated, then the opposite of that complexity must be‚Ä¶ markd_own_. This kind of solution, where it‚Äôs so smart it seems obvious in hindsight, is key to Markdown‚Äôs success. John worked to make a format that was so simple that anybody could pick it up in a few minutes, and powerful enough that it could help people express pretty much anything that they wanted to include while writing on the internet. At a technical level, it was also easy enough to implement that John could write the code himself to make it work with Movable Type, his publishing tool of choice. (Within days, people had implemented the same feature for most of the other blogging tools of the era; these days, virtually every app that you can type text into ships with Markdown support as a feature on day one.)&lt;/p&gt;
    &lt;p&gt;Prior to launch, John had enlisted our mutual friend, the late, dearly missed Aaron Swartz, as a beta tester. In addition to being extremely fluent in every detail of the blogging technologies of the time, Aaron was, most notably, seventeen years old. And though Aaron‚Äôs activism and untimely passing have resulted in him having been turned into something of a mythological figure, one of the greatest things about Aaron was that he could be a total pain in the ass, which made him terrific at reporting bugs in your software. (One of the last email conversations I ever had with Aaron was him pointing out some obscure bugs in an open source app I was working on at the time.) No surprise, Aaron instantly understood both the potential and the power of Markdown, and was a top-tier beta tester for the technology as it was created. His astute feedback helped finely hone the final product so it was ready for the world, and when Markdown quietly debuted in March of 2004, it was clear that text files around the web were about to get a permanent upgrade.&lt;/p&gt;
    &lt;p&gt;The most surprising part of what happened next wasn‚Äôt that everybody immediately started using it to write their blogs; that was, after all, what the tool was designed to do. It‚Äôs that everybody started using Markdown to do everything else, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hitting the Mark&lt;/head&gt;
    &lt;p&gt;It‚Äôs almost impossible to overstate the ubiquity of Markdown within the modern computer industry in the decades since its launch.&lt;/p&gt;
    &lt;p&gt;After being nagged about it by users for more than a decade, Google finally added support for Markdown to Google Docs, though it took them years of fiddly improvements to make it truly usable. Just last year, Microsoft added support for Markdown to its venerable Notepad app, perhaps in attempt to assuage the tempers of users who were still in disbelief that Notepad had been bloated with AI features. Nearly every powerful group messaging app, from Slack to WhatsApp to Discord, has support for Markdown in messages. And even the company that indirectly inspired all of this in the first place finally got on board: the most recent version of Apple Notes finally added support for Markdown. (It‚Äôs an especially striking launch by Apple due to its timing, shortly after John had used his platform as the most influential Apple writer in the world to blog about the utter failure of the ‚ÄúApple Intelligence‚Äù AI launch.)&lt;/p&gt;
    &lt;p&gt;But it‚Äôs not just the apps that you use on your phone or your laptop. For developers, Markdown has long been the lingua franca of the tools we string together to accomplish our work. On GitHub, the platform that nearly every developer in the world uses to share their code, nearly every single repository of code on the site has at least one Markdown file that‚Äôs used to describe its contents. Many have dozens of files describing all the different aspects of their project. And some of the repositories on GitHub consist of nothing but massive collections of Markdown files. The small tools and automations we run to perform routine tasks, the one-off reports that we generate to make sure something worked correctly, the confirmations that we have a system email out when something goes wrong, the temporary files we use when trying to recover some old data ‚Äî all of these default to being Markdown files.&lt;/p&gt;
    &lt;p&gt;As a result, there are now billions of Markdown files lying around on hard drives around the world. Billions more are stashed in the cloud. There are some on the phone in your pocket. Programmers leave them lying around wherever their code might someday be running. Your kid‚Äôs Nintendo Switch has Markdown files on it. If you‚Äôre listening to music, there‚Äôs probably a Markdown file on the memory chip of the tiny system that controls the headphones stuck in your ears. The Markdown is inside you right now!&lt;/p&gt;
    &lt;head rend="h2"&gt;Down For Whatever&lt;/head&gt;
    &lt;p&gt;So far, these were all things we could have foreseen when John first unleashed his little text tool on the world. I would have been surprised about how many people were using it, but not really the ways in which they were using it. If you‚Äôd have said ‚ÄúTwenty years in the future, all the different note-taking apps people use save their files using Markdown!‚Äù, I would have said, ‚ÄúOkay, that makes sense!‚Äù&lt;/p&gt;
    &lt;p&gt;What I wouldn‚Äôt have asked, though, was ‚ÄúIs John getting paid?‚Äù As hard as it may be to believe, back in 2004, the default was that people made new standards for open technologies like Markdown, and just shared them freely for the good of the internet, and the world, and then went on about their lives. If it happened to have unleashed billions of dollars of value for others, then so much the better. If they got some credit along the way, that was great, too. But mostly you just did it to solve a problem for yourself and for other like-minded people. And also, maybe, to help make sure that some jerk didn‚Äôt otherwise create some horrible proprietary alternative that would lock everybody into their terrible inferior version forever instead. (We didn‚Äôt have the word ‚Äúenshittification‚Äù yet, but we did have Cory Doctorow and we did have plain text files, so we kind of knew where things were headed.)&lt;/p&gt;
    &lt;p&gt;To give a sense of the vibe of that era, the term ‚Äúpodcasting‚Äù had been coined just a month before Markdown was released, and went into wider use that fall, and was similarly a radically open system that wasn‚Äôt owned by any big company and that empowered people to do whatever they wanted to do to express themselves. (And podcasting was another technology that Aaron Swartz helped improve by being a brilliant pain in the ass. But I‚Äôll save that story for another book-length essay.)&lt;/p&gt;
    &lt;p&gt;That attitude of being not-quite-_anti_commercial, but perhaps just not even really concerned with whether something was commercial or not seems downright quaint in an era when the tech tycoons are not just the wealthiest people in the world, but also some of the weirdest and most obnoxious as well. But the truth is, most people today who make technology are actually still exceedingly normal, and quite generous. It‚Äôs just that they‚Äôve been overshadowed by their bosses who are out of their minds and building rocket ships and siring hundreds of children and embracing overt white supremacy instead of making fun tools for helping you type text, like regular people do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Markdown Model&lt;/head&gt;
    &lt;p&gt;The part about not doing this stuff solely for money matters, because even the most advanced LLM systems today, what the big AI companies call their ‚Äúfrontier‚Äù models, require complex orchestration that‚Äôs carefully scripted by people who‚Äôve tuned their prompts for these systems through countless rounds of trial and error. They‚Äôve iterated and tested and watched for the results as these systems hallucinated or failed or ran amok, chewing up countless resources along the way. And sometimes, they generated genuinely astonishing outputs, things that are truly amazing to consider that modern technology can achieve. The rate of progress and evolution, even factoring in the mind-boggling amounts of investment that are going into these systems, is rivaled only by the initial development of the personal computer or the Internet, or the early space race.&lt;/p&gt;
    &lt;p&gt;And all of it ‚Äî all of it ‚Äî is controlled through Markdown files. When you see the brilliant work shown off from somebody who‚Äôs bragging about what they made ChatGPT generate for them, or someone is understandably proud about the code that they got Claude to create, all of the most advanced work has been prompted in Markdown. Though where the logic of Markdown was originally a very simple version of "use human language to tell the machine what to do", the implications have gotten far more dire when they use a format designed to help expresss "make this &lt;code&gt;**bold**&lt;/code&gt;" to tell the computer itself "&lt;code&gt;make this imaginary girlfriend more compliant&lt;/code&gt;".&lt;/p&gt;
    &lt;p&gt;But we already know that the Big AI companies are run by people who don't reckon with the implications of their work. They could never understand that every single project that's even moderately ambitious on these new AI platforms is being written up in files formatted according to this system created by one guy who has never asked for a dime for this work. An entire generation of AI coders has been born since Markdown was created who probably can‚Äôt even imagine that this technology even has an "inventor". It‚Äôs just always been here, like the Moon, or Rihanna.&lt;/p&gt;
    &lt;p&gt;But it‚Äôs important for everyone to know that the Internet, and the tech industry, don‚Äôt run without the generosity and genius of regular people. It is not just billion-dollar checks and Silicon Valley boardrooms that enable creativity over years, decades, or generations ‚Äî it‚Äôs often a guy with a day job who just gives a damn about doing something right, sweating the details and assuming that if he cares enough about what he makes then others will too. The majority of the technical infrastructure of the Internet was created in this way. For free, often by people in academia, or as part of their regular work, with no promise of some big payday or getting a ton of credit.&lt;/p&gt;
    &lt;p&gt;The people who make the real Internet and the real innovations also don‚Äôt look for ways to hurt the world around them, or the people around them. Sometimes, as in the case of Aaron, the world hurts them more than anyone should ever have to bear. I know not everybody cares that much about plain text files on the Internet; I will readily admit I am a huge nerd about this stuff in a way that maybe most normal people are not. But I do think everybody cares about some part of the wonderful stuff on the Internet in this way, and I want to fight to make sure that everybody can understand that it‚Äôs not just five terrible tycoons who built this shit. Real people did. Good people. I saw them do it.&lt;/p&gt;
    &lt;p&gt;The trillion-dollar AI industry's system for controlling their most advanced platforms is a plain text format one guy made up for his blog and then bounced off of a 17-year-old kid before sharing it with the world for free. You're welcome, Time Magazine's people of the year, The Architects of AI. Their achievement is every bit as impressive as yours.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Ten Technical Reasons Markdown Won&lt;/head&gt;
    &lt;p&gt;Okay, with some of the narrative covered, what can we learn from Markdown‚Äôs success? How did this thing really take off? What could we do if we wanted to replicate something like this in the modern era? Let‚Äôs consider a few key points:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Had a great brand.&lt;/head&gt;
    &lt;p&gt;Okay, let‚Äôs be real: ‚ÄúMarkdown‚Äù as a name is clever as hell. Get it it‚Äôs not markup, it‚Äôs mark down. You just can‚Äôt argue with that kind of logic. People who knew what the ‚ÄúM‚Äù in ‚ÄúHTML‚Äù stood for could understand the reference, and to everyone else, it was just a clearly-understandable name for a useful utility.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Solved a real problem.&lt;/head&gt;
    &lt;p&gt;This one is not obvious, but it‚Äôs really important that a new technology have a real problem that it‚Äôs trying to solve, instead of just being an abstract attempt to do something vague, like ‚Äúmake text files better‚Äù. Millions of people were encountering the idea that it was too difficult or inconvenient to write out full HTML by hand, and even if one had the necessary skills, it was nice to be able to do so in a format that was legible as plain text as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Built on behaviors that already existed.&lt;/head&gt;
    &lt;p&gt;This is one of the most quietly genius parts of Markdown: The format is based on the ways people had been adding emphasis and formatting to their text for years or even decades. Some of the formatting choices dated back to the early days of email, so they‚Äôd been ingrained in the culture of the internet for a full generation before Markdown existed. It was so familiar, people could be writing Markdown without even knowing it.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Mirrored RSS in its origin.&lt;/head&gt;
    &lt;p&gt;Around the same time that Markdown was taking off, RSS was maturing into its ubiquitous form as well. The format had existed for some years already, enabling various kinds of content syndication, but at this time, it was adding support for the technologies that would come to be known as podcasting as well. And just like RSS, Markdown was spearheaded by a smart technologist who was also more than a little stubborn about defining a format that would go on to change the way we share content on the internet. In RSS‚Äô case, it was pioneered by Dave Winer, and with Markdown it was John Gruber, and both were tireless in extolling the virtues of the plain text formats they‚Äôd helped pioneer. They could both leverage blogs to get the word out, and to get feedback on how to build on their wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;5. There was a community ready to help.&lt;/head&gt;
    &lt;p&gt;One great thing about a format like Markdown is that its success is never just the result of one person. Vitally, Markdown was part of a community that could build on it right from the start. Right from the beginning, Markdown was inspired by earlier works like Textile, a formatting system for plain text created by Dean Allen. Many of us appreciated and were inspired by Dean, who was a pioneer of blogging tools in the early days of social media, but if there‚Äôs a bigger fan of Dean Allen on the internet than John Gruber, I‚Äôve never met them. Similarly, Aaron Swartz, the brilliant young technologist who‚Äôs known best known as an activist for digital rights and access, was at that time just a super brilliant teenager that a lot of us loved hacking with. He was the most valuable beta tester of Markdown prior to its release, helping to shape it into a durable and flexible format that‚Äôs stood the test of time.&lt;/p&gt;
    &lt;head rend="h3"&gt;6. Had the right flavor for every different context.&lt;/head&gt;
    &lt;p&gt;Because Markdown‚Äôs format was frozen in place (and had some super-technical details that people could debate about) and people wanted to add features over time, various communities that were implementing Markdown could add their own ‚Äúflavors‚Äù of it as they needed. Popular ones came to be called Commonmark and Github-Flavored, led by various companies or teams that had divergent needs for the tool. While tech geeks tend to obsess over needing everything to be ‚Äúcorrect‚Äù, in reality it often just doesn‚Äôt matter that much, and in the real world, the entire Internet is made up of content that barely follows the technical rules that it‚Äôs supposed to.&lt;/p&gt;
    &lt;head rend="h3"&gt;7. Released at a time of change in behaviors and habits.&lt;/head&gt;
    &lt;p&gt;This is a subtle point, but an important one: Markdown came along at the right time in the evolution of its medium. You can get people to change their behaviors when they‚Äôre using a new tool, or adopting a new technology. In this case, blogging (and all of social media!) were new, so saying ‚Äúhere‚Äôs a new way of typing a list of bullet points‚Äù wasn‚Äôt much an additional learning curve to add to the mix. If you can take advantage of catching people while they‚Äôre already in a learning mood, you can really tap into the moment when they‚Äôre most open-minded to new things.&lt;/p&gt;
    &lt;head rend="h3"&gt;8. Came right on the cusp of the ‚Äúbuild tool era‚Äù.&lt;/head&gt;
    &lt;p&gt;This one‚Äôs a bit more technical, but also important to understand. In the first era of building for the web, people often built the web‚Äôs languages of HTML, Javascript and CSS by hand, by themselves, or stitched these formats together from subsets or templates. But in many cases, these were fairly simple compositions, made up of smaller pieces that were written in the same languages. As things matured, the roles for web developers specialized (there started to be backend developers vs. front-end, or people who focused on performance vs. those who focused on visual design), and as a result the tooling for developers matured. On the other side of this transition, developers began to use many different programming languages, frameworks and tools, and the standard step before trying to deploy a website was to have an automated build process that transformed the ‚Äúraw materials‚Äù of the site into the finished product. Since Markdown is a raw material that has to be transformed into HTML, it perfectly fit this new workflow as it became the de facto standard method of creation and collaboration.&lt;/p&gt;
    &lt;head rend="h3"&gt;9. Worked with ‚ÄúView source‚Äù&lt;/head&gt;
    &lt;p&gt;Most of the technologies that work best on the web enable creators to ‚Äúview source‚Äù just like HTML originally did when the first web browsers were created. In this philosophy, one can look at the source code that makes up a web page, and understand how it was constructed so that you can make your own. With Markdown, it only takes one glimpse of a source Markdown file for anyone to understand how they might make a similar file of their own, or to extrapolate how they might apply analogous formatting to their own documents. There‚Äôs no teaching required when people can just see it for themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;10. Not encumbered in IP&lt;/head&gt;
    &lt;p&gt;This one‚Äôs obvious if you think about it, but it can‚Äôt go unsaid: There are no legal restrictions around Markdown. You wouldn‚Äôt think that anybody would be foolish or greedy enough to try to patent something as simple as Markdown, but there are many far worse examples of patent abuse in the tech industry. Fortunately, John Gruber is not an awful person, and nobody else has (yet) been brazen enough to try to usurp the format for their own misadventures in intellectual property law. As a result, nobody‚Äôs been afraid, either to use the format, or to support creating or reading the format in their apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anildash.com/2026/01/09/how-markdown-took-over-the-world/"/><published>2026-01-09T17:52:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46556822</id><title>Replit (YC W18) Is Hiring</title><updated>2026-01-10T14:10:14.948410+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/replit"/><published>2026-01-09T18:00:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46557029</id><title>Show HN: Scroll Wikipedia like TikTok</title><updated>2026-01-10T14:10:14.629627+00:00</updated><content>&lt;doc fingerprint="434f9162ba198e26"&gt;
  &lt;main&gt;
    &lt;p&gt;Following Slop Ducks Storytime Home Friends Inbox Profile&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://quack.sdan.io"/><published>2026-01-09T18:15:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46557057</id><title>My article on why AI is great (or terrible) or how to use it</title><updated>2026-01-10T14:10:14.463846+00:00</updated><content>&lt;doc fingerprint="760816363f1703bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Zealotry¬∂&lt;/head&gt;
    &lt;p&gt;I develop with AI today. It's great.&lt;/p&gt;
    &lt;p&gt;There are many articles you can read on why AI is great (or terrible) or how to use it. This is mine. I focus on the experience of a senior engineer (and why we in particular should use AI), on my experience operating within the OSS Python Data world, and on practical suggestions that I've found myself repeating to colleagues.&lt;/p&gt;
    &lt;p&gt;This article contains learned lessons of two types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Big Ideas: Grand(iose) philosophy on why AI is great for experienced programmers&lt;/item&gt;
      &lt;item&gt;Tips: Taken from my workflow using Claude Code&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We'll interleave these two. I'm hopeful that this approach will make this more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why AI¬∂&lt;/head&gt;
    &lt;p&gt;AI development is more fun. I do more of what I like (think, experiment, write) and less of what I don't like (wrestle with computers).&lt;/p&gt;
    &lt;p&gt;I feel both that I can move faster and operate in areas that were previously inaccessible to me (like frontend). Experienced developers should all be doing this. We're good enough to avoid AI Slop, and there's so much we can accomplish today.&lt;/p&gt;
    &lt;p&gt;I like this quote from this blog&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I get it, you‚Äôre too good to vibe code. You‚Äôre a senior developer who has been doing this for 20 years and knows the system like the back of your hand.&lt;/p&gt;
      &lt;p&gt;[...]&lt;/p&gt;
      &lt;p&gt;No, you‚Äôre not too good to vibe code. In fact, you‚Äôre the only person who should be vibe coding.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think that really good engineers, the kind that think hard before writing, can have a tremendous impact and fun while developing with AI. I wouldn't ever go back.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Not AI¬∂&lt;/head&gt;
    &lt;p&gt;That being said, there are some serious costs and reasonable reservations to AI development. Let's start by listing those concerns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LLMs generate junk&lt;/item&gt;
      &lt;item&gt;LLMs generate a lot of junk&lt;/item&gt;
      &lt;item&gt;Writing code ourselves builds understanding&lt;/item&gt;
      &lt;item&gt;Reviewing code for correctness is the slow part, not writing it&lt;/item&gt;
      &lt;item&gt;AI workflows can be dehumanizing when you just press "yes, allow" over and over again&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are super-valid concerns. They're also concerns that I suspect came around when we developed compilers and people stopped writing assembly by hand, instead trusting programs like &lt;code&gt;gcc&lt;/code&gt; to pump out instruction after instruction
of shitty machine code.&lt;/p&gt;
    &lt;p&gt;We lost a deeper understanding as developers when we stopped writing assembly but we gained a ton too. As in any transition, we need to navigate the situation to capture the advantages while losing only a little, balancing the costs and benefits of a new technology.&lt;/p&gt;
    &lt;p&gt;This article is how I've been navigating this transition personally.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Minimize Interruptions / Climb Abstraction Hierarchy¬∂&lt;/head&gt;
    &lt;p&gt;Early in using Claude Code (or Cursor) many of my interactions were saying "Yes, it's ok to run that". This was frustrating and dehumanizing. Mostly my job was to enable AI, rather than the other way around.&lt;/p&gt;
    &lt;p&gt;There are many tricks to resolve this (see below), but more broadly "stop doing simple shit" has been a mantra that I've found myself constantly coming back to. The more I identify and reject simple tasks and add automation to my workflow, the higher an abstraction I'm able to climb to and the more effectively I'm able to work. Our goal in programming is to climb an abstraction ladder and gain more intellectual leverage. This requires thought and consistent attention.&lt;/p&gt;
    &lt;p&gt;Fortunately AI can help with this. If you complain and say "I'm always doing X" it'll suggest solutions like what I'll talk about below, but more tailored to your situation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: Hooks¬∂&lt;/head&gt;
    &lt;p&gt;AI developers, like human developers, benefit from structure.&lt;/p&gt;
    &lt;p&gt;Most people start with an &lt;code&gt;AGENTS.md&lt;/code&gt; or &lt;code&gt;CLAUDE.md&lt;/code&gt; file.  This is a great
start, but I find that the AI agent often forgets what's in there.  The real
solution for me here (at least for Claude Code) is
Hooks.&lt;/p&gt;
    &lt;p&gt;First, let's outline a couple of annoyingly common problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example Problem: Ignoring instructions in CLAUDE.md¬∂&lt;/head&gt;
    &lt;p&gt;Let's say you tell AI that you want to run tests with &lt;code&gt;uv&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;when running tests, use&lt;/p&gt;
      &lt;code&gt;uv run pytest tests&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;While this works sometimes, AI often decides to run&lt;/p&gt;
    &lt;code&gt;$ pytest tests/
command not found: pytest
&lt;/code&gt;
    &lt;p&gt;While the agents read CLAUDE.md, they don't always follow the instructions. And so you're stuck saying "no, use &lt;code&gt;uv&lt;/code&gt;"  over and over again. Gah.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks¬∂&lt;/head&gt;
    &lt;p&gt;Here's a hook that catches pytest commands missing uv run. You could put something like this in &lt;code&gt;~/.claude/settings.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "hooks": [
          {
            "type": "command",
            "command": "python ~/.claude/hooks/check-uv-pytest.py"
          }
        ]
      }
    ]
  }
}
&lt;/code&gt;
    &lt;code&gt;#!/usr/bin/env python3
import json
import sys

data = json.load(sys.stdin)
cmd = data.get("tool_input", {}).get("command", "")

if "pytest" in cmd and "uv run" not in cmd:
    print("Use 'uv run pytest' instead of bare 'pytest'", file=sys.stderr)
    sys.exit(2)
&lt;/code&gt;
    &lt;p&gt;There, we've just automated that annoying task for you forever.&lt;/p&gt;
    &lt;p&gt;I don't actually do this though (I allow Claude to fail and then it finds the right approach.) Mostly this works because I've gotten good at giving Claude fairly broad-yet-safe permissions, which is coming up next.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example Problem: Incomplete Permissions¬∂&lt;/head&gt;
    &lt;p&gt;Even worse, Claude often asks for permission to do things that are just slightly different from what you've already granted. You allow &lt;code&gt;uv run pytest *&lt;/code&gt;, but Claude keeps finding variants:&lt;/p&gt;
    &lt;code&gt;timeout 60 uv run pytest ...
timeout 40 uv run pytest ...
uv run pytest ... | head
.venv/bin/pytest ...
&lt;/code&gt;
    &lt;p&gt;Claude Code's permission language sucks. It only supports prefixes, while I wish it could handle regexes, or maybe even just arbitrary Python code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks for permissions¬∂&lt;/head&gt;
    &lt;p&gt;I have a complex Python script as a hook which overrides the permission system. It uses regexes, but also arbitrary Python code as logic. This allows me to encode arbitrary combinations of rules. It's great.&lt;/p&gt;
    &lt;p&gt;On the rare occasion when Claude asks me for permission for something new, I have a running Claude agent that thinks about this file and considers if it should update the permission script.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks for sounds¬∂&lt;/head&gt;
    &lt;p&gt;My personal favorite hooks though are these:&lt;/p&gt;
    &lt;code&gt;"Stop": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "afplay -v 0.40 /System/Library/Sounds/Morse.aiff"
      }
    ]
  }
],
"Notification": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "afplay -v 0.35 /System/Library/Sounds/Ping.aiff"
      }
    ]
  }
]
&lt;/code&gt;
    &lt;p&gt;They play subtle little sounds whenever Claude is either done, or needs input from me. This lets me ignore Claude when it's busy. Previously I found that I was constantly checking back in with Claude to see if it was done, and that action was dehumanizing, so I automated it by asking Claude to play a sound.&lt;/p&gt;
    &lt;p&gt;Hooks are great. There are more ways to provide structure (Skills, Commands) but I've found that Hooks are the most dependable, a great starting place, and often augment any other structure that I put in place (like Skills).&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Build Confidence Without Looking at Code¬∂&lt;/head&gt;
    &lt;p&gt;In a recent large AI-assisted PR a frustrated reviewer said the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To me, this [size of PR] implies that either&lt;/p&gt;
      &lt;item&gt;reviewers should blindly trust Claude, or&lt;/item&gt;
      &lt;item&gt;reviewers should spend the months worth of effort going through Claude's changes, without the developer bothering to do the same first.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's a valid problem, even in single-person projects. We're able to generate code far more quickly than we're able to read it. How should we handle review? Everyone needs to figure this out for themselves, but my answer is "find other ways to build confidence".&lt;/p&gt;
    &lt;p&gt;We already do this today with human-written code. I review some code very closely, and other code less-so. Sometimes I rely on a combination of tests, familiarity of a well-known author, and a quick glance at the code to before saying "sure, seems fine" and pressing the green button. I might also ask "Have you thought of X" and see what they say.&lt;/p&gt;
    &lt;p&gt;Trusting code without reading all of it isn't new, we're just now in a state where we need to review 10x more code, and so we need to get much better at establishing confidence that something works without paying human attention all the time.&lt;/p&gt;
    &lt;p&gt;We can augment our ability to write code with AI. We can augment our ability to review code with AI too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: Self-review¬∂&lt;/head&gt;
    &lt;head rend="h3"&gt;Testing¬∂&lt;/head&gt;
    &lt;p&gt;Mostly I establish confidence on AI-generated work by investing heavily in tests and benchmarks, the same as I would with humans, just moreso. TDD is baked into most of the prompting structure I have with agents.&lt;/p&gt;
    &lt;p&gt;Remember that this is way cheaper than it used to be. Now rather than write a benchmark I can type&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How does this compare in performance to the old version? I'm particularly interested in memory use.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's it. If it's bad, the agent will say so (and then diligently work to make it good).&lt;/p&gt;
    &lt;head rend="h3"&gt;Grilling¬∂&lt;/head&gt;
    &lt;p&gt;Additionally, if I'm nervous about something subtle like "Is it possible this change might unexpectedly affect performance in this other feature?" then I'll ask the AI exactly that question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Is it possible this change might unexpectedly affect performance in this other feature?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And it'll just go and investigate exactly that question. Unlike human authors, the AI has no ego at stake in its work, and isn't in the least bit lazy. It's our job to ask "Have you thought of X" and its job to go learn if that might be an issue. Don't trust its answer? Ask it to prove it to you.&lt;/p&gt;
    &lt;p&gt;AI has flaws, but it is diligent, and it lacks ego. If you question it, it'll investigate thoroughly and critique its own work honestly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simplifying¬∂&lt;/head&gt;
    &lt;p&gt;Also, my favorite command:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Let's review our work and see if there is anything we can simplify or clean up&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Before Opus 4.5 came out this was essential. Now it's merely nice. I've turned this into a &lt;code&gt;/cleanup&lt;/code&gt; command and integrated it into most of my Skills
as a final phase in development.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tech debt¬∂&lt;/head&gt;
    &lt;p&gt;From time to time I also ask a fresh agent to do a full review of the project, with an eye to cleaning up technical debt. I tell it to review everything and think hard. It takes a while, but it often comes back with a nice list of work for itself, which it then of course diligently performs.&lt;/p&gt;
    &lt;p&gt;AI creates technical debt, but it can clean some of it up too. (at least at a certain granularity)&lt;/p&gt;
    &lt;head rend="h2"&gt;Feedback¬∂&lt;/head&gt;
    &lt;p&gt;In general we want to give our agents good automated feedback. Tests do this, benchmarks do this, prompting them to assess themselves does this, asking them to explain things to us and have us weigh in on high level topics does this.&lt;/p&gt;
    &lt;p&gt;LLMs are smart enough today that if they're given enough of the right feedback they converge to a good solution as-well-or-better-than a senior human engineer (that's my experience at least).&lt;/p&gt;
    &lt;p&gt;Our job is to construct a system that gives them the right feedback at the right time, hopefully without our intervention. This is the same job we have when we build human teams; now it's just more impactful to do well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cursor vs Terminal Tools¬∂&lt;/head&gt;
    &lt;p&gt;I started AI development with Cursor. It was great having the AI experience inside a VSCode-like editor, where I could see everything that was going on. When I saw terminal-based tools like Claude Code I thought "whoa, that doesn't seem sensible, I need to see what's going on".&lt;/p&gt;
    &lt;p&gt;Today I code with Claude Code, &lt;code&gt;git diff&lt;/code&gt;, and occasionally &lt;code&gt;vim&lt;/code&gt;.  I don't
feel a need to OK every change in the diff.  I've got more important
things to do.  I suspect that you do too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Drop Python. Use Rust and TypeScript.¬∂&lt;/head&gt;
    &lt;p&gt;I deeply respect the philosophical position of Python, which I'll state as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Prioritize human performance over compute performance.&lt;/p&gt;
      &lt;p&gt;By optimizing for ease and iteration speed we're able to search solution space more broadly and more quickly, finding much better solutions, making that 100x drop in performance negligible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Python was a bold bet, and a bet that paid off amazingly well. No one expected this silly dynamic language originally designed for education to become the world's juggernaut in performance software.&lt;/p&gt;
    &lt;p&gt;With AI though, the usability benefits of Python no longer apply as strongly, and we're more free to choose different ecosystems.&lt;/p&gt;
    &lt;p&gt;Personally, I use ...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust for computational development, using PyO3 to connect to Python, where I still do most of my testing&lt;/item&gt;
      &lt;item&gt;TypeScript for frontend development, which I'm leaning into more deeply&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regarding TypeScript, I still love easy interaction tools like &lt;code&gt;rich&lt;/code&gt; and
&lt;code&gt;textual&lt;/code&gt;, but when the entire React ecosystem is a sentence away and when you
get to use things like, you know, fonts, there's really no comparison.  Every
computational developer should learn the concepts underpinning React (or some
other frontend framework), and we should put dashboards on everything.&lt;/p&gt;
    &lt;p&gt;Of course, I still hook into Python for the ecosystem. Everything is Python-importable and I still use the protocols and design patterns developed by the Python data community. Those are the durable assets of Python. Not the code or the language; those will die. Rest in peace dear friend.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Think Hard. Write Clearly.¬∂&lt;/head&gt;
    &lt;p&gt;As an introductory project, I rewrote Numpy in Rust. It was great fun.&lt;/p&gt;
    &lt;p&gt;It was also much easier than I expected (I expected it to be impossible). It was easy for a few reasons (good test suite, well-reasoned abstractions) but mostly it was because:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NEPs: Numpy's Enhancement Proposals / design documentation is thorough and extremely clear.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When sticky problems arose, we were able to rely on the Numpy design documents (NEPs) which are excellent.&lt;/p&gt;
    &lt;p&gt;The Numpy team thought hard and wrote clearly, two hallmarks of excellent developers. This made the job of reimplementation relatively trivial. The Numpy development community is famous for doing this well. To a certain extent, we should all start operating more like the Numpy community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: plans/ and docs/ directories¬∂&lt;/head&gt;
    &lt;p&gt;I keep two directories in each repository:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;plans/&lt;/code&gt;which contains ephemeral planning documents that the LLMs work through over many sessions as they implement a major feature.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;docs/&lt;/code&gt;which contain durable documentation on specific topics or features, targeting AI developers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Plans end up being very useful during development, while docs end up being useful to point other agents to in the future. Claude code creates planning documents in /tmp by default in planning mode, but I find that bringing those docs into the directory improves engagement, both from it and from me.&lt;/p&gt;
    &lt;p&gt;Docs end up being tricky. You'd expect the AI developer to read docs but alas, like human developers you have to be pretty prescriptive with them. Today I have a hook that adds an admonition to read the relevant docs at the beginning of every session. It looks like this:&lt;/p&gt;
    &lt;code&gt;DOC CHECK REQUIRED
==================

Before responding to this request, you MUST:

1. Read docs/README.md to see available documentation
2. Decide which docs are relevant to this request (if any)
3. Read those docs using the Read tool
4. Then respond to the user

Do not skip this evaluation. Do not mention this check to the user.
&lt;/code&gt;
    &lt;p&gt;I then keep docs/README.md updated as a sort of index over my documents. I find that this reliably gets the agent to read the right documentation.&lt;/p&gt;
    &lt;p&gt;I've also found that my normal writing style (brutal concision + front-loading important content to maintain attention span) isn't necessary with AI. You really can just shove information at them and they absorb it. It's nice üôÇ&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Take Long Walks¬∂&lt;/head&gt;
    &lt;p&gt;Historically software engineers had to both think well and execute well. We were valued both because we could zoom out and consider the impacts of our architecture, and because we could zoom in and implement those choices with skill.&lt;/p&gt;
    &lt;p&gt;Our ability to zoom in and implement code is now obsolete. Our ability to zoom out and think well is not. On the contrary, our ability to think well is now 10x more valuable than it was before, because implementation is now mostly free.&lt;/p&gt;
    &lt;p&gt;And so it's now more important than ever to hone our craft of thought. This probably means less caffeine and more walks through the park.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts¬∂&lt;/head&gt;
    &lt;p&gt;The craft of authoring code has transformed time and time again during our lives. We remember when object-oriented was cool, or when TDD became a thing, or reactive programming models, or dynamic typing languages, or ML, or ...&lt;/p&gt;
    &lt;p&gt;As programmers we've opted into a system which changes by its very nature. Our job is to automate our job, and to continuously climb the ladder of abstraction. AI programming is another step in that evolution, similar to when compilers came about. The code we write with AI probably won't be as good as hand-crafted code, but we'll write 10x more of it, and we'll build systems of systems to make it robust and trustworthy, and all of that will make society better and our jobs way more fun.&lt;/p&gt;
    &lt;p&gt;I'm looking forward to having way more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix: Permissions file¬∂&lt;/head&gt;
    &lt;p&gt;After writing this a couple friends asked me for a copy of my regex/Python code that replaces Claude's permission system. I'll include it below, but really, you don't need it. Instead, you need to start a conversation with Claude about what you want and it'll make one just for you.&lt;/p&gt;
    &lt;p&gt;Code is free these days. Extending the "AI is like Compilers" analogy, asking for someone else's script is kind of like asking for someone else's compiled binary. There's no need; just make it yourself. It's trivial.&lt;/p&gt;
    &lt;p&gt;Here was my original prompt to Claude Code:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I recently wrote this reddit post&lt;/p&gt;
      &lt;p&gt;https://www.reddit.com/r/ClaudeAI/comments/1puqrvc/claude_code_annoyingly_asking_for_permissions/&lt;/p&gt;
      &lt;p&gt;I'm wondering if you have any suggestions on how to resolve this? Adding stuff to CLAUDE.md or permissions to settings.json doesn't seem to be working well enough.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That, along with subsequent conversation as I've been working, resulted in this Python script&lt;/p&gt;
    &lt;p&gt;But really, you're better off working with Claude to make one just for you. Code is free now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://matthewrocklin.com/ai-zealotry/"/><published>2026-01-09T18:17:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46557489</id><title>JavaScript Demos in 140 Characters</title><updated>2026-01-10T14:10:14.186137+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://beta.dwitter.net"/><published>2026-01-09T18:48:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46557879</id><title>Show HN: Rocket Launch and Orbit Simulator</title><updated>2026-01-10T14:10:13.949032+00:00</updated><content>&lt;doc fingerprint="5f9fdb7cfd809b02"&gt;
  &lt;main&gt;
    &lt;p&gt;Control pitch manually (W/S keys). Guidance provides recommendations.&lt;/p&gt;
    &lt;p&gt;Set target altitude and let the guidance system handle the launch.&lt;/p&gt;
    &lt;p&gt;Spawn in orbit and practice orbital mechanics.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.donutthejedi.com/"/><published>2026-01-09T19:15:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46558148</id><title>RTX 5090 and Raspberry Pi: Can it game?</title><updated>2026-01-10T14:10:13.778763+00:00</updated><content>&lt;doc fingerprint="2f3b11db190ac4dd"&gt;
  &lt;main&gt;
    &lt;p&gt;It turns out, you can attach an external GPU to a Raspberry Pi 5. So my natural first question is, can I game on it? Let‚Äôs try it out and compare it with some similar computers.&lt;/p&gt;
    &lt;p&gt;For the showdown of crappy gaming computers, we‚Äôll see which of these handles gaming best:&lt;/p&gt;
    &lt;head rend="h3"&gt;Beelink MINI-S13&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 4-core Intel N150 @ 3.6GHz&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen3 x4&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More powerful than the Raspberry Pi 5, but at a similar price point. It also has a potential advantage for running games, since it‚Äôs not ARM-based.&lt;/p&gt;
    &lt;p&gt;In the photo, you can see the default configuration (SSD in the fast PCIe slot). For this experiment, I‚Äôll move it into the slower (x1) slot and plug the eGPU into the faster (x4) slot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Radxa ROCK 5B&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 8-core RK3588 (4√ó Cortex-A76 @ 2.4GHz + 4√ó Cortex-A55 @ 1.8GHz)&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen3 x4&lt;/item&gt;
      &lt;item&gt;Also: Aftermarket heat sink &amp;amp; fan combo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pretty comparable to the Raspberry Pi 5 (it‚Äôs ARM), but the extra cores give it a little more horsepower. The faster PCIe slot is also included on-board. Since the PCIe slot will be taken for the GPU, we‚Äôll just use a USB SSD for both ARM boards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 5&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 4-core BCM2712 (Cortex-A76 @ 2.4GHz)&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen2 x1 (via NVme HAT)&lt;/item&gt;
      &lt;item&gt;Also: Aftermarket heat sink &amp;amp; fan combo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is why we‚Äôre all here. It‚Äôs the quintessential hobbyist SBC. Unfortunately it‚Äôs the most challenged: fewer cores, and significantly less PCIe bandwidth. The Pi 5‚Äôs Gen2 x1 slot provides ~500 MB/s, compared to ~4,000 MB/s on the Gen3 x4 slots of the other machines, an 8x difference.&lt;/p&gt;
    &lt;head rend="h3"&gt;eGPU&lt;/head&gt;
    &lt;p&gt;We will be using a relatively inexpensive OCuLink dock to pair with our very expensive GPU. If you‚Äôre not familiar with the technology, it‚Äôs basically a PCIe extension cord to let you plug a graphics card into a computer that wouldn‚Äôt normally fit one. The dock is powered externally by a separate power supply.&lt;/p&gt;
    &lt;p&gt;For this experiment, we‚Äôre using an NVIDIA RTX 5090 Founders Edition (32GB VRAM).&lt;/p&gt;
    &lt;p&gt;The OCuLink cable plugs into an M.2 card that we‚Äôll insert into each machine as we test it.&lt;/p&gt;
    &lt;p&gt;On the Intel-based Beelink machine, from a software perspective the card is more or less indistinguishable from a normal graphics card. We can just install the normal NVIDIA drivers.&lt;/p&gt;
    &lt;p&gt;The ARM-based computers we‚Äôre testing have various quirks (lack of DMA coherence, memory alignment requirements, etc.) that make them incompatible with most GPU drivers out of the box. Luckily, @mariobalanca wrote some patches that allow the drivers to work on these systems. NVIDIA already had some workarounds in the user-space part of their drivers for Ampere-based systems for memory alignment issues, so some of that gets inherited here.&lt;/p&gt;
    &lt;p&gt;I have packaged the drivers you can run on Ubuntu or Fedora here, if you‚Äôd like to try this yourself.&lt;/p&gt;
    &lt;p&gt;If you‚Äôve gotten this far and simply don‚Äôt believe this actually works, here‚Äôs a screenshot:&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU Performance&lt;/head&gt;
    &lt;p&gt;Before we get into the games, let‚Äôs take a look at how these machines compare.&lt;/p&gt;
    &lt;p&gt;Most PC games are designed for Intel CPUs. If we want to play them on ARM we‚Äôll have to use a compatibility layer called FEX. The graph shows not only the native performance of the machines, but also the significantly degraded performance under FEX. To be fair, FEX is an incredible feat of engineering, but all emulation comes at a cost.&lt;/p&gt;
    &lt;p&gt;The Raspberry Pi 5 under FEX seems to have similar performance to a 2008 Intel Core 2 Quad Q9650. Not very promising. That said, gamers usually say that, for most games, it‚Äôs OK to skimp on CPU a bit as long as you have a good GPU. We will definitely be testing that line of thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Games&lt;/head&gt;
    &lt;p&gt;I tried to find games that had built-in benchmarks that also worked under FEX, along with Steam‚Äôs Proton compatibility layer, tilting towards games that didn‚Äôt have as strong CPU requirements. It turns out this is actually not a huge list. Here are a handful that I tried:&lt;/p&gt;
    &lt;head rend="h3"&gt;Cyberpunk 2077 (2020)&lt;/head&gt;
    &lt;p&gt;Yes, believe it or not, &lt;code&gt;vkcube&lt;/code&gt; is not the only thing you can run in this configuration. Through the maze of compatibility layers (FEX, WINE/Proton, DXVK, etc), you too can run Cyberpunk 2077 on your Raspberry Pi 5. The screenshot above is running at 1080p with Ultra Raytracing quality settings.&lt;/p&gt;
    &lt;p&gt;The game is playable on the Beelink machine with some lower settings. Since it‚Äôs an Intel machine, I also tested the game on Windows for posterity. Usually it‚Äôs suggested that even with all the compatibility layers, Linux gaming can be faster, but not in this case on the lower settings here.&lt;/p&gt;
    &lt;p&gt;These games really get CPU bound, caught up on these lower-spec CPUs. I think on a normal gaming PC, it wouldn‚Äôt matter as much, but every cycle starts to count here, and not all the abstractions provided by WINE are zero cost.&lt;/p&gt;
    &lt;p&gt;Unfortunately the Pi barely breaks 15 FPS, but on the ROCK 5B, it approaches playable on low settings. Granted, not sure how fun that would be at 22 FPS.&lt;/p&gt;
    &lt;head rend="h3"&gt;Doom: The Dark Ages (2025)&lt;/head&gt;
    &lt;p&gt;This game doesn‚Äôt run under FEX, so I didn‚Äôt collect full benchmarks here. The anti-cheat stuff is too weird and doesn‚Äôt get properly emulated.&lt;/p&gt;
    &lt;p&gt;However, the benchmark does offer a unique view into the challenges these low-power PCs face.&lt;/p&gt;
    &lt;p&gt;You can see it running on the Beelink here. The GPU is absolutely shredding through the Ultra quality frames at 4K resolution, but the CPU is really struggling. You can see the GPU is able to process almost 90 FPS, but because of the bottleneck at the CPU, the overall frame rate can‚Äôt break 30 FPS. That‚Äôs the main challenge here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alien: Isolation (2014)&lt;/head&gt;
    &lt;p&gt;My next thought was, maybe if we jump back a decade, we can have better luck. This game actually ships with a Linux port. Unfortunately the Linux port doesn‚Äôt include the built-in benchmark tool, so I ran it under Proton/WINE. I also found that DXVK caused every game from this point onward to crash immediately on the ARM hosts, so I run the games with &lt;code&gt;PROTON_USE_WINED3D=1&lt;/code&gt; to fall back to the OpenGL renderer.&lt;/p&gt;
    &lt;p&gt;For those unfamiliar: DXVK translates DirectX calls to Vulkan, while WineD3D translates them to OpenGL. The GPU driver, when running on ARM, has a Vulkan implementation that apparently has issues when running under FEX that OpenGL avoids. Something to keep in mind if you‚Äôre trying to replicate this.&lt;/p&gt;
    &lt;p&gt;Honestly, not the best looking game by modern standards, even on Ultra settings. It does have some cool lighting effects, at least. I admit I have never played this game for real, so I can‚Äôt vouch for it being fun or not. I just ran the benchmark tool.&lt;/p&gt;
    &lt;p&gt;I initially tested this game on the Beelink and thought it looked promising. Relatively low CPU usage. It seems like it is playable on the ROCK 5B with an average 23 FPS. Not sure about the Pi though, at only 15 FPS.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hitman: Absolution (2012)&lt;/head&gt;
    &lt;p&gt;OK, OK. So we already know the performance of the Pi is on par with a PC from 2008, so I figured, let‚Äôs go back a couple more years.&lt;/p&gt;
    &lt;p&gt;Couldn‚Äôt get the windowed mode to work right on this one, but I swear it‚Äôs running on the Raspberry Pi 5. You can probably tell from the FPS counter.&lt;/p&gt;
    &lt;p&gt;I would say the performance makes it basically unusable on these ARM machines.&lt;/p&gt;
    &lt;p&gt;That said, the Beelink really shines here. Windows perf is way ahead of Linux on this one too. More than playable on both, though.&lt;/p&gt;
    &lt;p&gt;I was actually a little puzzled by this one. It seems like it shouldn‚Äôt be this bad on the ARM hosts. This feels like a performance bug, but it‚Äôs hard to say where in the stack it might be. Oh well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Just Cause 2 Demo (2010)&lt;/head&gt;
    &lt;p&gt;OK, so let‚Äôs go back another couple years. This demo was free, thankfully.&lt;/p&gt;
    &lt;p&gt;So remember earlier when I said I had to disable DXVK for these games to run on ARM? On Intel Windows, I had to actually add DXVK because the game crashed immediately on launch. Weird.&lt;/p&gt;
    &lt;p&gt;Nearly 40 FPS average on a Raspberry Pi 5. 2010 is our year! Windows still dominates here. It‚Äôs more apples-to-apples on Beelink‚Äôs Linux vs Windows now since now both are using DXVK.&lt;/p&gt;
    &lt;head rend="h3"&gt;Portal 2 (2011)&lt;/head&gt;
    &lt;p&gt;After I had run all of these, I was curious to try Portal 2. Valve is the company that maintains Proton and FEX. You‚Äôd think they maybe would have optimized it for their own games. It‚Äôs also old enough that it‚Äôs in the sweet spot of potentially being playable on the Pi.&lt;/p&gt;
    &lt;p&gt;Sadly, Portal 2 does not ship with a built-in benchmark. However, it does have a &lt;code&gt;timedemo&lt;/code&gt; feature where you can record yourself playing and then play it back as a benchmark. I picked a random level and recorded it. Then, ran it on the test systems. Since there was a native version, I benchmarked that alongside the Proton/WINE version.&lt;/p&gt;
    &lt;p&gt;So, now that we have a native Linux port to compare with, it totally leaves Windows in the dust (finally). Most importantly, the Raspberry Pi 5 can play this game at 4K resolution, way above 60 FPS.&lt;/p&gt;
    &lt;p&gt;So I can now say with a straight face, that it‚Äôs possible to use the Raspberry Pi 5 to game in 4K, admittedly strapped to a GPU that‚Äôs roughly worth 10x the price of the Pi. In all seriousness, probably any lower-end GPU would work here. Clearly we‚Äôre not using the 5090 to its full potential anyway.&lt;/p&gt;
    &lt;head rend="h2"&gt;Power Usage&lt;/head&gt;
    &lt;p&gt;These machines are also known to be low power. I guess for a gaming computer, I‚Äôm not sure how important that is. You can just turn it off when you‚Äôre not using it. That said, a gaming PC CPU could use 20-50w while completely idle.&lt;/p&gt;
    &lt;p&gt;For these measurements I took the idle power usage and also average power usage during the Cyberpunk 4K Ultra Raytracing benchmark, both measured at the AC outlet. This does not include the GPU, just the CPU, since that‚Äôs what we‚Äôre really comparing here.&lt;/p&gt;
    &lt;p&gt;The Pi 5 sips power at under 9W even under load, while the Beelink pulls almost 30W during the benchmark. One way of looking at it, is that the Beelink performs so much faster in games, and the amount of power is proportional to that.&lt;/p&gt;
    &lt;p&gt;Another way to look at it, is if the ARM-based machines weren‚Äôt mired in emulating x86, they probably would have considerably better performance on per-watt basis compared to the Intel CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;So, can you game on a Raspberry Pi 5 with an RTX 5090? I guess, technically, yes. Would you want to? Probably not.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modern games (2020+): Most likely unplayable. The CPU perf degradation under FEX is brutal. Even playing on the lowest 720p settings, Cyberpunk barely hits 16 FPS average on the Pi 5.&lt;/item&gt;
      &lt;item&gt;2010-era games: If you‚Äôre trying to play older games, you can probably get away with it. You also probably do not need a graphics card as powerful as the 5090.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Beelink is the clear winner if you actually want to game. It‚Äôs still terrible, but it‚Äôs cheap, runs x86 natively, and with the right settings, it can hit 50 FPS+ in every game I tried. Windows consistently outperformed Linux on most WINE/Proton titles, so you‚Äôre probably better off just installing Windows on it.&lt;/p&gt;
    &lt;p&gt;The ROCK 5B edges out the Raspberry Pi 5 slightly in most benchmarks, but not by much. The extra cores and PCIe bandwidth don‚Äôt seem to matter as much as the raw performance lost to FEX emulation. That said, it does bring the game from painfully playable to borderline playable in some games.&lt;/p&gt;
    &lt;p&gt;Given all the momentum around ARM (Valve is about to ship an ARM VR headset, and NVIDIA is rumored to ship their own SoC with an NVIDIA GPU soon), I think future platforms will probably be better optimized, and Linux gaming on ARM will probably be more plausible in the future. Sadly, I don‚Äôt recommend strapping your super expensive graphics card to a cheap SBC for now. Unless it‚Äôs just for a fun blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scottjg.com/posts/2026-01-08-crappy-computer-showdown/"/><published>2026-01-09T19:33:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46560217</id><title>Start your meetings at 5 minutes past</title><updated>2026-01-10T14:10:13.699372+00:00</updated><content>&lt;doc fingerprint="b8e35a797d9298d9"&gt;
  &lt;main&gt;
    &lt;p&gt;I work as an Engineering Manager at Google, and my teams practice a simple habit ‚Äì we book all meetings to start at five minutes past the hour (or half hour).&lt;/p&gt;
    &lt;p&gt;This works better than trying to finish five minutes early. Meetings often don‚Äôt finish on time, and the impact is highest with back-to-back meetings. If you try to end at 1:55pm, you will likely talk until 2:00pm anyway, which then runs into the next meeting. But shifting the start time usually guarantees a break. Why? Because there is strong social pressure not to allow meetings to run much past the top of the hour when that is the official end-time. The same social pressure applies to meetings that end at the half-hour.&lt;/p&gt;
    &lt;p&gt;That short break changes the tone of a meeting. It takes a minute or two to move between events, even online. When people arrive at 1:05pm, they are settled and less stressed. You might fear that people will start arriving at 1:07pm, but I have seen the opposite. They respect the new time. They arrive by 1:05pm, ready to work.&lt;/p&gt;
    &lt;p&gt;Do we lose five minutes in every meeting? In theory, yes. But meetings rarely started on the dot anyway before this change.&lt;/p&gt;
    &lt;p&gt;On balance, it is a win. The best proof is that the entire org does it now (the org didn‚Äôt copy my team ‚Äî it just started organically), even though it is not mandatory. Like good code, a good team is built on small, sane details. Giving people five minutes to clear their heads, between back-to-back meetings, is a detail that works.&lt;/p&gt;
    &lt;p&gt;Try it ‚Äî you‚Äôll see it improves your day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://philipotoole.com/start-your-meetings-at-5-minutes-past/"/><published>2026-01-09T22:19:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46560445</id><title>‚ÄúErdos problem #728 was solved more or less autonomously by AI‚Äù</title><updated>2026-01-10T14:10:13.276585+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mathstodon.xyz/@tao/115855840223258103"/><published>2026-01-09T22:39:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46562583</id><title>OLED, Not for Me</title><updated>2026-01-10T14:10:13.012463+00:00</updated><content>&lt;doc fingerprint="b2b21d666e7028f5"&gt;
  &lt;main&gt;
    &lt;p&gt;When I switched from an iMac to a Mac mini in late 2024 I choose an ASUS ProArt 5K PA27JCV (24‚Ä≥, 60 Hz) for the monitor and while it looked great, it died after 14 months, seemingly with a backlight or power supply problem. ASUS‚Äô warranty support requires shipping the monitor back, potentially waiting 3-4 weeks, and then getting a replacement. And worse, the replacement could have dead pixels, as the ASUS warranty doesn‚Äôt consider ‚â§5 dark pixels a problem.&lt;/p&gt;
    &lt;p&gt;The old HP ZR2440w that I swapped in as a spare wasn‚Äôt cutting it, so with an indeterminate wait ahead of me, potentially receiving something with bad pixels, and my being vaguely interested in something larger and with a faster refresh rate I went looking at new monitors.&lt;/p&gt;
    &lt;p&gt;Coming to the realization that 4K is probably fine I picked up a Dell 32 Plus 4K QD-OLED Monitor ‚Äì S3225QC from Costco for $499. It was well reviewed online and looked pretty good when I played with one for about 20 minutes at Micro Center. When I got home and sat in front of it doing my normal things it looked a bit‚Ä¶ different‚Ä¶ almost as if my glasses weren‚Äôt working quite right. But I figured new monitor tech just needed some time for me to get accustomed to. After all, it had a very high contrast ratio and sharp pixels; maybe it‚Äôs just that?&lt;/p&gt;
    &lt;p&gt;After a few days it still didn‚Äôt feel right, so I began looking for a solution. Costco has a 90-day return window for computer monitors, so I had some time, but this didn‚Äôt look good; I wanted an answer soon.&lt;/p&gt;
    &lt;p&gt;I was fortunate to be able to borrow a Dell UltraSharp 32 4K USB-C Hub Monitor U3223QE for the weekend, which was perfect as being a being a high end display with the same resolution and panel size as the S3225QC I could compare them side by side. And in the end the LCD just looked better.&lt;/p&gt;
    &lt;p&gt;I took some macro photos of both displays and it turns out that what was bothering me was fringing, a problem common to OLEDs. It was hard to point out during normal use other than text-is-a-bit-blurry-and-weird , or like an oversharpened image, or almost like artifacted text in a JPEG image, but with photos it was much easier to see what‚Äôs going on. And better, the cause: the arrangement of the subpixels; the little red/blue/green dots that make up a pixel.&lt;/p&gt;
    &lt;p&gt;As shown above, the subpixles in the Dell S3225QC QD-OLED form a square with green on the top, a larger red pixel in the lower left, and smaller blue in the lower right. The Dell U3223QE, a typical LCD, has three vertical stripes making a square. The result being that high contrast edges look very different on an OLED, often with a strong off-color border ‚Äî or fringe ‚Äî along horizontal and vertical lines.&lt;/p&gt;
    &lt;p&gt;In the photos above, note the vertical part of the 1 which has red and green dots along its right side, and large red dots along the top of the 6 with green along the bottom. These are the strongly colored fringes. (On the LCD they appear white as the three equal size subpixels pixels act equally.)&lt;/p&gt;
    &lt;p&gt;This meant that things that I tend to do, text or fine lines in maps or CAD-type drawing, are not right at all on the pixel pattern found in this OLED panel. Beyond the pixel pattern, I also suspect that the much crisper pixels (defined points of light) contribute to the fringing having an artifacting-like effect.&lt;/p&gt;
    &lt;p&gt;This was much more pronounced when looking at light text on a dark background; the way that I read most websites. Visual Studio Code does a wonderful job demonstrating this problem:&lt;/p&gt;
    &lt;p&gt;This gets at why OLEDs make great TVs and gaming monitors. The contrast is outstanding, color is excellent, and high refresh rates are ideal for moving images and fast-response games. And there‚Äôs no noticeable fringing because edges are constantly moving across pixels; almost nothing is still. They also work great on small devices like phones where the pixel density is so high that fringing is too small to see.&lt;/p&gt;
    &lt;p&gt;But on desktop monitors for still things ‚Äî text and fine lines ‚Äî OLEDs currently just aren‚Äôt great; I guess that‚Äôs why office and productivity type monitors are still LCDs. Even though I don‚Äôt like being that person who returns computer stuff just because they don‚Äôt like it, I ended up returning the monitor after only four days of using it. The S3225QC and it‚Äôs QD-OLED just doesn‚Äôt work for me; it made my eyes feel funny to use.&lt;/p&gt;
    &lt;p&gt;Within the past few weeks LG has announced RGB stripe OLED panels which will resolve this problem, but there aren‚Äôt currently any monitors available using these panels, so back to an LCD I‚Äôll go. (It looks like ASUS and MSI will some them available soon, but only as wide-screen gaming monitors. And I suspect the first ones available will be fairly expensive.)&lt;/p&gt;
    &lt;p&gt;Whether this‚Äôll be buying my own U3223QE, perhaps a Dell U3225QE (adds 120 Hz scanning, an ambient light sensor, and a Thunderbolt dock), or just waiting for an ASUS PA27JCV to come back, I‚Äôm not sure‚Ä¶ But whatever I end up using will, for now, will be an LCD, not an OLED.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nuxx.net/blog/2026/01/09/oled-not-for-me/"/><published>2026-01-10T03:52:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46562790</id><title>Oh My Zsh adds bloat</title><updated>2026-01-10T14:10:12.769144+00:00</updated><content>&lt;doc fingerprint="2c529b59839ffb4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You probably don't need Oh My Zsh&lt;/head&gt;
    &lt;p&gt;Oh My Zsh is still getting recommended a lot. The main problem with Oh My Zsh is that it adds a lot of unnecessary bloat that affects shell startup time.&lt;/p&gt;
    &lt;p&gt;Since OMZ is written in shell scripts, every time you open a new terminal tab, it has to interpret all those scripts. Most likely, you don't need OMZ at all.&lt;/p&gt;
    &lt;p&gt;Here are the timings from the default setup with a few plugins (git, zsh-autosuggestions, zsh-autocomplete) that are usually recommended:&lt;/p&gt;
    &lt;code&gt;‚ûú  ~ /usr/bin/time -f "%e seconds" zsh -i -c exit
0.38 seconds
&lt;/code&gt;
    &lt;p&gt;And that's only for prompt and a new shell instance, without actually measuring the git plugin and virtual env plugins (which are often used for Python). Creating a new tab takes some time for your terminal, too. It feels like a whole second to me when opening a new tab in a folder with a git repository.&lt;/p&gt;
    &lt;p&gt;My workflows involve opening and closing up to hundreds of terminal or tmux tabs a day. I do everything from the terminal. Just imagine that opening a new tab in a text editor would take half a second every time.&lt;/p&gt;
    &lt;p&gt;Once in a while, it also checks for updates, which can take up to a few seconds when you open a new tab.&lt;/p&gt;
    &lt;p&gt;I see no reason in frequent updates for my shell configuration. Especially, when a lot of third-party plugins are getting updates too. Why would you want you shell to fetch updates?&lt;/p&gt;
    &lt;p&gt;My advice is to start simple and only add what you really need.&lt;/p&gt;
    &lt;head rend="h3"&gt;Minimal Zsh configuration&lt;/head&gt;
    &lt;p&gt;Here is the minimal Zsh configuration that works well as a starting point:&lt;/p&gt;
    &lt;code&gt;export HISTSIZE=1000000000
export SAVEHIST=$HISTSIZE
setopt EXTENDED_HISTORY
setopt autocd
autoload -U compinit; compinit
&lt;/code&gt;
    &lt;p&gt;It's an already pretty good setup with completions!&lt;/p&gt;
    &lt;p&gt;Some details about this configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HISTSIZE&lt;/code&gt;and&lt;code&gt;SAVEHIST&lt;/code&gt;set the size of your history.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EXTENDED_HISTORY&lt;/code&gt;adds timestamps to your history entries.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;autocd&lt;/code&gt;allows you to change directories without typing&lt;code&gt;cd&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;compinit&lt;/code&gt;initializes the Zsh completion system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Prompt customization&lt;/head&gt;
    &lt;p&gt;You also want to customize your prompt. For prompts, I'm using starship which is a fast and minimal prompt packed into a single binary.&lt;/p&gt;
    &lt;p&gt;The very old way of doing this in Oh My Zsh was to use plugins and custom themes. With starship, it's very simple and easy now. It replaces git, virtual environment and language specific plugins.&lt;/p&gt;
    &lt;p&gt;Here is my config for starship:&lt;/p&gt;
    &lt;code&gt;[aws]
disabled = true

[package]
disabled = true

[gcloud]
disabled = true

[azure]
disabled = true


[nodejs]
disabled = true

[character]
success_symbol = '[‚ûú](bold green)'

[cmd_duration]
min_time = 500
format = 'underwent [$duration](bold yellow)'

[directory]
truncation_length = 255
truncate_to_repo = false
use_logical_path = false
&lt;/code&gt;
    &lt;p&gt;Because cloud services are available globally, I've disabled them. I don't want them to be displayed on every prompt, since this adds visual noise.&lt;/p&gt;
    &lt;p&gt;Here is how my prompt looks like now:&lt;/p&gt;
    &lt;p&gt;This project uses both Python and Rust, they are highlighted in the prompt. When you run a command, it also shows how long it took to execute.&lt;/p&gt;
    &lt;p&gt;To enable it, add the following line to your &lt;code&gt;.zshrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;eval "$(starship init zsh)"
&lt;/code&gt;
    &lt;head rend="h3"&gt;History search&lt;/head&gt;
    &lt;p&gt;A lot of people use &lt;code&gt;zsh-autosuggestions&lt;/code&gt; plugin for history search.
I find it distracting, because it shows all suggestions as you type.&lt;/p&gt;
    &lt;p&gt;Instead, I prefer using fzf binded to &lt;code&gt;Ctrl+R&lt;/code&gt; for searching history.
It gives an interactive fuzzy search.&lt;/p&gt;
    &lt;p&gt;To enable it, add the following lines to your &lt;code&gt;.zshrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;source &amp;lt;(fzf --zsh)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Final startup time&lt;/head&gt;
    &lt;p&gt;After these changes, the startup should look as follows:&lt;/p&gt;
    &lt;code&gt;‚ùØ /usr/bin/time -f "%e seconds" zsh -i -c exit
0.07 seconds
&lt;/code&gt;
    &lt;head rend="h3"&gt;Miscellaneous tips&lt;/head&gt;
    &lt;p&gt;For Vim users, I also suggest enabling Vim mode in Zsh. It makes editing commands much faster.&lt;/p&gt;
    &lt;code&gt;set -o vi
# Fix for backspace in vi mode
bindkey -v '^?' backward-delete-char
&lt;/code&gt;
    &lt;p&gt;It works the same way as in Vim. By default, &lt;code&gt;zle&lt;/code&gt; (the library that reads the shell input) uses Emacs keybindings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;After switching from OMZ a year ago, it only took me a few days to get used to the new workflow. If you still missing some of the plugins, you can always load them manually.&lt;/p&gt;
    &lt;p&gt;Update:&lt;/p&gt;
    &lt;p&gt;Some people wonder why I open so many tabs. I use tmux and a terminal-based editor (&lt;code&gt;helix&lt;/code&gt;).
In tmux, I have popups for &lt;code&gt;lazygit&lt;/code&gt; and &lt;code&gt;yazi&lt;/code&gt; file manager.
Every time I need to check git history or browse files, I just open them.
They open on top of the current session as an overlay. You can view them as windows in IDEs.&lt;/p&gt;
    &lt;p&gt;I also use temporary splits to quickly run the code/tests and see the output. They count as separate shell sessions. I want to see code and output side by side, but I don't need it all the time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Comments&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; mtizim 2026-01-09 #&lt;p&gt;You probably don't need to switch away from Oh My Zsh:&lt;/p&gt;&lt;p&gt;‚ûú ~ time zsh -i -c exit zsh -i -c exit 0.02s user 0.03s system 114% cpu 0.044 total&lt;/p&gt;&lt;p&gt;‚ûú ~ omz plugin list --enabled Custom plugins: zsh-autosuggestions zsh-fzf-history-search&lt;/p&gt;&lt;p&gt;Built-in plugins: git&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Artem 2026-01-09 #&lt;p&gt;zsh-autocomplete is usually the plugins that slows downs popular setups.&lt;/p&gt;&lt;p&gt;Also, please keep in mind that this benchmark does not measure slowlines of git plugins.&lt;/p&gt;&lt;p&gt;Here is the output from&lt;/p&gt;&lt;code&gt;zsh-bench&lt;/code&gt;for OMZ:&lt;p&gt;==&amp;gt; benchmarking login shell of user main ...&lt;/p&gt;&lt;p&gt;creates_tty=0&lt;/p&gt;&lt;p&gt;has_compsys=1&lt;/p&gt;&lt;p&gt;has_syntax_highlighting=0&lt;/p&gt;&lt;p&gt;has_autosuggestions=1&lt;/p&gt;&lt;p&gt;has_git_prompt=0&lt;/p&gt;&lt;p&gt;first_prompt_lag_ms=603.751&lt;/p&gt;&lt;p&gt;first_command_lag_ms=615.419&lt;/p&gt;&lt;p&gt;command_lag_ms=3.517&lt;/p&gt;&lt;p&gt;input_lag_ms=3.093&lt;/p&gt;&lt;p&gt;exit_time_ms=53.762&lt;/p&gt;&lt;p&gt;My Zsh setup:&lt;/p&gt;&lt;p&gt;==&amp;gt; benchmarking login shell of user main ...&lt;/p&gt;&lt;p&gt;creates_tty=0&lt;/p&gt;&lt;p&gt;has_compsys=1&lt;/p&gt;&lt;p&gt;has_syntax_highlighting=0&lt;/p&gt;&lt;p&gt;has_autosuggestions=0&lt;/p&gt;&lt;p&gt;has_git_prompt=1&lt;/p&gt;&lt;p&gt;first_prompt_lag_ms=103.337&lt;/p&gt;&lt;p&gt;first_command_lag_ms=103.506&lt;/p&gt;&lt;p&gt;command_lag_ms=53.602&lt;/p&gt;&lt;p&gt;input_lag_ms=0.118&lt;/p&gt;&lt;p&gt;exit_time_ms=48.795&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Magic Wand 2026-01-10 #&lt;p&gt;Thanks for the write, so you still need OMZ feature? but now its replaced by starship?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Artem 2026-01-10 #&lt;p&gt;Starship is just a single replacement for prompt tweaks, before startship I had to use OMZ plugins and custom themes. So yes, it some sense I've replicated some of my old OMZ features with starship.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; martianlantern 2026-01-10 #&lt;p&gt;I have given up on any external bash configurator a long time ago, instead I write my own bash prompts these days, they lack functionality but I am much happy with them for now: https://martianlantern.github.io/2025/11/updating-my-bash-prompt/&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Jack Pearce 2026-01-10 #&lt;p&gt;thanks for this! OMZ startup time has been bugging me for a long time but I couldn't really be bothered to look for an alternative.&lt;/p&gt;&lt;p&gt;starship seems great, happy with that. Atuin is also excellent for history search.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rushter.com/blog/zsh-shell/"/><published>2026-01-10T04:35:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46564618</id><title>I got paid minimum wage to solve an impossible problem</title><updated>2026-01-10T14:10:12.667830+00:00</updated><content/><link href="https://tiespetersen.substack.com/p/i-got-paid-minimum-wage-to-solve"/><published>2026-01-10T10:53:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46564696</id><title>Allow me to introduce, the Citroen C15</title><updated>2026-01-10T14:10:11.598935+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eupolicy.social/@jmaris/115860595238097654"/><published>2026-01-10T11:12:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46564762</id><title>New information extracted from Snowden PDFs through metadata version analysis</title><updated>2026-01-10T14:10:07.685367+00:00</updated><content>&lt;doc fingerprint="22d661474f85de18"&gt;
  &lt;main&gt;&lt;p&gt;Previous parts:&lt;/p&gt;&lt;p&gt;We discovered that entire sections describing domestic U.S. intelligence facilities were deliberately removed from two published documents, while equivalent foreign facilities remained visible. The evidence exists in an unexpected place - the PDF metadata of documents published by The Intercept in 2016, and by The Intercept and the Australian Broadcasting Corporation in a 2017 collaborative investigation. To our knowledge, this is the first time this information has been revealed publicly. The removed sections reveal the operational designations and cover name structure for domestic U.S. NRO Mission Ground Stations.&lt;/p&gt;&lt;p&gt;Using PDF analysis tools, we found hidden text embedded in the metadata versioning of two documents published alongside investigative stories about NSA satellite surveillance facilities. These metadata artifacts prove that earlier versions of the documents contained detailed descriptions of domestic U.S. ground stations that were systematically scrubbed before publication (not just redacted with black boxes, but with text completely removed).&lt;/p&gt;What was published from the Snowden documents:&lt;list rend="ul"&gt;&lt;item&gt;Operational details for RAF Menwith Hill Station (UK)&lt;/item&gt;&lt;item&gt;Operational details for Pine Gap (Australia)&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;Potomac Mission Ground Station (PMGS) - Washington, DC. Public cover name: "Classic Wizard Reporting and Testing Center" (CWRTC).&lt;/item&gt;&lt;item&gt;Consolidated Denver Mission Ground Station (CDMGS) - Denver area. Public cover name: "Aerospace Data Facility" (ADF).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The facilities themselves are not unknown. "Aerospace Data Facility" at Buckley Space Force Base is publicly acknowledged as a National Reconnaissance Office (NRO) Mission Ground Station. "Classic Wizard Reporting and Testing Center" at Naval Research Laboratory is publicly acknowledged, though its designation as a Mission Ground Station is less clear. What's NOT public (until now) is the specific operational designations used in classified networks: "Consolidated Denver Mission Ground Station (CDMGS)" and "Potomac Mission Ground Station (PMGS)." The Snowden documents prove these are deliberate cover names (not just alternative terminology) and show exactly what's classified and what's not.&lt;/p&gt;&lt;head rend="h2"&gt;Hidden PDF versions&lt;/head&gt;&lt;p&gt;The first PDF document titled "Menwith satellite classification guide" has two versions in the file metadata: an older one and a newer one. The removed information exists in the earlier version, and is completely removed in the second, published version. This is not standard redaction with black boxes - the text was completely deleted from the visible document while remaining embedded in the PDF's internal version history.&lt;/p&gt;&lt;p&gt;Screenshot from the first version of the document, containing the hidden text (sections 5.1.5.2 - 5.1.5.6).&lt;/p&gt;&lt;p&gt;Screenshot from the second version of the document, where the text is removed.&lt;/p&gt;&lt;p&gt;The removed text:&lt;/p&gt;&lt;quote&gt;5.1.5.2 (U) Facility Name: Formally identified as the ,Mission Support Facility(MSF) also re- ferred as the Classic Wizard Reporting and Testing Center (CWRTC). 5.1.5.3 (S//TK) Cover Story: The fact of a cover story is S//TK, the cover story itself is unclas- sified. 5.1.5.4 (U) Software development, maintenance, testing, and communications support to a world-wide Navy communications and reporting system. 5.1.5.5 (U) Associations: 1. The term Potomac Mission Ground Station (PMGS)=S//TK 2. The term Classic Wizard Reporting and Testing Center (CWRTC)=UNCLASSIFIED 3. The term Naval Research Laboratory=UNCLASSIFIED 4. The fact that CWRTC is the cover name for the PMGS=S//TK 5. The fact that CWRTC is a communications and data relay location for the US=UNCLASSIFIED (no association w/NRO) 6. The fact that PMGS is located on the NRL=S//TK 7. The fact that the NRO has a MGS located on the NRL=S//TK 8. The fact that the CWRTC is located on the NRL=UNCLASSIFIED (no association w/NRO) 9. CWRTC associated w/NRO=S//TK 10. Association of NRO, CIA, or NSA personnel with the CWRTC=S//TK 11. Association of CWRTC with other NRO MGS=S//TK 12. Association of MSF with the NRO=S//TK. 13. Association of CWRTC with the ADF=UNCLASSIFIED (no association w/NRO) 5.1.5.6 (U) Visitors: CWRTC is housed within buildings 259 and 260 on the Naval Research Laboratory in Southwest Washington, DC.&lt;/quote&gt;&lt;p&gt;The second document "NRO SIGINT Guide for Pine Gap" also has two versions in the file metadata: an older one and a newer one.&lt;/p&gt;&lt;p&gt;Screenshot from the first version of the document, containing the hidden text (section 5.1.2).&lt;/p&gt;&lt;p&gt;Screenshot from the second version of the document, where the text is removed.&lt;/p&gt;&lt;p&gt;The removed text:&lt;/p&gt;&lt;quote&gt;5.1.2 (S//TK) Consolidated Denver Mission Ground Station (CDMGS) 5.1.2.1 (U) Facility Name: Aerospace Data Facility (ADF) 5.1.2.2 (S//TK) Cover Story: The fact of a cover story is S/TK, the cover story itself is unclassi- fied.&lt;/quote&gt;&lt;head rend="h2"&gt;Potomac Mission Ground Station (PMGS)&lt;/head&gt;&lt;p&gt;In the "hidden version" of the document "Menwith satellite classification guide" section 5.1.5 describes a facility formally identified by its cover name "Mission Support Facility (MSF)," also referred to as the "Classic Wizard Reporting and Testing Center (CWRTC)." The classified operational designation is Potomac Mission Ground Station (PMGS).&lt;/p&gt;&lt;p&gt;Location: Buildings 259 and 260, Naval Research Laboratory, Southwest Washington, DC.&lt;/p&gt;&lt;p&gt;Public cover story: "Software development, maintenance, testing, and communications support to a world-wide Navy communications and reporting system."&lt;/p&gt;&lt;p&gt;Actual function: Mission Ground Station in the National Reconnaissance Office's satellite intelligence network.&lt;/p&gt;&lt;p&gt;The document explicitly states that "the fact of a cover story is S//TK" - meaning even acknowledging that CWRTC is a cover name (rather than the facility's real identity) is classified Secret/Talent Keyhole.&lt;/p&gt;&lt;p&gt;The classification guide provides a detailed breakdown of what's public and what's secret:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;"Classic Wizard Reporting and Testing Center (CWRTC)" = UNCLASSIFIED&lt;/item&gt;&lt;item&gt;"Potomac Mission Ground Station (PMGS)" = S//TK (SECRET)&lt;/item&gt;&lt;item&gt;The fact that CWRTC is a cover name for PMGS = S//TK (SECRET)&lt;/item&gt;&lt;item&gt;Association of NRO, CIA, or NSA personnel with CWRTC = S//TK (SECRET)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This layered classification allows the facility to operate under a public identity while keeping its actual intelligence designation and function secret.&lt;/p&gt;&lt;head rend="h2"&gt;Consolidated Denver Mission Ground Station (CDMGS)&lt;/head&gt;&lt;p&gt;In the "hidden version" of the document "NRO SIGINT Guide for Pine Gap" section 5.1.2 describes a facility identified by the cover name "Aerospace Data Facility (ADF)". The classified operational designation is "Consolidated Denver Mission Ground Station (CDMGS)."&lt;/p&gt;&lt;p&gt;Location: Buckley Space Force Base, Aurora, Colorado&lt;/p&gt;&lt;p&gt;Public acknowledgment: Publicly known as "Aerospace Data Facility-Colorado." For example ADF-C is openly described in Wikipedia as an NRO facility for "command and control of reconnaissance satellites."&lt;/p&gt;&lt;p&gt;What's NOT public: The operational designation "Consolidated Denver Mission Ground Station" and the fact that "Aerospace Data Facility" is a cover name for it.&lt;/p&gt;&lt;p&gt;As with PMGS, the classification guide shows "the fact of a cover story is S//TK" - the deception itself is classified.&lt;/p&gt;&lt;p&gt;Both hidden facilities appear in an organizational classification table in the Pine Gap guide. The table's visual structure reveals which names refer to the same facilities through deliberate formatting. Thicker borders separate distinct facility groups, while thinner borders separate columns within each group (this might be a bit hard to see from the screenshot/document). Each facility appears multiple times - once under its classified operational designation, and again under its unclassified cover name(s):&lt;/p&gt;&lt;p&gt;The groupings show:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Denver facility: CDMGS (real designation) + ADF (cover name) + FSD (Field Station Denver, cover name)&lt;/item&gt;&lt;item&gt;Potomac facility: PMGS (real designation) + MSF (cover name) + CWRTC (cover name)&lt;/item&gt;&lt;item&gt;Menwith Hill: HMGS (real designation) + RAF MHS (cover name)&lt;/item&gt;&lt;item&gt;Pine Gap: AMGS (real designation) + JDFPG (cover name)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;For each facility, the real operational designation (CDMGS, PMGS, HMGS, AMGS) has all agency associations classified S//TK (Secret). But the cover names have NSA and NGA associations marked as unclassified (U) or For Official Use Only (U//FOUO).&lt;/p&gt;&lt;p&gt;This creates a classification system where you can publicly acknowledge intelligence work using sanitized facility names, while the actual operational designations remain secret.&lt;/p&gt;&lt;p&gt;The domestic facilities use two cover names each (ADF + FSD for Denver; MSF + CWRTC for Potomac), while foreign facilities use only one. This suggests layered operational security for facilities on U.S. soil - possibly to navigate Congressional oversight, legal restrictions on domestic intelligence operations, or different audiences requiring different levels of plausible deniability.&lt;/p&gt;&lt;p&gt;There is a deliberate pattern in these two classification guides: detailed sections describing specific U.S. facilities were removed, while equivalent foreign facility sections were published intact. The 2016 Menwith Hill guide had its PMGS section (5.1.5) completely removed. The 2017 Pine Gap guide had its CDMGS section (5.1.2) completely removed. Both guides retained their detailed descriptions of foreign facilities, including operational designations, cover stories, and visitor protocols.&lt;/p&gt;&lt;p&gt;U.S. facilities weren't entirely absent from the published documents. The Pine Gap classification table shows CDMGS, PMGS, ADF, and other U.S. facility designations alongside foreign facilities, revealing the structure of the Mission Ground Station network. Other published documents from both investigations mention U.S. facilities. What was specifically removed were the detailed classification guide sections that would have explained these U.S. facilities the same way Menwith Hill and Pine Gap were explained.&lt;/p&gt;&lt;head rend="h2"&gt;Who edited the documents?&lt;/head&gt;&lt;p&gt;PDF metadata provides forensic evidence of the editing process. The Pine Gap classification guide shows timestamps from July 31, 2017, three weeks before publication. Two versions were created minutes apart using Nitro Pro 8, a commercial PDF editor: version 1 at 13:48:54 (containing the CDMGS section) and version 2 at 13:50:48 (with CDMGS removed). The Intercept and ABC published identical PDFs with the same metadata artifacts, indicating the editing was done once and the same file shared between organizations.&lt;/p&gt;&lt;p&gt;The Intercept, as holder of the Snowden archive, likely handled technical document preparation for publications. The Menwith Hill classification guide, published solely by The Intercept in 2016, shows more thorough metadata sanitization but the same editorial pattern - domestic facility sections removed while foreign equivalents remain.&lt;/p&gt;&lt;p&gt;We contacted Ryan Gallagher, the journalist who led both investigations, to ask about the editorial decision to remove these sections. After more than a week, we have not received a response.&lt;/p&gt;&lt;p&gt;The next part will be a technical deep-dive into PDF metadata across the published Snowden documents. We found that many documents contain multiple versions in their metadata, revealing the editorial redaction process: visible NSA agents' usernames that were later removed, unblurred screenshots that were later redacted, and surveillance data that went through multiple rounds of redaction. We'll also document cases of failed redactions - including one where redacted text remained fully copyable, previously reported only by a Polish cybersecurity blog.&lt;/p&gt;&lt;head rend="h2"&gt;Notes&lt;/head&gt;&lt;p&gt;You can extract versions from a PDF file for example with a pdfresurrect tool (pdfresurrect -w filename.pdf).&lt;/p&gt;&lt;p&gt;You can download the document versions directly here:&lt;/p&gt;Menwith satellite classification guide versions: NRO SIGINT Guide for Pine Gap versions:&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://libroot.org/posts/going-through-snowden-documents-part-4/"/><published>2026-01-10T11:23:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46565132</id><title>A Eulogy for Dark Sky, a Data Visualization Masterpiece (2023)</title><updated>2026-01-10T14:10:07.468974+00:00</updated><content>&lt;doc fingerprint="bf2e3a527d562d9d"&gt;
  &lt;main&gt;
    &lt;p&gt;On January 1, 2023, Apple sunsetted (pun intended) the Dark Sky mobile app on iOS. Apple purchased the company behind the popular weather application in early 2020, then announced that it would be shutting down the Dark Sky applications (first on Android, then on iOS and web), and finally stated in 2022 that the forecast technology would be integrated into the Apple Weather app with iOS 16.&lt;/p&gt;
    &lt;p&gt;But Dark Sky was much more than just an API or a set of ‚Äúforecast technologies.‚Äù The design of the Dark Sky mobile application represented a hallmark of information design because the team clearly obsessed over how people would actually use the app on a daily basis.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a gallery of screenshots I personally took in the last year.&lt;/p&gt;
    &lt;p&gt;The design of Dark Sky was so wonderful that I could understand the shape of the weather at a glance, even from a zoomed out view of the app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common use cases for a weather app&lt;/head&gt;
    &lt;p&gt;Before we can dissect what makes the design of this app so special, let‚Äôs define the most common use cases for a weather app. A good weather app attempts to address a range of use cases in a person‚Äôs life. Here are some common use cases for a weather app, starting with the contextual goals first:&lt;/p&gt;
    &lt;p&gt;You‚Äôll notice that the questions I want to answer vary based on the context and situation I‚Äôm in. This is the perfect situation for software that embraces good information design principles.&lt;/p&gt;
    &lt;p&gt;In Magic Ink, Bret Victor defined information design ‚Äúas the design of context-sensitive information graphics.‚Äù Unlike static graphics, like a weather map in a newspaper, information graphics in software can be highly dynamic and can incorporate context from the user‚Äôs environment.&lt;/p&gt;
    &lt;p&gt;Dark Sky aggressively leaned into these ideas, and the team worked hard to turn nearly everything in the application into a context-sensitive information graphic. Let‚Äôs dive deeper into some examples where this notion is in full display.&lt;/p&gt;
    &lt;head rend="h2"&gt;Weather forecast for the day at a precise location&lt;/head&gt;
    &lt;p&gt;I‚Äôll start by listing my goals when trying to understand the upcoming weather for the day:&lt;/p&gt;
    &lt;p&gt;The default experience for the Dark Sky application is to show weather for the next 12 hours at my precise location. Here are three screenshots from the app, from different days and locations. What do you notice?&lt;/p&gt;
    &lt;p&gt;The app contextualizes the information that‚Äôs presented based on what‚Äôs most relevant.&lt;/p&gt;
    &lt;p&gt;In the left-most screenshot, the context of the weather storm that‚Äôs passing by my location and some potential light rain is emphasized to the user. The wind advisory and dip in the ‚Äúfeels like‚Äù temperature also make honorable mentions. In combination with the clickable Wind Advisory warning, I can quickly understand that the storm has passed my specific location but I should still be mindful of the wind.&lt;/p&gt;
    &lt;p&gt;In the middle screenshot, the storm front heading my direction, the imminent rain starting within the hour, and the hours of rain later in the day are emphasized front and center. Based on my interest in the rain, I scrolled down (not shown in screenshot) to switch from temperature to rain probability for the main weather view.&lt;/p&gt;
    &lt;p&gt;In the right-most screenshot, the temperature distribution throughout the day is emphasized.&lt;/p&gt;
    &lt;p&gt;There are lots of little details that are easy to miss, as well. For example, all three screenshots above start showing weather information starting at the current moment (‚Äúnow‚Äù) and onwards. This is brilliant, because I only occasionally care about weather from a few hours or days ago. (To accommodate those situations, the app did have a unique Time Machine view to explore past weather data.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Weather forecast for the week&lt;/head&gt;
    &lt;p&gt;Let‚Äôs revisit our goals when trying to understand the weather for the week:&lt;/p&gt;
    &lt;p&gt;While many other weather apps focus on helping me understand the weather at the city level, the Dark Sky app differentiates itself by enabling me to understand the weather with a hyperlocal view. I can view the weather at specific addresses and landmarks and understand how the weather might be different in a city center compared to the towns around it.&lt;/p&gt;
    &lt;p&gt;By default, the Dark Sky app shows the weekly weather summary at my current location (screenshot on the left). I can contextualize the weekly weather view by clicking the search icon and typing in the alternative location I‚Äôm interested in (screenshot on the right).&lt;/p&gt;
    &lt;p&gt;At a glance, I can quickly understand:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which days are likely to rain.&lt;/item&gt;
      &lt;item&gt;Which days have wide temperature ranges (low lows and high highs).&lt;/item&gt;
      &lt;item&gt;The general shape of the weather (micro-trends).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Gallery of subtle design elements&lt;/head&gt;
    &lt;p&gt;In the first few sections, I laid the groundwork by diving a bit more deeply into specific aspects of Dark Sky. Now that I‚Äôve hopefully engaged your critical design eye, I‚Äôd like to elevate other elements in the app‚Äôs design in a more rapid-fire fashion. I hope that this section will provide some ideas and inspiration for people designing their own data-rich applications.&lt;/p&gt;
    &lt;p&gt;Preserving temperature magnitudes in ranges&lt;/p&gt;
    &lt;p&gt;To represent data visually, the data needs to be mapped from ‚Äúdata space‚Äù to ‚Äúpixel space‚Äù. In some domains and scenarios, you want to ensure the magnitudes are carried over, and in others you want to rescale the values to a fixed scale so the resulting charts are more consistent in the space and pixels they take up.&lt;/p&gt;
    &lt;p&gt;In the Dark Sky app, the ‚Äútemperature pills‚Äù representing the forecasted temperatures for the upcoming week preserve their existing magnitude more effectively in the visualization. The temperature values are more tightly integrated with the visual representation, making the combined experience more amenable to quick comparison across multiple days.&lt;/p&gt;
    &lt;p&gt;Many weather apps opt for the design pattern on the right, where all temperature ranges are rescaled to take up the same amount of space in the app.&lt;/p&gt;
    &lt;p&gt;Replacing numbers with rough categories&lt;/p&gt;
    &lt;p&gt;Interpreting hourly distributions of rainfall or snowfall can be difficult. Is 0.25 inches of rain a lot over the next hour? What about 0.2 inches of snow per hour for the next hour, then 0.15 for the hour after that? How do I prepare for these circumstances?&lt;/p&gt;
    &lt;p&gt;To help users stay informed and take action, the app will often replace precise forecast distributions of rainfall and snowfall with rough categories instead. This design choice has two positive effects on users:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It contextualizes the forecast to simpler categories that can help us quickly make changes in our life if needed. &lt;list rend="ul"&gt;&lt;item&gt;Light snow that starts and stops means that my snow shoveling company won‚Äôt come out, but heavy snow means they likely will.&lt;/item&gt;&lt;item&gt;Heavy snow means I should probably make sure my outdoor furniture is better covered and ready to take the snow.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;It removes a sense of artificial precision that doesn‚Äôt really exist because weather forecasts fundamentally have very high uncertainty and error bands.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contextualized storm map&lt;/p&gt;
    &lt;p&gt;Simple color scales combined with arrows go a very long way to conveying relevant storm information.&lt;/p&gt;
    &lt;p&gt;Wind direction&lt;/p&gt;
    &lt;p&gt;Instead of conveying wind direction using text (‚ÄúNW‚Äù or ‚ÄúNorthwest‚Äù), the app uses arrows! If the wind shifts directions throughout the day, I can feel the wind direction changing using my body.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complaints from former Dark Sky users&lt;/head&gt;
    &lt;p&gt;As you can tell, I could probably write an entire book about the design of Dark Sky.&lt;/p&gt;
    &lt;p&gt;Instead, I want to share some passionate comments from other Dark Sky fans. While most of them aren‚Äôt data viz or design nerds, they feel that the Apple Weather app is not a sufficient replacement. These people relied on Dark Sky to make decisions and grew very attached to an application that integrated deeply into their lives.&lt;/p&gt;
    &lt;p&gt;Here are some examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;0000GKP on Reddit: ‚ÄúI have already made the transition to the Weather app. The information is presented in a much less efficient manner than Dark Sky so it takes more effort to get it, but it is all there (except for cloud cover).‚Äù&lt;/item&gt;
      &lt;item&gt;TheGeckoDude on Reddit: ‚ÄúIs there anything that has the precipitation graph similar to dark sky? The biggest thing I will miss from dark sky is that graph. I‚Äôm outside a lot and it is super helpful for knowing when to take cover or when it might rain. I need to find a feature like that because the vague weather precipitation info will not cut it for me.‚Äù&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Let‚Äôs make data shine!&lt;/head&gt;
    &lt;p&gt;Dark Sky started with publicly available data, augmented it with contextualized predictions, rigorously iterated on data visualization design, and packaged all of this into a contextualized experience to make weather data useful for me in my daily life.&lt;/p&gt;
    &lt;p&gt;While the availability of data has never been higher, we‚Äôre still missing software experiences that contextualize that data to make our lives better. Data alone isn‚Äôt enough.&lt;/p&gt;
    &lt;p&gt;The world needs more Dark Sky-like experiences to help us improve our spending habits, help us sleep better, and more. If you‚Äôre working on information software, I hope you can be inspired by the body of design and engineering work that Dark Sky pioneered.&lt;/p&gt;
    &lt;head rend="h5"&gt;Srini Kadamati&lt;/head&gt;
    &lt;p&gt;Srini Kadamati currently helps build better data management tools for biomedical research at Manifold.ai. Previously, he taught data visualization and data skills at Dataquest.io and Preset.io.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nightingaledvs.com/dark-sky-weather-data-viz/"/><published>2026-01-10T12:23:20+00:00</published></entry></feed>