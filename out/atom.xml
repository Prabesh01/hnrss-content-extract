<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-25T19:32:20.149164+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45672494</id><title>Jacqueline – A minimal i386 kernel written in Pascal</title><updated>2025-10-25T19:32:26.530597+00:00</updated><content>&lt;doc fingerprint="86c7eb1828d209ac"&gt;
  &lt;main&gt;
    &lt;p&gt;Jacqueline is an experimental bootloader written in Pascal (Free Pascal dialect) written for the i386 architecture, just because. Note that, unlike NativeOS, I have no plans to further develop Jacqueline once the system compiles and I'm able to start the image using an emulator.&lt;/p&gt;
    &lt;p&gt;Even while Pascal wasn't written with low-level programming in mind, it is nonetheless possible to do low-level systems programming with the Free Pascal dialect, as it supports features that are present in other low-level languages such as C, C++ or even Rust in unsafe mode, such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pointers, using the &lt;code&gt;^&lt;/code&gt;operator (such as&lt;code&gt;var intptr: ^integer&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Memory addresses, using the &lt;code&gt;@&lt;/code&gt;operator (let&lt;code&gt;var foo: integer&lt;/code&gt;, then&lt;code&gt;@foo&lt;/code&gt;yields the memory address of&lt;code&gt;foo&lt;/code&gt;as an&lt;code&gt;^integer&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Inline assembly interfacing, through the &lt;code&gt;asm&lt;/code&gt;keyword, à la C. In fact, I find more intuitive to move data between registers and variables in Free Pascal than in GNU C.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Free Pascal is able to generate standard object files (*.o), encoded in popular executable file formats such as PE and ELF, and thus, it is possible to make object files written originally in diverse languages such as C, Pascal or Assembly to interface each other.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An i386-elf toolchain (required for compiling the Assembly code and linking the final kernel image).&lt;/item&gt;
      &lt;item&gt;A Free Pascal distribution with 32 bit support (i.e., ppc386 is provided).&lt;/item&gt;
      &lt;item&gt;BSD make or GNU make.&lt;/item&gt;
      &lt;item&gt;QEMU to run the kernel.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;make qemu&lt;/code&gt;
    &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/danirod/jacqueline"/><published>2025-10-22T17:37:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45674002</id><title>Public Montessori programs strengthen learning outcomes at lower costs: study</title><updated>2025-10-25T19:32:26.224018+00:00</updated><content>&lt;doc fingerprint="ba34a29a1e098ba9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;National study finds public Montessori programs strengthen early learning outcomes—at sharply lower costs&lt;/head&gt;
    &lt;head rend="h5"&gt;Sadie Harley&lt;/head&gt;
    &lt;p&gt;scientific editor&lt;/p&gt;
    &lt;head rend="h5"&gt;Robert Egan&lt;/head&gt;
    &lt;p&gt;associate editor&lt;/p&gt;
    &lt;p&gt;The first national randomized trial of public Montessori preschool students showed stronger long-term outcomes by kindergarten, including elevated reading, memory, and executive function as compared to non-Montessori preschoolers.&lt;/p&gt;
    &lt;p&gt;The study of 588 children across two dozen programs nationwide shows an imperative to follow and study these outcomes through graduation and beyond.&lt;/p&gt;
    &lt;p&gt;A new national study led by researchers from the University of Virginia, University of Pennsylvania and the American Institutes for Research found that Montessori preschool programs (ages 3 to 6) in public schools deliver stronger early learning outcomes for children—and at a sharply lower cost to school districts and taxpayers.&lt;/p&gt;
    &lt;p&gt;The first randomized controlled trial of its kind, published in Proceedings of the National Academy of Sciences, tracked nearly 600 children across 24 public Montessori programs nationwide.&lt;/p&gt;
    &lt;p&gt;By the end of kindergarten, children who won a random lottery to attend public Montessori preschools outperformed their peers in reading, executive function, short-term memory, and social understanding—all while costing approximately $13,000 less per child than traditional preschool programs.&lt;/p&gt;
    &lt;p&gt;Those costs do not include anticipated savings from improved teacher morale and retention, a dynamic demonstrated in other data.&lt;/p&gt;
    &lt;p&gt;The findings, which have been vetted by third parties, contrast sharply with the prior common findings, where impacts of preschool were observed immediately following the program but then seemed to disappear by the end of kindergarten.&lt;/p&gt;
    &lt;p&gt;"These findings affirm what Maria Montessori believed over a century ago—that when we trust children to learn with purpose and curiosity, they thrive," said Angeline Lillard, Commonwealth Professor of Psychology at the University of Virginia. "Public Montessori programs are not only effective but cost-efficient."&lt;/p&gt;
    &lt;p&gt;"Montessori preschool programs are already being used in hundreds of U.S. public schools, and our research shows that they are having a positive impact in key areas of early learning," said Karen Manship, co-author and Managing Director at the American Institutes for Research.&lt;/p&gt;
    &lt;p&gt;"These findings provide valuable evidence to policymakers and educational leaders who are seeking to deliver better outcomes with increasingly limited resources."&lt;/p&gt;
    &lt;p&gt;"Montessori began in the low-income housing of early 20th century Rome," noted David Loeb of the University of Pennsylvania. "This research shows it still delivers on that promise for America's children today."&lt;/p&gt;
    &lt;head rend="h2"&gt;Key findings:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stronger early learning: Montessori children scored significantly higher in reading, memory, executive function, and understanding others' perspectives by the end of kindergarten.&lt;/item&gt;
      &lt;item&gt;Sustained benefits: Unlike many preschool programs where gains fade, Montessori students' relative outcomes improved over time.&lt;/item&gt;
      &lt;item&gt;Cost savings: When compared to traditional public preschool, Public Montessori programs cost $13,000 less per child across the three years from ages 3–6, due primarily to more efficient class structures, including harnessing the benefits of children teaching each other across age groups.&lt;/item&gt;
      &lt;item&gt;Teacher morale and retention: In practice, those cost savings are likely even higher due to prior prevailing evidence that Montessori teachers experience higher job satisfaction and lower turnover.&lt;/item&gt;
      &lt;item&gt;Benefits for all children: Effects were strongest among children from lower-income families, although children of all backgrounds benefited. These and other findings are a helpful reminder that Montessori was originally designed to reach low-income communities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dr. Maria Montessori opened her first classroom in 1907 in the working-class tenements of Rome, and pioneered an educational model rooted in children's natural drive to learn. Today, more than 600 U.S. public schools offer Montessori education.&lt;/p&gt;
    &lt;p&gt;This national study affirms that Montessori's century-old model is a highly effective approach to early education—delivering enduring benefits for children and communities alike.&lt;/p&gt;
    &lt;p&gt;The research also appears highly actionable for policymakers, because the results found the Montessori programs delivered better outcomes at sharply lower costs, and studies have demonstrated improved teacher morale and retention for Montessori programs.&lt;/p&gt;
    &lt;p&gt;The paper's co-authors include researchers from the American Institutes for Research (Juliette Berg, Maya Escueta, Alison Hauser) and UVA graduate student Emily Daggett.&lt;/p&gt;
    &lt;p&gt;More information: Lillard, Angeline S., A national randomized controlled trial of the impact of public Montessori preschool at the end of kindergarten, Proceedings of the National Academy of Sciences (2025). DOI: 10.1073/pnas.2506130122&lt;/p&gt;
    &lt;p&gt;Journal information: Proceedings of the National Academy of Sciences&lt;/p&gt;
    &lt;p&gt;Provided by University of Virginia&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://phys.org/news/2025-10-national-montessori-early-outcomes-sharply.html"/><published>2025-10-22T19:29:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45682164</id><title>Context engineering is sleeping on the humble hyperlink</title><updated>2025-10-25T19:32:25.953137+00:00</updated><content>&lt;doc fingerprint="a74f911328bb1b36"&gt;
  &lt;main&gt;
    &lt;p&gt;As we all learn more about Context Engineering for LLMs (see Anthropic’s post for an excellent primer), we’ve identified a few important limitations. Conversations should be append-only to maximize cacheability. Models are typically more responsive to “fresh” context close to the end of the window. Models typically perform worse when overwhelmed with large amounts of context.&lt;/p&gt;
    &lt;p&gt;With this in mind, a key tension comes into focus: the model needs access to all valuable context, BUT ONLY when that context is relevant to the task at hand.&lt;/p&gt;
    &lt;p&gt;Context engineering is effectively the practice of finding ways to manage this tension. Popular solutions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Retrieval Augmented Generation (RAG), which attempts to dynamically discover and load specific relevant context for the current query proactively.&lt;/item&gt;
      &lt;item&gt;Subagents, which encapsulate specialized instructions and tools to avoid polluting the main thread.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_*&lt;/code&gt;Tools, which allow the model to proactively request information that it deems relevant using tool calls.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There’s one technique that I feel is woefully underutilized by agents today: the humble hyperlink.&lt;/p&gt;
    &lt;head rend="h2"&gt;The obligatory human analogy&lt;/head&gt;
    &lt;p&gt;If you, a human, need to learn something without an LLM (let’s say something about an open source library), you will probably follow a trajectory that looks something like the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Do a Google search for the topic you need to understand&lt;/item&gt;
      &lt;item&gt;Click a relevant link to e.g. a docs page, read a high-level guide&lt;/item&gt;
      &lt;item&gt;Depending on your needs, maybe Cmd+Click a few more pages or the reference docs to open them in new tabs to review&lt;/item&gt;
      &lt;item&gt;Refer between your various open tabs as you complete your task&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you found an entrypoint through search, you were able to incrementally explore the topic through discovered links, filling your mental context with relevant information.&lt;/p&gt;
    &lt;p&gt;We can do the same thing with LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;HATEOAS in the era of Agents&lt;/head&gt;
    &lt;p&gt;The power of linked data is nothing new. Folks who have been building HTTP APIs for a long time might be familiar with HATEOAS, or “Hypertext as the Engine of Application State”. Purists have long claimed that a “truly” RESTful API should be fully self-describing, such that a client can explore and interact with it knowing nothing but an entrypoint in advance, with hyperlinks providing all necessary context to discover and consume additional endpoints.&lt;/p&gt;
    &lt;p&gt;This never worked in practice. Building hypertext APIs was too cumbersome and to actually consume APIs a human needed to understand the API structure in a useful manner anyway. So it was more useful just to have a “REST-ish” API and a good documentation page that humans could use. Creating “machine-readable” hyperlinked APIs that machines could navigate in theory but not in practice just wasn’t practical. LLMs change this dramatically.&lt;/p&gt;
    &lt;p&gt;When the machine can not only parse but also navigate the context and relevance of hyperlinks you have an actually useful paradigm: Hypertext as the Engine of Agent State.&lt;/p&gt;
    &lt;p&gt;This does apply to the web and HTTP APIs — I expect in the next few years we’ll see a resurgence of hypermedia concepts to make APIs more self-documenting (“dump the entire API schema as OpenAPI” is a start but not really sufficient).&lt;/p&gt;
    &lt;p&gt;But it also applies to local data, agent-specific data, really any data we want an agent to be able to discover and read.&lt;/p&gt;
    &lt;p&gt;So, how do we make all of our context linkable for agents?&lt;/p&gt;
    &lt;head rend="h2"&gt;One Tool to Read Them All&lt;/head&gt;
    &lt;p&gt;The scaffolding required to implement a powerful link-based context system is lightweight enough to be trivial. You need only:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A tool that accepts a list of URIs as arguments.&lt;/item&gt;
      &lt;item&gt;An entrypoint that brings at least one URI into context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a demonstrative example using Genkit in JS that uses system instructions as an entrypoint.&lt;/p&gt;
    &lt;p&gt;If we run the above code with a few different prompts, we see links in action:&lt;/p&gt;
    &lt;p&gt;The results are exactly what we’d hope:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the first test, no link reading was required and no links were read. The answer was right in the system prompt!&lt;/item&gt;
      &lt;item&gt;In the second test, reading one link was sufficient to reach a conclusion.&lt;/item&gt;
      &lt;item&gt;In the final test, the model understood to recursively fetch context linked from the first loaded document to reach a conclusion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, in ~30 lines of code with very little prompting and while using a cost-effective model, we’ve created a system that can dynamically load and correctly apply relevant context on-demand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benefits of Links&lt;/head&gt;
    &lt;p&gt;Links are a powerful tool in the context engineering toolbelt because of their simplicity, flexibility, and efficiency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Links are trivial to implement, and current models are “intuitively” good at understanding how to follow links.&lt;/item&gt;
      &lt;item&gt;Links can surface anywhere in the flow of a conversation. They can be specified in a system prompt, provided by the user, or returned by a tool.&lt;/item&gt;
      &lt;item&gt;Links are token-efficient because they use a small number of tokens to provide on-demand access to specific information. The model can load a link if it needs it but if it doesn’t few tokens are wasted.&lt;/item&gt;
      &lt;item&gt;Links are tool-efficient because they consolidate many types of reads into a single tool. You can have a &lt;code&gt;data://me&lt;/code&gt;link that dynamically loads information about the current user, a&lt;code&gt;file://foo.md&lt;/code&gt;link that loads a local file, and a&lt;code&gt;prompt://pet-help&lt;/code&gt;link that returns static instructions. You don’t need a separate tool for each type of data.&lt;/item&gt;
      &lt;item&gt;Links provide just-in-time context mitigating issues of context rot and recency bias in models. Because linked context is loaded when it’s needed by the model the context is “fresher” instead of overloading a system prompt.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;MCP Resources: The future is now(ish)&lt;/head&gt;
    &lt;p&gt;To make link-based context engineering a universal feature, we need a way to provide the linked content to the model. Many agents (and some model APIs) have built in various forms of “fetch URL” or “search the web” tools that can automatically fetch data from public sources. But the content we want to link might not always be available on the public internet.&lt;/p&gt;
    &lt;p&gt;The good news is we already have the exact primitive we need to solve this problem: MCP Resources. Resources allows servers to register URIs (or patterns of URIs) that can then be read on-demand by clients to provide static or dynamic content. Sounds perfect, right?&lt;/p&gt;
    &lt;p&gt;The bad news is I’m not aware of a single MCP client that makes MCP resources consumable by the model. Today, they only allow resources to be inserted by the user via @-mentions and similar devices. This doesn’t enable linking. But we’re so close!. We just need one critical thing:&lt;/p&gt;
    &lt;p&gt;Every MCP-enabled agent should expose a &lt;code&gt;read_resources&lt;/code&gt; tool that accepts one or more URIs and aggregates reading across all connected MCP servers (and probably web URLs as well).&lt;/p&gt;
    &lt;p&gt;There are other changes that would help: a mechanism of indexing / listing available MCP resources and exposing that in the system instructions, maybe even indexing MCP resources so that they can be searched using RAG techniques. But just enabling agents to access linked content opens up a world of new context engineering techniques.&lt;/p&gt;
    &lt;head rend="h2"&gt;Working with what you have&lt;/head&gt;
    &lt;p&gt;If you’re building your own agent, you don’t need any new technology to start making linked context — you can do it in a few dozen lines of code like I demonstrated above. But even if you’re trying to integrate with existing agents like Gemini CLI, Claude Code, or Cursor, you can still build with linked data patterns.&lt;/p&gt;
    &lt;p&gt;The Firebase MCP Server recently launched new capabilities including a &lt;code&gt;/firebase:init&lt;/code&gt; slash command for setting up Firebase in a project. We had some specific and pretty complex use cases in mind, so we built linked context into the MCP server itself:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We added support for MCP Resources to the Firebase MCP Server.&lt;/item&gt;
      &lt;item&gt;We created a read_resources MCP tool that was capable of reading resources (from the Firebase MCP Server only).&lt;/item&gt;
      &lt;item&gt;We created an MCP prompt that creates a guided workflow to walk the model step-by-step through configuring Firebase, including linked branching paths.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ve tested out this initialization flow against several popular coding agents that support MCP Prompts — each of them is able to understand and follow hyperlinks using our MCP server’s &lt;code&gt;read_resources&lt;/code&gt; tool and we’ve made onboarding to Firebase all the easier for it.&lt;/p&gt;
    &lt;p&gt;Effective context engineering is constantly evolving as models and agent harnesses improve; however, hyperlinks are such a powerfully efficient mechanism for information traversal that I can’t imagine a future of agents that doesn’t include linked context.&lt;/p&gt;
    &lt;p&gt;The next time you’re thinking of building half a dozen &lt;code&gt;get_*&lt;/code&gt; or &lt;code&gt;list_*&lt;/code&gt; tools for your agent, take a step back and consider: could the humble hyperlink get the job done instead?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mbleigh.dev/posts/context-engineering-with-links/"/><published>2025-10-23T14:24:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45682560</id><title>Luau's performance</title><updated>2025-10-25T19:32:25.724705+00:00</updated><content>&lt;doc fingerprint="ba7bb85b68e91340"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Performance&lt;/head&gt;
    &lt;p&gt;One of main goals of Luau is to enable high performance code, with gameplay code being the main use case. This can be viewed as two separate goals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make idiomatic code that wasn’t tuned faster&lt;/item&gt;
      &lt;item&gt;Enable even higher performance through careful tuning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both of these goals are important - it’s insufficient to just focus on the highly tuned code, and all things being equal we prefer to raise all boats by implementing general optimizations. However, in some cases it’s important to be aware of optimizations that Luau does and doesn’t do.&lt;/p&gt;
    &lt;p&gt;Worth noting is that Luau is focused on, first and foremost, stable high performance code in interpreted context. This is because JIT compilation is not available on many platforms Luau runs on, and AOT compilation would only work for code that Roblox ships (and even that does not always work). This is in stark contrast with LuaJIT that, while providing an excellent interpreter as well, focuses a lot of the attention on JIT (with many optimizations unavailable in the interpreter).&lt;/p&gt;
    &lt;p&gt;Having said that, Luau has been updated to include an optional JIT component for x64 and arm64 platforms. This component can compile a selected set of functions, including limiting compilation to functions or modules marked explicitly by the user. While functions can be compiled at any time, automated JIT compilation decisions based on statistics/tracing are not performed. Luau JIT takes into account the type annotations present in the source code to specialize code paths and at this time, doesn’t include runtime analysis of the types/values flowing through the program.&lt;/p&gt;
    &lt;p&gt;The rest of this document goes into some optimizations that Luau employs and how to best leverage them when writing code. The document is not complete - a lot of optimizations are transparent to the user and involve detailed low-level tuning of various parts that is not described here - and all of this is subject to change without notice, as it doesn’t affect the semantics of valid code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast bytecode interpreter&lt;/head&gt;
    &lt;p&gt;Luau features a very highly tuned portable bytecode interpreter. It’s similar to Lua interpreter in that it’s written in C, but it’s highly tuned to yield efficient assembly when compiled with Clang and latest versions of MSVC. On some workloads it can match the performance of LuaJIT interpreter which is written in highly specialized assembly. We are continuing to tune the interpreter and the bytecode format over time; while extra performance can be extracted by rewriting the interpreter in assembly, we’re unlikely to ever do that as the extra gains at this point are marginal, and we gain a lot from C in terms of portability and being able to quickly implement new optimizations.&lt;/p&gt;
    &lt;p&gt;Of course the interpreter isn’t typical C code - it uses many tricks to achieve extreme levels of performance and to coerce the compiler to produce efficient assembly. Due to a better bytecode design and more efficient dispatch loop it’s noticeably faster than Lua 5.x (including Lua 5.4 which made some of the changes similar to Luau, but doesn’t come close). The bytecode design was partially inspired by excellent LuaJIT interpreter. Most computationally intensive scripts only use the interpreter core loop and builtins, which on x64 compiles into ~16 KB, thus leaving half of the instruction cache for other infrequently called code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizing compiler&lt;/head&gt;
    &lt;p&gt;Unlike Lua and LuaJIT, Luau uses a multi-pass compiler with a frontend that parses source into an AST and a backend that generates bytecode from it. This carries a small penalty in terms of compilation time, but results in more flexible code and, crucially, makes it easier to optimize the generated bytecode.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Compilation throughput isn’t the main focus in Luau, but our compiler is reasonably fast; with all currently implemented optimizations enabled, it compiles 950K lines of Luau code in 1 second on a single core of a desktop Ryzen 5900X CPU, producing bytecode and debug information.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While bytecode optimizations are limited due to the flexibility of Luau code (e.g. &lt;code&gt;a * 1&lt;/code&gt; may not be equivalent to &lt;code&gt;a&lt;/code&gt; if &lt;code&gt;*&lt;/code&gt; is overloaded through metatables), even in absence of type information Luau compiler can perform some optimizations such as “deep” constant folding across functions and local variables, perform upvalue optimizations for upvalues that aren’t mutated, do analysis of builtin function usage, optimize the instruction sequences for multiple variable assignments, and some peephole optimizations on the resulting bytecode. The compiler can also be instructed to use more aggressive optimizations by enabling optimization level 2 (&lt;code&gt;-O2&lt;/code&gt; in CLI tools), some of which are documented further on this page.&lt;/p&gt;
    &lt;p&gt;Most bytecode optimizations are performed on individual statements or functions, however the compiler also does a limited amount of interprocedural optimizations; notably, calls to local functions can be optimized with the knowledge of the argument count or number of return values involved. Interprocedural optimizations are limited to a single module due to the compilation model.&lt;/p&gt;
    &lt;p&gt;Luau compiler is also able to use type information to do further optimizations. Because we control the entire stack (unlike e.g. TypeScript where the type information is discarded completely before reaching the VM), we have more flexibility there and can make some tradeoffs during codegen even if the type system isn’t completely sound. For example, it might be reasonable to assume that in presence of known types, we can infer absence of side effects for arithmetic operations and builtins - if the runtime types mismatch due to intentional violation of the type safety through global injection, the code will still be safely sandboxed. Type information is currently limited to small peephole optimizations, but it has a potential to unlock optimizations such as common subexpression elimination and allocation hoisting in the future, without having to rely on a JIT. These future optimizations opportunities are speculative pending further research.&lt;/p&gt;
    &lt;head rend="h2"&gt;Epsilon-overhead debugger&lt;/head&gt;
    &lt;p&gt;It’s important for Luau to have stable and predictable performance. Something that comes up in Lua-based environments often is the use of line hooks to implement debugging (both for breakpoints and for stepping). This is problematic because the support for hooks is typically not free in general, but importantly once the hook is enabled, calling the hook has a considerable overhead, and the hook itself may be very costly to evaluate since it will need to associate the script:line pair with the breakpoint information.&lt;/p&gt;
    &lt;p&gt;Luau does not support hooks at all, and relies on first-class support for breakpoints (using bytecode patching) and single-stepping (using a custom interpreter loop) to implement debugging. As a result, the presence of breakpoints doesn’t slow the script execution down - the only noticeable discrepancy between running code under a debugger and without a debugger should be in cases where breakpoints are evaluated and skipped based on breakpoint conditions, or when stepping over long-running fragments of code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inline caching for table and global access&lt;/head&gt;
    &lt;p&gt;Table access for field lookup is optimized in Luau using a mechanism that blends inline caching (classically used in Java/JavaScript VMs) and HREFs (implemented in LuaJIT). Compiler can predict the hash slot used by field lookup, and the VM can correct this prediction dynamically.&lt;/p&gt;
    &lt;p&gt;As a result, field access can be very fast in Luau, provided that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The field name is known at compile time. To make sure this is the case, &lt;code&gt;table.field&lt;/code&gt;notation is recommended, although the compiler will also optimize&lt;code&gt;table["field"]&lt;/code&gt;when the expression is known to be a constant string.&lt;/item&gt;
      &lt;item&gt;The field access doesn’t use metatables. The fastest way to work with tables in Luau is to store fields directly inside the table, and store methods in the metatable (see below); access to “static” fields in classic OOP designs is best done through &lt;code&gt;Class.StaticField&lt;/code&gt;instead of&lt;code&gt;object.StaticField&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The object structure is usually uniform. While it’s possible to use the same function to access tables of different shape - e.g. &lt;code&gt;function getX(obj) return obj.x end&lt;/code&gt;can be used on any table that has a field&lt;code&gt;"x"&lt;/code&gt;- it’s best to not vary the keys used in the tables too much, as it defeats this optimization.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The same optimization is applied to the custom globals declared in the script, although it’s best to avoid these altogether by using locals instead. Still, this means that the difference between &lt;code&gt;function&lt;/code&gt; and &lt;code&gt;local function&lt;/code&gt; is less pronounced in Luau.&lt;/p&gt;
    &lt;head rend="h2"&gt;Importing global access chains&lt;/head&gt;
    &lt;p&gt;While global access for library functions can be optimized in a similar way, this optimization breaks down when the global table is using sandboxing through metatables, and even when globals aren’t sandboxed, &lt;code&gt;math.max&lt;/code&gt; still requires two table accesses.&lt;/p&gt;
    &lt;p&gt;It’s always possible to “localize” the global accesses by using &lt;code&gt;local max = math.max&lt;/code&gt;, but this is cumbersome - in practice it’s easy to forget to apply this optimization. To avoid relying on programmers remembering to do this, Luau implements a special optimization called “imports”, where most global chains such as &lt;code&gt;math.max&lt;/code&gt; are resolved when the script is loaded instead of when the script is executed.&lt;/p&gt;
    &lt;p&gt;This optimization relies on being able to predict the shape of the environment table for a given function; this is possible due to global sandboxing, however this optimization is invalid in some cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;loadstring&lt;/code&gt;can load additional code that runs in context of the caller’s environment&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt;can directly modify the environment of any function&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The use of any of these functions performs a dynamic deoptimization, marking the affected environment as “impure”. The optimizations are only in effect on functions with “pure” environments - because of this, the use of &lt;code&gt;loadstring&lt;/code&gt;/&lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt; is not recommended. Note that &lt;code&gt;getfenv&lt;/code&gt; deoptimizes the environment even if it’s only used to read values from the environment.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Luau still supports these functions as part of our backwards compatibility promise, although we’d love to switch to Lua 5.2’s&lt;/p&gt;&lt;code&gt;_ENV&lt;/code&gt;as that mechanism is cleaner and doesn’t require costly dynamic deoptimization.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Fast method calls&lt;/head&gt;
    &lt;p&gt;Luau specializes method calls to improve their performance through a combination of compiler, VM and binding optimizations. Compiler emits a specialized instruction sequence when methods are called through &lt;code&gt;obj:Method&lt;/code&gt; syntax (while this isn’t idiomatic anyway, you should avoid &lt;code&gt;obj.Method(obj)&lt;/code&gt;). When the object in question is a Lua table, VM performs some voodoo magic based on inline caching to try to quickly discover the implementation of this method through the metatable.&lt;/p&gt;
    &lt;p&gt;For this to be effective, it’s crucial that &lt;code&gt;__index&lt;/code&gt; in a metatable points to a table directly. For performance reasons it’s strongly recommended to avoid &lt;code&gt;__index&lt;/code&gt; functions as well as deep &lt;code&gt;__index&lt;/code&gt; chains; an ideal object in Luau is a table with a metatable that points to itself through &lt;code&gt;__index&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When the object in question is a reflected userdata, a special mechanism called “namecall” is used to minimize the interop cost. In classical Lua binding model, &lt;code&gt;obj:Method&lt;/code&gt; is called in two steps, retrieving the function object (&lt;code&gt;obj.Method&lt;/code&gt;) and calling it; both steps are often implemented in C++, and the method retrieval needs to use a method object cache - all of this makes method calls slow.&lt;/p&gt;
    &lt;p&gt;Luau can directly call the method by name using the “namecall” extension, and an optimized reflection layer can retrieve the correct method quickly through more voodoo magic based on string interning and custom Luau features that aren’t exposed through Luau scripts.&lt;/p&gt;
    &lt;p&gt;As a result of both optimizations, common Lua tricks of caching the method in a local variable aren’t very productive in Luau and aren’t recommended either.&lt;/p&gt;
    &lt;head rend="h2"&gt;Specialized builtin function calls&lt;/head&gt;
    &lt;p&gt;Due to global sandboxing and the ability to dynamically deoptimize code running in impure environments, in pure environments we go beyond optimizing the interpreter and optimize many built-in functions through a “fastcall” mechanism.&lt;/p&gt;
    &lt;p&gt;For this mechanism to work, function call must be “obvious” to the compiler - it needs to call a builtin function directly, e.g. &lt;code&gt;math.max(x, 1)&lt;/code&gt;, although it also works if the function is “localized” (&lt;code&gt;local max = math.max&lt;/code&gt;); this mechanism doesn’t work for indirect function calls unless they were inlined during compilation, and doesn’t work for method calls (so calling &lt;code&gt;string.byte&lt;/code&gt; is more efficient than &lt;code&gt;s:byte&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The mechanism works by directly invoking a highly specialized and optimized implementation of a builtin function from the interpreter core loop without setting up a stack frame and omitting other work; additionally, some fastcall specializations are partial in that they don’t support all types of arguments, for example all &lt;code&gt;math&lt;/code&gt; library builtins are only specialized for numeric arguments, so calling &lt;code&gt;math.abs&lt;/code&gt; with a string argument will fall back to the slower implementation that will do string-&amp;gt;number coercion.&lt;/p&gt;
    &lt;p&gt;As a result, builtin calls are very fast in Luau - they are still slightly slower than core instructions such as arithmetic operations, but only slightly so. The set of fastcall builtins is slowly expanding over time and as of this writing contains &lt;code&gt;assert&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;typeof&lt;/code&gt;, &lt;code&gt;rawget&lt;/code&gt;/&lt;code&gt;rawset&lt;/code&gt;/&lt;code&gt;rawequal&lt;/code&gt;, &lt;code&gt;getmetatable&lt;/code&gt;/&lt;code&gt;setmetatable&lt;/code&gt;, &lt;code&gt;tonumber&lt;/code&gt;/&lt;code&gt;tostring&lt;/code&gt;, all functions from &lt;code&gt;math&lt;/code&gt; (except &lt;code&gt;noise&lt;/code&gt; and &lt;code&gt;random&lt;/code&gt;/&lt;code&gt;randomseed&lt;/code&gt;) and &lt;code&gt;bit32&lt;/code&gt;, and some functions from &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;table&lt;/code&gt; library.&lt;/p&gt;
    &lt;p&gt;Some builtin functions have partial specializations that reduce the cost of the common case further. Notably:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;assert&lt;/code&gt;is specialized for cases when the assertion return value is not used and the condition is truthy; this helps reduce the runtime cost of assertions to the extent possible&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bit32.extract&lt;/code&gt;is optimized further when field and width selectors are constant&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;select&lt;/code&gt;is optimized when the second argument is&lt;code&gt;...&lt;/code&gt;; in particular,&lt;code&gt;select(x, ...)&lt;/code&gt;is O(1) when using the builtin dispatch mechanism even though it’s normally O(N) in variadic argument count.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some functions from &lt;code&gt;math&lt;/code&gt; library like &lt;code&gt;math.floor&lt;/code&gt; can additionally take advantage of advanced SIMD instruction sets like SSE4.1 when available.&lt;/p&gt;
    &lt;p&gt;In addition to runtime optimizations for builtin calls, many builtin calls, as well as constants like &lt;code&gt;math.pi&lt;/code&gt;/&lt;code&gt;math.huge&lt;/code&gt;, can also be constant-folded by the bytecode compiler when using aggressive optimizations (level 2); this currently applies to most builtin calls with constant arguments and a single return value. For builtin calls that can not be constant folded, compiler assumes knowledge of argument/return count (level 2) to produce more efficient bytecode instructions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized table iteration&lt;/head&gt;
    &lt;p&gt;Luau implements a fully generic iteration protocol; however, for iteration through tables in addition to generalized iteration (&lt;code&gt;for .. in t&lt;/code&gt;) it recognizes three common idioms (&lt;code&gt;for .. in ipairs(t)&lt;/code&gt;, &lt;code&gt;for .. in pairs(t)&lt;/code&gt; and &lt;code&gt;for .. in next, t&lt;/code&gt;) and emits specialized bytecode that is carefully optimized using custom internal iterators.&lt;/p&gt;
    &lt;p&gt;As a result, iteration through tables typically doesn’t result in function calls for every iteration; the performance of iteration using generalized iteration, &lt;code&gt;pairs&lt;/code&gt; and &lt;code&gt;ipairs&lt;/code&gt; is comparable, so generalized iteration (without the use of &lt;code&gt;pairs&lt;/code&gt;/&lt;code&gt;ipairs&lt;/code&gt;) is recommended unless the code needs to be compatible with vanilla Lua or the specific semantics of &lt;code&gt;ipairs&lt;/code&gt; (which stops at the first &lt;code&gt;nil&lt;/code&gt; element) is required. Additionally, using generalized iteration avoids calling &lt;code&gt;pairs&lt;/code&gt; when the loop starts which can be noticeable when the table is very short.&lt;/p&gt;
    &lt;p&gt;Iterating through array-like tables using &lt;code&gt;for i=1,#t&lt;/code&gt; tends to be slightly slower because of extra cost incurred when reading elements from the table.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized table length&lt;/head&gt;
    &lt;p&gt;Luau tables use a hybrid array/hash storage, like in Lua; in some sense “arrays” don’t truly exist and are an internal optimization, but some operations, notably &lt;code&gt;#t&lt;/code&gt; and functions that depend on it, like &lt;code&gt;table.insert&lt;/code&gt;, are defined by the Luau/Lua language to allow internal optimizations. Luau takes advantage of that fact.&lt;/p&gt;
    &lt;p&gt;Unlike Lua, Luau guarantees that the element at index &lt;code&gt;#t&lt;/code&gt; is stored in the array part of the table. This can accelerate various table operations that use indices limited by &lt;code&gt;#t&lt;/code&gt;, and this makes &lt;code&gt;#t&lt;/code&gt; worst-case complexity O(logN), unlike Lua where the worst case complexity is O(N). This also accelerates computation of this value for small tables like &lt;code&gt;{ [1] = 1 }&lt;/code&gt; since we never need to look at the hash part.&lt;/p&gt;
    &lt;p&gt;The “default” implementation of &lt;code&gt;#t&lt;/code&gt; in both Lua and Luau is a binary search. Luau uses a special branch-free (depending on the compiler…) implementation of the binary search which results in 50+% faster computation of table length when it needs to be computed from scratch.&lt;/p&gt;
    &lt;p&gt;Additionally, Luau can cache the length of the table and adjust it following operations like &lt;code&gt;table.insert&lt;/code&gt;/&lt;code&gt;table.remove&lt;/code&gt;; this means that in practice, &lt;code&gt;#t&lt;/code&gt; is almost always a constant time operation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating and modifying tables&lt;/head&gt;
    &lt;p&gt;Luau implements several optimizations for table creation. When creating object-like tables, it’s recommended to use table literals (&lt;code&gt;{ ... }&lt;/code&gt;) and to specify all table fields in the literal in one go instead of assigning fields later; this triggers an optimization inspired by LuaJIT’s “table templates” and results in higher performance when creating objects. When creating array-like tables, if the maximum size of the table is known up front, it’s recommended to use &lt;code&gt;table.create&lt;/code&gt; function which can create an empty table with preallocated storage, and optionally fill it with a given value.&lt;/p&gt;
    &lt;p&gt;When the exact table shape isn’t known, Luau compiler can still predict the table capacity required in case the table is initialized with an empty literal (&lt;code&gt;{}&lt;/code&gt;) and filled with fields subsequently. For example, the following code creates a correctly sized table implicitly:&lt;/p&gt;
    &lt;code&gt;local v = {}
v.x = 1
v.y = 2
v.z = 3
return v
&lt;/code&gt;
    &lt;p&gt;When appending elements to tables, it’s recommended to use &lt;code&gt;table.insert&lt;/code&gt; (which is the fastest method to append an element to a table if the table size is not known). In cases when a table is filled sequentially, however, it can be more efficient to use a known index for insertion - together with preallocating tables using &lt;code&gt;table.create&lt;/code&gt; this can result in much faster code, for example this is the fastest way to build a table of squares:&lt;/p&gt;
    &lt;code&gt;local t = table.create(N)

for i=1,N do
	t[i] = i * i
end
&lt;/code&gt;
    &lt;head rend="h2"&gt;Native vector math&lt;/head&gt;
    &lt;p&gt;Luau uses tagged value storage - each value contains a type tag and the data that represents the value of a given type. Because of the need to store 64-bit double precision numbers and 64-bit pointers, we don’t use NaN tagging and have to pay the cost of 16 bytes per value.&lt;/p&gt;
    &lt;p&gt;We take advantage of this to provide a native value type that can store a 32-bit floating point vector with 3 components. This type is fundamental to game computations and as such it’s important to optimize the storage and the operations with that type - our VM implements first class support for all math operations and component manipulation, which essentially means we have native 3-wide SIMD support. For code that uses many vector values this results in significantly smaller GC pressure and significantly faster execution, and gives programmers a mechanism to hand-vectorize numeric code if need be.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized upvalue storage&lt;/head&gt;
    &lt;p&gt;Lua implements upvalues as garbage collected objects that can point directly at the thread’s stack or, when the value leaves the stack frame (and is “closed”), store the value inside the object. This representation is necessary when upvalues are mutated, but inefficient when they aren’t - and 90% or more of upvalues aren’t mutated in typical Lua code. Luau takes advantage of this by reworking upvalue storage to prioritize immutable upvalues - capturing upvalues that don’t change doesn’t require extra allocations or upvalue closing, resulting in faster closure allocation, faster execution, faster garbage collection and faster upvalue access due to better memory locality.&lt;/p&gt;
    &lt;p&gt;Note that “immutable” in this case only refers to the variable itself - if the variable isn’t assigned to it can be captured by value, even if it’s a table that has its contents change.&lt;/p&gt;
    &lt;p&gt;When upvalues are mutable, they do require an extra allocated object; we carefully optimize the memory consumption and access cost for mutable upvalues to reduce the associated overhead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closure caching&lt;/head&gt;
    &lt;p&gt;With optimized upvalue storage, creating new closures (function objects) is more efficient but still requires allocating a new object every time. This can be problematic for cases when functions are passed to algorithms like &lt;code&gt;table.sort&lt;/code&gt; or functions like &lt;code&gt;pcall&lt;/code&gt;, as it results in excessive allocation traffic which then leads to more work for garbage collector.&lt;/p&gt;
    &lt;p&gt;To make closure creation cheaper, Luau compiler implements closure caching - when multiple executions of the same function expression are guaranteed to result in the function object that is semantically identical, the compiler may cache the closure and always return the same object. This changes the function identity which may affect code that uses function objects as table keys, but preserves the calling semantics - compiler will only do this if calling the original (cached) function behaves the same way as a newly created function would. The heuristics used for this optimization are subject to change; currently, the compiler will cache closures that have no upvalues, or all upvalues are immutable (see previous section) and are declared at the module scope, as the module scope is (almost always) evaluated only once.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast memory allocator&lt;/head&gt;
    &lt;p&gt;Similarly to LuaJIT, but unlike vanilla Lua, Luau implements a custom allocator that is highly specialized and tuned to the common allocation workloads we see. The allocator design is inspired by classic pool allocators as well as the excellent &lt;code&gt;mimalloc&lt;/code&gt;, but through careful domain-specific tuning it beats all general purpose allocators we’ve tested, including &lt;code&gt;rpmalloc&lt;/code&gt;, &lt;code&gt;mimalloc&lt;/code&gt;, &lt;code&gt;jemalloc&lt;/code&gt;, &lt;code&gt;ptmalloc&lt;/code&gt; and &lt;code&gt;tcmalloc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean that memory allocation in Luau is free - it’s carefully optimized, but it still carries a cost, and a high rate of allocations requires more work from the garbage collector. The garbage collector is incremental, so short of some edge cases this rarely results in visible GC pauses, but can impact the throughput since scripts will interrupt to perform “GC assists” (helping clean up the garbage). Thus for high performance Luau code it’s recommended to avoid allocating memory in tight loops, by avoiding temporary table and userdata creation.&lt;/p&gt;
    &lt;p&gt;In addition to a fast allocator, all frequently used structures in Luau have been optimized for memory consumption, especially on 64-bit platforms, compared to Lua 5.1 baseline. This helps to reduce heap memory footprint and improve performance in some cases by reducing the memory bandwidth impact of garbage collection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized libraries&lt;/head&gt;
    &lt;p&gt;While the best performing code in Luau spends most of the time in the interpreter, performance of the standard library functions is critical to some applications. In addition to specializing many small and simple functions using the builtin call mechanism, we spend extra care on optimizing all library functions and providing additional functions beyond the Lua standard library that help achieve good performance with idiomatic code.&lt;/p&gt;
    &lt;p&gt;Functions from the &lt;code&gt;table&lt;/code&gt; library like &lt;code&gt;insert&lt;/code&gt;, &lt;code&gt;remove&lt;/code&gt; and &lt;code&gt;move&lt;/code&gt; have been tuned for performance on array-like tables, achieving 3x and more performance compared to un-tuned versions, and Luau provides additional functions like &lt;code&gt;table.create&lt;/code&gt; and &lt;code&gt;table.find&lt;/code&gt; to achieve further speedup when applicable. Our implementation of &lt;code&gt;table.sort&lt;/code&gt; is using &lt;code&gt;introsort&lt;/code&gt; algorithm which results in guaranteed worst case &lt;code&gt;NlogN&lt;/code&gt; complexity regardless of the input, and, together with the array-like specializations, helps achieve ~4x speedup on average.&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;string&lt;/code&gt; library, we use a carefully tuned dynamic string buffer implementation; it is optimized for smaller strings to reduce garbage created during string manipulation, and for larger strings it allows to produce a large string without extra copies, especially in cases where the resulting size is known ahead of time. Additionally, functions like &lt;code&gt;format&lt;/code&gt; have been tuned to avoid the overhead of &lt;code&gt;sprintf&lt;/code&gt; where possible, resulting in further speedups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Improved garbage collector pacing&lt;/head&gt;
    &lt;p&gt;Luau uses an incremental garbage collector which does a little bit of work every so often, and at no point does it stop the world to traverse the entire heap. The runtime will make sure that the collector runs interspersed with the program execution as the program allocates additional memory, which is known as “garbage collection assists”, and can also run in response to explicit garbage collection invocation via &lt;code&gt;lua_gc&lt;/code&gt;. In interactive environments such as video game engines it’s possible, and even desirable, to request garbage collection every frame to make sure assists are minimized, since that allows scheduling the garbage collection to run concurrently with other engine processing that doesn’t involve script execution.&lt;/p&gt;
    &lt;p&gt;Inspired by excellent work by Austin Clements on Go’s garbage collector pacer, we’ve implemented a pacing algorithm that uses a proportional–integral–derivative controller to estimate internal garbage collector tunables to reach a target heap size, defined as a percentage of the live heap data (which is more intuitive and actionable than Lua 5.x “GC pause” setting). Luau runtime also estimates the allocation rate making it easy (given uniform allocation rates) to adjust the per-frame garbage collection requests to do most of the required GC work outside of script execution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reduced garbage collector pauses&lt;/head&gt;
    &lt;p&gt;While Luau uses an incremental garbage collector, once per each collector cycle it runs a so-called “atomic” step. While all other GC steps can do very little work by only looking at a few objects at a given time, which means that the collector can have arbitrarily short pauses, the “atomic” step needs to traverse some amount of data that, in some cases, may scale with the application heap. Since atomic step is indivisible, it can result in occasional pauses on the order of tens of milliseconds, which is problematic for interactive applications. We’ve implemented a series of optimizations to help reduce the atomic step.&lt;/p&gt;
    &lt;p&gt;Normally objects that have been modified after the GC marked them in an incremental mark phase need to be rescanned during atomic phase, so frequent modifications of existing tables may result in a slow atomic step. To address this, we run a “remark” step where we traverse objects that have been modified after being marked once more (incrementally); additionally, the write barrier that triggers for object modifications changes the transition logic during remark phase to reduce the probability that the object will need to be rescanned.&lt;/p&gt;
    &lt;p&gt;Another source of scalability challenges is coroutines. Writes to coroutine stacks don’t use a write barrier, since that’s prohibitively expensive as they are too frequent. This means that coroutine stacks need to be traversed during atomic step, so applications with many coroutines suffer large atomic pauses. To address this, we implement incremental marking of coroutines: marking a coroutine makes it “inactive” and resuming a coroutine (or pushing extra objects on the coroutine stack via C API) makes it “active”. Atomic step only needs to traverse active coroutines again, which reduces the cost of atomic step by effectively making coroutine collection incremental as well.&lt;/p&gt;
    &lt;p&gt;While large tables can be a problem for incremental GC in general since currently marking a single object is indivisible, large weak tables are a unique challenge because they also need to be processed during atomic phase, and the main use case for weak tables - object caches - may result in tables with large capacity but few live objects in long-running applications that exhibit bursts of activity. To address this, weak tables in Luau can be marked as “shrinkable” by including &lt;code&gt;s&lt;/code&gt; as part of &lt;code&gt;__mode&lt;/code&gt; string, which results in weak tables being resized to the optimal capacity during GC. This option may result in missing keys during table iteration if the table is resized while iteration is in progress and as such is only recommended for use in specific circumstances.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized garbage collector sweeping&lt;/head&gt;
    &lt;p&gt;The incremental garbage collector in Luau runs three phases for each cycle: mark, atomic and sweep. Mark incrementally traverses all live objects, atomic finishes various operations that need to happen without mutator intervention (see previous section), and sweep traverses all objects in the heap, reclaiming memory used by dead objects and performing minor fixup for live objects. While objects allocated during the mark phase are traversed in the same cycle and thus may get reclaimed, objects allocated during the sweep phase are considered live. Because of this, the faster the sweep phase completes, the less garbage will accumulate; and, of course, the less time sweeping takes the less overhead there is from this phase of garbage collection on the process.&lt;/p&gt;
    &lt;p&gt;Since sweeping traverses the whole heap, we maximize the efficiency of this traversal by allocating garbage-collected objects of the same size in 16 KB pages, and traversing each page at a time, which is otherwise known as a paged sweeper. This ensures good locality of reference as consecutively swept objects are contiguous in memory, and allows us to spend no memory for each object on sweep-related data or allocation metadata, since paged sweeper doesn’t need to be able to free objects without knowing which page they are in. Compared to linked list based sweeping that Lua/LuaJIT implement, paged sweeper is 2-3x faster, and saves 16 bytes per object on 64-bit platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Function inlining and loop unrolling&lt;/head&gt;
    &lt;p&gt;By default, the bytecode compiler performs a series of optimizations that result in faster execution of the code, but they preserve both execution semantics and debuggability. For example, a function call is compiled as a function call, which may be observable via &lt;code&gt;debug.traceback&lt;/code&gt;; a loop is compiled as a loop, which may be observable via &lt;code&gt;lua_getlocal&lt;/code&gt;. To help improve performance in cases where these restrictions can be relaxed, the bytecode compiler implements additional optimizations when optimization level 2 is enabled (which requires using &lt;code&gt;-O2&lt;/code&gt; switch when using Luau CLI), namely function inlining and loop unrolling.&lt;/p&gt;
    &lt;p&gt;Only loops with loop bounds known at compile time, such as &lt;code&gt;for i=1,4 do&lt;/code&gt;, can be unrolled. The loop body must be simple enough for the optimization to be profitable; compiler uses heuristics to estimate the performance benefit and automatically decide if unrolling should be performed.&lt;/p&gt;
    &lt;p&gt;Only local functions (defined either as &lt;code&gt;local function foo&lt;/code&gt; or &lt;code&gt;local foo = function&lt;/code&gt;) can be inlined. The function body must be simple enough for the optimization to be profitable; compiler uses heuristics to estimate the performance benefit and automatically decide if each call to the function should be inlined instead. Additionally recursive invocations of a function can’t be inlined at this time, and inlining is completely disabled for modules that use &lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt; functions.&lt;/p&gt;
    &lt;p&gt;In both cases, in addition to removing the overhead associated with function calls or loop iteration, these optimizations can additionally benefit by enabling additional optimizations, such as constant folding of expressions dependent on loop iteration variable or constant function arguments, or using more efficient instructions for certain expressions when the inputs to these instructions are constants.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://luau.org/performance"/><published>2025-10-23T14:55:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45692984</id><title>Twake Drive – An open-source alternative to Google Drive</title><updated>2025-10-25T19:32:25.232465+00:00</updated><content>&lt;doc fingerprint="cb16d4485c6376c3"&gt;
  &lt;main&gt;
    &lt;p&gt; The open-source alternative to Google Drive. &lt;lb/&gt; Learn more » &lt;lb/&gt; Telegram | Website | Issues | Roadmap &lt;/p&gt;
    &lt;p&gt;To get a local copy up and running, please follow these simple steps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone the repo &lt;quote&gt;git clone https://github.com/linagora/twake-drive&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Run it with Docker &lt;code&gt;cd tdrive docker compose -f docker-compose.minimal.yml up&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Open http://localhost/ in a browser&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js (Version: &amp;gt;=18.x)&lt;/item&gt;
      &lt;item&gt;MongoDB&lt;/item&gt;
      &lt;item&gt;Yarn (recommended)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Launch MongoDB using&lt;/p&gt;
        &lt;quote&gt;docker run -p 27017:27017 -d mongo&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Launch frontend with&lt;/p&gt;
        &lt;quote&gt;cd tdrive/frontend/; yarn dev:start&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Launch backend with&lt;/p&gt;&lt;quote&gt;cd tdrive/backend/node/; SEARCH_DRIVER=mongodb DB_DRIVER=mongodb PUBSUB_TYPE=local \ DB_MONGO_URI=mongodb://localhost:27017 STORAGE_LOCAL_PATH=/[full-path-to-store-documents]/documents \ NODE_ENV=development yarn dev&lt;/quote&gt;&lt;p&gt;If you need more parameters, create/edit&lt;/p&gt;&lt;code&gt;tdrive/backend/node/config/development.json&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The app will be running on port 3000&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Twake Drive is licensed under Affero GPL v3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/linagora/twake-drive"/><published>2025-10-24T10:16:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45694856</id><title>First convex polyhedron found that can't pass through itself</title><updated>2025-10-25T19:32:24.971586+00:00</updated><content>&lt;doc fingerprint="a390899286a00301"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;First Shape Found That Can’t Pass Through Itself&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Imagine you’re holding two equal-size dice. Is it possible to bore a tunnel through one die that’s big enough for the other to slide through?&lt;/p&gt;
    &lt;p&gt;Perhaps your instinct is to say “Surely not!” If so, you’re not alone. In the late 1600s, an unidentified person placed a bet to that effect with Prince Rupert of the Rhine. Rupert — a nephew of Charles I of England who commanded the Royalist forces in the English Civil War — spent his sunset years studying metallurgy and glassmaking in his laboratory at Windsor Castle.&lt;/p&gt;
    &lt;p&gt;Rupert won the bet. The mathematician John Wallis, recounting the story in 1693, didn’t say whether Rupert wrote a proof or bored a hole through an actual cube. But Wallis himself proved mathematically that, if you drill a straight tunnel in the direction of one of the cube’s inner diagonals, it can be made wide enough to allow another cube through. It’s a tight squeeze: If you make the second cube just 4% larger, it will no longer fit.&lt;/p&gt;
    &lt;p&gt;It’s natural to wonder which other shapes have this property. “I think of this problem as being quite canonical,” said Tom Murphy, a software engineer at Google who has explored the question extensively in his free time. It “would have gotten rediscovered and rediscovered — aliens would have come to this one.”&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;The full menagerie of shapes is too diverse to get a handle on, so mathematicians tend to focus on convex polyhedra: shapes, like the cube, that have flat sides and no protrusions or indentations. When such a shape is much wider in some directions than others, it’s usually easy to find a straight tunnel that will allow another copy of the shape to pass through. But many famous convex polyhedra — for instance the dodecahedron, or the truncated icosahedron, the shape that forms a soccer ball — are highly symmetric and difficult to analyze. Among these, “for hundreds of years we only knew of the cube,” said Jakob Steininger, a mathematician at Statistics Austria, Austria’s federal statistics organization.&lt;/p&gt;
    &lt;p&gt;Then, in 1968, Christoph Scriba proved that the tetrahedron and octahedron also have the “Rupert property,” as mathematicians now call it. And in a burst of activity over the past decade, professional mathematicians and hobbyists have found Rupert tunnels through many of the most widely studied convex polyhedra, including the dodecahedron, icosahedron and soccer ball.&lt;/p&gt;
    &lt;p&gt;The Rupert property appeared to be so widespread that mathematicians conjectured a general rule: Every convex polyhedron will have the Rupert property. No one could find one that didn’t — until now.&lt;/p&gt;
    &lt;p&gt;In a paper posted online in August, Steininger and Sergey Yurkevich — a researcher at A&amp;amp;R Tech, an Austrian transportation systems company — describe a shape with 90 vertices and 152 faces that they’ve named the Noperthedron (after “Nopert,” a coinage by Murphy that combines “Rupert” and “nope”). Steininger and Yurkevich proved that no matter how you bore a straight tunnel through a Noperthedron, a second Noperthedron cannot fit through.&lt;/p&gt;
    &lt;p&gt;The proof required a mix of theoretical advances and massive computer calculations, and relies on a delicate property of the Noperthedron’s vertices. “It’s a miracle that it works,” Steininger said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Passing Through the Shadows&lt;/head&gt;
    &lt;p&gt;To see how one cube can pass through another, imagine holding a cube over a table and examining its shadow (assuming it’s illuminated from above). If you hold the cube in the standard position, the shadow is a square. But if you point one of the corners directly upward, the shadow is a regular hexagon.&lt;/p&gt;
    &lt;p&gt;In 1693, Wallis showed that the square shadow fits inside the hexagon, leaving a thin margin. That means that if you point a cube’s corner upward, you can bore a vertical tunnel that’s big enough for a second cube to pass through. About a century later, Pieter Nieuwland showed that a different orientation casts an even better shadow — one that can accommodate a cube more than 6% larger than the cube with the tunnel.&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;Every subsequent analysis of more complicated shapes has relied on this process of turning the shape in different directions and looking for one shadow that fits inside another. With the aid of computers, mathematicians have found Rupert passages through a wide variety of shapes. Some are incredibly tight fits — for instance, the passage in a “triakis tetrahedron” has a margin that’s only about 0.000002 times the length of the shape’s radius. “The world of mixing computation and discrete geometry has flowered to make these kinds of calculations possible,” said Joseph O’Rourke, an emeritus professor at Smith College.&lt;/p&gt;
    &lt;p&gt;Researchers who have written algorithms to find Rupert passages have noticed a curious dichotomy: For any given convex polyhedron, the algorithm seems to either find a passage almost immediately, or not find one at all. In the past five years, mathematicians have accumulated a small collection of holdout shapes for which no passage has been found.&lt;/p&gt;
    &lt;p&gt;“I’ve had my desktop churn for two weeks on trying the rhombicosidodecahedron,” said Benjamin Grimmer, an applied mathematician at Johns Hopkins University, referring to a solid made of 62 regular triangles, squares and pentagons. “That one just seems to resist any attempt.”&lt;/p&gt;
    &lt;p&gt;But such resistance doesn’t prove that a shape is a Nopert. There are infinitely many ways to orient a shape, and a computer can only check finitely many. Researchers don’t know whether the holdouts are true Noperts or just shapes whose Rupert passages are hard to find.&lt;/p&gt;
    &lt;p&gt;What they do know is that candidate Noperts are incredibly rare. Starting last year, Murphy began to construct hundreds of millions of shapes. These include random polyhedra, polyhedra whose vertices lie on a sphere, polyhedra with special symmetries, and polyhedra in which he moved one vertex to intentionally mess up a previous Rupert passage. His algorithm easily found Rupert tunnels for nearly every one.&lt;/p&gt;
    &lt;p&gt;The contrast between these quick results and the stubbornness of the Nopert holdouts made some mathematicians suspect that true Noperts do exist. But until August, all they had were suspicions.&lt;/p&gt;
    &lt;head rend="h2"&gt;No Passage&lt;/head&gt;
    &lt;p&gt;Steininger, now 30, and Yurkevich, 29, have been friends since they participated together as teenagers in mathematics Olympiad competitions. Even though both eventually left academia (after a doctorate for Yurkevich and a master’s for Steininger), they have continued to explore unsolved problems together.&lt;/p&gt;
    &lt;p&gt;“We just had pizza three hours ago, and we talked about math almost the whole time,” Steininger told Quanta. “That’s what we do.”&lt;/p&gt;
    &lt;p&gt;Five years ago, the pair happened upon a YouTube video of one cube passing through another, and they were instantly smitten. They developed an algorithm to search for Rupert tunnels and soon became convinced that some shapes were Noperts. In a 2021 paper, they conjectured that the rhombicosidodecahedron is not Rupert. Their work, which preceded Murphy’s and Grimmer’s recent explorations, was, “I think, the first to conjecture that there might be solids that don’t have this property,” Steininger said.&lt;/p&gt;
    &lt;p&gt;If you want to prove that a shape is a Nopert, you must rule out Rupert tunnels for every possible orientation of the two shapes. Each orientation can be written down as a collection of rotation angles. This collection of angles can then be represented as a point in a higher-dimensional “parameter space.”&lt;/p&gt;
    &lt;p&gt;Florentina Stadlbauer; Courtesy of Jakob Steininger&lt;/p&gt;
    &lt;p&gt;Suppose you choose an orientation for your two shapes, and the computer tells you that the second shadow sticks out past the border of the first shadow. This rules out one point in the parameter space.&lt;/p&gt;
    &lt;p&gt;But you may be able to rule out much more than a single point. If the second shadow sticks out significantly, it would require a big change to move it inside the first shadow. In other words, you can rule out not just your initial orientation but also “nearby” orientations — an entire block of points in the parameter space. Steininger and Yurkevich came up with a result they called their global theorem, which quantifies precisely how large a block you can rule out in these cases. By testing many different points, you can potentially rule out block after block in the parameter space.&lt;/p&gt;
    &lt;p&gt;If these blocks cover the entire parameter space, you’ll have proved that your shape is a Nopert. But the size of each block depends on how far the second shadow sticks out beyond the first, and sometimes it doesn’t stick out very far. For instance, suppose you start with the two shapes in exactly the same position, and then you slightly rotate the second shape. Its shadow will at most stick out just a tiny bit past the first shadow, so the global theorem will only rule out a tiny box. These boxes are too small to cover the whole parameter space, leaving the possibility that some point you’ve missed might correspond to a Rupert tunnel.&lt;/p&gt;
    &lt;p&gt;To deal with these small reorientations, the pair came up with a complement to their global theorem that they called the local theorem. This result deals with cases where you can find three vertices (or corner points) on the boundary of the original shadow that satisfy some special requirements. For instance, if you connect those three vertices to form a triangle, it must contain the shadow’s center point. The researchers showed that if these requirements are met, then any small reorientation of the shape will create a shadow that pushes at least one of the three vertices further outward. So the new shadow can’t lie inside the original shadow, meaning it doesn’t create a Rupert tunnel.&lt;/p&gt;
    &lt;p&gt;If your shape casts a shadow that lacks three appropriate vertices, the local theorem won’t apply. And all the previously identified Nopert candidates have at least one shadow with this problem. Steininger and Yurkevich sifted through a database of hundreds of the most symmetric and beautiful convex polyhedra, but they couldn’t find any shape whose shadows all worked. So they decided to generate a suitable shape themselves.&lt;/p&gt;
    &lt;p&gt;They developed an algorithm to construct shapes and test them for the three-vertices property. Eventually, the algorithm produced the Noperthedron, which is made of 150 triangles and two regular 15-sided polygons. It looks like a rotund crystal vase with a wide base and top; one fan of the work has already 3D-printed a copy to use as a pencil holder.&lt;/p&gt;
    &lt;p&gt;Peter Lely&lt;/p&gt;
    &lt;p&gt;Steininger and Yurkevich then divided the parameter space of orientations into approximately 18 million tiny blocks, and tested the center point of each block to see if its corresponding orientation produced a Rupert passage. None of them did. Next, the researchers showed that each block satisfied either the local or global theorem, allowing them to rule out the entire block. Since these blocks fill out the entire parameter space, this meant that there is no Rupert passage through the Noperthedron.&lt;/p&gt;
    &lt;p&gt;The “natural conjecture has been proved false,” O’Rourke said.&lt;/p&gt;
    &lt;p&gt;It remains to be seen whether mathematicians can use the new method to generate other Noperts, or if they can find a different local theorem that can handle candidates like the rhombicosidodecahedron. But now that mathematicians know that Noperts do exist, “we’re on sound footing to study other shapes,” Murphy said.&lt;/p&gt;
    &lt;p&gt;Murphy, who like Steininger and Yurkevich has been exploring the question for its own sake, independent of his day job, feels a kinship across the centuries with Prince Rupert. “I like that he chose to use his retirement to do math and science in his castle,” he said.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Steininger and Yurkevich are on the lookout for new questions to tackle. “We’re just humble mathematicians — we love working on such problems,” Steininger said. “We’ll keep doing that.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/first-shape-found-that-cant-pass-through-itself-20251024/"/><published>2025-10-24T14:12:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45695134</id><title>Unlocking free WiFi on British Airways</title><updated>2025-10-25T19:32:24.408216+00:00</updated><content>&lt;doc fingerprint="2f3d91353c233d96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Unlocking free WiFi on British Airways&lt;/head&gt;
    &lt;p&gt;I was recently flying between HKG &amp;amp; LHR via British Airways. Iâd done the same flight back in 2023, and remember relying on the in-flight entertainment for the 14 hour journey. However, this time on my way to London, they had an interesting offer: Free WiFi for âMessagingâ, for members of âThe British Airways Clubâ.&lt;/p&gt;
    &lt;p&gt;I was pretty sure I wasnât a member of any sort of club (Iâm only flying economy anyway); but turns out this is just the name of their frequent flyer program. Conveniently enough, youâre able to sign up for this via the captive portal while in the sky; and although it asks for your E-Mail you donât need to verify it (thereby allowing you to complete the signup without access to the internet).&lt;/p&gt;
    &lt;p&gt;Once signed in, the captive portal invited me to âStart sessionâ, which true to itâs word, let me start texting people. I tried Whatsapp, Signal, Wechat and Discord. The first three worked (though not for images), Discord expectedly did not. Not bad for free wifi!&lt;/p&gt;
    &lt;head rend="h2"&gt;How does it know?&lt;/head&gt;
    &lt;p&gt;This was the first question I had as soon as I confirmed messaging did work. Itâs 2025; everything should be encrypted in transit. So how does it know if Iâm using Whatsapp vs. Discord? One idea I had is it just somehow capped the bandwidth / data transfer of individual TCP connections; so when youâre sending a single message or two it gets through, but something larger would fail.&lt;/p&gt;
    &lt;p&gt;To test this, I used my phone to open up the classic: example.com. Unfortunately this didnât load - so there mustâve been a bit more going onâ¦&lt;/p&gt;
    &lt;p&gt;Thankfully I had my laptop on me, so the next step was to connect to WiFi with the devtools open to the network tab, and wireshark on the side for good measure. After registering for the WiFi again, it was time to play around a bit. Opening up something like example.com revealed a TCP reset in the wireshark, right after the Client Hello, and my brain immediately jumped to SNI. Itâs something thatâs really annoyed me about the TLS spec since itâs widely used by ISPs in India to block websites (although there is work being done to fix this; ECH (which was itself previously ESNI)).&lt;/p&gt;
    &lt;p&gt;tl;dr SNI reveals the domain name of EVERY website you connect to in the TLS handshake, before the tunnel is established! Although the actual contents of what youâre doing, on say, totallynondodgywebsite.com are encrypted, anyone on the wire can see that you connected to it (including ISPs). My guess was that they had a set of whitelisted domains used by messaging apps, and if they see anything else, they just reset the connection.&lt;/p&gt;
    &lt;p&gt;Sidebar: peopleâs reactions when I try to tell this are always extremely varied. Many of my non-technical friends think anything you do without a VPN is visible to everyone, while some slightly technical ones still think that the URL (including query params) is visible, but the responses are not. Finally there is some subset of people who believe TLS means all data is encrypted in transit between client &amp;amp; server, though they had no idea SNI leaks all the domains they visit!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing out the theory&lt;/head&gt;
    &lt;p&gt;Although BA blocks DNS queries to all (well all I could remember) public resolvers, they do resolve any domain you throw at them, including MX, TXT, HTTPS records. (This itself could be an interesting area of exploration; especially since the DNS resolution can be triggered before signing up for free WiFi. Something along the lines of arbitrary subdomains which represent the request payload, and a custom nameserver that returns responses via the TXT record or something. Anywayâ¦).&lt;/p&gt;
    &lt;p&gt;Getting the A record of my personal server, I made a TLS handshake to the IP address directly, without any SNI. This was then reset by BA; so the lack of SNI is also blocked!&lt;/p&gt;
    &lt;code&gt;$ openssl s_client -connect 95.217.167.10:443
Connecting to 95.217.167.10
CONNECTED(00000003)
write:errno=104
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 0 bytes and written 302 bytes
Verification: OK
---
New, (NONE), Cipher is (NONE)
Protocol: TLSv1.3
This TLS version forbids renegotiation.
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)
---
&lt;/code&gt;
    &lt;p&gt;The next step was to try and test some SNI that might go through. Off the top of my head, I knew &lt;code&gt;wa.me&lt;/code&gt; was used by Whatsapp for some stuff, so I decided to use it. The way SNI works is it tells the server which host you want to connect to, so it can present the right TLS certificate. In my case, my server did not have any cert for &lt;code&gt;wa.me&lt;/code&gt; , but NGINX seemingly just ignores the SNI if it doesnât exist and returns the first cert (I think; could also be related to my config but I didnât look to much into this).&lt;/p&gt;
    &lt;p&gt;But basically, as long as I (the client) donât care, I can complete the TLS connection for any random cert the server offers me, even if in the SNI I provide a domain I donât control (e.g. &lt;code&gt;wa.me&lt;/code&gt; in this case).&lt;/p&gt;
    &lt;code&gt;$ openssl s_client -connect 95.217.167.10:443 -servername wa.me
Connecting to 95.217.167.10
CONNECTED(00000003)
depth=2 C=US, O=Internet Security Research Group, CN=ISRG Root X1
verify return:1
depth=1 C=US, O=Let's Encrypt, CN=R3
verify return:1
depth=0 CN=mijia.mywaifu.best
verify error:num=10:certificate has expired
notAfter=Jul 22 13:03:02 2023 GMT
verify return:1
depth=0 CN=mijia.mywaifu.best
notAfter=Jul 22 13:03:02 2023 GMT
verify return:1
---
Certificate chain
&amp;lt;snip&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Success! Using a Whatsapp SNI tricked BA into thinking Iâm âmessagingâ, which allowed the TLS tunnel to be established. Since I am connected to the server, to make sure it works I wrote an HTTP/1.1 request within the socket; using the host header of a real website on my NGINX instance&lt;/p&gt;
    &lt;code&gt;GET / HTTP/1.1
Host: saxrag.com

HTTP/1.1 200 OK
Server: nginx/1.18.0 (Ubuntu)
Date: Fri, 09 May 2025 19:14:46 GMT
Content-Type: text/html
Content-Length: 4968
Last-Modified: Wed, 09 Apr 2025 07:52:54 GMT
Connection: keep-alive
ETag: "67f62756-1368"
Cache-Control: no-cache
Accept-Ranges: bytes
&amp;lt;snip&amp;gt;
&lt;/code&gt;
    &lt;p&gt;I successfully managed to request and receive my homepage! All ~5KiB of it, not bad. Now the challenge was to extend this to browse any websiteâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Enemies to Lovers&lt;/head&gt;
    &lt;p&gt;Ok, my relationship with SNI isnât as cliche as that, and I think weâre still enemies. But this opens up some exciting opportunities to say the least. If I can convince BA that Iâm connecting to &lt;code&gt;wa.me&lt;/code&gt;, I can potentially do whatever I want over that connection (under the guise of âmessagingâ). So the requirments were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Establish a TLS connection using the SNI &lt;code&gt;wa.me&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tunnel arbitrary traffic through that connection&lt;/item&gt;
      &lt;item&gt;Do all this without actually owning / controlling the &lt;code&gt;wa.me&lt;/code&gt;domain&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From my past experiences w/ reverse-engineering etc., the most obvious way to do this seemed to be an HTTPS proxy. It had to be HTTPS specifically, since the connection to proxy was going to be what Iâd âfakeâ as Whatsapp. If the TLS handshake to the HTTPS proxy had the SNI &lt;code&gt;wa.me&lt;/code&gt; , BA should let it through, and then I can make the real requests I want via the proxy.&lt;/p&gt;
    &lt;p&gt;Unfortunately I was in the air, and without easy access to the internet to manage my servers and the like, I couldnât quite set all of this up; Iâd have to do that while on holiday and test it on the flight back. I could try and emulate the BA restrictions etc. while on thr ground, but I decided to YOLO it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Setup&lt;/head&gt;
    &lt;p&gt;I managed to find one of my VPSs that wasnât already using port 443. Letâs assume the public IP was &lt;code&gt;333.333.333.333&lt;/code&gt; (yes I know octets donât go beyond &lt;code&gt;0xFF&lt;/code&gt;, if you really want my IP check the screenshots below). I then setup an HTTP proxy on it using tinyproxy. However this just sets up a basic HTTP proxy, which was listening on &lt;code&gt;127.0.0.1:8080&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To add the TLS layer, I used stunnel. For the TLS setup of stunnel, I just generated some self-signed certs via openSSL using all defaults, except the common name (CN), for which I used &lt;code&gt;wa.me&lt;/code&gt;, since I wanted to try and ensure max compatibility (e.g. the client doesnât reject due to unexpected SNI vs. CN, or the server not knowing which cert to provide).&lt;/p&gt;
    &lt;code&gt;openssl req -nodes -newkey ed25519 -keyout ssl.key -x509 -days 365 -out ssl.crt
&lt;/code&gt;
    &lt;p&gt;UPDATE: actually, on the client I decided to ignore TLS errors (self-signed cert), and stunnel didnât care about SNI, so this (&lt;code&gt;CN&lt;/code&gt;) didnât matter too much. But for more legit use cases it definitely does!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing it out&lt;/head&gt;
    &lt;p&gt;To just make sure the proxy worked as expected, I tried it via curl directly on the IP:&lt;/p&gt;
    &lt;code&gt;$ curl -x https://user:pass@333.333.333.333:443 ifconfig.co -v
*   Trying 333.333.333.333:443...
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
*  CAfile: /etc/ssl/certs/ca-certificates.crt
*  CApath: none
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (OUT), TLS alert, unknown CA (560):
* SSL certificate problem: self-signed certificate
* closing connection #0
curl: (60) SSL certificate problem: self-signed certificate
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the webpage mentioned above.
&lt;/code&gt;
    &lt;p&gt;Of course! I just randomly generated the certs on my VPS, not signed by a âtrustedâ CA or anything. Well, we can tell cURL to ignore TLS errors for the proxy with the &lt;code&gt;--proxy-insecure&lt;/code&gt; flag, and now it works; the response is the IP of my VPS.&lt;/p&gt;
    &lt;p&gt;However thereâs a problem - if I connect to the proxy directly via the IP, there is no SNI extension set, so this would get blocked. The SNI extension is set when connection to a domain, so I need to configure &lt;code&gt;wa.me&lt;/code&gt; to point to &lt;code&gt;333.333.333.333&lt;/code&gt;. This can be done via the hosts file of course, but cURL also provides a quick CLI hack via &lt;code&gt;--resolve&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;curl --resolve wa.me:443:333.333.333.333 -x https://username:password@wa.me ifconfig.co --proxy-insecure -v
&lt;/code&gt;
    &lt;p&gt;This tells cURL how to resolve the IP. With this, I could now see the SNI being set to &lt;code&gt;wa.me&lt;/code&gt; via wireshark, and the connection to the proxy succeeding (TLS errors about the self-signed cert ignored of course). Not bad, now time to wait for my flight back to Hong Kongâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing it in-flight&lt;/head&gt;
    &lt;p&gt;If Iâd messed something up, I was cooked, since without internet access I wouldnât be able to fix it! My flight back was at 1935hrs local time, but Iâd been up since 0400 thanks to an early morning flight in from Edinburgh, and then spent the day browsing the markets, having pints and watching the Emilia Romagna Grand Prix. The place I went to even had screens above the urinals!&lt;/p&gt;
    &lt;p&gt;Anyway, despite being up for ~16 hours already, I was ready to see if my work would, well, work. Once we were sky-high, I connected to the WiFi (from my laptop), signed up for the BA loyalty program, and activated the âMessagingâ plan. Trying the curl command from above, I got back an HTTP 200 from &lt;code&gt;ifconfig.co&lt;/code&gt; with my VPS IP; it worked! For good measure I tried cURLing some more websites like example.com, google.com etc. to make sure stuff seemed fine.&lt;/p&gt;
    &lt;p&gt;The next challenge was to extend this to web browsing. Thankfully most modern browsers support sending traffic through an HTTPS proxy, and chromium even has a flag to disable TLS cert warnings (so it wonât complain about my self-signed cert, which obviously doesnât belong to the real &lt;code&gt;wa.me&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;I also had to set a DNS record for &lt;code&gt;wa.me&lt;/code&gt; to &lt;code&gt;333.333.333.333&lt;/code&gt; in my hosts file, so chromium would set the SNI to &lt;code&gt;wa.me&lt;/code&gt; in the TLS handshake, but the connection would be made to my VPS. Since the bandwidth would probably be quite limited (owing to not just the internet on an airplane, but proxying it via a VPS in Netherlands), I decided to load a very simple, text-heavy website: Hacker News.&lt;/p&gt;
    &lt;p&gt;Bada bing bada boom! Looks like we cooked. I can actually browse HN using BAâs free âmessagingâ WiFi! (Note: the reason you can see the HTTP requests in plaintext in wireshark is because I used SSLKEYLOGFILE and configured wireshark to decrypt TLS).&lt;/p&gt;
    &lt;p&gt;Unfortunately, trying to load websites with heavier assets would fail, with images on simple text blogs loading line-by-line. Well, at least its some dial-up nostalgia!&lt;/p&gt;
    &lt;p&gt;My guess is that on the free WiFi, apart from the SNI checks, they also throttle the bandwidth. Maybe they anticipated this kind of circumvention. On the other hand, if this is really the internet speed that the full plan unlocksâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: ECH&lt;/head&gt;
    &lt;p&gt;Earlier above I talked about work being done to fix the SNI leakage: ECH. The scope of explaining how it works is out of scope here, but I do encourage you to read up on it. Pretty good stuff! Itâll help this section make more sense.&lt;/p&gt;
    &lt;p&gt;I operate an ECH test website, so I decided to do some more setup before my flight. I basically created another ECHConfig, with the public_name set to &lt;code&gt;wa.me&lt;/code&gt;. Iâve a bit of a guide on how to do this btw, though it could do with some improvements.&lt;/p&gt;
    &lt;p&gt;Anyway, since ECH world, the public SNI is purely for the server to complete the outer ClientHello, and since ECH clients set the public SNI based on the ECHConfig, I can type in my real domain in firefox, which will still use the &lt;code&gt;wa.me&lt;/code&gt; domain as the public SNI. The inner Client Hello will then occur securely, containing the real SNI of &lt;code&gt;rfc5746.mywaifu.best&lt;/code&gt;, and the handshake will complete with the âlegitâ CA-signed certificate for that domain.&lt;/p&gt;
    &lt;p&gt;This worked as well, and without any TLS ignore flags, since the actual cert for &lt;code&gt;rfc5746.mywaifu.best&lt;/code&gt; was signed by a âtrusted CAâ (Letâs Encrypt). Whatâs more interesting is that this worked even on a non-standart TLS port: &lt;code&gt;7443&lt;/code&gt;! Not sure exactly why, but Iâm not complaining.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on ECHConfig resolution&lt;/head&gt;
    &lt;p&gt;Typically, ECHConfigs should be resolved via encrypted DNS, such as DNS-over-HTTPS. I believe this is what firefox does by default. I am not 100% sure if this is what happened while I was in flight, since Iâd think the DoH would be blocked on messaging WiFi? Or maybe they allow the DoH SNI as well, since newer phones default to that. If any of you are flying BA anytime soon, try it out and let me know!&lt;/p&gt;
    &lt;head rend="h2"&gt;SNI: Donât blindly trust it&lt;/head&gt;
    &lt;p&gt;SNI, as the name indicates (sorry) is just a âhintâ of sorts, from the client to the server. If someone controls both sides (client &amp;amp; server), they can put whatever fake value they want in here, for middleboxes to sniff out and try to analyze. While this unfortunately does work for applications like censorship (where an ISP or country is trying to block a particular website), for use cases such as threat detection it should not be relied on; malwre authors can âspoofâ the SNI when connecting to their C&amp;amp;C, since they donât actually need it, but it may look more innocent to middleboxes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Questions?&lt;/head&gt;
    &lt;p&gt;I would be happy to answer any questions you have! You can contact me via email, and please use my PGP key to encrypt all communications. (Backup E-Mail (PGP Key))&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.saxrag.com/tech/reversing/2025/06/01/BAWiFi.html"/><published>2025-10-24T14:40:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45695621</id><title>Code like a surgeon</title><updated>2025-10-25T19:32:24.223728+00:00</updated><content>&lt;doc fingerprint="355491dd119110ed"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;October 2025&lt;/head&gt;
    &lt;head rend="h1"&gt;Code like a surgeon&lt;/head&gt;
    &lt;p&gt;A lot of people say AI will make us all “managers” or “editors”…but I think this is a dangerously incomplete view!&lt;/p&gt;
    &lt;p&gt;Personally, I’m trying to code like a surgeon.&lt;/p&gt;
    &lt;p&gt;A surgeon isn’t a manager, they do the actual work! But their skills and time are highly leveraged with a support team that handles prep, secondary tasks, admin. The surgeon focuses on the important stuff they are uniquely good at.&lt;/p&gt;
    &lt;p&gt;My current goal with AI coding tools is to spend 100% of my time doing stuff that matters. (As a UI prototyper, that mostly means tinkering with design concepts.)&lt;/p&gt;
    &lt;p&gt;It turns out there are a LOT of secondary tasks which AI agents are now good enough to help out with. Some things I’m finding useful to hand off these days:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Before attempting a big task, write a guide to relevant areas of the codebase&lt;/item&gt;
      &lt;item&gt;Spike out an attempt at a big change. Often I won’t use the result but I’ll review it as a sketch of where to go&lt;/item&gt;
      &lt;item&gt;Fix typescript errors or bugs which have a clear specification&lt;/item&gt;
      &lt;item&gt;Write documentation about what I’m building&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I often find it useful to run these secondary tasks async in the background – while I’m eating lunch, or even literally overnight!&lt;/p&gt;
    &lt;p&gt;When I sit down for a work session, I want to feel like a surgeon walking into a prepped operating room. Everything is ready for me to do what I’m good at.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mind the autonomy slider&lt;/head&gt;
    &lt;p&gt;Notably, there is a huge difference between how I use AI for primary vs secondary tasks.&lt;/p&gt;
    &lt;p&gt;For the core design prototyping work, I still do a lot of coding by hand, and when I do use AI, I’m more careful and in the details. I need fast feedback loops and good visibility. (eg, I like Cursor tab-complete here)&lt;/p&gt;
    &lt;p&gt;Whereas for secondary tasks, I’m much much looser with it, happy to let an agent churn for hours in the background. The ability to get the job done eventually is the most important thing; speed and visibility matter less. Claude Code has been my go-to for long unsupervised sessions but Codex CLI is becoming a strong contender there too, possibly my new favorite.&lt;/p&gt;
    &lt;p&gt;These are very different work patterns! Reminds me of Andrej Karpathy’s “autonomy slider” concept. It’s dangerous to conflate different parts of the autonomy spectrum – the tools and mindset that are needed vary quite a lot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your agent doesn’t need a career trajectory&lt;/head&gt;
    &lt;p&gt;The “software surgeon” concept is a very old idea – Fred Brooks attributes it to Harlan Mills in his 1975 classic “The Mythical Man-Month”. He talks about a “chief programmer” who is supported by various staff including a “copilot” and various administrators. Of course, at the time, the idea was to have humans be in these support roles.&lt;/p&gt;
    &lt;p&gt;OK, so there is a super obvious angle here, that “AI has now made this approach economically viable where it wasn’t before”, yes yes… but I am also noticing a more subtle thing at play, something to do with status hierarchies.&lt;/p&gt;
    &lt;p&gt;A lot of the “secondary” tasks are “grunt work”, not the most intellectually fulfilling or creative part of the work. I have a strong preference for teams where everyone shares the grunt work; I hate the idea of giving all the grunt work to some lower-status members of the team. Yes, junior members will often have more grunt work, but they should also be given many interesting tasks to help them grow.&lt;/p&gt;
    &lt;p&gt;With AI this concern completely disappears! Now I can happily delegate pure grunt work. And the 24/7 availability is a big deal. I would never call a human intern at 11pm and tell them to have a research report on some code ready by 7am… but here I am, commanding my agent to do just that!&lt;/p&gt;
    &lt;head rend="h2"&gt;Notion is for surgeons?&lt;/head&gt;
    &lt;p&gt;Finally I’ll mention a couple thoughts on how this approach to work intersects with my employer, Notion.&lt;/p&gt;
    &lt;p&gt;First, as an employee, I find it incredibly valuable right now to work at a place that is bullish on AI coding tools. Having support for heavy use of AI coding tools, and a codebase that’s well setup for it, is enabling serious productivity gains for me – especially as a newcomer to a big codebase.&lt;/p&gt;
    &lt;p&gt;Secondly, as a product – in a sense I would say we are trying to bring this way of working to a broader group of knowledge workers beyond programmers. When I think about how that will play out, I like the mental model of enabling everyone to “work like a surgeon”.&lt;/p&gt;
    &lt;p&gt;The goal isn’t to delegate your core work, it’s to identify and delegate the secondary grunt work tasks, so you can focus on the main thing that matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related reads&lt;/head&gt;
    &lt;p&gt;If you liked this perspective, you might enjoy reading these other posts I’ve written about the nature of human-AI collaboration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enough AI copilots! We need AI HUDs: “anyone serious about designing for AI should consider non-copilot form factors that more directly extend the human mind…”&lt;/item&gt;
      &lt;item&gt;AI-generated tools can make programming more fun: “Instead, I used AI to build a custom debugger UI… which made it more fun for me to do the coding myself…”&lt;/item&gt;
      &lt;item&gt;ChatGPT as muse, not oracle: “What if we were to think of LLMs not as tools for answering questions, but as tools for asking us questions and inspiring our creativity?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.geoffreylitt.com/2025/10/24/code-like-a-surgeon"/><published>2025-10-24T15:25:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698554</id><title>Harnessing America's heat pump moment</title><updated>2025-10-25T19:32:23.381734+00:00</updated><content>&lt;doc fingerprint="676f107f6a85b4ba"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Heat Pumped&lt;/item&gt;
      &lt;item&gt;Posts&lt;/item&gt;
      &lt;item&gt;Harnessing America’s Heat Pump Moment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Harnessing America’s Heat Pump Moment&lt;/head&gt;
    &lt;head rend="h2"&gt;The tech works. The policy’s in place. So why are heat pumps still a hard sell?&lt;/head&gt;
    &lt;p&gt;Editor’s note: This is a guest post by Joseph DeNatale, an entrepreneur and project coordinator at Jetson Home. It originally appeared in Climate Drift earlier this year, and is republished on Heat Pumped with permission.&lt;/p&gt;
    &lt;p&gt;Joseph interviewed me when he was researching the piece, and I was excited to see that the final product touched many topics that I've been wanting to write about.&lt;/p&gt;
    &lt;p&gt;A big thank you to Joseph and Climate Drift for sharing with the Heat Pumped community - it's incredibly in-depth. Since there’s so much to digest, we’re splitting it up into 5 parts that we'll be sharing over the next few weeks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Execution Is Everything: A Personal Perspective&lt;/head&gt;
    &lt;p&gt;As a small business owner, I’ve built a career not around inventing new things, but around making things happen: making sure systems run smoothly, projects get completed on time, and clients feel taken care of.&lt;/p&gt;
    &lt;p&gt;My work has been rooted in the real-world, hands-on, often chaotic rhythm of operations, logistics, and direct client service. Whether it’s organizing teams to execute live events, refining workflows to scale a growing business, or managing the delicate art of closing a sale, I’ve learned one simple truth: the hardest part is never the idea. It’s the execution.&lt;/p&gt;
    &lt;p&gt;So when I began diving into the world of home electrification—particularly heat pumps—that same truth surfaced again, just with higher stakes.&lt;/p&gt;
    &lt;p&gt;The technology isn’t the issue. In fact, the technology is there. It’s been there for decades, and it is continuing to improve. We’re not waiting on some magical breakthrough or futuristic device.&lt;/p&gt;
    &lt;p&gt;We’re waiting on people—mostly homeowners and home contractors, but also manufacturers and policy makers—to embrace, understand, and implement what already works.&lt;/p&gt;
    &lt;p&gt;This piece isn’t about reinventing the wheel. It’s about understanding why we’re not using the wheel we already have—and what it’s going to take, from the human side of the equation, to make heat pumps the obvious, accessible, and default choice for millions of American homes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heat Pumps Aren’t New—But This Moment Is&lt;/head&gt;
    &lt;p&gt;In the world of climate solutions, it’s easy to get distracted by what’s shiny and new—sleek devices, breakthrough technologies, futuristic models of sustainability.&lt;/p&gt;
    &lt;p&gt;But not every climate solution is some new-fangled wonder gadget. Some of them already exist. Some of them are sitting in basements and behind houses, quietly doing the work.&lt;/p&gt;
    &lt;p&gt;The heat pump is one of them.&lt;/p&gt;
    &lt;p&gt;Heat pumps are not new. In fact, the idea has been around for well over a century, and the technology has been used widely for decades—mostly in Europe and Asia, but also in pockets of the U.S.—for everything from water heating to whole-home climate control.&lt;/p&gt;
    &lt;p&gt;Modern heat pumps are highly efficient—anywhere from 2-4x more efficient than a furnace—and capable of replacing both a furnace and an air conditioner with a single system in virtually every climate. For millions of homes across the country, they offer a cleaner, quieter, and more precise way to stay comfortable year-round.&lt;/p&gt;
    &lt;p&gt;Importantly, heat pumps have also been shown to match or beat the operating costs of even the cheapest heating option—natural gas—in many cases. This has been demonstrated through both local and national studies. One study showed that over 90% of American households would save on energy bills by replacing worn-out heating equipment with the right-sized heat pump.&lt;/p&gt;
    &lt;p&gt;Installation costs vary wildly depending on many factors in a home, but with the introduction of generous incentives via the Inflation Reduction Act (IRA) and additional state programs, even these costs can be on-par with fossil fuel alternatives.&lt;/p&gt;
    &lt;p&gt;So why aren’t they everywhere?&lt;/p&gt;
    &lt;p&gt;The answer isn’t technical. It’s cultural, economic, and human.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Heat pumps are proven, efficient, and climate-friendly—but adoption is still slow.&lt;/p&gt;
    &lt;p&gt;The barrier isn’t the tech. It’s people:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Contractors who default to what they know&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Homeowners who need education and guidance&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fragmented market full of noise and misinformation&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This piece discusses these challenges, and then explores five keys to accelerating adoption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Educate homeowners so heat pumps feel familiar and trustworthy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Train the next-gen workforce and upskill legacy HVAC pros.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Leverage better tools and data to size and install systems right.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prioritize quality and trust to build social proof and demand.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Align policy to phase out one-way ACs and normalize heat pumps.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Execution—not invention—is what will move the needle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hold On.. What’s A Heat Pump Again?&lt;/head&gt;
    &lt;p&gt;If you’re reading this piece, you probably know what a heat pump is (and you can feel free to skip this section).&lt;/p&gt;
    &lt;p&gt;But if you’re among the uninitiated – like, believe it or not, most people – here’s a (very) quick primer. (Editor’s note: check out Heat Pumps 101 if you want to dive deeper)&lt;/p&gt;
    &lt;p&gt;A heat pump works by drawing thermal energy (heat) out of the atmosphere and “pumping” it into the home. This process works in reverse for cooling. (Source)&lt;/p&gt;
    &lt;head rend="h3"&gt;The 2-Way AC&lt;/head&gt;
    &lt;p&gt;The term “heat pump”, it turns out, is a fairly unhelpful name for most people. In fact, there are some leaders in the home electrification industry who believe the name itself is one of the barriers to adoption. It’s one of many ways that the heat pump is misunderstood.&lt;/p&gt;
    &lt;p&gt;Think of a heat pump as a “2-way AC.” An air conditioner cools your home by pulling heat from inside an enclosed space and transferring it outside. Your refrigerator works the same way.&lt;/p&gt;
    &lt;p&gt;A heat pump does the same thing, but can also reverse the process to bring heat into the home. It uses a few key components to make this happen:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The fan pulls air across the system’s coils to help move heat in or out of the space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The evaporator coil absorbs heat from the air inside your home (in cooling mode) or from the outside air (in heating mode).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The compressor pressurizes and moves a fluid called refrigerant through the system, enabling the heat transfer process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The refrigerant is the working fluid that captures and carries heat from one place to another—either out of your home or into it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s important to understand is that a heat pump does not create heat. It also doesn’t create cold (cold is the absence of heat, just like darkness is the absence of light). A heat pump simply transfers – pumps! – heat from one place to another.&lt;/p&gt;
    &lt;p&gt;“The difference between a heat pump and a one-way AC is just one valve. It still works perfectly fine as an air conditioner—there’s no difference. That’s why we’ve started calling them “two-way ACs” as an education tool. It helps people compare a two-way AC, which has a reverse gear, with a one-way AC—which, in my mind, is basically broken.”&lt;/p&gt;
    &lt;p&gt;But what about in the winter when it’s below freezing? In any environment where the temperature is above absolute zero (remember the Kelvin scale?) there is still a significant amount of heat in the air in the form of thermal energy.&lt;/p&gt;
    &lt;p&gt;That’s why a heat pump can still heat your home even on the coldest day of the year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Heat Pumps Matter&lt;/head&gt;
    &lt;p&gt;The fact that heat pumps simply transfer heat—and do not create it—gives them the potential to heat homes without doing the thing that humans have done since time immemorial to keep warm: burn stuff.&lt;/p&gt;
    &lt;p&gt;In the U.S., over half of all homes still rely on burning fossil fuels for heat. Replacing those systems with electric, air source heat pumps (ASHPs) can significantly reduce household emissions, especially as the grid gets cleaner and moves towards a higher percentage of renewable energy (i.e. not burning stuff).&lt;/p&gt;
    &lt;p&gt;And, because they’re so efficient, heat pumps can lower operating costs over time—although this is highly dependent on where you live, as the cost of fuel and electricity varies widely. They’re also safer (no burning stuff), can improve indoor air quality (again, no burning), and create healthier, more comfortable homes.&lt;/p&gt;
    &lt;p&gt;Finally, heat pumps are a crucial component of an energy-independent home. Paired with solar panels and battery storage, a homeowner can heat and cool their home entirely with energy they generate on their own. Try that with a furnace!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Metric&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Gas Furnace&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Air-Source Heat Pump (ASHP)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Fuel Source&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Natural gas, propane, or oil&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Electricity&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Heating/Cooling&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Heating only&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Heats and cools (dual function)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Air Quality&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Can introduce combustion byproducts; potential for CO&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;No combustion; generally better indoor air quality&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Health/Safety&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Risk of gas leaks, carbon monoxide&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;No combustion risk; safer for indoor environments&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Comfort&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Delivers blasts of hot air; on/off “short cycles”&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;More consistent, even heating/cooling with variable-speed options&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Initial Cost&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Typically lower (although the cost of a furnace + AC if replaced at the same time is often higher)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Often higher upfront, especially for cold-climate models. Costs can be lowered via incentive programs.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Operating Cost&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Depends on gas prices; cheaper where gas is low&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Can be lower, especially with efficient models + incentives and/or when paired with solar + battery storage&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Emissions&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Emits CO₂ and other GHGs&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Zero onsite emissions; cleaner with a green grid&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Climate Suitability&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Performs well in all climates&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cold-climate models now perform down to -15 to -20°F&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Incentives/Rebates&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Limited (varies by region)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Significant federal/state incentives available&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is not a marginal climate solution. According to the IEA, global heat pump adoption could reduce carbon emissions by half a billion tons annually—roughly equivalent to the annual emissions of all cars in Europe.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Heat Pump Moment Has Arrived&lt;/head&gt;
    &lt;p&gt;For years, heat pumps were a niche topic, something discussed by green building enthusiasts, early adopters, or homeowners with unusually high energy awareness.&lt;/p&gt;
    &lt;p&gt;But that’s no longer the case. Here are four reasons why:&lt;/p&gt;
    &lt;head rend="h3"&gt;Cultural Momentum Is Building&lt;/head&gt;
    &lt;p&gt;The electrification movement is no longer a fringe concept. The push to “electrify everything” has gained traction among policymakers, climate advocates, startups, utilities, and even popular media.&lt;/p&gt;
    &lt;p&gt;From Substack newsletters to YouTube explainers, there’s growing awareness that building decarbonization—and especially heating and cooling—is one of the most practical, scalable ways for regular people to cut their emissions. Campaigns like Rewiring America’s “Go Electric” initiative frame heat pumps not just as energy-efficient appliances, but as a gateway to modern, climate-aligned homes.&lt;/p&gt;
    &lt;p&gt;This momentum is turning into real action. Heat pumps have now outsold gas furnaces in the U.S. every year since 2022.&lt;/p&gt;
    &lt;head rend="h3"&gt;Federal and State Policy Is Aligned (For Now)&lt;/head&gt;
    &lt;p&gt;For the time being (Republicans’ “One Big, Beautiful Bill” notwithstanding), both federal and state governments are backing this transition with significant financial and structural support. Editor’s note: Ouch. Since this piece was originally written, OBBB passed, and most tax credits at the federal level phase out at the end of this year. If you’ve been on the fence about getting a heat pump, now might be a good time to act!&lt;/p&gt;
    &lt;p&gt;The Inflation Reduction Act (IRA) has introduced a suite of rebates, tax credits, and grant programs designed to make heat pumps more affordable and accessible. Single-family households can receive up to $8,000 in upfront rebates for heat pump installations and up to $2,000 in federal tax credits, not to mention additional support for electrical panel upgrades and home energy audits. Editor’s note: the IRA rebates are federally funded, but implemented at the state level. Not all states are participating, and some that are haven’t rolled out their programs yet. In other states like California, the funds are already exhausted.&lt;/p&gt;
    &lt;p&gt;State and local governments are also leading the way in the transition away from fossil fuels on both the demand and supply sides. Programs like Efficiency Maine, TECH Clean California, and Mass Save offer generous incentives and no-interest financing to homeowners that drive the cost of electrification upgrades down even further. Meanwhile, New York City has banned gas in new construction, and Massachusetts has ordered public utilities to begin phasing out natural gas, a move which is being studied in at least 11 other states.&lt;/p&gt;
    &lt;head rend="h3"&gt;Private Capital Is Following&lt;/head&gt;
    &lt;p&gt;The heat pump space is no longer just a niche for contractors and utilities—it’s attracting serious private investment. VC-backed companies like Quilt are reimagining the user experience with sleek, design-forward equipment and app-based controls. Others, like Elephant Energy and Forge, are building “heat pump concierge” platforms that manage the customer journey end-to-end—from sales to install to rebate navigation.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Cold-Climate Performance Myth Has Been Fully Debunked&lt;/head&gt;
    &lt;p&gt;One of the biggest myths about heat pumps—that they can’t handle cold weather—is now being debunked at scale. While older, single-speed models may have struggled in colder temperatures, especially when size and installed incorrectly, modern cold-climate, variable-speed air-source heat pumps can provide reliable heating even at outdoor temperatures of -20°F.&lt;/p&gt;
    &lt;p&gt;These systems are already in use in northern New England, the upper Midwest, and Canada. In Nordic countries—some of the coldest climates in the word—the technology has been viable for decades.&lt;/p&gt;
    &lt;p&gt;And yet, despite all this momentum, heat pump adoption is still slow.&lt;/p&gt;
    &lt;p&gt;Why? Because the hardest part isn’t scaling the technology. It’s aligning the people—contractors, homeowners, policymakers, and market actors—who need to make it happen.&lt;/p&gt;
    &lt;p&gt;“We’ve had the technology dialed for 20, 30, 40 years, depending on how you’re arguing it—but it’s not being applied. It’s a human problem. It’s not a technical one. The technical one has been solved.”&lt;/p&gt;
    &lt;p&gt;That’s where we go next.&lt;/p&gt;
    &lt;p&gt;This is part 1 in a 5 part series about challenges and solutions in accelerating heat pump adoption across the US. Stay tuned for the next issue!&lt;/p&gt;
    &lt;head rend="h2"&gt;Want a heat pump in your own home?&lt;/head&gt;
    &lt;p&gt;The first Heat Pumped group buy generated lots of enthusiasm! There are still a handful of slots left, but you’ll have to act fast if you’re interested. Sign-ups close later this month (or when all the slots fill, whichever comes first).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Do you want to participate in this group buy?&lt;/head&gt;
          &lt;p&gt;Fair &amp;amp; transparent heat pump pricing in the SF Bay Area and LA&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.heatpumped.org/p/harnessing-america-s-heat-pump-moment"/><published>2025-10-24T20:05:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698570</id><title>The Swift SDK for Android</title><updated>2025-10-25T19:32:23.208129+00:00</updated><content>&lt;doc fingerprint="360e51139294ee0b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing the Swift SDK for Android&lt;/head&gt;
    &lt;p&gt;Swift has matured significantly over the past decade — extending from cloud services to Windows applications, browser apps, and microcontrollers. Swift powers apps and services of all kinds, and thanks to its great interoperability, you can share code across platforms.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is an open group, free for anyone to join, that aims to expand Swift to Android. Today, we are pleased to announce nightly preview releases of the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;This milestone reflects months of effort by the Android workgroup, building on many years of grassroots community effort. With the SDK, developers can begin developing Android applications in Swift, opening new avenues for cross-platform development and accelerating innovation across the mobile ecosystem.&lt;/p&gt;
    &lt;p&gt;The Swift SDK for Android is available today, bundled with the Windows installer or downloadable separately for use on Linux or macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;p&gt;We’ve published a Getting Started guide to help you set up your first native Swift code on an Android device. The Swift for Android Examples help demonstrate end‑to‑end application workflows on Android.&lt;/p&gt;
    &lt;p&gt;With the Swift SDK for Android, you can now start porting your Swift packages to Android. Over 25% of packages in the Swift Package Index already build for Android, and the Community Showcase now indicates Android compatibility.&lt;/p&gt;
    &lt;p&gt;The swift-java project enables you to interoperate between Java and Swift. It is both a library and a code generator, enabling you to integrate Swift and Java in both directions by automatically generating safe and performant bindings. To learn about generating bindings to bring your business logic to Android, check out the recent Swift Server Side meetup talk by Mads Odgaard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;This preview release opens many new opportunities to continue improving these tools. We encourage you to share your experiences, ideas, tools and apps on the Swift forums. This post has been published on an associated thread for discussion, and new posts can be shared in the Android category.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is drafting a vision document, currently under review, for directing future work regarding Swift on Android. This vision will outline priority areas and guide community efforts to maximize impact across the ecosystem. In addition, we maintain a project board that tracks the status of major efforts, as well as official CI for the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;If you’re as excited as we are, join us and help make this ecosystem even better!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.swift.org/blog/nightly-swift-sdk-for-android/"/><published>2025-10-24T20:06:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698909</id><title>Study: MRI contrast agent causes harmful metal buildup in some patients</title><updated>2025-10-25T19:32:23.026072+00:00</updated><content>&lt;doc fingerprint="eeda31d2584a08f1"&gt;
  &lt;main&gt;&lt;p&gt;Editor's Note&lt;/p&gt;&lt;p&gt;New research offers a potential explanation for why some patients retain toxic metals long after undergoing an MRI.&lt;/p&gt;&lt;p&gt;Published in the journal Magnetic Resonance Imaging, the findings show that gadolinium contrast agents used in MRI scans may react with common dietary compounds to form harmful metal nanoparticles in the body. As detailed in an April 7 Newsweek report on the study, gadolinium-based contrast agents are injected to sharpen MRI images and are typically excreted without causing harm. However, gadolinium particles have been found lingering in the brain, kidneys, blood, and urine years after exposure, and the US Food and Drug Administration links retained gadolinium to nephrogenic systemic fibrosis (NSF).&lt;/p&gt;&lt;p&gt;The study specifically identifies a chemical reaction between gadolinium and oxalic acid—a compound found naturally in foods and produced in the body after ingesting vitamin C—as a likely contributor, Newsweek reports. Lab tests showed oxalic acid caused gadolinium to separate from its chelating agent and form nanoparticles capable of infiltrating cells in various organs.&lt;/p&gt;&lt;p&gt;Lead author Dr Brent Wagner told Newsweek he personally avoids vitamin C when undergoing MRI with contrast, citing its potential to increase gadolinium reactivity. “Metabolic milieu,” including high oxalic acid levels, could explain why some individuals experience severe symptoms while others do not, he said.&lt;/p&gt;&lt;p&gt;According to the article, nearly half of the patients found to have gadolinium traces in the body had received the contrast agent only once, suggesting that individual biology—not dosage—may influence risk. Dr Wagner theorized that nanoparticle formation could trigger a disproportionate immune response, with affected cells sending distress signals that intensify the body’s reaction.&lt;/p&gt;&lt;p&gt;The research team is now building an international patient registry to further study gadolinium accumulation. According to the article, the registry will collect blood, urine, hair, and fingernail samples to help identify individuals at greatest risk and understand long-term retention patterns.&lt;/p&gt;Read More &amp;gt;&amp;gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ormanager.com/briefs/study-mri-contrast-agent-causes-harmful-metal-buildup-in-some-patients/"/><published>2025-10-24T20:48:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45700663</id><title>What is intelligence? (2024)</title><updated>2025-10-25T19:32:22.751320+00:00</updated><content>&lt;doc fingerprint="9bbe08afcd469a02"&gt;
  &lt;main&gt;
    &lt;p&gt;Contents Close What is Intelligence? Foreword Preface Introduction Origins Abiogenesis Symbiogenesis Reproductive Functions Life as Computation Artificial Life Thermodynamics Dynamic Stability Complexification Virality Compression Embodiment Daisyworld Élan Vital Survival Being in Time Batting Average (No) Things in Themselves Anthropic Principle The Umwelt Within Latent Variables Modeling Learning by Evolving Cause by Effect Goodness and Truth Are Feelings Real? Interlude The Prehistory of Computation Cybernetics Love and War Killer App Behavior, Purpose, and Teleology Negative Feedback How We Know Universals Perceptrons Deep Learning Closing the Loop Learning Unkneading Transfer Green Screen Grandmother Cell Final Causes Meathead Neuromodulators Bootstrapping Beyond Reward Other Minds Forking Paths Children of Time Sphexish Matryoshka Dolls Intelligence Explosion Crew of Eight Homunculus Illusion and Reality Many Worlds Au Revoir Will What You Will What It Is Like to Be Weird Entanglement Zombie-Free Alters M-I-B The Interpreter Multifractal Boundaries Ourselves Block Diagram Recurrence Efference Copy Phenomenality Blindsight Subbasement Neocortex Social Neuroscience Transformers Language Sequence to Sequence Prediction Is All You Need Semantic Cosmology Alignment Attention But Is It Neuroscience? No Introspection Step by Step Generality Single System Hive Mind Modalities Pure Speech Babel Fish Testament Long Tails In-Context Learning Mary’s Room Parity Check As If Interlude No Perfect Heroes or Villains Evolutionary Transition Periodization Transitions Vulnerability Pecking Order Economics X-Risk Free Lunch Utility Big Tent Limits to Growth Tears of Joy Beyond Alignment Acknowledgments About the Author The Antikythera Book Series Glossary Bibliography Foreword Blaise Agüera y Arcas Lessons from AI About Evolution, Computing, and Minds ' Foreword&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://whatisintelligence.antikythera.org/"/><published>2025-10-25T01:21:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45700946</id><title>Key IOCs for Pegasus and Predator Spyware Removed with iOS 26 Update</title><updated>2025-10-25T19:32:22.527704+00:00</updated><content>&lt;doc fingerprint="f60d4907cab1114d"&gt;
  &lt;main&gt;
    &lt;p&gt;Blog&lt;/p&gt;
    &lt;head rend="h1"&gt;Key IOCs for Pegasus and Predator Spyware Cleaned With iOS 26 Update&lt;/head&gt;
    &lt;p&gt;By Matthias Frielingsdorf, VP of Research&lt;/p&gt;
    &lt;p&gt;Oct 21, 2025&lt;/p&gt;
    &lt;p&gt;As iOS 26 is being rolled out, our team noticed a particular change in how the operating system handles the shutdown.log file: it effectively erases crucial evidence of Pegasus and Predator spyware infections. This development poses a serious challenge for forensic investigators and individuals seeking to determine if their devices have been compromised at a time when spyware attacks are becoming more common.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;The Power of the shutdown.log&lt;/head&gt;
    &lt;p&gt;For years, the shutdown.log file has been an invaluable, yet often overlooked, artifact in the detection of iOS malware. Located within the Sysdiagnoses in the Unified Logs section (specifically, Sysdiagnose Folder -&amp;gt; system_logs.logarchive -&amp;gt; Extra -&amp;gt; shutdown.log), it has served as a silent witness to the activities occurring on an iOS device, even during its shutdown sequence.&lt;/p&gt;
    &lt;p&gt;In 2021, the publicly known version of Pegasus spyware was found to leave discernible traces within this shutdown.log. These traces provided a critical indicator of compromise, allowing security researchers to identify infected devices. However, the developers behind Pegasus, NSO Group, are constantly refining their techniques, and by 2022 Pegasus had evolved.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Pegasus's Evolving Evasion Tactics&lt;/head&gt;
    &lt;p&gt;While still leaving evidence in the shutdown.log, their methods became more sophisticated. Instead of leaving obvious entries, they began to completely wipe the shutdown.log file. Yet, even with this attempted erasure, their own processes still left behind subtle traces. This meant that even a seemingly clean shutdown.log that began with evidence of a Pegasus sample was, in itself, an indicator of compromise. Multiple cases of this behavior were observed until the end of 2022, highlighting the continuous adaptation of these malicious actors.&lt;/p&gt;
    &lt;p&gt;Following this period, it is believed that Pegasus developers implemented even more robust wiping mechanisms, likely monitoring device shutdown to ensure a thorough eradication of their presence from the shutdown.log. Researchers have noted instances where devices known to be active had their shutdown.log cleared, alongside other IOCs for Pegasus infections. This led to the conclusion that a cleared shutdown.log could serve as a good heuristic for identifying suspicious devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Predator's Similar Footprint&lt;/head&gt;
    &lt;p&gt;The sophisticated Predator spyware, observed in 2023, also appears to have learned from the past. Given that Predator was actively monitoring the shutdown.log, and considering the similar behavior seen in earlier Pegasus samples, it is highly probable that Predator, too, left traces within this critical log file.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;iOS 26: An Unintended Cleanse&lt;/head&gt;
    &lt;p&gt;With iOS 26 Apple introduced a changeâeither an intentional design decision or an unforeseen bugâthat causes the shutdown.log to be overwritten on every device reboot instead of appended with a new entry every time, preserving each as its own snapshot. This means that any user who updates to iOS 26 and subsequently restarts their device will inadvertently erase all evidence of older Pegasus and Predator detections that might have been present in their shutdown.log.&lt;/p&gt;
    &lt;p&gt;This automatic overwriting, while potentially intended for system hygiene or performance, effectively sanitizes the very forensic artifact that has been instrumental in identifying these sophisticated threats. It could hardly come at a worse time - spyware attacks have been a constant in the news and recent headlines show that high-power executives and celebrities, not just civil society, are being targeted.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Identifying Pegasus 2022: A Specific IOC&lt;/head&gt;
    &lt;p&gt;For those still on iOS versions prior to 26, a specific IOC for Pegasus 2022 infections involved the presence of a /private/var/db/com.apple.xpc.roleaccountd.staging/com.apple.WebKit.Networking entry within the shutdown.log. This particular IOC also revealed a significant shift in NSO Group's tactics: they began using normal system process names instead of easily identifiable, similarly named processes, making detection more challenging.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Correlating Logs for Deeper Insight (&amp;lt; iOS 18)&lt;/head&gt;
    &lt;p&gt;For devices running iOS 18 or earlier, a more comprehensive approach to detection involved correlating containermanagerd log entries with shutdown.log events. Containermanagerd logs contain boot events and can retain data for several weeks. By comparing these boot events with shutdown.log entries, investigators could identify discrepancies. For example, if numerous boot events were observed before shutdown.log entries, it suggested that something was amiss and potentially being hidden.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Before You Update&lt;/head&gt;
    &lt;p&gt;Given the implications of iOS 26's shutdown.log handling, it is crucial for users to take proactive steps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Before updating to iOS 26, immediately take and save a sysdiagnose of your device. This will preserve your current shutdown.log and any potential evidence it may contain.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Consider holding off on updating to iOS 26 until Apple addresses this issue, ideally by releasing a bug fix that prevents the overwriting of the shutdown.log on boot.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;More Blogs&lt;/head&gt;
    &lt;head rend="h3"&gt;Get Our Latest Blog Posts Delivered Straight to Your Inbox&lt;/head&gt;
    &lt;p&gt;Subscribe to our blog to receive the latest research and industry trends delivered straight to your inbox. Our blog content covers sophisticated mobile threats, unpatched vulnerabilities, smishing, and the latest industry news to keep you informed and secure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://iverify.io/blog/key-iocs-for-pegasus-and-predator-spyware-cleaned-with-ios-26-update"/><published>2025-10-25T02:31:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45701305</id><title>Meet the real screen addicts: the elderly</title><updated>2025-10-25T19:32:22.429062+00:00</updated><content/><link href="https://www.economist.com/international/2025/10/23/meet-the-real-screen-addicts-the-elderly"/><published>2025-10-25T04:09:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45702558</id><title>React vs. Backbone in 2025</title><updated>2025-10-25T19:32:22.272429+00:00</updated><content>&lt;doc fingerprint="a7f21d051da72e1d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;15 Years of Progress&lt;/head&gt;
    &lt;p&gt;Look at the two implementations above. The code is roughly the same length. They do exactly the same thing. One was written with a framework from 2010, the other with a framework that's had countless developer hours and a massive ecosystem behind it for over a decade.&lt;/p&gt;
    &lt;p&gt;The interesting part is not how much better React is—it's how little progress we've actually made.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Illusion of Simplicity&lt;/head&gt;
    &lt;p&gt;React looks cleaner. It reads better at first glance. But that readability comes at a cost: you're trading explicit simplicity for abstraction complexity.&lt;/p&gt;
    &lt;p&gt;The Backbone code is brutally honest about what it's doing. An event fires, a handler runs, you build some HTML, you put it in the DOM. It's verbose, sure, but there's no mystery. A junior developer can trace exactly what happens and when. The mental model is straightforward: "when this happens, do this."&lt;/p&gt;
    &lt;p&gt;The React code hides a lot. And once you move past simple examples, you hit problems that don't make sense until you understand React's internals.&lt;/p&gt;
    &lt;p&gt; Your input mysteriously clears itself. Turns out you switched a list item's key from a stable ID to an index, so React thinks it's a completely different component and remounts it, wiping state. Or maybe you forgot that &lt;code&gt;value&lt;/code&gt; can't be &lt;code&gt;undefined&lt;/code&gt;—React saw it flip from uncontrolled to controlled and reset the input.
        &lt;/p&gt;
    &lt;p&gt; You add a &lt;code&gt;useEffect&lt;/code&gt; to fetch data, and suddenly your app is stuck in an infinite loop. The dependency array includes an object that gets recreated every render, so React thinks it changed and runs the effect again. Now you need &lt;code&gt;useMemo&lt;/code&gt; and &lt;code&gt;useCallback&lt;/code&gt; sprinkled everywhere to "stabilize identities," which is a thing you never had to think about before.
        &lt;/p&gt;
    &lt;p&gt; Your click handler sees old state even though you just set it. That's a stale closure—the function captured the value from when it was created, and later renders don't magically update it. You either need to put the state in the dependency array (creating a new handler every time) or use functional updates like &lt;code&gt;setState(x =&amp;gt; x + 1)&lt;/code&gt;. Both solutions feel like workarounds.
        &lt;/p&gt;
    &lt;head rend="h3"&gt;Magic Has a High Price&lt;/head&gt;
    &lt;p&gt;These aren't edge cases. They're normal problems you hit building moderately complex apps. And debugging them requires understanding reconciliation algorithms, render phases, and how React's scheduler batches updates. Your code "just works" without you needing to understand why it works, which is nice until it breaks.&lt;/p&gt;
    &lt;p&gt;People say "you need to rebuild React from scratch to really understand it," and they're right. But that's kind of damning, isn't it? You shouldn't need to understand virtual DOM diffing, scheduling priorities, and concurrent rendering to build a password validator.&lt;/p&gt;
    &lt;p&gt;Backbone might be tedious, but it doesn't lie to you. jQuery is hackable. You can view source, understand it, and add to it easily. It's just DOM methods. React's abstraction layers make that much harder.&lt;/p&gt;
    &lt;head rend="h3"&gt;So, What's Next?&lt;/head&gt;
    &lt;p&gt;We understand the problem: event + state = UI. That's it. That's what both of these implementations are solving.&lt;/p&gt;
    &lt;p&gt;For massive apps with 1,000 components on the same page, maybe React's complexity is justified. But what the other 99% of apps? What about small apps that just want to do a job and don't need all the magic?&lt;/p&gt;
    &lt;p&gt;Is there a better model? Something feels as hard and steady as the DOM, but still feels intuitive to write? Something hackable like Backbone and jQuery were, where you can pop open devtools and understand what's happening?&lt;/p&gt;
    &lt;p&gt;— panphora&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://backbonenotbad.hyperclay.com/"/><published>2025-10-25T09:43:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45703556</id><title>Making a micro Linux distro (2023)</title><updated>2025-10-25T19:32:21.903648+00:00</updated><content>&lt;doc fingerprint="579b191fd9a42fad"&gt;
  &lt;main&gt;
    &lt;p&gt;In this article, we’ll talk about building up a tiny (micro) Linux “distribution” from scratch. This distribution really won’t do much, but it will be built from scratch.&lt;/p&gt;
    &lt;p&gt;We will build the Linux kernel on our own, and write some software to package our micro-distro.&lt;/p&gt;
    &lt;p&gt;Lastly, we are doing this example on the RISC-V architecture, specifically QEMU’s &lt;code&gt;riscv64 virt&lt;/code&gt; machine. There’s very little in this article that is specific to this architecture, so you might as well do an almost identical exercise for other architectures like &lt;code&gt;x86&lt;/code&gt;. We recently went through the RISC-V boot process with SBI and bare metal programming for RISC-V, so this is just a continuation up the software stack.&lt;/p&gt;
    &lt;p&gt;Warning: This article is a very simplified view of a Linux distribution. There are things written below that are not 100% accurate, but more like 99.9%. This article is meant for beginners and helping them form a basic mental framework for understanding Linux systems. More advanced users may be triggered by over-simplification in some parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;What is an OS kernel?&lt;/head&gt;
    &lt;p&gt;Let’s assume we’re working on a single-core machine. They’re still around us, maybe not in our laptops and phones, but in some smaller devices, and historically they have been actually widely used even in our “big” personal devices like desktops. The latter ones have been capable of running multiple program simultaneously for many years, even as single cores. We’ll get into what simultaneous really means in a bit, but for now let’s just note that the one of the operating system kernel’s big tasks is to make that happen.&lt;/p&gt;
    &lt;p&gt;If you go back to the articles about bare metal programming and SBI on RISC-V, you can see how at the lowest layers of software we interact with our I/O devices. It usually (most often, but not necessarily always) boils down to the CPU writing some data at the appropriate address. Imagine if the application developers had to keep all these addresses in mind and they had to know which values exactly to send to those addresses! That would mean we’d have far fewer applications today, but we don’t, and that’s owing to the operating system kernels which abstract away these details and provide some simple high-level interfaces instead. In the RISC-V SBI article, we looked at an example of such interface for Linux on &lt;code&gt;x86&lt;/code&gt; — instead of knowing which addresses to write to and what values to send there, we focused on the logic and basically just told to the OS kernel that “we want message so and so written to the standard output”, and then the OS kernel dealt with the details of interacting with the hardware. So that’s another big task for the OS kernel: managing the hardware on the machine and making the interaction with it easier.&lt;/p&gt;
    &lt;p&gt;Going further, the OS kernel offers some really high-level programming interfaces like the filesystems. This may or may not be about managing some hardware and abstracting operations over it. For example, the most common case for the filesystems, of course, is to store some data on the disk and retrieve it later, and this has to do with the OS kernel managing the hardware related to disks on the machine (i.e. sending some data to certain addresses, which makes those hard disk devices respond in some way). However, this is not always the case, the files are not always some data stored on disk, and so filesystem is an interface exposed to us, meaning it’s a way of talking to the OS kernel, not necessarily a way to talk to the data. We’ll cover the filesystems in great detail in some other article, but let’s keep this in mind for now — the OS kernel needs to provide a straightforward way of doing high-level things through multiple interfaces.&lt;/p&gt;
    &lt;p&gt;Finally, the last thing I wanted to cover about the kernels is that they provide a programming model. Remember how we mentioned (as I’m sure you already know) that multiple programs can run even on a single-core device simultaneously? The OS enables the running applications to be programmed to not even know about each other, in other words, an application can live its lifecycle acting like it is the only application running on the computer and no one else is touching its memory. Imagine a world where your Python Django server needs to know about that texting app on your device in order to be working — we’d have far fewer Django apps and texting apps, for sure, as coding them would quickly get gnarly. However, the apps can also know about each other’s existence on the same machine. The operating system kernel facilitates both. It gives a programming model in which you can insulate applications from each other, or join a few apps in isolation from other apps, etc.&lt;/p&gt;
    &lt;p&gt;Basically, the OS kernel does a lot of heavy lifting to enable you to run your code easily on a very generic and complicated machinery such as your smartphone. What is written above probably doesn’t do full justice to the kernels, they do a whole lot of things, but the few paragraphs above should give a fairly good idea of kernel’s main tasks, and there are many.&lt;/p&gt;
    &lt;p&gt;Linux is an extremely popular operating system kernel. It can be built to run on many architectures (really, a lot), it is open source and free to use. And a lot of people are “Linux users”, but what does it exactly mean that someone “uses Linux”? Those Linux users typically install something like Debian, or Ubuntu on their machines, and they use Linux that way, and what does that mean?&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a Linux distribution?&lt;/head&gt;
    &lt;p&gt;We talked above about what kernels do, i.e. what are their tasks and we said Linux is an OS kernel, but can we really just take bare Linux and as end users who just want to watch YouTube, do something with it? The answer is likely no, we need a lot more layers on top of Linux to get to firing up a Chrome browser and watching YouTube.&lt;/p&gt;
    &lt;p&gt;How to go all the way towards the top of the software stack where we can just use those super simple and intuitive apps like graphical web browsers? We have previously discussed the boot process, and we went all the way from the very first operations on the machine after the power on, to the moment we land in the operating system kernel. We did not cover the bootloaders in any detail, we just briefly mentioned them because we were able to get QEMU to directly load our fake kernel into the memory in one go, which is typically not possible with full blown systems like desktop Linux (there is an intermediate booting stage where the bootloader fetches the OS image from something like a disk, or maybe even network and loads it into the memory). The kernel we wrote was a fake little stub that does effectively nothing, and so we ended our last article at the point where the OS kernel is in memory and ready to go, it’s just we had no kernel to run.&lt;/p&gt;
    &lt;p&gt;Based on what we see above, I think the right mental model for the kernel right now is that it is the infrastructure for running user applications on a complex machine, but it really doesn’t do anything for the user’s business logic. This is what I meant when I said the bare Linux on its own cannot fire up Chrome and let you watch YouTube — it is merely the infrastructure that the application developer uses to implement Chrome, and its streaming capabilities.&lt;/p&gt;
    &lt;p&gt;However, the kernel alone is not infrastructure for Chrome to run. We need to run sort of “infrastructure on top of infrastructure” to achieve the full infrastructure to run Chrome. Again, much like in the SBI article, we’re just layering abstractions on top of each other in some way, so essentially there is nothing new here, just the way we do it.&lt;/p&gt;
    &lt;p&gt;For example, in order for a machine to connect to the Internet, the OS kernel first needs to be able to drive the network device on the machine to send the signals out of the machine (to the switch, router, another machine or whatever it is connected to). However, in Linux, there is more or less where the kernel stops. Which networks you connect to, are you using VPN, how do you assign IPs to your machine (statically or dynamically) and that kind of business, it happens in the upper layers of the infrastructure.&lt;/p&gt;
    &lt;p&gt;You may now guess where this is going — a Linux distribution is really the Linux kernel plus the infrastructure on top of the kernel infrastructure. Let’s dig into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does “infrastructure on top of infrastructure” run?&lt;/head&gt;
    &lt;p&gt;Again, the kernel does a whole bunch of things, a million times more than what we can cover in a single article, but it definitely has its limits and it doesn’t do all the heavy lifting on your everyday personal device — and this is where something outside of the kernel gets into the picture.&lt;/p&gt;
    &lt;p&gt;Disclaimer: You can get really creative with Linux in a million different ways, and from this point on we’re going with a very basic, textbook-like, simple view of what happens in the mainstream distributions. There are many super complex things we can do, and there are lots of details we’re leaving out, but my hope here is that you get a general idea and enough knowledge to be able to understand more advanced material on this topic; there is plenty of it on the Internet.&lt;/p&gt;
    &lt;p&gt;The reason why I wrote the disclaimer above is mainly because we’re going to be assuming that your Linux has a filesystem going forward, as this is the most common path. How many times have you seen a Linux deployment without a filesystem? It certainly seems possible to do, but it may be borderline useless except for some super edge/advanced cases, and we’ll disregard them in this article. Check out this page to get more idea of what I’m talking about.&lt;/p&gt;
    &lt;p&gt;So what is the stuff outside the kernel? It’s what we call the user code! It’s just a normal code that runs within the Linux environment, just like you run basically anything on your Linux machine. Sure, some code is more privileged than the other, and there are a million more details that can get involved, but let’s just focus on the main distinction here: when you are running Linux on a machine, there is kernel code running, as well as the user code running, and everything that’s a part of the kernel itself is running in the kernel space, and everything that is running on the machine that is not a part of the kernel is running in the user space, and they are fairly isolated from each other.&lt;/p&gt;
    &lt;p&gt;So this “infrastructure on top of infrastructure” that we have talked about runs in the user space. Sure, it needs to bubble down to the kernel for many primitives, and we’ve seen already how that happens. Linux has a well defined ABI that exposes a set of services that the user space code can invoke in the kernel space. And where does this user space code come into the picture?&lt;/p&gt;
    &lt;head rend="h2"&gt;The &lt;code&gt;init&lt;/code&gt; process (and its “children”)&lt;/head&gt;
    &lt;p&gt;Once the kernel is done loading and making itself comfortable on the machine, it kicks off the first bit of the code in user space — the &lt;code&gt;init&lt;/code&gt; process. This is a piece of user space code that lives in a binary that sits somewhere on your filesystem, and the kernel will look for it in a few locations, beginning with &lt;code&gt;/init&lt;/code&gt; (if it doesn’t find it there, it will give a few more shots at different locations before throwing its hands up). Let’s say the kernel found a binary in the filesystem at &lt;code&gt;/init&lt;/code&gt; — it’s going to start it and assign the ID &lt;code&gt;1&lt;/code&gt;. This is basically the only user process that the kernel will start: the &lt;code&gt;init&lt;/code&gt; process then is the ancestor of all other user space processes. This means that &lt;code&gt;init&lt;/code&gt; will start some other processes, these other processes will in turn start some other processes, and so on. Very shortly you have a bunch of processes running on your machine, hopefully each one of them useful for the desired operations on the machine. The machine should at this point start actively interacting with the world around it: whether we’re talking about a smartphone giving the UI to its user, an embedded device that collects data off the sensors and sending it into the cloud, etc. Additionally, the machine will often have various tools available that are not actively running on the machine, but can be invoked in certain situations for some high level operations (e.g. a Python script can invoke a couple of tools like &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;cat&lt;/code&gt; or something to get a snapshot of what’s going on with the machine and then sending the data somewhere). Quick note is that even these periodically-started or ad-hoc tools are in some way descendants of &lt;code&gt;init&lt;/code&gt;; it’s not too important to know now, but it’s good to keep in mind.&lt;/p&gt;
    &lt;p&gt;The collection of kernel, the processes that get launched right after the kernel, and the tools that are available at your disposal represent the Linux distribution. It’s essentially a packaging for the kernel alongside all these useful tools that do more around the machine than what the kernel alone does (but it still provides the infrastructure for everything outside of the kernel to run, nothing bypasses the kernel).&lt;/p&gt;
    &lt;p&gt;Even a distrbution minimally useful for everyday use can get crufty pretty quickly. If you go onto the path of building your own custom little distro, as we actually will now, you will almost inevitably hit a lot of roadblocks where something that you expect to be working is just not working and the full solution is either to code some of your own software to talk to the kernel to get something done on the system, or just use an off-the-shelf software to do so. The latter is the path of least resistance, and you’ll likely keep adding stuff until you end up with a deployment that can do something remotely useful for you. At this point, you will have likely accumulated a significant number of software packages.&lt;/p&gt;
    &lt;p&gt;On the other hand, you have probably heard people criticizing certain distributions as being “bloated”, probably meaning they accumulated so much complexity in their packaging, they waste a lot of hardware resources doing things that are not useful, etc. Without discipline, I can easily see distrbution developers just randomly throwing different tools at the system just to get that one missing thing going, without retroactively cleaning up the excess later and just moving onto the next feature where they do the same — a (sadly) common pattern in software engineering.&lt;/p&gt;
    &lt;p&gt;Some distributions draw the line at different places where they just make a decision for the user and do something on the system, versus letting the user make the full decision and be more hands on. For example, you can install Arch Linux in a minimal way where it’s just a little more than the kernel booted up with a shell. All the subsequent decisions are on you, and you have to be very hands on in order to get it to a point where it’s very graphical and highly interactive. Or you can decide it’s just not worth your time setting it up so much, and just install a very user-friendly Ubuntu distrbution, which may be “bloated” for someone’s taste, but it gets you up and running very fast (I personally like it).&lt;/p&gt;
    &lt;head rend="h2"&gt;Building our almost useless Linux micro distrbibution&lt;/head&gt;
    &lt;p&gt;Let’s get our hands dirty and build something that’s basically useless but we’ll actually end up booting it for real. You may want to refresh your memory on the RISC-V boot process, I think it will be rewarding here.&lt;/p&gt;
    &lt;p&gt;First things first, let’s build the kernel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building a Linux operating system for RISC-V&lt;/head&gt;
    &lt;p&gt;I’m on an &lt;code&gt;x86&lt;/code&gt; platform here, so I will depend heavily on the cross-platform toolchain to build things for RISC-V. You will likely do something similar (I’m not sure I have yet seen someone build the RISC-V kernel on RISC-V itself).&lt;/p&gt;
    &lt;p&gt;Let’s get the source code for Linux. Linux development is done on top of the Git version control system, but we’ll take a shortcut here and just download a tarball with the sources for one branch, we won’t be syncing the whole Linux codebase with all the Git branches, experimental stuff and so on. We’ll be downloading the tarball from &lt;code&gt;kernel.org&lt;/code&gt; for version &lt;code&gt;6.5.2&lt;/code&gt; (here). You can also just download any tarball for whatever the latest stable version is from kernel.org homepage. Once it’s downloaded, go ahead and unpack that. Let’s also &lt;code&gt;cd&lt;/code&gt; into that directory.&lt;/p&gt;
    &lt;p&gt;Now is the time to configure the build. The first step is to make the &lt;code&gt;defconfig&lt;/code&gt; which basically initiates your configuration file.&lt;/p&gt;
    &lt;p&gt;Note: Here and below, you may want to use a different &lt;code&gt;CROSS_COMPILE&lt;/code&gt; prefix, depending on how the cross compilation tool is identified on your machine&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- defconfig&lt;/code&gt;
    &lt;p&gt;This was hopefully quick and the &lt;code&gt;.config&lt;/code&gt; file should be generated. The config file should contain a lot of IDs for individual configurations and the values for those, very often in yes/no format (e.g. &lt;code&gt;CONFIG_FOO=y&lt;/code&gt; or &lt;code&gt;CONFIG_FOO=n&lt;/code&gt;). You could edit the file manually, but I personally wouldn’t recommend it, especially as a beginner (I don’t consider myself an expert at this either). A better way to edit this is through the &lt;code&gt;curses&lt;/code&gt;-based pseudo-interface. You can get there by running&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- menuconfig&lt;/code&gt;
    &lt;p&gt;This interface has a few benefits.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You have a more readable, folder-like overview of the configs.&lt;/item&gt;
      &lt;item&gt;There are insights into dependencies between the configs, i.e. it may only make sense to be able to enable config &lt;code&gt;foo&lt;/code&gt;if&lt;code&gt;bar&lt;/code&gt;and&lt;code&gt;baz&lt;/code&gt;are also enabled.&lt;/item&gt;
      &lt;item&gt;This interface has a search feature, activated by pressing the &lt;code&gt;/&lt;/code&gt;button (I don’t think you’ll get far by searching there in natural language; my way of getting around here is by searching on Google and finding which exactly config key am I looking for, for example&lt;code&gt;CONFIG_TTY_PRINTK&lt;/code&gt;). When you find what you’re looking for, hit the button you see in the parentheses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We won’t be tweaking anything here for now, let’s just exit and move on.&lt;/p&gt;
    &lt;p&gt;It’s time to build the kernel! Quick note here, the make process famously has the &lt;code&gt;-j&lt;/code&gt; flag, which basically sets the concurrency in the build process, meaning it allows the build process to run a few things simultaneously. If you want to build faster, but not sure what to do, count the number of cores, and if it’s something like 8, just pass the flag &lt;code&gt;-j8&lt;/code&gt; below, as so. I will run the command like this (I’m on a 16-core machine):&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- -j16&lt;/code&gt;
    &lt;p&gt;This can take some time, though for the RISC-V build, it shouldn’t take awfully long, but I would expect at least a few minutes.&lt;/p&gt;
    &lt;p&gt;Once this is done, you will probably see something like this near the very bottom:&lt;/p&gt;
    &lt;code&gt;OBJCOPY arch/riscv/boot/Image&lt;/code&gt;
    &lt;p&gt;and this is the file we will be feeding to QEMU.&lt;/p&gt;
    &lt;p&gt;Great, let’s fire up QEMU!&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image&lt;/code&gt;
    &lt;p&gt;Switching to the UART view, we see that OpenSBI tidily started and the Linux took over! Great! We even see some references to the SBI layer that we have discussed before:&lt;/p&gt;
    &lt;code&gt;[    0.000000] Linux version 6.5.2 (uros@uros-debian-desktop) (riscv64-linux-gnu-gcc (Debian 10.2.1-6) 10.2.1 20210110, GNU ld (GNU Binutils for Debian) 2.35.2) #1 SMP Mon Sep 11 00:45:40 PDT 2023
[    0.000000] Machine model: riscv-virtio,qemu
[    0.000000] SBI specification v0.2 detected
[    0.000000] SBI implementation ID=0x1 Version=0x8
[    0.000000] SBI TIME extension detected
[    0.000000] SBI IPI extension detected
[    0.000000] SBI RFENCE extension detected&lt;/code&gt;
    &lt;p&gt;After reading about the boot process, we should now have a full understanding of what is going on here. This happened super early in the boot phase. There is a lot happening in these logs, and I’ll highlight a few things:&lt;/p&gt;
    &lt;code&gt;[    0.000000] riscv: base ISA extensions acdfim&lt;/code&gt;
    &lt;p&gt;Seems like Linux is capable of dynamically figuring out the capability of the underlying RISC-V hardware. I’m not sure what exactly is the mechanism behind it, could it be somehow passed through the device tree that we mentioned in the previous article, or something in the ISA itself tells this to the kernel, I’m not sure.&lt;/p&gt;
    &lt;code&gt;[    0.000000] Kernel command line:&lt;/code&gt;
    &lt;p&gt;This is interesting, a kernel has a command line? Turns out that the kernel, much like your everyday binaries, has startup flags. The kernel bootloader usually sets those up — after all, it knows how to fire up the kernel, and this could simply be a part of the starting process. With QEMU, remember, we’re sort of short circuiting the whole bootloader thing, and with passing the &lt;code&gt;-kernel&lt;/code&gt; flag, we let QEMU also wear the bootloader hat here by loading the kernel image into the memory and starting it up. QEMU actually has a flag called &lt;code&gt;-append&lt;/code&gt; with which you can append to this kernel command line. The command line itself is baked into the config file under &lt;code&gt;Boot options&lt;/code&gt; somewhere, I leave it to the reader to search for it, and the QEMU flag basically lets you adjust it with a VM launch, instead of having to rebuild the kernel to tweak the command line. In this case, the command line is just blank by default.&lt;/p&gt;
    &lt;code&gt;[    0.003376] printk: console [tty0] enabled&lt;/code&gt;
    &lt;p&gt;I guess this means that &lt;code&gt;printk&lt;/code&gt; will now write to &lt;code&gt;tty0&lt;/code&gt;? &lt;code&gt;printk&lt;/code&gt; is basically a way to write out messages from the kernel space. Remember, your typical &lt;code&gt;printf&lt;/code&gt; from C’s &lt;code&gt;stdio.h&lt;/code&gt; is meant for running in the user space, not kernel space, so kernel space must have its own solution, and it is &lt;code&gt;printk&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[    0.211634] Serial: 8250/16550 driver, 4 ports, IRQ sharing disabled
[    0.221544] 10000000.uart: ttyS0 at MMIO 0x10000000 (irq = 12, base_baud = 230400) is a 16550A
[    0.222659] printk: console [ttyS0] enabled&lt;/code&gt;
    &lt;p&gt;Great, Linux knows there is UART at &lt;code&gt;0x10000000&lt;/code&gt;, just like we established before. Linux can now choose whether to use the SBI interface to drive the UART, or talk to it directly (if the S-mode allows it on that machine, that is). On many platforms, the OS can disregard that a lower level software like BIOS may offer to interact with the hardware, and from what I hear, this actually indeed happens a lot.&lt;/p&gt;
    &lt;p&gt;There’s also a lot of other stuff in the kernel logs:&lt;/p&gt;
    &lt;code&gt;[    0.250030] SuperH (H)SCI(F) driver initialized&lt;/code&gt;
    &lt;p&gt;I don’t think we need this? I guess we can go back to the kernel config and not bake this driver into the kernel and thus slim the kernel down. What we’re building here is a generic build, really. We didn’t customize anything and presumably the authors of the default config thought this is a reasonable default that should just run on a lot of different setups, so they probably included a lot of things to be on the safe side. If you’re working on smaller hardware, with less generous memory, CPU, etc. you do have to carefully choose what gets baked into the kernel and what doesn’t.&lt;/p&gt;
    &lt;p&gt;Additionally, this generic build is smart enough to figure out that the console should go to the right UART device, which is really handy for us. Otherwise, we’d probably have to do a bunch of configs like making sure TTY (let’s not overfocus on what this is now) is enabled, we want to enable printing to UART as the kernel boots, etc. All this is basically configurable in the &lt;code&gt;menuconfig&lt;/code&gt; interface.&lt;/p&gt;
    &lt;p&gt;We’ll keep it simple in this article, and we won’t customize anything in the kernel unless we have to.&lt;/p&gt;
    &lt;head rend="h4"&gt;First obstacles&lt;/head&gt;
    &lt;p&gt;Scrolling down closer to the bottom of the output, we see this:&lt;/p&gt;
    &lt;code&gt;[    0.330411] /dev/root: Can't open blockdev
[    0.330743] VFS: Cannot open root device "" or unknown-block(0,0): error -6
[    0.330984] Please append a correct "root=" boot option; here are the available partitions:
[    0.331648] List of all bdev filesystems:
[    0.331785]  ext3
[    0.331803]  ext2
[    0.331882]  ext4
[    0.331950]  vfat
[    0.332028]  msdos
[    0.332098]  iso9660
[    0.332181]
[    0.332405] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0)
[    0.332756] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.5.2 #1
[    0.333018] Hardware name: riscv-virtio,qemu (DT)
[    0.333248] Call Trace:
[    0.333442] [&amp;lt;ffffffff8000537a&amp;gt;] dump_backtrace+0x1c/0x24
[    0.333940] [&amp;lt;ffffffff808890f8&amp;gt;] show_stack+0x2c/0x38
[    0.334138] [&amp;lt;ffffffff80894a48&amp;gt;] dump_stack_lvl+0x3c/0x54
[    0.334318] [&amp;lt;ffffffff80894a74&amp;gt;] dump_stack+0x14/0x1c
[    0.334493] [&amp;lt;ffffffff80889500&amp;gt;] panic+0x102/0x29e
[    0.334683] [&amp;lt;ffffffff80a015c6&amp;gt;] mount_root_generic+0x1e8/0x29c
[    0.334891] [&amp;lt;ffffffff80a0186c&amp;gt;] mount_root+0x1f2/0x224
[    0.335108] [&amp;lt;ffffffff80a01a68&amp;gt;] prepare_namespace+0x1ca/0x222
[    0.335320] [&amp;lt;ffffffff80a010c8&amp;gt;] kernel_init_freeable+0x23e/0x262
[    0.335539] [&amp;lt;ffffffff80896264&amp;gt;] kernel_init+0x1e/0x10a
[    0.335714] [&amp;lt;ffffffff800034c2&amp;gt;] ret_from_fork+0xa/0x1c
[    0.336208] ---[ end Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ]---&lt;/code&gt;
    &lt;p&gt;Whoops, we crashed! The kernel has fallen into a panic.&lt;/p&gt;
    &lt;p&gt;Remember how we talked that pretty much always Linux needs a filesystem to be useful and how all the “infrastructure on top of infrastructure” is in the user space? Well, we didn’t really pass anything related to the filesystem explicitly and we surely didn’t pass any user space code to serve as the &lt;code&gt;init&lt;/code&gt;, though we didn’t even get to the latter.&lt;/p&gt;
    &lt;p&gt;You might imagine that the filesystem needs to be on a disk, but that’s not necessarily the case. We’ll talk some other time about filesystems in great detail, but you can really have a filesystem be backed by RAM memory too. And this is actually very often used by Linux, most notably in the boot up phase. When the kernel gets to where it crashed for us just now, in a normal, typical situation, it will find the whole, fully functional filesystem actually loaded into the RAM. If this confuses you, just think about it this way — a disk is just a bunch of bytes, just like RAM is, though RAM is faster but much smaller; conceptually they’re basically the same. Who and how loads this memory?&lt;/p&gt;
    &lt;p&gt;One way is to bake the filesystem directly into the kernel image. In this case, as the kernel loads, so does the initial, memory-backed filesystem, and our system would be ready to go if we had done that. If you don’t want to bulk up your kernel image and you want your initial filesystem to be loaded by some other means, like through a bootloader or something, then you package it separately. In QEMU case, we can shortcircuit things a little bit again, and make it wear a few more hats — we’ll make it also load the initial filesystem into the memory as well. If you’re interested in building the filesystem into the kernel, read the discussion here and try it as an exercise after you’re done with this guide.&lt;/p&gt;
    &lt;p&gt;This initial filesystem has a name: &lt;code&gt;initramfs&lt;/code&gt;. You’ll often hear it called &lt;code&gt;initrd&lt;/code&gt; too (I imagine &lt;code&gt;rd&lt;/code&gt; is short for ramdisk?). The latter is how QEMU takes in the filesystem for loading (&lt;code&gt;-initrd&lt;/code&gt; flag).&lt;/p&gt;
    &lt;p&gt;The filesystem is packaged as a &lt;code&gt;cpio&lt;/code&gt; archive, which is conceptually similar to &lt;code&gt;tar&lt;/code&gt;, but it’s not the same binary format. Short discussion can be read here.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building the &lt;code&gt;initramfs&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The only real requirement for the &lt;code&gt;initramfs&lt;/code&gt; from the kernel is that it has a binary it can start up as the &lt;code&gt;init&lt;/code&gt; process, and the first place where the kernel will look for it is at the filesystem root, so the path is &lt;code&gt;/init&lt;/code&gt;. If you have absolutely nothing else on your filesystem, it’s questionably useful, but this is the bare requirement. Let’s start by writing the &lt;code&gt;init&lt;/code&gt; process in C. This process can be really anything, Linux won’t stop you from writing a useless &lt;code&gt;init&lt;/code&gt;, it will happily just execute it. We can go with a ‘hello world’ then?&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main(int argc, char *argv[]) {
  printf("Hello world\n");
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Great, now let’s package it up into a &lt;code&gt;cpio&lt;/code&gt; archive.&lt;/p&gt;
    &lt;code&gt;riscv64-linux-gnu-gcc -static -o init init.c
cpio -o -H newc &amp;lt; file_list.txt &amp;gt; initramfs.cpio&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;file_list.txt&lt;/code&gt; has a single line:&lt;/p&gt;
    &lt;code&gt;init&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We’re building a static binary because we do not want to dynamically depend on the standard C library. The filesystem won’t have it, we’re making a filesystem with &lt;code&gt;init&lt;/code&gt;alone.&lt;/item&gt;
      &lt;item&gt;Linux expects the &lt;code&gt;initramfs&lt;/code&gt;archive to be built with the&lt;code&gt;-H newc&lt;/code&gt;flag.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s run QEMU.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /PATH/TO/NEWLY_BUILT/initramfs.cpio&lt;/code&gt;
    &lt;p&gt;The kernel stil falls into a panic, but a different one!&lt;/p&gt;
    &lt;code&gt;[    0.351894] Run /init as init process
Hello world
[    0.379006] Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000000
[    0.379360] CPU: 0 PID: 1 Comm: init Not tainted 6.5.2 #1
[    0.379597] Hardware name: riscv-virtio,qemu (DT)
[    0.379812] Call Trace:
[    0.380005] [&amp;lt;ffffffff8000537a&amp;gt;] dump_backtrace+0x1c/0x24
[    0.380724] [&amp;lt;ffffffff808890f8&amp;gt;] show_stack+0x2c/0x38
[    0.380906] [&amp;lt;ffffffff80894a48&amp;gt;] dump_stack_lvl+0x3c/0x54
[    0.381095] [&amp;lt;ffffffff80894a74&amp;gt;] dump_stack+0x14/0x1c
[    0.381283] [&amp;lt;ffffffff80889500&amp;gt;] panic+0x102/0x29e
[    0.381447] [&amp;lt;ffffffff80013fd0&amp;gt;] do_exit+0x760/0x766
[    0.381623] [&amp;lt;ffffffff80014154&amp;gt;] do_group_exit+0x24/0x70
[    0.381806] [&amp;lt;ffffffff800141b8&amp;gt;] __wake_up_parent+0x0/0x20
[    0.382009] [&amp;lt;ffffffff80895482&amp;gt;] do_trap_ecall_u+0xe6/0xfa
[    0.382218] [&amp;lt;ffffffff8000337c&amp;gt;] ret_from_exception+0x0/0x64
[    0.382808] ---[ end Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000000 ]---&lt;/code&gt;
    &lt;p&gt;I guess this just means &lt;code&gt;init&lt;/code&gt; shouldn’t finish, so it should be easy to fix? Let’s just make it print something every 10 seconds and never stop. Important to note: our output worked, we see a “Hello world” string!&lt;/p&gt;
    &lt;p&gt;We’ll write a new &lt;code&gt;init&lt;/code&gt;, but let’s also make our &lt;code&gt;initramfs&lt;/code&gt; a little more complex too. Let’s remember how we said that &lt;code&gt;init&lt;/code&gt; starts up all the other processes on the machine. Wouldn’t it be nice if we actually had some sort of a shell? After all, that’s what we typically have with Linux — shells go well with Linux. We’ll build a useless shell, the one that just tells us what we asked it to do (echoes back the input).&lt;/p&gt;
    &lt;p&gt;Let’s first write the &lt;code&gt;init&lt;/code&gt; process. Before it begins looping and printing something every 10 seconds, it has an important job of spawning our “little shell”. The way a process can spawn another process in Linux is through 2 operations: &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;exec&lt;/code&gt;. &lt;code&gt;fork&lt;/code&gt; will start a new process by literally cloning the current process at the moment of &lt;code&gt;fork&lt;/code&gt;. The way the underlying code can differentiate the “parent” and “child” processes after that is by checking the return value of the &lt;code&gt;fork&lt;/code&gt; operation. If it is 0, this means the process is the child process, and it’s a parent otherwise (-1 is returned in an error case).&lt;/p&gt;
    &lt;p&gt;Next, it’s not useful for us here to just keep executing the &lt;code&gt;init&lt;/code&gt; program in 2 different processes. That’s where one of the many &lt;code&gt;exec&lt;/code&gt; operations come into the picture. When I say there are many &lt;code&gt;exec&lt;/code&gt; operations available on Linux, I mean there are &lt;code&gt;execl&lt;/code&gt;, &lt;code&gt;execlp&lt;/code&gt;, &lt;code&gt;execle&lt;/code&gt;, etc. Take a look at more documentation here, please. We’re going with &lt;code&gt;execl&lt;/code&gt; here, and the first parameter is which binary do we want to launch. We’ll package our fake shell as the &lt;code&gt;little_shell&lt;/code&gt; binary on the root. The rest of the parameters do not really matter (as evidenced by the value of the second parameter). More important, the mechanism of this operation is that we’re calling into the kernel to take whatever is running in the current process and replace it with the program that is loaded for execution from the binary listed as the first parameter. This is how programs get launched on Linux and when you’re working in your Bash shell, and you end up launching a program, this is what happens — a sequence of &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;exec&lt;/code&gt;-style calls.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main(int argc, char *argv[]) {
  pid_t pid = fork();

  if (pid == -1) {
    printf("Unable to fork!");
    return -1;
  }

  if (pid == 0) {
    // This is a child process.
    int status = execl("/little_shell", "irrelevant", NULL);

    if (status == -1) {
      printf("Forked process cannot start the little_shell");
      return -2;
    }
  }

  int count = 1;

  while (1) {
    printf("Hello from the original init! %d\n", count);
    count++;
    sleep(10);
  }

  return 0;
}&lt;/code&gt;
    &lt;p&gt;We build the &lt;code&gt;init&lt;/code&gt; the same way as we did before:&lt;/p&gt;
    &lt;code&gt;riscv64-linux-gnu-gcc -static -o init init.c&lt;/code&gt;
    &lt;p&gt;For the “shell” we’re building, I want to get a little more creative. Why don’t we write this one in Go instead of old school C?&lt;/p&gt;
    &lt;code&gt;package main

import (
	"bufio"
	"fmt"
	"os"
)

func main() {
	fmt.Println("Hello world from Go!")

	reader := bufio.NewReader(os.Stdin)

	for {
		fmt.Print("Enter your command: ")
		line, _ := reader.ReadString('\n')
		fmt.Printf("Your command is: %s", line)
	}
}&lt;/code&gt;
    &lt;p&gt;I am able to cross compile this to RISC-V out-of-the-box with my &lt;code&gt;go&lt;/code&gt; compiler.&lt;/p&gt;
    &lt;code&gt;GOOS=linux GOARCH=riscv64 go build little_shell.go&lt;/code&gt;
    &lt;p&gt;Nice thing that I really like about Go is that it’s very easy to reference other remote repositories on GitHub to include libraries, and things get neatly packaged up statically. I’m not going to lie, the &lt;code&gt;little_shell&lt;/code&gt; Go binary is pretty thick, weighing in at 1.9M on my machine, compared to only 454K for the statically-linked simple init, but in the days of desktops/laptops/phones with hundreds of GB of storage, if you’re building a distro for these kinds of devices, you may want to consider the tradeoff.&lt;/p&gt;
    &lt;p&gt;Note, there are situations where you may not be able to simply run your Go binary just like that on top of a bare kernel, it could start throwing Go panics all over the place. In order to run Go, you need to build your kernel with the right features in it, futex support feature being one of them (I think I’ve identified only 2 in my past experience). If you encounter any problems running the Go applications and you suspect you may not have the right kernel support, carefully read through the panics and you will be able to identify what is missing. Good news here is that the default config for the RISC-V kernel is good enough for running Go.&lt;/p&gt;
    &lt;p&gt;Let’s update our &lt;code&gt;file_list.txt&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;init
little_shell&lt;/code&gt;
    &lt;p&gt;Pack it all up again:&lt;/p&gt;
    &lt;code&gt;cpio -o -H newc &amp;lt; file_list.txt &amp;gt; initramfs.cpio&lt;/code&gt;
    &lt;p&gt;Let’s run it!&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /PATH/TO/NEWLY_BUILT/initramfs.cpio&lt;/code&gt;
    &lt;code&gt;[    0.356314] Run /init as init process
Hello from the original init! 1
Hello world from Go!
Enter your command: [[[mkdir hello]]]
Your command is: mkdir hello
Enter your command: [[[ls]]]
Your command is: ls
Enter your command: Hello from the original init! 2
[[[echo 123]]
Your command is: echo 123
Enter your command: [[[exit]]]
Your command is: exit
Enter your command: Hello from the original init! 3
[[[I give up!]]]
Your command is: I give up!&lt;/code&gt;
    &lt;p&gt;The bits in this console excerpt enclosed with triple square brackets are my user-provided input over UART. You can see 3 things interleaved on the UART&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Original &lt;code&gt;init&lt;/code&gt;’s period output every 10 seconds.&lt;/item&gt;
      &lt;item&gt;Output from the &lt;code&gt;little_shell&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Input from the user.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are using the sole UART device on the virtual machine for all this, but that is not the only reason why everything is mixed up here. &lt;code&gt;init&lt;/code&gt; process prints to the standard output, just like &lt;code&gt;little_shell&lt;/code&gt; does, and you may not be aware of it, but any sort of print on Linux is a print to an open file. Standard output, as far as Linux knows, is a file that is opened by a process and you are printing to the standard output by writing to that file. When we &lt;code&gt;fork&lt;/code&gt;-ed the &lt;code&gt;little_shell&lt;/code&gt; from &lt;code&gt;init&lt;/code&gt;, the &lt;code&gt;little_shell&lt;/code&gt; inherited the open files from &lt;code&gt;init&lt;/code&gt;. So they are literally sharing all the standard input and output streams. Even if we had multiple I/O devices that we used on this machine, they’d still be sending outputs over to the same output stream. When &lt;code&gt;init&lt;/code&gt; was started, its standard output was set to produce content over to UART, and this behavior was simply inherited by the &lt;code&gt;little_shell&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And there we have it, we have a pretty useless, but home-made Linux distribution! Go ahead and send it over to your friends! :)&lt;/p&gt;
    &lt;p&gt;Jokes aside, you can make an exercise out of this and implement some sort of a mini shell out of this &lt;code&gt;little_shell&lt;/code&gt;. Instead of just echoing back the commands given to it, you could make it actually understand what &lt;code&gt;mkdir&lt;/code&gt; is. You can even have it fork off a process to execute that elsewhere. Sky is the limit, you’re in the Linux userspace!&lt;/p&gt;
    &lt;p&gt;Let’s just step back a little and see if Linux kernel achieved the initial few promises for us:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;It’s abstracting away the hardware. Our&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;and our shell didn’t know anything about the UART. All they knew was they’re writing to some Linux file handle. It happens to be mapped to something abstract in the Linux kernel that invokes the UART driver in the Linux kernel, which may or may not use the SBI under the hood (I have honestly not verified if the kernel removes its dependence on SBI after it boots).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;It offers some high-level programming paradigms, like filesystems. Our&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;process located the other binary through the filesystem (the path was trivial, the binary was right in the root, but still, the paradigm is there).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There is a pretty clean isolation between the processes running. Once the shell was forked off from the&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;, the processes were basically running independently. The memory was not shared between them and they didn’t have to worry about each other’s memory layout. They did share something else, though, like the file handles, but this is a consequence of how they were launched into running. Linux enables you to actually change some of this behavior, e.g. you can set up some shared memory between the processes, if you explicitly want to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are many other things the kernel does for us, but let’s just stop here for now and appreciate this. It may not look like a lot, but the kernel gives us a pretty solid, portable infrastructure with which we can develop high level software while often disregarding the complexities of the underlying machine.&lt;/p&gt;
    &lt;head rend="h3"&gt;So what is an operating system?&lt;/head&gt;
    &lt;p&gt;This is now a game of words in my opinion. In my view, what matters is that the reader now has an understanding of what Linux as the kernel is, what “infrastructure” it offers, and what is running in the user space and what is running in the kernel space.&lt;/p&gt;
    &lt;p&gt;Some people may call the kernel itself an operating system, some people will refer to the whole distribution as the operating system, or they may come up with something completely different. I hope that at this point you have a good understanding of what is happening on a machine once Linux is started and where the responsibilities of each component end (or you can at least imagine the boundaries on a more complex system).&lt;/p&gt;
    &lt;p&gt;I hope this was useful!&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus section: making an actually useful micro distribution with &lt;code&gt;u-root&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;I thought about wrapping up here, but it wouldn’t make for a flashy demo. Why don’t we instead boot into something that’s actually useful, meaning that you can do things you would typically do on a Linux-based system, like run your &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;mkdir&lt;/code&gt;, &lt;code&gt;echo&lt;/code&gt; and whatnot. Let’s stick with the kernel we have previously built, and add some useful “infrastructure on top of infrastructure” in the user space domain to make the whole machine more useful.&lt;/p&gt;
    &lt;p&gt;I really like the u-root project for this.&lt;/p&gt;
    &lt;p&gt;Note: The title of their project mentions Go bootloaders, and this may stump you because as a careful reader, you know that Go programs are not really something you can run on bare metal. These bootloaders are somewhat exotic userspace bootloaders, meaning that they will actually run on top of a live Linux kernel, and then use this amazing Linux mechanism called &lt;code&gt;kexec&lt;/code&gt; to re-load a different kernel into the memory from user space. We won’t be using these bootloaders for now, we’ll just focus on the other user space goodies they have available, but I thought a quick paragraph here would help the confused readers.&lt;/p&gt;
    &lt;p&gt;The reason why I like the &lt;code&gt;u-root&lt;/code&gt; project is because it’s so insanely easy to use. Its usage is a bit creative though, so there are really 2 steps here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install &lt;code&gt;u-root&lt;/code&gt;per their instructions. You should end up with a&lt;code&gt;u-root&lt;/code&gt;binary in your&lt;code&gt;PATH&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Now to actually generate a functional &lt;code&gt;initramfs&lt;/code&gt;with&lt;code&gt;u-root&lt;/code&gt;, the easiest way is to clone their Git repo and&lt;code&gt;cd&lt;/code&gt;your way into the directory that you just cloned. From there, you can cross-compile a fully functional user space set of tools with a single command.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/u-root/u-root.git
cd u-root
GOOS=linux GOARCH=riscv64 u-root&lt;/code&gt;
    &lt;p&gt;I get a few lines of output, the last being:&lt;/p&gt;
    &lt;code&gt;18:31:31 Successfully built "/tmp/initramfs.linux_riscv64.cpio" (size 14827284).&lt;/code&gt;
    &lt;p&gt;And that’s really it, this &lt;code&gt;cpio&lt;/code&gt; file can now be just ran with QEMU and you’ll boot right into a shell! Go through the &lt;code&gt;u-root&lt;/code&gt; documentation to understand how you can customize this &lt;code&gt;initramfs&lt;/code&gt; image you get, including what sort of changes you can make to the &lt;code&gt;init&lt;/code&gt; process behavior, but I think the default setup is so amazing to explore with.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /tmp/initramfs.linux_riscv64.cpio&lt;/code&gt;
    &lt;p&gt;Wow, this booted really smoothly! Providing the bottom of the UART output.&lt;/p&gt;
    &lt;code&gt;[    0.400269] Run /init as init process
2023/09/12 01:34:33 Welcome to u-root!
                              _
   _   _      _ __ ___   ___ | |_
  | | | |____| '__/ _ \ / _ \| __|
  | |_| |____| | | (_) | (_) | |_
   \__,_|    |_|  \___/ \___/ \__|
&lt;/code&gt;
    &lt;p&gt;And as you can see by the little &lt;code&gt;/#&lt;/code&gt; prompt, you’re actually in a shell! &lt;code&gt;u-root&lt;/code&gt;’s &lt;code&gt;init&lt;/code&gt; forked off a shell process and gave it the control over the UART.&lt;/p&gt;
    &lt;code&gt;/# ls
bbin
bin
buildbin
dev
env
etc
go
init
lib
lib64
proc
root
sys
tcz
tmp
ubin
usr
var
/# pwd
/
/# echo "Hello world!"
Hello world!&lt;/code&gt;
    &lt;p&gt;This little shell that &lt;code&gt;u-root&lt;/code&gt; gives even supports Tab-completion! I will say I have encountered some hiccups occassionally with it, it’s definitely not your full blown Bash, but it’s more than just a toy.&lt;/p&gt;
    &lt;p&gt;The standard tools like &lt;code&gt;ls&lt;/code&gt; seem to be taking the standard flags:&lt;/p&gt;
    &lt;code&gt;/# ls -lah
dtrwxrwxrwx root 0 420 B  Sep 12 01:35 .
drwxr-xr-x  root 0 2.1 kB Jan  1 00:00 bbin
drwxr-xr-x  root 0 80 B   Jan  1 00:00 bin
drwxrwxrwx  root 0 40 B   Sep 12 01:34 buildbin
drwxr-xr-x  root 0 12 kB  Sep 12 01:34 dev
drwxr-xr-x  root 0 40 B   Sep 12 01:35 directory
drwxr-xr-x  root 0 40 B   Jan  1 00:00 env
drwxr-xr-x  root 0 80 B   Sep 12 01:34 etc
drwxrwxrwx  root 0 60 B   Sep 12 01:34 go
Lrwxrwxrwx  root 0 9 B    Jan  1 00:00 init -&amp;gt; bbin/init
drwxrwxrwx  root 0 40 B   Sep 12 01:34 lib
drwxr-xr-x  root 0 40 B   Jan  1 00:00 lib64
dr-xr-xr-x  root 0 0 B    Sep 12 01:34 proc
drwx------  root 0 40 B   Sep 11 07:43 root
dr-xr-xr-x  root 0 0 B    Sep 12 01:34 sys
drwxr-xr-x  root 0 40 B   Jan  1 00:00 tcz
dtrwxrwxrwx root 0 60 B   Sep 12 01:34 tmp
drwxr-xr-x  root 0 40 B   Jan  1 00:00 ubin
drwxr-xr-x  root 0 60 B   Jan  1 00:00 usr
drwxr-xr-x  root 0 60 B   Jan  1 00:00 var&lt;/code&gt;
    &lt;head rend="h3"&gt;Visit google.com from this!&lt;/head&gt;
    &lt;p&gt;One last flashy thing — let’s connect to google.com from this VM with our custom user-land!&lt;/p&gt;
    &lt;p&gt;First, we need to attach a network device. We add &lt;code&gt;-device virtio-net-device,netdev=usernet -netdev user,id=usernet,hostfwd=tcp::10000-:22&lt;/code&gt; to our QEMU CLI. I think the last 2 numbers do not really matter as we won’t be SSH’ing into this machine (maybe you can do that exercise yourself, but I’m afraid it won’t be easy). The default kernel build should indeed bake in the &lt;code&gt;virtio&lt;/code&gt; network device drivers, so this should more or less just work.&lt;/p&gt;
    &lt;p&gt;We’ll need a working IP address, and we’ll use something from &lt;code&gt;u-root&lt;/code&gt; to obtain it. That something requires 3 things present in the kernel config: &lt;code&gt;CONFIG_VIRTIO_PCI&lt;/code&gt;, &lt;code&gt;CONFIG_HW_RANDOM_VIRTIO&lt;/code&gt; and &lt;code&gt;CONFIG_CRYPTO_DEV_VIRTIO&lt;/code&gt;. My default settings for the kernel have all that flipped to &lt;code&gt;y&lt;/code&gt;, so I’m good to go and you should be too, but you can double check just in case. If you have changed any kernel settings, please rebuild the kernel image.&lt;/p&gt;
    &lt;p&gt;Finally, we need to attach an RNG (doesn’t matter what it is) device to our QEMU machine so we can obtain our IP address. We simply add &lt;code&gt;-device virtio-rng-pci&lt;/code&gt; to our QEMU CLI.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /tmp/initramfs.linux_riscv64.cpio -device virtio-net-device,netdev=usernet -netdev user,id=usernet,hostfwd=tcp::10000-:22 -device virtio-rng-pci&lt;/code&gt;
    &lt;p&gt;Once we’re in, we can run &lt;code&gt;ip addr&lt;/code&gt; to see what’s our IP address.&lt;/p&gt;
    &lt;code&gt;/# ip addr
1: lo: &amp;lt;UP,LOOPBACK&amp;gt; mtu 65536 state UNKNOWN
    link/loopback
    inet 127.0.0.1 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 state DOWN
    link/ether 52:54:00:12:34:56
3: sit0: &amp;lt;0&amp;gt; mtu 1480 state DOWN
    link/sit&lt;/code&gt;
    &lt;p&gt;Our Ethernet is not set up. Let’s enable IPv4 networking (we don’t need 6). In this little setup, QEMU is running a virtualized network and it embeds a little DHCP server which can dynamically assign IPs (documentation is here). So let’s run a DHCP helper from &lt;code&gt;u-root&lt;/code&gt; for this by running&lt;/p&gt;
    &lt;code&gt;dhclient -ipv6=false&lt;/code&gt;
    &lt;p&gt;The output I got was the following:&lt;/p&gt;
    &lt;code&gt;2023/09/12 03:46:59 Bringing up interface eth0...
2023/09/12 03:47:00 Attempting to get DHCPv4 lease on eth0
2023/09/12 03:47:00 Got DHCPv4 lease on eth0: DHCPv4 Message
  opcode: BootReply
  hwtype: Ethernet
  hopcount: 0
  transaction ID: 0x05f008e1
  num seconds: 0
  flags: Unicast (0x00)
  client IP: 0.0.0.0
  your IP: 10.0.2.15
  server IP: 10.0.2.2
  gateway IP: 0.0.0.0
  client MAC: 52:54:00:12:34:56
  server hostname:
  bootfile name:
  options:
    Subnet Mask: ffffff00
    Router: 10.0.2.2
    Domain Name Server: 10.0.2.3
    IP Addresses Lease Time: 24h0m0s
    DHCP Message Type: ACK
    Server Identifier: 10.0.2.2
2023/09/12 03:47:00 Configured eth0 with IPv4 DHCP Lease IP 10.0.2.15/24
2023/09/12 03:47:00 Finished trying to configure all interfaces.&lt;/code&gt;
    &lt;p&gt;The QEMU documentation will tell you why pinging won’t work, so let’s not bother with pinging. Let’s just “visit” google.com!&lt;/p&gt;
    &lt;code&gt;wget http://google.com&lt;/code&gt;
    &lt;p&gt;You can now read the downloaded &lt;code&gt;index.html&lt;/code&gt; file!&lt;/p&gt;
    &lt;code&gt;cat index.html&lt;/code&gt;
    &lt;p&gt;You’ll get a lot of obfuscated JavaScript, but this is great! It means we have successfully visited google.com through &lt;code&gt;wget&lt;/code&gt;! I hope this sparks your imagination to do some other cool things with &lt;code&gt;u-root&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Package managers&lt;/head&gt;
    &lt;p&gt;You might intuitively understand at this point that some of the most important software of a Linux distro is the package manager. It’s really the gateway to getting the functionality on your machine that you need. What we went through here is more of an embedded flow: we generated these somewhat monolithic software images and if we want to update something, we rebuild the whole image and re-image the device. This doesn’t work for desktops, phones, etc. Package managers are there to update, add or remove the software on our machines. We won’t be talking about them here, just giving them a brief shoutout and you can hopefully imagine from the high level how they work and what do they do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The monster of &lt;code&gt;init&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;init&lt;/code&gt; we created is definitely just a toy, and in the end it just started some sort of a shell. However, make no mistake about it, &lt;code&gt;init&lt;/code&gt; is an incredibly important thing on a Linux system and getting it right is a science. You’ll see a lot of strong opinions on different &lt;code&gt;init&lt;/code&gt; systems for Linux online. &lt;code&gt;init&lt;/code&gt; doesn’t usually just spawn one process off and call it a day, it can set up a whole bunch of things like different devices, for example. As an exercise, just run &lt;code&gt;ls /dev&lt;/code&gt; from your &lt;code&gt;u-root&lt;/code&gt;-based build and see all those devices set up. A lot of them come from the &lt;code&gt;init&lt;/code&gt;’s setup and many are extremely useful. You can then read some of the &lt;code&gt;u-root&lt;/code&gt; source code to see what’s going on there in &lt;code&gt;init&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The code for this guide is available here, where you can just sync and build the &lt;code&gt;initramfs&lt;/code&gt; images.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://popovicu.com/posts/making-a-micro-linux-distro/"/><published>2025-10-25T13:01:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45703716</id><title>Synadia and TigerBeetle Commit $512k USD to the Zig Software Foundation</title><updated>2025-10-25T19:32:21.549961+00:00</updated><content>&lt;doc fingerprint="2d5ad9ff893f0b97"&gt;
  &lt;main&gt;
    &lt;p&gt;Synadia and TigerBeetle have together pledged a combined $512,000 USD to the Zig Software Foundation (ZSF) over the next two years, demonstrating a shared belief in Zig’s potential to shape the next era of high-performance, reliable, and maintainable software.&lt;/p&gt;
    &lt;p&gt;At Synadia, we help some of the world’s largest enterprises design and scale innovative architectures across cloud regions, cloud providers, and all the way to the far edge. Often described as a “decentralized nervous system,” we enable secure, reliable communication between services and data, no matter the environment or topology - so our customers can deliver incredible digital products and experiences.&lt;/p&gt;
    &lt;p&gt;From the beginning, our mission has been bold yet simple: to connect everything. Built on top of NATS.io, our platform enables organizations to modernize, digitize, and extend their systems all the way to the edge. Synadia’s technology enables organizations to build and scale microservices, streaming and telemetry platforms, and event sourcing systems, while leveraging modern data primitives such as key-value and object stores anywhere within those systems.&lt;/p&gt;
    &lt;p&gt;Our customers span industries from financial services and e-commerce to gaming, manufacturing, industrial IoT, connected and autonomous vehicles, energy systems, and embodied AI. They continually challenge us to push the limits of what’s possible - delivering secure, real-time communications and data movement anywhere with minimal overhead.&lt;/p&gt;
    &lt;p&gt;Beyond the language itself, the most impressive aspect of Zig has been the quality of projects built with it: TigerBeetle, Bun, Ghostty, and others. Among these, TigerBeetle stands out.&lt;/p&gt;
    &lt;p&gt;I first met Joran Dirk Greef, TigerBeetle’s founder, at the first Distributed Systems Conference in Cape Town, which they hosted. Since then, I’ve had the chance to see how Joran and his team approach engineering for their financial database product, guided by their philosophy called “TigerStyle.” It focuses on correctness, clarity, and reliability — values that deeply resonate with us at Synadia and our customers.&lt;/p&gt;
    &lt;p&gt;Our goals align closely. Synadia is building industrial-grade solutions designed for smaller, more efficient, and deterministic deployments. Like TigerBeetle, we believe software should be predictable, simple, and trustworthy by design.&lt;/p&gt;
    &lt;p&gt;When Joran and I met again in New York recently, he told me TigerBeetle would be increasing its support for the Zig Foundation and asked whether Synadia would like to join forces. I didn’t have to think twice. There’s no better company or person to partner with in supporting Zig.&lt;/p&gt;
    &lt;p&gt;We are grateful to Andrew Kelley, the founder and president of the Zig Software Foundation, whose leadership continues to inspire developers building serious systems software. Andrew’s cohesive and focused vision mirrors how we operate at Synadia.&lt;/p&gt;
    &lt;p&gt;We’re proud to support Andrew, Loris Cro, and the entire Zig community as they continue to advance the state of systems programming.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Zig’s approach to control, performance, and simplicity is redefining what’s possible in modern systems software. We’re honored to contribute alongside TigerBeetle to help the Zig Foundation continue this vital work.” — Derek Collison, Founder &amp;amp; CEO, Synadia&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“We’re delighted to stand with Synadia in supporting Zig’s growth. Together, we believe Zig will play a foundational role in the next generation of reliable distributed systems.” — Joran Dirk Greef, Founder &amp;amp; CEO, TigerBeetle&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Zig Software Foundation (ZSF) is a non-profit organization dedicated to supporting the development of Zig, a programming language designed for performance, reliability, and maintainability. Led by Andrew Kelley, Zig empowers developers to write robust software without hidden control flow or unpredictable behavior and is increasingly being adopted for systems, embedded, and high-performance applications.&lt;/p&gt;
    &lt;p&gt;Synadia Communications, Inc. is the creator of the Synadia Platform and the maintainer of the NATS.io ecosystem. Synadia provides secure, zero-trust messaging and connectivity across cloud, edge, and on-premises environments. Its technology powers mission-critical systems for leading enterprises in finance, manufacturing, automotive, energy, and AI.&lt;/p&gt;
    &lt;p&gt;TigerBeetle is the financial transactions database designed for mission-critical safety and performance to power the next thirty years of transaction processing.&lt;/p&gt;
    &lt;p&gt;News and content from across the community&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.synadia.com/blog/synadia-tigerbeetle-zig-foundation-pledge"/><published>2025-10-25T13:24:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45704419</id><title>Against SQL</title><updated>2025-10-25T19:32:21.049667+00:00</updated><content>&lt;doc fingerprint="f7281873efb9704c"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;TLDR&lt;/head&gt;
    &lt;p&gt;The relational model is great:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A shared universal data model allows cooperation between programs written in many different languages, running on different machines and with different lifespans.&lt;/item&gt;
      &lt;item&gt;Normalization allows updating data without worrying about forgetting to update derived data.&lt;/item&gt;
      &lt;item&gt;Physical data independence allows changing data-structures and query plans without having to change all of your queries.&lt;/item&gt;
      &lt;item&gt;Declarative constraints clearly communicate application invariants and are automatically enforced.&lt;/item&gt;
      &lt;item&gt;Unlike imperative languages, relational query languages don't have false data dependencies created by loop counters and aliasable pointers. This makes relational languages: &lt;list rend="ul"&gt;&lt;item&gt;A good match for modern machines. Data can be rearranged for more compact layouts, even automatic compression. Operations can be reordered for high cache locality, pipeline-friendly hot loops, simd etc.&lt;/item&gt;&lt;item&gt;Amenable to automatic parallelization.&lt;/item&gt;&lt;item&gt;Amenable to incremental maintenance.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But SQL is the only widely-used implementation of the relational model, and it is:&lt;/p&gt;
    &lt;p&gt;This isn't just a matter of some constant programmer overhead, like SQL queries taking 20% longer to write. The fact that these issues exist in our dominant model for accessing data has dramatic downstream effects for the entire industry:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complexity is a massive drag on quality and innovation in runtime and tooling&lt;/item&gt;
      &lt;item&gt;The need for an application layer with hand-written coordination between database and client renders useless most of the best features of relational databases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The core message that I want people to take away is that there is potentially a huge amount of value to be unlocked by replacing SQL, and more generally in rethinking where and how we draw the lines between databases, query languages and programming languages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inexpressive&lt;/head&gt;
    &lt;p&gt;Talking about expressiveness is usually difficult, since it's a very subjective measure. But SQL is a particularly inexpressive language. Many simple types and computations can't be expressed at all. Others require far more typing than they need to. And often the structure is fragile - small changes to the computation can require large changes to the code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can't be expressed&lt;/head&gt;
    &lt;p&gt;Let's start with the easiest examples - things that can't be expressed in SQL at all.&lt;/p&gt;
    &lt;p&gt;For example, SQL:2016 added support for json values. In most languages json support is provided by a library. Why did SQL have to add it to the language spec?&lt;/p&gt;
    &lt;p&gt;First, while SQL allows user-defined types, it doesn't have any concept of a sum type. So there is no way for a user to define the type of an arbitrary json value:&lt;/p&gt;
    &lt;code&gt;enum Json {
    Null,
    Bool(bool),
    Number(Number),
    String(String),
    Array(Vec&amp;lt;Value&amp;gt;),
    Object(Map&amp;lt;String, Value&amp;gt;),
}
&lt;/code&gt;
    &lt;p&gt;The usual response to complaints about the lack of sum types in sql is that you should use an id column that joins against multiple tables, one for each possible type.&lt;/p&gt;
    &lt;code&gt;create table json_value(id integer);
create table json_bool(id integer, value bool)
create table json_number(id integer, value double);
create table json_string(id integer, value text);
create table json_array(id integer);
create table json_array_elements(id integer, position integer, value json_value, foreign key (value) references json_value(id));
create table json_object(id integer);
create table json_object_properties(id integer, key text, value json_value, foreign key (value) references json_value(id));
&lt;/code&gt;
    &lt;p&gt;This works for data modelling (although it's still clunky because you must try joins against each of the tables at every use site rather than just ask the value which table it refers to). But this solution is clearly inappropriate for modelling a value like json that can be created inside scalar expressions, where inserts into some global table are not allowed.&lt;/p&gt;
    &lt;p&gt;Second, parsing json requires iteration. SQLs &lt;code&gt;with recursive&lt;/code&gt; is limited to linear recursion and has a bizarre choice of semantics - each step can access only the results from the previous step, but the result of the whole thing is the union of all the steps. This makes parsing, and especially backtracking, difficult. Most SQL databases also have a procedural sublanguage that has explicit iteration, but there are few commonalities between the languages in different databases. So there is no pure-SQL json parser that works across different databases.&lt;/p&gt;
    &lt;p&gt;Third, most databases have some kind of extension system that allows adding new types and functions using a regular programming language (usually c). Indeed, this is how json support first appeared in many databases. But again these extension systems are not at all standardized so it's not feasible to write a library that works across many databases.&lt;/p&gt;
    &lt;p&gt;So instead the best we can do is add json to the SQL spec and hope that all the databases implement it in a compatible way (they don't).&lt;/p&gt;
    &lt;p&gt;The same goes for xml, regular expressions, windows, multi-dimensional arrays, periods etc.&lt;/p&gt;
    &lt;p&gt;Compare how flink exposes windowing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The interface is made out of objects and function calls, both of which are first-class values and can be stored in variables and passed as function arguments.&lt;/item&gt;
      &lt;item&gt;The style of windowing is defined by a WindowAssigner which simply takes a row and returns a set of window ids.&lt;/item&gt;
      &lt;item&gt;Several common styles of windows are provided as library code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vs SQL:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The interface adds a substantial amount of new syntax to the language.&lt;/item&gt;
      &lt;item&gt;The windowing style is purely syntactic - it is not a value that can be assigned to a variable or passed to a function. This means that we can't compress common windowing patterns.&lt;/item&gt;
      &lt;item&gt;Only a few styles of windowing are provided and they are hard-coded into the language.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why is the SQL interface defined this way?&lt;/p&gt;
    &lt;p&gt;Much of this is simply cultural - this is just how new SQL features are designed.&lt;/p&gt;
    &lt;p&gt;But even if we wanted to mimic the flink interface we couldn't do it in SQL.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Functions are not values that can be passed around, and they can't take tables or other functions as arguments. So complex operations such as windowing can't be added as stdlib functions.&lt;/item&gt;
      &lt;item&gt;Without sum types we can't even express the hardcoded windowing styles as a value. So we're forced to add new syntax whenever we want to parameterize some operation with several options.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Verbose to express&lt;/head&gt;
    &lt;p&gt;Joins are at the heart of the relational model. SQL's syntax is not unreasonable in the most general case, but there are many repeated join patterns that deserve more concise expression.&lt;/p&gt;
    &lt;p&gt;By far the most common case for joins is following foreign keys. SQL has no special syntax for this:&lt;/p&gt;
    &lt;code&gt;select foo.id, quux.value 
from foo, bar, quux 
where foo.bar_id = bar.id and bar.quux_id = quux.id
&lt;/code&gt;
    &lt;p&gt;Compare to eg alloy, which has a dedicated syntax for this case:&lt;/p&gt;
    &lt;code&gt;foo.bar.quux
&lt;/code&gt;
    &lt;p&gt;Or libraries like pandas or flink, where it's trivial to write a function that encapsulates this logic:&lt;/p&gt;
    &lt;code&gt;fk_join(foo, 'bar_id', bar, 'quux_id', quux)
&lt;/code&gt;
    &lt;p&gt;Can we write such a function in sql? Most databases don't allow functions to take tables as arguments, and also require the column names and types of the input and output tables to be fixed when the function is defined. SQL:2016 introduced polymorphic table functions, which might allow writing something like &lt;code&gt;fk_join&lt;/code&gt; but so far only oracle has implemented them (and they didn't follow the spec!).&lt;/p&gt;
    &lt;p&gt;Verbose syntax for such core operations has chilling effects downstream, such as developers avoiding 6NF even in situations where it's useful, because all their queries would balloon in size.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fragile structure&lt;/head&gt;
    &lt;p&gt;There are many cases where a small change to a computation requires totally changing the structure of the query, but subqueries are my favourite because they're the most obvious way to express many queries and yet also provide so many cliffs to fall off.&lt;/p&gt;
    &lt;code&gt;-- for each manager, find their employee with the highest salary
&amp;gt; select
&amp;gt;   manager.name,
&amp;gt;   (select employee.name
&amp;gt;    from employee
&amp;gt;    where employee.manager = manager.name
&amp;gt;    order by employee.salary desc
&amp;gt;    limit 1)
&amp;gt; from manager;
 name  | name
-------+------
 alice | bob
(1 row)

-- what if we want to return the salary too?
&amp;gt; select
&amp;gt;   manager.name,
&amp;gt;   (select employee.name, employee.salary
&amp;gt;    from employee
&amp;gt;    where employee.manager = manager.name
&amp;gt;    order by employee.salary desc
&amp;gt;    limit 1)
&amp;gt; from manager;
ERROR:  subquery must return only one column
LINE 3:   (select employee.name, employee.salary
          ^

-- the only solution is to change half of the lines in the query
&amp;gt; select manager.name, employee.name, employee.salary
&amp;gt; from manager
&amp;gt; join lateral (
&amp;gt;   select employee.name, employee.salary
&amp;gt;   from employee
&amp;gt;   where employee.manager = manager.name
&amp;gt;   order by employee.salary desc
&amp;gt;   limit 1
&amp;gt; ) as employee
&amp;gt; on true;
 name  | name | salary
-------+------+--------
 alice | bob  |    100
(1 row)
&lt;/code&gt;
    &lt;p&gt;This isn't terrible in such a simple example, but in analytics it's not uncommon to have to write queries that are hundreds of lines long and have many levels of nesting, at which point this kind of restructuring is laborious and error-prone.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incompressible&lt;/head&gt;
    &lt;p&gt;Code can be compressed by extracting similar structures from two or more sections. For example, if a calculation was used in several places we could assign it to a variable and then use the variable in those places. Or if the calculation depended on different inputs in each place, we could create a function and pass the different inputs as arguments.&lt;/p&gt;
    &lt;p&gt;This is programming 101 - variables, functions and expression substitution. How does SQL fare on this front?&lt;/p&gt;
    &lt;head rend="h3"&gt;Variables&lt;/head&gt;
    &lt;p&gt;Scalar values can be assigned to variables, but only as a column inside a relation. You can't name a thing without including it in the result! Which means that if you want a temporary scalar variable you must introduce a new &lt;code&gt;select&lt;/code&gt; to get rid off it. And also name all your other values.&lt;/p&gt;
    &lt;code&gt;-- repeated structure
select a+((z*2)-1), b+((z*2)-1) from foo;

-- compressed?
select a2, b2 from (select a+tmp as a2, b+tmp as b2, (z*2)-1 as tmp from foo);
&lt;/code&gt;
    &lt;p&gt;You can use &lt;code&gt;as&lt;/code&gt; to name scalar values anywhere they appear. Except in a &lt;code&gt;group by&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;-- can't name this value
&amp;gt; select x2 from foo group by x+1 as x2;
ERROR:  syntax error at or near "as"
LINE 1: select x2 from foo group by x+1 as x2;

-- sprinkle some more select on it
&amp;gt; select x2 from (select x+1 as x2 from foo) group by x2;
 ?column?
----------
(0 rows)
&lt;/code&gt;
    &lt;p&gt;Rather than fix this bizarre oversight, the SQL spec allows a novel form of variable naming - you can refer to a column by using an expression which produces the same parse tree as the one that produced the column.&lt;/p&gt;
    &lt;code&gt;-- this magically works, even though x is not in scope in the select
&amp;gt; select (x + 1)*2 from foo group by x+1;
 ?column?
----------
(0 rows)

-- but this doesn't, because it isn't the same parse tree
&amp;gt; select (x + +1)*2 from foo group by x+1;
ERROR:  column "foo.x" must appear in the GROUP BY clause or be used in an aggregate function
LINE 1: select (x + +1)*2 from foo group by x+1;
                ^
&lt;/code&gt;
    &lt;p&gt;Of course, you can't use this feature across any kind of syntactic boundary. If you wanted to, say, assign this table to a variable or pass it to a function, then you need to both repeat the expression and explicitly name it;&lt;/p&gt;
    &lt;code&gt;&amp;gt; with foo_plus as (select x+1 from foo group by x+1)
&amp;gt; select (x+1)*2 from foo_plus;
ERROR:  column "x" does not exist
LINE 2: select (x+1)*2 from foo_plus;
                ^
               
&amp;gt; with foo_plus as (select x+1 as x_plus from foo group by x+1)
&amp;gt; select x_plus*2 from foo_plus;
 ?column?
----------
(0 rows)
&lt;/code&gt;
    &lt;p&gt;SQL was first used in the early 70s, but if your repeated value was a table then you were out of luck until CTEs were added in SQL:99.&lt;/p&gt;
    &lt;code&gt;-- repeated structure
select * 
from 
  (select x, x+1 as x2 from foo) as foo1 
left join 
  (select x, x+1 as x2 from foo) as foo2 
on 
  foo1.x2 = foo2.x;
  
-- compressed?
with foo_plus as 
  (select x, x+1 as x2 from foo)
select * 
from 
  foo_plus as foo1 
left join 
  foo_plus as foo2 
on 
  foo1.x2 = foo2.x;
&lt;/code&gt;
    &lt;head rend="h3"&gt;Functions&lt;/head&gt;
    &lt;p&gt;Similarly, if your repeated calculations have different inputs then you were out of luck until scalar functions were added in SQL:99.&lt;/p&gt;
    &lt;code&gt;-- repeated structure
select a+((x*2)-1), b+((y*2)-1) from foo;

-- compressed?
create function bar(integer, integer) returns integer
    as 'select $1+(($2*2)-1);'
    language sql;
select bar(a,x), bar(b,y) from foo;
&lt;/code&gt;
    &lt;p&gt;Functions that return tables weren't added until SQL:2003.&lt;/p&gt;
    &lt;code&gt;-- repeated structure
(select x from foo)
union
(select x+1 from foo)
union
(select x+2 from foo)
union
(select x from bar)
union
(select x+1 from bar)
union
(select x+2 from bar);

-- compressed?
create function increments(integer) returns setof integer 
    as $$
        (select $1) 
        union 
        (select $1+1) 
        union 
        (select $1+2);
    $$
    language sql;
(select increments(x) from foo)
union
(select increments(x) from bar);
&lt;/code&gt;
    &lt;p&gt;What if you want to compress a repeated calculation that produces more than one table as a result? Tough!&lt;/p&gt;
    &lt;p&gt;What if you want to compress a repeated calculation where one of the inputs is a table? The spec doesn't explicitly disallow this, but it isn't widely supported. SQL server can do it with this lovely syntax:&lt;/p&gt;
    &lt;code&gt;-- compressed?

create type foo_like as table (x int);

create function increments(@foo foo_like readonly) returns table
    as return
        (select x from @foo) 
        union 
        (select x+1 from @foo) 
        union 
        (select x+2 from @foo);
        
declare @foo as foo_like;
insert into @foo select * from foo;

declare @bar as foo_like;
insert into @bar select * from bar;

increments(@foo) union increments(@bar);
&lt;/code&gt;
    &lt;p&gt;Aside from the weird insistence that we can't just pass a table directly to our function, this example points to a more general problem: column names are part of types. If in our example &lt;code&gt;bar&lt;/code&gt; happened to have a different column name then we would have had to write:&lt;/p&gt;
    &lt;code&gt;increments(@foo) union increments(select y as x from @bar)
&lt;/code&gt;
    &lt;p&gt;Since columns names aren't themselves first-class this makes it hard to compress repeated structure that happens to involve different names:&lt;/p&gt;
    &lt;code&gt;-- repeated structure
select a,b,c,x,y,z from foo order by a,b,c,x,y,z;

-- fantasy land
with ps as (columns 'a,b,c,x,y,z')
select $ps from foo order by $ps
&lt;/code&gt;
    &lt;p&gt;The same is true of windows, collations, string encodings, the part argument to &lt;code&gt;extract&lt;/code&gt; ... pretty much anything that involves one of the several hundred SQL keywords.&lt;/p&gt;
    &lt;p&gt;Functions and types are also not first-class, so repeated structures involving different functions or types can't be compressed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expression substitution&lt;/head&gt;
    &lt;p&gt;To be able to compress repeated structure we must be able to replace the verbose version with the compressed version. In many languages, there is a principle that it's always possible to replace any expression with another expression that has the same value. SQL breaks this principle in (at least) two ways.&lt;/p&gt;
    &lt;p&gt;Firstly, it's only possible to substitute one expression for another when they are both the same type of expression. SQL has statements (DDL), table expressions and scalar expressions.&lt;/p&gt;
    &lt;p&gt;Using a scalar expression inside a table expression requires first wrapping the entire thing with a new &lt;code&gt;select&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Using a table expression inside a scalar expression is generally not possible, unless the table expression returns only 1 column and either a) the table expression is guaranteed to return at most 1 row or b) your usage fits into one of the hard-coded patterns such as &lt;code&gt;exists&lt;/code&gt;. Otherwise, as we saw in the most-highly-paid-employee example earlier, it must be rewritten as a lateral join against the nearest enclosing table expression.&lt;/p&gt;
    &lt;p&gt;Secondly, table expressions aren't all made equal. Some table expressions depend not only on the value of an inner expression, but the syntax. For example:&lt;/p&gt;
    &lt;code&gt;-- this is fine - the spec allows `order by` to see inside the `(select ...)`
-- and make use of a column `y` that doesn't exist in the returned value
&amp;gt; (select x from foo) order by y;
 x
---
 3
(1 row)

-- same value in the inner expression
-- but the spec doesn't have a syntactic exception for this case
&amp;gt; (select x from (select x from foo) as foo2) order by y;
ERROR:  column "y" does not exist
LINE 1: (select x from (select x from foo) as foo2) order by y;
&lt;/code&gt;
    &lt;p&gt;In such cases it's not possible to compress repeated structure without first rewriting the query to explicitly select and then drop the magic column:&lt;/p&gt;
    &lt;code&gt;select x from ((select x,y from foo) order by y);
&lt;/code&gt;
    &lt;head rend="h2"&gt;Non-porous&lt;/head&gt;
    &lt;p&gt;I took the term 'porous' from Some Were Meant For C, where Stephen Kell argues that the endurance of c is down to it's extreme openness to interaction with other systems via foreign memory, FFI, dynamic linking etc. He contrasts this with managed languages which don't allow touching anything in the entire memory space without first notifying the GC, have their own internal notions of namespaces and linking which they don't expose to the outside world, have closed build systems which are hard to interface with other languages' build systems etc.&lt;/p&gt;
    &lt;p&gt;For non-porous languages to succeed they have to eat the whole world - gaining enough users that the entire programming ecosystem can be duplicated within their walled garden. But porous languages can easily interact with existing systems and make use of existing libraries and tools.&lt;/p&gt;
    &lt;p&gt;Whether or not you like this argument as applied to c, the notion of porousness itself is a useful lens for system design. When we apply it to SQL databases, we see that individual databases are often porous in many aspects of their design but the mechanisms are almost always not portable. So while individual databases can be extended in many ways, the extensions can't be shared between databases easily and the SQL spec is still left trying to eat the whole world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Language level&lt;/head&gt;
    &lt;p&gt;Most SQL databases have language-level escape hatches for defining new types and functions via a mature programming language (usually c). The syntax for declaring these in SQL is defined in the spec but the c interface and calling convention is not, so these are not portable across different databases.&lt;/p&gt;
    &lt;code&gt;-- sql side

CREATE FUNCTION add_one(integer) RETURNS integer
     AS 'DIRECTORY/funcs', 'add_one'
     LANGUAGE C STRICT;
&lt;/code&gt;
    &lt;code&gt;// c side

#include "postgres.h"
#include &amp;lt;string.h&amp;gt;
#include "fmgr.h"
#include "utils/geo_decls.h"

PG_MODULE_MAGIC;

PG_FUNCTION_INFO_V1(add_one);

Datum
add_one(PG_FUNCTION_ARGS)
{
    int32   arg = PG_GETARG_INT32(0);

    PG_RETURN_INT32(arg + 1);
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Runtime level&lt;/head&gt;
    &lt;p&gt;Many SQL databases also have runtime-level extension mechanisms for creating new index types and storage methods (eg postgis) and also for supplying hints to the optimizer. Again, these extensions are not portable across different implementations. At this level it's hard to see how they could be, as they can be deeply entangled with design decisions in the database runtime, but it's worth noting that if they were portable then much of the SQL spec would not need to exist.&lt;/p&gt;
    &lt;p&gt;The SQL spec also has an extension SQL/MED which defines how to query data that isn't owned by the database, but it isn't widely or portably implemented.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interface level&lt;/head&gt;
    &lt;p&gt;At the interface level, the status quo is much worse. Each database has a completely different interface protocol.&lt;/p&gt;
    &lt;p&gt;The protocols I'm familiar with are all ordered, synchronous and allow returning only one relation at a time. Many don't even support pipelining. For a long time SQL also lacked any way to return nested structures and even now (with json support) it's incredibly verbose.&lt;/p&gt;
    &lt;p&gt;This meant that if you wanted to return, say, a list of user profiles and their followers, you would have to make multiple round-trips to the database. Latency considerations make this unfeasible over longer distances. This practically mandates the existence of an application layer whose main purpose is to coalesce multiple database queries and reassemble their nested structure using hand-written joins over the output relations - duplicating work that the database is supposed to be good at.&lt;/p&gt;
    &lt;p&gt;Protocols also typically return metadata as text in an unspecified format with no parser supplied (even if there is a binary protocol for SQL values, metadata is still typically returned as a 1-row 1-column table containing a string). This makes it harder than necessary to build any kind of tooling outside of the database. Eg if we wanted to parse plans and verify that they don't contain any table scans or nested loops.&lt;/p&gt;
    &lt;p&gt;Similarly, SQL is submitted to the database as a text format identical to what the programmer would type. Since the syntax is so complicated, it's difficult for other languages to embed, validate and escape SQL queries and to figure out what types they return. (Query parameters are not a panacea for escaping - often you need to vary query structure depending on user input, not just values).&lt;/p&gt;
    &lt;p&gt;SQL databases are also typically monolithic. You can't, for example, just send a query plan directly to postgres. Or call the planner as a library to help make operational forecasts based on projected future workloads. Looking at the value unlocked by eg pg_query gives the sense that there could be a lot to gain by exposing more of the innards of SQL systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complexity drag&lt;/head&gt;
    &lt;p&gt;In modern programming languages, the language itself consists of a small number of carefully chosen primitives. Programmers combine these to build up the rest of the functionality, which can be shared in the form of libraries. This lowers the burden on the language designers to foresee every possible need and allows new implementations to reuse existing functionality. Eg if you implement a new javascript interpreter, you get the whole javascript ecosystem for free.&lt;/p&gt;
    &lt;p&gt;Because SQL is so inexpressive, incompressible and non-porous it was never able to develop a library ecosystem. Instead, any new functionality that is regularly needed is added to the spec, often with it's own custom syntax. So if you develop a new SQL implementation you must also implement the entire ecosystem from scratch too because users can't implement it themselves.&lt;/p&gt;
    &lt;p&gt;This results in an enormous language.&lt;/p&gt;
    &lt;p&gt;The core SQL language is defined in part 2 (of 9) of the SQL 2016 spec. Part 2 alone is 1732 pages. By way of comparison, the javascript 2021 spec is 879 pages and the c++ 2020 spec is 1853 pages.&lt;/p&gt;
    &lt;p&gt;But the SQL spec is not even complete!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A quick grep of the SQL standard indicates 411 occurrences of implementation-defined behavior. And not in some obscure corner cases, this includes basic language features. For a programming language that would be ridiculous. But for some reason people accept the fact that SQL is incredibly under-specified, and that it is impossible to write even relatively simple analytical queries in a way that is portable across database systems.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notably, the spec does not define type inference at all, which means that the results of basic arithmetic are implementation-defined. Here is an example from the sqlite test suite in various databases:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT DISTINCT - + 34 + + - 26 + - 34 + - 34 + + COALESCE ( 93, COUNT ( * ) + + 44 - 16, - AVG ( + 86 ) + 12 ) / 86 * + 55 * + 46;
2402

postgres&amp;gt; SELECT DISTINCT - + 34 + + - 26 + - 34 + - 34 + + COALESCE ( 93, COUNT ( * ) + + 44 - 16, - AVG ( + 86 ) + 12 ) / 86 * + 55 * + 46;
       ?column?
-----------------------
 2607.9302325581395290
(1 row)

mariadb&amp;gt; SELECT DISTINCT - + 34 + + - 26 + - 34 + - 34 + + COALESCE ( 93, COUNT ( * ) + + 44 - 16, - AVG ( + 86 ) + 12 ) / 86 * + 55 * + 46;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '* ) + + 44 - 16, - AVG ( + 86 ) + 12 ) / 86 * + 55 * + 46' at line 1
&lt;/code&gt;
    &lt;p&gt;The spec also declares that certain operations should produce errors when evaluated, but since it doesn't define an evaluation order the decision is left down to the optimizer. A query that runs fine in your database today might return an error tomorrow on the same data if the optimizer produces a different plan.&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; select count(foo.bar/0) from (select 1 as bar) as foo where foo.bar = 0;
0

postgres&amp;gt; select count(foo.bar/0) from (select 1 as bar) as foo where foo.bar = 0;
ERROR:  division by zero

mariadb&amp;gt; select count(foo.bar/0) from (select 1 as bar) as foo where foo.bar = 0;
+------------------+
| count(foo.bar/0) |
+------------------+
|                0 |
+------------------+
1 row in set (0.001 sec)
&lt;/code&gt;
    &lt;p&gt;And despite being enormous and not even definining the whole language, the spec still manages to define a language so anemic that every database ends up with a raft of non-standard extensions to compensate.&lt;/p&gt;
    &lt;p&gt;Even if all the flaws I listed in the previous sections were to be fixed in the future, SQL already ate a monstrous amount of complexity in workarounds for those flaws and that complexity will never be removed from the spec. This complexity has a huge impact on the effort required to implement a new SQL engine.&lt;/p&gt;
    &lt;p&gt;To take an example close to my heart: Differential dataflow is a dataflow engine that includes support for automatic parallel execution, horizontal scaling and incrementally maintained views. It totals ~16kloc and was mostly written by a single person. Materialize adds support for SQL and various data sources. To date, that has taken ~128kloc (not including dependencies) and I estimate ~15-20 engineer-years. Just converting SQL to the logical plan takes ~27kloc, more than than the entirety of differential dataflow.&lt;/p&gt;
    &lt;p&gt;Similarly, sqlite looks to have ~212kloc and duckdb ~141kloc. The count for duckdb doesn't even include the parser that they (sensibly) borrowed from postgres, which at ~47kloc is much larger than the entire ~30kloc codebase for lua.&lt;/p&gt;
    &lt;p&gt;Materialize passes more than 7 million tests, including the entire sqlite logic test suite and much of the cockroachdb logic test suite. And yet they are still discovering (my) bugs in such core components as name resolution, which in any sane language would be trivial.&lt;/p&gt;
    &lt;p&gt;The entire database industry is hauling a massive SQL-shaped parachute behind them. This complexity creates a drag on everything downstream.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quality of implementation suffers&lt;/head&gt;
    &lt;p&gt;There is so much ground to cover that it's not possible to do a good job of all of it. Subqueries, for example, add some much-needed expressiveness to SQL but their use is usually not recommended because most databases optimize them poorly or not at all.&lt;/p&gt;
    &lt;p&gt;This affects UX too. Every SQL database I've used has terrible syntax errors.&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; with q17_part as (
   ...&amp;gt;   select p_partkey from part where
   ...&amp;gt;   p_brand = 'Brand#23'
   ...&amp;gt;   and p_container = 'MED BOX'
   ...&amp;gt; ),
   ...&amp;gt; q17_avg as (
   ...&amp;gt;   select l_partkey as t_partkey, 0.2 * avg(l_quantity) as t_avg_quantity
   ...&amp;gt;   from lineitem
   ...&amp;gt;   where l_partkey IN (select p_partkey from q17_part)
   ...&amp;gt;   group by l_partkey
   ...&amp;gt; ),
   ...&amp;gt; q17_price as (
   ...&amp;gt;   select
   ...&amp;gt;   l_quantity,
   ...&amp;gt;   l_partkey,
   ...&amp;gt;   l_extendedprice
   ...&amp;gt;   from
   ...&amp;gt;   lineitem
   ...&amp;gt;   where
   ...&amp;gt;   l_partkey IN (select p_partkeyfrom q17_part)
   ...&amp;gt; ),
   ...&amp;gt; select cast(sum(l_extendedprice) / 7.0 as decimal(32,2)) as avg_yearly
   ...&amp;gt; from q17_avg, q17_price
   ...&amp;gt; where
   ...&amp;gt; t_partkey = l_partkey and l_quantity &amp;lt; t_avg_quantity;   
Error: near "select": syntax error
&lt;/code&gt;
    &lt;p&gt;But it's hard to produce good errors when your grammar contains 1732 non-terminals. And several hundred keywords. And allows using (some) keywords as identifiers. And contains many many ambiguities which mean that typos are often valid but nonsensical SQL.&lt;/p&gt;
    &lt;head rend="h3"&gt;Innovation at the implementation level is gated&lt;/head&gt;
    &lt;p&gt;Incremental maintenance, parallel execution, provenance, equivalence checking, query synthesis etc. These show up in academic papers, produce demos for simplified subsets of SQL, and then disappear.&lt;/p&gt;
    &lt;p&gt;In the programming language world we have a smooth pipeline that takes basic research and applies it to increasingly realistic languages, eventually producing widely-used industrial-quality tools. But in the database world there is a missing step between demos on toy relational algebras and handling the enormity of SQL, down which most compelling research quietly plummets. Bringing anything novel to a usable level requires a substantial investment of time and money that most researchers simply don't have.&lt;/p&gt;
    &lt;head rend="h3"&gt;Portability is a myth&lt;/head&gt;
    &lt;p&gt;The spec is too large and too incomplete, and the incentives to follow the spec too weak. For example, the latest postgres docs note that "at the time of writing, no current version of any database management system claims full conformance to Core SQL:2016". It also lists a few dozen departures from the spec.&lt;/p&gt;
    &lt;p&gt;This is exacerbated by the fact that every database also has to invent myriad non-standard extensions to cover the weaknesses of standard SQL.&lt;/p&gt;
    &lt;p&gt;Where the average javascript program can be expected to work in any interpreter, and the average c program might need to macro-fy some compiler builtins, the average body of SQL queries will need serious editing to run on a different database and even then can't be expected to produce the same answers.&lt;/p&gt;
    &lt;p&gt;One of the big selling points for supporting SQL in a new database is that existing tools that emit SQL will be able to run unmodified. But in practice, such tools almost always end up maintaining separate backends for every dialect, so unless you match an existing database bug-for-bug you'll still have to add a new backend to every tool.&lt;/p&gt;
    &lt;p&gt;Similarly, users will be able to carry across some SQL knowledge, but will be regularly surprised by inconsistencies in syntax, semantics and the set of available types and functions. And unlike the programming language world they won't be able to carry across any existing code or libraries.&lt;/p&gt;
    &lt;p&gt;This means that the network effects of SQL are much weaker than they are for programming languages, which makes it all the more surprising that we have a bounty of programming languages but only one relational database language.&lt;/p&gt;
    &lt;head rend="h2"&gt;The application layer&lt;/head&gt;
    &lt;p&gt;The original idea of relational databases was that they would be queried directly from the client. With the rise of the web this idea died - SQL is too complex to be easily secured against adversarial input, cache invalidation for SQL queries is too hard, and there is no way to easily spawn background tasks (eg resizing images) or to communicate with the rest of the world (eg sending email). And the SQL language itself was not an appealing target for adding these capabilities.&lt;/p&gt;
    &lt;p&gt;So instead we added the 'application layer' - a process written in a reasonable programming language that would live between the database and the client and manage their communication. And we invented ORM to patch over the weaknesses of SQL, especially the lack of compressibility.&lt;/p&gt;
    &lt;p&gt;This move was necessary, but costly.&lt;/p&gt;
    &lt;p&gt;ORMs are prone to n+1 query bugs and feral concurrency. To rephrase, they are bad at efficiently querying data and bad at making use of transactions - two of the core features of relational databases.&lt;/p&gt;
    &lt;p&gt;As for the application layer: Converting queries into rest endpoints by hand is a lot of error-prone boilerplate work. Managing cache invalidation by hand leads to a steady supply of bugs. If endpoints are too fine-grained then clients have to make multiple roundtrip calls, but if they're too coarse then clients waste bandwidth on data they didn't need. And there is no hope of automatically notifying clients when the result of their query has changed.&lt;/p&gt;
    &lt;p&gt;The success of GraphQL shows that these pains are real and that people really do want to issue rich queries directly from the client. Compared to SQL, GraphQL is substantially easier to implement, easier to cache, has a much smaller attack surface, has various mechanisms for compressing common patterns, makes it easy to follow foreign keys and return nested results, has first-class mechanisms for interacting with foreign code and with the outside world, has a rich type system (with union types!), and is easy to embed in other languages.&lt;/p&gt;
    &lt;p&gt;Similarly for firebase (before it was acqui-smothered by google). It dropped the entire application layer and offered streaming updates to client-side queries, built-in access control, client-side caching etc. Despite offering very little in the way of runtime innovation compared to existing databases, it was able to succesfully compete by recognizing that the current division of database + sql + orm + application-layer is a historical accident and can be dramatically simplified.&lt;/p&gt;
    &lt;p&gt;The overall vibe of the NoSQL years was "relations bad, objects good". I fear that what many researchers and developers are taking away from the success of GraphQL and co is but a minor update - "relations bad, &lt;del&gt;objects&lt;/del&gt; graphs good".&lt;/p&gt;
    &lt;p&gt;This is a mistake. GraphQL is still more or less a relational model, as evidenced by the fact that it's typically backed by wrappers like hasura that allow taking advantage of the mature runtimes of relational databases. The key to the success of GraphQL was not doing away with relations, but recognizing and fixing the real flaws in SQL that were hobbling relational databases, as well as unbundling the query language from a single monolithic storage and execution engine.&lt;/p&gt;
    &lt;head rend="h2"&gt;After SQL?&lt;/head&gt;
    &lt;p&gt;To summarize:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Design flaws in the SQL language resulted in a language with no library ecosystem and a burdensome spec which limits innovation.&lt;/item&gt;
      &lt;item&gt;Additional design flaws in SQL database interfaces resulted in moving as much logic as possible to the application layer and limiting the use of the most valuable features of the database.&lt;/item&gt;
      &lt;item&gt;It's probably too late to fix either of these.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the idea of modelling data with a declarative disorderly language is still valuable. Maybe more so than ever, given the trends in hardware. What should a new language learn from SQL's successes and mistakes?&lt;/p&gt;
    &lt;p&gt;We can get pretty far by just negating every mistake listed in this post, while ensuring we retain the ability to produce and optimize query plans:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start with the structure that all modern languages have converged towards. &lt;list rend="ul"&gt;&lt;item&gt;Everything is an expression.&lt;/item&gt;&lt;item&gt;Variables and functions have compact syntax.&lt;/item&gt;&lt;item&gt;Few keywords - most things are stdlib functions rather than builtin syntax.&lt;/item&gt;&lt;item&gt;Have an explicit type system rather than totally disjoint syntax for scalar expressions vs table expressions.&lt;/item&gt;&lt;item&gt;Ensure that it's always possible to replace a given expression with another expression that has the same type and value.&lt;/item&gt;&lt;item&gt;Define a (non-implementation specific) system for distributing and loading (and unloading!) libraries.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Keep the spec simple and complete. &lt;list rend="ul"&gt;&lt;item&gt;Simple denotational semantics for the core language.&lt;/item&gt;&lt;item&gt;Completely specify type inference, error semantics etc.&lt;/item&gt;&lt;item&gt;Should be possible for an experienced engineer to throw together a slow but correct interpreter in a week or two.&lt;/item&gt;&lt;item&gt;Encode semantics in a model checker or theorem prover to eg test optimizations. Ship this with the spec.&lt;/item&gt;&lt;item&gt;Lean on wasm as an extension language - avoids having to spec arithemetic, strings etc if they can be defined as a library over some bits type.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Make it compressible. &lt;list rend="ul"&gt;&lt;item&gt;Allow functions to take relations and other functions are arguments (can be erased by specialization before planning, ala rust or julia).&lt;/item&gt;&lt;item&gt;Allow functions to operate on relations polymorphically (ie without having to fix the columns and types when writing the function).&lt;/item&gt;&lt;item&gt;Make column names, orderings, collations, window specifications etc first-class values rather than just syntax (can use staging ala zig's comptime if these need to be constant at planning time).&lt;/item&gt;&lt;item&gt;Compact syntax for simple joins (eg snowflake schemas, graph traversal).&lt;/item&gt;&lt;item&gt;True recursion / fixpoints (allows expressing iterative algorithms like parsing).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Make it porous. &lt;list rend="ul"&gt;&lt;item&gt;Allow defining new types, functions, indexes, plan operators etc via wasm plugins (with the calling convention etc in the spec).&lt;/item&gt;&lt;item&gt;Expose plans, hints etc via api (not via strings).&lt;/item&gt;&lt;item&gt;Spec both a human-friendly encoding and a tooling-friendly encoding (probably text vs binary like wasm). Ship an embedabble library that does parsing and type inference.&lt;/item&gt;&lt;item&gt;Make returning nested structures (eg json) ergonomic, or at least allow returning multiple relations.&lt;/item&gt;&lt;item&gt;Create a subset of the language that can be easily verified to run in reasonable time (eg no table scans, no nested loops).&lt;/item&gt;&lt;item&gt;Allow exposing subset to clients via graphql-like authorization rules.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Better layering. &lt;list rend="ul"&gt;&lt;item&gt;Separate as much as possible out into embeddable libraries (ala pg_query).&lt;/item&gt;&lt;item&gt;Expose storage, transaction, execution as apis. The database server just receives and executes wasm against these apis.&lt;/item&gt;&lt;item&gt;Distribute query parser/planner/compiler as a library so clients can choose to use modified versions to produce wasm for the server.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Strategies for actually getting people to use the thing are much harder.&lt;/p&gt;
    &lt;p&gt;Tackling the entire stack at once seems challenging. Rethinkdb died. Datomic is alive but the company was acquihired. Neo4j, on the other hand, seems to be catnip for investors, so who knows.&lt;/p&gt;
    &lt;p&gt;A safer approach is to first piggy-back on existing databases runtime. EdgeDB uses the postgres runtime. Logica compiles to SQL. GraphQL has compilers for many different query languages.&lt;/p&gt;
    &lt;p&gt;Another option is to find an untapped niche and work outward from there. I haven't seen this done yet, but there are a lot of relational-ish query niches. Pandas targets data cleaning/analysis. Datascript is a front-end database. Bloom targets distributed systems algorithms. Semmle targets code analysis. Other potential niches include embedded databases in applications (ala fossils use of sqlite), incremental functions from state to UI, querying as an interface to the state of complex programs etc.&lt;/p&gt;
    &lt;p&gt;In a niche with less competition you could first grow the language and then try to expand the runtime outwards to cover more potential usecases, similar to how sqlite started as a tcl extension and ended up becoming the defacto standard for application-embedded databases, a common choice for data publishing format, and a backend for a variety of data-processing tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Why do you want json in your database? I'm personally not that interested in storing json in tables (although I hear that it's useful for modelling sparse data). But many usecases require returning disparate data that does not fit into a single relation (eg a monitoring dashboard or the logged-in landing page of many websites). If you can't return all this data from a single query, you need an entire extra process co-located with the database whose job it is to coalesce multiple synchronous queries and then join the results together into some structured form to send to the client. This results in duplication of work in the database, hides some of the query structure from the query planner, opens up the potential for n+1 bugs, adds the possibility of partial failure etc. It's a lot of extra complexity that can be avoided if your database can just return all the data in one query (eg see how Hasura translates GraphQL to SQL). This is why people are excited about databases providing GraphQL support.&lt;/p&gt;
    &lt;p&gt;That code belongs in the application layer. Use the right tool for the job! If your database query language is not the right tool for querying data, that seems like a problem.&lt;/p&gt;
    &lt;p&gt;SQL is the only relational language that has even been successful. It's just the natural way of expressing relational queries. LINQ, spark, flink, kafka streams, pandas, dataframes are all widely used examples of an expression-based language-embedded approach to relational queries. Logica, logiql, differential datalog, semmle, datomic are all examples of commercially-deployed datalog-based relational query languages.&lt;/p&gt;
    &lt;p&gt;But SQL enables transactions, logical data independence, plan optimization etc. The relational data model enables those things. None of them require the language to be SQL. Eg logicblox and datomic manage to have transactions, transparent indexes, query planners etc while still having much simpler and more orthogonal query languages than SQL.&lt;/p&gt;
    &lt;p&gt;Javascript is crazy too! Javascript has improved dramatically over the last decade or two, to the point that compatibility between different vendors is almost complete. But imagine a javascript without libraries, without polyfills, where functions couldn't take collections as arguments and where &lt;code&gt;for&lt;/code&gt; loops had a different syntax in each engine. I would, for the record, totally endorse a SQL STRICT MODE which discarded all the silly edge cases and produced a simpler, more orthogonal language. But the database vendors have no incentive to do this - SQL is their moat.&lt;/p&gt;
    &lt;p&gt;SQL has been around for more than 50 years. So has COBOL. Thousands of engineer-years have been invested in the COBOL ecosystem. But it sure seems that noticing COBOL's flaws and designing better successors paid off in the long run. It turns out that we learned a lot about language design in the last 50 years.&lt;/p&gt;
    &lt;p&gt;Complaining is easy. Where's your solution? Perhaps the first step in trying to replace something would be to carefully analyze and discuss the strengths and weaknesses of that thing. Those who don't study history...&lt;/p&gt;
    &lt;p&gt;You just need to learn how SQL works. Bruh.&lt;/p&gt;
    &lt;p&gt;I'd like to finish with this quote from Michael Stonebraker, one of the most prominent figures in the history of relational databases:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;My biggest complaint about System R is that the team never stopped to clean up SQL... All the annoying features of the language have endured to this day. SQL will be the COBOL of 2020...&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.scattered-thoughts.net/writing/against-sql/"/><published>2025-10-25T15:00:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45705125</id><title>Rock Tumbler Instructions: Turning Rough Rocks into Beautiful Tumbled Stones</title><updated>2025-10-25T19:32:20.619490+00:00</updated><content>&lt;doc fingerprint="e3c401461f672ad1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Rock Tumbler Instructions&lt;/head&gt;&lt;head rend="h3"&gt;Directions for Turning Rough Rocks into Beautiful Tumbled Stones&lt;/head&gt;&lt;p&gt;Working to transform rough rock into beautiful tumbled stones gives most people a great feeling of accomplishment. It doesn't matter how old you are or how many batches of rock you have tumbled in the past - when you finish the last tumbling step, rinse off the polish, and see a super-bright luster on colorful polished stones - you are amazed at what you have done.&lt;/p&gt;&lt;head rend="h2"&gt;Rock Tumbling Is Easy&lt;/head&gt;&lt;p&gt;Using a rock tumbler to convert rough rock into sparkling tumbled stones is easy if you follow a simple procedure and observe a few rules. We are writing this to share the procedure that we have used for many years with a number of rotary tumblers.&lt;/p&gt;&lt;p&gt;This procedure works well with materials that have the following properties:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;of adequate quality to accept a polish&lt;/item&gt;&lt;item&gt;a Mohs hardness between 6 and 7&lt;/item&gt;&lt;item&gt;a size between 3/8" and 1 1/2"&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Chalcedony&lt;/head&gt;: agate, bloodstone, carnelian, chrysoprase, jasper, chert, flint, and petrified (silicified) wood.&lt;head rend="h3"&gt;Quartz&lt;/head&gt;: amethyst, aventurine, citrine, milky quartz, rock crystal, rose quartz, smoky quartz, tiger's-eye.&lt;head rend="h3"&gt;Rock Types&lt;/head&gt;: andesite, basalt, diorite, gabbro, granite, mookaite, novaculite, quartzite, unakite.&lt;head rend="h2"&gt;The "Golden Rules" of Rock Tumbling&lt;/head&gt;&lt;p&gt;We follow three "Golden Rules" in all aspects of rock tumbling. They are:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;"Garbage in means garbage out"&lt;/item&gt;&lt;item&gt;"Avoid contamination"&lt;/item&gt;&lt;item&gt;"Great results take time."&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Tumbling will enable you to turn the rough rock on the left side of this photo into the sparkling tumbled stones on the right side of the photo. The results are amazing!&lt;/p&gt;&lt;head rend="h3"&gt;"Garbage in Means Garbage Out"&lt;/head&gt;&lt;p&gt;If you start with garbage (low-quality rough), you should expect low-quality tumbled stones. So, don't hesitate to discard a rock that is porous, fractured, misshapen, or that is not expected to produce an attractive tumbled stone.&lt;/p&gt;&lt;p&gt;You will spend a lot of time and valuable supplies tumbling a batch of rocks. Using quality rough saves time, gives you better value for your money, and produces tumbled stones that are of much higher quality.&lt;/p&gt;&lt;p&gt;We buy lots of tumbling rough from online vendors as part of our hobby and to educate ourselves. We have the best experience buying rough from vendors who: 1) provide clear written descriptions and large clear photos of the rough they are selling, 2) show photos of tumbled stones that they produced themselves from the rock they are selling, and, 3) provide a detailed description of the steps that they followed to tumble the stones. We have the best experience buying rough from people who are actively involved in rock tumbling.&lt;/p&gt;&lt;head rend="h3"&gt;"Avoid Contamination"&lt;/head&gt;&lt;p&gt;You will use a different size tumbler grit for each step of the tumbling process. If coarse grit gets into your medium grit step, it will scratch up the rocks and you might need to do the medium grit step over again.&lt;/p&gt;&lt;p&gt;Avoiding this type of contamination is easy: just thoroughly clean the rocks, the tumbler barrel, and your tools when you change from one grit size to another.&lt;/p&gt;&lt;p&gt;Another way that contamination occurs is when you include rocks that are brittle, or have a granular texture. These rocks might break or shed grains in the tumbler. These grains and broken pieces can scratch up every rock in the barrel.&lt;/p&gt;&lt;p&gt;Here is a test that we use to detect rocks that will shed grains in the tumbler. We pick up a piece of rough in each hand. We then rub them together while applying a bit of pressure. If we are easily dislodging grains from the rock, we believe that the rock will likely shed grains during tumbling.&lt;/p&gt;&lt;p&gt;This type of contamination is also easy to avoid. Simply examine your rocks before tumbling, and don't tumble suspect rocks in the same barrel with quality rough. Tumble new types of rough or suspicious materials separately.&lt;/p&gt;&lt;head rend="h3"&gt;"Great Results Take Time"&lt;/head&gt;&lt;p&gt;Don't be in a hurry. Spend time doing a great job. If you tumble a batch of rocks through the coarse grit step and they still have a few rough edges or are not nicely rounded, don't hesitate to run them through the coarse grit step again. Also, spend the time needed to thoroughly clean your work area, tumbler barrel, rocks, and tools between steps to avoid contamination.&lt;/p&gt;&lt;p&gt;"Garbage in means garbage out." The rocks in this photo do not have the potential to become nice tumbled stones. A rock with voids should be thrown away - the voids will trap grit and contaminate your pre-polish and polishing steps. Protrusions can be trimmed off with a rock saw - and that might yield two nicely rounded rocks.&lt;/p&gt;&lt;head rend="h2"&gt;Inspecting Your Rough&lt;/head&gt;&lt;p&gt;Remember the rule "garbage in means garbage out." Practice that by starting with quality rough, and you will have a chance to produce high-quality tumbled stones. We prepare to tumble by examining our rough rock. If we find porous pieces that might carry grit from one step to the next, we discard them.&lt;/p&gt;&lt;p&gt;Rocks that are fractured will break while tumbling and scratch other rocks in the batch. When we see a fractured rock in our rough, we discard it or break it along that fracture before it is placed in the barrel.&lt;/p&gt;&lt;p&gt;For best results, your tumbler barrel should be loaded with rocks of mixed sizes (from about 1/4 inch up to about 1 1/2 inches in diameter for a 2-pound or 3-pound-capacity barrel). If we need more rocks to fill the barrel to the proper level, we often add rocks that were previously polished but have a rough spot or a blemish that, if ground away, will improve the rock's appearance.&lt;/p&gt;&lt;p&gt;Two final tips before we load the barrel:&lt;/p&gt;&lt;p&gt;1.) Tumbling works best when all of the rocks in the barrel are about the same hardness. If soft rocks are tumbled with harder rocks, the softer rocks will wear away quickly - before the harder rocks are properly shaped and smoothed.&lt;/p&gt;&lt;p&gt;2.) Tumbling works best when all rocks in the barrel are of the same type. If you mix rock types, problems can result - and they will be difficult to diagnose.&lt;/p&gt;&lt;p&gt;When loading the tumbler barrel, you should have pieces of rough with a range of particle sizes. We would mix the above sizes together in the barrel. If you load the barrel with just a few large pieces, there will be very few points of contact between the rocks in the load. Those points of contact are where grit is trapped between the rocks and where grinding occurs. If you have lots of small pieces of rough between the big pieces, there will be many points of contact between the rocks of the load, and the tumbling process will be faster and more effective.&lt;/p&gt;&lt;p&gt;If you don't have small pieces of rock to tumble, you can add small ceramic media to the tumbler barrel. Ceramic media are used as small-size "filler" in tumbling. These tiny cylinders will also act like roller bearings in the barrel and make your load tumble with a smooth action - that smooth action will improve the grinding in the barrel and keep your stones from being bruised. See our video about selecting the right tumbling media.&lt;/p&gt;&lt;head rend="h2"&gt;The Four-Step Tumbling Process&lt;/head&gt;&lt;p&gt;Now you are ready to begin what most people call the "Four-Step Tumbling Process." This is described below for a rotary tumbler with a three-pound-capacity barrel such as the Thumler's Model A-R1, Thumler's Model A-R2, Lortone Model 3A, or the Lortone Model 33B.&lt;/p&gt;&lt;p&gt;If you are tumbling with the Thumler's Model MP-1 tumbler (which has a two-pound-capacity barrel), you can follow the instructions below, but use about two level tablespoons of grit or polish in each of the tumbling steps (Step 1 through Step 4).&lt;/p&gt;&lt;head rend="h2"&gt;Loading the Tumbler Barrel&lt;/head&gt;&lt;p&gt;Before you load the tumbler barrel, be sure that it is perfectly clean. There should be no grit or rock fragments left in the barrel from a previous tumble. To prevent leaks, the rim of the barrel and the lid should be totally free from grit or rock particles.&lt;/p&gt;&lt;p&gt;Once you have a clean barrel, add enough rock to fill the barrel about 1/2 to 2/3 full. With small tumblers it is best to tumble rocks that are between about 1/4" and 1 1/2 inches in size. If you don't have enough rough to fill the barrel at least 2/3 full, the rocks might be tossed around in the tumbler and bruised. (Varieties of quartz bruise very easily.)&lt;/p&gt;&lt;p&gt;It is best to add a variety of rock sizes to the barrel. If you use only large pieces there will be very few contact points between the rocks and very little grinding will occur. If you add a range of rock sizes the small rocks will fill the spaces between the large rocks, creating many more points of contact between the rocks. Grinding occurs when particles of grit get caught between the rocks - so the more points of contact you have, the more effective the grinding.&lt;/p&gt;&lt;p&gt;When tumbling you will place enough rocks in the barrel to make it about 1/2 to 2/3 full. Then, add about two level tablespoons of grit for each pound of rock. Finally, add enough water to almost cover the rock. Now seal the barrel and place it on the tumbler.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 1 - Coarse Grind&lt;/head&gt;&lt;p&gt;The first step of the four-step tumbling process is to run the rocks in the tumbler with coarse grit. We begin with a barrel that is about 1/2 to 2/3 full of tumbling rough, then add two level tablespoons of coarse grit (we use 60/90 grit silicon carbide) for each pound of rock. Then, add water until the water line is just below the top of the rocks. Seal the barrel and run for about seven days.&lt;/p&gt;&lt;p&gt;At the end of seven days, open the barrel. You will find a barrel of rocks in very muddy water! Dump the contents into a screen or a colander over a plastic bucket and rinse off every speck of grit and mud. Wear safety glasses to protect your eyes from a splash of mud.&lt;/p&gt;&lt;p&gt;Used grit and rock mud should never be washed down a drain. It can clog your plumbing system. We wash rocks in a plastic colander over a plastic bucket to keep the mud out of the drain.&lt;/p&gt;&lt;head rend="h3"&gt;Inspecting the Rocks:&lt;/head&gt;Now that you have washed the rocks, it is time to inspect them. Your goal is to determine if they are ready to move on to STEP 2, or if another week in STEP 1 would improve their appearance. We almost always tumble the rocks for a second week in coarse grit. We believe that improves their shape and removes more blemishes from their surface. Then, we usually move all of the rocks to the medium grit step.&lt;head rend="h3"&gt;Perfectionist Tumbling:&lt;/head&gt;Some people want to have more control over the tumbling process and only admit excellent rocks into STEP 2. These people sort their rocks into three categories:&lt;list rend="ul"&gt;&lt;item&gt;1) those that are ready for STEP 2&lt;/item&gt;&lt;item&gt;2) those that could be improved by another week in STEP 1&lt;/item&gt;&lt;item&gt;3) those that should be discarded or trimmed and returned to STEP 1&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here are some rocks right out of STEP 1. Note how they are covered with a gray "mud." This mud is spent grit and tiny rock particles that were worn off of the rocks during tumbling. Wash the rocks thoroughly so none of this grit goes into STEP 2. We wash our rocks in a colander over a plastic bucket so none of the mud goes down the drain. &lt;lb/&gt;IT IS VERY IMPORTANT TO WASH THE MUD FROM THE ROCKS IMMEDIATELY. If the rock mud is allowed to dry on the rocks, it is almost impossible to wash off.&lt;/p&gt;&lt;p&gt;IT IS VERY IMPORTANT TO WASH THE MUD FROM THE ROCKS IMMEDIATELY. If the rock mud is allowed to dry on the rocks, it is almost impossible to wash off.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 2 - Medium Grind&lt;/head&gt;&lt;p&gt;The second step of the four-step tumbling process is to run the rocks in the tumbler with medium grit. Before you begin it is extremely important to clean all of the coarse grit and rock mud from the rocks, from the tumbler barrel, and from the barrel lid. It is very important to avoid having even a few grains of coarse grit in the medium grind step.&lt;/p&gt;&lt;p&gt;During STEP 1, your rocks were reduced in size. When you return them to the barrel for STEP 2, they will probably not fill the barrel to the recommended 1/2 to 2/3 full level. If the barrel is only 1/2 full or less, the rocks can be tossed violently around in the tumbler. This can break or damage fragile materials such as quartz. So, when tumbling quartz or another fragile material, we always add enough ceramic media (or some rocks that need a little more tumbling) to bring the barrel up to the 1/2 to 2/3 full level.&lt;/p&gt;&lt;p&gt;(This is less important with varieties of chalcedony because it is a more durable material. However, if your tumbler barrel travels at more than about 60 revolutions per minute, we recommend adding enough ceramic media to bring it up to the 2/3 full level regardless of what type of rock is being tumbled.)&lt;/p&gt;&lt;p&gt;After your barrel is at the proper level, add two level tablespoons of medium grit (we use 110/220 grit or 150/220 grit silicon carbide) for each pound of rock (and ceramic media). Then add water until the water line is just below the top of the rocks. Now tumble for seven days.&lt;/p&gt;&lt;p&gt;At the end of seven days, open the barrel and clean all of the grit from the rocks, barrel, and lid (don't let any grit go down the drain). At this point in the tumbling process, a dry rock should have a smooth frosted surface. Inspect the rocks, looking for any that are cracked or broken. If you find any, these rocks should be discarded or saved for the next time you run Step 1.&lt;/p&gt;&lt;p&gt;Used grit and rock mud should never be washed down a drain. It can clog your plumbing system. We wash rocks in a plastic colander over a plastic bucket to keep the mud out of the drain.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 3 - Fine Grind / Pre-polish&lt;/head&gt;&lt;p&gt;The third step of the four-step tumbling process is a week in a fine grit such as 600 grit or 500 grit silicon carbide. Begin with a barrel that is perfectly clean. Place your rough and any ceramics that are with them into the barrel, and add two level tablespoons of fine grit per pound of material. Then add water until it fills the barrel up to just below the top of the rocks. Run this for about seven days, and then do a thorough cleaning of the rocks, the barrel, and the lid.&lt;/p&gt;&lt;p&gt;Remove any rocks that have broken or show signs of fracturing. At this point in the process, the rocks should be extremely smooth, and some of them might display a slight luster.&lt;/p&gt;&lt;p&gt;Be very clean! Before you replace the lid on your barrel, be sure that both the lid and the rim are perfectly clean. This will allow the lid to fit tightly and prevent leaks.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 4 - Polish&lt;/head&gt;&lt;p&gt;Now you are down to the final step - the one that puts a bright shine onto your tumbled stones. Be sure that the rocks and the equipment are perfectly clean. (Some people have an extra barrel that they use only for the polishing step.) A few specks of grit could ruin a great polish.&lt;/p&gt;&lt;p&gt;Place the rocks in the barrel and add two level tablespoons of rock polish (we use TXP aluminum oxide powder for almost all of our rotary tumbling) per pound of material in the barrel. Add water to just below the top of the rocks. Then, close the barrel and run for about seven days.&lt;/p&gt;&lt;p&gt;When you finish this step, your rocks should be bright and shiny. If they are, congratulations! Admire them for a while and share them with your friends.&lt;/p&gt;&lt;p&gt;If the stones have an extremely smooth surface but do not shine, they might need to be cleaned up using the burnishing step described below. If they have scratches on them, then you will need to go back to STEP 2 and repeat the medium grind, fine grind, and polishing steps.&lt;/p&gt;&lt;p&gt;For burnishing we grate up a bar of Ivory Soap with a vegetable grater. Then we add 1/2 tablespoon of grated soap for each pound of rock plus enough warm water to almost cover the rocks. See our video about burnishing polished stones.&lt;/p&gt;&lt;head rend="h2"&gt;Burnishing&lt;/head&gt;&lt;p&gt;Sometimes our stones are a little "hazy" when they come out of the polish, or small particles of polish are in micro-size crevices. We shine and clean them up by tumbling for an hour or so in soapy water. This is called "burnishing."&lt;/p&gt;&lt;p&gt;To burnish, we place the stones in our polish barrel with the normal amount of water, and then we add about 1/2 tablespoon of grated "Ivory" bar soap for each pound of rock (we use "ORIGINAL" Ivory soap - don't use a soap with aloe or abrasive or any other additive - honestly, just get a bar of Ivory soap). Burnishing usually makes the tumbled stones a little brighter, but sometimes it really kicks up the shine.&lt;/p&gt;&lt;p&gt;Print a copy of our free tumbling log and use it to keep your records.&lt;/p&gt;&lt;p&gt;Here are a few of our favorite tumbled stones!&lt;/p&gt;&lt;head rend="h2"&gt;Keeping Records&lt;/head&gt;&lt;p&gt;It is easy to forget what day you started the tumbler or what type of grit was used - especially if you are running multiple tumblers. Keeping records will keep you on track and provide a history that will help you learn. We record material tumbled, start date, abrasive used, media used, finishing date and duration, along with any comments or observations about the results.&lt;/p&gt;&lt;p&gt;To help you with your record keeping, we have prepared a printable tumbling log.&lt;/p&gt;&lt;p&gt;We usually have multiple tumblers running here, and we record every barrel of rock that we tumble on these logs. Even if your memory is better than ours, record-keeping is a good idea. When you learn something that works or something that doesn't, you will have it recorded. This information can help you repeat great results and avoid repeating bad ones. Also, we have trouble remembering which day a barrel of rocks was started. Using the log takes away the chance of forgetting.&lt;/p&gt;&lt;head rend="h2"&gt;Happy Tumbling!&lt;/head&gt;&lt;head rend="h3"&gt;RockTumbler.com Authors&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Hobart M. King has decades of rock tumbling experience and writes most of the articles on RockTumbler.com. He has a PhD in geology and is a GIA graduate gemologist. He also writes the articles about rocks, minerals and gems on Geology.com.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rocktumbler.com/tips/rock-tumbler-instructions/"/><published>2025-10-25T16:32:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45705381</id><title>Libera Chat receives legal advice that the Online Safety Act does not apply to</title><updated>2025-10-25T19:32:20.451488+00:00</updated><content>&lt;doc fingerprint="1e189153792602e7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The good advice&lt;/head&gt;
    &lt;p&gt;First of all, a massive thank you to everyone who donated since our last post. Our income on Liberapay has roughly quadrupled from what it was before the post. We have also had people reach out to us for large one-time monetary and hardware donations. Your support is truly appreciated!&lt;/p&gt;
    &lt;p&gt;And now for a followup from our last post. TL;DR: the legal firm we’ve engaged has sent us a memo indicating that in their opinion we can reasonably argue we do not have sufficient links to the UK for the Online Safety Act to be applicable to us. They also believe we would be at low risk of attempted enforcement action even if Ofcom does consider us to be in-scope for the OSA. We will continue to ensure that this is the case by keeping internal estimates of our UK user base and by continuing with our current efforts to keep Libera.Chat reasonably safe. We have no plans to institute any ID requirements for the forseeable future.&lt;/p&gt;
    &lt;p&gt;If that’s all you wanted to know, then feel free to stop here. However, we feel it’s in the best interest of online communities like ours for us to summarise the advice we were given in hopes that it will be useful to others. This is not legal advice from us to you. This advice was provided to Libera Chat as an assessment of our specific case. We accept no responsibility if you decide to apply advice given to us to your own online service.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this even matter?&lt;/head&gt;
    &lt;p&gt;You might be asking why we’ve even bothered to get legal advice on this matter. Libera Chat (the non-profit that runs the Libera.Chat IRC network) is based in Sweden. Our bank is Swedish, and we do not rely on any UK-based payment providers. We have a few servers in the UK, but they can be migrated on short notice. In other words, the British government has relatively little authority over us. The most damaging action they can reasonably take is to instruct internet service providers in the UK to deny access to us.&lt;/p&gt;
    &lt;p&gt;Relatedly, some online communities have decided that they want to minimise the authority the British government has over them. In response to critical analyses of the OSA pointing out its potential for regulatory overreach, some online communities have taken the understandable precaution of entirely blocking access from known UK IP addresses, thus cutting off any reasonable argument that they somehow have links to the UK (more on that later).&lt;/p&gt;
    &lt;p&gt;The end result is the same: a denial of service to people in the UK solely because of the country they live in. It’s not an insurmountable barrier to access in either case, but it shouldn’t be necessary for individuals in the UK to look into censorship-defeating proxies just to engage with free software developers and peer-directed projects that choose to have a community on our IRC network. It doesn’t serve our users, it doesn’t serve our communities, and it doesn’t serve the UK open source movement. Therefore, it’s in everyone’s best interest for us look into what’s necessary to keep things from getting to the point where users in the UK cannot access Libera.Chat, and that means getting guidance on the OSA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who does the OSA apply to?&lt;/head&gt;
    &lt;p&gt;As the OSA is fairly vague in its definitions, Ofcom has significant latitude in deciding where the thresholds are for whether an organisation meets certain criteria or not. Ofcom also hasn’t been forthcoming with its opinions on where those thresholds are, so there are relatively few hard guarantees about the applicability of the OSA. Still, there is a strong argument that while we definitely meet one of the criteria for the OSA to apply to us, we do not meet the other.&lt;/p&gt;
    &lt;p&gt;The OSA applies to online service providers that provide a regulated service and have links to the UK. We unarguably provide a regulated service because Libera.Chat is a so-called U2U service, i.e. it “allows ‘user-generated content’ to be encountered by another user of the service”. This is an incredibly broad class of services. Some exceptions are made for user content that is posted in relation to service content (e.g. the comment section of a blog) and a few other service types, but none of them reasonably apply to us. Every chat service, forum, federated social media server, or code forge counts as a regulated service, and therefore meets one of the criteria for the OSA to apply to them.&lt;/p&gt;
    &lt;p&gt;So be it. What about our links to the UK? To quote the memo:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An online service provider has “links to the UK” for the purposes of the OSA if any one or more of the following apply:&lt;/p&gt;
      &lt;item&gt;the service has a “significant number of UK users”&lt;/item&gt;
      &lt;item&gt;UK users form a “target market” of the service; or&lt;/item&gt;
      &lt;item&gt;the service is capable of being used by individuals in the UK, and there are reasonable grounds to believe that there is a material risk of significant harm to individuals in the UK presented by the content generated by the service.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;One factor that does not automatically give us links to the UK is the fact that we have staff members in the UK. Curiously, employees of the service provider who do not engage with that service as users are actually excluded for the purposes of determining whether a service has a “significant number of UK users”. Our staffers are also users, but our UK staffers make up an insignificant portion of our user base.&lt;/p&gt;
    &lt;p&gt;Speaking of which, the memo implies that “significance” in this context is interpreted as being relative to the population of the UK, not relative to the user base of the service. We have seen risk assessments that take the other interpretation and consider their UK user base to be “significant” because it makes up a large portion of their overall user base, but the advice we received suggests we should not use this interpretation. The exact fraction of the UK’s online population that must use a given service to be considered “significant” is unknown, but based on our counsel’s observations of Ofcom’s previous regulatory actions, it appears to be much higher than our internal estimates of how large our UK user base is.&lt;/p&gt;
    &lt;p&gt;The “target market” criterion is meant to capture services with a low number of UK users that target the UK specifically. While our target market (people interested in using an IRC-based platform for discussing free software or other peer-directed projects) is inclusive of UK users, it isn’t specifically for them. Our network is predominantly English-speaking, but we do not promote, direct, or tailor our service to UK users in particular.&lt;/p&gt;
    &lt;p&gt;Finally, there is no atypical material risk of significant harm to individuals in the UK presented by the messages on Libera.Chat. We block spam and client exploits. We are proactive in ensuring that our network’s acceptable use policy is upheld. We do not tolerate incitement to violence, doxxing, or defamation. And finally, we do not provide file hosting that can be used to distribute pornographic or sexual abuse media, though when we sought legal advice from the firm, we acknowledged the existence of DCC as a commonly-supported mechanism for transferring files using an IRC network to establish a peer-to-peer connection.&lt;/p&gt;
    &lt;p&gt;In the coming weeks, we will be finalising a statement similar to this risk assessment that we can provide to Ofcom should we ever be contacted by them about the OSA.&lt;/p&gt;
    &lt;head rend="h2"&gt;What if the OSA does apply to us?&lt;/head&gt;
    &lt;p&gt;While it is our opinion that the OSA does not apply to us, Ofcom might disagree, and appealing that disagreement would likely involve further legal expenses. So, what is the risk that Ofcom would decide to try to impose fines or other regulatory penalties on us?&lt;/p&gt;
    &lt;p&gt;For the time being, services like ours do not appear to be Ofcom’s priority. Currently, according to our legal sources, the focus appears to be file and image hosts that are at high risk of being used to transmit sexually-explicit depictions of minors. IRC has been used as a way to facilitate piracy, but those days are generally in the past thanks to more attractive options. Even if they weren’t, using Libera.Chat for this purpose is risky. We prefer to exercise the minimum power necessary to keep the network clean, but that doesn’t mean we don’t have the tools necessary to proactively stop the network from being used for piracy or CSAM distribution.&lt;/p&gt;
    &lt;p&gt;We have also been reassured that Ofcom is very likely to contact us with concerns before attempting any sort of action against us. There are some classes of concerns that we would certainly be willing to hear out, and we do prefer a constructive approach to problem resolution where possible. We’re confident that there isn’t anything for them to be reasonably concerned about, but we are willing to engage with good-faith reports of potential abuse of our service.&lt;/p&gt;
    &lt;head rend="h2"&gt;Will Libera.Chat ever require my ID?&lt;/head&gt;
    &lt;p&gt;We have no plans to require users to provide us with proof of identity and will take every reasonable measure to avoid requiring it. The justification for us to compromise the privacy of our users given the content we forbid on Libera.Chat is not adequate, and the risk of material harm should an identity verification mechanism compromise our users’ privacy far outweighs the plausible harms caused by not having such a system. Such violations of privacy aren’t hypothetical; another chat platform recently was affected by a data breach that potentially exposed the legal identities of tens of thousands of its users.&lt;/p&gt;
    &lt;p&gt;That said, it’s conceivable that legislation will be created that could apply to us and could force us to identify or spy on our users. If that happens, we will evaluate our options once drafts of such legislation reach a point where they can conceivably pass. Until then, we hope that the general public will remain vocally opposed to such attempts at overreach. Popular opposition stalled Chat Control earlier this month. There will probably always be efforts to compromise the free internet, but their success is not inevitable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://libera.chat/news/advised"/><published>2025-10-25T17:07:40+00:00</published></entry></feed>