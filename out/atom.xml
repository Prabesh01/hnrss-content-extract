<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-23T14:17:59.218087+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46721802</id><title>Show HN: isometric.nyc – giant isometric pixel art map of NYC</title><updated>2026-01-23T14:18:07.313043+00:00</updated><link href="https://cannoneyed.com/isometric-nyc/"/><published>2026-01-22T16:52:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46721897</id><title>AnswerThis (YC F25) Is Hiring</title><updated>2026-01-23T14:18:06.883239+00:00</updated><content>&lt;doc fingerprint="3d09585cf826800c"&gt;
  &lt;main&gt;
    &lt;p&gt;End-to-end workspace to accelerate scientific discovery&lt;/p&gt;
    &lt;p&gt;We crossed $1M ARR in 8 months. 200,000+ researchers at Stanford, MIT, and Amazon use us to do literature reviews 10x faster.&lt;lb/&gt; Now we're building something bigger: the system of record for scientists where they can find papers, analyze experiments, and write their drafts while collaborating with other scientists as well as our AI agents. &lt;lb/&gt; You should apply if you:&lt;lb/&gt; → Ship fast and learn faster &lt;lb/&gt; → Know the agentic AI stack cold (vector DBs, graph RAG, agent memory) &lt;lb/&gt; → Have built full-stack products that scaled past 1M users &lt;lb/&gt; → Actually care about accelerating scientific discovery&lt;lb/&gt; Bonus: You've published research yourself. &lt;lb/&gt; Don't apply if you:&lt;lb/&gt; → Can't be in SF, in person &lt;lb/&gt; → Haven't used the product yet &lt;lb/&gt; → Don't want to talk to customers &lt;lb/&gt; $120K-$200K + equity. We're a small team backed by YC. &lt;lb/&gt; Reach out on careers [at] answerthis.io&lt;lb/&gt; Tell us what you hate about AnswerThis, what you love, and one project you're proud of alongside your resume.&lt;lb/&gt; Science moves too slowly. Help us fix that.&lt;/p&gt;
    &lt;p&gt;We move fast. The whole process can be done in 2-3 weeks.&lt;/p&gt;
    &lt;p&gt;AnswerThis is building the system of record for scientists—where researchers can find papers, analyze experiments, and write drafts while collaborating with other scientists and AI agents.&lt;/p&gt;
    &lt;p&gt;We crossed $1M ARR in 8 months. 200,000+ researchers at Stanford, MIT, Amazon, and top institutions worldwide use us daily. We're backed by Y Combinator (F25) and cash-flow positive.&lt;/p&gt;
    &lt;p&gt;Science moves too slowly. Grant applications take months. Literature reviews take weeks. Researchers spend more time on paperwork than on discovery. We're fixing that.&lt;/p&gt;
    &lt;p&gt;You'll be joining a small, fast team in SF that ships constantly and talks to customers every day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/answerthis/jobs/r5VHmSC-ai-agent-orchestration"/><published>2026-01-22T17:00:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46723384</id><title>I was banned from Claude for scaffolding a Claude.md file?</title><updated>2026-01-23T14:18:06.358958+00:00</updated><content>&lt;doc fingerprint="c61ded10eca0acfa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Why does SSH send 100 packets per keystroke?&lt;/head&gt;
    &lt;p&gt;And why do I care?&lt;/p&gt;
    &lt;p&gt;Jan 22, 2026&lt;/p&gt;
    &lt;p&gt;Here are a few lines of summarized &lt;code&gt;tcpdump&lt;/code&gt; output for an ssh session where I send a single keystroke:&lt;/p&gt;
    &lt;code&gt;$ ./first_lines_of_pcap.sh single-key.pcap
  1   0.000s  CLIENT-&amp;gt;SERVER   36 bytes
  2   0.007s  SERVER-&amp;gt;CLIENT  564 bytes
  3   0.015s  CLIENT-&amp;gt;SERVER    0 bytes
  4   0.015s  CLIENT-&amp;gt;SERVER   36 bytes
  5   0.015s  SERVER-&amp;gt;CLIENT   36 bytes
  6   0.026s  CLIENT-&amp;gt;SERVER    0 bytes
  7   0.036s  CLIENT-&amp;gt;SERVER   36 bytes
  8   0.036s  SERVER-&amp;gt;CLIENT   36 bytes
  9   0.046s  CLIENT-&amp;gt;SERVER    0 bytes
 10   0.059s  CLIENT-&amp;gt;SERVER   36 bytes
&lt;/code&gt;
    &lt;p&gt;I said a “few” because there are a lot of these lines.&lt;/p&gt;
    &lt;code&gt;$ ./summarize_pcap.sh single-key.pcap
Total packets: 270

  36-byte msgs:   179 packets ( 66.3%)   6444 bytes
  Other data:       1 packet  (  0.4%)    564 bytes
  TCP ACKs:        90 packets ( 33.3%)

  Data sent:      6444 bytes in 36-byte messages,  564 bytes in other data
  Ratio:          11.4x more data in 36-byte messages than other data

  Data packet rate: ~90 packets/second (avg 11.1 ms between data packets)
&lt;/code&gt;
    &lt;p&gt;That is a lot of packets for one keypress. What’s going on here? Why do I care?&lt;/p&gt;
    &lt;head class="sc-4d1d4ca-1 bowwWe"&gt;here's those scripts if you're curious&lt;/head&gt;
    &lt;code&gt;# first_lines_of_pcap.sh
tshark -r "$1" \
  -T fields -e frame.number -e frame.time_relative -e ip.src -e ip.dst -e tcp.len | \
  awk 'NR&amp;lt;=10 {dir = ($3 ~ /71\.190/ ? "CLIENT-&amp;gt;SERVER" : "SERVER-&amp;gt;CLIENT");
       printf "%3d  %6.3fs  %-4s  %3s bytes\n", $1, $2, dir, $5}'
&lt;/code&gt;
    &lt;code&gt;# summarize_pcap.sh
tshark -r "$1" -Y "frame.time_relative &amp;lt;= 2.0" -T fields -e frame.time_relative -e tcp.len | awk '
  {
      count++
      payload = $2

      if (payload == 0) {
          acks++
      } else if (payload == 36) {
          mystery++
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      } else {
          game_data++
          game_bytes = payload
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      }
  }
  END {
      print "Total packets:", count
      print ""
      printf "  36-byte msgs:   %3d packets (%5.1f%%)  %5d bytes\n", mystery, 100*mystery/count, mystery*36
      printf "  Other data:     %3d packet  (%5.1f%%)  %5d bytes\n", game_data, 100*game_data/count, game_bytes
      printf "  TCP ACKs:       %3d packets (%5.1f%%)\n", acks, 100*acks/count
      print ""
      printf "  Data sent:      %d bytes in 36-byte messages,  %d bytes in other data\n", mystery*36, game_bytes
      printf "  Ratio:          %.1fx more data in 36-byte messages than other data\n", (mystery*36)/game_bytes
      print ""
      avg_ms = (sum_data_deltas / data_intervals) * 1000
      printf "  Data packet rate: ~%d packets/second (avg %.1f ms between data packets)\n", int(1000/avg_ms + 0.5), avg_ms
  }'
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discovery&lt;/head&gt;
    &lt;p&gt;I am working on a high-performance game that runs over ssh. The TUI for the game is created in bubbletea 1 and sent over ssh via wish.&lt;/p&gt;
    &lt;p&gt;I have also forked bubbletea to make it faster. Stay tuned!&lt;/p&gt;
    &lt;p&gt;The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.&lt;/p&gt;
    &lt;p&gt;So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s outstanding profiling tools to look at what’s going on.&lt;/p&gt;
    &lt;p&gt;Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.&lt;/p&gt;
    &lt;p&gt;At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.&lt;/p&gt;
    &lt;p&gt;But wait.&lt;/p&gt;
    &lt;p&gt;If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?&lt;/p&gt;
    &lt;head rend="h2"&gt;Investigation&lt;/head&gt;
    &lt;p&gt;As part of debugging the test harness issue, I used &lt;code&gt;tcpdump&lt;/code&gt; to log game traffic with and without the breaking change. Something like:&lt;/p&gt;
    &lt;code&gt;# The game runs on port 22
timeout 30s tcpdump -i eth0 'port 22' -w with-breaking-change.pcap
# Revert change
timeout 30s tcpdump -i eth0 'port 22' -w without-breaking-change.pcap
&lt;/code&gt;
    &lt;p&gt;Our breaking change stopped us from rendering our game over ssh. So &lt;code&gt;with-breaking-change.pcap&lt;/code&gt; contains packets that represent the overhead of each connection without actually rendering the game.&lt;/p&gt;
    &lt;p&gt;I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.&lt;/p&gt;
    &lt;code&gt;Wanna take a look yourself? I put with-breaking-change.pcap in this directory

--

Wow! Here's what I found:

Packet Size Distribution (413,703 total packets):
274,907 packets (66%): Exactly 36 bytes
138,778 packets (34%): 0 bytes (TCP ACKs)
18 packets (&amp;lt;0.1%): 72 bytes
&lt;/code&gt;
    &lt;p&gt;Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.&lt;/p&gt;
    &lt;p&gt;This was baffling to me (and to Claude Code). We kicked around several ideas like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSH flow control messages&lt;/item&gt;
      &lt;item&gt;PTY size polling or other status checks&lt;/item&gt;
      &lt;item&gt;Some quirk of bubbletea or wish&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One thing stood out - these exchanges were initiated by my ssh client (stock ssh installed on MacOS) - not by my server.&lt;/p&gt;
    &lt;p&gt;On a hunch, I took a &lt;code&gt;tcpdump&lt;/code&gt; of a regular ssh session.&lt;/p&gt;
    &lt;code&gt;# on my mac, in one tab
sudo tcpdump -ien0 'port 22'

# on my mac, in another tab
ssh $some_vm_of_mine
&lt;/code&gt;
    &lt;p&gt;I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the &lt;code&gt;tcpdump&lt;/code&gt; output.&lt;/p&gt;
    &lt;p&gt;I saw the exact same pattern! What in the world?&lt;/p&gt;
    &lt;head rend="h2"&gt;Root cause&lt;/head&gt;
    &lt;p&gt;Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;ssh -vvv&lt;/code&gt; gave me a pretty good sense of what was going on:&lt;/p&gt;
    &lt;code&gt;debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent) 
debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;20ms&lt;/code&gt; is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.&lt;/p&gt;
    &lt;p&gt;In 2023, ssh added keystroke timing obfuscation. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.&lt;/p&gt;
    &lt;p&gt;That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where latency is critical.&lt;/p&gt;
    &lt;head rend="h2"&gt;Remediation&lt;/head&gt;
    &lt;p&gt;Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass &lt;code&gt;ObscureKeystrokeTiming=no&lt;/code&gt; when starting up ssh sessions.&lt;/p&gt;
    &lt;p&gt;This worked great. CPU usage dropped dramatically and bots still received valid data.&lt;/p&gt;
    &lt;p&gt;But this is hardly a solution in the real world. I want &lt;code&gt;ssh mygame&lt;/code&gt; to Just Work without asking users to pass options that they might not understand.&lt;/p&gt;
    &lt;p&gt;Claude Code originally didn’t have much faith that we could disable this functionality server-side.&lt;/p&gt;
    &lt;p&gt;generated with simon wilson's excellent claude-code-transcripts tool&lt;/p&gt;
    &lt;p&gt;Fortunately, the description I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).&lt;/p&gt;
    &lt;code&gt;Log message:
Introduce a transport-level ping facility

This adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG
to implement a ping capability. These messages use numbers in the "local
extensions" number space and are advertised using a "[email protected]"
ext-info message with a string version number of "0".
&lt;/code&gt;
    &lt;p&gt;The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the &lt;code&gt;[email protected]&lt;/code&gt; extension. What if we just…don’t advertise &lt;code&gt;[email protected]&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;I searched go’s ssh library for &lt;code&gt;[email protected]&lt;/code&gt; and found the commit where support was added. The commit was tiny and seemed very easy to revert.&lt;/p&gt;
    &lt;p&gt;I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (go’s replace directive makes forking a library very easy).&lt;/p&gt;
    &lt;p&gt;Then I re-ran my test harness. The results were…very good:&lt;/p&gt;
    &lt;code&gt;Total CPU  29.90%          -&amp;gt; 11.64%
Syscalls   3.10s           -&amp;gt; 0.66s
Crypto     1.6s            -&amp;gt; 0.11s
Bandwidth  ~6.5 Mbit/sec   -&amp;gt; ~3 Mbit/sec
&lt;/code&gt;
    &lt;p&gt;Claude was also pretty pumped:&lt;/p&gt;
    &lt;p&gt;yes it's 1:30 am what of it&lt;/p&gt;
    &lt;p&gt;Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.&lt;/p&gt;
    &lt;p&gt;But this is a huge improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &amp;gt;50% drop was unimaginable to me.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging with LLMs was fun&lt;/head&gt;
    &lt;p&gt;I’ve been thinking about whether LLMs remove parts of the problem-solving process that I enjoy. But I’ve gotta say, debugging this problem using Claude Code was super fun.&lt;/p&gt;
    &lt;p&gt;I am familiar enough with &lt;code&gt;tcpdump&lt;/code&gt;, &lt;code&gt;tshark&lt;/code&gt;, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.&lt;/p&gt;
    &lt;p&gt;There were still edge cases. At some point in my confusion I switched to ChatGPT and it very confidently told me that my tcpdump output was normal ssh behavior:&lt;/p&gt;
    &lt;p&gt;do all chatgpt messages have this tone and formatting now?&lt;/p&gt;
    &lt;p&gt;And then doubled down when I pushed back:&lt;/p&gt;
    &lt;p&gt;no!!!&lt;/p&gt;
    &lt;p&gt;Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”&lt;/p&gt;
    &lt;p&gt;When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”&lt;/p&gt;
    &lt;p&gt;I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.&lt;/p&gt;
    &lt;p&gt;But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.&lt;/p&gt;
    &lt;p&gt;Besides. Being in the loop is fun. How else would I write this post?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hugodaniel.com/posts/claude-code-banned-me/"/><published>2026-01-22T18:38:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46723990</id><title>Why does SSH send 100 packets per keystroke?</title><updated>2026-01-23T14:18:06.118896+00:00</updated><content>&lt;doc fingerprint="c61ded10eca0acfa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Why does SSH send 100 packets per keystroke?&lt;/head&gt;
    &lt;p&gt;And why do I care?&lt;/p&gt;
    &lt;p&gt;Jan 22, 2026&lt;/p&gt;
    &lt;p&gt;Here are a few lines of summarized &lt;code&gt;tcpdump&lt;/code&gt; output for an ssh session where I send a single keystroke:&lt;/p&gt;
    &lt;code&gt;$ ./first_lines_of_pcap.sh single-key.pcap
  1   0.000s  CLIENT-&amp;gt;SERVER   36 bytes
  2   0.007s  SERVER-&amp;gt;CLIENT  564 bytes
  3   0.015s  CLIENT-&amp;gt;SERVER    0 bytes
  4   0.015s  CLIENT-&amp;gt;SERVER   36 bytes
  5   0.015s  SERVER-&amp;gt;CLIENT   36 bytes
  6   0.026s  CLIENT-&amp;gt;SERVER    0 bytes
  7   0.036s  CLIENT-&amp;gt;SERVER   36 bytes
  8   0.036s  SERVER-&amp;gt;CLIENT   36 bytes
  9   0.046s  CLIENT-&amp;gt;SERVER    0 bytes
 10   0.059s  CLIENT-&amp;gt;SERVER   36 bytes
&lt;/code&gt;
    &lt;p&gt;I said a “few” because there are a lot of these lines.&lt;/p&gt;
    &lt;code&gt;$ ./summarize_pcap.sh single-key.pcap
Total packets: 270

  36-byte msgs:   179 packets ( 66.3%)   6444 bytes
  Other data:       1 packet  (  0.4%)    564 bytes
  TCP ACKs:        90 packets ( 33.3%)

  Data sent:      6444 bytes in 36-byte messages,  564 bytes in other data
  Ratio:          11.4x more data in 36-byte messages than other data

  Data packet rate: ~90 packets/second (avg 11.1 ms between data packets)
&lt;/code&gt;
    &lt;p&gt;That is a lot of packets for one keypress. What’s going on here? Why do I care?&lt;/p&gt;
    &lt;head class="sc-4d1d4ca-1 bowwWe"&gt;here's those scripts if you're curious&lt;/head&gt;
    &lt;code&gt;# first_lines_of_pcap.sh
tshark -r "$1" \
  -T fields -e frame.number -e frame.time_relative -e ip.src -e ip.dst -e tcp.len | \
  awk 'NR&amp;lt;=10 {dir = ($3 ~ /71\.190/ ? "CLIENT-&amp;gt;SERVER" : "SERVER-&amp;gt;CLIENT");
       printf "%3d  %6.3fs  %-4s  %3s bytes\n", $1, $2, dir, $5}'
&lt;/code&gt;
    &lt;code&gt;# summarize_pcap.sh
tshark -r "$1" -Y "frame.time_relative &amp;lt;= 2.0" -T fields -e frame.time_relative -e tcp.len | awk '
  {
      count++
      payload = $2

      if (payload == 0) {
          acks++
      } else if (payload == 36) {
          mystery++
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      } else {
          game_data++
          game_bytes = payload
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      }
  }
  END {
      print "Total packets:", count
      print ""
      printf "  36-byte msgs:   %3d packets (%5.1f%%)  %5d bytes\n", mystery, 100*mystery/count, mystery*36
      printf "  Other data:     %3d packet  (%5.1f%%)  %5d bytes\n", game_data, 100*game_data/count, game_bytes
      printf "  TCP ACKs:       %3d packets (%5.1f%%)\n", acks, 100*acks/count
      print ""
      printf "  Data sent:      %d bytes in 36-byte messages,  %d bytes in other data\n", mystery*36, game_bytes
      printf "  Ratio:          %.1fx more data in 36-byte messages than other data\n", (mystery*36)/game_bytes
      print ""
      avg_ms = (sum_data_deltas / data_intervals) * 1000
      printf "  Data packet rate: ~%d packets/second (avg %.1f ms between data packets)\n", int(1000/avg_ms + 0.5), avg_ms
  }'
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discovery&lt;/head&gt;
    &lt;p&gt;I am working on a high-performance game that runs over ssh. The TUI for the game is created in bubbletea 1 and sent over ssh via wish.&lt;/p&gt;
    &lt;p&gt;I have also forked bubbletea to make it faster. Stay tuned!&lt;/p&gt;
    &lt;p&gt;The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.&lt;/p&gt;
    &lt;p&gt;So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s outstanding profiling tools to look at what’s going on.&lt;/p&gt;
    &lt;p&gt;Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.&lt;/p&gt;
    &lt;p&gt;At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.&lt;/p&gt;
    &lt;p&gt;But wait.&lt;/p&gt;
    &lt;p&gt;If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?&lt;/p&gt;
    &lt;head rend="h2"&gt;Investigation&lt;/head&gt;
    &lt;p&gt;As part of debugging the test harness issue, I used &lt;code&gt;tcpdump&lt;/code&gt; to log game traffic with and without the breaking change. Something like:&lt;/p&gt;
    &lt;code&gt;# The game runs on port 22
timeout 30s tcpdump -i eth0 'port 22' -w with-breaking-change.pcap
# Revert change
timeout 30s tcpdump -i eth0 'port 22' -w without-breaking-change.pcap
&lt;/code&gt;
    &lt;p&gt;Our breaking change stopped us from rendering our game over ssh. So &lt;code&gt;with-breaking-change.pcap&lt;/code&gt; contains packets that represent the overhead of each connection without actually rendering the game.&lt;/p&gt;
    &lt;p&gt;I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.&lt;/p&gt;
    &lt;code&gt;Wanna take a look yourself? I put with-breaking-change.pcap in this directory

--

Wow! Here's what I found:

Packet Size Distribution (413,703 total packets):
274,907 packets (66%): Exactly 36 bytes
138,778 packets (34%): 0 bytes (TCP ACKs)
18 packets (&amp;lt;0.1%): 72 bytes
&lt;/code&gt;
    &lt;p&gt;Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.&lt;/p&gt;
    &lt;p&gt;This was baffling to me (and to Claude Code). We kicked around several ideas like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSH flow control messages&lt;/item&gt;
      &lt;item&gt;PTY size polling or other status checks&lt;/item&gt;
      &lt;item&gt;Some quirk of bubbletea or wish&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One thing stood out - these exchanges were initiated by my ssh client (stock ssh installed on MacOS) - not by my server.&lt;/p&gt;
    &lt;p&gt;On a hunch, I took a &lt;code&gt;tcpdump&lt;/code&gt; of a regular ssh session.&lt;/p&gt;
    &lt;code&gt;# on my mac, in one tab
sudo tcpdump -ien0 'port 22'

# on my mac, in another tab
ssh $some_vm_of_mine
&lt;/code&gt;
    &lt;p&gt;I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the &lt;code&gt;tcpdump&lt;/code&gt; output.&lt;/p&gt;
    &lt;p&gt;I saw the exact same pattern! What in the world?&lt;/p&gt;
    &lt;head rend="h2"&gt;Root cause&lt;/head&gt;
    &lt;p&gt;Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;ssh -vvv&lt;/code&gt; gave me a pretty good sense of what was going on:&lt;/p&gt;
    &lt;code&gt;debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent) 
debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;20ms&lt;/code&gt; is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.&lt;/p&gt;
    &lt;p&gt;In 2023, ssh added keystroke timing obfuscation. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.&lt;/p&gt;
    &lt;p&gt;That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where latency is critical.&lt;/p&gt;
    &lt;head rend="h2"&gt;Remediation&lt;/head&gt;
    &lt;p&gt;Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass &lt;code&gt;ObscureKeystrokeTiming=no&lt;/code&gt; when starting up ssh sessions.&lt;/p&gt;
    &lt;p&gt;This worked great. CPU usage dropped dramatically and bots still received valid data.&lt;/p&gt;
    &lt;p&gt;But this is hardly a solution in the real world. I want &lt;code&gt;ssh mygame&lt;/code&gt; to Just Work without asking users to pass options that they might not understand.&lt;/p&gt;
    &lt;p&gt;Claude Code originally didn’t have much faith that we could disable this functionality server-side.&lt;/p&gt;
    &lt;p&gt;generated with simon wilson's excellent claude-code-transcripts tool&lt;/p&gt;
    &lt;p&gt;Fortunately, the description I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).&lt;/p&gt;
    &lt;code&gt;Log message:
Introduce a transport-level ping facility

This adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG
to implement a ping capability. These messages use numbers in the "local
extensions" number space and are advertised using a "[email protected]"
ext-info message with a string version number of "0".
&lt;/code&gt;
    &lt;p&gt;The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the &lt;code&gt;[email protected]&lt;/code&gt; extension. What if we just…don’t advertise &lt;code&gt;[email protected]&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;I searched go’s ssh library for &lt;code&gt;[email protected]&lt;/code&gt; and found the commit where support was added. The commit was tiny and seemed very easy to revert.&lt;/p&gt;
    &lt;p&gt;I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (go’s replace directive makes forking a library very easy).&lt;/p&gt;
    &lt;p&gt;Then I re-ran my test harness. The results were…very good:&lt;/p&gt;
    &lt;code&gt;Total CPU  29.90%          -&amp;gt; 11.64%
Syscalls   3.10s           -&amp;gt; 0.66s
Crypto     1.6s            -&amp;gt; 0.11s
Bandwidth  ~6.5 Mbit/sec   -&amp;gt; ~3 Mbit/sec
&lt;/code&gt;
    &lt;p&gt;Claude was also pretty pumped:&lt;/p&gt;
    &lt;p&gt;yes it's 1:30 am what of it&lt;/p&gt;
    &lt;p&gt;Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.&lt;/p&gt;
    &lt;p&gt;But this is a huge improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &amp;gt;50% drop was unimaginable to me.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging with LLMs was fun&lt;/head&gt;
    &lt;p&gt;I’ve been thinking about whether LLMs remove parts of the problem-solving process that I enjoy. But I’ve gotta say, debugging this problem using Claude Code was super fun.&lt;/p&gt;
    &lt;p&gt;I am familiar enough with &lt;code&gt;tcpdump&lt;/code&gt;, &lt;code&gt;tshark&lt;/code&gt;, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.&lt;/p&gt;
    &lt;p&gt;There were still edge cases. At some point in my confusion I switched to ChatGPT and it very confidently told me that my tcpdump output was normal ssh behavior:&lt;/p&gt;
    &lt;p&gt;do all chatgpt messages have this tone and formatting now?&lt;/p&gt;
    &lt;p&gt;And then doubled down when I pushed back:&lt;/p&gt;
    &lt;p&gt;no!!!&lt;/p&gt;
    &lt;p&gt;Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”&lt;/p&gt;
    &lt;p&gt;When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”&lt;/p&gt;
    &lt;p&gt;I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.&lt;/p&gt;
    &lt;p&gt;But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.&lt;/p&gt;
    &lt;p&gt;Besides. Being in the loop is fun. How else would I write this post?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/"/><published>2026-01-22T19:27:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46725288</id><title>Capital One to acquire Brex for $5.15B</title><updated>2026-01-23T14:18:05.965904+00:00</updated><content>&lt;doc fingerprint="e1d8a53d0af29fa9"&gt;
  &lt;main&gt;
    &lt;p&gt;Jan 22 (Reuters) - Capital One Financial (COF.N) said on Thursday it will acquire fintech firm Brex in a cash and stock deal valued at $5.15 billion and reported a rise in quarterly profit on the back of higher interest income from its credit card debt.&lt;/p&gt;
    &lt;p&gt;Shares of the consumer lender fell more than 5% following the announcement of the deal, but robust results helped them pare losses to trade 1.5% lower.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;The move comes as dealmakers prepare for another strong year in 2026, with a record slate of transactions expected as executives pursue scale to navigate rising economic and geopolitical uncertainties.&lt;/p&gt;
    &lt;p&gt;The deal, which is expected to close in mid‑2026, will be carried out on an approximate 50-50 cash-stock basis, Capital One said.&lt;/p&gt;
    &lt;p&gt;Brex operates in corporate cards and expense management software used by firms such as DoorDash (DASH.O) and Robinhood (HOOD.O), which could give Capital One greater exposure and reduce its reliance on consumer credit, cushioning it against the impact of economic downturns.&lt;/p&gt;
    &lt;p&gt;Brex operates in more than 120 countries according to its website.&lt;/p&gt;
    &lt;p&gt;Capital One said the fintech firm's chief executive and founder, Pedro Franceschi, will remain at the helm following the transaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;FOURTH-QUARTER EARNINGS&lt;/head&gt;
    &lt;p&gt;U.S. consumer spending rose at a solid pace in November and October, suggesting the economy was on track for a third consecutive quarter of strong growth.&lt;/p&gt;
    &lt;p&gt;Economic momentum has been underpinned largely by resilient household demand as well as a narrowing trade deficit, with imports declining in response to President Donald Trump's broad tariff increases.&lt;/p&gt;
    &lt;p&gt;However, the tariffs have pushed up the prices of many goods, weighing unevenly across income groups.&lt;/p&gt;
    &lt;p&gt;Economists say spending strength is increasingly concentrated among higher-income households, while lower- and middle-income consumers have limited scope to switch to cheaper alternatives.&lt;/p&gt;
    &lt;p&gt;Capital One's net interest income — the difference between what it makes on loans and pays out on deposits — rose 54% to $12.47 billion in the fourth quarter from a year ago.&lt;/p&gt;
    &lt;p&gt;The McLean, Virginia-based company's net income available to common stockholders came in at $2.06 billion, or $3.26 per share, for the quarter, compared with $1.02 billion, or $2.67 per share, a year earlier.&lt;/p&gt;
    &lt;head rend="h2"&gt;CREDIT CARD CAP CONUNDRUM&lt;/head&gt;
    &lt;p&gt;Trump said last week he was calling for a one‑year cap on credit card interest rates at 10% starting January 20, but offered few details on how the proposal would be implemented or how companies would be compelled to comply.&lt;/p&gt;
    &lt;p&gt;Banking industry groups have pushed back against the proposal, warning it would restrict the availability of credit for everyday consumers.&lt;/p&gt;
    &lt;p&gt;JPMorgan Chase (JPM.N) CEO Jamie Dimon said on Wednesday a proposal to cap credit card interest rates would amount to economic disaster.&lt;/p&gt;
    &lt;p&gt;However, Bank of America (BAC.N) is considering options to offer new credit cards with an interest rate of 10% to satisfy Trump's demands, a source familiar with the matter said on Thursday.&lt;/p&gt;
    &lt;p&gt;The introduction of an interest rate cap would deal a significant blow to Capital One Financial, which has one of the most credit-card‑dependent business models among major U.S. lenders.&lt;/p&gt;
    &lt;p&gt;"We feel strongly that a cap on interest rates would catalyze a number of unintended consequences," CEO Richard Fairbank said in a call with analysts.&lt;/p&gt;
    &lt;p&gt;He added that lack of credit would result in reduced consumer spending and likely bring on a recession.&lt;/p&gt;
    &lt;p&gt;Reporting by Pritam Biswas in Bengaluru; Editing by Shreya Biswas&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/"/><published>2026-01-22T21:23:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46725300</id><title>Scaling PostgreSQL to power 800M ChatGPT users</title><updated>2026-01-23T14:18:05.769397+00:00</updated><content>&lt;doc fingerprint="9558893f469078c2"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Scaling PostgreSQL to power 800 million ChatGPT users&lt;/head&gt;&lt;p&gt;By Bohan Zhang, Member of the Technical Staff&lt;/p&gt;&lt;p&gt;For years, PostgreSQL has been one of the most critical, under-the-hood data systems powering core products like ChatGPT and OpenAI’s API. As our user base grows rapidly, the demands on our databases have increased exponentially, too. Over the past year, our PostgreSQL load has grown by more than 10x, and it continues to rise quickly.&lt;/p&gt;&lt;p&gt;Our efforts to advance our production infrastructure to sustain this growth revealed a new insight: PostgreSQL can be scaled to reliably support much larger read-heavy workloads than many previously thought possible. The system (initially created by a team of scientists at University of California, Berkeley) has enabled us to support massive global traffic with a single primary Azure PostgreSQL flexible server instance(opens in a new window) and nearly 50 read replicas spread over multiple regions globally. This is the story of how we’ve scaled PostgreSQL at OpenAI to support millions of queries per second for 800 million users through rigorous optimizations and solid engineering; we’ll also cover key takeaways we learned along the way.&lt;/p&gt;&lt;p&gt;After the launch of ChatGPT, traffic grew at an unprecedented rate. To support it, we rapidly implemented extensive optimizations at both the application and PostgreSQL database layers, scaled up by increasing the instance size, and scaled out by adding more read replicas. This architecture has served us well for a long time. With ongoing improvements, it continues to provide ample runway for future growth.&lt;/p&gt;&lt;p&gt;It may sound surprising that a single-primary architecture can meet the demands of OpenAI’s scale; however, making this work in practice isn’t simple. We’ve seen several SEVs caused by Postgres overload, and they often follow the same pattern: an upstream issue causes a sudden spike in database load, such as widespread cache misses from a caching-layer failure, a surge of expensive multi-way joins saturating CPU, or a write storm from a new feature launch. As resource utilization climbs, query latency rises and requests begin to time out. Retries then further amplify the load, triggering a vicious cycle with the potential to degrade the entire ChatGPT and API services.&lt;/p&gt;&lt;p&gt;Although PostgreSQL scales well for our read-heavy workloads, we still encounter challenges during periods of high write traffic. This is largely due to PostgreSQL’s multiversion concurrency control (MVCC) implementation, which makes it less efficient for write-heavy workloads. For example, when a query updates a tuple or even a single field, the entire row is copied to create a new version. Under heavy write loads, this results in significant write amplification. It also increases read amplification, since queries must scan through multiple tuple versions (dead tuples) to retrieve the latest one. MVCC introduces additional challenges such as table and index bloat, increased index maintenance overhead, and complex autovacuum tuning. (You can find a deep-dive on these issues in a blog I wrote with Prof. Andy Pavlo at Carnegie Mellon University called The Part of PostgreSQL We Hate the Most(opens in a new window), cited(opens in a new window) in the PostgreSQL Wikipedia page.)&lt;/p&gt;&lt;p&gt;To mitigate these limitations and reduce write pressure, we’ve migrated, and continue to migrate, shardable (i.e. workloads that can be horizontally partitioned), write-heavy workloads to sharded systems such as Azure Cosmos DB, optimizing application logic to minimize unnecessary writes. We also no longer allow adding new tables to the current PostgreSQL deployment. New workloads default to the sharded systems.&lt;/p&gt;&lt;p&gt;Even as our infrastructure has evolved, PostgreSQL has remained unsharded, with a single primary instance serving all writes. The primary rationale is that sharding existing application workloads would be highly complex and time-consuming, requiring changes to hundreds of application endpoints and potentially taking months or even years. Since our workloads are primarily read-heavy, and we’ve implemented extensive optimizations, the current architecture still provides ample headroom to support continued traffic growth. While we’re not ruling out sharding PostgreSQL in the future, it’s not a near-term priority given the sufficient runway we have for current and future growth.&lt;/p&gt;&lt;p&gt;In the following sections, we’ll dive into the challenges we faced and the extensive optimizations we implemented to address them and prevent future outages, pushing PostgreSQL to its limits and scaling it to millions of queries per second (QPS).&lt;/p&gt;&lt;p&gt;Challenge: With only one writer, a single-primary setup can’t scale writes. Heavy write spikes can quickly overload the primary and impact services like ChatGPT and our API.&lt;/p&gt;&lt;p&gt;Solution: We minimize load on the primary as much as possible—both reads and writes—to ensure it has sufficient capacity to handle write spikes. Read traffic is offloaded to replicas wherever possible. However, some read queries must remain on the primary because they’re part of write transactions. For those, we focus on ensuring they’re efficient and avoid slow queries. For write traffic, we’ve migrated shardable, write-heavy workloads to sharded systems such as Azure CosmosDB. Workloads that are harder to shard but still generate high write volume take longer to migrate, and that process is still ongoing. We also aggressively optimized our applications to reduce write load; for example, we’ve fixed application bugs that caused redundant writes and introduced lazy writes, where appropriate, to smooth traffic spikes. In addition, when backfilling table fields, we enforce strict rate limits to prevent excessive write pressure.&lt;/p&gt;&lt;p&gt;Challenge: We identified several expensive queries in PostgreSQL. In the past, sudden volume spikes in these queries would consume large amounts of CPU, slowing both ChatGPT and API requests.&lt;/p&gt;&lt;p&gt;Solution: A few expensive queries, such as those joining many tables together, can significantly degrade or even bring down the entire service. We need to continuously optimize PostgreSQL queries to ensure they’re efficient and avoid common Online Transaction Processing (OLTP) anti-patterns. For example, we once identified an extremely costly query that joined 12 tables, where spikes in this query were responsible for past high-severity SEVs. We should avoid complex multi-table joins whenever possible. If joins are necessary, we learned to consider breaking down the query and move complex join logic to the application layer instead. Many of these problematic queries are generated by Object-Relational Mapping frameworks (ORMs), so it’s important to carefully review the SQL they produce and ensure it behaves as expected. It’s also common to find long-running idle queries in PostgreSQL. Configuring timeouts like idle_in_transaction_session_timeout is essential to prevent them from blocking autovacuum.&lt;/p&gt;&lt;p&gt;Challenge: If a read replica goes down, traffic can still be routed to other replicas. However, relying on a single writer means having a single point of failure—if it goes down, the entire service is affected.&lt;/p&gt;&lt;p&gt;Solution: Most critical requests only involve read queries. To mitigate the single point of failure in the primary, we offloaded those reads from the writer to replicas, ensuring those requests can continue serving even if the primary goes down. While write operations would still fail, the impact is reduced; it’s no longer a SEV0 since reads remain available.&lt;/p&gt;&lt;p&gt;To mitigate primary failures, we run the primary in High-Availability (HA) mode with a hot standby, a continuously synchronized replica that is always ready to take over serving traffic. If the primary goes down or needs to be taken offline for maintenance, we can quickly promote the standby to minimize downtime. The Azure PostgreSQL team has done significant work to ensure these failovers remain safe and reliable even under very high load. To handle read replica failures, we deploy multiple replicas in each region with sufficient capacity headroom, ensuring that a single replica failure doesn’t lead to a regional outage.&lt;/p&gt;&lt;p&gt;Challenge: We often encounter situations where certain requests consume a disproportionate amount of resources on PostgreSQL instances. This can lead to degraded performance for other workloads running on the same instances. For example, a new feature launch can introduce inefficient queries that heavily consume PostgreSQL CPU, slowing down requests for other critical features.&lt;/p&gt;&lt;p&gt;Solution: To mitigate the “noisy neighbor” problem, we isolate workloads onto dedicated instances to ensure that sudden spikes in resource-intensive requests don’t impact other traffic. Specifically, we split requests into low-priority and high-priority tiers and route them to separate instances. This way, even if a low-priority workload becomes resource-intensive, it won’t degrade the performance of high-priority requests. We apply the same strategy across different products and services as well, so that activity from one product does not affect the performance or reliability of another.&lt;/p&gt;&lt;p&gt;Challenge: Each instance has a maximum connection limit (5,000 in Azure PostgreSQL). It’s easy to run out of connections or accumulate too many idle ones. We’ve previously had incidents caused by connection storms that exhausted all available connections.&lt;/p&gt;&lt;p&gt;Solution: We deployed PgBouncer as a proxy layer to pool database connections. Running it in statement or transaction pooling mode allows us to efficiently reuse connections, greatly reducing the number of active client connections. This also cuts connection setup latency: in our benchmarks, the average connection time dropped from 50 milliseconds (ms) to 5 ms. Inter-region connections and requests can be expensive, so we co-locate the proxy, clients, and replicas in the same region to minimize network overhead and connection use time. Moreover, PgBouncer must be configured carefully. Settings like idle timeouts are critical to prevent connection exhaustion.&lt;/p&gt;&lt;p&gt;Challenge: A sudden spike in cache misses can trigger a surge of reads on the PostgreSQL database, saturating CPU and slowing user requests.&lt;/p&gt;&lt;p&gt;Solution: To reduce read pressure on PostgreSQL, we use a caching layer to serve most of the read traffic. However, when cache hit rates drop unexpectedly, the burst of cache misses can push a large volume of requests directly to PostgreSQL. This sudden increase in database reads consumes significant resources, slowing down the service. To prevent overload during cache-miss storms, we implement a cache locking (and leasing) mechanism so that only a single reader that misses on a particular key fetches the data from PostgreSQL. When multiple requests miss on the same cache key, only one request acquires the lock and proceeds to retrieve the data and repopulate the cache. All other requests wait for the cache to be updated rather than all hitting PostgreSQL at once. This significantly reduces redundant database reads and protects the system from cascading load spikes.&lt;/p&gt;&lt;p&gt;Challenge: The primary streams Write Ahead Log (WAL) data to every read replica. As the number of replicas increases, the primary must ship WAL to more instances, increasing pressure on both network bandwidth and CPU. This causes higher and more unstable replica lag, which makes the system harder to scale reliably.&lt;/p&gt;&lt;p&gt;Solution: We operate nearly 50 read replicas across multiple geographic regions to minimize latency. However, with the current architecture, the primary must stream WAL to every replica. Although it currently scales well with very large instance types and high-network bandwidth, we can’t keep adding replicas indefinitely without eventually overloading the primary. To address this, we’re collaborating with the Azure PostgreSQL team on cascading replication(opens in a new window), where intermediate replicas relay WAL to downstream replicas. This approach allows us to scale to potentially over a hundred replicas without overwhelming the primary. However, it also introduces additional operational complexity, particularly around failover management. The feature is still in testing; we’ll ensure it’s robust and can fail over safely before rolling it out to production.&lt;/p&gt;&lt;p&gt;Challenge: A sudden traffic spike on specific endpoints, a surge of expensive queries, or a retry storm can quickly exhaust critical resources such as CPU, I/O, and connections, which causes widespread service degradation.&lt;/p&gt;&lt;p&gt;Solution: We implemented rate-limiting across multiple layers—application, connection pooler, proxy, and query—to prevent sudden traffic spikes from overwhelming database instances and triggering cascading failures. It’s also crucial to avoid overly short retry intervals, which can trigger retry storms. We also enhanced the ORM layer to support rate limiting and when necessary, fully block specific query digests. This targeted form of load shedding enables rapid recovery from sudden surges of expensive queries.&lt;/p&gt;&lt;p&gt;Challenge: Even a small schema change, such as altering a column type, can trigger a full table rewrite(opens in a new window). We therefore apply schema changes cautiously—limiting them to lightweight operations and avoiding any that rewrite entire tables.&lt;/p&gt;&lt;p&gt;Solution: Only lightweight schema changes are permitted, such as adding or removing certain columns that do not trigger a full table rewrite. We enforce a strict 5-second timeout on schema changes. Creating and dropping indexes concurrently is allowed. Schema changes are restricted to existing tables. If a new feature requires additional tables, they must be in alternative sharded systems such as Azure CosmosDB rather than PostgreSQL. When backfilling a table field, we apply strict rate limits to prevent write spikes. Although this process can sometimes take over a week, it ensures stability and avoids any production impact.&lt;/p&gt;&lt;p&gt;This effort demonstrates that with the right design and optimizations, Azure PostgreSQL can be scaled to handle the largest production workloads. PostgreSQL handles millions of QPS for read-heavy workloads, powering OpenAI’s most critical products like ChatGPT and the API platform. We added nearly 50 read replicas, while keeping replication lag near zero, maintained low-latency reads across geo-distributed regions, and built sufficient capacity headroom to support future growth.&lt;/p&gt;&lt;p&gt;This scaling works while still minimizing latency and improving reliability. We consistently deliver low double-digit millisecond p99 client-side latency and five-nines availability in production. And over the past 12 months, we’ve had only one SEV-0 PostgreSQL incident (it occurred during the viral launch(opens in a new window) of ChatGPT ImageGen, when write traffic suddenly surged by more than 10x as over 100 million new users signed up within a week.)&lt;/p&gt;&lt;p&gt;While we’re happy with how far PostgreSQL has taken us, we continue to push its limits to ensure we have sufficient runway for future growth. We’ve already migrated the shardable write-heavy workloads to our sharded systems like CosmosDB. The remaining write-heavy workloads are more challenging to shard—we’re actively migrating those as well to further offload writes from the PostgreSQL primary. We’re also working with Azure to enable cascading replication so we can safely scale to significantly more read replicas.&lt;/p&gt;&lt;p&gt;Looking ahead, we’ll continue to explore additional approaches to further scale, including sharded PostgreSQL or alternative distributed systems, as our infrastructure demands continue to grow.&lt;/p&gt;&lt;head rend="h2"&gt;Author&lt;/head&gt;Bohan Zhang&lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;&lt;p&gt;Special thanks to Jon Lee, Sicheng Liu, Chaomin Yu, and Chenglong Hao, who contributed to this post, and to the entire team that helped scale PostgreSQL. We’d also like to thank the Azure PostgreSQL team for their strong partnership.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/scaling-postgresql/"/><published>2026-01-22T21:24:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46726526</id><title>Improving the usability of C libraries in Swift</title><updated>2026-01-23T14:18:05.412939+00:00</updated><content>&lt;doc fingerprint="6e025913b407b699"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Improving the usability of C libraries in Swift&lt;/head&gt;
    &lt;p&gt;There are many interesting, useful, and fun C libraries in the software ecosystem. While one could go and rewrite these libraries in Swift, usually there is no need, because Swift provides direct interoperability with C. With a little setup, you can directly use existing C libraries from your Swift code.&lt;/p&gt;
    &lt;p&gt;When you use a C library directly from Swift, it will look and feel similar to using it from C. That can be useful if you’re following sample code or a tutorial written in C, but it can also feel out of place. For example, here’s a small amount of code using a C API:&lt;/p&gt;
    &lt;code&gt;  var instanceDescriptor = WGPUInstanceDescriptor()
  let instance = wgpuCreateInstance(&amp;amp;instanceDescriptor)
  var surfaceDescriptor = WGPUSurfaceDescriptor()
  let surface = wgpuInstanceCreateSurface(instance, &amp;amp;surfaceDescriptor)
  if wgpuSurfacePresent(&amp;amp;surface) == WGPUStatus_Error {
      // report error
  }
  wgpuSurfaceRelease(surface)
  wgpuInstanceRelease(instance)
&lt;/code&gt;
    &lt;p&gt;The C library here that Swift is using comes from the webgpu-headers project, which vends a C header (&lt;code&gt;webgpu.h&lt;/code&gt;) that is used by several implementations of WebGPU. WebGPU  is a technology that enables web developers to use the system’s GPU (Graphics Processing Unit) from the browser. For the purposes of this post, you don’t really need to know anything about WebGPU: I’m using it as an example of a typical C library, and the techniques described in this blog post apply to lots of other well-designed C libraries.&lt;/p&gt;
    &lt;p&gt;The Swift code above has a very “C” feel to it. It has global function calls with prefixed names like &lt;code&gt;wgpuInstanceCreateSurface&lt;/code&gt; and global integer constants like &lt;code&gt;WGPUStatus_Error&lt;/code&gt;. It pervasively uses unsafe pointers, some of which are managed with explicit reference counting, where the user provides calls to &lt;code&gt;wpuXYZAddRef&lt;/code&gt; and &lt;code&gt;wgpuXYZRelease&lt;/code&gt; functions. It works, but it doesn’t feel like Swift, and inherits various safety problems of C.&lt;/p&gt;
    &lt;p&gt;Fortunately, we can improve this situation, providing a safer and more ergonomic interface to WebGPU from Swift that feels like it belongs in Swift. More importantly, we can do so without changing the WebGPU implementation: Swift provides a suite of annotations that you can apply to C headers to improve the way in which the C APIs are expressed in Swift. These annotations describe common conventions in C that match up with Swift constructs, projecting a more Swift-friendly interface on top of the C code.&lt;/p&gt;
    &lt;p&gt;In this post, I’m going to use these annotations to improve how Swift interacts with the WebGPU C code. By the end, we’ll be able to take advantage of Swift features like argument labels, methods, enums, and automatic reference counting, like this:&lt;/p&gt;
    &lt;code&gt;  var instanceDescriptor = WGPUInstanceDescriptor()
  let instance = WGPUInstance(descriptor: &amp;amp;instanceDescriptor)
  var surfaceDescriptor = WGPUSurfaceDescriptor()
  let surface = instance.createSurface(descriptor: &amp;amp;surfaceDescriptor)
  if surface.present() == .error {
      // report error
  }
  // Swift automatically deallocates the instance and surface when we're done
&lt;/code&gt;
    &lt;p&gt;These same annotations can be used for any C library to provide a safer, more ergonomic development experience in Swift without changing the C library at all.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Some of what is covered in this post requires bug fixes that first became available in Swift 6.2.3.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Setup: Writing a module map&lt;/head&gt;
    &lt;p&gt;A module map is a way of layering a Swift-friendly modular structure on top of C headers. You can create a module map for the WebGPU header by writing the following to a file &lt;code&gt;module.modulemap&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;module WebGPU {
  header "webgpu.h"
  export *
}
&lt;/code&gt;
    &lt;p&gt;The easiest thing to do is to put &lt;code&gt;module.modulemap&lt;/code&gt; alongside the header itself. For my experiment here, I put it in the root directory of my &lt;code&gt;webgpu-headers&lt;/code&gt; checkout. If you’re in a Swift package, put it into its own target with this layout:&lt;/p&gt;
    &lt;code&gt;├── Package.swift
└── Sources
    └── WebGPU
        ├── include
        │   ├── webgpu.h
        │   └── module.modulemap
        └── WebGPU.c (empty file)
&lt;/code&gt;
    &lt;p&gt;If you reference this &lt;code&gt;WebGPU&lt;/code&gt; target from elsewhere in the package, you can &lt;code&gt;import WebGPU&lt;/code&gt; to get access to the C APIs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Seeing the results&lt;/head&gt;
    &lt;p&gt;There are a few ways to see what the Swift interface for a C library looks like.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;swift-synthesize-interface&lt;/code&gt;tool in Swift 6.2+ prints the Swift interface to the terminal.&lt;/item&gt;
      &lt;item&gt;Xcode’s “Swift 5 interface” counterpart to the &lt;code&gt;webgpu.h&lt;/code&gt;header will show how the header has been mapped into Swift.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s do it from the command line, using &lt;code&gt;swift-synthesize-interface&lt;/code&gt;. From the directory containing &lt;code&gt;webgpu.h&lt;/code&gt; and &lt;code&gt;module.modulemap&lt;/code&gt;, run:&lt;/p&gt;
    &lt;code&gt;xcrun swift-synthesize-interface -I . -module-name WebGPU -target arm64-apple-macos15 -sdk /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX26.0.sdk
&lt;/code&gt;
    &lt;p&gt;The leading &lt;code&gt;xcrun&lt;/code&gt; and the &lt;code&gt;-sdk&lt;/code&gt; argument with the path is only needed on macOS; on other platforms, make sure &lt;code&gt;swift-synthesize-interface&lt;/code&gt; is in your path. The &lt;code&gt;-target&lt;/code&gt; operation is the triple provided if you run &lt;code&gt;swiftc -print-target-info&lt;/code&gt;. It looks like this:&lt;/p&gt;
    &lt;code&gt;{
  "compilerVersion": "Apple Swift version 6.2 (swiftlang-6.2.2.15.4 clang-1700.3.15.2)",
  "target": {
    "triple": "arm64-apple-macosx15.0",
    "unversionedTriple": "arm64-apple-macosx",
    "moduleTriple": "arm64-apple-macos",
    "compatibilityLibraries": [ ],
    "librariesRequireRPath": false
  },
  "paths": { ... }
}
&lt;/code&gt;
    &lt;p&gt;The output of &lt;code&gt;swift-synthesize-interface&lt;/code&gt; is the Swift API for the WebGPU module, directly translated from C. For example, this code from the WebGPU header:&lt;/p&gt;
    &lt;code&gt;typedef enum WGPUAdapterType {
    WGPUAdapterType_DiscreteGPU = 0x00000001,
    WGPUAdapterType_IntegratedGPU = 0x00000002,
    WGPUAdapterType_CPU = 0x00000003,
    WGPUAdapterType_Unknown = 0x00000004,
    WGPUAdapterType_Force32 = 0x7FFFFFFF
} WGPUAdapterType WGPU_ENUM_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;is translated into:&lt;/p&gt;
    &lt;code&gt;public struct WGPUAdapterType : Hashable, Equatable, RawRepresentable {
    public init(_ rawValue: UInt32)
    public init(rawValue: UInt32)
    public var rawValue: UInt32
}

public var WGPUAdapterType_DiscreteGPU: WGPUAdapterType { get }
public var WGPUAdapterType_IntegratedGPU: WGPUAdapterType { get }
public var WGPUAdapterType_CPU: WGPUAdapterType { get }
public var WGPUAdapterType_Unknown: WGPUAdapterType { get }
public var WGPUAdapterType_Force32: WGPUAdapterType { get }
&lt;/code&gt;
    &lt;p&gt;and there are lots of global functions like this:&lt;/p&gt;
    &lt;code&gt;public func wgpuComputePipelineGetBindGroupLayout(_ computePipeline: WGPUComputePipeline!, _ groupIndex: UInt32) -&amp;gt; WGPUBindGroupLayout!
public func wgpuComputePipelineSetLabel(_ computePipeline: WGPUComputePipeline!, _ label: WGPUStringView)
public func wgpuComputePipelineAddRef(_ computePipeline: WGPUComputePipeline!)
public func wgpuComputePipelineRelease(_ computePipeline: WGPUComputePipeline!)
&lt;/code&gt;
    &lt;p&gt;It’s a starting point! You can absolutely write Swift programs using these WebGPU APIs, and they’ll feel a lot like writing them in C. Let’s see what we can do to make it better.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cleaning up enumeration types&lt;/head&gt;
    &lt;p&gt;C enums can be used for several things. Sometimes they really represent a choice among a number of alternatives. Sometimes they represent flags in a set of options, from which you can choose several. Sometimes they’re just a convenient way to create a bunch of named constants. Swift conservatively imports enum types as wrappers over the underlying C type used to store values of the enum (e.g., &lt;code&gt;WGPUAdapterType&lt;/code&gt; wraps a &lt;code&gt;UInt32&lt;/code&gt;) and makes the enumerators into global constants. It covers all of the possible use cases, but it isn’t nice.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;WGPUAdapterType&lt;/code&gt; enum really is a choice among one of several options, which would be best represented as an &lt;code&gt;enum&lt;/code&gt; in Swift. If we were willing to modify the header, we could apply the &lt;code&gt;enum_extensibility&lt;/code&gt; attribute to the enum, like this:&lt;/p&gt;
    &lt;code&gt;typedef enum __attribute__((enum_extensibility(closed))) WGPUAdapterType {
    WGPUAdapterType_DiscreteGPU = 0x00000001,
    WGPUAdapterType_IntegratedGPU = 0x00000002,
    WGPUAdapterType_CPU = 0x00000003,
    WGPUAdapterType_Unknown = 0x00000004,
    WGPUAdapterType_Force32 = 0x7FFFFFFF
} WGPUAdapterType WGPU_ENUM_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;This works, and results in a much nicer Swift API:&lt;/p&gt;
    &lt;code&gt;@frozen public enum WGPUAdapterType : UInt32, @unchecked Sendable {
    case discreteGPU = 1
    case integratedGPU = 2
    case CPU = 3
    case unknown = 4
    case force32 = 2147483647
}
&lt;/code&gt;
    &lt;p&gt;Now, we get an &lt;code&gt;enum&lt;/code&gt; that we can switch over, and nice short case names, e.g.,&lt;/p&gt;
    &lt;code&gt;switch adapterType {
  case .discreteGPU, .integratedGPU:
    print("definitely a GPU")
  default:
    print("not so sure")
}
&lt;/code&gt;
    &lt;p&gt;That’s great, but I already broke my rule: no header modifications unless I have to!&lt;/p&gt;
    &lt;head rend="h2"&gt;API notes&lt;/head&gt;
    &lt;p&gt;The problem of needing to layer information on top of existing C headers is not a new one. As noted earlier, Swift relies on a Clang feature called API notes to let us express this same information in a separate file, so we don’t have to edit the header. In this case, we create a file called &lt;code&gt;WebGPU.apinotes&lt;/code&gt; (the name &lt;code&gt;WebGPU&lt;/code&gt; matches the module name from &lt;code&gt;module.modulemap&lt;/code&gt;), which is a YAML file describing the extra information. We’ll start with one that turns &lt;code&gt;WGPUAdapterType&lt;/code&gt; into an &lt;code&gt;enum&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;---
Name: WebGPU
Tags:
- Name: WGPUAdapterType
  EnumExtensibility: closed
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Tags&lt;/code&gt; here is a term used in the C and C++ standard to refer to enum, struct, union, or class types. Any information about those types in the API notes file will go into that section.&lt;/p&gt;
    &lt;p&gt;Put &lt;code&gt;WebGPU.apinotes&lt;/code&gt; alongside the &lt;code&gt;module.modulemap&lt;/code&gt;, and now &lt;code&gt;WGPUAdapterType&lt;/code&gt; gets mapped into a &lt;code&gt;Swift&lt;/code&gt; enum. For a package, the structure will look like this:&lt;/p&gt;
    &lt;code&gt;├── Package.swift
└── Sources
    └── WebGPU
        ├── include
        │   ├── webgpu.h
        │   ├── WebGPU.apinotes
        │   └── module.modulemap
        └── WebGPU.c (empty file)
&lt;/code&gt;
    &lt;p&gt;We’ll be adding more to this API notes file as we keep digging through the interface.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reference-counted object types&lt;/head&gt;
    &lt;p&gt;The WebGPU header has a number of “object” types that are defined like this:&lt;/p&gt;
    &lt;code&gt;typedef struct WGPUBindGroupImpl* WGPUBindGroup WGPU_OBJECT_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;This gets imported into Swift as an alias for an opaque pointer type, which is… not great:&lt;/p&gt;
    &lt;code&gt;public typealias WGPUBindGroup = OpaquePointer
&lt;/code&gt;
    &lt;p&gt;WebGPU object types are reference counted, and each object type has corresponding &lt;code&gt;AddRef&lt;/code&gt; and &lt;code&gt;Release&lt;/code&gt; functions to increment and decrement the reference count, like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT void wgpuBindGroupAddRef(WGPUBindGroup bindGroup) WGPU_FUNCTION_ATTRIBUTE;
WGPU_EXPORT void wgpuBindGroupRelease(WGPUBindGroup bindGroup) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;Of course, you can use these functions in Swift exactly how you do in C, making sure to balance out calls to &lt;code&gt;AddRef&lt;/code&gt; and &lt;code&gt;Release&lt;/code&gt;, but then it would be every bit as unsafe as C.&lt;/p&gt;
    &lt;p&gt;We can do better with &lt;code&gt;SWIFT_SHARED_REFERENCE&lt;/code&gt;. It’s a macro (defined in the &lt;code&gt;&amp;lt;swift/bridging&amp;gt;&lt;/code&gt; header) that can turn a reference-counted C type like the above into an automatically reference-counted &lt;code&gt;class&lt;/code&gt; in Swift. Here’s how we would use it in the header:&lt;/p&gt;
    &lt;code&gt;typedef struct SWIFT_SHARED_REFERENCE(wgpuBindGroupAddRef, wgpuBindGroupRelease) WGPUBindGroupImpl* WGPUBindGroup WGPU_OBJECT_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;Now, &lt;code&gt;WGPUBindGroup&lt;/code&gt; gets imported like this:&lt;/p&gt;
    &lt;code&gt;public class WGPUBindGroupImpl { }
public typealias WGPUBindGroup = WGPUBindGroupImpl
&lt;/code&gt;
    &lt;p&gt;The extra typealias is a little unexpected, but overall this is a huge improvement: Swift is treating &lt;code&gt;WGPUBindGroup&lt;/code&gt; as a class, meaning that it automatically manages retains and releases for you! This is both an ergonomic win (less code to write) and a safety win, because it’s eliminated the possibility of mismanaging these instances.&lt;/p&gt;
    &lt;p&gt;There’s one more thing: when dealing with reference-counting APIs, you need to know whether a particular function that returns an object is expecting you to call “release” when you’re done. In the WebGPU header, this information is embedded in a comment:&lt;/p&gt;
    &lt;code&gt;/**
 * @returns
 * This value is @ref ReturnedWithOwnership.
 */
WGPU_EXPORT WGPUBindGroup wgpuDeviceCreateBindGroup(WGPUDevice device, WGPUBindGroupDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;“ReturnedWithOwnership” here means that the result of the call has already been retained one extra time, and the caller is responsible for calling “release” when they are done with it. The &lt;code&gt;&amp;lt;swift/bridging&amp;gt;&lt;/code&gt; header has a &lt;code&gt;SWIFT_RETURNS_RETAINED&lt;/code&gt; macro that expresses this notion. One can use it like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT WGPUBindGroup wgpuDeviceCreateBindGroup(WGPUDevice device, WGPUBindGroupDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE SWIFT_RETURNS_RETAINED;
&lt;/code&gt;
    &lt;p&gt;Now, Swift will balance out the retain that &lt;code&gt;wgpuDeviceCreateBindGroup&lt;/code&gt; has promised to do by performing the extra release once you’re done using the object. Once these annotations are done, we’re all set with a more ergonomic and memory-safe API for this C library. There’s no need to ever call &lt;code&gt;wgpuBindGroupRelease&lt;/code&gt; or &lt;code&gt;wgpuBindGroupAddRef&lt;/code&gt; yourself.&lt;/p&gt;
    &lt;p&gt;We’ve hacked up our header again, so let’s undo that and move all of this out to API notes. To turn a type into a foreign reference type, we augment the &lt;code&gt;Tags&lt;/code&gt; section of our API notes with the same information, but in YAML form:&lt;/p&gt;
    &lt;code&gt;- Name: WGPUBindGroupImpl
  SwiftImportAs: reference
  SwiftReleaseOp: wgpuBindGroupRelease
  SwiftRetainOp: wgpuBindGroupAddRef
&lt;/code&gt;
    &lt;p&gt;That makes &lt;code&gt;WGPUBindGroupImpl&lt;/code&gt; import as a class type, with the given retain and release functions. We can express the “returns retained” behavior of the &lt;code&gt;wgpuDeviceCreateBindGroup&lt;/code&gt; function like this:&lt;/p&gt;
    &lt;code&gt;Functions:
- Name: wgpuDeviceCreateBindGroup
  SwiftReturnOwnership: retained
&lt;/code&gt;
    &lt;p&gt;That’s enums and classes, so now let’s tackle… functions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Importing functions&lt;/head&gt;
    &lt;p&gt;A typical function from &lt;code&gt;webgpu.h&lt;/code&gt;, like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT void wgpuQueueWriteBuffer(
    WGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset, 
    void const * data, size_t size
) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;will come into Swift like this:&lt;/p&gt;
    &lt;code&gt;public func wgpuQueueWriteBuffer(_ queue: WGPUQueue!, _ buffer: WGPUBuffer!, _ bufferOffset: UInt64, _ data: UnsafeRawPointer!, _ size: Int)
&lt;/code&gt;
    &lt;p&gt;Note that &lt;code&gt;_&lt;/code&gt; on each parameter, which means that we won’t use argument labels for anything when we call it:&lt;/p&gt;
    &lt;code&gt;wgpuQueueWriteBuffer(myQueue, buffer, position, dataToWrite, bytesToWrite)
&lt;/code&gt;
    &lt;p&gt;That matches C, but it isn’t as clear as it could be in Swift. Let’s clean this up by providing a better name in Swift that includes argument labels. We can do so using &lt;code&gt;SWIFT_NAME&lt;/code&gt; (also in &lt;code&gt;&amp;lt;swift/bridging&amp;gt;&lt;/code&gt;), like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT void wgpuQueueWriteBuffer(
      WGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset,
      void const * data, size_t size
  ) WGPU_FUNCTION_ATTRIBUTE 
    SWIFT_NAME("wgpuQueueWriteBuffer(_:buffer:bufferOffset:data:size:)");
&lt;/code&gt;
    &lt;p&gt;Within the parentheses, we have each of the argument labels that we want (or &lt;code&gt;_&lt;/code&gt; meaning “no label”), each followed by a &lt;code&gt;:&lt;/code&gt;. This is how one describes a full function name in Swift. Once we’ve made this change to the Swift name, the C function comes into Swift with argument labels, like this:&lt;/p&gt;
    &lt;code&gt;public func wgpuQueueWriteBuffer(_ queue: WGPUQueue!, buffer: WGPUBuffer!, bufferOffset: UInt64, data: UnsafeRawPointer!, size: Int)
&lt;/code&gt;
    &lt;p&gt;That makes the call site more clear and self-documenting:&lt;/p&gt;
    &lt;code&gt;wgpuQueueWriteBuffer(myQueue, buffer: buffer, offset: position, data: dataToWrite, size: bytesToWrite)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Importing functions as methods&lt;/head&gt;
    &lt;p&gt;There is more usable structure in this API. Note that the &lt;code&gt;wgpuQueueWriteBuffer&lt;/code&gt; function takes, as its first argument, an instance of &lt;code&gt;WGPUQueue&lt;/code&gt;. Most of the C functions in &lt;code&gt;WebGPU.h&lt;/code&gt; are like this, because these are effectively functions that operate on their first argument. In a language that has methods, they would be methods. Swift has methods, so let’s make them methods!&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT void wgpuQueueWriteBuffer(
      WGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset, void const * data, size_t size) 
  WGPU_FUNCTION_ATTRIBUTE SWIFT_NAME("WGPUQueueImpl.writeBuffer(self:buffer:bufferOffset:data:size:)");
&lt;/code&gt;
    &lt;p&gt;There are three things to notice about this &lt;code&gt;SWIFT_NAME&lt;/code&gt; string:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It starts with &lt;code&gt;WGPUQueueImpl.&lt;/code&gt;, which tells Swift to make this function a member inside&lt;code&gt;WGPUQueueImpl&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Let’s change the function name to &lt;code&gt;writeBuffer&lt;/code&gt;, because we no longer need the&lt;code&gt;wgpuQueue&lt;/code&gt;prefix to distinguish it from other “write buffer” operations on other types.&lt;/item&gt;
      &lt;item&gt;The name of the first argument in parentheses is &lt;code&gt;self&lt;/code&gt;, which indicates that the&lt;code&gt;self&lt;/code&gt;argument (in Swift) should be passed as that positional argument to the C function. The other arguments are passed in-order.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that this also requires &lt;code&gt;WGPUQueue(Impl)&lt;/code&gt; to be imported as a &lt;code&gt;class&lt;/code&gt;, as we did earlier for &lt;code&gt;WGPUBindGroupImpl&lt;/code&gt;. Once we’ve done so, we get a much-nicer Swift API:&lt;/p&gt;
    &lt;code&gt;extension WGPUQueueImpl {
  /**
   * Produces a @ref DeviceError both content-timeline (`size` alignment) and d
evice-timeline
   * errors defined by the WebGPU specification.
   */
  public func writeBuffer(buffer: WGPUBuffer!, bufferOffset: UInt64, data: UnsafeRawPointer!, size: Int)
}
&lt;/code&gt;
    &lt;p&gt;We’ve hacked up the header again, but didn’t have to. In &lt;code&gt;WebGPU.apinotes&lt;/code&gt;, you can put a &lt;code&gt;SwiftName&lt;/code&gt; attribute on any entity. For &lt;code&gt;wgpuQueueWriteBuffer&lt;/code&gt;, it would look like this (in the &lt;code&gt;Functions&lt;/code&gt; section):&lt;/p&gt;
    &lt;code&gt;- Name: wgpuQueueWriteBuffer
  SwiftName: WGPUQueueImpl.writeBuffer(self:buffer:bufferOffset:data:size:)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Importing functions as properties&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;WebGPU.h&lt;/code&gt; has a number of &lt;code&gt;Get&lt;/code&gt; functions that produce information about some aspect of a type. Here are two for the &lt;code&gt;WGPUQuerySet&lt;/code&gt; type:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT uint32_t wgpuQuerySetGetCount(WGPUQuerySet querySet) WGPU_FUNCTION_ATTRIBUTE;
WGPU_EXPORT WGPUQueryType wgpuQuerySetGetType(WGPUQuerySet querySet) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;With the &lt;code&gt;SWIFT_NAME&lt;/code&gt; tricks above, we can turn these into “get” methods on &lt;code&gt;WGPUQuerySet&lt;/code&gt;, like this:&lt;/p&gt;
    &lt;code&gt;extension WGPUQuerySetImpl {
    public func getCount() -&amp;gt; UInt32
    public func getType() -&amp;gt; WGPUQueryType
}
&lt;/code&gt;
    &lt;p&gt;That’s okay, but it’s not what you’d do in Swift. Let’s go one step further and turn them into read-only computed properties. To do so, use the &lt;code&gt;getter:&lt;/code&gt; prefix on the Swift name we define. We’ll skip ahead to the YAML form that goes into API notes:&lt;/p&gt;
    &lt;code&gt;- Name: wgpuQuerySetGetCount
  SwiftName: getter:WGPUQuerySetImpl.count(self:)
- Name: wgpuQuerySetGetType
  SwiftName: getter:WGPUQuerySetImpl.type(self:)
&lt;/code&gt;
    &lt;p&gt;And now, we arrive at a nice Swift API:&lt;/p&gt;
    &lt;code&gt;extension WGPUQuerySetImpl {
    public var count: UInt32 { get }
    public var type: WGPUQueryType { get }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Importing functions as initializers&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;SWIFT_NAME&lt;/code&gt; can also be used to import a function that returns a new instance as a Swift initializer. For example, this function creates a new &lt;code&gt;WGPUInstance&lt;/code&gt; (which we assume is getting imported as a &lt;code&gt;class&lt;/code&gt; like we’ve been doing above):&lt;/p&gt;
    &lt;code&gt;/**
 * Create a WGPUInstance
 *
 * @returns
 * This value is @ref ReturnedWithOwnership.
 */
WGPU_EXPORT WGPUInstance wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;We can turn this into a Swift initializer, which is used to create a new object, using the same &lt;code&gt;SWIFT_NAME&lt;/code&gt; syntax but where the method name is &lt;code&gt;init&lt;/code&gt;. Here is the YAML form that goes into API notes:&lt;/p&gt;
    &lt;code&gt;- Name: wgpuCreateInstance
  SwiftReturnOwnership: retained
  SwiftName: WGPUInstanceImpl.init(descriptor:)
&lt;/code&gt;
    &lt;p&gt;and here is the resulting Swift initializer:&lt;/p&gt;
    &lt;code&gt;extension WGPUInstanceImpl {
    /**
     * Create a WGPUInstance
     *
     * @returns
     * This value is @ref ReturnedWithOwnership.
     */
    public /*not inherited*/ init!(descriptor: UnsafePointer&amp;lt;WGPUInstanceDescriptor&amp;gt;!)
}
&lt;/code&gt;
    &lt;p&gt;Now, one can create a new &lt;code&gt;WGPUInstance&lt;/code&gt; with the normal object-creation syntax, e.g.,&lt;/p&gt;
    &lt;code&gt;let instance = WGPUInstance(descriptor: myDescriptor)
&lt;/code&gt;
    &lt;head rend="h2"&gt;Another Boolean type?&lt;/head&gt;
    &lt;p&gt;The WebGPU header defines its own Boolean type. I wish everyone would use C99’s &lt;code&gt;_Bool&lt;/code&gt; and be done with it, but alas, here are the definitions for WebGPUs Boolean types:&lt;/p&gt;
    &lt;code&gt;#define WGPU_TRUE (UINT32_C(1))
#define WGPU_FALSE (UINT32_C(0))
typedef uint32_t WGPUBool;
&lt;/code&gt;
    &lt;p&gt;This means that &lt;code&gt;WGPUBool&lt;/code&gt; will come in to Swift as a &lt;code&gt;UInt32&lt;/code&gt;. The two macros aren’t available in Swift at all: they’re “too complicated” to be recognized as integral constants. Even if they were available in Swift, it still wouldn’t be great because we want to use &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;false&lt;/code&gt; for Boolean values in Swift, not &lt;code&gt;WGPU_TRUE&lt;/code&gt; and &lt;code&gt;WGPU_FALSE&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To make &lt;code&gt;WGPUBool&lt;/code&gt; easier to use from Swift, we’re first going to map that typedef to its own &lt;code&gt;struct&lt;/code&gt; that stores the underlying &lt;code&gt;UInt32&lt;/code&gt;, giving it an identity separate from &lt;code&gt;UInt32&lt;/code&gt;. We can do this using a &lt;code&gt;SwiftWrapper&lt;/code&gt; API note within the &lt;code&gt;Typedefs&lt;/code&gt; section of the file, like this:&lt;/p&gt;
    &lt;code&gt;- Name: WGPUBool
  SwiftWrapper: struct
&lt;/code&gt;
    &lt;p&gt;Now, we get &lt;code&gt;WGPUBool&lt;/code&gt; imported like this:&lt;/p&gt;
    &lt;code&gt;public struct WGPUBool : Hashable, Equatable, RawRepresentable {
    public init(_ rawValue: UInt32)
    public init(rawValue: UInt32)
}
&lt;/code&gt;
    &lt;p&gt;To be able to use &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;false&lt;/code&gt; literals with this new &lt;code&gt;WGPUBool&lt;/code&gt;, we can write a little bit of Swift code that makes this type conform to the &lt;code&gt;ExpressibleByBooleanLiteral&lt;/code&gt; protocol, like this:&lt;/p&gt;
    &lt;code&gt;extension WGPUBool: ExpressibleByBooleanLiteral {
  init(booleanLiteral value: Bool) {
    self.init(rawValue: value ? 1 : 0)
  }
}
&lt;/code&gt;
    &lt;p&gt;That’s it! Better type safety (you cannot confuse a &lt;code&gt;WGPUBool&lt;/code&gt; with any other integer value) and the convenience of Boolean literals in Swift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Option sets&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;webgpu.h&lt;/code&gt; describes a set of flags using a &lt;code&gt;typedef&lt;/code&gt; of the &lt;code&gt;WGPUFlags&lt;/code&gt; type (a 64-bit unsigned integer) along with a set of global constants for the different flag values. For example, here is the &lt;code&gt;WGPUBufferUsage&lt;/code&gt; flag type and some of its constants:&lt;/p&gt;
    &lt;code&gt;typedef WGPUFlags WGPUBufferUsage;
static const WGPUBufferUsage WGPUBufferUsage_MapRead = 0x0000000000000001;
static const WGPUBufferUsage WGPUBufferUsage_MapWrite = 0x0000000000000002;
static const WGPUBufferUsage WGPUBufferUsage_CopySrc = 0x0000000000000004;
static const WGPUBufferUsage WGPUBufferUsage_Index = 0x0000000000000010;
&lt;/code&gt;
    &lt;p&gt;Similar to what we saw with &lt;code&gt;WGPUBool&lt;/code&gt;, &lt;code&gt;WGPUBufferUsage&lt;/code&gt; is a &lt;code&gt;typedef&lt;/code&gt; of a &lt;code&gt;typedef&lt;/code&gt; of a &lt;code&gt;uint64_t&lt;/code&gt;. There’s no type safety in this C API, and one could easily mix up these flags with, say, those of &lt;code&gt;WGPUMapMode&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;typedef WGPUFlags WGPUMapMode;
static const WGPUMapMode WGPUMapMode_Read = 0x0000000000000001;
static const WGPUMapMode WGPUMapMode_Write = 0x0000000000000002;
&lt;/code&gt;
    &lt;p&gt;We can do better, by layering more structure for the Swift version of this API using the same &lt;code&gt;SwiftWrapper&lt;/code&gt; approach from &lt;code&gt;WGPUBool&lt;/code&gt;. This goes into the &lt;code&gt;Typedefs&lt;/code&gt; section of API notes:&lt;/p&gt;
    &lt;code&gt;Typedefs:
- Name: WGPUBufferUsage
  SwiftWrapper: struct
&lt;/code&gt;
    &lt;p&gt;Now, &lt;code&gt;WGPUBufferUsage&lt;/code&gt; comes in as its own &lt;code&gt;struct&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;public struct WGPUBufferUsage : Hashable, Equatable, RawRepresentable {
    public init(_ rawValue: WGPUFlags)
    public init(rawValue: WGPUFlags)
}
&lt;/code&gt;
    &lt;p&gt;The initializers let you create a &lt;code&gt;WGPUBufferUsage&lt;/code&gt; from a &lt;code&gt;WGPUFlags&lt;/code&gt; value, and there is also a &lt;code&gt;rawValue&lt;/code&gt; property to get a &lt;code&gt;WGPUFlags&lt;/code&gt; value out of a &lt;code&gt;WGPUBufferInstance&lt;/code&gt;, so the raw value is always there… but the default is to be type safe. Additionally, those global constants will come in as members of &lt;code&gt;WGPUBufferUsage&lt;/code&gt;, like this:&lt;/p&gt;
    &lt;code&gt;extension WGPUBufferUsage {
    /**
     * The buffer can be *mapped* on the CPU side in *read* mode (using @ref WGPUMapMode_Read).
     */
    public static var _MapRead: WGPUBufferUsage { get }

    /**
     * The buffer can be *mapped* on the CPU side in *write* mode (using @ref WGPUMapMode_Write).
     *
     * @note This usage is **not** required to set `mappedAtCreation` to `true` in @ref WGPUBufferDescriptor.
     */
    public static var _MapWrite: WGPUBufferUsage { get }

    /**
     * The buffer can be used as the *source* of a GPU-side copy operation.
     */
    public static var _CopySrc: WGPUBufferUsage { get }

    /**
     * The buffer can be used as the *destination* of a GPU-side copy operation.
     */
    public static var _CopyDst: WGPUBufferUsage { get }
}
&lt;/code&gt;
    &lt;p&gt;This means that, if you’re passing a value of type &lt;code&gt;WPUBufferUsage&lt;/code&gt;, you can use the shorthand “leading dot” syntax. For example:&lt;/p&gt;
    &lt;code&gt;func setBufferUsage(_ usage: WGPUBufferUsage) { ... }

setBufferUsage(._MapRead)
&lt;/code&gt;
    &lt;p&gt;Swift has dropped the common &lt;code&gt;WPUBufferUsage&lt;/code&gt; prefix from the constants when it made them into members. However, the resulting names aren’t great. We can rename them by providing a &lt;code&gt;SwiftName&lt;/code&gt; in the API notes file within the &lt;code&gt;Globals&lt;/code&gt; section:&lt;/p&gt;
    &lt;code&gt;Globals:
- Name: WGPUBufferUsage_MapRead
  SwiftName: WGPUBufferUsage.mapRead
- Name: WGPUBufferUsage_MapWrite
  SwiftName: WGPUBufferUsage.mapWrite
&lt;/code&gt;
    &lt;p&gt;We can go one step further by making the &lt;code&gt;WGPUBufferUsage&lt;/code&gt; type conform to Swift’s &lt;code&gt;OptionSet&lt;/code&gt; protocol. If we revise the API notes like this:&lt;/p&gt;
    &lt;code&gt;Typedefs:
- Name: WGPUBufferUsage
  SwiftWrapper: struct
  SwiftConformsTo: Swift.OptionSet
&lt;/code&gt;
    &lt;p&gt;Now, we get the nice option-set syntax we expect in Swift:&lt;/p&gt;
    &lt;code&gt;let usageFlags: WGPUBufferUsage = [.mapRead, .mapWrite]
&lt;/code&gt;
    &lt;head rend="h2"&gt;Nullability&lt;/head&gt;
    &lt;p&gt;Throughout &lt;code&gt;webgpu.h&lt;/code&gt;, the &lt;code&gt;WGPU_NULLABLE&lt;/code&gt; macro is used to indicate pointers that can be NULL. The implication is that any pointer that is not marked with &lt;code&gt;WGPU_NULLABLE&lt;/code&gt; cannot be NULL. For example, here is the definition of &lt;code&gt;wgpuCreateInstance&lt;/code&gt; we used above:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT WGPUInstance wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;WGPU_NULLABLE&lt;/code&gt; indicates that it’s acceptable to pass a NULL pointer in as the &lt;code&gt;descriptor&lt;/code&gt; parameter. Clang already has nullability specifiers to express this information. We could alter the declaration in the header to express that this parameter is nullable but the result type is never NULL, like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT WGPUInstance _Nonnull wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * _Nullable descriptor) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;This eliminates the implicitly-unwrapped optionals (&lt;code&gt;!&lt;/code&gt;) from the signature of the initializer, so we end up with one that explicitly accepts a &lt;code&gt;nil&lt;/code&gt; descriptor argument and always returns a new instance (never &lt;code&gt;nil&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;extension WGPUInstanceImpl {
    /**
     * Create a WGPUInstance
     *
     * @returns
     * This value is @ref ReturnedWithOwnership.
     */
    public /*not inherited*/ init(descriptor: UnsafePointer&amp;lt;WGPUInstanceDescriptor&amp;gt;?)
}
&lt;/code&gt;
    &lt;p&gt;Now, I did cheat by hacking the header. Instead, we can express this with API notes on the parameters and result type by extending the entry we already have for &lt;code&gt;wgpuCreateInstance&lt;/code&gt; like this:&lt;/p&gt;
    &lt;code&gt;- Name: wgpuCreateInstance
  SwiftReturnOwnership: retained
  SwiftName: WGPUInstanceImpl.init(descriptor:)
  Parameters:
  - Position: 0
    Nullability: O
  ResultType: "WGPUInstance _Nonnull"
&lt;/code&gt;
    &lt;p&gt;To specific nullability of pointer parameters, one can identify them by position (where 0 is the first parameter to the function) and then specify whether the parameter should come into Swift as optional (&lt;code&gt;O&lt;/code&gt;, corresponds to &lt;code&gt;_Nullable&lt;/code&gt;), non-optional (&lt;code&gt;N&lt;/code&gt;, corresponds to &lt;code&gt;_Nonnull&lt;/code&gt;) or by left unspecified as an implicitly-unwrapped optional (&lt;code&gt;U&lt;/code&gt;, corresponds to &lt;code&gt;_Null_unspecified&lt;/code&gt;). For the result type, it’s a little different: we specified the result type along with the nullability specifier, i.e., &lt;code&gt;WGPUInstance _Nonnull&lt;/code&gt;. The end result of these annotations is the same as the modified header, so we can layer nullability information on top of the header.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scripting the creation of &lt;code&gt;WebGPU.apinotes&lt;/code&gt; WebGPU.apinotes section" href="#scripting-the-creation-of-webgpuapinotes"&amp;gt;
             
          &lt;/head&gt;
    &lt;p&gt;&lt;code&gt;webgpu.h&lt;/code&gt; is about 6,400 lines long, and is regenerated from a database of the API as needed. Each of the WebGPU implementations seems to augment or tweak the header a bit. So, rather than grind through and manually do annotations, I wrote a little Swift script to “parse” &lt;code&gt;webgpu.h&lt;/code&gt;, identify its patterns, and generate &lt;code&gt;WebGPU.apinotes&lt;/code&gt; for most of what is discussed in this post. The entirety of the script is here. It reads &lt;code&gt;webgpu.h&lt;/code&gt; from standard input and prints &lt;code&gt;WebGPU.apinotes&lt;/code&gt; to standard output.&lt;/p&gt;
    &lt;p&gt;Because &lt;code&gt;webgpu.h&lt;/code&gt; is generated, it has a very regular structure that we can pick up on via regular expressions. For example:&lt;/p&gt;
    &lt;code&gt;// Enum definitions, marked by WGPU_ENUM_ATTRIBUTE.
let enumMatcher = /} (?&amp;lt;name&amp;gt;\w+?) WGPU_ENUM_ATTRIBUTE/

// Object definitions, marked by WGPU_OBJECT_ATTRIBUTE.
let objectMatcher = /typedef struct (?&amp;lt;implName&amp;gt;\w+?)\* (?&amp;lt;name&amp;gt;\w+?) WGPU_OBJECT_ATTRIBUTE;/

// Function declarations, marked by WGPU_FUNCTION_ATTRIBUTE
let functionMatcher = /WGPU_EXPORT (?&amp;lt;nullableResult&amp;gt;WGPU_NULLABLE ?)?(?&amp;lt;resultType&amp;gt;\w+?) (?&amp;lt;name&amp;gt;\w+?)\((?&amp;lt;parameters&amp;gt;.*\)?) WGPU_FUNCTION_ATTRIBUTE;/
let parameterMatcher = /(?&amp;lt;type&amp;gt;[^),]+?) (?&amp;lt;name&amp;gt;\w+?)[),]/
&lt;/code&gt;
    &lt;p&gt;That’s enough to identify all of the enum types (so we can emit the &lt;code&gt;EnumExtensibility: closed&lt;/code&gt; API notes), object types (to turn them into shared references), and functions (which get nicer names and such). The script is just a big &lt;code&gt;readLine&lt;/code&gt; loop that applies the regexes to capture all of the various types and functions, then does some quick classification before printing out the API notes. The resulting API notes are in WebGPU.apinotes, and the generated Swift interface after these API notes are applied is here. You can run it with, e.g.,&lt;/p&gt;
    &lt;code&gt;swift -enable-bare-slash-regex webgpu_apinotes.swift &amp;lt; webgpu.h
&lt;/code&gt;
    &lt;p&gt;This script full of regular expressions is, admittedly, a bit of a hack. A better approach for an arbitrary C header would be to use &lt;code&gt;libclang&lt;/code&gt; to properly parse the headers. For WebGPU specifically, the webgpu-headers project contains a database from which the header is generated, and one could also generate API notes directly from that header. Regardless of how you get there, many C libraries have well-structured headers with conventions that can be leveraged to create safer, more ergonomic projections in Swift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Swiftifying your favorite C library&lt;/head&gt;
    &lt;p&gt;The techniques described in this post can be applied to just about any C library. To do so, I recommend setting up a small package like the one described here for WebGPU, so you can iterate quickly on example code to get a feel for how the Swift projection of the C API will work. The annotations might not get you all the way to the best Swift API, but they are a lightweight way to get most of the way there. Feel free to also extend the C types to convenience APIs that make sense in Swift, like I did above to make &lt;code&gt;WGPUBool&lt;/code&gt; conform to &lt;code&gt;ExpressibleByBooleanLiteral&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A little bit of annotation work on your favorite C library can make for a safer, more ergonomic, more Swifty experience of working with that library.&lt;/p&gt;
    &lt;head rend="h2"&gt;Postscript: Thoughts for improving the generated webgpu.h&lt;/head&gt;
    &lt;p&gt;The regular structure of &lt;code&gt;webgpu.h&lt;/code&gt; helped considerably when trying to expose the API nicely in Swift. That said, there are a few ways in which &lt;code&gt;webgpu.h&lt;/code&gt; could be improved to require less annotation for this purpose:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;WGPU_ENUM_ATTRIBUTE&lt;/code&gt;would be slightly nicer if placed on the&lt;code&gt;enum&lt;/code&gt;itself, rather than on the&lt;code&gt;typedef&lt;/code&gt;. If it were there, we could use&lt;code&gt;#define WGPU_ENUM_ATTRIBUTE __attribute__((enum_extensibility(closed)))&lt;/code&gt;&lt;p&gt;and not have to generate any API notes to bring these types in as proper enums in Swift.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;WGPU_OBJECT_ATTRIBUTE&lt;/code&gt;could provide the names of the retain and release operations and be placed on the&lt;code&gt;struct&lt;/code&gt;itself. If it were there, we could use&lt;code&gt;#define WGPU_OBJECT_ATTRIBUTE(RetainFn,ReleaseFn) SWIFT_SHARED_REFERENCE(RetainFn,ReleaseFn)&lt;/code&gt;&lt;p&gt;and not have to generate any API notes to bring these types in as classes in Swift.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;WGPU_NULLABLE&lt;/code&gt;could be placed on the pointer itself (i.e., after the&lt;code&gt;*&lt;/code&gt;) rather than at the beginning of the type, to match the position of Clang’s nullability attributes. If it were placed there, then&lt;code&gt;#define WGPU_NULLABLE _Nullable&lt;/code&gt;&lt;p&gt;would work with Clangs’ longstanding nullable-types support. Swift would then import such pointers as optional types (with&lt;/p&gt;&lt;code&gt;?&lt;/code&gt;). Moreover, if some macros&lt;code&gt;WGPU_ASSUME_NONNULL_BEGIN&lt;/code&gt;and&lt;code&gt;WGPU_ASSUME_NONNULL_END&lt;/code&gt;were placed at the beginning and end of the header, they could be mapped to Clang’s pragmas to assume that any pointer not marked “nullable” is always non-null:&lt;code&gt;#define WGPU_ASSUME_NONNULL_BEGIN #pragma clang assume_nonnull begin #define WGPU_ASSUME_NONNULL_END #pragma clang assume_nonnull end&lt;/code&gt;&lt;p&gt;This would eliminate all of the implicitly unwrapped optionals (marked&lt;/p&gt;&lt;code&gt;!&lt;/code&gt;in the Swift interface), making it easier to use safely.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/"/><published>2026-01-22T23:34:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46727587</id><title>Bugs Apple Loves</title><updated>2026-01-23T14:18:05.159597+00:00</updated><content>&lt;doc fingerprint="e3cc5b7e4366455c"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading Apple's bugs&lt;/p&gt;
    &lt;p&gt;Why else would they keep them around for so long?&lt;/p&gt;
    &lt;p&gt;Total time wasted by humanity because Apple won't fix these&lt;/p&gt;
    &lt;p&gt;Calculating...&lt;/p&gt;
    &lt;p&gt;and counting&lt;/p&gt;
    &lt;p&gt;Every bug is different. But the math is always real.&lt;lb/&gt;Think our numbers are wrong? Edit them yourself.&lt;/p&gt;
    &lt;code&gt;Users Affected × Frequency × Time Per Incident&lt;/code&gt;
    &lt;p&gt;How many Apple users hit this bug, how often, and how long they suffer each time.&lt;/p&gt;
    &lt;code&gt;Σ (Workaround Time × Participation Rate)&lt;/code&gt;
    &lt;p&gt;The extra time spent by people who try to fix what Apple won't.&lt;/p&gt;
    &lt;code&gt;Years Unfixed × Pressure Factor&lt;/code&gt;
    &lt;p&gt;How long Apple has known about this and how urgent the task usually is.&lt;/p&gt;
    &lt;code&gt;Human Hours Wasted ÷ Engineering Hours to Fix&lt;/code&gt;
    &lt;p&gt;How many times over Apple could have fixed it with the productivity they've destroyed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bugsappleloves.com"/><published>2026-01-23T02:24:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46728808</id><title>I built a light that reacts to radio waves [video]</title><updated>2026-01-23T14:18:04.354191+00:00</updated><content>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=moBCOEiqiPs"/><published>2026-01-23T05:34:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46729368</id><title>Proton Spam and the AI Consent Problem</title><updated>2026-01-23T14:18:04.112569+00:00</updated><content>&lt;doc fingerprint="ef5f40914ea2becf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Proton Spam and the AI Consent Problem&lt;/head&gt;
    &lt;p&gt;On Jan 14th Proton sent out an email newsletter with the subject line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Introducing Projects - Try Lumoâs powerful new feature now&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Lumo is Protonâs &lt;/p&gt;
    &lt;p&gt;There is a problem with this email. And Iâm not talking about the question of how exactly AI aligns with Protonâs core values of privacy and security.&lt;/p&gt;
    &lt;p&gt;The problem is I had already explicitly opted out of Lumo emails.&lt;/p&gt;
    &lt;p&gt;That toggle for âLumo product updatesâ is unchecked. Lumo is the only topic Iâm not subscribed to. Proton has over a dozen newsletters, including some crypto nonsense. I opt-in to everything but Lumo, I gave an undeniable no to Lumo emails.&lt;/p&gt;
    &lt;p&gt;So the email I received from Proton is spam, right?&lt;/p&gt;
    &lt;p&gt;My understanding is that spam is a violation of GDPR and UK data protection laws. Regardless, Protonâs email is a clear abuse of their own service towards a paying business customer.&lt;/p&gt;
    &lt;p&gt;Before I grab my pitchfork I emailed Proton support.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proton Support&lt;/head&gt;
    &lt;p&gt;Despite the subject line and contents, and despite the âFrom Lumoâ name and &lt;code&gt;@lumo.proton.me&lt;/code&gt; address, maybe this was an honest mistake?&lt;/p&gt;
    &lt;p&gt;Protonâs first reply explained how to opt-out.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hello David,&lt;/p&gt;
      &lt;p&gt;Thank you for contacting us.&lt;/p&gt;
      &lt;p&gt;You can unsubscribe from the newsletters if you do the following:&lt;/p&gt;
      &lt;p&gt;- Log in to your account at https://account.protonvpn.com/login&lt;/p&gt;
      &lt;p&gt;- Navigate to the Account category&lt;/p&gt;
      &lt;p&gt;- Disable the check-marks under âEmail subscriptionsâ&lt;/p&gt;
      &lt;p&gt;- If you need additional assistance, let me know.&lt;/p&gt;
      &lt;p&gt;[screenshot of the same opt-out toggle]&lt;/p&gt;
      &lt;p&gt;-Have a nice day.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;John Support directs me to the exact same âLumo product updatesâ toggle I had already unchecked. I replied explaining that I had already opted out. Support replies saying theyâre âchecking this with the teamâ then later replies again asking for screenshots.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Can you make sure to send me a screenshot of this newsletter option disabled, as well as the date when the last message was sent to you regarding the Lumo offer?&lt;/p&gt;
      &lt;p&gt;You can send me a screenshot of the whole message, including the date.&lt;/p&gt;
      &lt;p&gt;Is it perhaps 14 January 2026 that you received the message?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I found that last line curious, are they dealing with other unhappy customers? Maybe Iâm reading too much into it.&lt;/p&gt;
    &lt;p&gt;I sent the screenshots and signed off with âDonât try to pretend this fits into another newsletter category.â&lt;/p&gt;
    &lt;p&gt;After more âchecking this with the teamâ I got a response today.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In this case, the mentioned newsletter is for promoting Lumo Business Suit to Business-related plans.&lt;/p&gt;
      &lt;p&gt;Hence, why you received it, as Product Updates and Email Subscription are two different things.&lt;/p&gt;
      &lt;p&gt;In the subscription section, you will see the âEmail Subscriptionâ category, where you can disable the newsletter in order to avoid getting it in the future.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If I understand correctly, Proton are claiming this email is the âProton for Business newsletterâ. Not the âLumo product updatesâ newsletter.&lt;/p&gt;
    &lt;p&gt;I donât know about you, but I think thatâs baloney. Proton Support had five full business days to come up with a better excuse. Please tell me, how can I have been any more explicit about opting out of Lumo emails, only to receive âTry Lumoâ âFrom Lumoâ, and be told that is not actually a Lumo email?&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-Consent&lt;/head&gt;
    &lt;p&gt;Has anyone else noticed that the AI industry canât take ânoâ for an answer? AI is being force-fed into every corner of tech. Itâs unfathomable to them that some of us arenât interested.&lt;/p&gt;
    &lt;p&gt;The entire AI industry is built upon a common principle of non-consent. They laugh in the face of IP and copyright law. AI bots DDoS websites and lie about user-agents. Can it get worse than the sickening actions of Grok? I dread to think.&lt;/p&gt;
    &lt;p&gt;As Proton has demonstrated above, and Mozilla/Firefox recently too, the AI industry simply will not accept ânoâ as an answer. Some examples like spam are more trivial than others, but the growing trend is vile and disturbing.&lt;/p&gt;
    &lt;p&gt;I do not want your AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Update for 23rd January&lt;/head&gt;
    &lt;p&gt;I guess someone at Microsoft read my post and said âhold my beerâ. This morning I woke up to a lovely gift in my inbox; âBuild Al agents with the new GitHub Copilot SDKâ.&lt;/p&gt;
    &lt;p&gt;GitHub Ensloppification is moving faster than I can delete my account for good. (Itâs an unfortunate requirement for client projects.) For the record, I have never said âyesâ to any GitHub newsletter. Even before Copilot I disabled every possible GitHub email notification.&lt;/p&gt;
    &lt;p&gt;The âUnsubscribeâ link provides the hidden newsletter list. There is nothing within GitHub account settings I can find to disable spam.&lt;/p&gt;
    &lt;p&gt;As expected, Microsoft has opted me in without my consent. The wheels are falling off at GitHub. The brutally slow front-end UI. The embarrassingly lacklustre Actions CI. Now this sloppy tripe everywhere. Reminder to developers: GitHub is not Git.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dbushell.com/2026/01/22/proton-spam/"/><published>2026-01-23T07:01:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46730214</id><title>Replacing Protobuf with Rust to go 5 times faster</title><updated>2026-01-23T14:18:03.916940+00:00</updated><content>&lt;doc fingerprint="ae1a93792c310158"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Replacing Protobuf with Rust to go 5 times faster&lt;/head&gt;
    &lt;p&gt;Jan 22nd, 2026&lt;lb/&gt;Lev Kokotov&lt;/p&gt;
    &lt;p&gt;PgDog is a proxy for scaling PostgreSQL. Under the hood, we use &lt;code&gt;libpg_query&lt;/code&gt; to parse and understand SQL queries. Since PgDog is written in Rust, we use its Rust bindings to interface with the core C library. 
Those bindings use Protobuf (de)serialization to work uniformly across different programming languages, e.g., the popular Ruby pg_query gem.&lt;/p&gt;
    &lt;p&gt;Protobuf is fast, but not using Protobuf is faster. We forked pg_query.rs and replaced Protobuf with direct C-to-Rust (and back to C) bindings, using bindgen and Claude-generated wrappers. This resulted in a 5x improvement in parsing queries, and a 10x improvement in deparsing (Postgres AST to SQL string conversion).&lt;/p&gt;
    &lt;head rend="h5"&gt;Results&lt;/head&gt;
    &lt;p&gt;You can reproduce these by cloning our fork and running the benchmark tests:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Function&lt;/cell&gt;
        &lt;cell role="head"&gt;Queries per second&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;pg_query::parse&lt;/code&gt; (Protobuf)&lt;/cell&gt;
        &lt;cell&gt;613&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;pg_query::parse_raw&lt;/code&gt; (Direct C to Rust)&lt;/cell&gt;
        &lt;cell&gt;3357 (5.45x faster)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;pg_query::deparse&lt;/code&gt; (Protobuf)&lt;/cell&gt;
        &lt;cell&gt;759&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;pg_query::deparse_raw&lt;/code&gt; (Direct Rust to C)&lt;/cell&gt;
        &lt;cell&gt;7319 (9.64x faster)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;The process&lt;/head&gt;
    &lt;p&gt;The first step is always profiling. We use samply, which integrates nicely with the Firefox profiler. Samply is a sampling profiler: it measures how much time code spends running CPU instructions in each function. It works by inspecting the application call stack thousands of times per second. The more time is spent inside a particular function (or span, as they are typically called), the slower that code is. This is how we discovered &lt;code&gt;pg_query_parse_protobuf&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;This is the entrypoint to the &lt;code&gt;libpg_query&lt;/code&gt; C library, used by all pg_query bindings. The function that wraps the actual Postgres parser, &lt;code&gt;pg_query_raw_parse&lt;/code&gt;, barely registered on the flame graph. Parsing queries isn’t free, but the Postgres parser itself is very quick and has been optimized for a long time. With the hot spot identified, our first instinct was to do nothing and just add a cache.&lt;/p&gt;
    &lt;head rend="h4"&gt;Caching mostly works&lt;/head&gt;
    &lt;p&gt;Caching is a trade-off between memory and CPU utilization, and memory is relatively cheap (latest DRAM crunch notwithstanding). The cache is mutex-protected, uses the LRU algorithm and is backed by a hashmap1. The query text is the key and the Abstract Syntax Tree is the value, which expects most apps to use prepared statements. The query text contains placeholders instead of actual values and is therefore reusable, for example:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM users WHERE id = $1;
&lt;/code&gt;
    &lt;p&gt;While the &lt;code&gt;id&lt;/code&gt; parameter can change between invocations, the prepared statement does not, so we could cache its static AST in memory.&lt;/p&gt;
    &lt;p&gt;This works pretty well, but eventually we ran into a couple of issues:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Some ORMs can have bugs that generate thousands of unique statements, e.g., &lt;code&gt;value IN ($1, $2, $3)&lt;/code&gt;instead of&lt;code&gt;value = ANY($1)&lt;/code&gt;, which causes a lot of cache misses&lt;/item&gt;
      &lt;item&gt;Applications use old PostgreSQL client drivers which don’t support prepared statements, e.g., Python’s &lt;code&gt;psycopg2&lt;/code&gt;package&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The clock on Protobuf was ticking and we needed to act. So, like a lot of engineers these days, we asked an LLM to just do it for us.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tight constraints&lt;/head&gt;
    &lt;p&gt;I’m going to preface this section by saying that the vast majority of PgDog’s source code is written by a human. AI is not in a position to one-shot a connection pooler, load balancer and database sharder. However, when scoped to a very specific, well-defined and most importantly machine-verifiable task, it can work really well.&lt;/p&gt;
    &lt;p&gt;The prompt we started with was pretty straightforward:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;libpg_query is a library that wraps the PostgreSQL parser in an API. pg_query.rs is a Rust wrapper around libpg_query which uses Protobuf for (de)serialization. Replace Protobuf with bindgen-generated Rust structs that map directly to the Postgres AST.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And after two days of back and forth between us and the machine, it worked. We ended up with 6,000 lines of recursive Rust that manually mapped C types and structs to Rust structs, and vice versa. We made the switch for &lt;code&gt;parse&lt;/code&gt;, &lt;code&gt;deparse&lt;/code&gt; (used in our new query rewrite engine, which we’ll talk about in another post), &lt;code&gt;fingerprint&lt;/code&gt; and &lt;code&gt;scan&lt;/code&gt;. These four methods are heavily used in PgDog to make sharding work, and we immediately saw a 25% improvement in pgbench benchmarks2.&lt;/p&gt;
    &lt;p&gt;Just to be clear: we had a lot of things going for us already that made this possible. First, pg_query has a Protobuf spec for protoc (and Prost, the Protobuf Rust implementation) to generate bindings, so Claude was able to get a comprehensive list of structs it needed to extract from C, along with the expected data types.&lt;/p&gt;
    &lt;p&gt;Second, pg_query.rs was already using bindgen, so we had to just copy/paste some invocations around to get the AST structs included in bindgen’s output.&lt;/p&gt;
    &lt;p&gt;And last, and definitely not least, pg_query.rs already had a working &lt;code&gt;parse&lt;/code&gt; and &lt;code&gt;deparse&lt;/code&gt; implementation, so we could test our AI-generated code against its output. This was entirely automated and verifiable: for each test case that used &lt;code&gt;parse&lt;/code&gt;, we included a call to &lt;code&gt;parse_raw&lt;/code&gt;, compared their results and if they differed by even one byte, Claude Code had to go back and try again.&lt;/p&gt;
    &lt;head rend="h4"&gt;The implementation&lt;/head&gt;
    &lt;p&gt;The translation code between Rust and C uses &lt;code&gt;unsafe&lt;/code&gt; Rust functions that wrap Rust structs to C structs. The C structs are then passed to the Postgres/libpg_query C API which does the actual work of building the AST.&lt;/p&gt;
    &lt;p&gt;The result is converted back to Rust using a recursive algorithm: each node in the AST has its own converter function which accepts an &lt;code&gt;unsafe&lt;/code&gt; C pointer and returns a safe Rust struct. Much like the name suggests, the AST is a tree, which is stored in an array:&lt;/p&gt;
    &lt;code&gt;unsafe fn convert_list_to_raw_stmts(
    list: *mut bindings_raw::List
) -&amp;gt; Vec&amp;lt;protobuf::RawStmt&amp;gt; {
    // C-to-Rust conversion.
}
&lt;/code&gt;
    &lt;p&gt;For each node in the list, the implementation calls &lt;code&gt;convert_node&lt;/code&gt;, which then handles each one of the 100s of tokens available in the SQL grammar:&lt;/p&gt;
    &lt;code&gt;unsafe fn convert_node(
    node_ptr: *mut bindings_raw::Node
) -&amp;gt; Option&amp;lt;protobuf::Node&amp;gt; {
    // This is basically C in Rust, so we better check for nulls!
    if node_ptr.is_null() {
        return None;
    }

    match (*node_ptr).type_ {
        // SELECT statement root node.
        bindings_raw::NodeTag_T_SelectStmt =&amp;gt; {
            let stmt = node_ptr as *mut bindings_raw::SelectStmt;
            Some(protobuf::node::Node::SelectStmt(Box::new(convert_select_stmt(&amp;amp;*stmt))))
        }
        
        // INSERT statement root node.
        bindings_raw::NodeTag_T_InsertStmt =&amp;gt; {
            let stmt = node_ptr as *mut bindings_raw::InsertStmt;
            Some(protobuf::node::Node::InsertStmt(Box::new(convert_insert_stmt(&amp;amp;*stmt))))
        }
        
        // ... 100s more nodes.
    }
}
&lt;/code&gt;
    &lt;p&gt;For nodes that contain other nodes, we recurse on &lt;code&gt;convert_node&lt;/code&gt; again until the algorithm reaches the leaves (nodes with no children) and terminates. For nodes that contain scalars, like a number (e.g., &lt;code&gt;5&lt;/code&gt;) or text (e.g., &lt;code&gt;'hello world'&lt;/code&gt;), the data type is copied into a Rust analog, e.g., &lt;code&gt;i32&lt;/code&gt; or &lt;code&gt;String&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The end result is &lt;code&gt;protobuf::ParseResult&lt;/code&gt;, a Rust struct generated by Prost from the pg_query API Protobuf specification, but populated by native Rust code instead of Prost’s deserializer. Reusing existing structs reduces the chance of errors considerably: we can compare &lt;code&gt;parse&lt;/code&gt; and &lt;code&gt;parse_raw&lt;/code&gt; outputs, using the derived &lt;code&gt;PartialEq&lt;/code&gt; trait, and ensure that both are identical, in testing.&lt;/p&gt;
    &lt;p&gt;While recursive algorithms have a questionable reputation in the industry because bad ones can cause stack overflows, they are very fast. Recursion requires no additional memory allocation because all of its working space, the stack, is created on program startup. It also has excellent CPU cache locality because the instructions for the next invocation of the same function are already in the CPU L1/L2/L3 cache. Finally and arguably more importantly, they are just easier to read and understand than iterative implementations, which helps us, the humans, with debugging.&lt;/p&gt;
    &lt;p&gt;Just for good measure, we tried generating an iterative algorithm, but it ended up being slower than Prost. The main cause (we think) was unnecessary memory allocations, hashmap lookups of previously converted nodes, and too much overhead from walking the tree several times. Meanwhile, recursion processes each AST node exactly once and uses the stack pointer to track its position in the tree. If you have any ideas on how to make an iterative algorithm work better, let us know!&lt;/p&gt;
    &lt;head rend="h3"&gt;Closing thoughts&lt;/head&gt;
    &lt;p&gt;Reducing the overhead from using the Postgres parser in PgDog makes a huge difference for us. As a network proxy, our budget for latency, memory utilization, and CPU cycles is low. After all, we aren’t a real database…yet! This change improves performance from two angles: we use less CPU and we do less work, so PgDog is faster and cheaper to run.&lt;/p&gt;
    &lt;p&gt;If stuff like this is interesting to you, reach out. We are looking for a Founding Software Engineer to help us grow and build the next iteration of horizontal scaling for PostgreSQL.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pgdog.dev/blog/replace-protobuf-with-rust"/><published>2026-01-23T09:03:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46730346</id><title>The state of modern AI text to speech systems for screen reader users</title><updated>2026-01-23T14:18:03.615960+00:00</updated><content>&lt;doc fingerprint="85deb33a0b1ab384"&gt;
  &lt;main&gt;
    &lt;p&gt;If you're not a screen reader user yourself, you might be surprised to learn that the text to speech technology used by most blind people hasn't changed in the last 30 years. While text to speech has taken the sighted world by storm, in everything from personal assistants to GPS to telephone systems, the voices used by blind folks have remained mostly static. This is largely intentional. The needs of a blind text to speech user are vastly different than those of a sighted user. While sighted users prefer voices that are natural, conversational, and as human-like as possible, blind users tend to prefer voices that are fast, clear, predictable, and efficient. This results in a preference among blind users for voices that sound somewhat robotic, but can be understood at high rates of speed, often upwards of 800 to 900 words per minute. The speaking rate of an average person hovers around 200 to 250 words per minute, for comparison.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this difference in needs has resulted in blind people getting left out of the explosion of text to speech advancement, and has caused many problems. First, the voice that is preferred by the majority of western English blind users, called Eloquence, was last updated in 2003. While it is so overwhelmingly popular that even Apple was eventually pressured to add the voice to iPhone, mac, Apple TV, and Apple Watch, even they were forced to use an emulation layer. As Eloquence is a 32-bit voice last compiled in 2003, it cannot run in modern software without some sort of emulation or bridge. If the sourcecode to Eloquence still exists and can be compiled, even large companies like Apple haven't managed to find or compile it. As the NVDA screen reader moves from being a 32-bit application to a 64-bit one, keeping eloquence running with it has been a challenge that I and many other community members have spent a lot of time and effort solving. The eloquence libraries also have many known security issues, and anyone using the libraries today is forced to understand and program around them, as Eloquence itself can never be updated or fixed. These stopgap solutions are entirely untenable, and are likely to take us only so far. A better solution is urgently needed.&lt;/p&gt;
    &lt;p&gt;The second problem this has caused is for those who speak languages other than English. As most modern text to speech voices are created by and for sighted users, blind users begin to find that the voices available in less popular languages are inefficient, overly conversational, slow, and otherwise unsatisfactory. While espeak-ng is an open-source text to speech system that attempts to support hundreds of languages while meeting the needs of blind users, it brings a different set of problems to the table. First, many of the languages it supports were added based on pronunciation rules taken from Wikipedia articles, without involving speakers of the language. Second, Espeak-ng is based directly on Speak, a text to speech system written by Jonathan Duddington in 1995 for RISC OS on the BBC Micro, meaning that espeak users today continue to have to live with many of the design decisions made back in 1995 for an operating system that no longer exists. Third, looking at the Espeak-ng repository, it seems to only have one or two active maintainers. While this is obviously better than the zero active maintainers of Eloquence, it could still become a problem in the future.&lt;/p&gt;
    &lt;p&gt;These are the reasons that I'm always interested in advancements in text to speech, and am actively keeping my ears open for something that takes advantage of modern technology, while continuing to suit the needs of screen reader users like myself.&lt;/p&gt;
    &lt;p&gt;Over the holiday break, I decided to take a look at two modern AI-based text to speech systems, and see if they could be added to NVDA. I chose two models, because they advertised themselves as fast, able to run without a GPU, and responsive. The first was supertonic, and the second was Kitten TTS. As both models require 64-bit Python, I wrote the addons for the 64-bit alpha of NVDA. However, other than making development easier, this had little effect on the results.&lt;/p&gt;
    &lt;p&gt;Unfortunately, doing this work uncovered a number of issues that I believe are common to all of the modern AI-based text to speech systems, and make them unsuitable for use in screen readers. The first issue is dependency bloat. In order to bundle these systems as NVDA addons, developers are required to include a vast multitude of large and complex Python packages. In the case of Kitten TTS, the number is around 103, and just over 30 for supertonic. As the standard building and packaging methods for NVDA addons do not support specifying and building requirements, these dependencies need to be manually copied over, included in any github repositories, and cannot be automatically updated. Loading all of these dependencies directly into NVDA also causes the screen reader to load slower, use more system resources, and opens NVDA users up to any security issue in any of these libraries. As a screen reader needs access to the entire system, this is far from ideal.&lt;/p&gt;
    &lt;p&gt;The second issue is accuracy. These modern systems are developed to sound human, natural, and conversational. Unfortunately this seems to come at the expense of accuracy. In my testing, both models had a tendency to skip words, read numbers incorrectly, chop off short utterances, and ignore prosody hints from text punctuation. Kitten TTS is slightly better here, as it uses a deterministic phonemizer (the same one used by espeak, actually) to determine the correct way to pronounce words, leaving only the generation of the speech itself up to AI. But never the less, Kitten TTS is still far from perfectly accurate. When it comes to use in a screen reader, skipping words, or reading numbers incorrectly, is unacceptable.&lt;/p&gt;
    &lt;p&gt;The third issue is speed. Supertonic has the edge, here, but even it is far too slow. Unlike older text to speech systems, Supertonic and Kitten TTS cannot begin generating speech until they have an entire chunk of text. Supertonic is slightly faster, as it can stream result audio as it becomes available, whereas Kitten TTS cannot start speaking until all of the audio for the chunk is fully generated. But for use in a screen reader, a text to speech system needs to begin generating speech as quickly as possible, rather than waiting for an entire phrase or sentence. Users of screen readers quickly jump through text and frequently interrupt the screen reader, and thus require the text to speech system to be able to quickly discard and restart speech.&lt;/p&gt;
    &lt;p&gt;The fourth and final issue is control. Older text to speech systems make changing the pitch, speed, volume, breathiness, roughness, headsize, and other parameters of the voice easy. This allows screen reader users to customize the voice to our exact needs, as well as offering the ability to change the characteristics of the voice in real time based on the formatting or other attributes of the text. AI text to speech models, being trained on data from a particular set of speakers, cannot offer this customization. Instead, they inherit the speaking speed, pitch, volume, and other characteristics that were present in the training data. Kitten TTS and Supertonic both offer basic speed control, however it is highly variable from voice to voice and utterance to utterance. This leads to a loss of functionality that many blind users depend on.&lt;/p&gt;
    &lt;p&gt;If you'd like to experience these issues for yourself, feel free to follow the links above to my GitHub repositories. They offer ready to install addons that can be installed and used with the 64-bit NVDA alphas.&lt;/p&gt;
    &lt;p&gt;I'm picking on Kitten TTS and Supertonic not because they're particularly bad for the above problems, but because they're the models that are the state of the art in AI text to speech right now when it comes to speed and size. Other models, like Kokoro, exhibit all of the same issues, but more so.&lt;/p&gt;
    &lt;p&gt;So what's the way forward for blind screen reader users? Sadly, I don't know. Modern text to speech research has little to no overlap with our requirements. Using Eloquence, the system that many blind people find best, is becoming increasingly untenable. ESpeak uses an odd architecture originally designed for computers in 1995, and has few maintainers. Blastbay Studios has done some interesting work to create a text to speech voice using modern design and technology, that meets the requirements of blind users. But it's a closed-source product with a single maintainer, that also suffers from a lack of pronunciation accuracy. In an ideal world, someone would re-implement Eloquence as a set of open source libraries. However, doing so would require expertise in linguistics, digital signal processing, and audiology, as well as excellent programming abilities. My suspicion is that modernizing the text to speech stack that is preferred by blind power-users is an effort that would require several million dollars of funding at minimum. Instead, we'll probably wind up having to settle for text to speech voices that are "good enough", while being nowhere near as fast and efficient as what we have currently. Personally, I intend to keep Eloquence limping along for as long as I can, until the layers of required emulation and bridges make real time use impossible. Perhaps at that point AI will be good enough that it can be prompted to create a text to speech system that's up to our standards. Or, more hopefully, articles like this one may bring attention to the issues, and bring our community together to recognize the problems and find solutions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stuff.interfree.ca/2026/01/05/ai-tts-for-screenreaders.html"/><published>2026-01-23T09:24:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46730504</id><title>AI Usage Policy</title><updated>2026-01-23T14:18:03.031143+00:00</updated><content>&lt;doc fingerprint="eb7ad05e09566404"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md"/><published>2026-01-23T09:50:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46730671</id><title>Why I don't have fun with Claude Code</title><updated>2026-01-23T14:18:02.743571+00:00</updated><content>&lt;doc fingerprint="2b585153e86f0a9d"&gt;
  &lt;main&gt;
    &lt;p&gt;LLM-based coding agents like Claude Code &amp;amp; Codex are all the rage right now. Rightfully so, because these tools are actually getting good. They’re actually at the point where people, both programmers and less technical users, can use them to create features or even entire projects with decent results. I have a lot of feelings that I can’t cover in one blog post, but one thing feels like it’s becoming clear to me: I’ll likely never love a tool like Claude Code, even if I do use it, because I value the task it automates.&lt;/p&gt;
    &lt;p&gt;Whenever I use Claude Code, I notice that I stop having fun making software. That’s interesting, because many people report the opposite: that coding agents make computers fun again! I believe that this is due to a difference in values.&lt;/p&gt;
    &lt;p&gt;Like other technologies, AI coding tools help us automate tasks: specifically, the ones we don’t value. I use my dishwasher because I don’t value the process of hand-washing dishes. I only value the end result: clean dishes. Fabric is created with mechanical looms &amp;amp; knitting machines, because the economic value is the resulting fabric, not the process of creating it. Yet I still crochet &amp;amp; knit some items by hand, because I do enjoy the process.&lt;/p&gt;
    &lt;p&gt;People who love using AI to create software are loving it because they don’t value the act of creating &amp;amp; understanding the software. At least, they value it far less than the end result. I think that’s quite a normal view of software &amp;amp; computing: it is a means to an end. But to me, creating &amp;amp; understanding software is a worthwhile pursuit on its own! I enjoy the process of representing a problem in code, and I enjoy learning &amp;amp; building a mental model of systems so that I can better understand and debug them. The resulting software product may have value, but it’s not the only value, or even the primary value to me. Put simply, I’m not a “product-focused” developer.&lt;/p&gt;
    &lt;p&gt;Now this isn’t to say that I value writing all code. Plenty of code is boring! I won’t benefit much from writing hundreds of lines of boilerplate code. And it’s not to say that I don’t value the product, or care deeply about designing software that solves problems! But the reason I got into software, and the reason I continue to do it, is that I just really like computers and want to learn more about them. The current zeitgeist of AI coding is to use the AI to do as much as possible as quickly as possible, and for me that just throws out the baby with the bathwater.&lt;/p&gt;
    &lt;p&gt;For the product-focused people out there, I’m sure a tool like Claude Code is a godsend. Finally, you can tell a system what you want and get a result, give feedback, and iterate. You can still make some technical decisions, but in a sense you become a “manager” of AI systems. You speak in terms of results and requirements, and you let your underlings handle the details. If the only value being created is the end result, then that totally makes sense. But for me, it’s just not the case.&lt;/p&gt;
    &lt;p&gt;As a result of this, I’m starting to be more conscious about my goals when it comes to software. When I crochet or knit something, my goal is not just to get a blanket, stuffed animal, scarf, etc. I could quickly buy those things cheaply. The goal is to create something with my hands and my time. The value of the object, for myself or especially as a gift, is in the time and care I put in while making it. I’m starting to apply that mentality to my goals about software.&lt;/p&gt;
    &lt;p&gt;I used to say that I wanted to “make X” for some value of X. But usually I want to build that X so that I can learn something. Maybe I want to understand the problem better. Or maybe I want to start a project in a new programming language so I can learn it. I’m finding it more helpful to acknowledge those things as part of the goal, and discern the extent to which I care about the process versus the result.&lt;/p&gt;
    &lt;p&gt;This is helpful because when I’m honest about my goal, I can choose the right process to actually get what I want. There are definitely tasks where I just value the result, not the process. I can learn &amp;amp; use these AI tools in those areas, leaving myself more time for the parts of the process I do enjoy. I’ve always possessed the laziness said to be a virtue of programmers. It’s an easy sell if I can automate something that I genuinely don’t want to be doing! But if I actually want to learn something, I have to do it the hard way.&lt;/p&gt;
    &lt;p&gt;That’s great for me, but what about my career? While I may value the process of coding and the learning I get from it, the real world tends to be cold and uncaring. The economy may not value my learning (at least in the short-term), and employers are turning more and more to AI to “increase productivity.” It would be foolish for me to believe that AI won’t change our industry. That said, I think there is a lot of nuance to whether professional software engineers are truly in danger, and I think that this question of values &amp;amp; goals directly ties into that nuance. The value of software development, even at big companies, is not the output alone.&lt;/p&gt;
    &lt;p&gt;My job is not actually to write code. I am employed to fix customer bugs in Linux. I read a lot of code. I write code to help diagnose, and ultimately fix those bugs. I write code to reproduce and get more familiar with some bugs. I work on debuggers and related tooling, in order to create more powerful tools to help me in debugging. I come up with ways to debug issues while respecting the (frequently onerous) constraints of customers.&lt;/p&gt;
    &lt;p&gt;So when I write code, the trick isn’t usually knowing how to write the code, but knowing what the feature should be.1 Even when the coding task itself is straightforward, I still find I get value out of it. I’m always building my knowledge and experience so I can do the job better. While there may be plenty of software where it doesn’t matter that an LLM wrote it, I do think that there will continue to be huge swaths of software where it does matter for a human to write it. There is so much value (knowledge &amp;amp; expertise) generated by that process, which is incredibly important for systems that need reliability and debuggability. Ultimately, I don’t want my computer’s OS to be vibe-coded, nor my bank’s systems, nor my car software.&lt;/p&gt;
    &lt;p&gt;I know better than to prognosticate, and I definitely don’t think anybody should trust my opinion on whether my own job will be eliminated. But I’d guess that it depends on the extent my goals &amp;amp; values align with the work that I do. I’ve been lucky enough to find a job that aligns with my values: it’s about technical expertise; understanding &amp;amp; debugging systems. So long as that alignment holds, I’m cautiously optimistic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://brennan.io/2026/01/23/claude-code/"/><published>2026-01-23T10:12:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46730885</id><title>Booting from a vinyl record (2020)</title><updated>2026-01-23T14:18:01.194681+00:00</updated><content>&lt;doc fingerprint="5c71fe289328686"&gt;
  &lt;main&gt;
    &lt;p&gt;Most PCs tend to boot from a primary media storage, be it a hard disk drive, or a solid-state drive, perhaps from a network, or – if all else fails – the USB stick or the boot DVD comes to the rescue… Fun, eh? Boring! Why don’t we try to boot from a record player for a change?&lt;/p&gt;
    &lt;p&gt;64 512 byte DOS boot disk on a 10″ record, total playing time 06:10 on 45 rpm&lt;/p&gt;
    &lt;p&gt;Update February 2022: Click here to observe the very same vinyl ramdisk booted on an IBM PCjr!&lt;lb/&gt; So this nutty little experiment connects a PC, or an IBM PC to be exact, directly onto a record player through an amplifier. I made a small ROM on-chip boot loader that operates the built-in “cassette interface” of the PC (that was hardly ever used), which will now be invoked by the BIOS if all the other boot options fail, i.e. floppy disk and the hard drive. The turntable spins an analog recording of a small bootable read-only RAM drive, which is 64K in size. This contains a FreeDOS kernel, modified by me to cram it into the memory constraint, a micro variant of COMMAND.COM and a patched version of INTERLNK, that allows file transfer through a printer cable, modified to be runnable on FreeDOS. The bootloader reads the disk image from the audio recording through the cassette modem, loads it to memory and boots the system on it. Simple huh?&lt;/p&gt;
    &lt;p&gt;The vinyl loader code, in a ROM&lt;lb/&gt; (It can also reside on a hard drive or a floppy, but that’d be cheating)&lt;/p&gt;
    &lt;p&gt;And now to get more technical: this is basically a merge between BootLPT/86 and 5150CAXX, minus the printer port support. It also resides in a ROM, in the BIOS expansion socket, but it does not have to. The connecting cable between the PC and the record player amplifier is the same as with 5150CAXX, just without the line-in (PC data out) jack.&lt;lb/&gt; The “cassette interface” itself is just PC speaker timer channel 2 for the output, and 8255A-5 PPI port C channel 4 (PC4, I/O port 62h bit 4) for the input. BIOS INT 15h routines are used for software (de)modulation.&lt;lb/&gt; The boot image is the same 64K BOOTDISK.IMG “example” RAM drive that can be downloaded at the bottom of the BootLPT article. This has been turned into an “IBM cassette tape”-protocol compliant audio signal using 5150CAXX, and sent straight to a record cutting lathe.&lt;lb/&gt; Vinyls are cut with an RIAA equalization curve that a preamp usually reverses during playback, but not perfectly. So some signal correction had to be applied from the amplifier, as I couldn’t make it work right with the line output straight from the phono preamp. In my case, involving a vintage Harman&amp;amp;Kardon 6300 amplifier with an integrated MM phono preamp, I had to fade the treble all the way down to -10dB/10kHz, increase bass equalization to approx. +6dB/50Hz and reduce the volume level to approximately 0.7 volts peak, so it doesn’t distort. All this, naturally, with any phase and loudness correction turned off.&lt;lb/&gt; Of course, the cassette modem does not give a hoot in hell about where the signal is coming from. Notwithstanding, the recording needs to be pristine and contain no pops or loud crackles (vinyl) or modulation/frequency drop-outs (tape) that will break the data stream from continuing. However, some wow is tolerated, and the speed can be 2 or 3 percent higher or lower too.&lt;/p&gt;
    &lt;p&gt;Bootloader in a ROM; being an EPROM for a good measure&lt;/p&gt;
    &lt;p&gt;And that’s it! For those interested, the bootloader binary designed for a 2364 chip (2764s can be used, through an adaptor), can be obtained here. It assumes an IBM 5150 with a monochrome screen and at least 512K of RAM, which kind of reminds me of my setup (what a coincidence). The boot disk image can be obtained at the bottom of the BootLPT/86 article, and here’s its analog variant, straight from the grooves 🙂&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://boginjr.com/it/sw/dev/vinyl-boot/"/><published>2026-01-23T10:39:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46731432</id><title>Show HN: Whosthere: A LAN discovery tool with a modern TUI, written in Go</title><updated>2026-01-23T14:18:00.690969+00:00</updated><content>&lt;doc fingerprint="b709fb1d9ea3f390"&gt;
  &lt;main&gt;
    &lt;p&gt;Local Area Network discovery tool with a modern Terminal User Interface (TUI) written in Go. Discover, explore, and understand your LAN in an intuitive way.&lt;/p&gt;
    &lt;p&gt;Whosthere performs unprivileged, concurrent scans using mDNS and SSDP scanners. Additionally, it sweeps the local subnet by attempting TCP/UDP connections to trigger ARP resolution, then reads the ARP cache to identify devices on your Local Area Network. This technique populates the ARP cache without requiring elevated privileges. All discovered devices are enhanced with OUI lookups to display manufacturers when available.&lt;/p&gt;
    &lt;p&gt;Whosthere provides a friendly, intuitive way to answer the question every network administrator asks: "Who's there on my network?"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modern TUI: Navigate and explore discovered devices intuitively.&lt;/item&gt;
      &lt;item&gt;Fast &amp;amp; Concurrent: Leverages multiple discovery methods simultaneously.&lt;/item&gt;
      &lt;item&gt;No Elevated Privileges Required: Runs entirely in user-space.&lt;/item&gt;
      &lt;item&gt;Device Enrichment: Uses OUI lookup to show device manufacturers.&lt;/item&gt;
      &lt;item&gt;Integrated Port Scanner: Optional service discovery on found hosts (only scan devices with permission!).&lt;/item&gt;
      &lt;item&gt;Daemon Mode with HTTP API: Run in the background and integrate with other tools.&lt;/item&gt;
      &lt;item&gt;Theming &amp;amp; Configuration: Personalize the look and behavior via YAML configuration.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;brew tap ramonvermeulen/whosthere
brew install whosthere&lt;/code&gt;
    &lt;p&gt;Or with Go:&lt;/p&gt;
    &lt;code&gt;go install github.com/ramonvermeulen/whosthere@latest&lt;/code&gt;
    &lt;p&gt;Or build from source:&lt;/p&gt;
    &lt;code&gt;git clone github.com/ramonvermeulen/whosthere.git
cd whosthere
make build&lt;/code&gt;
    &lt;p&gt;Run the TUI for interactive discovery:&lt;/p&gt;
    &lt;code&gt;whosthere&lt;/code&gt;
    &lt;p&gt;Run as a daemon with HTTP API:&lt;/p&gt;
    &lt;code&gt;whosthere daemon --port 8080&lt;/code&gt;
    &lt;p&gt;Additional command line options can be found by running:&lt;/p&gt;
    &lt;code&gt;whosthere --help&lt;/code&gt;
    &lt;p&gt;Whosthere is supported on the following platforms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux&lt;/item&gt;
      &lt;item&gt;macOS&lt;/item&gt;
      &lt;item&gt;Windows (maybe in the future, contributions welcome!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start regex search&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;k&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Up&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;j&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Down&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;g&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Go to top&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;G&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Go to bottom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;y&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Copy IP of selected device&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;enter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show device details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;CTRL+t&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Toggle theme selector&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;CTRL+c&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Stop application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ESC&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Clear search / Go back&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;p&lt;/code&gt; (details view)&lt;/cell&gt;
        &lt;cell&gt;Start port scan on device&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;tab&lt;/code&gt; (modal view)&lt;/cell&gt;
        &lt;cell&gt;Switch button selection&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;WHOSTHERE_CONFIG&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to the configuration file, to be able to overwrite the default location.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;WHOSTHERE_LOG&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set the log level (e.g., &lt;code&gt;debug&lt;/code&gt;, &lt;code&gt;info&lt;/code&gt;, &lt;code&gt;warn&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt;). Defaults to &lt;code&gt;info&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Whosthere can be configured via a YAML configuration file. By default, it looks for the configuration file in the following order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Path specified in the &lt;code&gt;WHOSTHERE_CONFIG&lt;/code&gt;environment variable (if set)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;$XDG_CONFIG_HOME/whosthere/whosthere.yaml&lt;/code&gt;(if&lt;code&gt;XDG_CONFIG_HOME&lt;/code&gt;is set)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.config/whosthere/whosthere.yaml&lt;/code&gt;(otherwise)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When not running in TUI mode, logs are also written to the console output.&lt;/p&gt;
    &lt;p&gt;Example of the default configuration file:&lt;/p&gt;
    &lt;code&gt;# How often to run discovery scans
scan_interval: 20s

# Maximum duration for each scan
# If you set this too low some scanners or the sweeper might not complete in time
scan_duration: 10s

# Splash screen configuration
splash:
  enabled: true
  delay: 1s

# Theme configuration
theme:
  # Configure the theme to use for the TUI, complete list of available themes at:
  # https://github.com/ramonvermeulen/whosthere/tree/main/internal/ui/theme/theme.go
  # Set name to "custom" to use the custom colors below
  # For any color that is not configured it will take the default theme value as fallback
  name: default

  # Custom theme colors (uncomment and set name: custom to use)
  # primitive_background_color: "#000a1a"
  # contrast_background_color: "#001a33"
  # more_contrast_background_color: "#003366"
  # border_color: "#0088ff"
  # title_color: "#00ffff"
  # graphics_color: "#00ffaa"
  # primary_text_color: "#cceeff"
  # secondary_text_color: "#6699ff"
  # tertiary_text_color: "#ffaa00"
  # inverse_text_color: "#000a1a"
  # contrast_secondary_text_color: "#88ddff"

# Scanner configuration
scanners:
  mdns:
    enabled: true
  ssdp:
    enabled: true
  arp:
    enabled: true

# Port scanner configuration
port_scanner:
  timeout: 5s
  # List of TCP ports to scan on discovered devices
  tcp: [21, 22, 23, 25, 80, 110, 135, 139, 143, 389, 443, 445, 993, 995, 1433, 1521, 3306, 3389, 5432, 5900, 8080, 8443, 9000, 9090, 9200, 9300, 10000, 27017]

# Uncomment the next line to configure a specific network interface - uses OS default if not set
# network_interface: lo0&lt;/code&gt;
    &lt;p&gt;When running Whosthere in daemon mode, it exposes an very simplistic HTTP API with the following endpoints:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/devices&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get list of all discovered devices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/device/{ip}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get details of a specific device&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/health&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Health check&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Theme can be configured via the configuration file, or at runtime via the &lt;code&gt;CTRL+t&lt;/code&gt; key binding.
A complete list of available themes can be found here, feel free to open a PR to add your own theme!&lt;/p&gt;
    &lt;p&gt;Example of theme configuration:&lt;/p&gt;
    &lt;code&gt;theme:
  name: cyberpunk&lt;/code&gt;
    &lt;p&gt;When the &lt;code&gt;name&lt;/code&gt; is set to &lt;code&gt;custom&lt;/code&gt;, the other color options can be used to create your own custom theme.&lt;/p&gt;
    &lt;p&gt;Logs are written to the application's state directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;$XDG_STATE_HOME/whosthere/app.log&lt;/code&gt;(if&lt;code&gt;XDG_STATE_HOME&lt;/code&gt;is set)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.local/state/whosthere/app.log&lt;/code&gt;(otherwise)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When not running in TUI mode, logs are also output to the console.&lt;/p&gt;
    &lt;p&gt;For clipboard functionality to work:&lt;/p&gt;
    &lt;p&gt;Runtime requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux (X11): X11 client library (e.g., &lt;code&gt;libx11-6&lt;/code&gt;on Ubuntu,&lt;code&gt;libX11&lt;/code&gt;on Fedora/Arch, often pre-installed).&lt;/item&gt;
      &lt;item&gt;Linux (Wayland): Not natively supported. May require XWayland.&lt;/item&gt;
      &lt;item&gt;macOS/Windows: No dependencies.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build requirements (when compiling from source):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux: X11 development package (&lt;code&gt;libx11-dev&lt;/code&gt;,&lt;code&gt;libX11-devel&lt;/code&gt;, or&lt;code&gt;libx11&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whosthere is intended for use on networks where you have permission to perform network discovery and scanning, such as your own home network. Unauthorized scanning of networks may be illegal and unethical. Always obtain proper authorization before using this tool on any network.&lt;/p&gt;
    &lt;p&gt;Contributions and suggestions such as feature requests, bug reports, or improvements are welcome! Feel free to open issues or submit pull requests on the GitHub repository. Please make sure to discuss any major changes on a Github issue before implementing them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ramonvermeulen/whosthere"/><published>2026-01-23T11:54:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46731612</id><title>Presence in Death</title><updated>2026-01-23T14:18:00.299613+00:00</updated><content>&lt;doc fingerprint="bf9c9a2f1fa79b9c"&gt;
  &lt;main&gt;
    &lt;p&gt;In what Tibetan Buddhists call tukdam (Wylie transliteration: thugs dam), experienced meditators die in meditative equipoise. Their bodies do not show the usual signs of death—such as smell, rigor mortis, or decomposition—for days or even weeks after their clinical deaths. They appear lifelike, and many even remain sitting upright in meditation posture. From the Tibetan Buddhist point of view, the meditators are resting in a subtle state of consciousness with an associated subtle material energy present in the body. They are still in the process of dying. Yet according to modern biomedical and legal definitions, they are dead. Many cases of tukdam have now been scientifically documented.&lt;/p&gt;
    &lt;p&gt;For my 2022 documentary film Tukdam: Between Worlds and research for my PhD in medical anthropology, I have been following the Tukdam Project, a groundbreaking scientific research initiative lead from the Center for Healthy Minds at the University of Wisconsin—Madison and headed by renowned neuroscientist Richard Davidson.* It has focused on documenting tukdam bodies and trying to understand why the decomposition process seems to be delayed. His Holiness the Dalai Lama initiated this multidisciplinary project, which has been carried out with Tibetan collaborators from Delek Hospital in Dharamshala and Men-Tsee-Khang (Tibetan Medical and Astro-Science Institute) traditional Tibetan medicine doctors in India. In recent years, Russian and Indian scientific collaborators have also joined the effort to understand tukdam scientifically.&lt;/p&gt;
    &lt;p&gt;In following the project, I have been struck by the differing expectations and even cross-purposes that the Tibetan and scientific parties seem to harbor. Tibetans hope the research may reveal something about a subtle nature of consciousness that continues beyond clinical and brain death, and which is held to be responsible for keeping the bodies fresh. The Dalai Lama also seems to be invested in this research because of its potential to reveal something about the nature of consciousness that transcends the brain-body complex and even this life.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;IN TUKDAM, CLINICALLY DEAD MEDITATORS ARE SAID TO DWELL IN THE LUMINOSITY OF EMPTINESS.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While the non-decaying tukdam body signifies the presence of consciousness for Tibetans, this is not obvious from a biophysical scientific perspective. Indeed, the Center for Healthy Minds has been looking for possible residual activity in the brainstem—a primitive part of the brain not thought to be involved in consciousness—as a factor contributing to the unusual integration of the bodies. In 2021, the research team published a null-finding stating they had not found any activity in the brain so far. Compelled to operate within a biophysical paradigm, scientists are also interested in possible changes to cell metabolism and breakdown, brought about by years of meditation practice, as perhaps contributing to the pristine postmortem state of tukdam bodies. But taking samples from the bodies has so far been out of the question due to cultural sensitivity and the sacredness of these deaths. Tibetans are concerned that invasive procedures could disturb the postmortem meditative state and the potential it carries for spiritual liberation and achieving a good rebirth.&lt;/p&gt;
    &lt;p&gt;An exchange from Tukdam: Between Worlds illustrates some of the tensions and cross-purposes with which the scientists and Tibetan parties have been operating, although there have been developments in the research and collaboration since the time of shooting in 2019 to early 2020. The scene shows a meeting where Dr. Dylan Lott, who was then the Tukdam Project manager in India, presents the current state of the research and its findings to Tibetan project collaborators. The Dalai Lama’s personal physician, Dr. Tsetan Dorji Sadutshang, expresses frustration over the lack of results from years of research and what he sees as a misguided approach to explain tukdam in neuroscientific terms. According to the Tibetan view, something far more subtle than the “gross” mind related to the brain and senses is responsible for the physical signs of tukdam.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Dr. Tsetan:&lt;/p&gt;
      &lt;p&gt;To me, from my understanding of His Holiness’ hope from this project, really is to have some proof that there is some sort of consciousness . . . or a mind continuing, that goes on beyond this life, basically. The only first kindergarten step is really to say: Is there a difference between a gross mind and a subtle mind?&lt;/p&gt;
      &lt;p&gt;Dr. Lott:&lt;/p&gt;
      &lt;p&gt;We cannot prove rebirth. We cannot prove mind. We cannot prove subtle mind. What we can do is look at the effects of those practices on the body that are unusual and that Western science, or medical science, doesn’t have a good explanation for.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As is the case with science, it is not obvious to all Buddhist traditions that a non-decomposing body proves the presence of consciousness. The medieval Chinese Chan tradition, for example, also records miraculously preserved meditators’ bodies. These did not, however, signify present consciousness, which according to widespread Buddhist doctrine, departs immediately at death to be reborn or to enter nirvana. Here the body is preserved due to purification by religious practice and virtue accumulated while alive.&lt;/p&gt;
    &lt;p&gt;There is something paradoxical in taking the non-decaying body as evidence for a consciousness that transcends the body and physicality. However, here we should be careful to note that in the Tibetan Buddhist tantric tradition different levels of mind are associated with different types of embodiment. Subtler forms of consciousness are associated with tantric subtle bodies or physiologies familiar to advanced tantric practitioners. As these are in a way two sides of the same coin, mind always affects body and vice versa. Such subtle bodies are arguably different from, though connected to, the “gross” biomedical body that scientists work with, which also shows effects of subtle levels of mind and embodiment.&lt;/p&gt;
    &lt;p&gt;Tibetan tradition exhibits a great deal of sophistication and specificity when it comes to signs or ways of ascertaining whether a person is in tukdam—as well as when it ends. The body will slump over if it was sitting upright; smell and normal signs of decomposition will appear. In accordance with subtle tantric physiology, red and white liquids may come out the nostrils and genitals. These are all signs that even the most subtle consciousness has departed. For Tibetans, final death occurs when the mind leaves the body, which could be weeks after clinical death in cases of tukdam,or hours to days for “normal” deaths.&lt;/p&gt;
    &lt;p&gt;There is also a tantalizing tradition of ending tukdam that could be seen as indicative of consciousness. If tukdam goes on too long, it may be ended by ringing a bell near the ear of the deceased practitioner, saying certain prayers, or asking them to end their meditation. The body then reportedly collapses and decomposition takes over. This could imply responsivity on the part of the deceased meditators, bearing on questions of consciousness. In an interview piece that did not make the final cut of my documentary, the Dalai Lama recounted this story:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;One lama I had a very close connection [with] . . . Last few years his physical [condition] was very, very weak. And then in my last meeting in Mongolia, I told him, “Now time has come. You have to think about your next life.” Then around the end of the year, with New Year soon, I received one message from him: “When I should die? Where I should die?” . . . Quite silly, “where I should die, when I should die!” Then accordingly I also answered a silly sort of answer, “You should die in Mongolia. The time, not end of the year but the beginning of the year, New Year.” He exactly, I think in the first week of the New Year, then he died. Then I sent my representative with my [ceremonial silk] scarf. I think it took two days . . . When my representative reached his place and put the scarf which I sent . . . on his neck, then he ended his tukdam. [Dalai Lama makes gesture of body slumping over.] So these are quite mysterious things. There are some elements to control. It is quite obvious as soon as the tukdam ends, the physical [body] clearly shows real death, the real corpse.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;People often report feeling a meditative calm or presence when entering the room of someone in tukdam. Some of the American scientists researching tukdam also said they felt it. But such things seem difficult to measure and to be firmly in the realm of first-person experience as opposed to the third-person observation of natural science. This does not, however, necessarily make such perceptions less real. As another Tukdam Project collaborator, senior Tibetan medicine doctor Tsewang Tamdin, told me, “Just because something is invisible does not mean it does not exist.”&lt;/p&gt;
    &lt;p&gt;As in life and death, the dynamic of presence and absence is central to tukdam. Here we come to a basic conflict between the Tibetan Buddhist and current biomedical views of death. For the latter, death unequivocally means absence. Once the heart shuts down, brain death quickly follows “unless it’s been inflicted before the heart stopped” and the person is gone. But for Tibetan Buddhists, there is presence, or mind, in death.&lt;/p&gt;
    &lt;p&gt;*Richard Davidson is a member of the Rubin Museum’s advisory board.&lt;/p&gt;
    &lt;p&gt;Donagh Coleman is a Finnish-Irish-American filmmaker. Previous award-winning documentaries with international festival and TV exposure include Stone Pastures and A Gesar Bard’s Tale. Donagh’s films have been shown by the European Commission and museums such as MoMA and the Rubin Museum. Donagh is currently doing a PhD in medical anthropology at University of California, Berkeley and holds degrees in philosophy and psychology and music and media technologies from Trinity College Dublin, as well as a master’s in Asian studies from University of California, Berkeley.&lt;/p&gt;
    &lt;p&gt;Get the latest news and stories from the Rubin, plus occasional information on how to support our work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rubinmuseum.org/presence-in-death/"/><published>2026-01-23T12:16:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46731748</id><title>What has Docker become?</title><updated>2026-01-23T14:18:00.133862+00:00</updated><content>&lt;doc fingerprint="3d97b07b1d651558"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What has Docker become?&lt;/head&gt;
    &lt;head rend="h5"&gt;Posted on January 20, 2026 • 4 minutes • 851 words&lt;/head&gt;
    &lt;p&gt;It’s weird to see Docker Inc (the company) struggle to find its place in 2026. What started as the company that revolutionized how we deploy applications has been through multiple identity crises, pivoting from one strategy to another in search of sustainable revenue and market relevance.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Identity Crisis&lt;/head&gt;
    &lt;p&gt;Docker’s journey reads like a startup trying to find product-market fit, except Docker already had product-market fit - they created the containerization standard that everyone uses. The problem is that Docker the technology became so successful that Docker the company struggled to monetize it. When your core product becomes commoditized and open source, you need to find new ways to add value.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Swarm Exit&lt;/head&gt;
    &lt;p&gt;Docker Swarm was Docker’s attempt to compete with Kubernetes in the orchestration space. But Kubernetes won that battle decisively, and Docker eventually sold Swarm. This was a clear signal that Docker was stepping back from trying to be the full-stack container platform and instead focusing on what they could uniquely provide.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Developer Tools Pivot&lt;/head&gt;
    &lt;p&gt;For a while, Docker seemed to focus on developer experience. This made sense - developers are Docker’s core users, and improving their workflow could be a differentiator. Docker Scout emerged from the acquisition of Atomist in June 2022, bringing “software supply chain” capabilities. Scout allows Docker to see not just what’s in a container, but how it was built and where vulnerabilities are. This was a smart move toward security and observability, areas where Docker could add real value.&lt;/p&gt;
    &lt;p&gt;Docker also acquired AtomicJar, the company behind Testcontainers, adding shift-left testing capabilities. Testcontainers lets developers run real dependencies (databases, message queues, etc.) in containers during testing, making integration tests more reliable and closer to production environments.&lt;/p&gt;
    &lt;head rend="h2"&gt;The AI Pivot&lt;/head&gt;
    &lt;p&gt;Then came the AI pivot. Docker Model Runner entered the scene, positioning Docker as a platform for running AI models. Docker Compose expanded to support AI agents and models. Docker Offload was introduced for cloud-scale GPU execution of AI tasks. Partnerships with Google Cloud, Microsoft Azure, and AI SDKs (CrewAI, LangGraph, Vercel AI SDK) followed.&lt;/p&gt;
    &lt;p&gt;The acquisition of MCP Defender in September 2025 further cemented Docker’s move into AI security, focusing on securing agentic AI infrastructure and runtime threat detection. This was a significant shift - from developer tools to AI infrastructure.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hardened Images Move&lt;/head&gt;
    &lt;p&gt;Suddenly, Docker moved into the hardened images space. In December 2025, Docker made over 1,000 Docker Hardened Images free and open source under Apache 2.0, reducing vulnerabilities by up to 95% compared to traditional images. This move was likely triggered by Chainguard’s success in the secure container image space. Chainguard had been building a business around minimal, secure container images, and Docker needed to respond.&lt;/p&gt;
    &lt;p&gt;Making hardened images free was a bold move - it’s hard to compete with free, especially when it’s open source. But it also raises questions about Docker’s business model. If you’re giving away your security features for free, what are you selling?&lt;/p&gt;
    &lt;head rend="h2"&gt;Leadership Changes and Acquisition Speculation&lt;/head&gt;
    &lt;p&gt;In February 2025, Docker replaced CEO Scott Johnston (who led the company since 2019) with Don Johnson, a former Oracle Cloud Infrastructure founder and executive vice president. This leadership transition has prompted tech analysts to anticipate a potential acquisition by a major cloud provider. The CEO swap, combined with the strategic pivots, suggests Docker may be positioning itself for sale rather than building a standalone business.&lt;/p&gt;
    &lt;head rend="h2"&gt;What This All Means&lt;/head&gt;
    &lt;p&gt;Docker’s strategic shifts tell a story of a company searching for its place in a market it helped create. The containerization technology Docker pioneered became so successful that it became infrastructure - something everyone uses but no one wants to pay for directly.&lt;/p&gt;
    &lt;p&gt;The pivots from orchestration (Swarm) to developer tools (Scout, Testcontainers) to AI (Model Runner, MCP Defender) to security (Hardened Images) show a company trying different approaches to find sustainable revenue. Each pivot makes sense in isolation, but together they paint a picture of a company without a clear long-term vision.&lt;/p&gt;
    &lt;p&gt;The hardened images move is particularly interesting because it’s defensive - responding to Chainguard’s success rather than leading with innovation. Making it free and open source is a strong competitive move, but it doesn’t solve the fundamental business model question.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future&lt;/head&gt;
    &lt;p&gt;Docker the technology isn’t going anywhere. It’s too embedded in the infrastructure of modern software development. But Docker the company? That’s less clear. The leadership change, acquisition speculation, and rapid strategic pivots suggest Docker Inc may be positioning itself for an exit rather than building a long-term independent business.&lt;/p&gt;
    &lt;p&gt;For developers, this doesn’t change much. Docker containers will continue to work, and the open source nature of Docker means the technology will persist regardless of what happens to the company. But it’s worth watching how Docker Inc’s search for identity plays out - it could affect the ecosystem of tools and services built around containers.&lt;/p&gt;
    &lt;p&gt;The irony is that Docker created a standard so successful that it became infrastructure, and infrastructure is hard to monetize. Docker Inc’s struggle to find its place is a cautionary tale about the challenges of building a business around open source technology that becomes too successful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tuananh.net/2026/01/20/what-has-docker-become/"/><published>2026-01-23T12:36:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46731996</id><title>Microsoft mishandling example.com</title><updated>2026-01-23T14:17:59.822988+00:00</updated><content>&lt;doc fingerprint="af11ed5a5a169e95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft mishandling example.com&lt;/head&gt;
    &lt;p&gt;TL;DR: Since at least February 2020, Microsoft's Autodiscover service has incorrectly routed the IANA-reserved &lt;code&gt;example.com&lt;/code&gt; to Sumitomo Electric Industries' mail servers at sei.co.jp, potentially sending test credentials there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Problem&lt;/head&gt;
    &lt;p&gt;While setting up &lt;code&gt;email@example.com&lt;/code&gt; as a dummy account in Outlook (on both Windows and macOS), Outlook consistently auto-configured it to use &lt;code&gt;imapgms.jnet.sei.co.jp&lt;/code&gt; (IMAP) and &lt;code&gt;smtpgms.jnet.sei.co.jp&lt;/code&gt; (SMTP) despite &lt;code&gt;example.com&lt;/code&gt; being an IANA-reserved domain that should not resolve to real services.&lt;/p&gt;
    &lt;p&gt;The same behavior appeared on different machines, profiles, networks, and DNS resolvers, including a newly provisioned Windows 365 Cloud PC:&lt;/p&gt;
    &lt;head rend="h2"&gt;Confirmation&lt;/head&gt;
    &lt;head rend="h3"&gt;DNS verification&lt;/head&gt;
    &lt;p&gt;Confirm that &lt;code&gt;example.com&lt;/code&gt; has no DNS records pointing to &lt;code&gt;sei.co.jp&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;%&lt;code&gt;dig MX example.com +short&lt;/code&gt;0 . %&lt;code&gt;dig CNAME autodiscover.example.com +short&lt;/code&gt;(no response) %&lt;code&gt;dig SRV _autodiscover._tcp.example.com +short&lt;/code&gt;(no response)&lt;/quote&gt;
    &lt;p&gt;The domain has a null MX record (indicating it doesn't accept email) and no Autodiscover DNS entries, confirming the misconfiguration exists entirely within Microsoft's database.&lt;/p&gt;
    &lt;head rend="h3"&gt;Microsoft autodiscover API response&lt;/head&gt;
    &lt;p&gt;Microsoft's Autodiscover service misconfiguration can be confirmed via &lt;code&gt;curl -v -u "email@example.com:password" "https://prod.autodetect.outlook.cloud.microsoft/autodetect/detect?app=outlookdesktopBasic"&lt;/code&gt;:&lt;/p&gt;
    &lt;head&gt;View full output&lt;/head&gt;
    &lt;quote&gt;* Host prod.autodetect.outlook.cloud.microsoft:443 was resolved. * IPv6: (none) * IPv4: 172.169.69.94 * Trying 172.169.69.94:443... * Connected to prod.autodetect.outlook.cloud.microsoft (172.169.69.94) port 443 * ALPN: curl offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * CAfile: /etc/ssl/cert.pem * CApath: none * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-AES256-GCM-SHA384 / [blank] / UNDEF * ALPN: server accepted h2 * Server certificate: * subject: C=US; ST=WA; L=Redmond; O=Microsoft Corporation; CN=autodetect.outlookmobile.com * start date: Nov 1 12:31:46 2025 GMT * expire date: Jan 30 12:31:46 2026 GMT * subjectAltName: host "prod.autodetect.outlook.cloud.microsoft" matched cert's "*.autodetect.outlook.cloud.microsoft" * issuer: C=US; O=Microsoft Corporation; CN=Microsoft Azure RSA TLS Issuing CA 03 * SSL certificate verify ok. * using HTTP/2 * Server auth using Basic with user 'email@example.com' * [HTTP/2] [1] OPENED stream for https://prod.autodetect.outlook.cloud.microsoft/autodetect/detect?app=outlookdesktopBasic * [HTTP/2] [1] [:method: GET] * [HTTP/2] [1] [:scheme: https] * [HTTP/2] [1] [:authority: prod.autodetect.outlook.cloud.microsoft] * [HTTP/2] [1] [:path: /autodetect/detect?app=outlookdesktopBasic] * [HTTP/2] [1] [authorization: Basic ZW1haWxAZXhhbXBsZS5jb206cGFzc3dvcmQ=] * [HTTP/2] [1] [user-agent: curl/8.7.1] * [HTTP/2] [1] [accept: */*] &amp;gt; GET /autodetect/detect?app=outlookdesktopBasic HTTP/2 &amp;gt; Host: prod.autodetect.outlook.cloud.microsoft &amp;gt; Authorization: Basic ZW1haWxAZXhhbXBsZS5jb206cGFzc3dvcmQ= &amp;gt; User-Agent: curl/8.7.1 &amp;gt; Accept: */* &amp;gt; * Request completely sent off &amp;lt; HTTP/2 200 &amp;lt; content-type: application/json; charset=utf-8 &amp;lt; date: Mon, 08 Dec 2025 21:32:58 GMT &amp;lt; server: Kestrel &amp;lt; strict-transport-security: max-age=2592000 &amp;lt; x-olm-source-endpoint: /detect &amp;lt; x-provider-id: seeatest &amp;lt; x-debug-support: eyJkZWNpc2lvbiI6ImF1dG9EdjIgPiBhdXRvRHYxID4gZml4ZWQgZGIgcHJvdmlkZXIgPiBmaXhlZCBkYiBkb21haW4gcHJvdG9jb2xzID4gZGIgcHJvdmlkZXIgPiBkYiBkb21haW4gcHJvdG9jb2xzIiwiYXV0b0QiOnsidjIiOm51bGwsInYxIjpudWxsfSwiZGIiOnsicHJvdmlkZXIiOnsiRG9tYWluSWQiOm51bGwsIklkIjoic2VlYXRlc3QiLCJTZXJ2aWNlIjpudWxsLCJQcm90b2NvbHMiOlt7InByb3RvY29sIjoic210cCIsIkRvbWFpbiI6bnVsbCwiSG9zdG5hbWUiOiJzbXRwZ21zLmpuZXQuc2VpLmNvLmpwIiwiUG9ydCI6NDY1LCJFbmNyeXB0aW9uIjoiU3NsIiwiSXNDcm93ZHNvdXJjZWQiOm51bGwsIkZlZWRiYWNrcyI6bnVsbCwiSW5zZWN1cmUiOm51bGwsIlNlY3VyZSI6IlRydWUiLCJVc2VybmFtZSI6IntlbWFpbH0iLCJWYWxpZGF0ZWQiOmZhbHNlLCJBdXRvZGlzY292ZXIiOm51bGwsIkFhZCI6bnVsbH0seyJwcm90b2NvbCI6ImltYXAiLCJEb21haW4iOm51bGwsIkhvc3RuYW1lIjoiaW1hcGdtcy5qbmV0LnNlaS5jby5qcCIsIlBvcnQiOjk5MywiRW5jcnlwdGlvbiI6IlNzbCIsIklzQ3Jvd2Rzb3VyY2VkIjpudWxsLCJGZWVkYmFja3MiOm51bGwsIkluc2VjdXJlIjpudWxsLCJTZWN1cmUiOiJUcnVlIiwiVXNlcm5hbWUiOiJ7ZW1haWx9IiwiVmFsaWRhdGVkIjpmYWxzZSwiQXV0b2Rpc2NvdmVyIjpudWxsLCJBYWQiOm51bGx9XSwiQ3JlYXRlZEF0IjoiMjAyMC0wMi0wM1QwNTozMToyMy4yOTgwMjQ4IiwiVXBkYXRlZEF0IjoiMjAyMC0wMi0wM1QwOToxMjo1OS4wMjQ1ODciLCJQcmVkaWNhdGVzIjpudWxsLCJBdXRvRHYyRW5kcG9pbnQiOm51bGwsIkNvbW1lbnQiOm51bGwsIkZlZWRiYWNrcyI6bnVsbCwiSXNDcm93ZHNvdXJjZWQiOmZhbHNlfSwiZG9tYWluIjp7ImZpeGVkIjpmYWxzZSwiYXV0b0R2MkVuZHBvaW50IjpudWxsLCJwcm92aWRlcklkIjoic2VlYXRlc3QiLCJwcm90b2NvbHMiOm51bGx9fX0= &amp;lt; x-autodv2-error: ENOTFOUND &amp;lt; x-feedback-token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJEIjoiZXhhbXBsZS5jb20iLCJQSSI6InNlZWF0ZXN0IiwiUyI6W10sIlAiOlsiaW1hcHM6Ly9pbWFwZ21zLmpuZXQuc2VpLmNvLmpwOjk5MyIsInNtdHBzOi8vc210cGdtcy5qbmV0LnNlaS5jby5qcDo0NjUiXSwiUFQiOiJpbWFwIHNtdHAiLCJleHAiOjE3NjUyMzMxNzgsImlhdCI6MTc2NTIyOTU3OH0.-ohD7c9hytRZK_b4EJ0M5Tke7hl8u1wjsMYRV71GZik &amp;lt; x-dns-prefetch-control: off &amp;lt; x-frame-options: SAMEORIGIN &amp;lt; x-download-options: noopen &amp;lt; x-content-type-options: nosniff &amp;lt; x-xss-protection: 1; mode=block &amp;lt; x-instance-id: autodetect-deployment-76fffc487d-wfs4b &amp;lt; x-response-time: 3472 ms &amp;lt; x-request-id: f1b6525f-6d11-4add-a0e4-0b677d89f9eb &amp;lt; x-autodetect-cv: f1b6525f-6d11-4add-a0e4-0b677d89f9eb &amp;lt; * Connection #0 to host prod.autodetect.outlook.cloud.microsoft left intact {"email":"email@example.com","services":[],"protocols":[{"protocol":"imap","hostname":"imapgms.jnet.sei.co.jp","port":993,"encryption":"ssl","username":"email@example.com","validated":false},{"protocol":"smtp","hostname":"smtpgms.jnet.sei.co.jp","port":465,"encryption":"ssl","username":"email@example.com","validated":false}]}%&lt;/quote&gt;
    &lt;p&gt;The JSON response:&lt;/p&gt;
    &lt;quote&gt;{ "email": "email@example.com", "services": [], "protocols": [ { "protocol": "imap", "hostname": "imapgms.jnet.sei.co.jp", "port": 993, "encryption": "ssl", "username": "email@example.com", "validated": false }, { "protocol": "smtp", "hostname": "smtpgms.jnet.sei.co.jp", "port": 465, "encryption": "ssl", "username": "email@example.com", "validated": false } ] }&lt;/quote&gt;
    &lt;head rend="h3"&gt;Decoded debug header&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;x-debug-support&lt;/code&gt; header (Base64-decoded) reveals additional details:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Provider ID&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;seeatest&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Created&lt;/cell&gt;
        &lt;cell&gt;2020-02-03 05:31:23 UTC&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Updated&lt;/cell&gt;
        &lt;cell&gt;2020-02-03 09:12:59 UTC&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;IsCrowdsourced&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This misconfiguration has existed for nearly six years and was not crowdsourced. It appears to have been manually added to Microsoft's database.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related&lt;/head&gt;
    &lt;p&gt;❧ 2026-01-01&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tinyapps.org/blog/microsoft-mishandling-example-com.html"/><published>2026-01-23T13:04:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46732213</id><title>Radicle: The Sovereign Forge</title><updated>2026-01-23T14:17:59.656995+00:00</updated><content>&lt;doc fingerprint="a0671596b436ca2c"&gt;
  &lt;main&gt;
    &lt;p&gt;Radicle is a sovereign {code forge} built on Git.&lt;/p&gt;
    &lt;head rend="h1"&gt;Synopsis&lt;/head&gt;
    &lt;p&gt;Radicle is an open source, peer-to-peer code collaboration stack built on Git. Unlike centralized code hosting platforms, there is no single entity controlling the network. Repositories are replicated across peers in a decentralized manner, and users are in full control of their data and workflow.&lt;/p&gt;
    &lt;p&gt; The Radicle &lt;code&gt;heartwood&lt;/code&gt; repository. Repository ID
  &lt;code&gt;rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5&lt;/code&gt;.
&lt;/p&gt;
    &lt;head rend="h1"&gt;Get started&lt;/head&gt;
    &lt;quote&gt;ð¾ ·&lt;/quote&gt;
    &lt;p&gt;To install Radicle, simply run the command below from your shell, or go to the download page.&lt;/p&gt;
    &lt;code&gt;curl -sSLf https://radicle.xyz/install | sh&lt;/code&gt;
    &lt;p&gt;Alternatively, you can build from source.&lt;/p&gt;
    &lt;p&gt;For now, Radicle only works on Linux, macOS and BSD variants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Radicle Desktop ð¥ï¸&lt;/head&gt;
    &lt;p&gt;For a graphical collaborative experience check out the Radicle Desktop client, as well.&lt;/p&gt;
    &lt;head rend="h1"&gt;How it works&lt;/head&gt;
    &lt;p&gt;The Radicle protocol leverages cryptographic identities for code and social artifacts, utilizes Git for efficient data transfer between peers, and employs a custom gossip protocol for exchanging repository metadata.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your Data, Forever and Secure&lt;/head&gt;
    &lt;p&gt;All social artifacts are stored in Git, and signed using public-key cryptography. Radicle verifies the authenticity and authorship of all data for you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unparalleled Autonomy&lt;/head&gt;
    &lt;p&gt;Radicle enables users to run their own nodes, ensuring censorship-resistant code collaboration and fostering a resilient network without reliance on third-parties.&lt;/p&gt;
    &lt;head rend="h2"&gt;Local-first&lt;/head&gt;
    &lt;p&gt;Radicle is local-first, providing always-available functionality even without internet access. Users own their data, making migration, backup, and access easy both online and offline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evolvable &amp;amp; Extensible&lt;/head&gt;
    &lt;p&gt;Radicleâs Collaborative Objects (COBs) provide Radicleâs social primitive. This enables features such as issues, discussions and code review to be implemented as Git objects. Developers can extend Radicleâs capabilities to build any kind of collaboration flow they see fit.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modular by Design&lt;/head&gt;
    &lt;p&gt;The Radicle Stack comes with a CLI, web interface and TUI, that are backed by the Radicle Node and HTTP Daemon. Itâs modular, so any part can be swapped out and other clients can be developed.&lt;/p&gt;
    &lt;quote&gt;âââââââââââââââââââââââââââââââââââââ â Radicle CLI ââ Radicle Web â âââââââââââââââââââââââââââââââââââââ âââââââââââââââââââââââââââââââââââââ â Radicle Repository â â ââââââââââ ââââââââââ âââââââââââ â â â code â â issues â â patches â â â ââââââââââ ââââââââââ âââââââââââ â âââââââââââââââââââââââââââââââââââââ¤ â Radicle Storage (Git) â âââââââââââââââââââââââââââââââââââââ âââââââââââââââââââââââââââââââââââââ â Radicle Node ââ Radicle HTTPD â ââââââââââââââââââ¤âââââââââââââââââââ¤ â NoiseXK ââ HTTP + JSON â âââââââââââââââââââââââââââââââââââââ&lt;/quote&gt;
    &lt;head rend="h1"&gt;Contributing&lt;/head&gt;
    &lt;p&gt;Radicle is free and open source software under the MIT and Apache 2.0 licenses. Get involved by contributing code.&lt;/p&gt;
    &lt;head rend="h1"&gt;Updates&lt;/head&gt;
    &lt;p&gt;Follow us on ð Mastodon, ð¦ Bluesky or ð¦ Twitter to stay updated, join our community on ð¬ Zulip, or Subscribe&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;14.01.2026 Radicle 1.6.0 released. â¨&lt;/item&gt;
      &lt;item&gt;30.09.2025 Radicle 1.5.0 released.&lt;/item&gt;
      &lt;item&gt;04.09.2025 Radicle 1.4.0 released.&lt;/item&gt;
      &lt;item&gt;12.08.2025 Radicle 1.3.0 released.&lt;/item&gt;
      &lt;item&gt;17.07.2025 Radicle 1.2.1 released.&lt;/item&gt;
      &lt;item&gt;13.06.2025 Radicle Desktop is out. ð¥ï¸&lt;/item&gt;
      &lt;item&gt;02.06.2025 Radicle 1.2.0 released.&lt;/item&gt;
      &lt;item&gt;05.12.2024 Radicle 1.1.0 released.&lt;/item&gt;
      &lt;item&gt;10.09.2024 Radicle 1.0.0 released.&lt;/item&gt;
      &lt;item&gt;26.03.2024 Radicle 1.0.0-rc.1 released.&lt;/item&gt;
      &lt;item&gt;10.03.2024 New Radicle homepage.&lt;/item&gt;
      &lt;item&gt;05.03.2024 Radicle Guides launch.&lt;/item&gt;
      &lt;item&gt;05.03.2024 Radicle makes it to the top of Hacker News!&lt;/item&gt;
      &lt;item&gt;18.04.2023 Radicle heartwood is announced.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Blog&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;14.08.2025 Jujutsu + Radicle = â¤ï¸&lt;/item&gt;
      &lt;item&gt;12.08.2025 Canonical References&lt;/item&gt;
      &lt;item&gt;23.07.2025 Using Radicle CI for Development&lt;/item&gt;
      &lt;item&gt;30.05.2025 How we used Radicle with GitHub Actions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Feedback&lt;/head&gt;
    &lt;p&gt;If you have feedback, join our Zulip or send us an email at feedback@radicle.xyz. Emails sent to this address are automatically posted to our #feedback channel on Zulip.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://radicle.xyz"/><published>2026-01-23T13:25:42+00:00</published></entry></feed>