<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-22T12:56:44.539865+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45962656</id><title>Event Sourcing in Go: From Zero to Production</title><updated>2025-11-22T12:56:54.212378+00:00</updated><content>&lt;doc fingerprint="c73083e130d09486"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Event Sourcing in Go: From Zero to Production&lt;/head&gt;
    &lt;p&gt;Event sourcing: append-only architecture processing 10K events/sec with complete history, time travel debugging, and CQRS. From theory to production implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Event sourcing provides complete audit trail and time-travel debugging capabilities&lt;/item&gt;
      &lt;item&gt;CQRS separation enables independent scaling of reads and writes&lt;/item&gt;
      &lt;item&gt;Snapshots are essential for performance with large event streams&lt;/item&gt;
      &lt;item&gt;Proper event versioning and migration strategies prevent production disasters&lt;/item&gt;
      &lt;item&gt;Event streaming with Kafka enables real-time projections and system integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Why Event Sourcing?&lt;/item&gt;
      &lt;item&gt;Core Concepts in 5 Minutes&lt;/item&gt;
      &lt;item&gt;Production Event Store&lt;/item&gt;
      &lt;item&gt;Aggregate Root Pattern&lt;/item&gt;
      &lt;item&gt;CQRS: Command and Query Separation&lt;/item&gt;
      &lt;item&gt;Snapshots for Performance&lt;/item&gt;
      &lt;item&gt;Event Streaming with Kafka&lt;/item&gt;
      &lt;item&gt;Temporal Queries (Time Travel)&lt;/item&gt;
      &lt;item&gt;Saga Pattern for Distributed Transactions&lt;/item&gt;
      &lt;item&gt;Security Considerations&lt;/item&gt;
      &lt;item&gt;Testing Strategy&lt;/item&gt;
      &lt;item&gt;Production Monitoring&lt;/item&gt;
      &lt;item&gt;Performance Optimizations&lt;/item&gt;
      &lt;item&gt;Migration from Traditional System&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why Event Sourcing?&lt;/head&gt;
    &lt;p&gt;Your database shows current state. But how did it get there? Who changed what? When? Why?&lt;/p&gt;
    &lt;code&gt;-- Traditional: Current state only
SELECT balance FROM accounts WHERE id = 123;
-- Result: 1000

-- Event sourced: Complete history
SELECT * FROM events WHERE aggregate_id = 123;
-- Shows every deposit, withdrawal, fee, interest&lt;/code&gt;
    &lt;p&gt;We needed audit trail for financial compliance. Event sourcing gave us that plus time travel, debugging superpowers, and perfect scalability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Core Concepts in 5 Minutes&lt;/head&gt;
    &lt;p&gt;Event sourcing stores state changes as a sequence of events rather than overwriting data. Instead of UPDATE statements that destroy history, we append immutable events that tell the complete story.&lt;/p&gt;
    &lt;p&gt;Traditional systems show what IS. Event sourcing shows what HAPPENED. This distinction transforms debugging, auditing, and analytics. When a bug corrupts data, we can replay events to find exactly when and how it occurred.&lt;/p&gt;
    &lt;p&gt;Events are facts about the past - they cannot be changed or deleted. This immutability provides natural audit logging and enables powerful patterns like temporal queries and retroactive fixes.&lt;/p&gt;
    &lt;p&gt;State becomes a left-fold over events. Current balance isn't stored; it's calculated by replaying all deposits and withdrawals. This sounds slow but with snapshots and projections, it's actually faster than traditional systems for many use cases.&lt;/p&gt;
    &lt;code&gt;// Events capture business intent
type AccountOpened struct {
    AccountID string
    Currency  string
}

type MoneyDeposited struct {
    AccountID string
    Amount    decimal.Decimal
}

// State derived from event history
func (a *Account) Apply(event Event) {
    // Rebuild state by replaying events
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Production Event Store&lt;/head&gt;
    &lt;p&gt;A production event store needs to handle millions of events efficiently. Our PostgreSQL-based implementation processes 10K events/second with proper indexing and partitioning. The append-only nature makes it extremely fast - no updates, no deletes, just inserts.&lt;/p&gt;
    &lt;p&gt;Event ordering is critical for consistency. We use database sequences per aggregate to ensure events are applied in the correct order. This prevents race conditions where concurrent operations might corrupt state.&lt;/p&gt;
    &lt;p&gt;The schema design balances normalization with query performance. Event data is stored as JSON for flexibility, while frequently queried fields (aggregate_id, event_type) are indexed columns. This hybrid approach enables both fast queries and schema evolution.&lt;/p&gt;
    &lt;p&gt;Metadata tracks important context: user ID, correlation ID, causation ID. This audit trail proves invaluable for debugging and compliance. Every state change is traceable to its origin.&lt;/p&gt;
    &lt;code&gt;type EventStore struct {
    db *sql.DB
}

type StoredEvent struct {
    ID            uuid.UUID
    AggregateID   string
    EventType     string
    EventVersion  int
    EventData     json.RawMessage
    Metadata      json.RawMessage
    OccurredAt    time.Time
}

// Append-only schema with proper indexes
const schema = `
CREATE TABLE events (
    id UUID PRIMARY KEY,
    aggregate_id VARCHAR(255) NOT NULL,
    event_type VARCHAR(255) NOT NULL,
    event_version INT NOT NULL,
    event_data JSONB NOT NULL,
    metadata JSONB,
    occurred_at TIMESTAMP NOT NULL,
    recorded_at TIMESTAMP NOT NULL DEFAULT NOW(),
    
    -- Ensure events are ordered per aggregate
    UNIQUE(aggregate_id, event_version),
    
    -- Indexes for queries
    INDEX idx_aggregate (aggregate_id, event_version),
    INDEX idx_event_type (event_type),
    INDEX idx_occurred_at (occurred_at)
);

-- Global event sequence for ordering
CREATE SEQUENCE IF NOT EXISTS global_event_sequence;
ALTER TABLE events ADD COLUMN global_sequence BIGINT DEFAULT nextval('global_event_sequence');
CREATE INDEX idx_global_sequence ON events(global_sequence);
`

func (es *EventStore) SaveEvents(ctx context.Context, aggregateID, aggregateType string, events []Event, expectedVersion int) error {
    tx, err := es.db.BeginTx(ctx, nil)
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    // Check optimistic concurrency
    var currentVersion int
    err = tx.QueryRow(`
        SELECT COALESCE(MAX(event_version), 0) 
        FROM events 
        WHERE aggregate_id = $1`,
        aggregateID,
    ).Scan(¬§tVersion)
    
    if err != nil {
        return err
    }
    
    if currentVersion != expectedVersion {
        return fmt.Errorf("concurrency conflict: expected version %d, got %d", 
            expectedVersion, currentVersion)
    }
    
    // Save events
    version := expectedVersion
    for _, event := range events {
        version++
        
        eventData, err := json.Marshal(event)
        if err != nil {
            return err
        }
        
        metadata := map[string]interface{}{
            "user_id":     ctx.Value("user_id"),
            "trace_id":    ctx.Value("trace_id"),
            "source":      ctx.Value("source"),
        }
        metadataJSON, _ := json.Marshal(metadata)
        
        _, err = tx.Exec(`
            INSERT INTO events (
                aggregate_id, aggregate_type, event_type, 
                event_version, event_data, metadata, occurred_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7)`,
            aggregateID,
            aggregateType,
            event.EventType(),
            version,
            eventData,
            metadataJSON,
            event.OccurredAt(),
        )
        
        if err != nil {
            return err
        }
    }
    
    return tx.Commit()
}

func (es *EventStore) GetEvents(ctx context.Context, aggregateID string, fromVersion int) ([]StoredEvent, error) {
    rows, err := es.db.QueryContext(ctx, `
        SELECT 
            id, aggregate_id, aggregate_type, event_type,
            event_version, event_data, metadata, 
            occurred_at, recorded_at
        FROM events
        WHERE aggregate_id = $1 AND event_version &amp;gt; $2
        ORDER BY event_version`,
        aggregateID, fromVersion,
    )
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var events []StoredEvent
    for rows.Next() {
        var e StoredEvent
        err := rows.Scan(
            &amp;amp;e.ID, &amp;amp;e.AggregateID, &amp;amp;e.AggregateType,
            &amp;amp;e.EventType, &amp;amp;e.EventVersion, &amp;amp;e.EventData,
            &amp;amp;e.Metadata, &amp;amp;e.OccurredAt, &amp;amp;e.RecordedAt,
        )
        if err != nil {
            return nil, err
        }
        events = append(events, e)
    }
    
    return events, nil
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Aggregate Root Pattern&lt;/head&gt;
    &lt;code&gt;type AggregateRoot struct {
    ID               string
    Version          int
    uncommittedEvents []Event
}

func (a *AggregateRoot) RecordEvent(event Event) {
    a.uncommittedEvents = append(a.uncommittedEvents, event)
    a.Version++
}

func (a *AggregateRoot) GetUncommittedEvents() []Event {
    return a.uncommittedEvents
}

func (a *AggregateRoot) MarkEventsAsCommitted() {
    a.uncommittedEvents = []Event{}
}

// Example: Account aggregate
type Account struct {
    AggregateRoot
    Balance  decimal.Decimal
    Currency string
    Status   string
}

func (a *Account) Deposit(amount decimal.Decimal) error {
    if amount.LessThanOrEqual(decimal.Zero) {
        return fmt.Errorf("invalid deposit amount: %v must be positive", amount)
    }
    
    event := MoneyDeposited{
        AccountID: a.ID,
        Amount:    amount,
        Timestamp: time.Now(),
    }
    
    a.Apply(event)
    a.RecordEvent(event)
    return nil
}

func (a *Account) Withdraw(amount decimal.Decimal) error {
    if amount.GreaterThan(a.Balance) {
        return fmt.Errorf("insufficient funds: attempting to withdraw %v from balance %v", amount, a.Balance)
    }
    
    event := MoneyWithdrawn{
        AccountID: a.ID,
        Amount:    amount,
        Timestamp: time.Now(),
    }
    
    a.Apply(event)
    a.RecordEvent(event)
    return nil
}

func (a *Account) Apply(event Event) {
    switch e := event.(type) {
    case MoneyDeposited:
        a.Balance = a.Balance.Add(e.Amount)
    case MoneyWithdrawn:
        a.Balance = a.Balance.Sub(e.Amount)
    }
}&lt;/code&gt;
    &lt;head rend="h2"&gt;CQRS: Command and Query Separation&lt;/head&gt;
    &lt;code&gt;// Write side: Commands modify aggregates
type CommandHandler struct {
    eventStore *EventStore
    eventBus   *EventBus
}

func (h *CommandHandler) Handle(cmd Command) error {
    switch c := cmd.(type) {
    case DepositMoney:
        return h.handleDeposit(c)
    case WithdrawMoney:
        return h.handleWithdraw(c)
    }
    return errors.New("unknown command")
}

func (h *CommandHandler) handleDeposit(cmd DepositMoney) error {
    // Load aggregate from events
    account := &amp;amp;Account{}
    events, err := h.eventStore.GetEvents(ctx, cmd.AccountID, 0)
    if err != nil {
        return err
    }
    
    for _, e := range events {
        account.Apply(e)
    }
    
    // Execute business logic
    err = account.Deposit(cmd.Amount)
    if err != nil {
        return err
    }
    
    // Save new events
    err = h.eventStore.SaveEvents(
        ctx, 
        account.ID, 
        "Account",
        account.GetUncommittedEvents(),
        account.Version,
    )
    if err != nil {
        return err
    }
    
    // Publish for projections
    for _, event := range account.GetUncommittedEvents() {
        h.eventBus.Publish(event)
    }
    
    return nil
}

// Read side: Projections for queries
type AccountProjection struct {
    db *sql.DB
}

func (p *AccountProjection) Handle(event Event) error {
    switch e := event.(type) {
    case MoneyDeposited:
        _, err := p.db.Exec(`
            UPDATE account_projections 
            SET balance = balance + $1, updated_at = NOW()
            WHERE account_id = $2`,
            e.Amount, e.AccountID,
        )
        return err
        
    case MoneyWithdrawn:
        _, err := p.db.Exec(`
            UPDATE account_projections 
            SET balance = balance - $1, updated_at = NOW()
            WHERE account_id = $2`,
            e.Amount, e.AccountID,
        )
        return err
    }
    return nil
}

// Query handler reads from projections
type QueryHandler struct {
    db *sql.DB
}

func (q *QueryHandler) GetAccountBalance(accountID string) (decimal.Decimal, error) {
    var balance decimal.Decimal
    err := q.db.QueryRow(`
        SELECT balance FROM account_projections WHERE account_id = $1`,
        accountID,
    ).Scan(&amp;amp;balance)
    return balance, err
}&lt;/code&gt;
    &lt;head rend="h3"&gt;‚ö†Ô∏è Eventual Consistency Tradeoff&lt;/head&gt;
    &lt;p&gt;CQRS introduces eventual consistency between write and read models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Events are written immediately to the event store&lt;/item&gt;
      &lt;item&gt;Projections update asynchronously (typically milliseconds to seconds)&lt;/item&gt;
      &lt;item&gt;Queries may return stale data until projections catch up&lt;/item&gt;
      &lt;item&gt;Design your UX to handle this: optimistic UI updates, "processing" states, or read-your-writes guarantees where critical&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Snapshots for Performance&lt;/head&gt;
    &lt;code&gt;type Snapshot struct {
    AggregateID string
    Version     int
    Data        []byte
    CreatedAt   time.Time
}

func (es *EventStore) SaveSnapshot(ctx context.Context, snapshot Snapshot) error {
    _, err := es.db.ExecContext(ctx, `
        INSERT INTO snapshots (aggregate_id, version, data, created_at)
        VALUES ($1, $2, $3, $4)
        ON CONFLICT (aggregate_id) 
        DO UPDATE SET version = $2, data = $3, created_at = $4`,
        snapshot.AggregateID,
        snapshot.Version,
        snapshot.Data,
        snapshot.CreatedAt,
    )
    return err
}

func (es *EventStore) GetSnapshot(ctx context.Context, aggregateID string) (*Snapshot, error) {
    var s Snapshot
    err := es.db.QueryRowContext(ctx, `
        SELECT aggregate_id, version, data, created_at
        FROM snapshots
        WHERE aggregate_id = $1`,
        aggregateID,
    ).Scan(&amp;amp;s.AggregateID, &amp;amp;s.Version, &amp;amp;s.Data, &amp;amp;s.CreatedAt)
    
    if err == sql.ErrNoRows {
        return nil, nil
    }
    return &amp;amp;s, err
}

// Load aggregate with snapshot optimization
func LoadAccount(es *EventStore, accountID string) (*Account, error) {
    account := &amp;amp;Account{}
    
    // Try to load snapshot
    snapshot, err := es.GetSnapshot(ctx, accountID)
    if err != nil {
        return nil, err
    }
    
    fromVersion := 0
    if snapshot != nil {
        // Restore from snapshot
        err = json.Unmarshal(snapshot.Data, account)
        if err != nil {
            return nil, err
        }
        fromVersion = snapshot.Version
    }
    
    // Apply events after snapshot
    events, err := es.GetEvents(ctx, accountID, fromVersion)
    if err != nil {
        return nil, err
    }
    
    for _, e := range events {
        account.Apply(e)
    }
    
    // Create new snapshot every 100 events
    if len(events) &amp;gt; 100 {
        snapshotData, _ := json.Marshal(account)
        es.SaveSnapshot(ctx, Snapshot{
            AggregateID: accountID,
            Version:     account.Version,
            Data:        snapshotData,
            CreatedAt:   time.Now(),
        })
    }
    
    return account, nil
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Event Streaming with Kafka&lt;/head&gt;
    &lt;code&gt;type EventStreamer struct {
    eventStore *EventStore
    producer   *kafka.Writer
    lastSeq    int64
}

func (s *EventStreamer) StreamEvents(ctx context.Context) {
    ticker := time.NewTicker(100 * time.Millisecond)
    defer ticker.Stop()
    
    for {
        select {
        case &amp;lt;-ctx.Done():
            return
        case &amp;lt;-ticker.C:
            s.publishNewEvents(ctx)
        }
    }
}

func (s *EventStreamer) publishNewEvents(ctx context.Context) {
    rows, err := s.eventStore.db.QueryContext(ctx, `
        SELECT 
            global_sequence, aggregate_id, event_type, 
            event_data, occurred_at
        FROM events
        WHERE global_sequence &amp;gt; $1
        ORDER BY global_sequence
        LIMIT 1000`,
        s.lastSeq,
    )
    if err != nil {
        return
    }
    defer rows.Close()
    
    var messages []kafka.Message
    var maxSeq int64
    
    for rows.Next() {
        var seq int64
        var aggregateID, eventType string
        var eventData json.RawMessage
        var occurredAt time.Time
        
        rows.Scan(&amp;amp;seq, &amp;amp;aggregateID, &amp;amp;eventType, &amp;amp;eventData, &amp;amp;occurredAt)
        
        messages = append(messages, kafka.Message{
            Topic: fmt.Sprintf("events.%s", eventType),
            Key:   []byte(aggregateID),
            Value: eventData,
            Headers: []kafka.Header{
                {Key: "event_type", Value: []byte(eventType)},
                {Key: "occurred_at", Value: []byte(occurredAt.Format(time.RFC3339))},
            },
        })
        
        maxSeq = seq
    }
    
    if len(messages) &amp;gt; 0 {
        err := s.producer.WriteMessages(ctx, messages...)
        if err == nil {
            s.lastSeq = maxSeq
        }
    }
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Temporal Queries (Time Travel)&lt;/head&gt;
    &lt;code&gt;// Get account state at specific point in time
func (es *EventStore) GetAggregateAtTime(ctx context.Context, aggregateID string, pointInTime time.Time) (*Account, error) {
    events, err := es.db.QueryContext(ctx, `
        SELECT event_type, event_data
        FROM events
        WHERE aggregate_id = $1 AND occurred_at &amp;lt;= $2
        ORDER BY event_version`,
        aggregateID, pointInTime,
    )
    if err != nil {
        return nil, err
    }
    defer events.Close()
    
    account := &amp;amp;Account{}
    for events.Next() {
        var eventType string
        var eventData json.RawMessage
        events.Scan(&amp;amp;eventType, &amp;amp;eventData)
        
        event := deserializeEvent(eventType, eventData)
        account.Apply(event)
    }
    
    return account, nil
}

// Replay events for debugging
func ReplayEvents(es *EventStore, from, to time.Time, handler func(Event)) error {
    rows, err := es.db.Query(`
        SELECT event_type, event_data, occurred_at
        FROM events
        WHERE occurred_at BETWEEN $1 AND $2
        ORDER BY global_sequence`,
        from, to,
    )
    if err != nil {
        return err
    }
    defer rows.Close()
    
    for rows.Next() {
        var eventType string
        var eventData json.RawMessage
        var occurredAt time.Time
        
        rows.Scan(&amp;amp;eventType, &amp;amp;eventData, &amp;amp;occurredAt)
        event := deserializeEvent(eventType, eventData)
        handler(event)
    }
    
    return nil
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Saga Pattern for Distributed Transactions&lt;/head&gt;
    &lt;code&gt;type TransferSaga struct {
    ID          string
    FromAccount string
    ToAccount   string
    Amount      decimal.Decimal
    State       string
    CompletedSteps []string
}

func (s *TransferSaga) Handle(event Event) ([]Command, error) {
    switch e := event.(type) {
    case TransferInitiated:
        return []Command{
            WithdrawMoney{AccountID: e.FromAccount, Amount: e.Amount},
        }, nil
        
    case MoneyWithdrawn:
        if e.AccountID == s.FromAccount {
            s.CompletedSteps = append(s.CompletedSteps, "withdrawn")
            return []Command{
                DepositMoney{AccountID: s.ToAccount, Amount: s.Amount},
            }, nil
        }
        
    case MoneyDeposited:
        if e.AccountID == s.ToAccount {
            s.State = "completed"
            return []Command{
                MarkTransferComplete{TransferID: s.ID},
            }, nil
        }
        
    case WithdrawFailed:
        s.State = "failed"
        return nil, nil
        
    case DepositFailed:
        // Compensate - refund the withdrawal
        return []Command{
            DepositMoney{AccountID: s.FromAccount, Amount: s.Amount},
        }, nil
    }
    
    return nil, nil
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Event Store Consistency Warning&lt;/head&gt;
    &lt;p&gt;Event stores require careful attention to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimistic concurrency control to prevent data corruption&lt;/item&gt;
      &lt;item&gt;Event ordering guarantees within aggregates&lt;/item&gt;
      &lt;item&gt;Backup and recovery procedures for event streams&lt;/item&gt;
      &lt;item&gt;Event schema evolution and versioning strategies&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Security Considerations&lt;/head&gt;
    &lt;head rend="h3"&gt;Event Sourcing Security Best Practices&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Event Encryption: Encrypt sensitive data in event payloads&lt;/item&gt;
      &lt;item&gt;Access Control: Role-based access to event streams and projections&lt;/item&gt;
      &lt;item&gt;Audit Trail: Include user context and authorization in event metadata&lt;/item&gt;
      &lt;item&gt;Data Privacy: Implement "right to be forgotten" through cryptographic erasure&lt;/item&gt;
      &lt;item&gt;Replay Security: Ensure event replay doesn't bypass current security rules&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;// Secure event with encryption
type SecureEvent struct {
    BaseEvent
    EncryptedPayload []byte
    KeyID           string
    Nonce           []byte
}

// GDPR-compliant cryptographic erasure
type GDPREventStore struct {
    *EventStore
    keyManager *KeyManager
}

func (ges *GDPREventStore) ForgetUser(ctx context.Context, userID string) error {
    events, err := ges.GetEventsByUser(ctx, userID)
    if err != nil {
        return fmt.Errorf("failed to find user events: %w", err)
    }
    
    for _, event := range events {
        if err := ges.keyManager.RevokeKey(event.KeyID); err != nil {
            return fmt.Errorf("failed to revoke key %s: %w", event.KeyID, err)
        }
    }
    
    return ges.MarkUserForgotten(ctx, userID)
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Testing Strategy&lt;/head&gt;
    &lt;head rend="h3"&gt;üìä Event Sourcing Testing Framework&lt;/head&gt;
    &lt;p&gt;Comprehensive testing approach for event-sourced systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Event Store Tests: Test consistency, concurrency, and durability&lt;/item&gt;
      &lt;item&gt;Aggregate Tests: Unit test business logic and invariants&lt;/item&gt;
      &lt;item&gt;Projection Tests: Verify read model consistency&lt;/item&gt;
      &lt;item&gt;Integration Tests: End-to-end command/query flows&lt;/item&gt;
      &lt;item&gt;Event Schema Tests: Test event evolution and migration&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;// Event store integration test
func TestEventStore(t *testing.T) {
    es := setupTestEventStore(t)
    defer es.Close()
    
    t.Run("ConcurrencyControl", func(t *testing.T) {
        aggregateID := uuid.New().String()
        
        // First save succeeds
        err := es.SaveEvents(context.Background(), aggregateID, "Account", 
            []Event{&amp;amp;AccountOpened{AccountID: aggregateID}}, 0)
        require.NoError(t, err)
        
        // Second save with wrong version fails
        err = es.SaveEvents(context.Background(), aggregateID, "Account", 
            []Event{&amp;amp;MoneyDeposited{AccountID: aggregateID}}, 0)
        require.Error(t, err)
        require.Contains(t, err.Error(), "concurrency conflict")
    })
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Production Monitoring&lt;/head&gt;
    &lt;code&gt;// Event store metrics
type Metrics struct {
    EventsWritten   prometheus.Counter
    EventsRead      prometheus.Counter
    SnapshotCreated prometheus.Counter
    WriteLatency    prometheus.Histogram
    ReadLatency     prometheus.Histogram
}

// Health checks
func (es *EventStore) HealthCheck() error {
    // Check write capability
    testEvent := HealthCheckEvent{
        ID:        uuid.New().String(),
        Timestamp: time.Now(),
    }
    
    err := es.SaveEvents(ctx, "health", "HealthCheck", []Event{testEvent}, 0)
    if err != nil {
        return fmt.Errorf("write check failed: %w", err)
    }
    
    // Check read capability
    events, err := es.GetEvents(ctx, "health", 0)
    if err != nil {
        return fmt.Errorf("read check failed: %w", err)
    }
    
    if len(events) == 0 {
        return errors.New("no events found")
    }
    
    return nil
}

// Lag monitoring
func MonitorProjectionLag(db *sql.DB) {
    ticker := time.NewTicker(10 * time.Second)
    for range ticker.C {
        var lag time.Duration
        db.QueryRow(`
            SELECT MAX(NOW() - updated_at) 
            FROM projection_checkpoints`
        ).Scan(&amp;amp;lag)
        
        projectionLag.Set(lag.Seconds())
        
        if lag &amp;gt; 5*time.Minute {
            alert("Projection lag exceeds 5 minutes")
        }
    }
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance Optimizations&lt;/head&gt;
    &lt;code&gt;// 1. Batch event writes
func (es *EventStore) SaveEventsBatch(events []EventWithAggregate) error {
    // Use COPY for bulk insert
    stmt, err := es.db.Prepare(pq.CopyIn("events",
        "aggregate_id", "aggregate_type", "event_type",
        "event_version", "event_data", "occurred_at"))
    if err != nil {
        return err
    }
    
    for _, e := range events {
        _, err = stmt.Exec(e.AggregateID, e.AggregateType,
            e.EventType, e.Version, e.Data, e.OccurredAt)
        if err != nil {
            return err
        }
    }
    
    return stmt.Close()
}

// 2. Parallel projection updates
func UpdateProjectionsParallel(events []Event) {
    var wg sync.WaitGroup
    ch := make(chan Event, 100)
    
    // Start workers
    for i := 0; i &amp;lt; 10; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            for event := range ch {
                updateProjection(event)
            }
        }()
    }
    
    // Send events
    for _, e := range events {
        ch &amp;lt;- e
    }
    close(ch)
    wg.Wait()
}

// 3. Cache aggregates
var aggregateCache = cache.New(5*time.Minute, 10*time.Minute)

func LoadAccountCached(es *EventStore, accountID string) (*Account, error) {
    if cached, found := aggregateCache.Get(accountID); found {
        return cached.(*Account), nil
    }
    
    account, err := LoadAccount(es, accountID)
    if err != nil {
        return nil, err
    }
    
    aggregateCache.Set(accountID, account, cache.DefaultExpiration)
    return account, nil
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Migration from Traditional System&lt;/head&gt;
    &lt;code&gt;// Generate events from existing state
func MigrateToEventSourcing(db *sql.DB, es *EventStore) error {
    rows, err := db.Query(`
        SELECT id, balance, created_at, updated_at
        FROM accounts`)
    if err != nil {
        return err
    }
    defer rows.Close()
    
    for rows.Next() {
        var id string
        var balance decimal.Decimal
        var createdAt, updatedAt time.Time
        
        rows.Scan(&amp;amp;id, &amp;amp;balance, &amp;amp;createdAt, &amp;amp;updatedAt)
        
        // Create initial event
        events := []Event{
            AccountOpened{
                AccountID: id,
                Timestamp: createdAt,
            },
        }
        
        // Infer deposit event from balance
        if balance.GreaterThan(decimal.Zero) {
            events = append(events, MoneyDeposited{
                AccountID: id,
                Amount:    balance,
                Timestamp: updatedAt,
            })
        }
        
        es.SaveEvents(ctx, id, "Account", events, 0)
    }
    
    return nil
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Lessons from Production&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Before (CRUD)&lt;/cell&gt;
        &lt;cell role="head"&gt;After (Event Sourcing)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Write throughput&lt;/cell&gt;
        &lt;cell&gt;1K/sec&lt;/cell&gt;
        &lt;cell&gt;10K/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Read latency p99&lt;/cell&gt;
        &lt;cell&gt;5ms&lt;/cell&gt;
        &lt;cell&gt;2ms (projections)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Audit completeness&lt;/cell&gt;
        &lt;cell&gt;60%&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Debug time&lt;/cell&gt;
        &lt;cell&gt;Hours&lt;/cell&gt;
        &lt;cell&gt;Minutes (replay)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Storage cost&lt;/cell&gt;
        &lt;cell&gt;$1K/month&lt;/cell&gt;
        &lt;cell&gt;$3-5K/month&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;When NOT to Use Event Sourcing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CRUD is sufficient (most apps)&lt;/item&gt;
      &lt;item&gt;No audit requirements&lt;/item&gt;
      &lt;item&gt;Simple domain logic&lt;/item&gt;
      &lt;item&gt;Team unfamiliar with the pattern&lt;/item&gt;
      &lt;item&gt;Storage cost is critical&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Verdict&lt;/head&gt;
    &lt;p&gt;Event sourcing isn't free. 3-5x storage cost (events + projections + snapshots). Complex to implement. Mental model shift.&lt;/p&gt;
    &lt;p&gt;But for financial systems, audit-heavy domains, or complex business logic? It's transformative. Complete history, perfect audit trail, time travel debugging, and horizontal scalability.&lt;/p&gt;
    &lt;p&gt;Start small: Event source one aggregate. See the benefits. Then expand. Don't go all-in immediately.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://skoredin.pro/blog/golang/event-sourcing-go"/><published>2025-11-18T08:18:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969398</id><title>The Connectivity Standards Alliance Announces Zigbee 4.0 and Suzi</title><updated>2025-11-22T12:56:54.020094+00:00</updated><content>&lt;doc fingerprint="e6f1189c4330f441"&gt;
  &lt;main&gt;
    &lt;p&gt;DAVIS, California ‚Äì November 18, 2025 ‚Äì The Connectivity Standards Alliance (Alliance) announced today the release of Zigbee 4.0 and Suzi, the new brand for Zigbee‚Äôs Sub-GHz feature, two major milestones in advancing the foundation of secure, interoperable, and scalable IoT connectivity with a range of optional features, enabling product manufacturers to choose the specific set that best fits their product‚Äôs use case and needs. Together, these innovations mark the next evolution of Zigbee technology, building on decades of proven performance to deliver greater range, reliability, and security across global networks.&lt;/p&gt;
    &lt;head rend="h4"&gt;Strengthening Security, Range, and Interoperability&lt;/head&gt;
    &lt;p&gt;Zigbee 4.0 lays the groundwork for harmonizing traditional Zigbee and Smart Energy devices, delivering greater interoperability across universal networks. This release simplifies certification processes and supports enhanced information exchange, creating a more complete smart home solution. Comprehensive and proactive security updates aligned with evolving international security standards implement cryptographic agility and additional mechanisms to protect the network. Extending its capabilities beyond the 2.4GHz band, Zigbee 4.0 introduces support for the European 800 MHz and North American 900 MHz PHY, providing increased signal strength, range, and coverage. Fully backward compatible with Zigbee 3.0 and Smart Energy, it ensures continuity with more than a billion Zigbee devices already deployed worldwide while introducing improvements to network stability, user experience, and device commissioning in dense networks.&lt;/p&gt;
    &lt;p&gt;Delivering significant security enhancements aligned with evolving global standards, the latest release reinforces Zigbee‚Äôs position as a trusted solution for secure, modern connectivity. Advanced capabilities such as Dynamic Link Key, Device Interview, and Smart Energy Authentication Level Control offer greater control and resilience across connected networks. These capabilities strengthen device authentication, enable selective communication based on security levels, and ensure only trusted devices join the network. New tools such as Restricted Mode, Secured Channel, PAN ID Changes, and Trust Center Swap Out offer improved flexibility and management for ecosystems and installers by enhancing protection, allowing efficient Trust Center replacement, and preventing unauthorized network changes.&lt;/p&gt;
    &lt;p&gt;With Advanced Frame Counter Synchronization, Zigbee 4.0 prevents replay attacks and synchronizes precise message validation between endpoints. Robust improvements such as standardized network-level retries, more reliable data polling for sleepy end devices, and expanded use of APS acknowledgements increase overall network performance and reduce message loss. Features like Formalized Parent Selection, Unique Link Key Monitoring, and Trust Center Connectivity improve network resilience, ensuring that devices maintain secure connections, can rejoin when necessary, and operate even in complex network environments.&lt;/p&gt;
    &lt;p&gt;Elevating both usability and scalability, Zigbee 4.0 is designed to simplify and strengthen device interoperability. Through Zigbee Direct, users can seamlessly onboard and control devices via Bluetooth Low Energy (BLE) without a hub. Batch Commissioning enables efficient, simultaneous setup of multiple devices, simplifying residential and commercial deployments. Additionally, sleepy-to-sleepy communication using Coordinated Sample Listening (CSL) allows direct, low-power exchanges between devices, optimizing energy and further extending battery life. Collectively, these advancements position Zigbee 4.0 as a forward-looking standard, continuing to evolve with industry needs and future market requirements.&lt;/p&gt;
    &lt;head rend="h4"&gt;Introducing Suzi: Expanding Connectivity Through Long-Range Mesh Networking&lt;/head&gt;
    &lt;p&gt;Alongside Zigbee 4.0, the Alliance introduces Suzi, the new brand for the standards-based wireless technology that extends the reach and reliability of IoT connectivity through long-range, Sub-GHz mesh networking.&lt;/p&gt;
    &lt;p&gt;Built on the proven Zigbee network layer, Suzi combines long-range performance, low power consumption, and multi-vendor interoperability to unlock new opportunities in residential, commercial, and industrial applications. From connecting outdoor living spaces to enabling large-scale networks in buildings and cities, Suzi delivers robust, efficient communication in environments demanding extended coverage and minimal interference.&lt;/p&gt;
    &lt;p&gt;Adhering to the same strong security principles that define all Alliance technologies, Suzi aligns with international standards to ensure a secure and trusted ecosystem. Its framework allows developers, manufacturers, and consumers the freedom to build and deploy interoperable devices from a global ecosystem of trusted suppliers.&lt;/p&gt;
    &lt;p&gt;The Suzi Certification Program is planned to open in the first half of 2026, enabling manufacturers to begin certifying products that bring the benefits of long-range, low-power mesh networking to the connected world.&lt;/p&gt;
    &lt;head rend="h4"&gt;A Connected Future, Built on Proven Foundations&lt;/head&gt;
    &lt;p&gt;Together, Zigbee 4.0 and Suzi demonstrate the Alliance‚Äôs dedication to strengthening global IoT networks through open innovation and collective progress. By combining enhanced security, simplified onboarding, and extended range, these new features expand the reach and resilience of the smart ecosystem, making secure, intelligent connectivity accessible everywhere.&lt;/p&gt;
    &lt;p&gt;Learn more about Zigbee and Suzi. Developers interested in learning more about these enhancements can access the Zigbee 4.0 specification documents here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zigbee Base Device Behavior v3.1 Specification&lt;/item&gt;
      &lt;item&gt;Zigbee Core R23.2 Specification&lt;/item&gt;
      &lt;item&gt;Zigbee Device Type Library v1.0 Specification&lt;/item&gt;
      &lt;item&gt;Zigbee Direct v1.1 Specification&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;About the Connectivity Standards Alliance&lt;/head&gt;
    &lt;p&gt;The Connectivity Standards Alliance is the foundation and future of the Internet of Things (IoT). Established in 2002, its wide-ranging global membership collaborates to create and evolve universal open standards for the products transforming the way we live, work, and play. With its Members‚Äô deep and diverse expertise, robust certification programs, and a full suite of open IoT solutions, the Alliance is leading the movement toward a more intuitive, imaginative, and useful world.&lt;/p&gt;
    &lt;p&gt;The Connectivity Standards Alliance Board of Directors is comprised of executives Allegion, Amazon, Apple, ASSA ABLOY, Bosch, CableLabs, Comcast, Espressif, Eve by ABB, Fortune Brands, Google, Haier, Huawei, IKEA, Infineon Technologies AG, LEEDARSON, Legrand, LG Electronics, Lutron Electronics, Midea, Nordic Semiconductor, NXP Semiconductors, OPPO, Resideo Technologies, Samsung Electronics, Schneider Electric, Siemens, Signify (Philips Hue and WiZ), Silicon Labs, Somfy, STMicroelectronics, Tuya, and Verizon.&lt;/p&gt;
    &lt;p&gt;Learn more about the Alliance at www.csa-iot.org; and follow us on: X, Facebook, and LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://csa-iot.org/newsroom/the-connectivity-standards-alliance-announces-zigbee-4-0-and-suzi-empowering-the-next-generation-of-secure-interoperable-iot-devices/"/><published>2025-11-18T17:35:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005111</id><title>We should all be using dependency cooldowns</title><updated>2025-11-22T12:56:53.726741+00:00</updated><content>&lt;doc fingerprint="af811b9b5ac90ef0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 21, 2025 Tags: oss, security&lt;/p&gt;
    &lt;p&gt;TL;DR: Dependency cooldowns are a free, easy, and incredibly effective way to mitigate the large majority of open source supply chain attacks. More individual projects should apply cooldowns (via tools like Dependabot and Renovate) to their dependencies, and packaging ecosystems should invest in first-class support for cooldowns directly in their package managers.&lt;/p&gt;
    &lt;p&gt;√¢Supply chain security√¢ is a serious problem. It√¢s also seriously overhyped, in part because dozens of vendors have a vested financial interest in convincing your that their framing of the underlying problem1 is (1) correct, and (2) worth your money.&lt;/p&gt;
    &lt;p&gt;What√¢s consternating about this is that most open source supply chain attacks have the same basic structure:&lt;/p&gt;
    &lt;p&gt;An attacker compromises a popular open source project, typically via a stolen credential or CI/CD vulnerabilty (such as √¢pwn requests√¢ in GitHub Actions).&lt;/p&gt;
    &lt;p&gt;The attacker introduces a malicious change to the project and uploads it somewhere that will have maximum effect (PyPI, npm, GitHub releases, &amp;amp;c., depending on the target).&lt;/p&gt;
    &lt;p&gt;At this point, the clock has started, as the attacker has moved into the public.&lt;/p&gt;
    &lt;p&gt;Users pick up the compromised version of the project via automatic dependency updates or a lack of dependency pinning.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the aforementioned vendors are scanning public indices as well as customer repositories for signs of compromise, and provide alerts upstream (e.g. to PyPI).&lt;/p&gt;
    &lt;p&gt;Notably, vendors are incentivized to report quickly and loudly upstream, as this increases the perceived value of their services in a crowded field.&lt;/p&gt;
    &lt;p&gt;Upstreams (PyPI, npm, &amp;amp;c.) remove or disable the compromised package version(s).&lt;/p&gt;
    &lt;p&gt;End-user remediation begins.&lt;/p&gt;
    &lt;p&gt;The key thing to observe is that the gap between (1) and (2) can be very large2 (weeks or months), while the gap between (2) and (5) is typically very small: hours or days. This means that, once the attacker has moved into the actual exploitation phase, their window of opportunity to cause damage is pretty limited.&lt;/p&gt;
    &lt;p&gt;We can see this with numerous prominent supply chain attacks over the last 18 months3:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attack&lt;/cell&gt;
        &lt;cell role="head"&gt;Approx. Window of Opportunity&lt;/cell&gt;
        &lt;cell role="head"&gt;References&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xz-utils&lt;/cell&gt;
        &lt;cell&gt;√¢ 5 weeks4&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 1)&lt;/cell&gt;
        &lt;cell&gt;12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 2)&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tj-actions&lt;/cell&gt;
        &lt;cell&gt;3 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;chalk&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nx&lt;/cell&gt;
        &lt;cell&gt;4 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rspack&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num2words&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kong Ingress Controller&lt;/cell&gt;
        &lt;cell&gt;√¢ 10 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;web3.js&lt;/cell&gt;
        &lt;cell&gt;5 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(Each of these attacks has significant downstream effect, of course, but only within their window of opportunity. Subsequent compromises from each, like Shai-Hulud, represent new windows of opportunity where the attackers regrouped and pivoted onto the next set of compromised credentials.)&lt;/p&gt;
    &lt;p&gt;My takeaway from this: some windows of opportunity are bigger, but the majority of them are under a week long. Consequently, ordinary developers can avoid the bulk of these types of attacks by instituting cooldowns on their dependencies.&lt;/p&gt;
    &lt;p&gt;A √¢cooldown√¢ is exactly what it sounds like: a window of time between when a dependency is published and when it√¢s considered suitable for use. The dependency is public during this window, meaning that √¢supply chain security√¢ vendors can work their magic while the rest of us wait any problems out.&lt;/p&gt;
    &lt;p&gt;I love cooldowns for several reasons:&lt;/p&gt;
    &lt;p&gt;They√¢re empirically effective, per above. They won√¢t stop all attackers, but they do stymie the majority of high-visibiity, mass-impact supply chain attacks that have become more common.&lt;/p&gt;
    &lt;p&gt;They√¢re incredibly easy to implement. Moreover, they√¢re literally free to implement in most cases: most people can use Dependabot√¢s functionality, Renovate√¢s functionality, or the functionality build directly into their package manager5.&lt;/p&gt;
    &lt;p&gt;This is how simple it is in Dependabot:&lt;/p&gt;
    &lt;p&gt;(Rinse and repeat for other ecosystems as needed.)&lt;/p&gt;
    &lt;p&gt;Cooldowns enforce positive behavior from supply chain security vendors: vendors are still incentivized to discover and report attacks quickly, but are not as incentivized to emit volumes of blogspam about √¢critical√¢ attacks on largely underfunded open source ecosystems.&lt;/p&gt;
    &lt;p&gt;In the very small sample set above, 8/10 attacks had windows of opportunity of less than a week. Setting a cooldown of 7 days would have prevented the vast majority of these attacks from reaching end users (and causing knock-on attacks, which several of these were). Increasing the cooldown to 14 days would have prevented all but 1 of these attacks6.&lt;/p&gt;
    &lt;p&gt;Cooldowns are, obviously, not a panacea: some attackers will evade detection, and delaying the inclusion of potentially malicious dependencies by a week (or two) does not fundamentally alter the fact that supply chain security is a social trust problem, not a purely technical one. Still, an 80-90% reduction in exposure through a technique that is free and easy seems hard to beat.&lt;/p&gt;
    &lt;p&gt;Related to the above, it√¢s unfortunate that cooldowns aren√¢t baked directly into more packaging ecosystems: Dependabot and Renovate are great, but even better would be if the package manager itself (as the source of ground truth) could enforce cooldowns directly (including of dependencies not introduced or bumped through automated flows).&lt;/p&gt;
    &lt;p&gt;The problem being, succinctly: modern software stacks are complex and opaque, with little to no difference in privilege between first-party code and third-party dependencies.√Ç ‚Ü©&lt;/p&gt;
    &lt;p&gt;In part because of the prevalence of long-lived, overscoped credentials. Long-lived credentials let attackers operate on their own (comfortable) timelines; this is why Trusted Publishing is such a useful (but not wholly sufficient) technique for reducing the attacker√¢s attack staging window.√Ç ‚Ü©&lt;/p&gt;
    &lt;p&gt;Filippo Valsorda has an excellent compilation of recent supply chain compromises here.√Ç ‚Ü©&lt;/p&gt;
    &lt;p&gt;The xz-utils attack is a significant outlier, both in its scope and the length of its window of opportunity. In this case, I√¢ve measured from the attacker√¢s first backdoored release (v5.6.0, 2024-02-24) to the time of rollback within Debian (2024-03-28).√Ç ‚Ü©&lt;/p&gt;
    &lt;p&gt;For example, pnpm√¢s &lt;code&gt;minimumReleaseAge&lt;/code&gt;.
           uv also has &lt;code&gt;exclude-newer&lt;/code&gt;, 
           although this specifies an absolute cutoff rather than a rolling cooldown.√Ç¬†‚Ü©&lt;/p&gt;
    &lt;p&gt;Notably, the only attack that would have stymied a 14-day cooldown is xz-utils, which is also the most technically, logistically, and socially advanced of all of the attacks.√Ç ‚Ü©&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.yossarian.net/2025/11/21/We-should-all-be-using-dependency-cooldowns"/><published>2025-11-21T14:50:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005388</id><title>Make product worse, get money</title><updated>2025-11-22T12:56:53.568978+00:00</updated><content>&lt;doc fingerprint="652b19d9bbe30db8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Make product worse, get money&lt;/head&gt;
    &lt;p&gt;I recently asked why people seem to hate dating apps so much. In response, 80% of you emailed me some version of the following theory:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The thing about dating apps is that if they do a good job and match people up, then the matched people will quit the app and stop paying. So they have an incentive to string people along but not to actually help people find long-term relationships.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;May I explain why I don‚Äôt find this type of theory very helpful?&lt;/p&gt;
    &lt;p&gt;I‚Äôm not saying that I think it‚Äôs wrong, mind you. Rather, my objection is that while the theory is phrased in terms of dating apps, the same basic pattern applies to basically anyone who is trying to make money by doing anything.&lt;/p&gt;
    &lt;p&gt;For example, consider a pizza restaurant. Try these theories on for size:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza: ‚ÄúThe thing about pizza restaurants is that if they use expensive ingredients or labor-intensive pizza-making techniques, then it costs more to make pizza. So they have an incentive to use low-cost ingredients and labor-saving shortcuts.‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza II: ‚ÄúThe thing about pizza restaurants is that if they have nice tables separated at a comfortable distance, then they can‚Äôt fit as many customers. So they have an incentive to use tiny tables and cram people in cheek by jowl.‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza III: ‚ÄúThe thing about pizza restaurants is that if they sell big pizzas, then people will eat them and stop being hungry, meaning they don‚Äôt buy additional pizza. So they have an incentive to serve tiny low-calorie pizzas.‚Äù&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See what I mean? You can construct similar theories for other domains, too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Cars: ‚ÄúThe thing about automakers is that making cars safe is expensive. So they have an incentive to make unsafe cars.‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Videos: ‚ÄúThe thing about video streaming is that high-resolution video uses more expensive bandwidth. So they have an incentive to use low-resolution.‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Blogging: ‚ÄúThe thing about bloggers is that research is time-consuming. So they have an incentive to be sloppy about the facts.‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Durability: ‚ÄúThe thing about {lightbulb, car, phone, refrigerator, cargo ship} manufacturing is that if you make a {lightbulb, car, phone, refrigerator, cargo ship} that lasts a long time, then people won‚Äôt buy new ones. So there‚Äôs an incentive to make {lightbulbs, cars, phones, refrigerators, cargo ships} that break quickly.‚Äù&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All these theories can be thought of as instances of two general patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Make product worse, get money: ‚ÄúThe thing about selling goods or services is that making goods or services better costs money. So people have an incentive to make goods and services worse.‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Raise price, get money: ‚ÄúThe thing about selling goods and services is that if you raise prices, then you get more money. So people have an incentive to raise prices.‚Äù&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Are these theories wrong? Not exactly. But it sure seems like something is missing.&lt;/p&gt;
    &lt;p&gt;I‚Äôm sure most pizza restauranteurs would be thrilled to sell lukewarm 5 cm cardboard discs for $300 each. They do in fact have an incentive to do that, just as predicted by these theories! Yet, in reality, pizza restaurants usually sell pizzas that are made out of food. So clearly these theories aren‚Äôt telling the whole story.&lt;/p&gt;
    &lt;p&gt;Say you have a lucrative business selling 5 cm cardboard discs for $300. I am likely to think, ‚ÄúI like money. Why don‚Äôt I sell pizzas that are only mostly cardboard, but also partly made of flour? And why don‚Äôt I sell them for $200, so I can steal Valued Reader‚Äôs customers?‚Äù But if I did that, then someone else would probably set prices at only $100, or even introduce cardboard-free pizzas, and this would continue until hitting some kind of equilibrium.&lt;/p&gt;
    &lt;p&gt;Sure, producers want to charge infinity dollars for things that cost them zero dollars to make. But consumers want to pay zero dollars for stuff that‚Äôs infinitely valuable. It‚Äôs in the conflict between these desires that all interesting theories live.&lt;/p&gt;
    &lt;p&gt;This is why I don‚Äôt think it‚Äôs helpful to point out that people have an incentive to make their products worse. Of course they do. The interesting question is, why are they able to get away with it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Reasons stuff is bad&lt;/head&gt;
    &lt;p&gt;First reason stuff is bad: People are cheap&lt;/p&gt;
    &lt;p&gt;Why are seats so cramped on planes? Is it because airlines are greedy? Sure. But while they might be greedy, I don‚Äôt think they‚Äôre dumb. If you do a little math, you can calculate that if airlines were to remove a single row of seats, they could add perhaps 2.5 cm (1 in) of extra legroom for everyone, while only decreasing the number of paying customers by around 3%. (This is based on a 737 with single-class, but you get the idea.)&lt;/p&gt;
    &lt;p&gt;So why don‚Äôt airlines rip out a row of seats, raise prices by 3% and enjoy the reduced costs for fuel and customer service? The only answer I can see is that people, on average, aren‚Äôt actually willing to pay 3% more for 2.5 cm more legroom. We want a worse but cheaper product, and so that‚Äôs what we get.&lt;/p&gt;
    &lt;p&gt;I think this is the most common reason stuff is ‚Äúbad‚Äù. It‚Äôs why Subway sandwiches are so soggy, why video games are so buggy, and why IKEA furniture and Primark clothes fall apart so quickly.&lt;/p&gt;
    &lt;p&gt;It‚Äôs good when things are bad for this reason. Or at least, that‚Äôs the premise of capitalism: When companies cut costs, that‚Äôs the invisible hand redirecting resources to maximize social value, or whatever. Companies may be motivated by greed. And you may not like it, since you want to pay zero dollars for infinite value. But this is markets working as designed.&lt;/p&gt;
    &lt;p&gt;Second reason stuff is bad: Information asymmetries&lt;/p&gt;
    &lt;p&gt;Why is it that almost every book / blog / podcast about longevity is such garbage? Well, we don‚Äôt actually know many things that will reliably increase longevity. And those things are mostly all boring / hard / non-fun. And even if you do all of them, it probably only adds a couple of years in expectation. And telling people these facts is not a good way to find suckers who will pay you lots of money for your unproven supplements / seminars / etc.&lt;/p&gt;
    &lt;p&gt;True! But it doesn‚Äôt explain why all longevity stuff is so bad. Why don‚Äôt honest people tell the true story and drive all the hucksters out of business? I suspect the answer is that unless you have a lot of scientific training and do a lot of research, it‚Äôs basically impossible to figure out just how huckstery all the hucksters really are.&lt;/p&gt;
    &lt;p&gt;I think this same basic phenomenon explains why some supplements contain heavy metals, why some food contains microplastics, why restaurants use so much butter and salt, why rentals often have crappy insulation, and why most cars seem to only be safe along dimensions included in crash test scores. When consumers can‚Äôt tell good from evil, evil triumphs.&lt;/p&gt;
    &lt;p&gt;Third reason stuff is bad: People have bad taste&lt;/p&gt;
    &lt;p&gt;Sometimes stuff is bad because people just don‚Äôt appreciate the stuff you consider good. Examples are definitionally controversial, but I think this includes restaurants in cities where all restaurants are bad, North American tea, and travel pants. This reason has a blurry boundary with information asymmetries, as seen in ultrasonic humidifiers or products that use Sucralose instead of aspartame for ‚Äúsafety‚Äù.&lt;/p&gt;
    &lt;p&gt;Fourth reason stuff is bad: Pricing power&lt;/p&gt;
    &lt;p&gt;Finally, sometimes stuff is bad because markets aren‚Äôt working. Sometimes a company is selling a product but has some kind of ‚Äúmoat‚Äù that makes it hard for anyone else to compete with them, e.g. because of some technological or regulatory barrier, control of some key resource or location, intellectual property, a beloved brand, or network effects.&lt;/p&gt;
    &lt;p&gt;If that‚Äôs true, then those companies don‚Äôt have to worry as much about someone else stealing their business, and so (because everyone is axiomatically greedy) they will find ways to make their product cheaper and/or raise prices up until the price is equal to the full value it provides to the marginal consumer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Why is food so expensive at sporting events? Yes, people have no alternatives. But people know food is expensive at sporting events. And they don‚Äôt like it. Instead of selling water for $17, why don‚Äôt venues sell water for $2 and raise ticket prices instead? I don‚Äôt know. Probably something complicated, like that expensive food allows you to extract extra money from rich people without losing business from non-rich people.&lt;/p&gt;
    &lt;p&gt;So of course dating apps would love to string people along for years instead of finding them long-term relationships, so they keep paying money each month. I wouldn‚Äôt be surprised if some people at those companies have literally thought, ‚ÄúMaybe we should string people along for years instead of finding them long-term relationships, so they keep paying money each month, I love money so much.‚Äù&lt;/p&gt;
    &lt;p&gt;But if they are actually doing that (which is unclear to me) or if they are bad in some other way, then how do they get away with it? Why doesn‚Äôt someone else create a competing app that‚Äôs better and thereby steal all their business? It seems like the answer has to be either ‚Äúbecause that‚Äôs impossible‚Äù or ‚Äúbecause people don‚Äôt really want that‚Äù. That‚Äôs where the mystery begins.&lt;/p&gt;
    &lt;p&gt;Dating: A mysterious constellation of facts ¬∑ life economics&lt;/p&gt;
    &lt;p&gt;So much blood ¬∑ conspiracy economics&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dynomight.net/worse/"/><published>2025-11-21T15:23:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006016</id><title>Show HN: Wealthfolio 2.0- Open source investment tracker. Now Mobile and Docker</title><updated>2025-11-22T12:56:53.387493+00:00</updated><content>&lt;doc fingerprint="27ab40bb69b94b92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grow Wealth. Keep Control.&lt;/head&gt;
    &lt;head rend="h2"&gt;A beautiful, Private and Open-Source investment tracker that runs locally on all your devices.&lt;/head&gt;
    &lt;head rend="h2"&gt;WHY CHOOSE WEALTHFOLIO?&lt;/head&gt;
    &lt;p&gt;A beautiful portfolio tracker that respects your privacy and your data&lt;/p&gt;
    &lt;head rend="h3"&gt;Privacy-First Approach&lt;/head&gt;
    &lt;p&gt;Your data never leaves your device. As an open-source project, we prioritize security and transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple and Beautifully Crafted&lt;/head&gt;
    &lt;p&gt;Powerful features wrapped in an elegant, easy-to-use interface. Simplicity meets sophistication.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Hidden Costs&lt;/head&gt;
    &lt;p&gt;Free to use with optional one-time payment. No subscriptions or recurring fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE ESSENTIALS YOU NEED TO TRACK YOUR WEALTH&lt;/head&gt;
    &lt;p&gt;No More Messy Spreadsheets or Privacy Concerns - Just You and Your Secure, Personal Wealth Companion Application&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Aggregation&lt;/head&gt;
    &lt;p&gt;Gather all your investment and savings accounts in one place. See everything at a glance, from stocks to savings! Import your CSV statements from your broker or bank.&lt;/p&gt;
    &lt;head rend="h4"&gt;Comprehensive View&lt;/head&gt;
    &lt;p&gt;See all your accounts in one place.&lt;/p&gt;
    &lt;head rend="h4"&gt;CSV Import&lt;/head&gt;
    &lt;p&gt;Easily import your CSV statements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Holdings Overview&lt;/head&gt;
    &lt;p&gt;Get a clear picture of what's in your portfolio. Stocks, ETFs, or Cryptocurrencies - know what you have and how it's performing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Portfolio Insights&lt;/head&gt;
    &lt;p&gt;Understand your asset allocation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance Tracking&lt;/head&gt;
    &lt;p&gt;Monitor how your investments are doing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance Dashboard&lt;/head&gt;
    &lt;p&gt;See how your investments stack up, all in one place. Compare your accounts side by side, check if you are beating the S&amp;amp;P 500, and track your favorite ETFs without the hassle. No fancy jargon - just clear, useful charts that help you understand how your money is really doing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compare Your Accounts&lt;/head&gt;
    &lt;p&gt;See which accounts are doing best.&lt;/p&gt;
    &lt;head rend="h4"&gt;Beat the Market?&lt;/head&gt;
    &lt;p&gt;Check how you stack up against some popular indexes and ETFs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Income Tracking&lt;/head&gt;
    &lt;p&gt;Monitor dividends and interest income across your entire portfolio. Get a clear view of your passive income streams, helping you make informed decisions about your investments.&lt;/p&gt;
    &lt;head rend="h4"&gt;Dividend Monitoring&lt;/head&gt;
    &lt;p&gt;Track your dividend income.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interest Income&lt;/head&gt;
    &lt;p&gt;Keep an eye on interest earnings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Performance&lt;/head&gt;
    &lt;p&gt;Track your accounts' holdings and performance over time. See how a particular account is performing, and how it's changing over time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Historical Data&lt;/head&gt;
    &lt;p&gt;View past performance trends.&lt;/p&gt;
    &lt;head rend="h4"&gt;Account Analysis&lt;/head&gt;
    &lt;p&gt;Analyze individual account performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goals Tracking&lt;/head&gt;
    &lt;p&gt;Set your savings targets clearly. Distribute your funds across these objectives, assigning a specific percentage to each. Keep an eye on your progress.&lt;/p&gt;
    &lt;head rend="h4"&gt;Target Setting&lt;/head&gt;
    &lt;p&gt;Define your financial goals.&lt;/p&gt;
    &lt;head rend="h4"&gt;Progress Monitoring&lt;/head&gt;
    &lt;p&gt;Track your progress towards goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Contribution Rooms and Limit Tracking&lt;/head&gt;
    &lt;p&gt;Stay on top of your contribution limits for tax-advantaged accounts like IRAs, 401(k)s, or TFSAs. Track your available contribution room and avoid over-contributing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Limit Awareness&lt;/head&gt;
    &lt;p&gt;Know your contribution limits.&lt;/p&gt;
    &lt;head rend="h4"&gt;Avoid Over-Contribution&lt;/head&gt;
    &lt;p&gt;Prevent excess contributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extend Wealthfolio with Powerful Add-ons&lt;/head&gt;
    &lt;head rend="h3"&gt;Investment Fees Tracker&lt;/head&gt;
    &lt;p&gt;Track and analyze investment fees across your portfolio with detailed analytics and insights&lt;/p&gt;
    &lt;head rend="h3"&gt;Goal Progress Tracker&lt;/head&gt;
    &lt;p&gt;Track your investment progress towards target amounts with a visual representation&lt;/p&gt;
    &lt;head rend="h3"&gt;Stock Trading Tracker&lt;/head&gt;
    &lt;p&gt;Simple swing stock trading tracker with performance analytics and calendar views&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wealthfolio.app/?v=2.0"/><published>2025-11-21T16:34:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006082</id><title>You can make PS2 games in JavaScript</title><updated>2025-11-22T12:56:53.240931+00:00</updated><content/><link href="https://jslegenddev.substack.com/p/you-can-now-make-ps2-games-in-javascript"/><published>2025-11-21T16:42:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006616</id><title>Helping Valve to power up Steam devices</title><updated>2025-11-22T12:56:52.546118+00:00</updated><content>&lt;doc fingerprint="961b0d5348912672"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Helping Valve to Power Up Steam Devices&lt;/head&gt;
    &lt;p&gt;Last week, Valve stunned the computer gaming world by unveiling three new gaming devices at once: the Steam Frame, a wireless VR headset; the Steam Machine, a gaming console in the vein of a PlayStation or Xbox; and the Steam Controller, a handheld game controller. Successors to the highly successful Valve Index and Steam Deck, these devices are set to be released in the coming year.&lt;/p&gt;
    &lt;p&gt;Igalia has long worked with Valve on SteamOS, which will power the Machine and Frame, and is excited to be contributing to these new devices, particularly the Frame. The Frame, unlike the Machine or Deck which have x86 CPUs, runs on an ARM-based CPU.&lt;/p&gt;
    &lt;p&gt;Under normal circumstances, this would mean that only games compiled to run on ARM chips could be played on the Frame. In order to get around this barrier, a translation layer called FEX is used to run applications compiled for x86 chips (which are used in nearly all gaming PCs) on ARM chips by translating the x86 machine code into ARM64 machine code.&lt;/p&gt;
    &lt;p&gt;√¢If you love video games, like I do, working on FEX with Valve is a dream come true,√¢ said Paulo Matos, an engineer with Igalia√¢s Compilers Team. Even so, the challenges can be daunting, because making sure the translation is working often requires manual QA rather than automated testing. √¢You have to start a game, sometimes the error shows up in the colors or sound, or how the game behaves when you break down the door in the second level. Just debugging this can take a while,√¢ said Matos. √¢For optimization work I did early last year, I used a game called Psychonauts to test it. I must have played the first 3 to 4 minutes of the game many, many times for debugging. Looking at my history, Steam tells me I played it for 29 hours, but it was always the first few minutes, nothing else.√¢&lt;/p&gt;
    &lt;p&gt;Beyond the CPU, the Qualcomm Adreno 750 GPU used in the Steam Frame introduced its own set of challenges when it came to running desktop games, and other complex workloads, on these devices. Doing so requires a rock-solid Vulkan driver that can ensure correctness, eliminating major rendering bugs, while maintaining high performance. This is a very difficult combination to achieve, and yet that√¢s exactly what we√¢ve done for Valve with Mesa3D Turnip, a FOSS Vulkan driver for Qualcomm Adreno GPUs.&lt;/p&gt;
    &lt;p&gt;Before we started our work, critical optimizations such as LRZ (which you can learn more about from our blog post here) or the autotuner (and its subsequent overhaul) weren√¢t in place. Even worse, there wasn√¢t support for the Adreno 700-series GPUs at all, which we eventually added along with support for tiled rendering.&lt;/p&gt;
    &lt;p&gt;√¢We implemented many Vulkan extensions and reviewed numerous others,√¢ said Danylo Piliaiev, an engineer on the Graphics Team. √¢Over the years, we ensured that D3D11, D3D12, and OpenGL games rendered correctly through DXVK, vkd3d-proton, and Zink, investigating many rendering issues along the way. We achieved higher correctness than the proprietary driver and, in many cases, Mesa3D Turnip is faster as well.√¢&lt;/p&gt;
    &lt;p&gt;We√¢ve worked with many wonderful people from Valve, Google, and other companies to iterate on the Vulkan driver over the years in order to introduce new features, bug fixes, performance improvements, as well as debugging workflows. Some of those people decided to join Igalia later on, such as our colleague and Graphics Team developer Emma Anholt. √¢I√¢ve been working on Mesa for 22 years, and it√¢s great to have a home now where I can keep doing that work, across hardware projects, where the organization prioritizes the work experience of its developers and empowers them within the organization.√¢&lt;/p&gt;
    &lt;p&gt;Valve√¢s support in all this cannot be understated, either. Their choice to build their devices using open software like Mesa3D Turnip and FEX means they√¢re committed to working on and supporting improvements and optimizations that become available to anyone who uses the same open-source projects.&lt;/p&gt;
    &lt;p&gt;√¢We√¢ve received a lot of positive feedback about significantly improved performance and fewer rendering glitches from hobbyists who use these projects to run PC games on Android phones as a result of our work,√¢ said Dhruv Mark Collins, another Graphics Team engineer working on Turnip. √¢And it goes both ways! We√¢ve caught a couple of nasty bugs because of that widespread testing, which really emphasizes why the FOSS model is beneficial for everyone involved.√¢&lt;/p&gt;
    &lt;p&gt;An interesting area of graphics driver development is all the compiler work that is involved. Vulkan drivers such as Mesa3D Turnip need to process shader programs sent by the application to the GPU, and these programs govern how pixels in our screens are shaded or colored with geometry, textures, and lights while playing games. Job Noorman, an engineer from our Compilers Team, made significant contributions to the compiler used by Mesa3D Turnip. He also contributed to the Mesa3D NIR shader compiler, a common part that all Mesa drivers use, including RADV (most popularly used on the Steam Deck) or V3DV (used on Raspberry Pi boards).&lt;/p&gt;
    &lt;p&gt;As is normal for Igalia, while we focused on delivering results for our customer, we also made our work as widely useful as possible. For example: √¢While our target throughout our work has been the Snapdragon 8 Gen 3 that√¢s in the Frame, much of our work extends back through years of Snapdragon hardware, and we regression test it to make sure it stays Vulkan conformant,√¢ said Anholt. This means that Igalia√¢s work for the Frame has consistently passed Vulkan√¢s Conformance Test Suite (CTS) of over 2.8 million tests, some of which Igalia is involved in creating.&lt;/p&gt;
    &lt;p&gt;Our very own Vulkan CTS expert Ricardo Garc√Éa says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Igalia and other Valve contractors actively participate in several areas inside the Khronos Group, the organization maintaining and developing graphics API standards like Vulkan. We contribute specification fixes and feedback, and we are regularly involved in the development of many new Vulkan extensions. Some of these end up being critical for game developers, like mesh shading. Others ensure a smooth and efficient translation of other APIs like DirectX to Vulkan, or help take advantage of hardware features to ensure applications perform great across multiple platforms, both mobile like the Steam Frame or desktop like the Steam Machine. Having Vulkan CTS coverage for these new extensions is a critical step in the release process, helping make sure the specification is clear and drivers implement it correctly, and Igalia engineers have contributed millions of source code lines and tests since our collaboration with Valve started.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A huge challenge we faced in moving forward with development is ensuring that we didn√¢t introduce regressions, small innocent-seeming changes can completely break rendering on games in a way that even CTS might not catch. What automated testing could be done was often quite constrained, but Igalians found ways to push through the barriers. √¢I made a continuous integration test to automatically run single-frame captures of a wide range of games spanning D3D11, D3D9, D3D8, Vulkan, and OpenGL APIs,√¢ said Piliaiev, about the development covered in his recent XDC 2025 talk, √¢ensuring that we don√¢t have rendering or performance regressions.√¢&lt;/p&gt;
    &lt;p&gt;Looking ahead, Igalia√¢s work for Valve will continue to deliver benefits to the wider Linux Gaming ecosystem. For example, the Steam Frame, as a battery-powered VR headset, needs to deliver high performance within a limited power budget. A way to address this is to create a more efficient task scheduler, which is something Changwoo Min of Igalia√¢s Kernel Team has been working on. As he says, √¢I have been developing a customized CPU scheduler for gaming, named LAVD: Latency-criticality Aware Virtual Deadline scheduler.√¢&lt;/p&gt;
    &lt;p&gt;In general terms, a scheduler automatically identifies critical tasks and dynamically boosts their deadlines to improve responsiveness. Most task schedulers don√¢t take energy consumption into account, but the Rust-based LAVD is different. √¢LAVD makes scheduling decisions considering each chip√¢s performance versus energy trade-offs. It measures and predicts the required computing power on the fly, then selects the best set of CPUs to meet that demand with minimal energy consumption,√¢ said Min.&lt;/p&gt;
    &lt;p&gt;One of our other kernel engineers, Melissa Wen, has been working on AMD kernel display drivers to maintain good color management and HDR support for SteamOS across AMD hardware families, both for the Steam Deck and the Steam Machine. This is especially important with newer display hardware in the Steam Machine, which features some notable differences in color capabilities, aiming for more powerful and efficient color management which necessitated driver work.&lt;/p&gt;
    &lt;p&gt;√¢¬¶and that√¢s a wrap! We will continue our efforts toward improving future versions of SteamOS, and with a partner as strongly supportive as Valve, we expect to do more work to make Linux gaming even better. If any of that sounded interesting and you√¢d like to work with us to tackle tricky problems of your own, please get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.igalia.com/2025/11/helpingvalve.html"/><published>2025-11-21T17:29:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46008156</id><title>Discontinuation of ARM Notebook with Snapdragon X Elite SoC</title><updated>2025-11-22T12:56:51.447477+00:00</updated><content>&lt;doc fingerprint="600c35d002b48faf"&gt;
  &lt;main&gt;
    &lt;p&gt;We ship your order to almost all countries, in Europe mostly even free of charge! The respective shipping costs and the cost threshold above which we will cover the costs for you can be found here or for international shipping in the table below.&lt;/p&gt;
    &lt;p&gt;There are no shipping costs within Germany for goods worth ‚Ç¨100 or more.&lt;/p&gt;
    &lt;p&gt;No matter how many small articles you order, such as USB stick card reader, LAN adapters or fan articles, with us, you pay a maximum of 7.99 ‚Ç¨ shipping costs.&lt;/p&gt;
    &lt;p&gt;You can check all occurring shipping costs or if we even deliver for free right before sending your order!&lt;/p&gt;
    &lt;p&gt;Here are the shipping costs as well as the amount threshold for your order. The threshold is referring to the total amount of your order, which enables free shipping.&lt;/p&gt;
    &lt;p&gt;For orders outside the EU there might be additional duties, taxes or charges needed to be paid by the customer. These don't have to be paid to the supplier, but to local authorities. Please check for any details with your local customs or tax authorities before ordering! But as a benefit you don't have to pay German taxes, this means you save up to 19%!&lt;lb/&gt; Due to the Brexit and the associated changes, there may be delays of several days in customs clearance on site for deliveries to the UK. This is not within our sphere of influence, so we ask for your understanding.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Country&lt;/cell&gt;
        &lt;cell&gt;Shipping Fee&lt;/cell&gt;
        &lt;cell&gt;Free Shipping From&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Albania&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Andorra&lt;/cell&gt;
        &lt;cell&gt;59,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Belgium&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bulgaria&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Denmark&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Estonia&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Faroe Islands&lt;/cell&gt;
        &lt;cell&gt;129,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Finland&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;France&lt;/cell&gt;
        &lt;cell&gt;9,99 EUR&lt;/cell&gt;
        &lt;cell&gt;120 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Greece&lt;/cell&gt;
        &lt;cell&gt;22,90 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;United Kingdom&lt;/cell&gt;
        &lt;cell&gt;9,99 EUR&lt;/cell&gt;
        &lt;cell&gt;120 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hong Kong&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;India&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ireland&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Island&lt;/cell&gt;
        &lt;cell&gt;129,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Italy&lt;/cell&gt;
        &lt;cell&gt;9,99 EUR&lt;/cell&gt;
        &lt;cell&gt;120 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Japan&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Canada&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Croatia&lt;/cell&gt;
        &lt;cell&gt;34,90 EUR&lt;/cell&gt;
        &lt;cell&gt;500 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Latvia&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Lithuania&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Luxembourg&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Macau&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Malta&lt;/cell&gt;
        &lt;cell&gt;34,90 EUR&lt;/cell&gt;
        &lt;cell&gt;500 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Macedonia&lt;/cell&gt;
        &lt;cell&gt;59,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Moldova&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Monaco&lt;/cell&gt;
        &lt;cell&gt;19,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Montenegro&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Netherlands&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Norway&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Austria&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Poland&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Portugal&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Romania&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;San Marino&lt;/cell&gt;
        &lt;cell&gt;9,99 EUR&lt;/cell&gt;
        &lt;cell&gt;120 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sweden&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Switzerland&lt;/cell&gt;
        &lt;cell&gt;13,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Serbia&lt;/cell&gt;
        &lt;cell&gt;34,90 EUR&lt;/cell&gt;
        &lt;cell&gt;500 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Singapore&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Slovakia&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Slovenia&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Spain (without Canary Islands)&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Czech Republic&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hungary&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;USA including Hawaii&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;United Arabic Emirates&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cyprus&lt;/cell&gt;
        &lt;cell&gt;34,90 EUR&lt;/cell&gt;
        &lt;cell&gt;500 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qatar&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If not stated differently in the article's description, we deliver goods in:&lt;/p&gt;
    &lt;p&gt;For orders paid in advance, the delivery time starts with receipt of the payment. Please keep in mind that there is no delivery on Sundays or on holidays.&lt;lb/&gt; For goods delivered as download, there will be no shipping fees due.&lt;lb/&gt; Access data for downloads are sent out via e-mail 1-3 working days after contract formation. For orders with advanced payment, we will deliver after receiving the payment. You can download the item by using the link sent to you via e-mail.&lt;lb/&gt; Self-pick-up of orders is not possible, unfortunately.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tuxedocomputers.com/en/Discontinuation-of-ARM-notebooks-with-Snapdragon-X-Elite-SoC.tuxedo"/><published>2025-11-21T19:46:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46008769</id><title>Pixar: The Early Days A never-before-seen 1996 interview</title><updated>2025-11-22T12:56:51.151721+00:00</updated><content>&lt;doc fingerprint="2501819f3360cdbd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pixar: The Early Days&lt;/head&gt;
    &lt;p&gt;A never-before-seen 1996 interview&lt;/p&gt;
    &lt;p&gt;November 18, 2025&lt;/p&gt;
    &lt;p&gt;To mark Toy Story‚Äôs 30th anniversary, we‚Äôre sharing a never-before-seen interview with Steve from November 22, 1996‚Äîexactly one year after the film debuted in theaters.&lt;/p&gt;
    &lt;p&gt;Toy Story was the world‚Äôs first entirely computer-animated feature-length film. An instant hit with audiences and critics, it also transformed Pixar, which went public the week after its premiere. Buoyed by Toy Story‚Äôs success, Pixar‚Äôs stock price closed at nearly double its initial offering, giving it a market valuation of approximately $1.5 billion and marking the largest IPO of 1995. The following year, Toy Story was nominated for three Academy Awards en route to winning a Special Achievement Oscar in March. In July, Pixar announced that it would close its television-commercial unit to focus primarily on feature films. By the time of the interview, the team had grown by 70 percent in less than a year; A Bug‚Äôs Life was in production; and behind the scenes, Steve was using his new leverage to renegotiate Pixar‚Äôs partnership with Disney.&lt;/p&gt;
    &lt;p&gt;In this footage, Steve reveals the long game behind Pixar‚Äôs seeming overnight success. With striking clarity, he explains how its business model gives artists and engineers a stake in their creations, and he reflects on what Disney‚Äôs hard-won wisdom taught him about focus and discipline. He also talks about the challenge of leading a team so talented that it inverts the usual hierarchy, the incentives that inspire people to stay with the company, and the deeper purpose that unites them all: to tell stories that last and put something of enduring value into the culture.&lt;/p&gt;
    &lt;p&gt;At Pixar, Steve collaborated closely with president Ed Catmull and refined a management approach centered on creating the conditions for talent to thrive. When he returned to Apple a few weeks after this interview, his experience at Pixar shaped how he saw his role as CEO: building a company on timeless ideas made new through technology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stay Hungry, Stay Foolish&lt;/head&gt;
    &lt;p&gt;Celebrating Steve‚Äôs timeless address&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stevejobsarchive.com/stories/pixar-early-days"/><published>2025-11-21T20:45:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46009591</id><title>LAPD helicopter tracker with real-time operating costs</title><updated>2025-11-22T12:56:50.884696+00:00</updated><link href="https://lapdhelicoptertracker.com/"/><published>2025-11-21T22:11:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46009894</id><title>Personal blogs are back, should niche blogs be next?</title><updated>2025-11-22T12:56:50.546923+00:00</updated><content>&lt;doc fingerprint="3b8090076bc54e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Personal blogs are back, should niche blogs be next?&lt;/head&gt;
    &lt;p&gt;20 November 2025&lt;/p&gt;
    &lt;p&gt;When it comes to blogging there are few rules. Write content that is somehow meaningful might be one of them though. I think it‚Äôs down to the individual to determine what constitutes meaningful.&lt;/p&gt;
    &lt;p&gt;In the hey-day, the so-called golden age of blogging, there were plenty of people prepared to offer definitions of meaningful, and how to write accordingly. It was natural. The web was once awash with all sorts of blogs. Likewise people who wanted to show others how to blog ‚Äúsuccessfully‚Äù.&lt;/p&gt;
    &lt;p&gt;Again, the definition of successful resided with the individual, but it was obvious this involved monetary return for some people. And why not. If you‚Äôre going to invest time and energy in creating a resource that is useful to other people, why shouldn‚Äôt you earn money, make a living even, from it?&lt;/p&gt;
    &lt;p&gt;One of these people blogging about blogging was Melbourne based Australian writer and author Darren Rowse, who launched his blogging resource Problogger in 2004. Without going into detail, because you can look it up for yourself, Rowse, as one of the earlier bloggers about blogging, did, and still does presumably, rather well for himself.&lt;/p&gt;
    &lt;p&gt;Rowse‚Äôs writing, and that of his contributors, attracted numerous readers keen to learn what they could about blogging, and the potential to make money from it.&lt;/p&gt;
    &lt;p&gt;Problogger is what‚Äôs called a niche blog. As a blog about blogging, it has a reasonably singular focus. Some people considered this niche principle to be a core tenet of blogging. There was this idea, in the earlier days of blogging, which possibly still persists, that blogs would do better if they had a speciality. Not only were search engines said to be in favour the approach, but the author of a speciality, or niche blog, would generally be considered to be an expert, of some sort, in their field.&lt;/p&gt;
    &lt;p&gt;A master of one trade, rather than the proverbial jack of all trades.&lt;/p&gt;
    &lt;p&gt;Regardless, the world was once full of blogs on every topic imaginable. It was a great time to be alive. If you wanted to learn about something in particular, there was a blog for you. Some publications featured quality content, others required a little fact checking, while some were definitely to be taken with a pinch of salt.&lt;/p&gt;
    &lt;p&gt;But niche blogging was never a format that suited everyone. There are people who did, still do, well, writing about a range, sometimes a wide range, of topics. Kottke is one of the better known blogs that does not have a specific speciality. Here, the publication itself is the speciality. To repeat what I wrote in the first sentence of this article: the rules of blogging are few.&lt;/p&gt;
    &lt;p&gt;But the facets of blogging covered at Problogger, and numerous other similar websites, usually only applied to blogs of a commercial nature. That‚Äôs not to say one or two personal bloggers might have looked at the tips posted there for increasing their audience, or improving their writing though. But in my view, personal bloggers were not, are not, part of Problogger‚Äôs target audience.&lt;/p&gt;
    &lt;p&gt;It‚Äôs been a long time since I last wrote about Problogger, let alone visited the website, maybe fifteen plus years, but a recent mention of it by Kev Quick, via ldstephens, caught my eye. But I don‚Äôt believe Rowse is being critical, in any way, of personal bloggers because they do not adhere to a niche or speciality publishing format. That‚Äôs not what Problogger, or Rowse, is about.&lt;/p&gt;
    &lt;p&gt;But this started me thinking, and writing another of my long posts.&lt;/p&gt;
    &lt;p&gt;In an age where social media, and influencers, have usurped blogs and their A-List authors, in the jostle for supremacy, it has to be wondered what role websites like Problogger still have. Only a handful of blogs generate liveable incomes today. Despite the doom and gloom though, the form has not completely died off. A backlash against social media, and a growing IndieWeb/SmallWeb community, has precipitated a revival in personal websites.&lt;/p&gt;
    &lt;p&gt;This is a largely non-commercial movement. Of course, there‚Äôs nothing wrong with personal websites. Many of us started out with them in the early days of the web. But the web was not only intended for personal journals. It was a vehicle for sharing all manner of information. The web could also empower individuals, and partnerships, to not only set up shop online, be that blogs, or quite literally shops, but potentially make a living at the same time.&lt;/p&gt;
    &lt;p&gt;But with the revival of personal blogs well underway, I think it‚Äôs time to bring niche blogs back into the fold. I‚Äôm talking about well written, quality, topic focused resources. This is material fast vanishing from the web, leaving ever diminishing options to source useful and accurate information. What are the alternatives? The misinformation morass that is social media? Being served AI generated summaries in response to search engine queries? A web choke full of AI slop?&lt;/p&gt;
    &lt;p&gt;At the same time, I‚Äôm not advocating for a return of niche blogs plastered with adverts, and popup boxes urging visitors to subscribe to say a newsletter, before they‚Äôve even had a chance to blink at what they came to read.&lt;/p&gt;
    &lt;p&gt;I‚Äôm talking about work produced by independent writers, with an interest in their subject matter, who are not backed by large media organisations, or private equity. This is bringing back reliable sources of information, that also recompenses the content writers in some way. Hopefully we‚Äôve learned a few lessons about monetisation since the earlier wave of niche blogging. We know it is possible to generate revenue without compromising the reader experience.&lt;/p&gt;
    &lt;p&gt;A resurgence in personal blogging is the first step in rebuilding a vibrant, thriving, web, or if you like, blogosphere. Now the focus needs to be on restoring the flow of accessible and trusted information.&lt;/p&gt;
    &lt;p&gt;RELATED CONTENT&lt;/p&gt;
    &lt;p&gt;blogs, history, IndieWeb, self publishing, SmallWeb, technology, trends&lt;/p&gt;
    &lt;head rend="h3"&gt;There's 2 comments on this post&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt; On 22 November 2025 at 11:34 AM, Jorge Arango said:&lt;p&gt;Thanks for sharing. I‚Äôd like to believe a resurgence of personal blogs is underway. Is there data that substantiates this claim?&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://disassociated.com/personal-blogs-back-niche-blogs-next/"/><published>2025-11-21T22:40:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46010329</id><title>How I learned Vulkan and wrote a small game engine with it (2024)</title><updated>2025-11-22T12:56:49.536226+00:00</updated><content>&lt;doc fingerprint="23684b734fb50739"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I learned Vulkan and wrote a small game engine with it&lt;/head&gt;
    &lt;p&gt;tl;dr: I learned some Vulkan and made a game engine with two small game demos in 3 months.&lt;/p&gt;
    &lt;p&gt;The code for the engine and the games can be found here: https://github.com/eliasdaler/edbr&lt;/p&gt;
    &lt;head rend="h2"&gt;Table Of Contents&lt;/head&gt;
    &lt;p&gt;This article documents my experience of learning Vulkan and writing a small game/engine with it. It took me around 3 months to do it without any previous knowledge of Vulkan (I had previous OpenGL experience and some experience with making game engines, though).&lt;/p&gt;
    &lt;p&gt;The engine wasn‚Äôt implemented as a general purpose engine, which is probably why it took me a few months (and not years) to achieve this. I started by making a small 3D game and separated reusable parts into the ‚Äúengine‚Äù afterwards. I can recommend everyone to follow the same process to not get stuck in the weeds (see ‚ÄúBike-shedding‚Äù section below for more advice).&lt;/p&gt;
    &lt;head rend="h2"&gt;Preface&lt;/head&gt;
    &lt;p&gt;I‚Äôm a professional programmer, but I‚Äôm self-taught in graphics programming. I started studying graphics programming around 1.5 years ago by learning OpenGL and writing a 3D engine in it.&lt;/p&gt;
    &lt;p&gt;The engine I wrote in Vulkan is mostly suited for smaller level-based games. I‚Äôll explain things which worked for me, but they might not be the most efficient. My implementation would probably still be a good starting point for many people.&lt;/p&gt;
    &lt;quote&gt;Hopefully, this article will help make some things about Vulkan clearer to you. But you also need to be patient. It took me months to implement what I have today and I did it by cutting corners in many places. But if a self-taught programmer like me can build something with Vulkan, then so can you!&lt;/quote&gt;
    &lt;head rend="h2"&gt;Learning graphics programming&lt;/head&gt;
    &lt;quote&gt;This is a very high level overview of how I learned some graphics programming myself. If there‚Äôs interest, I might write another article with more resources and helpful guidelines.&lt;/quote&gt;
    &lt;p&gt;If you haven‚Äôt done any graphics programming before, you should start with OpenGL. It‚Äôs much easier to learn it and not get overwhelmed by all the complexity that Vulkan has. A lot of your OpenGL and graphics programming knowledge will be useful when you start doing things with Vulkan later.&lt;/p&gt;
    &lt;p&gt;Ideally, you should at least get a textured model displayed on the screen with some simple Blinn-Phong lighting. I can also recommend doing some basic shadow mapping too, so that you learn how to render your scene from a different viewpoint and to a different render target, how to sample from depth textures and so on.&lt;/p&gt;
    &lt;p&gt;I can recommend using the following resources to learn OpenGL:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://learnopengl.com/&lt;/item&gt;
      &lt;item&gt;Anton‚Äôs OpenGL 4 Tutorials book&lt;/item&gt;
      &lt;item&gt;Thorsten Thorm√É¬§hlen‚Äôs lectures lectures (watch the first 6 videos, the rest might be a bit too advanced)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sadly, most OpenGL resources don‚Äôt teach the latest OpenGL 4.6 practices. They make writing OpenGL a lot more enjoyable. If you learn them, transitioning to Vulkan will be much easier (I only learned about OpenGL 3.3 during my previous engine development, though, so it‚Äôs not a necessity).&lt;/p&gt;
    &lt;p&gt;Here are some resources which teach you the latest OpenGL practices:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://juandiegomontoya.github.io/modern_opengl.html&lt;/item&gt;
      &lt;item&gt;https://github.com/fendevel/Guide-to-Modern-OpenGL-Functions&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;It‚Äôs also good to have some math knowledge, especially linear algebra: how to work with vectors, transformation matrices and quaternions. My favorite book about linear algebra/math is 3D Math Primer for Graphics and Game Development by F. Dunn and I. Parbery. You don‚Äôt need to read it all in one go - use it as a reference if some math in the OpenGL resources above doesn‚Äôt make sense to you.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Bike-shedding and how to avoid it&lt;/head&gt;
    &lt;p&gt;https://en.wikipedia.org/wiki/Law_of_triviality&lt;/p&gt;
    &lt;p&gt;Ah, bike-shedding‚Ä¶ Basically, it‚Äôs a harmful pattern of overthinking and over-engineering even the simplest things. It‚Äôs easy to fall into this trap when doing graphics programming (especially when doing Vulkan since you need to make many choices when implementing an engine with it).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Always ask yourself ‚ÄúDo I really need this?‚Äù, ‚ÄúWill this thing ever become a bottleneck?‚Äù.&lt;/item&gt;
      &lt;item&gt;Remember that you can always rewrite any part of your game/engine later.&lt;/item&gt;
      &lt;item&gt;Don‚Äôt implement something unless you need it right now. Don‚Äôt think ‚ÄúWell, a good engine needs X, right‚Ä¶?‚Äù.&lt;/item&gt;
      &lt;item&gt;Don‚Äôt try to make a general purpose game engine. It‚Äôs probably even better to not think about ‚Äúthe engine‚Äù at first and write a simple game.&lt;/item&gt;
      &lt;item&gt;Make a small game first - a Breakout clone, for example. Starting your engine development by doing a Minecraft clone with multiplayer support is probably not a good idea.&lt;/item&gt;
      &lt;item&gt;Be wary of people who tend to suggest complicated solutions to simple problems.&lt;/item&gt;
      &lt;item&gt;Don‚Äôt look too much at what other people do. I‚Äôve seen many over-engineered engines on GitHub - sometimes they‚Äôre that complex for a good reason (and there are years of work behind them). But you probably don‚Äôt need most of that complexity, especially for simpler games.&lt;/item&gt;
      &lt;item&gt;Don‚Äôt try to make magical wrappers around Vulkan interfaces prematurely, especially while you‚Äôre still learning Vulkan.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Get it working first. Leave ‚ÄúTODO‚Äù/‚ÄúFIXME‚Äù comments in some places. Then move on to the next thing. Try to fix ‚ÄúTODO‚Äù/‚ÄúFIXME‚Äù places only when they really become problematic or bottleneck your performance. You‚Äôll be surprised to see how many things won‚Äôt become a problem at all.&lt;/p&gt;
    &lt;quote&gt;Some of this advice only applies when you‚Äôre working alone on a hobby project. Of course, it‚Äôs much harder to rewrite something from scratch when others start to depend on it and a ‚Äútemp hack‚Äù becomes a fundamental part of the engine which is very hard to change without breaking many things.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Why Vulkan?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Ask yourself if you need to learn a graphics API at all. If your main goal is to make a game as soon as possible, then you might be better off using something like Godot or Unreal Engine.&lt;/p&gt;
      &lt;p&gt;However, there‚Äôs nothing wrong with reinventing the wheel or doing something from scratch. Especially if you do it just for fun, to get into graphics programming or to get an in-depth knowledge about how something works.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The situation with graphic APIs in 2024 is somewhat complicated. It all depends on the use case: DirectX seems like the most solid choice for most AAA games. WebGL or WebGPU are the only two choices for doing 3D graphics on the web. Metal is the go-to graphics API on macOS and iOS (though you can still do Vulkan there via MoltenVK).&lt;/p&gt;
    &lt;p&gt;My use case is simple: I want to make small 3D games for desktop platforms (Windows and Linux mostly). I also love open source technology and open standards. So, it was a choice between OpenGL and Vulkan for me.&lt;/p&gt;
    &lt;p&gt;OpenGL is a good enough choice for many small games. But it‚Äôs very unlikely that it‚Äôll get new versions in the future (so you can‚Äôt use some newest GPU capabilities like ray tracing), it‚Äôs deprecated on macOS and its future is uncertain.&lt;/p&gt;
    &lt;p&gt;WebGPU was also a possible choice. Before learning Vulkan, I learned some of it. It‚Äôs a pretty solid API, but I had some problems with it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It‚Äôs still not stable and there‚Äôs not a lot of tutorials and examples for it. This tutorial is fantastic, though.&lt;/item&gt;
      &lt;item&gt;WGSL is an okay shading language, but I just find its syntax not as pleasant as GLSL‚Äôs (note that you can write in GLSL and then load compiled SPIR-V on WebGPU native).&lt;/item&gt;
      &lt;item&gt;On desktop, it‚Äôs essentially a wrapper around other graphic APIs (DirectX, Vulkan, Metal).This introduces additional problems for me: &lt;list rend="ul"&gt;&lt;item&gt;It can‚Äôt do things some things that Vulkan or DirectX can do.&lt;/item&gt;&lt;item&gt;It has more limitations than native graphic APIs since it needs to behave similarly between them.&lt;/item&gt;&lt;item&gt;RenderDoc captures become confusing as they differ between the platforms (you can get DirectX capture on Windows and Vulkan capture on Linux) and you don‚Äôt have 1-to-1 mapping between WebGPU calls and native API calls.&lt;/item&gt;&lt;item&gt;Using Dawn and WGPU feels like using bgfx or sokol. You don‚Äôt get the same degree of control over the GPU and some of the choices/abstractions might not be the most pleasant for you.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;No bindless textures (WIP discussion here).&lt;/item&gt;
      &lt;item&gt;No push constants (WIP discussion here).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Still, I think that WebGPU is a better API than OpenGL/WebGL and can be more useful to you than Vulkan in some use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Validation errors are much better than in OpenGL/WebGL and not having global state helps a lot.&lt;/item&gt;
      &lt;item&gt;It‚Äôs also kind of similar to Vulkan in many things, so learning a bit of it before diving into Vulkan also helped me a lot.&lt;/item&gt;
      &lt;item&gt;It requires a lot less boilerplate to get things on the screen (compared to Vulkan).&lt;/item&gt;
      &lt;item&gt;You don‚Äôt have to deal with explicit synchronization which makes things much simpler.&lt;/item&gt;
      &lt;item&gt;You can make your games playable inside the browser.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Learning Vulkan&lt;/head&gt;
    &lt;p&gt;Learning Vulkan seemed like an impossible thing for me previously. It felt like you needed to have many years of AAA game graphics programming experience to be able to do things in it. You also hear people saying ‚Äúyou‚Äôre basically writing a graphics driver when writing in Vulkan‚Äù which also made Vulkan sounds like an incredibly complicated thing.&lt;/p&gt;
    &lt;p&gt;I have also checked out some engines written in Vulkan before and was further demotivated by seeing tons of scary abstractions and files named like &lt;code&gt;GPUDevice.cpp&lt;/code&gt; or &lt;code&gt;GPUAbstraction.cpp&lt;/code&gt; which had thousands of lines of scary C++ code.&lt;/p&gt;
    &lt;p&gt;The situation has changed over the years. Vulkan is not as complicated as it was before. First of all, Khronos realized that some parts of Vulkan were indeed very complex and introduced some newer features which made many things much simpler (for example, dynamic rendering). Secondly, some very useful libraries which reduce boilerplate were implemented. And finally, there are a lot of fantastic resources which make learning Vulkan much easier than it was before.&lt;/p&gt;
    &lt;p&gt;The best Vulkan learning resource which helped me get started was vkguide. If you‚Äôre starting from scratch, just go through it all (you might stop at ‚ÄúGPU driver rendering‚Äù chapter at first - many simple games probably won‚Äôt need this level of complexity)&lt;/p&gt;
    &lt;p&gt;Vulkan Lecture Series by TU Wien also nicely teaches Vulkan basics (you can probably skip ‚ÄúReal-Time Ray Tracing‚Äù chapter for now). I especially found a lecture on synchronization very helpful.&lt;/p&gt;
    &lt;p&gt;Here are some more advanced Vulkan books that also helped me:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3D Graphics Rendering Cookbook by Sergey Kosarevsky and Viktor Latypov. There is the second edition in the writing and it‚Äôs promising to be better than the first one. The second edition is not released yet, but the source code for it can be found here: https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook-Second-Edition&lt;/item&gt;
      &lt;item&gt;Mastering Graphics Programming with Vulkan by Marco Castorina, Gabriel Sassone. Very advanced book which explains some of the ‚Äúcutting edge‚Äù graphics programming concepts (I mostly read it to understand where to go further, but didn‚Äôt have time to implement most of it). The source code for it can be found here: https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs the result of my first month of learning Vulkan:&lt;/p&gt;
    &lt;p&gt;By this point I had:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;glTF model loading&lt;/item&gt;
      &lt;item&gt;Compute skinning&lt;/item&gt;
      &lt;item&gt;Frustum culling&lt;/item&gt;
      &lt;item&gt;Shadow mapping and cascaded shadow maps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, doing it for the 3rd time (I had it implemented it all in OpenGL and WebGPU before) certainly helped. Once you get to this point, Vulkan won‚Äôt seem as scary anymore.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs see how the engine works and some useful things I learned.&lt;/p&gt;
    &lt;head rend="h2"&gt;Engine overview and frame analysis&lt;/head&gt;
    &lt;p&gt;https://github.com/eliasdaler/edbr&lt;/p&gt;
    &lt;p&gt;My engine is called EDBR (Elias Daler‚Äôs Bikeshed Engine) and was initially started as a project for learning Vulkan. It quickly grew into a somewhat usable engine which I‚Äôm going to use for my further projects.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;At the time of writing this article, the source code line counts are as follows:&lt;/p&gt;
      &lt;item&gt;Engine itself: 19k lines of code&lt;/item&gt;
      &lt;item&gt;6.7k LoC related to graphics,&lt;/item&gt;
      &lt;item&gt;2k LoC are light abstractions around Vulkan&lt;/item&gt;
      &lt;item&gt;3D cat game: 4.6k LoC&lt;/item&gt;
      &lt;item&gt;2D platformer game: 1.2k LoC&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;I copy-pasted some non-graphics related stuff from my previous engine (e.g. input handling and audio system) but all of the graphics and many other core systems were rewritten from scratch. I feel like it was a good way to do it instead of trying to cram Vulkan into my old OpenGL abstractions.&lt;/p&gt;
    &lt;quote&gt;You can follow the commit history which shows how I started from clearing the screen, drawing the first triangle, drawing a textured quad and so on. It might be easier to understand the engine when it was simpler and smaller.&lt;/quote&gt;
    &lt;p&gt;Let‚Äôs see how this frame in rendered:&lt;/p&gt;
    &lt;quote&gt;Most of the steps will be explained in more detail below.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Skinning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, models with skeletal animations are skinned in the compute shader. The compute shader takes unskinned mesh and produces a buffer of vertices which are then used instead of the original mesh in later rendering steps. This allows me to treat static and skinned meshes similarly in shaders and not do skinning repeatedly in different rendering steps.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CSM (Cascaded Shadow Mapping)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I use a 4096x4096 depth texture with 3 slices for cascaded shadow mapping. The first slice looks like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Geometry + shading&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All the models are drawn and shading is calculated using the shadow map and light info. I use a PBR model which is almost identical to the one described in Physically Based Rendering in Filament. The fragment shader is quite big and does calculation for all the lights affecting the drawn mesh in one draw call:&lt;/p&gt;
    &lt;p&gt;Everything is drawn into a multi-sampled texture. Here‚Äôs how it looks after resolve:&lt;/p&gt;
    &lt;p&gt;(Open the previous two screenshots in the next tab and flip between the tabs to see the difference more clearly)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Depth resolve&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Depth resolve step is performed manually via a fragment shader. I just go through all the fragments of multi-sample depth texture and write the minimum value into the non-MS depth texture (it‚Äôll be useful in the next step).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Post FX&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some post FX is applied - right now it‚Äôs only depth fog (I use ‚Äúdepth resolve‚Äù texture from the previous step here), afterwards tone-mapping and bloom will also be done here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UI&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dialogue UI is drawn. Everything is done in one draw call (more is explained in ‚ÄúDrawing many sprites‚Äù section)&lt;/p&gt;
    &lt;p&gt;And that‚Äôs it! It‚Äôs pretty basic right now and would probably become much more complex in the future (see ‚ÄúFuture work‚Äù section).&lt;/p&gt;
    &lt;head rend="h2"&gt;General advice&lt;/head&gt;
    &lt;head rend="h3"&gt;Recommended Vulkan libraries&lt;/head&gt;
    &lt;p&gt;There are a couple of libraries which greatly improve the experience of writing Vulkan. Most of them are already used in vkguide, but I still want to highlight how helpful they were to me.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;vk-bootstrap - https://github.com/charles-lunarg/vk-bootstrap&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;vk-bootstrap simplifies a lot of Vulkan boilerplate: physical device selection, swapchain creation and so on.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt like big wrappers around graphic APIs because they tend to be very opinionated. Plus, you need to keep a mental map of ‚Äúwrapper function vs function in the API spec‚Äù in your head at all times.&lt;/p&gt;
    &lt;p&gt;Thankfully, vk-bootstrap is not like this. It mostly affects the initialization step of your program and doesn‚Äôt attempt to be a wrapper around every Vulkan function.&lt;/p&gt;
    &lt;quote&gt;When I was learning Vulkan, I started doing Vulkan from scratch, without using any 3rd party libraries. Replacing big amounts of the initialization code with vk-bootstrap was a joy. It‚Äôs really worth it.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vulkan Memory Allocator (VMA) - https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôll be honest, I used VMA without even learning about how to allocate memory in Vulkan manually. I read about it in the Vulkan spec later - I‚Äôm glad that I didn‚Äôt have to do it on my own.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;volk&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Volk was very useful for me for simplifying extension function loading. For example, if you want to use very useful &lt;code&gt;vkSetDebugUtilsObjectNameEXT&lt;/code&gt; for setting debug names for your objects (useful for RenderDoc captures and validation errors), you‚Äôll need to do this if you don‚Äôt use volk:&lt;/p&gt;
    &lt;code&gt;// store this pointer somewhere
PFN_vkSetDebugUtilsObjectNameEXT pfnSetDebugUtilsObjectNameEXT;

// during your game init
pfnSetDebugUtilsObjectNameEXT = (PFN_vkSetDebugUtilsObjectNameEXT)
    vkGetInstanceProcAddr(instance, "vkSetDebugUtilsObjectNameEXT");

// and finally in your game code
pfnSetDebugUtilsObjectNameEXT(device, ...);
&lt;/code&gt;
    &lt;p&gt;With volk, all the extensions are immediately loaded after you call &lt;code&gt;volkInitialize&lt;/code&gt; and you don‚Äôt need to store these pointers everywhere. You just include &lt;code&gt;volk.h&lt;/code&gt; and call &lt;code&gt;vkSetDebugUtilsObjectNameEXT&lt;/code&gt; - beautiful!&lt;/p&gt;
    &lt;head rend="h3"&gt;GfxDevice abstraction&lt;/head&gt;
    &lt;p&gt;I have a &lt;code&gt;GfxDevice&lt;/code&gt; class which encapsulates most of the commonly used functionality and stores many objects that you need for calling Vulkan functions (&lt;code&gt;VkDevice&lt;/code&gt;, &lt;code&gt;VkQueue&lt;/code&gt; and so on). A single &lt;code&gt;GfxDevice&lt;/code&gt; instance is created on the startup and then gets passed around.&lt;/p&gt;
    &lt;p&gt;It handles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vulkan context initialization.&lt;/item&gt;
      &lt;item&gt;Swapchain creation and management.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;beginFrame&lt;/code&gt;returns a new&lt;code&gt;VkCommandBuffer&lt;/code&gt;which is later used in all the drawing steps.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;endFrame&lt;/code&gt;does drawing to the swapchain and does sync between the frames.&lt;/item&gt;
      &lt;item&gt;Image creation and loading textures from files.&lt;/item&gt;
      &lt;item&gt;Buffer creation.&lt;/item&gt;
      &lt;item&gt;Bindless descriptor set management (see ‚ÄúBindless descriptors‚Äù section below).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That‚Äôs‚Ä¶ a lot of things. However, it‚Äôs not that big: &lt;code&gt;GfxDevice.cpp&lt;/code&gt; is only 714 lines at the time of writing this article. It‚Äôs more convenient to pass one object into the function instead of many (&lt;code&gt;VkDevice&lt;/code&gt;, &lt;code&gt;VkQueue&lt;/code&gt;, &lt;code&gt;VmaAllocator&lt;/code&gt; and so on).&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling shaders&lt;/head&gt;
    &lt;p&gt;In Vulkan, you can use any shading language which compiles to SPIR-V - that means that you can use GLSL, HLSL and others. I chose GLSL because I already knew it from my OpenGL experience.&lt;/p&gt;
    &lt;p&gt;You can pre-compile your shaders during the build step or compile them on the fly. I do it during the build so that my shader loading runtime code is simpler. I also don‚Äôt have an additional runtime dependency on the shader compiler. Also, shader errors are detected during the build step and I don‚Äôt get compile errors during the runtime.&lt;/p&gt;
    &lt;p&gt;I use glslc (from shaderc project, it‚Äôs included in Vulkan SDK) which allows you to specify a &lt;code&gt;DEPFILE&lt;/code&gt; in CMake which is incredibly useful when you use shader includes. If you change a shader file, all files which include it are recompiled automatically. Without the &lt;code&gt;DEPFILE&lt;/code&gt;, CMake won‚Äôt be able to see which files shader files need to be recompiled and will only recompile the file which was changed.&lt;/p&gt;
    &lt;p&gt;My CMake script for building shaders looks like this:&lt;/p&gt;
    &lt;code&gt;function (target_shaders target shaders)
    set(SHADERS_BUILD_DIR "${CMAKE_CURRENT_BINARY_DIR}/shaders")
    file(MAKE_DIRECTORY "${SHADERS_BUILD_DIR}")
    foreach (SHADER_PATH ${SHADERS})
        get_filename_component(SHADER_FILENAME "${SHADER_PATH}" NAME)
        set(SHADER_SPIRV_PATH "${SHADERS_BUILD_DIR}/${SHADER_FILENAME}.spv")
        set(DEPFILE "${SHADER_SPIRV_PATH}.d")
        add_custom_command(
          COMMENT "Building ${SHADER_FILENAME}"
          OUTPUT "${SHADER_SPIRV_PATH}"
          COMMAND ${GLSLC} "${SHADER_PATH}" -o "${SHADER_SPIRV_PATH}" -MD -MF ${DEPFILE} -g
          DEPENDS "${SHADER_PATH}"
          DEPFILE "${DEPFILE}"
        )
        list(APPEND SPIRV_BINARY_FILES ${SHADER_SPIRV_PATH})
    endforeach()

    set(shaders_target_name "${target}_build_shaders")
    add_custom_target(${shaders_target_name}
      DEPENDS ${SPIRV_BINARY_FILES}
    )
    add_dependencies(${target} ${shaders_target_name})
endfunction()
&lt;/code&gt;
    &lt;p&gt;and then in the main CMakeLists file:&lt;/p&gt;
    &lt;code&gt;set(SHADERS
    skybox.frag
    skinning.comp
    ... // etc
)

# prepend shaders directory path
get_target_property(EDBR_SOURCE_DIR edbr SOURCE_DIR)
set(EDBR_SHADERS_DIR "${EDBR_SOURCE_DIR}/src/shaders/")
list(TRANSFORM SHADERS PREPEND "${EDBR_SHADERS_DIR}")

target_shaders(game ${SHADERS})
&lt;/code&gt;
    &lt;p&gt;Now, when you build a &lt;code&gt;game&lt;/code&gt; target, shaders get built automatically and the resulting SPIR-V files are put into the binary directory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Push constants, descriptor sets and bindless descriptors&lt;/head&gt;
    &lt;p&gt;Passing data to shaders in OpenGL is much simpler than it is in Vulkan. In OpenGL, you could just do this:&lt;/p&gt;
    &lt;p&gt;In shader:&lt;/p&gt;
    &lt;code&gt;uniform float someFloat;
&lt;/code&gt;
    &lt;p&gt;In C++ code:&lt;/p&gt;
    &lt;code&gt;const auto loc = glGetUniformLocation(shader, "someFloat");
glUseProgram(shader);
glUniform1f(loc, 42.f);
&lt;/code&gt;
    &lt;p&gt;You can also use explicit uniform location like this.&lt;/p&gt;
    &lt;p&gt;In shader:&lt;/p&gt;
    &lt;code&gt;layout(location = 20) uniform float someFloat;
&lt;/code&gt;
    &lt;p&gt;In code:&lt;/p&gt;
    &lt;code&gt;const auto loc = 20;
glUniform1f(loc, 42.f);
&lt;/code&gt;
    &lt;p&gt;In Vulkan, you need to group your uniforms into ‚Äúdescriptor sets‚Äù:&lt;/p&gt;
    &lt;code&gt;// set 0
layout (set = 0, binding = 0) uniform float someFloat;
layout (set = 0, binding = 1) uniform mat4 someMatrix;
// set 1
layout (set = 1, binding = 0) uniform float someOtherFloat;
... // etc.
&lt;/code&gt;
    &lt;p&gt;Now, this makes things a lot more complicated, because you need to specify descriptor set layout beforehand, use descriptor set pools and allocate descriptor sets with them, do the whole &lt;code&gt;VkWriteDescriptorSet&lt;/code&gt; + &lt;code&gt;vkUpdateDescriptorSets&lt;/code&gt; thing, call &lt;code&gt;vkCmdBindDescriptorSets&lt;/code&gt; for each descriptor set and so on.&lt;/p&gt;
    &lt;p&gt;I‚Äôll explain later how I avoided using descriptor sets by using bindless descriptors and buffer device access. Basically, I only have one ‚Äúglobal‚Äù descriptor set for bindless textures and samplers, and that‚Äôs it. Everything else is passed via push constants which makes everything much easier to handle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pipeline pattern&lt;/head&gt;
    &lt;p&gt;I separate drawing steps into ‚Äúpipeline‚Äù classes.&lt;/p&gt;
    &lt;p&gt;Most of them look like this:&lt;/p&gt;
    &lt;code&gt;class PostFXPipeline {
public:
    void init(GfxDevice&amp;amp; gfxDevice, VkFormat drawImageFormat);
    void cleanup(VkDevice device);

    void draw(
        VkCommandBuffer cmd,
        GfxDevice&amp;amp; gfxDevice,
        const GPUImage&amp;amp; drawImage,
        const GPUImage&amp;amp; depthImage,
        const GPUBuffer&amp;amp; sceneDataBuffer);

private:
    VkPipelineLayout pipelineLayout;
    VkPipeline pipeline;

    struct PushConstants {
        VkDeviceAddress sceneDataBuffer;
        std::uint32_t drawImageId;
        std::uint32_t depthImageId;
    };
};
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;init&lt;/code&gt;loads needed shaders and initializes&lt;code&gt;pipeline&lt;/code&gt;and&lt;code&gt;pipelineLayout&lt;/code&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;void PostFXPipeline::init(GfxDevice&amp;amp; gfxDevice, VkFormat drawImageFormat)
{
    const auto&amp;amp; device = gfxDevice.getDevice();

    const auto pcRange = VkPushConstantRange{
        .stageFlags = VK_SHADER_STAGE_FRAGMENT_BIT,
        .offset = 0,
        .size = sizeof(PushConstants),
    };

    const auto layouts = std::array{gfxDevice.getBindlessDescSetLayout()};
    const auto pushConstantRanges = std::array{pcRange};
    pipelineLayout = vkutil::createPipelineLayout(device, layouts, pushConstantRanges);

    const auto vertexShader =
        vkutil::loadShaderModule("shaders/fullscreen_triangle.vert.spv", device);
    const auto fragShader =
        vkutil::loadShaderModule("shaders/postfx.frag.spv", device);
    pipeline = PipelineBuilder{pipelineLayout}
                   .setShaders(vertexShader, fragShader)
                   .setInputTopology(VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST)
                   .setPolygonMode(VK_POLYGON_MODE_FILL)
                   .disableCulling()
                   .setMultisamplingNone()
                   .disableBlending()
                   .setColorAttachmentFormat(drawImageFormat)
                   .disableDepthTest()
                   .build(device);
    vkutil::addDebugLabel(device, pipeline, "postFX pipeline");

    vkDestroyShaderModule(device, vertexShader, nullptr);
    vkDestroyShaderModule(device, fragShader, nullptr);
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;init&lt;/code&gt; function is usually called once during the engine initialization. &lt;code&gt;PipelineBuilder&lt;/code&gt; abstraction is described in vkguide here. I modified it a bit to use the Builder pattern to be able to chain the calls.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;cleanup&lt;/code&gt;does all the needed cleanup. It usually simply destroys the pipeline and its layout:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;void PostFXPipeline::cleanup(VkDevice device)
{
    vkDestroyPipeline(device, pipeline, nullptr);
    vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;draw&lt;/code&gt;is called each frame and all the needed inputs are passed as arguments. It‚Äôs assumed that the sync is performed outside of the&lt;code&gt;draw&lt;/code&gt;call (see ‚ÄúSynchronization‚Äù section below). Some pipelines are only called once per frame - some either take&lt;code&gt;std::vector&lt;/code&gt;of objects to draw or are called like this:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;for (const auto&amp;amp; mesh : meshes) {
    somePipeline.draw(cmd, gfxDevice, mesh, ...);
}
&lt;/code&gt;
    &lt;p&gt;The typical &lt;code&gt;draw&lt;/code&gt; function looks like this:&lt;/p&gt;
    &lt;code&gt;void PostFXPipeline::draw(
    VkCommandBuffer cmd,
    GfxDevice&amp;amp; gfxDevice,
    const GPUImage&amp;amp; drawImage,
    const GPUImage&amp;amp; depthImage,
    const GPUBuffer&amp;amp; sceneDataBuffer)
{
    // Bind the pipeline
    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_GRAPHICS, pipeline);

    // Bind the bindless descriptor set
    gfxDevice.bindBindlessDescSet(cmd, pipelineLayout);

    // Handle push constants
    const auto pcs = PushConstants{
        // BDA - explained below
        .sceneDataBuffer = sceneDataBuffer.address,
        // bindless texture ids - no need for desc. sets!
        // explained below
        .drawImageId = drawImage.getBindlessId(),
        .depthImageId = depthImage.getBindlessId(),
    };
    vkCmdPushConstants(
        cmd, pipelineLayout, VK_SHADER_STAGE_FRAGMENT_BIT, 0, sizeof(PushConstants), &amp;amp;pcs);

    // Finally, do some drawing. Here we're drawing a fullscreen triangle
    // to do a full-screen effect.
    vkCmdDraw(cmd, 3, 1, 0, 0);
}
&lt;/code&gt;
    &lt;p&gt;Note another thing: it‚Äôs assumed that &lt;code&gt;draw&lt;/code&gt; is called between &lt;code&gt;vkCmdBeginRendering&lt;/code&gt; and &lt;code&gt;vkCmdEndRendering&lt;/code&gt; - the render pass itself doesn‚Äôt care what texture it renders to - the caller of &lt;code&gt;draw&lt;/code&gt; is responsible for that. It makes things simpler and allows you to do several draws to the same render target, e.g.:&lt;/p&gt;
    &lt;code&gt;// handy wrapper for creating VkRenderingInfo
const auto renderInfo = vkutil::createRenderingInfo({
    .renderExtent = drawImage.getExtent2D(),
    .colorImageView = drawImage.imageView,
    .colorImageClearValue = glm::vec4{0.f, 0.f, 0.f, 1.f},
    .depthImageView = depthImage.imageView,
    .depthImageClearValue = 0.f,
    // for MSAA
    .resolveImageView = resolveImage.imageView,
});

vkCmdBeginRendering(cmd, &amp;amp;renderInfo.renderingInfo);

// draw meshes
for (const auto&amp;amp; mesh : meshesToDraw) {
    meshPipeline.draw(cmd, gfxDevice, mesh, ...);
}
// draw sky
skyboxPipeline.draw(cmd, gfxDevice, camera);

vkCmdEndRendering(cmd);
&lt;/code&gt;
    &lt;quote&gt;I use&lt;code&gt;VK_KHR_dynamic_rendering&lt;/code&gt;everywhere. I don‚Äôt use Vulkan render passes and subpasses at all. I‚Äôve heard that they‚Äôre more efficient on tile-based GPUs, but I don‚Äôt care about mobile support for now.&lt;code&gt;VK_KHR_dynamic_rendering&lt;/code&gt;just makes everything much easier.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Using programmable vertex pulling (PVP) + buffer device address (BDA)&lt;/head&gt;
    &lt;p&gt;I have one vertex type for all the meshes. It looks like this:&lt;/p&gt;
    &lt;code&gt;struct Vertex {
    vec3 position;
    float uv_x;
    vec3 normal;
    float uv_y;
    vec4 tangent;
};
&lt;/code&gt;
    &lt;quote&gt;Of course, you can greatly optimize it using various methods, but it‚Äôs good enough for me for now. The&lt;code&gt;uv_x&lt;/code&gt;/&lt;code&gt;uv_y&lt;/code&gt;separation comes from vkguide - I think it‚Äôs a nice idea to get good alignment and not waste any bytes&lt;/quote&gt;
    &lt;p&gt;The vertices are accessed in the shader like this:&lt;/p&gt;
    &lt;code&gt;layout (buffer_reference, std430) readonly buffer VertexBuffer {
    Vertex vertices[];
};

layout (push_constant, scalar) uniform constants
{
    VertexBuffer vertexBuffer;
    ... // other stuff
} pcs;

void main()
{
    Vertex v = pcs.vertexBuffer.vertices[gl_VertexIndex];
    ...
}
&lt;/code&gt;
    &lt;p&gt;PVP frees you from having to define vertex format (no more VAOs like in OpenGL or &lt;code&gt;VkVertexInputBindingDescription&lt;/code&gt; + &lt;code&gt;VkVertexInputAttributeDescription&lt;/code&gt; in Vulkan). BDA also frees you from having to bind a buffer to a descriptor set - you just pass an address to your buffer which contains vertices in push constants and that‚Äôs it.&lt;/p&gt;
    &lt;code&gt;
  Also note the  scalar layout for push constants. I use it for all the buffers too. Compared to ‚Äústd430‚Äù layout, it makes alignment a lot more easy to handle - it almost works the same as in C++ and greatly reduces the need for ‚Äúpadding‚Äù members in C++ structs.
&lt;/code&gt;
    &lt;head rend="h3"&gt;Bindless descriptors&lt;/head&gt;
    &lt;p&gt;Textures were painful to work with even in OpenGL - you had ‚Äútexture slots‚Äù which were awkward to work with. You couldn‚Äôt just sample any texture from the shader if it wasn‚Äôt bound to a texture slot beforehand. &lt;code&gt;ARB_bindless_texture&lt;/code&gt; changed that and made many things easier.&lt;/p&gt;
    &lt;p&gt;Vulkan doesn‚Äôt have the exact same functionality, but it has something similar. You can create big descriptor sets which look like this:&lt;/p&gt;
    &lt;code&gt;// bindless.glsl
layout (set = 0, binding = 0) uniform texture2D textures[];
...
layout (set = 0, binding = 1) uniform sampler samplers[];
&lt;/code&gt;
    &lt;p&gt;You‚Äôll need to maintain a list of all your textures using some ‚Äúimage manager‚Äù and when a new texture is loaded, you need to insert it into the &lt;code&gt;textures&lt;/code&gt; array. The index at which you inserted it becomes a bindless ‚Äútexture id‚Äù which then can be used to sample it in shaders. Now you can pass these ids in your push constants like this:&lt;/p&gt;
    &lt;code&gt;layout (push_constant, scalar) uniform constants
{
  uint textureId;
  ...
} pcs;
&lt;/code&gt;
    &lt;p&gt;and then you can sample your texture in the fragment shader like this:&lt;/p&gt;
    &lt;code&gt;// bindless.glsl
#define NEAREST_SAMPLER_ID 0
...

vec4 sampleTexture2DNearest(uint texID, vec2 uv) {
    return texture(nonuniformEXT(sampler2D(textures[texID], samplers[NEAREST_SAMPLER_ID])), uv);
}

// shader.frag
vec4 color = sampleTexture2DNearest(pcs.textureId, inUV);
&lt;/code&gt;
    &lt;p&gt;Two things to note:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I chose separate image samplers so that I could sample any texture using different samplers. Common samplers (nearest, linear with anisotropy, depth texture samplers) are created and put into &lt;code&gt;samplers&lt;/code&gt;array on the startup.&lt;/item&gt;
      &lt;item&gt;The wrapper function makes the process of sampling a lot more convenient.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
  The placement of  nonuniformEXT is somewhat tricky and is explained very well here.
&lt;/code&gt;
    &lt;p&gt;I use bindless ids for the mesh material buffer which looks like this:&lt;/p&gt;
    &lt;code&gt;struct MaterialData {
    vec4 baseColor;
    vec4 metallicRoughnessEmissive;
    uint diffuseTex;
    uint normalTex;
    uint metallicRoughnessTex;
    uint emissiveTex;
};

layout (buffer_reference, std430) readonly buffer MaterialsBuffer {
    MaterialData data[];
} materialsBuffer;
&lt;/code&gt;
    &lt;p&gt;Now I can only pass material ID in my push constants and then sample texture like this in the fragment shader:&lt;/p&gt;
    &lt;code&gt;MaterialData material = materials[pcs.materialID];
vec4 diffuse = sampleTexture2DLinear(material.diffuseTex, inUV);
...
&lt;/code&gt;
    &lt;p&gt;Neat! No more bulky descriptor sets, just one int per material in the push constants.&lt;/p&gt;
    &lt;p&gt;You can also put different texture types into the same set like this (this is needed for being able to access textures of types other than &lt;code&gt;texture2D&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;layout (set = 0, binding = 0) uniform texture2D textures[];
layout (set = 0, binding = 0) uniform texture2DMS texturesMS[];
layout (set = 0, binding = 0) uniform textureCube textureCubes[];
layout (set = 0, binding = 0) uniform texture2DArray textureArrays[];
&lt;/code&gt;
    &lt;p&gt;And here‚Äôs how you can sample &lt;code&gt;textureCube&lt;/code&gt; with a linear sampler (note that we use &lt;code&gt;textureCubes&lt;/code&gt; here instead of &lt;code&gt;textures&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;vec4 sampleTextureCubeLinear(uint texID, vec3 p) {
    return texture(nonuniformEXT(samplerCube(textureCubes[texID], samplers[NEAREST_SAMPLER_ID])), p);
}
&lt;/code&gt;
    &lt;p&gt;Here‚Äôs a very good article on using bindless textures in Vulkan:&lt;/p&gt;
    &lt;p&gt;https://jorenjoestar.github.io/post/vulkan_bindless_texture/&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling dynamic data which needs to be uploaded every frame&lt;/head&gt;
    &lt;p&gt;I find it useful to pre-allocate big arrays of things and push stuff to them in every frame. Basically, you can pre-allocate an array of N structs (or matrices) and then start at index 0 at each new frame and push things to it from the CPU. Then, you can access all these items in your shaders. For example, I have all joint matrices stored in one big &lt;code&gt;mat4&lt;/code&gt; array and the skinning compute shader accesses joint matrices of a particular mesh using start index passed via push constants (more about it will be explained later).&lt;/p&gt;
    &lt;p&gt;Here are two ways of doing this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;Have N buffers on GPU and swap between them.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;vkguide explains the concept of ‚Äúin flight‚Äù frames pretty well. To handle this parallelism properly, you need to have one buffer for the ‚Äúcurrently drawing‚Äù frame and one buffer for ‚Äúcurrently recording new drawing commands‚Äù frame to not have races. (If you have more frames in flight, you‚Äôll need to allocate more than 2 buffers)&lt;/p&gt;
    &lt;p&gt;This means that you need to preallocate 2 buffers on GPU. You write data from CPU to GPU to the first buffer during the first frame. While you record the second frame, GPU reads from the first buffer while you write new data to the second buffer. On the third frame, GPU reads from the second buffer and you write new info to the first buffer‚Ä¶ and so on.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;One buffer on GPU and N ‚Äústaging‚Äù buffers on CPU&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This might be useful if you need to conserve some memory on the GPU.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs see how it works in my engine:&lt;/p&gt;
    &lt;code&gt;class NBuffer {
public:
    void init(
        GfxDevice&amp;amp; gfxDevice,
        VkBufferUsageFlags usage,
        std::size_t dataSize,
        std::size_t numFramesInFlight,
        const char* label);

    void cleanup(GfxDevice&amp;amp; gfxDevice);

    void uploadNewData(
        VkCommandBuffer cmd,
        std::size_t frameIndex,
        void* newData,
        std::size_t dataSize,
        std::size_t offset = 0);

    const GPUBuffer&amp;amp; getBuffer() const { return gpuBuffer; }

private:
    std::size_t framesInFlight{0};
    std::size_t gpuBufferSize{0};
    std::vector&amp;lt;GPUBuffer&amp;gt; stagingBuffers;
    GPUBuffer gpuBuffer;
    bool initialized{false};
};

void NBuffer::init(
    GfxDevice&amp;amp; gfxDevice,
    VkBufferUsageFlags usage,
    std::size_t dataSize,
    std::size_t numFramesInFlight,
    const char* label)
{
    ...

    gpuBuffer = gfxDevice.createBuffer(
        dataSize, usage | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VMA_MEMORY_USAGE_AUTO_PREFER_DEVICE);
    vkutil::addDebugLabel(gfxDevice.getDevice(), gpuBuffer.buffer, label);

    for (std::size_t i = 0; i &amp;lt; numFramesInFlight; ++i) {
        stagingBuffers.push_back(gfxDevice.createBuffer(
            dataSize, usage | VK_BUFFER_USAGE_TRANSFER_SRC_BIT, VMA_MEMORY_USAGE_AUTO_PREFER_HOST));
    }

    ...
}
&lt;/code&gt;
    &lt;p&gt;Note how staging buffers are created using VMA‚Äôs &lt;code&gt;PREFER_HOST&lt;/code&gt; flag and the ‚Äúmain‚Äù buffer from which we read in the shader is using the &lt;code&gt;PREFER_DEVICE&lt;/code&gt; flag.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how new data is uploaded (full implementation):&lt;/p&gt;
    &lt;code&gt;void NBuffer::uploadNewData(
    VkCommandBuffer cmd,
    std::size_t frameIndex,
    void* newData,
    std::size_t dataSize,
    std::size_t offset) const
{
    assert(initialized);
    assert(frameIndex &amp;lt; framesInFlight);
    assert(offset + dataSize &amp;lt;= gpuBufferSize &amp;amp;&amp;amp; "NBuffer::uploadNewData: out of bounds write");

    if (dataSize == 0) {
        return;
    }

    // sync with previous read
    ... // READ BARRIER CODE HERE

    auto&amp;amp; staging = stagingBuffers[frameIndex];
    auto* mappedData = reinterpret_cast&amp;lt;std::uint8_t*&amp;gt;(staging.info.pMappedData);
    memcpy((void*)&amp;amp;mappedData[offset], newData, dataSize);

    const auto region = VkBufferCopy2{
        .sType = VK_STRUCTURE_TYPE_BUFFER_COPY_2,
        .srcOffset = (VkDeviceSize)offset,
        .dstOffset = (VkDeviceSize)offset,
        .size = dataSize,
    };
    const auto bufCopyInfo = VkCopyBufferInfo2{
        .sType = VK_STRUCTURE_TYPE_COPY_BUFFER_INFO_2,
        .srcBuffer = staging.buffer,
        .dstBuffer = gpuBuffer.buffer,
        .regionCount = 1,
        .pRegions = &amp;amp;region,
    };

    vkCmdCopyBuffer2(cmd, &amp;amp;bufCopyInfo);

    // sync with write
    ... // WRITE BARRIER CODE HERE
}
&lt;/code&gt;
    &lt;p&gt;I‚Äôd go with the first approach for most cases (more data on GPU, but no need for manual sync) unless you need to conserve GPU memory for some reason. I‚Äôve found no noticeable difference in performance between two approaches, but it might matter if you are uploading huge amounts of data to GPU on each frame.&lt;/p&gt;
    &lt;head rend="h3"&gt;Destructors, deletion queue and cleanup&lt;/head&gt;
    &lt;p&gt;Now, this might be somewhat controversial‚Ä¶ but I didn‚Äôt find much use of the deletion queue pattern used in vkguide. I don‚Äôt really need to allocated/destroy new objects on every frame.&lt;/p&gt;
    &lt;p&gt;Using C++ destructors for Vulkan object cleanup is not very convenient either. You need to wrap everything in custom classes, add move constructors and move &lt;code&gt;operator=&lt;/code&gt;‚Ä¶ It adds an additional layer of complexity.&lt;/p&gt;
    &lt;p&gt;In most cases, the cleanup of Vulkan objects happens in one place - and you don‚Äôt want to accidentally destroy some in-use object mid-frame by accidentally destroying some wrapper object.&lt;/p&gt;
    &lt;p&gt;It‚Äôs also harder to manage lifetimes when you have cleanup in happening in the destructor. For example, suppose you have a case like this:&lt;/p&gt;
    &lt;code&gt;struct SomeClass {
    SomeOtherClass b;

    void init() {
        ...
    }

    void cleanup() {
        ...
    }
}
&lt;/code&gt;
    &lt;p&gt;If you want to cleanup &lt;code&gt;SomeOtherClass&lt;/code&gt; resources (e.g. the instance of &lt;code&gt;SomeOtherClass&lt;/code&gt; has a &lt;code&gt;VkPipeline&lt;/code&gt; object) during &lt;code&gt;SomeClass::cleanup&lt;/code&gt;, you can‚Äôt do that if the cleanup of &lt;code&gt;SomeOtherClass&lt;/code&gt; is performed in its destructor.&lt;/p&gt;
    &lt;p&gt;Of course, you can do this:&lt;/p&gt;
    &lt;code&gt;struct SomeClass {
    std::unique_ptr&amp;lt;SomeOtherClass&amp;gt; b;

    void init() {
        b = std::make_unique&amp;lt;SomeOtherClass&amp;gt;();
        ...
    }

    void cleanup() {
        b.reset();
        ...
    }
}
&lt;/code&gt;
    &lt;p&gt;‚Ä¶ but I don‚Äôt like how it introduces a dynamic allocation and requires you to do write more code (and it‚Äôs not that much different from calling a &lt;code&gt;cleanup&lt;/code&gt; function manually).&lt;/p&gt;
    &lt;p&gt;Right now, I prefer to clean up stuff directly, e.g.&lt;/p&gt;
    &lt;code&gt;class SkyboxPipeline {
public:
    void cleanup(VkDevice device) {
        vkDestroyPipeline(device, pipeline, nullptr);
        vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
    }

private:
    VkPipelineLayout pipelineLayout;
    VkPipeline pipeline;
    ...
}

// in GameRenderer.cpp:
void GameRenderer::cleanup(VkDevice device) {
    ...
    skyboxPipeline.cleanup(device);
    ...
}
&lt;/code&gt;
    &lt;p&gt;This approach is not perfect - first of all, it‚Äôs easy to forget to call &lt;code&gt;cleanup&lt;/code&gt; function, This is not a huge problem since you get a validation error in case you forget to cleanup some Vulkan resources on shutdown:&lt;/p&gt;
    &lt;code&gt;Validation Error: [ VUID-vkDestroyDevice-device-05137 ] Object 0: handle = 0x4256c1000000005d, type = VK_OBJECT_TYPE_PIPELINE_LAYOUT; | MessageID = 0x4872eaa0 | vkCreateDevice():  OBJ ERROR : For VkDevice 0x27bd530[], VkPipelineLayout 0x4256c1000000005d[] has not been destroyed. The Vulkan spec states: All child objects created on device must have been destroyed prior to destroying device (https://vulkan.lunarg.com/doc/view/1.3.280.1/linux/1.3-extensions/vkspec.html#VUID-vkDestroyDevice-device-05137)
&lt;/code&gt;
    &lt;p&gt;VMA also triggers asserts if you forget to free some buffer/image allocated with it.&lt;/p&gt;
    &lt;p&gt;I find it convenient to have all the Vulkan cleanup happening explicitly in one place. It makes it easy to track when the objects get destroyed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Synchronization&lt;/head&gt;
    &lt;p&gt;Synchronization in Vulkan is difficult. OpenGL and WebGPU do it for you - if you read from some texture/buffer, you know that it will have the correct data and you won‚Äôt get problems with data races. With Vulkan, you need to be explicit and this is usually where things tend to get complicated.&lt;/p&gt;
    &lt;p&gt;Right now I manage most of the complexities of sync manually in one place. I separate my drawing into ‚Äúpasses‚Äù/pipelines (as described above) and then insert barriers between them. For example, the skinning pass writes new vertex data into GPU memory. Shadow mapping pass reads this data to render skinned meshes into the shadow map. Sync in my code looks like this:&lt;/p&gt;
    &lt;code&gt;// do skinning in compute shader
for (const auto&amp;amp; mesh : skinnedMeshes) {
    skinningPass.doSkinning(gfxDevice, mesh);
}

{
    // Sync skinning with CSM
    // This is a "fat" barrier and you can potentially optimize it
    // by specifying all the buffers that the next pass will read from
    const auto memoryBarrier = VkMemoryBarrier2{
        .sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER_2,
        .srcStageMask = VK_PIPELINE_STAGE_2_COMPUTE_SHADER_BIT,
        .srcAccessMask = VK_ACCESS_2_SHADER_WRITE_BIT,
        .dstStageMask = VK_PIPELINE_STAGE_2_VERTEX_SHADER_BIT,
        .dstAccessMask = VK_ACCESS_2_MEMORY_READ_BIT,
    };
    const auto dependencyInfo = VkDependencyInfo{
        .sType = VK_STRUCTURE_TYPE_DEPENDENCY_INFO,
        .memoryBarrierCount = 1,
        .pMemoryBarriers = &amp;amp;memoryBarrier,
    };
    vkCmdPipelineBarrier2(cmd, &amp;amp;dependencyInfo);
}

// do shadow mapping
shadowMappingPass.draw(gfxDevice, ...);
&lt;/code&gt;
    &lt;p&gt;Of course, this can be automated/simplified using render graphs. This is something that I might implement in the future. Right now I‚Äôm okay with doing manual sync. vkconfig‚Äôs ‚Äúsynchronization‚Äù validation layer also helps greatly in finding sync errors.&lt;/p&gt;
    &lt;p&gt;The following resources were useful for understanding synchronization:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;https://themaister.net/blog/2019/08/14/yet-another-blog-explaining-vulkan-synchronization/&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;More implementation notes&lt;/head&gt;
    &lt;head rend="h3"&gt;Drawing many sprites&lt;/head&gt;
    &lt;p&gt;With bindless textures, it‚Äôs easy to draw many sprites using one draw call without having to allocate vertex buffers at all.&lt;/p&gt;
    &lt;p&gt;First of all, you can emit vertex coordinates and UVs using &lt;code&gt;gl_VertexIndex&lt;/code&gt; in your vertex shader like this:&lt;/p&gt;
    &lt;code&gt;void main()
{
    uint b = 1 &amp;lt;&amp;lt; (gl_VertexIndex % 6);
    vec2 baseCoord = vec2((0x1C &amp;amp; b) != 0, (0xE &amp;amp; b) != 0);
    ...
}
&lt;/code&gt;
    &lt;p&gt;This snippet produces this set of values:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;gl_VertexIndex&lt;/cell&gt;
        &lt;cell role="head"&gt;baseCoord&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;(0,0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;(0,1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;(1,1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;(1,1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;(1,0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;(0,0)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All the sprite draw calls are combined into &lt;code&gt;SpriteDrawBuffer&lt;/code&gt; which looks like this in GLSL:&lt;/p&gt;
    &lt;code&gt;struct SpriteDrawCommand {
    mat4 transform; // could potentially be mat2x2...
    vec2 uv0; // top-left uv coord
    vec2 uv1; // bottom-right uv coord
    vec4 color; // color by which texture is multiplied
    uint textureID; // sprite texture
    uint shaderID; // explained below
    vec2 padding; // padding to satisfy "scalar" requirements
};

layout (buffer_reference, scalar) readonly buffer SpriteDrawBuffer {
    SpriteDrawCommand commands[];
};
&lt;/code&gt;
    &lt;p&gt;On CPU/C++ side, it looks almost the same:&lt;/p&gt;
    &lt;code&gt;struct SpriteDrawCommand {
    glm::mat4 transform;
    glm::vec2 uv0; // top-left uv coordinate
    glm::vec2 uv1; // bottom-right uv coodinate
    LinearColor color; // color by which texture is multiplied by
    std::uint32_t textureId; // sprite texture
    std::uint32_t shaderId; // explained below
    glm::vec2 padding; // padding
};

std::vector&amp;lt;SpriteDrawCommand&amp;gt; spriteDrawCommands;
&lt;/code&gt;
    &lt;p&gt;I create two fixed size buffers on the GPU and then upload the contents of &lt;code&gt;spriteDrawCommands&lt;/code&gt; (using techniques described above in the ‚ÄúHandling dynamic data‚Äù section).&lt;/p&gt;
    &lt;p&gt;The sprite renderer is used like this:&lt;/p&gt;
    &lt;code&gt;// record commands
renderer.beginDrawing();
{
    renderer.drawSprite(sprite, pos);
    renderer.drawText(font, "Hello");
    renderer.drawRect(...);
}
renderer.endDrawing();

// do actual drawing later:
renderer.draw(cmd, gfxDevice, ...);
&lt;/code&gt;
    &lt;code&gt;
  The same renderer also draws text, rectangles and lines in my engine. For example, the text is just N ‚Äúdraw sprite‚Äù commands for a string composed of N glyphs. Solid color rectangles and lines are achieved by using a 1x1 pixel white texture and multiplying it by  SpriteCommand::color in the fragment shader.
&lt;/code&gt;
    &lt;p&gt;And finally, here‚Äôs how the command to do the drawing looks like inside &lt;code&gt;SpriteRenderer::draw&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;vkCmdDraw(cmd, 6, spriteDrawCommands.size(), 0, 0);
// 6 vertices per instance, spriteDrawCommands.size() instances in total
&lt;/code&gt;
    &lt;p&gt;The complete sprite.vert looks like this:&lt;/p&gt;
    &lt;code&gt;#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_buffer_reference : require

#include "sprite_commands.glsl"

layout (push_constant) uniform constants
{
    mat4 viewProj; // 2D camera matrix
    SpriteDrawBuffer drawBuffer; // where sprite draw commands are stored
} pcs;

layout (location = 0) out vec2 outUV;
layout (location = 1) out vec4 outColor;
layout (location = 2) flat out uint textureID;
layout (location = 3) flat out uint shaderID;

void main()
{
    uint b = 1 &amp;lt;&amp;lt; (gl_VertexIndex % 6);
    vec2 baseCoord = vec2((0x1C &amp;amp; b) != 0, (0xE &amp;amp; b) != 0);

    SpriteDrawCommand command = pcs.drawBuffer.commands[gl_InstanceIndex];

    gl_Position = pcs.viewProj * command.transform * vec4(baseCoord, 0.f, 1.f);
    outUV = (1.f - baseCoord) * command.uv0 + baseCoord * command.uv1;
    outColor = command.color;
    textureID = command.textureID;
    shaderID = command.shaderID;
}
&lt;/code&gt;
    &lt;p&gt;All the parameters of the sprite draw command are self-explanatory, but &lt;code&gt;shaderID&lt;/code&gt; needs a bit of clarification. Currently, I use it to branch inside the fragment shader:&lt;/p&gt;
    &lt;code&gt;...

#define SPRITE_SHADER_ID 0
#define TEXT_SHADER_ID   1

void main()
{
    vec4 texColor = sampleTexture2DNearest(textureID, inUV);

    // text drawing is performed differently...
    if (shaderID == TEXT_SHADER_ID) {
        // glyph atlas uses single-channel texture
        texColor = vec4(1.0, 1.0, 1.0, texColor.r);
    }

    if (texColor.a &amp;lt; 0.1) {
        discard;
    }

    outColor = inColor * texColor;
}
&lt;/code&gt;
    &lt;p&gt;This allows me to draw sprites differently depending on this ID without having to change pipelines. Of course, it can be potentially bad for the performance. This can be improved by drawing sprites with the same shader ID in batches. You‚Äôll only need to switch pipelines when you encounter a draw command with a different shader ID.&lt;/p&gt;
    &lt;p&gt;The sprite renderer is very efficient: it can draw 10 thousand sprites in just 315 microseconds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compute skinning&lt;/head&gt;
    &lt;p&gt;I do skinning for skeletal animation in a compute shader. This allows me to have the same vertex format for all the meshes.&lt;/p&gt;
    &lt;p&gt;Basically, I just take the mesh‚Äôs vertices (not skinned) and joint matrices and produce a new buffer of vertices which are used in later rendering stages.&lt;/p&gt;
    &lt;p&gt;Suppose you spawn three cats with identical meshes:&lt;/p&gt;
    &lt;p&gt;All three of them can have different animations. They all have an identical ‚Äúinput‚Äù mesh. But the ‚Äúoutput‚Äù vertex buffer will differ between them, which means that you need to pre-allocate a vertex buffer for each instance of the mesh.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how the skinning compute shader looks like:&lt;/p&gt;
    &lt;code&gt;#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_buffer_reference : require

#include "vertex.glsl"

struct SkinningDataType {
    ivec4 jointIds;
    vec4 weights;
};

layout (buffer_reference, std430) readonly buffer SkinningData {
    SkinningDataType data[];
};

layout (buffer_reference, std430) readonly buffer JointMatrices {
    mat4 matrices[];
};

layout (push_constant) uniform constants
{
    JointMatrices jointMatrices;
    uint jointMatricesStartIndex;
    uint numVertices;
    VertexBuffer inputBuffer;
    SkinningData skinningData;
    VertexBuffer outputBuffer;
} pcs;

layout (local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

mat4 getJointMatrix(int jointId) {
    return pcs.jointMatrices.matrices[pcs.jointMatricesStartIndex + jointId];
}

void main()
{
    uint index = gl_GlobalInvocationID.x;
    if (index &amp;gt;= pcs.numVertices) {
        return;
    }

    SkinningDataType sd = pcs.skinningData.data[index];
    mat4 skinMatrix =
        sd.weights.x * getJointMatrix(sd.jointIds.x) +
        sd.weights.y * getJointMatrix(sd.jointIds.y) +
        sd.weights.z * getJointMatrix(sd.jointIds.z) +
        sd.weights.w * getJointMatrix(sd.jointIds.w);

    Vertex v = pcs.inputBuffer.vertices[index];
    v.position = vec3(skinMatrix * vec4(v.position, 1.0));

    pcs.outputBuffer.vertices[index] = v;
}
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I store all joint matrices in a big array and populate it every frame (and also pass the starting index in the array for each skinned mesh, &lt;code&gt;jointMatricesStartIndex&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Skinning data is not stored inside each mesh vertex, a separate buffer of &lt;code&gt;num_vertices&lt;/code&gt;elements is used.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After the skinning is performed, all the later rendering stages use this set of vertices Thee rendering process for static and skinned meshes becomes identical, thanks to that.&lt;/p&gt;
    &lt;quote&gt;Anton‚Äôs OpenGL 4 Tutorials book has the best skinning implementation guide I‚Äôve ever read. Game Engine Architecture by Jason Gregory has nice explanations about skinning/skeletal animation math as well.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Game / renderer separation&lt;/head&gt;
    &lt;p&gt;I have a game/renderer separation which uses a simple concept of ‚Äúdraw commands‚Äù. In the game logic, I use entt, but the renderer doesn‚Äôt know anything about entities or ‚Äúgame objects‚Äù. It only knows about the lights, some scene parameters (like fog, which skybox texture to use etc) and meshes it needs to draw.&lt;/p&gt;
    &lt;p&gt;The renderer‚Äôs API looks like this in action:&lt;/p&gt;
    &lt;code&gt;void Game::generateDrawList()
{
    renderer.beginDrawing();

    // Add lights
    const auto lights = ...; // get list of all active lights
    for (const auto&amp;amp;&amp;amp; [e, tc, lc] : lights.each()) {
        renderer.addLight(lc.light, tc.transform);
    }

    // Render static meshes
    const auto staticMeshes = ...; // list of entities with static meshes
    for (const auto&amp;amp;&amp;amp; [e, tc, mc] : staticMeshes.each()) {
        // Each "mesh" can have multiple submeshes similar to how
        // glTF separates each "mesh" into "primitives".
        for (std::size_t i = 0; i &amp;lt; mc.meshes.size(); ++i) {
            renderer.drawMesh(mc.meshes[i], tc.worldTransform, mc.castShadow);
        }
    }

    // Render meshes with skeletal animation
    const auto skinnedMeshes = ...; // list of entities with skeletal animations
    for (const auto&amp;amp;&amp;amp; [e, tc, mc, sc] : skinnedMeshes.each()) {
        renderer.drawSkinnedMesh(
            mc.meshes, sc.skinnedMeshes, tc.worldTransform,
            sc.skeletonAnimator.getJointMatrices());
    }

    renderer.endDrawing();
}
&lt;/code&gt;
    &lt;p&gt;When you call &lt;code&gt;drawMesh&lt;/code&gt; or &lt;code&gt;drawSkinnedMesh&lt;/code&gt;, the renderer creates a mesh draw command and puts it in &lt;code&gt;std::vector&amp;lt;MeshDrawCommand&amp;gt;&lt;/code&gt; which are then iterated through during the drawing process. The &lt;code&gt;MeshDrawCommand&lt;/code&gt; looks like this:&lt;/p&gt;
    &lt;code&gt;
struct SkinnedMesh {
    GPUBuffer skinnedVertexBuffer;
};

struct MeshDrawCommand {
    MeshId meshId;
    glm::mat4 transformMatrix;
    math::Sphere worldBoundingSphere;

    const SkinnedMesh* skinnedMesh{nullptr};
    std::uint32_t jointMatricesStartIndex;
    bool castShadow{true};
};
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;meshId&lt;/code&gt;is used for looking up static meshes in&lt;code&gt;MeshCache&lt;/code&gt;- it‚Äôs a simple&lt;code&gt;std::vector&lt;/code&gt;of references to vertex buffers on GPU.&lt;/item&gt;
      &lt;item&gt;If the mesh has a skeleton, &lt;code&gt;jointMatricesStartIndex&lt;/code&gt;is used during compute skinning and&lt;code&gt;skinnedMesh-&amp;gt;skinnedVertexBuffer&lt;/code&gt;is used for all the rendering afterwards (instead of&lt;code&gt;meshId&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;worldBoundingSphere&lt;/code&gt;is used for frustum culling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This separation is nice because the renderer is clearly separated from the game logic. You can also do something more clever as described here if sorting draw commands becomes a bottleneck.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scene loading and entity prefabs&lt;/head&gt;
    &lt;p&gt;I use Blender as a level editor and export it as glTF. It‚Äôs easy to place objects, colliders and lights there. Here‚Äôs how it looks like:&lt;/p&gt;
    &lt;p&gt;Writing your own level editor would probably take months (years!), so using Blender instead saved me quite a lot of time.&lt;/p&gt;
    &lt;p&gt;It‚Äôs important to mention how I use node names for spawning some objects. For example, you can see an object named &lt;code&gt;Interact.Sphere.Diary&lt;/code&gt; selected in the screenshot above. The part before the first dot is the prefab name (in this case ‚ÄúInteract‚Äù). The ‚ÄúSphere‚Äù part is used by the physics system to create a sphere physics body for the object (‚ÄúCapsule‚Äù and ‚ÄúBox‚Äù can also be used, otherwise the physics shape is created using mesh vertices).&lt;/p&gt;
    &lt;p&gt;Some models are pretty complex and I don‚Äôt want to place them directly into the level glTF file as it‚Äôll greatly increase each level‚Äôs size. I just place an ‚ÄúEmpty-&amp;gt;Arrows‚Äù object and name it something like ‚ÄúCat.NearStore‚Äù. This will spawn ‚ÄúCat‚Äù prefab and attach ‚ÄúNearStore‚Äù tag to it for runtime identification.&lt;/p&gt;
    &lt;p&gt;Prefabs are written in JSON and look like this:&lt;/p&gt;
    &lt;code&gt;{
  "scene": {
    "scene": "assets/models/cato.gltf"
  },
  "movement": {
    "maxSpeed": [4, 4, 4]
  },
  "physics": {
    "type": "dynamic",
    "bodyType": "virtual_character",
    "bodyParams": {
        ...
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;During the level loading process, if the node doesn‚Äôt have a corresponding prefab, it‚Äôs loaded as-is and its mesh data is taken from the glTF file itself (this is mostly used for static geometry). If the node has a corresponding prefab loaded, it‚Äôs created instead. Its mesh data is loaded from the external glTF file - only transform is copied from the original glTF node (the one in the level glTF file).&lt;/p&gt;
    &lt;quote&gt;Once glTFX is released and the support for it is added to Blender, things might be even easier to handle as you‚Äôll be able to reference external glTF files with it.&lt;/quote&gt;
    &lt;head rend="h3"&gt;MSAA&lt;/head&gt;
    &lt;p&gt;Using forward rendering allowed me to easily implement MSAA. Here‚Äôs a comparison of how the game looks without AA and with MSAA on:&lt;/p&gt;
    &lt;p&gt;MSAA is explained well here: https://vulkan-tutorial.com/Multisampling&lt;/p&gt;
    &lt;p&gt;Here‚Äôs another good article about MSAA: https://therealmjp.github.io/posts/msaa-overview/ and potential problems you can have with it (especially with HDR and tone-mapping).&lt;/p&gt;
    &lt;head rend="h3"&gt;UI&lt;/head&gt;
    &lt;p&gt;My UI system was inspired by Roblox‚Äôs UI API: https://create.roblox.com/docs/ui&lt;/p&gt;
    &lt;p&gt;Basically, the UI can calculate its own layout without me having to hard code each individual element‚Äôs size and position. Basically it relies on the following concepts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Origin is an anchor around which the UI element is positioned. If origin is &lt;code&gt;(0, 0)&lt;/code&gt;, setting UI element‚Äôs position to be&lt;code&gt;(x,y)&lt;/code&gt;will make its upper-left pixel have (x,y) pixel coordinate. If the origin is&lt;code&gt;(1, 1)&lt;/code&gt;, then the element‚Äôs bottom-right corner will be positioned at&lt;code&gt;(x, y)&lt;/code&gt;. If the origin is (0.5, 1) then it will be positioned using bottom-center point as the reference.&lt;/item&gt;
      &lt;item&gt;Relative size makes the children‚Äôs be proportional to parent‚Äôs size. If (1,1) then the child element will have the same size as the parent element. If it‚Äôs (0.5, 0.5) then it‚Äôll have half the size of the parent. If the parent uses children‚Äôs size as a guide, then if a child has (0.5, 0.25) relative size, the parent‚Äôs width will be 2x larger and the height will be 4x larger.&lt;/item&gt;
      &lt;item&gt;Relative position uses parent‚Äôs size as a guide for positioning. It‚Äôs useful for centering elements, for example if you have an element with (0.5, 0.5) origin and (0.5, 0.5) relative position, it‚Äôll be centered inside its parent element.&lt;/item&gt;
      &lt;item&gt;You can also set pixel offsets for both position and size separately (they‚Äôre called &lt;code&gt;offsetPosition&lt;/code&gt;and&lt;code&gt;offsetSize&lt;/code&gt;in my codebase).&lt;/item&gt;
      &lt;item&gt;You can also set a fixed size for the elements if you don‚Äôt want them to ever be resized.&lt;/item&gt;
      &lt;item&gt;The label/image element size is determined using its content.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some examples of how it can be used to position child elements:&lt;/p&gt;
    &lt;p&gt;a) The child (yellow) has relative size (0.5, 1), relative position of (0.5, 0.5) and origin (0.5, 0.5) (alternatively, the relative position can be (0.5, 0.0) and origin at (0.5, 0.0) in this case). Its parent (green) will be two times wider, but will have the same height. The child element will be centered inside the parent.&lt;/p&gt;
    &lt;p&gt;b) The child (yellow) has origin (1, 1), fixed size (w,h) and absolute offset of (x,y) - this way, the item can be positioned relative to the bottom-right corner of its parent (green)&lt;/p&gt;
    &lt;p&gt;Let‚Äôs see how sizes and positions of UI elements are calculated (implementation in EDBR).&lt;/p&gt;
    &lt;p&gt;First, sizes of all elements are calculated recursively. Then positions are computed based on the previously computed sizes and specified offset positions. Afterwards all elements are drawn recursively - parent element first, then its children etc.&lt;/p&gt;
    &lt;p&gt;When calculating the size, most elements either have a ‚Äúfixed‚Äù size (which you can set manually, e.g. you can set some button to always be 60x60 pixels) or their size is computed based on their content. For example, for label elements, their size is computed using the text‚Äôs bounding box. For image elements, their size equals the image size and so on.&lt;/p&gt;
    &lt;p&gt;If an element has an ‚ÄúAuto-size‚Äù property, it needs to specify which child will be used to calculate its size. For example, the menu nine-slice can have several text labels inside the ‚Äúvertical layout‚Äù element - the bounding boxes will be calculated first, then their sizes will be summed up - then, the parent‚Äôs size is calculated.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take a look at a simple menu with bounding boxes displayed:&lt;/p&gt;
    &lt;p&gt;Here, root &lt;code&gt;NineSliceElement&lt;/code&gt; is marked as ‚ÄúAuto-size‚Äù. To compute its size, it first computes the size of its child (&lt;code&gt;ListLayoutElement&lt;/code&gt;). This recursively computes the sizes of each button, sums them up and adds some padding (&lt;code&gt;ListLayoutElement&lt;/code&gt; also makes the width of each button the same based on the maximum width in the list).&lt;/p&gt;
    &lt;head rend="h3"&gt;Dear ImGui and sRGB issues&lt;/head&gt;
    &lt;p&gt;I love Dear ImGui. I used it to implement many useful dev and debug tools (open the image in a new tab to see them better):&lt;/p&gt;
    &lt;p&gt;It has some problems with sRGB, though. I won‚Äôt explain it in detail, but basically if you use sRGB framebuffer, Dear ImGui will look wrong in many ways, see the comparison:&lt;/p&gt;
    &lt;p&gt;Sometimes you can see people doing hacks by doing &lt;code&gt;pow(col, vec4(2.2))&lt;/code&gt; with Dear ImGui‚Äôs colors but it still doesn‚Äôt work properly with alpha and produces incorrect color pickers.&lt;/p&gt;
    &lt;p&gt;I ended up writing my own Dear ImGui backend and implementing DilligentEngine‚Äôs workaround which is explained in detail here and here.&lt;/p&gt;
    &lt;quote&gt;Writing it wasn‚Äôt as hard as I expected. I only need to write the rendering part, while ‚Äúlogic/OS interaction‚Äù part (input event processing, clipboard etc.) is still handled by default Dear ImGui SDL backend in my case.&lt;/quote&gt;
    &lt;p&gt;There are some additional benefits of having my own backend:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It supports bindless texture ids, so I can draw images by simply calling &lt;code&gt;ImGui::Image(bindlessTextureId, ...)&lt;/code&gt;. Dear ImGui‚Äôs Vulkan backend requires you to ‚Äúregister‚Äù textures by calling&lt;code&gt;ImGui_ImplVulkan_AddTexture&lt;/code&gt;for each texture before you can call&lt;code&gt;ImGui::Image&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;It can properly draw linear and non-linear images by passing their format into backend (so that sRGB images are not gamma corrected twice when they‚Äôre displayed)&lt;/item&gt;
      &lt;item&gt;Initializing and dealing with it is easier as it does Vulkan things in the same way as the rest of my engine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Other stuff&lt;/head&gt;
    &lt;p&gt;There are many parts of the engine not covered there because they‚Äôre not related to Vulkan. I still feel like it‚Äôs good to mention them briefly for the sake of completion.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I use Jolt Physics for physics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Integrating it into the engine was pretty easy. Right now I mostly use it for collision resolution and basic character movement.&lt;/p&gt;
    &lt;p&gt;The samples are fantastic. The docs are very good too.&lt;/p&gt;
    &lt;p&gt;I especially want to point out how incredible &lt;code&gt;JPH::CharacterVirtual&lt;/code&gt; is. It handles basic character movement so well. I remember spending days trying to get proper slope movement in Bullet to work. With Jolt, it just worked ‚Äúout of the box‚Äù.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how it basically works (explaining how it works properly would probably require me to write quite a big article):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You add your shapes to Jolt‚Äôs world.&lt;/item&gt;
      &lt;item&gt;You run the simulation.&lt;/item&gt;
      &lt;item&gt;You get new positions of your physics objects and use these positions to render objects in their current positions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I use entt for the entity-component-system part.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It has worked great for me so far. Previously I had my own ECS implementation, but decided to experiment with a 3rd party ECS library to have less code to maintain.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I use openal-soft, libogg and libvorbis for audio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The audio system is mostly based on these articles: https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I use Tracy for profiling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Integrating it was very easy (read the PDF doc, it‚Äôs fantastic!) and it helped me avoid tons of bike-shedding by seeing how little time something, which I thought was ‚Äúinefficient‚Äù, really took.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I gained from switching to Vulkan&lt;/head&gt;
    &lt;p&gt;There are many nice things I got after switching to Vulkan:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No more global state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This makes abstractions a lot easier. With OpenGL abstractions/engines, you frequently see ‚Äúshader.bind()‚Äù calls, state trackers, magic RAII, which automatically binds/unbinds objects and so on. There‚Äôs no need for that in Vulkan - it‚Äôs easy to write functions which take some objects as an input and produce some output - stateless, more explicit and easier to reason about.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;API is more pleasant to work with overall - I didn‚Äôt like ‚Äúbinding‚Äù things and the whole ‚Äúglobal state machine‚Äù of OpenGL.&lt;/item&gt;
      &lt;item&gt;You need to write less abstractions overall.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With OpenGL, you need to write a lot of abstractions to make it all less error-prone‚Ä¶ Vulkan‚Äôs API requires a lot less of this, in my experience. And usually the abstractions that you write map closer to Vulkan‚Äôs ‚Äúraw‚Äù functions, compared to OpenGL abstractions which hide manipulation of global state and usually call several functions (and might do some stateful things for optimization).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Better validation errors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Validation errors are very good in Vulkan. While OpenGL has &lt;code&gt;glDebugMessageCallback&lt;/code&gt;, it doesn‚Äôt catch that many issues and you‚Äôre left wondering why your texture looks weird, why your lighting is broken and so on. Vulkan has more extensive validation which makes the debugging process much better.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debugging in RenderDoc&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I can now debug shaders in RenderDoc. It looks like this:&lt;/p&gt;
    &lt;p&gt;With OpenGL I had to output the values to some texture and color-pick them‚Ä¶ which took a lot of time. But now I can debug vertex and fragment shaders easily.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More consistent experience across different GPUs and OSes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With OpenGL, drivers on different GPUs and OSes worked differently from each other which made some bugs pop up only on certain hardware configurations. It made the process of debugging them hard. I still experienced some slight differences between different GPUs in Vulkan, but it‚Äôs much less prevalent compared to OpenGL.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ability to use better shading languages in the future&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GLSL is a fine shading language, but there are some new shading languages which promise to be more feature-complete, convenient and readable, for example:&lt;/p&gt;
    &lt;p&gt;I might explore them in the future and see if they offer me something that GLSL lacks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More control over every aspect of the graphics pipeline.&lt;/item&gt;
      &lt;item&gt;Second system effect, but good&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My first OpenGL engine was written during the process of learning graphics programming from scratch. Many abstractions were not that good and rewriting them with some graphics programming knowledge (and some help from vkguide) helped me implement a much cleaner system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Street cred&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And finally, it makes me proud to be able to say ‚ÄúI have a custom engine written in Vulkan and it works‚Äù. Sometimes people start thinking about you as a coding wizard and it makes me happy and proud of my work. :)&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;There are many things that I plan to do in the future, here‚Äôs a list of some of them:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sign-distance field font support (good article about implementing them)&lt;/item&gt;
      &lt;item&gt;Loading many images and generating mipmaps in parallel (or use image formats which already have mipmaps stored inside of them)&lt;/item&gt;
      &lt;item&gt;Bloom.&lt;/item&gt;
      &lt;item&gt;Volumetric fog.&lt;/item&gt;
      &lt;item&gt;Animation blending.&lt;/item&gt;
      &lt;item&gt;Render graphs.&lt;/item&gt;
      &lt;item&gt;Ambient occlusion.&lt;/item&gt;
      &lt;item&gt;Finishing the game? (hopefully‚Ä¶)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Overall, I‚Äôm quite satisfied with what I managed to accomplish. Learning Vulkan was quite difficult, but it wasn‚Äôt as hard as I imagined. It taught me a lot about graphics programming and modern APIs and now I have a strong foundation to build my games with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://edw.is/learning-vulkan/"/><published>2025-11-21T23:28:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46010806</id><title>Sharper MRI scans may be on horizon thanks to new physics-based model</title><updated>2025-11-22T12:56:49.374507+00:00</updated><content/><link href="https://news.rice.edu/news/2025/sharper-mri-scans-may-be-horizon-thanks-new-physics-based-model"/><published>2025-11-22T00:30:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46011978</id><title>Moss Survives 9 Months in Space Vacuum</title><updated>2025-11-22T12:56:48.531085+00:00</updated><content>&lt;doc fingerprint="3f88d37b68b59460"&gt;
  &lt;main&gt;
    &lt;p&gt;Mosses are already known for coping with harsh radiation, dehydration, and long freezes. Now scientists have pushed them even further by exposing their spore capsules to open space for nine months, and most of them survived.&lt;/p&gt;
    &lt;p&gt;The team worked with spreading earthmoss (Physcomitrium patens), a small moss species used widely as a plant model by researchers. Its spore-containing capsules were mounted on the outside of the International Space Station (ISS), where they experienced direct solar radiation, vacuum conditions, and sharp temperature swings during each orbit.&lt;/p&gt;
    &lt;p&gt;Under those conditions, cells usually break down quickly. So the researchers were surprised by what came back. ‚ÄúWe expected almost zero survival, but the result was the opposite,‚Äù says Hokkaido University biologist Tomomichi Fujita. More than 80 percent of the spores still germinated once they returned to Earth.&lt;/p&gt;
    &lt;p&gt;Also Read: Microbe That Could Turn Martian Dust into Oxygen&lt;/p&gt;
    &lt;p&gt;The team detected a small drop in chlorophyll a, but the other pigments remained stable. The spores grew normally in follow-up tests, showing no signs of major stress from their time in orbit.&lt;/p&gt;
    &lt;p&gt;This kind of toughness fits with the evolutionary history of mosses. Bryophytes ‚Äî the group that includes mosses, liverworts, and hornworts ‚Äî were among the first plants to move from water onto land about 500 million years ago. Their spores had to withstand drying and direct sunlight long before soils existed, which may explain why their protective structures still hold up so well today.&lt;/p&gt;
    &lt;p&gt;The results place moss spores alongside the few organisms known to tolerate direct space exposure, including tardigrades and certain microbes. Their survival also adds to ongoing discussions about what types of life might endure extreme environments beyond Earth.&lt;/p&gt;
    &lt;p&gt;According to the researchers, this durability could matter for future experiments on the Moon or Mars. Mosses need very little soil and can pull nutrients directly from rock, making them candidates for early ecosystem tests in extraterrestrial settings.&lt;/p&gt;
    &lt;p&gt;‚ÄúUltimately, we hope this work opens a new frontier toward constructing ecosystems in extraterrestrial environments such as the Moon and Mars,‚Äù says Fujita.&lt;/p&gt;
    &lt;p&gt;The research was published in iScience. Read the study here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scienceclock.com/moss-survives-9-months-in-space-vacuum/"/><published>2025-11-22T03:57:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46012328</id><title>Superman copy found in mum's attic is most valuable comic ever at $9.12M</title><updated>2025-11-22T12:56:48.363525+00:00</updated><content>&lt;doc fingerprint="b72b5181609a05bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Superman copy found in mum's attic is most valuable comic ever at $9.12m&lt;/head&gt;
    &lt;p&gt;While cleaning out their late mother's California loft last Christmas, three brothers made a life-changing discovery under a pile of faded newspapers: one of the first Superman comics ever made.&lt;/p&gt;
    &lt;p&gt;An original copy of the June 1939 first edition on the Man of Steel's adventures, it was in a remarkably pristine condition.&lt;/p&gt;
    &lt;p&gt;Now it has become the highest-priced comic book ever sold, fetching $9.12m (¬£7m) at auction.&lt;/p&gt;
    &lt;p&gt;Texas-based Heritage Auctions, which hosted Thursday's sale, called it the "pinnacle of comic collecting".&lt;/p&gt;
    &lt;p&gt;The brothers found six comic books, including Superman #1, in the loft underneath a stack of newspapers inside a cardboard box and surrounded by cobwebs in 2024, Heritage said.&lt;/p&gt;
    &lt;p&gt;They waited a few months before contacting the auction house, but once they did, Heritage Auctions vice-president Lon Allen visited them in San Francisco within days, according to the auction house.&lt;/p&gt;
    &lt;p&gt;The brothers, who have chosen to withhold their names, are "in their 50s and 60s, and their mom had always told them she had an expensive comics collection but never showed them", Mr Allen said.&lt;/p&gt;
    &lt;p&gt;"It's a twist on the old 'Mom threw away my comics' story."&lt;/p&gt;
    &lt;p&gt;Their mother had held onto the comic books since she and her brother bought them between the Great Depression and the beginning of World War Two, Heritage said.&lt;/p&gt;
    &lt;p&gt;Mr Allen added that the cool northern California climate was perfect for preserving old paper.&lt;/p&gt;
    &lt;p&gt;"If it had been in an attic here in Texas, it would have been ruined," he said.&lt;/p&gt;
    &lt;p&gt;That helped CGC, a large third-party comics grading service, give this copy of Superman #1 a 9.0 rating on a 10-point scale, topping the previous record of 8.5.&lt;/p&gt;
    &lt;p&gt;And at its sale price of over $9m, including buyer's premium, Superman #1 easily beat the previous highest-priced comic book ever sold by $3m.&lt;/p&gt;
    &lt;p&gt;Action Comics No. 1, the 1938 work that first introduced Superman, sold for $6m last year.&lt;/p&gt;
    &lt;p&gt;The youngest brother said in a press release by the auction house that the box had remained forgotten in the back of attic.&lt;/p&gt;
    &lt;p&gt;"As the years unfolded, life brought about a series of losses and changes," he said. "The demands of everyday survival took centre stage, and the box of comics, once set aside with care and intention, was forgotten. Until last Christmas."&lt;/p&gt;
    &lt;p&gt;He added: "This isn't simply a story about old paper and ink. This was never just about a collectible.&lt;/p&gt;
    &lt;p&gt;"This is a testament to memory, family and the unexpected ways the past finds its way back to us."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c8e9rp0knj6o"/><published>2025-11-22T05:21:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46013579</id><title>Libpng 1.6.51: Four buffer overflow vulnerabilities fixed</title><updated>2025-11-22T12:56:47.428864+00:00</updated><content>&lt;doc fingerprint="34629ed63034e08e"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;quote&gt;Message-ID: &amp;lt;CAAoVtZw-pkvsSTaXAHjDdUC3NRDwvwVNT8D4BpO5z3d79W-FVg@mail.gmail.com&amp;gt; Date: Sat, 22 Nov 2025 03:27:35 +0200 From: Cosmin Truta &amp;lt;ctruta@...il.com&amp;gt; To: oss-security@...ts.openwall.com Subject: libpng 1.6.51: Four buffer overflow vulnerabilities fixed: CVE-2025-64505, CVE-2025-64506, CVE-2025-64720, CVE-2025-65018 Hello, everyone, libpng 1.6.51 has been released to address four buffer overflow vulnerabilities discovered through fuzzing and security research. This release fixes two high-severity and two moderate-severity CVEs affecting libpng 1.6.0 through 1.6.50. CVE-2025-64505 (CVSS 6.1, Moderate): Heap buffer over-read in png_do_quantize via malformed palette index. CVE-2025-64506 (CVSS 6.1, Moderate): Heap buffer over-read in png_write_image_8bit with 8-bit input and convert_to_8bit enabled. CVE-2025-64720 (CVSS 7.1, High): Out-of-bounds read in png_image_read_composite via palette premultiplication with PNG_FLAG_OPTIMIZE_ALPHA. CVE-2025-65018 (CVSS 7.1, High): Heap buffer overflow in png_combine_row triggered via png_image_finish_read when processing 16-bit interlaced PNGs with 8-bit output format. All vulnerabilities require user interaction (processing a malicious PNG file) and can result in information disclosure and/or denial of service. CVE-2025-65018 may enable arbitrary code execution via heap corruption in certain heap configurations. GitHub Security Advisories: - CVE-2025-64505: https://github.com/pnggroup/libpng/security/advisories/GHSA-4952-h5wq-4m42 - CVE-2025-64506: https://github.com/pnggroup/libpng/security/advisories/GHSA-qpr4-xm66-hww6 - CVE-2025-64720: https://github.com/pnggroup/libpng/security/advisories/GHSA-hfc7-ph9c-wcww - CVE-2025-65018: https://github.com/pnggroup/libpng/security/advisories/GHSA-7wv6-48j4-hj3g Fixes: - CVE-2025-64505: https://github.com/pnggroup/libpng/commit/6a528eb5fd0dd7f6de1c39d30de0e41473431c37 - CVE-2025-64506: https://github.com/pnggroup/libpng/commit/2bd84c019c300b78e811743fbcddb67c9d9bf821 - CVE-2025-64720: https://github.com/pnggroup/libpng/commit/08da33b4c88cfcd36e5a706558a8d7e0e4773643 - CVE-2025-65018: https://github.com/pnggroup/libpng/commit/16b5e3823918840aae65c0a6da57c78a5a496a4d https://github.com/pnggroup/libpng/commit/218612ddd6b17944e21eda56caf8b4bf7779d1ea Note: CVE-2025-65018 requires both commits for correct remediation. Release: https://github.com/pnggroup/libpng/releases/tag/v1.6.51 Credit: Samsung-PENTEST (CVE-2025-64505, CVE-2025-64506, CVE-2025-64720), weijinjinnihao (CVE-2025-64506), yosiimich (CVE-2025-65018), with analysis by Fabio Gritti and John Bowler. Users should upgrade to libpng 1.6.51 immediately. Cosmin Truta libpng maintainer&lt;/quote&gt;
    &lt;p&gt;Powered by blists - more mailing lists&lt;/p&gt;
    &lt;p&gt;Please check out the Open Source Software Security Wiki, which is counterpart to this mailing list.&lt;/p&gt;
    &lt;p&gt;Confused about mailing lists and their use? Read about mailing lists on Wikipedia and check out these guidelines on proper formatting of your messages.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.openwall.com/lists/oss-security/2025/11/22/1"/><published>2025-11-22T10:08:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46013633</id><title>Jack Ma's family shifted wealth to UK after years-long 'disappearance'</title><updated>2025-11-22T12:56:46.693284+00:00</updated><content>&lt;doc fingerprint="2b9055afb58216e1"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Billionaire‚Äôs wife bought London mansion as he reconciled with Chinese authorities&lt;/head&gt;
    &lt;p&gt;The family of Chinese billionaire Jack Ma bought a ¬£19.5 million London mansion amid a rapprochement with Chinese authorities after years of scrutiny and political exile.&lt;/p&gt;
    &lt;p&gt;Ma‚Äôs wife, Cathy Ying Zhang, acquired the six-bedroom Edwardian house, formerly the Italian embassy, in London‚Äôs elite Belgravia district in October 2024, property records show.&lt;/p&gt;
    &lt;p&gt;The purchase came after Ma‚Äôs return to public life after disappearing from view in the aftermath of a speech criticising China‚Äôs financial system. It could be seen as a ‚Äúprecautionary diversification‚Äù in case Ma again provokes Beijing‚Äôs ire, said Sari Arho Havr√©n, a China specialist at the Royal United Services Institute.&lt;/p&gt;
    &lt;p&gt;‚ÄúWealthy families are hedging against regime risk‚Äîone never knows when policies may turn hostile again,‚Äù she said. ‚ÄúAffluent families are diversifying quietly. Rule of law societies still hold considerable appeal.‚Äù&lt;/p&gt;
    &lt;p&gt;Ma, 61, is the founder of Alibaba Group, whose online commerce platforms have earned him a fortune of around $30 billion. The Belgravia house, the Ma family‚Äôs first known property holding in the UK, may have been funded by the sale of Alibaba shares in 2023, Havr√©n said.&lt;/p&gt;
    &lt;p&gt;Ma‚Äôs wife Zhang, who has taken Singaporean citizenship, is reportedly the sole director of an offshore company that Ma used to buy a ch√¢teau and vineyards in France.&lt;/p&gt;
    &lt;p&gt;Last year it was reported that Zhang spent up to $38 million on three commercial properties in Singapore. The buying spree is part of a trend that has seen prominent Chinese businesspeople move money abroad for fear of asset freezes or capital controls.&lt;/p&gt;
    &lt;p&gt;Many have left China altogether. As many as 13,800 ‚Äúhigh-net-worth individuals‚Äù emigrated in 2024‚Äîa 28 percent rise from 2022, according to investment migration consultants Henley &amp;amp; Partners.&lt;/p&gt;
    &lt;p&gt;The sale of the Belgravia mansion, managed by Knight Frank and Beauchamp Estates and handled by law firm Withers LLP, was rushed through ahead of a rise in the UK‚Äôs stamp duty surcharge for overseas buyers, according to a November 2024 report that did not name the buyer.&lt;/p&gt;
    &lt;p&gt;Beauchamp and Knight Frank declined to comment. Zhang and Ma Withers did not respond to questions put to them via Withers.&lt;/p&gt;
    &lt;p&gt;In 2015, it was reported that Ma family purchased ‚ÄòAsia‚Äôs most expensive home‚Äô in Hong Kong‚Äôs Victoria Peak which was formerly owned by the Belgian government. In the same year, it was reported that Ma had bought a 28,000 acre property in upstate New York for $23 million.&lt;/p&gt;
    &lt;p&gt;Ma vanished from public view in late 2020 after he criticised China‚Äôs financial regulators. Beijing reportedly punished him with a fine of nearly $3 billion and halted a stock market listing by Ant Group, an offshoot of Alibaba.&lt;/p&gt;
    &lt;p&gt;He resurfaced in China in 2023 after an apparent reconciliation with the administration of President Xi Jinping, occasionally attending public events. In February 2025, he was seen shaking Xi‚Äôs hand at event with Chinese industry leaders. However, Ma‚Äôs public remarks went unreported by official state media, prompting analysts to suggest that he had not been ‚Äúcompletely rehabilitated‚Äù.&lt;/p&gt;
    &lt;p&gt;In April, The Guardian reported that Chinese authorities enlisted Ma as part of a campaign to pressure a dissident businessman to return to China from France to help prosecute an official who had angered the regime.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey said I‚Äôm the only one who can persuade you to return,‚Äù Ma reportedly told the unnamed businessman in a telephone call. The Chinese government called the allegations ‚Äúpure fabrication‚Äù.&lt;/p&gt;
    &lt;p&gt;Headline picture: Beauchamp Estates&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.source-material.org/jack-ma-bought-uk-home-after-years-long-disappearance/"/><published>2025-11-22T10:19:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46013816</id><title>My private information is worth $30</title><updated>2025-11-22T12:56:46.521307+00:00</updated><content>&lt;doc fingerprint="fe9893002d67ae5b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My private information is worth $30&lt;/head&gt;
    &lt;p&gt;A couple of weeks ago, I was notified that I can be part of class action settlement against University of Minnesota for a data breach that exposed my personal information. According to the details, In 2021, the University of Minnesota experienced a data breach that exposed personal information of "individuals who submitted information to the university as a prospective student, attended the university as a student, worked at the university as an employee or participated in university programs between 1989 and Aug. 10, 2021." source. I'm an alumnus of this university, so my information was part of that breach.&lt;/p&gt;
    &lt;p&gt;The university of course as a classical cooperative entity took the easy route that the legal system provides. They refuse to admit any wrongdoing, but they agreed to pay $5 million to settle the class action lawsuit. The settlement is open to anyone who had their personal information exposed in the breach, which includes names, addresses, dates of birth, Social Security numbers, and other sensitive data.&lt;/p&gt;
    &lt;p&gt;What is more insulting than that the university did not issue a formal apology to the affected individuals, is that they are offering a mere $30 per person as compensation for the breach. Yes to be honest they include this standard 24 months of dark web monitoring and identity theft protection services, but the value of my personal information is set to $30. Which even would be less if the number of people submitting exceeds the funding available for the settlement.&lt;/p&gt;
    &lt;p&gt;So according the university that sends me two or three emails per week asking me to donate to them, my personal information is worth $30. I understand that my Social Security number and other personal information got exposed in other breaches (Thanks to &lt;code&gt;T-mobile&lt;/code&gt; and others). But the current status quo is that it does not matter whether it is a commercial entity or a public one, they will act in the same way. They will not take responsibility for their actions, and they will not compensate you for the damage they caused. They will just offer you a small amount of money and hope that you will forget about it.&lt;/p&gt;
    &lt;p&gt;The University of Minnesota is not the only one doing this. Many other institutions and companies have been caught in data breaches and have offered similar settlements. But it is still disappointing to see that they are not taking the issue seriously. This same university which promised a life access to email address which they did not honor, is now offering me $30 for my personal information. It is a slap in the face to all of us who have been affected by this breach. So I will not be submitting a claim for the settlement. I will not be accepting their offer of $30. I would have much preferred if they had taken responsibility for their actions and issued a healthy apology. But they did not. This would have been a good start. But they did not. And they will not.&lt;/p&gt;
    &lt;p&gt;The basic problem is that they do not care about us. They care about their reputation and their bottom line. They do not care about the damage they caused to our personal information. They do not care about the trust they have broken. They just want to move on and forget about it. When this happens from a corporation or a company, I can understand it. But when it happens from a public institution that is supposed to serve the public interest, it is unacceptable. How would I trust anything coming from them in the future? They have shown that they don't care about their alumni or their students.&lt;/p&gt;
    &lt;p&gt;The regulation is very weak, and the courts/laws are not doing enough to hold these institutions accountable. The fines are too low, and the settlements are too small. The only way to change this is to demand better regulations and stronger penalties for data breaches. We need to hold these institutions accountable for their actions and make them pay for the damage they cause. If the fines and compensation were higher, then the incentives would be aligned, and they would take data security more seriously. And would invest more in protecting our personal information instead of the ever-increasing administrative costs and salaries of the top executives.&lt;/p&gt;
    &lt;p&gt;US Universities are not only charging high tuition fees for education, but they are charging even researchers with external grants to use their facilities. If you get NSF or NIH grant, you have to pay the university a percentage of the grant as an indirect cost. The percentage varies from one university to another, but it is usually around 50%. This means that if you get a 100,000 USD grant, the university will take out 50,000 USD as indirect costs (NSF or NIH will end up paying 150,000 USD). This is a huge amount of money that could be used for research, but it is going to the university's administrative costs and salaries of the ever-increasing number of administrators.&lt;/p&gt;
    &lt;p&gt;For what it is worth that the universities is currently under fire for a variety of reasons, mostly politically motivated, but there are many valid reasons to be critical of the way they are run. The way they handle data breaches is just one of them. The amount of disrespect they show to their alumni and students is another. The way they prioritize administrative costs over education and research is yet another. It is time for us to demand better from our universities and hold them accountable for their actions.&lt;/p&gt;
    &lt;p&gt;After writing this post and trying to proofread it, I realized that I repeated "My personal information is worth $30" multiple times. I guess it is a sign that I am still angry about it. But also realized that if I had written this in Arabic it would have been much more concise. The poetic nature of writing in grievance in Arabic is much more effective than in English. But I will leave that for another time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.melashri.net/micro/privacy-price/"/><published>2025-11-22T11:04:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46013935</id><title>Agent Design Is Still Hard</title><updated>2025-11-22T12:56:46.282533+00:00</updated><content>&lt;doc fingerprint="67169c5569238917"&gt;
  &lt;main&gt;
    &lt;p&gt;written on November 21, 2025&lt;/p&gt;
    &lt;p&gt;I felt like it might be a good time to write about some new things I‚Äôve learned. Most of this is going to be about building agents, with a little bit about using agentic coding tools.&lt;/p&gt;
    &lt;p&gt;TL;DR: Building agents is still messy. SDK abstractions break once you hit real tool use. Caching works better when you manage it yourself, but differs between models. Reinforcement ends up doing more heavy lifting than expected, and failures need strict isolation to avoid derailing the loop. Shared state via a file-system-like layer is an important building block. Output tooling is surprisingly tricky, and model choice still depends on the task.&lt;/p&gt;
    &lt;p&gt;When you build your own agent, you have the choice of targeting an underlying SDK like the OpenAI SDK or the Anthropic SDK, or you can go with a higher level abstraction such as the Vercel AI SDK or Pydantic. The choice we made a while back was to adopt the Vercel AI SDK but only the provider abstractions, and to basically drive the agent loop ourselves. At this point we would not make that choice again. There is absolutely nothing wrong with the Vercel AI SDK, but when you are trying to build an agent, two things happen that we originally didn‚Äôt anticipate:&lt;/p&gt;
    &lt;p&gt;The first is that the differences between models are significant enough that you will need to build your own agent abstraction. We have not found any of the solutions from these SDKs that build the right abstraction for an agent. I think this is partly because, despite the basic agent design being just a loop, there are subtle differences based on the tools you provide. These differences affect how easy or hard it is to find the right abstraction (cache control, different requirements for reinforcement, tool prompts, provider-side tools, etc.). Because the right abstraction is not yet clear, using the original SDKs from the dedicated platforms keeps you fully in control. With some of these higher-level SDKs you have to build on top of their existing abstractions, which might not be the ones you actually want in the end.&lt;/p&gt;
    &lt;p&gt;We also found it incredibly challenging to work with the Vercel SDK when it comes to dealing with provider-side tools. The attempted unification of messaging formats doesn‚Äôt quite work. For instance, the web search tool from Anthropic routinely destroys the message history with the Vercel SDK, and we haven‚Äôt yet fully figured out the cause. Also, in Anthropic‚Äôs case, cache management is much easier when targeting their SDK directly instead of the Vercel one. The error messages when you get things wrong are much clearer.&lt;/p&gt;
    &lt;p&gt;This might change, but right now we would probably not use an abstraction when building an agent, at least until things have settled down a bit. The benefits do not yet outweigh the costs for us.&lt;/p&gt;
    &lt;p&gt;Someone else might have figured it out. If you‚Äôre reading this and think I‚Äôm wrong, please drop me a mail. I want to learn.&lt;/p&gt;
    &lt;p&gt;The different platforms have very different approaches to caching. A lot has been said about this already, but Anthropic makes you pay for caching. It makes you manage cache points explicitly, and this really changes the way you interact with it from an agent engineering level. I initially found the manual management pretty dumb. Why doesn‚Äôt the platform do this for me? But I‚Äôve fully come around and now vastly prefer explicit cache management. It makes costs and cache utilization much more predictable.&lt;/p&gt;
    &lt;p&gt;Explicit caching allows you to do certain things that are much harder otherwise. For instance, you can split off a conversation and have it run in two different directions simultaneously. You also have the opportunity to do context editing. The optimal strategy here is unclear, but you clearly have a lot more control, and I really like having that control. It also makes it much easier to understand the cost of the underlying agent. You can assume much more about how well your cache will be utilized, whereas with other platforms we found it to be hit and miss.&lt;/p&gt;
    &lt;p&gt;The way we do caching in the agent with Anthropic is pretty straightforward. One cache point is after the system prompt. Two cache points are placed at the beginning of the conversation, where the last one moves up with the tail of the conversation. And then there is some optimization along the way that you can do.&lt;/p&gt;
    &lt;p&gt;Because the system prompt and the tool selection now have to be mostly static, we feed a dynamic message later to provide information such as the current time. Otherwise, this would trash the cache. We also leverage reinforcement during the loop much more.&lt;/p&gt;
    &lt;p&gt;Every time the agent runs a tool you have the opportunity to not just return data that the tool produces, but also to feed more information back into the loop. For instance, you can remind the agent about the overall objective and the status of individual tasks. You can also provide hints about how the tool call might succeed when a tool fails. Another use of reinforcement is to inform the system about state changes that happened in the background. If you have an agent that uses parallel processing, you can inject information after every tool call when that state changed and when it is relevant for completing the task.&lt;/p&gt;
    &lt;p&gt;Sometimes it‚Äôs enough for the agent to self-reinforce. In Claude Code, for instance, the todo write tool is a self-reinforcement tool. All it does is take from the agent a list of tasks that it thinks it should do and echo out what came in. It‚Äôs basically just an echo tool; it really doesn‚Äôt do anything else. But that is enough to drive the agent forward better than if the only task and subtask were given at the beginning of the context and too much has happened in the meantime.&lt;/p&gt;
    &lt;p&gt;We also use reinforcements to inform the system if the environment changed during execution in a way that‚Äôs problematic for the agent. For instance, if our agent fails and retries from a certain step forward but the recovery operates off broken data, we inject a message informing it that it might want to back off a couple of steps and redo an earlier step.&lt;/p&gt;
    &lt;p&gt;If you expect a lot of failures during code execution, there is an opportunity to hide those failures from the context. This can happen in two ways. One is to run tasks that might require iteration individually. You would run them in a subagent until they succeed and only report back the success, plus maybe a brief summary of approaches that did not work. It is helpful for an agent to learn about what did not work in a subtask because it can then feed that information into the next task to hopefully steer away from those failures.&lt;/p&gt;
    &lt;p&gt;The second option doesn‚Äôt exist in all agents or foundation models, but with Anthropic you can do context editing. So far we haven‚Äôt had a lot of success with context editing, but we believe it‚Äôs an interesting thing we would love to explore more. We would also love to learn if people have success with it. What is interesting about context editing is that you should be able to preserve tokens for further down the iteration loop. You can take out of the context certain failures that didn‚Äôt drive towards successful completion of the loop, but only negatively affected certain attempts during execution. But as with the point I made earlier: it is also useful for the agent to understand what didn‚Äôt work, but maybe it doesn‚Äôt require the full state and full output of all the failures.&lt;/p&gt;
    &lt;p&gt;Unfortunately, context editing will automatically invalidate caches. There is really no way around it. So it can be unclear when the trade-off of doing that compensates for the extra cost of trashing the cache.&lt;/p&gt;
    &lt;p&gt;As I mentioned a couple of times on this blog already, most of our agents are based on code execution and code generation. That really requires a common place for the agent to store data. Our choice is a file system‚Äîin our case a virtual file system‚Äîbut that requires different tools to access it. This is particularly important if you have something like a subagent or subinference.&lt;/p&gt;
    &lt;p&gt;You should try to build an agent that doesn‚Äôt have dead ends. A dead end is where a task can only continue executing within the sub-tool that you built. For instance, you might build a tool that generates an image, but is only able to feed that image back into one more tool. That‚Äôs a problem because you might then want to put those images into a zip archive using the code execution tool. So there needs to be a system that allows the image generation tool to write the image to the same place where the code execution tool can read it. In essence, that‚Äôs a file system.&lt;/p&gt;
    &lt;p&gt;Obviously it has to go the other way around too. You might want to use the code execution tool to unpack a zip archive and then go back to inference to describe all the images so that the next step can go back to code execution and so forth. The file system is the mechanism that we use for that. But it does require tools to be built in a way that they can take file paths to the virtual file system to work with.&lt;/p&gt;
    &lt;p&gt;So basically an &lt;code&gt;ExecuteCode&lt;/code&gt; tool would have access to the same file system as
the &lt;code&gt;RunInference&lt;/code&gt; tool which could take a &lt;code&gt;path&lt;/code&gt; to a file on that same
virtual file system.&lt;/p&gt;
    &lt;p&gt;One interesting thing about how we structured our agent is that it does not represent a chat session. It will eventually communicate something to the user or the outside world, but all the messages that it sends in between are usually not revealed. The question is: how does it create that message? We have one tool which is the output tool. The agent uses it explicitly to communicate to the human. We then use a prompt to instruct it when to use that tool. In our case the output tool sends an email.&lt;/p&gt;
    &lt;p&gt;But that turns out to pose a few other challenges. One is that it‚Äôs surprisingly hard to steer the wording and tone of that output tool compared to just using the main agent loop‚Äôs text output as the mechanism to talk to the user. I cannot say why this is, but I think it‚Äôs probably related to how these models are trained.&lt;/p&gt;
    &lt;p&gt;One attempt that didn‚Äôt work well was to have the output tool run another quick LLM like Gemini 2.5 Flash to adjust the tone to our preference. But this increases latency and actually reduces the quality of the output. In part, I think the model just doesn‚Äôt word things correctly and the subtool doesn‚Äôt have sufficient context. Providing more slices of the main agentic context into the subtool makes it expensive and also didn‚Äôt fully solve the problem. It also sometimes reveals information in the final output that we didn‚Äôt want to be there, like the steps that led to the end result.&lt;/p&gt;
    &lt;p&gt;Another problem with an output tool is that sometimes it just doesn‚Äôt call the tool. One of the ways in which we‚Äôre forcing this is we remember if the output tool was called. If the loop ends without the output tool, we inject a reinforcement message to encourage it to use the output tool.&lt;/p&gt;
    &lt;p&gt;Overall our choices for models haven‚Äôt dramatically changed so far. I think Haiku and Sonnet are still the best tool callers available, so they make for excellent choices in the agent loop. They are also somewhat transparent with regards to what the RL looks like. The other obvious choices are the Gemini models. We so far haven‚Äôt found a ton of success with the GPT family of models for the main loop.&lt;/p&gt;
    &lt;p&gt;For the individual sub-tools, which in part might also require inference, our current choice is Gemini 2.5 if you need to summarize large documents or work with PDFs and things like that. That is also a pretty good model for extracting information from images, in particular because the Sonnet family of models likes to run into a safety filter which can be annoying.&lt;/p&gt;
    &lt;p&gt;There‚Äôs also probably the very obvious realization that token cost alone doesn‚Äôt really define how expensive an agent. A better tool caller will do the job in fewer tokens. There are some cheaper models available than sonnet today, but they are not necessarily cheaper in a loop.&lt;/p&gt;
    &lt;p&gt;But all things considered, not that much has changed in the last couple of weeks.&lt;/p&gt;
    &lt;p&gt;We find testing and evals to be the hardest problem here. This is not entirely surprising, but the agentic nature makes it even harder. Unlike prompts, you cannot just do the evals in some external system because there‚Äôs too much you need to feed into it. This means you want to do evals based on observability data or instrumenting your actual test runs. So far none of the solutions we have tried have convinced us that they found the right approach here. Unfortunately, I have to report that at the moment we haven‚Äôt found something that really makes us happy. I hope we‚Äôre going to find a solution for this because it is becoming an increasingly frustrating aspect of building an agent.&lt;/p&gt;
    &lt;p&gt;As for my experience with coding agents, not really all that much has changed. The main new development is that I‚Äôm trialing Amp more. In case you‚Äôre curious why: it‚Äôs not that it‚Äôs objectively a better agent than what I‚Äôm using, but I really quite like the way they‚Äôre thinking about agents from what they‚Äôre posting. The interactions of the different sub agents like the Oracle with the main loop is beautifully done, and not many other harnesses do this today. It‚Äôs also a good way for me to validate how different agent designs work. Amp, similar to Claude Code, really feels like a product built by people who also use their own tool. I do not feel every other agent in the industry does this.&lt;/p&gt;
    &lt;p&gt;That‚Äôs just a random assortment of things that I feel might also be worth sharing:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lucumr.pocoo.org/2025/11/21/agents-are-hard/"/><published>2025-11-22T11:27:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46013960</id><title>ADHD and Monotropism (2023)</title><updated>2025-11-22T12:56:44.855722+00:00</updated><content>&lt;doc fingerprint="9418997626f309b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Fergus Murray with Sonny Hallett (2023)&lt;/p&gt;
    &lt;p&gt;Monotropism was formulated as a theory of autism. It seeks to explain the experiences and traits of autistic people in terms of a tendency for resources like attention to be concentrated on a small number of things at a time, with little left over for everything else. Through this lens we can make sense of autistic social, sensory and executive functioning differences, as laid out in Monotropism ‚Äì Explanations.&lt;/p&gt;
    &lt;p&gt;As time has gone on, it has become clear that many diagnosed with Attention Deficit Hyperactivity Disorder (ADHD) also identify strongly with many aspects of monotropism. I want to explore this by looking at the diverse ways that autism and ADHD present; where the traits associated with ADHD fit in with monotropism in an obvious way, and where they might seem to be in tension; and what this might mean for how we think about diagnoses and neurodiversity. Much of what I have to say here is necessarily speculative, all of it calls for further research, and parts of it may be in tension with some of the ways that many people are used to talking about neurodivergence.&lt;/p&gt;
    &lt;p&gt;The way that ADHD and autism are characterised in diagnostic manuals is completely different. ADHD is treated as primarily an attentional difference; autism as chiefly social in nature. Where descriptions do overlap, they can seem contradictory: autism is apparently characterised by rigid, restricted interests, while ADHD is said to cause impulsive behaviour and an inability to concentrate.&lt;/p&gt;
    &lt;p&gt;So the facts that anywhere from 30% to 80% of autistic people seemingly fit the diagnostic criteria for ADHD, and the two clearly run in the same families, might initially seem surprising. It cries out for an explanation. One possibility is that autism and ADHD ‚Äì or a Kinetic Cognitive Style (KCS), as I prefer to call it ‚Äì share an underlying cause. Monotropism has been put forward as one candidate for this, for example in Patrick Dwyer‚Äôs Revisiting Monotropism.&lt;/p&gt;
    &lt;p&gt;It is well established that autism can manifest very differently in different people, in ways that can seem contradictory. We know that autism can come with hyperlexia, or serious language difficulties. We know that it‚Äôs associated with sensory seeking and sensory avoidance. We understand that it might come with with crystal-clear memories, or forgetfulness. All of these things can coexist in one person, or just a selection.&lt;/p&gt;
    &lt;p&gt;With this in mind, it is perhaps not such a stretch to suggest that impulsivity, inattention and hyperactivity might share cognitive or neurological roots with their apparent opposites, like inflexibility, hyperfocus and inertia. When and how such traits manifest might depend on a person‚Äôs interests and experiences, or it might have to do with innate neurocognitive differences. Understanding this kind of variation fully would take far more research on the life experiences and psychological development of people with a variety of cognitive styles, without assuming that current diagnostic categories reflect objectively real categories of human being.&lt;/p&gt;
    &lt;p&gt;Impulsivity could come from the monotropic tendency to lose awareness of things as soon as our attention shifts away from them. Inattention is a very familiar thing among autistic people ‚Äì not an attention deficit, which was never the right term, but profound difficulty steering attention in directions which don‚Äôt align with our current interests. Hyperfocusing is common with KCS, as it is with autism.&lt;/p&gt;
    &lt;p&gt;Hyperactivity can refer to a need to keep moving, which bears a striking resemblance to the autistic need to stim. It can also refer to a cognitive tendency which is a little harder to reconcile with how monotropism has been characterised: a habit of hopping mentally from one thing to another. In contrast, difficulty shifting from one attention tunnel to another has been a central feature of the ways monotropism has been described. This tension is worth digging into.&lt;/p&gt;
    &lt;p&gt;It might be that a Kinetic Cognitive Style arises out of a combination of a relatively monotropic processing style combined with other factors ‚Äì difficulty accessing flow states, for example, as suggested by some recent research (Grotewiel et al 2022). There are all kinds of reasons why people might not be able to enter ‚Äòflowy focus tunnels‚Äò, as Jamie Knight calls them. They might have too many distractions, or too much nervous energy; they might not feel safe enough to lose themselves in the flow; they might have had bad experiences being told off for doing so, or been wrenched out of them too many times. They might just be too depleted to be able to connect deeply with their passions, something which also occurs during autistic burnout.&lt;/p&gt;
    &lt;p&gt;We know that novelty-seeking is a trait that varies greatly between people. It‚Äôs also possible that some people just have naturally very mobile attention, which might compensate for the monotropic tendency for attention to get sucked into one thing at a time. And maybe some of that apparent attention-hopping happens within an attention tunnel anyway, and other people just aren‚Äôt seeing the connections! KCS might look like polytropism sometimes, but I think that can be misleading. I delayed getting my own autism assessment for years because I mistook my serial monotropism for polytropism: I told myself I was multi-tasking, when it would probably be more accurate to say I repeatedly forgot what I was supposed to be doing.&lt;/p&gt;
    &lt;p&gt;Meanwhile, it is likely that monotropism doesn‚Äôt necessarily give rise to autism in the sense required by diagnostic manuals ‚Äì but that above a certain level of intensity, or in combination with other factors, it causes the familiar social differences, fixity and so on. An early intense interest in other people, and how they behave, might equip someone with tools that will allow them to avoid being seen as too socially weird. The ability to present a ‚Äònormal-looking‚Äô face to the world is likely a major factor in the under-identification of autistic girls, who face far more social pressure to blend in than boys do. None of this changes a person‚Äôs cognitive style; but then, autism, like ADHD, has always been assessed based on outward presentation. One hope for Monotropism as a theory is that it helps us to make sense of these things from an internal perspective, rather than looking only at the surface level.&lt;/p&gt;
    &lt;p&gt;It is, I think, too early to say with any confidence that autism and ADHD (or KCS) share a common root in monotropism, but the overlapping traits of the people receiving each label clearly demand some kind of explanation, and preliminary results do suggest that each is strongly correlated with monotropism ‚Äì especially in combination. With any luck, we will see a good deal more research on this in coming years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://monotropism.org/adhd/"/><published>2025-11-22T11:31:30+00:00</published></entry></feed>