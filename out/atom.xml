<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-08-30T20:34:33.171581+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45071722</id><title>Show HN: Hacker News em dash user leaderboard pre-ChatGPT</title><updated>2025-08-30T20:39:25.837662+00:00</updated><content>&lt;doc fingerprint="3518d6c6043b3b90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms&lt;/head&gt;
    &lt;head rend="h2"&gt;What is attention?&lt;/head&gt;
    &lt;p&gt;In any autoregressive model, the prediction of the future tokens is based on some preceding context. However, not all the tokens within this context equally contribute to the prediction, because some tokens might be more relevant than others. The attention mechanism addresses this by allowing the model to concentrate on the important context words selectively, while generating each output word or token. Consider the popular example that explains the attention mechanism.&lt;/p&gt;
    &lt;p&gt;“The animal didn’t cross the street because it was too tired”.&lt;/p&gt;
    &lt;p&gt;In this sentence, the pronoun “it” could refer to either “animal” or “street”. Attention helps the model to associate “it” with “animal” rather than “street” by weighing the relative importance of each word. This helps the model to understand the relationships between words and capture the contextual meaning in various NLP tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;How is attention calculated?&lt;/head&gt;
    &lt;p&gt;There are various types of attention mechanisms today, beginning with the Multi-Head Attention (MHA), which introduced the attention concept in the seminal paper. More recently, advanced variants like Multi-Latent Head Attention (MHLA) have been employed in popular models like Deepseek. This blog aims to cover the fundamentals of each attention mechanism, including the core ideas, advantages, limitations, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Concepts in Attention Mechanisms&lt;/head&gt;
    &lt;p&gt;Before diving into specific types of attention, we need to understand some fundamental concepts that underpin all the various attention mechanisms.&lt;/p&gt;
    &lt;p&gt;The main idea behind the attention mechanism is to dynamically weigh, and focus on relevant parts of inputs. Attention is required in both the encoding and decoding stages. But in this blog, we will be discussing this from a decoder's point of view.&lt;/p&gt;
    &lt;p&gt;During each generation step, we need to understand the attention weights, which help us to get a better contextual representation for the next word prediction. At its core, attention operates through three fundamental components — queries, keys, and values — that work together with attention scores to create a flexible, context-aware vector representation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Query (Q): The query is a vector that represents the current token for which the model wants to compute attention.&lt;/item&gt;
      &lt;item&gt;Key (K): Keys are vectors that represent the elements in the context against which the query is compared, to determine the relevance.&lt;/item&gt;
      &lt;item&gt;Attention Scores: These are computed using Query and Key vectors to determine the amount of attention to be paid to each context token.&lt;/item&gt;
      &lt;item&gt;Value (V): Values are the vectors that represent the actual contextual information. After calculating the attention scores using Query and Key vectors, these scores are applied against Value vectors to get the final context vector&lt;/item&gt;
      &lt;item&gt;KV Caching: Since the key and value vectors are for previous tokens, we can skip this computation for those tokens that are already calculated. KV caching stores the precomputed keys and values from the previous computations, which helps in faster decoding in autoregressive models by reusing the cached vectors. However, the Query vectors cannot be cached, since they are calculated for the current token.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To understand how each of these vectors are scores are calculated you can refer to this blog.&lt;/p&gt;
    &lt;p&gt;The high-level concepts remain consistent across all types of attention mechanisms. However, the key difference lies in how efficiently each of them executes the attention process without compromising on performance. Innovations focus on computational speed, reducing memory usage, improving scalability across longer sequences, etc.&lt;/p&gt;
    &lt;p&gt;Now, let's dive into each of these techniques&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Head Attention (MHA)&lt;/head&gt;
    &lt;p&gt;In multi-head attention, for computing the attention weights for the ith token, first, a query vector is calculated for that token. To calculate the attention weights for the token, this query vector is compared with all the preceding tokens. For that, key vectors are calculated for all the preceding tokens. These comparisons will generate an attention score, which is then used to produce a weighted score for each token using the corresponding value vectors.&lt;/p&gt;
    &lt;p&gt;In multi-head attention, this process is repeated in parallel across multiple attention “heads”. Each head has its own query, value, and key vectors, using which it calculates the relationship between the words. The final output context vector will be the concatenated output from all the attention heads.&lt;/p&gt;
    &lt;p&gt;Now, this seems straightforward. However, as the context grows, the number of Key and Value vectors will increase dramatically, because these vectors need to be calculated and stored for all the context tokens. For a sequence length of n, each query vector must be compared against all n key vectors and then perform the weighted combination using n value vectors. This results in a quadratic complexity in both computation and memory.&lt;/p&gt;
    &lt;p&gt;KV cache can help in reducing the computation and memory overhead during inference. But as the context grows, the size of the cache grows linearly with sequence length to store all the keys and values for all the preceding tokens. KV cache reduces the redundant computations, but will not reduce the fundamental cost of attending to all the previous tokens.&lt;/p&gt;
    &lt;p&gt;Models using MHA – Bert, RoBerta, T5, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Query Attention (MQA)&lt;/head&gt;
    &lt;p&gt;A significant challenge with MHA was the high computational and memory overhead associated with storing and processing separate Key and Value vectors for each attention head.&lt;/p&gt;
    &lt;p&gt;MQA addresses this problem by using multiple query heads but sharing a common set of Key and Value vectors across all the heads. In other words, there are still “h” distinct Query projections using which the model attends the current token from multiple perspectives. But the same Key and Value vectors are used for every head.&lt;/p&gt;
    &lt;p&gt;This approach will greatly reduce the memory bandwidth requirements without significantly sacrificing the model performance. By sharing the Key and Value vectors, MQA enables an efficient inference, especially for Large language models with long context lengths.&lt;/p&gt;
    &lt;p&gt;Here, the Key and Value vectors need to be calculated only once for a token instead of “h” times, which reduces the computation cost of Key/Value projection. But note that for calculating the attention score, each query head is still multiplied by the Key vectors and then weighed using the Value vectors. So this remains the same.&lt;/p&gt;
    &lt;p&gt;Also, with MQA only one set of Key-Value pairs needs to be cached, regardless of the number of Query heads. This lets the KV cache size grow gradually as the sequence length grows, leading to much lower memory requirements when compared to MHA&lt;/p&gt;
    &lt;p&gt;Models using MQA – PaLM, Falcon&lt;/p&gt;
    &lt;head rend="h2"&gt;Grouped Query Attention (GQA)&lt;/head&gt;
    &lt;p&gt;Grouped Query attention offers a balance between the MHA and MQA. As we saw earlier, traditional MHA requires significant memory and computation overhead due to separate Key-Value vectors for each Query head, and the computation overhead even increases as the number of heads increases. MQA addresses this by having a shared Key-Value, which reduces the computation cost and memory, but it may impact the model performance.&lt;/p&gt;
    &lt;p&gt;GQA offers a compromise between these two extremes. Instead of having a common Key-Value for all the heads, GQA divides the Query heads into “g” groups and lets each group share a common Key and Value head. We can say, MHA and MQA come as two extreme cases of GQA, with g=1 leading to MQA and g=h leading to MHA. This approach reduces the memory and computational requirements compared to MHA while retaining a better performance than MQA.&lt;/p&gt;
    &lt;p&gt;Models using GQA – Llama2, Llama3, Mistral&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Head Latent Attention (MHLA)&lt;/head&gt;
    &lt;p&gt;While GQA performs better than MQA, but still may not match MHA’s performance in some complex tasks.&lt;/p&gt;
    &lt;p&gt;MHLA is a recent innovation in transformer architecture introduced in models like DeepSeek. Its main goal is to dramatically reduce memory usage and accelerate inference, especially for large language models (LLMs), without loss in model performance.&lt;/p&gt;
    &lt;p&gt;The idea is to attain a performance near MHA. So we need to consider separate Key value heads for each attention head, like in MHA, but also improve the inference speed by reducing the memory overhead for storing the large amounts of Key value vectors.&lt;/p&gt;
    &lt;p&gt;MHLA addresses the challenge of high memory usage and slow inference by compressing the Key and Value representations into a much smaller latent space using low-rank projections. Specifically, instead of storing the full Key and Value vectors for every token and head, MHLA applies a linear transformation that projects these vectors into a lower-dimensional space.&lt;/p&gt;
    &lt;p&gt;So during the inference:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A down-projection weight matrix W(DKV) is introduced and is multiplied with the input sequence to obtain a compressed latent vector C(KV) for keys and Values. This latent vector is stored in cache, which is significantly smaller in size when compared to the full key and Value vectors&lt;/item&gt;
      &lt;item&gt;This is then multiplied by an up-projection matrix W(UK) and W(UV) to get the Key and Value vectors&lt;/item&gt;
      &lt;item&gt;Additionally, the matrix W(KR) is used to produce a decoupled Key that carries the Rotary Positional embedding&lt;/item&gt;
      &lt;item&gt;Additionally, the same process is done for attention Queries as well, which will reduce the activation memory during training&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MHLA supports switching between two computation paradigms for different stages. During the training stage, which is computationally intensive, it operates similarly to MHA, where the computational overhead is slightly lower than conventional MHA. During inference, it can seamlessly switch to a paradigm similar to MQA. Here, the cached KV head interacts with all query heads to produce the final output.&lt;/p&gt;
    &lt;p&gt;Models using MHLA– Deepseek- V2, Deep seek V2&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In addition to the topics discussed, there are various innovative methods that are designed to optimise the challenges of the traditional attention technique. Some of these include sparse attention, efficient attention, memory augmented attention, etc. These approaches reflect the focus on ongoing research for making the attention more scalable, faster, and adaptable across various tasks and requirements.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this post! Let me know if you liked it, have questions, or spotted an error. Please feel free to contact or follow me through LinkedIn, Twitter, or Medium.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gally.net/miscellaneous/hn-em-dash-user-leaderboard.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45072160</id><title>From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms</title><updated>2025-08-30T20:39:25.591700+00:00</updated><content>&lt;doc fingerprint="3518d6c6043b3b90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms&lt;/head&gt;
    &lt;head rend="h2"&gt;What is attention?&lt;/head&gt;
    &lt;p&gt;In any autoregressive model, the prediction of the future tokens is based on some preceding context. However, not all the tokens within this context equally contribute to the prediction, because some tokens might be more relevant than others. The attention mechanism addresses this by allowing the model to concentrate on the important context words selectively, while generating each output word or token. Consider the popular example that explains the attention mechanism.&lt;/p&gt;
    &lt;p&gt;“The animal didn’t cross the street because it was too tired”.&lt;/p&gt;
    &lt;p&gt;In this sentence, the pronoun “it” could refer to either “animal” or “street”. Attention helps the model to associate “it” with “animal” rather than “street” by weighing the relative importance of each word. This helps the model to understand the relationships between words and capture the contextual meaning in various NLP tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;How is attention calculated?&lt;/head&gt;
    &lt;p&gt;There are various types of attention mechanisms today, beginning with the Multi-Head Attention (MHA), which introduced the attention concept in the seminal paper. More recently, advanced variants like Multi-Latent Head Attention (MHLA) have been employed in popular models like Deepseek. This blog aims to cover the fundamentals of each attention mechanism, including the core ideas, advantages, limitations, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Concepts in Attention Mechanisms&lt;/head&gt;
    &lt;p&gt;Before diving into specific types of attention, we need to understand some fundamental concepts that underpin all the various attention mechanisms.&lt;/p&gt;
    &lt;p&gt;The main idea behind the attention mechanism is to dynamically weigh, and focus on relevant parts of inputs. Attention is required in both the encoding and decoding stages. But in this blog, we will be discussing this from a decoder's point of view.&lt;/p&gt;
    &lt;p&gt;During each generation step, we need to understand the attention weights, which help us to get a better contextual representation for the next word prediction. At its core, attention operates through three fundamental components — queries, keys, and values — that work together with attention scores to create a flexible, context-aware vector representation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Query (Q): The query is a vector that represents the current token for which the model wants to compute attention.&lt;/item&gt;
      &lt;item&gt;Key (K): Keys are vectors that represent the elements in the context against which the query is compared, to determine the relevance.&lt;/item&gt;
      &lt;item&gt;Attention Scores: These are computed using Query and Key vectors to determine the amount of attention to be paid to each context token.&lt;/item&gt;
      &lt;item&gt;Value (V): Values are the vectors that represent the actual contextual information. After calculating the attention scores using Query and Key vectors, these scores are applied against Value vectors to get the final context vector&lt;/item&gt;
      &lt;item&gt;KV Caching: Since the key and value vectors are for previous tokens, we can skip this computation for those tokens that are already calculated. KV caching stores the precomputed keys and values from the previous computations, which helps in faster decoding in autoregressive models by reusing the cached vectors. However, the Query vectors cannot be cached, since they are calculated for the current token.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To understand how each of these vectors are scores are calculated you can refer to this blog.&lt;/p&gt;
    &lt;p&gt;The high-level concepts remain consistent across all types of attention mechanisms. However, the key difference lies in how efficiently each of them executes the attention process without compromising on performance. Innovations focus on computational speed, reducing memory usage, improving scalability across longer sequences, etc.&lt;/p&gt;
    &lt;p&gt;Now, let's dive into each of these techniques&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Head Attention (MHA)&lt;/head&gt;
    &lt;p&gt;In multi-head attention, for computing the attention weights for the ith token, first, a query vector is calculated for that token. To calculate the attention weights for the token, this query vector is compared with all the preceding tokens. For that, key vectors are calculated for all the preceding tokens. These comparisons will generate an attention score, which is then used to produce a weighted score for each token using the corresponding value vectors.&lt;/p&gt;
    &lt;p&gt;In multi-head attention, this process is repeated in parallel across multiple attention “heads”. Each head has its own query, value, and key vectors, using which it calculates the relationship between the words. The final output context vector will be the concatenated output from all the attention heads.&lt;/p&gt;
    &lt;p&gt;Now, this seems straightforward. However, as the context grows, the number of Key and Value vectors will increase dramatically, because these vectors need to be calculated and stored for all the context tokens. For a sequence length of n, each query vector must be compared against all n key vectors and then perform the weighted combination using n value vectors. This results in a quadratic complexity in both computation and memory.&lt;/p&gt;
    &lt;p&gt;KV cache can help in reducing the computation and memory overhead during inference. But as the context grows, the size of the cache grows linearly with sequence length to store all the keys and values for all the preceding tokens. KV cache reduces the redundant computations, but will not reduce the fundamental cost of attending to all the previous tokens.&lt;/p&gt;
    &lt;p&gt;Models using MHA – Bert, RoBerta, T5, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Query Attention (MQA)&lt;/head&gt;
    &lt;p&gt;A significant challenge with MHA was the high computational and memory overhead associated with storing and processing separate Key and Value vectors for each attention head.&lt;/p&gt;
    &lt;p&gt;MQA addresses this problem by using multiple query heads but sharing a common set of Key and Value vectors across all the heads. In other words, there are still “h” distinct Query projections using which the model attends the current token from multiple perspectives. But the same Key and Value vectors are used for every head.&lt;/p&gt;
    &lt;p&gt;This approach will greatly reduce the memory bandwidth requirements without significantly sacrificing the model performance. By sharing the Key and Value vectors, MQA enables an efficient inference, especially for Large language models with long context lengths.&lt;/p&gt;
    &lt;p&gt;Here, the Key and Value vectors need to be calculated only once for a token instead of “h” times, which reduces the computation cost of Key/Value projection. But note that for calculating the attention score, each query head is still multiplied by the Key vectors and then weighed using the Value vectors. So this remains the same.&lt;/p&gt;
    &lt;p&gt;Also, with MQA only one set of Key-Value pairs needs to be cached, regardless of the number of Query heads. This lets the KV cache size grow gradually as the sequence length grows, leading to much lower memory requirements when compared to MHA&lt;/p&gt;
    &lt;p&gt;Models using MQA – PaLM, Falcon&lt;/p&gt;
    &lt;head rend="h2"&gt;Grouped Query Attention (GQA)&lt;/head&gt;
    &lt;p&gt;Grouped Query attention offers a balance between the MHA and MQA. As we saw earlier, traditional MHA requires significant memory and computation overhead due to separate Key-Value vectors for each Query head, and the computation overhead even increases as the number of heads increases. MQA addresses this by having a shared Key-Value, which reduces the computation cost and memory, but it may impact the model performance.&lt;/p&gt;
    &lt;p&gt;GQA offers a compromise between these two extremes. Instead of having a common Key-Value for all the heads, GQA divides the Query heads into “g” groups and lets each group share a common Key and Value head. We can say, MHA and MQA come as two extreme cases of GQA, with g=1 leading to MQA and g=h leading to MHA. This approach reduces the memory and computational requirements compared to MHA while retaining a better performance than MQA.&lt;/p&gt;
    &lt;p&gt;Models using GQA – Llama2, Llama3, Mistral&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Head Latent Attention (MHLA)&lt;/head&gt;
    &lt;p&gt;While GQA performs better than MQA, but still may not match MHA’s performance in some complex tasks.&lt;/p&gt;
    &lt;p&gt;MHLA is a recent innovation in transformer architecture introduced in models like DeepSeek. Its main goal is to dramatically reduce memory usage and accelerate inference, especially for large language models (LLMs), without loss in model performance.&lt;/p&gt;
    &lt;p&gt;The idea is to attain a performance near MHA. So we need to consider separate Key value heads for each attention head, like in MHA, but also improve the inference speed by reducing the memory overhead for storing the large amounts of Key value vectors.&lt;/p&gt;
    &lt;p&gt;MHLA addresses the challenge of high memory usage and slow inference by compressing the Key and Value representations into a much smaller latent space using low-rank projections. Specifically, instead of storing the full Key and Value vectors for every token and head, MHLA applies a linear transformation that projects these vectors into a lower-dimensional space.&lt;/p&gt;
    &lt;p&gt;So during the inference:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A down-projection weight matrix W(DKV) is introduced and is multiplied with the input sequence to obtain a compressed latent vector C(KV) for keys and Values. This latent vector is stored in cache, which is significantly smaller in size when compared to the full key and Value vectors&lt;/item&gt;
      &lt;item&gt;This is then multiplied by an up-projection matrix W(UK) and W(UV) to get the Key and Value vectors&lt;/item&gt;
      &lt;item&gt;Additionally, the matrix W(KR) is used to produce a decoupled Key that carries the Rotary Positional embedding&lt;/item&gt;
      &lt;item&gt;Additionally, the same process is done for attention Queries as well, which will reduce the activation memory during training&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MHLA supports switching between two computation paradigms for different stages. During the training stage, which is computationally intensive, it operates similarly to MHA, where the computational overhead is slightly lower than conventional MHA. During inference, it can seamlessly switch to a paradigm similar to MQA. Here, the cached KV head interacts with all query heads to produce the final output.&lt;/p&gt;
    &lt;p&gt;Models using MHLA– Deepseek- V2, Deep seek V2&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In addition to the topics discussed, there are various innovative methods that are designed to optimise the challenges of the traditional attention technique. Some of these include sparse attention, efficient attention, memory augmented attention, etc. These approaches reflect the focus on ongoing research for making the attention more scalable, faster, and adaptable across various tasks and requirements.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this post! Let me know if you liked it, have questions, or spotted an error. Please feel free to contact or follow me through LinkedIn, Twitter, or Medium.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24"/></entry><entry><id>https://news.ycombinator.com/item?id=45073746</id><title>Hardening Firefox – a checklist for improved browser privacy</title><updated>2025-08-30T20:39:25.393645+00:00</updated><content>&lt;doc fingerprint="dc8ad1d63ca9eb26"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Firefox Privacy Checklist&lt;/head&gt;&lt;p&gt;/ 3 min read&lt;/p&gt;Updated:&lt;head class="font-semibold hover:marker:text-accent cursor-pointer"&gt;Table of Contents&lt;/head&gt;&lt;p&gt;This checklist will walk you (and me) through the settings and extensions I use to improve my privacy when using Firefox.&lt;/p&gt;&lt;p&gt;If you’re looking for a web browser that offers a high degree of privacy out of the box with minimal setup, Brave is a common choice. However, I prefer Firefox for several reasons:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Firefox is developed by the nonprofit organization Mozilla.&lt;/item&gt;&lt;item&gt;I value Mozilla’s commitment to open source software.&lt;/item&gt;&lt;item&gt;Firefox is not based on Chromium. Brave, like most browsers, is based on Chromium, which is developed primarily by Google.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;While there are many web browsers to choose from, I’ve decided Firefox is best for me. This post is a checklist of how I’ve configured it to better protect my privacy while browsing the web.&lt;/p&gt;&lt;head rend="h3"&gt;1. Basic Privacy Settings&lt;/head&gt;&lt;p&gt;Access Firefox’s settings by clicking the menu button (three horizontal lines) in the top-right corner and selecting “Settings.”&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Change Default Search Engine: In the Search tab, change the “Default Search Engine” to a privacy-respecting option like DuckDuckGo.&lt;/item&gt;&lt;item&gt;Enable HTTPS-Only Mode: In the Privacy &amp;amp; Security tab, scroll down to “HTTPS-Only Mode” and select “Enable HTTPS-Only Mode in all windows.”&lt;/item&gt;&lt;item&gt;Disable Telemetry: Still in Privacy &amp;amp; Security, scroll to “Firefox Data Collection and Use” and uncheck all the boxes to stop Firefox from sending data back to Mozilla.&lt;/item&gt;&lt;item&gt;Set Enhanced Tracking Protection to Strict: Under Privacy &amp;amp; Security, set “Enhanced Tracking Protection” to Strict. This offers stronger protection against trackers. If a site breaks, you can easily disable it for that specific site by clicking the shield icon in the address bar.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;2. Recommended Extensions&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Install uBlock Origin: A comprehensive content blocker that stops ads and tracking scripts, which speeds up page loading and enhances privacy.&lt;/item&gt;&lt;item&gt;Install ClearURLs: This extension automatically removes tracking elements from URLs, helping prevent another form of web tracking.&lt;/item&gt;&lt;item&gt;Install Privacy Badger: From the Electronic Frontier Foundation, this extension automatically learns to block invisible trackers. Instead of relying on blocklists, it discovers trackers based on behavior.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;3. Advanced Configuration (&lt;code&gt;about:config&lt;/code&gt;)&lt;/head&gt;&lt;p&gt;To access this, type &lt;code&gt;about:config&lt;/code&gt; into the address bar and accept the warning.&lt;/p&gt;&lt;p&gt;Warning: Changing advanced configuration preferences can impact Firefox performance or security. Proceed with caution.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Isolate Cookies to the First-Party Domain: &lt;list rend="ul"&gt;&lt;item&gt;Search for &lt;code&gt;privacy.firstparty.isolate&lt;/code&gt;and set its value to&lt;code&gt;true&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;This prevents cookies from tracking you from one site to another, but it can break single sign-on on some websites.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Search for &lt;/item&gt;&lt;item&gt;Resist Fingerprinting: &lt;list rend="ul"&gt;&lt;item&gt;I previously set &lt;code&gt;privacy.resistFingerprinting&lt;/code&gt;to&lt;code&gt;true&lt;/code&gt;to make my browser fingerprint less unique.&lt;/item&gt;&lt;item&gt;It caused minor display issues on some sites and broke image uploads to Bluesky, so I set it back to &lt;code&gt;false&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;I previously set &lt;/item&gt;&lt;/list&gt;&lt;p&gt;By following this checklist, you can significantly improve your privacy while using Firefox. Please let me know if I’m missing anything in the comments.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andrewmarder.net/firefox/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074071</id><title>Nokia’s legendary font makes for a great user interface font</title><updated>2025-08-30T20:39:24.998782+00:00</updated><content>&lt;doc fingerprint="1c9023579ba34fd7"&gt;
  &lt;main&gt;
    &lt;p&gt;If you’re of a certain age (and not American), there’s a specific corporate font you’re most likely aware of. You may not know its exact name, and you may not actively remember it, but once you see it, you know exactly what you’re looking at. The font’s called Nokia Sans (and Nokia Serif), and it was used by pretty much every single Nokia device between roughly 2002 and 2013 or so, when it was replaced by a very bland font made by Bruno Maag (with help from the person who designed Comic Sans) that they used after that.&lt;/p&gt;
    &lt;p&gt;I can’t remember why, exactly, but I got majorly nostalgic for Nokia’s characteristic, recognisable font, and decided to see if it would work as a user interface font. Now, the font is still owned by Nokia and I couldn’t find a proper place to download it, but I eventually stumbled upon a site that had each individual variant listed for download. I downloaded each of them, installed them using KDE’s font installation method, and tried it out as my user interface font.&lt;/p&gt;
    &lt;p&gt;You’ll quickly discover you shouldn’t use the regular variant, but should instead opt for the Nokia Sans Wide variant. Back in 2011, when Nokia originally announced it was replacing Nokia Sans, the creator of the font, Erik Spiekermann, responded to the announcement on his blog. Apparently, one of the major reasons for Nokia to change fonts was that they claimed Nokia Sans wouldn’t work as a user interface font, but Spiekermann obviously disagrees, pointing specifically to the Wide variant. In fact, Spiekermann does not pull any punches.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;After 10 years it was high time to look at Nokia’s typefaces as the dominant visual voice of the brand but whoever decided on a completely new direction was either not aware of what was available or was persuaded by Bruno Maag to start over. Bruno may not create the most memorable typefaces, but he certainly knows how to sell them. And technically, their fonts are excellent. Too bad they didn’t have the confidence to work with me on an update. Instead they’re throwing out ten years of brand recognition in favour of blandness.&lt;/p&gt;↫ Erik Spiekermann&lt;/quote&gt;
    &lt;p&gt;I was pleasently surprised by just how nice the font looks when used as a general user interface font. It’s extremely legible at a variety of sizes, and has a ton of character without becoming gimmicky or overbearing. What originally started as mere curiosity has now become my UI font of choice on all my machines, finally displacing Inter after many years of uncontested service. Of course, all of this is deeply personal and 95% an issue of taste, but I wanted to write about it to see if I’m just entirely crazy, or if there’s some method to my madness.&lt;/p&gt;
    &lt;p&gt;Do note that I’m using high DPI displays, and KDE on Wayland, and that all of this may look different on Windows or macOS, or on displays with lower DPI. One of Inter’s strengths is that it renders great on both high and lower DPI displays, but since I don’t have any lower DPI displays anymore, I can’t test it in such an environment. I’m also not entirely sure about the legal status of downloading fonts like this, but I am fairly sure you’re at least allowed to use non-free fonts for personal, non-commercial use, but please don’t quote me on that. Since downloading each variant of these Nokia fonts is annoying, I’d love to create and upload a zip file containing all of them, but I’m sure that’s illegal.&lt;/p&gt;
    &lt;p&gt;I’m not a font connoisseur, so I may be committing a huge faux pas here? Not that I care, but reading about font nerds losing their minds over things I never even noticed is always highly entertaining.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.osnews.com/story/143222/it-turns-out-nokias-legendary-font-makes-for-a-great-general-user-interface-font/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074110</id><title>De minimis exemption ends</title><updated>2025-08-30T20:34:39.701752+00:00</updated><content>&lt;doc fingerprint="5ffa111a148a3cee"&gt;
  &lt;main&gt;
    &lt;p&gt;Physical limits of NIC, 10M CC, 5M RPS, 1M CPS&lt;/p&gt;
    &lt;p&gt;Kernel bypass, lockless, no scheduling, no interruption&lt;/p&gt;
    &lt;p&gt;Easy adaption for any application&lt;/p&gt;
    &lt;p&gt;With the rapid development of NIC, the poor performance of data packets processing with Linux kernel has become the bottleneck. However, the rapid development of the Internet needs high performance of network processing, kernel bypass has caught more and more attention. There are various similar technologies appear, such as DPDK, NETMAP and PF_RING. The main idea of kernel bypass is that Linux is only used to deal with control flow, all data streams are processed in user space. Therefore, kernel bypass can avoid performance bottlenecks caused by kernel packet copy, thread scheduling, system calls and interrupt. Furthermore, kernel bypass can achieve higher performance with multi optimizing methods. Within various techniques, DPDK has been widely used because of its more thorough isolation from kernel scheduling and active community support.&lt;/p&gt;
    &lt;p&gt;F-Stack is an open source network framework with high performance based on DPDKï¼ include an user space TCP/IP stack(port FreeBSD 11.0 stable), Posix API(Socket, Epoll, Kqueue), Progamming SDK(Coroutine) and some apps(Nginx, Redis) interface.&lt;/p&gt;
    &lt;p&gt;Currently, there are various products in Tencent Cloud has used the F-Stack, such as DKDNS(DNSPod's authorization DNS server), HttpDNS (D+), COS access module, CDN access module, etc..&lt;/p&gt;
    &lt;p&gt;Now let's start using F-Stack to launch a HTTP server on EC2 with Nginx.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.washingtonpost.com/business/2025/08/30/de-minimis-tax-canceled-orders-delays/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074115</id><title>F-Stack – A network development kit with high performance based on DPDK</title><updated>2025-08-30T20:34:38.364716+00:00</updated><content>&lt;doc fingerprint="5ffa111a148a3cee"&gt;
  &lt;main&gt;
    &lt;p&gt;Physical limits of NIC, 10M CC, 5M RPS, 1M CPS&lt;/p&gt;
    &lt;p&gt;Kernel bypass, lockless, no scheduling, no interruption&lt;/p&gt;
    &lt;p&gt;Easy adaption for any application&lt;/p&gt;
    &lt;p&gt;With the rapid development of NIC, the poor performance of data packets processing with Linux kernel has become the bottleneck. However, the rapid development of the Internet needs high performance of network processing, kernel bypass has caught more and more attention. There are various similar technologies appear, such as DPDK, NETMAP and PF_RING. The main idea of kernel bypass is that Linux is only used to deal with control flow, all data streams are processed in user space. Therefore, kernel bypass can avoid performance bottlenecks caused by kernel packet copy, thread scheduling, system calls and interrupt. Furthermore, kernel bypass can achieve higher performance with multi optimizing methods. Within various techniques, DPDK has been widely used because of its more thorough isolation from kernel scheduling and active community support.&lt;/p&gt;
    &lt;p&gt;F-Stack is an open source network framework with high performance based on DPDKï¼ include an user space TCP/IP stack(port FreeBSD 11.0 stable), Posix API(Socket, Epoll, Kqueue), Progamming SDK(Coroutine) and some apps(Nginx, Redis) interface.&lt;/p&gt;
    &lt;p&gt;Currently, there are various products in Tencent Cloud has used the F-Stack, such as DKDNS(DNSPod's authorization DNS server), HttpDNS (D+), COS access module, CDN access module, etc..&lt;/p&gt;
    &lt;p&gt;Now let's start using F-Stack to launch a HTTP server on EC2 with Nginx.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.f-stack.org/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074147</id><title>Agent Client Protocol</title><updated>2025-08-30T20:34:38.156148+00:00</updated><content>&lt;doc fingerprint="7d89da1d4bcc10c1"&gt;
  &lt;main&gt;
    &lt;p&gt;The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).The protocol is still under development, but it should be complete enough to build interesting user experiences using it.&lt;/p&gt;
    &lt;p&gt;AI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users. This creates several problems:&lt;/p&gt;
    &lt;p&gt;Integration overhead: Every new agent-editor combination requires custom work&lt;/p&gt;
    &lt;p&gt;Limited compatibility: Agents work with only a subset of available editors&lt;/p&gt;
    &lt;p&gt;Developer lock-in: Choosing an agent often means accepting their available interfaces&lt;/p&gt;
    &lt;p&gt;ACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents. This decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.&lt;/p&gt;
    &lt;p&gt;ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://agentclientprotocol.com/overview/introduction"/></entry><entry><id>https://news.ycombinator.com/item?id=45074157</id><title>FBI cyber cop: Salt Typhoon pwned 'nearly every American'</title><updated>2025-08-30T20:34:37.886394+00:00</updated><content>&lt;doc fingerprint="f3b8b8159e0ca529"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FBI cyber cop: Salt Typhoon pwned 'nearly every American'&lt;/head&gt;
    &lt;head rend="h2"&gt;Plus millions of other people across 80+ countries&lt;/head&gt;
    &lt;p&gt;China's Salt Typhoon cyberspies hoovered up information belonging to millions of people in the United States over the course of the years-long intrusion into telecommunications networks, according to a top FBI cyber official.&lt;/p&gt;
    &lt;p&gt;"There's a good chance this espionage campaign has stolen information from nearly every American," Michael Machtinger, deputy assistant director for the FBI's cyber division, told The Register.&lt;/p&gt;
    &lt;p&gt;"There's a thought among the public that if you don't work in a sensitive area that the PRC might be interested in for its traditional espionage activities, then you are safe, they will not target you," he said, during a Thursday interview with The Register. "As we have seen from Salt Typhoon, this is no longer an assumption that anyone can afford to make."&lt;/p&gt;
    &lt;p&gt;The Beijing-backed spying campaign began at least in 2019 but wasn't uncovered by US authorities until last fall. On Wednesday, US law enforcement and intelligence agencies along with those from 12 other countries warned the ongoing espionage activity expanded far beyond nine American telcos and government networks. According to Machtinger, at least 80 countries were hit by the digital intrusions.&lt;/p&gt;
    &lt;p&gt;Around 200 American organizations were compromised by the espionage activity, Machtinger said, including the previously disclosed telecommunications firms such as Verizon and AT&amp;amp;T.&lt;/p&gt;
    &lt;p&gt;Yesterday's joint security alert also pointed the allies' collective finger at three China-based entities affiliated with Salt Typhoon: Sichuan Juxinhe Network Technology, Beijing Huanyu Tianqiong Information Technology, and Sichuan Zhixin Ruijie Network Technology. These companies, and likely others, provide cyber products and services to China's Ministry of State Security and People's Liberation Army, the governments said.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;What the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;"This is one of the most consequential cyber espionage breaches that we've ever seen in the United States," Machtinger said.&lt;/p&gt;
    &lt;p&gt;"What this really underscores is that what the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space," he added. "And that should really set off alarm bells for us — not only in the United States. The scale of indiscriminate targeting is unlike what we've seen in the past."&lt;/p&gt;
    &lt;p&gt;This indiscriminate targeting, as the FBI and White House security officials have previously noted, allowed Beijing’s snoops to geo-locate millions of mobile phone users, monitor their internet traffic, and, in some cases, record their phone calls. Victims reportedly included President Donald Trump and Vice President JD Vance.&lt;/p&gt;
    &lt;p&gt;Machtinger declined to confirm whether Trump and Vance were among those surveilled, but did say that victims included more than 100 current and former presidential administration officials.&lt;/p&gt;
    &lt;p&gt;"As we look at the impact on the different sets of victims," he said, Salt Typhoon collected "bulk information from millions of Americans."&lt;/p&gt;
    &lt;p&gt;For the more targeted group of individuals, "most of whom are very high-profile, current and former presidential administration officials, and campaign appointees from both major political parties," the data collection went much deeper, Machtinger added. "Down to intercepting actual content."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you thought China's Salt Typhoon was booted off critical networks, think again&lt;/item&gt;
      &lt;item&gt;China's Salt Typhoon spies spotted on US govt networks before telcos, CISA boss says&lt;/item&gt;
      &lt;item&gt;This is the FBI, open up. China's Volt Typhoon is on your network&lt;/item&gt;
      &lt;item&gt;How does China keep stealing our stuff, wonders DoD group responsible for keeping foreign agents out&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition to Salt Typhoon, the feds over the past year have issued warnings about other Chinese cyber operations. These include Volt Typhoon intruders, who infected hundreds of outdated routers to build a botnet and break into US critical infrastructure facilities. The Beijing-backed crew, we would later learn, was prepositioning itself and readying destructive cyberattacks.&lt;/p&gt;
    &lt;p&gt;Another China-linked crew, Silk Typhoon has spent more than a decade compromising IT and cloud providers to steal sensitive data from their government, technology, education, and legal and professional services customers.&lt;/p&gt;
    &lt;p&gt;China is not the only source of threats, Machtinger noted. Russia, Iran, North Korea, plus along with home-grown and international cybercriminals and ransomware crooks, assault computers and networks of both individuals and organizations, every day.&lt;/p&gt;
    &lt;p&gt;"These actors are going to continue their efforts, and they're going to get more sophisticated," Machtinger said. "We need to make sure that we, a nation, are taking cybersecurity seriously, updating systems, removing end-of-life devices, and making it as hard and costly as possible for the myriad of actors that are out there to successfully compromise." ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/08/28/fbi_cyber_cop_salt_typhoon/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074248</id><title>Cognitive Load is what matters</title><updated>2025-08-30T20:34:37.294551+00:00</updated><content>&lt;doc fingerprint="373978120d250fed"&gt;
  &lt;main&gt;&lt;p&gt;Readable version | Chinese translation | Korean translation | Turkish translation&lt;/p&gt;&lt;p&gt;It is a living document, last update: August 2025. Your contributions are welcome!&lt;/p&gt;&lt;p&gt;There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.&lt;/p&gt;&lt;p&gt;Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high cognitive load. It's not some fancy abstract concept, but rather a fundamental human constraint. It's not imagined, it's there and we can feel it.&lt;/p&gt;&lt;p&gt;Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Cognitive load is how much a developer needs to think in order to complete a task.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly four such chunks in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.&lt;/p&gt;&lt;p&gt;Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, the author had created a high cognitive load for us.&lt;/p&gt;&lt;p&gt;We should reduce the cognitive load in our projects as much as possible.&lt;/p&gt;&lt;p&gt;Intrinsic - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.&lt;/p&gt;&lt;p&gt;Extraneous - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.&lt;/p&gt;&lt;p&gt;Let's jump straight to the concrete practical examples of extraneous cognitive load.&lt;/p&gt;&lt;p&gt;We will refer to the level cognitive load as follows:&lt;code&gt;🧠&lt;/code&gt;: fresh working memory, zero cognitive load&lt;code&gt;🧠++&lt;/code&gt;: two facts in our working memory, cognitive load increased&lt;code&gt;🤯&lt;/code&gt;: cognitive overload, more than 4 facts&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Our brain is much more complex and unexplored, but we can go with this simplistic model.&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;if val &amp;gt; someConstant // 🧠+
    &amp;amp;&amp;amp; (condition2 || condition3) // 🧠+++, prev cond should be true, one of c2 or c3 has be true
    &amp;amp;&amp;amp; (condition4 &amp;amp;&amp;amp; !condition5) { // 🤯, we are messed up by this point
    ...
}&lt;/code&gt;&lt;p&gt;Introduce intermediate variables with meaningful names:&lt;/p&gt;&lt;code&gt;isValid = val &amp;gt; someConstant
isAllowed = condition2 || condition3
isSecure = condition4 &amp;amp;&amp;amp; !condition5 
// 🧠, we don't need to remember the conditions, there are descriptive variables
if isValid &amp;amp;&amp;amp; isAllowed &amp;amp;&amp;amp; isSecure {
    ...
}&lt;/code&gt;&lt;code&gt;if isValid { // 🧠+, okay nested code applies to valid input only
    if isSecure { // 🧠++, we do stuff for valid and secure input only
        stuff // 🧠+++
    }
} &lt;/code&gt;&lt;p&gt;Compare it with the early returns:&lt;/p&gt;&lt;code&gt;if !isValid
    return
 
if !isSecure
    return

// 🧠, we don't really care about earlier returns, if we are here then all good

stuff // 🧠+&lt;/code&gt;&lt;p&gt;We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.&lt;/p&gt;&lt;p&gt;We are asked to change a few things for our admin users: &lt;code&gt;🧠&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;AdminController extends UserController extends GuestController extends BaseController&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Ohh, part of the functionality is in &lt;code&gt;BaseController&lt;/code&gt;, let's have a look: &lt;code&gt;🧠+&lt;/code&gt;&lt;lb/&gt; Basic role mechanics got introduced in &lt;code&gt;GuestController&lt;/code&gt;: &lt;code&gt;🧠++&lt;/code&gt;&lt;lb/&gt; Things got partially altered in &lt;code&gt;UserController&lt;/code&gt;: &lt;code&gt;🧠+++&lt;/code&gt;&lt;lb/&gt; Finally we are here, &lt;code&gt;AdminController&lt;/code&gt;, let's code stuff! &lt;code&gt;🧠++++&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Oh, wait, there's &lt;code&gt;SuperuserController&lt;/code&gt; which extends &lt;code&gt;AdminController&lt;/code&gt;. By modifying &lt;code&gt;AdminController&lt;/code&gt; we can break things in the inherited class, so let's dive in &lt;code&gt;SuperuserController&lt;/code&gt; first: &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Prefer composition over inheritance. We won't go into detail - there's plenty of material out there.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Method, class and module are interchangeable in this context&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.&lt;/p&gt;&lt;p&gt;Deep module - simple interface, complex functionality&lt;lb/&gt; Shallow module - interface is relatively complex to the small functionality it provides&lt;/p&gt;&lt;p&gt;Having too many shallow modules can make it difficult to understand the project. Not only do we have to keep in mind each module responsibilities, but also all their interactions. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, linear thinking is more natural to us humans.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Information hiding is paramount, and we don't hide as much complexity in shallow modules.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.&lt;/p&gt;&lt;p&gt;Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The best components are those that provide powerful functionality yet have a simple interface.&lt;/p&gt;&lt;lb/&gt;John K. Ousterhout&lt;/quote&gt;&lt;p&gt;The interface of the UNIX I/O is very simple. It has only five basic calls:&lt;/p&gt;&lt;code&gt;open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)&lt;/code&gt;&lt;p&gt;A modern implementation of this interface has hundreds of thousands of lines of code. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;This deep module example is taken from the book A Philosophy of Software Design by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper On the Criteria To Be Used in Decomposing Systems into Modules. Both are essential reads. Other related readings: A Philosophy of Software Design vs Clean Code, It's probably time to stop recommending Clean Code, Small Functions considered Harmful.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.&lt;/p&gt;&lt;p&gt;All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So MetricsProviderFactoryFactory seems to be just fine. The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that? Something went wrong.&lt;/p&gt;&lt;p&gt;We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A module should be responsible to one, and only one, user or stakeholder.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.&lt;/p&gt;&lt;p&gt;But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.&lt;/p&gt;&lt;p&gt;This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.&lt;/p&gt;&lt;p&gt;I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. Diagnostic difficulty in integration space skyrocketed. Both time to market and cognitive load were unacceptably high. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". Hello, you got to stop dreaming big.&lt;/p&gt;&lt;p&gt;The Tanenbaum-Torvalds debate argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.&lt;/p&gt;&lt;p&gt;A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.&lt;/p&gt;&lt;p&gt;We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.&lt;/p&gt;&lt;p&gt;If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, when you come back later, you would have to recreate that thought process!&lt;/p&gt;&lt;p&gt;You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;These statements are made by none other than Rob Pike.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Reduce cognitive load by limiting the number of choices.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Language features are OK, as long as they are orthogonal to each other.&lt;/p&gt;&lt;head&gt;Thoughts from an engineer with 20 years of C++ experience ⭐️&lt;/head&gt;&lt;p&gt;I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!&lt;/p&gt;&lt;p&gt;I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.&lt;/p&gt;&lt;p&gt;Like, can you imagine, the token&lt;/p&gt;&lt;code&gt;||&lt;/code&gt; has a different meaning in &lt;code&gt;requires ((!P&amp;lt;T&amp;gt; || !Q&amp;lt;T&amp;gt;))&lt;/code&gt; and in &lt;code&gt;requires (!(P&amp;lt;T&amp;gt; || Q&amp;lt;T&amp;gt;))&lt;/code&gt;. The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.&lt;p&gt;You can't allocate space for a trivial type and just&lt;/p&gt;&lt;code&gt;memcpy&lt;/code&gt; a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.&lt;p&gt;Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you will face that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03.&lt;/p&gt;&lt;code&gt;🤯&lt;/code&gt;&lt;p&gt;There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, but if the value is known statically, then...&lt;/p&gt;&lt;code&gt;🤯&lt;/code&gt;&lt;p&gt;This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons (extraneous cognitive load).&lt;/p&gt;&lt;p&gt;I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.&lt;/p&gt;&lt;p&gt;By no means I am trying to blame C++. I love the language. It's just that I am tired now.&lt;/p&gt;&lt;p&gt;Thanks to 0xd34df00d for writing.&lt;/p&gt;&lt;p&gt;On the backend we return:&lt;code&gt;401&lt;/code&gt; for expired jwt token&lt;code&gt;403&lt;/code&gt; for not enough access&lt;code&gt;418&lt;/code&gt; for banned users&lt;/p&gt;&lt;p&gt;The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:&lt;code&gt;401&lt;/code&gt; is for expired jwt token // &lt;code&gt;🧠+&lt;/code&gt;, ok just temporary remember it&lt;code&gt;403&lt;/code&gt; is for not enough access // &lt;code&gt;🧠++&lt;/code&gt;&lt;code&gt;418&lt;/code&gt; is for banned users // &lt;code&gt;🧠+++&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Frontend developers would (hopefully) introduce some kind &lt;code&gt;numeric status -&amp;gt; meaning&lt;/code&gt; dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.&lt;/p&gt;&lt;p&gt;Then QA engineers come into play: "Hey, I got &lt;code&gt;403&lt;/code&gt; status, is that expired token or not enough access?"
QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.&lt;/p&gt;&lt;p&gt;Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:&lt;/p&gt;&lt;code&gt;{
    "code": "jwt_has_expired"
}&lt;/code&gt;&lt;p&gt;Cognitive load on the frontend side: &lt;code&gt;🧠&lt;/code&gt; (fresh, no facts are held in mind)&lt;lb/&gt; Cognitive load on the QA side: &lt;code&gt;🧠&lt;/code&gt;&lt;/p&gt;&lt;p&gt;The same rule applies to all sorts of numeric statuses (in the database or wherever) - prefer self-describing strings. We are not in the era of 640K computers to optimise for memory.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;People spend time arguing between&lt;/p&gt;&lt;code&gt;401&lt;/code&gt;and&lt;code&gt;403&lt;/code&gt;, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.&lt;/quote&gt;&lt;p&gt;P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like "login" and "permissions" to reduce the cognitive load.&lt;/p&gt;&lt;p&gt;Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.&lt;/p&gt;&lt;p&gt;Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.&lt;/p&gt;&lt;p&gt;Rob Pike once said:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A little copying is better than a little dependency.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.&lt;/p&gt;&lt;p&gt;All your dependencies are your code. Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (because things go wrong) is painful.&lt;/p&gt;&lt;p&gt;There's a lot of "magic" in frameworks. By relying too heavily on a framework, we force all upcoming developers to learn that "magic" first. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.&lt;/p&gt;&lt;p&gt;Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;By no means do we advocate to invent everything from scratch!&lt;/p&gt;&lt;p&gt;We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.&lt;/p&gt;&lt;p&gt;There is a certain engineering excitement about all this stuff.&lt;/p&gt;&lt;p&gt;I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Abstraction is supposed to hide complexity, here it just adds indirection. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture’s layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.&lt;/p&gt;&lt;p&gt;If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and implicit interfaces.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;With a sufficient number of users of an API,&lt;/p&gt;&lt;lb/&gt;it does not matter what you promise in the contract:&lt;lb/&gt;all observable behaviors of your system&lt;lb/&gt;will be depended on by somebody.&lt;/quote&gt;&lt;p&gt;We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. We spent the next 10 months on dealing with out-of-order events and other challenges. It's now funny to say that abstractions helps us replace components quickly.&lt;/p&gt;&lt;p&gt;So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future? Plus, in most cases, that future of replacing some core component never happens.&lt;/p&gt;&lt;p&gt;These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. Discuss.&lt;/p&gt;&lt;p&gt;Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.&lt;/p&gt;&lt;p&gt;Layers of abstraction aren't free of charge, they are to be held in our limited working memory.&lt;/p&gt;&lt;p&gt;Domain-driven design has some great points, although it is often misinterpreted. People say "We write code in DDD", which is a bit strange, because DDD is about problem space, not about solution space.&lt;/p&gt;&lt;p&gt;Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.&lt;/p&gt;&lt;p&gt;Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The problem is that familiarity is not the same as simplicity. They feel the same — that same ease of moving through a space without much mental effort — but for very different reasons. Every “clever” (read: “self-indulgent”) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get “the new kid” to critique the code before they get too institutionalised!&lt;/p&gt;&lt;p&gt;It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.&lt;/p&gt;&lt;p&gt;In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: “When there are only 2 or 3 conditionals, adding another one doesn’t make any difference. By the time there are 20 or 30 conditionals, adding another one doesn’t make any difference!”&lt;/p&gt;&lt;p&gt;There is no “simplifying force” acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.&lt;/p&gt;&lt;p&gt;Thanks to Dan North for his comment.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.&lt;/p&gt;&lt;p&gt;The more mental models there are to learn, the longer it takes for a new developer to deliver value.&lt;/p&gt;&lt;p&gt;Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.&lt;/p&gt;&lt;p&gt;If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Our architecture is a standard CRUD app architecture, a Python monolith on top of Postgres&lt;/item&gt;&lt;item&gt;How Instagram scaled to 14 million users with only 3 engineers&lt;/item&gt;&lt;item&gt;The companies where we were like ”woah, these folks are smart as hell” for the most part failed&lt;/item&gt;&lt;item&gt;One function that wires up the entire system. If you want to know how the system works - go read it&lt;/item&gt;&lt;/list&gt;&lt;p&gt;These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.&lt;/p&gt;&lt;p&gt;Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.&lt;/p&gt;&lt;p&gt;Imagine for a moment that what we inferred in the second chapter isn’t actually true. If that’s the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. Do not do this to your colleagues.&lt;/p&gt;&lt;p&gt;We should reduce any cognitive load above and beyond what is intrinsic to the work we do.&lt;/p&gt;&lt;head&gt;Comments&lt;/head&gt;&lt;p&gt;Rob Pike&lt;lb/&gt;Nice article.&lt;/p&gt;&lt;p&gt;Andrej Karpathy (ChatGPT, Tesla)&lt;lb/&gt;Nice post on software engineering. Probably the most true, least practiced viewpoint.&lt;/p&gt;&lt;p&gt;Elon Musk&lt;lb/&gt;True.&lt;/p&gt;&lt;p&gt;Addy Osmani (Chrome, the most complex software system in the world)&lt;lb/&gt;I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.&lt;/p&gt;&lt;p&gt;The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."&lt;/p&gt;&lt;p&gt;What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.&lt;/p&gt;&lt;p&gt;And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.&lt;/p&gt;&lt;p&gt;Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.&lt;/p&gt;&lt;p&gt;One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.&lt;/p&gt;&lt;p&gt;Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility.&lt;/p&gt;&lt;p&gt;Sometimes the simplest solution is the best one, even in a complex system.&lt;/p&gt;&lt;p&gt;antirez (Redis)&lt;lb/&gt;Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.&lt;/p&gt;&lt;p&gt;A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.&lt;/p&gt;&lt;p&gt;Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).&lt;/p&gt;&lt;p&gt;These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D&lt;/p&gt;&lt;p&gt;A developer from the internet&lt;lb/&gt;You would not hire me... I sell myself on my track record of released enterprise projects.&lt;/p&gt;&lt;p&gt;I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.&lt;/p&gt;&lt;p&gt;I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.&lt;/p&gt;&lt;p&gt;I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.&lt;/p&gt;&lt;p&gt;Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/zakirullin/cognitive-load"/></entry><entry><id>https://news.ycombinator.com/item?id=45074312</id><title>Bcachefs Goes to "Externally Maintained"</title><updated>2025-08-30T20:34:36.899306+00:00</updated><content>&lt;doc fingerprint="6cd3180fd8a64cb7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bcachefs goes to "externally maintained"&lt;/head&gt;
    &lt;p&gt; Posted Aug 29, 2025 17:30 UTC (Fri) by intelfx (subscriber, #130118) [Link] (14 responses) &amp;gt; This change also suggests, though, that the immediate removal of bcachefs from the mainline kernel is not in the cards. How would that work? If further changes are not accepted into mainline, but fs/bcachefs/ is not being removed thereof, then what exactly happens to it (the physical code living under that path)? Is it going to slowly bit-rot, with users being instructed to ignore in-tree code and use some other tree instead? Posted Aug 29, 2025 17:42 UTC (Fri) by zdzichu (subscriber, #17118) [Link] (13 responses) Posted Aug 29, 2025 19:21 UTC (Fri) by NYKevin (subscriber, #129325) [Link] (12 responses) But there are a number of unstated assumptions here. The most important problem is how this upstreaming process will work. I can think of a few different alternatives, but the most straightforward option is for Kent to designate somebody. That person would then be responsible for all interaction with the kernel process, including sending emails, responding to code reviews, changing the code as requested (or telling Kent to do so and relaying his responses), etc. This strikes me as a highly difficult and thankless job that I certainly would not want to do. You could easily end up recreating exactly the same argument that Kent was regularly getting into (over release schedules, merge windows, etc.), but by proxy instead of directly. Ideally, Kent stops caring about the kernel's release processes altogether, and takes a mindset of "the kernel is [designee's] problem, and I don't have to deal with it aside from applying a few patches every now and then." Most of the alternatives are worse. If we instead suppose that the kernel takes bcachefs code without Kent's explicit approval or involvement, then the kernel upstream is a de facto fork of bcachefs (or they're just mirroring him, but there are governance problems with that). I'm not convinced that Linus et al. want to maintain a fork in this situation. Posted Aug 29, 2025 22:11 UTC (Fri) by koverstreet (✭ supporter ✭, #4296) [Link] (11 responses) And release process is something I care deeply about, for the simple reason that I support my code. I respond to nearly all of the user bug reports and stare at the test dashboards; I want users to have the most stable and trustworthy code I can provide. Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports. So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says. Posted Aug 30, 2025 4:57 UTC (Sat) by NYKevin (subscriber, #129325) [Link] (1 responses) The distros are downstreams. If they want to package and ship your code, in whatever way they see fit, you've already given them permission to do so. And they are not shy about exercising that permission. I remember several years ago, jwz asked Debian to stop shipping XScreenSaver, because he disagreed with their practice of backporting fixes to old versions. Debian said no, and XScreenSaver is still in the repository today. As you might imagine, some rather harsh words were exchanged, but in the end, both sides went back to their respective corners of the internet and proceeded to mostly ignore each other. Linus, however, is not in the business of playing that game. If there's nobody actively maintaining (his copy of) bcachefs, then I find it hard to believe it's going to be allowed to stick around indefinitely. &amp;gt; So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says. My interpretation of events is that there are only three long-term paths that make sense here: * You accept that you cannot control what appears in Linus's tree, but would prefer that some recent-ish version of bcachefs is there (as opposed to no bcachefs or a very old bcachefs). You designate somebody as I've described upthread, they upstream patches at whatever rate Linus is willing to take them, and everybody is more or less willing to live with the result. I do not see any plausible outcome where you are allowed to control what appears in Linus's tree. He has very explicitly closed the door on that. For expository purposes, and because you are a functioning adult, I have assumed that you will accept this lack of control, but that does not actually matter - one of the above scenarios will inevitably play out, regardless of your opinion of it. The only choice you have at this point is whether it's the first bullet or one of the other two. I do not say this to be cruel. Based on your words in this and other threads, I genuinely believe that this process has been very painful for you, and I doubt you enjoy being reminded that Linus's tree does, in fact, belong to Linus (I'm sure other developers have screamed that at you enough times by now). Unfortunately, this is not a matter of right or wrong. It is a matter of power. You are aggrieved about something that neither Linus, nor anybody else on LKML, is prepared to recognize as an injury to you. Regardless of whether that is the right way or the wrong way of looking at it, Linus is going to conduct the kernel's release cycle as he sees fit. The *healthy* way of looking at it is to accept that that is not within your power to change, and redirect your attention to the things you can change. Posted Aug 30, 2025 16:30 UTC (Sat) by ttuttle (subscriber, #51118) [Link] Posted Aug 30, 2025 7:57 UTC (Sat) by paravoid (subscriber, #32869) [Link] (6 responses) Debian was not even close to the topic at hand, and yet you felt the need to bring it up, just to attack someone, and with information that is misrepresenting the truth. This is something you've done before, and you were very recently called out in lkml for it. Stop. To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another. It was ultimately removed from unstable because noone was able to get through. Source: I am one of them. Posted Aug 30, 2025 11:44 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (2 responses) He did so anyways, and then swapped out bindgen for an old version that was explicitly unsupported according to the Cargo.toml, which broke the build, and he sat on it and Debian users stopped getting updates (I didn't even see a report until months later). This resulted in users being unable to access their filesystems. There was briefly a buggy version of bcachefs-tools that couldn't pass mount options correctly; users in every other distro got a fix quickly, but Debian users did not - and we found out about this when a lot of users weren't able to mount in degraded mode after having a drive die. What you're doing is conflating technical criticism with personal, and then using that as an excuse to ramp up the drama. Technical criticism, including pointing out failures of processes, has to be ok for engineering to function, otherwise we don't learn from our mistakes. That can make for a harsh learning environment, but when you're shipping critical system components that have to work, that's what you signed up for; we have responsibilities. The person in question was warned explicitly that what he was doing was a bad idea; he could have at any point said "this is too complicated an issue for me to handle; I'll let someone else take this one" (and there are mechanisms in Debian process for obtaining exceptions to process rules that could have avoided this, by simply skipping the Rust dependency unbundling with a clear explanation of why); he ignored advice and plowed ahead, and a lot of people were affected by those actions. When we work on this kind of code, we have to be responsible for the work we do, including our mistakes. Posted Aug 30, 2025 14:25 UTC (Sat) by ma4ris8 (subscriber, #170509) [Link] (1 responses) Listen part: I'm trying to repeat roughly the same as you wrote above, to show that I listened you: First you state that maintainer switched Rust dependencies for the packaged versions from Debian. He changed Rust dependencies anyways, and then swapped out bindgen into older version, Important end question: Did I repeat (re-phrase in text) precisely what you wrote? Answer part: For me it sounds like there were some mistakes done by both you and others. How to communicate (listen) effectively, to heal relationships? This way of listening is mentioned in What I showed, is one way to restore human relationships, with Linus and others: If you get a backslash, you was just given an opportunity to listen the complaint. By doing this just very slightly to not burden others, I've seen that sometimes this listening technique helps on-line, in addition of meeting face to face. Posted Aug 30, 2025 18:21 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] Why are you trying to bothsides this? You seem to have the facts straight, but I'm not at all clear on what you think I did wrong. All this was explained clearly, calmly and patiently to the Debian package maintainer when he started; he decided to do it his way, and when the breakage became apparent I asked if he was going to fix it and he just said "nope, too complicated" and walked off. So I got stuck with warning bcachefs users away from Debian, and he wrote a screed of a blog post about how impossible I am to work with. Sorry, but from where I sit that just looks crazy. I'm all about focusing on the human aspect, sitting down with people and having open and honest conversations. I do that regularly, and believe me I and others have tried ratcheting down the tensions, bringing the focus back to the technical and looking for ways to make this easier and take things in little steps. The whole rest of the 6.16 merge cycle after the journal_rewind fiasco was just that, from myself and others; we've tried to bridge the gap, bring the focus back to the technical, look for ways to make things work - it doesn't seem to be getting us anywhere. Posted Aug 30, 2025 12:24 UTC (Sat) by muase (subscriber, #178466) [Link] (2 responses) I know it's not the distros' fault; it's simply how LTS has to work in practice – however I can understand the frustration that arises if there seems to be an opportunity to finally update a package(set)... and then that opportunity is missed, and now the dev knows that they have to endure those obsolete bug reports for another n-year release cycle. It definitely didn't read as "just to attack someone". &amp;gt; To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another. Tbh, the only personal attack I see here is from you; and as an outsider, this is not very informative – your frustration may be absolutely legit, but this reply doesn't suit your case. If the communication is public, do you have a link or something? :) Posted Aug 30, 2025 18:27 UTC (Sat) by paravoid (subscriber, #32869) [Link] (1 responses) Kent in https://lore.kernel.org/linux-bcachefs/wona7sjqodu7jgchtx... called part of a maintainer's job as "bullshit, make-work job", told Debian to "develop a better and more practical minded attitude" and to "stop wasting my time with this stupid bullshit and I can get back to real work". The issues we had spent a lot of our volunteer time to fix were very real issues, many of them upstream, and one in the Rust ecosystem. At the time this was sent, all issues were fixed, or were on the way to be fixed, and a recent bcachefs-tools package with all of the appropriate dependencies was a few weeks away from getting to Debian testing. bcachefs-tools was orphaned by its maintainer a few weeks later; myself and another contributor (the two of us had done all recent advancements), stopped investing our time as well. The package has remained orphaned since, for about a year. Anyone can pick it up, but noone has, and that's not because of technical difficulties (as far as packages go, it's pretty trivial). As an aside, the very existence of this thread was as a "PSA" to his users to avoid Debian and Fedora, telling them that "you'll want to be on a more modern distro". *Two weeks later*, he responded in https://lore.kernel.org/lkml/nxyp62x2ruommzyebdwincu26kmi... to Linus that he expects the "major distros" to pick up bcachefs soon. Whether he was dishonest or just naive, I'll leave that to your judgement. The above was just a small sample. There were literally dozens of responses of this style at time, random offensive comments etc., across multiple mediums (mailing lists, IRC, Reddit, etc.). I am not keeping a file though, as I don't feel the need to convince anyone with hard evidence. You don't know me, and I understand that my opinion may not be of much value to you. I hope, though, that you and others may see this as one tiny part of a broader pattern of countless long-time contributors across multiple projects expressing that they have been alienated and driven away by Kent's conduct and sense of entitlement, and that they have good reasons for it. Posted Aug 30, 2025 19:20 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] But please do try to put yourself in my shoes; that was after getting a bunch of bug reports from Debian users, and there had been a _lot_ of fail at that point in how the Debian packaging was handled. I do have to reiterate: the unbundling of Rust dependencies should not have happened for bcachefs-tools, there was no technical reason for that, all my explanations were met with "but that's our policy", and no amount of reasoning was getting anywhere; and the Debian packager breaking the build and sitting on it just should not have happened. I do sincerely hope you can analyze how things went from the other end and ask yourself what could have been done better to avoid this, because from my end, this was an intensely frustrating issue, and it wasn't being taken seriously and it had very real effects. Before you start focusing on language and diplomacy, you really need ask yourself if the technical decisionmaking leading up to that point was sound. When we get breakage as bad as what happened with the Debian package, you can expect the kind of frustration I was voicing there, and "bullshit, make-work projects" still seems to accurately describe what Debain's been doing with Rust dependency unbundling. When we're dealing with critical system components, you cannot focus just on language and diplomacy and ignore the decisionmaking; that's ignoring our most basic responsibilities. Posted Aug 30, 2025 17:12 UTC (Sat) by DemiMarie (subscriber, #164188) [Link] For anything that has to happen before the filesystem can be accessed at all, it might make sense to have an option for the userspace mount helper to do the work. In this case, the userspace helper has far fewer disadvantages I know of. My dream would be for bcachefs to have SQLite’s level of testing and input validation, or (even better) formal verification. Either would massively reduce the rate of bugs making it into a release, but neither is reasonable to ask for outside of a suitably-priced commercial engagement. Posted Aug 30, 2025 18:53 UTC (Sat) by ATLief (subscriber, #166135) [Link] That’s entirely expected given the circumstances; if longstanding Linux developers collectively don’t want to work with you, then they wouldn’t want to individually work with you either. Posted Aug 29, 2025 17:38 UTC (Fri) by ahippo (subscriber, #154692) [Link] (5 responses) This one works better for me somehow: Posted Aug 29, 2025 17:50 UTC (Fri) by Poliorcetics (subscriber, #165001) [Link] (3 responses) Posted Aug 29, 2025 18:39 UTC (Fri) by ewen (subscriber, #4772) [Link] (Looks like a fairly recent fix too — post is 2025-08-28 — so might take a while for the fix to roll out.) Ewen Posted Aug 29, 2025 19:03 UTC (Fri) by ahippo (subscriber, #154692) [Link] (1 responses) Posted Aug 29, 2025 21:03 UTC (Fri) by alspnost (guest, #2763) [Link] Posted Aug 30, 2025 13:17 UTC (Sat) by Baughn (subscriber, #124425) [Link] I can’t access Anubis-protected pages on my iPad. They take several minutes to pass the test, assuming it doesn’t overheat first. Posted Aug 29, 2025 17:39 UTC (Fri) by JMB (guest, #74439) [Link] (3 responses) And I am hoping that quality of code is still key ... And it is unfortunately connected to the 'external maintenance' Posted Aug 29, 2025 17:47 UTC (Fri) by zdzichu (subscriber, #17118) [Link] (2 responses) Posted Aug 30, 2025 0:28 UTC (Sat) by josh (subscriber, #17465) [Link] (1 responses) Posted Aug 30, 2025 5:17 UTC (Sat) by awilfox (guest, #124923) [Link] Posted Aug 29, 2025 18:13 UTC (Fri) by mb (subscriber, #50428) [Link] (2 responses) &amp;gt;the immediate removal of bcachefs from the mainline kernel is not in the cards. What? Please just remove it instead of leaving code in the mainline that doesn't receive fixes for known bugs any longer. Posted Aug 29, 2025 18:49 UTC (Fri) by tux3 (subscriber, #101245) [Link] One could still hope for things to resolve differently. Perhaps some other way or other person is found to keep patches flowing. I'm not holding my breath, but there'll be plenty of time to delete things if and when that's what it comes down to. Posted Aug 29, 2025 22:47 UTC (Fri) by hailfinger (subscriber, #76962) [Link] Posted Aug 29, 2025 21:36 UTC (Fri) by birdie (guest, #114905) [Link] &lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;lb/&gt; &amp;gt;&lt;lb/&gt; &amp;gt; Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.&lt;lb/&gt; * You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They continue to ship an old bcachefs for (at least) the rest of the current release cycle, but eventually it bitrots and they delete it. You might or might not choose to ship it out-of-tree like ZFS, and various distros might or might not package some version of it for you (whether you want them to or not).&lt;lb/&gt; * You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They fork bcachefs or mirror it from your out-of-tree version, and slightly-old or modified versions continue to appear in the kernel indefinitely. As I explained, I think this is less likely, but I don't want to entirely discount it.&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;lb/&gt; I hope that you get my point of listening well in order to carefully heal the relationships.&lt;lb/&gt; You explained that it was a bad idea, for multiple reasons: statically linked dependencies, and&lt;lb/&gt; invalidating all your active testing.&lt;lb/&gt; which broke the build for Debian, and file system users stopped getting updates.&lt;lb/&gt; You wrote many items into one message. I answered only for the first one,&lt;lb/&gt; to keep the answer small enough. Some progress, but further messages&lt;lb/&gt; could increase coverage.&lt;lb/&gt; The unfortunate end result was, that Debian users had problems with the bug.&lt;lb/&gt; I didn't get from your message, the outcome of the relationships between persons:&lt;lb/&gt; whether personal relationships were worsened, stayed the same, or healed in the&lt;lb/&gt; end (each relation individually).&lt;lb/&gt; https://www.verywellmind.com/what-is-active-listening-302...&lt;lb/&gt; "Paraphrasing and reflecting back what has been said"&lt;lb/&gt; ( Those who know psychology, know these things ).&lt;lb/&gt; You could try to restore relationships with just listening others. Choose carefully&lt;lb/&gt; messaging cases, in which you think that you won't cause much backslash,&lt;lb/&gt; but you could have progress with healing the relationship by listening to the other.&lt;lb/&gt; Repeat in nearly the same words the whole complaint, &lt;lb/&gt; so that the other one feels of being heard fully.&lt;lb/&gt; Try to at least have progress, thus please listen carefully the mentioned&lt;lb/&gt; complaint by repeating it. You can have pauses, like answering another day, to reduce the burden.&lt;lb/&gt; Please don't open up any new problems. If you do (I do mistakes sometimes),&lt;lb/&gt; and get a backslash as a heated answer, please listen and repeat it carefully,&lt;lb/&gt; to reduce the impact.&lt;lb/&gt; you could both improve your communication skills,&lt;lb/&gt; and perhaps others could learn from it too,&lt;lb/&gt; and perhaps then relations with other stakeholders, like maintainers,&lt;lb/&gt; and Linus, could be restored into a level that you can co-operate efficiently together again.&lt;lb/&gt; I'm trying to improve my communicating skills in the contexts of&lt;lb/&gt; change management for "OWASP top 10", and AI adoption.&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;A few suggestions (which you don’t have to follow)&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;A broken link?&lt;/head&gt;&lt;lb/&gt; https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/...&lt;head&gt;A broken link?&lt;/head&gt;&lt;head&gt;A broken link?&lt;/head&gt;&lt;head&gt;A broken link?&lt;/head&gt;&lt;lb/&gt; Thank you for pointing me to that blog post!&lt;lb/&gt; My phone indeed has an odd number of cores.&lt;head/&gt; Fascinating - this is a whole new thing to me, but I guess I'm also "vulnerable", since I have a Pixel 8 Pro with 9 cores! &lt;head&gt;A broken link?&lt;/head&gt;&lt;head&gt;A broken link?&lt;/head&gt;&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;lb/&gt; Maybe the E-Mail has a typo ... b -&amp;gt; v ???&lt;lb/&gt; but when 6.15 is EoL and one is recommended to change to 6.16&lt;lb/&gt; just before a version with just ext4 bugs fixed ... and still further&lt;lb/&gt; ext4 bugs fixed after that ... it seem to be a general problem of&lt;lb/&gt; quality concerning Linux FSs / automatic testing not in good shape.&lt;lb/&gt; which is the topic here.&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;head&gt;Why not removed?&lt;/head&gt;&lt;lb/&gt; This is the most harmful (to the users) thing that could have been done now.&lt;head&gt;Why not removed?&lt;/head&gt;&lt;head&gt;Why not removed?&lt;/head&gt;&lt;lb/&gt; 1. Minimal result: Users can continue to use bcachefs with newer kernels without having to patch the kernel, they just won't get bug fixes, but there will be no functional regression&lt;lb/&gt; 2. Improvement with some effort by users: Users willing to patch the kernel can still apply any patches provided by Kent&lt;lb/&gt; 3. Optimal result: A unicorn with the ability to work with Linus and Kent at the same time may appear, resulting in fixes from Kent being merged with the timing and criteria wanted by Linus&lt;head&gt;Not so bad&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/Articles/1035736/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074467</id><title>AI models need a virtual machine</title><updated>2025-08-30T20:34:36.284716+00:00</updated><content>&lt;doc fingerprint="bd5fd5d284b383c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Applications using AI embed the AI model in a framework that interfaces between the model and the rest of the system, providing needed services such as tool calling, context retrieval, etc. Software for early chatbots took user input, called the LLM, and returned the result to the user; essentially just a read-eval-print loop. But, as the capabilities of LLMs have evolved and extension mechanisms, such as MCP were defined, the complexities of the control software that calls the LLM have increased. AI software systems require the same qualities that an operating system provides, including security, isolation, extensibility, and portability. For example, when an AI model needs to be given a file as part of its context, access control must be established that determines if the model should be allowed to view that file. We believe it is time to consider standardizing the ways in which the AI models are embedded into software and think of that control software layer as a virtual machine, where one of the machine instructions, albeit a super-powerful one, is to call the LLM.&lt;/p&gt;
    &lt;p&gt;Our approach decouples model development from integration logic, allowing any model to “plug in” to a rich software ecosystem that includes tools, security controls, memory abstractions, etc. Similar to the impact that the Java Virtual Machine had, creating a specification of a VM for the AI orchestrator could enable a “write once, run anywhere” execution environment for AI models while at the same time providing familiar constraints and governance to maintain security and privacy in existing software systems. Below we outline related work in this direction, the motivation behind it, and the key benefits of an AI Model VM.&lt;/p&gt;
    &lt;p&gt;Introduction&lt;/p&gt;
    &lt;p&gt;AI models are being leveraged in existing software as application copilots, embedded in IDEs, and with the rise of the MCP protocol, are increasingly able to use tools, implement agents, etc. This rapid evolution of valuable use cases brings with it a greater need to ensure that the AI-powered applications maintain privacy, are secure, and operate correctly. Guarantees of security and privacy are best provided if the underlying system is secure by design and not added on to systems as an afterthought. We take the Java Virtual Machine (JVM) as our inspiration in making the case for the importance of a standard AI Virtual Machine. The Java Virtual Machine guarantees memory safety by design, defines access control policies, and prevents code injection with bytecode verification. These properties allow Java programs running on the JVM to be executed with trust despite being shipped remotely, enabling “write once, run anywhere” software distribution.&lt;/p&gt;
    &lt;p&gt;How does the JVM relate to applications that use AI models? We used the following example to explain:&lt;/p&gt;
    &lt;p&gt;The diagram illustrates the role of the software layer that interacts with an AI model, which we call the Model Virtual Machine (MVM). That layer intermediates between the model and the rest of the world. For example, a chatbot user might type a prompt (1) that the MVM then sends unmodified to the AI model (2). In practice, the MVM will add additional context, including the system prompt, chat history, to the AI model input as well. The AI model generates a response, which in the example requires a specific tool to be called (3). This response has a specific format that is mutually agreed upon between the model and the MVM, such as MCP. In our example, because it is important to restrict the model from making undesired tool calls, the MVM first consults the list of allowed tools (4) before deciding to call the tool the model requested (5). This check (4) guarantees that the model doesn’t make unauthorized tool calls. Every commercial system using AI models requires some version of this control software.&lt;/p&gt;
    &lt;p&gt;We make the analogy that the interface with the LLM should be a virtual machine. If that is the case, what are the instructions that the machine can execute? Here are examples of operations that existing AI model interfaces have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Certifying, loading, initializing, and unloading a given AI model&lt;/item&gt;
      &lt;item&gt;Calling a model with context&lt;/item&gt;
      &lt;item&gt;Parsing the output from the model&lt;/item&gt;
      &lt;item&gt;Certifying, loading, initializing, and unloading tools&lt;/item&gt;
      &lt;item&gt;Calling a tool&lt;/item&gt;
      &lt;item&gt;Parsing the results from a tool call&lt;/item&gt;
      &lt;item&gt;Storing the results from a tool call into memory&lt;/item&gt;
      &lt;item&gt;Asking the user for input&lt;/item&gt;
      &lt;item&gt;Adding content to a history memory&lt;/item&gt;
      &lt;item&gt;Standard control constructs such as conditionals, sequencing, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A VM would support all of these operations in a well-typed context where constraints are placed on the calls made, the arguments passed, etc.&lt;/p&gt;
    &lt;p&gt;Existing Work Informs What is Needed&lt;/p&gt;
    &lt;p&gt;Some of the required elements of a well-specified interface are emerging in AI systems explored in academic work and in applications that are widely deployed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI’s Structured Tool Calling Protocols: OpenAI introduced a JSON-based function calling API that lets models invoke code-defined functions in a structured way. This approach, along with OpenAI’s plugin system (which uses OpenAPI specifications for tools), showed how structured tool-calling protocols can reduce ambiguity and simplify integration.&lt;/item&gt;
      &lt;item&gt;Anthropic’s Model Context Protocol (MCP, 2024): MCP is an open protocol for connecting AI assistants to external data and tools, explicitly aiming to be a universal interface. “Think of MCP like a USB-C port for AI applications,” Anthropic explains. Instead of every service having a custom AI integration, MCP provides a common schema and client-server approach. Despite being relatively new, MCP adoption, including in large companies, has been rapid.&lt;/item&gt;
      &lt;item&gt;Secure Orchestrators – FIDES &amp;amp; AC4A (2025): Security remains a weak point in current AI systems. Two recent projects propose runtime-level controls. FIDES (by Microsoft Research) enforces information-flow policies on agents by tracking data confidentiality labels and adding new agent actions like “inspect” to limit what agents can access (where a quarantined LLM can safely summarize restricted data) (paper). AC4A (Access Control for Agents) (manuscript in preparation) takes an OS-style approach: All tools and data are organized into hierarchies (like files and folders), and the agent must request read/write access for each resource. AC4A’s runtime intercepts every agent action and blocks anything not permitted, forcing a least-privilege operation mode. These projects show how a standard AI VM could include built-in security and access control, just as modern operating systems do. Even with strong access controls built into a VM specification, AI models present new security challenges that need to be considered in the design. For example, an AI model, when prevented from accessing a particular item of data, might use its chain-of-thought reasoning to devise ways to gather accessible data that allows it to infer the inaccessible item. As such, security researchers have to devise new mitigations to prevent AI models taking adversarial actions even with the virtual machine constraints.&lt;/item&gt;
      &lt;item&gt;Open-Source Agent Runtimes: Several projects are actively building general-purpose runtimes for AI. For example, langchain and Semantic Kernel provide numerous common runtime services that make writing reliable AI-enabled applications easier. The AI Controller Interface (AICI) (later renamed llguidance), integrates a lightweight VM into the model-serving pipeline, allowing developers to script and constrain model behavior at a low level (e.g., control of generations token-by-token).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Defining a specification for a VM interface for AI systems from these emerging approaches will require more than an agreement on protocols and APIs. Because AI systems derive their behavior from training data, model training data must reflect the specification of the VM interface so that the models and the VM model interface can co-evolve. This will enable otherwise diverse models to exhibit broadly compatible behavior with respect to the VM interface specification.&lt;/p&gt;
    &lt;p&gt;Benefits of a Well-Specified AI Model VM&lt;/p&gt;
    &lt;p&gt;As mentioned, many applications that leverage AI models require reliability, privacy, and security. In addition, new models are developed almost daily and updating the model being used by an application is often necessary. Given this confluence of factors, creating robust AI software presents significant engineering challenges. We believe that a specification of the interface between the AI model and the surrounding software that interfaces to it will address some of these challenges.&lt;/p&gt;
    &lt;p&gt;The need for an AI Model VM specification is driven by several clear motivations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Separation of Concerns: An interface specification enforces a clean separation between model logic and integration logic. This means models become interchangeable components. You could swap in a new model (or move an agent to a different platform) and, as long as both adhere to the standard, everything still works. Likewise, virtual machine implementors can increase the performance, security, and tooling of the virtual machine while maintaining compatibility with the AI model interfaces.&lt;/item&gt;
      &lt;item&gt;Built-in Safety and Governance: A VM specification can enforce safety by design. By routing all tool usage and external access through a well-defined interface, it becomes easier to apply permission checks, audit logs, and fail-safes. As shown by projects like AC4A, the VM can act as a gatekeeper, restricting what models can do unless explicitly authorized. This creates a safer deployment solution for powerful AI systems: even if the model behaves unpredictably, the VM layer can contain its effects. Standards bodies could even define security requirements (e.g., certain calls must always require user confirmation), creating a shared foundation of trust. Similar to the benefits of signed assemblies in the Common Language Runtime, have a certification process around loading and unloading models and tools ensures the end-to-end security of the supply chain.&lt;/item&gt;
      &lt;item&gt;Transparent Performance &amp;amp; Resource Tracking: A VM specification could also give developers visibility to runtime diagnostics. Post-execution manifests could report model performance, resource consumption, and data access level which helps developers evaluate overall efficiency and performance. Benchmarks for accuracy, utility, and responsiveness can be supported directly in the VM interface across models and platforms.&lt;/item&gt;
      &lt;item&gt;Verifiability of Model Output: Leveraging a VM specification, experts can explore integrating formal methods to verify their model behavior. Techniques such as zero-knowledge proofs could confirm the integrity of model outputs without sensitive internal logic. While still emerging, this possibility hints at new levels of trust and accountability in AI systems and should be carefully considered during development.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;We argue that a well-specified AI Model Virtual Machine is needed. Developments occurring in multiple directions, including work from tech companies, startups, and academia, all motivate the need for a VM specification that lets AI models safely and seamlessly interact with the world around them. The motivation is clear – reducing complexity and unlocking interoperability – and the potential benefits range from technical (faster development, modular upgrades) to strategic (cross-platform AI ecosystems, improved safety). From enforcing controls for security and privacy, to potentially formal proof capabilities for trust, the opportunities are wide-ranging. Learning a lesson from older generations of software virtualization, a VM specification can increase AI systems portability, interoperability, security, and reliability. The purpose of this document is to highlight these issues and start engaging with the community on building a consensus that such a specification is needed and what it should include.&lt;/p&gt;
    &lt;p&gt;Biographies:&lt;/p&gt;
    &lt;p&gt;Shraddha Barke is a Senior Researcher at Microsoft Research in Redmond, Washington in the Research in Software Engineering (RiSE) group. Her research interests include AI for proof generation, training AI models for program-reasoning tasks using RL and improving the reliability of AI agents.&lt;/p&gt;
    &lt;p&gt;Betül Durak is a Principal Researcher at Microsoft Research in Redmond, Washington in Security, Privacy, and Cryptography group. Her research interests broadly include security analysis as well as secure and private protocol designs motivated from real world problems.&lt;/p&gt;
    &lt;p&gt;Dan Grossman is a Professor at the University of Washington and the Vice Director of the Paul G. Allen School of Computer Science &amp;amp; Engineering. His research interests are in programming languages, particularly in applying programming languages concepts and analyses to emerging domains.&lt;/p&gt;
    &lt;p&gt;Peli de Halleux is a Principal Research Software Developer Engineer in Redmond, Washington working in the Research in Software Engineering (RiSE) group. His research interests include empowering individuals to build LLM-powered applications more efficiently.&lt;/p&gt;
    &lt;p&gt;Emre Kıcıman is a Senior Principal Research Manager and Head of Research for Copilot Tuning at Microsoft. His research interests include causal methods, the security of AI, and applications of LLM and AI-based systems, together with their implications for people and society.&lt;/p&gt;
    &lt;p&gt;Reshabh K Sharma is a PhD student at the University of Washington. His research lies at the intersection of PL/SE and LLMs, focusing on developing infrastructure and tools to create better LLM-based system that are easier to develop reliably and correctly.&lt;/p&gt;
    &lt;p&gt;Ben Zorn is a Partner Researcher at Microsoft Research in Redmond, Washington working in (and previously having co-managed) the Research in Software Engineering (RiSE) group. His research interests include programming language design and implementation, end-user programing, and AI software including technology for ensuring responsible AI.&lt;/p&gt;
    &lt;p&gt;Disclaimer: These posts are written by individual contributors to share their thoughts on the SIGPLAN blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGPLAN or its parent organization, ACM.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074895</id><title>Condor's Cuzco RISC-V Core at Hot Chips 2025</title><updated>2025-08-30T20:34:36.025197+00:00</updated><content>&lt;doc fingerprint="c2da901fa4165780"&gt;
  &lt;main&gt;
    &lt;p&gt;Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive. Andes formed Condor in 2023, so Condor is a relatively young player on the RISC-V scene. However, Andes does have RISC-V design experience prior to Condor’s formation with a few RISC-V cores under their belt from years past.&lt;/p&gt;
    &lt;p&gt;Condor is presenting their Cuzco core at Hot Chips 2025. This core is a heavyweight within the RISC-V scene, with wide out-of-order execution and a modern branch predictor and some new time based tricks. It’s in the same segment as high performance RISC-V designs like SiFive’s P870 and Veyron’s V1. Like those cores, Cuzco should stand head and shoulders above currently in-silicon RISC-V cores like Alibaba T-HEAD’s C910 and SiFive’s P550.&lt;/p&gt;
    &lt;p&gt;Besides being a wide out-of-order design, Cuzco uses mostly static scheduling in the backend to save power and reduce complexity. Condor calls this a “time-based” scheduling scheme. I’ll cover more on this later, but it’s important to note that this is purely an implementation detail. It doesn’t require ISA modifications or special treatment from the compiler for optimal performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Core Overview&lt;/head&gt;
    &lt;p&gt;Cuzco is a 8-wide out-of-order core with a 256 entry ROB and clock speed targets around 2 GHz SS (Slow-Slow) to 2.5 GHz (Typical-Typical) on TSMC’s 5nm process. The pipeline has 12 stages counting from instruction fetch to data cache access completion. However, a 10 cycle mispredict penalty probably more accurately describes the core’s pipeline length relative to its competitors.&lt;/p&gt;
    &lt;p&gt;As a licensed core, Cuzco is meant to be highly configurable to widen its target market. The core is built from a variable number of execution slices. Customization options also include L2 TLB size, off-cluster bus widths, and L2/L3 capacity. Condor can also adjust the size of various internal core structures to meet customer performance requirements. Cuzco cores are arranged into clusters with up to eight cores. Clusters interface with the system via a CHI bus, so customers can bring their own network-on-chip (NoC) to hit higher core counts via multi-cluster setups.&lt;/p&gt;
    &lt;head rend="h1"&gt;Frontend&lt;/head&gt;
    &lt;p&gt;Cuzco’s frontend starts with a sophisticated branch predictor, as is typical for modern cores targeting any reasonable performance level. Conditional branches are handled via a TAGE-SC-L predictor. TAGE stands for Tagged Geometric, a technique that uses multiple tables each handling a different history length. It seeks to efficiently use branch predictor storage by selecting the most appropriate history length for each branch, as opposed to older techniques that use a fixed history length. The SC (Statistical Corrector) part handles the small subset of branches where TAGE doesn’t work well, and can invert the prediction if it sees TAGE often getting things wrong under certain circumstances. Finally, L indicates a loop predictor. A loop predictor is simply a set of counters that come into play for branches that are taken a certain number of times, then not taken once. If the branch predictor detects such loop behavior, the loop predictor can let it avoid mispredicting on the last iteration of the loop. Basically, TAGE-SC-L is an augmented version of the basic TAGE predictor.&lt;/p&gt;
    &lt;p&gt;AMD’s Zen 2, Ampere’s AmpereOne, and Qualcomm’s Oryon also use TAGE predictors of some sort, and achieve excellent branch prediction accuracy. AMD, Ampere, and Qualcomm also likely augment the basic TAGE prediction strategy in some way. How Cuzco’s TAGE predictor performs will depend on how large its history tables are, as well as how well the predictor is tuned (selection of index vs tag bits, history lengths, distribution of storage budget across TAGE tables, etc). For Cuzco’s part, they’ve disclosed that the TAGE predictor’s base component uses a 16K entry table of bimodal counters.&lt;/p&gt;
    &lt;p&gt;Branch target caching on Cuzco is provided by a 8K entry branch target buffer (BTB) split into two levels. Condor’s slides show the BTB hit/miss occurring on the cycle after instruction cache access starts, so a taken branch likely creates a single pipeline bubble. Returns are predicted using a 32 entry return stack. Cuzco also has an indirect branch predictor, which is typical on modern CPUs.&lt;/p&gt;
    &lt;p&gt;Cuzco’s instruction fetch logic feeds from a 64 KB 8-way set associative instruction cache, and speeds up address translations with a 64 entry fully associative TLB. The instruction fetch stages pull an entire 64B cacheline into the ICQ (instruction cache queue), and then pull instructions from that into an instruction queue (XIQ). The decoders feed from the XIQ, and can handle up to eight instructions per cycle.&lt;/p&gt;
    &lt;head rend="h1"&gt;Rename and Allocate&lt;/head&gt;
    &lt;p&gt;Much of the action in Condor’s presentation relates to the rename and allocate stage, which acts as a bridge between the frontend and out-of-order backend. In most out-of-order cores, the renamer carries out register renaming and allocates resources in the backend. Then, the backend dynamically schedules instructions as their dependencies become available. Cuzco’s renamer goes a step further and predicts instruction schedules as well.&lt;/p&gt;
    &lt;p&gt;One parallel to this is Nvidia’s static scheduling in Kepler and subsequent GPU architectures. Both simplify scheduling by telling an instruction to execute a certain number of cycles in the future, rather than having hardware dynamically check for dependencies. But Nvidia does this in their compiler because GPU ISAs aren’t standardized. Cuzco still uses hardware to create dynamic schedules, but moves that job into the rename/allocate stage rather than the schedulers in the backend. Schedulers can be expensive structures in conventional out-of-order CPUs, because they have to check whether instructions are ready to execute every cycle. On Cuzco, the backend schedulers can simply wait a specified number of cycles, and then issue an instruction knowing the dependencies will be ready by then.&lt;/p&gt;
    &lt;p&gt;To carry out time-based scheduling, Cuzco maintains a Time Resource Matrix (TRM), which tracks utilization of various resources like execution ports, functional units, and data buses for a certain number of cycles in the future. The TRM can look 256 cycles into the future, which keeps storage requirements under control. Because searching a 256 row matrix in hardware would be extremely expensive, Cuzco only looks for available resources in a small window after an instruction’s dependencies are predicted to be ready. Condor found searching a window of eight cycles provided a good tradeoff. Because the renamer can handle up to eight instructions, it at most has to access 64 rows in the TRM per cycle. If the renamer can’t find free resources in the search window, the instruction will be stalled at the ID2 stage.&lt;/p&gt;
    &lt;p&gt;Another potential limitation is the TRM size, which could be a limitation for long latency instructions. However, the longest latency instructions tend to be loads that miss cache. Cuzco always assumes a L1D hit for TRM scheduling, and uses replay to handle L1D misses. That means stalls at ID2 from TRM size limitations should also be rare.&lt;/p&gt;
    &lt;p&gt;Compared to a hypothetical “greedy” setup, where the core is able to create a perfect schedule with execution resource limitations in mind, limiting the TRM search window decreases performance by a few percent. Condor notes that creating a core to match the “greedy” figure may not even be possible. A conventional out-of-order core wouldn’t have TRM-related restrictions, but may face difficulties creating an optimal schedule for other reasons. For example, a distributed scheduler may have several micro-ops become ready in one scheduling queue, and face “false” delays even though free execution units may be available on other scheduling queues.&lt;/p&gt;
    &lt;p&gt;Static scheduling only works when instruction latencies are known ahead of time. Some instructions have variable latency, like loads that can miss caches or TLBs, encounter bank conflicts, or require store forwarding. As mentioned before, Cuzco uses instruction replay to handle variable latency instructions and the associated dynamic behavior. The renamer does take some measures to reduce replays, like checking to see if a load gets its address from the same register as a prior store. However, it doesn’t attempt to predict memory dependencies like Intel’s Core 2, and also doesn’t try to predict whether a load will miss cache.&lt;/p&gt;
    &lt;head rend="h1"&gt;Out-of-Order Backend&lt;/head&gt;
    &lt;p&gt;Out of order execution in Cuzco is relatively simple, because the rename/allocate stage takes care of figuring out when instructions will execute. Each instruction is simply held within the schedulers until a specified number of cycles pass, after which it’s sent for execution. If the rename/allocate stage guesses wrong, replay gets handled via “poison” bits. The erroneously executed instruction’s result data is effectively marked as poisoned, and any instructions consuming that data will get re-executed. Replaying instructions costs power and wastes execution throughput, so replays should ideally be a rare event. 70.07 replays per 1000 instructions feels like a bit of a high figure, but likely isn’t a major problem because execution resources are rarely a limitation in an out-of-order core. Taking about 7% more execution resources may be an acceptable tradeoff, considering most modern chips rarely use their core width in a sustained fashion.&lt;/p&gt;
    &lt;p&gt;Execution resources are grouped into slices, each of which have a pair of pipelines. A slice can execute all of the core’s supported RISC-V instructions, making it easy to scale execution resources by changing slice count. Each slice consists of a set of execution queues (XEQs), which hold micro-ops waiting for a functional unit. Cuzco has XEQs per functional unit, unlike conventional designs that tend to have a scheduling queue that feeds all functional units attached to an execution port. Four register read ports supply operands to the slice, and two write ports handle result writeback. Bus conflicts are handled by the TRM as well. A slice cannot execute more than two micro-ops per cycle, even doing so would not oversubscribe the register read ports. For example, a slice can’t issue an integer add, a branch, and a load in the same cycle even though that would only require four register inputs.&lt;/p&gt;
    &lt;p&gt;XEQs are sized to match workload characteristics, much like tuning a distributed scheduler. While XEQ sizes can be set to match customer requirements, Condor was able to give some figures for a baseline configuration. ALUs get 16 entry queues, while branches and address generation units (LS) get 8 entry queues. XEQ sizes are adjustable in powers of two, from 2 to 32 entries. There’s generally a single cycle of latency for forwarding between slices. The core can be configured to do zero cycle cross-slice forwarding, but that would be quite difficult to pull off.&lt;/p&gt;
    &lt;p&gt;On the vector side, Cuzco supports 256/512-bit VLENs via multiple micro-ops, which are distributed across the execution slices. Execution units are natively 64 bits wide. There’s one FMA unit per slice, so peak FP32 throughput is eight FMA operations per cycle, or 16 FLOPS when counting the add and multiply as separate operations. FP adds execute with 2 cycle latency, while FP multiplies and multiply-adds have four cycle latency. The two cycle FP add latency is nice to see, and matches recent cores like Neoverse N1 and Intel’s Golden Cove, albeit at much lower clocks.&lt;/p&gt;
    &lt;head rend="h1"&gt;Load/Store&lt;/head&gt;
    &lt;p&gt;Cuzco’s load/store unit has a 64 entry load queue, a 64 entry store queue, and a 64 entry queue for data cache misses. Loads can leave the load queue after accessing the data cache, likely creating behavior similar to AMD’s Zen series where the out-of-order backend can have far more loads pending retirement than the documented load queue capacity would suggest. The core has four load/store pipelines in a four slice configuration, or one pipeline per slice. Maximum load bandwidth is 64B/cycle, achievable with vector loads.&lt;/p&gt;
    &lt;p&gt;The L1D is physically indexed and physically addressed (PIPT), so address translation has to complete before L1D access.To speed up address translation, Cuzco has a 64 entry fully associative data TLB. The L2 TLB is 4-way set associative, and can have 1K, 2K, or 4K entries. Cuzco’s core private, unified L2 cache has configurable capacity as well. An example 2 MB L2 occupies 1.04 mm2 on TSMC 5nm.&lt;/p&gt;
    &lt;p&gt;Eight cores per cluster share a L3 cache, which is split into slices to handle bandwidth demands from multiple cores. Each slice can deliver 64B/cycle, and slice count matches core count. Thus Cuzco enjoys 64B/cycle of load bandwidth throughout the cache hierarchy, of course with the caveat that L3 bandwidth may be lower if accesses from different cores clash into the same slice. Cores and L3 slices within a cluster are linked by a crossbar. The L3 cache can run at up to core clock. Requests to the system head out through a 64B/cycle CHI interface. System topology beyond the cluster is up to the implementer.&lt;/p&gt;
    &lt;p&gt;Replays for cache misses are carried out by rescheduling the data consumer to a later time when data is predicted to be ready. Thus a L3 hit would cause a consuming instruction to be executed three times - once for the predicted L1D hit, once for the predicted L2 hit, and a final time for the L3 hit with the correct data.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;High performance CPU design has settled down over the past couple decades, and converged on an out-of-order execution model. There’s no denying that out-of-order execution is difficult. Numerous alternatives have been tried through the years but didn’t have staying power. Intel’s Itanium sought to use an ISA-based approach, but failed to unseat the company’s own x86 cores that used out-of-order execution. Nvidia’s Denver tried to dynamically compile ARM instructions into microcode bundles, but that approach was not carried forward. All successful high performance designs today generally use the same out-of-order execution strategy, albeit with plenty of variation. That’s driven by the requirements of ISA compatibility, and the need to deliver high single threaded performance across a broad range of applications. Breaking from the mould is obviously fraught with peril.&lt;/p&gt;
    &lt;p&gt;Condor seeks to break from the mould, but does so deep in the core in a way that should be invisible to software a functional perspective, and mostly invisible from a performance perspective. The core runs RISC-V instructions and thus benefits from that software ecosystem, unlike Itanium. It doesn’t rely on a compiled microcode cache like Denver, so it doesn’t end up running in a degraded performance beyond what a typical OoO core would see when dealing with poor code locality. Finally, instruction replay effectively creates dynamic schedules and handles cache misses&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot"/></entry><entry><id>https://news.ycombinator.com/item?id=45075048</id><title>You Have to Feel It</title><updated>2025-08-30T20:34:35.855565+00:00</updated><content>&lt;doc fingerprint="b887f3b997e7a4e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;You Have to Feel It&lt;/head&gt;
    &lt;p&gt;You see a series of checkboxes checked. Schedules met. Requirements satisfied. Demos delivered. It's a good day. Good job, you, good job! A promotion is in sight.&lt;/p&gt;
    &lt;p&gt;But you didn't feel it. You didn't feel it.&lt;/p&gt;
    &lt;p&gt;We, as people, feel something with every interaction. Frustration, joy, relief, confidence. A feeling. A person interacts with our work. Our work evokes a feeling. The feeling matters. The feeling is part of the work. The desired feeling is part of the requirements.&lt;/p&gt;
    &lt;p&gt;When you feel it, you know. The feature makes you smile when you use it. It fits right in, like it was always meant to be there. You want to use it again. You want to tell people about it.&lt;/p&gt;
    &lt;p&gt;This is the difference. This is what metrics, specifications, and demos miss. They don't capture the feeling. For the people who will use and live in the work, the feeling is part of their daily experience. Which means you can't stop at checking the boxes on paper. You have to sit with it, use it, live with it.&lt;/p&gt;
    &lt;p&gt;You have to feel it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/feel-it"/></entry><entry><id>https://news.ycombinator.com/item?id=45075664</id><title>GAO warns of privacy risks in using facial recognition in rental housing</title><updated>2025-08-30T20:34:35.485131+00:00</updated><content>&lt;doc fingerprint="af9382da503f2f87"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RENTAL HOUSING&lt;/head&gt;
    &lt;head rend="h1"&gt;Use and Federal Oversight of Property Technology&lt;/head&gt;
    &lt;p&gt;Report to Congressional Requesters&lt;/p&gt;
    &lt;p&gt;United States Government Accountability Office&lt;/p&gt;
    &lt;p&gt;For more information, contact Alicia Puente Cackley at CackleyA@gao.gov.&lt;/p&gt;
    &lt;p&gt;Highlights of GAO-25-107196, a report to congressional requesters&lt;/p&gt;
    &lt;p&gt;Use and Federal Oversight of Property Technology&lt;/p&gt;
    &lt;head rend="h3"&gt;Why GAO Did This Study&lt;/head&gt;
    &lt;p&gt;Some policymakers have raised questions about the use of property technology tools in the rental housing market, including their potential to produce discriminatory or unfair outcomes for renters. GAO was asked to assess various aspects of property technology use in the rental housing market. This report examines (1) the use of four selected property technologyÂ tools, (2) their potential benefits and risks for owners and renters, and (3) federal agenciesâ oversight of these tools.Â&lt;/p&gt;
    &lt;p&gt;GAO focused on four commonly used types of property technology tools (see figure). GAO reviewed studies by federal agencies and advocacy and industry groups; agency guidance and documentation; and rulemakings, legal cases, and enforcement actions issued in 2019â2024. GAO also interviewed officials of four federal agencies responsible for enforcing statutes that address housing discrimination; anticompetitive, unfair, or deceptive acts affecting commerce; and the use of consumer credit reports; and representatives of 12 property technology companies, 10 public housing agencies, and nine advocacy or industry groups (nongeneralizable sample groups, selected for their expertise in or use of these technologies).&lt;/p&gt;
    &lt;head rend="h3"&gt;What GAO Recommends&lt;/head&gt;
    &lt;p&gt;GAO recommends that HUD provide more specific written direction to public housing agencies on the use of facial recognition technology.Â&lt;/p&gt;
    &lt;head rend="h3"&gt;What GAO Found&lt;/head&gt;
    &lt;p&gt;Property technology broadly refers to the use of software, digital platforms, and other digital tools used in the housing market. Property owners and renters use these technologies for functions including advertising, touring, leasing, and financial management of rental housing. These tools may incorporate computer algorithms and artificial intelligence.&lt;/p&gt;
    &lt;p&gt;Property technology tools used for advertising, tenant screening, rent-setting, and facial recognition have both benefits and risks. For example, facial recognition technology can enhance safety, according to three industry associations and all 10 of the public housing agencies in GAOâs review. However, these tools also may pose risks related to transparency, discriminatory outcomes, and privacy. For instance, potential renters may struggle to understand, and owners to explain, the basis for screening decisions made by algorithms. Facial recognition systems also might misidentify individuals from certain demographic groups, and property owners might use surveillance information without renter consent, according to advocacy groups GAO interviewed.Â Â Â&lt;/p&gt;
    &lt;p&gt;The four federal agencies took several actions to address these risks. To combat alleged misleading and discriminatory advertising on rental platforms, agencies pursued legal action and obtained settlements requiring changes to advertising practices and improved compliance with the Fair Housing Act. They also took enforcement actions against tenant screening companies for using inaccurate or outdated data.&lt;/p&gt;
    &lt;p&gt;However, all 10 public housing agencies stated public housing agencies would benefit from additional direction on use of facial recognition technology. The Department of Housing and Urban Developmentâs (HUD) current guidance to these agencies is high-level and does not provide specific direction on key operational issues, such as managing privacy risks or sharing data with law enforcement. More detailed written direction could provide public housing agencies additional clarity on the use of facial recognition technology and better address tenant privacy concerns.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Abbreviations&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;CFPB&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Consumer Financial Protection Bureau&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;DOJ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Department of Justice&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;FCRA&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Fair Credit Reporting Act&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;FTC&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Federal Trade Commission&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;HUD&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Department of Housing and Urban Development&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;PHA&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;public housing agency&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;proptech&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;property technology&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;This is a work of the U.S. government and is not subject to copyright protection in the United States. The published product may be reproduced and distributed in its entirety without further permission from GAO. However, because this work may contain copyrighted images or other material, permission from the copyright holder may be necessary if you wish to reproduce this material separately.&lt;/p&gt;
    &lt;p&gt;July 10, 2025&lt;/p&gt;
    &lt;p&gt;The Honorable Elizabeth Warren&lt;lb/&gt; Ranking Member&lt;lb/&gt; Committee on Banking, Housing, and Urban Affairs&lt;lb/&gt; United States Senate&lt;/p&gt;
    &lt;p&gt;The Honorable Maxine Waters&lt;lb/&gt; Ranking Member&lt;lb/&gt; Committee on Financial Services&lt;lb/&gt; House of Representatives&lt;/p&gt;
    &lt;p&gt;Property owners and renters increasingly rely on property technology (proptech), which in the rental housing context broadly refers to software, digital platforms, and other digital tools used for advertising, leasing, management, and maintenance. These tools may incorporate technologies such as algorithms and artificial intelligence, including machine learning models.[1]&lt;/p&gt;
    &lt;p&gt;Proptech tools can make it easier for renters to search for, view, and lease housing, and for owners to manage their units. But some policymakers have raised questions about the use of these tools and the potential for discriminatory, unfair, or anticompetitive outcomes for renters.&lt;/p&gt;
    &lt;p&gt;You asked us to assess various aspects of proptech use in the rental housing market. This report examines (1) selectedÂ proptechÂ tools available in the rental market, how they are used, and by whom; (2) the benefits and risks that selectedÂ proptechÂ tools may pose for owners and renters; and (3) steps taken by federal agencies to oversee these selectedÂ proptechÂ tools.&lt;/p&gt;
    &lt;p&gt;To identify available proptech tools, we reviewed reports from federal regulators, academics, industry groups, and advocacy organizations and identified thirty-four tools. We then purposively selected four tools that incorporate artificial intelligence and are used by owners and renters in the rental housing process: advertising platforms, tenant screening tools, rent-setting software, and facial recognition technology.[2]&lt;/p&gt;
    &lt;p&gt;For the first two objectives, we interviewed representatives of five advocacy organizations, four industry associations (that represent owners and managers), and two organizations participating in the federal Fair Housing Initiative Program (which seeks to combat housing discrimination), as well as a nongeneralizable sample of 12 companies that provided the selected proptech tools. These consisted of three advertising companies, three tenant screening companies, four facial recognition companies, and two rent-setting software companies. We also conducted semi-structured interviews with representatives of a nongeneralizable sample of 10 public housing agencies (PHA).&lt;/p&gt;
    &lt;p&gt;For the third objective, we reviewed guidance and documentation from the Consumer Financial Protection Bureau (CFPB), the Department of Justice (DOJ) the Federal Trade Commission (FTC), and Department of Housing and Urban Development (HUD), final rulemakings, federal court orders, enforcement actions, and relevant advisory opinions issued by these agencies from 2019 through 2024.[3] We analyzed HUD communications to PHAs regarding use of surveillance technology and assessed them against relevant federal internal control standards.[4]&lt;/p&gt;
    &lt;p&gt;For all three objectives, we reviewed relevant laws, regulations, agency reports and guidance, and interviewed representatives from four federal agencies: the CFPB, DOJ, FTC, and HUD.&lt;/p&gt;
    &lt;p&gt;See appendix I for additional information about our scope and methodology.&lt;/p&gt;
    &lt;p&gt;We conducted this performance audit from November 2023 to July 2025 in accordance with generally accepted government auditing standards. Those standards require that we plan and perform the audit to obtain sufficient, appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives. We believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background&lt;/head&gt;
    &lt;head rend="h4"&gt;Federal Agency Enforcement Roles and Responsibilities&lt;/head&gt;
    &lt;p&gt;Federal agencies generally do not have a direct role in monitoring or overseeing the use of proptech tools in the rental housing market. Instead, several agencies are tasked with enforcing statutes that broadly address housing discrimination; anticompetitive, unfair, or deceptive acts affecting commerce; and the use of consumer credit reports. The federal agencies with oversight and enforcement responsibilities for laws relevant to selected proptech tools include HUD, FTC, CFPB, and DOJ (see table 1).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Law &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Key requirements&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Key federal agencies with statutory oversight and enforcement responsibilities&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Fair Housing Act&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Prohibits discrimination in the sale, rental, or financing of housing, and other housing-related decisions based on race, color, religion, sex, national origin, familial status, or disability.&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;DOJ, HUDa&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Fair Credit Reporting Act&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Requires a permissible purpose to obtain a consumer credit report (including tenant screening reports) and that consumer reporting agencies follow reasonable procedures to assure the maximum possible accuracy of consumer reports. Imposes disclosure requirements on users of consumer reports who take adverse action on credit applications based on information contained in a consumer report. Imposes requirements on furnishers of information to consumer reporting agencies for consumer reports regarding the accuracy and integrity of furnished information.&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;CFPB,b DOJc, FTC&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Section 5 of the Federal Trade Commission Act&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Prohibits unfair methods of competition and unfair or deceptive acts or practices in or affecting commerce.&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FTC&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Sections 1 and 2 of the Sherman Antitrust Act&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Outlaws all contracts, combinations, and conspiracies that unreasonably restrain or monopolize interstate and foreign commerce or trade practices including price fixing and bid rigging.&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;DOJ&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: GAO analysis of relevant laws applicable to the Consumer Financial Protection Bureau (CFPB), Department of Justice (DOJ), Federal Trade Commission (FTC), and Department of Housing and Urban Development (HUD). Â | Â GAOâ25â107196&lt;/p&gt;
    &lt;p&gt;aDOJ may bring Fair Housing Act cases on its own or based on referral from HUD when HUD finds reasonable cause, issues a charge of discrimination, and a party elects to proceed in federal court.&lt;/p&gt;
    &lt;p&gt;bOn February 9, 2025, the National Treasury Employees Union and others filed a lawsuit in the District Court for the District of Columbia alleging that the actions of the Acting Director, including actions regarding staffing and enforcement work, violated the Administrative Procedure Act and the Dodd-Frank Consumer Protection and Wall Street Reform Act and were unconstitutional because they interfered with Congressâ ability to appropriate funds and create statutory functions for agencies. Natâl Treasury Emp. Union, et al. v. Vought, No. 1:25-cv-00381 (D.D.C. filed Feb. 9, 2025). As of May 2025, the litigation is active and continues in both US Circuit and Appeals Court.&lt;/p&gt;
    &lt;p&gt;cFTC and CFPB share enforcement of the Fair Credit Reporting Act as it applies to tenant screening, which is coordinated under a memorandum of understanding. DOJ may assist or supervise related federal court cases.&lt;/p&gt;
    &lt;p&gt;More specifically, HUDâs Office of Fair Housing and Equal Opportunity is responsible for enforcing the Fair Housing Act, which prohibits discrimination in nearly all housing and housing-related transactions based on protected characteristics.[5] This office issues guidance to assist owners and companies in complying with Fair Housing Act requirements. It also processes and investigates complaints alleging civil rights violations and conducts compliance reviews of HUD funding recipients. After a complaint is filed, HUD attempts to conciliate the matter before issuing a charge or dismissal.&lt;/p&gt;
    &lt;p&gt;If conciliation fails, HUD may refer the matter to DOJ. DOJ may bring a case in federal court if HUD investigated the complaint or issued a charge and one of the parties elected to proceed in court. In fair housing cases, DOJ can seek injunctive reliefâsuch as training and policy changesâmonetary damages, and civil penalties in pattern or practice cases.[6]&lt;/p&gt;
    &lt;p&gt;FTC and CFPB enforce the Fair Credit Reporting Act (FCRA), including oversight of consumer reporting agencies.[7] These agencies assemble or evaluate consumer informationâsuch as employment, criminal, rental, eviction, and credit historyâinto consumer reports provided to third parties, including rental property owners who use them to determine eligibility for rental housing. Both FTC and CFBP can investigate and initiate enforcement action against consumer reporting agencies that violate FCRA requirements, such as those for ensuring accuracy.&lt;/p&gt;
    &lt;p&gt;In addition to enforcement, FTC and CFPB seek input from and publish resources for the public.[8] Both CFPB and FTC can bring actions for alleged FCRA violations.Â FTC can seek injunctive relieve on its own but must seek civil penalties through DOJ in federal district court.Â CFPB can bring its own cases alleging FCRA violations for civil penalties and injunctive or other relief.&lt;/p&gt;
    &lt;p&gt;FTC enforces Section 5 of the FTC Act, which prohibits unfair methods of competition or deceptive acts or practices affecting commerce.[9] Under this authority, FTC may investigate and initiate enforcement actions if it has reason to believe a violation occurred or is occurring. The FTC Act allows the agency to seek cease-and-desist orders or injunctive relief, impose civil penalties for violations of its orders or certain rules, and obtain consumer redress in certain circumstances.&lt;/p&gt;
    &lt;head rend="h4"&gt;HUD Administration and Oversight of Subsidized Housing&lt;/head&gt;
    &lt;p&gt;In addition to enforcing the Fair Housing Act, HUD plays a role in administering and overseeing subsidized housing programs.&lt;/p&gt;
    &lt;p&gt;Office of Public and Indian Housing. This office administers the Public Housing and Housing Choice Voucher programs, two of HUDâs largest subsidized housing programs.[10] PHAsâtypically municipal, county, or state agencies created under state lawâoperate both programs. Under the Public Housing program, HUD provides subsidies to PHAs, which own and operate rental housing designated for eligible low-income households. Under the Housing Choice Voucher program, HUD provides rental subsidies, through PHAs, that renters can use to obtain housing in the private market.&lt;/p&gt;
    &lt;p&gt;Office of Policy Development and Research. This office develops fair market rents, which are used to determine the maximum allowable rent for Housing Choice Voucher recipients. Fair market rent generally reflects the cost of renting a moderately priced unit in a local housing market. HUD calculates and publishes these rents annually for thousands of locations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Owners and Renters Use ProptechÂ Tools for Listing, Searching, Screening, and Rent-Setting&lt;/head&gt;
    &lt;p&gt;Owners and renters may use one or more of the four types of proptech tools we reviewedâadvertising platforms, tenant screening tools, rent-setting software, and facial recognition technologyâfor functions including advertising, rent-setting, screening tenants, and controlling access.&lt;/p&gt;
    &lt;p&gt;Advertising platforms. These platforms allow owners to list rental properties on websites for prospective renters to view (see fig. 1). Features may include targeted advertising, virtual touring, and rent estimates. Algorithms and machine learning may be used to personalize search results by tailoring recommendations based on user data. Other platform features may include rental-management tools for landlords, such as application intake, tenant screening, electronic lease signing, and rent collection.&lt;/p&gt;
    &lt;p&gt;Tenant screening tools. Property owners use tenant screening tools to assess the suitability of prospective renters for a rental unit (see fig. 2). The tools assemble and evaluate tenant background information such as credit and criminal history, employment and income verification, and rental payment or eviction history to generate a screening report. Owners may use these reports to attempt to evaluate an applicantâs likelihood of fulfilling lease obligations.[11] Some tools use machine learning to analyze patterns in applicant data to attempt to predict a prospective renterâs reliability based on historical trends. These predictions may be presented as a score or recommendation that owners consider when deciding whether to approve an applicant.&lt;/p&gt;
    &lt;p&gt;Rent-setting software. Also known as revenue management software, these tools help property owners determine rents by generating data-driven pricing recommendations (see fig. 3). They may use proprietary or public data, such as occupancy rates, market trends, and comparable unit prices. This tool may also apply artificial intelligence techniques, including machine learning, to forecast demand and suggest optimal rental levels. The software allows owners to adjust rents in response to current market conditions and their own pricing objectives.&lt;/p&gt;
    &lt;p&gt;Facial recognition technology. Building owners use facial recognition technology for security, including access control (see fig. 4). For example, cameras installed at building entrances use software to verify a renterâs identity by comparing their face to a database of stored images. A successful match allows entry. The software uses computer vision, a type of artificial intelligence, to recognize faces in real time, verify identities from images, and improve accuracy over time through learning.&lt;/p&gt;
    &lt;head rend="h3"&gt;Selected Proptech Tools Can Offer Convenience and Safety but Also Pose Privacy, Bias, and Other Risks&lt;/head&gt;
    &lt;head rend="h4"&gt;Online Listing Platforms Can Benefit Owners and Renters but Carry Risks Such as Misrepresentation&lt;/head&gt;
    &lt;p&gt;Online listing platforms can provide several benefits for owners and renters, according to the industry associations and advertising companies we interviewed, including the following:&lt;/p&gt;
    &lt;p&gt;Wider advertising reach. Owners can use third-party advertising features to extend the visibility of their listings across multiple platforms. These tools use algorithms to target potential renters more efficiently than traditional print advertising. They also can save owners time by creating one listing that can be displayed across various websites and social medial platforms. Broader exposure can help fill vacancies more quickly and maximize rental income.&lt;/p&gt;
    &lt;p&gt;Convenience and cost savings of virtual tours. Listing platforms allow potential renters to take virtual toursâoften in three-dimensional formatsâthat show a unitâs floor layout. Virtual touring is available at any time and may reduce travel and other costs. It also helps individuals with mobility or accessibility limitations assess whether a unit meets their needs without having to physically be present at the unit.&lt;/p&gt;
    &lt;p&gt;Cost savings and efficiencies from universal applications. One company with which we spoke offers âuniversalâ rental applications for potential renters and property owners. The service is free for owners who accept the application. Prospective renters submit a single form and pay one fee to apply to multiple units over a 30-day period.&lt;/p&gt;
    &lt;p&gt;However, representatives of advocacy groups and officials from federal agencies we interviewed noted that advertising tools posed potential risks to renters, including potential misrepresentation and discrimination.&lt;/p&gt;
    &lt;p&gt;Misrepresentation of listings or costs. Owners may post fraudulent or misleading images, including altered or staged photographs. For example, potential renters may rely on pictures and videos provided in a virtual tour, but owners could manipulate these images to make rooms appear larger or smaller than they are. Owners also may make unclear statements about rent costs. For example, rent may be advertised online in a way that obscures actual costs, or fees for amenities like gyms or conference rooms may be hidden until move-in.&lt;/p&gt;
    &lt;p&gt;Discriminatory advertising. Platforms or owners may limit who sees listings or include language that discourages renters on a prohibited basis, such as by protected classes. For example, phrases such as âno childrenâ or âno wheelchairsâ may violate the Fair Housing Act by discriminating on the basis of familial status or disability. Unlike traditional print advertising, where a listing is provided in writing and a landlord subsequently assesses applicants, algorithms may analyze characteristics such as income or location and steer certain users to or from specific listings before they can apply. This practice may reduce housing access for minorities, women, families with children, and individuals with disabilities.&lt;/p&gt;
    &lt;p&gt;Online platforms use several approaches to help mitigate these risks, according to representatives from one of the three advertising companies we interviewed. For example, this company said its platform includes a rental cost and fee calculator that helps potential renters understand the full cost of a unit. The tool lists associated fees and expenses, such as monthly costs for utilities, parking, and pet fees, as well as one-time charges like security deposits and application or administrative fees.Â This company described a set of controls designed to prevent discriminatory advertising. These include keyword detection logic to filter potentially discriminatory languageâfor instance, blocking phrases that discourage families with children. The platform also displays information about relevant federal, state, and local fair housing laws to owners before they upload a listing. In addition, the website allows users to reportÂ potential discriminatory content for manual review by the company.&lt;/p&gt;
    &lt;head rend="h4"&gt;Screening Technology Can Help Owners Manage Risks, but Inaccurate Information May Result Denials&lt;/head&gt;
    &lt;p&gt;According to industry associations, and representatives from two companies we interviewed, owners benefit from tenant screening tools in part because they help mitigate renter-based risks, including the following:&lt;/p&gt;
    &lt;p&gt;Failure to meet lease obligations. Screening tools may help owners assess background information to reduce the risk a renter might not fulfill lease terms. For example, reviewing a prospective renterâs rent payment and credit history may help owners identify applicants with a lower risk of nonpayment.&lt;/p&gt;
    &lt;p&gt;Fraud risks. Screening algorithms assist owners in verifying information that potential renters provide. This verification helps reduce risks of identity fraud (false names, Social Security numbers, or birth dates) and synthetic fraud (fabricated documents, such as pay stubs).&lt;/p&gt;
    &lt;p&gt;Potential risks and challenges related to using screening reports include the following:&lt;/p&gt;
    &lt;p&gt;Inaccurate information. Tenant screening reports often contain inaccurate information, which can lead to unwarranted rental application denials. For example, of the approximately 26,700 screening-related complaints submitted to CFPB from January 2019 to September 2022, approximately 17,200 were related to inaccurate information.[12] Specifically, these complaints noted challenges obtaining housingâfor instance, due to information erroneously included in their report; outdated information that legally should not have been included; and inaccurate arrest, criminal, and eviction records.&lt;/p&gt;
    &lt;p&gt;Model transparency issues. As we previously reported, models using algorithms can produce unreliable and invalid results.[13] Advocacy groups noted that algorithms may present a recommendation to the owner without disclosing the specific data used or how it was weighted. As a result, owners and renters may be unable to identify or correct errors, and renters may be unable to submit mitigating information to improve their chances of securing housing.&lt;/p&gt;
    &lt;p&gt;Disparate impact. Screening algorithms that rely on criminal history and eviction records may have an adverse impact on minorities, including Black and Hispanic applicants, according to advocacy group officials.[14] For example, algorithms that recommend rejecting applicants with any criminal or eviction record may disproportionately affect these groups due to their overrepresentation in the criminal justice and housing court systems. This may contribute to reduced housing access and increased housing instability. In addition, algorithms may not differentiate between serious offenses and minor ones that are unlikely to affect renterâs reliability.&lt;/p&gt;
    &lt;p&gt;Representatives of tenant screening companies we interviewed reported taking several steps to help mitigate these risks. For example, one company allows potential renters to review their background information and submit comments to the owner before applying. Renters can flag inaccuracies or provide a narrative to explain circumstances the owner might otherwise overlook. In addition, three companies told us they review and subsequently correct inaccurate information on a tenant screening report if notified by a renter or owner.&lt;/p&gt;
    &lt;head rend="h4"&gt;Rent-Setting Algorithms Offer Owners Pricing Insights but May Lead to Higher Rents&lt;/head&gt;
    &lt;p&gt;Representatives from two industry associations, HUD officials, and representatives from one company offering rent-setting software noted that owners and renters may benefit from the use of tools that provide rent-setting algorithms in several ways, including the following:&lt;/p&gt;
    &lt;p&gt;Responsiveness to market changes. Rent-setting algorithms may help owners adjust rents more quickly in response to changing market conditions. This can help owners achieve their desired occupancy rates, keep their properties at their desired level relative to the market, and minimize revenue lost from unintended vacancies.&lt;/p&gt;
    &lt;p&gt;Improved accuracy in HUD fair market rents. HUD incorporates data from private market sources that includes a rent-setting tool to enhance the accuracy and representativeness of its fair market rent calculations.[15] According to HUD officials, incorporating private-market data helps address gaps in public data sources, such as census data, which may be outdated or lack coverage in certain markets. HUD officials stated that more accurate and timely fair market rent calculations can help renters with Housing Choice Vouchers find suitable, affordable housing.&lt;/p&gt;
    &lt;p&gt;However, representatives of advocacy groups also told us that renters face several risks when owners use rent-setting algorithms, including the following:&lt;/p&gt;
    &lt;p&gt;Reduced bargaining power. When owners rely on rent-setting algorithms to standardize rental prices across a geographic market, advertised rents may increase and renters may have less ability to negotiate lower prices. Algorithms may recommend rents that owners treat as a market benchmark, limiting flexibility to lower prices, even when individual renters attempt to negotiate. Advocacy groups expressed concern that this effect is especially pronounced in tight housing markets, where limited supply further constrains rentersâ leverage.&lt;/p&gt;
    &lt;p&gt;Potential increases in rental housing costs. According to University of Pennsylvania researchers, owners using rent-setting software adjusted rents more responsively to changing market conditions compared to other property owners.[16] This included increasing rents and reducing occupancy rates during periods of economic growth.[17]Â Moreover, this pattern was also found at the geographic level when, during periods of economic growth, higher levels of rent-setting software were associated with higher rent levels and lower occupancy rates. Advocacy groups we spoke to reiterated these findings, noting that dependent on market conditions, the use of a rent-setting algorithm can lead to higher rents for some renters.&lt;/p&gt;
    &lt;head rend="h4"&gt;Facial Recognition Technology Can Enhance Security but Pose Privacy Risks&lt;/head&gt;
    &lt;p&gt;Representatives from three of the four industry associations and all 10 of the PHAs we spoke to told us that the use of facial recognition technology can enhance safety for both private and subsidized rental housing. Owners and PHAs may install surveillance cameras equipped with facial recognition technology to improve property security. Industry association and PHA officials overseeing properties with such technology told us that it can enhance safety by helping ensure that only renters and their authorized guests can enter buildings. They noted that the technology may reduce the risk of unauthorized individuals entering public housing facilities and engaging in criminal activity.&lt;/p&gt;
    &lt;p&gt;However, representatives from advocacy organizations we interviewed raised concerns about the use of facial recognition technology in rental housing, citing risks related to accuracy, privacy, and informed consent.&lt;/p&gt;
    &lt;p&gt;Error rates. Advocacy groups we interviewed expressed concerns about facial recognition technologyâs higher error rates for identifying and verifying individuals from certain demographicsâparticularly Black women. In the rental housing context, such inaccuracies could result in frequent access denials for some individuals.[18] Representatives of facial recognition companies cited several factors that may contribute to these errors, including poor lighting, facial expressions, and obscured facial features. They also noted that data qualityâincluding outdated or low-resolution images used for comparisonâmay also affect accuracy.[19]&lt;/p&gt;
    &lt;p&gt;Privacy and consent. Facial recognition technology relies on the use of biometric information, which is unique to each person. Representatives from advocacy groups we interviewed expressed concern that surveillance data collected by owners could be used without renter consent. For example, owners could share this information with law enforcement or use it for action against a renter, such as an eviction or fine. Additionally, representatives of 6 of the ten PHAs we interviewed expressed uncertainty about what steps they should take to obtain consent when using facial recognition technology as part of their housing operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Federal Agencies Addressed Some Proptech Risks, but HUD Could Further Mitigate Risks&lt;/head&gt;
    &lt;p&gt;Federal agencies took several actions to address risks related to selected proptech tools.[20] However, HUD has opportunities to further mitigate risks related to facial recognition technology in public housing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Agencies Took Steps to Address Allegedly Misleading and Discriminatory Advertising Practices&lt;/head&gt;
    &lt;p&gt;To address potential risks associated with online listing platformsâspecifically, misleading advertisements and discriminatory advertisingâFTC, HUD, and DOJ initiated legal actions and HUD issued guidance.&lt;/p&gt;
    &lt;p&gt;Misleading advertisements. In 2022, FTC initiated a lawsuit against Roomster, a platform for rental housing and roommate listings.[21] FTC alleged that the company participated in deceptive acts or practices in violation of Section 5 of the FTC Act because its website contained inaccurate rental listings and misleading or fake consumer reviews. A federal court issued a stipulated order requiring the company to stop misrepresenting its listings, investigate any consumer complaints, and pay a fine.&lt;/p&gt;
    &lt;p&gt;Discriminatory advertising practices. In March 2019, following a complaint and investigation, HUD issued a Charge of Discrimination against Meta alleging that its advertising platform violated the Fair Housing Act by allowing advertisers to target or exclude users based on protected characteristics. For example, Metaâs platform included a toggle feature that allowed advertisers to exclude men or women from viewing an advertisement.[22]&lt;/p&gt;
    &lt;p&gt;The case was referred to DOJ, which filed suit in in 2022. DOJ alleged that Metaâs advertising delivery system used algorithms that considered traits such as familial status, race, religion, and sex.[23] As part of the settlement, Meta agreed to stop using an advertising tool known as the Special Audience Tool, develop a new system that would address racial and other disparities caused by its use of personalization algorithms in its advertisement delivery system for housing, and eliminate targeting options related to protected characteristics under the Fair Housing Act.&lt;/p&gt;
    &lt;p&gt;In April 2024, HUDâs Office of Fair Housing and Equal Opportunity issued guidance outlining advertising practices that may violate the Fair Housing Act when used to categorize or target online users.[24] These include advertisements that discourage certain groups from applying, offer different prices or conditions based on protected characteristics, or steer individuals toward specific neighborhoods. The guidance advised housing providers to avoid targeting options that directly or indirectly relate to protected characteristics. It also encouraged platforms to test their systems to ensure advertisements are not delivered in a discriminatory fashion.&lt;/p&gt;
    &lt;head rend="h4"&gt;Agencies Took Enforcement and Other Actions to Address Tenant Screening and Reporting Issues&lt;/head&gt;
    &lt;p&gt;Federal agencies have taken steps to address issues related to the accuracy and potential adverse impact of tenant screening tools. These steps include issuing guidance, taking enforcement actions and filing a statement of interest.&lt;/p&gt;
    &lt;p&gt;Guidance on Tenant Screening Tools. Also in April 2024, HUD issued guidance on applying the Fair Housing Act to screening of rental housing applicants.[25] HUDâs Office of Fair Housing and Equal Opportunity published the guidance on its public website. The guidance also addressed the application of the Fair Housing Act to tenant screening processes that use artificial intelligence and machine learning. It included best practices for rental housing owners and tenant screening companies to support compliance with fair housing laws and mitigate artificial intelligence-related and other risks. To address inaccuracy risks, the guidance recommended that owners give applicants an opportunity to dispute the accuracy or relevance of negative information in a tenant screening report. To improve transparency, it advised tenant screening companies to include all relevant information behind a denial decision, particularly when artificial intelligence tools are used and the outcome may be difficult to explain. In February 2025, we observed that HUD had removed the tenant screening guidance from its public website. HUD provided two explanations for the removal: (1) In March 2025, HUD officials told us the guidance was taken down as part of an agencywide review of its policies and guidance to ensure consistency with an executive order; and (2) in March 2025, HUD told us the agency was updating its website and the guidance remained in effect.[26]&lt;/p&gt;
    &lt;p&gt;AppFolio. In December 2020, DOJ and FTC, sued AppFolio, Inc. a tenant screening company, for violating FCRA. The complaint alleged that the company used incorrect and obsolete eviction and criminal arrest recordsâsome more than 7 years oldâin its tenant screening reports.[27] The agencies also alleged the company failed to use reasonable procedures for accuracy, relying on third-party data without sufficient verification. As part of the settlement, the company agreed to pay a fine and implement corrective measures to improve its accuracy procedures.&lt;/p&gt;
    &lt;p&gt;TransUnion Rental Screening Solutions. In October 2023, FTC and CFPB jointly sued TransUnion Rental Screening Solutions and its parent company for violating FCRA.[28] The agencies alleged the company failed to follow reasonable procedures to ensure maximum possible accuracy of report content, including reporting sealed or incorrect eviction records, and did not disclose the sources of third-party information when requested by consumers. The stipulated settlements filed with the complaint in October 2023 required the company to enhance its procedures to ensure maximum possible accuracy for verifying eviction data and to pay consumer redress and a civil penalty.&lt;/p&gt;
    &lt;p&gt;Five background screening companies. In September 2023, FTC initiated an enforcement action against five background screening companies that advertised tenant screening reports (among other things), alleging violations of FCRA and Section 5 of the FTC Act.[29] FTC alleged that the companies inaccurately reported nonexistent or irrelevant criminal recordsâsuch as traffic violationsâand allowed subscribers to edit their reports without adequate verification. In October 2023, a federal court entered a stipulated order that required the companies to cease certain practices unless they implemented compliance measures aligned with FCRA.&lt;/p&gt;
    &lt;p&gt;Statement of interest. In January 2023, DOJ and HUD submitted a statement of interest in the federal court case Louis et al. v. SafeRent Solutions and Metropolitan Management Group.[30] In the case, the two plaintiffs were Black rental applicants who used housing vouchers and alleged that SafeRentâs algorithm-based tenant screening tool assigned them low âSafeRentâ scores, resulting in the denial of their rental applications. The plaintiffs also alleged that the tool disproportionately affected Black and Hispanic applicants by relying on factors such as credit history and non-housing-related debts, while failing to account for of housing vouchers as a reliable source of income. The agencies asserted that the Fair Housing Act applies to tenant screening companies that provide data used by housing owners to make suitability determinations. Accordingly, DOJ and HUD stated that such companies are prohibited from engaging in practices that result in discrimination based on protected characteristics.[31] In December 2024, the court approved a settlement between SafeRent, the other defendants, and the class of plaintiffs, and the case was subsequently dismissed.&lt;/p&gt;
    &lt;head rend="h4"&gt;HUD Has Opportunities to Further Mitigate Risks Relating to Facial Recognition&lt;/head&gt;
    &lt;p&gt;In September 2023, HUDâs Office of Public and Indian Housing published a letter advising PHAs to balance security concerns with their public housing residentsâ privacy rights when using surveillance technology.[32] However, the letter does not provide specific direction on key operational issues regarding facial recognition technology. For example, it does not discuss how PHAs should manage privacy risks or share data with law enforcement.&lt;/p&gt;
    &lt;p&gt;Representatives of all 10 PHAs we interviewed stated they would benefit from additional information and direction from HUD on their use of facial recognition technology, especially on the following topics:&lt;/p&gt;
    &lt;p&gt;Â·Â Â Â Â Â Â Purpose specification. Six of the 10 PHAs wanted HUD to clarify the permitted uses of facial recognition technology. While they primarily use it to control building access for tenants and their authorized guests, they expressed interest in guidance on other potential uses, such as whether and how to disclose data to third parties, including law enforcement.&lt;/p&gt;
    &lt;p&gt;Â·Â Â Â Â Â Â Renter consent. Six of the 10 PHAs wanted guidance on what constitutes adequate renter consent. For example, they questioned whether posting signs about the use of facial recognition systems was sufficient. They also sought clarity on whether written consent is required before including tenant facial images in system databases, and what steps to take if a renter declined to provide consent.&lt;/p&gt;
    &lt;p&gt;Â·Â Â Â Â Â Â Data management. Five of the 10 PHAs wanted guidance on managing data collected through facial recognition systems. For example, they sought clarity on how long to retain images after a tenant moved out.&lt;/p&gt;
    &lt;p&gt;Â·Â Â Â Â Â Â Accuracy. One PHA wanted HUD to provide guidance on mitigating potential accuracy concerns.&lt;/p&gt;
    &lt;p&gt;HUD officials stated the agency has no plans to revise the September 2023 letter or issue additional written direction on facial recognition technology, citing the need to preserve PHAsâ autonomy in implementing it. However, as discussed earlier, representatives of six PHAs we interviewed expressed uncertainty about what steps they should take to obtain consent when using facial recognition technology as part of their housing operations.&lt;/p&gt;
    &lt;p&gt;HUD officials also stated that developing new guidance would require surveying about 3,300 PHAs to identify their information gaps, straining limited resources. However, the risks of facial recognition technology are well documented. For example, we have previously reported on concerns related to accuracy and the use of the technology without an individualâs consent.[33] As a result, we believe HUD could develop written direction for PHAs without the use of a survey.&lt;/p&gt;
    &lt;p&gt;According to federal internal control standards, program managers should externally communicate the necessary quality information to achieve the entityâs objectives.[34] By providing additional direction on use of facial recognition technology, HUD could help PHAs it oversees mitigate privacy and accuracy concerns and offer clarity on key issues such as purpose, consent, and data management.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;The selected proptech tools we reviewed offer benefits to individuals searching for, living in, or owning or managing rental properties. However, they also can pose risksâparticularly related to the accuracy of personal information and the potential for misrepresentation and discrimination. Federal agencies took steps to address these risks, but HUD has opportunities to further support PHAs it oversees. Providing additional written direction on the appropriate use of facial recognition technology would give PHAs greater clarity and help mitigate privacy and accuracy concerns.&lt;/p&gt;
    &lt;head rend="h3"&gt;Recommendation for Executive Action&lt;/head&gt;
    &lt;p&gt;The Secretary of HUD should ensure that the Assistant Secretary for Public and Indian Housing provides additional written direction to public housing agencies on the use of facial recognition technology. For example, this direction could specify permitted uses of the technology, define what constitutes renter consent, and address data management and accuracy concerns.&lt;/p&gt;
    &lt;head rend="h3"&gt;Agency Comments&lt;/head&gt;
    &lt;p&gt;We provided a draft of this report to HUD, CFPB, FTC, and DOJ for review and comment. These agencies provided technical comments, which we incorporated as appropriate.&lt;/p&gt;
    &lt;p&gt;As agreed with your office, unless you publicly announce the contents of this report earlier, we plan no further distribution until 30 days from the report date. At that time, we will send copies to the appropriate congressional committees, the Secretary of Housing and Urban Development, Chair of Federal Trade Commission, Acting Director of the Consumer Financial Protection Bureau, Attorney General, and other interested parties. In addition, the report will be available at no charge on the GAO website atÂ https://www.gao.gov.&lt;/p&gt;
    &lt;p&gt;If you or your staff have any questions about this report, please contact me at cackleya@gao.gov. Contact points for our Offices of Congressional Relations and Public Affairs may be found on the last page of this report. GAO staff who made key contributions to this report are listed in appendix III.&lt;/p&gt;
    &lt;p&gt;Sincerely,&lt;/p&gt;
    &lt;p&gt;Alicia Puente Cackley&lt;lb/&gt; Director, Financial Markets and Community Investment&lt;/p&gt;
    &lt;p&gt;This report examines (1) selectedÂ property technologyÂ (proptech) tools available in the rental housing market, how they are used, and by whom; (2) the benefits and risks that selectedÂ proptechÂ tools may pose for owners and renters; and (3) steps taken by federal agencies to oversee these selectedÂ proptech tools.&lt;/p&gt;
    &lt;p&gt;We identified thirty four proptech tools through a review of reports from federal regulators, academics, industry groups, and advocacy groups. We focused on tools that incorporate artificial intelligence and are used by owners and renters in the rental housing process. We purposively selected four types of proptech tools for examination: advertising platforms, tenant screening tools, rent-setting software, and facial recognition technology. These tools are not representative of all proptech tools used in the private and subsidized rental housing markets.&lt;/p&gt;
    &lt;p&gt;To gather information on owner and renter use of the selected proptech toolsâand the associated benefits and risksâwe interviewed representatives of a purposeful, nongeneralizable sample of 12 companies that provide such tools. These consisted of three advertising companies, three tenant screening companies, four facial recognition companies, and two rent-setting software companies. To identify these companies, we reviewed research reports, and publicly available lists of firms offering the selected tools to generate a list of companies. From that list, we selected companies that offered one or more of the tools and were responsive to our outreach. The information obtained from these interviews cannot be generalized to all companies that offer proptech tools in the private and subsidized rental housing markets.&lt;/p&gt;
    &lt;p&gt;In addition, we interviewed representatives from four industry associations and five advocacy organizations, selected because they published reports from 2019 to 2024 about the selected tools.[35] We also interviewed officials from two organizations funded by the Fair Housing Initiative Program, a federal program designed to assist people who believe they have been victims of housing discrimination.[36] We selected the two organizations because they work directly with renters, are in the metropolitan area with the largest number of public housing units based on Department of Housing and Urban Development (HUD) data and were recipients of HUD Fair Housing Initiative Program grants within the past 2 years.&lt;/p&gt;
    &lt;p&gt;To understand the use of and benefits and risks of the selected proptech tools in the subsidized housing market, we conducted semi-structured interviews with representatives of a nongeneralizable sample of 10 public housing agencies (PHA). We selected these PHAs to achieve a diversity in PHA size, Census region, and use of facial recognition technology. The information we gathered from these interviews cannot be generalized to the approximately 3300 PHAs.&lt;/p&gt;
    &lt;p&gt;To identify steps taken by federal agencies to oversee the selected proptech tools, we reviewed agency guidance, final rulemakings, federal court orders, agency enforcement actions, and relevant advisory opinions issued from 2019 through 2024. We analyzed relevant HUD documentation and interviewed HUD and selected PHA officials on HUD efforts to communicate to PHAs on the use of surveillance technology in their operations. We also compared a HUD communication to PHAs about using surveillance technology in their operations against federal internal control standards.[37] We determined that the internal control principle that program managers should externally communicate the necessary quality information to achieve the entityâs objectives was significant to this objective.&lt;/p&gt;
    &lt;p&gt;To address all three objectives, we also reviewed relevant laws, including the Fair Housing Act, the Fair Credit Reporting Act, the Federal Trade Commission Act, and the Sherman Antitrust Act. We also reviewed relevant regulations, such as HUDâs regulation on discriminatory advertising. In addition, we interviewed representatives from the following federal agencies: Consumer Financial Protection Bureau, Department of Justice, Federal Trade Commission, and HUD. Within HUD, we interviewed officials from the Office of Public and Indian Housing, Office of Multifamily Housing, Office of Policy Development and Research, and Office of Fair Housing and Equal Opportunity.[38]&lt;/p&gt;
    &lt;p&gt;We conducted this performance audit from November 2023 to July 2025 in accordance with generally accepted government auditing standards. Those standards require that we plan and perform the audit to obtain sufficient, appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives. We believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix II: Selected Actions Federal Agencies Took Related to Property Technology Use in the Rental Housing Context, 2019â2024&lt;/head&gt;
    &lt;p&gt;This appendix provides information on selected legal actions or guidance initiated by federal agencies from 2019 through 2024 related to tenant screening, advertising platforms, or rent-setting software.&lt;/p&gt;
    &lt;p&gt;Table 2: Status of Selected Legal Actions Federal Agencies Took Related to Use of Property Technology Tools in the Rental Housing Context, as of April 2025&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;DOJ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Litigation&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Rent-setting software. In 2024, the Department of Justice (DOJ) and attorneys general from eight states initiated a civil antitrust lawsuit against RealPage, Inc, a company that develops and sells rent-setting software.a The complaint alleges that the companyâs software enables rental housing owners to share confidential, competitively sensitive informationâsuch as rental prices and lease termsâallowing them to align rents and reduce competition in violation of the Sherman Act. In December 2024, RealPage moved to dismiss, arguing that its software lacked market-wide influence and that DOJ failed to show anticompetitive effects. Additionally, in 2025, DOJ amended its complaint to include certain property owners.b&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;HUD-DOJ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Amicus brief&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Tenant screening. In Connecticut Fair Housing Center v. CoreLogic Rental Property Solutions LLC, the Department of Housing and Urban Development (HUD) and DOJ filed a joint amicus brief in support of plaintiffs-appellants-cross-appellees. The brief asserted that tenant screening companies such as CoreLogic are subject to the Fair Housing Actâeven if they are not the entities rejecting or accepting potential rentersâbecause their reports to owners can be used to make housing unavailable. The agencies also underscored that blanket exclusions based on criminal history can disproportionately affect minority groups.c They cited HUDâs 2016 guidance warning that such practices may violate the Fair Housing Act if not based on a substantial, legitimate, nondiscriminatory interest.d&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;DOJ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Statement of interest&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Rent-setting software. In September 2023, plaintiffs filed an amended complaint alleging that RealPage and several property-management companies engaged in a price-fixing conspiracy in violation of Section 1 of the Sherman Act. They claimed RealPageâs rent-setting software facilitated the conspiracy by aggregating and sharing nonpublic rental data among competing property owners and managers. In November 2023, DOJ submitted a statement of interest arguing that such software enables owners to share nonpublic, sensitive information, potentially leading to coordinated rental housing prices.e This coordination, DOJ asserted, harms competition among owners and keeps prices artificially high for renters.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;FTC-DOJ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Statement of interest&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Rent-setting software. In September 2023, the plaintiff filed a class action lawsuit in federal court against Yardi Systems, Inc. and multiple property management companies.f The lawsuit alleges that the defendants collaborated to fix rental housing prices using Yardiâs rent-setting software. In December 2023, the defendants moved to dismiss, contending that each company independently chose to use Yardiâs software and set rents on its own. In March 2024, DOJ and the Federal Trade Commission (FTC) issued a joint statement of interest. They noted that ownersâ collective reliance on Yardiâs rent-setting algorithm could facilitate price-fixing agreements in potential violation of the Sherman Act. The agencies also noted that even without direct communication among owners or strict adherence to algorithmâs recommendations, shared use of the technology might lead to coordinated pricing, harming consumers through reduced competition. In December 2024, the federal court denied the motion to dismiss.g&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: GAO analysis. Â | Â GAOâ25â107196&lt;/p&gt;
    &lt;p&gt;aUnited States v. RealPage, Inc., No. 1:24-cv-710 (M.D.N.C. filed Aug. 23, 2024).&lt;/p&gt;
    &lt;p&gt;bThe six owners are Greystar Real Estate Partners, Cushman &amp;amp; Wakefield, Camden Property Trust, LivCor, Pinnacle Property Management Services, and Willow Bridge Property Company. The January 2025 amended complaint initially included Cortland Management, but on the same date the United States and Cortland reached a final judgement where Cortland agree to certain changes and to refrain from certain actions. In February 2025, following the amended complaint, RealPage filed another motion to dismiss the amended complaint which contained similar arguments on these points. As of June 26, 2025, the parties are awaiting court decisions on a motion to dismiss and a proposed final judgment.&lt;/p&gt;
    &lt;p&gt;cIn Connecticut Fair Housing Center v. CoreLogic Rental Property Solutions, the plaintiff alleged that CoreLogicâs tenant screening tool, CrimeSAFE, discriminated based on race, national origin, and disability when the toolâs use resulted in the denial of a disabled Latino man with no criminal conviction from moving in with his mother, alleging violations of the Fair Housing Act, Fair Credit Reporting Act, and the Connecticut Unfair Trade Practice Act. No. 3:18-cv-705 (D. Conn.). The United States District Court for the District of Connecticut ruled in favor of the plaintiff for the Fair Credit Reporting Act claim but ruled in CoreLogicâs favor for the Fair Housing Act and Connecticut Unfair Trade Practice Act claims. The plaintiffs appealed the district courtâs denial of their Fair Housing Act claims, among others, and CoreLogic then cross-appealed the courtâs decision relating to the Fair Credit Reporting Act claim. In November 2024, the parties, and the United States as amicus curiae supporting the plaintiff, presented arguments in the United States Court of Appeals for the Second Circuit. As of June 26, 2025, the Second Circuit had not issued a decision. Conn. Fair Hous. Ctr. v. CoreLogic Rental Prop. Sol., LLC, No. 23-1166 (2d Cir. argued Nov. 20, 2024).&lt;/p&gt;
    &lt;p&gt;dDepartment of Housing and Urban Development, Office of General Counsel, Guidance on Application of Fair Housing Standards to the Use of Criminal Records by Providers of Housing and Real Estate-Related Transactions (Washington D.C.: April 2016); and Guidance on Fair Housing Act Protections for Tenant Screening Practices Involving Criminal History (Washington D.C.: June 2022).&lt;/p&gt;
    &lt;p&gt;eIn re RealPage, No. 3:23-MD-3071 (M.D. Tenn). In February 2025, following the amended complaint, RealPage filed another motion to dismiss the amended complaint, which contained similar arguments on these points. As of June 2025, the court had not held oral arguments for RealPageâs motion to dismiss. The other defendant property management companies subsequently filed separate motions to dismiss on their own behalf. Settlement agreements have been reached with multiple defendants. As of June 2025, the case was ongoing.&lt;/p&gt;
    &lt;p&gt;fDuffy v. Yardi Sys., Inc., No. 2:23-cv-01391 (W.D. Wash.).&lt;/p&gt;
    &lt;p&gt;gAs of June, 2025, the case was ongoing.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Topic&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Status&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Summary of advisory opinion&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Name-only matching procedures&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Withdrawn&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;In November 2021, the Consumer Financial Protection Bureau (CFPB) published an advisory opinion stating that name-only matching proceduresâthose based solely on the similarity of first and last namesâdo not meet the Fair Credit Reporting Actâs (FCRA) requirement to use reasonable procedures to ensure maximum possible accuracy when generating consumer reports.a The opinion notes that relying solely on a personâs first and last name could lead to inaccurate information being included in a consumer report. CFPB withdrew this guidance on May 12, 2025.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Facially false data&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Active&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;In October 2022, CFPB published an advisory opinion stating that consumer reporting agencies must implement reasonable procedures to detect and prevent the inclusion of facially false (logically inconsistent) data when generating consumer reports.b Examples include reporting a delinquency that predates the account opening or an account closure date that predates the consumerâs listed date of birth.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Report accuracy&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Withdrawn&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;In January 2024, CFPB issued an advisory opinion on background reports.c The opinion clarifies that consumer reporting agencies must implement reasonable procedures to ensure that reports do not include information that is duplicative, expunged, sealed, or otherwise restricted from public access. Each adverse item is also subject to its own reporting period. For example, a criminal charge that did not result in a conviction generally cannot be reported more than 7 years after the date of the charge. CFPB withdrew this guidance on May 12, 2025.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;File disclosures&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Withdrawn&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;In January 2024, CFPB issued an advisory opinion on file disclosures.d The opinion underscores that under FCRA, consumers have the right to request and obtain all information in their consumer file at the time of request. The opinion explains how consumer reporting agencies must fulfill this request even if the consumer does not explicitly ask for a âcomplete fileâ and also must disclose the sources used to generate the report. CFPB withdrew this guidance on May 12, 2025.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: GAO analysis of CFPB advisory opinions. Â | Â GAOâ25â107196&lt;/p&gt;
    &lt;p&gt;CFPB issues advisory opinions to publicly address uncertainty about its existing regulations and provide guidance to regulated entities.&lt;/p&gt;
    &lt;p&gt;aConsumer Financial Protection Bureau, Fair Credit Reporting: Name-Only Matching Procedures (Washington, D.C.: Nov. 4, 2021).&lt;/p&gt;
    &lt;p&gt;bConsumer Financial Protection Bureau, Fair Credit Reporting: Facially False Data (Washington D.C.: Oct. 20, 2022).&lt;/p&gt;
    &lt;p&gt;cConsumer Financial Protection Bureau, Fair Credit Reporting; Background Screening (Washington D.C.: Jan. 11, 2024).&lt;/p&gt;
    &lt;p&gt;dConsumer Financial Protection Bureau, Fair Credit Reporting: File Disclosures (Washington, D.C.: Jan. 11, 2024).&lt;/p&gt;
    &lt;head rend="h3"&gt;GAO Contact&lt;/head&gt;
    &lt;p&gt;Alicia Puente Cackley, CackleyA@gao.gov.&lt;/p&gt;
    &lt;head rend="h3"&gt;Staff Acknowledgments&lt;/head&gt;
    &lt;p&gt;In addition to the contact named above, Cory Marzullo (Assistant Director), Brandon Jones (Analyst in Charge), Daniel Horowitz, Lydie Loth, Marc Molino, Barbara Roesmann, Jessica Sandler, Tristan Shaughnessy, Norma-Jean Simon, and Sean Worobec made key contributions to this report.&lt;/p&gt;
    &lt;p&gt;The Government Accountability Office, the audit, evaluation, and investigative arm of Congress, exists to support Congress in meeting its constitutional responsibilities and to help improve the performance and accountability of the federal government for the American people. GAO examines the use of public funds; evaluates federal programs and policies; and provides analyses, recommendations, and other assistance to help Congress make informed oversight, policy, and funding decisions. GAOâs commitment to good government is reflected in its core values of accountability, integrity, and reliability.&lt;/p&gt;
    &lt;p&gt;Obtaining Copies of GAO Reports and Testimony&lt;/p&gt;
    &lt;p&gt;The fastest and easiest way to obtain copies of GAO documents at no cost is through our website. Each weekday afternoon, GAO posts on its website newly released reports, testimony, and correspondence. You can also subscribe to GAOâs email updates to receive notification of newly posted products.&lt;/p&gt;
    &lt;p&gt;Order by Phone&lt;/p&gt;
    &lt;p&gt;The price of each GAO publication reflects GAOâs actual cost of production and distribution and depends on the number of pages in the publication and whether the publication is printed in color or black and white. Pricing and ordering information is posted on GAOâs website, https://www.gao.gov/ordering.htm.&lt;/p&gt;
    &lt;p&gt;Place orders by calling (202) 512-6000, toll free (866) 801-7077, or &lt;lb/&gt; TDD (202) 512-2537.&lt;/p&gt;
    &lt;p&gt;Orders may be paid for using American Express, Discover Card, MasterCard, Visa, check, or money order. Call for additional information.&lt;/p&gt;
    &lt;p&gt;Connect with GAO&lt;/p&gt;
    &lt;p&gt;Connect with GAO on X, LinkedIn, Instagram, and YouTube.&lt;lb/&gt; Subscribe to our Email Updates. Listen to our Podcasts.&lt;lb/&gt; Visit GAO on the web at https://www.gao.gov.&lt;/p&gt;
    &lt;p&gt;To Report Fraud, Waste, and Abuse in Federal Programs&lt;/p&gt;
    &lt;p&gt;Contact FraudNet:&lt;/p&gt;
    &lt;p&gt;Website: https://www.gao.gov/about/what-gao-does/fraudnet&lt;/p&gt;
    &lt;p&gt;Automated answering system: (800) 424-5454&lt;/p&gt;
    &lt;p&gt;Media Relations&lt;/p&gt;
    &lt;p&gt;Sarah Kaczmarek, Managing Director, Media@gao.gov&lt;/p&gt;
    &lt;p&gt;Congressional Relations&lt;/p&gt;
    &lt;p&gt;A. Nicole Clowers, Managing Director, CongRel@gao.gov&lt;/p&gt;
    &lt;p&gt;General Inquiries&lt;/p&gt;
    &lt;p&gt;[1]Artificial intelligence, in general, refers to computer systems that can solve problems and perform tasks that have traditionally required human intelligence and that continually get better at their assigned tasks. The White House, Office of Science and Technology Policy, American Artificial Intelligence Initiative: Year One Annual Report (Washington, D.C.: February 2020). Machine learning, a type of artificial intelligence, uses algorithms to identify patterns in information.&lt;/p&gt;
    &lt;p&gt;[2]We use âtenant screeningâ throughout this report to refer to tools that seek to assess the suitability of prospective renters for rental housing. In all other contexts, we use ârenterâ rather than âtenant.â We use âownersâ to refer to individuals who own rental property, as well as those individuals or entitiesâsuch as property managersâacting on their behalf.&lt;/p&gt;
    &lt;p&gt;[3]Several resources we reviewed rely on the disparate-impact theory of liability, mentioned below. Under Executive Order 14281, executive departments and agencies are tasked withâamong other thingsârepealing or amending rules and regulations to the extent they contemplate disparate-impact liability and evaluating and taking appropriate action regarding pending investigations and proceedings and existing consent judgments and permanent injunctions relying on theories of disparate-impact liability. Executive Order 14281: Restoring Equality of Opportunity and Meritocracy, 90 Fed. Reg. 17537 (Apr. 28, 2025). As of May 8, 2025, the rules, regulations, and legal actions mentioned below have not been amended or repealed, though they may be affected to the extent they rely on the disparate-impact theory of liability. On February 9, 2025, the National Treasury Employees Union and others filed a lawsuit in the District Court for the District of Columbia alleging that the actions of the Acting Director of CFPB including regarding staffing and enforcement work violated the Administrative Procedure Act and were unconstitutional because they violated the Congressional mandate in the Dodd-Frank Wall Street Reform and Consumer Protection Act for CFPB to perform its statutory functions. Natâl Treasury Emp. Union, et al. v. Vought, 1:25-cv-00381 (D.D.C. filed Feb. 9, 2025). As of May 2025, the litigation is active and continues in both the District Court for the District of Columbia and the DC Circuit Court of Appeals.&lt;/p&gt;
    &lt;p&gt;[4]GAO, Standards for Internal Control in the Federal Government, GAOâ14â704G (Washington, D.C.: Sept. 10, 2014).&lt;/p&gt;
    &lt;p&gt;[5]Fair Housing Act, Pub. L. No. 90-284, 82 Stat. 73, 81â89, Â§Â§ 801â819 (1968), as amended (codified at 42 U.S.C. Â§Â§Â 3601â3619). Protected characteristics under the act include race, color, religion, national origin, sex, disability, and familial status. HUD also provides funding to Fair Housing Initiative Program organizations to assist individuals who believe they have been victims of housing discrimination. These organizations also conduct preliminary investigations of discriminatory claims, including sending âtestersâ to properties suspected of practicing housing discrimination. State and local governments may enforce their own statutes and ordinances that are substantially equivalent to the Fair Housing Act.&lt;/p&gt;
    &lt;p&gt;[6]If an election is made, HUD refers the case to DOJ, which then files a complaint in federal court. If no election is made, HUD will litigate the case before its administrative law judges.&lt;/p&gt;
    &lt;p&gt;[7]15 U.S.C. Â§Â§ 1681-1681. CFPB also has supervisory authority over covered persons, including certain consumer reporting agencies, with respect to compliance with FCRA. 12 U.S.C Â§Â§ 5511(c)(4); 5481(6), (14), (15)(A)(ix); 5514â5516.&lt;/p&gt;
    &lt;p&gt;[8]In 2023, FTC and CFPB jointly issued a request for information seeking public input on the use of algorithms in tenant screening. The request also asked about the potential for discriminatory outcomes related to the use of criminal history and eviction records in the rental housing process. Agency officials told us they issued the request for general information-gathering purposes.&lt;/p&gt;
    &lt;p&gt;[9]15 U.S.C. Â§ 45.&lt;/p&gt;
    &lt;p&gt;[10]As of May, 2025, approximately 970,000 households were living in public housing. The Housing Choice Voucher program subsidizes the rents of more than 2.3 million households. We did not include HUDâs Office of Multifamily Housing in our review because it is not directly responsible for overseeing the use of the selected proptech tools in subsidized properties, according to officials.&lt;/p&gt;
    &lt;p&gt;[11]The information is typically obtained from sources such as potential renters, third-party vendors, courthouses, and other consumer reporting agencies.&lt;/p&gt;
    &lt;p&gt;[12]Consumer Financial Protection Bureau, Consumer Snapshot: Tenant Background Checks (Washington D.C.: November 2022). CFPBâs reported data were the most current data available as of May 2025.&lt;/p&gt;
    &lt;p&gt;[13]GAO, Artificial Intelligence in Health Care: Benefits and Challenges of Technologies to Augment Patient Care, GAOâ21â7SP (Washington, D.C.: Nov. 30, 2020).&lt;/p&gt;
    &lt;p&gt;[14]There are two principal theories of liability for discrimination: disparate treatment and disparate impact. Disparate impact can occur when a facially neutral policy or practice may be unlawfully discriminatory because it has a disproportionately negative impact on members of a protected class without a legitimate business need or where that need could be achieved as well through less discriminatory means. Disparate impact may occur when an algorithm uses a variable that does not directly refer to a protected class but still leads to disparate outcomes for certain groups. In 2015, the Supreme Court upheld the application of disparate impact under the Fair Housing Act in Tex. Depât of Hous. &amp;amp; Cmty. Aff. v. The Inclusive Cmties. Project, Inc., 576 U.S. 519 (2015). In April 2025, the President issued an Executive Order establishing a policy to eliminate the use of disparate impact liability in all contexts to the maximum degree possible. The Executive Order directs all federal agencies to deprioritize enforcement of statutes and regulations that include disparate impact liability. Executive Order 14281: Restoring Equity of Opportunity and Meritocracy, 90 Fed. Reg. 17537 (Apr. 28, 2025).&lt;/p&gt;
    &lt;p&gt;[15]In 2024, HUD supplemented public data with six private-sector data sources as part of its methodology to determine fair market rents. These sources were: (1) Apartment List Rent Estimate; (2) CoStar Group average effective rent; (3) CoreLogicâs single-family, combined 3-bedroom median rent index; (4) Moodyâs Analytics average gross revenue per unit; (5) RealPageâs average effective rent per unit; and (6) Zillowâs Observed Rent Index.&lt;/p&gt;
    &lt;p&gt;[16]Calder-Wang, Sophie and Gi Heung Kim, Algorithmic Pricing in Multifamily Rentals: Efficiency Gains or Price Coordination? (Aug. 16, 2024).&lt;/p&gt;
    &lt;p&gt;[17]The study also found that owners that used rent-setting software decreased rents to increase occupancy rates during economic downturns.&lt;/p&gt;
    &lt;p&gt;[18]We previously reported that accuracy in facial recognition has been found to be variable and demographically biased. See GAO, Biometric Identification Technologies: Considerations to Address Information Gaps and Other Stakeholder Concerns, GAOâ24â106293 (Washington, D.C: Apr. 22, 2024).&lt;/p&gt;
    &lt;p&gt;[19]For additional information on the accuracy of facial recognition technology across demographics, see GAO, Facial Recognition Technology: Privacy and Accuracy Issues Related to Commercial Uses, GAOâ20â522 (Washington, D.C.: July 13, 2020).&lt;/p&gt;
    &lt;p&gt;[20]We limited our discussion of agency actions to finalized legal judgments and executed compliance agreements. Appendix II includes information on currently active legal actions initiated by HUD, DOJ, and FTC related to the proptech tools we reviewed. Executive Order 14281 requires agencies to evaluate existing consent judgments and permanent injunctions that rely on theories of disparate impact liability and to take appropriate actions with respect to matters consistent with the order. The order eliminates the use of disparate impact liability in all contexts to the maximum degree possible. The agencies have 90 days from April 23, 2025 to complete the review. The agencies may make decisions that impact the cases listed in this report. As of May 6, 2025, the agencies have not made public notification of any changes to these closed cases based on Executive Order 14281.&lt;/p&gt;
    &lt;p&gt;[21]Fed. Trade Commân et al. v. Roomster, 1:22-cv-07389 (S.D.N.Y.). Separately, another defendant was ordered to stop selling consumer reviews or endorsements and fined.&lt;/p&gt;
    &lt;p&gt;[22]Facebook, a social media platform owned by Meta, displays advertisements to users, including those for rental housing opportunities.&lt;/p&gt;
    &lt;p&gt;[23]United States v. Meta Platforms, Inc, f/k/a Facebook, Inc., No. 1:22-cv-05187 (S.D.N.Y).&lt;/p&gt;
    &lt;p&gt;[24]Department of Housing and Urban Development, Guidance on Application of the Fair Housing Act to the Advertising of Housing, Credit, and Other Real Estate-Related Transactions through Digital Platforms (Washington, D.C.: April 2024).&lt;/p&gt;
    &lt;p&gt;[25]Department of Housing and Urban Development, Guidance on the Application of the Fair Housing Act to the Screening of Applicants for Rental Housing (Washington, D.C.: April 2024). HUD issued the guidance in response to Executive Order 14110, which required HUD to issue guidance on the use of artificial intelligence in housing decisions. Executive Order 14110 was rescinded on January 20, 2025, by Executive Order 14148.&lt;/p&gt;
    &lt;p&gt;[26]The White House, Ending Radical and Wasteful Government DEI Programs and Preferencing, Executive Order 14151 (Washington, D.C.: Jan. 20, 2025).&lt;/p&gt;
    &lt;p&gt;[27]United States v. AppFolio, Inc., No. 1:20-cv-03563 (D.D.C.).&lt;/p&gt;
    &lt;p&gt;[28]Fed. Trade Commân v. TransUnion Rental Screening Sol., No. 1:23-cv-02659 (D. Colo.). See also, In re TransUnion, 2023-CFPB-0011 (Oct. 12, 2023).&lt;/p&gt;
    &lt;p&gt;[29]Fed. Trade Commân v. Instant Checkmate, LLC et al., No. 3:23-cv-01674 (S.D. Cal.). The five affiliated companies are Instant Checkmate, Truthfinder, Intelicare Direct, The Control Group Media Company, and PubRec. FTC alleged in its complaint that all five companies operated as a common enterprise while engaging in the alleged unlawful acts and practices.Â Â&lt;/p&gt;
    &lt;p&gt;[30]Louis et al v. SafeRent Sol., LLC and Metro. Mgmt. Grp., No. 1:22-cv-10800 (D. Mass.) DOJ is authorized under 28 U.S.C. Â§Â§ 516 and 517 to file statements of interest in federal and state court cases between private parties in which the United States has an interest.&lt;/p&gt;
    &lt;p&gt;[31]In July 2023, the court denied the defendantsâ motions to dismiss the Fair Housing Act claim and permitted the case to proceed against SafeRent and the other defendants. The court dismissed certain counts against SafeRent that were unrelated to the Fair Housing Act.&lt;/p&gt;
    &lt;p&gt;[32]Department of Housing and Urban Development, Office of Public and Indian Housing, Letter to Public Housing Agencies (Sep. 22, 2023). As stated previously, as of May 2025, approximately 970,000 households were living in public housing. As administrator of the public housing program, HUD is responsible for providing guidance to PHAs on operations, including use of surveillance technology.&lt;/p&gt;
    &lt;p&gt;[35]The industry associations are the Consumer Data Industry Association, National Apartments Association, National Multifamily Housing Council, and Security Industry Association. The advocacy organizations are the National Fair Housing Alliance, National Low Income Housing Coalition, American Civil Liberties Union, Center for Democracy and Technology, and National Consumer Law Center.&lt;/p&gt;
    &lt;p&gt;[36]The two Fair Housing Initiative Program organizations are Brooklyn Legal Service and the Fair Housing Justice Center.&lt;/p&gt;
    &lt;p&gt;[37]GAO, Standards for Internal Control in the Federal Government, GAOâ14â704G (Washington, D.C.: Sept. 10, 2014).&lt;/p&gt;
    &lt;p&gt;[38]We did not include HUDâs Office of Multifamily Housing in our review because it is not directly responsible for overseeing the use of the selected proptech tools in subsidized properties, according to officials.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://files.gao.gov/reports/GAO-25-107196/index.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45076367</id><title>University of Cambridge Cognitive Ability Test</title><updated>2025-08-30T20:34:34.994851+00:00</updated><content>&lt;doc fingerprint="effc4490994a316d"&gt;
  &lt;main&gt;
    &lt;p&gt;If this message will not disappear in a moment, you might need to use other browser.&lt;/p&gt;
    &lt;p&gt;Your browser does not support javascript. Please enable javascript and refresh your browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://planning.e-psychometrics.com/test/icar60"/></entry><entry><id>https://news.ycombinator.com/item?id=45077143</id><title>The Rise of Hybrid PHP: Blending PHP with Go and Rust</title><updated>2025-08-30T20:34:34.904446+00:00</updated><content>&lt;doc fingerprint="3f8b1a3609745aaa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Rise of Hybrid PHP: Blending PHP with Go and Rust&lt;/head&gt;
    &lt;p&gt;Original - 2025-08-30 - yeknava - 2 minute read&lt;/p&gt;
    &lt;p&gt;We used to develop our application as a single DDD monolith (let’s call it the mother) with several smaller microservices around it (the children) to gain some specific advantages. Most of these microservices were built in Go, while the core monolithic service was developed in PHP 8.3.&lt;/p&gt;
    &lt;p&gt;This stack served us well for a long time. The Go microservices efficiently handled our high-throughput requests, and the carefully designed monolith allowed our relatively small backend team to deliver features quickly and with confidence. It was a good balance: speed where we needed it most, and stability and productivity everywhere else.&lt;/p&gt;
    &lt;p&gt;As many of you may have experienced, 80% of your traffic often targets only 20% of your APIs—the well-known Pareto principle. And unsurprisingly, those hot 20% endpoints are usually the ones where performance matters the most. In the past, our strategies included writing highly optimized code, adding extreme caching layers, or extracting certain parts into Go-based microservices. While effective, these approaches added complexity and operational overhead.&lt;/p&gt;
    &lt;p&gt;But now, thanks to new capabilities in the PHP ecosystem and the rise of powerful libraries and runtimes, it’s becoming much easier to keep more logic inside the monolith while still achieving excellent performance. Let’s look at a few exciting options:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. FFI (Foreign Function Interface)&lt;/head&gt;
    &lt;p&gt;PHP’s FFI feature allows you to call C code directly from PHP. This opens the door to system-level operations or performance-critical logic without leaving your PHP project. Of course, you need to be mindful of context switching costs, but for the right use cases, it’s a game-changer.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Rust-Based Extensions&lt;/head&gt;
    &lt;p&gt;If writing raw C isn’t your cup of tea, you can now write PHP extensions in Rust (or even Zig). This lets you offload heavy, performance-sensitive parts of your application to safe, memory-efficient, compiled code. Rust, in particular, offers memory safety guarantees without sacrificing speed, which makes it a great fit for extensions that need to be both reliable and fast.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Go-Based Extensions with FrankenPHP&lt;/head&gt;
    &lt;p&gt;We’ve recently switched to FrankenPHP (after seeing it become officially supported by the PHP Foundation). Running PHP in FrankenPHP’s worker mode is impressively fast—sometimes over 4x faster in our benchmarks compared to traditional setups.&lt;/p&gt;
    &lt;p&gt;Even more exciting, a recent release introduced the ability to write PHP extensions in Go. This feature is something we are actively exploring because it would let us build high-performance APIs in Go and expose them seamlessly inside our PHP monolith. That way, we can combine the productivity of PHP with the raw speed of Go, without needing to split everything into separate services.&lt;/p&gt;
    &lt;head rend="h3"&gt;But Why Not Just Rewrite Everything in Go or Rust?&lt;/head&gt;
    &lt;p&gt;It’s a fair question—and one we’ve asked ourselves too. There are two main reasons why we don’t simply migrate the entire backend:&lt;/p&gt;
    &lt;p&gt;1. Rewriting is costly. Many applications are already large and stable. Rewrites are risky, time-consuming, and often introduce more problems than they solve. In most scenarios, a rewrite should be the very last option.&lt;/p&gt;
    &lt;p&gt;2. PHP is still a great fit. For the majority of the application, PHP does the job well. It’s fast enough, developer-friendly, and supported by a large ecosystem. For those few cases where you truly need maximum performance, you can now selectively write parts in Go or Rust as extensions—rather than rewriting the entire system.&lt;/p&gt;
    &lt;p&gt;In short, the modern PHP ecosystem gives us the best of both worlds: the ability to build quickly and confidently in PHP, while still having powerful options (C, Rust, Go) for performance-critical parts. This hybrid approach lets us stay productive without sacrificing speed where it matters most.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yekdeveloper.com/p/4-the-rise-of-hybrid-php"/></entry><entry><id>https://news.ycombinator.com/item?id=45077291</id><title>Are we decentralized yet?</title><updated>2025-08-30T20:34:34.649130+00:00</updated><content>&lt;doc fingerprint="24d16fd9c88f72be"&gt;
  &lt;main&gt;
    &lt;p&gt;This page measures the concentration of the Fediverse and the Atmosphere according to the HerfindahlâHirschman Index (HHI), an indicator from economics used to measure competition between firms in an industry. Mathematically, HHI is the sum of the squares of market shares of all servers.&lt;/p&gt;
    &lt;p&gt;Values close to zero indicate perfectly competitive markets (eg. many servers, with users spread evenly), while values close to 10000 indicate highly concentrated monopolies (eg. most users on a single server). In economics, values below 100 are considered "Highly Competitive", below 1500 is "Unconcentrated", and above 2500 is considered "Highly Concentrated".&lt;/p&gt;
    &lt;p&gt;This site currently measures the concentration of user data for active users: in the Fediverse, this data is on servers (also known as instances); in the Atmosphere, it is on the PDSes that host users' data repos. Other metrics and comparisons are possible, and I'm interested in exploring them.&lt;/p&gt;
    &lt;p&gt;Code and data are available on GitHub. Comments and pull requests, including other metrics for measuring distribution and resiliency, are welcome!&lt;/p&gt;
    &lt;p&gt;By Rob Ricci: @ricci@discuss.systems / @ricci.io&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arewedecentralizedyet.online/"/></entry><entry><id>https://news.ycombinator.com/item?id=45077345</id><title>The space race is transforming Southern California's economy – again</title><updated>2025-08-30T20:34:34.487218+00:00</updated><content>&lt;doc fingerprint="1f9d855351b4bde9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Share via&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Southern California’s space economy is being revitalized with a new wave of startups pushing technological boundaries.&lt;/item&gt;
      &lt;item&gt;A report by the Los Angeles Economic Development Corp. shows the county’s aerospace and defense industries added 11,000 jobs between 2022 and 2024. The jobs had an average wage of $141,110 — more than twice the county average.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In a giant Long Beach warehouse near where Boeing used to build the C-17 cargo jet, Vast is fabricating what could be the first commercial space station to circle Earth.&lt;/p&gt;
    &lt;p&gt;Just up the road in El Segundo, Varda Space Industries has grown molecular crystals in microgravity with few impurities for pharmaceuticals that one day could be injected in cancer patients.&lt;/p&gt;
    &lt;p&gt;And a little south in Seal Beach, a scrappy company called AstroForge aims to land a satellite on an asteroid just a football field wide and mine possibly billions in platinum riches.&lt;/p&gt;
    &lt;p&gt;The companies aren’t anomalies but rather three examples of Southern California’s growing space economy, which shrank after the Cold War but is being revitalized with a new wave of startups pushing technological boundaries.&lt;/p&gt;
    &lt;p&gt;Southern California has an illustrious aerospace heritage, starting with some of the first rocket experiments at what was to become the Jet Propulsion Laboratory — with the region later producing the X-15 rocket plane, the Apollo capsule and the space shuttles.&lt;/p&gt;
    &lt;p&gt;Today’s innovators are following in the flight path of a relative newcomer: Elon Musk’s SpaceX, which set up shop in El Segundo in 2002 to draw on the region’s deep engineering talent and successfully pioneered the development of low-cost reusable rockets.&lt;/p&gt;
    &lt;p&gt;Though the company has since moved to Texas, its main operations remain in Hawthorne and many of the new companies have been founded by SpaceX alumni — or are reliant on its Falcon 9 workhorse rocket, which recently surpassed 500 launches.&lt;/p&gt;
    &lt;p&gt;“The massive drop in the cost of getting mass into orbit, and the frequency with which they do launches ... is almost exclusively due to SpaceX, “ said Andrew Sather, a partner at Initialized Capital, a San Francisco venture capital firm that invested in AstroForge.&lt;/p&gt;
    &lt;p&gt;Some 128 aerospace, artificial intelligence and companies in other fields have been founded by former SpaceX employees — with 96 started in the last five years still in operation, according to the alumnifounders.com website run by a San Francisco tech executive.&lt;/p&gt;
    &lt;p&gt;Nearly half, or 63, were founded in Southern California, including 20 in aerospace. No other region comes close, including Silicon Valley or the Pacific Northwest, where Jeff Bezos’ Blue Origin rocket company is based in Kent, Wash.&lt;/p&gt;
    &lt;p&gt;Some companies just come to the region to be close to same talent pool and aerospace manufacturing base that first attracted SpaceX. Rocket Lab, which launches small satellites, was founded in New Zealand but moved to the region in 2013 and opened new headquarters in Long Beach in 2020.&lt;/p&gt;
    &lt;p&gt;And it’s not just the contrails that reflect the region’s rebound — it’s also the money.&lt;/p&gt;
    &lt;p&gt;Some of Silicon Valley’s leading investors have placed bets here, including Khosla Ventures, Andreessen Horowitz and Peter Thiel, whose Founders Fund was the lead investor in a June $2.5 billion funding round for Anduril, a Costa Mesa maker of drones and other autonomous defense systems now valued at $30 billion.&lt;/p&gt;
    &lt;p&gt;A forthcoming report by the Los Angeles Economic Development Corp. shows the county’s aerospace and defense industries added 11,000 jobs between 2022 and 2024. While those 58,700 plus total jobs are well below the historic peak, they had an average wage of $141,110 — more than twice the county average.&lt;/p&gt;
    &lt;p&gt;“A lot of folks have kind of made the assumption that the aerospace and defense industry has left the entire region,” said Stephen Cheung, chief executive of the organization. “What they didn’t see is a lot of the manufacturing was still here, and over the last 10 years, you’ve been seeing this transition into space commercialization, and now that’s stimulating a whole new ecosystem.”&lt;/p&gt;
    &lt;head rend="h2"&gt;A new ecosystem&lt;/head&gt;
    &lt;p&gt;Consider the ambitions of Vast. The Long Beach startup wants to build a replacement for the most expensive object created by humanity: the $150 billion International Space Station, which is being retired in 2030 after three decades in orbit.&lt;/p&gt;
    &lt;p&gt;The football field-sized vehicle was the product of 15 nations, while the privately funded Vast is spending a fraction of that amount to launch its first small station next year, which it hopes will convince NASA to choose it over Blue Origin and other fledgling competitors to replace the ISS.&lt;/p&gt;
    &lt;p&gt;It might have a leg up with its SpaceX connection. Its Haven-1 space station is designed to fit in the nose cone of a Falcon 9, while the astronauts would be ferried up later in a SpaceX Dragon capsule.&lt;/p&gt;
    &lt;p&gt;Still, the startup has a long way to go.&lt;/p&gt;
    &lt;p&gt;“To become a proven space station company, we need to build one, launch one, and we need to have a crew visit it and come back home safely,” says Max Haot, an aerospace entrepreneur named chief executive in 2023 by founder Jed McCaleb, who made his fortune in cryptocurrency and is spending at least $1 billion on the venture.&lt;/p&gt;
    &lt;p&gt;After years of delay that have put it behind SpaceX, Boeing is set to launch its new Starliner capsule with two crew members to the International Space Station.&lt;/p&gt;
    &lt;p&gt;The company started in El Segundo in 2021 with just a few dozen employees, many of whom worked at SpaceX. It now has close to 1,000 and a recently expanded 189,000-square-foot headquarters complex — in the same Long Beach neighborhood where Boeing built the C-17 Globemaster cargo jet before that line was shut in 2015.&lt;/p&gt;
    &lt;p&gt;Faced with such a huge loss, Long Beach rezoned the neighborhood to attract advanced manufacturers such as aerospace companies, which now total about 30 across the city, including headquarters and major operations.&lt;/p&gt;
    &lt;p&gt;Among them is Relativity Space, a startup founded the same year the C-17 line shut down, that has developed 3-D printed rocket engines to further cut launch costs. The potential attracted billionaire Eric Schmidt, the former Google chief executive, who this year took over the business.&lt;/p&gt;
    &lt;p&gt;“At the end of the day, companies want to be around other companies where employees can bump into each other at the coffee shop,” said Long Beach Mayor Rex Richardson. “We’ve prioritized bringing these companies because they bring higher-quality jobs.”&lt;/p&gt;
    &lt;p&gt;Last week, in Vast’s manufacturing building, a machine tool was precisely cutting flat aluminum pieces for the shell of the spacecraft, forming a honeycomb pattern that provides strength but at a lower weight. At other stations, avionics and life support systems were being tested.&lt;/p&gt;
    &lt;p&gt;“If you look at it, it’s very simple; not only is the talent all here ... but we rely on a lot of partners to manufacture and process parts. There’s just an incredible amount of local facilities and skills that you need to build space systems,” Haot said of the area.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing pharmaceuticals in space&lt;/head&gt;
    &lt;p&gt;Vast’s first headquarters was in El Segundo, and that’s not just happenstance. The city has long been a favorite for aerospace companies and it’s where SpaceX opened its original headquarters on East Grand Avenue — before it moved when it needed more space.&lt;/p&gt;
    &lt;p&gt;Will Bruey, a former SpaceX avionics engineer, was recruited by a Bay Area venture capitalist in 2020 to co-found a microgravity pharmaceutical developer called Varda Space Industries. He had a choice of where to set up shop — and the decision was easy.&lt;/p&gt;
    &lt;p&gt;SpaceX wants to launch nearly 100 rockets into space off Vandenberg Space Force Base, despite complaints and objects from local officials of rattling sonic booms and unknown impacts to the state’s wildlife.&lt;/p&gt;
    &lt;p&gt;Bruey, 36, wrote a computer program to scrape recruiting websites such as LinkedIn and found the city had more of the aerospace, electrical, structural and mechanical engineers that he needed than any other place. “El Segundo was at the peak of the heat map — all the engineering you need,” said the chief executive.&lt;/p&gt;
    &lt;p&gt;Varda now employs more than 140 and has attracted $329 million in venture capital. It’s on its fourth mission and has become the first startup with Federal Aviation Administration approval to return commercial payloads from space.&lt;/p&gt;
    &lt;p&gt;The company makes automated labs the size of a cylindrical desktop speaker that it sends up to orbit in capsules and satellites it also builds. There, in microgravity, the miniature labs grow molecular crystals that are more pure than those produced in Earth’s gravity.&lt;/p&gt;
    &lt;p&gt;The concept was proven in 2019 by Merck in an experiment done aboard the International Space Station, and Varda partners with pharmaceutical companies to grow crystals for drugs to combat cancer and HIV. A purer drug formulation, for example, could allow an IV-based infusion to be replaced by a small injection.&lt;/p&gt;
    &lt;p&gt;“What we do is we take molecules that are known to be effective in the human body, and we can create new formulations that otherwise would be impossible,” Bruey said.&lt;/p&gt;
    &lt;p&gt;The fast-growing company operates out of a 61,000-square-foot building not far from the Los Angeles Air Force Base, which develops, tests and maintains military satellite constellations. It also recently signed a lease for 10,000 more square feet nearby for lab space focusing on its drug development.&lt;/p&gt;
    &lt;p&gt;The metamorphosis of the city into a haven for aerospace startups is exemplified by the transformation of the Smoky Hollow neighborhood — so named because it once was notorious for the gas clouds that settled in the area from the adjacent Chevron refinery.&lt;/p&gt;
    &lt;p&gt;Once populated by small shops that serviced the refinery, today it’s cleaned up and the home of aerospace and advanced manufacturers — helped along by fiber-optic cable the city had telecommunications providers install.&lt;/p&gt;
    &lt;p&gt;“That is one of those magical points in time that really flipped the switch on that area for an innovation economy enabled by technology,” said Mayor Chris Pimentel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mining minerals from asteroids&lt;/head&gt;
    &lt;p&gt;If Varda’s goals seem ambitious, other fledgling companies have plans that seem even quixotic.&lt;/p&gt;
    &lt;p&gt;The cosmos is riddled with evidence that the universe began with an exotic, unfathomably rapid expansion, but scientists don’t know why it happened. NASA JPL’s SPHEREx mission will seek answers to the mystery of cosmic ‘inflation.’&lt;/p&gt;
    &lt;p&gt;One of them is AstroForge, a three-year-old Seal Beach firm whose ambitions aren’t even captured by the phrase “moon shot.” It’s aiming to mine metals from a type of asteroid with high concentrations of platinum.&lt;/p&gt;
    &lt;p&gt;Co-founder and chief executive Matt Gialich knows the idea sounds fantastic but said SpaceX faced the same skepticism.&lt;/p&gt;
    &lt;p&gt;“Whenever you see a technological shift that has not been done before, it’s always going to seem like science fiction,” said the 39-year-old former Virgin Galactic avionics engineer.&lt;/p&gt;
    &lt;p&gt;NASA actually has already landed a spacecraft called OSIRIS-REx on an asteroid and dropped off a small sample of material back to Earth two years ago. But that scientific mission cost taxpayers some $1.2 billion, while AstroForge has raised just $60 million and prior asteroid mining startups such as Planetary Resources never got off the ground.&lt;/p&gt;
    &lt;p&gt;AstroForge’s last mission to take high-resolution asteroid images failed when it lost contact with the probe, but Gialich is pushing ahead with a more challenging mission next year to complete a landing.&lt;/p&gt;
    &lt;p&gt;“If we depart from Falcon and we never turn on and never make contact with the spacecraft, I think we go bankrupt as a company, and we probably deserve to,” he said. “If we make it all the way to the asteroid and it’s the wrong type of asteroid, I think we’ll have no problem raising a huge amount of additional capital.”&lt;/p&gt;
    &lt;head rend="h3"&gt;More to Read&lt;/head&gt;
    &lt;p&gt;Inside the business of entertainment&lt;/p&gt;
    &lt;p&gt;The Wide Shot brings you news, analysis and insights on everything from streaming wars to production — and what it all means for the future.&lt;/p&gt;
    &lt;p&gt;You may occasionally receive promotional content from the Los Angeles Times.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.latimes.com/business/story/2025-08-28/how-the-new-space-economy-is-transforming-southern-california"/></entry><entry><id>https://news.ycombinator.com/item?id=45077353</id><title>Meta and Yandex Disclosure: Covert Web-to-App Tracking via Localhost on Android</title><updated>2025-08-30T20:34:34.327221+00:00</updated><content>&lt;doc fingerprint="28a3e21018a25bca"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Disclosure: Covert Web-to-App Tracking via Localhost on Android&lt;/head&gt;
    &lt;p&gt;We disclose a novel tracking method by Meta and Yandex potentially affecting billions of Android users. We found that native Android apps—including Facebook, Instagram, and several Yandex apps including Maps and Browser—silently listen on fixed local ports for tracking purposes.&lt;/p&gt;
    &lt;p&gt;These native Android apps receive browsers' metadata, cookies and commands from the Meta Pixel and Yandex Metrica scripts embedded on thousands of web sites. These JavaScripts load on users' mobile browsers and silently connect with native apps running on the same device through localhost sockets. As native apps access programatically device identifiers like the Android Advertising ID (AAID) or handle user identities as in the case of Meta apps, this method effectively allows these organizations to link mobile browsing sessions and web cookies to user identities, hence de-anonymizing users' visiting sites embedding their scripts.&lt;/p&gt;
    &lt;p&gt;This web-to-app ID sharing method bypasses typical privacy protections such as clearing cookies, Incognito Mode and Android's permission controls. Worse, it opens the door for potentially malicious apps eavesdropping on users’ web activity.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does this work?&lt;/head&gt;
    &lt;p&gt;While there are subtle differences in the way Meta and Yandex bridge web and mobile contexts and identifiers, both of them essentially misuse the unvetted access to localhost sockets. The Android OS allows any installed app with the INTERNET permission to open a listening socket on the loopback interface (127.0.0.1). Browsers running on the same device also access this interface without user consent or platform mediation. This allows JavaScript embedded on web pages to communicate with native Android apps and share identifiers and browsing habits, bridging ephemeral web identifiers to long-lived mobile app IDs using standard Web APIs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Meta/Facebook Pixel sharing _fbp cookie from web to Meta Android apps&lt;/head&gt;
    &lt;p&gt;The Meta (Facebook) Pixel JavaScript, when loaded in an Android mobile web browser, transmits the first-party _fbp cookie using WebRTC to UDP ports 12580–12585 to any app on the device that is listening on those ports. We found Meta-owned Android apps Facebook (including version 515.0.0.23.90) and Instagram (including version 382.0.0.43.84), available on the Google Play Store, listening on these port range.&lt;/p&gt;
    &lt;p&gt;The Meta Pixel uses a technique known as SDP Munging to insert the _fbp cookie contents to the SDP "ice-ufrag" field, resulting in a Binding Request STUN message sent to the loopback address as the following figure shows. This data flow cannot be observed using Chrome's regular debugging tools (such as DevTools).&lt;/p&gt;
    &lt;p&gt;The entire flow of the _fbp cookie from web to native and the server is as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The user opens the native Facebook or Instagram app, which eventually is sent to the background and creates a background service to listen for incoming traffic on a TCP port (12387 or 12388) and a UDP port (the first unoccupied port in 12580-12585). Users must be logged-in with their credentials on the apps.&lt;/item&gt;
      &lt;item&gt;The user opens their browser and visits a website integrating the Meta Pixel.&lt;/item&gt;
      &lt;item&gt;At this stage, websites may ask for consent depending on the website's and visitor's locations.&lt;/item&gt;
      &lt;item&gt;The Meta Pixel script sends the _fbp cookie to the native Instagram or Facebook app via WebRTC (STUN) SDP Munging.&lt;/item&gt;
      &lt;item&gt;The Meta Pixel script also sends the _fbp value in a request to https://www.facebook.com/tr along with other parameters such as page URL (dl), website and browser metadata, and the event type (ev) (e.g., PageView, AddToCart, Donate, Purchase).&lt;/item&gt;
      &lt;item&gt;The Facebook or Instagram apps receive the _fbp cookie from the Meta Pixel JavaScript running on the browser. The apps transmit _fbp as a GraphQL mutation to (https://graph[.]facebook[.]com/graphql) along with other persistent user identifiers, linking users' fbp ID (web visit) with their Facebook or Instagram account.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On or around May 17th, Meta Pixel added a new method to their script that sends the _fbp cookie using WebRTC TURN instead of STUN. The new TURN method avoids SDP Munging, which Chrome developers publicly announced to disable following our disclosure. As of June 2, 2025, we have not observed the Facebook or Instagram applications actively listening on these new ports.&lt;/p&gt;
    &lt;head rend="h4"&gt;About the _fbp cookie&lt;/head&gt;
    &lt;p&gt; According to Meta’s Cookies Policy, the _fbp cookie &lt;quote&gt;identifies browsers for the purposes of providing advertising and site analytics services and has a lifespan of 90 days&lt;/quote&gt;. The cookie is present on approximately 25% of the top million websites, making it the 3rd most common first-party cookie of the web, according to Web Almanac 2024. &lt;/p&gt;
    &lt;p&gt;A first-party cookie implies that it cannot be used to track users across websites, as it is set under the website’s domain. That means the same user has different _fbp cookies on different websites. However, the method we disclose allows the linking of the different _fbp cookies to the same user, which bypasses existing protections and runs counter to user expectations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Yandex using localhost communications since 2017&lt;/head&gt;
    &lt;p&gt;Yandex Metrica script initiates HTTP requests with long and opaque parameters to localhost through specific TCP ports: 29009, 29010, 30102, and 30103. Our investigation revealed that Yandex-owned applications—such as Yandex Maps and Yandex Navigator, Yandex Search, and Yandex Browser— actively listen on these ports. Furthermore, our analysis indicates that the domain yandexmetrica[.]com is resolving to the loopback address 127.0.0.1, and that the Yandex Metrica script transmits data via HTTPS to local ports 29010 and 30103. This design choice obfuscates the data exfiltration process, thereby complicating conventional detection mechanisms.&lt;/p&gt;
    &lt;p&gt;Yandex apps contact a Yandex domain (startup[.]mobile[.]yandex[.]net, or similar) to retrieve the list of ports to listen to. The endpoint returns a JSON containing the local ports (e.g., 30102, 29009) and a “first_delay_seconds” parameter which we believe is used to delay the initiation of the service. On one of our test devices, first_delay_seconds roughly corresponded to the number of seconds it took for the Yandex app to begin listening on local ports (~3 days).&lt;/p&gt;
    &lt;p&gt;After receiving the localhost HTTP requests from the Yandex Metrica script, the mobile app responds with a Base64-encoded binary payload embedding and bridging the Android Advertising ID (AAID) along other identifiers accesible from Java APIs like the Google's advertising ID and UUIDs, potentially Yandex-specific. As opposed to Meta's Pixel case, all this information is aggregated and uploaded together to the Yandex Metrica server (e.g., mc[.]yango[.]com) by the JavaScript code running on the web browser, rather than by the native app. In the case of Yandex, the native app acts as a proxy to collect native Android-specific identifiers and transfering them to the browser context through localhost sockets.&lt;/p&gt;
    &lt;p&gt;The entire flow of the Yandex communication from web to native and the server is as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The user opens one of the native Yandex apps, which eventually is sent to the background and creates a background service to listen for incoming traffic on two HTTP ports (29009 and 30102) and two HTTPS ports (29010 and 30103).&lt;/item&gt;
      &lt;item&gt;The user opens their browser and visits a website integrating the Yandex Metrica script.&lt;/item&gt;
      &lt;item&gt;The Yandex script sends a request to their servers to obtain obfuscated parameters.&lt;/item&gt;
      &lt;item&gt;These obfuscated parameters are send to the localhost via both HTTP and HTTPS. This happens to a url that either directly contains the IP address 127.0.0.1, or the the yandexmetrica[.]com domain, which resolves to 127.0.0.1.&lt;/item&gt;
      &lt;item&gt;The Yandex Metrica SDK in the app receives these parameters and responds to the Yandex Metrica script on the website with a 200 OK response containing encrypted Device IDs.&lt;/item&gt;
      &lt;item&gt;The Yandex Metrica script on the website receives these IDs and sends them to their servers alongside the obfuscated parameters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This table shows the Yandex owned apps we found listening on localhost ports. For each app, we also list their unique package name and the version used for testing.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Yandex app&lt;/cell&gt;
        &lt;cell role="head"&gt;Package name&lt;/cell&gt;
        &lt;cell role="head"&gt;Tested version&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Yandex Maps&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ru.yandex.yandexmaps&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;23.5.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Yandex Navigator&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ru.yandex.yandexnavi&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;23.5.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Yandex Browser&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;com.yandex.browser&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;25.4.1.100&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Yandex Search&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;com.yandex.searchapp&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;25.41&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Metro in Europe — Vienna&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ru.yandex.metro&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;3.7.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Yandex Go: Taxi Food&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ru.yandex.taxi&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;5.24.1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Additional risk: Browsing history leak&lt;/head&gt;
    &lt;p&gt;Using HTTP requests for web-to-native ID sharing (i.e. not WebRTC STUN or TURN) may expose users browsing history to third-parties. A malicious third-party Android application that also listens on the aforementioned ports can intercept the HTTP requests sent by the Yandex Metrica script and the first, now-unused, implementation of Meta’s communication channel by monitoring the Origin HTTP header.&lt;/p&gt;
    &lt;p&gt;We developed a proof-of-concept app to demonstrate the feasibility of this browsing history harvesting by a malicious third-party app. We found that browsers such as Chrome, Firefox and Edge are susceptible to this form of browsing history leakage in both default and private browsing modes. Brave browser was unaffected by this issue due to their blocklist and the blocking of requests to the localhost; and DuckDuckGo was only minimally affected due to missing domains in their blocklist.&lt;/p&gt;
    &lt;p&gt;While the possibility for other apps to listen to these ports exist, we have not observed any other app, not owned by Meta or Yandex, listening to these ports.&lt;/p&gt;
    &lt;p&gt;Due to Yandex using HTTP requests for its localhost communications, any app listening on the required ports can monitor the website a user visited with these tracking capabilities as demonstrated by the video above. We first open our proof of concept app, which listens to the ports used by Yandex, and send it to the background. Next, we visit five websites across different browsers. Afterwards, we can see the URLs of these five sites listed in the app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Affected Sites&lt;/head&gt;
    &lt;p&gt;According to BuiltWith, a website that tracks web technology adoption: Meta Pixel is embedded on over 5.8 million websites. Yandex Metrica, on the other hand, is present on close to 3 million websites. According to HTTP Archive, an open and public dataset that runs monthly crawls of ~16 million websites, Meta Pixel and Yandex Metrica are present on 2.4 million and 575,448 websites, respectively.&lt;/p&gt;
    &lt;p&gt;Top-100k homepage crawls: We performed two web crawls on the top 100k sites (based on CrUX rankings) from servers located in Frankfurt and in New York to measure how widespread the use of localhost sockets is across sites. The following table shows the number of sites found affected for each case. The first column for each region shows the number of sites embedding these trackers when accepting all cookie consent forms. The second column (labeled as "no consent") reports the number of sites actively attempting to perform localhost communications by default as soon as the user loads them on their browser, i.e., potentially without user consent.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Script name&lt;/cell&gt;
        &lt;cell role="head"&gt;US presence&lt;/cell&gt;
        &lt;cell role="head"&gt;US no consent&lt;/cell&gt;
        &lt;cell role="head"&gt;Europe presence&lt;/cell&gt;
        &lt;cell role="head"&gt;Europe no consent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Meta Pixel&lt;/cell&gt;
        &lt;cell&gt;17,223&lt;/cell&gt;
        &lt;cell&gt;13,468 (78.2%)&lt;/cell&gt;
        &lt;cell&gt;15,677&lt;/cell&gt;
        &lt;cell&gt;11,890 (75.8%)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Yandex Metrica&lt;/cell&gt;
        &lt;cell&gt;1,312&lt;/cell&gt;
        &lt;cell&gt;1,095 (83.5%)&lt;/cell&gt;
        &lt;cell&gt;1,260&lt;/cell&gt;
        &lt;cell&gt;1,064 (84.4%)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Sites with Meta Pixel&lt;/head&gt;
    &lt;p&gt;Sites where Facebook/Meta pixel script attempts to share the _fbp ID with their Android apps using WebRTC. The table shows the URL and CrUX rank of the site, and if they were found in the EU or US crawl.&lt;/p&gt;
    &lt;p&gt; URL: URL of the affected website. &lt;lb/&gt; Ranking: Ranking bin as determined by the CrUX ranking (1000, 5000, 10000, 50000 or 100000) &lt;lb/&gt; EU and US: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yes: ID sharing observed in this region.&lt;/item&gt;
      &lt;item&gt;No: ID sharing not observed in this region.&lt;/item&gt;
      &lt;item&gt;⚠️: Shares _fbp ID before any consent is given or the site does not have a consent form.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Sites with Yandex Metrica&lt;/head&gt;
    &lt;p&gt;Sites where Yandex script sends HTTP requests to localhost ports. The table shows the URL and CrUX rank of the site, and if we observed a request to localhost in the EU or US crawl.&lt;/p&gt;
    &lt;p&gt; URL: URL of the affected website. &lt;lb/&gt; Rank: CrUX Rank bin; indicates popularity (top 1000, 5000, 10000, 50000 or 100000) &lt;lb/&gt; EU and US: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yes: ID sharing observed in this region.&lt;/item&gt;
      &lt;item&gt;No: ID sharing not observed in this region.&lt;/item&gt;
      &lt;item&gt;⚠️: Shares ID before any consent is given or the site does not have a consent form.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We would like to note that this crawling campaign is not exhaustive nor complete. Its purpose is to assess the prevalence of these behaviors in a representative sample of web sites. Therefore, the absence of a website in this list does not necessarily imply that it does not integrate these tracking capabilities.&lt;/p&gt;
    &lt;p&gt;Crawling methodology: When visiting the site, our crawler waits for the page to load, after which it accepts all cookies on the site (if any). It then waits a further ten seconds, while collecting all HTTP requests, WebSocket frames and WebRTC function calls made during the visit. Analyzing this data, we found Meta’s pixel sending to localhost on 15,677 sites accessed from the EU and 17,223 sites accessed from the US. Yandex metrica was found on 1,260 and 1,312 sites accessed from the EU and US, respectively.&lt;/p&gt;
    &lt;p&gt;To assess whether these tracking practices occur without potential user consent, we performed a second crawling campaign without interacting with the cookie consent window (if any is prsented to the user). Even without actively giving consent to these sites (i.e., not accepting any cookies or no consent window), the Meta Pixel sends the fbp cookie to localhost on 11,890 and 13,468 sites accessed from the EU and US potentially without consent, respectively. In the case of Yandex, it triggers the localhost requests on 1,064 and 1,095 sites in the EU and US potentially without consent, respectively.&lt;/p&gt;
    &lt;head rend="h2"&gt;When did this start?&lt;/head&gt;
    &lt;p&gt;The following table shows the evolution of the methods used by Yandex and Meta over time, listing the date on which each method was first observed based on historical HTTP Archive data.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Start date (first seen)&lt;/cell&gt;
        &lt;cell role="head"&gt;End date (last seen)&lt;/cell&gt;
        &lt;cell role="head"&gt;Ports&lt;/cell&gt;
        &lt;cell role="head"&gt;Har files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;HTTP&lt;/cell&gt;
        &lt;cell&gt;Feb 2017&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;29009, 30102&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4, 5,&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;HTTPS&lt;/cell&gt;
        &lt;cell&gt;May 2018&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;29010, 30103&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Meta&lt;/cell&gt;
        &lt;cell&gt;HTTP&lt;/cell&gt;
        &lt;cell&gt;Sep 2024&lt;/cell&gt;
        &lt;cell&gt;Oct 2024*&lt;/cell&gt;
        &lt;cell&gt;12387&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4, 5,&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Websocket&lt;/cell&gt;
        &lt;cell&gt;Nov 2024&lt;/cell&gt;
        &lt;cell&gt;Jan 2025&lt;/cell&gt;
        &lt;cell&gt;12387&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4, 5,&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;WebRTC STUN (w/ SDP Munging)&lt;/cell&gt;
        &lt;cell&gt;Nov 2024&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;12580-12585&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4, 5,&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WebRTC TURN** (w/o SDP Munging)&lt;/cell&gt;
        &lt;cell&gt;May 2025&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;12586-12591&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;*: Meta Pixel script was last seen sending via HTTP in Oct 2024, but Facebook and Instagram apps still listen on this port today. They also listen on port 12388 for HTTP, but we have not found any script sending to 12388.&lt;/item&gt;
      &lt;item&gt;**: Meta Pixel script sends to these ports, but Meta apps do not listen on them (yet?). We speculate that this behavior could be due to slow/gradual app rollout.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Abuse Vectors&lt;/head&gt;
    &lt;p&gt;This novel tracking method exploits unrestricted access to localhost sockets on the Android platforms, including most Android browsers. As we show, these trackers perform this practice without user awareness, as current privacy controls (e.g., sandboxing approaches, mobile platform and browser permissions, web consent models, incognito modes, resetting mobile advertising IDs, or clearing cookies) are insufficient to control and mitigate it.&lt;/p&gt;
    &lt;p&gt;We note that localhost communications may be used for legitimate purposes such as web development. However, the research community has raised concerns about localhost sockets becoming a potential vector for data leakage and persistent tracking. To the best of our knowledge, however, no evidence of real-world abuse for persistent user tracking across platforms has been reported until our disclosure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;Our responsible disclosure to major Android browser vendors led to several patches attempting to mitigate this issue; some already deployed, others currently in development. We thank all participating vendors (Chrome, Mozilla, DuckDuckGo, and Brave) for their active collaboration and constructive engagement throughout the process. Other Chromium-based browsers should follow upstream code changes to patch their own products.&lt;/p&gt;
    &lt;p&gt;However, beyond these short-term fixes, fully addressing the issue will require a broader set of measures as they are not covering the fundamental limitations of platforms' sandboxing methods and policies. These include user-facing controls to alert users about localhost access, stronger platform policies accompanied by consistent and strict enforcement actions to proactively prevent misuse, and enhanced security around Android’s interprocess communication (IPC) mechanisms, particularly those relying on localhost connections.&lt;/p&gt;
    &lt;p&gt;This table shows an overview of our browser tests.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Browser&lt;/cell&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;Yandex&lt;/cell&gt;
        &lt;cell role="head"&gt;Mitigations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Chrome&lt;/cell&gt;
        &lt;cell&gt;136.0.7103.125&lt;/cell&gt;
        &lt;cell&gt;Affected&lt;/cell&gt;
        &lt;cell&gt;Affected&lt;/cell&gt;
        &lt;cell&gt;Version 137, released on 26 May 2025, shipped countermeasures to block abused ports and disable the specific form of SDP munging that Meta Pixel used. As of 2 June 2025, these defenses are being trialed for a subset of Chrome users and will likely be released to general public soon. Our tests indicated these protections block the currently used forms of Meta and Yandex localhost communications. In the long term, the proposed Local Network Access standard may provide a more principle-based solution to limit this type of abuse.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Microsoft Edge&lt;/cell&gt;
        &lt;cell&gt;136.0.3240.50&lt;/cell&gt;
        &lt;cell&gt;Affected&lt;/cell&gt;
        &lt;cell&gt;Affected&lt;/cell&gt;
        &lt;cell&gt;(unknown)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Firefox&lt;/cell&gt;
        &lt;cell&gt;138.0.2&lt;/cell&gt;
        &lt;cell&gt;Affected&lt;/cell&gt;
        &lt;cell&gt;Not affected.1&lt;/cell&gt;
        &lt;cell&gt;Version 139 will include countermeasures to block the ports involved.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;DuckDuckGo&lt;/cell&gt;
        &lt;cell&gt;5.233.0&lt;/cell&gt;
        &lt;cell&gt;Minimally affected2, 3&lt;/cell&gt;
        &lt;cell&gt;Not affected3&lt;/cell&gt;
        &lt;cell&gt;Blocklist amended.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Brave&lt;/cell&gt;
        &lt;cell&gt;1.78.102&lt;/cell&gt;
        &lt;cell&gt;Not affected3&lt;/cell&gt;
        &lt;cell&gt;Not affected3, 4&lt;/cell&gt;
        &lt;cell&gt;Not affected. Localhost communications require user consent since 2022, and implements a blocklist.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;SDP Munging of ICE credentials is blocked, however UDP communications to TURN ports are not yet blocked (they will be blocked in v138). Meta apps that we tested do not listen on the TURN ports as of the time of release, but Meta Pixel scripts already sends to TURN ports.&lt;/item&gt;
      &lt;item&gt;Three alternate Yandex domains were missing from DuckDuckGo's blocklist, but the domains appeared on a very small number of websites (31/100K). DuckDuckGo quickly amended their blocklist to fix this blindspot.&lt;/item&gt;
      &lt;item&gt;Blocklist based protections.&lt;/item&gt;
      &lt;item&gt;Blocks localhost requests to "127.0.0.1" and "localhost".&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Are website owners aware?&lt;/head&gt;
    &lt;p&gt;We do not find any public documentation by Meta or Yandex’ official documentation describing this method and its purpose. For Meta Pixel, we found several complaints from puzzled website owners questioning why Meta Pixel communicates with localhost in Facebook developer forums by September 2024:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Facebook SDK config file making call to localhost&lt;/item&gt;
      &lt;item&gt;Why does my Pixel Javascript access http://localhost when in an embedded web view on Android&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; These complaints come from all around the world. No official response from Meta representatives is found on these threads. One developer stated in Sept. 2024: &lt;quote&gt;No acknowledgement has come from Meta at all on this though. My support request with them got a generic response and then ignored thereafter.&lt;/quote&gt;&lt;quote&gt;It's a shame accountability hasn't been taken, we rely on these tools to work properly and have no control over them so at the very least we should be given an explanation on what went wrong.&lt;/quote&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Are end-users aware?&lt;/head&gt;
    &lt;p&gt;It is plausible that users browsing the Internet and visiting sites integrating Yandex and Meta’s ID bridging between web and native apps, may not be fully aware of this behavior. In fact, the novel tracking method works even if the user:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Is not logged in to Facebook, Instagram or Yandex on their mobile browsers&lt;/item&gt;
      &lt;item&gt;Uses Incognito Mode&lt;/item&gt;
      &lt;item&gt;Clears their cookies or other browsing data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This tracking method defeats Android's inter-process isolation and tracking protections based on partitioning, sandboxing, or clearing client-side state.&lt;/p&gt;
    &lt;p&gt;Preliminary results suggest that these practices may be implemented in websites without explicit and appropriate cookie consent forms. If a site loads the Facebook or Yandex scripts before a user has given consent to the appropriate cookies, this behaviour will still be triggered.&lt;/p&gt;
    &lt;head rend="h2"&gt;☝️ Questions &amp;amp; Answers&lt;/head&gt;
    &lt;p&gt;Q: Why did Facebook stop using this technique on the day of your public release?&lt;/p&gt;
    &lt;p&gt;A: We don’t know ¯\_(ツ)_/¯, but we're happy to see that Android users are no longer affected by this type of abuse after our disclosure (for now).&lt;/p&gt;
    &lt;p&gt;Q: Is this research peer-reviewed?&lt;/p&gt;
    &lt;p&gt;A: Our findings were confirmed by certain parties that we disclosed to, but the study isn’t peer-reviewed yet. We chose not to delay public disclosure for the publication cycle due to the severity of the active abuse.&lt;/p&gt;
    &lt;p&gt;Q: Did Meta or Yandex disclose this method in their documentation?&lt;/p&gt;
    &lt;p&gt;A: We found no public technical documentation from Meta or Yandex describing this specific localhost-based communication technique. In Facebook's developer forums, some raised concerns about Pixel scripts contacting localhost, but they received no explanation.&lt;/p&gt;
    &lt;p&gt;Q: Does this only affect Android users? What about iOS or other platforms?&lt;/p&gt;
    &lt;p&gt;A: We have only obtained empirical evidence of this web-to-native ID bridging Meta and Yandex web scripts, which exclusively targeted mobile Android users. No evidence of abuse has been observed in iOS browsers and apps that we tested. That said, similar data sharing between iOS browsers and native apps is technically possible. iOS browsers, which are all based on WebKit, allow developers to programmatically establish localhost connections and apps can listen on local ports. It is possible that technical and policy restrictions for running native apps in the background may explain why iOS users were not targeted by these trackers. We note, however, that our iOS analysis is still preliminary and this behavior might have also violated PlayStore policies. Beyond mobile platforms, web-to-native ID bridging could also pose a threat on desktop OSes and smart TV platforms, but we have not yet investigated these platforms.&lt;/p&gt;
    &lt;head&gt;📁 Additional Resources&lt;/head&gt;
    &lt;p&gt;- Video showing Yandex sending localhost requests. The left window shows a remote debugging inspector window of the browser on the Android phone. The right window shows the screen of the Android phone, with our proof-of-concept app at the top and the browser at the bottom:&lt;/p&gt;
    &lt;p&gt;- Video showing Meta Pixel sending localhost STUN requests. The left window shows Wireshark, a program that monitors web traffic. The right window shows the browser visiting a website. Note that the requests only start sending once the page is loaded when emulating a Android phone (Pixel 7) and the breakpoint added to the Facebook script is passed:&lt;/p&gt;
    &lt;p&gt;- Image showing Meta Pixel parameters for which ports and protocols to contact localhost.&lt;/p&gt;
    &lt;p&gt;- Image showing Meta Pixel performing SDP Munging to insert the _fbp cookie value.&lt;/p&gt;
    &lt;p&gt;- Image showing Meta Pixel using STUN to _pass fbp cookie value to mobile apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://localmess.github.io?new"/></entry><entry><id>https://news.ycombinator.com/item?id=45077460</id><title>Sleeper AI agents and how Anthropic detects them [video]</title><updated>2025-08-30T20:34:33.438038+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=Z3WMt_ncgUI"/></entry></feed>