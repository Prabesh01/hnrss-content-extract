<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-04T07:32:14.466000+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45455164</id><title>Where it's at://</title><updated>2025-10-04T07:32:20.838472+00:00</updated><content>&lt;doc fingerprint="9e199d86dcdee5d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Where It's at://&lt;/head&gt;
    &lt;p&gt;October 2, 2025&lt;/p&gt;
    &lt;p&gt;You might have heard about the AT protocol (if not, read this!)&lt;/p&gt;
    &lt;p&gt;Together, all servers speaking the AT protocol comprise the atmosphere—a web of hyperlinked JSON. Each piece of JSON on the atmosphere has its own &lt;code&gt;at://&lt;/code&gt; URI:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;at://ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://danabra.mov/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://tessa.germnetwork.com/pub.leaflet.publication/3lzz6juivnc2d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But where do they point, exactly?&lt;/p&gt;
    &lt;p&gt;Given an &lt;code&gt;at://&lt;/code&gt; URI, how do you locate the corresponding JSON?&lt;/p&gt;
    &lt;p&gt;In this post, I’ll show you the exact process of resolving an &lt;code&gt;at://&lt;/code&gt; URI step by step. Turns out, this is also a great way to learn the details of how &lt;code&gt;at://&lt;/code&gt; works.&lt;/p&gt;
    &lt;p&gt;Let’s start with the structure of a URI itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;The User as the Authority&lt;/head&gt;
    &lt;p&gt;As you might know, a URI often contains a scheme (for example, &lt;code&gt;https://&lt;/code&gt;), an authority (like &lt;code&gt;wikipedia.com&lt;/code&gt;), a path (like &lt;code&gt;/Main_Page&lt;/code&gt;), and maybe a query.&lt;/p&gt;
    &lt;p&gt;In most protocols, including &lt;code&gt;https://&lt;/code&gt;, the authority part points at whoever’s hosting the data. Whoever created this data is either not present, or is in the path:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;at://&lt;/code&gt; protocol flips that around.&lt;/p&gt;
    &lt;p&gt;In &lt;code&gt;at://&lt;/code&gt; URIs, whoever created the data is the authority, in the most literal sense:&lt;/p&gt;
    &lt;p&gt;The user is the authority for their own data. Whoever’s hosting the data could change over time, and is not directly included in an &lt;code&gt;at://&lt;/code&gt; URI. To find out the actual physical server hosting that JSON, you’re gonna need to take a few steps.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Post in the Atmosphere&lt;/head&gt;
    &lt;p&gt;Let’s try to resolve this &lt;code&gt;at://&lt;/code&gt; URI to the piece of JSON it represents:&lt;/p&gt;
    &lt;p&gt;An easy way to resolve an &lt;code&gt;at://&lt;/code&gt; URI is to use an SDK or a client app. Let’s try an online client, for example, pdsls or Taproot or atproto-browser. They’ll figure out the physical server where its JSON is currently hosted, and show that JSON for you.&lt;/p&gt;
    &lt;p&gt;The above &lt;code&gt;at://&lt;/code&gt; URI points at this JSON, wherever it is currently being hosted:&lt;/p&gt;
    &lt;p&gt;You can guess by the &lt;code&gt;$type&lt;/code&gt; field being &lt;code&gt;"app.bsky.feed.post"&lt;/code&gt; that this is some kind of a post (which might explain why it has fields like &lt;code&gt;text&lt;/code&gt; and &lt;code&gt;langs&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;However, note that this piece of JSON represents a certain social media post itself, not a web page or a piece of some app. It’s pure data as a piece of JSON, not a piece of UI. You may think of the &lt;code&gt;$type&lt;/code&gt; stating the data format; the &lt;code&gt;app.bsky.*&lt;/code&gt; prefix tells us that the &lt;code&gt;bsky.app&lt;/code&gt; application might know something about what to do with it. Other applications may also consume and produce data in this format.&lt;/p&gt;
    &lt;p&gt;A careful reader might notice that the &lt;code&gt;uri&lt;/code&gt; in the JSON block is also an &lt;code&gt;at://&lt;/code&gt; URI but it’s slightly different from the original &lt;code&gt;at://&lt;/code&gt; URI we requested:&lt;/p&gt;
    &lt;p&gt;In particular, the short &lt;code&gt;ruuuuu.de&lt;/code&gt; authority has expanded into a longer &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt; authority. Maybe that’s the physical host?&lt;/p&gt;
    &lt;p&gt;Actually, no, that’s not the physical host either—it’s something called an identity. Turns out, resolving an &lt;code&gt;at://&lt;/code&gt; URI is done in three distinct steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Resolve the handle to an identity (“who are you?”)&lt;/item&gt;
      &lt;item&gt;Resolve that identity to a hosting (“who holds your data?”)&lt;/item&gt;
      &lt;item&gt;Request the JSON from that hosting (“what is the data?”)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s go through each of these steps and see how they work.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Handles to Identities&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;at://&lt;/code&gt; URIs you’ve seen earlier are fragile because they use handles.&lt;/p&gt;
    &lt;p&gt;Here, &lt;code&gt;ruuuuu.de&lt;/code&gt;, &lt;code&gt;danabra.mov&lt;/code&gt;, and &lt;code&gt;tessa.germnetwork.com&lt;/code&gt; are handles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;at://ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://danabra.mov/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://tessa.germnetwork.com/pub.leaflet.publication/3lzz6juivnc2d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(Read more about domains as “internet handles” here.)&lt;/p&gt;
    &lt;p&gt;The user may choose to change their &lt;code&gt;at://&lt;/code&gt; handle later, and it is important for that not to break any links between pieces of JSON already on the network.&lt;/p&gt;
    &lt;p&gt;This is why, before you store an &lt;code&gt;at://&lt;/code&gt; URI, you should turn it into a canonical form by resolving the handle to something that never changes—an identity. An identity is like an account ID, but global and meant for the entire web. There are two mechanisms to resolve a handle to an identity (also known as a “DID”):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query the DNS TXT record at &lt;code&gt;_atproto.&amp;lt;handle&amp;gt;&lt;/code&gt;looking for&lt;code&gt;did=???&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Make an HTTPS GET to &lt;code&gt;https://&amp;lt;handle&amp;gt;/.well-known/atproto-did&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The thing you’re looking for, the DID, is going to have a shape like &lt;code&gt;did:something:whatever&lt;/code&gt;. (We’ll revisit what that means later.)&lt;/p&gt;
    &lt;p&gt;For example, let’s try to resolve &lt;code&gt;ruuuuu.de&lt;/code&gt; via the DNS mechanism:&lt;/p&gt;
    &lt;p&gt;Found it!&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;ruuuuu.de&lt;/code&gt; handle claims to be owned by &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;, whoever that may be. That’s all that we wanted to know at this point:&lt;/p&gt;
    &lt;p&gt;Note this doesn’t prove their association yet. We’ll need to verify that whoever controls the &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt; identity “agrees” with &lt;code&gt;ruuuuu.de&lt;/code&gt; being their handle. The mapping is bidirectional. But we’ll confirm that in a later step.&lt;/p&gt;
    &lt;p&gt;Now let’s try to resolve &lt;code&gt;danabra.mov&lt;/code&gt; using the DNS route:&lt;/p&gt;
    &lt;p&gt;That also worked! The &lt;code&gt;danabra.mov&lt;/code&gt; handle claims to be owned by the &lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt; identity, whoever that may be:&lt;/p&gt;
    &lt;p&gt;This DID looks a bit different than what you saw earlier but it’s also a valid DID. Again, it’s important to emphasize we’ve not confirmed the association yet.&lt;/p&gt;
    &lt;p&gt;Subdomains like &lt;code&gt;barackobama.bsky.social&lt;/code&gt; can also be handles.&lt;/p&gt;
    &lt;p&gt;Let’s try to resolve it:&lt;/p&gt;
    &lt;p&gt;The DNS mechanism didn’t work, so let’s try with HTTPS:&lt;/p&gt;
    &lt;p&gt;That worked! This means that &lt;code&gt;barackobama.bsky.social&lt;/code&gt; handle claims to be owned by the &lt;code&gt;did:plc:5c6cw3veuqruljoy5ahzerfx&lt;/code&gt; identity, whoever that is:&lt;/p&gt;
    &lt;p&gt;So you get the idea. When you see a handle, you can probe it with DNS and HTTPS to see if it claims to be owned by some identity (a DID). If you found a DID, you’ll then be able to (1) verify it actually owns that handle, and (2) locate the server that hosts the data for that DID. And that will be the server you’ll ask for the JSON.&lt;/p&gt;
    &lt;p&gt;In practice, if you’re building with AT, you’ll likely want to either deploy your own handle/did resolution cache or hit an existing one. (Here’s one implementation.)&lt;/p&gt;
    &lt;head rend="h3"&gt;AT Permalinks&lt;/head&gt;
    &lt;p&gt;Now you know how handles resolve to identities, also known as DIDs. Unlike handles, which change over time, DIDs never change—they’re immutable.&lt;/p&gt;
    &lt;p&gt;These &lt;code&gt;at://&lt;/code&gt; links, which use handles, are human-readable but fragile:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;at://ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://danabra.mov/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://tessa.germnetwork.com/pub.leaflet.publication/3lzz6juivnc2d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They will break if one of us changes a handle again.&lt;/p&gt;
    &lt;p&gt;In contrast, the &lt;code&gt;at://&lt;/code&gt; links below, which use DIDs, will not break until we either delete our accounts, delete these records, or permanently take down our hosting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;at://did:web:iam.ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://did:plc:fpruhuo22xkm5o7ttr2ktxdo/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://did:plc:ad4m72ykh2evfdqen3qowxmg/pub.leaflet.publication/3lzz6juivnc2d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, really, this is the “true form” of an &lt;code&gt;at://&lt;/code&gt; URI:&lt;/p&gt;
    &lt;p&gt;Think of &lt;code&gt;at://&lt;/code&gt; links with DIDs as “permalinks”. Any application storing &lt;code&gt;at://&lt;/code&gt; URIs should store them in this canonical form so that logical links between our pieces of JSON don’t break when we change our handles or change our hosting.&lt;/p&gt;
    &lt;p&gt;Now that you know how to resolve a handle to a DID, you want to do two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that whoever owns this DID actually goes by that handle.&lt;/item&gt;
      &lt;item&gt;Find the server that hosts all the data for this DID.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can do both of these things by fetching a piece of JSON called the DID Document. You can think of it as sort of a “passport” for a given DID.&lt;/p&gt;
    &lt;p&gt;How you do that depends on what kind of DID it is.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Identities to Hosting&lt;/head&gt;
    &lt;p&gt;Currently, there are two kinds of DIDs, known as DID methods, supported by the AT protocol: &lt;code&gt;did:web&lt;/code&gt; (a W3C draft) and &lt;code&gt;did:plc&lt;/code&gt; (specified by Bluesky).&lt;/p&gt;
    &lt;p&gt;Let’s compare them.&lt;/p&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;did:web&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;ruuuuu.de&lt;/code&gt; handle claims to be owned by &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;To check this claim, let’s find the DID Document for &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;. The &lt;code&gt;did:web&lt;/code&gt; method is a specification that specifies an algorithm for that.&lt;/p&gt;
    &lt;p&gt;In short, you cut off the &lt;code&gt;did:web:&lt;/code&gt; from the DID, append &lt;code&gt;/.well-known/did.json&lt;/code&gt; to the end, and run an HTTPS GET request:&lt;/p&gt;
    &lt;p&gt;This DID Document looks sleep-inducing but it tells us three important things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to refer to them. The &lt;code&gt;alsoKnownAs&lt;/code&gt;field confirms that whoever controls&lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;indeed wants to use&lt;code&gt;@ruuuuu.de&lt;/code&gt;as a handle. ✅&lt;/item&gt;
      &lt;item&gt;How to verify the integrity of their data. The &lt;code&gt;publicKeyMultibase&lt;/code&gt;field tells us the public key with which all changes to their data are signed.&lt;/item&gt;
      &lt;item&gt;Where their data is stored. The &lt;code&gt;serviceEndpoint&lt;/code&gt;field tells us the actual server with their data. Rudy’s data is currently hosted at&lt;code&gt;https://blacksky.app&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A DID Document really is like an internet passport for an identity: here’s their handle, here’s their signature, and here’s their location. It connects a handle to a hosting while letting the identity owner change either the handle or the hosting.&lt;/p&gt;
    &lt;p&gt;Users who interact with &lt;code&gt;@ruuuuu.de&lt;/code&gt; on different apps in the atmosphere don’t need to know or care about his DID or about his current hosting (and whether it moves). From their perspective, his current handle is the only relevant identifier. As for developers, they’ll refer to him by DID, which conveniently never changes.&lt;/p&gt;
    &lt;p&gt;All of this sounds great, but there is one big downside to the &lt;code&gt;did:web&lt;/code&gt; identity. If &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt; ever loses control of the &lt;code&gt;iam.ruuuuu.de&lt;/code&gt; domain, he will lose control over his DID Document, and thus over his entire identity.&lt;/p&gt;
    &lt;p&gt;Let’s have a look at an alternative to &lt;code&gt;did:web&lt;/code&gt; that avoids this problem.&lt;/p&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;did:plc&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;We already know the &lt;code&gt;danabra.mov&lt;/code&gt; handle claims to be owned by the &lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt; identity (actually, that’s me!)&lt;/p&gt;
    &lt;p&gt;To check this claim, let’s find the DID Document for &lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;did:plc&lt;/code&gt; method is a specification that specifies an algorithm for that.&lt;/p&gt;
    &lt;p&gt;Essentially, you need to hit the &lt;code&gt;https://plc.directory&lt;/code&gt; service with a &lt;code&gt;GET&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;The DID Document itself works exactly the same way. It specifies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to refer to me. The &lt;code&gt;alsoKnownAs&lt;/code&gt;field confirms that whoever controls&lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt;uses&lt;code&gt;@danabra.mov&lt;/code&gt;as a handle. ✅&lt;/item&gt;
      &lt;item&gt;How to verify the integrity of my data. The &lt;code&gt;publicKeyMultibase&lt;/code&gt;field tells us the public key with which all changes to my data are signed.&lt;/item&gt;
      &lt;item&gt;Where my data is stored. The &lt;code&gt;serviceEndpoint&lt;/code&gt;field tells us the actual server with my data. It’s currently at&lt;code&gt;https://morel.us-east.host.bsky.network&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s visualize this:&lt;/p&gt;
    &lt;p&gt;Although my handle is &lt;code&gt;@danabra.mov&lt;/code&gt;, the actual server storing my data is currently &lt;code&gt;https://morel.us-east.host.bsky.network&lt;/code&gt;. I’m happy to keep hosting it there but I’m thinking of moving it to a host I control in the future. I can change both my handle and my hosting without disruption to my social apps.&lt;/p&gt;
    &lt;p&gt;Unlike Rudy, who has a &lt;code&gt;did:web&lt;/code&gt; identity, I stuck with &lt;code&gt;did:plc&lt;/code&gt; (which is the default one when you create an account on Bluesky) so that I’m not irrecovably tying myself to any web domain. “PLC” officially stands for a “Public Ledger of Credentials”—essentially, it is like an npm registry but for DID Documents. (Fun fact: originally PLC meant “placeholder” but they’ve decided it’s a good tradeoff.)&lt;/p&gt;
    &lt;p&gt;The upside of a &lt;code&gt;did:plc&lt;/code&gt; identity is that I can’t lose my identity if I forget to renew a domain, or if something bad happens at the top level to my TLD.&lt;/p&gt;
    &lt;p&gt;The downside of a &lt;code&gt;did:plc&lt;/code&gt; identity is that whoever operates the PLC registry has some degree of control over my identity. They can’t outright change it because every version is recursively signed with the hash of the previous version, every past version is queryable, and the hash of the initial version is the DID itself.&lt;/p&gt;
    &lt;p&gt;However, in theory, whoever operates the PLC registry could deny my requests to update the DID Document, or refuse to serve some information about it. Bluesky is currently moving PLC to an independent legal entity in Switzerland to address some of these concerns. The AT community is also thinking and experimenting.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Hosting to JSON&lt;/head&gt;
    &lt;p&gt;So far, you’ve learned how to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Resolve a handle to a DID.&lt;/item&gt;
      &lt;item&gt;Grab the DID Document for that DID.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That actually tells you enough to get the JSON by its &lt;code&gt;at://&lt;/code&gt; URI!&lt;/p&gt;
    &lt;p&gt;Each DID Document includes the &lt;code&gt;serviceEndpoint&lt;/code&gt; which is the actual hosting. That’s the service you can hit by HTTPS to grab any JSON record it stores.&lt;/p&gt;
    &lt;p&gt;For example, the &lt;code&gt;@ruuuuu.de&lt;/code&gt; handle resolves to &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;, and its DID Document has a &lt;code&gt;serviceEndpoint&lt;/code&gt; pointing at &lt;code&gt;https://blacksky.app&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To get the &lt;code&gt;at://ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt; record, hit the &lt;code&gt;https://blacksky.app&lt;/code&gt; server with the &lt;code&gt;com.atproto.repo.getRecord&lt;/code&gt; endpoint, passing different parts of the &lt;code&gt;at://&lt;/code&gt; URI as parameters:&lt;/p&gt;
    &lt;p&gt;And there it is:&lt;/p&gt;
    &lt;p&gt;Now let’s get &lt;code&gt;at://danabra.mov/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;@danabra.mov&lt;/code&gt;handle resolves to&lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The DID Document for &lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt;points at&lt;code&gt;https://morel.us-east.host.bsky.network&lt;/code&gt;as the current hosting.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s hit it:&lt;/p&gt;
    &lt;p&gt;And there you have it:&lt;/p&gt;
    &lt;p&gt;And that’s how you resolve an &lt;code&gt;at://&lt;/code&gt; URI.&lt;/p&gt;
    &lt;p&gt;Exercise: In the record above, the &lt;code&gt;subject&lt;/code&gt; is a link to another record. Figure out the handle of its owner and the contents of that record. Use pdsls to check your answer.&lt;/p&gt;
    &lt;head rend="h3"&gt;In Conclusion&lt;/head&gt;
    &lt;p&gt;To resolve an arbitrary &lt;code&gt;at://&lt;/code&gt; URI, you need to follow three steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Resolve the handle to an identity (using DNS and/or HTTPS).&lt;/item&gt;
      &lt;item&gt;Resolve that identity to a hosting (using the DID Document).&lt;/item&gt;
      &lt;item&gt;Request the JSON from that hosting (by hitting it with &lt;code&gt;getRecord&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re building a client app or a small project, an SDK will handle all of this for you. However, for good performance, you’ll want to hit a resolution cache instead of doing DNS/HTTPS lookups on every request. QuickDID is one such cache. You can also check out the pdsls source to see how exactly it handles resolution.&lt;/p&gt;
    &lt;p&gt;In practice, a lot of apps don’t end up needing to resolve &lt;code&gt;at://&lt;/code&gt; URIs or load JSON records because they receive data from the network via a websocket and aggregate it in a local database. If that’s your approach, you’ll still use the &lt;code&gt;at://&lt;/code&gt; URIs as unique identifiers for user-created data, but the data itself will get pushed to you rather than pulled by you. Still, it’s useful to know that you can fetch it on demand.&lt;/p&gt;
    &lt;p&gt;The AT protocol is fundamentally an abstraction over HTTP, DNS, and JSON. But by standardizing how these pieces fit together—putting the user in the authority position, separating identity from hosting, and making data portable—it turns the web into a place where your content belongs to you, not to the apps that display it.&lt;/p&gt;
    &lt;p&gt;There’s more to explore in the atmosphere, but now you know where it’s &lt;code&gt;at://&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://overreacted.io/where-its-at/"/><published>2025-10-02T20:31:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45463319</id><title>I turned the Lego Game Boy into a working Game Boy</title><updated>2025-10-04T07:32:20.602880+00:00</updated><content>&lt;doc fingerprint="15cf6c54ebc4b5f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I turned the Lego Game Boy into a working Game Boy part. 1&lt;/head&gt;
    &lt;p&gt;Through my documentation of Game Boy boards, I have drawn up schematics of each device. I know them pretty well. Check out my board scan wiki https://wiki.nataliethenerd.com/&lt;/p&gt;
    &lt;p&gt;I jokingly made this tweet when the kit was announced, but decided to actually do it.&lt;/p&gt;
    &lt;p&gt;I know from experience of routing Game Boy CPU PCBs that there isn't much to it. There's the RAM, CPU, some decoupling capacitors and power regulation. &lt;lb/&gt;Note: I went with the MGB (Pocket) CPU rather than DMG for a couple of reasons.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They are pretty much the same&lt;/item&gt;
      &lt;item&gt;I have more of them&lt;/item&gt;
      &lt;item&gt;They are cheaper and easier to get. This opens up the project to more people&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The DMG CPU has external VRAM, the MGB CPU has internal VRAM and in a very space conscious build that was the biggest factor.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pre Planning&lt;/head&gt;
    &lt;p&gt;I only had the press pictures to work off. I used the dimensions to scale the image on my PC and from that I got measurements for the screen inserts; since that's where I plan to put the Game Boy.&lt;/p&gt;
    &lt;p&gt;I incorporated the power circuit I use for my Safer Charger boards, changed the power switch to a soft latching power button, added pin outs for the button matrix and audio.&lt;/p&gt;
    &lt;p&gt;I didn't really know what the buttons on the Lego would be like, but the fact that they could be pressed was enough for me to know I could implement them. At the moment I have them wired up to custom 3D printed *toy brick* parts. Same with the USB C&lt;/p&gt;
    &lt;p&gt;I am currently working on refining the board now I have the Lego build in my hands. This project will be released in full once I am finished with it - so stay tuned!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.nataliethenerd.com/i-turned-the-lego-game-boy-into-a-working-game-boy-part-1/"/><published>2025-10-03T14:18:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464145</id><title>Lessons learned from building an infrastructure devtool</title><updated>2025-10-04T07:32:20.464529+00:00</updated><content>&lt;doc fingerprint="a4f68039cd4e68fb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Lessons learned from building an infrastructure devtool&lt;/head&gt;
    &lt;p&gt;Unexpected lessons learned from three years of building Nango&lt;/p&gt;
    &lt;p&gt;Three years ago, we started Nango: Developer infrastructure for product integrations.&lt;/p&gt;
    &lt;p&gt;Today, it powers integrations with 500+ APIs for hundreds of SaaS and AI companies.&lt;/p&gt;
    &lt;p&gt;Our journey to get here was anything but linear.&lt;/p&gt;
    &lt;p&gt;For this post we gathered some of our most unexpected lessons learned. We hope they help other teams who are also building developer infrastructure products.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Great abstractions are the key to great infrastructure products&lt;/head&gt;
    &lt;p&gt;As developer infrastructure, our abstractions are our product.&lt;/p&gt;
    &lt;p&gt;For instance, the interface we choose to let your app authenticate with 500+ APIs defines what we handle, the available configuration options, and how you can integrate this feature in your own application. Different abstractions lead to very different experiences.&lt;/p&gt;
    &lt;p&gt;For Nango, our abstractions must be powerful enough to solve a real problem end-to-end, yet flexible enough to accommodate diverse use cases, external APIs, and tech stacks.&lt;/p&gt;
    &lt;p&gt;In our experience, when you get the abstractions right, everything else falls into place: Tools fit like a glove, APIs make intuitive sense, and configuration options cover exactly what customers want to tune.&lt;/p&gt;
    &lt;p&gt;Building great abstractions has an element of craft, and you can see it in the tools engineers love: Linear for project management, Vercel for running apps, Supabase for full-stack products, Knock for notifications, Resend for emails, and Infisical for secrets management.&lt;/p&gt;
    &lt;p&gt;I don't think this is a coincidence. Deep domain expertise and best practices help create abstractions that truly fit. But the final touches, such as great API design, intuitive property names, and just the right amount of configuration, are more art than science.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. To build great abstractions, you need deep expertise&lt;/head&gt;
    &lt;p&gt;Infrastructure is the foundation others build on. If you're not truly an expert in your space, users feel it immediately: abstractions are off, "obvious" features are missing, and support lacks depth.&lt;/p&gt;
    &lt;p&gt;When we started Nango, we had already built dozens of integrations for previous products. We believed we were experts, but in reality, we were probably only at an intermediate level. It took thousands of hours working in production across many different use cases for us to become true experts.&lt;/p&gt;
    &lt;p&gt;We were fortunate that customers asked us to handle more of their integration challenges. We formed an "integrations engineering" team and built hundreds of integrations for them. Although this approach was difficult to scale, it gave us direct experience with what it really takes to build great integrations. We shared what we learned from this period here.&lt;/p&gt;
    &lt;p&gt;Without this intense dogfooding, we might never have learned the deep insights needed to build truly helpful integrations infrastructure.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. For some problems, lower level is better&lt;/head&gt;
    &lt;p&gt;We started with a high-level solution: "Outsource the problem of integrations to us, and we'll solve it end-to-end."&lt;/p&gt;
    &lt;p&gt;But while developing hundreds of integrations for customers, we realized this approach was flawed. Integrations are key product features. You can't bolt them on and magically sync whatever customers need with any external API. For integrations to succeed, each interaction must solve a clear end-user problem and fit with existing product features.&lt;/p&gt;
    &lt;p&gt;The more we abstracted integrations, the less control customers had, and the lower their success rate. For integrations, we learned that lower level is better, even if it means we can't solve as much of the problem for them.&lt;/p&gt;
    &lt;p&gt;At first, this was counterintuitive. But I think it applies to many infrastructure devtools: for some problems, control is more important than an end-to-end solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Getting the abstractions right matters more than speed&lt;/head&gt;
    &lt;p&gt;Common wisdom is to ship as fast as you can to learn in production. This assumes change is cheap, which is true for many products, but not for developer infrastructure that customers rely on for production traffic.&lt;/p&gt;
    &lt;p&gt;In our experience, taking 50% more time to get the abstractions right has paid huge dividends in speed down the line:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Less rework of previous features = fewer breaking changes&lt;/item&gt;
      &lt;item&gt;Good abstractions = solid foundations for future features to build on top&lt;/item&gt;
      &lt;item&gt;Good abstractions = better intuitive understanding by customers = less support overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;5. The path to product-market fit is not linear&lt;/head&gt;
    &lt;p&gt;Startup journeys are often presented as linear paths from start to success. But most products aren't born into PMF. They go through turbulent iteration periods where progress is more spurious than linear.&lt;/p&gt;
    &lt;p&gt;Nango was no exception. We went through at least 5 major iterations to reach our current product.&lt;/p&gt;
    &lt;p&gt;Each iteration led to acceleration as we unlocked deeper understanding, a wider customer base, and an improved product. But after a while, we'd hit slowdowns as if we'd hit an invisible tarpit. Or even a wall.&lt;/p&gt;
    &lt;p&gt;We learned not to get attached to any product iteration, but to stay focused on the problem. Which parts resonated with customers? What caused friction? Is there an approach we haven't tried?&lt;/p&gt;
    &lt;p&gt;It's hard to know if some iterations could have been avoided. In hindsight, breaking points seem obvious, and it feels like we could have been faster to detect them.&lt;/p&gt;
    &lt;p&gt;But ultimately, the only thing that mattered was not giving up. Through our iterations, we kept accruing deeper expertise about how integrations truly work across hundreds of B2B software products.&lt;/p&gt;
    &lt;p&gt;I think these insights were crucial for us to build the product we have today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;When we started Nango, none of the Nango founders had ever built an infrastructure devtool. Every blog post and lesson learned from others was helpful for us.&lt;/p&gt;
    &lt;p&gt;Luckily, the infrastructure devtools community is very open about sharing learnings, wins and failures. I hope this post helps others learn from our own journey.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nango.dev/blog/lessons-learned-building-infrastructure-devtool"/><published>2025-10-03T15:33:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464429</id><title>Jeff Bezos says AI is in a bubble but society will get 'gigantic' benefits</title><updated>2025-10-04T07:32:20.298814+00:00</updated><content>&lt;doc fingerprint="49cf831b4a053dfc"&gt;
  &lt;main&gt;
    &lt;p&gt;TURIN, Italy — Artificial intelligence is currently in an "industrial bubble" but the technology is "real" and will bring big benefits to society, Amazon Founder Jeff Bezos said on Friday.&lt;/p&gt;
    &lt;p&gt;The term bubble usually refers to a period of inflated stock prices or valuations of companies that have disconnected from the fundamentals of a business. One of the most famous bubbles that burst was the 2000 dotcom crash where the value of internet companies plummeted.&lt;/p&gt;
    &lt;p&gt;Exor CEO John Elkann asked Bezos on stage at Italian Tech Week in Turin, Italy whether there were signs that the current AI industry is in bubble.&lt;/p&gt;
    &lt;p&gt;"This is a kind of industrial bubble," the Amazon founder said.&lt;/p&gt;
    &lt;p&gt;Bezos laid out some of the key characteristics of bubbles, noting that when they happen, stock prices are "disconnected from the fundamentals" of a business.&lt;/p&gt;
    &lt;p&gt;"The second thing that happens is that people get very excited like they are today about artificial intelligence," Bezos added.&lt;/p&gt;
    &lt;p&gt;During bubbles, every experiment or idea gets funded, he told the audience.&lt;/p&gt;
    &lt;p&gt;"The good ideas and the bad ideas. And investors have a hard time in the middle of this excitement, distinguishing between the good ideas and the bad ideas. And that's also probably happening today," Bezos said.&lt;/p&gt;
    &lt;p&gt;"But that doesn't mean anything that is happening isn't real. AI is real, and it is going to change every industry."&lt;/p&gt;
    &lt;p&gt;Bezos gave the example of a six-person company receiving billions of dollars of funding. This is "very unusual behavior" and yet this kind of activity is happening today, he said without making it clear what company he was referring to.&lt;/p&gt;
    &lt;p&gt;The billionaire however, said that ultimately industrial bubbles could be positive. He pointed to the biotech and pharmaceutical company bubble in the 1990s, which led to the development of some life-saving drugs even though many companies eventually went bust.&lt;/p&gt;
    &lt;p&gt;"The [bubbles] that are industrial are not nearly as bad, it can even be good, because when the dust settles and you see who are the winners, societies benefits from those inventions," Bezos said.&lt;/p&gt;
    &lt;p&gt;"That is what is going to happen here too. This is real, the benefits to society from AI are going to be gigantic."&lt;/p&gt;
    &lt;head rend="h2"&gt;AI bubble warnings grow&lt;/head&gt;
    &lt;p&gt;Bezos is not the only major business figure warning about an AI bubble. In August, OpenAI CEO Sam Altman reportedly said the AI market is in a bubble, and many investors have also raised the issue.&lt;/p&gt;
    &lt;p&gt;Goldman Sachs CEO David Solomon on Friday expressed some concerns about stock market levels amid the AI hype.&lt;/p&gt;
    &lt;p&gt;"When [investors are] excited, they tend to think about the good things that can go right, and they diminish the things you should be skeptical about that can go wrong ... There will be a reset, there will be a check at some point, there will be a drawdown," Solomon said at Italian Tech Week. "The extent of that will depend on how long this [bull run] goes."&lt;/p&gt;
    &lt;p&gt;Karim Moussalem, chief investment officer of equities at Selwood Asset Management, said last week that the "AI trade is beginning to resemble one of the great speculative manias of market history."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2025/10/03/jeff-bezos-ai-in-an-industrial-bubble-but-society-to-benefit.html"/><published>2025-10-03T16:00:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464632</id><title>Cancellations in async Rust</title><updated>2025-10-04T07:32:19.914698+00:00</updated><content>&lt;doc fingerprint="310499976ca2b6bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cancelling async Rust&lt;/head&gt;
    &lt;p&gt;This is an edited, written version of my RustConf 2025 talk about cancellations in async Rust. Like the written version of my RustConf 2023 talk, I’ve tried to retain the feel of a talk while making it readable as a standalone blog entry. Some links:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Video of the talk on YouTube.&lt;/item&gt;
      &lt;item&gt;Slides on Google Slides.&lt;/item&gt;
      &lt;item&gt;Repository with links and notes on GitHub.&lt;/item&gt;
      &lt;item&gt;Coverage on Linux Weekly News.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Introduction#&lt;/head&gt;
    &lt;p&gt;Let’s start with a simple example – you decide to read from a channel in a loop and gather a bunch of messages:&lt;/p&gt;
    &lt;code&gt;loop {
    match rx.recv().await {
        Ok(msg) =&amp;gt; process(msg),
        Err(_) =&amp;gt; return,
    }
}
&lt;/code&gt;
    &lt;p&gt;All good, nothing wrong with this, but you realize sometimes the channel is empty for long periods of time, so you add a timeout and print a message:&lt;/p&gt;
    &lt;code&gt;loop {
    match timeout(Duration::from_secs(5), rx.recv()).await {
        Ok(Ok(msg)) =&amp;gt; process(msg),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no messages for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;There’s nothing wrong with this code—it behaves as expected.&lt;/p&gt;
    &lt;p&gt;Now you realize you need to write a bunch of messages out to a channel in a loop:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match tx.send(msg).await {
        Ok(_) =&amp;gt; println!("sent successfully"),
        Err(_) =&amp;gt; return,
    }
}
&lt;/code&gt;
    &lt;p&gt;But sometimes the channel gets too full and blocks, so you add a timeout and print a message:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match timeout(Duration::from_secs(5), tx.send(msg)).await {
        Ok(Ok(_)) =&amp;gt; println!("sent successfully"),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no space for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;It turns out that this code is often incorrect, because not all messages make their way to the channel.&lt;/p&gt;
    &lt;p&gt;Hi, I’m Rain, and this post is about cancelling async Rust. This post is split into three parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What is cancellation? It’s an extremely powerful part of async Rust but also one that is very hard to reason thoroughly about.&lt;/item&gt;
      &lt;item&gt;Analyzing cancellations: Going deep into their mechanics and providing some helpful ways to think about them.&lt;/item&gt;
      &lt;item&gt;What can be done? Solutions, including practical guidance, and real bugs we’ve found and fixed in production codebases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before we begin, I want to lay my cards on the table – I really love async Rust!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I gave a talk at RustConf a couple years ago talking about how async Rust is a great fit for signal handling in complex applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m also the author of cargo-nextest, a next-generation test runner for Rust, where async Rust is the best way I know of to express some really complex algorithms that I wouldn’t know how to express otherwise. I wrote a blog post about this a few years ago.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now, I work at Oxide Computer Company, where we make cloud-in-a-box computers. We make vertically integrated systems where you provide power and networking on one end, and the software you want to run on the other end, and we take care of everything in between.&lt;/p&gt;
    &lt;p&gt;Of course, we use Rust everywhere, and in particular we use async Rust extensively for our higher-level software, such as storage, networking and the customer-facing management API. But along the way we’ve encountered a number of issues around async cancellation, and a lot of this post is about what we learned along the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. What is cancellation?#&lt;/head&gt;
    &lt;p&gt;What does cancellation mean? Logically, a cancellation is exactly what it sounds like: you start some work, and then change your mind and decide to stop doing that work.&lt;/p&gt;
    &lt;p&gt;As you might imagine this is a useful thing to do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You may have started a large download or a long network request&lt;/item&gt;
      &lt;item&gt;Maybe you’ve started reading a file, similar to the &lt;code&gt;head&lt;/code&gt;command.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But then you change your mind: you want to cancel it rather than continue it to completion.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancellations in synchronous Rust#&lt;/head&gt;
    &lt;p&gt;Before we talk about async Rust, it’s worth thinking about how you’d do cancellations in synchronous Rust.&lt;/p&gt;
    &lt;p&gt;One option is to have some kind of flag you periodically check, maybe stored in an atomic:&lt;/p&gt;
    &lt;code&gt;while !should_cancel.load(Ordering::Relaxed) {
    expensive_operation();
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The code that wishes to perform the cancellation can set that flag.&lt;/item&gt;
      &lt;item&gt;Then, the code which checks that flag can exit early.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach is fine for smaller bits of code but doesn’t really scale well to large chunks of code since you’d have to sprinkle these checks everywhere.&lt;/p&gt;
    &lt;p&gt;A related option, if you’re working with a framework as part of your work, is to panic with a special payload of some kind.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If that feels strange to you, you’re not alone! But the Salsa framework for incremental computation, used by—among other things—rust-analyzer, uses this approach.&lt;/item&gt;
      &lt;item&gt;Something I learned recently was that this only works on build targets which have a notion of panic unwinding, or being able to bubble up the panic. Not all platforms support this, and in particular, Wasm doesn’t. This means that Salsa cancellations don’t work if you build rust-analyzer for Wasm.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A third option is to kill the whole process. This is a very heavyweight approach, but an effective one in case you spawn processes to do your work.&lt;/p&gt;
    &lt;p&gt;Rather than kill the whole process, can you kill a single thread?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;While some OSes have APIs to perform this action, they tend to warn very strongly against it. That’s because in general, most code is just not ready for a thread disappearing from underneath.&lt;/item&gt;
      &lt;item&gt;In particular, thread killing is not permitted by safe Rust, since it can cause serious corruption. For example, Rust mutexes would likely stay locked forever.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these options are suboptimal or of limited use in some way. In general, the way I think about it is that there isn’t a universal protocol for cancellation in synchronous Rust.&lt;/p&gt;
    &lt;p&gt;In contrast, there is such a protocol in async Rust, and in fact cancellations are extraordinarily easy to perform in async Rust.&lt;/p&gt;
    &lt;p&gt;Why is that so? To understand that, let’s look at what a future is.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is a future?#&lt;/head&gt;
    &lt;p&gt;Here’s a simple example of a future:&lt;/p&gt;
    &lt;code&gt;// This creates a state machine.
let future = async {
    let data = request().await;
    process(data).await
};

// Nothing executes yet. `future` is just a struct in memory.
&lt;/code&gt;
    &lt;p&gt;In this future, you first perform a network request which returns some data, and then you process it.&lt;/p&gt;
    &lt;p&gt;The Rust compiler looks at this future and generates a state machine, which is just a struct or enum in memory:&lt;/p&gt;
    &lt;code&gt;// The compiler generates something like:
enum MyFuture {
    Start,
    WaitingForNetwork(NetworkFuture),
    WaitingForProcess(ProcessFuture, Data),
    Done(Result),
}

// It's just data, no running code!
&lt;/code&gt;
    &lt;p&gt;If you’ve written async Rust before the &lt;code&gt;async&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; keywords, you’ve probably written code like it by hand. It’s basically just an enum describing all the possible states the future can be in.&lt;/p&gt;
    &lt;p&gt;The compiler also generates an implementation of the &lt;code&gt;Future&lt;/code&gt; trait for this future:&lt;/p&gt;
    &lt;code&gt;impl Future for MyFuture {
    fn poll(/* ... */) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt; {
        match self {
            Start =&amp;gt; { /* ... */ }
            WaitingForNetwork(fut) =&amp;gt; { /* ... */ }
            // etc
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;and when you call &lt;code&gt;.await&lt;/code&gt; on the future, it gets translated down to this underlying &lt;code&gt;poll&lt;/code&gt; function. It is only when &lt;code&gt;await&lt;/code&gt; or this &lt;code&gt;poll&lt;/code&gt; function is called that something actually happens.&lt;/p&gt;
    &lt;p&gt;Note that this is diametrically opposed to how async works in other languages like Go, JavaScript, or C#. In those languages, when you create a future to await on, it starts doing its thing, immediately, in the background:&lt;/p&gt;
    &lt;code&gt;// JavaScript: starts running immediately
const promise = fetch('/api/data');
&lt;/code&gt;
    &lt;p&gt;That’s regardless of whether you await it or not.&lt;/p&gt;
    &lt;p&gt;In Rust, this &lt;code&gt;get&lt;/code&gt; call does nothing until you actually call &lt;code&gt;.await&lt;/code&gt; on it:&lt;/p&gt;
    &lt;code&gt;// Rust: just data, does nothing!
let future = reqwest::get("/api/data");
&lt;/code&gt;
    &lt;p&gt;I know I sound a bit like a broken record here, but if you can take away one thing from this post, it would be that futures are passive, and completely inert until awaited or polled.&lt;/p&gt;
    &lt;head rend="h3"&gt;The universal protocol#&lt;/head&gt;
    &lt;p&gt;So what does the universal protocol to cancel futures look like? It is simply to drop the future, or to not await it, or poll it any more. Since a future is just a state machine, you can throw it away at any time the poll function isn’t actively being called.&lt;/p&gt;
    &lt;code&gt;let future = some_async_work();
drop(future); // cancelled
&lt;/code&gt;
    &lt;p&gt;The upshot of all this is that any Rust future can be cancelled at any await point.&lt;/p&gt;
    &lt;p&gt;Given how hard cancellation tends to be in synchronous environments, the ability to easily cancel futures in async Rust is extraordinarily powerful—in many ways its greatest strength!&lt;/p&gt;
    &lt;p&gt;But there is a flip side, which is that cancelling futures is far, far too easy. This is for two reasons.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;First, it’s just way too easy to quietly drop a future. As we’re going to see, there are all kinds of code patterns that lead to silently dropping futures.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now this wouldn’t be so bad, if not for the second reason: that cancellation of parent futures propagates down to child futures.&lt;/p&gt;
        &lt;p&gt;Because of Rust’s single ownership model, child futures are owned by parent ones. If a parent future is dropped or cancelled, the same happens to the child.&lt;/p&gt;
        &lt;p&gt;To figure out whether a child future’s cancellation can cause issues, you have to look at its parent, and grandparent, and so on. Reasoning about cancellation becomes a very complicated non-local operation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;2. Analyzing cancellations#&lt;/head&gt;
    &lt;p&gt;I’m going to cover some examples in a bit, but before we do that I want to talk about a couple terms, some of which you might have seen references to already.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancel safety and cancel correctness#&lt;/head&gt;
    &lt;p&gt;The first term is cancel safety. You might have seen mentions of this in the Tokio documentation. Cancel safety, as generally defined, means the property of a future that can be cancelled (i.e. dropped) without any side effects.&lt;/p&gt;
    &lt;p&gt;For example, a Tokio sleep future is cancel safe: you can just stop waiting on the sleep and it’s completely fine.&lt;/p&gt;
    &lt;code&gt;let future = tokio::time::sleep();
drop(future); // this has no side effects
&lt;/code&gt;
    &lt;p&gt;An example of a future that is not cancel safe is Tokio’s MPSC send, which sends a message over a channel:&lt;/p&gt;
    &lt;code&gt;let message = /* ... */;
let future = sender.send(message);
drop(future); // message is lost!
&lt;/code&gt;
    &lt;p&gt;If this future is dropped, the message is lost forever.&lt;/p&gt;
    &lt;p&gt;The important thing is that cancel safety is a local property of an individual future.&lt;/p&gt;
    &lt;p&gt;But cancel safety is not all that one needs to care about. What actually matters is the context the cancellation happens in, or in other words whether the cancellation actually causes some kind of larger property in the system to be violated.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For example, if you drop a future which sends a message, but for whatever reason you don’t care about the message any more, it’s not really a bug!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To capture this I tend to use a different term called cancel correctness, which I define as a global property of system correctness in the face of cancellations. (This isn’t a standard term, but it’s a framing I’ve found really helpful in understanding cancellations.)&lt;/p&gt;
    &lt;p&gt;When is cancel correctness violated? It requires three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The system has a cancel-unsafe future somewhere within it. As we’ll see, many APIs that are cancel-unsafe can be reworked to be cancel-safe. If there aren’t any cancel-unsafe futures in the system, then the system is cancel correct.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A cancel-unsafe future is actually cancelled. This may sound a bit trivial, but if cancel-unsafe futures are always run to completion, then the system can’t have cancel correctness bugs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Cancelling the future violates some property of a system. This could be data loss as with&lt;/p&gt;&lt;code&gt;Sender::send&lt;/code&gt;, some kind of invariant violation, or some kind of cleanup that must be performed but isn’t.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So a lot of making Rust async robust is about trying to tackle one of these three things.&lt;/p&gt;
    &lt;p&gt;I want to zoom in for a second on invariant violations and talk about an example of a Tokio API that is very prone to cancel correctness issues: Tokio mutexes.&lt;/p&gt;
    &lt;head rend="h3"&gt;The pain of Tokio mutexes#&lt;/head&gt;
    &lt;p&gt;The way Tokio mutexes work is: you create a mutex, you lock it which gives you mutable access to the data underneath, and then you unlock it by releasing the mutex.&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// Access guard.data, protected by the mutex...
drop(guard);
&lt;/code&gt;
    &lt;p&gt;If you look at the &lt;code&gt;lock&lt;/code&gt; function’s documentation, in the “cancel safety” section it says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This method uses a queue to fairly distribute locks in the order they were requested. Cancelling a call to lock makes you lose your place in the queue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Okay, so not totally cancel safe, but the only kind of unsafety is fairness, which doesn’t sound too bad.&lt;/p&gt;
    &lt;p&gt;But the problems lie in what you actually do with the mutex. In practice, most uses of mutexes are in order to temporarily violate invariants that are otherwise upheld when a lock isn’t held.&lt;/p&gt;
    &lt;p&gt;I’ll use a real world example of a cancel correctness bug that we found at my job at Oxide: we had code to manage a bunch of data sent over by our computers, which we call sleds. The shared state was guarded by a mutex, and a typical operation was:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Obtain a lock on the mutex.&lt;/item&gt;
      &lt;item&gt;Obtain the sled-specific data by value, moving it to an invalid &lt;code&gt;None&lt;/code&gt;state.&lt;/item&gt;
      &lt;item&gt;Perform an action.&lt;/item&gt;
      &lt;item&gt;Set the sled-specific data back to the next valid state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a rough sketch of what that looks like:&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// guard.data is Option&amp;lt;T&amp;gt;: Some to begin with
let data = guard.data.take(); // guard.data is now None

let new_data = process_data(data);
guard.data = Some(new_data); // guard.data is Some again
&lt;/code&gt;
    &lt;p&gt;This is all well and good, but the problem is that the action being performed actually had an await point within it:&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// guard.data is Option&amp;lt;T&amp;gt;: Some to begin with
let data = guard.data.take(); // guard.data is now None

// DANGER: cancellation here leaves data in None state!
let new_data = process_data(data).await;
guard.data = Some(new_data); // guard.data is Some again
&lt;/code&gt;
    &lt;p&gt;If the code that operated on the mutex got cancelled at that await point, then the data would be stuck in the invalid &lt;code&gt;None&lt;/code&gt; state. Not great!&lt;/p&gt;
    &lt;p&gt;And keep in mind the non-local reasoning aspect: when doing this analysis, you need to look at the whole chain of callers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancellation patterns#&lt;/head&gt;
    &lt;p&gt;Now that we’ve talked about some of the bad things that can happen during cancellations, it’s worth asking what kinds of code patterns lead to futures being cancelled.&lt;/p&gt;
    &lt;p&gt;The most straightforward example, and maybe a bit of a silly one, is that you create a future but simply forget to call &lt;code&gt;.await&lt;/code&gt; on it.&lt;/p&gt;
    &lt;code&gt;some_async_work(); // missing .await
&lt;/code&gt;
    &lt;p&gt;Now Rust actually warns you if you don’t call &lt;code&gt;.await&lt;/code&gt; on the future:&lt;/p&gt;
    &lt;code&gt;warning: unused implementer of `Future` that must be used
   |
11 |     some_async_work();
   |     ^^^^^^^^^^^^^^^^^
   |
   = note: futures do nothing unless you `.await` or poll them
&lt;/code&gt;
    &lt;p&gt;But a code pattern I’ve sometimes made mistakes with is that the future returns a &lt;code&gt;Result&lt;/code&gt;, and you want to ignore the result so you assign it to an underscore like so:&lt;/p&gt;
    &lt;code&gt;let _ = some_async_work(); // future returns Result
&lt;/code&gt;
    &lt;p&gt;If I forget to call &lt;code&gt;.await&lt;/code&gt; on the future, Rust doesn’t warn me about it at all, and then I’m left scratching my head about why this code didn’t run. I know this sounds really silly and basic, but I’ve made this mistake a bunch of times.&lt;/p&gt;
    &lt;p&gt;(After my talk, it was pointed out to me that Clippy 1.67 and above have a &lt;code&gt;let_underscore_future&lt;/code&gt; warn-by-default lint for this. Hooray!)&lt;/p&gt;
    &lt;p&gt;Another example of futures being cancelled is &lt;code&gt;try&lt;/code&gt; operations, such as Tokio’s &lt;code&gt;try_join&lt;/code&gt; macro. For example:&lt;/p&gt;
    &lt;code&gt;async fn do_stuff_async() -&amp;gt; Result&amp;lt;(), &amp;amp;'static str&amp;gt; {
    // async work
}

async fn more_async_work() -&amp;gt; Result&amp;lt;(), &amp;amp;'static str&amp;gt; {
    // more here
}

let res = tokio::try_join!(
    do_stuff_async(),
    more_async_work(),
);

// ...
&lt;/code&gt;
    &lt;p&gt;If you call &lt;code&gt;try_join&lt;/code&gt; with a bunch of futures, and all of them succeed, it’s all good. But if one of them fails, the rest simply get cancelled.&lt;/p&gt;
    &lt;p&gt;In fact, at Oxide we had a pretty bad bug around this: we had code to stop a bunch of services, all expressed as futures. We used &lt;code&gt;try_join&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;try_join!(
    stop_service_a(),
    stop_service_b(),
    stop_service_c(),
)?;
&lt;/code&gt;
    &lt;p&gt;If one of these operations failed for whatever reason, we would stop running the code to wait for the other services to exit. Oops!&lt;/p&gt;
    &lt;p&gt;But perhaps the most well-known source of cancellations is Tokio’s &lt;code&gt;select&lt;/code&gt; macro. Select is this incredibly beautiful operation. It is called with a set of futures, and it drives all of them forward concurrently:&lt;/p&gt;
    &lt;code&gt;tokio::select! {
    result1 = future1 =&amp;gt; handle_result1(result1),
    result2 = future2 =&amp;gt; handle_result2(result2),
}
&lt;/code&gt;
    &lt;p&gt;Each future has a code block associated with it (above, &lt;code&gt;handle_result1&lt;/code&gt; and &lt;code&gt;handle_result2&lt;/code&gt;). If one of the futures completes, the corresponding code block is called. But also, all of the other futures are always cancelled!&lt;/p&gt;
    &lt;p&gt;For a variety of reasons, select statements in general, and select loops in particular, are particularly prone to cancel correctness issues. So a lot of the documentation about cancel safety talks about select loops. But I want to emphasize here that select is not the only source of cancellations, just a particularly notable one.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. What can be done?#&lt;/head&gt;
    &lt;p&gt;So, now that we’ve looked at all of these issues with cancellations, what can be done about it?&lt;/p&gt;
    &lt;p&gt;First, I want to break the bad news to you – there is no general, fully reliable solution for this in Rust today. But in our experience there are a few patterns that have been successful at reducing the likelihood of cancellation bugs.&lt;/p&gt;
    &lt;p&gt;Going back to our definition of cancel correctness, there are three prongs all of which come together to produce a bug:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A cancel-unsafe future exists&lt;/item&gt;
      &lt;item&gt;This cancel-unsafe future is cancelled&lt;/item&gt;
      &lt;item&gt;The cancellation violates a system property&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most solutions we’ve come up with try and tackle one of these prongs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Making futures cancel-safe#&lt;/head&gt;
    &lt;p&gt;Let’s look at the first prong: the system has a cancel-unsafe future somewhere in it. Can we use code patterns to make futures be cancel-safe? It turns out we can! I’ll give you two examples here.&lt;/p&gt;
    &lt;p&gt;The first is MPSC sends. Let’s come back to the example from earlier where we would lose messages entirely:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match timeout(Duration::from_secs(5), tx.send(msg)).await {
        Ok(Ok(_)) =&amp;gt; println!("sent successfully"),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no space for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;Can we find a way to make this cancel safe?&lt;/p&gt;
    &lt;p&gt;In this case, yes, and we do so by breaking up the operation into two parts:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    loop {
        match timeout(Duration::from_secs(5), tx.reserve()).await {
            Ok(Ok(permit)) =&amp;gt; { permit.send(msg); break; }
            Ok(Err(_)) =&amp;gt; return,
            Err(_) =&amp;gt; println!("no space for 5 seconds"),
        }
    }
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The first component is the operation to reserve a permit or slot in the channel. This is an initial async operation that’s cancel-safe.&lt;/item&gt;
      &lt;item&gt;The second is to actually send the message, which is an operation that becomes infallible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(I want to put an asterisk here that reserve is not entirely cancel-safe, since Tokio’s MPSC follows a first-in-first-out pattern and dropping the future means losing your place in line. Keep this in mind for now.)&lt;/p&gt;
    &lt;p&gt;The second is with Tokio’s &lt;code&gt;AsyncWrite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you’ve written synchronous Rust you’re probably familiar with the &lt;code&gt;write_all&lt;/code&gt; method, which writes an entire buffer out:&lt;/p&gt;
    &lt;code&gt;use std::io::Write;

let buffer: &amp;amp;[u8] = /* ... */;
writer.write_all(buffer)?;
&lt;/code&gt;
    &lt;p&gt;In synchronous Rust, this is a great API. But within async Rust, the &lt;code&gt;write_all&lt;/code&gt; pattern is absolutely not cancel safe! If the future is dropped before completion, you have no idea how much of this buffer was written out.&lt;/p&gt;
    &lt;code&gt;use tokio::io::AsyncWriteExt;

let buffer: &amp;amp;[u8] = /* ... */;
writer.write_all(buffer).await?; // Not cancel-safe!
&lt;/code&gt;
    &lt;p&gt;But there’s an alternative API that is cancel-safe, called &lt;code&gt;write_all_buf&lt;/code&gt;. This API is carefully designed to enable the reporting of partial progress, and it doesn’t just accept a buffer, but rather something that looks like a cursor on top of it:&lt;/p&gt;
    &lt;code&gt;use tokio::io::AsyncWriteExt;

let mut buffer: io::Cursor&amp;lt;&amp;amp;[u8]&amp;gt; = /* ... */;
writer.write_all_buf(&amp;amp;mut buffer).await?;
&lt;/code&gt;
    &lt;p&gt;When part of the buffer is written out, the cursor is advanced by that number of bytes. So if you call &lt;code&gt;write_all_buf&lt;/code&gt; in a loop, you’ll be resuming from this partial progress, which works great.&lt;/p&gt;
    &lt;head rend="h3"&gt;Not cancelling futures#&lt;/head&gt;
    &lt;p&gt;Going back to the three prongs: the second prong is about actually cancelling futures. What code patterns can be used to not cancel futures? Here are a couple of examples.&lt;/p&gt;
    &lt;p&gt;The first one is, in a place like a select loop, resume futures rather than cancelling them each time. You’d typically achieve this by pinning a future, and then polling a mutable reference to that future. For example:&lt;/p&gt;
    &lt;code&gt;let mut future = Box::pin(channel.reserve());
loop {
    tokio::select! {
        result = &amp;amp;mut future =&amp;gt; break result,
        _ = other_condition =&amp;gt; continue,
    }
}
&lt;/code&gt;
    &lt;p&gt;Coming back to our example of MPSC sends, the one asterisk with &lt;code&gt;reserve&lt;/code&gt; is that cancelling it makes you lose your place in line. Instead, if you pin the &lt;code&gt;reserve&lt;/code&gt; future and poll a mutable reference to it, you don’t lose your place in line.&lt;/p&gt;
    &lt;p&gt;(Does the difference here matter? It depends, but you can now have this strategy available to you.)&lt;/p&gt;
    &lt;p&gt;The second example is to use tasks. I mentioned earlier that futures are Rust are diametrically opposed to similar notions in languages like JavaScript. Well, there’s an alternative in async Rust that’s much closer to the JavaScript idea, and that’s tasks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unlike futures which are driven by the caller, tasks are driven by the runtime (such as Tokio).&lt;/item&gt;
      &lt;item&gt;With Tokio, dropping a handle to a task does not cause it to be cancelled, which means they’re a good place to run cancel-unsafe code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A fun example is that at Oxide, we have an HTTP server called Dropshot. Previously, whenever an HTTP request came in, we’d use a future for it, and drop the future if the TCP connection was closed.&lt;/p&gt;
    &lt;code&gt;// Before: Future cancelled on TCP close
handle_request(req).await;
&lt;/code&gt;
    &lt;p&gt;This was really bad because future cancellations could happen due to the behavior of not just the parent future, but of a process that was running across a network! This is a rather extreme form of non-local reasoning.&lt;/p&gt;
    &lt;p&gt;We addressed this by spinning up a task for each HTTP request, and by running the code to completion even if the connection is closed:&lt;/p&gt;
    &lt;code&gt;// After: Task runs to completion
tokio::spawn(handle_request(req));
&lt;/code&gt;
    &lt;head rend="h3"&gt;Systematic solutions?#&lt;/head&gt;
    &lt;p&gt;The last thing I want to say is that this sucks!&lt;/p&gt;
    &lt;p&gt;The promise of Rust is that you don’t need to do this kind of non-local reasoning—that you can analyze small bits of code for local correctness, and scale that up to global correctness. Almost everything in Rust, from &lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;&amp;amp;mut&lt;/code&gt; to &lt;code&gt;unsafe&lt;/code&gt;, is geared towards making that possible. Future cancellations fly directly in the face of that, and I think they’re probably the least Rusty part of Rust. This is all really unfortunate.&lt;/p&gt;
    &lt;p&gt;Can we come up with something more systematic than this kind of ad-hoc reasoning?&lt;/p&gt;
    &lt;p&gt;There doesn’t exist anything in safe Rust today, but there are a few different ideas people have come up with. I wanted to give a nod to those ideas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async drop would let you run async code when a future is cancelled. This would handle some, though not all, of the cases we discussed today.&lt;/item&gt;
      &lt;item&gt;There’s also a couple different proposals for what are called linear types, where you could force some code to be run on drop, or mark a particular future as non-cancellable (once it’s been created it must be driven to completion).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these options have really significant implementation challenges, though. This blog post from boats covers some of these solutions, and the implementation challenges with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;In this post, we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Saw that futures are passive&lt;/item&gt;
      &lt;item&gt;Introduced cancel safety and cancel correctness as concepts&lt;/item&gt;
      &lt;item&gt;Examined some bugs that can occur with cancellation&lt;/item&gt;
      &lt;item&gt;Looked at some recommendations you can use to mitigate the downsides of cancellation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of the recommendations are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Avoid Tokio mutexes&lt;/item&gt;
      &lt;item&gt;Rewrite APIs to make futures cancel-safe&lt;/item&gt;
      &lt;item&gt;Find ways to ensure that cancel-unsafe futures are driven to completion&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There’s a very deep well of complexity here, a lot more than I can cover in one blog post:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why are futures passive, anyway?&lt;/item&gt;
      &lt;item&gt;Cooperative cancellation: cancellation tokens&lt;/item&gt;
      &lt;item&gt;Actor model as an alternative to Tokio mutexes&lt;/item&gt;
      &lt;item&gt;Task aborts&lt;/item&gt;
      &lt;item&gt;Structured concurrency&lt;/item&gt;
      &lt;item&gt;Relationship to panic safety and mutex poisoning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re curious about any of these, check out this link where I’ve put together a collection of documents and blog posts about these concepts. In particular, I’d recommend reading these two Oxide RFDs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RFD 397 Challenges with async/await in the control plane by David Pacheco&lt;/item&gt;
      &lt;item&gt;RFD 400 Dealing with cancel safety in async Rust by myself&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you for reading this post to the end! And thanks to many of my coworkers at Oxide for reviewing the talk and the RFDs linked above, and for suggestions and constructive feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sunshowers.io/posts/cancelling-async-rust/"/><published>2025-10-03T16:18:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45466086</id><title>PEP 810 – Explicit lazy imports</title><updated>2025-10-04T07:32:19.461999+00:00</updated><content>&lt;doc fingerprint="d0324610caa1e4dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PEP 810 – Explicit lazy imports&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Author:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Pablo Galindo &amp;lt;pablogsal at python.org&amp;gt;, Germán Méndez Bravo &amp;lt;german.mb at gmail.com&amp;gt;, Thomas Wouters &amp;lt;thomas at python.org&amp;gt;, Dino Viehland &amp;lt;dinoviehland at gmail.com&amp;gt;, Brittany Reynoso &amp;lt;brittanyrey at gmail.com&amp;gt;, Noah Kim &amp;lt;noahbkim at gmail.com&amp;gt;, Tim Stumbaugh &amp;lt;me at tjstum.com&amp;gt;&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Discussions-To:&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Discourse thread&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Status:&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Draft&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Type:&lt;/item&gt;
      &lt;item rend="dd-4"&gt;Standards Track&lt;/item&gt;
      &lt;item rend="dt-5"&gt;Created:&lt;/item&gt;
      &lt;item rend="dd-5"&gt;02-Oct-2025&lt;/item&gt;
      &lt;item rend="dt-6"&gt;Python-Version:&lt;/item&gt;
      &lt;item rend="dd-6"&gt;3.15&lt;/item&gt;
      &lt;item rend="dt-7"&gt;Post-History:&lt;/item&gt;
      &lt;item rend="dd-7"&gt;03-Oct-2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;This PEP introduces syntax for lazy imports as an explicit language feature:&lt;/p&gt;
    &lt;code&gt;lazy import json
lazy from json import dumps
&lt;/code&gt;
    &lt;p&gt;Lazy imports defer the loading and execution of a module until the first time the imported name is used, in contrast to ‘normal’ imports, which eagerly load and execute a module at the point of the import statement.&lt;/p&gt;
    &lt;p&gt;By allowing developers to mark individual imports as lazy with explicit syntax, Python programs can reduce startup time, memory usage, and unnecessary work. This is particularly beneficial for command-line tools, test suites, and applications with large dependency graphs.&lt;/p&gt;
    &lt;p&gt;This proposal preserves full backwards compatibility: normal import statements remain unchanged, and lazy imports are enabled only where explicitly requested.&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;The dominant convention in Python code is to place all imports at the module level, typically at the beginning of the file. This avoids repetition, makes import dependencies clear and minimizes runtime overhead by only evaluating an import statement once per module.&lt;/p&gt;
    &lt;p&gt;A major drawback with this approach is that importing the first module for an execution of Python (the “main” module) often triggers an immediate cascade of imports, and optimistically loads many dependencies that may never be used. The effect is especially costly for command-line tools with multiple subcommands, where even running the command with &lt;code&gt;--help&lt;/code&gt; can load dozens of
unnecessary modules and take several seconds. This basic example demonstrates
what must be loaded just to get helpful feedback to the user on how to run the
program at all. Inefficiently, the user incurs this overhead again when they
figure out the command they want and invoke the program “for real.”&lt;/p&gt;
    &lt;p&gt;A somewhat common way to delay imports is to move the imports into functions (inline imports), but this practice requires more work to implement and maintain, and can be subverted by a single inadvertent top-level import. Additionally, it obfuscates the full set of dependencies for a module. Analysis of the Python standard library shows that approximately 17% of all imports outside tests (nearly 3500 total imports across 730 files) are already placed inside functions or methods specifically to defer their execution. This demonstrates that developers are already manually implementing lazy imports in performance-sensitive code, but doing so requires scattering imports throughout the codebase and makes the full dependency graph harder to understand at a glance.&lt;/p&gt;
    &lt;p&gt;The standard library provides the &lt;code&gt;LazyLoader&lt;/code&gt; class to
solve some of these inefficiency problems. It permits imports at the module
level to work mostly like inline imports do. Many scientific Python
libraries have adopted a similar pattern, formalized in
SPEC 1.
There’s also the third-party lazy_loader package, yet another
implementation of lazy imports. Imports used solely for static type checking
are another source of potentially unneeded imports, and there are similarly
disparate approaches to minimizing the overhead. The various approaches used
here to defer or remove eager imports do not cover all potential use-cases for
a general lazy import mechanism. There is no clear standard, and there are
several drawbacks including runtime overhead in unexpected places, or worse
runtime introspection.&lt;/p&gt;
    &lt;p&gt;This proposal introduces syntax for lazy imports with a design that is local, explicit, controlled, and granular. Each of these qualities is essential to making the feature predictable and safe to use in practice.&lt;/p&gt;
    &lt;p&gt;The behavior is local: laziness applies only to the specific import marked with the &lt;code&gt;lazy&lt;/code&gt; keyword, and it does not cascade recursively into other
imports. This ensures that developers can reason about the effect of laziness
by looking only at the line of code in front of them, without worrying about
whether imported modules will themselves behave differently. A &lt;code&gt;lazy import&lt;/code&gt;
is an isolated decision each time it is used, not a global shift in semantics.&lt;/p&gt;
    &lt;p&gt;The semantics are explicit. When a name is imported lazily, the binding is created in the importing module immediately, but the target module is not loaded until the first time the name is accessed. After this point, the binding is indistinguishable from one created by a normal import. This clarity reduces surprises and makes the feature accessible to developers who may not be deeply familiar with Python’s import machinery.&lt;/p&gt;
    &lt;p&gt;Lazy imports are controlled, in the sense that deferred loading is only triggered by the importing code itself. In the general case, a library will only experience lazy imports if its own authors choose to mark them as such. This avoids shifting responsibility onto downstream users and prevents accidental surprises in library behavior. Since library authors typically manage their own import subgraphs, they retain predictable control over when and how laziness is applied.&lt;/p&gt;
    &lt;p&gt;The mechanism is also granular. It is introduced through explicit syntax on individual imports, rather than a global flag or implicit setting. This allows developers to adopt it incrementally, starting with the most performance-sensitive areas of a codebase. As this feature is introduced to the community, we want to make the experience of onboarding optional, progressive, and adaptable to the needs of each project.&lt;/p&gt;
    &lt;p&gt;Lazy imports provide several concrete advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Command-line tools are often invoked directly by a user, so latency – in particular startup latency – is quite noticeable. These programs are also typically short-lived processes (contrasted with, e.g., a web server). With lazy imports, only the code paths actually reached will import a module. This can reduce startup time by 50-70% in practice, providing a significant improvement to a common user experience and improving Python’s competitiveness in domains where fast startup matters most.&lt;/item&gt;
      &lt;item&gt;Type annotations frequently require imports that are never used at runtime. The common workaround is to wrap them in &lt;code&gt;if TYPE_CHECKING:&lt;/code&gt;blocks [1]. With lazy imports, annotation-only imports impose no runtime penalty, eliminating the need for such guards and making annotated codebases cleaner.&lt;/item&gt;
      &lt;item&gt;Large applications often import thousands of modules, and each module creates function and type objects, incurring memory costs. In long-lived processes, this noticeably raises baseline memory usage. Lazy imports defer these costs until a module is needed, keeping unused subsystems unloaded. Memory savings of 30-40% have been observed in real workloads.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Rationale&lt;/head&gt;
    &lt;p&gt;The design of this proposal is centered on clarity, predictability, and ease of adoption. Each decision was made to ensure that lazy imports provide tangible benefits without introducing unnecessary complexity into the language or its runtime.&lt;/p&gt;
    &lt;p&gt;It is also worth noting that while this PEP outlines one specific approach, we list alternate implementation strategies for some of the core aspects and semantics of the proposal. If the community expresses a strong preference for a different technical path that still preserves the same core semantics or there is fundamental disagreement over the specific option, we have included the brainstorming we have already completed in preparation for this proposal as reference.&lt;/p&gt;
    &lt;p&gt;The choice to introduce a new &lt;code&gt;lazy&lt;/code&gt; keyword reflects the need for explicit
syntax. Import behavior is too fundamental to be left implicit or hidden
behind global flags or environment variables. By marking laziness directly at
the import site, the intent is immediately visible to both readers and tools.
This avoids surprises, reduces the cognitive burden of reasoning about
imports, and keeps lazy import semantics in line with Python’s tradition of
explicitness.&lt;/p&gt;
    &lt;p&gt;Another important decision is to represent lazy imports with proxy objects in the module’s namespace, rather than by modifying dictionary lookup. Earlier approaches experimented with embedding laziness into dictionaries, but this blurred abstractions and risked affecting unrelated parts of the runtime. The dictionary is a fundamental data structure in Python – literally every object is built on top of dicts – and adding hooks to dictionaries would prevent critical optimizations and complicate the entire runtime. The proxy approach is simpler: it behaves like a placeholder until first use, at which point it resolves the import and rebinds the name. From then on, the binding is indistinguishable from a normal import. This makes the mechanism easy to explain and keeps the rest of the interpreter unchanged.&lt;/p&gt;
    &lt;p&gt;Compatibility for library authors was also a key concern. Many maintainers need a migration path that allows them to support both new and old versions of Python at once. For this reason, the proposal includes the &lt;code&gt;__lazy_modules__&lt;/code&gt; global as a transitional mechanism. A module can
declare which imports should be treated as lazy (by listing the module names
as strings), and on Python 3.15 or later those imports will become lazy
automatically, as if they were imported with the &lt;code&gt;lazy&lt;/code&gt; keyword. On earlier
versions the declaration is ignored, leaving imports eager. This gives authors
a practical bridge until they can rely on the keyword as the canonical syntax.&lt;/p&gt;
    &lt;p&gt;Finally, the feature is designed to be adopted incrementally. Nothing changes unless a developer explicitly opts in, and adoption can begin with just a few imports in performance-sensitive areas. This mirrors the experience of gradual typing in Python: a mechanism that can be introduced progressively, without forcing projects to commit globally from day one. Notably, the adoption can also be done from the “outside in”, permitting CLI authors to introduce lazy imports and speed up user-facing tools, without requiring changes to every library the tool might use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other design decisions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The scope of laziness is deliberately local and non-recursive. A lazy import only affects the specific statement where it appears; it does not cascade into other modules or submodules. This choice is crucial for predictability. When developers read code, they can reason about import behavior line by line, without worrying about hidden laziness deeper in the dependency graph. The result is a feature that is powerful but still easy to understand in context.&lt;/item&gt;
      &lt;item&gt;In addition, it is useful to provide a mechanism to activate or deactivate lazy imports at a global level. While the primary design centers on explicit syntax, there are scenarios – such as large applications, testing environments, or frameworks – where enabling laziness consistently across many modules provides the most benefit. A global switch makes it easy to experiment with or enforce consistent behavior, while still working in combination with the filtering API to respect exclusions or tool-specific configuration. This ensures that global adoption can be practical without reducing flexibility or control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Specification&lt;/head&gt;
    &lt;head rend="h3"&gt;Grammar&lt;/head&gt;
    &lt;p&gt;A new soft keyword &lt;code&gt;lazy&lt;/code&gt; is added. A soft keyword is a context-sensitive
keyword that only has special meaning in specific grammatical contexts;
elsewhere it can be used as a regular identifier (e.g., as a variable name).
The &lt;code&gt;lazy&lt;/code&gt; keyword only has special meaning when it appears before import
statements:&lt;/p&gt;
    &lt;code&gt;import_name:
    | 'lazy'? 'import' dotted_as_names

import_from:
    | 'lazy'? 'from' ('.' | '...')* dotted_name 'import' import_from_targets
    | 'lazy'? 'from' ('.' | '...')+ 'import' import_from_targets
&lt;/code&gt;
    &lt;head rend="h4"&gt;Syntax restrictions&lt;/head&gt;
    &lt;p&gt;The soft keyword is only allowed at the global (module) level, not inside functions, class bodies, with &lt;code&gt;try&lt;/code&gt;/&lt;code&gt;with&lt;/code&gt; blocks, or &lt;code&gt;import *&lt;/code&gt;. Import
statements that use the soft keyword are potentially lazy. Imports that
can’t be lazy are unaffected by the global lazy imports flag, and instead are
always eager.&lt;/p&gt;
    &lt;p&gt;Examples of syntax errors:&lt;/p&gt;
    &lt;code&gt;# SyntaxError: lazy import not allowed inside functions
def foo():
    lazy import json

# SyntaxError: lazy import not allowed inside classes
class Bar:
    lazy import json

# SyntaxError: lazy import not allowed inside try/except blocks
try:
    lazy import json
except ImportError:
    pass

# SyntaxError: lazy import not allowed inside with blocks
with suppress(ImportError):
    lazy import json

# SyntaxError: lazy from ... import * is not allowed
lazy from json import *
&lt;/code&gt;
    &lt;head rend="h3"&gt;Semantics&lt;/head&gt;
    &lt;p&gt;When the &lt;code&gt;lazy&lt;/code&gt; keyword is used, the import becomes potentially lazy.
Unless lazy imports are disabled or suppressed (see below), the module is not
loaded immediately at the import statement; instead, a lazy proxy object is
created and bound to the name. The actual module is loaded on first use of
that name.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;import sys

lazy import json

print('json' in sys.modules)  # False - module not loaded yet

# First use triggers loading
result = json.dumps({"hello": "world"})

print('json' in sys.modules)  # True - now loaded
&lt;/code&gt;
    &lt;p&gt;A module may contain a &lt;code&gt;__lazy_modules__&lt;/code&gt; attribute, which is a
sequence of fully qualified module names (strings) to make potentially lazy
(as if the &lt;code&gt;lazy&lt;/code&gt; keyword was used). This attribute is checked on each
&lt;code&gt;import&lt;/code&gt; statement to determine whether the import should be made
potentially lazy. When a module is made lazy this way, from-imports using
that module are also lazy, but not necessarily imports of sub-modules.&lt;/p&gt;
    &lt;p&gt;The normal (non-lazy) import statement will check the global lazy imports flag. If it is “enabled”, all imports are potentially lazy (except for imports that can’t be lazy, as mentioned above.)&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;__lazy_modules__ = ["json"]
import json
print('json' in sys.modules)  # False
result = json.dumps({"hello": "world"})
print('json' in sys.modules)  # True
&lt;/code&gt;
    &lt;p&gt;If the global lazy imports flag is set to “disabled”, no potentially lazy import is ever imported lazily, and the behavior is equivalent to a regular import statement: the import is eager (as if the lazy keyword was not used).&lt;/p&gt;
    &lt;p&gt;For a potentially lazy import, the lazy imports filter (if set) is called with the name of the module doing the import, the name of the module being imported, and (if applicable) the fromlist. If the lazy import filter returns &lt;code&gt;True&lt;/code&gt;, the potentially lazy import becomes a lazy import. Otherwise, the
import is not lazy, and the normal (eager) import continues.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lazy import mechanism&lt;/head&gt;
    &lt;p&gt;When an import is lazy, &lt;code&gt;__lazy_import__&lt;/code&gt; is called instead of
&lt;code&gt;__import__&lt;/code&gt;. &lt;code&gt;__lazy_import__&lt;/code&gt; has the same function signature as
&lt;code&gt;__import__&lt;/code&gt;. It adds the module name to &lt;code&gt;sys.lazy_modules&lt;/code&gt;, a set of
fully-qualified module names which have been lazily imported at some point
(primarily for diagnostics and introspection), and returns a “lazy module
object.”&lt;/p&gt;
    &lt;p&gt;The implementation of &lt;code&gt;from ... import&lt;/code&gt; (the &lt;code&gt;IMPORT_FROM&lt;/code&gt; bytecode
implementation) checks if the module it’s fetching from is a lazy module
object, and if so, returns a lazy object for each name instead.&lt;/p&gt;
    &lt;p&gt;The end result of this process is that lazy imports (regardless of how they are enabled) result in lazy objects being assigned to global variables.&lt;/p&gt;
    &lt;p&gt;Lazy module objects do not appear in &lt;code&gt;sys.modules&lt;/code&gt;, they’re just listed in
the &lt;code&gt;sys.lazy_modules&lt;/code&gt; set. Under normal operation lazy objects should only
end up stored in global variables, and the common ways to access those
variables (regular variable access, module attributes) will resolve lazy
imports (“reify”) and replace them when they’re accessed.&lt;/p&gt;
    &lt;p&gt;It is still possible to expose lazy objects through other means, like debuggers. This is not considered a problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reification&lt;/head&gt;
    &lt;p&gt;When a lazy object is first used, it needs to be reified. This means resolving the import at that point in the program and replacing the lazy object with the concrete one. Reification imports the module in the same way as it would have been if it had been imported eagerly, barring intervening changes to the import system (e.g. to &lt;code&gt;sys.path&lt;/code&gt;, &lt;code&gt;sys.meta_path&lt;/code&gt;, &lt;code&gt;sys.path_hooks&lt;/code&gt; or
&lt;code&gt;__import__&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Reification still calls &lt;code&gt;__import__&lt;/code&gt; to resolve the import. When the module
is first reified, it’s removed from &lt;code&gt;sys.lazy_modules&lt;/code&gt; (even if there are
still other unreified lazy references to it). When a package is reified and
submodules in the package were also previously lazily imported, those
submodules are not automatically reified but they are added to the reified
package’s globals (unless the package already assigned something else to the
name of the submodule).&lt;/p&gt;
    &lt;p&gt;If reification fails (e.g., due to an &lt;code&gt;ImportError&lt;/code&gt;), the exception is
enhanced with chaining to show both where the lazy import was defined and
where it was first accessed (even though it propagates from the code that
triggered reification). This provides clear debugging information:&lt;/p&gt;
    &lt;code&gt;# app.py - has a typo in the import
lazy from json import dumsp  # Typo: should be 'dumps'

print("App started successfully")
print("Processing data...")

# Error occurs here on first use
result = dumsp({"key": "value"})
&lt;/code&gt;
    &lt;p&gt;The traceback shows both locations:&lt;/p&gt;
    &lt;code&gt;App started successfully
Processing data...
Traceback (most recent call last):
  File "app.py", line 2, in &amp;lt;module&amp;gt;
    lazy from json import dumsp
ImportError: deferred import of 'json.dumsp' raised an exception during resolution

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "app.py", line 8, in &amp;lt;module&amp;gt;
    result = dumsp({"key": "value"})
             ^^^^^
ImportError: cannot import name 'dumsp' from 'json'. Did you mean: 'dump'?
&lt;/code&gt;
    &lt;p&gt;This exception chaining clearly shows: (1) where the lazy import was defined, (2) that it was deferred, and (3) where the actual access happened that triggered the error.&lt;/p&gt;
    &lt;p&gt;Reification does not automatically occur when a module that was previously lazily imported is subsequently eagerly imported. Reification does not immediately resolve all lazy objects (e.g. &lt;code&gt;lazy from&lt;/code&gt; statements) that
referenced the module. It only resolves the lazy object being accessed.&lt;/p&gt;
    &lt;p&gt;Accessing a lazy object (from a global variable or a module attribute) reifies the object. Accessing a module’s &lt;code&gt;__dict__&lt;/code&gt; reifies all lazy objects in
that module. Operations that indirectly access &lt;code&gt;__dict__&lt;/code&gt; (such as
&lt;code&gt;dir()&lt;/code&gt;) also trigger this behavior.&lt;/p&gt;
    &lt;p&gt;Example using &lt;code&gt;__dict__&lt;/code&gt; from external code:&lt;/p&gt;
    &lt;code&gt;# my_module.py
import sys
lazy import json

print('json' in sys.modules)  # False - still lazy

# main.py
import sys
import my_module

# Accessing __dict__ from external code DOES reify all lazy imports
d = my_module.__dict__

print('json' in sys.modules)  # True - reified by __dict__ access
print(type(d['json']))  # &amp;lt;class 'module'&amp;gt;
&lt;/code&gt;
    &lt;p&gt;However, calling &lt;code&gt;globals()&lt;/code&gt; does not trigger reification – it returns
the module’s dictionary, and accessing lazy objects through that dictionary
still returns lazy proxy objects that need to be manually reified upon use. A
lazy object can be resolved explicitly by calling the &lt;code&gt;get&lt;/code&gt; method. Other,
more indirect ways of accessing arbitrary globals (e.g. inspecting
&lt;code&gt;frame.f_globals&lt;/code&gt;) also do not reify all the objects.&lt;/p&gt;
    &lt;p&gt;Example using &lt;code&gt;globals()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import sys
lazy import json

# Calling globals() does NOT trigger reification
g = globals()

print('json' in sys.modules)  # False - still lazy
print(type(g['json']))  # &amp;lt;class 'lazy_import'&amp;gt;

# Explicitly reify using the get() method
resolved = g['json'].get()

print(type(resolved))  # &amp;lt;class 'module'&amp;gt;
print('json' in sys.modules)  # True - now loaded
&lt;/code&gt;
    &lt;head rend="h2"&gt;Reference Implementation&lt;/head&gt;
    &lt;p&gt;A reference implementation is available at: https://github.com/LazyImportsCabal/cpython/tree/lazy&lt;/p&gt;
    &lt;head rend="h3"&gt;Bytecode and adaptive specialization&lt;/head&gt;
    &lt;p&gt;Lazy imports are implemented through modifications to four bytecode instructions: &lt;code&gt;IMPORT_NAME&lt;/code&gt;, &lt;code&gt;IMPORT_FROM&lt;/code&gt;, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;, and
&lt;code&gt;LOAD_NAME&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;lazy&lt;/code&gt; syntax sets a flag in the &lt;code&gt;IMPORT_NAME&lt;/code&gt; instruction’s oparg
(&lt;code&gt;oparg &amp;amp; 0x01&lt;/code&gt;). The interpreter checks this flag and calls
&lt;code&gt;_PyEval_LazyImportName()&lt;/code&gt; instead of &lt;code&gt;_PyEval_ImportName()&lt;/code&gt;, creating a
lazy import object rather than executing the import immediately. The
&lt;code&gt;IMPORT_FROM&lt;/code&gt; instruction checks whether its source is a lazy import
(&lt;code&gt;PyLazyImport_CheckExact()&lt;/code&gt;) and creates a lazy object for the attribute
rather than accessing it immediately.&lt;/p&gt;
    &lt;p&gt;When a lazy object is accessed, it must be reified. The &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;
instruction (used in function scopes) and &lt;code&gt;LOAD_NAME&lt;/code&gt; instruction (used at
module and class level) both check whether the object being loaded is a lazy
import. If so, they call &lt;code&gt;_PyImport_LoadLazyImportTstate()&lt;/code&gt; to perform the
actual import and store the module in &lt;code&gt;sys.modules&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This check incurs a very small cost on each access. However, Python’s adaptive interpreter can specialize &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; after observing that a lazy import
has been reified. After several executions, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; becomes
&lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;, which accesses the module dictionary directly without
checking for lazy imports.&lt;/p&gt;
    &lt;p&gt;Examples of the bytecode generated:&lt;/p&gt;
    &lt;code&gt;lazy import json  # IMPORT_NAME with flag set
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;IMPORT_NAME              1 (json + lazy)
&lt;/code&gt;
    &lt;code&gt;lazy from json import dumps  # IMPORT_NAME + IMPORT_FROM
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;IMPORT_NAME              1 (json + lazy)
IMPORT_FROM              1 (dumps)
&lt;/code&gt;
    &lt;code&gt;lazy import json
x = json  # Module-level access
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;LOAD_NAME                0 (json)
&lt;/code&gt;
    &lt;code&gt;lazy import json

def use_json():
    return json.dumps({})  # Function scope
&lt;/code&gt;
    &lt;p&gt;Before any calls:&lt;/p&gt;
    &lt;code&gt;LOAD_GLOBAL              0 (json)
LOAD_ATTR                2 (dumps)
&lt;/code&gt;
    &lt;p&gt;After several calls, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; specializes to &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;LOAD_GLOBAL_MODULE       0 (json)
LOAD_ATTR_MODULE         2 (dumps)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Lazy imports filter&lt;/head&gt;
    &lt;p&gt;This PEP adds two new functions to the &lt;code&gt;sys&lt;/code&gt; module to manage the lazy
imports filter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;sys.set_lazy_imports_filter(func)&lt;/code&gt;- Sets the filter function. The&lt;code&gt;func&lt;/code&gt;parameter must have the signature:&lt;code&gt;func(importer: str, name: str, fromlist: tuple[str, ...] | None) -&amp;gt; bool&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sys.get_lazy_imports_filter()&lt;/code&gt;- Returns the currently installed filter function, or&lt;code&gt;None&lt;/code&gt;if no filter is set.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The filter function is called for every potentially lazy import, and must return &lt;code&gt;True&lt;/code&gt; if the import should be lazy. This allows for fine-grained
control over which imports should be lazy, useful for excluding modules with
known side-effect dependencies or registration patterns.&lt;/p&gt;
    &lt;p&gt;The filter mechanism serves as a foundation that tools, debuggers, linters, and other ecosystem utilities can leverage to provide better lazy import experiences. For example, static analysis tools could detect modules with side effects and automatically configure appropriate filters. In the future (out of scope for this PEP), this foundation may enable better ways to declaratively specify which modules are safe for lazy importing, such as package metadata, type stubs with lazy-safety annotations, or configuration files. The current filter API is designed to be flexible enough to accommodate such future enhancements without requiring changes to the core language specification.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;import sys

def exclude_side_effect_modules(importer, name, fromlist):
    """
    Filter function to exclude modules with import-time side effects.

    Args:
        importer: Name of the module doing the import
        name: Name of the module being imported
        fromlist: Tuple of names being imported (for 'from' imports), or None

    Returns:
        True to allow lazy import, False to force eager import
    """
    # Modules known to have important import-time side effects
    side_effect_modules = {'legacy_plugin_system', 'metrics_collector'}

    if name in side_effect_modules:
        return False  # Force eager import

    return True  # Allow lazy import

# Install the filter
sys.set_lazy_imports_filter(exclude_side_effect_modules)

# These imports are checked by the filter
lazy import data_processor        # Filter returns True -&amp;gt; stays lazy
lazy import legacy_plugin_system  # Filter returns False -&amp;gt; imported eagerly

print('data_processor' in sys.modules)       # False - still lazy
print('legacy_plugin_system' in sys.modules) # True - loaded eagerly

# First use of data_processor triggers loading
result = data_processor.transform(data)
print('data_processor' in sys.modules)       # True - now loaded
&lt;/code&gt;
    &lt;head rend="h3"&gt;Global lazy imports control&lt;/head&gt;
    &lt;p&gt;The global lazy imports flag can be controlled through:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;-X lazy_imports=&amp;lt;mode&amp;gt;&lt;/code&gt;command-line option&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;PYTHON_LAZY_IMPORTS=&amp;lt;mode&amp;gt;&lt;/code&gt;environment variable&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;sys.set_lazy_imports(mode)&lt;/code&gt;function (primarily for testing)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Where &lt;code&gt;&amp;lt;mode&amp;gt;&lt;/code&gt; can be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;"default"&lt;/code&gt;(or unset): Only explicitly marked lazy imports are lazy&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;"enabled"&lt;/code&gt;: All module-level imports (except in&lt;code&gt;try&lt;/code&gt;or&lt;code&gt;with&lt;/code&gt;blocks and&lt;code&gt;import *&lt;/code&gt;) become potentially lazy&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;"disabled"&lt;/code&gt;: No imports are lazy, even those explicitly marked with&lt;code&gt;lazy&lt;/code&gt;keyword&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the global flag is set to &lt;code&gt;"enabled"&lt;/code&gt;, all imports at the global level
of all modules are potentially lazy except for those inside a &lt;code&gt;try&lt;/code&gt; or
&lt;code&gt;with&lt;/code&gt; block or any wild card (&lt;code&gt;from ... import *&lt;/code&gt;) import.&lt;/p&gt;
    &lt;p&gt;If the global lazy imports flag is set to &lt;code&gt;"disabled"&lt;/code&gt;, no potentially
lazy import is ever imported lazily, the import filter is never called, and
the behavior is equivalent to a regular &lt;code&gt;import&lt;/code&gt; statement: the import is
eager (as if the lazy keyword was not used).&lt;/p&gt;
    &lt;head rend="h2"&gt;Backwards Compatibility&lt;/head&gt;
    &lt;p&gt;Lazy imports are opt-in. Existing programs continue to run unchanged unless a project explicitly enables laziness (via &lt;code&gt;lazy&lt;/code&gt; syntax,
&lt;code&gt;__lazy_modules__&lt;/code&gt;, or an interpreter-wide switch).&lt;/p&gt;
    &lt;head rend="h3"&gt;Unchanged semantics&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Regular &lt;code&gt;import&lt;/code&gt;and&lt;code&gt;from ... import ...&lt;/code&gt;statements remain eager unless explicitly made potentially lazy by the local or global mechanisms provided.&lt;/item&gt;
      &lt;item&gt;Dynamic import APIs remain eager and unchanged: &lt;code&gt;__import__()&lt;/code&gt;and&lt;code&gt;importlib.import_module()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Import hooks and loaders continue to run under the standard import protocol when a lazy object is reified.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Observable behavioral shifts (opt-in only)&lt;/head&gt;
    &lt;p&gt;These changes are limited to bindings explicitly made lazy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Error timing. Exceptions that would have occurred during an eager import (for example &lt;code&gt;ImportError&lt;/code&gt;or&lt;code&gt;AttributeError&lt;/code&gt;for a missing member) now occur at the first use of the lazy name.&lt;quote&gt;# With eager import - error at import statement import broken_module # ImportError raised here # With lazy import - error deferred lazy import broken_module print("Import succeeded") broken_module.foo() # ImportError raised here on first use&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Side-effect timing. Import-time side effects in lazily imported modules occur at first use of the binding, not at module import time.&lt;/item&gt;
      &lt;item&gt;Import order. Because modules are imported on first use, the order in which modules are imported may differ from how they appear in code.&lt;/item&gt;
      &lt;item&gt;Presence in ``sys.modules``. A lazily imported module does not appear in &lt;code&gt;sys.modules&lt;/code&gt;until first use. After reification, it must appear in&lt;code&gt;sys.modules&lt;/code&gt;. If some other code eagerly imports the same module before first use, the lazy binding resolves to that existing (lazy) module object when it is first used.&lt;/item&gt;
      &lt;item&gt;Proxy visibility. Before first use, the bound name refers to a lazy proxy. Indirect introspection that touches the value may observe a proxy lazy object representation. After first use, the name is rebound to the real object and becomes indistinguishable from an eager import.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Thread-safety and reification&lt;/head&gt;
    &lt;p&gt;First use of a lazy binding follows the existing import-lock discipline. Exactly one thread performs the import and atomically rebinds the importing module’s global to the resolved object. Concurrent readers thereafter observe the real object.&lt;/p&gt;
    &lt;p&gt;Lazy imports are thread-safe and have no special considerations for free-threading. A module that would normally be imported in the main thread may be imported in a different thread if that thread triggers the first access to the lazy import. This is not a problem: the import lock ensures thread safety regardless of which thread performs the import.&lt;/p&gt;
    &lt;p&gt;Subinterpreters are supported. Each subinterpreter maintains its own &lt;code&gt;sys.lazy_modules&lt;/code&gt; and import state, so lazy imports in one subinterpreter
do not affect others.&lt;/p&gt;
    &lt;head rend="h3"&gt;Typing and tools&lt;/head&gt;
    &lt;p&gt;Type checkers and static analyzers may treat &lt;code&gt;lazy&lt;/code&gt; imports as ordinary
imports for name resolution. At runtime, annotation-only imports can be marked
&lt;code&gt;lazy&lt;/code&gt; to avoid startup overhead. IDEs and debuggers should be prepared to
display lazy proxies before first use and the real objects thereafter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security Implications&lt;/head&gt;
    &lt;p&gt;There are no known security vulnerabilities introduced by lazy imports.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Teach This&lt;/head&gt;
    &lt;p&gt;The new &lt;code&gt;lazy&lt;/code&gt; keyword will be documented as part of the language standard.&lt;/p&gt;
    &lt;p&gt;As this feature is opt-in, new Python users should be able to continue using the language as they are used to. For experienced developers, we expect them to leverage lazy imports for the variety of benefits listed above (decreased latency, decreased memory usage, etc) on a case-by-case basis. Developers interested in the performance of their Python binary will likely leverage profiling to understand the import time overhead in their codebase and mark the necessary imports as &lt;code&gt;lazy&lt;/code&gt;. In addition, developers can mark imports
that will only be used for type annotations as &lt;code&gt;lazy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Below is guidance on how to best take advantage of lazy imports and how to avoid incompatibilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When adopting lazy imports, users should be aware that eliding an import until it is used will result in side effects not being executed. In turn, users should be wary of modules that rely on import time side effects. Perhaps the most common reliance on import side effects is the registry pattern, where population of some external registry happens implicitly during the importing of modules, often via decorators but sometimes implemented via metaclasses or &lt;code&gt;__init_subclass__&lt;/code&gt;. Instead, registries of objects should be constructed via explicit discovery processes (e.g. a well-known function to call).&lt;quote&gt;# Problematic: Plugin registers itself on import # my_plugin.py from plugin_registry import register_plugin @register_plugin("MyPlugin") class MyPlugin: pass # In main code: lazy import my_plugin # Plugin NOT registered yet - module not loaded! # Better: Explicit discovery # plugin_registry.py def discover_plugins(): from my_plugin import MyPlugin register_plugin(MyPlugin) # In main code: plugin_registry.discover_plugins() # Explicit loading&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Always import needed submodules explicitly. It is not enough to rely on a different import to ensure a module has its submodules as attributes. Plainly, unless there is an explicit &lt;code&gt;from . import bar&lt;/code&gt;in&lt;code&gt;foo/__init__.py&lt;/code&gt;, always use&lt;code&gt;import foo.bar; foo.bar.Baz&lt;/code&gt;, not&lt;code&gt;import foo; foo.bar.Baz&lt;/code&gt;. The latter only works (unreliably) because the attribute&lt;code&gt;foo.bar&lt;/code&gt;is added as a side effect of&lt;code&gt;foo.bar&lt;/code&gt;being imported somewhere else.&lt;/item&gt;
      &lt;item&gt;Users who are moving imports into functions to improve startup time, should instead consider keeping them where they are but adding the &lt;code&gt;lazy&lt;/code&gt;keyword. This allows them to keep dependencies clear and avoid the overhead of repeatedly re-resolving the import but will still speed up the program.&lt;quote&gt;# Before: Inline import (repeated overhead) def process_data(data): import json # Re-resolved on every call return json.dumps(data) # After: Lazy import at module level lazy import json def process_data(data): return json.dumps(data) # Loaded once on first call&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Avoid using wild card (star) imports, as those are always eager.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Q: How does this differ from the rejected PEP 690?&lt;/p&gt;
    &lt;p&gt;A: PEP 810 takes an explicit, opt-in approach instead of PEP 690’s implicit global approach. The key differences are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Explicit syntax: &lt;code&gt;lazy import foo&lt;/code&gt;clearly marks which imports are lazy.&lt;/item&gt;
      &lt;item&gt;Local scope: Laziness only affects the specific import statement, not cascading to dependencies.&lt;/item&gt;
      &lt;item&gt;Simpler implementation: Uses proxy objects instead of modifying core dictionary behavior.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: What happens when lazy imports encounter errors?&lt;/p&gt;
    &lt;p&gt;A: Import errors (&lt;code&gt;ImportError&lt;/code&gt;, &lt;code&gt;ModuleNotFoundError&lt;/code&gt;, syntax errors) are
deferred until first use of the lazy name. This is similar to moving an import
into a function. The error will occur with a clear traceback pointing to the
first access of the lazy object.&lt;/p&gt;
    &lt;p&gt;The implementation provides enhanced error reporting through exception chaining. When a lazy import fails during reification, the original exception is preserved and chained, showing both where the import was defined and where it was first used:&lt;/p&gt;
    &lt;code&gt;Traceback (most recent call last):
  File "test.py", line 1, in &amp;lt;module&amp;gt;
    lazy import broken_module
ImportError: deferred import of 'broken_module' raised an exception during resolution

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "test.py", line 3, in &amp;lt;module&amp;gt;
    broken_module.foo()
    ^^^^^^^^^^^^^
  File "broken_module.py", line 2, in &amp;lt;module&amp;gt;
    1/0
ZeroDivisionError: division by zero
&lt;/code&gt;
    &lt;p&gt;Q: How do lazy imports affect modules with import-time side effects?&lt;/p&gt;
    &lt;p&gt;A: Side effects are deferred until first use. This is generally desirable for performance, but may require code changes for modules that rely on import-time registration patterns. We recommend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use explicit initialization functions instead of import-time side effects&lt;/item&gt;
      &lt;item&gt;Call initialization functions explicitly when needed&lt;/item&gt;
      &lt;item&gt;Avoid relying on import order for side effects&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Can I use lazy imports with &lt;code&gt;from ... import ...&lt;/code&gt; statements?&lt;/p&gt;
    &lt;p&gt;A: Yes, as long as you don’t use &lt;code&gt;from ... import *&lt;/code&gt;. Both &lt;code&gt;lazy import
foo&lt;/code&gt; and &lt;code&gt;lazy from foo import bar&lt;/code&gt; are supported. The &lt;code&gt;bar&lt;/code&gt; name will be
bound to a lazy object that resolves to &lt;code&gt;foo.bar&lt;/code&gt; on first use.&lt;/p&gt;
    &lt;p&gt;Q: Does &lt;code&gt;lazy from module import Class&lt;/code&gt; load the entire module or just
the class?&lt;/p&gt;
    &lt;p&gt;A: It loads the entire module, not just the class. This is because Python’s import system always executes the complete module file – there’s no mechanism to execute only part of a &lt;code&gt;.py&lt;/code&gt; file. When you first access
&lt;code&gt;Class&lt;/code&gt;, Python:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Loads and executes the entire &lt;code&gt;module.py&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;Extracts the &lt;code&gt;Class&lt;/code&gt;attribute from the resulting module object&lt;/item&gt;
      &lt;item&gt;Binds &lt;code&gt;Class&lt;/code&gt;to the name in your namespace&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is identical to eager &lt;code&gt;from module import Class&lt;/code&gt; behavior. The only
difference with lazy imports is that steps 1-3 happen on first use instead of
at the import statement.&lt;/p&gt;
    &lt;code&gt;# heavy_module.py
print("Loading heavy_module")  # This ALWAYS runs when module loads

class MyClass:
    pass

class UnusedClass:
    pass  # Also gets defined, even though we don't import it

# app.py
lazy from heavy_module import MyClass

print("Import statement done")  # heavy_module not loaded yet
obj = MyClass()                  # NOW "Loading heavy_module" prints
                                 # (and UnusedClass gets defined too)
&lt;/code&gt;
    &lt;p&gt;Key point: Lazy imports defer when a module loads, not what gets loaded. You cannot selectively load only parts of a module – Python’s import system doesn’t support partial module execution.&lt;/p&gt;
    &lt;p&gt;Q: What about type annotations and &lt;code&gt;TYPE_CHECKING&lt;/code&gt; imports?&lt;/p&gt;
    &lt;p&gt;A: Lazy imports eliminate the common need for &lt;code&gt;TYPE_CHECKING&lt;/code&gt; guards. You
can write:&lt;/p&gt;
    &lt;code&gt;lazy from collections.abc import Sequence, Mapping  # No runtime cost

def process(items: Sequence[str]) -&amp;gt; Mapping[str, int]:
    ...
&lt;/code&gt;
    &lt;p&gt;Instead of:&lt;/p&gt;
    &lt;code&gt;from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from collections.abc import Sequence, Mapping

def process(items: Sequence[str]) -&amp;gt; Mapping[str, int]:
    ...
&lt;/code&gt;
    &lt;p&gt;Q: What’s the performance overhead of lazy imports?&lt;/p&gt;
    &lt;p&gt;A: The overhead is minimal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero overhead after first use thanks to the adaptive interpreter optimizing the slow path away.&lt;/item&gt;
      &lt;item&gt;Small one-time cost to create the proxy object.&lt;/item&gt;
      &lt;item&gt;Reification (first use) has the same cost as a regular import.&lt;/item&gt;
      &lt;item&gt;No ongoing performance penalty unlike &lt;code&gt;importlib.util.LazyLoader&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarking with the pyperformance suite shows the implementation is performance neutral when lazy imports are not used.&lt;/p&gt;
    &lt;p&gt;Q: Can I mix lazy and eager imports of the same module?&lt;/p&gt;
    &lt;p&gt;A: Yes. If module &lt;code&gt;foo&lt;/code&gt; is imported both lazily and eagerly in the same
program, the eager import takes precedence and both bindings resolve to the
same module object.&lt;/p&gt;
    &lt;p&gt;Q: How do I migrate existing code to use lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Migration is incremental:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Identify slow-loading modules using profiling tools.&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;lazy&lt;/code&gt;keyword to imports that aren’t needed immediately.&lt;/item&gt;
      &lt;item&gt;Test that side-effect timing changes don’t break functionality.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;__lazy_modules__&lt;/code&gt;for compatibility with older Python versions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: What about star imports (&lt;code&gt;from module import *&lt;/code&gt;)?&lt;/p&gt;
    &lt;p&gt;A: Wild card (star) imports cannot be lazy - they remain eager. This is because the set of names being imported cannot be determined without loading the module. Using the &lt;code&gt;lazy&lt;/code&gt; keyword with star imports will be a syntax
error. If lazy imports are globally enabled, star imports will still be eager.&lt;/p&gt;
    &lt;p&gt;Q: How do lazy imports interact with import hooks and custom loaders?&lt;/p&gt;
    &lt;p&gt;A: Import hooks and loaders work normally. When a lazy object is first used, the standard import protocol runs, including any custom hooks or loaders that were in place at reification time.&lt;/p&gt;
    &lt;p&gt;Q: What happens in multi-threaded environments?&lt;/p&gt;
    &lt;p&gt;A: Lazy import reification is thread-safe. Only one thread will perform the actual import, and the binding is atomically updated. Other threads will see either the lazy proxy or the final resolved object.&lt;/p&gt;
    &lt;p&gt;Q: Can I force reification of a lazy import without using it?&lt;/p&gt;
    &lt;p&gt;A: Yes, accessing a module’s &lt;code&gt;__dict__&lt;/code&gt; will reify all lazy objects in that
module. Individual lazy objects can be resolved by calling their &lt;code&gt;get()&lt;/code&gt;
method.&lt;/p&gt;
    &lt;p&gt;Q: What’s the difference between &lt;code&gt;globals()&lt;/code&gt; and &lt;code&gt;mod.__dict__&lt;/code&gt; for lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Calling &lt;code&gt;globals()&lt;/code&gt; returns the module’s dictionary without reifying lazy
imports – you’ll see lazy proxy objects when accessing them through the
returned dictionary. However, accessing &lt;code&gt;mod.__dict__&lt;/code&gt; from external code
reifies all lazy imports in that module first. This design ensures:&lt;/p&gt;
    &lt;code&gt;# In your module:
lazy import json

g = globals()
print(type(g['json']))  # &amp;lt;class 'lazy_import'&amp;gt; - your problem

# From external code:
import sys
mod = sys.modules['your_module']
d = mod.__dict__
print(type(d['json']))  # &amp;lt;class 'module'&amp;gt; - reified for external access
&lt;/code&gt;
    &lt;p&gt;This distinction means adding lazy imports and calling &lt;code&gt;globals()&lt;/code&gt; is your
responsibility to manage, while external code accessing &lt;code&gt;mod.__dict__&lt;/code&gt;
always sees fully loaded modules.&lt;/p&gt;
    &lt;p&gt;Q: Why not use &lt;code&gt;importlib.util.LazyLoader&lt;/code&gt; instead?&lt;/p&gt;
    &lt;p&gt;A: &lt;code&gt;LazyLoader&lt;/code&gt; has significant limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires verbose setup code for each lazy import.&lt;/item&gt;
      &lt;item&gt;Has ongoing performance overhead on every attribute access.&lt;/item&gt;
      &lt;item&gt;Doesn’t work well with &lt;code&gt;from ... import&lt;/code&gt;statements.&lt;/item&gt;
      &lt;item&gt;Less clear and standard than dedicated syntax.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Will this break tools like &lt;code&gt;isort&lt;/code&gt; or &lt;code&gt;black&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;A: Tools will need updates to recognize the &lt;code&gt;lazy&lt;/code&gt; keyword, but the changes
should be minimal since the import structure remains the same. The keyword
appears at the beginning, making it easy to parse.&lt;/p&gt;
    &lt;p&gt;Q: How do I know if a library is compatible with lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Most libraries should work fine with lazy imports. Libraries that might have issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Those with essential import-time side effects (registration, monkey-patching).&lt;/item&gt;
      &lt;item&gt;Those that expect specific import ordering.&lt;/item&gt;
      &lt;item&gt;Those that modify global state during import.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When in doubt, test lazy imports with your specific use cases.&lt;/p&gt;
    &lt;p&gt;Q: What happens if I globally enable lazy imports mode and a library doesn’t work correctly?&lt;/p&gt;
    &lt;p&gt;A: Note: This is an advanced feature. You can use the lazy imports filter to exclude specific modules that are known to have problematic side effects:&lt;/p&gt;
    &lt;code&gt;import sys

def my_filter(importer, name, fromlist):
    # Don't lazily import modules known to have side effects
    if name in {'problematic_module', 'another_module'}:
        return False  # Import eagerly
    return True  # Allow lazy import

sys.set_lazy_imports_filter(my_filter)
&lt;/code&gt;
    &lt;p&gt;The filter function receives the importer module name, the module being imported, and the fromlist (if using &lt;code&gt;from ... import&lt;/code&gt;). Returning &lt;code&gt;False&lt;/code&gt;
forces an eager import.&lt;/p&gt;
    &lt;p&gt;Alternatively, set the global mode to &lt;code&gt;"disabled"&lt;/code&gt; via &lt;code&gt;-X
lazy_imports=disabled&lt;/code&gt; to turn off all lazy imports for debugging.&lt;/p&gt;
    &lt;p&gt;Q: Can I use lazy imports inside functions?&lt;/p&gt;
    &lt;p&gt;A: No, the &lt;code&gt;lazy&lt;/code&gt; keyword is only allowed at module level. For
function-level lazy loading, use traditional inline imports or move the import
to module level with &lt;code&gt;lazy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Q: What about forwards compatibility with older Python versions?&lt;/p&gt;
    &lt;p&gt;A: Use the &lt;code&gt;__lazy_modules__&lt;/code&gt; global for compatibility:&lt;/p&gt;
    &lt;code&gt;# Works on Python 3.15+ as lazy, eager on older versions
__lazy_modules__ = ['expensive_module', 'expensive_module_2']
import expensive_module
from expensive_module_2 import MyClass
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;__lazy_modules__&lt;/code&gt; attribute is a list of module name strings. When
an import statement is executed, Python checks if the module name being
imported appears in &lt;code&gt;__lazy_modules__&lt;/code&gt;. If it does, the import is
treated as if it had the &lt;code&gt;lazy&lt;/code&gt; keyword (becoming potentially lazy). On
Python versions before 3.15 that don’t support lazy imports, the
&lt;code&gt;__lazy_modules__&lt;/code&gt; attribute is simply ignored and imports proceed
eagerly as normal.&lt;/p&gt;
    &lt;p&gt;This provides a migration path until you can rely on the &lt;code&gt;lazy&lt;/code&gt; keyword. For
maximum predictability, it’s recommended to define &lt;code&gt;__lazy_modules__&lt;/code&gt;
once, before any imports. But as it is checked on each import, it can be
modified between &lt;code&gt;import&lt;/code&gt; statements.&lt;/p&gt;
    &lt;p&gt;Q: How do explicit lazy imports interact with PEP-649/PEP-749&lt;/p&gt;
    &lt;p&gt;A: If an annotation is not stringified, it is an expression that is evaluated at a later time. It will only be resolved if the annotation is accessed. In the example below, the &lt;code&gt;fake_typing&lt;/code&gt; module is only loaded when the user
inspects the &lt;code&gt;__annotations__&lt;/code&gt; dictionary. The &lt;code&gt;fake_typing&lt;/code&gt; module would
also be loaded if the user uses &lt;code&gt;annotationlib.get_annotations()&lt;/code&gt; or
&lt;code&gt;getattr&lt;/code&gt; to access the annotations.&lt;/p&gt;
    &lt;code&gt;lazy from fake_typing import MyFakeType
def foo(x: MyFakeType):
  pass
print(foo.__annotations__)  # Triggers loading the fake_typing module
&lt;/code&gt;
    &lt;p&gt;Q: How do lazy imports interact with &lt;code&gt;dir()&lt;/code&gt;, &lt;code&gt;getattr()&lt;/code&gt;, and
module introspection?&lt;/p&gt;
    &lt;p&gt;A: Accessing lazy imports through normal attribute access or &lt;code&gt;getattr()&lt;/code&gt;
will trigger reification. Calling &lt;code&gt;dir()&lt;/code&gt; on a module will reify all lazy
imports in that module to ensure the directory listing is complete. This is
similar to accessing &lt;code&gt;mod.__dict__&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;lazy import json

# Before any access
# json not in sys.modules

# Any of these trigger reification:
dumps_func = json.dumps
dumps_func = getattr(json, 'dumps')
dir(json)
# Now json is in sys.modules
&lt;/code&gt;
    &lt;p&gt;Q: Do lazy imports work with circular imports?&lt;/p&gt;
    &lt;p&gt;A: Lazy imports don’t automatically solve circular import problems. If two modules have a circular dependency, making the imports lazy might help only if the circular reference isn’t accessed during module initialization. However, if either module accesses the other during import time, you’ll still get an error.&lt;/p&gt;
    &lt;p&gt;Example that works (deferred access in functions):&lt;/p&gt;
    &lt;code&gt;# user_model.py
lazy import post_model

class User:
    def get_posts(self):
        # OK - post_model accessed inside function, not during import
        return post_model.Post.get_by_user(self.name)

# post_model.py
lazy import user_model

class Post:
    @staticmethod
    def get_by_user(username):
        return f"Posts by {username}"
&lt;/code&gt;
    &lt;p&gt;This works because neither module accesses the other at module level – the access happens later when &lt;code&gt;get_posts()&lt;/code&gt; is called.&lt;/p&gt;
    &lt;p&gt;Example that fails (access during import):&lt;/p&gt;
    &lt;code&gt;# module_a.py
lazy import module_b

result = module_b.get_value()  # Error! Accessing during import

def func():
    return "A"

# module_b.py
lazy import module_a

result = module_a.func()  # Circular dependency error here

def get_value():
    return "B"
&lt;/code&gt;
    &lt;p&gt;This fails because &lt;code&gt;module_a&lt;/code&gt; tries to access &lt;code&gt;module_b&lt;/code&gt; at import time,
which then tries to access &lt;code&gt;module_a&lt;/code&gt; before it’s fully initialized.&lt;/p&gt;
    &lt;p&gt;The best practice is still to avoid circular imports in your code design.&lt;/p&gt;
    &lt;p&gt;Q: Will lazy imports affect the performance of my hot paths?&lt;/p&gt;
    &lt;p&gt;A: After first use, lazy imports have zero overhead thanks to the adaptive interpreter. The interpreter specializes the bytecode (e.g., &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;
becomes &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;) which eliminates the lazy check on subsequent
accesses. This means once a lazy import is reified, accessing it is just as
fast as a normal import.&lt;/p&gt;
    &lt;code&gt;lazy import json

def use_json():
    return json.dumps({"test": 1})

# First call triggers reification
use_json()

# After 2-3 calls, bytecode is specialized
use_json()
use_json()
&lt;/code&gt;
    &lt;p&gt;You can observe the specialization using &lt;code&gt;dis.dis(use_json, adaptive=True)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;=== Before specialization ===
LOAD_GLOBAL              0 (json)
LOAD_ATTR                2 (dumps)

=== After 3 calls (specialized) ===
LOAD_GLOBAL_MODULE       0 (json)
LOAD_ATTR_MODULE         2 (dumps)
&lt;/code&gt;
    &lt;p&gt;The specialized &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt; and &lt;code&gt;LOAD_ATTR_MODULE&lt;/code&gt; instructions
are optimized fast paths with no overhead for checking lazy imports.&lt;/p&gt;
    &lt;p&gt;Q: What about &lt;code&gt;sys.modules&lt;/code&gt;? When does a lazy import appear there?&lt;/p&gt;
    &lt;p&gt;A: A lazily imported module does not appear in &lt;code&gt;sys.modules&lt;/code&gt; until it’s
reified (first used). Once reified, it appears in &lt;code&gt;sys.modules&lt;/code&gt; just like
any eager import.&lt;/p&gt;
    &lt;code&gt;import sys
lazy import json

print('json' in sys.modules)  # False

result = json.dumps({"key": "value"})  # First use

print('json' in sys.modules)  # True
&lt;/code&gt;
    &lt;p&gt;Q: Why you chose ``lazy`` as the keyword name?&lt;/p&gt;
    &lt;p&gt;A: Not “why”… memorize! :)&lt;/p&gt;
    &lt;head rend="h2"&gt;Alternate Implementation Ideas&lt;/head&gt;
    &lt;p&gt;Here are some alternative design decisions that were considered during the development of this PEP. While the current proposal represents what we believe to be the best balance of simplicity, performance, and maintainability, these alternatives offer different trade-offs that may be valuable for implementers to consider or for future refinements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leveraging a subclass of dict&lt;/head&gt;
    &lt;p&gt;Instead of updating the internal dict object to directly add the fields needed to support lazy imports, we could create a subclass of the dict object to be used specifically for Lazy Import enablement. This would still be a leaky abstraction though - methods can be called directly such as &lt;code&gt;dict.__getitem__&lt;/code&gt; and it would impact the performance of globals lookup in
the interpreter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternate keyword names&lt;/head&gt;
    &lt;p&gt;For this PEP, we decided to propose &lt;code&gt;lazy&lt;/code&gt; for the explicit keyword as it
felt the most familar to those already focused on optimizing import overhead.
We also considered a variety of other options to support explicit lazy
imports. The most compelling alternates were &lt;code&gt;defer&lt;/code&gt; and &lt;code&gt;delay&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rejected Ideas&lt;/head&gt;
    &lt;head rend="h3"&gt;Modification of the dict object&lt;/head&gt;
    &lt;p&gt;The initial PEP for lazy imports (PEP 690) relied heavily on the modification of the internal dict object to support lazy imports. We recognize that this data structure is highly tuned, heavily used across the codebase, and very performance sensitive. Because of the importance of this data structure and the desire to keep the implementation of lazy imports encapsulated from users who may have no interest in the feature, we’ve decided to invest in an alternate approach.&lt;/p&gt;
    &lt;p&gt;The dictionary is the foundational data structure in Python. Every object’s attributes are stored in a dict, and dicts are used throughout the runtime for namespaces, keyword arguments, and more. Adding any kind of hook or special behavior to dicts to support lazy imports would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prevent critical interpreter optimizations including future JIT compilation.&lt;/item&gt;
      &lt;item&gt;Add complexity to a data structure that must remain simple and fast.&lt;/item&gt;
      &lt;item&gt;Affect every part of Python, not just import behavior.&lt;/item&gt;
      &lt;item&gt;Violate separation of concerns – the hash table shouldn’t know about the import system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Past decisions that violated this principle of keeping core abstractions clean have caused significant pain in the CPython ecosystem, making optimization difficult and introducing subtle bugs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Placing the &lt;code&gt;lazy&lt;/code&gt; keyword in the middle of from imports&lt;/head&gt;
    &lt;p&gt;While we found &lt;code&gt;from foo lazy import bar&lt;/code&gt; to be a really intuitive placement
for the new explicit syntax, we quickly learned that placing the &lt;code&gt;lazy&lt;/code&gt;
keyword here is already syntactically allowed in Python. This is because
&lt;code&gt;from . lazy import bar&lt;/code&gt; is legal syntax (because whitespace does not
matter.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Placing the &lt;code&gt;lazy&lt;/code&gt; keyword at the end of import statements&lt;/head&gt;
    &lt;p&gt;We discussed appending lazy to the end of import statements like such &lt;code&gt;import
foo lazy&lt;/code&gt; or &lt;code&gt;from foo import bar, baz lazy&lt;/code&gt; but ultimately decided that
this approach provided less clarity. For example, if multiple modules are
imported in a single statement, it is unclear if the lazy binding applies to
all of the imported objects or just a subset of the items.&lt;/p&gt;
    &lt;head rend="h3"&gt;Returning a proxy dict from &lt;code&gt;globals()&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;An alternative to reifying on &lt;code&gt;globals()&lt;/code&gt; or exposing lazy objects would be
to return a proxy dictionary that automatically reifies lazy objects when
they’re accessed through the proxy. This would seemingly give the best of both
worlds: &lt;code&gt;globals()&lt;/code&gt; returns immediately without reification cost, but
accessing items through the result would automatically resolve lazy imports.&lt;/p&gt;
    &lt;p&gt;However, this approach is fundamentally incompatible with how &lt;code&gt;globals()&lt;/code&gt; is
used in practice. Many standard library functions and built-ins expect
&lt;code&gt;globals()&lt;/code&gt; to return a real &lt;code&gt;dict&lt;/code&gt; object, not a proxy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;exec(code, globals())&lt;/code&gt;requires a real dict.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;eval(expr, globals())&lt;/code&gt;requires a real dict.&lt;/item&gt;
      &lt;item&gt;Functions that check &lt;code&gt;type(globals()) is dict&lt;/code&gt;would break.&lt;/item&gt;
      &lt;item&gt;Dictionary methods like &lt;code&gt;.update()&lt;/code&gt;would need special handling.&lt;/item&gt;
      &lt;item&gt;Performance would suffer from the indirection on every access.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The proxy would need to be so transparent that it would be indistinguishable from a real dict in almost all cases, which is extremely difficult to achieve correctly. Any deviation from true dict behavior would be a source of subtle bugs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reifying lazy imports when &lt;code&gt;globals()&lt;/code&gt; is called&lt;/head&gt;
    &lt;p&gt;Calling &lt;code&gt;globals()&lt;/code&gt; returns the module’s namespace dictionary without
triggering reification of lazy imports. Accessing lazy objects through the
returned dictionary yields the lazy proxy objects themselves. This is an
intentional design decision for several reasons:&lt;/p&gt;
    &lt;p&gt;The key distinction: Adding a lazy import and calling &lt;code&gt;globals()&lt;/code&gt; is the
module author’s concern and under their control. However, accessing
&lt;code&gt;mod.__dict__&lt;/code&gt; from external code is a different scenario – it crosses
module boundaries and affects someone else’s code. Therefore, &lt;code&gt;mod.__dict__&lt;/code&gt;
access reifies all lazy imports to ensure external code sees fully realized
modules, while &lt;code&gt;globals()&lt;/code&gt; preserves lazy objects for the module’s own
introspection needs.&lt;/p&gt;
    &lt;p&gt;Technical challenges: It is impossible to safely reify on-demand when &lt;code&gt;globals()&lt;/code&gt; is called because we cannot return a proxy dictionary – this
would break common usages like passing the result to &lt;code&gt;exec()&lt;/code&gt; or other
built-ins that expect a real dictionary. The only alternative would be to
eagerly reify all lazy imports whenever &lt;code&gt;globals()&lt;/code&gt; is called, but this
behavior would be surprising and potentially expensive.&lt;/p&gt;
    &lt;p&gt;Performance concerns: It is impractical to cache whether a reification scan has been performed with just the globals dictionary reference, whereas module attribute access (the primary use case) can efficiently cache reification state in the module object itself.&lt;/p&gt;
    &lt;p&gt;Use case rationale: The chosen design makes sense precisely because of this distinction: adding a lazy import and calling &lt;code&gt;globals()&lt;/code&gt; is your
problem to manage, while having lazy imports visible in &lt;code&gt;mod.__dict__&lt;/code&gt;
becomes someone else’s problem. By reifying on &lt;code&gt;__dict__&lt;/code&gt; access but not on
&lt;code&gt;globals()&lt;/code&gt;, we ensure external code always sees fully loaded modules while
giving module authors control over their own introspection.&lt;/p&gt;
    &lt;p&gt;Note that three options were considered:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;or&lt;code&gt;mod.__dict__&lt;/code&gt;traverses and resolves all lazy objects before returning.&lt;/item&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;or&lt;code&gt;mod.__dict__&lt;/code&gt;returns the dictionary with lazy objects present.&lt;/item&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;returns the dictionary with lazy objects, but&lt;code&gt;mod.__dict__&lt;/code&gt;reifies everything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We chose the third option because it properly delineates responsibility: if you add lazy imports to your module and call &lt;code&gt;globals()&lt;/code&gt;, you’re responsible
for handling the lazy objects. But external code accessing your module’s
&lt;code&gt;__dict__&lt;/code&gt; shouldn’t need to know about your lazy imports – it gets fully
resolved modules.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We would like to thank Paul Ganssle, Yury Selivanov, Łukasz Langa, Lysandros Nikolaou, Pradyun Gedam, Mark Shannon, Hana Joo and the Python Google team, the Python team(s) @ Meta, the Python @ HRT team, the Bloomberg Python team, the Scientific Python community, everyone who participated in the initial discussion of PEP 690, and many others who provided valuable feedback and insights that helped shape this PEP.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;head rend="h2"&gt;Copyright&lt;/head&gt;
    &lt;p&gt;This document is placed in the public domain or under the CC0-1.0-Universal license, whichever is more permissive.&lt;/p&gt;
    &lt;p&gt;Source: https://github.com/python/peps/blob/main/peps/pep-0810.rst&lt;/p&gt;
    &lt;p&gt;Last modified: 2025-10-03 20:29:13 GMT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pep-previews--4622.org.readthedocs.build/pep-0810/"/><published>2025-10-03T18:24:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45466588</id><title>Jules, remote coding agent from Google Labs, announces API</title><updated>2025-10-04T07:32:19.283583+00:00</updated><content>&lt;doc fingerprint="2f0466b1ac650a9d"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;head rend="h2"&gt; Jules in the command line &lt;/head&gt; October 2, 2025 &lt;p&gt;Weâre launching Jules Tools, a new command-line interface designed to give you direct control over your AI coding agent, making it scriptable, customizable, and easy to integrate into your existing workflows.&lt;/p&gt;&lt;p&gt;Key Features:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Direct Control: Create tasks (jules remote new), list active sessions (jules remote list), and monitor Jules without leaving your command line.&lt;/item&gt;&lt;item&gt;Apply Patches Locally: Instantly pull work-in-progress code from an active Jules session and apply it to your local machine. This lets you test changes immediately, without waiting for a commit to GitHub.&lt;/item&gt;&lt;item&gt;Scriptable &amp;amp; Composable: Integrate Jules into your automations by piping in output from other tools like gh, jq, or cat.&lt;/item&gt;&lt;item&gt;Interactive Dashboard: For a more guided experience, launch the built-in terminal user interface (TUI) to create and manage tasks step-by-step.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to Install:&lt;/p&gt;&lt;p&gt;Install globally via npm: &lt;code&gt;npm install -g @google/jules&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Or run directly without a permanent installation: &lt;code&gt;npx @google/jules&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Starter Commands to Try:&lt;/p&gt;&lt;p&gt;See all available commands: &lt;code&gt;jules help&lt;/code&gt;&lt;/p&gt;&lt;p&gt;List all repos connected to Jules: &lt;code&gt;jules remote list --repo&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a new task in a specific repo: &lt;code&gt;jules remote new --repo torvalds/linux --session "write unit tests"&lt;/code&gt;&lt;/p&gt;&lt;head rend="h5"&gt;A Note for Google Workspace Users&lt;/head&gt;&lt;p&gt;Support for workspace users is coming later in October!&lt;/p&gt;&lt;p&gt;If you run into any issues, please share your experience with us via in-app feedback or on our Discord channel.&lt;/p&gt;&lt;head rend="h2"&gt; Jules gains memory! &lt;/head&gt; September 30, 2025 &lt;p&gt;Jules Memory for Repositories: Weâre excited to introduce a new Memory feature! Jules now has the ability to learn from your interactions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;How it works: During a task, Jules will save your preferences, nudges, and corrections.&lt;/item&gt;&lt;item&gt;The benefit: The next time you run the same or a similar task in that specific repository, Jules will reference its memory to better anticipate your needs and follow your established patterns, leading to more accurate results with less guidance.&lt;/item&gt;&lt;item&gt;Settings: You can toggle memory on or off for the repo in the repo settings page under âKnowledgeâ&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Tell Jules exactly what file to work on using file selector &lt;/head&gt; September 29, 2025 &lt;p&gt;You can now tell Jules exactly which files to work with for any given task. Use the new file selector to easily and precisely reference specific files.&lt;/p&gt;&lt;p&gt;This removes ambiguity and gives you more granular control over Julesâs actions, helping to tighten the context for your task.&lt;/p&gt;&lt;head rend="h2"&gt; Jules Acts on PR Feedback &lt;/head&gt; September 23, 2025 &lt;p&gt;Jules is now able to read and respond to your comments on pull requests!&lt;/p&gt;&lt;p&gt;When you start a review, Jules will add a ð emoji to each comment to let you know itâs been read. Based on your feedback, Jules will then push a commit with the requested changes.&lt;/p&gt;&lt;p&gt;For more control, you can switch to Reactive Mode in your global Jules UI settings. In this mode, Jules will only act on comments where you specifically mention &lt;code&gt;@Jules&lt;/code&gt;.&lt;/p&gt;&lt;head rend="h2"&gt; All Hands on Deck! &lt;/head&gt; September 19, 2025 &lt;p&gt;Ahoy, mateys! To celebrate International Talk Like a Pirate Day, weâve given Jules a temporary map to the treasure.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Jules Speaks Pirate: Youâll find your AI agentâs responses are a bit moreâ¦ swashbucklingâ¦ for today only.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Same Great Logic: Fear not! Beneath the eyepatch and Jolly Roger, itâs the same powerful coding engine ready to help you plunder that backlog and send bugs to Davy Jonesâ locker.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Image upload &lt;/head&gt; September 9, 2025 &lt;p&gt;You can now upload images when creating a task in Jules. Use this to show frontend bugs, design inspiration, UI mocks, or any visual context you want Jules to consider while generating code.&lt;/p&gt;&lt;p&gt;For now:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Only JPEG and PNG formats are supported.&lt;/item&gt;&lt;item&gt;You can uplaod as many images as you want, as long as the total size is under 5MB.&lt;/item&gt;&lt;item&gt;Image upload is only supported at task creation (weâre working on enabling it for follow-up prompts soon).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note: If your task involves using assets (e.g. logos) directly in code, those must still be committed to your GitHub repo.&lt;/p&gt;&lt;p&gt;Read more about Jules image support.&lt;/p&gt;&lt;head rend="h2"&gt; Stacked Diff &lt;/head&gt; September 4, 2025 &lt;p&gt;To improve the code review experience, weâve introduced a new stacked layout for the diff viewer. This change displays diffs for multiple files vertically on a single screen. The stacked view makes it easier to see related changes across your codebase at a glance, providing better context and speeding up your review process.&lt;/p&gt;&lt;p&gt;Changes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The diff viewer now stacks file changes vertically by default&lt;/item&gt;&lt;item&gt;You can also toggle back to the previous tabbed diff viewer&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Improved Jules Critic &lt;/head&gt; September 3, 2025 &lt;p&gt;Weâve shipped significant improvements to the Jules critic agent, making its feedback more insightful and reliable. To increase transparency and give you more insight into its evaluation process, you can now see the criticâs real-time analysis as it works.&lt;/p&gt;&lt;p&gt;Changes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The criticâs thought process is now visible in the UI, showing its step-by-step evaluation of the code in real-time.&lt;/item&gt;&lt;item&gt;The criticâs now incorporates more contextual information when making decisions, leading to more accurate and relevant feedback on potential bugs and logic flaws.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Jules Sample Prompts &lt;/head&gt; September 2, 2025 &lt;p&gt;To help new users get started with Jules, weâve added sample prompts to the home page. These static prompts provide examples of how to use Jules and can be added to the text box with a single click.&lt;/p&gt;&lt;p&gt;Changes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Sample prompts are now displayed on the home page for all users.&lt;/item&gt;&lt;item&gt;Clicking on a sample prompt will add the text of the prompt to the input box.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Render images in the diff viewer &lt;/head&gt; August 22, 2025 &lt;p&gt;Jules now intelligently renders images within the diff viewer, providing an immediate visual context for your modifications.&lt;/p&gt;&lt;p&gt;This means:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Instant Visual Feedback: When Jules generates images (like charts, diagrams, or web UI screenshots), youâll see the actual image in the diff, not just its code representation.&lt;/item&gt;&lt;item&gt;Streamlined Workflow: No need to switch between tools or download files to see the results. Jules keeps everything in one place.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Try it out! Ask Jules to render an output, like a graph based on data, and commit it to your repository. Youâll be able to see the generated image seamlessly within your diff viewer.&lt;/p&gt;&lt;head rend="h2"&gt; Export at any time &lt;/head&gt; August 15, 2025 &lt;p&gt;Youâre now in full control of when your code gets to GitHub. No need to wait for a task to finish or ask Jules to do it for you. At any point during a task, just click the GitHub icon in the top right to publish the current work-in-progress as a new branch or open a pull request. This gives you more flexibility and control to review, test, or take over whenever youâre ready.&lt;/p&gt;&lt;head rend="h2"&gt; Increasing the VM Size to 20GB &lt;/head&gt; August 15, 2025 &lt;p&gt;We heard your feedback about running into disk space limits on larger projects. To address this, weâve significantly increased the available disk space in the Jules VM to 20GB. This provides more room for large dependencies, build artifacts, and complex repositories, reducing disk-related failures so Jules can tackle bigger tasks. Happy Julesing!&lt;/p&gt;&lt;head rend="h2"&gt; Interactive Plan &lt;/head&gt; August 8, 2025 &lt;p&gt;Meet Interactive Plan. Instead of jumping straight to the solution, Jules will now read your codebase, ask clarifying questions, and work with you to refine the plan. This collaborative approach gives you more control and ensures youâre on the same page, leading to higher-quality code and a more reliable solution.&lt;/p&gt;&lt;p&gt;In summary:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Trigger the interactive plan from the dropdown when you start a task&lt;/item&gt;&lt;item&gt;Jules will start a brainstorm with you and ask clarifying questions&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Jules can surf the web &lt;/head&gt; August 8, 2025 &lt;p&gt;Jules can now proactively search the web for relevant content, documentation, or code snippets to help complete your tasks. This means Jules can get the information it needs, resulting in more accurate and successful task completion.&lt;/p&gt;&lt;p&gt;In Summary:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Jules can find the latest documentation for dependencies/libraries youâre using&lt;/item&gt;&lt;item&gt;Jules can proactively find examples or code snippets that can help inform its implementation&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note: web search works best when working on technical documentation. Queries like: âWhat is the latest news today?â are not supported.&lt;/p&gt;&lt;head rend="h2"&gt; Critic Agent &lt;/head&gt; August 8, 2025 &lt;p&gt;Great developers donât just write code, they question it. And now, so does Jules. Weâve built the Jules critic agent to ensure that every line of code isnât just functional, but robust, secure, and efficient. It acts as an internal peer reviewer, challenging every proposed change to elevate the quality of the final output.&lt;/p&gt;&lt;p&gt;Some high level notes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Critic-augmented generation: The Jules critic is integrated directly into the generation process. Every proposed change undergoes adversarial review before completion.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Improved code quality: The critic flags subtle bugs, missed edge cases, and inefficient code. Jules then uses this feedback to improve the patch in real-time.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;A new kind of review: The critic is not just another linter or test. It understands the intent and context behind code, similar to a human peer reviewer.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Built on research: This feature draws on research into multi-step, tool interactive critiquing and actor-critic reinforcement learning, where an âactorâ generates and a âcriticâ evaluates.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Jules can test web-apps and show you the results &lt;/head&gt; August 7, 2025 &lt;p&gt;Next time you are working on a front end project with Jules, ask it to verify its work and itâll render the website and send you back a screenshot!&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Ask Jules to complete a web development task and to verify the front end&lt;/item&gt;&lt;item&gt;Jules will send you a screenshot of the front end along with any code changes&lt;/item&gt;&lt;item&gt;The default Jules base image now includes Playwright for front end testing&lt;/item&gt;&lt;item&gt;Users can also add images in the form of public URLs for Jules to use as input&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Jules is out of beta! &lt;/head&gt; August 6, 2025 &lt;p&gt;Today we are thrilled to announce that Jules is no longer in beta! Since launch just two months ago, Jules has passed over 140k public commits. Thank you to our amazing beta users for all your support and feedback.&lt;/p&gt;&lt;p&gt;In addition, weâre launching our pricing plans to unlock higher task limits, along with a bunch of quality improvements in the Jules app and agent. Here are the details:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Get higher task limits through the Google AI Pro and Ultra plans. More details at Limits and Plans.&lt;/item&gt;&lt;item&gt;Jules now uses the power of Gemini 2.5 thinking when creating its plan, resulting in higher quality plans and more complete tasks&lt;/item&gt;&lt;item&gt;Numerous bug fixes so Jules gets stuck less, and is better at following your instructions in agents.md&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Environment snapshots for faster tasks &lt;/head&gt; August 5, 2025 &lt;p&gt;Jules now creates a snapshot of your environment when you add environment setup scripts. For complicated environment, users should see faster and more consistent task execution.&lt;/p&gt;&lt;p&gt;In summary:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Jules will now snapshot your environment when you provide an environment setup script&lt;/item&gt;&lt;item&gt;Snapshots are loaded automatically next time you run a task&lt;/item&gt;&lt;item&gt;This provides for faster task startups, especially for complex environments&lt;/item&gt;&lt;item&gt;You can find environment configuration by clicking the âcodebaseâ in the left hand panel, or by clicking the âconfigure environmentâ button in the task pane.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt; Open A PR directly from Jules &lt;/head&gt; August 4, 2025 &lt;p&gt;Closing the loop from task to merge ð¤&lt;/p&gt;&lt;p&gt;Jules can now open a pull request directly from the UI. After a task completes, just use the new dropdown next to the âPublish Branchâ button to open a PR. Jules will request to merge the newly published branch into main, streamlining your entire workflow. Less context switching, faster merging.&lt;/p&gt;&lt;head rend="h2"&gt; Added Bun runtime support &lt;/head&gt; July 18, 2025 &lt;p&gt;Jules now supports Bun. You can run tasks using Bun out of the box, no extra setup required. This expands compatibility for projects that use Bun instead of Node.&lt;/p&gt;&lt;p&gt;Read more about the jules base image and what tooling works with Jules.&lt;/p&gt;&lt;head rend="h2"&gt; Improved task controls and other ð UI delight &lt;/head&gt; July 3, 2025 &lt;list rend="ul"&gt;&lt;item&gt;Pause, resume, and delete tasksâwithout losing your sense of place. Available from sidebar and repo view. You can even quickly copy task urls!&lt;/item&gt;&lt;item&gt;Non-urgent task icons are now more recessive&lt;/item&gt;&lt;item&gt;Certain hover statesâwhich did not look goodâhave been toned back.&lt;/item&gt;&lt;item&gt;System messages have more consistent padding and borders&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Learn more about running a task.&lt;/p&gt;&lt;head rend="h2"&gt; Jules now listens to GitHub issues &lt;/head&gt; June 26, 2025 &lt;p&gt;Add the label âjulesâ to any GitHub issue to start a task in Jules. Thatâs itâlabel on, task live.&lt;/p&gt;&lt;p&gt;How to summon Jules:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Open a GitHub issue.&lt;/item&gt;&lt;item&gt;Click the gear next to âLabelsâ.&lt;/item&gt;&lt;item&gt;Add the label âjules.â&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Make sure the Jules GitHub App has access to your repo. After that, Jules takes it from there. Read more about running tasks in Jules!&lt;/p&gt;&lt;head rend="h2"&gt; Jules Agent Update: Faster, Smarter, More Reliable &lt;/head&gt; June 20, 2025 &lt;p&gt;Weâve shipped a big upgrade to the Jules agent under the hood.&lt;/p&gt;&lt;p&gt;Whatâs new:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Smarter context. Jules reads from AGENTS.md if itâs in your repo.&lt;/item&gt;&lt;item&gt;Improved performance. Tasks now complete fasterâno numbers to share just yet, but youâll feel it.&lt;/item&gt;&lt;item&gt;Significantly reduced punting. We tightened the loop to keep Jules moving forward.&lt;/item&gt;&lt;item&gt;More reliable setup. If youâve added an environment setup script, Jules now runs it consistently.&lt;/item&gt;&lt;item&gt;Better test habits. Jules is more likely to write and run tests on its own.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Check out the Getting Started guide to learn more about AGENTS.md support.&lt;/p&gt; June 18, 2025 &lt;p&gt;Weâve overhauled the Jules development environment to move beyond the default Ubuntu 24.04 LTS packages. This includes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Explicitly installing newer versions of key toolchains like Rust, Node, and Python, addressing long-standing version issues.&lt;/item&gt;&lt;item&gt;Adding finer-grained control over installation steps via custom scripts instead of relying solely on apt.&lt;/item&gt;&lt;item&gt;Introducing support for multiple runtimes, improved isolation, and version pinning to reduce drift and better match developer expectations.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;These changes unblock several issues developers encountered with outdated dependencies and improve alignment with modern project requirements.&lt;/p&gt;&lt;p&gt;Read about the Jules environment setup to learn more about whatâs pre-installed.&lt;/p&gt;&lt;head rend="h2"&gt; Customization and Efficiency Enhancements &lt;/head&gt; June 6, 2025 &lt;p&gt;Performance upgrades: Enjoy a smoother, faster Jules experience with recent under-the-hood improvements.&lt;/p&gt;&lt;p&gt;Quickly copy and download code: New copy and download buttons are now available in the code view pane, making it easier to grab your code directly from Jules.&lt;/p&gt;&lt;p&gt;Stay focused with task modals: Initiate multiple tasks seamlessly through a new modal option, allowing you to keep your context and workflow intact. Learn more about kicking off tasks.&lt;/p&gt;&lt;p&gt;Adjustable code panel: Customize your workspace by adjusting the width of the code panel to your preferred viewing experience.&lt;/p&gt;&lt;p&gt;Check out the docs to learn more about how to download code that Jules writes.&lt;/p&gt;&lt;head rend="h2"&gt; A faster, smoother and more reliable Jules &lt;/head&gt; May 30, 2025 &lt;p&gt;This week, our focus has been on improving reliability, fixing our GitHub integration, and scaling capacity.&lt;/p&gt;&lt;p&gt;Hereâs whatâs we shipped:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Updated our limits to 60 tasks per day, 5 concurrent.&lt;/item&gt;&lt;item&gt;We substantially improved the reliability of the GitHub sync. Export to GitHub should also be fixed on previously created tasks.&lt;/item&gt;&lt;item&gt;Weâve decreased the number of failure cases by 2/3&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Learn more about usage limits.&lt;/p&gt;&lt;head rend="h2"&gt; Improving Stablity &lt;/head&gt; May 22, 2025 &lt;p&gt;Weâve been heads down improving stability and fixing bugsâbig and smallâto make Jules faster, smoother, and more reliable for you.&lt;/p&gt;&lt;p&gt;Hereâs whatâs fixed:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Upgraded our queuing system and added more compute to reduce wait times during peak usage&lt;/item&gt;&lt;item&gt;Publish Branch button is now part of the summary UI in the activity feed so itâs easier to find&lt;/item&gt;&lt;item&gt;Bug vixes for task status and mobile&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Learn more about how to publish a branch on GitHub.&lt;/p&gt;&lt;head rend="h2"&gt; Jules is here &lt;/head&gt; May 19, 2025 &lt;p&gt;Today, weâre launching Jules, a new AI coding agent.&lt;/p&gt;&lt;p&gt;Jules helps you move faster by working asynchronously on tasks in your GitHub repo. It can fix bugs, update dependencies, migrate code, and add new features.&lt;/p&gt;&lt;p&gt;Once you give Jules a task, it spins up a fresh dev environment in a VM, installs dependencies, writes tests, makes the changes, runs the tests, and opens a pull request. Jules shows its work as it makes progress, so you never have to guess what code itâs writing, or what itâs thinking.&lt;/p&gt;&lt;p&gt;What Jules can do today&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Fix bugs with test verified patches&lt;/item&gt;&lt;item&gt;Handle version bumps and dependency upgrades&lt;/item&gt;&lt;item&gt;Perform scoped code transformations&lt;/item&gt;&lt;item&gt;Migrate code across languages or frameworks&lt;/item&gt;&lt;item&gt;Ship isolated, scoped, features&lt;/item&gt;&lt;item&gt;Open PRs with runnable code and test results&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Get started with the Jules documentation, and visit jules.google.com to run your first Jules task.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jules.google/docs/changelog/"/><published>2025-10-03T19:08:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45467166</id><title>AMD's EPYC 9355P: Inside a 32 Core Zen 5 Server Chip</title><updated>2025-10-04T07:32:19.148713+00:00</updated><content>&lt;doc fingerprint="6ef3d4bf70380ac1"&gt;
  &lt;main&gt;
    &lt;p&gt;High core count chips are headline grabbers. But maxing out the metaphorical core count slider isn’t the only way to go. Server players like Intel, AMD, and Arm aim for scalable designs that cover a range of core counts. Not all applications can take advantage of the highest core count models in their lineups, and per-core performance still matters.&lt;/p&gt;
    &lt;p&gt;AMD’s EPYC 9355P is a 32 core part. But rather than just being a lower core count part, the 9355P pulls levers to let each core count for more. First, it clocks up to 4.4 GHz. AMD has faster clocking chips in its server lineup, but 4.4 GHz is still a good bit higher than the 3.7 or 4.1 GHz that 128 or 192 core Zen 5 SKUs reach. Then, AMD uses eight CPU dies (CCDs) to house those 32 cores. Each CCD only has four cores enabled out of the eight physically present, but still has its full 32 MB of L3 cache usable. That provides a high cache capacity to core count ratio. Finally, each CCD connects to the IO die using a “GMI-Wide” setup, giving each CCD 64B/cycle of bandwidth to the rest of the system in both the read and write directions. GMI here stands for Global Memory Interconnect. Zen 5’s server IO die has 16 GMI links to support up to 16 CCDs for high core count parts, plus some xGMI (external) links to allow a dual socket setup. GMI-Wide uses two links per CCD, fully utilizing the IO die’s GMI links even though the EPYC 9355P only has eight CCDs.&lt;/p&gt;
    &lt;head rend="h1"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;Dell has kindly provided a PowerEdge R6715 for testing, and it came equipped with the aforementioned AMD EPYC 9355P along with 768 GB of DDR5-5200. The 12 memory controllers on the IO die provide a 768-bit memory bus, so the setup provides just under 500 GB/s of theoretical bandwidth. Besides providing a look into how a lower core count SKU behaves, we have BMC access which provides an opportunity to investigate different NUMA setups.&lt;/p&gt;
    &lt;p&gt;We’d also like to thank Zack and the rest of the fine folks at ZeroOne Technology for hosting the Dell PowerEdge R6715 at no cost to us.&lt;/p&gt;
    &lt;head rend="h1"&gt;Memory Subsystem and NUMA Characteristics&lt;/head&gt;
    &lt;p&gt;NPS1 mode stripes memory accesses across all 12 of the chip’s memory controllers, presenting software with a monolithic view of memory at the cost of latency. DRAM latency in that mode is slightly better than what Intel’s Xeon 6 achieves in SNC3 mode. SNC3 on Intel divides the chip into three NUMA nodes that correspond to its compute dies. The EPYC 9355P has good memory latency in that respect, but it falls behind compared to the Ryzen 9 9900X with DDR5-5600. Interconnects that tie more agents together tend to have higher latency. On AMD’s server platform, the Infinity Fabric network within the IO die has to connect up to 16 CCDs with 12 memory controllers and other IO, so the higher latency isn’t surprising.&lt;/p&gt;
    &lt;p&gt;Cache performance is similar across AMD’s desktop and server Zen 5 implementations, with the server variant only losing because of lower clock speeds. That’s not a surprise because AMD reuses the same CCDs on desktop and server products. But it does create a contrast to Intel’s approach, where client and server memory subsystems differ starting at L3. Intel trades L3 latency for capacity and the ability to a logical L3 instance across more cores.&lt;/p&gt;
    &lt;p&gt;Different NUMA configurations can subdivide EPYC 9355P, associating cores with the closest memory controllers to improve latency. NPS2 divides the chip into two hemispheres, and has 16 cores form a NUMA node with the six memory controllers on one half of the die. NPS4 divides the chip into quadrants, each with two CCDs and three memory controllers. Finally, the chip can present each CCD as a NUMA node. Doing so makes it easier to pin threads to cores that share a L3 cache, but doesn’t affect how memory is interleaved across channels. Memory addresses are still assigned to memory controllers according to the selected NPS1/2/4 scheme, which is a separate setting.&lt;/p&gt;
    &lt;p&gt;NPS2 and NPS4 only provide marginal latency improvements, and latency remains much higher than in a desktop platform. At the same time, crossing NUMA boundaries comes with little penalty. Apparently requests can traverse the huge IO die quite quickly, adding 20-30 ns at worst. I’m not sure what the underlying Infinity Fabric topology looks like, but the worst case unloaded cross-node latencies were under 140 ns. On Xeon 6, latency starts higher and can climb over 180 ns when cores on one compute die access memory controllers on the compute die at the other end of the chip.&lt;/p&gt;
    &lt;p&gt;EPYC 9355P can get close to theoretical memory bandwidth in any of the three NUMA nodes, as long as code keeps accesses within each node. NPS2 and NPS4 offer slightly better bandwidth, at the cost of requiring code to be NUMA aware. I tried to cause congestion on Infinity Fabric by having cores on each NUMA node access memory on another. That does lower achieved bandwidth, but not by a huge amount.&lt;/p&gt;
    &lt;p&gt;An individual NPS4 node achieves 117.33 GB/s to its local memory pool, and just over 107 GB/s to the memory on the other three nodes. The bandwidth penalty is minor, but a bigger potential pitfall is lower bandwidth to each NUMA node’s memory pool. Two CCDs can draw more bandwidth than the three memory controllers they’re associated with. Manually distributing memory accesses across NUMA nodes can improve bandwidth for a workload contained within one NUMA node’s cores. But doing so in practice may be an intricate exercise.&lt;/p&gt;
    &lt;p&gt;In general, EPYC 9355P has very mild NUMA characteristics and little penalty associated with running the chip in NPS1 or NPS2 mode. I imagine just using NPS1 mode would work well enough in the vast majority of cases, with little performance to be gained from carrying out NUMA optimizations.&lt;/p&gt;
    &lt;head rend="h1"&gt;Looking into GMI-Wide&lt;/head&gt;
    &lt;p&gt;GMI-Wide is AMD’s attempt to address bandwidth pinch points between CCDs and the rest of the system. With GMI-Wide, a single CCD can achieve 99.8 GB/s of read bandwidth, significantly more than the 62.5 GB/s from a Ryzen 9 9900X CCD with GMI-Narrow. GMI-Wide also allows better latency control under high bandwidth load. The Ryzen 9 9900X suffers from a corner case where a single core pulling maximum bandwidth can saturate the GMI-Narrow link and starve out another latency sensitive thread. That sends latency to nearly 500 ns, as observed by a latency test thread sharing a CCD with a thread linearly traversing an array. Having more threads generate bandwidth load seems to make QoS mechanisms kick in, which slightly reduces bandwidth throughput but brings latency back under control.&lt;/p&gt;
    &lt;p&gt;I previously wrote about loaded memory latency on the Ryzen 9 9950X when testing the system remotely, and thought it controlled latency well under high bandwidth load. But back then, George (Cheese) set that system up with very fast DDR5-8000 along with a higher 2.2 GHz FCLK. A single core was likely unable to monopolize off-CCD bandwidth in that setup, avoiding the corner case seen on my system. GMI-Wide increases off-CCD bandwidth by a much larger extent and has a similar effect. Under increasing bandwidth load, GMI-WIde can both achieve more total bandwidth and control latency better than its desktop single-link counterpart.&lt;/p&gt;
    &lt;p&gt;A read-modify-write pattern gets maximum bandwidth from GMI-Wide by exercising both the read and write paths. It doesn’t scale perfectly, but it’s a substantial improvement over using only reads or writes. A Ryzen 9 9900X CCD can theoretically get 48B/cycle to the IO die with a 2:1 read-to-write ratio. I tried modifying every other cacheline to achieve this ratio, but didn’t get better bandwidth probably because the memory controller is limited by a 32B/cycle link to Infinity Fabric. However, mixing in writes does get rid of the single bandwidth thread corner case, possibly because a single thread doesn’t saturate the 32B/cycle read link when mixing reads and writes.&lt;/p&gt;
    &lt;p&gt;On the desktop platform, latency under high load gets worse possibly because writes contend with reads at the DRAM controller. The DDR bus is unidirectional, and must waste cycles on “turnarounds” to switch between read and write mode. Bandwidth isn’t affected, probably because the Infinity Fabric bottleneck leaves spare cycles at the memory controller, which can absorb those turnarounds. However, reads from the latency test thread may be delayed while the memory controller drains writes before switching the bus back to read mode.&lt;/p&gt;
    &lt;p&gt;On the EPYC 9355P in NPS1 mode, bandwidth demands from a single GMI-Wide CCD leave plenty of spare cycles across the 12 memory controllers, so there’s little latency or bandwidth penalty when mixing reads and writes. The same isn’t true in NPS4 mode, where a GMI-Wide link can outmatch a NPS4 node’s three memory controllers. Everything’s fine with just reads, which actually benefit possibly because of lower latency and not having to traverse as much of the IO die. But with a read-modify-write pattern, bandwidth drops from 134 GB/s in NPS1 mode to 96.6 GB/s with NPS4. Latency gets worse too, rising to 248 ns. Again, NPS4 is something to be careful with, particularly if applications might require high bandwidth from a small subset of cores.&lt;/p&gt;
    &lt;head rend="h1"&gt;SPEC CPU2017&lt;/head&gt;
    &lt;p&gt;From a single thread perspective, the EPYC 9355P falls some distance behind the Ryzen 9 9900X. Desktop CPUs are designed around single threaded performance, so that’s to be expected. But with boost turned off on the desktop CPU to match clock speeds, performance is surprisingly close. Higher memory latency still hurts the EPYC 9355P, but it’s within striking distance.&lt;/p&gt;
    &lt;p&gt;NUMA modes barely make any difference. NPS4 technically wins, but by an insignificant margin. The latency advantage was barely measurable anyway. Compared to the more density optimized Graviton 4 and Xeon 6 6975P-C, the EPYC 9355P delivers noticeably better single threaded performance.&lt;/p&gt;
    &lt;p&gt;CCD-level bandwidth pinch points are worth a look too, since that’s traditionally been a distinguishing factor between AMD’s EPYC and more logically monolithic designs. Here, I’m filling a quad core CCD by running SPEC CPU2017’s rate tests with eight copies. I did the same on the Ryzen 9 9900X, pinning the eight copies to four cores and leaving the CCD’s other two cores unused. I bound the test to a single NUMA node on all tested setups.&lt;/p&gt;
    &lt;p&gt;SPEC’s floating point suite starts to tell a different story now. Several tests within the floating point suite are bandwidth hungry even from a single core. 549.fotonik3d for example pulled 28.23 GB/s from Meteor Lake’s memory controller when I first went through SPEC CPU2017’s tests. Running eight copies in parallel would multiply memory bandwidth demands, and that’s where server memory subsystems shine.&lt;/p&gt;
    &lt;p&gt;In 549.fotonik3d, high bandwidth demands make the Ryzen 9 9900X’s unloaded latency advantage irrelevant. The 9900X even loses to Redwood Cove cores on Xeon 6. The EPYC 9355P does very well in this test against both the 9900X and Xeon 6. Intel’s interconnect strategy tries to keep the chip logically monolithic and doesn’t have pinch points at cluster boundaries. But each core on Xeon 6 can only get to ~33 GB/s of DRAM bandwidth at best, using an even mix of reads and writes. AMD’s GMI-Wide can more than match that, and Intel’s advantage doesn’t show through in this scenario. However, Intel does have a potential advantage against more density focused AMD SKUs where eight cores sit in front of a narrower link.&lt;/p&gt;
    &lt;p&gt;NPS4 is also detrimental to the EPYC 9355P’s performance in this test. It only provides a minimal latency benefit at the cost of lower per-node bandwidth. The bandwidth part seems to hurt here, and taking the extra latency of striping accesses across 6 or 12 memory controllers gives a notable performance improvement.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;Core count isn’t the last word in server design. A lot of scenarios are better served by lower core count parts. Applications might not scale to fill a high core count chip. Bandwidth bound workloads might not benefit from adding cores. Traditionally lower core count server chips just traded core counts for higher clock speeds. Today, chips like the EPYC 9355P do a bit more, using both wider CCD-to-IOD links and more cache to maximize per-core performance.&lt;/p&gt;
    &lt;p&gt;Looking at EPYC 9355P’s NUMA characteristics reveals very consistent memory performance across NUMA modes. Intel’s Xeon 6 may be more monolithic from a caching point of view, but AMD’s DRAM access performance feels more monolithic than Intel’s. AMD made a tradeoff back in the Zen 2 days where they took lower local memory latency in exchange for more even memory performance across the socket. Measured latencies on EPYC 9355P are a bit higher than figures on the Zen 2 slide above. DDR5 is higher latency, and the Infinity Fabric topology is probably more complex these days to handle more CCDs and memory channels.&lt;/p&gt;
    &lt;p&gt;But the big picture remains. AMD’s Turin platform handles well in NPS1 mode, and cross-node penalties are low in NPS2/NPS4 modes. Those characteristics likely carry over across the Zen 5 EPYC SKU stack. It’s quite different from Intel’s Xeon 6 platform, which places memory controllers on compute dies like Zen 1 did. For now, AMD’s approach seems to be better at the DRAM level. Intel’s theoretical latency advantage in SNC3 mode doesn’t show through, and AMD gets to reap the benefits of a hub-and-spoke model while not getting hit where it should have downsides.&lt;/p&gt;
    &lt;p&gt;AMD seems to have found a good formula back in the Zen 2 days, and they’re content with reinforcing success. Intel is furiously iterating to find a setup that preserves a single level, logically monolithic interconnect while scaling well across a range of core counts. And of course, there’s Arm chips, which generally lean towards a single level monolithic interconnect too. It’ll be interesting to watch what all of these players do going forward as they continue to iterate and refine their designs.&lt;/p&gt;
    &lt;p&gt;And again, we’d like to thank both Dell and ZeroOne for, respectively, providing and hosting this PowerEdge R6715 without both of whom this article wouldn’t have been possible.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/amds-epyc-9355p-inside-a-32-core"/><published>2025-10-03T20:01:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45467500</id><title>Offline card payments should be possible no later than 1 July 2026</title><updated>2025-10-04T07:32:18.614517+00:00</updated><content>&lt;doc fingerprint="cb8cfa25092dfffd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Offline card payments should be possible no later than 1 July 2026&lt;/head&gt;
    &lt;p&gt;Press release The Riksbank and representatives from the payment market have today reached an agreement to increase the possibility to make offline card payments for essential goods. The agreement is an important step in the work to strengthen Sweden's payment preparedness and increase resilience to disruptions in the digital payments system. The goal is for the measures to be in place no later than 1 July 2026.&lt;/p&gt;
    &lt;p&gt;“In Sweden, we pay digitally to a large degree and the use of cash is low. The general public being able to pay by card for example for food and medicines even in the event of a serious breakdown in data communication, that is offline, is a milestone in our intensified efforts to strengthen emergency preparedness”, says Governor Erik Thedéen.&lt;/p&gt;
    &lt;p&gt;The agreement describes the measures that participants in Swedish card payments – card issuers, card networks, card acquirers, the retail sector and the Riksbank – will implement to increase the possibility of offline payments by card. For instance, financial agents will adapt their regulatory frameworks, and the retail trade will introduce technological solutions. The Riksbank is leading this work and is responsible for monitoring its implementation.&lt;/p&gt;
    &lt;p&gt;“We are very pleased that all participants involved are taking responsibility for strengthening Sweden's payment readiness. Some are covered by the Riksbank's regulations, but far from all. We regard the fact that so many are nevertheless choosing to contribute as very positive for Sweden's overall civil preparedness”, concludes Erik Thedéen.&lt;/p&gt;
    &lt;p&gt;The online function shall apply to physical payment cards and accompanying PIN code when purchasing essential goods such as food, medicine and fuel. The Riksbank will continue its work on enabling offline payments for other payment methods after 1 July 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.riksbank.se/en-gb/press-and-published/notices-and-press-releases/press-releases/2025/offline-card-payments-should-be-possible-no-later-than-1-july-2026/"/><published>2025-10-03T20:36:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45467543</id><title>Interstellar Object 3I/Atlas Passed Mars Last Night</title><updated>2025-10-04T07:32:18.348900+00:00</updated><content>&lt;doc fingerprint="fc5cf18231e00f9b"&gt;
  &lt;main&gt;
    &lt;p&gt;The world’s 3rd known interstellar object – 3I/ATLAS – has made its closest approach to Mars. The approach took place at 4 UTC on October 3, 2025 (11 p.m. CDT on October 2). At that time, the comet was approximately 18 million miles (29 million kilometers) from Mars. It was the object’s closest approach to any planet during its one-time journey through our solar system.&lt;/p&gt;
    &lt;p&gt;As of this writing (10 UTC on October 3), we have not seen any new images from the pass. But multiple space agencies, including NASA and the European Space Agency (ESA), are coordinating observations using various spacecraft and orbiters around Mars. Instruments on ESA’s Mars Express and ExoMars Trace Gas Orbiter, as well as NASA’s Mars Reconnaissance Orbiter, are focusing on capturing detailed data from this interstellar visitor. In an October 2 story from AP, Marcia Dunn reported:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Both of the European Space Agency’s satellites around Mars are already aiming their cameras at the comet, which is only the 3rd interstellar object known to have passed our way. NASA’s satellite and rovers at the red planet are also available to assist in the observations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Previously, Marshall Eubanks of Space Initiatives had said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;During the Mars close approach, the Mars Reconnaissance Orbiter will observe 3I with HiRISE, observing between 1 – 4 a.m. on October 2, and the CaSSIS camera on ESA’s Trace Gas Orbiter and the Mars Express’ High Resolution Stereo Camera will be observing on October 3.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;3I/ATLAS will reach perihelion, its closest point to the sun, on October 29, 2025. Its perihelion distance will be roughly 1.36 astronomical units (AU) from the sun – just inside the orbit of Mars.&lt;/p&gt;
    &lt;p&gt;If you’re interested in tracking the object, NASA’s Eyes on the Solar System tool offers interactive simulations of its path. Also, NASA just launched a new page devoted to 3I/ATLAS. And the latest updates on ESA observations are here&lt;/p&gt;
    &lt;p&gt;Please note that 3I/ATLAS will not be visible to the unaided eye from Earth at this Mars approach, or at any time. It will be possible to view the object with 8-inch (20 cm) or larger telescopes … but the best time for that won’t come until November. If you spot it then, you’ll be in good company. Between November 2 and 25, ESA’s Jupiter Icy Moons Explorer (Juice) will be observing the comet with various instruments. As Juice looks towards 3I/ATLAS so soon after its closest approach to the sun, it is likely to have the best view of the comet in a very active state, with a bright halo around its nucleus and a long tail stretching out behind it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interstellar object 3I/ATLAS: A look backward&lt;/head&gt;
    &lt;p&gt;Where did 3I/ATLAS come from? We know it came from the Sagittarius direction in our sky; that is, it came from the direction of the center of our Milky Way galaxy. But there are billions of stars in that direction. Which one is the home system of this object?&lt;/p&gt;
    &lt;p&gt;There have been many studies and ideas. One team of scientists, led by Xabier Pérez-Couto of the University of A Coruña in Spain, traced the path of interstellar object 3I/ATLAS back 10 million years. The astronomers were seeking its origin star, or any stars that might have perturbed its path as it traveled from its point of origin to our solar system.&lt;/p&gt;
    &lt;p&gt;The researchers examined 3I/ATLAS’s trajectory with the help of the Gaia space observatory’s data on stars. For 12 years, Gaia collected data on billions of stars in our Milky Way galaxy, precisely noting their positions again and again and thereby determining their motions. These astronomers’ calculations took them more than 100 million astronomical units (AU, or Earth-sun units) from our solar system. With these data in hand, researchers said they identified 93 nominal “encounters” for 3I/ATLAS, 62 of which were “significant.” Yet, they found that none of those encounters produced any meaningful perturbation of ATLAS’s orbit.&lt;/p&gt;
    &lt;p&gt;So, in other words, all of those 93 (or 62) encounters happened too fast, with the stars too far from 3I/ATLAS to meaningfully impact its trajectory. In the end, they didn’t find a star along 3I/ATLAS’s path that might have been responsible for bringing this 3rd-known interstellar object to us.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tracing 3I/ATLAS’ path, a daunting task&lt;/head&gt;
    &lt;p&gt;And, as you might imagine, tracing 3I/ATLAS’ path backward through the galaxy is a daunting task. That’s in part because small uncertainties in orbits and stellar motions grow rapidly over time. But based on the researchers’ analyses of the interstellar object’s vertical motion in the galaxy (its path is known to weave up and down in the galactic disk), they concluded that it likely originated from the Milky Way’s thin disk, not its thick disk as was mentioned some months ago. The thin disk contains somewhat younger objects than the thick disk. But the researchers’ paper said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[3I/ATLAS] may nonetheless be an old object, consistent with ejection from a long-lived primordial planetesimal disk in an early-formed system.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The scientists published their not-yet peer-reviewed paper on arXiv on September 10, 2025.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unlocking galactic mysteries with 3I/ATLAS&lt;/head&gt;
    &lt;p&gt;3I/ATLAS is thought to have been drifting through interstellar space for many billions of years before encountering our solar system. Pérez-Couto and team said that the interstellar comet is a:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;… key probe of the galactic population of icy planetesimals.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, the formation of solar systems is a messy process. In a solar system’s earliest days, rocks and pockets of gas and dust bang into each other and get swept up into clumps, which eventually get big enough to begin gathering yet more rocks, gas and dust to themselves via the force of gravity. Thus, planets come to be, astronomers think. According to theories of planet formation, clearing processes are also common, and those sometimes involve material – often the outer, icy regions of debris – getting ejected from a system altogether. As the paper said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;… interstellar space should be filled with planetesimals.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Other possibilities&lt;/head&gt;
    &lt;p&gt;Plus, there are other ways these interstellar interlopers might have achieved their lonely paths through our Milky Way galaxy. The possibilities range from close passages of other stars to tidal fragmentation of comets. So, as the paper said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Identifying the origin of interstellar objects is key to understanding planet formation efficiency, the distribution of volatiles and organics in the galaxy, and the dynamical pathways by which planetary systems evolve.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;All that from a small chunk of icy stuff (we know it’s icy in part because 3I/ATLAS has formed a tail, as icy comets do)!&lt;/p&gt;
    &lt;head rend="h3"&gt;EarthSky interview with Colin Orion Chandler&lt;/head&gt;
    &lt;p&gt;On August 7, 2025, NASA shared an updated estimate of the size of the object’s nucleus, or core. Shortly after the object was first identified on July 1, 2025, 3I/ATLAS was estimated to have a diameter of about 12 miles (20 km). Then in late July – using data from the new Vera C. Rubin Observatory in Chile – the size estimate dropped to 6 miles (10 km). The latest analysis uses data from the NASA/ESA Hubble Space Telescope. It reduces the estimated diameter of 3I/ATLAS’s nucleus still further, to 3.5 miles (5.6 km).&lt;/p&gt;
    &lt;p&gt;And, the astronomers using Hubble data said, the object could be even smaller, as small as 1,050 feet (320 meters) across!&lt;/p&gt;
    &lt;p&gt;EarthSky’s Deborah Byrd interviewed Colin Orion Chandler of the DiRAC Institute of the University of Washington about size estimates for 3I/ATLAS. Watch in the player below, or on YouTube.&lt;/p&gt;
    &lt;p&gt;By the way, the two previously known interstellar objects are 1I/ ‘Oumuamua and 2I/Borisov. ‘Oumuamua’s size is thought to be about 656 feet (200 meters) across at its widest (you’ll recall it has an elongated shape). And Borisov is thought to be less than 3,280 feet (1 km) across.&lt;/p&gt;
    &lt;head rend="h3"&gt;An early EarthSky interview with Matthew Hopkins&lt;/head&gt;
    &lt;p&gt;Shortly after the discovery of 3I/ATLAS – on July 1, 2025 – astronomers were saying it was likely the oldest comet we’ve ever seen. That claim came from University of Oxford astronomer Matthew Hopkins, whose analysis suggested 3I/ATLAS might be more than 7 billion years old, predating our solar system by more than 3 billion years! Hear him explain in the player below, or on YouTube.&lt;/p&gt;
    &lt;head rend="h3"&gt;EarthSky interview with Colin Snodgrass&lt;/head&gt;
    &lt;p&gt;Scientists first spotted 3I/ATLAS in early July 2025. And since then, one question has been asked countless times: will we send out a spacecraft to take a closer look? EarthSky’s Will Triggs spoke to University of Edinburgh astronomer Colin Snodgrass on August 21, 2025, to find out the answer. Colin essentially said, no, we don’t have time to organize a space mission specifically for 3I/ATLAS. But he talked about a future mission, the European Space Agency’s Comet Interceptor. This upcoming spacecraft will be primed to intercept future interstellar objects. Watch Will’s interview with Colin in the player below, or on YouTube.&lt;/p&gt;
    &lt;p&gt;It’s worth noting that the behavior of 3I/ATLAS is much like the signature of previously seen sun-bound comets originating within our solar system. But 3I/ATLAS is moving fast. In fact, it’s traveling through our solar system at roughly 130,000 miles per hour (210,000 kph). That’s the highest velocity ever recorded for a solar system visitor.&lt;/p&gt;
    &lt;head rend="h3"&gt;How they spotted interstellar object 3I/ ATLAS&lt;/head&gt;
    &lt;p&gt;The Asteroid Terrestrial-impact Last Alert System (ATLAS) – a system of survey telescopes – detected our new interstellar visitor on July 1, 2025. And the Minor Planet Center confirmed its interstellar nature the following day (July 2, 2025), naming it 3I/ATLAS (or C/2025 N1). The “3I” means it’s the 3rd interstellar visitor that we’ve found. Its trajectory and speed revealed it as an object not from our solar system, but from another star system.&lt;/p&gt;
    &lt;p&gt;The Hubble Space Telescope imaged the object on July 21, 2025. See the post from Bluesky below.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hubble Space Telescope images of interstellar comet 3I/ATLAS are out! These were taken 5 hours ago. Plenty of cosmic rays peppering the images, but the comet's coma looks very nice and puffy. Best of luck to the researchers trying to write up papers for this… archive.stsci.edu/proposal_sea… ?&lt;/p&gt;
      &lt;p&gt;— astrafoxen (@astrafoxen.bsky.social) July 21, 2025 at 4:28 PM&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;It’s still heading sunward&lt;/head&gt;
    &lt;p&gt;Our new visitor will get its closest to the sun – at about 2 astronomical units (AU), or twice as far as Earth is from the sun – in October. As it reaches perihelion – its closest point to the sun – it will be traveling at almost 15,500 miles per hour (25,000 kph).&lt;/p&gt;
    &lt;p&gt;The speedy nature of Comet 3I/ATLAS is another indication of its interstellar nature. It has to be moving at a blistering pace in order to escape the sun’s gravitational pull.&lt;/p&gt;
    &lt;p&gt;Marshall Eubanks, a physicist and Very-long-baseline interferometry radio astronomer and co-founder of Space Initiatives, said the comet will come within about 0.4 AU of Mars in October. That would make it just barely observable by the Mars Reconnaissance Orbiter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Morning star charts here&lt;/head&gt;
    &lt;p&gt;After Comet 3I/ATLAS makes its close approach to the sun, you can find it in the morning sky.&lt;/p&gt;
    &lt;p&gt;Bottom line: Interstellar object 3I/ATLAS swept closest to Mars at 11 p.m. CDT on October 2 (4 UTC on October 3). Read about plans to observe it with spacecraft.&lt;lb/&gt; Via:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://earthsky.org/space/new-interstellar-object-candidate-heading-toward-the-sun-a11pl3z/"/><published>2025-10-03T20:40:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45467717</id><title>TrueVault (YC W14) Is Hiring a BDR (Ex-ECommerce Manager)</title><updated>2025-10-04T07:32:17.853083+00:00</updated><content>&lt;doc fingerprint="2d0ed018ae96a6ad"&gt;
  &lt;main&gt;
    &lt;p&gt;We make Privacy Software for SMBs.&lt;/p&gt;
    &lt;p&gt;We’re looking for a Business Development Rep with real DTC eCommerce experience to help brands simplify privacy compliance. If you’ve been an eCommerce Manager before and want to pivot into SaaS sales, this is your chance to turn that experience into a revenue-driving role at a Y Combinator–backed startup.&lt;/p&gt;
    &lt;p&gt;Privacy is one of the most fundamental rights we have.&lt;/p&gt;
    &lt;p&gt;At TrueVault, we believe that when businesses have access to tools that make compliance simple, respecting consumer privacy becomes the obvious choice — and everyone wins. That’s why we build software that helps brands comply with complex privacy laws without drowning in legal costs or red tape.&lt;/p&gt;
    &lt;p&gt;We’ve cracked the code on making compliance something companies can handle themselves — no armies of lawyers required.&lt;/p&gt;
    &lt;p&gt;We’re a Y Combinator–backed startup based in San Francisco, obsessed with building products that solve hard problems and relentless about making our customers successful.&lt;/p&gt;
    &lt;p&gt;Joining TrueVault as a Business Development Representative (BDR) offers a unique opportunity to combine your first-hand DTC eCommerce experience with building relationships and driving revenue for a fast-growing SaaS company.&lt;/p&gt;
    &lt;p&gt;We aren’t looking for a traditional BDR. Instead, we want someone who’s been in the shoes of our customers — specifically, someone who has worked as an eCommerce Manager at a DTC brand. You understand the realities of running an online store, the pressure of growth targets, and the complexity of compliance — because you’ve lived it. Now, you’ll get to use that knowledge to connect with peers, build trust, and introduce them to how TrueVault can make their lives easier.&lt;/p&gt;
    &lt;p&gt;This role will give you exposure to a high-performing sales team, hands-on coaching, and a front-row seat to scaling a Y Combinator-backed startup.&lt;/p&gt;
    &lt;p&gt;We are seeking a Business Development Representative (BDR) with direct experience as an eCommerce Manager at a DTC brand. This is a sales development role — you’ll be on the front lines reaching out to prospective customers via emails, calls and social media, engaging them in conversations about their compliance challenges, and booking discovery meetings for our Account Executives.&lt;/p&gt;
    &lt;p&gt;Your unique background as an eCommerce Manager will allow you to connect authentically with prospects, understand their day-to-day pain points, and speak credibly about the value TrueVault brings.&lt;/p&gt;
    &lt;p&gt;This is a fully remote role, but you must be based in the United States.&lt;/p&gt;
    &lt;p&gt;This role is designed to be a launchpad into SaaS sales. High performers will have the opportunity to advance into Account Executive roles within 12–18 months, with higher OTE potential and greater ownership of deals. You’ll gain direct exposure to closing strategies, complex sales cycles, and leadership visibility that accelerates your career trajectory.&lt;/p&gt;
    &lt;p&gt;You’ll work directly with our CEO, who heads up our GTM team, as well as collaborating closely with Account Executives and Partnerships. This means hands-on coaching, direct mentorship, and visibility into how strategic deals are structured and closed.&lt;/p&gt;
    &lt;p&gt;We believe in setting clear, achievable expectations. Success in this role will be measured by:&lt;/p&gt;
    &lt;p&gt;This role offers $100,000 OTE, comprised of:&lt;/p&gt;
    &lt;p&gt;Please submit your application through WaaS.&lt;/p&gt;
    &lt;p&gt;Attach your resume in PDF format (no other formats will be accepted).&lt;/p&gt;
    &lt;p&gt;In the “What interests you about TrueVault” text box, include the following:&lt;/p&gt;
    &lt;p&gt;Why we ask for this: Writing persuasive outreach emails is a core part of the BDR role. This exercise isn’t about creating a “perfect” email — it’s about showing us your approach, how you think about personalization, and how you’d engage a real prospect. Keep it short, natural, and in your own voice.&lt;/p&gt;
    &lt;p&gt;We take applicant privacy seriously. Please review our Job Applicant Privacy Policy to understand how we collect and process your personal information in accordance with applicable laws, including the CCPA.&lt;/p&gt;
    &lt;p&gt;TrueVault builds software tools that help businesses comply with consumer data privacy laws. We believe if businesses have access to products that make getting and staying compliant simple, straightforward, and fully automated, respecting consumers' data privacy becomes the sensible default. And we all benefit from that.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/truevault/jobs/FaC8Apo-ecommerce-manager-bdr"/><published>2025-10-03T21:00:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45468698</id><title>Zig builds are getting faster</title><updated>2025-10-04T07:32:17.675009+00:00</updated><content>&lt;doc fingerprint="adc8b06e29012720"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;Zig Builds Are Getting Faster&lt;/head&gt;
    &lt;p&gt;Andrew Kelley famously (or infamously, depending on your views) said "the compiler is too damn slow, that's why we have bugs."1&lt;/p&gt;
    &lt;p&gt;As a result, one of the primary stated goals of Zig for years has been faster compile times. The Zig team has been working on extremely hard problems to make this a reality (such as yeeting LLVM, writing their own code generation backends, building their own linkers, and marching towards incremental compilation in general).2&lt;/p&gt;
    &lt;p&gt;The fruits of this multi-year labor are finally starting to show with Zig 0.15.1. The Ghostty project just completed upgrading to Zig 0.15.1, and I'd like to share some real-world build times.3&lt;/p&gt;
    &lt;head rend="h2"&gt;Build Script Compilation&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 7sec 167ms&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 1sec 702ms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to build the &lt;code&gt;build.zig&lt;/code&gt; script itself. The
times above were measured by running &lt;code&gt;zig build --help&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A well-written build script should only rebuild itself rarely. However, this is a cost every new uncached source build will pay (e.g. a user downloading the project to build from source one time). As such, it directly impacts the time to build a usable binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Full Uncached Ghostty Binary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 41sec&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 32sec&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This includes the time to build the build script itself. Given the prior results, Zig 0.15 is building everything else ~2 seconds faster. But, you can still see in wall time the change in this initial build time.&lt;/p&gt;
    &lt;p&gt;Important: most of this is still using LLVM. Ghostty still can't fully build and link using the self-hosted x86_64 backend, since the backend still has bugs. So, this just shows the general improvements in the Zig compiler itself, even with LLVM in the picture.&lt;/p&gt;
    &lt;p&gt;Once Ghostty can use the self-hosted x86_64 backend completely, I expect this time to plummet to around 25 seconds or less, fully half the time it would take with Zig 0.14.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incremental Build (Ghostty Executable)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 19sec&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 16sec&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to rebuild Ghostty after a one-line change to the most core terminal emulation code (adding a log function call to the escape sequence parser).&lt;/p&gt;
    &lt;p&gt;This build has a fully cached build script and dependency graph, so it is only rebuilding what it needs to. Incremental compilation in Zig isn't functional yet, so this still recompiles a considerable amount of code. Additionally, as with the prior section, this is still using LLVM. By simply dropping LLVM out of the picture, I expect this time to drop to around 12 seconds or so (less the time LLVM is emitting).&lt;/p&gt;
    &lt;p&gt;Going further, once Zig supports incremental compilation, I expect we'll be able to measure incremental builds like this within milliseconds at worst. But, let's wait and see when that is reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incremental Build (libghostty-vt)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 2sec 884ms&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 975ms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to rebuild only libghostty-vt after a one-line change. Unlike the Ghostty executable, &lt;code&gt;libghostty-vt&lt;/code&gt;
is fully functional with the self-hosted x86_64 backend, so this
shows the differences in build times without LLVM in the picture.&lt;/p&gt;
    &lt;p&gt;Similar to the Ghostty executable, this is still rebuilding the full Zig module for &lt;code&gt;libghostty-vt&lt;/code&gt;, since incremental compilation isn't
fully functional yet. I expect this to also drop to single-digit milliseconds
at worst once incremental compilation is a reality.&lt;/p&gt;
    &lt;p&gt;But still, a sub-second build time for a non-trivial library is amazing. This is the library I'm spending most of my time working on right now, and even in a few short days since upgrading to Zig 0.15.1, I've felt a huge difference in my workflow. Previously, I might tab out to read an email between builds or tests, but now its so fast I can stay in flow in my terminal.&lt;/p&gt;
    &lt;p&gt;This improvement is most indicative of what's to come in the short term. The self-hosted x86_64 backend is already stable enough to build all debug builds by default and the aarch64 backend is getting there, too. We aren't able to build the full Ghostty executable yet, but I bet this will get ironed out within months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Faster Builds Are Here&lt;/head&gt;
    &lt;p&gt;As you can see, building Ghostty with Zig 0.15.1 is faster in every single scenario, despite the fact that a lot of Ghostty still can't even take advantage of the self-hosted backend! And despite the fact that incremental compilation isn't functional yet!&lt;/p&gt;
    &lt;p&gt;I've loved betting on Zig for Ghostty, and I love that they're focusing on compile times. These improvements are real, and they're here now. And I suspect in the next couple years, the results posted today will look downright slow. 😜&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Timestamped link: https://youtu.be/5eL_LcxwwHg?t=565 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This ignores an astronomical amount of work that has gone into making every aspect of the Zig compiler faster, more parallelizable, etc. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;All measurements done on the same x86_64 Linux machine. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/zig-builds-getting-faster"/><published>2025-10-03T22:45:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45469285</id><title>Binary Formats Gallery</title><updated>2025-10-04T07:32:17.413580+00:00</updated><content>&lt;doc fingerprint="c3776c9df8eba8b9"&gt;
  &lt;main&gt;
    &lt;p&gt;All formats in this gallery have formal specifications in Kaitai Struct language. They can be used:&lt;/p&gt;
    &lt;p&gt;For a summary of all entries with associated metadata, see File Format Cross-References.&lt;/p&gt;
    &lt;p&gt;If you've done any format specs using Kaitai Struct that you won't mind to share with the rest of the community — you're most welcome to do so! It's very easy to do:&lt;/p&gt;
    &lt;p&gt;.ksy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://formats.kaitai.io/"/><published>2025-10-04T00:19:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45469437</id><title>Sora Update #1</title><updated>2025-10-04T07:32:16.871711+00:00</updated><content>&lt;doc fingerprint="b1768ad0762346be"&gt;
  &lt;main&gt;
    &lt;p&gt;We have been learning quickly from how people are using Sora and taking feedback from users, rightsholders, and other interested groups. We of course spent a lot of time discussing this before launch, but now that we have a product out we can do more than just theorize.&lt;lb/&gt;We are going to make two changes soon (and many more to come).&lt;lb/&gt;First, we will give rightsholders more granular control over generation of characters, similar to the opt-in model for likeness but with additional controls.&lt;/p&gt;
    &lt;p&gt;We are hearing from a lot of rightsholders who are very excited for this new kind of "interactive fan fiction" and think this new kind of engagement will accrue a lot of value to them, but want the ability to specify how their characters can be used (including not at all). We assume different people will try very different approaches and will figure out what works for them. But we want to apply the same standard towards everyone, and let rightsholders decide how to proceed (our aim of course is to make it so compelling that many people want to). There may be some edge cases of generations that get through that shouldn't, and getting our stack to work well will take some iteration. &lt;lb/&gt;In particular, we'd like to acknowledge the remarkable creative output of Japan--we are struck by how deep the connection between users and Japanese content is!&lt;lb/&gt;Second, we are going to have to somehow make money for video generation. People are generating much more than we expected per user, and a lot of videos are being generated for very small audiences. We are going to try sharing some of this revenue with rightsholders who want their characters generated by users. The exact model will take some trial and error to figure out, but we plan to start very soon. Our hope is that the new kind of engagement is even more valuable than the revenue share, but of course we we want both to be valuable.&lt;/p&gt;
    &lt;p&gt;Please expect a very high rate of change from us; it reminds me of the early days of ChatGPT. We will make some good decisions and some missteps, but we will take feedback and try to fix the missteps very quickly. We plan to do our iteration on different approaches in Sora, but then apply it consistently across our products.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.samaltman.com/sora-update-number-1"/><published>2025-10-04T00:39:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45469468</id><title>Track which Electron apps slow down macOS 26 Tahoe</title><updated>2025-10-04T07:32:16.763848+00:00</updated><content>&lt;doc fingerprint="a48986b98402a725"&gt;
  &lt;main&gt;
    &lt;p&gt;shamelectron Tracking problematic Electron apps macOS Tahoe. Major GPU performance issue on macOS 26 ( electron/electron#48376 ). Status Overview 33 total applications tracked Last updated 6s ago updates automatically every 12 hours 7 fixed 26 not fixed Are you an Electron app developer? Bump Electron to at least versions v38.2.0 , v37.6.0 and v36.9.2 status app social 1Password ask @1Password to bump Electron Arduino IDE ask @arduino to bump Electron Balena Etcher ask @balena_io to bump Electron Beeper ask @beeper to bump Electron Bitwarden ask @bitwarden to bump Electron Bruno ask @use_bruno to bump Electron Claude App ask @claudeai to bump Electron Cluely Cursor ask @cursor_ai to bump Electron Discord ask @discord to bump Electron Discord (Canary) Docker Desktop Dropbox ask @dropbox to bump Electron Element ask @element_hq to bump Electron Figma GitHub Desktop ask @github to bump Electron GitKraken ask @gitkraken to bump Electron HEY ask @heyhey to bump Electron Kiro ask @kirodotdev to bump Electron LM Studio ask @lmstudio to bump Electron Logseq ask @logseq to bump Electron Notion ask @notion to bump Electron Notion Calendar ask @notion to bump Electron Notion Mail ask @notion to bump Electron Obsidian Pocket Casts ask @pocketcasts to bump Electron Postman ask @postman to bump Electron Salea Logic ask @salea to bump Electron Signal Slack ask @slackhq to bump Electron Visual Studio Code ask @code to bump Electron Visual Studio Code (Insiders) Windsurf ask @windsurf_ai to bump Electron&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avarayr.github.io/shamelectron/"/><published>2025-10-04T00:45:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45469579</id><title>New antibiotic targets IBD and AI predicted how it would work</title><updated>2025-10-04T07:32:16.668004+00:00</updated><content/><link href="https://healthsci.mcmaster.ca/new-antibiotic-targets-ibd-and-ai-predicted-how-it-would-work-before-scientists-could-prove-it/"/><published>2025-10-04T01:09:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45470206</id><title>Toyota runs a car-hacking event to boost security (2024)</title><updated>2025-10-04T07:32:16.542197+00:00</updated><content>&lt;doc fingerprint="775970e409ba9767"&gt;
  &lt;main&gt;
    &lt;p&gt;Toyota organizes a car-hacking event that captivates participating students. We uncovered the important purpose behind the event.&lt;/p&gt;
    &lt;p&gt;What comes to mind when you hear the word âhacking?â&lt;/p&gt;
    &lt;p&gt;Generally, it means âan act of unauthorized access into a system or network, intended to cause harm.â&lt;/p&gt;
    &lt;p&gt;In films and comics, hackers are often portrayed as criminals.&lt;/p&gt;
    &lt;p&gt;You may have watched scenes where such hackers tap away on keyboards as lines of numbers and symbols fill the computer screen to steal data and hijack machines.&lt;/p&gt;
    &lt;p&gt;How, then, would you feel about Toyota holding a car-hacking event?&lt;/p&gt;
    &lt;head rend="h3"&gt;Not just in Japan&lt;/head&gt;
    &lt;p&gt;Many students have gathered in a conference room. All are engrossed in a unique competition unfolding on their computer screens.&lt;/p&gt;
    &lt;p&gt;The event is Hack Festa*, a chance for IT students to pit their hacking skills against each other.&lt;/p&gt;
    &lt;p&gt;*Jointly organized by Toyota, Toyota Motor North America, Inc., and Toyota Tsusho Systems US, Inc.&lt;/p&gt;
    &lt;p&gt;The students form groups of around four and tackle the assigned tasks as a team. All of the challenges are car-related, ranging from controlling a vehicleâs speed adjustment mechanism to overwriting engine RPMs.&lt;/p&gt;
    &lt;p&gt;Their efforts are tested on simulators, earning points for every task cleared. The team with the highest score wins.&lt;/p&gt;
    &lt;p&gt;Aside from Japan, the events are also held in the United States and Ireland.&lt;/p&gt;
    &lt;p&gt;Why is Toyota hosting such events? Isnât hacking harmful?&lt;/p&gt;
    &lt;p&gt;When we posed these questions to the event producer, Hisashi Oguma, a project general manager at InfoTech-IS, explained that we first had to understand the challenges facing the auto industry.&lt;/p&gt;
    &lt;head rend="h3"&gt;With convenience comes new threats&lt;/head&gt;
    &lt;p&gt;Hisashi Oguma, Ph.D., Project General Manager/Principal Researcher, InfoTech-IS&lt;/p&gt;
    &lt;p&gt;Today, cars are evolving in unprecedented ways through internet connectivity, with automated driving and connected cars as obvious examples.&lt;/p&gt;
    &lt;p&gt;Another concept that has emerged recently is the SDV (Software Defined Vehicle). Simply put, it involves updating the onboard software after purchase, continuing to expand the carâs functionality as you would a smartphone.&lt;/p&gt;
    &lt;p&gt;Internet integration improves convenience. At the same time, it also makes it easier for malicious third parties to hack car systems.&lt;/p&gt;
    &lt;p&gt;The cybersecurity that protects customers from such harm has become more important than ever, not only for Toyota but for the entire auto industry.&lt;/p&gt;
    &lt;p&gt;In serious cases, a hacked car could mean a loss of control over its basic functions of driving, turning, and stopping.&lt;/p&gt;
    &lt;p&gt;As an example, the video below shows a simulation of what might happen if steering wheel control is compromised while driving.&lt;/p&gt;
    &lt;p&gt;Would you be able to keep your cool if this happened in real life?&lt;/p&gt;
    &lt;p&gt;Ogumaâs division researches ways to prevent such harm. It organizes the Hack Festa to help counter potential threats.&lt;/p&gt;
    &lt;p&gt;A hacking event to boost cybersecurity. It sounds contradictory, but what on earth does it mean?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://toyotatimes.jp/en/spotlights/1061.html"/><published>2025-10-04T03:11:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45470513</id><title>Starship's eleventh flight test</title><updated>2025-10-04T07:32:16.050417+00:00</updated><link href="https://www.spacex.com/launches/starship-flight-11"/><published>2025-10-04T04:25:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45470842</id><title>Optimizing a 6502 image decoder – part II: assembly</title><updated>2025-10-04T07:32:15.101384+00:00</updated><content>&lt;doc fingerprint="53459004633407d3"&gt;
  &lt;main&gt;
    &lt;p&gt;In the first part of this article, I focused on the algorithm itself, removing parts I wouldn’t use (like color), making it simpler, less loopy, etc. This yielded a ten-times improvement on a modern architecture, but would still be very slow if simply translated to assembly by a compiler like cc65.&lt;/p&gt;
    &lt;p&gt;In this part, I’ll focus on things one can do to get “speed” out of a 1MHz 8-bits processor. It will include a number of “tricks”, some of them being largely accepted as “good code” – like predictive branching or lookup tables, others now being regarded as “ugly”, “dangerous”, “bad practice” – like self-modifying code, global variables, absent bound-checking, etc. Taking shortcuts was very often necessary 30 years ago and caring about contracts, scopes and separation and so on was… less of a thing.&lt;/p&gt;
    &lt;p&gt;For memory, here is the reference C algorithm.&lt;/p&gt;
    &lt;p&gt;Aligning your buffers&lt;/p&gt;
    &lt;p&gt;Let’s get that out of the way right from the start: you don’t want the extra page-crossing penalty. Align the buffers, (stuff the holes with something that fits if memory is an issue), and be done with it. Bonus: patching addresses will only require patching the page byte.&lt;/p&gt;
    &lt;p&gt;Sticking to 8-bits&lt;/p&gt;
    &lt;p&gt;Anything involving 16-bits will be at least twice slower. Probably three times slower. When presented with a algorithm using &lt;code&gt;int&lt;/code&gt; types, verify every variable. Is it really an int? Does it fit in 8 bits? Anything that fits in 8 bits should be 8 bits. Sometimes it can look like an &lt;code&gt;uint8&lt;/code&gt; could overflow, but is it by more than one bit? &lt;code&gt;r = (a + b) &amp;gt;&amp;gt; 1&lt;/code&gt; if &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;  are 8-bits might look like it would overflow, but thanks to the carry bit, it would not. In this decoder, there were a lot of variables that were not initially, but ended up being 8-bits, and this helped a lot. The bitbuffer was 32 bits initially. I got it down to 16-bits easily, and it did seem like a good idea to have a second byte ready so the first one would always be complete and allow for direct matching… But shifting 16-bits was much more intensive than shifting and 8-bits bitbuffer 1 bit at at time. &lt;/p&gt;
    &lt;p&gt;Avoiding stack function parameters&lt;/p&gt;
    &lt;p&gt;On 6502, a lot of small things become functions. For example,&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;val = (val_from_last[last] * factor) &amp;gt;&amp;gt; 4;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;would be compiled by cc65, approximately, as&lt;/p&gt;
    &lt;code&gt;ldx last             ; load index
lda val_from_last,x  ; load table value
jsr pusha0           ; push as parameter for multiplication
lda factor           ; load multiplicator
jsr tosmula0         ; multiply (returns 16-bits in A/X)
jsr shrax4           ; shift right 4 (returns in A/X)&lt;/code&gt;
    &lt;p&gt;This is good in the sense that this is small, but it is slow. It also performs a 16-bits by 16-bits multiplication, which is much slower than we need here as both members are 8-bits. I wrote a multiplication function that takes its parameters from the A and X registers, and that code became:&lt;/p&gt;
    &lt;code&gt;ldy last
ldx val_from_last,y
lda factor
jsr mult8x8r16
jsr shrax4
&lt;/code&gt;
    &lt;p&gt;Inlining operations&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;jsr shrax4&lt;/code&gt; comes with a 12-cycles penalty, so it gets inlined:&lt;/p&gt;
    &lt;code&gt;.macro SHRAX4
stx tmp1    ; 3 cycles
lsr tmp1    ; 5 - total 8
ror a       ; 2 - total 10
lsr tmp1    ;           15
ror a       ;           17
lsr tmp1    ;           22
ror a       ;           24
lsr tmp1    ;           29
ror a       ;           31
ldx tmp1    ;           34
.endmacro

jsr mult8x8r16
SHRAX4          ; 34 cycles
sta result      ; 37
stx result+1    ; 40&lt;/code&gt;
    &lt;p&gt;This still takes a large number of cycles, especially as we have to do &amp;gt;&amp;gt; 4 a large number of times (4 times per pair of columns for each line, which represents 7 seconds of CPU time), so…&lt;/p&gt;
    &lt;p&gt;Lookup tables&lt;/p&gt;
    &lt;p&gt;Gaining speed is very often at the expense of code size, but luckily, in this project, memory is not really an issue (The I/O cache is 14kB, which is ludicrous). As usual in this case, building lookup tables is the way to go, when possible. It is sometimes not possible when working on 16-bits integers (because a single 16-bits wide lookup table would amount to all of the main memory an Apple II has), but in the case of shifting right by 4, this is very possible: given a 16-bit number &lt;code&gt;ABCDEFGH IJKLMNOP&lt;/code&gt; where each letter represents one bit, shift it &amp;gt;&amp;gt; 4, it becomes &lt;code&gt;0000ABCD EFGHIJKL&lt;/code&gt;. It can be done with two 256-byte lookup tables, one for shifting right by 4, one for shifting left. What we want to do is:&lt;/p&gt;
    &lt;code&gt;low_byte = (high_byte&amp;lt;&amp;lt;4 | low_byte&amp;gt;&amp;gt;4), high_byte = high_byte&amp;gt;&amp;gt;4&lt;/code&gt;
    &lt;p&gt;Building these tables is done only once at startup, in a very simple way:&lt;/p&gt;
    &lt;code&gt;        ldy     #0           ; Our index (0-255)
:       tya                  ; Transfer to A,
        asl                  ; shift it
        asl
        asl
        asl
        sta     _shiftl4,y   ; Store the result
        iny
        bne     :-&lt;/code&gt;
    &lt;p&gt;Afterwards, we can simply shift the result of our multiplication this way:&lt;/p&gt;
    &lt;code&gt;jsr mult8x8r16     ; Returns with high byte in X, low in A
tay                ; 2  - Low byte to Y,
lda _shiftr4,y     ; 6  - Shift its high nibble,
ora _shiftl4,x     ; 10 - OR it with the high byte low nibble,
sta result         ; 13 - Store low byte
lda _shiftr4,x     ; 17 - Shift the high byte
sta result+1       ; 20 - And done!&lt;/code&gt;
    &lt;p&gt;We just divided by two the time needed to shift numbers by 4, left or right. As this is done 150.000 times, this translates to a 3 seconds improvement.&lt;/p&gt;
    &lt;p&gt;More avoidance of stack parameters&lt;/p&gt;
    &lt;p&gt;In the previous example, avoiding stack use was easy as we were multiplying two 8 bits numbers, fitting in two registers. How to do the same when our function needs two 16-bits integers ? There are only three registers. In this case, the cc65 compiler way is to push the first 16-bit int on the software stack, load the second one in AX, call the function which will store AX in zero-page and pop the first parameter from the stack. This is very slow, so instead of doing&lt;/p&gt;
    &lt;code&gt;lda ...
ldx ...
jsr pushax
lda ...
ldx ...
jsr mult16x16r16 ; entry point that stores AX and pops stack&lt;/code&gt;
    &lt;p&gt;We will do:&lt;/p&gt;
    &lt;code&gt;lda ...
sta tmp1
ldx ...
stx tmp1+1
lda ...
ldx ...
jsr mult16x16r16_direct ; entry point after the stack pop&lt;/code&gt;
    &lt;p&gt;This has a second advantage in our algorithm, where there is a place doing 320 multiplications with one of the parameters always identical: the identical one can be stored only once, before the loop.&lt;/p&gt;
    &lt;p&gt;Shifting smartly&lt;/p&gt;
    &lt;p&gt;In a similar manner, there is a place in the code where &lt;code&gt;val = factor &amp;lt;&amp;lt; 7&lt;/code&gt;. This could be inlined as seven pairs of &lt;code&gt;asl/rol&lt;/code&gt;, costing 49 cycles, or we can notice that as &lt;code&gt;factor&lt;/code&gt; is 8-bits, seven bits of the low byte end in the high byte, the high bit of the low byte is the original low bit, and the rest is zeroes:&lt;/p&gt;
    &lt;code&gt;; factor&amp;lt;&amp;lt;7: 00000000 ABCDEFGH becomes 0ABCDEFG H0000000
lsr     a    ; 2 - shift low byte &amp;gt;&amp;gt;1, (puts low bit to carry),
tax          ; 4 - move what remains to high byte (0ABCDEFG)
lda     #0   ; 6 - reinit low byte to zero,
ror     a    ; 8 - put initial low bit back to high bit of low byte
&lt;/code&gt;
    &lt;p&gt;Self-patching code&lt;/p&gt;
    &lt;p&gt;In this algorithm, there are a number of things that are set every couple of rows, and used multiple times per pixel. To keep the same example, &lt;code&gt;factor&lt;/code&gt; is set at start of a couple rows and then used four times in the 320 columns loop. That means setting it is done 120 times, using it 153600 times. Instead of reloading it, we will just patch our code so that&lt;/p&gt;
    &lt;code&gt;        lda factor
        ldx val
        jsr mult8x8r16

becomes:

factor1:lda #$FF
        ldx val
        jsr mult8x8r16&lt;/code&gt;
    &lt;p&gt;This does not spare a lot, but it ends up counting. The same trick can be used to spare usage of zero-page variables when you have to put a temporary result away to reuse it later. In this case, no speed is gained (&lt;code&gt;sta zp/lda zp&lt;/code&gt; is 3+3 cycles, &lt;code&gt;sta abs/lda #imm&lt;/code&gt; is 4+2), but less ZP variables are needed, and ZP variables are few, so this is useful (example, lines 539/548).&lt;/p&gt;
    &lt;p&gt;Branch prediction&lt;/p&gt;
    &lt;p&gt;This is equivalent to the LIKELY()/UNLIKELY() macros you can find in kernel code. As the programmer, you know better than the compiler whether a condition is likely to be true or not, and you can adapt your branching to spare one cycle (a branch taken is 1 more cycle than one not taken on 6502) on the most common case.&lt;/p&gt;
    &lt;p&gt;For example, when iterating over more than 256 items in an array, you will have to increment the high byte of your pointer when reaching 256, so you read $AA00, $AA01, … $AAFF, $AB00, $AB01, etc. One usual way to do that is:&lt;/p&gt;
    &lt;code&gt;loop:
    iny
    bne :+
    inc buffer+1
:   lda buffer,y
    ; ... do things
    jmp loop&lt;/code&gt;
    &lt;p&gt;Here, the &lt;code&gt;bne&lt;/code&gt; will take 3 cycles 255 times and 2 cycles the 256th time (767 cycles). Instead,&lt;/p&gt;
    &lt;code&gt;incr_buffer_high:
    inc buffer+1
    jmp continue_loop

loop:
    iny
    beq incr_buffer_high
continue_loop:
    lda buffer,y
    ; ... do things
    jmp loop&lt;/code&gt;
    &lt;p&gt;Will take 2*255 + 3 cycles + 3 cycles for the extra jump back, about 33% less. This requires finding places out of the main code path to put these “out-of-band” helpers, but, luckily, there often are some between a &lt;code&gt;jmp&lt;/code&gt; and a entry point label. In this project, this is also very useful on the bitbuffer, which shifts bits one at a time about 500.000 times, and only needs to refill the buffer every 8 bits, and which reads bytes about 60.000 times, and only needs to increment the cache pointer page 234 times.&lt;/p&gt;
    &lt;p&gt;16-bits buffers splitting&lt;/p&gt;
    &lt;p&gt;Buffers of 16-bits numbers are a real pain in the ass, especially when they are more than 128 items long: to access buffer[index], you have to double index, increment the page if index &amp;gt; 127, increment index, get the high byte, decrement index, get the low byte:&lt;/p&gt;
    &lt;code&gt;    lda index
    asl
    bcc :+
    inc buffer+1
:   tay
    iny
    lda (buffer),y
    tax
    dey
    lda (buffer),y&lt;/code&gt;
    &lt;p&gt;Splitting them into two arrays of low bytes / high bytes is much easier (and makes page crossing only an issue for arrays bigger than 255 items):&lt;/p&gt;
    &lt;code&gt;    ldy index
    ldx buffer_high,y
    lda buffer_low,y&lt;/code&gt;
    &lt;p&gt;Hardcode pointers&lt;/p&gt;
    &lt;p&gt;The main loop accesses the next_line[] buffer approximately 32 times per pair of columns, so it’s referenced at about 64 places (for low and high value bytes). The “keep code small” way of handling that is to use indirect addressing, &lt;code&gt;lda (zp_ptr),y&lt;/code&gt;, but that has drawbacks: first, we access that buffer at indexes col, col+1, col+2, col+3 so this would require four zeropage pointers, offset from each other by one – using one ZP pointer and offsetting Y would make clean page crossing impossible. Even if it was possible, shifting the Y index back and forth would be complicated and time-consuming. Second, indirect addressing takes 5 cycles instead of 4 for &lt;code&gt;lda $nnnn,y&lt;/code&gt;, meaning a penalty of (120*320*64*2) = 5M cycles. Instead, all of those are hardcoded with a label in front, and the page bytes require patching twice per row. It is ugly (see set_buf_pages), but takes only 30k cycles, translating to a large gain.&lt;/p&gt;
    &lt;p&gt;Looking at the data&lt;/p&gt;
    &lt;p&gt;I delved into “what kind of data do I read and decode” late in the project, as I didn’t think it would help a lot at first. After all, the operations have to be done, don’t they? But…&lt;/p&gt;
    &lt;p&gt;Sometimes, the data makes you feel lucky. For example, there is a buffer values adjustment taking place at every row start, and it requires 320 multiplications to scale the next_line buffer:&lt;/p&gt;
    &lt;code&gt;    val = some 16-bits integer;

    for (i = 0; i &amp;lt; DATABUF_SIZE-1; i++) {
      next_line[i] = (val * next_line[i]) / 256;
    }&lt;/code&gt;
    &lt;p&gt;When I printed that &lt;code&gt;val&lt;/code&gt;1, I noticed that it was 255 more often than any other value – actually it is 255 more than 70% of the time over all my test pictures. So this means that I could rewrite the previous multiplication line in this case:&lt;/p&gt;
    &lt;code&gt;   (255 * next_line[i]) / 256
 = (next_line[i]*256 - next_line[i]) / 256
 =  next_line[i] - next_line[i]/256
 =  next_line[i] - (next_line[i]&amp;gt;&amp;gt;8)&lt;/code&gt;
    &lt;p&gt;So the previous for loop could be special-cased:&lt;/p&gt;
    &lt;code&gt;if (val == 255) {
    for (i = 0; i &amp;lt; DATABUF_SIZE-1; i++) {
       next_line[i] -= next_line[i]&amp;gt;&amp;gt;8;
    }
} else {
    for (i = 0; i &amp;lt; DATABUF_SIZE-1; i++) {
       next_line[i] = (val * next_line[i]) / 256;
    }
}&lt;/code&gt;
    &lt;p&gt;Translated into assembly, this is very much simpler than a multiplication (20-30 cycles instead of 150):&lt;/p&gt;
    &lt;code&gt;    ldx     i
    lda     next_line_low,x    ; Low byte of next_line[x] in A,
    sec
    sbc     next_line_high,x   ; Subtract high byte from low :
                               ; -(next_line[i]&amp;gt;&amp;gt;8)
    bcs     :+
    dec     next_line_high,x   ; Decrement high byte if needed
:   sta     next_line_low,x&lt;/code&gt;
    &lt;p&gt;Approximating&lt;/p&gt;
    &lt;p&gt;One of the worst things in this algorithm is that final pixel values are computed from 16-bits numbers, divided by our dear &lt;code&gt;factor&lt;/code&gt;, and then clamped between 0 and 255 (the 16-bits numbers are signed to make things worse, clamping is at both ends). For a 320×240 image, this means 153600 divisions (~45 seconds) with a precise division method. I already explained that in the previous article, as it translates to a large net gain even on modern architecture, but I’ll detail a bit more here.&lt;/p&gt;
    &lt;p&gt;So we’re going to approximate it too, and replace the division method with the inverse multiplication/shifting method, getting division time down to 30 seconds.&lt;/p&gt;
    &lt;p&gt;But this is far from enough: as the division factor only changes every few rows, this calls for a division lookup table. For it to fit in memory, I only store the result of “round hexadecimal numbers”: 0x80/n, 0x180/n, 0x280/n, … 0xFF80/n. Yes, this approximates the approximation, and the result image, of course, is different, and some pixels have their value offset by… 1, which is very invisible.&lt;/p&gt;
    &lt;p&gt;The result of the clamping is also stored in that table, so 1) no check is needed at the time of finalizing a pixel and 2) as soon as the division’s result high byte is not zero when building the table, you can stop dividing. The table is built like this:&lt;/p&gt;
    &lt;code&gt;      lda     #0
      sta     wordcnt           ; persist counter, division
                                ; will use all registers

:     ldx     wordcnt           ; load denominator (X/A)
      lda     #$80
fact: ldy     #$FF              ; load divisor (patched in)
      jsr     approx_div16x8    ; divide AX by Y

      ldy     wordcnt
      bmi     overflows_neg     ; stop if result &amp;lt; 0 
                                ; (high byte had high bit set)

      sta     dyndiv,y          ; Store the low byte of the result
      txa                       ; Check if high byte is 0,
      bne     overflows         ; Stop if not: result &amp;gt; 256
      inc     wordcnt           ; Increment the counter,
      bne     :-                ; Go do the next division
      jmp     done

overflows:                      ; Fill the overflows with 255...
      lda     #$FF
:     sta     dyndiv,y
      iny
      bmi     overflows_neg     ; As long as they're positive,
      bne     :-

overflows_neg:
      lda     #$00              ; Then fill the underflows with 0
:     sta     dyndiv,y
      iny
      bne     :-

done:
&lt;/code&gt;
    &lt;p&gt;This takes about one second per picture because now we’re only dividing ~5000 times per image. Once the table is built, storing a pixel after the 16-bit value is computed, which was:&lt;/p&gt;
    &lt;code&gt;    ; ... compute 16-bits value
    ldy factor          ; load factor
    jsr approx_div16x8  ; divide
    cpx #0              ; is result high byte 0 ?
    beq store           ; yes, we can store the result
    bmi underflow       ; is it negative ?
overflow:
    lda #$FF            ; no, so store 255
    jmp store
underflow:
    lda #$00            ; yes, so store 0
store:
    ldy column          ; Reload column, trashed by division
    sta output_buffer,y ; Store value! &lt;/code&gt;
    &lt;p&gt;and took about 200 cycles… becomes this:&lt;/p&gt;
    &lt;code&gt;    ; ... compute 16-bits value
    lda dyndiv,x
    sta buffer,y&lt;/code&gt;
    &lt;p&gt;Which takes 8 cycles.&lt;/p&gt;
    &lt;p&gt;Look at data again&lt;/p&gt;
    &lt;p&gt;Another round of printf()s showed that the division factor is 48 more than 60% of the time. I devoted 256 extra bytes to a static div48 table, initialized once at start, never changing, so that I only have to rebuild the current row’s division table when factor is not 48 (and not the same as before). Dividing now takes 300ms per picture, as there are less than 2.000 divisions taking place.&lt;/p&gt;
    &lt;p&gt;Look at data again&lt;/p&gt;
    &lt;p&gt;I was discontent with the Huffman decoder algorithm. It reads bits, one at at time, until it matches a valid Huff code in the current Huff tree, and returns the associated value. Some of the Huffman trees (which, weirdly, are static and not picture-data dependent) are quite simple, others less so. There are a few trees that have only two leafs (0 and 1) and for which there’s only one bit to read. A few ones have four (00, 01, 10, 11), two bits to read. A few ones have simple codes that are easy to match without counting read bits (00, 01, 10, 110, 1110, etc). But others have codes starting with a zero that forces me to count the bits in order to do the matching (00, 010, 011, 100, 101: reading ’10’ is not the same as reading ‘010’, ’10’ is not a valid code, but ‘010’ ends up in the same offset in the code/value array). There are some places in the decoding where data is fetched from a given tree (always the same), others were it could fetch from multiple different trees.&lt;/p&gt;
    &lt;p&gt;The general decoder algorithm had to take all these cases into account. It also required to patch in the tree to use in the decoder before each read, always (two patches, one for the code/value match array, one for the code/numbits array). The small trees still took a full page. Etc. I modified those, used a jump table instead of a subroutine, made basically one decoder algorithm per tree, at the expense of readability. This took 7 more seconds out, and allowed me to stuff 6 pages of tables into a single one, sparing memory to allocate to the I/O buffer (meaning less reads of more data, also faster when one reads from a floppy with seek times close to the second).&lt;/p&gt;
    &lt;p&gt;Finally, Huffman decoding matches a code to a value, which is necessary when we’re decoding data (in _decode_row) because we will use it, but useless when skipping data we don’t use (in _consume_extra). So, the decoders got specialized again, only detecting valid codes when skipping data, without returning the corresponding value.&lt;/p&gt;
    &lt;p&gt;Not checking things&lt;/p&gt;
    &lt;p&gt;In computing, it is now well-known that functions should have contracts, and should explicitely and cleanly fail when their contract is not respected. For example, &lt;code&gt;checkUser(User a)&lt;/code&gt; should start with &lt;code&gt;if (a == NULL)&lt;/code&gt; before anything else, handle the case cleanly, and that should be in the unit tests, too. Callers should check for the return values, and act accordingly.&lt;/p&gt;
    &lt;p&gt;Here, we take our memory locations and values and we do the thing we’re supposed to do. If the data is invalid, at best we’ll get a weird image out, at worst we’ll crash hard. For example, dcraw’s algorithm checked that column was still &amp;gt; 0 when doing repeats:&lt;/p&gt;
    &lt;code&gt;for (rep=0; rep &amp;lt; 8 &amp;amp;&amp;amp; rep &amp;lt; nreps &amp;amp;&amp;amp; col &amp;gt; 0; rep++) {
  col -= 2;
  ...
}&lt;/code&gt;
    &lt;p&gt;This is useless. Images are correctly encoded so that repeats don’t underflow col.&lt;/p&gt;
    &lt;p&gt;In conclusion&lt;/p&gt;
    &lt;p&gt;I think that’s about it on this algorithm, and I won’t be able to do much better. I may still have a bit of room left in the control Huffman decoders, but I don’t think I have anymore big wins awaiting. Nevertheless, I’m kind of proud of the level of optimization I achieved on this algorithm that was never intended to run on a potato chip incapable of multiplying in hardware, and not even able to shift more than one bit at a time. I hope the dive was interesting and if you have ideas, I’m all ears!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Stats the easy way: add printf(“val %d\n”, val) to the C code, then&lt;code&gt;run &amp;gt; out.txt; grep ^val | sort | uniq -c | sort -n&lt;/code&gt;↩︎&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.colino.net/wordpress/en/archives/2025/10/03/optimizing-a-6502-image-decoder-part-ii-assembly/"/><published>2025-10-04T05:51:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45471136</id><title>Alibaba cloud FPGA: the $200 Kintex UltraScale+</title><updated>2025-10-04T07:32:14.815151+00:00</updated><content>&lt;doc fingerprint="bd18a05c4875d6f6"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction#&lt;/head&gt;
    &lt;p&gt;I was recently in the market for a new FPGA to start building my upcoming projects on.&lt;/p&gt;
    &lt;p&gt;Due to the scale of my upcoming projects a Xilinx series 7 UltraScale+ FPGA of the Virtex family would be perfect, but a Kintex series FPGA will be sufficient for early prototyping. Due to not wanting to part ways with the eye watering amounts of money that is required for an Vivado enterprise edition license my choice was effectively narrowed to the FPGA chips available under the WebPack version of Vivado.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly Xilinx are well aware of how top of the range the Virtex series are, and doesn’t offer any Virtex UltraScale+ chips with the webpack license. That said, they do offer support for two very respectable Kintex UltraScale+ FPGA models, the &lt;code&gt;XCKU3P&lt;/code&gt; and the &lt;code&gt;XCKU5P&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;These two chips are far from being small hobbyist toys, with the smaller &lt;code&gt;XCUK3P&lt;/code&gt; already boasting +162K LUTs and
16 GTY transceivers, capable, depending on the physical constraints imposed by the chip packaging of
operating at up to 32.75Gb/s.&lt;/p&gt;
    &lt;p&gt;Now that the chip selection has been narrowed down I set out to look for a dev board.&lt;/p&gt;
    &lt;p&gt;My requirements for the board where that it featured :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;at least 2 SFP+ or 1 QSFP connector&lt;/item&gt;
      &lt;item&gt;a JTAG interface&lt;/item&gt;
      &lt;item&gt;a PCIe interface at least x8 wide&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As to where to get the board from, my options where :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Design the board myself&lt;/item&gt;
      &lt;item&gt;Get the AXKU5 or AXKU3 from Alinx&lt;/item&gt;
      &lt;item&gt;See what I could unearth on the second hand market&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Although option &lt;code&gt;1&lt;/code&gt; could have been very interesting, designing a
dev board with both a high speed PCIe and ethernet interface was not the goal of
today’s project.&lt;/p&gt;
    &lt;p&gt;As for option &lt;code&gt;2&lt;/code&gt;,
Alinx is newer vendor that is still building up its credibility in the west,
their technical documentation is a bit sparse, but the feedback seems to be positive with no major issues being reported.
Most importantly, Alinx provided very fairly priced development boards
in the 900 to 1050 dollar range ( +150$ for the HPC FMC SFP+ extension board ).
Although these are not cheap by any metric, compared to the competitions
price point, they are the best value.&lt;/p&gt;
    &lt;p&gt;Option &lt;code&gt;2&lt;/code&gt; was coming up ahead until I stumbled upon this ebay listing :&lt;/p&gt;
    &lt;p&gt;For 200$ this board featured a &lt;code&gt;XCKU3P-FFVB676&lt;/code&gt;, 2 SPF+ connector and a x8 PCIe interface.
On the flip side it came with no documentation whatsoever, no guaranty it worked, and the
faint promise in the listing that there was a JTAG interface.
A sane person would likely have dismissed this as an interesting internet oddity, a remanence
of what happens when a generation of accelerator cards gets phased out in favor of the next,
or maybe just an expensive paperweight.&lt;/p&gt;
    &lt;p&gt;But I like a challenge, and the appeal of unlocking the 200$ Kintex UltraScale+ development board was too great to ignore.&lt;/p&gt;
    &lt;p&gt;As such, I aim for this article to become the documentation paving the way to though this mirage.&lt;/p&gt;
    &lt;head rend="h2"&gt;The debugger challenge#&lt;/head&gt;
    &lt;p&gt;Xilinx’s UG908 Programming and Debugging User Guide (Appendix D) specifies their blessed JTAG probe ecosystem for FPGA configuration and debug. Rather than dropping $100+ on yet another proprietary dongle that’ll collect dust after the project ends, I’m exploring alternatives. The obvious tradeoff: abandoning Xilinx’s toolchain means losing ILA integration. However, the ILA fundamentally just captures samples and streams them via JTAG USER registers, there’s nothing preventing us from building our own logic analyzer with equivalent functionality and a custom host interface.&lt;/p&gt;
    &lt;p&gt;Enter OpenOCD. While primarily targeting ARM/RISC-V SoCs, it maintains an impressive database of supported probe hardware and provides granular control over JTAG operations. More importantly, it natively supports SVF (Serial Vector Format), a vendor-neutral bitstream format that Vivado can export.&lt;/p&gt;
    &lt;p&gt;The documentation landscape is admittedly sparse for anything beyond 7-series FPGAs, and the most recent OpenOCD documentation I could unearth was focused on Zynq ARM core debugging rather than fabric configuration. But the fundamentals remain sound: JTAG is JTAG, SVF is standardized, and the boundary scan architecture hasn’t fundamentally changed.&lt;/p&gt;
    &lt;p&gt;The approach should be straightforward: generate SVF from Vivado, feed it through OpenOCD with a commodity JTAG adapter, and validate the configuration. Worst case, we’ll need to patch some adapter-specific quirks or boundary scan chain register addresses. Time to find out if this theory holds up in practice.&lt;/p&gt;
    &lt;head rend="h2"&gt;The plan#&lt;/head&gt;
    &lt;p&gt;So, to resume, the current plan is to buy a second hand hardware accelerator of eBay at a too good to be true price, and try to configure it with an unofficial probe using open source software without any clear official support.&lt;lb/&gt;The answer to the obvious question you are thinking if you, like me, have been around the block a few times is: many things.&lt;/p&gt;
    &lt;p&gt;As such, we need a plan for approaching this. The goal of this plan is to outline incremental steps that will build upon themselves with the end goal of being able to use this as a dev board.&lt;/p&gt;
    &lt;head rend="h3"&gt;1 - Confirming the board works#&lt;/head&gt;
    &lt;p&gt;First order of business will be to confirm the board is showing signs of working as intended.&lt;/p&gt;
    &lt;p&gt;There is a high probability that the flash wasn’t wiped before this board was sold off, as such the previous bitstream should still be in the flash. Given this board was used as an accelerator, we should be able to use that to confirm the board is working by either checking if the board is presenting itself as a PCIe endpoint or if the SFP’s are sending the ethernet PHY idle sequence.&lt;/p&gt;
    &lt;head rend="h3"&gt;2 - Connecting a debugger to it#&lt;/head&gt;
    &lt;p&gt;The next step is going to be to try and connect the debugger. The eBay listing advertised there is a JTAG interface, but the picture is grainy enough that where that JTAG is and what pins are available is unclear.&lt;/p&gt;
    &lt;p&gt;Additionally, we have no indication of what devices are daisy chained together onto the JTAG scan chain. This is an essential question for flashing over JTAG, so it will need to be figured out.&lt;/p&gt;
    &lt;p&gt;At this point, it would also be strategic to try and do some more probing into the FPGA via JTAG. Xilinx FPGAs exposes a handful of useful system registers accessible over JTAG. The most well known of these interfaces is the SYSMON, which allows us, among other things, to get real time temperature and voltage reading from inside the chip. Although openOCD doesn’t have SYSMON support out of the box it would be worth while to build it, to :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Familiarise myself with openOCD scripting, this might come in handy when building my ILA replacement down the line&lt;/item&gt;
      &lt;item&gt;Having an easy side channel to monitor FPGA operating parameters&lt;/item&gt;
      &lt;item&gt;Make a contribution to openOCD as it have support for the interfacing with XADC but not SYSMON&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;3 - Figuring out the Pinout#&lt;/head&gt;
    &lt;p&gt;The hardest part will be figuring out the FPGA’s pinout and my clock sources. The questions that need answering are :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;what external clocks sources do I have, what are there frequencies and which pins are they connected to&lt;/item&gt;
      &lt;item&gt;which transceivers are the SFPs connected to&lt;/item&gt;
      &lt;item&gt;which transceivers is the PCIe connected to&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;4 - Writing a bitstream#&lt;/head&gt;
    &lt;p&gt;For now I will be focusing on writing a temporary configurations over JTAG to the CCLs and not re-writing the flash.&lt;/p&gt;
    &lt;p&gt;That plan is to trying writing either the bitstream directly though openOCD’s &lt;code&gt;virtex2&lt;/code&gt; + &lt;code&gt;pld&lt;/code&gt; drivers, or by replaying the
SVF generated by Vivado.&lt;/p&gt;
    &lt;p&gt;Since I believe a low iteration time is paramount to project velocity and getting big things done, I also want automatize all of the Vivado flow from taking the rtl to the SVF generation.&lt;/p&gt;
    &lt;p&gt;Simple enough ?&lt;/p&gt;
    &lt;head rend="h2"&gt;Liveness test#&lt;/head&gt;
    &lt;p&gt;A few days later my prize arrived via express mail.&lt;/p&gt;
    &lt;p&gt;Unexpectedly it even came with a free 25G SFP28 Huawei transceiver rated for a 300m distance and a single 1m long OS2 fiber patch cable. This was likely not intentional as the transceiver was jammed in the SFP cage, but it was still very generous of them to include the fiber patch cable.&lt;/p&gt;
    &lt;p&gt;The board also came with a travel case and half of a PCIe to USB adapter and a 12V power supply that one could use to power the board as a standalone device. Although this standalone configuration will not be of any use to me, for those looking to develop just networking interfaces without any PCIe interface, this could come in handy.&lt;/p&gt;
    &lt;p&gt;Overall the board looked a little worn, but both the transceiver cages and PCIe connectors didn’t look to be damaged.&lt;/p&gt;
    &lt;head rend="h3"&gt;Standalone configuration#&lt;/head&gt;
    &lt;p&gt;Before real testing could start I first did a small power-up test using the PCIe to USB adapter that the seller provided. I was able to do a quick check using the LEDs and the FPGAs dissipated heat that the board seemed to be powering up at a surface level (pun intended).&lt;/p&gt;
    &lt;head rend="h3"&gt;PCIe interface#&lt;/head&gt;
    &lt;p&gt;Since I didn’t want to directly plug mystery hardware into my prized build server, I decided to use a Raspberry Pi 5 as my sacrificial test device and got myself an external PCIe adapter.&lt;/p&gt;
    &lt;p&gt;It just so happened that the latest Raspberry Pi version, the Pi 5, now features an external PCIe Gen 2.0 x1 interface. Though our FPGA can handle up to a PCIe Gen 3.0 and the board had a x8 wide interface, since PCIe standard is backwards compatible and the number of lanes on the interface can be downgraded, plugging our FPGA with this Raspberry Pi will work.&lt;/p&gt;
    &lt;p&gt;After both the Raspberry and the FPGA were booted, I SSHed into my rpi and started looking for the PCIe enumeration sequence logged from the Linux PCIe core subsystem.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;dmesg&lt;/code&gt; log :&lt;/p&gt;
    &lt;code&gt;[    0.388790] pci 0000:00:00.0: [14e4:2712] type 01 class 0x060400
[    0.388817] pci 0000:00:00.0: PME# supported from D0 D3hot
[    0.389752] pci 0000:00:00.0: bridge configuration invalid ([bus 00-00]), reconfiguring
[    0.495733] brcm-pcie 1000110000.pcie: link up, 5.0 GT/s PCIe x1 (!SSC)
[    0.495759] pci 0000:01:00.0: [dabc:1017] type 00 class 0x020000
&lt;/code&gt;
    &lt;head rend="h4"&gt;Background information#&lt;/head&gt;
    &lt;p&gt;Since most people might not be intimately as familiar with PCIe terminology, allow me to quickly document what is going on here.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;0000:00:00.0&lt;/code&gt;: is the identifier of a specific PCIe device connected through the PCIe network
to the kernel, it read as &lt;code&gt;domain&lt;/code&gt;:&lt;code&gt;bus&lt;/code&gt;:&lt;code&gt;device&lt;/code&gt;.&lt;code&gt;function&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;[14e4:2712]&lt;/code&gt;: is the device’s &lt;code&gt;[vendor id:device id]&lt;/code&gt;, these vendor id identifiers are
assigned by the PCI standard body to hardware vendors. Vendors are then free to define there
own vendor id’s.&lt;/p&gt;
    &lt;p&gt;The full list of official vendor id’s and released device id can be found : https://admin.pci-ids.ucw.cz/read/PC/14e4 or in the linux kernel code : https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;type 01&lt;/code&gt;: PCIe has two types of devices, bridges allowing the connection of multiple downstream devices to an
upstream device, and endpoints are the leafs.
Bridges are of type &lt;code&gt;01&lt;/code&gt; and endpoints of type &lt;code&gt;00&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;class 0x60400&lt;/code&gt;: is the PCIe device class, it categorizes the kind of function the device performs. It
uses the following format &lt;code&gt;0x[Base Class (8 bits)][Sub Class (8 bits)][Programming Interface (8 bits)]&lt;/code&gt;,
( note : the sub class field might be unused ).&lt;/p&gt;
    &lt;p&gt;A list of class and sub class identifiers can be found: https://admin.pci-ids.ucw.cz/read/PD or again in the linux codebase : https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158&lt;/p&gt;
    &lt;head rend="h4"&gt;Dmesg log#&lt;/head&gt;
    &lt;p&gt;The two most interesting lines of the &lt;code&gt;dmesg&lt;/code&gt; log are :&lt;/p&gt;
    &lt;code&gt;[    0.388790] pci 0000:00:00.0: [14e4:2712] type 01 class 0x060400
[    0.495759] pci 0000:01:00.0: [dabc:1017] type 00 class 0x020000
&lt;/code&gt;
    &lt;p&gt;Firstly the PCIe subsystem logs that at &lt;code&gt;0000:00:00.0&lt;/code&gt; it has discovered a Broadcom BCM2712 PCIe Bridge ( vendor id &lt;code&gt;14e4&lt;/code&gt;, device id &lt;code&gt;0x2712&lt;/code&gt; ).This bridge (type &lt;code&gt;01&lt;/code&gt;) class &lt;code&gt;0x0604xx&lt;/code&gt; tells us it is a PCI-to-PCI bridge, meaning it is essentially creating additional PCIe lanes downstream for endpoint devices or additional bridges.&lt;/p&gt;
    &lt;p&gt;The subsystem then discovers a second device at &lt;code&gt;0000:01:00.0&lt;/code&gt;, this is an endpoint (type &lt;code&gt;00&lt;/code&gt;), and class &lt;code&gt;0x02000&lt;/code&gt; tells us it is an ethernet networking equipment.&lt;lb/&gt;Of note &lt;code&gt;dabc&lt;/code&gt; doesn’t correspond to a known vendor id.
When designing a PCIe interface in hardware these
are parameters we can configured. Additionally, among the different ways Linux uses to identify which driver to load for a PCIe device
the vendor id and device id can be used for matching. Supposing we are implementing custom logic, in order to prevent any bug where the wrong driver
might be loaded, it is best to use a separate vendor id.
This also helps identify your custom accelerator at a glance and use it to load your custom driver.&lt;/p&gt;
    &lt;p&gt;As such, it is not surprising to see an unknown vendor id appear for an FPGA, this with the class as an ethernet networking device is a strong hint this is our board.&lt;/p&gt;
    &lt;head rend="h4"&gt;Full PCIe device status#&lt;/head&gt;
    &lt;p&gt;Dmesg logs have already given us a good indication that our FPGA board and its PCIe interface was working but to confirm with certainty that the device with vendor id &lt;code&gt;dabc&lt;/code&gt; is our FPGA we now turn to &lt;code&gt;lspci&lt;/code&gt;.
&lt;code&gt;lspci -vvv&lt;/code&gt; is the most verbose output and gives us a full overview of the detected PCIe devices capabilities and current configurations.&lt;/p&gt;
    &lt;p&gt;Broadcom bridge:&lt;/p&gt;
    &lt;code&gt;0000:00:00.0 PCI bridge: Broadcom Inc. and subsidiaries BCM2712 PCIe Bridge (rev 21) (prog-if 00 [Normal decode])
        Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;gt;SERR- &amp;lt;PERR- INTx-
        Latency: 0
        Interrupt: pin A routed to IRQ 38
        Bus: primary=00, secondary=01, subordinate=01, sec-latency=0
        Memory behind bridge: [disabled] [32-bit]
        Prefetchable memory behind bridge: 1800000000-182fffffff [size=768M] [32-bit]
        Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;lt;SERR- &amp;lt;PERR-
        BridgeCtl: Parity- SERR- NoISA- VGA- VGA16- MAbort- &amp;gt;Reset- FastB2B-
                PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
        Capabilities: [48] Power Management version 3
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold-)
                Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=1 PME-
        Capabilities: [ac] Express (v2) Root Port (Slot-), MSI 00
                DevCap: MaxPayload 512 bytes, PhantFunc 0
                        ExtTag- RBE+
                DevCtl: CorrErr- NonFatalErr- FatalErr- UnsupReq-
                        RlxdOrd+ ExtTag- PhantFunc- AuxPwr+ NoSnoop+
                        MaxPayload 512 bytes, MaxReadReq 512 bytes
                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 5GT/s, Width x1, ASPM L0s L1, Exit Latency L0s &amp;lt;2us, L1 &amp;lt;4us
                        ClockPM+ Surprise- LLActRep- BwNot+ ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s, Width x1
                        TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt+
                RootCap: CRSVisible+
                RootCtl: ErrCorrectable- ErrNon-Fatal- ErrFatal- PMEIntEna+ CRSVisible+
                RootSta: PME ReqID 0000, PMEStatus- PMEPending-
                DevCap2: Completion Timeout: Range ABCD, TimeoutDis+ NROPrPrP- LTR+
                         10BitTagComp- 10BitTagReq- OBFF Via WAKE#, ExtFmt- EETLPPrefix-
                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
                         FRS- LN System CLS Not Supported, TPHComp- ExtTPHComp- ARIFwd+
                         AtomicOpsCap: Routing- 32bit- 64bit- 128bitCAS-
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled, ARIFwd-
                         AtomicOpsCtl: ReqEn- EgressBlck-
                LnkCap2: Supported Link Speeds: 2.5-5GT/s, Crosslink- Retimer- 2Retimers- DRS+
                LnkCtl2: Target Link Speed: 5GT/s, EnterCompliance- SpeedDis-
                         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
                         Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete- EqualizationPhase1-
                         EqualizationPhase2- EqualizationPhase3- LinkEqualizationRequest-
                         Retimer- 2Retimers- CrosslinkRes: unsupported, DRS-
                         DownstreamComp: Link Up - Present
        Capabilities: [100 v1] Advanced Error Reporting
                UESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
                CESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
                CEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
                AERCap: First Error Pointer: 00, ECRCGenCap+ ECRCGenEn- ECRCChkCap+ ECRCChkEn-
                        MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
                HeaderLog: 00000000 00000000 00000000 00000000
                RootCmd: CERptEn+ NFERptEn+ FERptEn+
                RootSta: CERcvd- MultCERcvd- UERcvd- MultUERcvd-
                         FirstFatal- NonFatalMsg- FatalMsg- IntMsg 0
                ErrorSrc: ERR_COR: 0000 ERR_FATAL/NONFATAL: 0000
        Capabilities: [160 v1] Virtual Channel
                Caps:   LPEVC=0 RefClk=100ns PATEntryBits=1
                Arb:    Fixed- WRR32- WRR64- WRR128-
                Ctrl:   ArbSelect=Fixed
                Status: InProgress-
                VC0:    Caps:   PATOffset=00 MaxTimeSlots=1 RejSnoopTrans-
                        Arb:    Fixed- WRR32- WRR64- WRR128- TWRR128- WRR256-
                        Ctrl:   Enable+ ID=0 ArbSelect=Fixed TC/VC=ff
                        Status: NegoPending- InProgress-
        Capabilities: [180 v1] Vendor Specific Information: ID=0000 Rev=0 Len=028 &amp;lt;?&amp;gt;
        Capabilities: [240 v1] L1 PM Substates
                L1SubCap: PCI-PM_L1.2+ PCI-PM_L1.1+ ASPM_L1.2+ ASPM_L1.1+ L1_PM_Substates+
                          PortCommonModeRestoreTime=8us PortTPowerOnTime=10us
                L1SubCtl1: PCI-PM_L1.2- PCI-PM_L1.1- ASPM_L1.2- ASPM_L1.1-
                           T_CommonMode=1us LTR1.2_Threshold=0ns
                L1SubCtl2: T_PwrOn=10us
        Capabilities: [300 v1] Secondary PCI Express
                LnkCtl3: LnkEquIntrruptEn- PerformEqu-
                LaneErrStat: 0
        Kernel driver in use: pcieport
&lt;/code&gt;
    &lt;p&gt;FPGA board:&lt;/p&gt;
    &lt;code&gt;0000:01:00.0 Ethernet controller: Device dabc:1017
        Subsystem: Red Hat, Inc. Device a001
        Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;gt;SERR- &amp;lt;PERR- INTx-
        Region 0: Memory at 1820000000 (64-bit, prefetchable) [disabled] [size=2K]
        Region 2: Memory at 1800000000 (64-bit, prefetchable) [disabled] [size=512M]
        Capabilities: [40] Power Management version 3
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)
                Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=0 PME-
        Capabilities: [70] Express (v2) Endpoint, MSI 00
                DevCap: MaxPayload 1024 bytes, PhantFunc 0, Latency L0s &amp;lt;64ns, L1 &amp;lt;1us
                        ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset- SlotPowerLimit 0W
                DevCtl: CorrErr+ NonFatalErr+ FatalErr+ UnsupReq+
                        RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+
                        MaxPayload 512 bytes, MaxReadReq 512 bytes
                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s (downgraded), Width x1 (downgraded)
                        TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-
                DevCap2: Completion Timeout: Range BC, TimeoutDis+ NROPrPrP- LTR-
                         10BitTagComp- 10BitTagReq- OBFF Not Supported, ExtFmt- EETLPPrefix-
                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
                         FRS- TPHComp- ExtTPHComp-
                         AtomicOpsCap: 32bit- 64bit- 128bitCAS-
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled,
                         AtomicOpsCtl: ReqEn-
                LnkCap2: Supported Link Speeds: 2.5-8GT/s, Crosslink- Retimer- 2Retimers- DRS-
                LnkCtl2: Target Link Speed: 8GT/s, EnterCompliance- SpeedDis-
                         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
                         Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete- EqualizationPhase1-
                         EqualizationPhase2- EqualizationPhase3- LinkEqualizationRequest-
                         Retimer- 2Retimers- CrosslinkRes: unsupported
        Capabilities: [100 v1] Advanced Error Reporting
                UESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
                CESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
                CEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
                AERCap: First Error Pointer: 00, ECRCGenCap- ECRCGenEn- ECRCChkCap- ECRCChkEn-
                        MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
                HeaderLog: 00000000 00000000 00000000 00000000
        Capabilities: [1c0 v1] Secondary PCI Express
                LnkCtl3: LnkEquIntrruptEn- PerformEqu-
                LaneErrStat: 0
&lt;/code&gt;
    &lt;p&gt;For our board, the following lines are particularly interesting:&lt;/p&gt;
    &lt;code&gt;                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s (downgraded), Width x1 (downgraded)0x060400
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;LnkCap&lt;/code&gt; tells us about the full capabilities of this PCIe device, here we can see that
the current design supports PCIe Gen 3.0 x8.
The &lt;code&gt;LnkSta&lt;/code&gt; tells us the current configuration, here we have been downgraded to PCIe Gen 2.0 at 5GT/s with a width of only x1.&lt;/p&gt;
    &lt;p&gt;During startup of when a new PCIe device is plugged, PCIe performs a link speed and width negotiation where it tries to reach the highest supported stable configuration for the current system. In our current system, though our FPGA is capable of 8GT/s, as it is located downstream of the Broadcom bridge with a maximum link capacity of Gen 2.0 ( 5GT/s ), the FPGA has been downgraded to 5GT/s.&lt;/p&gt;
    &lt;p&gt;As for the width of x1, that is expected since the Broadcom bridge is also only x1 wide, and our board’s other 7 PCIe lanes are literally hanging over the side.&lt;/p&gt;
    &lt;p&gt;Thus, we can finally confirm that this is our board and that the PCIe interface is working. We can now proceed to establishing the JTAG connection.&lt;/p&gt;
    &lt;head rend="h2"&gt;JTAG interface#&lt;/head&gt;
    &lt;p&gt;Xilinx FPGAs can be configured by writing a bitstream to their internal CMOS Configuration Latches (CCL). CCL is SRAM memory and volatile, thus the configuration is re-done on every power cycle. For devices in the field this bitstream would be read from an external SPI memory during initialization, or written from an external device, such as an embedded controller. But for development purposes overwriting the contents of the CCLs over JTAG is acceptable.&lt;/p&gt;
    &lt;p&gt;This configuration is done by shifting in the entire FPGA bitstream into the device’s configuration logic over the JTAG bus.&lt;/p&gt;
    &lt;head rend="h3"&gt;FPGA board JTAG interface#&lt;/head&gt;
    &lt;p&gt;As promised by the original eBay listing the board did come with an accessible JTAG interface, and gloriously enough, this time there wasn’t even the need for any additional soldering.&lt;/p&gt;
    &lt;p&gt;In addition to a power reference, and ground, conformely to the Xilinx JTAG interface it featured the four mandatory signals comprising the JTAG TAP :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TCK Test Clock&lt;/item&gt;
      &lt;item&gt;TMS Test Mode Select&lt;/item&gt;
      &lt;item&gt;TDI Test Data Input&lt;/item&gt;
      &lt;item&gt;TDO Test Data Output&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of note, the JTAG interface can also come with an independent reset signal. But since Xilinx JTAG interfaces do not have this independent reset signal, we be using the JTAG FSM reset state for our reset signal.&lt;/p&gt;
    &lt;p&gt;This interface layout doesn’t follow a standard layout so I cannot just plug in one of my debug probes, it requires some re-wiring.&lt;/p&gt;
    &lt;head rend="h3"&gt;Segger JLINK :heart:#&lt;/head&gt;
    &lt;p&gt;I do not own an AMD approved JTAG programmer.&lt;/p&gt;
    &lt;p&gt;Traditionally speaking, the Segger JLink is used for debugging embedded CPUs let them be standalone or in a Zynq, and not for configuring FPGAs.&lt;/p&gt;
    &lt;p&gt;That said, all we need to do is use JTAG to shift in a bitstream to the CCLs, so technically speaking any programmable device with 4 sufficiently fast GPIOs can be used as a JTAG programmer. Additionally, the JLink is well supported by OpenOCD, the JLink’s libraries are open source, and I happened to own one.&lt;/p&gt;
    &lt;head rend="h4"&gt;Wiring#&lt;/head&gt;
    &lt;p&gt;Rewiring :&lt;/p&gt;
    &lt;p&gt;JTAG is a parallel protocol where &lt;code&gt;TDI&lt;/code&gt; and &lt;code&gt;TMS&lt;/code&gt; will be captured according to &lt;code&gt;TCK&lt;/code&gt;.
Because of this, good JTAG PCB trace length matching is advised in order to minimize skew.&lt;/p&gt;
    &lt;p&gt;Ideally a custom connector with length matched traces to work as an interface between the JLink’s probe and a board specific connector would be used.&lt;/p&gt;
    &lt;p&gt;Yet, here we are shoving breadboard wires between our debugger and the board. Since OpenOCD allows us to easily control the debugger clock speed, we can increase the skew tolerance by slowing down the TCK clock signal. As such there is no immediate need for a custom connector but we will not be able to reach the maximum JTAG speeds.&lt;/p&gt;
    &lt;p&gt;No issues were encountered at these speeds.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenOCD#&lt;/head&gt;
    &lt;p&gt;OpenOCD is a free and open source on-chip debugger software that aims to be compatible with as many probes, boards and chips as possible.&lt;/p&gt;
    &lt;p&gt;Since OpenOCD has support for the standard SVF file format, my plan for my flashing flow will be to use Vivado to generate the SVF and have OpenOCD flash it. Now, some of you might be starting to notice that I am diverging quite far from the well lit path of officially supported tools. Not only am I using a not officially supported debug probe, but I am also using some obscure open source software with questionable support for interfacing with Xilinx UltraScale+ FPGAs. You might be wondering, given that the officially supported tools can already prove themselves to be a headache to get working properly, why am I seemingly making my life even harder?&lt;/p&gt;
    &lt;p&gt;The reason is quite simple: when things inevitably start going wrong, as they will, having an entirely open toolchain, allows me to have more visibility as to what is going on and the ability to fix it. I cannot delve into a black box.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building OpenOCD#&lt;/head&gt;
    &lt;p&gt;By default the version of OpenOCD that I got on my server via the official packet manager was outdated and missing features I will need.&lt;/p&gt;
    &lt;p&gt;Also, since saving the ability to modify OpenOCD’s source code could come in handy, I decided to re-build it from source.&lt;/p&gt;
    &lt;p&gt;Thus, in the following logs, I will be running OpenOCD version &lt;code&gt;0.12.0+dev-02170-gfcff4b712&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note : I have also re-build the JLink libs from source.&lt;/p&gt;
    &lt;head rend="h3"&gt;Determining the scan chain#&lt;/head&gt;
    &lt;p&gt;Since I do not have the schematics for the board I do not know how many devices are daisy-chainned on the board JTAG bus. Also, I want to confirm if the FPGA on the ebay listing is actually the one on the board. In JTAG, each chained device exposes an accessible &lt;code&gt;IDCODE&lt;/code&gt; register used to identify the manufacturer, device type, and revision number.&lt;/p&gt;
    &lt;p&gt;When setting up the JTAG server, we typically define the scan chain by specifying the expected &lt;code&gt;IDCODE&lt;/code&gt; for each TAP and the corresponding instruction register length, so that instructions can be correctly aligned and routed to the intended device.
Given this is an undocumented board off Ebay, I do not know what the chain looks like.
Fortunately, OpenOCD has an autoprobing functionality, to do a blind interrogation in an attempt to discover
the available devices.&lt;/p&gt;
    &lt;p&gt;Thus, my first order of business was doing this autoprobing.&lt;/p&gt;
    &lt;p&gt;In OpenOCD the autoprobing is done when the configuration does not specify any taps.&lt;/p&gt;
    &lt;code&gt;source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED

reset_config none
&lt;/code&gt;
    &lt;p&gt;The blind interrogation successfully discovered a single device on the chain with an &lt;code&gt;IDCODE&lt;/code&gt; of &lt;code&gt;0x04a63093&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test/autoprob$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
none separate
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Warn : There are no enabled taps.  AUTO PROBING MIGHT NOT WORK!!
Info : JTAG tap: auto0.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : AUTO auto0.tap - use "jtag newtap auto0 tap -irlen 2 -expected-id 0x04a63093"
Error: IR capture error at bit 2, saw 0x3ffffffffffffff5 not 0x...3
Warn : Bypassing JTAG setup events due to errors
Warn : gdb services need one or more targets defined
&lt;/code&gt;
    &lt;p&gt;Comparing against the &lt;code&gt;UltraScale Architecture Configuration User Guide (UG570)&lt;/code&gt; we see that this &lt;code&gt;IDCODE&lt;/code&gt; matches up
precisely with the expected value for the &lt;code&gt;KU3P&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;By default OpenOCD assumes a JTAG IR length of 2 bits, while our FPGA has an IR length of 6 bits. This is the cause behind the IR capture error encountered during autoprobing. By updating the script with an IR length of 6 bits we can re-detect the FPGA with no errors.&lt;/p&gt;
    &lt;code&gt;source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED

reset_config none

jtag newtap auto_detect tap -irlen 6
&lt;/code&gt;
    &lt;p&gt;Output :&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test/autoprob$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Info : JTAG tap: auto_detect.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
&lt;/code&gt;
    &lt;p&gt;Based on the probing, this is the JTAG scan chain for our board :&lt;/p&gt;
    &lt;head rend="h3"&gt;System Monitor Registers#&lt;/head&gt;
    &lt;p&gt;Previous generations of Xilinx FPGA had a system called the XADC that, among other features, allowed you to acquire chip temperature and voltage readings. The newer UltraScale and UltraScale+ family have deprecated this XADC module in favor of the SYSMON (and SYSMON4) which allows you to also get these temperature readings, just better.&lt;/p&gt;
    &lt;p&gt;Unfortunately, openOCD didn’t have support for reading the SYSMON over JTAG out of the box, so I will be adding it.&lt;/p&gt;
    &lt;p&gt;To be more precise, the Kintex UltraScale+ has a SYSMON4 and not a SYSMON. For full context, there are 3 flavors of SYSMON:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SYSMON1&lt;/code&gt;used in the Kintex and Virtex UltraScale series&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SYSMON4&lt;/code&gt;used in the Kintex, Virtex and in the Zynq programmable logic for the UltraScale+ series&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SYSMON&lt;/code&gt;used in the Zynq in the processing system of the UltraScale+ series.&lt;lb/&gt;Yes, you read that correctly the Zynq of the UltraScale+ series features not one, but at least two unique SYSMON instances.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the purpose of this article, all these instances are similar enough that I will be using the terms SYSMON4 and SYSMON interchangeably.&lt;/p&gt;
    &lt;p&gt;In order for the JTAG to interact with the SYSMON, we first need to write the &lt;code&gt;SYSMON_DRP&lt;/code&gt; command to the
JTAG Instruction Register (IR).
Based on the documentation, we see that this command has a value of &lt;code&gt;0x37&lt;/code&gt;, which funnily enough,
is the same command code as the XADC, solidifying the SYSMON as the XADC’s descendant.&lt;/p&gt;
    &lt;p&gt;The SYSMON offers a lot more additional functionalities than just being used to read voltage and temperature, but for today’s use case we will not be using any of that. Rather, we will focus only on reading a subset of the SYSMON status registers.&lt;/p&gt;
    &lt;p&gt;These status registers are located at addresses &lt;code&gt;(00h-3Fh, 80h-BFh)&lt;/code&gt;,
and contain the measurement results of the analog-to-digital conversions, the flag registers,
and the calibration coefficients. We can select which address we wish to read by writing the
address to the Data Register (DR) over JTAG and the data will be read out of &lt;code&gt;TDO&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# SPDX-License-Identifier: GPL-2.0-or-later

# Xilinx SYSMON4 support
#
# Based on UG580, used for UltraScale+ Xilinx FPGA
# This code implements access through the JTAG TAP.
#
# build a 32 bit DRP command for the SYSMON DRP
proc sysmon_cmd {cmd addr data} {
	array set cmds {
		NOP 0x00
		READ 0x01
		WRITE 0x02
	}
	return [expr {($cmds($cmd) &amp;lt;&amp;lt; 26) | ($addr &amp;lt;&amp;lt; 16) | ($data &amp;lt;&amp;lt; 0)}]
}

# Status register addresses
# Some addresses (status registers 0-3) have special function when written to.
proc SYSMON {key} {
	array set addrs {
		TEMP 0x00
		VCCINT 0x01
		VCCAUX 0x02
		VPVN 0x03
		VREFP 0x04
		VREFN 0x05
		VCCBRAM 0x06
		SUPAOFFS 0x08
		ADCAOFFS 0x09
		ADCAGAIN 0x0a
		VCCPINTLP 0x0d
		VCCPINTFP 0x0e
		VCCPAUX 0x0f
		VAUX0 0x10
		VAUX1 0x11
		VAUX2 0x12
		VAUX3 0x13
		VAUX4 0x14
		VAUX5 0x15
		VAUX6 0x16
		VAUX7 0x17
		VAUX8 0x18
		VAUX9 0x19
		VAUX10 0x1a
		VAUX11 0x1b
		VAUX12 0x1c
		VAUX13 0x1d
		VAUX14 0x1e
		VAUX15 0x1f
		MAXTEMP 0x20
		MAXVCC 0x21
		MAXVCCAUX 0x22
	}
	return $addrs($key)
}

# transfer
proc sysmon_xfer {tap cmd addr data} {
	set ret [drscan $tap 32 [sysmon_cmd $cmd $addr $data]]
	runtest 10
	return [expr "0x$ret"]
}

# sysmon register write
proc sysmon_write {tap addr data} {
	sysmon_xfer $tap WRITE $addr $data
}

# sysmon register read, non-pipelined
proc sysmon_read {tap addr} {
	sysmon_xfer $tap READ $addr 0
	return [sysmon_xfer $tap NOP 0 0]
}


# Select the sysmon DR, SYSMON_DRP has the same binary code value as the XADC
proc sysmon_select {tap} {
	set SYSMON_IR 0x37
	irscan $tap $SYSMON_IR
	runtest 10
}

# convert 16 bit temperature measurement to Celsius
proc sysmon_temp_internal {code} {
	return [expr {$code * 509.314/(1 &amp;lt;&amp;lt; 16) - 280.23}]
}

# convert 16 bit supply voltage measurments to Volt
proc sysmon_sup {code} {
	return [expr {$code * 3./(1 &amp;lt;&amp;lt; 16)}]
}

# measure all internal voltages
proc sysmon_report {tap} {
	puts "Sysmon status report :"
	sysmon_select $tap
	foreach ch [list TEMP MAXTEMP] {
		echo "$ch [format %.2f [sysmon_temp_internal [sysmon_read $tap [SYSMON $ch]]]] C"
	}
	foreach ch [list VCCINT MAXVCC VCCAUX MAXVCCAUX] {
		echo "$ch [format %.3f [sysmon_sup [sysmon_read $tap [SYSMON $ch]]]] V"	
	}
}
&lt;/code&gt;
    &lt;p&gt;I added a report that reads the current chip temperature, internal and external voltages as well as the maximum values for these recorded since FPGA power cycle, to my flashing script output:&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-20:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
set chipname XCKU3P
Read temperature sysmon 4
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.819 V
Info : clock speed 1 kHz
Info : JTAG tap: XCKU3P.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
--------------------
Sysmon status report :
TEMP 31.12 C
MAXTEMP 34.62 C
VCCINT 0.852 V
MAXVCC 0.855 V
VCCAUX 1.805 V
MAXVCCAUX 1.807 V
&lt;/code&gt;
    &lt;head rend="h2"&gt;Pinout#&lt;/head&gt;
    &lt;p&gt;To my indescribable joy I happened to stumble onto this gold mine, in which we get the board pinout. This most likely fell off a truck: https://blog.csdn.net/qq_37650251/article/details/145716953&lt;/p&gt;
    &lt;p&gt;So far this pinout looks correct.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Pin Index&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;IO Standard&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
        &lt;cell role="head"&gt;Bank&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;diff_100mhz_clk_p&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;E18&lt;/cell&gt;
        &lt;cell&gt;BANK67&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;diff_100mhz_clk_n&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;D18&lt;/cell&gt;
        &lt;cell&gt;BANK67&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;sfp_mgt_clk_p&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;K7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;sfp_mgt_clk_n&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;K6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;sfp_1_txn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;sfp_1_txp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;sfp_1_rxn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;A3&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;sfp_1_rxp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;A4&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;sfp_2_txn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;D6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;sfp_2_txp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;D7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;sfp_2_rxn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B1&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;sfp_2_rxp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B2&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;SFP_1_MOD_DEF_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;SFP_1_TX_FAULT&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;SFP_1_LOS&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;SFP_1_LED&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;SFP_2_MOD_DEF_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;E11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;SFP_2_TX_FAULT&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;F9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;SFP_2_LOS&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;E10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;SFP_2_LED&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_SFP_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_SFP_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_SFP_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_SFP_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_EEPROM_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;G10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_EEPROM_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;G9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_EEPROM_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;J15&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_EEPROM_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;J14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_R&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;29&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_G&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_H&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;31&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_3&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;34&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_4&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;pcie_mgt_clkn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T6&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell&gt;pcie_mgt_clkp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T7&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;pcie_tx0_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;R4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;pcie_tx1_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;U4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;pcie_tx2_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;W4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;pcie_tx3_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AA4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;41&lt;/cell&gt;
        &lt;cell&gt;pcie_tx4_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AC4&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;pcie_tx5_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD6&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;pcie_tx6_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE8&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;pcie_tx7_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF6&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;pcie_rx0_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;P1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;46&lt;/cell&gt;
        &lt;cell&gt;pcie_rx1_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell&gt;pcie_rx2_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;V1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;pcie_rx3_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;Y1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;49&lt;/cell&gt;
        &lt;cell&gt;pcie_rx4_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AB1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;pcie_rx5_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;51&lt;/cell&gt;
        &lt;cell&gt;pcie_rx6_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE3&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;52&lt;/cell&gt;
        &lt;cell&gt;pcie_rx7_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;53&lt;/cell&gt;
        &lt;cell&gt;pcie_tx0_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;R5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;54&lt;/cell&gt;
        &lt;cell&gt;pcie_tx1_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;U5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;55&lt;/cell&gt;
        &lt;cell&gt;pcie_tx2_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;W5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;56&lt;/cell&gt;
        &lt;cell&gt;pcie_tx3_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AA5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;57&lt;/cell&gt;
        &lt;cell&gt;pcie_tx4_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AC5&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;58&lt;/cell&gt;
        &lt;cell&gt;pcie_tx5_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD7&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;59&lt;/cell&gt;
        &lt;cell&gt;pcie_tx6_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE9&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;60&lt;/cell&gt;
        &lt;cell&gt;pcie_tx7_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF7&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;61&lt;/cell&gt;
        &lt;cell&gt;pcie_rx0_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;P2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;62&lt;/cell&gt;
        &lt;cell&gt;pcie_rx1_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;63&lt;/cell&gt;
        &lt;cell&gt;pcie_rx2_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;V2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;pcie_rx3_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;Y2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;65&lt;/cell&gt;
        &lt;cell&gt;pcie_rx4_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AB2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;66&lt;/cell&gt;
        &lt;cell&gt;pcie_rx5_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;67&lt;/cell&gt;
        &lt;cell&gt;pcie_rx6_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE4&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;68&lt;/cell&gt;
        &lt;cell&gt;pcie_rx7_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;69&lt;/cell&gt;
        &lt;cell&gt;pcie_perstn_rst&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Global clock#&lt;/head&gt;
    &lt;p&gt;On high end FPGAs like the UltraScale+ family, high-speed global clocks are typically driven from external sources using differential pairs for better signal integrity.&lt;/p&gt;
    &lt;p&gt;According to the pinout we have two such differential pairs.&lt;/p&gt;
    &lt;p&gt;First I must determine the nature of these external reference clocks to see how I can use them to drive my clocks.&lt;/p&gt;
    &lt;p&gt;These differential pairs are provided over the following pins:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;100MHz : {E18, D18}&lt;/item&gt;
      &lt;item&gt;156.25MHz : {K7, K6}&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Judging by the naming and the frequencies, the 156.25MHz clock is likely my SFP reference clock, and the 100MHz can be used as my global clock.&lt;/p&gt;
    &lt;p&gt;We can confirm by querying the pin properties.&lt;/p&gt;
    &lt;p&gt;K6 properties :&lt;/p&gt;
    &lt;code&gt;Vivado% report_property [get_package_pins K6]
Property                Type    Read-only  Value
BANK                    string  true       227
BUFIO_2_REGION          string  true       TR
CLASS                   string  true       package_pin
DIFF_PAIR_PIN           string  true       K7
IS_BONDED               bool    true       1
IS_DIFFERENTIAL         bool    true       1
IS_GENERAL_PURPOSE      bool    true       0
IS_GLOBAL_CLK           bool    true       0
IS_LOW_CAP              bool    true       0
IS_MASTER               bool    true       0
IS_VREF                 bool    true       0
IS_VRN                  bool    true       0
IS_VRP                  bool    true       0
MAX_DELAY               int     true       38764
MIN_DELAY               int     true       38378
NAME                    string  true       K6
PIN_FUNC                enum    true       MGTREFCLK0N_227
PIN_FUNC_COUNT          int     true       1
PKGPIN_BYTEGROUP_INDEX  int     true       0
PKGPIN_NIBBLE_INDEX     int     true       0
&lt;/code&gt;
    &lt;p&gt;E18 properties :&lt;/p&gt;
    &lt;code&gt;Vivado% report_property [get_package_pins E18]
Property                Type    Read-only  Value
BANK                    string  true       67
BUFIO_2_REGION          string  true       TL
CLASS                   string  true       package_pin
DIFF_PAIR_PIN           string  true       D18
IS_BONDED               bool    true       1
IS_DIFFERENTIAL         bool    true       1
IS_GENERAL_PURPOSE      bool    true       1
IS_GLOBAL_CLK           bool    true       1
IS_LOW_CAP              bool    true       0
IS_MASTER               bool    true       1
IS_VREF                 bool    true       0
IS_VRN                  bool    true       0
IS_VRP                  bool    true       0
MAX_DELAY               int     true       87126
MIN_DELAY               int     true       86259
NAME                    string  true       E18
PIN_FUNC                enum    true       IO_L11P_T1U_N8_GC_67
PIN_FUNC_COUNT          int     true       2
PKGPIN_BYTEGROUP_INDEX  int     true       8
PKGPIN_NIBBLE_INDEX     int     true       2
&lt;/code&gt;
    &lt;p&gt;This tells us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The differential pairings are correct: {K6, K7}, {E18, D18}&lt;/item&gt;
      &lt;item&gt;We can easily use the 100MHz as a source to drive our global clocking network&lt;/item&gt;
      &lt;item&gt;The 156.25MHz clock is to be used as the reference clock for our GTY transceivers and lands on bank 227 as indicated by the &lt;code&gt;PIN_FUNC&lt;/code&gt;property&lt;code&gt;MGTREFCLK0N_227&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;We cannot directly use the 156.25MHz clock to drive our global clock network&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all this we have sufficient information to write a constraint file (&lt;code&gt;xdc&lt;/code&gt;) for this board.&lt;/p&gt;
    &lt;head rend="h2"&gt;Test design#&lt;/head&gt;
    &lt;p&gt;Further sections will be using the following design files.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;top.v&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;module top (
    input wire Clk_100mhz_p_i, 
    input wire Clk_100mhz_n_i,

    output wire [3:0] Led_o 
);
    wire        clk_ibuf;
    reg  [28:0] ctr_q; 
    reg         unused_ctr_q;


    IBUFDS #(
        .DIFF_TERM("TRUE"),
        .IOSTANDARD("LVDS")
    ) m_ibufds (
        .I(Clk_100mhz_p_i),
        .IB(Clk_100mhz_n_i),
        .O(clk_ibuf)
    );

    BUFG m_bufg (
        .I(clk_ibuf),
        .O(clk)
    );

    always @(posedge clk)
        { unused_ctr_q, ctr_q } &amp;lt;= ctr_q + 29'b1;    
    
    assign Led_o = ctr_q[28:25];
endmodule
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;alibaba_cloud.xdc&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;# Global clock signal 
set_property -dict {LOC E18 IOSTANDARD LVDS} [get_ports Clk_100mhz_p_i]
set_property -dict {LOC D18 IOSTANDARD LVDS} [get_ports Clk_100mhz_n_i]
create_clock -period 10 -name clk_100mhz [get_ports Clk_100mhz_p_i]

# LEDS
set_property -dict {LOC B11 IOSTANDARD LVCMOS18} [get_ports { Led_o[0]}]
set_property -dict {LOC C11 IOSTANDARD LVCMOS18} [get_ports { Led_o[1]}]
set_property -dict {LOC A10 IOSTANDARD LVCMOS18} [get_ports { Led_o[2]}]
set_property -dict {LOC B10 IOSTANDARD LVCMOS18} [get_ports { Led_o[3]}]
&lt;/code&gt;
    &lt;head rend="h2"&gt;Writing the bitstream#&lt;/head&gt;
    &lt;p&gt;My personal belief is that one of the most important contributors to design quality is iteration cost. The lower your iteration cost, the higher your design quality is going to be.&lt;/p&gt;
    &lt;p&gt;As such I will invest the small upfront cost to have the workflow be as streamlined as efficiently feasible.&lt;/p&gt;
    &lt;p&gt;Thus, my workflow evolved into doing practically everything over the command line interfaces and only interacting with the tools, Vivado in this case, through tcl scripts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vivado flow#&lt;/head&gt;
    &lt;p&gt;The goal of this flow is to, given a few verilog design and constraint files produce a SVF file. Our steps are :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;creat the Vivado project &lt;code&gt;setup.tcl&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;run the implementation &lt;code&gt;build.tcl&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;generate the bitstream and the SVF &lt;code&gt;gen.tcl&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I will be using &lt;code&gt;make&lt;/code&gt; to kick off and manage the dependencies between the different steps, though I recognise this isn’t a widespread practice for hardware projects. &lt;code&gt;make&lt;/code&gt; is a highly flexible, reliable and powerful tool and I believe its ability to tie together any type of workflow makes it a prime tool for this use case.&lt;/p&gt;
    &lt;p&gt;We will be invoking Vivado in batch mode, this allows us to provide a tcl script alongside script arguments, the format is as following :&lt;/p&gt;
    &lt;code&gt;vivado -mode batch &amp;lt;path to tcl script&amp;gt; -tclargs &amp;lt;script args&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Though this allows us to easily break down our flow into incremental stages, invoking a single script in batch mode has the drawback of restarting Vivado and needing to re-load the project or the project checkpoint on each invocation.&lt;/p&gt;
    &lt;p&gt;As the project size grows so will the project load time, so segmenting the flow into a large number of independent scripts comes at an increasing cost.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Makefile&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;SHELL := /bin/bash

VIVADO_PRJ_DIR=prj
VIVADO_PRJ_NAME=$(VIVADO_PRJ_DIR)
VIVADO_PRJ_PATH=$(VIVADO_PRJ_DIR)/$(VIVADO_PRJ_NAME).xpr
VIVADO_CHECKPOINT_PATH=$(VIVADO_PRJ_DIR)/$(VIVADO_PRJ_NAME)_checkpoint.dcp

VIVADO_CMD=vivado -mode batch -source

SRC_PATH=src
OUT_DIR=out


all: setup build gen

$(VIVADO_PRJ_PATH):  
    mkdir -p $(VIVADO_PRJ_DIR)
    $(VIVADO_CMD) setup.tcl -tclargs $(VIVADO_PRJ_DIR) $(VIVADO_PRJ_NAME)

setup: $(VIVADO_PRJ_PATH) 

$(VIVADO_CHECKPOINT_PATH): $(VIVADO_PRJ_PATH) $(wildcard $(SRC_PATH)/*.xdc) $(wildcard $(SRC_PATH)/*.v)
    $(VIVADO_CMD) build.tcl -tclargs $(VIVADO_PRJ_PATH) $(SRC_PATH) $(VIVADO_CHECKPOINT_PATH)

build: $(VIVADO_CHECKPOINT_PATH)

$(OUT_DIR)/$(VIVADO_PRJ_NAME).svf: $(VIVADO_CHECKPOINT_PATH) 
    mkdir -p $(OUT_DIR)
    $(VIVADO_CMD) gen.tcl -tclargs $(VIVADO_CHECKPOINT_PATH) $(OUT_DIR)

gen: $(OUT_DIR)/$(VIVADO_PRJ_NAME).svf

flash: $(OUT_DIR)/$(VIVADO_PRJ_NAME).svf
    openocd	

clean: 
    rm -rf $(VIVADO_PRJ_DIR)
    rm -rf $(OUT_DIR)
    rm -f vivado*{log,jou}
    rm -f webtalk*{log,jou}
    rm -f usage_statistics_webtalk*{html,xml}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;setup.tcl&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set project_dir [lindex $argv 0]
set project_name [lindex $argv 1]

puts "Creating project $project_name at path [pwd]/$project_dir"
create_project -part xcku3p-ffvb676-2-e -force $project_name $project_dir

close_project
exit 0
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;build.tcl&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set project_path [lindex $argv 0]
set src_path [lindex $argv 1]
set checkpoint_path [lindex $argv 2]
puts "Implementation script called with project path $project_path and src path $src_path, generating checkpoint at $checkpoint_path"

open_project $project_path 

# load src
read_verilog [glob -directory $src_path *.v]
read_xdc [glob -directory $src_path *.xdc]


# synth
synth_design -top top

# implement
opt_design
place_design
route_design
phys_opt_design

write_checkpoint $checkpoint_path -force 
close_project
exit 0
&lt;/code&gt;
    &lt;head rend="h4"&gt;Generating the SVF file#&lt;/head&gt;
    &lt;p&gt;The SVF for Serial Vector Format is a human readable, vendor agnostic specification used to specify JTAG bus operations.&lt;/p&gt;
    &lt;p&gt;Example SVF file, test program:&lt;/p&gt;
    &lt;code&gt;! Initialize UUT
STATE RESET;
! End IR scans in DRPAUSE
ENDIR DRPAUSE;
! End DR scans in DRPAUSE
ENDDR DRPAUSE;
! 24 bit IR header
HIR 24 TDI (FFFFFF);
! 3 bit DR header
HDR 3 TDI (7);
! 16 bit IR trailer
TIR 16 TDI (FFFF);
! 2 bit DR trailer
TDR 2 TDI (3);
! 8 bit IR scan, load BIST opcode
SIR 8 TDI (41) TDO (81) MASK (FF);
! 16 bit DR scan, load BIST seed
SDR 16 TDI (ABCD);
! RUNBIST for 95 TCK Clocks
RUNTEST 95 TCK ENDSTATE IRPAUSE;
! 16 bit DR scan, check BIST status
SDR 16 TDI (0000) TDO(1234) MASK(FFFF);
! Enter Test-Logic-Reset
STATE RESET;
! End Test Program
&lt;/code&gt;
    &lt;p&gt;Vivado can generate a hardware aware SVF file containing the configuration sequence for an FPGA board, allowing us to write a bitstream.&lt;/p&gt;
    &lt;p&gt;Given the SVF file literally contains the bitstream written in clear hexademical, in the file, our first step is to generate our design’s bitstream.&lt;/p&gt;
    &lt;p&gt;Vivado proper isn’t the software that generates the SVF file, this task is done by the hardware manager which handles all of the configuration.&lt;/p&gt;
    &lt;p&gt;We can launch a new instance &lt;code&gt;open_hw_manager&lt;/code&gt; and connect to it &lt;code&gt;connect_hw_server&lt;/code&gt;.
Since JTAG is a daisy chained bus, and given the SVF file is just a standardised way of specifying
JTAG bus operations, in order to generate a correct JTAG configuration sequence, we must inform the hardware manger
of our scan chain.&lt;/p&gt;
    &lt;p&gt;During our earlier probing of the scan chain, we have established that our FPGA is the only device on the chain. We inform the hardware manager of this by creating a new device configuration ( the term “device” refers to the “board” here ) and add our fpga to the chain using the &lt;code&gt;create_hw_device -part &amp;lt;device name&amp;gt;&lt;/code&gt;.When we have multiple
devices we should register them following the order in which they appear on the chain.&lt;/p&gt;
    &lt;p&gt;Finally to generate the SVF file, we must select the device we wish to program with &lt;code&gt;program_hw_device &amp;lt;hw_device&amp;gt;&lt;/code&gt;,
then write out the SVF to the file using &lt;code&gt;write_hw_svf &amp;lt;path to svf file&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;gen.tcl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;set checkpoint_path [lindex $argv 0]
set out_dir [lindex $argv 1]
puts "SVF generation script called with checkpoint path $checkpoint_path, generating to $out_dir"

open_checkpoint $checkpoint_path

# defines
set hw_target "alibaba_board_svf_target"
set fpga_device "xcku3p"
set bin_path "$out_dir/[current_project]"

write_bitstream "$bin_path.bit" -force

open_hw_manager

# connect to hw server with default config
connect_hw_server
puts "connected to hw server at [current_hw_server]"

create_hw_target $hw_target
puts "current hw target [current_hw_target]"

open_hw_target

# single device on scan chain
create_hw_device -part $fpga_device
puts "scan chain : [get_hw_devices]"

set_property PROGRAM.FILE "$bin_path.bit" [get_hw_device]

#select device to program
program_hw_device [get_hw_device]

# generate svf file
write_hw_svf -force "$bin_path.svf"

close_hw_manager
exit 0
&lt;/code&gt;
    &lt;head rend="h3"&gt;Configuring the FPGA using OpenOCD#&lt;/head&gt;
    &lt;p&gt;Although not widespread openOCD has a very nice &lt;code&gt;svf&lt;/code&gt; execution command :&lt;/p&gt;
    &lt;quote&gt;&lt;head&gt;18.1 SVF: Serial Vector Format#&lt;/head&gt;&lt;p&gt;The Serial Vector Format, better known as SVF, is a way to represent JTAG test patterns in text files. In a debug session using JTAG for its transport protocol, OpenOCD supports running such test files.&lt;/p&gt;&lt;code&gt;[Command]svf filename [-tap tapname] [[-]quiet] [[-]nil] [[-]progress] [[-]ignore_error]&lt;/code&gt;&lt;p&gt;This issues a JTAG reset (Test-Logic-Reset) and then runs the SVF script from filename. Arguments can be specified in any order; the optional dash doesn’t affect their se- mantics.&lt;/p&gt;&lt;p&gt;Command options:&lt;/p&gt;&lt;code&gt;-tap&lt;/code&gt;tapname ignore IR and DR headers and footers specified by the SVF file with HIR, TIR, HDR and TDR commands; instead, calculate them automatically according to the current JTAG chain configuration, targeting tapname;&lt;code&gt;[-]quiet&lt;/code&gt;do not log every command before execution;&lt;code&gt;[-]nil&lt;/code&gt;“dry run”, i.e., do not perform any operations on the real interface;&lt;code&gt;[-]progress&lt;/code&gt;enable progress indication;&lt;code&gt;[-]ignore&lt;/code&gt;_error continue execution despite TDO check errors.&lt;/quote&gt;
    &lt;p&gt;We invoke it in our openOCD script using the &lt;code&gt;-progress&lt;/code&gt; option for additional logging.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;openocd&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set svf_path "out/project_prj_checkpoint.svf"

source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED 
reset_config none

# jlink config

set CHIPNAME XCKU3P
set CHIP $CHIPNAME
puts "set chipname "$CHIP

source [find ../openocd/tcl/cpld/xilinx-xcu.cfg]

source [find ../openocd/tcl/fpga/xilinx-sysmon.cfg]

init 

puts "--------------------"

sysmon_report $CHIP.tap

puts "--------------------"

# program
if {![file exists $svf_path]} {
    puts "Svf path not found : $svf_path"
    exit
}

svf $svf_path -progress 
 
exit 
&lt;/code&gt;
    &lt;p&gt;Flashing sequence log :&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
set chipname XCKU3P
Read temperature sysmon 4
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Info : JTAG tap: XCKU3P.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
--------------------
Sysmon status report :
TEMP 50.46 C
MAXTEMP 52.79 C
VCCINT 0.846 V
MAXVCC 0.860 V
VCCAUX 1.799 V
MAXVCCAUX 1.809 V
--------------------
svf processing file: "out/project_prj_checkpoint.svf"
  0%  TRST OFF;
  0%  ENDIR IDLE;
  0%  ENDDR IDLE;
  0%  STATE RESET;
  0%  STATE IDLE;
  0%  FREQUENCY 1.00E+07 HZ;
adapter speed: 10000 kHz
  0%  HIR 0 ;
  0%  TIR 0 ;
  0%  HDR 0 ;
  0%  TDR 0 ;
  0%  SIR 6 TDI (09) ;
  0%  SDR 32 TDI (00000000) TDO (04a63093) MASK (0fffffff) ;
  0%  STATE RESET;
  0%  STATE IDLE;
  0%  SIR 6 TDI (0b) ;
  0%  SIR 6 TDI (14) ;
  0%  RUNTEST 0.100000 SEC;
  0%  RUNTEST 10000 TCK;
  0%  SIR 6 TDI (14) TDO (11) MASK (31) ;
  0%  SIR 6 TDI (05) ;
 95%  ffffffffffff) ;
 95%  SIR 6 TDI (09) TDO (31) MASK (11) ;
 95%  STATE RESET;
 95%  RUNTEST 5 TCK;
 95%  SIR 6 TDI (05) ;
 95%  SDR 160 TDI (0000000400000004800700140000000466aa9955) ;
 95%  SIR 6 TDI (04) ;
 95%  SDR 32 TDI (00000000) TDO (3f5e0d40) MASK (08000000) ;
 95%  STATE RESET;
 95%  RUNTEST 5 TCK;
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
&lt;/code&gt;
    &lt;p&gt;Resulting in a successfully configured our FPGA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;For $200 we got a fully working decommissioned Alibaba Cloud accelerator featuring a Kintex UltraScale+ FPGA with an easily accessible debugging/programming interface and enough pinout information to define our own constraint files.&lt;/p&gt;
    &lt;p&gt;We also have a fully automated Vivado workflow to implement our designs and the ability to write the bitstream, and interface with the FPGA’s internal JTAG accessible registers using an open source programming tool without the need for an official Xilinx programmer.&lt;/p&gt;
    &lt;p&gt;In the end, this project delivered an at least 5x cost savings over commercial boards (compared to the lowest cost $900-1050 Alinx alternatives), making this perhaps the most cost effective entry point for a Kintex UltraScale+ board.&lt;/p&gt;
    &lt;head rend="h2"&gt;External ressources#&lt;/head&gt;
    &lt;p&gt;Xilinx Vivado Supported Devices : https://docs.amd.com/r/en-US/ug973-vivado-release-notes-install-license/Supported-Devices&lt;/p&gt;
    &lt;p&gt;Official Xilinx dev board : https://www.amd.com/en/products/adaptive-socs-and-fpgas/evaluation-boards/ek-u1-kcu116-g.html&lt;/p&gt;
    &lt;p&gt;Alinx Kintex UltraScale+ dev boards : https://www.en.alinx.com/Product/FPGA-Development-Boards/Kintex-UltraScale-plus.html&lt;/p&gt;
    &lt;p&gt;UltraScale Architecture Configuration User Guide (UG570) : https://docs.amd.com/r/en-US/ug570-ultrascale-configuration/Device-Resources-and-Configuration-Bitstream-Lengths?section=gyn1703168518425__table_vyh_4hs_szb&lt;/p&gt;
    &lt;p&gt;UltraScale Architecture System Monitor User Guide (UG580): https://docs.amd.com/v/u/en-US/ug580-ultrascale-sysmon&lt;/p&gt;
    &lt;p&gt;Vivado Design Suite Tcl Command Reference Guide (UG835): https://docs.amd.com/r/en-US/ug835-vivado-tcl-commands/Tcl-Initialization-Scripts&lt;/p&gt;
    &lt;p&gt;PCI vendor/device ID database: https://admin.pci-ids.ucw.cz/read/PC/14e4&lt;/p&gt;
    &lt;p&gt;PCI device classes: https://admin.pci-ids.ucw.cz/read/PD&lt;/p&gt;
    &lt;p&gt;Linux kernel PCI IDs: https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256&lt;/p&gt;
    &lt;p&gt;Linux kernel PCI classes: https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158&lt;/p&gt;
    &lt;p&gt;Truck-kun pinout: https://blog.csdn.net/qq_37650251/article/details/145716953&lt;/p&gt;
    &lt;p&gt;Ebay listing: https://www.ebay.com/itm/167626831054?_trksid=p4375194.c101800.m5481&lt;/p&gt;
    &lt;p&gt;OpenOCD documentation: https://openocd.org/doc-release/pdf/openocd.pdf&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://essenceia.github.io/projects/alibaba_cloud_fpga/"/><published>2025-10-04T06:49:58+00:00</published></entry></feed>