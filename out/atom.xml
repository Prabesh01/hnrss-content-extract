<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-03T05:40:09.005541+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46120181</id><title>Addressing the adding situation</title><updated>2025-12-03T05:40:17.183043+00:00</updated><content>&lt;doc fingerprint="aaebfc0e603b9ac8"&gt;
  &lt;main&gt;
    &lt;p&gt;Written by me, proof-read by an LLM. &lt;lb/&gt;Details at end.&lt;/p&gt;
    &lt;p&gt;Yesterday we saw how compilers zero registers efficiently. Today let’s look at something a tiny bit less trivial (though not by much): adding two integers. What do you think a simple x86 function to add two ints1 would look like? An &lt;code&gt;add&lt;/code&gt;, right? Let’s take a look!&lt;/p&gt;
    &lt;p&gt;Probably not what you were thinking, right? x86 is unusual in mostly having a maximum of two operands per instruction2. There’s no &lt;code&gt;add&lt;/code&gt; instruction to add &lt;code&gt;edi&lt;/code&gt; to &lt;code&gt;esi&lt;/code&gt;, putting the result in &lt;code&gt;eax&lt;/code&gt;. On an ARM machine this would be a simple &lt;code&gt;add r0, r0, r1&lt;/code&gt; or similar, as ARM has a separate destination operand. On x86, things like &lt;code&gt;add&lt;/code&gt; are not &lt;code&gt;result = lhs + rhs&lt;/code&gt; but &lt;code&gt;lhs += rhs&lt;/code&gt;. This can be a limitation, as we don’t get to control which register the result goes into, and we in fact lose the old value of &lt;code&gt;lhs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So how do compilers work around this limitation? The answer lies in an unexpected place - the sophisticated memory addressing system of the x86. Nearly every operand can be a memory reference - there’s no specific “load” or “store”; a &lt;code&gt;mov&lt;/code&gt; can just refer to memory directly. Those memory references are pretty rich: you can refer to memory addressed by a constant, relative to a register, or relative to a register plus an offset (optionally multiplied by 1, 2, 4 or 8). Something like &lt;code&gt;add eax, word ptr [rdi + rsi * 4 + 0x1000]&lt;/code&gt; is still a single instruction3!&lt;/p&gt;
    &lt;p&gt;Sometimes you don’t want to access the memory at one of these complex addresses, you just want to calculate what the address would be. Sort of like C’s “address-of” (&lt;code&gt;&amp;amp;&lt;/code&gt;) operator. That’s what &lt;code&gt;lea&lt;/code&gt; (Load Effective Address) does: it calculates the address without touching memory.&lt;/p&gt;
    &lt;p&gt;Why is this useful for addition? Well, if we’re not actually accessing memory, we can abuse the addressing hardware as a calculator! That complex addressing mode with its register-plus-register-times-scale is really just shifting and adding - so &lt;code&gt;lea&lt;/code&gt; becomes a cheeky way to do three-operand addition4.&lt;/p&gt;
    &lt;p&gt;The compiler writes our simple addition in terms of the address of memory at &lt;code&gt;rdi&lt;/code&gt; offset by &lt;code&gt;rsi&lt;/code&gt;. We get a full add of two registers and we get to specify the destination too. You’ll notice that the operands are referenced as &lt;code&gt;rdi&lt;/code&gt; and &lt;code&gt;rsi&lt;/code&gt; (the 64-bit version) even though we only wanted a 32-bit add: because we are using the memory addressing system it unconditionally calculates a 64-bit address. However, in this case it doesn’t matter; those top bits5 are discarded when the result is written to the 32-bit &lt;code&gt;eax&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;lea&lt;/code&gt; often saves an instruction, is useful if both of the operands are still needed later on in other calculations (as it leaves them unchanged), and can execute on x86’s multiple execution units in the same cycle. Compilers know this though, so you don’t have to worry!&lt;/p&gt;
    &lt;p&gt;See the video that accompanies this post.&lt;/p&gt;
    &lt;p&gt;This post is day 2 of Advent of Compiler Optimisations 2025, a 25-day series exploring how compilers transform our code.&lt;/p&gt;
    &lt;p&gt;This post was written by a human (Matt Godbolt) and reviewed and proof-read by LLMs and humans.&lt;/p&gt;
    &lt;p&gt;Support Compiler Explorer on Patreon or GitHub, or by buying CE products in the Compiler Explorer Shop.&lt;/p&gt;
    &lt;p&gt;The Linux system I’m compiling for here passes parameters in &lt;code&gt;edi&lt;/code&gt; and &lt;code&gt;esi&lt;/code&gt;, and expects the result in &lt;code&gt;eax&lt;/code&gt;. We’ll cover calling conventions later in the series. ↩&lt;/p&gt;
    &lt;p&gt;Though some AVX instructions and some multiplies do allow a separate destination. ↩&lt;/p&gt;
    &lt;p&gt;As someone who grew up with 6502, and then 32-bit ARM, coming to the x86 ISA was quite a shock. The x86 is truly a “Complex Instruction Set Computer”. ↩&lt;/p&gt;
    &lt;p&gt;Three-operand meaning we can specify two source registers and a separate destination, unlike &lt;code&gt;add&lt;/code&gt; which overwrites one of its operands. ↩&lt;/p&gt;
    &lt;p&gt;Those top bits should be zero, as the ABI requires it: the compiler relies on this here. Try editing the example above to pass and return &lt;code&gt;long&lt;/code&gt;s to compare. ↩&lt;/p&gt;
    &lt;p&gt;Matt Godbolt is a C++ developer living in Chicago. He works for Hudson River Trading on super fun but secret things. He is one half of the Two's Complement podcast. Follow him on Mastodon or Bluesky.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xania.org/202512/02-adding-integers"/><published>2025-12-02T11:30:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46120611</id><title>Python Data Science Handbook</title><updated>2025-12-03T05:40:17.095320+00:00</updated><content>&lt;doc fingerprint="a7599437eedc495a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Python Data Science Handbook&lt;/head&gt;
    &lt;p&gt;Jake VanderPlas&lt;/p&gt;
    &lt;p&gt;This website contains the full text of the Python Data Science Handbook by Jake VanderPlas; the content is available on GitHub in the form of Jupyter notebooks.&lt;/p&gt;
    &lt;p&gt;The text is released under the CC-BY-NC-ND license, and code is released under the MIT license.&lt;/p&gt;
    &lt;p&gt;If you find this content useful, please consider supporting the work by buying the book!&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Preface¶&lt;/head&gt;
    &lt;head rend="h3"&gt;1. IPython: Beyond Normal Python¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Help and Documentation in IPython&lt;/item&gt;
      &lt;item&gt;Keyboard Shortcuts in the IPython Shell&lt;/item&gt;
      &lt;item&gt;IPython Magic Commands&lt;/item&gt;
      &lt;item&gt;Input and Output History&lt;/item&gt;
      &lt;item&gt;IPython and Shell Commands&lt;/item&gt;
      &lt;item&gt;Errors and Debugging&lt;/item&gt;
      &lt;item&gt;Profiling and Timing Code&lt;/item&gt;
      &lt;item&gt;More IPython Resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2. Introduction to NumPy¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Understanding Data Types in Python&lt;/item&gt;
      &lt;item&gt;The Basics of NumPy Arrays&lt;/item&gt;
      &lt;item&gt;Computation on NumPy Arrays: Universal Functions&lt;/item&gt;
      &lt;item&gt;Aggregations: Min, Max, and Everything In Between&lt;/item&gt;
      &lt;item&gt;Computation on Arrays: Broadcasting&lt;/item&gt;
      &lt;item&gt;Comparisons, Masks, and Boolean Logic&lt;/item&gt;
      &lt;item&gt;Fancy Indexing&lt;/item&gt;
      &lt;item&gt;Sorting Arrays&lt;/item&gt;
      &lt;item&gt;Structured Data: NumPy's Structured Arrays&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;3. Data Manipulation with Pandas¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Introducing Pandas Objects&lt;/item&gt;
      &lt;item&gt;Data Indexing and Selection&lt;/item&gt;
      &lt;item&gt;Operating on Data in Pandas&lt;/item&gt;
      &lt;item&gt;Handling Missing Data&lt;/item&gt;
      &lt;item&gt;Hierarchical Indexing&lt;/item&gt;
      &lt;item&gt;Combining Datasets: Concat and Append&lt;/item&gt;
      &lt;item&gt;Combining Datasets: Merge and Join&lt;/item&gt;
      &lt;item&gt;Aggregation and Grouping&lt;/item&gt;
      &lt;item&gt;Pivot Tables&lt;/item&gt;
      &lt;item&gt;Vectorized String Operations&lt;/item&gt;
      &lt;item&gt;Working with Time Series&lt;/item&gt;
      &lt;item&gt;High-Performance Pandas: eval() and query()&lt;/item&gt;
      &lt;item&gt;Further Resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;4. Visualization with Matplotlib¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple Line Plots&lt;/item&gt;
      &lt;item&gt;Simple Scatter Plots&lt;/item&gt;
      &lt;item&gt;Visualizing Errors&lt;/item&gt;
      &lt;item&gt;Density and Contour Plots&lt;/item&gt;
      &lt;item&gt;Histograms, Binnings, and Density&lt;/item&gt;
      &lt;item&gt;Customizing Plot Legends&lt;/item&gt;
      &lt;item&gt;Customizing Colorbars&lt;/item&gt;
      &lt;item&gt;Multiple Subplots&lt;/item&gt;
      &lt;item&gt;Text and Annotation&lt;/item&gt;
      &lt;item&gt;Customizing Ticks&lt;/item&gt;
      &lt;item&gt;Customizing Matplotlib: Configurations and Stylesheets&lt;/item&gt;
      &lt;item&gt;Three-Dimensional Plotting in Matplotlib&lt;/item&gt;
      &lt;item&gt;Geographic Data with Basemap&lt;/item&gt;
      &lt;item&gt;Visualization with Seaborn&lt;/item&gt;
      &lt;item&gt;Further Resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;5. Machine Learning¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What Is Machine Learning?&lt;/item&gt;
      &lt;item&gt;Introducing Scikit-Learn&lt;/item&gt;
      &lt;item&gt;Hyperparameters and Model Validation&lt;/item&gt;
      &lt;item&gt;Feature Engineering&lt;/item&gt;
      &lt;item&gt;In Depth: Naive Bayes Classification&lt;/item&gt;
      &lt;item&gt;In Depth: Linear Regression&lt;/item&gt;
      &lt;item&gt;In-Depth: Support Vector Machines&lt;/item&gt;
      &lt;item&gt;In-Depth: Decision Trees and Random Forests&lt;/item&gt;
      &lt;item&gt;In Depth: Principal Component Analysis&lt;/item&gt;
      &lt;item&gt;In-Depth: Manifold Learning&lt;/item&gt;
      &lt;item&gt;In Depth: k-Means Clustering&lt;/item&gt;
      &lt;item&gt;In Depth: Gaussian Mixture Models&lt;/item&gt;
      &lt;item&gt;In-Depth: Kernel Density Estimation&lt;/item&gt;
      &lt;item&gt;Application: A Face Detection Pipeline&lt;/item&gt;
      &lt;item&gt;Further Machine Learning Resources&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jakevdp.github.io/PythonDataScienceHandbook/"/><published>2025-12-02T12:38:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46121539</id><title>Zig's new plan for asynchronous programs</title><updated>2025-12-03T05:40:16.670869+00:00</updated><content>&lt;doc fingerprint="b612da05ee6f12c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Zig's new plan for asynchronous programs&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The designers of the Zig programming language have been working to find a suitable design for asynchronous code for some time. Zig is a carefully minimalist language, and its initial design for asynchronous I/O did not fit well with its other features. Now, the project has announced (in a Zig SHOWTIME video) a new approach to asynchronous I/O that promises to solve the function coloring problem, and allows writing code that will execute correctly using either synchronous or asynchronous I/O.&lt;/p&gt;
    &lt;p&gt;In many languages (including Python, JavaScript, and Rust), asynchronous code uses special syntax. This can make it difficult to reuse code between synchronous and asynchronous parts of a program, introducing a number of headaches for library authors. Languages that don't make a syntactical distinction (such as Haskell) essentially solve the problem by making everything asynchronous, which typically requires the language's runtime to bake in ideas about how programs are allowed to execute.&lt;/p&gt;
    &lt;p&gt;Neither of those options was deemed suitable for Zig. Its designers wanted to find an approach that did not add too much complexity to the language, that still permitted fine control over asynchronous operations, and that still made it relatively painless to actually write high-performance event-driven I/O. The new approach solves this by hiding asynchronous operations behind a new generic interface, Io.&lt;/p&gt;
    &lt;p&gt;Any function that needs to perform an I/O operation will need to have access to an instance of the interface. Typically, that is provided by passing the instance to the function as a parameter, similar to Zig's Allocator interface for memory allocation. The standard library will include two built-in implementations of the interface: Io.Threaded and Io.Evented. The former uses synchronous operations except where explicitly asked to run things in parallel (with a special function; see below), in which case it uses threads. The latter (which is still a work-in-progress) uses an event loop and asynchronous I/O. Nothing in the design prevents a Zig programmer from implementing their own version, however, so Zig's users retain their fine control over how their programs execute.&lt;/p&gt;
    &lt;p&gt;Loris Cro, one of Zig's community organizers, wrote an explanation of the new behavior to justify the approach. Synchronous code is not much changed, other than using the standard library functions that have moved under Io, he explained. Functions like the example below, which don't involve explicit asynchronicity, will continue to work. This example creates a file, sets the file to close at the end of the function, and then writes a buffer of data to the file. It uses Zig's try keyword to handle errors, and defer to ensure the file is closed. The return type, !void, indicates that it could return an error, but doesn't return any data:&lt;/p&gt;
    &lt;quote&gt;const std = @import("std"); const Io = std.Io; fn saveFile(io: Io, data: []const u8, name: []const u8) !void { const file = try Io.Dir.cwd().createFile(io, name, .{}); defer file.close(io); try file.writeAll(io, data); }&lt;/quote&gt;
    &lt;p&gt;If this function is given an instance of Io.Threaded, it will create the file, write data to it, and then close it using ordinary system calls. If it is given an instance of Io.Evented, it will instead use io_uring, kqueue, or some other asynchronous backend suitable to the target operating system. In doing so, it might pause the current execution and go work on a different asynchronous function. Either way, the operation is guaranteed to be complete by the time writeAll() returns. A library author writing a function that involves I/O doesn't need to care about which of these things the ultimate user of the library chooses to do.&lt;/p&gt;
    &lt;p&gt;On the other hand, suppose that a program wanted to save two files. These operations could profitably be done in parallel. If a library author wanted to enable that, they could use the Io interface's async() function to express that it does not matter which order the two files are saved in:&lt;/p&gt;
    &lt;quote&gt;fn saveData(io: Io, data: []const u8) !void { // Calls saveFile(io, data, "saveA.txt") var a_future = io.async(saveFile, .{io, data, "saveA.txt"}); var b_future = io.async(saveFile, .{io, data, "saveB.txt"}); const a_result = a_future.await(io); const b_result = b_future.await(io); try a_result; try b_result; const out: Io.File = .stdout(); try out.writeAll(io, "save complete"); }&lt;/quote&gt;
    &lt;p&gt; When using an Io.Threaded instance, the async() function &lt;del&gt;doesn't actually&lt;/del&gt; isn't actually required to do anything asynchronously [although the actual implementation may dispatch the function to a separate thread, depending on how it was configured] — it can just run the provided function right away. So, with that version of the interface, the function first saves file A and then file B. With an Io.Evented instance, the operations are actually asynchronous, and the program can save both files at once. &lt;/p&gt;
    &lt;p&gt;The real advantage of this approach is that it turns asynchronous code into a performance optimization. The first version of a program or library can write normal straight-line code. Later, if asynchronicity proves to be useful for performance, the author can come back and write it using asynchronous operations. If the ultimate user of the function has not enabled asynchronous execution, nothing changes. If they have, though, the function becomes faster transparently — nothing about the function signature or how it interacts with the rest of the code base changes.&lt;/p&gt;
    &lt;p&gt; One problem, however, is with programs where two parts are actually required to execute simultaneously for correctness. For example, suppose that a program wants to listen for connections on a port and simultaneously respond to user input. In that scenario, it wouldn't be correct to wait for a connection and only then ask for user input. For that use case, the Io interface provides a separate function, &lt;del&gt;asyncConcurrent()&lt;/del&gt;concurrent() [this function was renamed during development; concurrent() is the most recent name] that explicitly asks for the provided function to be run in parallel. Io.Threaded uses a thread in a thread pool to accomplish this. Io.Evented treats it exactly the same as a normal call to async(). &lt;/p&gt;
    &lt;quote&gt;const socket = try openServerSocket(io); var server = try io.concurrent(startAccepting, .{io, socket}); defer server.cancel(io) catch {}; try handleUserInput(io);&lt;/quote&gt;
    &lt;p&gt;If the programmer uses async() where they should have used concurrent(), that is a bug. Zig's new model does not (and cannot) prevent programmers from writing incorrect code, so there are still some subtleties to keep in mind when adapting existing Zig code to use the new interface.&lt;/p&gt;
    &lt;p&gt; The style of code that results from this design is a bit more verbose than languages that give asynchronous functions special syntax, but Andrew Kelley, creator of the language, said that "&lt;quote&gt;it reads like standard, idiomatic Zig code.&lt;/quote&gt;" In particular, he noted that this approach lets the programmer use all of Zig's typical control-flow primitives, such as try and defer; it doesn't introduce any new language features specific to asynchronous code. &lt;/p&gt;
    &lt;p&gt;To demonstrate this, Kelley gave an example of using the new interface to implement asynchronous DNS resolution. The standard getaddrinfo() function for querying DNS information falls short because, although it makes requests to multiple servers (for IPv4 and IPv6) in parallel, it waits for all of the queries to complete before returning an answer. Kelley's example Zig code returns the first successful answer, canceling the other inflight requests.&lt;/p&gt;
    &lt;p&gt;Asynchronous I/O in Zig is far from done, however. Io.Evented is still experimental, and doesn't have implementations for all supported operating systems yet. A third kind of Io, one that is compatible with WebAssembly, is planned (although, as that issue details, implementing it depends on some other new language features). The original pull request for Io lists 24 planned follow-up items, most of which still need work.&lt;/p&gt;
    &lt;p&gt;Still, the overall design of asynchronous code in Zig appears to be set. Zig has not yet had its 1.0 release, because the community is still experimenting with the correct way to implement many features. Asynchronous I/O was one of the larger remaining priorities (along with native code generation, which was also enabled by default for debug builds on some architectures this year). Zig seems to be steadily working its way toward a finished design — which should decrease the number of times Zig programmers are asked to rewrite their I/O because the interface has changed again.&lt;/p&gt;
    &lt;p&gt; Posted Dec 2, 2025 17:00 UTC (Tue) by smurf (subscriber, #17840) [Link] (7 responses) Doesn't seem much different from tagging everything you want to async-ize with "async" and "await" … Also I'm interested in how zig plans to manage an event loop this way. I mean you need to save and restore the call stack somehow, and stacks may not exactly be small in a production setup. Posted Dec 2, 2025 18:53 UTC (Tue) by daroc (editor, #160859) [Link] (1 responses) As for managing the event loop: I believe the plan is for there to be built-in functions that can start executing a function on a user-provided stack, so the event loop can allocate separate stacks and then give them to the running functions. But Zig has also had a long-term goal to eventually be able to statically determine the needed stack size of any given function, at which point it should be possible to write comptime code that does better than that. Posted Dec 2, 2025 21:20 UTC (Tue) by softball (subscriber, #160655) [Link] Similar to Go, which also has "one and a half colors". Functions taking a context.Context as their first argument are most likely performing I/O. It's not required though: a number-crunching function performing no I/O might still check the context for cancellation every so often. Likewise, I/O without taking a context.Context is possible. Similarly, in async languages (Rust, ...), a function marked async might end up performing no I/O at all (no await points). A function marked sync might spawn its own runtime (or grab the global one) and start spawning futures (= called async functions) on it. Lots of grey areas. One thing I wonder about is mixing threading for CPU-bound work and eventing for I/O-bound work. In Rust, one solution is having the application be fundamentally async (tokio, ...) and hand around a dedicated threadpool (rayon, ...). If there's enough available parallelism, both can coexist without interference and communicate via channels. Rust makes this explicit and relatively ergonomic at compile time (Send + Sync, ...). I wonder how equivalent Zig code would look like (I suppose Io would be the evented variant, and for the CPU-bound work just spawn OS threads normally). Posted Dec 2, 2025 20:09 UTC (Tue) by quotemstr (subscriber, #45331) [Link] Posted Dec 2, 2025 21:18 UTC (Tue) by excors (subscriber, #95769) [Link] (3 responses) Arguably that's a good case of colouring, because the presence of IO _should_ be part of your API: users ought to be aware of the security and performance and portability and testability implications of an API that accesses the filesystem/network, and should have some control over it. But users shouldn't have to care about serial IO vs concurrent IO - that's just an implementation detail and a performance optimisation - and in this model they don't have to care, because the API is IO-coloured either way, unlike the async/await model where migrating to concurrent IO changes your API colouring. That's similar to how the use of dynamic memory allocation should be part of your API (and in Zig it is); it's too important to hide. And error handling (in most languages that don't use exceptions). And the general concept of dependency injection. I suppose the main downside is that once you start making everything an explicit part of every API and avoiding implicit global state, it gets annoyingly verbose, and it's hard to read code when the important logic is obscured by boilerplate. But I think it's interesting to see Zig try a different tradeoff here. Posted Dec 2, 2025 22:23 UTC (Tue) by khim (subscriber, #9252) [Link] That's much better solution that what Rust did. Of course Zig has the benefits of hindsight. In practice there are more than two colors — except in Rust it's not obvious from the source and almost unmanageable in practice. That's because you couldn't simply pass any random In Zig one may simply have more than two implementations of the interface. People are talking about “two colors” because in practice that's something that actually works, but try to mix two executors in one program in Rust… and the whole thing falls apart, you couldn't do that. It's not “2 colors” problem, but “2+ colors problem”. Posted Dec 2, 2025 23:02 UTC (Tue) by sionescu (subscriber, #59410) [Link] (1 responses) Posted Dec 3, 2025 2:55 UTC (Wed) by Cyberax (✭ supporter ✭, #52523) [Link] Posted Dec 2, 2025 21:40 UTC (Tue) by dcoutts (subscriber, #5387) [Link] (1 responses) Yes there's an analogy in there somewhere but no. Asynchronous code and threaded code have some similarities but are different. Async code is about trying to do cooperative execution in a single thread (and often with little to no runtime support). Threaded code (with language support) typically means a runtime system with a thread scheduler, and some compiler support to implement proper thread pre-emption. In Haskell in particular (which is what I'm familiar with) the compiler doesn't need to make everything async. It compiles to very traditional-looking low level sequential code. The only magic is the compiler inserts yield points (where it anyway has to do stack or heap checks), and yes there is a thread scheduler in the runtime system (and thread synchronisation primitives interact with the scheduler too of course). Turning everything async is a rather different compiler transformation. Posted Dec 2, 2025 21:58 UTC (Tue) by daroc (editor, #160859) [Link] Compare a Rust async function that does some work, and then goes to work on another async function due to an .await, and then finishes its work. That is quite conceptually similar to a Haskell function that does some work, demands another thunk, and then finishes its work. They're really quite similar in that they don't usually involve the runtime, unless there's some multithreading going on or its time for a context switch. In both languages, the operation (.await or forcing a thunk) are theoretically implemented with a call instruction, but can in practice have the compiler inline parts or do them ahead of time if it can prove that they're used later. In both languages, the in-progress state of these things is partly stored in registers and mostly stored in a specific object in memory. I accept that it's not a perfect analogy. There are serious differences between the language, and in particular the GHC runtime's "everything is a function, even data" approach is pretty different from Rust's "everything is data, even async functions" approach. But I also think that it's not a misleading comparison when the language mechanisms are solving similar problems (letting computation occur in an order that doesn't strictly match the order that a traditional strict, imperative language would demand) in a similar way (by using specialized objects in memory that a runtime helps to manage, but that can do basic interactions between objects just by calling through the appropriate function pointer). &lt;head&gt;One and a half colors&lt;/head&gt;&lt;head&gt;One and a half colors&lt;/head&gt;&lt;head&gt;One and a half colors&lt;/head&gt;&lt;head&gt;One and a half colors&lt;/head&gt;&lt;head&gt;One and a half colors&lt;/head&gt;&lt;head&gt;One and a half colors&lt;/head&gt;&lt;code&gt;async&lt;/code&gt; function into any random executor… the functions that do actual work have to match the executor or else the whole things falls apart — and these functions are invisible in Rust's &lt;code&gt;async fn&lt;/code&gt; signature.&lt;head&gt;One and a half colors&lt;/head&gt;&lt;head&gt;One and a half colors&lt;/head&gt;&lt;head&gt;I see what you mean but...&lt;/head&gt;&lt;head&gt;I see what you mean but...&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1046084/4c048ee008e1c70e/"/><published>2025-12-02T14:31:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46121870</id><title>OpenAI declares 'code red' as Google catches up in AI race</title><updated>2025-12-03T05:40:16.511063+00:00</updated><content>&lt;doc fingerprint="bda6ded3906683a6"&gt;
  &lt;main&gt;
    &lt;p&gt;The tides are turning in the AI race, and the pressure is getting to OpenAI. Chief executive Sam Altman reportedly declared a “code red” on Monday, urging staff to improve its flagship product ChatGPT, an indicator that the startup’s once-unassailable lead is eroding as competitors like Google and Anthropic close in.&lt;/p&gt;
    &lt;head rend="h1"&gt;OpenAI declares ‘code red’ as Google catches up in AI race&lt;/head&gt;
    &lt;p&gt;Google’s own ‘code red’ response to ChatGPT has started paying off.&lt;/p&gt;
    &lt;p&gt;Google’s own ‘code red’ response to ChatGPT has started paying off.&lt;/p&gt;
    &lt;p&gt;In the memo, reported by the Wall Street Journal and The Information, Altman said the company will be delaying initiatives like ads, shopping and health agents, and a personal assistant, Pulse, to focus on improving ChatGPT. This includes core features like greater speed and reliability, better personalization, and the ability to answer more questions, he said.&lt;/p&gt;
    &lt;p&gt;There will be a daily call for those tasked with improving the chatbot, the memo said, and Altman encouraged temporary team transfers to speed up development.&lt;/p&gt;
    &lt;p&gt;The newfound urgency illustrates an inflection point for OpenAI as it spends hundreds of billions of dollars to fund growth and figures out a path to future profitability. It is also something of a full-circle moment in the AI race. Google, which declared its own “code red” after the arrival of ChatGPT, is a particular concern. Google’s AI user base is growing — helped by the success of popular tools like the Nano Banana image model — and its latest AI model, Gemini 3, blew past its competitors on many industry benchmarks and popular metrics.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/news/836212/openai-code-red-chatgpt"/><published>2025-12-02T15:00:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46121889</id><title>Mistral 3 family of models released</title><updated>2025-12-03T05:40:16.165513+00:00</updated><content>&lt;doc fingerprint="2d4d8f5d864404ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Mistral 3&lt;/head&gt;
    &lt;p&gt;The next generation of &lt;lb/&gt; open multimodal and multilingual AI&lt;/p&gt;
    &lt;p&gt;Today, we announce Mistral 3, the next generation of Mistral models. Mistral 3 includes three state-of-the-art small, dense models (14B, 8B, and 3B) and Mistral Large 3 – our most capable model to date – a sparse mixture-of-experts trained with 41B active and 675B total parameters. All models are released under the Apache 2.0 license. Open-sourcing our models in a variety of compressed formats empowers the developer community and puts AI in people’s hands through distributed intelligence.&lt;/p&gt;
    &lt;p&gt;The Ministral models represent the best performance-to-cost ratio in their category. At the same time, Mistral Large 3 joins the ranks of frontier instruction-fine-tuned open-source models.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistral Large 3: A state-of-the-art open model&lt;/head&gt;
    &lt;p&gt;Mistral Large 3 is one of the best permissive open weight models in the world, trained from scratch on 3000 of NVIDIA’s H200 GPUs. Mistral Large 3 is Mistral’s first mixture-of-experts model since the seminal Mixtral series, and represents a substantial step forward in pretraining at Mistral. After post-training, the model achieves parity with the best instruction-tuned open-weight models on the market on general prompts, while also demonstrating image understanding and best-in-class performance on multilingual conversations (i.e., non-English/Chinese).&lt;/p&gt;
    &lt;p&gt;Mistral Large 3 debuts at #2 in the OSS non-reasoning models category (#6 amongst OSS models overall) on the LMArena leaderboard.&lt;/p&gt;
    &lt;p&gt;We release both the base and instruction fine-tuned versions of Mistral Large 3 under the Apache 2.0 license, providing a strong foundation for further customization across the enterprise and developer communities. A reasoning version is coming soon!&lt;/p&gt;
    &lt;head rend="h3"&gt;Mistral, NVIDIA, vLLM &amp;amp; Red Hat join forces to deliver faster, more accessible Mistral 3&lt;/head&gt;
    &lt;p&gt;Working in conjunction with vLLM and Red Hat, Mistral Large 3 is very accessible to the open-source community. We’re releasing a checkpoint in NVFP4 format, built with llm-compressor. This optimized checkpoint lets you run Mistral Large 3 efficiently on Blackwell NVL72 systems and on a single 8×A100 or 8×H100 node using vLLM.&lt;/p&gt;
    &lt;p&gt;Delivering advanced open-source AI models requires broad optimization, achieved through a partnership with NVIDIA. All our new Mistral 3 models, from Large 3 to Ministral 3, were trained on NVIDIA Hopper GPUs to tap high-bandwidth HBM3e memory for frontier-scale workloads. NVIDIA’s extreme co-design approach brings hardware, software, and models together. NVIDIA engineers enabled efficient inference support for TensorRT-LLM and SGLang for the complete Mistral 3 family, for efficient low-precision execution.&lt;/p&gt;
    &lt;p&gt;For Large 3’s sparse MoE architecture, NVIDIA integrated state-of-the-art Blackwell attention and MoE kernels, added support for prefill/decode disaggregated serving, and collaborated with Mistral on speculative decoding, enabling developers to efficiently serve long-context, high-throughput workloads on GB200 NVL72 and beyond. On the edge, delivers optimized deployments of the Ministral models on DGX Spark, RTX PCs and laptops, and Jetson devices, giving developers a consistent, high-performance path to run these open models from data center to robot.&lt;/p&gt;
    &lt;p&gt;We are very thankful for the collaboration and want to thank vLLM, Red Hat, and NVIDIA in particular.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ministral 3: State-of-the-art intelligence at the edge&lt;/head&gt;
    &lt;p&gt;For edge and local use cases, we release the Ministral 3 series, available in three model sizes: 3B, 8B, and 14B parameters. Furthermore, for each model size, we release base, instruct, and reasoning variants to the community, each with image understanding capabilities, all under the Apache 2.0 license. When married with the models’ native multimodal and multilingual capabilities, the Ministral 3 family offers a model for all enterprise or developer needs.&lt;/p&gt;
    &lt;p&gt;Furthermore, Ministral 3 achieves the best cost-to-performance ratio of any OSS model. In real-world use cases, both the number of generated tokens and model size matter equally. The Ministral instruct models match or exceed the performance of comparable models while often producing an order of magnitude fewer tokens.&lt;/p&gt;
    &lt;p&gt;For settings where accuracy is the only concern, the Ministral reasoning variants can think longer to produce state-of-the-art accuracy amongst their weight class - for instance 85% on AIME ‘25 with our 14B variant.&lt;/p&gt;
    &lt;head rend="h2"&gt;Available Today&lt;/head&gt;
    &lt;p&gt;Mistral 3 is available today on Mistral AI Studio, Amazon Bedrock, Azure Foundry, Hugging Face (Large 3 &amp;amp; Ministral), Modal, IBM WatsonX, OpenRouter, Fireworks, Unsloth AI, and Together AI. In addition, coming soon on NVIDIA NIM and AWS SageMaker.&lt;/p&gt;
    &lt;head rend="h3"&gt;One more thing… customization with Mistral AI&lt;/head&gt;
    &lt;p&gt;For organizations seeking tailored AI solutions, Mistral AI offers custom model training services to fine-tune or fully adapt our models to your specific needs. Whether optimizing for domain-specific tasks, enhancing performance on proprietary datasets, or deploying models in unique environments, our team collaborates with you to build AI systems that align with your goals. For enterprise-grade deployments, custom training ensures your AI solution delivers maximum impact securely, efficiently, and at scale.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get started with Mistral 3&lt;/head&gt;
    &lt;p&gt;The future of AI is open. Mistral 3 redefines what’s possible with a family of models built for frontier intelligence, multimodal flexibility, and unmatched customization. Whether you’re deploying edge-optimized solutions with Ministral 3 or pushing the boundaries of reasoning with Mistral Large 3, this release puts state-of-the-art AI directly into your hands.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Mistral 3?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Frontier performance, open access: Achieve closed-source-level results with the transparency and control of open-source models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multimodal and multilingual: Build applications that understand text, images, and complex logic across 40+ native languages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scalable efficiency: From 3B to 675B parameters, choose the model that fits your needs, from edge devices to enterprise workflows.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Agentic and adaptable: Deploy for coding, creative collaboration, document analysis, or tool-use workflows with precision.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Next Steps&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Explore the model documentation:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Technical documentation for customers is available on our AI Governance Hub&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Start building: Ministral 3 and Large 3 on Hugging Face, or deploy via Mistral AI’s platform for instant API access and API pricing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customize for your needs: Need a tailored solution? Contact our team to explore fine-tuning or enterprise-grade training.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Share your projects, questions, or breakthroughs with us: Twitter/X, Discord, or GitHub.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We believe that the future of AI should be built on transparency, accessibility, and collective progress. With this release, we invite the world to explore, build, and innovate with us, unlocking new possibilities in reasoning, efficiency, and real-world applications.&lt;/p&gt;
    &lt;p&gt;Together, let’s turn understanding into action.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mistral.ai/news/mistral-3"/><published>2025-12-02T15:01:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46124179</id><title>School cell phone bans and student achievement</title><updated>2025-12-03T05:40:14.994678+00:00</updated><content>&lt;doc fingerprint="3429e01309a30ea7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;School Cell Phone Bans and Student Achievement&lt;/head&gt;
    &lt;p&gt;Two years after the imposition of a student cell phone ban, student test scores in a large urban school district were significantly higher than before, David N. Figlio and Umut Özek find in The Impact of Cell Phone Bans in Schools on Student Outcomes: Evidence from Florida (NBER Working Paper 34388). The study examines data from one of the 10 largest school districts in the United States, a large urban county-level school district in Florida. While Florida's statewide law banned cell phone use during instructional time, this district implemented a stricter policy requiring students to keep phones silenced and stored in backpacks during the entire school day, including lunch and transitions between classes.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An all-day cell phone ban within a Florida school district improved test scores, particularly for male students and in middle and high schools.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The researchers combined two datasets to conduct this analysis. First, they accessed student administrative data for the year prior to the ban (AY 2022–23) and two years following the ban (AY 2023–24 and AY 2024–25). These data are reported to the district three times annually and include information on student demographics, attendance, disciplinary actions, and standardized test scores. Second, they examined building-level smartphone activity data from Advan for district schools. This data traced the average number of unique smartphone pings between 9 am and 1 pm on school days. To isolate the effects of student usage, the team compared normal school days to professional-only working days. They then compared the last two months of AY 2022–23 (pre-ban) to the first two months of AY 2023–24 and AY 2024–25 (post-ban) and found an average drop in usage of approximately two-thirds. The relative level of usage reduction was used to sort the district’s schools into high-effect (top tercile of pre-ban usage) and low-effect (bottom tercile of pre-ban usage) pools.&lt;/p&gt;
    &lt;p&gt;During the first month of the ban (September 2023), student suspensions rose 25 percent relative to the same month of the prior school year. Elevated disciplinary rates persisted for the full school year. The effects were particularly stark among Black male students, whose in-school suspension rates increased 30 percent at the highly affected schools. Even among the most affected schools and population groups, however, disciplinary action rates fell to near pre-ban levels by the start of the following school year. The researchers posited that this represented a period of adjustment to the new policy rather than an indication of a long-term negative effect of the ban’s implementation.&lt;/p&gt;
    &lt;p&gt;There were no statistically significant changes in test scores during the first year of the ban, when disciplinary rates were high. During the second year of the ban, in contrast, test scores increased significantly, with positive effects concentrated during the spring semester (scores increased 1.1 percentiles, on average). The researchers suggest that this may be due to the higher stakes of spring tests, which can affect grade advancement and high school graduation. Test score improvements were also concentrated among male students (up 1.4 percentiles, on average) and among middle and high school students (up 1.3 percentiles, on average).&lt;/p&gt;
    &lt;p&gt;When comparing high-effect and low-effect schools, the researchers note significant reductions in unexcused absences during the two years following the cell phone ban. They posit that increased attendance could explain as much as half of the test score improvements noted in their primary analysis.&lt;/p&gt;
    &lt;p&gt;- Emma Salomon&lt;/p&gt;
    &lt;p&gt;The researchers thank the Smith Richardson Foundation for generous research funding.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nber.org/digest/202512/school-cell-phone-bans-and-student-achievement"/><published>2025-12-02T17:58:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46124205</id><title>100k TPS over a billion rows: the unreasonable effectiveness of SQLite</title><updated>2025-12-03T05:40:14.811544+00:00</updated><content>&lt;doc fingerprint="c64e13dcc23a3381"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;100000 TPS over a billion rows: the unreasonable effectiveness of SQLite&lt;/head&gt;
    &lt;p&gt;SQLite doesn't have MVCC! It only has a single writer! SQLite is for phones and mobile apps (and the occasional airliner)! For web servers use a proper database like Postgres! In this article I'll go over why being embedded and a single writer are not deficiencies but actually allow SQLite to scale so unreasonably well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prelude&lt;/head&gt;
    &lt;p&gt;For the code examples I will be using Clojure. But, what they cover should be applicable to most programming language.&lt;/p&gt;
    &lt;p&gt;The machine these benchmarks run on has the following specs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MacBook Pro (2021)&lt;/item&gt;
      &lt;item&gt;Chip: Apple M1 Pro&lt;/item&gt;
      &lt;item&gt;Memory: 16 GB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These benchmarks are not meant to be perfect or even optimal. They are merely to illustrate that it's relatively easy to achieve decent write throughput with SQLite. Usual benchmark disclaimers apply.&lt;/p&gt;
    &lt;head rend="h2"&gt;Defining TPS&lt;/head&gt;
    &lt;p&gt;When I say TPS I don't mean writes/updates per second. I'm talking about transactions per second, specifically interactive transactions that are common when building web applications. By interactive transactions I mean transactions where you execute some queries, run some application code and then execute more queries. For example:&lt;/p&gt;
    &lt;code&gt;BEGIN;
UPDATE accounts SET balance = balance - 100.00
    WHERE name = 'Alice';
-- some application code runs
UPDATE accounts SET balance = balance + 100.00
    WHERE name = 'Bob';
COMMIT;
&lt;/code&gt;
    &lt;p&gt;Transactions are useful because they let you rollback the state of your changes if your application encounters a problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;The benchmark harness&lt;/head&gt;
    &lt;p&gt;To simulate requests we spin up &lt;code&gt;n&lt;/code&gt; virtual threads (green threads) that each execute a function &lt;code&gt;f&lt;/code&gt; this is analogous to handlers on a web server and will give us similar contention. Worth noting that this is high burst. I.e we will reach &lt;code&gt;n&lt;/code&gt; level concurrent requests as fast as the system can spin up the virtual threads.&lt;/p&gt;
    &lt;code&gt;(defmacro tx-per-second [n &amp;amp; body]
  `(let [ids#   (range 0 ~n)
         start# (. System (nanoTime))]
     (-&amp;gt;&amp;gt; ids#
       ;; Futures are using virtual threads so blocking is not slow
       (mapv (fn [_#] (future ~@body)))
       (run! deref))
     (int (/ ~n (/ (double (- (. System (nanoTime)) start#)) 1000000000.0)))))
&lt;/code&gt;
    &lt;p&gt;For the Clojure programmers among you &lt;code&gt;future&lt;/code&gt; has been altered to use virtual threads. So, we can spin up millions if we need to.&lt;/p&gt;
    &lt;code&gt;;; Make futures use virtual threads
(set-agent-send-executor!
  (Executors/newVirtualThreadPerTaskExecutor))
(set-agent-send-off-executor!
  (Executors/newVirtualThreadPerTaskExecutor))
&lt;/code&gt;
    &lt;p&gt;We'll be using Postgres as our network database (I'm using Postgres, but the same applies to MySQL etc) with a high performance connection pool optimised for our number of cores.&lt;/p&gt;
    &lt;code&gt;(defonce pg-db
  (jdbc/with-options
    (connection/-&amp;gt;pool
      HikariDataSource
      {:dbtype          "postgres"
       :dbname          "thedb"
       :username        (System/getProperty "user.name")
       :password        ""
       :minimumIdle     8
       :maximumPoolSize 8})
    {}))
&lt;/code&gt;
    &lt;p&gt;We'll be using SQLite with a single writer connection and a number of reader connections equal to our number of cores.&lt;/p&gt;
    &lt;code&gt;(defonce lite-db
  (d/init-db! "database.db"
    {:pool-size 8
     :pragma {:cache_size         15625
              :page_size          4096
              :journal_mode       "WAL"
              :synchronous        "NORMAL"
              :temp_store         "MEMORY"
              :busy_timeout       5000}}))
&lt;/code&gt;
    &lt;p&gt;Our databases will have a simple schema:&lt;/p&gt;
    &lt;code&gt;(jdbc/execute! pg-db
  ["CREATE TABLE IF NOT EXISTS account(id INT PRIMARY KEY, balance INT)"])
(d/q (lite-db :writer)
  ["CREATE TABLE IF NOT EXISTS account(id PRIMARY KEY, balance INT)"])
&lt;/code&gt;
    &lt;p&gt;And each contain a billion rows:&lt;/p&gt;
    &lt;code&gt;(-&amp;gt;&amp;gt; (range 0 (* 1000 1000 1000))
  (partition-all 32000)
  (run!
    (fn [batch]
      (jdbc-sql/insert-multi! pg-db :account
        (mapv (fn [id] {:id id :balance 1000000000}) batch)))))
        
(-&amp;gt;&amp;gt; (range 0 (* 1000 1000 1000))
  (partition-all 100000)
  (run!
    (fn [batch]
      (d/with-write-tx [tx (lite-db :writer)]
        (run!
          (fn [id]
            (d/q tx
              ["INSERT INTO account(id, balance) VALUES (?,?)" id 1000000000]))
          batch)))))
&lt;/code&gt;
    &lt;p&gt;Our user distribution will follow a power law. I.e the top X percent will be involved in most of the transactions. We have a billion users, so in practice most of those won't be active, or be active rarely. &lt;code&gt;0.9995&lt;/code&gt; means 99.95% of transactions will be done by 0.05% of users. This still means around 100000 unique active users at any given time. &lt;/p&gt;
    &lt;p&gt;The reason we are using a power law, is that's a very common distribution for a lot of real products. If you think about a credit card payment system, in the context of retail, the largest number of transactions are most likely with a few large retailers (Amazon, Walmart etc).&lt;/p&gt;
    &lt;code&gt;(defn pareto-user []
  (rand-pareto (* 1000 1000 1000) 0.9995))
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;rand-pareto&lt;/code&gt; turns a random distribution into a power law distribution.&lt;/p&gt;
    &lt;code&gt;(defn rand-pareto [r p]
  (let [a (/ (Math/log (- 1.0 p)) (Math/log p))
        x (rand)
        y (/ (- (+ (Math/pow x a) 1.0)
               (Math/pow (- 1.0 x) (/ 1.0 a)))
            2.0)]
    (long (* r y))))
&lt;/code&gt;
    &lt;head rend="h2"&gt;Network database&lt;/head&gt;
    &lt;p&gt;Let's start with a network database.&lt;/p&gt;
    &lt;code&gt;(tx-per-second 100000
  (jdbc/with-transaction [tx pg-db]
    (jdbc/execute! tx (credit-random-account))
    (jdbc/execute! tx (debit-random-account))))
    
;; =&amp;gt; 13756 TPS
&lt;/code&gt;
    &lt;p&gt;A respectable 13756 TPS.&lt;/p&gt;
    &lt;p&gt;However, normally a network database will not be on the same server as our application. So let's simulate some network latency. Let's say you have 5ms latency between your app server and your database.&lt;/p&gt;
    &lt;code&gt;(tx-per-second 10000
  (jdbc/with-transaction [tx pg-db]
    (jdbc/execute! tx (credit-random-account))
    (Thread/sleep 5)
    (jdbc/execute! tx (debit-random-account))))
    
;; =&amp;gt; 1214 TPS
&lt;/code&gt;
    &lt;p&gt;Note: virtual threads do not sleep a real thread. They instead park allowing the underlying carrier thread to resume another virtual thread.&lt;/p&gt;
    &lt;p&gt;What if we increase that latency to 10ms?&lt;/p&gt;
    &lt;code&gt;(tx-per-second 10000
  (jdbc/with-transaction [tx pg-db]
    (jdbc/execute! tx (credit-random-account))
    (Thread/sleep 10)
    (jdbc/execute! tx (debit-random-account))))
    
;; =&amp;gt; 702 TPS
&lt;/code&gt;
    &lt;p&gt;But, wait our transactions are not serialisable, which they need to be if we want consistent transaction processing (SQLite is isolation serialisable by design). We better fix that and handle retries.&lt;/p&gt;
    &lt;code&gt;(tx-per-second 10000
  (loop []
    (let [result
          (try
            (jdbc/with-transaction [tx pg-db {:isolation :serializable}]
              (jdbc/execute! tx (credit-random-account))
              (Thread/sleep 10)
              (jdbc/execute! tx  (debit-random-account)))
            (catch Exception _ nil))]
      (when-not result (recur)))))

;; =&amp;gt; 660 TPS
&lt;/code&gt;
    &lt;p&gt;What if the interactive transaction has an extra query (an extra network hop)?&lt;/p&gt;
    &lt;code&gt;(tx-per-second 10000
  (loop []
    (let [result
          (try
            (jdbc/with-transaction [tx pg-db {:isolation :serializable}]
              (jdbc/execute! tx (credit-random-account))
              (Thread/sleep 10)
              (jdbc/execute! tx  (debit-random-account))
              (Thread/sleep 10)
              (jdbc/execute! tx  (debit-random-account)))
            (catch Exception _ nil))]
      (when-not result (recur)))))

;; =&amp;gt; 348 TPS
&lt;/code&gt;
    &lt;p&gt;348 TPS! What's going on here? Amdahl's Law strikes!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We're holding transactions with row locks across a network with high contention because of the power law. What's terrifying about this is no amount of additional (cpu/servers/memory) is going to save us. This is a hard limit caused by the network. What's worse, in any unexpected increase in latency will exacerbate the problem. Which also means you can't have application servers in different data centres than your database (because of the increased latency).&lt;/p&gt;
    &lt;p&gt;I learnt this the hard way building an emoji based tipping bot for discord. At the time I didn't understand why we were hitting this hard limit in TPS. We ended up sacrificing the convenience of interactive transactions and moving everything into stored procedures (meaning no locks across the network). However, in a lot of domains this isn't possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Embedded means no network&lt;/head&gt;
    &lt;p&gt;Let's see how SQLite fares.&lt;/p&gt;
    &lt;code&gt;(tx-per-second 1000000
  (d/with-write-tx [tx (lite-db :writer)]
    (d/q tx (credit-random-account))
    (d/q tx (debit-random-account))))

;; =&amp;gt; 44096 TPS
&lt;/code&gt;
    &lt;p&gt;44096 TPS! By eliminating the network SQLite massively reduces the impact of Amdahl's law.&lt;/p&gt;
    &lt;head rend="h2"&gt;Single writer lets you batch&lt;/head&gt;
    &lt;p&gt;We don't need to stop there though. Because, SQLite is a single writer we can batch. sqlite4clj provides a convenient dynamic batching function. Batch size grows dynamically with the workload and producers don't have to block when the consumer is busy. Effectively it self optimises for latency and throughput.&lt;/p&gt;
    &lt;code&gt;(defn batch-fn [db batch]
  @(on-pool! lite-write-pool
     (d/with-write-tx [tx db]
       (run! (fn [thunk] (thunk tx)) batch))))
       
(defonce tx!
  (b/async-batcher-init! lite-db
    {:batch-fn #'batch-fn}))
&lt;/code&gt;
    &lt;p&gt;Note: to Clojure/Java programmers we're using a thread pool as SQLite should be treated as CPU not IO, so we don't want it starving our virtual threads (io green threads).&lt;/p&gt;
    &lt;code&gt;(tx-per-second 1000000
  @(tx!
     (fn [tx]
       (d/q tx (credit-random-account))
       (d/q tx (debit-random-account)))))
       
;; =&amp;gt; 186157 TPS
&lt;/code&gt;
    &lt;p&gt;But, wait I hear you cry! That's cheating we now don't have isolated transaction failure. Batching is sacrificing fine grained transaction. You're right! Let's fix that.&lt;/p&gt;
    &lt;code&gt;(tx-per-second 1000000
  @(tx!
     (fn  [tx]
       (d/q tx ["SAVEPOINT inner_tx"])
       (try
         (d/q tx (credit-random-account))
         (d/q tx (debit-random-account))
         (catch Throwable _
           (d/q tx ["ROLLBACK inner_tx"])))
       (d/q tx ["RELEASE inner_tx"]))))
       
;; =&amp;gt; 121922 TPS
&lt;/code&gt;
    &lt;p&gt;SQLite supports nested transactions with &lt;code&gt;SAVEPOINT&lt;/code&gt; this lets us have fine-grained transaction rollback whilst still batching our writes. If a transaction fails it won't cause the batch to fail. The only case where the whole batch will fail is in the case of power loss/or a hard crash.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about concurrent reads?&lt;/head&gt;
    &lt;p&gt;Generally systems have a mix of reads and writes, somewhere in the region of 75% reads to 25% writes. So let's add some writes.&lt;/p&gt;
    &lt;code&gt;(tx-per-second 1000000
  (on-pool! lite-read-pool
    (d/q (lite-db :reader)
      ["select * from account where id = ? limit 1" (pareto-user)]))
  (on-pool! lite-read-pool
    (d/q (lite-db :reader)
      ["select * from account where id = ? limit 1" (pareto-user)]))
  (on-pool! lite-read-pool
    (d/q (lite-db :reader)
      ["select * from account where id = ? limit 1" (pareto-user)]))
  @(tx!
     (fn  [tx]
       (d/q tx ["SAVEPOINT inner_tx"])
       (try
         (d/q tx (credit-random-account))
         (d/q tx (debit-random-account))
         (catch Throwable _
           (d/q tx ["ROLLBACK inner_tx"])))
       (d/q tx ["RELEASE inner_tx"]))))
       
;; =&amp;gt; 102545 TPS
&lt;/code&gt;
    &lt;p&gt;102545 TPS!&lt;/p&gt;
    &lt;p&gt;Note: to Clojure/Java programmers we're using a separate read thread pool so that reads don't starve writes.&lt;/p&gt;
    &lt;head rend="h2"&gt;TPS Report&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Postgres&lt;/cell&gt;
        &lt;cell role="head"&gt;SQLite&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;no network&lt;/cell&gt;
        &lt;cell&gt;13756&lt;/cell&gt;
        &lt;cell&gt;44096&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5ms&lt;/cell&gt;
        &lt;cell&gt;1214&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10ms&lt;/cell&gt;
        &lt;cell&gt;702&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10ms serializable&lt;/cell&gt;
        &lt;cell&gt;660&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;batch&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;186157&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;batch savepoint&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;121922&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;batch savepoint + reads&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;102545&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Hopefully, this post helps illustrate the unreasonable effectiveness of SQLite as well as the challenges you can run in with Amdahl's law and network databases like postgres.&lt;/p&gt;
    &lt;p&gt;The full benchmark code can be found here.&lt;/p&gt;
    &lt;p&gt;Further Reading:&lt;/p&gt;
    &lt;p&gt;If you want to learn more about Amdahl's law, power laws and how they interact with network databases I highly recommend listening to this interview with Joran Greef and watching his talk 1000x: The Power of an Interface for Performance by Joran Dirk Greef.&lt;/p&gt;
    &lt;p&gt;If you want to read about how much further you can scale SQLite checkout Scaling SQLite to 4M QPS on a single server (EC2 vs Bare Metal).&lt;/p&gt;
    &lt;p&gt;If you're thinking of running SQLite in production and wondering how to create streaming replicas, backups and projections checkout litestream.&lt;/p&gt;
    &lt;p&gt;If you still don't think a single machine can handle your workload it's worth reading Scalability! But at what COST?.&lt;/p&gt;
    &lt;p&gt;Thanks to Everyone on the Datastar discord who read drafts of this and gave me feedback.&lt;/p&gt;
    &lt;p&gt;Discussion&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andersmurphy.com/2025/12/02/100000-tps-over-a-billion-rows-the-unreasonable-effectiveness-of-sqlite.html"/><published>2025-12-02T17:59:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46124267</id><title>Anthropic acquires Bun</title><updated>2025-12-03T05:40:14.620557+00:00</updated><content>&lt;doc fingerprint="3325915d7bc72e5c"&gt;
  &lt;main&gt;
    &lt;p&gt;TLDR: Bun has been acquired by Anthropic. Anthropic is betting on Bun as the infrastructure powering Claude Code, Claude Agent SDK, and future AI coding products &amp;amp; tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;What doesn't change:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bun stays open-source &amp;amp; MIT-licensed&lt;/item&gt;
      &lt;item&gt;Bun continues to be extremely actively maintained&lt;/item&gt;
      &lt;item&gt;The same team still works on Bun&lt;/item&gt;
      &lt;item&gt;Bun is still built in public on GitHub&lt;/item&gt;
      &lt;item&gt;Bun's roadmap will continue to focus on high performance JavaScript tooling, Node.js compatibility &amp;amp; replacing Node.js as the default server-side runtime for JavaScript&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Claude Code ships as a Bun executable to millions of users. If Bun breaks, Claude Code breaks. Anthropic has direct incentive to keep Bun excellent.&lt;/p&gt;
    &lt;head rend="h3"&gt;What changes:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We will help make coding tools like Claude Code &amp;amp; Claude Agent SDK faster &amp;amp; smaller&lt;/item&gt;
      &lt;item&gt;We get a closer first look at what's around the corner for AI coding tools, and make Bun better for it&lt;/item&gt;
      &lt;item&gt;Bun will ship faster.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How Bun started&lt;/head&gt;
    &lt;p&gt;Almost five years ago, I was building a Minecraft-y voxel game in the browser. The codebase got kind of large, and the iteration cycle time took 45 seconds to test if changes worked. Most of that time was spent waiting for the Next.js dev server to hot reload.&lt;/p&gt;
    &lt;p&gt;This was frustrating, and I got really distracted trying to fix it.&lt;/p&gt;
    &lt;p&gt;I started porting esbuild's JSX &amp;amp; TypeScript transpiler from Go to Zig. Three weeks later, I had a somewhat working JSX &amp;amp; TypeScript transpiler.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Early benchmark from a new JavaScript bundler. It transpiles JSX files:&lt;/p&gt;— Jarred Sumner (@jarredsumner) May 5, 2021&lt;lb/&gt;- 3x faster than esbuild&lt;lb/&gt;- 94x faster than swc&lt;lb/&gt;- 197x faster than babel pic.twitter.com/NBRt9ESu2d&lt;/quote&gt;
    &lt;p&gt;I spent much of that first year in a very cramped apartment in Oakland, just coding and tweeting about Bun.&lt;/p&gt;
    &lt;head rend="h4"&gt;The runtime&lt;/head&gt;
    &lt;p&gt;To get Next.js server side rendering to work, we needed a JavaScript runtime. And JavaScript runtimes need an engine to interpret &amp;amp; JIT compile the code.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The start time difference between JavaScriptCore and V8 is interesting. JavaScriptCore seems to start around 4x faster.&lt;/p&gt;— Jarred Sumner (@jarredsumner) May 26, 2021&lt;lb/&gt;It's possible this is due to the specifics of their respective CLIs though (rather than about JavaScript execution) pic.twitter.com/xd5tSbWf6p&lt;/quote&gt;
    &lt;p&gt;So after about a month of reading WebKit's source code trying to figure out how to embed JavaScriptCore with the same flexibility as what Safari does, I had the very initial version of Bun's JavaScript runtime.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bun v0.1.0&lt;/head&gt;
    &lt;p&gt;Bun v0.1.0 was released in July of 2022. A bundler, a transpiler, a runtime (designed to be a drop-in replacement for Node.js), test runner, and a package manager - all in one. We ended up reaching 20k GitHub stars in the first week.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Introducing Bun - an incredibly fast all-in-one JavaScript runtime. https://t.co/Yt6tAcnBQs&lt;/p&gt;— Jarred Sumner (@jarredsumner) July 5, 2022&lt;/quote&gt;
    &lt;p&gt;Those first two weeks after the release were one of the craziest weeks of my life. My job switched from writing code all day to replying to people all day. We raised a $7 million seed round led by Kleiner Perkins (thanks Bucky &amp;amp; Leigh Marie! And also Shrav Mehta), I took a salary and convinced a handful of engineers to move to San Francisco and help build Bun.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;got keys &amp;amp; desks for oven’s office today pic.twitter.com/bfTmRaF7Oh&lt;/p&gt;— Jarred Sumner (@jarredsumner) October 8, 2022&lt;/quote&gt;
    &lt;head rend="h3"&gt;Bun v1.0.0&lt;/head&gt;
    &lt;p&gt;Bun started to feel more stable, so we shipped Bun v1.0 in September of 2023.&lt;/p&gt;
    &lt;p&gt;Production usage started to pick up and we raised a $19 million Series A led by Khosla Ventures (thanks Nikita &amp;amp; Jon!), grew the team to 14 people and got a slightly larger office.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bun v1.1&lt;/head&gt;
    &lt;p&gt;After all this time, we still didn't have Windows support. And every day, people asked us the same question: "when will Bun support Windows?"&lt;/p&gt;
    &lt;p&gt;So we added Windows support and called it Bun v1.1. Our Windows support was pretty rough at first, but we've made a lot of progress since then.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bun v1.2&lt;/head&gt;
    &lt;p&gt;Bun v1.2 made big improvements to Node.js compatibility, added a builtin PostgreSQL client and S3 client. We also started seeing production usage from companies like X and Midjourney. Tailwind's standalone CLI is built with Bun.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bun v1.3&lt;/head&gt;
    &lt;p&gt;Bun v1.3 added a builtin frontend dev server, a Redis client, a MySQL client, several improvements to &lt;code&gt;bun install&lt;/code&gt; and improved Node.js compatibility. The real feature: continued increasing production usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI started to get good&lt;/head&gt;
    &lt;p&gt;In late 2024, AI coding tools went from "cool demo" to "actually useful." And a ton of them are built with Bun.&lt;/p&gt;
    &lt;p&gt;Bun's single-file executables turned out to be perfect for distributing CLI tools. You can compile any JavaScript project into a self-contained binary—runs anywhere, even if the user doesn't have Bun or Node installed. Works with native addons. Fast startup. Easy to distribute.&lt;/p&gt;
    &lt;p&gt;Claude Code, FactoryAI, OpenCode, and others are all built with Bun.&lt;/p&gt;
    &lt;head rend="h3"&gt;I got obsessed with Claude Code&lt;/head&gt;
    &lt;p&gt;I started using Claude Code myself. I got kind of obsessed with it.&lt;/p&gt;
    &lt;p&gt;Over the last several months, the GitHub username with the most merged PRs in Bun's repo is now a Claude Code bot. We have it set up in our internal Discord and we mostly use it to help fix bugs. It opens PRs with tests that fail in the earlier system-installed version of Bun before the fix and pass in the fixed debug build of Bun. It responds to review comments. It does the whole thing.&lt;/p&gt;
    &lt;p&gt;This feels approximately a few months ahead of where things are going. Certainly not years.&lt;/p&gt;
    &lt;head rend="h3"&gt;The road ahead&lt;/head&gt;
    &lt;p&gt;Today, Bun makes $0 in revenue.&lt;/p&gt;
    &lt;p&gt;One of the most common questions I get is about sustainability. Questions like:&lt;/p&gt;
    &lt;p&gt;"How does Bun become a business?"&lt;/p&gt;
    &lt;p&gt;"If I bet my work project or company's tech stack on Bun, will it still be around in five or ten years?"&lt;/p&gt;
    &lt;p&gt;Our default answer was always some version of "we'll eventually build a cloud hosting product.", vertically integrated with Bun’s runtime &amp;amp; bundler.&lt;/p&gt;
    &lt;p&gt;But the world when I first started working on Bun is different from the world today. AI coding tools are this massive change to how developers do productive work, and the infrastructure layer matters more when agents are writing code.&lt;/p&gt;
    &lt;p&gt;Forcing ourselves down the prescribed path felt wrong when AI coding tools are getting this good, this fast.&lt;/p&gt;
    &lt;head rend="h3"&gt;The walk&lt;/head&gt;
    &lt;p&gt;We've been prioritizing issues from the Claude Code team for several months now. I have so many ideas all the time and it's really fun. Many of these ideas also help other AI coding products.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, I went on a four hour walk with Boris from the Claude Code team. We talked about Bun. We talked about where AI coding is going. We talked about what it would look like for Bun's team to join Anthropic. Then we did that about 3 more times over the next few weeks. Then I did that with many of their competitors. I think Anthropic is going to win.&lt;/p&gt;
    &lt;p&gt;Betting on Anthropic sounded like a more interesting path. To be in the center of things. To work alongside the team building the best AI coding product.&lt;/p&gt;
    &lt;head rend="h3"&gt;This is a little bit crazy&lt;/head&gt;
    &lt;p&gt;At the time of writing, Bun's monthly downloads grew 25% last month (October, 2025), passing 7.2 million monthly downloads. We had over 4 years of runway to figure out monetization. We didn't have to join Anthropic.&lt;/p&gt;
    &lt;p&gt;Instead of putting our users &amp;amp; community through "Bun, the VC-backed startups tries to figure out monetization" – thanks to Anthropic, we can skip that chapter entirely and focus on building the best JavaScript tooling.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why this makes sense&lt;/head&gt;
    &lt;p&gt;When people ask "will Bun still be around in five or ten years?", answering with "we raised $26 million" isn't a great answer. Investors eventually need a return.&lt;/p&gt;
    &lt;p&gt;But there's a bigger question behind that: what does software engineering even look like in two to three years?&lt;/p&gt;
    &lt;p&gt;AI coding tools are getting really good, really fast and they're using Bun’s single-file executables to ship CLIs and agents that run everywhere.&lt;/p&gt;
    &lt;p&gt;If most new code is going to be written, tested, and deployed by AI agents:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The runtime and tooling around that code become way more important.&lt;/item&gt;
      &lt;item&gt;You get a lot more code overall, written &amp;amp; tested a lot faster.&lt;/item&gt;
      &lt;item&gt;Humans are more detached from every individual line, so the environment it runs in has to be fast and predictable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bun started with a focus on making developers faster. AI coding tools do a similar thing. It’s a natural fit.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bun joins Anthropic&lt;/head&gt;
    &lt;p&gt;So that's why we're joining Anthropic.&lt;/p&gt;
    &lt;p&gt;Anthropic is investing in Bun as the infrastructure powering Claude Code, Claude Agent SDK, and future AI coding products. Our job is to make Bun the best place to build, run, and test AI-driven software — while continuing to be a great general-purpose JavaScript runtime, bundler, package manager, and test runner.&lt;/p&gt;
    &lt;p&gt;Being part of Anthropic gives Bun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Long-term stability. a home and resources so people can safely bet their stack on Bun.&lt;/item&gt;
      &lt;item&gt;A front-row seat to where AI coding tools are headed, so we can shape Bun around that future instead of guessing from the outside.&lt;/item&gt;
      &lt;item&gt;More firepower. We’re hiring engineers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And for existing users, the core promise stays the same:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bun remains open-source &amp;amp; MIT-licensed.&lt;/item&gt;
      &lt;item&gt;Bun is still built in public.&lt;/item&gt;
      &lt;item&gt;The same team still works on Bun.&lt;/item&gt;
      &lt;item&gt;We’re still obsessed with making JavaScript and TypeScript faster to install, build, run and test.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Anthropic gets a runtime that’s aligned with where software development is going. We get to work on the most interesting version of that future.&lt;/p&gt;
    &lt;p&gt;This is going to be really fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;
    &lt;p&gt;Q: Is Bun still open-source &amp;amp; MIT-licensed?&lt;lb/&gt;A: Yes.&lt;/p&gt;
    &lt;p&gt;Q: Will Bun still be developed in public on GitHub?&lt;lb/&gt;A: Yes. We’ll still be extremely active on GitHub issues &amp;amp; pull requests.&lt;/p&gt;
    &lt;p&gt;Q: Does Bun still care about Node.js compatibility &amp;amp; being a drop-in replacement for Node.js?&lt;lb/&gt;A: Yes.&lt;/p&gt;
    &lt;p&gt;Q: Is the same team still working on Bun full-time?&lt;lb/&gt;A: Yes. And now we get access to the resources of the world’s premier AI Lab instead of a small VC-backed startup making $0 in revenue&lt;/p&gt;
    &lt;p&gt;Q: What does this mean for Bun’s roadmap?&lt;lb/&gt;A: Bun’s team will be working more closely with the Claude Code team, and it probably will look similar to the relationship between Google Chrome &amp;lt;&amp;gt; V8, Safari &amp;lt;&amp;gt; JavaScriptCore, Mozilla Firefox &amp;lt;&amp;gt; SpiderMonkey, but with more independence to prioritize the wide variety of ways people &amp;amp; companies use Bun today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bun.com/blog/bun-joins-anthropic"/><published>2025-12-02T18:05:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46124324</id><title>IBM CEO says there is 'no way' spending on AI data centers will pay off</title><updated>2025-12-03T05:40:14.457390+00:00</updated><content>&lt;doc fingerprint="4f74925d2cffe6ed"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IBM's CEO walked through some napkin math on data centers— and said that there's "no way" to turn a profit at current costs.&lt;/item&gt;
      &lt;item&gt;"$8 trillion of CapEx means you need roughly $800 billion of profit just to pay for the interest," Arvind Krishna told "Decoder."&lt;/item&gt;
      &lt;item&gt;Krishna was skeptical of that current tech would reach AGI, putting the likelihood between 0-1%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AI companies are spending billions on data centers in the race to AGI. IBM CEO Arvind Krishna has some thoughts on the math behind those bets.&lt;/p&gt;
    &lt;p&gt;Data center spending is on the rise. During Meta's recent earnings call, words like "capacity" and AI "infrastructure" were frequently used. Google just announced that it wants to eventually build them in space. The question remains: will the revenue generated from data centers ever justify all the capital expenditure?&lt;/p&gt;
    &lt;p&gt;On the "Decoder" podcast, Krishna concluded that there was likely "no way" these companies would make a return on their capex spending on data centers.&lt;/p&gt;
    &lt;p&gt;Couching that his napkin math was based on today's costs, "because anything in the future is speculative," Kirshna said that it takes about $80 billion to fill up a one-gigawatt data center.&lt;/p&gt;
    &lt;p&gt;"Okay, that's today's number. So, if you are going to commit 20 to 30 gigawatts, that's one company, that's $1.5 trillion of capex," he said.&lt;/p&gt;
    &lt;p&gt;Krishna also referenced the depreciation of the AI chips inside data centers as another factor: "You've got to use it all in five years because at that point, you've got to throw it away and refill it," he said.&lt;/p&gt;
    &lt;p&gt;Investor Michael Burry has recently taken aim at Nvidia over depreciating concerns, leading to a downturn in AI stocks.&lt;/p&gt;
    &lt;p&gt;"If I look at the total commits in the world in this space, in chasing AGI, it seems to be like 100 gigawatts with these announcements," Krishna said.&lt;/p&gt;
    &lt;p&gt;At $80 billion each for 100 gigawatts, that sets Krishna's price tag for computing commitments at roughly $8 trillion.&lt;/p&gt;
    &lt;p&gt;"It's my view that there's no way you're going to get a return on that, because $8 trillion of capex means you need roughly $800 billion of profit just to pay for the interest," he said.&lt;/p&gt;
    &lt;p&gt;Reaching that number of gigawatts has required massive spending from AI companies — and pushes for outside help. In an October letter to the White House's Office of Science and Technology Policy, OpenAI CEO Sam Altman recommended that the US add 100 gigawatts in energy capacity every year.&lt;/p&gt;
    &lt;p&gt;"Decoder" host Nilay Patel pointed out that Altman believed OpenAI could generate a return on its capital expenditures. OpenAI has committed to spending some $1.4 trillion in a variety of deals. Here, Krishna said he diverged from Altman.&lt;/p&gt;
    &lt;p&gt;"That's a belief," Krishna said. "That's what some people like to chase. I understand that from their perspective, but that's different from agreeing with them."&lt;/p&gt;
    &lt;p&gt;Krishna clarified that he wasn't convinced that the current set of technologies would get us to AGI, a yet to be reached technological breakthrough generally agreed to be when AI is capable of completing complex tasks better than humans. He pegged the chances of achieving it without a further technological breakthrough at 0-1%.&lt;/p&gt;
    &lt;p&gt;Several other high-profile leaders have been skeptical of the acceleration to AGI. Marc Benioff said that he was "extremely suspect" of the AGI push, analogizing it to hypnosis. Google Brain founder Andrew Ng said that AGI was "overhyped," and Mistral CEO Arthur Mensch said that AGI was a "marketing move."&lt;/p&gt;
    &lt;p&gt;Even if AGI is the goal, scaling compute may not be the enough. OpenAI cofounder Ilya Sutskever said in November that the age of scaling was over, and that even 100x scaling of LLMs would not be completely transformative. "It's back to the age of research again, just with big computers," he said.&lt;/p&gt;
    &lt;p&gt;Krishna, who began his career at IBM in 1990 before rising to eventually be named CEO in 2020 and chairman in 2021, did praise the current set of AI tools.&lt;/p&gt;
    &lt;p&gt;"I think it's going to unlock trillions of dollars of productivity in the enterprise, just to be absolutely clear," he said.&lt;/p&gt;
    &lt;p&gt;But AGI will require "more technologies than the current LLM path," Krisha said. He proposed fusing hard knowledge with LLMs as a possible future path.&lt;/p&gt;
    &lt;p&gt;How likely is that to reach AGI? "Even then, I'm a 'maybe,'" he said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.businessinsider.com/ibm-ceo-big-tech-ai-capex-data-center-spending-2025-12"/><published>2025-12-02T18:10:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46124892</id><title>Cursed circuits: charge pump voltage halver</title><updated>2025-12-03T05:40:14.335379+00:00</updated><content/><link href="https://lcamtuf.substack.com/p/cursed-circuits-charge-pump-voltage"/><published>2025-12-02T18:47:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46125155</id><title>Amazon launches Trainium3</title><updated>2025-12-03T05:40:14.078723+00:00</updated><content>&lt;doc fingerprint="7f58135a25beadfd"&gt;
  &lt;main&gt;
    &lt;p&gt;Amazon Web Services, which has been building its own AI training chips for years now, just introduced a new version known as Trainium3 that comes with some impressive specs.&lt;/p&gt;
    &lt;p&gt;The cloud provider, which made the announcement Tuesday at AWS re:Invent 2025, also teased the next product on its AI training product roadmap: Trainium4, which is already in the works and will be able to work with Nvidia’s chips.&lt;/p&gt;
    &lt;p&gt;AWS used its annual tech conference to formally launch Trainium3 UltraServer, a system powered by the company’s state-of-the art, 3 nanometer Trainium3 chip, as well as its homegrown networking tech. As you might expect, the third-generation chip and system offer big bumps in performance for AI training and inference over the second-generation chip, according to AWS.&lt;/p&gt;
    &lt;p&gt;AWS says the system is more than 4x faster, with 4x more memory, not just for training, but for delivering AI apps at peak demand. Additionally, thousands of UltraServers can be linked together to provide an app with up to 1 million Trainium3 chips — 10x the previous generation. Each UltraServer can host 144 chips, according to the company.&lt;/p&gt;
    &lt;p&gt;Perhaps more importantly, AWS says the chips and systems are also 40% more energy efficient than the previous generation. While the world races to build bigger data centers powered by astronomical gigawatts of electricity, data center giant AWS is trying to make systems that drink less, not more.&lt;/p&gt;
    &lt;p&gt;It is, obviously, in AWS’s direct interests to do so. But in its classic, Amazon cost-conscious way, it promises that these systems save its AI cloud customers money, too.&lt;/p&gt;
    &lt;p&gt;AWS customers like Anthropic (of which Amazon is also an investor), Japan’s LLM Karakuri, SplashMusic, and Decart have already been using the third-gen chip and system and significantly cut their inference costs, Amazon said.&lt;/p&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;p&gt;AWS also presented a bit of a roadmap for the next chip, Trainium4, which is already in development. AWS promised the chip will provide another big step up in performance and support Nvidia’s NVLink Fusion high-speed chip interconnect technology.&lt;/p&gt;
    &lt;p&gt;This means the AWS Trainium4-powered systems will be able to interoperate and extend their performance with Nvidia GPUs while still using Amazon’s homegrown, lower-cost server rack technology.&lt;/p&gt;
    &lt;p&gt;It’s worth noting, too, that Nvidia’s CUDA (Compute Unified Device Architecture) has become the de facto standard that all the major AI apps are built to support. The Trainium4-powered systems may make it easier to woo big AI apps built with Nvidia GPUs in mind to Amazon’s cloud.&lt;/p&gt;
    &lt;p&gt;Amazon did not announce a timeline for Trainium4. If the company follows previous rollout timelines, we’ll likely hear more about Trainium4 at next year’s conference.&lt;/p&gt;
    &lt;p&gt;Follow along with all of TechCrunch’s coverage of the annual enterprise tech event here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sponsored: Watch re:Invent industry streams&lt;/head&gt;
    &lt;p&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flags&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/"/><published>2025-12-02T19:04:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46126141</id><title>Free static site generator for small restaurants and cafes</title><updated>2025-12-03T05:40:13.812494+00:00</updated><content>&lt;doc fingerprint="d684a94baad7f3d5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h2"&gt; Disclaimer&lt;/head&gt;
      &lt;p&gt; This is not a real restaurant,&lt;/p&gt;
      &lt;head rend="h2"&gt; About US&lt;/head&gt;
      &lt;p&gt; Pasta boy’s started in ma’s kitchen after a plate of ma’s spaggite in old town meatball. 20 years later they are still slerping noddles.&lt;/p&gt;
      &lt;head rend="h2"&gt; Orders to GO&lt;/head&gt;
      &lt;p&gt; We do orders to go, call us and place an order for pick up&lt;/p&gt;
      &lt;head rend="h2"&gt; This was an example of using localcafe lite&lt;/head&gt;
      &lt;p&gt; You can use localcafe lite for free and also host static restaurant menu sites for free using github pages.&lt;/p&gt;
      &lt;p&gt; Learn more about this project at https://github.com/Local-Cafe/localcafe-lite&lt;/p&gt;
      &lt;head rend="h3"&gt; Free / No Monthly Fees&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; This project is open source and free &lt;/item&gt;
        &lt;item&gt; This project can host for free on GitHub Pages, Netlify, or Cloudflare Pages &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt; Static Website&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; Fast page loads - everything pre-generated &lt;/item&gt;
        &lt;item&gt; No database or server required &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt; Online Menu&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; Display your full menu with photos, descriptions, and prices &lt;/item&gt;
        &lt;item&gt; Single prices or multiple options (small/large, hot/iced, etc.) &lt;/item&gt;
        &lt;item&gt; Customers filter by tags (vegetarian, gluten-free, breakfast, lunch) &lt;/item&gt;
        &lt;item&gt; Update by editing simple text files &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt; Location &amp;amp; Maps&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; Show one location or multiple locations &lt;/item&gt;
        &lt;item&gt; Automatic maps - just provide your address &lt;/item&gt;
        &lt;item&gt; Each location has its own hours, phone, and email &lt;/item&gt;
        &lt;item&gt; Maps adjust to any screen size &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt; Photo Slideshow&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; Homepage displays rotating photos with smooth transitions &lt;/item&gt;
        &lt;item&gt; Supports single image or multiple images &lt;/item&gt;
        &lt;item&gt; Photos fade between each other automatically &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt; Mobile Responsive&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; Works on all phones and tablets &lt;/item&gt;
        &lt;item&gt; Menu and navigation adapt to screen size &lt;/item&gt;
        &lt;item&gt; No pinching or zooming required &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt; Social Sharing&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; Links shared on Facebook, Twitter, Instagram show rich previews &lt;/item&gt;
        &lt;item&gt; Displays your photo and description automatically &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt; Images in example provided by https://pixabay.com/ &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lite.localcafe.org/"/><published>2025-12-02T20:08:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46126217</id><title>Paged Out</title><updated>2025-12-03T05:40:12.571298+00:00</updated><content>&lt;doc fingerprint="6d2c1476114285af"&gt;
  &lt;main&gt;
    &lt;p&gt;Paged Out! is a free experimental (one article == one page) technical magazine about programming (especially programming tricks!), hacking, security hacking, retro computers, modern computers, electronics, demoscene, and other similar topics.&lt;/p&gt;
    &lt;p&gt;It's made by the community for the community. And it's not-for-profit (though in time, we hope it will be self-sustained) - this means that the issues will always be free to download, share, and print. If you're interested in more details, check our our FAQ and About pages!&lt;/p&gt;
    &lt;p&gt;You can get printed issues at events and print-on-demand bookstores. You'll find more info here.&lt;/p&gt;
    &lt;p&gt;Issue #7 (Oct'25): Best kind of readme&lt;lb/&gt; Download counter: 161864&lt;lb/&gt; Print counter: 1016 (updated manually)&lt;/p&gt;
    &lt;p&gt;Prints:&lt;/p&gt;
    &lt;p&gt;Issue #6 (Mar'25): Stay a while and read&lt;lb/&gt; Download counter: 141302&lt;lb/&gt; Print counter: 2702 (updated manually)&lt;/p&gt;
    &lt;p&gt;Prints:&lt;/p&gt;
    &lt;p&gt;Issue #5 (Nov'24): All your page are belong to us&lt;lb/&gt; Download counter: 105399&lt;/p&gt;
    &lt;p&gt;What's missing:&lt;/p&gt;
    &lt;p&gt;Issue #4 (Jun'24): The epic Paged Out! story continues&lt;lb/&gt; Download counter: 116948&lt;/p&gt;
    &lt;p&gt;Note: This is a "beta build" of the PDF, i.e. we will be re-publishing it with various improvements multiple times. What's missing:&lt;/p&gt;
    &lt;p&gt;Issue #3 (Dec'23): The resurrected Paged Out!&lt;lb/&gt; Download counter: 122695&lt;/p&gt;
    &lt;p&gt;Note: This is a "beta build" of the PDF, i.e. we will be re-publishing it with various improvements multiple times. What's missing:&lt;/p&gt;
    &lt;p&gt;Issue #2 (Nov'19): The second Paged Out!&lt;lb/&gt; Download counter: 127568&lt;/p&gt;
    &lt;p&gt;Note: This is a "beta 2 build" of the PDF, i.e. we will be re-publishing it with various improvements multiple times. What's missing:&lt;/p&gt;
    &lt;p&gt;Issue #1 (Aug'19): The first Paged Out! issue has arrived!&lt;lb/&gt; Download counter: 260793&lt;lb/&gt; Print counter: 500 (updated manually)&lt;/p&gt;
    &lt;p&gt;Note: This is a "beta 1 build" of the PDF, i.e. we will be re-publishing it with various improvements multiple times. What's missing:&lt;/p&gt;
    &lt;p&gt;Additionally, here's another Paged Out! wallpaper by ReFiend:&lt;/p&gt;
    &lt;p&gt;If you like our work, how about writing an article for Paged Out!? It's only one page after all - easy. ;)&lt;/p&gt;
    &lt;p&gt; Next issue progress tracker (unit of measurement: article count):&lt;/p&gt;
    &lt;p&gt;Sure! There are a couple of ways to get notified when the issue will be out:&lt;/p&gt;
    &lt;p&gt;We will only send e-mails to this group about new Paged Out! issues (both the free electronic ones and special issues if we ever get to that). No spam will be sent there and (if you subscribe to the group) your e-mail will be visible only to group owners.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pagedout.institute"/><published>2025-12-02T20:14:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46126822</id><title>Delty (YC X25) Is Hiring</title><updated>2025-12-03T05:40:12.002282+00:00</updated><content>&lt;doc fingerprint="397cb4713f17ede8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;About Us&lt;/head&gt;
      &lt;p&gt;Delty is building the world’s first “AI Staff Engineer.” Unlike typical code-generation tools, Delty is trained on a team’s codebase, documentation, and system history — giving it a system-level understanding of architecture, conventions, and constraints. Delty helps engineering teams design enterprise-scale software systems, make architectural decisions, and enable AI coding agents to work with real system context.&lt;/p&gt;
      &lt;p&gt;Delty was founded by former engineering leaders from Google, including co-founders with deep experience at YouTube and in large-scale infrastructure. You’ll get to work alongside people who built massive systems at scale — a chance to learn a lot and contribute meaningfully from day one.&lt;/p&gt;
      &lt;p&gt;We believe in solving hard problems together as a team, iterating quickly, and building software with long-term thinking and ownership.&lt;/p&gt;
      &lt;head rend="h3"&gt;What You’ll Do&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Work full-stack: design and build features spanning front-end, back-end, data storage and processing.&lt;/item&gt;
        &lt;item&gt;Build new product modules and services from scratch — or evolve existing ones — guided by context-aware system design.&lt;/item&gt;
        &lt;item&gt;Work with AI and machine learning: integrate large-language models (LLMs), process large or long-form text data, apply traditional ML (e.g. regression, data pipelines), and build tooling around AI-driven flows.&lt;/item&gt;
        &lt;item&gt;Make architectural decisions — choose frameworks, data models, APIs, storage solutions — balancing trade-offs between performance, scalability, maintainability, and complexity.&lt;/item&gt;
        &lt;item&gt;Collaborate closely with co-founders and other engineers to translate product vision into a working, maintainable codebase.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What We’re Looking For&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;At least 3 years of full-stack engineering experience, including substantial work with AI/ML.&lt;/item&gt;
        &lt;item&gt;Strong skills across front-end, back-end, databases/data storage — and demonstrated ability to design end-to-end systems.&lt;/item&gt;
        &lt;item&gt;Experience working with or integrating AI/ML — LLMs, data pipelines, long-form text processing, traditional ML like regression or statistical modeling.&lt;/item&gt;
        &lt;item&gt;Good design sense and architectural thinking: you understand trade-offs (scalability vs complexity, speed vs maintainability) and can choose wisely based on constraints.&lt;/item&gt;
        &lt;item&gt;Comfort working in a fast-paced startup-style environment: nimble, iterative, high ownership.&lt;/item&gt;
        &lt;item&gt;Bonus: prior startup experience, or even having been a founder — we value entrepreneurial thinking, self-direction, and willingness to wear multiple hats.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Why join&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Learn from seasoned Google engineers: As former Google engineers who built systems at YouTube and Google Pay, we’ve operated at massive scale. Working alongside us gives you a chance to build similar systems and learn best practices, scale thinking, and software design deeply.&lt;/item&gt;
        &lt;item&gt;High impact: At a small but ambitious team, your contributions will influence architecture, product direction, and core features. You will have real ownership and see the effects of your work quickly.&lt;/item&gt;
        &lt;item&gt;Grow fast: We’re iterating rapidly; you’ll be exposed to the full stack, AI/ML pipelines, system architecture, data modeling, and product-level decisions — a fast-track to becoming a senior engineer or technical lead.&lt;/item&gt;
        &lt;item&gt;Challenging and meaningful work: We’re tackling the hardest part of software engineering: bridging AI-generated prototypes and robust, scalable enterprise-grade systems. If you enjoy thinking deeply about systems and building reliable, maintainable foundations — this is for you.&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/delty/jobs/aPWMaiq-full-stack-software-engineer"/><published>2025-12-02T21:00:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46126964</id><title>Ecosia: The greenest AI is here</title><updated>2025-12-03T05:40:11.706311+00:00</updated><content>&lt;doc fingerprint="14666b5f3aa6ed3a"&gt;
  &lt;main&gt;
    &lt;p&gt;While the AI race is raging, we’ve been building a better alternative. One that’s helpful, private, and optional — and that puts the planet first.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI, but thoughtful&lt;/head&gt;
    &lt;p&gt;AI-powered chatbots and search tools are fast becoming the way people ask questions online. To meet this moment — and to keep using 100% of our profits for the planet — we’re rolling out two features today, alongside a refreshed look.&lt;/p&gt;
    &lt;p&gt;Overviews give you a quick summary at the top of your search results, always with citations so you can explore the original sources yourself.&lt;/p&gt;
    &lt;p&gt;Prefer the classic experience? You can turn Overviews off with a single click.&lt;/p&gt;
    &lt;p&gt;For more detailed questions or ongoing conversations, try AI Search — an interactive chat mode where you can ask anything, from plant-based recipes to travel ideas. You can also receive eco tips rooted in the latest environmental science, if you choose.&lt;/p&gt;
    &lt;p&gt;As a not-for-profit company, we can afford to do things differently. AI Search uses smaller, more efficient models, and we avoid energy-heavy features like video generation altogether.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI that answers to the planet&lt;/head&gt;
    &lt;p&gt;Reducing AI’s footprint isn’t enough — we’re here to make a positive impact. That’s why we generate more renewable energy than our AI features use, from 100% clean sources like solar and wind.&lt;/p&gt;
    &lt;p&gt;We’ve invested €18M in renewable energy projects — expanding solar parks and adding clean power to the grid. The energy we generate helps displace fossil fuels and accelerate the transition to renewable energy.&lt;/p&gt;
    &lt;p&gt;We use tools like the AI Energy Score and Ecologits to select efficient models and track their energy use — keeping our process transparent, and ourselves accountable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your data stays yours&lt;/head&gt;
    &lt;p&gt;Our new features respect your privacy as much as they respect the planet. We collect only what’s necessary to deliver a great product, and not a byte more.&lt;/p&gt;
    &lt;p&gt;Earlier this year, we launched an independent European search index, which already powers AI Overviews and some of our search results. Building our own infrastructure gives us more control over the technology, so we can make it greener and more privacy-friendly.&lt;/p&gt;
    &lt;p&gt;Unlike Big Tech, we don’t run email, maps, or payment platforms, so we couldn’t piece together your life even if we wanted to. That’s not our business, and it never will be. As a European company, we’re bound by strict privacy laws like the GDPR, which means your data stays yours.&lt;/p&gt;
    &lt;p&gt;AI shouldn’t come at the cost of privacy. After all, we’re here for the trees, not your data.&lt;/p&gt;
    &lt;head rend="h2"&gt;For people and the planet&lt;/head&gt;
    &lt;p&gt;We’re learning as we go, and we’d love your thoughts along the way. Tell us what works, what doesn’t, and what you’d like to see next at AI.feedback@ecosia.org. Together, we can shape a future that’s not just more intelligent, but kinder, too. It’s the smartest thing to do.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.ecosia.org/ecosia-ai/"/><published>2025-12-02T21:14:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46127868</id><title>Exploring Large HTML Documents on the Web</title><updated>2025-12-03T05:40:11.300057+00:00</updated><content>&lt;doc fingerprint="930e18300d254bc7"&gt;
  &lt;main&gt;
    &lt;p&gt;Matt Zeunert (in/mattzeunert) is the founder of DebugBear, a web performance monitoring tool.&lt;/p&gt;
    &lt;p&gt;Most HTML documents are relatively small, providing a starting point for other resources on the page to load.&lt;/p&gt;
    &lt;p&gt;But why do some websites load several megabytes of HTML code? Usually it’s not that there’s a lot of content on the page, but rather that other types of resources are embedded within the document.&lt;/p&gt;
    &lt;p&gt;In this article, we’ll look at examples of large HTML documents around the web and peek into the code to see what’s making them so big.&lt;/p&gt;
    &lt;p&gt;HTML on the web is full of surprises. In the process of writing this article I rebuilt most of the DebugBear HTML Size Analyzer. If your HTML contains scripts that contain JSON that contains HTML that contains CSS that contains images – that’s supported now!&lt;/p&gt;
    &lt;head rend="h2"&gt;Embedded images&lt;/head&gt;
    &lt;p&gt;Base64 encoding is a way to turn images into text, so that they can be embedded in a text file like HTML or CSS. Embedding images directly in the HTML has a big advantage: the browser no longer needs to make a separate request to display the image.&lt;/p&gt;
    &lt;p&gt;However, for large files it’s likely to cause problems. For example, the image can no longer be cached independently, and the image will be prioritized in the same way as the document content, while usually it’s ok for images to load later.&lt;/p&gt;
    &lt;p&gt;Here’s an example of PNG files that are embedded in HTML using data URLs. &lt;/p&gt;
    &lt;p&gt;There are different variations of this pattern:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sometimes it’s a single multi-megabyte image that was included accidentally, other times there are hundreds of small icons that added up over time&lt;/item&gt;
      &lt;item&gt;I saw a site using responsive images together with data URLs. One goal of responsive images is only loading images at the minimum necessary resolution, but embedding all versions in the HTML has the opposite effect.&lt;/item&gt;
      &lt;item&gt;Indirectly embedded images: &lt;list rend="ul"&gt;&lt;item&gt;Inline SVGs that are themselves a thin wrapper around PNG or JPEG&lt;/item&gt;&lt;item&gt;Background images from inlined CSS stylesheets&lt;/item&gt;&lt;item&gt;Images within JSON data (more on that later 😬)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s an example of a style tag that contains 201 rules with embedded background images. &lt;/p&gt;
    &lt;head rend="h2"&gt;Inline CSS&lt;/head&gt;
    &lt;p&gt;Large inline CSS is usually due to images. However, long selectors from deeply nested CSS also contribute to CSS and HTML size.&lt;/p&gt;
    &lt;p&gt;In the example below, the HTML contains 20 inline style tags with similar content (variations like “header”, “header-mobile” and “header-desktop”). Most selectors are over 200 characters long, and as a result 47% of the overall stylesheet content consists of selectors instead of style declarations.&lt;/p&gt;
    &lt;p&gt;However, the HTML compresses well due to repetition within the selectors, and the size goes from 20.5 megabytes to only 2.3 megabytes after GZIP compression.&lt;/p&gt;
    &lt;head rend="h2"&gt;Embedded fonts&lt;/head&gt;
    &lt;p&gt;Like images, fonts are also sometimes encoded as Base64. For one or two small fonts this can actually work well, as text can render with the proper font right away.&lt;/p&gt;
    &lt;p&gt;However, when many fonts are embedded, it means visitors have to wait for these fonts to finish downloading before page content can render.&lt;/p&gt;
    &lt;head rend="h2"&gt;Client-side application state&lt;/head&gt;
    &lt;p&gt;Many modern websites are built as JavaScript applications. It would be slow to only show content after all JavaScript and required data has loaded, so during the initial page load the HTML is also rendered on the server.&lt;/p&gt;
    &lt;p&gt;Once the client-side application code has loaded, the static HTML is “hydrated”: the page content is made interactive with JavaScript, and client-side code takes control of future content updates.&lt;/p&gt;
    &lt;p&gt;Normally client-side code makes fetch requests to API endpoints on the backend to load in required data. But, since the initial client-side render requires the same data as the server-side rendering process, servers embed the hydration state in the final HTML. Then, the client-side hydration can take place right after loading all JavaScript, without making any additional API requests.&lt;/p&gt;
    &lt;p&gt;As you can guess, this hydration state can be big! You can identify it based on script tags that reference framework-specific keywords like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Next.js: &lt;code&gt;self.__next_f.push&lt;/code&gt;or&lt;code&gt;__NEXT_DATA__&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Nuxt: &lt;code&gt;__NUXT_DATA__&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Redux: &lt;code&gt;__PRELOADED_STATE__&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Apollo: &lt;code&gt;__APOLLO_STATE__&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Angular: &lt;code&gt;ng-state&lt;/code&gt;or similar&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__INITIAL_STATE__&lt;/code&gt;or&lt;code&gt;__INITIAL_DATA__&lt;/code&gt;in many custom setups&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In a local development environment with little data the size of the hydration state might not be noticeable. But as more data is added to the production database, the hydration state also grows. For example, a list of hotels references 3,561 different images (which, thankfully, are not embedded as Base64 😅). &lt;/p&gt;
    &lt;p&gt;If you pass Base64 images into your front-end components, they will also end up in the hydration state.&lt;/p&gt;
    &lt;p&gt;This website has 42 images embedded within the JSON data inside of the HTML document. The biggest image has a size of 2.5 megabytes. &lt;/p&gt;
    &lt;p&gt;There’s a surprising amount of nesting going. In the previous example we have images in JSON in a script in the HTML.&lt;/p&gt;
    &lt;p&gt;But we can go deeper than that! Let’s dive into our next example:&lt;/p&gt;
    &lt;p&gt;After digging into the hydration state, we find 52 products with a &lt;code&gt;judgmeWidget&lt;/code&gt; property. The value of this property is itself an HTML fragment!&lt;/p&gt;
    &lt;p&gt;Let’s put one of those values into the HTML Size Analyzer. Once again, most of the HTML is actually embedded JSON code, this time in the form of a data-json attribute on a div!&lt;/p&gt;
    &lt;p&gt;And what’s the name of the biggest property in that JSON? &lt;code&gt;body_html&lt;/code&gt; 😂😂😂&lt;/p&gt;
    &lt;head rend="h2"&gt;Other causes of large HTML&lt;/head&gt;
    &lt;p&gt;A few more examples I’ve seen during my research:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A 4-megabyte inline script&lt;/item&gt;
      &lt;item&gt;Unexpected metadata from Figma&lt;/item&gt;
      &lt;item&gt;A megamenu with over 7,000 items and 1,300 inline SVGs&lt;/item&gt;
      &lt;item&gt;Responsive images with 180 supported sizes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are still some large websites that still don’t apply GZIP or Brotli compression to their HTML. So while there’s not a lot of code, you still get a large transfer size.&lt;/p&gt;
    &lt;p&gt;Seeing a 53 kilobyte &lt;code&gt;NREUM&lt;/code&gt; script is also always frustrating: many websites embed New Relic’s end user monitoring script directly into the document &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt;. If you measure user experience you really want to avoid that performance impact!&lt;/p&gt;
    &lt;head rend="h2"&gt;How does HTML size impact page speed?&lt;/head&gt;
    &lt;p&gt;HTML code needs to be downloaded and parsed as part of the page load process. The more time this takes, the longer visitors have to wait for content to show up.&lt;/p&gt;
    &lt;p&gt;Browsers also assign a high priority to HTML content, assuming all of it is essential page content. That can mean that non-critical hydration state is downloaded before render-blocking stylesheets and JavaScript files are loaded.&lt;/p&gt;
    &lt;p&gt;You can see an example of that in this request waterfall from the DebugBear website speed test. While the browser knows about the other files early on, all bandwidth is instead consumed by the document.&lt;/p&gt;
    &lt;p&gt;Embedding images or fonts in the HTML also means that these files can’t be cached and re-used across pages. Instead they need to be redownloaded for every page load on the website.&lt;/p&gt;
    &lt;p&gt;Is time spent parsing HTML also a concern? On my MacBook it takes about 6 milliseconds to parse one megabyte of HTML code. In contrast, the low-end phone I use for testing takes about 80 milliseconds per megabyte. So for very large documents, CPU processing starts becoming a factor worth thinking about.&lt;/p&gt;
    &lt;head rend="h2"&gt;Websites with large HTML can still be fast&lt;/head&gt;
    &lt;p&gt;As you can tell, I might have a bit of an obsession with HTML size. But is it really a problem for many real visitors?&lt;/p&gt;
    &lt;p&gt;I don’t want to make large HTML files out to be a bigger issue than they really are. Most visitors coming to your website today probably have reasonably fast connections and devices. Other web performance problems tend to be more pressing. (Like actually running the JavaScript application code that’s using the hydration state.)&lt;/p&gt;
    &lt;p&gt;Pages also don’t need to download the full HTML document before they can start rendering. Here you can see that the document and important stylesheets are loaded in parallel. As a result, the main content renders before the document is fully loaded.&lt;/p&gt;
    &lt;p&gt;The real visitor data from Google’s Chrome User Experience Report (CrUX) shows that this website typically renders under 2 seconds. And that’s on a mobile device!&lt;/p&gt;
    &lt;p&gt;Still, the large document is definitely slowing the page down. One indicator of that is that the Largest Contentful Paint (LCP) image does not show up right away after loading. Instead, CrUX reports 584 milliseconds of render delay.&lt;/p&gt;
    &lt;p&gt;This tells us that the render-blocking stylesheet, which competes with other resources on the main website server, is loading more slowly than images from a different server.&lt;/p&gt;
    &lt;p&gt;It’s worth taking a quick look at your website HTML and to check what it actually contains. Often there are quick high-impact fixes you can make.&lt;/p&gt;
    &lt;p&gt;When images are inlined in HTML or CSS code it’s often intended to be a performance optimization. But a good setup can make it too easy to add more images later on without ever looking at the file being embedded. Consider adding guardrails to your CI build to catch unintended jumps in file size.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://calendar.perfplanet.com/2025/exploring-large-html-documents-on-the-web/"/><published>2025-12-02T22:32:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46128286</id><title>DOOM could have had PC Speaker Music</title><updated>2025-12-03T05:40:10.605405+00:00</updated><content>&lt;doc fingerprint="a77a35716886da8a"&gt;
  &lt;main&gt;
    &lt;p&gt;I'm guessing everyone here has played DOOM before, or at least seen someone else play the game.&lt;lb/&gt; It would also not be of any news for most here, that DOOM has specific hard-coded sound drivers which directly talk to the sound hardware.&lt;lb/&gt; Now, many PCs didn't have a dedicated (let alone supported) sound card for DOOM. What people often overlook is the PC Speaker driver that DOOM comes with. Mostly as it can only play back sound effects (and does so quite poorly too). Many times, it ends up disabled rather than being used.&lt;lb/&gt; For a long time, it has been speculated that the PC Speaker driver never supported audio as it would have been too resource intensive to drive the interface in real-time while performing game logic. Now, on a 286, I would totally understand this reasoning, but on a processor as fast as a 486? No chance it wouldn't work!&lt;lb/&gt; Introducing: The PC Speaker sndserver patch!&lt;lb/&gt; I had decided that the only way to answer the question of if, was to try it. And try it I did:&lt;lb/&gt; https://youtu.be/bRHyQPhA_9A&lt;lb/&gt; A few weeks ago, I had written a file format for efficiently playing PC Speaker tunes on a 32-bit system, requiring only a few integer operations to turn the data into a valid call for the input/misc/pcspkr device. The format being called pcsp and working as follows:&lt;lb/&gt; A song is made up of an array of 32 bit tone cells consisting of&lt;lb/&gt; - a 16 bit frequency value in Hz&lt;lb/&gt; - a 4 bit duration scale (second*10^-scale)&lt;lb/&gt; - a 12 bit duration value&lt;lb/&gt; Now, all I really had to do to get PC Speaker music working in DOOM, was to implement a priority mixer for it in sndserver.&lt;lb/&gt; The ground work for which already existed in the existing Adlib target.&lt;lb/&gt; Surprisingly, running the game with and without the patch showed no noticeable speed differences.&lt;lb/&gt; Will this patch become public? Yes, soon.&lt;lb/&gt; I do not feel comfortable with publishing it yet as I currently only have the E1M1 soundtrack implemented and also would like to fix a few other issues with the sndserver on modern Linux while I have the chance.&lt;/p&gt;
    &lt;head rend="h2"&gt;DOOM could have had PC Speaker Music!&lt;/head&gt;
    &lt;head rend="h3"&gt;DOOM could have had PC Speaker Music!&lt;/head&gt;
    &lt;p&gt;~-~-~ MSD - Making your old devices useful again since 2022! ~-~-~&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lenowo.org/viewtopic.php?t=45"/><published>2025-12-02T23:19:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46129476</id><title>Kohler Can Access Pictures from "End-to-End Encrypted" Toilet Camera</title><updated>2025-12-03T05:40:09.976485+00:00</updated><content>&lt;doc fingerprint="bc70d459793042ec"&gt;
  &lt;main&gt;
    &lt;p&gt;In October Kohler launched Dekota, a $600-plus-monthly-subscription device that attaches to the rim of your toilet and collects images and data from inside, promising to track and provide insights on gut health, hydration, and more. To allay the obvious privacy concerns, the company emphasizes the sensors are only pointed down, into the bowl, and assures potential buyers that the data collected by the device and app are protected with "end-to-end encryption”.&lt;/p&gt;
    &lt;p&gt;Kohler Health’s homepage, the page for the Kohler Health App, and a support page all use the term “end-to-end encryption” to describe the protection the app provides for data. Many media outlets included the claim in their articles covering the launch of the product.&lt;/p&gt;
    &lt;p&gt;However, responses from the company make it clear that—contrary to common understanding of the term—Kohler is able to access data collected by the device and associated application. Additionally, the company states that the data collected by the device and app may be used to train AI models.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is End-to-End Encryption?&lt;/head&gt;
    &lt;p&gt;"End-to-end encryption", or E2EE, is a method of securing data that ensures only the sender and their chosen recipient are able to view it. Correctly implemented, it prevents other parties, including the developer of the application, from accessing the protected data. E2EE is best known for its use in messaging applications like WhatsApp, iMessage, and Signal, where it allows users to communicate securely and privately without worrying about their messages being seen by prying eyes at the app developers, internet service providers, and even governments.&lt;/p&gt;
    &lt;p&gt;E2EE also provides an additional layer of protection if the servers of the application developer are compromised by an attacker. Any data stored on those servers will be meaningless to the attacker, which can significantly reduce the impact of a breach. For a more detailed look at E2EE, see A Deep Dive on End-to-End Encryption from the Electronic Frontier Foundation.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is Kohler Doing?&lt;/head&gt;
    &lt;p&gt;The initial issue with Kohler using the term “end-to-end encryption” is that it’s not obvious how it could apply to their product. The term is generally used for applications that allow some kind of communication between users, and Kohler Health doesn’t have any user-to-user sharing features. So while one “end” would be the user, it’s not clear what the other end would be.&lt;/p&gt;
    &lt;p&gt;I thought Kohler might actually have implemented a related data protection method known as “client-side encryption”, used by services like Apple’s iCloud and the password manager 1Password. This technique allows an application to back up a user’s data to the developers servers, or synchronize data between multiple devices owned by a user, without allowing anyone but the user to access the data.&lt;/p&gt;
    &lt;p&gt;But emails exchanged with Kohler’s privacy contact clarified that the other “end” that can decrypt the data is Kohler themselves: “User data is encrypted at rest, when it’s stored on the user's mobile phone, toilet attachment, and on our systems. Data in transit is also encrypted end-to-end, as it travels between the user's devices and our systems, where it is decrypted and processed to provide our service.”&lt;/p&gt;
    &lt;p&gt;They additionally told me “We have designed our systems and processes to protect identifiable images from access by Kohler Health employees through a combination of data encryption, technical safeguards, and governance controls.”&lt;/p&gt;
    &lt;p&gt;What Kohler is referring to as E2EE here is simply HTTPS encryption between the app and the server, something that has been basic security practice for two decades now, plus encryption at rest.&lt;/p&gt;
    &lt;head rend="h3"&gt;How is Kohler Using the Data?&lt;/head&gt;
    &lt;p&gt;If Kohler can access the data stored on its servers, what are they doing with it? While I don’t have a precise answer, there are indications they’re using it for purposes beyond simply providing a service to the user. This may include training AI models.&lt;/p&gt;
    &lt;p&gt;In response to my question about their use of E2EE, Kohler told me “our algorithms are trained on de-identified data only.” When signing up for an account on the app, the user is prompted to allow Kolher to use the data to "research, develop, and improve its products and technology, and to de-identify [the user’s] data for lawful purposes.”&lt;/p&gt;
    &lt;p&gt;And the privacy policy states data may be used “To create aggregated, de-identified and/or anonymized data, which we may use and share with third parties for our lawful business purposes, including to analyze and improve the Kohler Health Platform and our other products and services, to promote our business, and to train our AI and machine learning models.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://varlogsimon.leaflet.pub/3m6zrw6k2bs2p?interactionDrawer=quotes"/><published>2025-12-03T02:06:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46130187</id><title>Japanese game devs face font dilemma as license increases from $380 to $20k</title><updated>2025-12-03T05:40:09.824658+00:00</updated><content>&lt;doc fingerprint="cd6c1fd3988ec7f0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Japanese devs face font licensing dilemma as leading provider increases annual plan price from $380 to $20,000+&lt;/head&gt;
    &lt;p&gt;"This is a little-known issue, but it's become a huge problem"&lt;/p&gt;
    &lt;p&gt;Japanese game makers are struggling to locate affordable commercial fonts after one of the country's leading font licensing services raised the cost of its annual plan from around $380 to $20,500 (USD).&lt;/p&gt;
    &lt;p&gt;As reported by Gamemakers and GameSpark and translated by Automaton, Fontworks LETS discontinued its game licence plan at the end of November.&lt;/p&gt;
    &lt;p&gt;The expensive replacement plan – offered through Fontwork's parent company, Monotype – doesn't even provide local pricing for Japanese developers, and comes with a 25,000 user-cap, which is likely not workable for Japan's bigger studios.&lt;/p&gt;
    &lt;p&gt;The problem is further compounded by the difficulties and complexities of securing fonts that can accurately transcribe Kanji and Katakana characters.&lt;/p&gt;
    &lt;p&gt;"This is a little-known issue, but it's become a huge problem in some circles," wrote CEO of development studio Indie-Us Games.&lt;/p&gt;
    &lt;p&gt;UI/UX designer Yamanaka stressed that this would be particularly problematic for live service games; even if studios moved quickly and switched to fonts available through an alternate licensee, they will have to re-test, re-validate, and re-QA check content already live and in active use.&lt;/p&gt;
    &lt;p&gt;The crisis could even eventually force some Japanese studios to rebrand entirely if their corporate identity is tied to a commercial font they can no longer afford to license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gamesindustry.biz/japanese-devs-face-font-licensing-dilemma-as-leading-provider-increases-annual-plan-price-from-380-to-20000"/><published>2025-12-03T04:03:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46130233</id><title>Understanding ECDSA</title><updated>2025-12-03T05:40:09.551341+00:00</updated><content>&lt;doc fingerprint="1efcc89f0ba71b95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Understanding ECDSA&lt;/head&gt;
    &lt;head rend="h2"&gt;Prerequisites and audience&lt;/head&gt;
    &lt;p&gt;In this article, we'll try to understand how ECDSA (Elliptic Curve Digital Signature Algorithm) works.&lt;/p&gt;
    &lt;p&gt;The version I have in mind is the one used by the Ethereum blockchain. Since my interest lies in security, we'll also explore the signature malleability attack.&lt;/p&gt;
    &lt;p&gt;I expect you to be familiar with Public Key Cryptography and how it can be used to sign messages, at least conceptually.&lt;/p&gt;
    &lt;p&gt;You'll only need to know basic math, so abstract algebra is not a requirement. I'll introduce the bare minimum as we go. My exposition will be deliberately unsophisticated, favoring ease of understanding over conciseness and elegance.&lt;/p&gt;
    &lt;p&gt;The reader I have in mind is someone dissatisfied with the many superficial, hand-wavy explanations of ECDSA often found in articles and books aimed at developers and auditors, but who doesn't have the time or interest to go all the way down the rabbit hole and learn cryptography in a thorough and systematic way.&lt;/p&gt;
    &lt;p&gt;If you, like me, work in a field where you need to have a working knowledge of multiple disciplines, you'll probably appreciate this kind of compromise.&lt;/p&gt;
    &lt;p&gt;Finally, this might also serve as an introduction to the topic before you turn to more serious and academic literature.&lt;/p&gt;
    &lt;head rend="h2"&gt;Not your typical article&lt;/head&gt;
    &lt;p&gt;You can think of this section as a kind of disclaimer.&lt;/p&gt;
    &lt;p&gt;This article is the result of an exercise where I start from a vague understanding of a topic and try to connect all the dots and fill in all the gaps on my own, without relying on any external sources of information. This means no books, no LLMs, and no internet.&lt;/p&gt;
    &lt;p&gt;For the exercise to be effective, it needs to be written with an audience in mind, forcing you to keep track of what you've already explained and what you can expect the reader to know. It also helps you do a better job because you feel more exposed.&lt;/p&gt;
    &lt;p&gt;Have you ever gone back to something you learned in the past and realized you forgot most of it? Your knowledge has become sparse and all you remember are some facts disconnected from each other.&lt;/p&gt;
    &lt;p&gt;Can you restore the original picture on your own?&lt;/p&gt;
    &lt;p&gt;If you succeed, your final understanding will be much deeper than the one you'd have if you relied on external help such as books and notes.&lt;/p&gt;
    &lt;p&gt;With this article, I go a step further and try to connect the dots with knowledge that I never had to begin with. The fact that it's possible is what makes mathematical topics so special.&lt;/p&gt;
    &lt;p&gt;That should explain why I wrote it, but why should you read it?&lt;/p&gt;
    &lt;p&gt;Well, you get to read something:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Constructive in nature, since most of the formulas and derivations have to be recreated from scratch.&lt;/item&gt;
      &lt;item&gt;Insightful, since I share some of my intuition and mental models, which is somewhat unusual in more rigorous settings.&lt;/item&gt;
      &lt;item&gt;Naive, as I observe and notice some connections for the first time, possibly making my exposition more engaging but also less polished.&lt;/item&gt;
      &lt;item&gt;Non-authoritative, demanding your full attention and critical thinking to spot inconsistencies.&lt;/item&gt;
      &lt;item&gt;Non-standard, since some facts may be stated or named differently from official literature.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your role is that of an auditor or verifier, constantly trying to find any inconsistencies and non sequiturs in what I wrote: I'm the generator and you the discriminator. In a (constructively) adversarial setting, this would be an iterative process.&lt;/p&gt;
    &lt;p&gt;It goes without saying that this article is meant to be read linearly, from the start.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modular arithmetic&lt;/head&gt;
    &lt;p&gt;It's all around us:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Mon&lt;/cell&gt;
        &lt;cell role="head"&gt;Tue&lt;/cell&gt;
        &lt;cell role="head"&gt;Wed&lt;/cell&gt;
        &lt;cell role="head"&gt;Thu&lt;/cell&gt;
        &lt;cell role="head"&gt;Fri&lt;/cell&gt;
        &lt;cell role="head"&gt;Sat&lt;/cell&gt;
        &lt;cell role="head"&gt;Sun&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;29&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;31&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If we're just interested in the day of the week, then the numbers in the same column are equivalent. What do they have in common? The fact that the difference between any two of them is always a multiple of \(7\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(29-8 = 21 = 3\cdot 7\)&lt;/item&gt;
      &lt;item&gt;\(22-15 = 7\)&lt;/item&gt;
      &lt;item&gt;\(31-17 = 14 = 2\cdot 7\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since numbers in the same column are equivalent, we can represent all of them by the smallest one. Let's call it the representative of the column. If we do that, we end up with \(7\) numbers:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Mon&lt;/cell&gt;
        &lt;cell role="head"&gt;Tue&lt;/cell&gt;
        &lt;cell role="head"&gt;Wed&lt;/cell&gt;
        &lt;cell role="head"&gt;Thu&lt;/cell&gt;
        &lt;cell role="head"&gt;Fri&lt;/cell&gt;
        &lt;cell role="head"&gt;Sat&lt;/cell&gt;
        &lt;cell role="head"&gt;Sun&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That's not ideal for a calendar, but it makes sense: we just add multiples of \(7\) to the starting numbers to recover the missing ones.&lt;/p&gt;
    &lt;p&gt;How do we get the representative from a number? For instance, what's the representative of \(45\)? Well, \(45 = 3 + 7\cdot 6\), so the representative is \(3\). Indeed, starting from \(3\), we add a multiple of \(7\) to get \(45\).&lt;/p&gt;
    &lt;p&gt;Now what's \(3\) with respect to \(45\)? It's the remainder of \(45\) divided by \(7\). We can get that number by using the mod(ulo) operator: \(45\ \mathrm{mod}\ 7 = 3\), or &lt;code&gt;45 % 7 == 3&lt;/code&gt;, in many programming languages.&lt;/p&gt;
    &lt;p&gt;Beware:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In JS: &lt;code&gt;-45 % 7&lt;/code&gt;is \(-3\)&lt;/item&gt;
      &lt;item&gt;In Solidity: &lt;code&gt;-45 % 7&lt;/code&gt;is \(-3\)&lt;/item&gt;
      &lt;item&gt;In Python: &lt;code&gt;-45 % 7&lt;/code&gt;is \(4\)&lt;/item&gt;
      &lt;item&gt;In Math: \(-45\ \mathrm{mod}\ 7\) is \(4\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both values make sense, since they're separated by \(7\) and, thus, in the same column, or equivalence class, i.e. the class of all equivalent elements. But we want the representative, so \(4\) is preferred. Observe that \(-45 = -7\cdot 7 + 4\).&lt;/p&gt;
    &lt;p&gt;Basically, any time we're outside the window \(\{0, \ldots, 6\}\), we add or subtract \(7\) as many times as we need to land in the window.&lt;/p&gt;
    &lt;p&gt;Note that &lt;code&gt;((n % 7) + 7) % 7&lt;/code&gt; will give the representative in any language, since:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;n % 7&lt;/code&gt;is in \(\{-6, -5, \ldots, 0, \ldots, 5, 6\}\)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(n % 7) + 7&lt;/code&gt;is in \(\{1, \ldots, 13\}\)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;((n % 7) + 7) % 7&lt;/code&gt;is in \(\{0, \ldots, 6\}\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Observe that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;adding \(7\) doesn't change the equivalence class&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(x % 7) % 7&lt;/code&gt;is just&lt;code&gt;x % 7&lt;/code&gt;. This property is called idempotency (same power): reapplying the operation doesn't increase the extent of the effect, i.e. it gives the same result.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of writing mod operators everywhere, we can say that we're computing mod \(p\):&lt;/p&gt;
    &lt;p&gt;That's equivalent to&lt;/p&gt;
    &lt;p&gt;which is a pain to write.&lt;/p&gt;
    &lt;p&gt;If we're only dealing with addition and multiplication, then we can insert as many "mod \(p\)" as we want wherever we want, so these two expressions are equivalent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\((123456 \cdot 345678 + 876876234)\ \mathrm{mod}\ p\)&lt;/item&gt;
      &lt;item&gt;\([(((123456\ \mathrm{mod}\ p)\cdot (345678 \ \mathrm{mod}\ p))\ \mathrm{mod}\ p) + (876876234\ \mathrm{mod}\ p)]\ \mathrm{mod}\ p\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's not true for exponentiation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(2^8\ \mathrm{mod}\ 7 = 4\)&lt;/item&gt;
      &lt;item&gt;\(2^{8\ \mathrm{mod}\ 7} = 2^1 = 2\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ECDSA doesn't rely on exponentiation, so we don't need to talk about it.&lt;/p&gt;
    &lt;p&gt;We still don't know how to divide mod \(p\). That is, we don't know how to compute, say, \(3/4\) mod \(p\), or whether it even exists.&lt;/p&gt;
    &lt;p&gt;What does dividing by \(4\) do? It does something that can be reversed by multiplying by \(4\). So the two operations cancel out and are equivalent to multiplying by \(1\), the neutral element. In other words, we must have&lt;/p&gt;
    &lt;p&gt;That's usually written as&lt;/p&gt;
    &lt;p&gt;where \(a^{-1}\) is called the multiplicative inverse of \(a\).&lt;/p&gt;
    &lt;p&gt;As an aside, the additive inverse, or opposite, is simply \(-n\), since \(n + (-n) = 0\), where \(0\) is the neutral element of addition. Of course, we can compute \(-n\ \mathrm{mod}\ p\) to get the representative of \(-n\).&lt;/p&gt;
    &lt;p&gt;Let's find \(x\) such that \(4\cdot x = 1\ (\mathrm{mod}\ 7)\) by using simple brute force:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(4\cdot 0 = 0\)&lt;/item&gt;
      &lt;item&gt;\(4\cdot 1 = 4\)&lt;/item&gt;
      &lt;item&gt;\(4\cdot 2 = 1\)&lt;/item&gt;
      &lt;item&gt;\(4\cdot 3 = 5\)&lt;/item&gt;
      &lt;item&gt;\(4\cdot 4 = 2\)&lt;/item&gt;
      &lt;item&gt;\(4\cdot 5 = 6\)&lt;/item&gt;
      &lt;item&gt;\(4\cdot 6 = 3\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I omitted "\(\left(\mathrm{mod}\ 7\right)\)" for convenience. I'll do that often from now on.&lt;/p&gt;
    &lt;p&gt;As we can see, \(4^{-1} = 2\). That's because \(4\cdot 2 = 8 = 8-7 = 1\).&lt;/p&gt;
    &lt;p&gt;Let's go back to our \(3/4\):&lt;/p&gt;
    &lt;p&gt;Indeed:&lt;/p&gt;
    &lt;p&gt;so we get \(3\) back.&lt;/p&gt;
    &lt;p&gt;An important fact to know is that a number \(a\) is always invertible mod \(p\), as long as it's coprime with \(p\), i.e. their GCD (greatest common divisor) is \(1\).&lt;/p&gt;
    &lt;p&gt;Proof (safely skippable)&lt;/p&gt;
    &lt;p&gt;Let's define \(r_x = a\cdot x\ \mathrm{mod}\ p\). Let \(r\) be the sequence \(r_0, \ldots, r_{p-1}\).&lt;/p&gt;
    &lt;p&gt;Again, I'll omit "mod \(p\)" for notational convenience.&lt;/p&gt;
    &lt;p&gt;If \(r_x = r_y\), i.e. \(a\cdot x = a\cdot y\), then \(a(x-y) = 0\), which means that \(a(x-y)\) is divisible by \(p\). If \(a\) and \(p\) are coprime, then \(x-y\) must be divisible by \(p\), so \(x-y = 0\), i.e. \(x=y\). In other words, \(x\neq y\) implies that \(r_x \neq r_y\).&lt;/p&gt;
    &lt;p&gt;This means that \(r\) has \(p\) distinct values in \(\{0, \ldots, p-1\}\), i.e. \(r\) is a permutation of the sequence \(0, \ldots, p-1\). In particular, \(r\) contains exactly one \(1\), so there's exactly one \(x\) such that \(a\cdot x = 1\).&lt;/p&gt;
    &lt;p&gt;End of proof!&lt;/p&gt;
    &lt;p&gt;As an example, let's look again at the brute-forcing we did above to find \(4^{-1}\ \mathrm{mod}\ 7\) and note that the results are a permutation of the numbers from \(0\) to \(6\), so they contain exactly one \(1\). That's expected since \(4\) and \(7\) are coprime.&lt;/p&gt;
    &lt;p&gt;Observe that when \(p\) is prime, all numbers from \(1\) to \(p-1\) are coprime with it, so they're all invertible.&lt;/p&gt;
    &lt;p&gt;Technically, the set of representatives \(0, \ldots, p-1\) is often denoted by \(\mathbb{Z}_p\) or \(\mathbb{Z}/p\mathbb{Z}\). It's obtained by partitioning the integers into equivalence classes (our calendar columns, but extended to all integers) and representing each class by a representative in \(\{0, \ldots, p-1\}\). That's what we did informally.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Euclidean algorithm&lt;/head&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;You can safely skip this section, if you already know or don't care about how the multiplicative inverse can be computed in practice. If you're interested in the method of generating functions you might still want to read the Fibonacci numbers subsection, though.&lt;/p&gt;
    &lt;p&gt;For a fast and practical way to compute the multiplicative inverse, we can use the extended Euclidean algorithm (EEA).&lt;/p&gt;
    &lt;p&gt;The Euclidean algorithm (EA) can be used to efficiently compute \(\mathrm{GCD}(a, p)\), and its extended version returns two integers \(x\) and \(y\) such that&lt;/p&gt;
    &lt;p&gt;If \(a\) and \(p\) are coprime, then&lt;/p&gt;
    &lt;p&gt;This means that \(x\) is the multiplicative inverse of \(a\) mod \(p\).&lt;/p&gt;
    &lt;p&gt;How does the algorithm work? It's very simple.&lt;/p&gt;
    &lt;p&gt;The first observation is that&lt;/p&gt;
    &lt;p&gt;and, by symmetry,&lt;/p&gt;
    &lt;p&gt;We will prove this later.&lt;/p&gt;
    &lt;p&gt;Since we can subtract repeatedly, we can also use the mod operator:&lt;/p&gt;
    &lt;p&gt;This way, we can reduce the two arguments very quickly. Note that in a real implementation we only need one &lt;code&gt;mod&lt;/code&gt; per step, since one of the two has clearly no effect.&lt;/p&gt;
    &lt;p&gt;Let's use it to compute \(\mathrm{GCD}(784, 495)\):&lt;/p&gt;
    &lt;p&gt;The second column shows how we got the new values. Since we obtained \(\mathrm{GCD}(3, 1)\), the GCD is \(1\), i.e. \(784\) and \(495\) are coprime.&lt;/p&gt;
    &lt;p&gt;The extended version of the algorithm uses the second column in a simple way. To start, we notice that the equation at the bottom of the second column is already in the right form, i.e.&lt;/p&gt;
    &lt;p&gt;However, we want the expression with respect to the initial values \(784\) and \(495\).&lt;/p&gt;
    &lt;p&gt;The solution is easy: we just do substitutions as we go up the second column, starting from the bottom:&lt;/p&gt;
    &lt;p&gt;Indeed, \(495\cdot 255 - 784\cdot 161 = 1\).&lt;/p&gt;
    &lt;p&gt;So now we know that \(495\cdot 255 = 1\ (\mathrm{mod}\ 784)\).&lt;/p&gt;
    &lt;p&gt;Now, the only thing missing is to prove that&lt;/p&gt;
    &lt;p&gt;Let me write \(a|b\) to mean that \(a\) divides \(b\), i.e. \(b = ah\) for some integer \(h\).&lt;/p&gt;
    &lt;p&gt;If two numbers divide each other, they must be equal, so we just need to prove that, for any integer \(k\),&lt;/p&gt;
    &lt;p&gt;Indeed, we can then argue that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(\mathrm{GCD}(a, b) \mid \mathrm{GCD}(a, b-a)\)&lt;/item&gt;
      &lt;item&gt;\(\mathrm{GCD}(a, b-a) \mid \mathrm{GCD}(a, (b-a)+a)\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's prove that&lt;/p&gt;
    &lt;p&gt;Let \(d_1\) be the GCD on the left and \(d_2\) the one on the right. It's clear that \(d_1|u\) and \(d_1|v\), which implies that \(d_1|(v+ku)\). Now we'd like to conclude that \(d_1|d_2\).&lt;/p&gt;
    &lt;p&gt;Unfortunately, we only proved that \(d_1\) is a common divisor of \(u\) and \(v+ku\) so far.&lt;/p&gt;
    &lt;p&gt;Let's show that if \(d'\) divides both \(a\) and \(b\), then it also divides \(d = \mathrm{GCD}(a,b)\).&lt;/p&gt;
    &lt;p&gt;Proof (safely skippable)&lt;/p&gt;
    &lt;p&gt;We can always express \(d\) and \(d'\) as&lt;/p&gt;
    &lt;p&gt;where, as indicated, \(u\) and \(v\) are coprime.&lt;/p&gt;
    &lt;p&gt;Observe that if \(u\) and \(v\) weren't coprime, their common divisor would be absorbed by \(\mathrm{GCD}(d, d')\), so we'd have the same situation as above but for \(u'=u/\mathrm{GCD}(u,v)\) and \(v'=v/\mathrm{GCD}(u,v)\).&lt;/p&gt;
    &lt;p&gt;Since \(a\) and \(b\) are divisible by both \(d'\) and \(d\), then \(a' = a/\mathrm{GCD}(d, d')\) and \(b' = b/\mathrm{GCD}(d, d')\) must still be divisible by \(u\) and \(v\). So:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(u k_1 = a' = v k_2\)&lt;/item&gt;
      &lt;item&gt;\(u h_1 = b' = v h_2\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;for some integers \(k_1\), \(k_2\), \(h_1\), and \(h_2\).&lt;/p&gt;
    &lt;p&gt;Since \(u\) and \(v\) are coprime, then \(u|k_2\) and \(u|h_2\), i.e. \(k_2 = u k_3\) and \(h_2 = u h_3\) for some integers \(k_3\) and \(h_3\). Therefore:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(a' = uv k_3\implies a = uv \mathrm{GCD}(d, d') k_3 = ud k_3\)&lt;/item&gt;
      &lt;item&gt;\(b' = uv h_3\implies b = uv \mathrm{GCD}(d, d') h_3 = ud h_3\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means that \(ud\) is a common divisor of \(a\) and \(b\), and \(u&amp;gt;1\) would imply that we found a greater divisor than \(d\), their GCD.&lt;/p&gt;
    &lt;p&gt;Since \(u = 1\), then \(d' = \mathrm{GCD}(d, d')\), i.e. \(d'|d\).&lt;/p&gt;
    &lt;p&gt;End of proof!&lt;/p&gt;
    &lt;p&gt;I seem to recall that some people include this property in the definition of the GCD itself, but I think that's slightly redundant.&lt;/p&gt;
    &lt;p&gt;Anyway, we're done!&lt;/p&gt;
    &lt;p&gt;Wait! How fast is this algorithm? Let's look at the reduction again:&lt;/p&gt;
    &lt;p&gt;I can see two super Fibonacci sequences. Here's the green one:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Green Seq.&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;83&lt;/cell&gt;
        &lt;cell&gt;206&lt;/cell&gt;
        &lt;cell&gt;289&lt;/cell&gt;
        &lt;cell&gt;495&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Green Mult.&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fibonacci numbers form a sequence \(F_0, F_1, F_2, \ldots\) where the recurrence relation is \(F_i=F_{i-2}+F_{i-1}\), for \(i=2, 3, \ldots\).&lt;/p&gt;
    &lt;p&gt;In our case, however, the recurrence relation is \(F_i = F_{i-2}+F_{i-1}\cdot M_{i-1}\), where \(F\) is on the first row and \(M\) on the second row of the table.&lt;/p&gt;
    &lt;p&gt;As an example, I highlighted 4 elements in the table: \(206 = 40 + 83\cdot 2\). I call this a super Fibonacci sequence because the multipliers make it grow faster than the regular one (corresponding to all \(M_i=1\)).&lt;/p&gt;
    &lt;p&gt;Fibonacci numbers grow exponentially, so the number of steps necessary to reach a number \(n\) is \(\Theta(\log n)\).&lt;/p&gt;
    &lt;p&gt;Since our sequence is even faster, the number of steps is lower and all we can say for now is that the worst-case complexity of EEA is \(O(\log n)\).&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Technically, \(\Theta\) denotes exact growth, \(O\) denotes an upper bound, and \(\Omega\) denotes a lower bound.&lt;/p&gt;
    &lt;p&gt;For instance, \(n = O(n^2)\) is correct, though in practice people often use \(O\) when they really mean \(\Theta\).&lt;/p&gt;
    &lt;p&gt;Moreover, \(n = O(n^2)\) really means \(n \in O(n^2)\), but the former notation is more common than the latter.&lt;/p&gt;
    &lt;p&gt;Can we think of a very slow sequence? But, of course! We can build it starting from the bottom and always choosing \(M_i=1\):&lt;/p&gt;
    &lt;p&gt;Those are basically two Fibonacci sequences! This tells us that the worst case of the EEA is indeed logarithmic or, to be precise, \(\Theta(\log (\min\{a, b\}))\). Why min? Because we have two sequences: the green and the red one. Since they start and end together, the faster one dominates the other and faster growth means shorter sequence, so the time complexity is \(\Theta(\min\{\log a, \log b\})\), i.e. \(\Theta(\log (\min\{a, b\}))\).&lt;/p&gt;
    &lt;p&gt;I had no idea that the EA had such a connection with the Fibonacci numbers before writing this section. As always, check my reasoning!&lt;/p&gt;
    &lt;head rend="h4"&gt;Fibonacci numbers&lt;/head&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;You can safely skip this section. You don't need it for the rest of the article, but if you want to learn about generating functions, I think this is a good opportunity.&lt;/p&gt;
    &lt;p&gt;I want to find the base for the logarithm that appears in the time complexity of the EA and EEA algorithms.&lt;/p&gt;
    &lt;p&gt;If we assume that Fibonacci numbers grow exponentially, i.e. \(F_i\sim b^i\), then:&lt;/p&gt;
    &lt;p&gt;We divide by \(b^i\) and get \(b^2-b-1 = 0\), whose positive solution is&lt;/p&gt;
    &lt;p&gt;That's the well-known golden ratio.&lt;/p&gt;
    &lt;p&gt;We started from the assumption that the growth is exponential, but what's the exact expression for the \(n\)-th Fibonacci number, just to make sure we're correct?&lt;/p&gt;
    &lt;p&gt;Let \(V_0\) be the vector of Fibonacci numbers \(F_i\):&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;\(V_0:\)&lt;/cell&gt;
        &lt;cell&gt;\(F_0\)&lt;/cell&gt;
        &lt;cell&gt;\(F_1\)&lt;/cell&gt;
        &lt;cell&gt;\(F_2\)&lt;/cell&gt;
        &lt;cell&gt;\(F_3\)&lt;/cell&gt;
        &lt;cell&gt;\(F_4\)&lt;/cell&gt;
        &lt;cell&gt;\(F_5\)&lt;/cell&gt;
        &lt;cell&gt;\(F_6\)&lt;/cell&gt;
        &lt;cell&gt;\(\ldots\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Now let's introduce the two shifted versions \(V_1\) and \(V_2\):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;\(V_0:\)&lt;/cell&gt;
        &lt;cell&gt;\(F_0\)&lt;/cell&gt;
        &lt;cell&gt;\(F_1\)&lt;/cell&gt;
        &lt;cell&gt;\(F_2\)&lt;/cell&gt;
        &lt;cell&gt;\(F_3\)&lt;/cell&gt;
        &lt;cell&gt;\(F_4\)&lt;/cell&gt;
        &lt;cell&gt;\(F_5\)&lt;/cell&gt;
        &lt;cell&gt;\(F_6\)&lt;/cell&gt;
        &lt;cell&gt;\(\ldots\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;\(V_1:\)&lt;/cell&gt;
        &lt;cell&gt;\(F_0\)&lt;/cell&gt;
        &lt;cell&gt;\(F_1\)&lt;/cell&gt;
        &lt;cell&gt;\(F_2\)&lt;/cell&gt;
        &lt;cell&gt;\(F_3\)&lt;/cell&gt;
        &lt;cell&gt;\(F_4\)&lt;/cell&gt;
        &lt;cell&gt;\(F_5\)&lt;/cell&gt;
        &lt;cell&gt;\(\ldots\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(V_2:\)&lt;/cell&gt;
        &lt;cell&gt;\(F_0\)&lt;/cell&gt;
        &lt;cell&gt;\(F_1\)&lt;/cell&gt;
        &lt;cell&gt;\(F_2\)&lt;/cell&gt;
        &lt;cell&gt;\(F_3\)&lt;/cell&gt;
        &lt;cell&gt;\(F_4\)&lt;/cell&gt;
        &lt;cell&gt;\(\ldots\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We can see that, from the third column onward, \(V_0 = V_1 + V_2\), because of the relation \(F_{i+2} = F_{i+1} + F_{i}\).&lt;/p&gt;
    &lt;p&gt;The advantage of the vector approach is that we don't have to deal with the index \(i\) anymore. In a sense, we vectorized the loop and abstracted away the annoying index. The drawback is that we lost some algebraic power because, unless we introduce other operations, we don't even know how to express the fact that \(V_1\) is a shifted version of \(V_0\).&lt;/p&gt;
    &lt;p&gt;Instead of reinventing the wheel, why don't we use a power series instead of a simple vector? I'm thinking of something like this:&lt;/p&gt;
    &lt;p&gt;The individual elements are kept separated thanks to the different powers of \(x\), and we inherit lots of algebraic properties from power series!&lt;/p&gt;
    &lt;p&gt;For instance, it's easy to see that \(P_1(x) = x P_0(x)\) and \(P_2(x) = x^2 P_0(x)\).&lt;/p&gt;
    &lt;p&gt;Moreover, we can state algebraically what we observed before:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We can see that, from the third column onward, \(V_0 = V_1 + V_2\), because of the relation \(F_{i+2} = F_{i+1} + F_{i}\).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With power series, that becomes&lt;/p&gt;
    &lt;p&gt;Note that we simply removed the unwanted terms in the first two columns:&lt;/p&gt;
    &lt;p&gt;To reiterate, we expressed all the following relations at once:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(F_2 = F_1 + F_0\)&lt;/item&gt;
      &lt;item&gt;\(F_3 = F_2 + F_1\)&lt;/item&gt;
      &lt;item&gt;\(F_4 = F_3 + F_2\)&lt;/item&gt;
      &lt;item&gt;\(\ldots\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can now express \(P_1\) and \(P_2\) in terms of \(P_0\). For convenience, let's write \(P\) instead of \(P_0(x)\):&lt;/p&gt;
    &lt;p&gt;Now we solve for \(P\):&lt;/p&gt;
    &lt;p&gt;Since Fibonacci numbers start with \(0\) and \(1\), let's substitute \(F_0=0\) and \(F_1=1\):&lt;/p&gt;
    &lt;p&gt;That's in implicit form. If we can put \(P\) in explicit form, then we can read the expression for the generic coefficient of \(x^i\), which, by construction, is the \(i\)-th Fibonacci number! Here's the form we want:&lt;/p&gt;
    &lt;p&gt;The coefficient \(\alpha_i\) is bound to be a general expression for \(F_i\).&lt;/p&gt;
    &lt;p&gt;How do we do that?&lt;/p&gt;
    &lt;p&gt;Let's take the simplest power series we can think of and find both the explicit and implicit forms for it:&lt;/p&gt;
    &lt;p&gt;We can use the same trick, i.e. the shift:&lt;/p&gt;
    &lt;p&gt;Maybe surprisingly:&lt;/p&gt;
    &lt;p&gt;Written more formally, we proved that&lt;/p&gt;
    &lt;p&gt;We don't care about convergence, as we only want to read the coefficients of the series. As long as what we do is algebraically correct, we should be fine. We might say that we're repurposing some algebraic machinery to do "something else".&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;We say a series converges if it evaluates to a real number. Otherwise, we say it diverges. For instance, the following series clearly converges: $$ \sum_{i=0}^\infty \left(\frac{1}{10}\right)^i = 1 + 0.1 + 0.01 + \ldots = 1.1111\ldots $$&lt;/p&gt;
    &lt;p&gt;In the Fibonacci case, we want to find \(\alpha_i\) such that&lt;/p&gt;
    &lt;p&gt;Now that we've witnessed how the simple case works, we should have more confidence that this method might just work! It might still look like magic, though.&lt;/p&gt;
    &lt;p&gt;How do we close the gap between the simple case and the Fibonacci case?&lt;/p&gt;
    &lt;p&gt;First, we notice that the denominator is factorizable:&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;p&gt;These are the same solutions we found for \(b\) at the start of this section, but multiplied by \(-1\).&lt;/p&gt;
    &lt;p&gt;Now we can split the expression into two simpler ones:&lt;/p&gt;
    &lt;p&gt;We just need to find the appropriate \(A\) and \(B\) to get \(-x\) at the numerator:&lt;/p&gt;
    &lt;p&gt;We want \(-x = x(A+B) - c_2 A - c_1 B\), so we must have&lt;/p&gt;
    &lt;p&gt;The solutions are&lt;/p&gt;
    &lt;p&gt;Therefore:&lt;/p&gt;
    &lt;p&gt;If we can convert each of the two parts into explicit form, then we're done, since explicit forms sum nicely: we just sum the corresponding coefficients.&lt;/p&gt;
    &lt;p&gt;Now we divide numerator and denominator of the left part by \(c_1\) and of the right part by \(c_2\):&lt;/p&gt;
    &lt;p&gt;We change some signs:&lt;/p&gt;
    &lt;p&gt;Success:&lt;/p&gt;
    &lt;p&gt;Expanding and simplifying:&lt;/p&gt;
    &lt;p&gt;We can simplify it further, since \(c_1 c_2 = -1\) and \(c_1 - c_2 = \sqrt5\):&lt;/p&gt;
    &lt;p&gt;At last:&lt;/p&gt;
    &lt;p&gt;with&lt;/p&gt;
    &lt;p&gt;Let's check this with the simplest and dumbest Python code possible:&lt;/p&gt;
    &lt;code&gt;from math import sqrt

s5 = sqrt(5)
c1 = (-1 + s5) / 2
c2 = (-1 - s5) / 2

def fib(i):
    return (-1)**i * (c2**i - c1**i)/s5

[int(fib(i)) for i in range(20)]
&lt;/code&gt;
    &lt;p&gt;Fingers crossed:&lt;/p&gt;
    &lt;p&gt;Phew...&lt;/p&gt;
    &lt;p&gt;If we substitute \(c_1\) and \(c_2\) in the formula, we get&lt;/p&gt;
    &lt;p&gt;We obtained the same solutions we found for \(b\), and we get the same asymptotic growth as well.&lt;/p&gt;
    &lt;p&gt;Indeed, the numerator grows as \((1+\sqrt5)^i\):&lt;/p&gt;
    &lt;p&gt;So, \(F_i \sim \frac{\phi^i}{\sqrt5} = \Theta(\phi^i)\), where \(\phi = \frac{1+\sqrt5}{2}\).&lt;/p&gt;
    &lt;p&gt;By the way, \(P(x) = \sum_{i=0}^\infty F_i x^i\) is called a generating function.&lt;/p&gt;
    &lt;head rend="h2"&gt;secp256k1&lt;/head&gt;
    &lt;p&gt;Ethereum's ECDSA uses the elliptic curve secp256k1, defined as&lt;/p&gt;
    &lt;p&gt;where \(p\) is a very big prime number.&lt;/p&gt;
    &lt;p&gt;Here's what the continuous version (i.e. without mod) of the curve looks like:&lt;/p&gt;
    &lt;p&gt;The continuous elliptic curve is the set of all points \((x, y)\) in the plane such that \(y^2 = x^3 + 7\). Because \(y\) is squared, the curve is symmetric about the X-axis, i.e. \((x, y)\) is on the curve if and only if \((x, -y)\) is.&lt;/p&gt;
    &lt;p&gt;When we switch to mod \(p\), things get complicated:&lt;/p&gt;
    &lt;p&gt;To draw that picture I used \(p = 97\), a small prime. The blue line is the continuous curve, while the dots are the solutions in \(\mathbb{Z}_p\times\mathbb{Z}_p\). Note that those solutions must always be finitely many, since they lie on a \(p\times p\) grid.&lt;/p&gt;
    &lt;p&gt;This figure only shows the upper right part (1st quadrant) of the previous one, so we can't see the symmetry of the continuous curve. Yet, the points in \(\mathbb{Z}_p\times\mathbb{Z}_p\) show a new symmetry: they're reflected across the horizontal line \(y = p/2\). That makes sense:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\((x, y)\) lies on the continuous curve if and only if \((x, -y)\) also lies on it.&lt;/item&gt;
      &lt;item&gt;If \(y\in\mathbb{Z}_p\), then \(-y = p-y\).&lt;/item&gt;
      &lt;item&gt;This means that \((x, y)\) lies on the mod curve if and only if \((x, p-y)\) also lies on it.&lt;/item&gt;
      &lt;item&gt;\(y = p-y\) gives us \(y = p/2\). In the figure the axis of symmetry is \(y = 97/2 = 48.5\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's plot the negative solutions as well:&lt;/p&gt;
    &lt;p&gt;As we can see, the part above is identical to the part below, since adding \(p\) or \(-p\) to a coordinate doesn't change anything mod \(p\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Group&lt;/head&gt;
    &lt;p&gt;A group \(G\) is a set of elements equipped with a binary operation, \(+\), such that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For any elements \(a, b\in G\), we must have \(a+b \in G\).&lt;/item&gt;
      &lt;item&gt;There's a neutral element, or identity, \(0\), so that \(a+0 = 0+a = a\) for every \(a\in G\).&lt;/item&gt;
      &lt;item&gt;For every \(a\in G\), there exists an additive inverse, or opposite, of \(a\), denoted as \(-a\), such that \(a+(-a) = (-a)+a = 0\).&lt;list rend="ul"&gt;&lt;item&gt;Note: \(a+(-b)\) can also be written as \(a-b\).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;We also want associativity, i.e., for all \(a,b,c\in G\), we must have \(a + (b+c) = (a+b) + c\). So, we can drop the parentheses and write \(a+b+c\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If for all \(a, b\in G\) we have \(a+b = b+a\), then \(G\) is abelian or commutative.&lt;/p&gt;
    &lt;p&gt;Notice that we can't have two distinct identities \(0_1 \neq 0_2\), since&lt;/p&gt;
    &lt;p&gt;Let's break it down:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(0_1+0_2\) can be simplified in two ways:&lt;list rend="ul"&gt;&lt;item&gt;Since \(0_1\) is an identity, then it disappears, so \(0_1+0_2 = 0_2\)&lt;/item&gt;&lt;item&gt;Since \(0_2\) is an identity, then it disappears, so \(0_1+0_2 = 0_1\)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Therefore, the two identities must be equal.&lt;/p&gt;
    &lt;p&gt;The same goes for the additive inverse. If \(x\) and \(y\) are opposites of \(a\) then:&lt;/p&gt;
    &lt;p&gt;That means that:&lt;/p&gt;
    &lt;p&gt;So there's only one inverse per element.&lt;/p&gt;
    &lt;p&gt;Given a subset \(S\) of elements from \(G\), we can define the subgroup \(G_S\) of \(G\) generated by \(S\) as the smallest group that includes all the elements of \(S\). We say that \(S\) is a generator of \(G_S\).&lt;/p&gt;
    &lt;p&gt;In this article we'll only describe in detail the case where \(S\) has just one element. For simplicity, we'll use the same symbol for the subgroup and its generator, so a generator \(G\) generates the (sub)group \(G\) defined as follows:&lt;/p&gt;
    &lt;p&gt;That's very cumbersome to read and write, so let's define&lt;/p&gt;
    &lt;p&gt;Now we can rewrite the definition as&lt;/p&gt;
    &lt;p&gt;or even&lt;/p&gt;
    &lt;p&gt;where we define \(0G = 0\).&lt;/p&gt;
    &lt;head rend="h2"&gt;A group for the elliptic curve&lt;/head&gt;
    &lt;p&gt;ECDSA defines a group over the points on the curve mod \(p\). To do that, we first need to define an addition between points.&lt;/p&gt;
    &lt;p&gt;Here's how it's done on the continuous version of the elliptic curve:&lt;/p&gt;
    &lt;p&gt;If \(P\) and \(Q\) are two points on the curve, then \(P+Q\) is the reflection across the X-axis of the intersection between the curve and the line passing through \(P\) and \(Q\).&lt;/p&gt;
    &lt;p&gt;The line through \(P\) and \(Q\) always intersects the curve at a third point, except when the line is perfectly vertical. In that case, the line is said to intersect the curve at \(0\), called the point at infinity. The point \(0\) acts as the identity, but is not really part of the curve, so it needs to be handled as a special case.&lt;/p&gt;
    &lt;p&gt;Now, observe that the line through \(R = (x, y)\) and \(-R = (x, -y)\) is vertical, so \(R+(-R)=0\), as suggested by the "\(-\)" sign.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Usually, the point at infinity is denoted by \(\mathcal{O}\) and the origin by \(0 = (0, 0)\). However, since we have no need for the origin in this context, we'll denote the point at infinity by \(0\), stressing the fact that it's the zero of the group.&lt;/p&gt;
    &lt;p&gt;When \(P = Q\), the line through \(P\) and \(Q\) is taken to be the tangent to the curve at \(P\) (or, equivalently, \(Q\)). It makes sense:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Just imagine fixing \(P\) and sliding \(Q\) along the curve from one side of \(P\) to the other.&lt;/item&gt;
      &lt;item&gt;If we want the animation of the line during the sliding to be continuous, the line must be the tangent when \(P = Q\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After all, the animation is continuous when the slope of the line is continuous, and the slope is continuous at a point when it's equal to its limit, i.e. the derivative, at that point.&lt;/p&gt;
    &lt;p&gt;Here's a figure with a fixed point \(P\) and secants through \(P\) and several \(Q_i\) points that converge to \(P\). I chose a color map such that the closer a \(Q_i\) is to \(P\), the bluer the secant through it becomes.&lt;/p&gt;
    &lt;p&gt;By the way, we still count the tangent line as intersecting the elliptic curve in three points, with two coinciding at the point of tangency.&lt;/p&gt;
    &lt;p&gt;But how is it possible that even an "almost vertical" line through two points on the curve always intersects the curve at a third point? Such a line intersects the curve either at a point towards \((+\infty, +\infty)\) or \((+\infty, -\infty)\).&lt;/p&gt;
    &lt;p&gt;For \(x\to+\infty\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;line:&lt;list rend="ul"&gt;&lt;item&gt;\(y = mx + q \sim mx\)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;curve:&lt;list rend="ul"&gt;&lt;item&gt;\(y^2 = x^3 + 7 \sim x^3 \implies\)&lt;list rend="ul"&gt;&lt;item&gt;\(y \sim x^{3/2}\) (upper branch)&lt;/item&gt;&lt;item&gt;\(y \sim -x^{3/2}\) (lower branch)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;\(y^2 = x^3 + 7 \sim x^3 \implies\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What we're saying is that when \(x\) becomes very big, additive terms such as \(q\) and \(7\) are dwarfed and can be ignored, so the line grows like \(mx\), while the curve grows like \(x^{3/2}\). The curve grows asymptotically faster, so, when \(m &amp;gt; 0\), the upper branch of the curve will hit the line from below and cross it sooner or later. Similarly, when \(m &amp;lt; 0\), the lower branch of the curve will hit the line from above and cross it.&lt;/p&gt;
    &lt;p&gt;Here's a visual example for \(m&amp;gt;0\):&lt;/p&gt;
    &lt;head rend="h3"&gt;Finding the intersection point&lt;/head&gt;
    &lt;p&gt;The algebra is easy enough. We just compute the intersection between the curve and the line. Let's start from the two points \(P = (x_P, y_P)\) and \(Q = (x_Q, y_Q)\).&lt;/p&gt;
    &lt;p&gt;If the line is vertical, the third intersection point is \(0\). Otherwise, its equation has the form \(y = mx + q\), where&lt;/p&gt;
    &lt;p&gt;We need to solve for \(x\) and \(y\) the system&lt;/p&gt;
    &lt;p&gt;We substitute the first equation into the second and get&lt;/p&gt;
    &lt;p&gt;Before giving in to despair, we remember that we already know two solutions, \(x_P\) and \(x_Q\), and we're looking for the third point \(-R = (x_{-R}, y_{-R})\). This means that the LHS of the equation in \(x\) must be factorizable as&lt;/p&gt;
    &lt;p&gt;Let's expand it to get&lt;/p&gt;
    &lt;p&gt;The second term is all we need: for the two LHS to be equal, we must have&lt;/p&gt;
    &lt;p&gt;The \(y\) coordinate is simply \(y_{-R} = m x_{-R} + q\).&lt;/p&gt;
    &lt;p&gt;Therefore, the sum of the two points can be computed as&lt;/p&gt;
    &lt;p&gt;As we can see, finding the sum of two points requires subtractions, multiplications, and one division to compute \(m\). To sum two points on the curve mod \(p\), we just need to do the calculations mod \(p\). We know how to "divide" mod \(p\), so we're good.&lt;/p&gt;
    &lt;p&gt;Let's briefly go through the case with \(P=Q=(x,y)\). Recall that we need to consider the tangent to the curve at \((x,y)\). Note that for \(y=0\) the tangent is perfectly vertical, so the result is \(0\) (look at the figure). For \(y\neq 0\), we need to compute the derivative.&lt;/p&gt;
    &lt;p&gt;We start from the equation&lt;/p&gt;
    &lt;p&gt;and derive both sides with respect to \(x\):&lt;/p&gt;
    &lt;p&gt;We solve for the derivative:&lt;/p&gt;
    &lt;p&gt;That's our \(m\).&lt;/p&gt;
    &lt;p&gt;A complete implementation will also handle the (trivial) edge cases with \(P=0\) or \(Q=0\), of course.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why reflect the intersection point&lt;/head&gt;
    &lt;p&gt;Having to reflect the intersection point might seem a little arbitrary at first, but if we think about it, not reflecting it is problematic. Indeed, since the three points lie on the same line, without reflection all the following equations would hold:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(P + Q = R\)&lt;/item&gt;
      &lt;item&gt;\(P + R = Q\)&lt;/item&gt;
      &lt;item&gt;\(Q + R = P\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By summing the first two equations, we'd get \(2P+Q+R=R+Q\), i.e. \(P=0\). Analogously, by summing the last two equations, we'd get \(R=0\). Since \(P=Q=R=0\), that rule would only work for \(0\).&lt;/p&gt;
    &lt;p&gt;The correct rule will look less asymmetric if we think of it as \(P+Q+R=0\), which gives&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(P+Q=-R\)&lt;/item&gt;
      &lt;item&gt;\(P+R=-Q\)&lt;/item&gt;
      &lt;item&gt;\(Q+R=-P\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But what about this point at infinity? Where does it come from? All I know is that it has to do with the so-called projective space.&lt;/p&gt;
    &lt;p&gt;I got somewhat acquainted with that space when I was doing 3D graphics. In 3D, we may add a 4th coordinate, so that \((x, y, z)\) is represented by \((wx, wy, wz, w)\) and some computations become more regular (i.e. with fewer exceptions). At the end, we divide by \(w\) to go back to the usual coordinates.&lt;/p&gt;
    &lt;p&gt;There's also the 2D case when we project a 3D scene onto a 2D screen: \(\pi(x, y, z) = (x/z, y/z)\), where I used \(\pi\) for projection. This has to do with how we perceive the world, so that the farther an object is from us, the smaller it looks (assuming, from our POV, that \(z\) is the distance of the object from us).&lt;/p&gt;
    &lt;head rend="h3"&gt;Projective space&lt;/head&gt;
    &lt;p&gt;Let's say we have some 3D space. We make it projective by imposing that, in general, \((x, y, z) \sim (\lambda x, \lambda y, \lambda z)\) for all \(\lambda\neq 0\), and \((x, y, z)\neq(0, 0, 0)\), where \(\sim\) means equivalent. In words, all non-zero scalings of a non-zero point are equivalent. Those are all the points, origin excluded, on the same line through the origin.&lt;/p&gt;
    &lt;p&gt;The classes partition the punctured (i.e. without the origin) 3D space. The origin, if included, would be in its own singleton class \(\{0\}\) anyway. If we instead allowed \(\lambda = 0\), then two points \(x\) and \(y\) on different lines through the origin would violate the equivalence relation: \(x\sim 0\) and \(y\sim 0\), but \(x\nsim y\).&lt;/p&gt;
    &lt;p&gt;Indeed, an equivalence relation must follow three rules:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reflexivity: \(x\sim x\)&lt;/item&gt;
      &lt;item&gt;symmetry: \(x\sim y\iff y\sim x\)&lt;/item&gt;
      &lt;item&gt;transitivity: \(x\sim y \wedge y\sim z\implies x\sim z\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;where "\(\wedge\)" means "and".&lt;/p&gt;
    &lt;p&gt;To remember the rules, we can just think of equality, which is also an equivalence relation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(a=a\)&lt;/item&gt;
      &lt;item&gt;\(a=b\iff b=a\)&lt;/item&gt;
      &lt;item&gt;\(a=b\wedge b=c\implies a=c\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, if \(x\sim 0\) and \(0\sim y\), then we must have \(x\sim y\). If \(0\) is equivalent to elements that belong to different classes, then it breaks transitivity.&lt;/p&gt;
    &lt;p&gt;Since the origin doesn't belong to the projective space, any generic point \((x, y, z)\) in it is to be considered non-zero.&lt;/p&gt;
    &lt;p&gt;Back to our elliptic equation. On the 2D plane, the equation is \(y^2 = x^3 + 7\), but that won't work in the projective space. Since \((x, y, z)\sim (\lambda x, \lambda y, \lambda z)\), with \(\lambda\neq 0\), we'd like for \((\lambda x, \lambda y, \lambda z)\) to be on the curve whenever \((x, y, z)\) is.&lt;/p&gt;
    &lt;p&gt;Let's write the equation in the projective space as&lt;/p&gt;
    &lt;p&gt;and do the substitution \((X, Y, Z) = (\lambda x, \lambda y, \lambda z)\):&lt;/p&gt;
    &lt;p&gt;We want that to hold whenever \((x, y, z)\) is a solution, i.e. whenever \(y^2 = x^3 + 7\). For that to happen, the equation must factorize as&lt;/p&gt;
    &lt;p&gt;so that when the second factor is \(0\), the equation holds regardless of the factor with \(\lambda\).&lt;/p&gt;
    &lt;p&gt;We still have a \(Z\) to add, so why not use it to balance the degree of the terms? That is:&lt;/p&gt;
    &lt;p&gt;Now the substitution gives&lt;/p&gt;
    &lt;p&gt;We did it, but what about that annoying extra \(z\)? If we want to recover the original equation, we need to set \(z=1\), i.e. we need to restrict ourselves to \((x, y, 1)\).&lt;/p&gt;
    &lt;p&gt;That's perfectly fine, though: the original curve lies on the \(z=1\) plane, while on each \(z=\lambda\) plane, with \(\lambda\neq 0\), lies a \(\lambda\)-scaled version of the original curve:&lt;/p&gt;
    &lt;p&gt;With this setup, we can say that either all the elements of an equivalence class (a punctured line through the origin) are on the curve or none of them are.&lt;/p&gt;
    &lt;p&gt;There's actually an easier way to get the equation in \(x\), \(y\), and \(z\) coordinates. The original 2D curve is embedded in the 3D space by adding a \(z = 1\) coordinate, i.e.&lt;/p&gt;
    &lt;p&gt;Starting from a generic point \((X, Y, Z)\) with \(Z\neq 0\), we can go back to the 2D case by just dividing by \(Z\) and dropping the third coordinate, i.e.&lt;/p&gt;
    &lt;p&gt;Now, let's substitute \(x = X/Z\) and \(y = Y/Z\) into the starting equation and get rid of the denominators:&lt;/p&gt;
    &lt;p&gt;One can also apply other projections. For instance, \(x = X/Z^2\) and \(y = Y/Z^3\) lead to&lt;/p&gt;
    &lt;p&gt;This is actually nicer and used to greatly speed up computations. It's called Jacobian projection.&lt;/p&gt;
    &lt;p&gt;We still haven't solved the mystery of the point at infinity. That was the main reason why we decided to explore projective spaces.&lt;/p&gt;
    &lt;head rend="h4"&gt;Point at infinity&lt;/head&gt;
    &lt;p&gt;We know that a vertical line intersects the planar elliptic curve either at no points at all or at the points \(P\), \(-P\), and \(0\) for some point \(P\), where \(0\) is the so-called point at infinity. Let's try to make sense of it.&lt;/p&gt;
    &lt;p&gt;On the plane, a vertical line has equation \(x = k\), but in our projective space that's the equation of a plane. Like with the elliptic curve, we want to upgrade the equation so that if \((x, y, z)\) is on the line, then so is \((\lambda x, \lambda y, \lambda z)\). We use the same substitution as before:&lt;/p&gt;
    &lt;p&gt;So the equation of the vertical plane becomes \(X = kZ\), which represents a family of \(Z\)-scaled vertical lines. The equation \(X = kZ\) makes sense since, as \(Z\) gets closer to \(0\), the line must also get closer to the origin because everything, curve included, gets scaled down.&lt;/p&gt;
    &lt;p&gt;Let's find the intersections between the curve and the line by solving&lt;/p&gt;
    &lt;p&gt;We substitute the second equation into the first:&lt;/p&gt;
    &lt;p&gt;That can be rewritten as&lt;/p&gt;
    &lt;p&gt;For \(Z\neq 0\), we can divide by \(Z\), so we're left with&lt;/p&gt;
    &lt;p&gt;which gives two solutions for each \(Z\neq 0\) because of that \(Y^2\). Those two solutions correspond to two points \(P\) and \(-P\).&lt;/p&gt;
    &lt;p&gt;For \(Z = 0\), we get \(X = 0\) from the second equation, i.e. the solutions are \((0, \lambda, 0)\) for \(\lambda\neq 0\), which is the Y-axis without the origin. We can take \((0, 1, 0)\) as the representative of that class, which is exactly the point at infinity. As we can see, it doesn't live in any plane with the curve, so it doesn't exist in our original 2D space, but we already knew that.&lt;/p&gt;
    &lt;p&gt;This reminded me that we never designated representatives for the equivalence classes. Let's see:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each class \(C\) is a punctured line through the origin.&lt;/item&gt;
      &lt;item&gt;If \(C\) intersects the plane \(Z = 1\) at some point \(P = (X, Y, 1)\):&lt;list rend="ul"&gt;&lt;item&gt;\(\mathrm{repr}(C) = P\)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Else:&lt;list rend="ul"&gt;&lt;item&gt;\(C\) must lie on the plane \(Z = 0.\)&lt;/item&gt;&lt;item&gt;If \(C\) intersects the line \(\{Y=1; Z=0\}\) at some point \(P = (X, 1, 0)\):&lt;list rend="ul"&gt;&lt;item&gt;\(\mathrm{repr}(C) = P\)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Else:&lt;list rend="ul"&gt;&lt;item&gt;\(C\) lies on the X-axis, i.e. \(\{Y = Z = 0\}\).&lt;/item&gt;&lt;item&gt;\(C\) has the form \((X, 0, 0)\).&lt;/item&gt;&lt;item&gt;\(\mathrm{repr}(C) = (1, 0, 0)\)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, we have three groups of representatives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\((X, Y, 1)\): on the curve if and only if \(Y^2 = X^3 + 7\).&lt;/item&gt;
      &lt;item&gt;\((X, 1, 0)\): on the curve if and only if \(X = 0\), which gives the point at infinity \((0, 1, 0)\).&lt;/item&gt;
      &lt;item&gt;\((1, 0, 0)\): not on the curve.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Addition in projective space&lt;/head&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;You can safely skip this section!&lt;/p&gt;
    &lt;p&gt;Computing \(m\), the slope of the line through the points \((x_1, y_1)\) and \((x_2, y_2)\), requires a division, which, mod \(p\), is a relatively expensive operation (compared to simple additions and multiplications).&lt;/p&gt;
    &lt;p&gt;Let's recall how to compute the point \((x_3, y_3) = (x_1, y_1) + (x_2, y_2)\), assuming \(x_1\neq x_2\):&lt;/p&gt;
    &lt;p&gt;To go from the plane to the 3D projective space, we proceed as we did before with the elliptic curve and the line equations, i.e. we make the substitution \((x,y) = (X/Z, Y/Z)\).&lt;/p&gt;
    &lt;p&gt;Let's start with \(m\):&lt;/p&gt;
    &lt;p&gt;We define&lt;/p&gt;
    &lt;p&gt;Therefore:&lt;/p&gt;
    &lt;p&gt;Now we deal with \(x_3\):&lt;/p&gt;
    &lt;p&gt;It's \(y_3\)'s turn:&lt;/p&gt;
    &lt;p&gt;We substituted \(X_3\) into \(Y_3\), so we could factor out \(Z_3\), since \(X_3 = Z_3 (\ldots)\).&lt;/p&gt;
    &lt;p&gt;We end up with&lt;/p&gt;
    &lt;p&gt;and by choosing \(Z_3 = D^3 Z_1 Z_2\), we get rid of the denominators:&lt;/p&gt;
    &lt;p&gt;We can clean that up further by defining&lt;/p&gt;
    &lt;p&gt;which results in&lt;/p&gt;
    &lt;p&gt;Note that further micro-optimizations are possible. For instance, we shouldn't compute \(D^2\) and \(D^3\) separately.&lt;/p&gt;
    &lt;p&gt;I hope my calculations are correct, since this is my first time doing them. Either way, I'm satisfied with the result from a pedagogical point of view. I hope you are as well. As we can see, the common denominator was put in \(Z_3\) to avoid intermediate divisions. Now we can add many points together without any division and only do one single division when we want to go back to our 2D plane:&lt;/p&gt;
    &lt;p&gt;That's one slow (at least in \(\mathbb{Z}_p\)) division and 2 fast multiplications.&lt;/p&gt;
    &lt;p&gt;Note that the \(P=Q\) case is handled similarly. Moreover, the same approach will also work for the Jacobian projection, i.e. for \((x, y) = (X/Z^2, Y/Z^3)\).&lt;/p&gt;
    &lt;p&gt;This is almost a philosophical observation. When we substitute \(x\) with \(X/Z\), we're not promoting \(x\) to a fraction, but we're reexpressing it as a fraction, since they're assumed to be equal.&lt;/p&gt;
    &lt;p&gt;For example, if \(x\) is a simple integer in \(\mathbb{Z}\) (without mod) and we replace it with \(X/Z\) where \(X\) and \(Z\) are also in \(\mathbb{Z}\), then we're ranking up from integers to rational numbers, which is a promotion. Indeed, unless \(X\) is divisible by \(Z\) or we're willing to lose some information, we won't be able to go back to \(x\) when the time comes.&lt;/p&gt;
    &lt;p&gt;In the ECDSA case, though, \(x\) is in \(\mathbb{Z}_p\), with \(p\) prime, so \(X/Z\) is also in \(\mathbb{Z}_p\): we're not promoting \(x\) to something more, but just reexpressing it.&lt;/p&gt;
    &lt;p&gt;Let's say we have a huge matrix that can be factorized into two small matrices because it's low-rank (i.e. many of its rows or columns are linear combinations of a selected few). Instead of carrying around the huge matrix during the computations, we may want to keep it in factorized form, \((L, R)\), and then update the factorized form itself:&lt;/p&gt;
    &lt;p&gt;One way to find \(f\) and \(g\) is to do the substitution \(M = LR\) into whatever expression we want to evaluate involving \(M\), and put the result back into factorized form. Note that if we end up with \(L=I\) or \(R=I\) (where \(I\) is the identity matrix), then the factorization is useless for our purposes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Back to the group&lt;/head&gt;
    &lt;p&gt;ECDSA uses the addition operation we defined above to generate a group from a predetermined generator \(G\). All the considered points are on the curve mod \(p\), meaning that their coordinates are in \(\mathbb{Z}_p\). Here's the group:&lt;/p&gt;
    &lt;p&gt;Notice that \(G\) has only a finite number of elements, which is to be expected since the points lie on a \(p\times p\) grid, which contains \(p^2\) distinct points at most.&lt;/p&gt;
    &lt;p&gt;We start from \(0\) and keep adding \(G\) until we loop, i.e. we get a point that we've already seen. Let's assume this is our current list:&lt;/p&gt;
    &lt;p&gt;We assume we've just looped, so the first \(h\) elements are all distinct, and \(hG = kG\), with \(k &amp;lt; h\).&lt;/p&gt;
    &lt;p&gt;We must have \(0 = hG-kG = (h-k)G\). The only elements in the list that can be 0 are the first one and the last one. Since \(h-k&amp;gt;0\), \((h-k)G\) must be the last element, so \(h-k=h\), which gives \(k=0\). This means that when we loop we restart from \(0\).&lt;/p&gt;
    &lt;p&gt;So we end up with the following group:&lt;/p&gt;
    &lt;p&gt;\(G\) has order \(N\), i.e. it has \(N\) elements. This looping should remind you of \(\mathbb{Z}_N\):&lt;/p&gt;
    &lt;p&gt;Indeed, \(\mathbb{Z}_N\) is also a (commutative) group:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;group operation: \(+\)&lt;/item&gt;
      &lt;item&gt;identity: \(0\)&lt;/item&gt;
      &lt;item&gt;inverse of \(x\): \(-x\)&lt;list rend="ul"&gt;&lt;item&gt;so that \(x + (-x) = (-x) + x = 0\)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Moreover, it's generated by \(1\):&lt;/p&gt;
    &lt;p&gt;If we couldn't inspect the single elements, and we just used the group laws, we'd actually be unable to tell \(G\) and \(\mathbb{Z}_N\) apart.&lt;/p&gt;
    &lt;p&gt;For instance, let's say we're given, as programmers, an opaque type &lt;code&gt;Element&lt;/code&gt; together with an identity &lt;code&gt;zero&lt;/code&gt; and an operation &lt;code&gt;add&lt;/code&gt;. Would we be able to tell whether we're working with \(G\) or \(\mathbb{Z}_N\) without looking inside &lt;code&gt;Element&lt;/code&gt; or at the implementation? No, we wouldn't. By defining a common API, we abstracted away the differences.&lt;/p&gt;
    &lt;p&gt;So, we've got ourselves an isomorphism between groups:&lt;/p&gt;
    &lt;p&gt;More formally, let's define \(f: a\mapsto aG\), which is a bijection, i.e. a 1-1 mapping between all the elements of \(\mathbb{Z}_N\) and all the elements of \(G\). This means that we can invert \(f\) and use \(f^{-1}\) to go the other direction (however computationally expensive it is to do).&lt;/p&gt;
    &lt;p&gt;Then the equation above can be rewritten as&lt;/p&gt;
    &lt;p&gt;That means that we can either&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;transform the addends into \(G\)'s elements and then add them up using \(G\)'s addition operation, or&lt;/item&gt;
      &lt;item&gt;sum the addends using \(\mathbb{Z}_N\)'s addition operation and then transform the result into the corresponding element in \(G\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The final result will be the same.&lt;/p&gt;
    &lt;p&gt;Another way of saying this is that we can move back and forth between \(G\) and \(\mathbb{Z}_N\) without losing information.&lt;/p&gt;
    &lt;p&gt;For example, for \(N = 7\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(3G + 5G = 8G = 7G + G = 0 + G = G\)&lt;list rend="ul"&gt;&lt;item&gt;because \(7G = 0\)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;\((3 + 5)G = (8)G = (1)G = G\)&lt;list rend="ul"&gt;&lt;item&gt;because \(8\ \mathrm{mod}\ 7 = 1\)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the first case the looping comes from \(G\), while in the second case it comes from \(\mathbb{Z}_7\). In the second case we only worked inside the parentheses, i.e. with the numbers in \(\mathbb{Z}_7\): we didn't touch \(G\) at all.&lt;/p&gt;
    &lt;p&gt;The idea is to work with numbers in \(\mathbb{Z}_N\), which are easier to work with, and then transform them into points by multiplying them by \(G\). While it's very easy to go from \(k\) to \(kG\), it's computationally infeasible to go back from \(kG\) to \(k\).&lt;/p&gt;
    &lt;p&gt;It shouldn't surprise that ECDSA represents private keys as numbers in \(\mathbb{Z}_N\), and then multiplies them by \(G\) to get the associated public keys. This makes recovering the private key from a public key computationally infeasible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Signing a message&lt;/head&gt;
    &lt;p&gt;I'll cheat a little and read my notes for the signing part of the algorithm:&lt;/p&gt;
    &lt;code&gt;* Message to sign:
    z = 256-bit message digest

* Signature generation (in F_n, i.e. mod n):
    k = rand_unif({1, ..., n-1})        # ephemeral nonce
    R = k * G = (R_x, R_y)
    r = R_x mod n                       # 1st component
    if r = 0, restart and choose a new k
    s = (k^{-1} * (z + r * d)) mod n    # 2nd component [d = account private key]
    if s = 0, restart and choose a new k
    v = R_y mod 2               # AKA recid, recovery_id, or is_y_odd
    signature = (r, s, v)
&lt;/code&gt;
    &lt;p&gt;Let's see:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(G\) is both:&lt;list rend="ul"&gt;&lt;item&gt;the group of points on the elliptic curve (with coordinates in \(\mathbb{Z}_p\), with \(p\) prime)&lt;/item&gt;&lt;item&gt;the generator of that group&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;\(n\), a prime number, is the order of \(G\)&lt;/item&gt;
      &lt;item&gt;\(z\) is the message hash&lt;/item&gt;
      &lt;item&gt;\(d\) is the private key of the account&lt;/item&gt;
      &lt;item&gt;\(k^{-1}\) is the multiplicative inverse mod \(n\), i.e. \(k\cdot k^{-1} = 1\ (\mathrm{mod}\ n)\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that I wrote \(F_n\) or, better, \(\mathbb{F}_n\) instead of \(\mathbb{Z}_n\) because, when \(n\) is prime, the latter is actually a field. The coordinates we've been working with for all this time are in \(\mathbb{Z}_p\), which is also a field, since \(p\) is prime. That's why I wrote "some 3D space" before: depending on which field we choose, we'll end up with a different 3D space.&lt;/p&gt;
    &lt;p&gt;We basically already observed that \(\mathbb{Z}_n\), with \(n\) prime, is a field, but we never spelled it out.&lt;/p&gt;
    &lt;p&gt;That's actually why our math works both in the continuous case and mod \(p\). The theory only requires that the coordinates are elements of a field. It doesn't matter which one.&lt;/p&gt;
    &lt;p&gt;A field \(\mathbb{F}\) is a set equipped with two binary operations, addition and multiplication, such that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(\mathbb{F}\) is a commutative group under addition&lt;/item&gt;
      &lt;item&gt;\(\mathbb{F}\setminus\{0\}\) is a commutative group under multiplication&lt;/item&gt;
      &lt;item&gt;A distributive law holds:&lt;list rend="ul"&gt;&lt;item&gt;(a+b)c = ac + bc&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that \(\mathbb{F}\setminus\{0\}\) means "\(\mathbb{F}\) minus \(\{0\}\)", i.e. \(\mathbb{F}\) without the element \(0\).&lt;/p&gt;
    &lt;p&gt;\(\mathbb{Z}_n\setminus\{0\}\), with \(n\) prime, is a group under multiplication because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiplication is associative: \(a(bc) = (ab)c = abc\).&lt;/item&gt;
      &lt;item&gt;There's an identity: \(1\)&lt;/item&gt;
      &lt;item&gt;Every (non-zero) element \(x\) has inverse, i.e. the famous multiplicative inverse mod \(n\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The \(0\) element has no multiplicative inverse since there's no \(x\) such that \(0\cdot x = 1\). That would be against the very definition of \(0\) as the identity for the addition operation.&lt;/p&gt;
    &lt;p&gt;When \(n\) is not prime, we lose the group under multiplication because the inverse doesn't exist for all elements. For instance, let's consider mod \(6\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(3\cdot 0 = 0\)&lt;/item&gt;
      &lt;item&gt;\(3\cdot 1 = 3\)&lt;/item&gt;
      &lt;item&gt;\(3\cdot 2 = 0\)&lt;/item&gt;
      &lt;item&gt;\(3\cdot 3 = 3\)&lt;/item&gt;
      &lt;item&gt;\(3\cdot 4 = 0\)&lt;/item&gt;
      &lt;item&gt;\(3\cdot 5 = 3\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since \(3\) and \(6\) are not coprime, \(3\) has no inverse.&lt;/p&gt;
    &lt;p&gt;When \(n\) is not a prime number, \(\mathbb{Z}_n\) is just a (commutative) ring, which has a weaker structure.&lt;/p&gt;
    &lt;p&gt;Well-known examples of fields are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(\mathbb{Q}\): the rational numbers&lt;/item&gt;
      &lt;item&gt;\(\mathbb{R}\): the real numbers&lt;/item&gt;
      &lt;item&gt;\(\mathbb{C}\): the complex numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The integers are clearly not a field since, for instance, \(3x = 1\) has no integer solution, so \(3\) has no multiplicative inverse. So, by adding a mod \(p\), with \(p\) prime, we gain more structure, and we get ourselves a field!&lt;/p&gt;
    &lt;p&gt;Back to the algorithm. The first two lines are pretty easy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We generate a random (uniformly distributed) temporary nonce \(k\) (a number to be used only once ever)&lt;/item&gt;
      &lt;item&gt;We convert \(k\) into the associated point \(R\) on the curve&lt;list rend="ul"&gt;&lt;item&gt;The point has coordinates \((R_x, R_y)\) (mod \(p\))&lt;/item&gt;&lt;item&gt;It's computationally infeasible to go back from \(R\) to \(k\).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Note that \(n &amp;lt; p\), otherwise \(r\) would just be \(R_x\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since \(d\) is the private key, then \(dG\) is the public key. We'll call it \(Q\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Verifying the signature&lt;/head&gt;
    &lt;p&gt;Given \(r\), \(s\), \(Q\), and the message, we can verify the signature by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hashing the message to get \(z\)&lt;/item&gt;
      &lt;item&gt;recovering \(R\)&lt;/item&gt;
      &lt;item&gt;checking that \(R_x\ \mathrm{mod \ n} = r\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We know that \(R = kG\) and that \(s\) contains \(k\), but in inverse form. If we invert \(s\) and multiply it by \(G\), we get&lt;/p&gt;
    &lt;p&gt;Mhm... if we had \(d\), we could compute \((z+rd)\) and use it to cancel \((z+rd)^{-1}\) and get \(R\).&lt;/p&gt;
    &lt;p&gt;While we don't have \(d\), we do have \(Q = dG\), which means that although we can't compute \((z + rd)\), we can compute \((z + rd)G\):&lt;/p&gt;
    &lt;p&gt;If we multiply that by \(s^{-1}\) we get&lt;/p&gt;
    &lt;p&gt;In words, we factor out \(G\) from \(zG + rQ\) and form \(s^{-1}G\), which is just \((z+rd)^{-1}R\), as we saw at the beginning.&lt;/p&gt;
    &lt;p&gt;We did it! Now we check that \(r = R_x\ \mathrm{mod}\ n\).&lt;/p&gt;
    &lt;p&gt;Here's the algorithm:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(w = s^{-1}\ \mathrm{mod}\ n\)&lt;/item&gt;
      &lt;item&gt;\(u_1 = (z\cdot w)\ \mathrm{mod}\ n\)&lt;/item&gt;
      &lt;item&gt;\(u_2 = (r\cdot w)\ \mathrm{mod}\ n\)&lt;/item&gt;
      &lt;item&gt;\(R = u_1\cdot G + u_2\cdot Q\)&lt;/item&gt;
      &lt;item&gt;check that \(r = R_x\ \mathrm{mod}\ n\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Observe that \(((zw)G + (rw)Q)\) is more efficient than \(s^{-1}(zG + rQ)\) because, in the former, \(w\) multiplies two numbers, while, in the latter, \(s^{-1}\) multiplies a point.&lt;/p&gt;
    &lt;head rend="h3"&gt;Recovering \(Q\)&lt;/head&gt;
    &lt;p&gt;Now the only problem is that in Ethereum the public key \(Q\) doesn't come with the signature. However, we can recover it from \(r\), \(s\), and \(v\).&lt;/p&gt;
    &lt;p&gt;As we know, \(Q = dG\) and \(s = k^{-1}(z+rd)\), so we should try multiplying \(s\) by \(G\):&lt;/p&gt;
    &lt;p&gt;To solve for \(Q\), we need to get rid of that \(k^{-1}\). We can't just multiply \(sG\) by \(k\) because we don't know \(k\)... but wait:&lt;/p&gt;
    &lt;p&gt;Therefore:&lt;/p&gt;
    &lt;p&gt;Unfortunately, we don't know \(R\). But can we recover it? We know \(r = R_x\), so we only need to recover \(R_y\), since \(R = (R_x, R_y)\). We're forgetting something, though: \(r = R_x\ \mathrm{mod}\ n\). Recall that the coordinates of the points on the curve are in \(\mathbb{Z}_p\), not \(\mathbb{Z}_n\). We need to recover the original \(R_x\) from \(r\).&lt;/p&gt;
    &lt;p&gt;We know that \(R_x \in \{0, \ldots, p-1\}\), and, apparently, \(n\) is just a little smaller than \(p\), which means that \(R_x = r + jn\) for some very small \(j\). We start from \(j = 0\) and keep trying as long as \(r + jn &amp;lt; p\). We say that \(r + jn\) is a candidate for \(R_x\). Let's call the current candidate simply \(x\).&lt;/p&gt;
    &lt;p&gt;Given \(x\), we can recover \(y\) by using the equation \(y^2 = x^3 + 7\ (\mathrm{mod}\ p)\) itself and solve for \(y\). There are fast algorithms to do that. If there's no solution, we try the next candidate. Otherwise, we get two possible solutions: \(y\) and \(-y\). If you recall, \(v = R_y\ \mathrm{mod}\ 2\), which tells us the solution to pick:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;if \(y\ \mathrm{mod}\ 2 = v\), we choose \(y\)&lt;/item&gt;
      &lt;item&gt;otherwise, we choose \(-y\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One might wonder why we preserve the least significant bit of \(R_y\) to select the correct \(y\). That's because if \(y\) is in \(\{0, \ldots, p-1\}\), then \(-y\ \mathrm{mod}\ p = p-y\). It's clear that \(y + (p-y) = p\), which is odd (being a big prime), which implies that only one of \(y\) and \(-y\) can be odd (or even) mod \(p\).&lt;/p&gt;
    &lt;p&gt;Anyway, once we have \(R = (x, y)\), we compute \(Q = r^{-1}(sR - zG)\).&lt;/p&gt;
    &lt;p&gt;Now we must check that \(Q\) is valid, i.e. that \(Q\) is on the curve. If it's not, then we try the next candidate.&lt;/p&gt;
    &lt;p&gt;We should actually check that \(Q\) is in \(G\), but, apparently, \(G\) contains all the solutions of \(y^2 = x^3 + 7\ \mathrm{mod}\ p\), so if \(Q\) is on the curve then it's also in \(G\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Signature malleability attack&lt;/head&gt;
    &lt;p&gt;We left this for last, but after all we've been through, this is disappointingly straightforward.&lt;/p&gt;
    &lt;p&gt;If \((r, s, v)\) is a signature created by signing a message \(M\) with a private key \(d\), then so is \((r, n-s, 1-v)\).&lt;/p&gt;
    &lt;p&gt;That's it. The problem arises when someone blacklists \((r, s, v)\) (once it's been used) believing that this will prevent double spending. An attacker will use the signature \((r, n-s, 1-v)\) to send the same message for a second time, bypassing the blacklist.&lt;/p&gt;
    &lt;p&gt;Instead, programs should use nonces contained directly in the messages and blacklist the nonces or the messages themselves.&lt;/p&gt;
    &lt;p&gt;Let's see why both signatures are valid.&lt;/p&gt;
    &lt;p&gt;Let's recall the signing algorithm:&lt;/p&gt;
    &lt;code&gt;* Message to sign:
    z = 256-bit message digest

* Signature generation (in F_n, i.e. mod n):
    k = rand_unif({1, ..., n-1})        # ephemeral nonce
    R = k * G = (R_x, R_y)
    r = R_x mod n                       # 1st component
    if r = 0, restart and choose a new k
    s = (k^{-1} * (z + r * d)) mod n    # 2nd component [d = account private key]
    if s = 0, restart and choose a new k
    v = R_y mod 2               # AKA recid, recovery_id, or is_y_odd
    signature = (r, s, v)
&lt;/code&gt;
    &lt;p&gt;Now let's see what happens if we use \(-k\) instead of \(k\):&lt;/p&gt;
    &lt;p&gt;That is, given the signature computed with \(k\), we can trivially get the one computed with \(-k\).&lt;/p&gt;
    &lt;p&gt;Basically, by using \(-k\) instead of \(k\), we reflect \(R\) across the X-axis and flip \(v\) to signal that we switched the \(y\) coordinate.&lt;/p&gt;
    &lt;head rend="h2"&gt;The end&lt;/head&gt;
    &lt;p&gt;I hope you enjoyed the ride and deepened your understanding of ECDSA as much as I did.&lt;/p&gt;
    &lt;p&gt;If you spot any errors, you're welcome to open an issue or leave a comment below, but keep in mind that the article's nature will remain as stated in the disclaimer.&lt;/p&gt;
    &lt;p&gt;I won't be revisiting this years later unless something truly significant comes up.&lt;/p&gt;
    &lt;p&gt;Until next time!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avidthinker.github.io/2025/11/28/understanding-ecdsa/"/><published>2025-12-03T04:13:41+00:00</published></entry></feed>