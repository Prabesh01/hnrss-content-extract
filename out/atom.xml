<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-30T20:12:08.705057+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46431028</id><title>Go away Python</title><updated>2025-12-30T20:12:17.210900+00:00</updated><content>&lt;doc fingerprint="5f7ec79370d86d1d"&gt;
  &lt;main&gt;
    &lt;p&gt;lorentz app blog experiments&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lorentz.app/blog-item.html?id=go-shebang"/><published>2025-12-30T08:50:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46431560</id><title>Netflix Open Content</title><updated>2025-12-30T20:12:16.753210+00:00</updated><content>&lt;doc fingerprint="235793111fd515b3"&gt;
  &lt;main&gt;
    &lt;p&gt;At Netflix, we are always exploring ways to make our content look and sound even better. To provide a common reference for prototyping bleeding-edge technologies within entertainment, technology and academic circles without compromising the security of our original and licensed programming, we've developed test titles oriented around documentary, live action, and animation.&lt;/p&gt;
    &lt;p&gt;Many open source assets are available from each project listed below. Our hope is this will encourage more experimentation, learning, and discovery that will benefit the whole industry. Many of these titles are also streaming on Netflix and are best enjoyed with any HDR configured device with your Premium subscription.&lt;/p&gt;
    &lt;p&gt;You can download single files directly through your web browser, but for large files and long frame sequences, you may wish to use command line tools. Guidance is included below. Ad Blockers may cause errors in your downloading process, so try turning it off if you have issues.&lt;/p&gt;
    &lt;p&gt;Our open source content is available under the Creative Commons Attribution 4.0 International Public License.&lt;/p&gt;
    &lt;p&gt;We’ve had great success remastering titles like Knights of Sidonia, Flavours of Youth and Godzilla from SDR to HDR over the last few years. But what if we increase the resolution and create anime with HDR in mind from conception? Working with Japan's Production I.G, we set out to create the first 4K HDR Atmos anime short and discover what would need to change in anime workflows.&lt;/p&gt;
    &lt;p&gt;Read more about Sol Levante in the Netflix Tech Blog.&lt;/p&gt;
    &lt;p&gt;HDR10 2020 ST2084 UHD 24fps 1000nit&lt;/p&gt;
    &lt;p&gt;DolbyVision PQ/P3 D65 UHD 24fps IMF&lt;/p&gt;
    &lt;p&gt;Atmos ADM and DAMF files&lt;/p&gt;
    &lt;p&gt;ProTools Final Mix Session and Mastered Session&lt;/p&gt;
    &lt;p&gt;4K HDR 16bit P3/PQ D65 Dolby Vision 2.9 XML + VDM&lt;/p&gt;
    &lt;p&gt;Animatics, storyboard, selected After Effects projects, PSDs, and TGA in-betweens.&lt;/p&gt;
    &lt;p&gt;A live action test piece, Nocturne was shot at 120fps and sought to investigate footage with spatially complex scenes that mimic other professionally generated content to challenge codecs on both the encoding and decoding sides. This piece was also mastered in Dolby Vision and Atmos.&lt;/p&gt;
    &lt;p&gt;120fps Video Display Master&lt;/p&gt;
    &lt;p&gt;60fps Video Display Master&lt;/p&gt;
    &lt;p&gt;ADM WAV File&lt;/p&gt;
    &lt;p&gt;Upon noticing the contrast in dark shadows against the bright sky and sparks from a welder working on a new Netflix building, the encoding team acquired a Sony PMW-F55 and the AXS-R5 RAW recorder to shoot 16-bit RAW SQ and produce Sparks.&lt;/p&gt;
    &lt;p&gt;Sparks was shot 4K HFR and finished at 4000 nits using ACES. Read more about Sparks on the Netflix Tech Blog.&lt;/p&gt;
    &lt;p&gt;4K P3 PQ 4000nits Dolby Vision IMF&lt;/p&gt;
    &lt;p&gt;HDR10 1000nit PQ 2020 image sequence&lt;/p&gt;
    &lt;p&gt;4K P3/PQ 4000nits EXR&lt;/p&gt;
    &lt;p&gt;4K P3/PQ 4000nits EXR Dolby Vision metadata&lt;/p&gt;
    &lt;p&gt;ACES 59.94fps image sequence&lt;/p&gt;
    &lt;p&gt;Original Camera Files&lt;/p&gt;
    &lt;p&gt;Following the industry shift from “more pixels” to “better pixels,” we produced Meridian, our first test title to tell a story. Meridian was mastered in Dolby Vision high dynamic range (HDR) with a P3-D65 color space and PQ (perceptual quantizer) transfer function. It also contained a Dolby Atmos mix, multiple language tracks, and subtitles. You can read more about Meridian on Variety.&lt;/p&gt;
    &lt;p&gt;UHD IMF&lt;/p&gt;
    &lt;p&gt;Zipped UHD VDM&lt;/p&gt;
    &lt;p&gt;UHD 4k 5994 HDR P3/PQ (mp4)&lt;/p&gt;
    &lt;p&gt;UHD VDM Image Sequence&lt;/p&gt;
    &lt;p&gt;Dolby Atmos Metadata File&lt;/p&gt;
    &lt;p&gt;Atmos BWAV ADM&lt;/p&gt;
    &lt;p&gt;TIFF Sequence&lt;/p&gt;
    &lt;p&gt;We felt a need to include animated content in our test title library, so we partnered with Blender Foundation and Fotokem’s Keep Me Posted to re-grade Cosmos Laundromat, an award-winning short film in Dolby Vision HDR.&lt;/p&gt;
    &lt;p&gt;Cosmos Laundromat is an Open Movie project created by Blender Studio and directed by Mathieu Auvray. The film was made entirely in Blender, an open source 3D content creation tool.&lt;/p&gt;
    &lt;p&gt;2k 24p HDR P3/PQ (mp4)&lt;/p&gt;
    &lt;p&gt;EXR to TIFF Nuke Script File&lt;/p&gt;
    &lt;p&gt;EXR Sequence&lt;/p&gt;
    &lt;p&gt;Chimera is technically comparable to El Fuente, but its scenes are more representative of existing titles on Netflix. The dinner scene attempts to recreate a codec-challenging sequence from House of Cards.&lt;/p&gt;
    &lt;p&gt;Prior to Chimera, there wasn’t any open source 4K test material that exhibited real world live action material.&lt;/p&gt;
    &lt;p&gt;DCI 4k 2398p HDR P3/PQ&lt;/p&gt;
    &lt;p&gt;DCI 4k 5994p HDR P3/PQ&lt;/p&gt;
    &lt;p&gt;TIFF Sequence: DCI 4k 2398p&lt;/p&gt;
    &lt;p&gt;TIFF Sequence: DCI 4k 5994p&lt;/p&gt;
    &lt;p&gt;As the demand for more pixels increased, so did appropriate test content. Documentary short El Fuente was shot in Mexico by a local DP at 4K at both 48 and 59.94 fps to meet increasing resolution and frame rate requirements.&lt;/p&gt;
    &lt;p&gt;Boat: 4096x2160 60fps 10bit 420 (YUV4Mpeg format)&lt;/p&gt;
    &lt;p&gt;FoodMarket: 4096x2160 60fps 10bit 420 (YUV4Mpeg format)&lt;/p&gt;
    &lt;p&gt;The assets on this page can be browsed on the Netflix OpenContent bucket here:&lt;/p&gt;
    &lt;p&gt;http://download.opencontent.netflix.com/&lt;/p&gt;
    &lt;p&gt;You can download single files directly through your web browser on each page, but for large files and long frame sequences, you may wish to use command line tools such as aws cli. Instructions are posted on their website. After installation, you should be able to download the public assets. Try running these sample commands. If the download is interrupted, you can run the same command immediately and aws cli will resume the download where it left off.&lt;/p&gt;
    &lt;p&gt;Download a single file (0.7 kB):&lt;/p&gt;
    &lt;p&gt;Usage: aws s3 cp --no-sign-request &amp;lt;s3 URI&amp;gt; &amp;lt;local destination&amp;gt;&lt;/p&gt;
    &lt;p&gt;Example: aws s3 cp --no-sign-request s3://download.opencontent.netflix.com/TechblogAssets/Sparks/sparks_license.txt .&lt;/p&gt;
    &lt;p&gt;Sync entire directory (1406.1 MB):&lt;/p&gt;
    &lt;p&gt;Usage: aws s3 sync --no-sign-request &amp;lt;s3 URI&amp;gt; &amp;lt;local destination&amp;gt;&lt;/p&gt;
    &lt;p&gt;Example: aws s3 sync --no-sign-request s3://download.opencontent.netflix.com/TechblogAssets/CosmosLaundromat/encodes/ .&lt;/p&gt;
    &lt;p&gt;The download links on this page will bring you to the root directory for that title's assets. You may need to navigate around to find the folder you want.&lt;/p&gt;
    &lt;p&gt;Last updated 2022-04-26&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://opencontent.netflix.com/"/><published>2025-12-30T10:11:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46432311</id><title>Non-Zero-Sum Games</title><updated>2025-12-30T20:12:16.370065+00:00</updated><content>&lt;doc fingerprint="b8854dca448ee835"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi, I'm Non-Zero-Sum James, your companion on this exploration of win-win games and how they are essential for a better future. Each week we'll explore a new aspect of game theory, moral philosophy, ethical economics and artificial intelligence—looking to solve the complex problems we face in our world together.&lt;/p&gt;
    &lt;p&gt;All the posts are connected through the lens of non-zero-sum games, but they fall into a few broad categories. You can start your journey with whatever appeals to you:&lt;/p&gt;
    &lt;quote&gt;"It is well to remember that the entire universe, with one trifling exception, is composed of others."&lt;lb/&gt;—John Holmes&lt;/quote&gt;
    &lt;p&gt;Your thoughts and contributions are welcome. Share, debate, and co-create in the comments.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nonzerosum.games/"/><published>2025-12-30T11:42:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46432451</id><title>Crimson (YC X25) is hiring founding engineers in London</title><updated>2025-12-30T20:12:15.795956+00:00</updated><content>&lt;doc fingerprint="9c99af386544742c"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Crimson is the AI platform for high-stakes litigation. We're working with top law firms in the UK and US to streamline how complex disputes are run. Our platform drafts pleadings and submissions, analyzes judgments and orders, summarizes transcripts and locates key evidence in seconds.&lt;/p&gt;
      &lt;p&gt;We're a team of three co-founders with deep technical and domain expertise. Our users are lawyers who trust us with their most sensitive case files. They care about security, accuracy, reliability and speed, and so do we.&lt;/p&gt;
      &lt;p&gt;We're looking for an exceptional full-stack engineer to join us as one of our first employees. You'll ship production code from day one and own major features end-to-end. That means talking to users, scoping the problem, building the solution and improving it over time.&lt;/p&gt;
      &lt;p&gt;What you'll do&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Contribute to the entire stack, from cloud infrastructure to prompting to UX (Python backend, Next.js with TypeScript frontend, PostgreSQL, SOTA LLMs. Deployed to Azure via IaC (Bicep) with CI/CD pipelines powered by GitHub Actions)&lt;/item&gt;
        &lt;item&gt;Collaborate closely with users to understand how lawyers work and what they need&lt;/item&gt;
        &lt;item&gt;Architect and scale document ingestion and processing pipelines to power fast, accurate search and data extraction for large volumes of legal documents&lt;/item&gt;
        &lt;item&gt;Develop intelligent, multi-step agent workflows that can autonomously handle complex legal tasks - from surfacing key testimony across depositions to generating fact chronologies and auto-detecting inconsistencies in opposing counsel’s filings&lt;/item&gt;
        &lt;item&gt;Design and bring to life intuitive AI-native user experiences tailored to litigation workflows&lt;/item&gt;
        &lt;item&gt;Improve system performance, stability and observability as we scale&lt;/item&gt;
        &lt;item&gt;Help shape our engineering culture and team - from best practices to hiring and mentorship&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;This is for you if …&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You’re driven, self-directed and exceptionally capable&lt;/item&gt;
        &lt;item&gt;You’re obsessed with building outstanding products for demanding users&lt;/item&gt;
        &lt;item&gt;You’re excited to work on hard problems, take ownership and push things through&lt;/item&gt;
        &lt;item&gt;You have high agency and a bias for shipping&lt;/item&gt;
        &lt;item&gt;You have excellent product judgement, communication skills and attention to detail&lt;/item&gt;
        &lt;item&gt;Bonus points if you have an interest in legal tech&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Why join us&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Over the next few years, the legal industry will change beyond recognition. You’ll be at the heart of that change, helping us build a category-defining company&lt;/item&gt;
        &lt;item&gt;We’re backed by YC and other top investors&lt;/item&gt;
        &lt;item&gt;We’re working with major UK and US law firms&lt;/item&gt;
        &lt;item&gt;You’ll get ownership, equity and the opportunity to shape the business and lead engineering teams&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Details&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Location: London (minimum 3 days/week in our office near Bank)&lt;/item&gt;
        &lt;item&gt;Salary: £80-120k&lt;/item&gt;
        &lt;item&gt;Meaningful equity&lt;/item&gt;
        &lt;item&gt;Contact: tell us about things you’ve built by emailing founders [at] crimson [dot] law. Please include a link to your LinkedIn profile. We’ll get back to you within 24h.&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/crimson/jobs/kCikzj1-founding-engineer-full-stack"/><published>2025-12-30T12:00:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46432862</id><title>Times New American: A Tale of Two Fonts</title><updated>2025-12-30T20:12:15.619363+00:00</updated><content>&lt;doc fingerprint="bec4a14217f20d83"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Times New American: A Tale of Two Fonts&lt;/head&gt;
    &lt;p&gt;A less romantic truth is that aesthetic standards rarely travel alone; power tends to follow in their wake. An episode at the U.S. State Department this month makes exactly this point.&lt;/p&gt;
    &lt;p&gt;On December 9, Secretary of State Marco Rubio issued a memo titled “Return to Tradition” that required all State Department documents to switch back to 14-point Times New Roman, overturning a Biden-era directive from 2023 that had turned to 15-point Calibri.&lt;/p&gt;
    &lt;p&gt;Frankly, most people likely view both of these simply as “standard typefaces” without distinguishing much difference between them. So why would an institution of the State Department’s scale bother, twice in three years, to take a stance on something as seemingly trivial as a default typeface?&lt;/p&gt;
    &lt;p&gt;John Gruber, an Apple-sphere blogger with a well-known appetite for political commentary, obtained the full text of Rubio’s memo and published it. 1 (It is worth reading first.) Rubio’s rationale, in simplified form, has three parts. First, serif typefaces are said to better communicate professionalism, formality, and authority in official documents (¶¶ 6–8). Second, using a serif typeface is aligning with the White House, the courts, and the State Department’s own historical practice (¶ 9). Third, the 2023 decision was a “cosmetic” gesture associated with diversity, equity, inclusion, and accessibility (DEIA) politics, and the reversion a correction to that (¶ 10).&lt;/p&gt;
    &lt;p&gt;Commentary on American partisan politics is beyond the scope of this article. Still, in neutral terms, Trump’s second term has been marked by an unusually rapid and sweeping effort to repeal or reverse the prior administration’s policies, with DEIA among the earliest targets. The memo itself cites Executive Order 14151, signed on the first day of the term, that instructed federal agencies to terminate all DEIA-related activities, offices, positions, policies, programs, and contracts.&lt;/p&gt;
    &lt;p&gt;That makes the political element of this typography decision fairly plain: it coheres with, and signals loyalty to, a broader anti-DEIA agenda. The remaining question is whether it is only politics. Put differently, how persuasive are Rubio’s first two, ostensibly nonpolitical claims about design and conventions? Or are they merely pretexts?&lt;/p&gt;
    &lt;p&gt;To recap, a serif typeface is one with extra decorative strokes, or “serifs,” at the ends of main strokes. A popular narrative links serifs to stone inscriptions: Roman craftsmen would sketch letter outlines on stone and carve along them; at stroke endings and corners, the chisel work flared outward, leaving the small protrusions we now call serifs. That lineage likely underwrites the memo’s association of serifs with “tradition,” “formality,” and “ceremony.”&lt;/p&gt;
    &lt;p&gt;However, most people don’t actually know this history, and many cannot reliably distinguish serif from sans-serif in the first place. The general public doesn’t perceive serif typefaces as professional and authoritative, a priori, before prioritizing their use in formal settings. Instead, people first observe that government, academia, and corporate workplaces disproportionately use serif faces — or are trained to use them — and only then infer that serifs must mean professionalism and authority.&lt;/p&gt;
    &lt;p&gt;Even if we limit ourselves to design and historical considerations, Times New Roman, despite being a serif typeface, possesses little of the “professional, solemn, and authoritative” aura. The typeface was designed in 1931 for The Times of London, and newspaper typefaces are typically engineered to print cleanly on cheap paper, conserve space, and support rapid scanning.&lt;/p&gt;
    &lt;p&gt;Those goals are visible in the details. The strokes of Times New Roman are relatively thin (leaving tolerance for ink spread on newsprint), the letterforms are narrow, and the x-height (the height of the lowercase “x”) is comparatively large. There is nothing inherently wrong with such functional design; it simply doesn’t map neatly onto the “traditional” look of older serifs. On a modern, high-resolution display, the typeface can appear spindly, more utilitarian than ceremonial.&lt;/p&gt;
    &lt;p&gt;Indeed, the stronger explanation for Times New Roman’s long reign isn’t aesthetic excellence, but practicality and inertia. Times New Roman was among the small set of typefaces bundled with early versions of Windows. It was also promoted as “web-safe,” meaning webmasters could reasonably assume it would render properly across platforms. In the early era of digitalization, choosing Times New Roman was often less a deliberate endorsement than a default imposed by limited options. Over time, the habit hardened into a standard, and institutions began to require it without much reflection, effectively borrowing their own authority to confer authority upon the typeface.&lt;/p&gt;
    &lt;p&gt;Professionals who genuinely focus on typography have advised against Times New Roman. For example, type designer Matthew Butterick eloquently comments:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When Times New Roman appears in a book, document, or advertisement, it connotes apathy. It says, “I submitted to the typeface of least resistance.” Times New Roman isn’t a typeface choice so much as the absence of a typeface choice, like the blackness of deep space isn’t a color. To look at Times New Roman is to gaze into the void.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Similarly, the U.S. Court of Appeals for the Eighth Circuit, in its formatting advice for lawyers, specifically cautions:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Typographic decisions should be made for a purpose. The Times of London chose the typeface Times New Roman to serve an audience looking for a quick read. Lawyers don’t want their audience to read fast and throw the document away; they want to maximize retention. Achieving that goal requires a different approach — different typefaces, different column widths, different writing conventions. Briefs are like books rather than newspapers. The most important piece of advice we can offer is this: read some good books and try to make your briefs more like them.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As for the other U.S. official bodies Rubio cites in the memo, many don’t actually use Times New Roman either. The Supreme Court’s rules require booklet-format filings to be set in the Century family, and its own opinions are typeset in Century Schoolbook from that family. Originating in the 19th century, the typeface features more expansive proportions, balanced stroke contrast, and an elegant form, exuding a far more assertive presence than Times New Roman. As the name suggests, it also began life as a textbook face, optimized for legibility. With proper typesetting, it reads far better than a haphazardly produced Word document set in Times New Roman.&lt;/p&gt;
    &lt;p&gt;Looking at the legislature, the official PDFs of U.S. Congressional bills use Cheltenham for titles and De Vinne for body text. De Vinne, first released in 1902, shares similarities in style with Century Schoolbook but features stronger stroke contrast and more decorative serifs, giving it an “engraved” quality. Objectively speaking, this design borders on being a display typeface — imagine the logotype of Harper’s Bazaar, Didot — and is somewhat tiring to read in body text. But when it comes to conveying ceremony and solemnity, it’s far more qualified than Times New Roman. (After a bill is enacted into law, it will be typeset in New Century Schoolbook.)&lt;/p&gt;
    &lt;p&gt;Even the Trump administration, to which Rubio pledges allegiance, contradicts the “serif tradition” by using a fashionable tall, high-contrast serif (Instrument Serif) on the White House website. It may look a bit mannered by government standards — an impression no less bolstered by its bombastic rhetoric — but it does manage to appear assertive and emphatic. Swap in Times New Roman and “AMERICA IS BACK” would read more like a mutter.&lt;/p&gt;
    &lt;p&gt;Thus, the design and historical reasons cited in Rubio’s memo don’t hold up. The formality and authority of serif typefaces are largely socially constructed, and Times New Roman’s origin story and design constraints don’t express these qualities. If Times New Roman carries authority at all, it’s primarily borrowed from the authority of institutions that have adhered to it. If the sincere goal were to “return to tradition” by returning to a serif, there are many choices with deeper pedigree and more fitting gravitas.&lt;/p&gt;
    &lt;p&gt;At this point, it might sound as though the argument is trending toward a defense of the Department’s earlier choice: Calibri. Unfortunately, Calibri is also a poor fit for formal contexts. While seriousness and authority aren’t the exclusive province of serifs, Calibri does little to convey those traits.&lt;/p&gt;
    &lt;p&gt;Typographically, Calibri is a humanist sans-serif. Such typefaces tend to have open, rounded forms and generous apertures (look at the wide openings in letters like a, c, e, and s). Calibri takes that softness especially far: terminals are visibly rounded, and many letters appear almost handwritten, to the extent that its designer described its quality as “warm and soft.”&lt;/p&gt;
    &lt;p&gt;There’s nothing inherently wrong with this style, but one would hardly want an official document or legal contract to appear “warm and soft.” That is why I have long disliked Microsoft’s decision to make Calibri the default Office typeface starting with Office 2007. A default body typeface should be neutral and versatile, not exude a temperature. (Microsoft replaced Calibri with Aptos as the default in 2023, but inertia being what it is, Aptos still appears relatively rarely in the wild.)&lt;/p&gt;
    &lt;p&gt;To be fair, the State Department’s 2023 change was justified less as a matter of taste than as an accessibility and inclusion initiative. That is, to make documents easier to read for individuals with various physical and cognitive conditions. This goal is commendable in itself, but the means were, at best, loosely connected to the end, much like many inclusive measures that were once fashionable in U.S. politics and business in recent years.&lt;/p&gt;
    &lt;p&gt;First, Calibri was not designed with accessibility in mind. It was commissioned by Microsoft to promote its ClearType technology, with the design objective of appearing clear on the low-resolution displays of its time. This means it prioritizes smoothness under specific sub-pixel rendering techniques, rather than ensuring the glyphs are easy to tell apart. If accessibility were truly the goal, one might select a typeface created for that purpose. For example, Atkinson Hyperlegible addresses character differentiation by adding serifs, exaggerating shapes, and slanting strokes, making it legible even under low-vision conditions. In contrast, Calibri has no anti-ambiguity design: the uppercase &lt;code&gt;I&lt;/code&gt; and lowercase &lt;code&gt;l&lt;/code&gt; are nearly identical. So much for “accessibility.”&lt;/p&gt;
    &lt;p&gt;Furthermore, accessibility doesn’t depend solely on a document’s appearance but more on its internal structure and presentation mechanisms. For instance, the W3C’s Web Content Accessibility Guidelines (WCAG) state that accessible content should be perceivable, operable, understandable, and robust. This means that documents should have proper semantic structure (so tools like screen readers can interpret content correctly), support customizable layouts and fonts, and be compatible with various applications and devices. If these principles were met, the specific font used would matter little, as users can access the content with their preferred tools in their preferred manner. Conversely, if a document is technically crude, like a scanned PDF — as many official documents are — the use of an “inclusive” font is merely self-congratulatory.&lt;/p&gt;
    &lt;p&gt;If one insisted on a sans-serif for official writing, there are many better candidates than Calibri: Frutiger (common in airport wayfinding), Myriad (used by Apple for years), the cool and serious Univers (or a well-set Helvetica Neue), or contemporary neutral workhorses like Inter. If a “made in America” signal mattered, Public Sans (funded under the 21st Century Integrated Digital Experience Act passed during Trump’s first term) and used by many U.S. government websites is also a good option.&lt;/p&gt;
    &lt;p&gt;Therefore, Rubio’s criticism that the previous move was “cosmetic,” while being politically charged, isn’t entirely unfounded.&lt;/p&gt;
    &lt;p&gt;Taken together, the Department had previously pursued a defensible goal with a poorly matched design intervention and landed on an ill-fitting typeface. Now, for political motives, it has reversed that decision and returned to a bland, unremarkable default. Between the two, Times New Roman may be the lesser evil: it is more widely recognized, and it doesn’t clash with the official context as overtly as Calibri does. Still, Rubio, or whoever drafted the memo for him, could have been more candid. There was no need to dress up a political gesture with faux-erudite claims or to lavish praise on a mediocre typeface.&lt;/p&gt;
    &lt;p&gt;Because Times New Roman just will not make America great again.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The authenticity of the text cannot be independently verified, of course. It is, however, consistent with reports from reputable media outlets. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hsu.cy/2025/12/times-new-american/"/><published>2025-12-30T12:56:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46432916</id><title>Approachable Swift Concurrency</title><updated>2025-12-30T20:12:15.349927+00:00</updated><content>&lt;doc fingerprint="7470a7176c2d9444"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fucking Approachable&lt;lb/&gt;Swift Concurrency&lt;/head&gt;
    &lt;p&gt;Finally understand async/await, Tasks, and why the compiler keeps yelling at you.&lt;/p&gt;
    &lt;p&gt;Huge thanks to Matt Massicotte for making Swift concurrency understandable. Put together by Pedro Piñera. Found an issue? [email protected]&lt;/p&gt;
    &lt;p&gt;In the tradition of fuckingblocksyntax.com and fuckingifcaseletsyntax.com&lt;/p&gt;
    &lt;head rend="h2"&gt;Async Code: async/await&lt;/head&gt;
    &lt;p&gt;Most of what apps do is wait. Fetch data from a server - wait for the response. Read a file from disk - wait for the bytes. Query a database - wait for the results.&lt;/p&gt;
    &lt;p&gt;Before Swift's concurrency system, you'd express this waiting with callbacks, delegates, or Combine. They work, but nested callbacks get hard to follow, and Combine has a steep learning curve.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;async/await&lt;/code&gt; gives Swift a new way to handle waiting. Instead of callbacks, you write code that looks sequential - it pauses, waits, and resumes. Under the hood, Swift's runtime manages these pauses efficiently. But making your app actually stay responsive while waiting depends on where code runs, which we'll cover later.&lt;/p&gt;
    &lt;p&gt;An async function is one that might need to pause. You mark it with &lt;code&gt;async&lt;/code&gt;, and when you call it, you use &lt;code&gt;await&lt;/code&gt; to say "pause here until this finishes":&lt;/p&gt;
    &lt;code&gt;func fetchUser(id: Int) async throws -&amp;gt; User {
    let url = URL(string: "https://api.example.com/users/\(id)")!
    let (data, _) = try await URLSession.shared.data(from: url)  // Suspends here
    return try JSONDecoder().decode(User.self, from: data)
}

// Calling it
let user = try await fetchUser(id: 123)
// Code here runs after fetchUser completes&lt;/code&gt;
    &lt;p&gt;Your code pauses at each &lt;code&gt;await&lt;/code&gt; - this is called suspension. When the work finishes, your code resumes right where it left off. Suspension gives Swift the opportunity to do other work while waiting.&lt;/p&gt;
    &lt;head rend="h3"&gt;Waiting for them&lt;/head&gt;
    &lt;p&gt;What if you need to fetch several things? You could await them one by one:&lt;/p&gt;
    &lt;code&gt;let avatar = try await fetchImage("avatar.jpg")
let banner = try await fetchImage("banner.jpg")
let bio = try await fetchBio()&lt;/code&gt;
    &lt;p&gt;But that's slow - each waits for the previous one to finish. Use &lt;code&gt;async let&lt;/code&gt; to run them in parallel:&lt;/p&gt;
    &lt;code&gt;func loadProfile() async throws -&amp;gt; Profile {
    async let avatar = fetchImage("avatar.jpg")
    async let banner = fetchImage("banner.jpg")
    async let bio = fetchBio()

    // All three are fetching in parallel!
    return Profile(
        avatar: try await avatar,
        banner: try await banner,
        bio: try await bio
    )
}&lt;/code&gt;
    &lt;p&gt;Each &lt;code&gt;async let&lt;/code&gt; starts immediately. The &lt;code&gt;await&lt;/code&gt; collects the results.&lt;/p&gt;
    &lt;head rend="h4"&gt;await needs async&lt;/head&gt;
    &lt;p&gt;You can only use &lt;code&gt;await&lt;/code&gt; inside an &lt;code&gt;async&lt;/code&gt; function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Managing Work: Tasks&lt;/head&gt;
    &lt;p&gt;A Task is a unit of async work you can manage. You've written async functions, but a Task is what actually runs them. It's how you start async code from synchronous code, and it gives you control over that work: wait for its result, cancel it, or let it run in the background.&lt;/p&gt;
    &lt;p&gt;Let's say you're building a profile screen. Load the avatar when the view appears using the &lt;code&gt;.task&lt;/code&gt; modifier, which cancels automatically when the view disappears:&lt;/p&gt;
    &lt;code&gt;struct ProfileView: View {
    @State private var avatar: Image?

    var body: some View {
        avatar
            .task { avatar = await downloadAvatar() }
    }
}&lt;/code&gt;
    &lt;p&gt;If users can switch between profiles, use &lt;code&gt;.task(id:)&lt;/code&gt; to reload when the selection changes:&lt;/p&gt;
    &lt;code&gt;struct ProfileView: View {
    var userID: String
    @State private var avatar: Image?

    var body: some View {
        avatar
            .task(id: userID) { avatar = await downloadAvatar(for: userID) }
    }
}&lt;/code&gt;
    &lt;p&gt;When the user taps "Save", create a Task manually:&lt;/p&gt;
    &lt;code&gt;Button("Save") {
    Task { await saveProfile() }
}&lt;/code&gt;
    &lt;p&gt;What if you need to load the avatar, bio, and stats all at once? Use a &lt;code&gt;TaskGroup&lt;/code&gt; to fetch them in parallel:&lt;/p&gt;
    &lt;code&gt;try await withThrowingTaskGroup(of: Void.self) { group in
    group.addTask { avatar = try await downloadAvatar(for: userID) }
    group.addTask { bio = try await fetchBio(for: userID) }
    group.addTask { stats = try await fetchStats(for: userID) }
    try await group.waitForAll()
}&lt;/code&gt;
    &lt;p&gt;Tasks inside a group are child tasks, linked to the parent. A few things to know:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cancellation propagates: cancel the parent, and all children get cancelled too&lt;/item&gt;
      &lt;item&gt;Errors: a thrown error cancels siblings and rethrows, but only when you consume results with &lt;code&gt;next()&lt;/code&gt;,&lt;code&gt;waitForAll()&lt;/code&gt;, or iteration&lt;/item&gt;
      &lt;item&gt;Completion order: results arrive as tasks finish, not the order you added them&lt;/item&gt;
      &lt;item&gt;Waits for all: the group doesn't return until every child completes or is cancelled&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is structured concurrency: work organized in a tree that's easy to reason about and clean up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Things Run: From Threads to Isolation Domains&lt;/head&gt;
    &lt;p&gt;So far we've talked about when code runs (async/await) and how to organize it (Tasks). Now: where does it run, and how do we keep it safe?&lt;/p&gt;
    &lt;head rend="h4"&gt;Most apps just wait&lt;/head&gt;
    &lt;p&gt;Most app code is I/O-bound. You fetch data from a network, await a response, decode it, and display it. If you have multiple I/O operations to coordinate, you resort to tasks and task groups. The actual CPU work is minimal. The main thread can handle this fine because &lt;code&gt;await&lt;/code&gt; suspends without blocking.&lt;/p&gt;
    &lt;p&gt;But sooner or later, you'll have CPU-bound work: parsing a giant JSON file, processing images, running complex calculations. This work doesn't wait for anything external. It just needs CPU cycles. If you run it on the main thread, your UI freezes. This is where "where does code run" actually matters.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Old World: Many Options, No Safety&lt;/head&gt;
    &lt;p&gt;Before Swift's concurrency system, you had several ways to manage execution:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Approach&lt;/cell&gt;
        &lt;cell role="head"&gt;What it does&lt;/cell&gt;
        &lt;cell role="head"&gt;Tradeoffs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Thread&lt;/cell&gt;
        &lt;cell&gt;Direct thread control&lt;/cell&gt;
        &lt;cell&gt;Low-level, error-prone, rarely needed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GCD&lt;/cell&gt;
        &lt;cell&gt;Dispatch queues with closures&lt;/cell&gt;
        &lt;cell&gt;Simple but no cancellation, easy to cause thread explosion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OperationQueue&lt;/cell&gt;
        &lt;cell&gt;Task dependencies, cancellation, KVO&lt;/cell&gt;
        &lt;cell&gt;More control but verbose and heavyweight&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Combine&lt;/cell&gt;
        &lt;cell&gt;Reactive streams&lt;/cell&gt;
        &lt;cell&gt;Great for event streams, steep learning curve&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All of these worked, but safety was entirely on you. The compiler couldn't help if you forgot to dispatch to main, or if two queues accessed the same data simultaneously.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem: Data Races&lt;/head&gt;
    &lt;p&gt;A data race happens when two threads access the same memory at the same time, and at least one is writing:&lt;/p&gt;
    &lt;code&gt;var count = 0

DispatchQueue.global().async { count += 1 }
DispatchQueue.global().async { count += 1 }

// Undefined behavior: crash, memory corruption, or wrong value&lt;/code&gt;
    &lt;p&gt;Data races are undefined behavior. They can crash, corrupt memory, or silently produce wrong results. Your app works fine in testing, then crashes randomly in production. Traditional tools like locks and semaphores help, but they're manual and error-prone.&lt;/p&gt;
    &lt;head rend="h4"&gt;Concurrency amplifies the problem&lt;/head&gt;
    &lt;p&gt;The more concurrent your app is, the more likely data races become. A simple iOS app might get away with sloppy thread safety. A web server handling thousands of simultaneous requests will crash constantly. This is why Swift's compile-time safety matters most in high-concurrency environments.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Shift: From Threads to Isolation&lt;/head&gt;
    &lt;p&gt;Swift's concurrency model asks a different question. Instead of "which thread should this run on?", it asks: "who is allowed to access this data?"&lt;/p&gt;
    &lt;p&gt;This is isolation. Rather than manually dispatching work to threads, you declare boundaries around data. The compiler enforces these boundaries at build time, not runtime.&lt;/p&gt;
    &lt;head rend="h4"&gt;Under the hood&lt;/head&gt;
    &lt;p&gt;Swift Concurrency is built on top of libdispatch (the same runtime as GCD). The difference is the compile-time layer: actors and isolation are enforced by the compiler, while the runtime handles scheduling on a cooperative thread pool limited to your CPU's core count.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Three Isolation Domains&lt;/head&gt;
    &lt;p&gt;1. MainActor&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;@MainActor&lt;/code&gt; is a global actor that represents the main thread's isolation domain. It's special because UI frameworks (UIKit, AppKit, SwiftUI) require main thread access.&lt;/p&gt;
    &lt;code&gt;@MainActor
class ViewModel {
    var items: [Item] = []  // Protected by MainActor isolation
}&lt;/code&gt;
    &lt;p&gt;When you mark something &lt;code&gt;@MainActor&lt;/code&gt;, you're not saying "dispatch this to the main thread." You're saying "this belongs to the main actor's isolation domain." The compiler enforces that anything accessing it must either be on MainActor or &lt;code&gt;await&lt;/code&gt; to cross the boundary.&lt;/p&gt;
    &lt;head rend="h4"&gt;When in doubt, use @MainActor&lt;/head&gt;
    &lt;p&gt;For most apps, marking your ViewModels with &lt;code&gt;@MainActor&lt;/code&gt; is the right choice. Performance concerns are usually overblown. Start here, optimize only if you measure actual problems.&lt;/p&gt;
    &lt;p&gt;2. Actors&lt;/p&gt;
    &lt;p&gt;An actor protects its own mutable state. It guarantees that only one piece of code can access its data at a time:&lt;/p&gt;
    &lt;code&gt;actor BankAccount {
    var balance: Double = 0

    func deposit(_ amount: Double) {
        balance += amount  // Safe: actor guarantees exclusive access
    }
}

// From outside, you must await to cross the boundary
await account.deposit(100)&lt;/code&gt;
    &lt;p&gt;Actors are not threads. An actor is an isolation boundary. The Swift runtime decides which thread actually executes actor code. You don't control that, and you don't need to.&lt;/p&gt;
    &lt;p&gt;3. Nonisolated&lt;/p&gt;
    &lt;p&gt;Code marked &lt;code&gt;nonisolated&lt;/code&gt; opts out of actor isolation. It can be called from anywhere without &lt;code&gt;await&lt;/code&gt;, but it cannot access the actor's protected state:&lt;/p&gt;
    &lt;code&gt;actor BankAccount {
    var balance: Double = 0

    nonisolated func bankName() -&amp;gt; String {
        "Acme Bank"  // No actor state accessed, safe to call from anywhere
    }
}

let name = account.bankName()  // No await needed&lt;/code&gt;
    &lt;head rend="h4"&gt;Approachable Concurrency: Less Friction&lt;/head&gt;
    &lt;p&gt;Approachable Concurrency simplifies the mental model with two Xcode build settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SWIFT_DEFAULT_ACTOR_ISOLATION&lt;/code&gt;=&lt;code&gt;MainActor&lt;/code&gt;: Everything runs on MainActor unless you say otherwise&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SWIFT_APPROACHABLE_CONCURRENCY&lt;/code&gt;=&lt;code&gt;YES&lt;/code&gt;:&lt;code&gt;nonisolated&lt;/code&gt;async functions stay on the caller's actor instead of jumping to a background thread&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;New Xcode 26 projects have both enabled by default. When you need CPU-intensive work off the main thread, use &lt;code&gt;@concurrent&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;// Runs on MainActor (the default)
func updateUI() async { }

// Runs on background thread (opt-in)
@concurrent func processLargeFile() async { }&lt;/code&gt;
    &lt;head rend="h4"&gt;The Office Building&lt;/head&gt;
    &lt;p&gt;Think of your app as an office building. Each isolation domain is a private office with a lock on the door. Only one person can be inside at a time, working with the documents in that office.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;MainActor&lt;/code&gt;is the front desk - where all customer interactions happen. There's only one, and it handles everything the user sees.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;actor&lt;/code&gt;types are department offices - Accounting, Legal, HR. Each protects its own sensitive documents.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;nonisolated&lt;/code&gt;code is the hallway - shared space anyone can walk through, but no private documents live there.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can't just barge into someone's office. You knock (&lt;code&gt;await&lt;/code&gt;) and wait for them to let you in.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Can Cross Isolation Domains: Sendable&lt;/head&gt;
    &lt;p&gt;Isolation domains protect data, but eventually you need to pass data between them. When you do, Swift checks if it's safe.&lt;/p&gt;
    &lt;p&gt;Think about it: if you pass a reference to a mutable class from one actor to another, both actors could modify it simultaneously. That's exactly the data race we're trying to prevent. So Swift needs to know: can this data be safely shared?&lt;/p&gt;
    &lt;p&gt;The answer is the &lt;code&gt;Sendable&lt;/code&gt; protocol. It's a marker that tells the compiler "this type is safe to pass across isolation boundaries":&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sendable types can cross safely (value types, immutable data, actors)&lt;/item&gt;
      &lt;item&gt;Non-Sendable types can't (classes with mutable state)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;// Sendable - it's a value type, each place gets a copy
struct User: Sendable {
    let id: Int
    let name: String
}

// Non-Sendable - it's a class with mutable state
class Counter {
    var count = 0  // Two places modifying this = disaster
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Making Types Sendable&lt;/head&gt;
    &lt;p&gt;Swift automatically infers &lt;code&gt;Sendable&lt;/code&gt; for many types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Structs and enums with only &lt;code&gt;Sendable&lt;/code&gt;properties are implicitly&lt;code&gt;Sendable&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Actors are always &lt;code&gt;Sendable&lt;/code&gt;because they protect their own state&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@MainActor&lt;/code&gt;types are&lt;code&gt;Sendable&lt;/code&gt;because MainActor serializes access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For classes, it's harder. A class can conform to &lt;code&gt;Sendable&lt;/code&gt; only if it's &lt;code&gt;final&lt;/code&gt; and all its stored properties are immutable:&lt;/p&gt;
    &lt;code&gt;final class APIConfig: Sendable {
    let baseURL: URL      // Immutable
    let timeout: Double   // Immutable
}&lt;/code&gt;
    &lt;p&gt;If you have a class that's thread-safe through other means (locks, atomics), you can use &lt;code&gt;@unchecked Sendable&lt;/code&gt; to tell the compiler "trust me":&lt;/p&gt;
    &lt;code&gt;final class ThreadSafeCache: @unchecked Sendable {
    private let lock = NSLock()
    private var storage: [String: Data] = [:]
}&lt;/code&gt;
    &lt;head rend="h4"&gt;@unchecked Sendable is a promise&lt;/head&gt;
    &lt;p&gt;The compiler won't verify thread safety. If you're wrong, you'll get data races. Use sparingly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Approachable Concurrency: Less Friction&lt;/head&gt;
    &lt;p&gt;With Approachable Concurrency, Sendable errors become much rarer:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If code doesn't cross isolation boundaries, you don't need Sendable&lt;/item&gt;
      &lt;item&gt;Async functions stay on the caller's actor instead of hopping to a background thread&lt;/item&gt;
      &lt;item&gt;The compiler is smarter about detecting when values are used safely&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Enable it by setting &lt;code&gt;SWIFT_DEFAULT_ACTOR_ISOLATION&lt;/code&gt; to &lt;code&gt;MainActor&lt;/code&gt; and &lt;code&gt;SWIFT_APPROACHABLE_CONCURRENCY&lt;/code&gt; to &lt;code&gt;YES&lt;/code&gt;. New Xcode 26 projects have both enabled by default. When you do need parallelism, mark functions &lt;code&gt;@concurrent&lt;/code&gt; and then think about Sendable.&lt;/p&gt;
    &lt;head rend="h4"&gt;Photocopies vs. Original Documents&lt;/head&gt;
    &lt;p&gt;Back to the office building. When you need to share information between departments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Photocopies are safe - If Legal makes a copy of a document and sends it to Accounting, both have their own copy. They can scribble on them, modify them, whatever. No conflict.&lt;/item&gt;
      &lt;item&gt;Original signed contracts must stay put - If two departments could both modify the original, chaos ensues. Who has the real version?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;Sendable&lt;/code&gt; types are like photocopies: safe to share because each place gets its own independent copy (value types) or because they're immutable (nobody can modify them). Non-&lt;code&gt;Sendable&lt;/code&gt; types are like original contracts: passing them around creates the potential for conflicting modifications.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Isolation Is Inherited&lt;/head&gt;
    &lt;p&gt;You've seen that isolation domains protect data, and Sendable controls what crosses between them. But how does code end up in an isolation domain in the first place?&lt;/p&gt;
    &lt;p&gt;When you call a function or create a closure, isolation flows through your code. With Approachable Concurrency, your app starts on &lt;code&gt;MainActor&lt;/code&gt;, and that isolation propagates to the code you call, unless something explicitly changes it. Understanding this flow helps you predict where code runs and why the compiler sometimes complains.&lt;/p&gt;
    &lt;head rend="h3"&gt;Function Calls&lt;/head&gt;
    &lt;p&gt;When you call a function, its isolation determines where it runs:&lt;/p&gt;
    &lt;code&gt;@MainActor func updateUI() { }      // Always runs on MainActor
func helper() { }                    // Inherits caller's isolation
@concurrent func crunch() async { }  // Explicitly runs off-actor&lt;/code&gt;
    &lt;p&gt;With Approachable Concurrency, most of your code inherits &lt;code&gt;MainActor&lt;/code&gt; isolation. The function runs where the caller runs, unless it explicitly opts out.&lt;/p&gt;
    &lt;head rend="h3"&gt;Closures&lt;/head&gt;
    &lt;p&gt;Closures inherit isolation from the context where they're defined:&lt;/p&gt;
    &lt;code&gt;@MainActor
class ViewModel {
    func setup() {
        let closure = {
            // Inherits MainActor from ViewModel
            self.updateUI()  // Safe, same isolation
        }
        closure()
    }
}&lt;/code&gt;
    &lt;p&gt;This is why SwiftUI's &lt;code&gt;Button&lt;/code&gt; action closures can safely update &lt;code&gt;@State&lt;/code&gt;: they inherit MainActor isolation from the view.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tasks&lt;/head&gt;
    &lt;p&gt;A &lt;code&gt;Task { }&lt;/code&gt; inherits actor isolation from where it's created:&lt;/p&gt;
    &lt;code&gt;@MainActor
class ViewModel {
    func doWork() {
        Task {
            // Inherits MainActor isolation
            self.updateUI()  // Safe, no await needed
        }
    }
}&lt;/code&gt;
    &lt;p&gt;This is usually what you want. The task runs on the same actor as the code that created it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking Inheritance: Task.detached&lt;/head&gt;
    &lt;p&gt;Sometimes you want a task that doesn't inherit any context:&lt;/p&gt;
    &lt;code&gt;@MainActor
class ViewModel {
    func doHeavyWork() {
        Task.detached {
            // No actor isolation, runs on cooperative pool
            let result = await self.expensiveCalculation()
            await MainActor.run {
                self.data = result  // Explicitly hop back
            }
        }
    }
}&lt;/code&gt;
    &lt;head rend="h4"&gt;Task.detached is usually wrong&lt;/head&gt;
    &lt;p&gt;The Swift team recommends Task.detached as a last resort. It doesn't inherit priority, task-local values, or actor context. Most of the time, regular &lt;code&gt;Task&lt;/code&gt; is what you want. If you need CPU-intensive work off the main actor, mark the function &lt;code&gt;@concurrent&lt;/code&gt; instead.&lt;/p&gt;
    &lt;head rend="h4"&gt;Walking Through the Building&lt;/head&gt;
    &lt;p&gt;When you're in the front desk office (MainActor), and you call someone to help you, they come to your office. They inherit your location. If you create a task ("go do this for me"), that assistant starts in your office too.&lt;/p&gt;
    &lt;p&gt;The only way someone ends up in a different office is if they explicitly go there: "I need to work in Accounting for this" (&lt;code&gt;actor&lt;/code&gt;), or "I'll handle this in the back office" (&lt;code&gt;@concurrent&lt;/code&gt;).&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting It All Together&lt;/head&gt;
    &lt;p&gt;Let's step back and see how all the pieces fit.&lt;/p&gt;
    &lt;p&gt;Swift Concurrency can feel like a lot of concepts: &lt;code&gt;async/await&lt;/code&gt;, &lt;code&gt;Task&lt;/code&gt;, actors, &lt;code&gt;MainActor&lt;/code&gt;, &lt;code&gt;Sendable&lt;/code&gt;, isolation domains. But there's really just one idea at the center of it all: isolation is inherited by default.&lt;/p&gt;
    &lt;p&gt;With Approachable Concurrency enabled, your app starts on &lt;code&gt;MainActor&lt;/code&gt;. That's your starting point. From there:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Every function you call inherits that isolation&lt;/item&gt;
      &lt;item&gt;Every closure you create captures that isolation&lt;/item&gt;
      &lt;item&gt;Every &lt;code&gt;Task { }&lt;/code&gt;you spawn inherits that isolation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don't have to annotate anything. You don't have to think about threads. Your code runs on &lt;code&gt;MainActor&lt;/code&gt;, and the isolation just propagates through your program automatically.&lt;/p&gt;
    &lt;p&gt;When you need to break out of that inheritance, you do it explicitly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;@concurrent&lt;/code&gt;says "run this on a background thread"&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;actor&lt;/code&gt;says "this type has its own isolation domain"&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Task.detached { }&lt;/code&gt;says "start fresh, inherit nothing"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And when you pass data between isolation domains, Swift checks that it's safe. That's what &lt;code&gt;Sendable&lt;/code&gt; is for: marking types that can safely cross boundaries.&lt;/p&gt;
    &lt;p&gt;That's it. That's the whole model:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Isolation propagates from &lt;code&gt;MainActor&lt;/code&gt;through your code&lt;/item&gt;
      &lt;item&gt;You opt out explicitly when you need background work or separate state&lt;/item&gt;
      &lt;item&gt;Sendable guards the boundaries when data crosses between domains&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the compiler complains, it's telling you one of these rules was violated. Trace the inheritance: where did the isolation come from? Where is the code trying to run? What data is crossing a boundary? The answer is usually obvious once you ask the right question.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where to Go From Here&lt;/head&gt;
    &lt;p&gt;The good news: you don't need to master everything at once.&lt;/p&gt;
    &lt;p&gt;Most apps only need the basics. Mark your ViewModels with &lt;code&gt;@MainActor&lt;/code&gt;, use &lt;code&gt;async/await&lt;/code&gt; for network calls, and create &lt;code&gt;Task { }&lt;/code&gt; when you need to kick off async work from a button tap. That's it. That handles 80% of real-world apps. The compiler will tell you if you need more.&lt;/p&gt;
    &lt;p&gt;When you need parallel work, reach for &lt;code&gt;async let&lt;/code&gt; to fetch multiple things at once, or &lt;code&gt;TaskGroup&lt;/code&gt; when the number of tasks is dynamic. Learn to handle cancellation gracefully. This covers apps with complex data loading or real-time features.&lt;/p&gt;
    &lt;p&gt;Advanced patterns come later, if ever. Custom actors for shared mutable state, &lt;code&gt;@concurrent&lt;/code&gt; for CPU-intensive processing, deep &lt;code&gt;Sendable&lt;/code&gt; understanding. This is framework code, server-side Swift, complex desktop apps. Most developers never need this level.&lt;/p&gt;
    &lt;head rend="h4"&gt;Start simple&lt;/head&gt;
    &lt;p&gt;Don't optimize for problems you don't have. Start with the basics, ship your app, and add complexity only when you hit real problems. The compiler will guide you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Watch Out: Common Mistakes&lt;/head&gt;
    &lt;head rend="h3"&gt;Thinking async = background&lt;/head&gt;
    &lt;code&gt;// This STILL blocks the main thread!
@MainActor
func slowFunction() async {
    let result = expensiveCalculation()  // Synchronous work = blocking
    data = result
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;async&lt;/code&gt; means "can pause." The actual work still runs wherever it runs. Use &lt;code&gt;@concurrent&lt;/code&gt; (Swift 6.2) or &lt;code&gt;Task.detached&lt;/code&gt; for CPU-heavy work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating too many actors&lt;/head&gt;
    &lt;code&gt;// Over-engineered
actor NetworkManager { }
actor CacheManager { }
actor DataManager { }

// Better - most things can live on MainActor
@MainActor
class AppState { }&lt;/code&gt;
    &lt;p&gt;You need a custom actor only when you have shared mutable state that can't live on &lt;code&gt;MainActor&lt;/code&gt;. Matt Massicotte's rule: introduce an actor only when (1) you have non-&lt;code&gt;Sendable&lt;/code&gt; state, (2) operations on that state must be atomic, and (3) those operations can't run on an existing actor. If you can't justify it, use &lt;code&gt;@MainActor&lt;/code&gt; instead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Making everything Sendable&lt;/head&gt;
    &lt;p&gt;Not everything needs to cross boundaries. If you're adding &lt;code&gt;@unchecked Sendable&lt;/code&gt; everywhere, step back and ask if the data actually needs to move between isolation domains.&lt;/p&gt;
    &lt;head rend="h3"&gt;Using MainActor.run when you don't need it&lt;/head&gt;
    &lt;code&gt;// Unnecessary
Task {
    let data = await fetchData()
    await MainActor.run {
        self.data = data
    }
}

// Better - just make the function @MainActor
@MainActor
func loadData() async {
    self.data = await fetchData()
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;MainActor.run&lt;/code&gt; is rarely the right solution. If you need MainActor isolation, annotate the function with &lt;code&gt;@MainActor&lt;/code&gt; instead. It's clearer and the compiler can help you more. See Matt's take on this.&lt;/p&gt;
    &lt;head rend="h3"&gt;Blocking the cooperative thread pool&lt;/head&gt;
    &lt;code&gt;// NEVER do this - risks deadlock
func badIdea() async {
    let semaphore = DispatchSemaphore(value: 0)
    Task {
        await doWork()
        semaphore.signal()
    }
    semaphore.wait()  // Blocks a cooperative thread!
}&lt;/code&gt;
    &lt;p&gt;Swift's cooperative thread pool has limited threads. Blocking one with &lt;code&gt;DispatchSemaphore&lt;/code&gt;, &lt;code&gt;DispatchGroup.wait()&lt;/code&gt;, or similar calls can cause deadlocks. If you need to bridge sync and async code, use &lt;code&gt;async let&lt;/code&gt; or restructure to stay fully async.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating unnecessary Tasks&lt;/head&gt;
    &lt;code&gt;// Unnecessary Task creation
func fetchAll() async {
    Task { await fetchUsers() }
    Task { await fetchPosts() }
}

// Better - use structured concurrency
func fetchAll() async {
    async let users = fetchUsers()
    async let posts = fetchPosts()
    await (users, posts)
}&lt;/code&gt;
    &lt;p&gt;If you're already in an async context, prefer structured concurrency (&lt;code&gt;async let&lt;/code&gt;, &lt;code&gt;TaskGroup&lt;/code&gt;) over creating unstructured &lt;code&gt;Task&lt;/code&gt;s. Structured concurrency handles cancellation automatically and makes the code easier to reason about.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cheat Sheet: Quick Reference&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Keyword&lt;/cell&gt;
        &lt;cell role="head"&gt;What it does&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;async&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Function can pause&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;await&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pause here until done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Task { }&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start async work, inherits context&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Task.detached { }&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start async work, no inherited context&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;@MainActor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Runs on main thread&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;actor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Type with isolated mutable state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;nonisolated&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Opts out of actor isolation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Sendable&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Safe to pass between isolation domains&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;@concurrent&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Always run on background (Swift 6.2+)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;async let&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start parallel work&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;TaskGroup&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dynamic parallel work&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Further Reading&lt;/head&gt;
    &lt;head rend="h4"&gt;Matt Massicotte's Blog (Highly Recommended)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Swift Concurrency Glossary - Essential terminology&lt;/item&gt;
      &lt;item&gt;An Introduction to Isolation - The core concept&lt;/item&gt;
      &lt;item&gt;When should you use an actor? - Practical guidance&lt;/item&gt;
      &lt;item&gt;Non-Sendable types are cool too - Why simpler is better&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fuckingapproachableswiftconcurrency.com/en/"/><published>2025-12-30T13:01:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46432999</id><title>The British empire's resilient subsea telegraph network</title><updated>2025-12-30T20:12:15.015742+00:00</updated><content>&lt;doc fingerprint="a5d313b1b0391af3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;The British Empire's Resilient Subsea Telegraph Network&lt;/head&gt;
    &lt;p&gt;The British empire had largely completed its Red Line cable network by 1902. This network allowed news and messages to be delivered in a few minutes or several hours at most depending on the message queue's length. It spanned the globe and formed a network ring so traffic could be routed in the opposite direction in case of disruption. It was, as Dr. Michael Delaunay has argued, a highly resilient network. Besides the ring configuration, the network relied on multiple cables between any pair of given end points to ensure uptime. The British military believed it would be impossible for an enemy to cut enough cables on any route to sever all communications between any given pair of end points. The Committee of Imperial Defense concluded that 57 cables must be shut down to isolate the British Isles from the Red Line network. The figure was 15 for Canada and 7 for South Africa. The Empire was self sufficient in terms of manufacturing the components for a subsea telegraph cable and repairing it. Its navy had no peers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://subseacables.blogspot.com/2025/12/the-british-empires-resilient-subsea.html"/><published>2025-12-30T13:10:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46433352</id><title>Igniting the GPU: From Kernel Plumbing to 3D Rendering on RISC-V</title><updated>2025-12-30T20:12:14.784664+00:00</updated><content>&lt;doc fingerprint="8fddc85941f0b928"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction: Enabling the Hardware&lt;/head&gt;
    &lt;p&gt;For years, PowerVR GPUs ubiquitous in the embedded world relied entirely on out of tree vendor drivers (often named &lt;code&gt;pvrsrvkm&lt;/code&gt;). While source code was provided in Board Support Packages, these drivers were never accepted into the mainline kernel due to their non standard architecture.&lt;/p&gt;
    &lt;p&gt;That changed when Imagination Technologies announced their commitment to an upstream, open source driver. The resulting &lt;code&gt;drm/imagination&lt;/code&gt; driver has been upstream for some time, but it wasn’t usable on RISC-V platforms like the T-HEAD TH1520 (used in the Lichee Pi 4A).&lt;/p&gt;
    &lt;p&gt;This marks a significant milestone: with the enablement work described below, the TH1520 becomes the first RISC-V SoC to feature fully mainline, hardware accelerated 3D graphics support.&lt;/p&gt;
    &lt;p&gt;This effort has followed a long road of development, generating significant community interest along the way from the initial driver support discussions to the power sequencing challenges, and finally culminating in the official upstream merge in Linux 6.18.&lt;/p&gt;
    &lt;p&gt;While the GPU driver itself is generic, the hardware surrounding the GPU on this SoC specifically the power, clock, and reset controllers required significant enablement work before the GPU could actually be probed.&lt;/p&gt;
    &lt;p&gt;This post details the architectural “plumbing” required to bring up the full graphics stack on the TH1520. This involved implementing the necessary platform drivers to handle the SoC’s power sequencing, enabling the mainline &lt;code&gt;drm/imagination&lt;/code&gt; driver for RISC-V, and validating the stack with a modern, Vulkan based userspace.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1: The Dependency Chain&lt;/head&gt;
    &lt;p&gt;Enabling the GPU wasn’t just a matter of changing a Kconfig entry. The TH1520 GPU subsystem is gated behind a chain of hardware dependencies that had no existing Linux drivers.&lt;/p&gt;
    &lt;p&gt;To reach the point where I could submit the final patch enabling the PowerVR driver for RISC-V, I first had to implement and upstream the drivers for these underlying subsystems.&lt;/p&gt;
    &lt;p&gt;The hierarchy looks like this, from the bottom up:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Mailbox (&lt;code&gt;mailbox-th1520&lt;/code&gt;): The SoC uses a safety coprocessor (E902) to manage power. The first step was writing a mailbox driver to establish a physical communication link between the main CPUs and this coprocessor.&lt;/item&gt;
      &lt;item&gt;Firmware Protocol (&lt;code&gt;thead-aon-protocol&lt;/code&gt;): On top of the mailbox, I implemented the AON (Always-On) firmware protocol. This driver handles the specific message format required to request power state changes from the coprocessor.&lt;/item&gt;
      &lt;item&gt;Power Domains (&lt;code&gt;pmdomain-thead&lt;/code&gt;): With the protocol active, I could expose the GPU’s power rail as a standard Linux Generic Power Domain (GenPD). This allows the kernel to manage the GPU’s power state generically.&lt;/item&gt;
      &lt;item&gt;Resets and Clocks: Finally, I extended the clock driver (&lt;code&gt;clk-th1520-vo&lt;/code&gt;) and implemented a new reset controller (&lt;code&gt;reset-th1520&lt;/code&gt;) to handle the specific requirements of the Video Output (VO) subsystem where the GPU resides.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The Power Sequencer: A Novel Application&lt;/head&gt;
    &lt;p&gt;With the platform drivers in place, one integration challenge remained. The TH1520 requires a specific, time sensitive sequence to power up the GPU: enable the power domain, wait for voltage stabilization, and then de-assert resets in a specific order.&lt;/p&gt;
    &lt;p&gt;Historically, power sequencing in the kernel was mostly confined to the MMC/Bluetooth subsystems (for toggling GPIOs on WiFi chips). However, the kernel recently introduced a generic Power Sequencing (&lt;code&gt;pwrseq&lt;/code&gt;) subsystem (authored by Bartosz Golaszewski) to standardize this problem.&lt;/p&gt;
    &lt;p&gt;During the upstream review process, Ulf Hansson (the Power Management subsystem maintainer) suggested that the TH1520’s GPU was the perfect candidate for this new framework. It behaves almost like an external component: it needs a dedicated “manager” to orchestrate its wake-up routine before the main driver can even touch it.&lt;/p&gt;
    &lt;p&gt;I implemented this in &lt;code&gt;pwrseq-thead-gpu&lt;/code&gt;. The most interesting part of this driver is the &lt;code&gt;match&lt;/code&gt; function, which allows the sequencer to “adopt” the GPU’s resources:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;The Integration in &lt;code&gt;drm/imagination&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;To make this work, I also had to introduce a small but strategic change to the generic &lt;code&gt;drm/imagination&lt;/code&gt; driver (see commit &lt;code&gt;e38e8391f30b&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Following a suggestion from Matt Coster, I implemented a new abstraction, &lt;code&gt;pvr_power_sequence_ops&lt;/code&gt;. This interface allows the driver to select its power strategy at runtime based on the device compatible string, keeping the core driver logic generic while accommodating platform specific needs.&lt;/p&gt;
    &lt;p&gt;For the TH1520, the driver simply selects the &lt;code&gt;pwrseq&lt;/code&gt; backend:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This architecture offers three major benefits:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clean Abstraction: The GPU driver doesn’t need to know about T-HEAD’s specific reset order or microsecond delays. It simply calls the generic &lt;code&gt;pwr_ops-&amp;gt;power_on()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Inversion of Control: The sequencer “steals” the resource handles (clocks and resets) from the GPU’s device tree node during the match phase (lines 19-24 in the first snippet). This allows the sequencer to control resources that conceptually belong to the GPU, ensuring the correct power up order without modifying the GPU driver logic.&lt;/item&gt;
      &lt;item&gt;Strict Ordering: By centralizing this logic in a dedicated driver, we guarantee that the &lt;code&gt;clkgen&lt;/code&gt;reset (controlled by the parent node) and the&lt;code&gt;gpu_core&lt;/code&gt;reset (controlled by the consumer node) are de-asserted in the exact order required by the hardware manual.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Part 2: The Display Pipeline (Connecting the Pixels)&lt;/head&gt;
    &lt;p&gt;Powering up the GPU is a massive victory, but it solves only half the problem. A GPU can render beautiful 3D scenes into memory, but without a Display Controller to scan those buffers out to a screen, you’re still looking at a black terminal.&lt;/p&gt;
    &lt;p&gt;On the TH1520, the display duties are handled by a Verisilicon DC8200 IP block, connected to a Synopsys DesignWare HDMI bridge.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ecosystem Note: If you are following the RISC-V space, this IP might sound familiar. The StarFive JH7110 (used in the VisionFive 2) uses the exact same Verisilicon DC8200 display controller.&lt;/p&gt;
      &lt;p&gt;I am actually working on enabling the display stack for the JH7110 in parallel. While the IP is the same, the integration is vastly different the JH7110 has a complex circular dependency between the HDMI PHY and the clock generator that requires a complete architectural rethink. But that is a story for a future blog post.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;The Collaborative Puzzle&lt;/head&gt;
    &lt;p&gt;While I focused on the TH1520 power sequencing and GPU enablement, the display driver work here was led by Icenowy Zheng, another brilliant engineer in the RISC-V ecosystem.&lt;/p&gt;
    &lt;p&gt;This is the beauty of upstream kernel development: you don’t have to build the world alone. Icenowy has been working on a generic DRM driver for Verisilicon display controllers, adapting it to support the specific HDMI PHY found on the TH1520.&lt;/p&gt;
    &lt;p&gt;Since these patches are currently in the review process (v4), they aren’t in mainline yet. To build the working demo, I applied Icenowy’s patch series on top of mainline kernel.&lt;/p&gt;
    &lt;p&gt;With Icenowy’s display driver handling the “scan out” and my infrastructure handling the “power up,” we finally had a complete pipeline: Memory -&amp;gt; GPU Render -&amp;gt; Memory -&amp;gt; Display Controller -&amp;gt; HDMI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: The “Vulkan-Only” Future&lt;/head&gt;
    &lt;p&gt;Now that the kernel could talk to the hardware, we needed a userspace stack to render graphics.&lt;/p&gt;
    &lt;p&gt;Historically, enabling a new GPU meant writing two massive drivers for Mesa: one for Vulkan and one for OpenGL. But the open-source graphics world has shifted. The &lt;code&gt;drm/imagination&lt;/code&gt; driver is designed to be Vulkan-native.&lt;/p&gt;
    &lt;p&gt;Instead of writing a complex, legacy OpenGL driver, we use Zink.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Stack: Rendering vs. Display&lt;/head&gt;
    &lt;p&gt;Since the TH1520 uses a split DRM architecture, the flow isn’t just a straight line. The GPU and Display Controller are separate devices that share data via memory (DMA-BUF).&lt;/p&gt;
    &lt;code&gt;      [ Application (glmark2) ]
                 │
                 ▼
      [    Zink (OpenGL)      ]
                 │
                 ▼
      [ Mesa PowerVR (Vulkan) ]
                 │
      ┌──────────┴──────────┐
      │     Linux Kernel    │
      ▼                     ▼
[ GPU Driver ]       [ Display Driver ]
 (Render Node)         (KMS/Card Node)
      │                     │
      ▼        DMA-BUF      ▼
 [ GPU HW ] ──(Memory)──▶ [ Display HW ] ──▶ HDMI&lt;/code&gt;
    &lt;p&gt;This separation is why the kernel plumbing in Part 1 (GPU) and Part 2 (Display) had to be done independently before they could work together.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building the Stack (Reproduction Guide)&lt;/head&gt;
    &lt;p&gt;For those who want to reproduce this on their own Lichee Pi 4A, exact version matching is critical.&lt;/p&gt;
    &lt;p&gt;1. The Kernel I used Linux 6.19 as the base, with unmerged Display Controller patches applied on top. You can find the exact tree here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kernel Branch: &lt;code&gt;github.com/mwilczy/linux/tree/blog_code&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Mesa (Userspace) I used a fork of Icenowy Zheng’s work, which includes the necessary glue to make Zink play nicely with this specific hardware combination.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mesa Branch: &lt;code&gt;github.com/mwilczy/mesa&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here is the exact Meson configuration I used to build a pure Vulkan+Zink stack:&lt;/p&gt;
    &lt;code&gt;meson setup build \
    -D buildtype=release \
    -D platforms=x11,wayland \
    -D vulkan-drivers=imagination \
    -D gallium-drivers=zink \
    -D glx=disabled \
    -D gles1=disabled \
    -D gles2=enabled \
    -D egl=enabled \
    -D tools=imagination \
    -D glvnd=disabled&lt;/code&gt;
    &lt;head rend="h2"&gt;Part 4: The Result&lt;/head&gt;
    &lt;p&gt;With the kernel compiled (including the pending display patches) and the Mesa stack built, we can finally run accelerated 3D workloads.&lt;/p&gt;
    &lt;head rend="h3"&gt;The “Secret Sauce” (Environment Variables)&lt;/head&gt;
    &lt;p&gt;Because the driver is still in active development and not yet fully conformant, we need to pass a few flags to convince Mesa to run.&lt;/p&gt;
    &lt;p&gt;The most important one is &lt;code&gt;PVR_I_WANT_A_BROKEN_VULKAN_DRIVER=1&lt;/code&gt;. Without this, the driver safeguards would prevent loading. We also force the use of the Zink driver and explicitly select our device:&lt;/p&gt;
    &lt;code&gt;export PVR_I_WANT_A_BROKEN_VULKAN_DRIVER=1
export GALLIUM_DRIVER=zink
export MESA_VK_DEVICE_SELECT=1010:36104182!&lt;/code&gt;
    &lt;head rend="h3"&gt;The Benchmark&lt;/head&gt;
    &lt;p&gt;I started a Weston compositor session using the DRM backend:&lt;/p&gt;
    &lt;code&gt;weston --backend=drm-backend.so --continue-without-input &amp;amp;&lt;/code&gt;
    &lt;p&gt;And then, the moment of truth - running &lt;code&gt;glmark2-es2-wayland&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Above: glmark2 running on the Lichee Pi 4A.&lt;/p&gt;
    &lt;p&gt;Here is the output, confirming we are running fully accelerated on the PowerVR GPU via Zink:&lt;/p&gt;
    &lt;code&gt;root@revyos-lpi4a:~/test_mesa/Vulkan/build4/bin# glmark2-es2-wayland
MESA: warning: Core count fetching is unimplemented. Setting 1 for now.
WARNING: powervr is not a conformant Vulkan implementation, testing use only.
=======================================================
    glmark2 2023.01
=======================================================
    OpenGL Information
    GL_VENDOR:      Mesa
    GL_RENDERER:    zink Vulkan 1.2(PowerVR B-Series BXM-4-64 MC1 (IMAGINATION_OPEN_SOURCE_MESA))
    GL_VERSION:     OpenGL ES 2.0 Mesa 26.0.0-devel (git-601d20e81e)
    Surface Config: buf=32 r=8 g=8 b=8 a=8 depth=24 stencil=0 samples=0
    Surface Size:   800x600 windowed
=======================================================
[build] use-vbo=false:[  510.861554] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 524288 bytes), total 32768 (slots), used 4 (slots)
[  510.887018] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1847296 bytes), total 32768 (slots), used 36 (slots)
[  510.900771] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1847296 bytes), total 32768 (slots), used 36 (slots)
[  510.923956] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1921024 bytes), total 32768 (slots), used 0 (slots)
[  510.954656] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 458752 bytes), total 32768 (slots), used 0 (slots)
[  510.966931] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 458752 bytes), total 32768 (slots), used 0 (slots)
[  511.038586] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1921024 bytes), total 32768 (slots), used 0 (slots)
[  512.165193] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1900544 bytes), total 32768 (slots), used 10 (slots)
[  512.187871] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1900544 bytes), total 32768 (slots), used 10 (slots)
[  512.209524] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 745472 bytes), total 32768 (slots), used 0 (slots)
 FPS: 67 FrameTime: 14.960 ms
[build] use-vbo=true: FPS: 98 FrameTime: 10.252 ms
[texture] texture-filter=nearest: FPS: 97 FrameTime: 10.332 ms
[texture] texture-filter=linear: FPS: 93 FrameTime: 10.868 ms
[texture] texture-filter=mipmap: FPS: 101 FrameTime: 9.957 ms
[shading] shading=gouraud: FPS: 93 FrameTime: 10.851 ms
[shading] shading=blinn-phong-inf: FPS: 98 FrameTime: 10.274 ms
[shading] shading=phong: FPS: 97 FrameTime: 10.356 ms
[shading] shading=cel: FPS: 91 FrameTime: 11.086 ms
[bump] bump-render=high-poly: FPS: 75 FrameTime: 13.404 ms
[bump] bump-render=normals: FPS: 97 FrameTime: 10.356 ms
[bump] bump-render=height: FPS: 88 FrameTime: 11.449 ms
[effect2d] kernel=0,1,0;1,-4,1;0,1,0;: FPS: 94 FrameTime: 10.646 ms
[effect2d] kernel=1,1,1,1,1;1,1,1,1,1;1,1,1,1,1;: FPS: 57 FrameTime: 17.590 ms
[pulsar] light=false:quads=5:texture=false: FPS: 92 FrameTime: 10.953 ms
[desktop] blur-radius=5:effect=blur:passes=1:separable=true:windows=4: FPS: 10 FrameTime: 105.957 ms
[desktop] effect=shadow:windows=4: FPS: 37 FrameTime: 27.465 ms

...&lt;/code&gt;
    &lt;p&gt;We have successfully turned “dark silicon” into a modern, Vulkan capable graphics platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Bringing a new GPU architecture to life in the mainline kernel is never a solo effort. It requires navigating complex subsystems - from power domains to clocks and relies heavily on the patience and expertise of subsystem maintainers.&lt;/p&gt;
    &lt;p&gt;This work went through many iterations, and the code is significantly better thanks to the rigorous feedback from the community.&lt;/p&gt;
    &lt;p&gt;A huge thank you to everyone who helped review the code, suggested architectural improvements, and tested the stack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Marek Szyprowski - For the guidance and mentorship throughout the upstreaming process.&lt;/item&gt;
      &lt;item&gt;Drew Fustini - For his long standing work maintaining the TH1520 platform.&lt;/item&gt;
      &lt;item&gt;Krzysztof Kozłowski - For ensuring the Device Tree bindings were strictly compliant.&lt;/item&gt;
      &lt;item&gt;Ulf Hansson - For his guidance on the AON power domains and for suggesting the use of the Power Sequencing framework, which simplified the architecture significantly.&lt;/item&gt;
      &lt;item&gt;Bartosz Gołaszewski - For creating the Power Sequencing subsystem and helping merge the TH1520 driver.&lt;/item&gt;
      &lt;item&gt;Matt Coster - For reviewing the driver changes and helping navigate the PowerVR internals.&lt;/item&gt;
      &lt;item&gt;Stephen Boyd - For the feedback on the video output clock controller.&lt;/item&gt;
      &lt;item&gt;Philipp Zabel - For reviewing the reset controller implementation.&lt;/item&gt;
      &lt;item&gt;Icenowy Zheng - For the incredible work on the display controller and Mesa/Zink integration.&lt;/item&gt;
      &lt;item&gt;Jassi Brar - For reviewing the mailbox driver implementation.&lt;/item&gt;
      &lt;item&gt;Conor Dooley - For reviewing Device Tree patches.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mwilczynski.dev/posts/riscv-gpu-zink/"/><published>2025-12-30T13:55:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46433661</id><title>Hive (YC S14) Is Hiring a Staff Software Engineer (Data Systems)</title><updated>2025-12-30T20:12:14.354044+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/hive.co/cb0dc490-0e32-4734-8d91-8b56a31ed497"/><published>2025-12-30T14:31:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46433915</id><title>What Happened to Abit Motherboards</title><updated>2025-12-30T20:12:14.157142+00:00</updated><content>&lt;doc fingerprint="39579387f502b633"&gt;
  &lt;main&gt;
    &lt;p&gt;At the end of the year in 2008, one of the most legendary motherboard manufacturers of all time sadly went out of business. I am talking about Abit. What happened to Abit motherboards? A combination of factors took it down, including declining quality, loss of a key engineer, and a good old-fashioned scandal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Abit took 7 years to become an overnight sensation&lt;/head&gt;
    &lt;p&gt;Abit wasn’t exactly a newcomer when I first learned about them in 1996. The company was founded in 1989 and made a number of 386SX, 386DX, and 486 motherboards. But it was the early hardware sites like Tom’s Hardware Guide and Anandtech that really helped to put Abit on the map during the Socket 7 era and distinguish them from the rest of the Taiwainese motherboard makers. The Abit IT5H was a Socket 7 board based on the Intel 430HX chipset that performed extremely well.&lt;/p&gt;
    &lt;head rend="h3"&gt;The jumperless Abit IT5H&lt;/head&gt;
    &lt;p&gt;Thing is, we already had an HX-based board that performed really well. Asus had those bases covered with its P55T2P4. What made Abit special was its board was jumperless. When you installed a processor, it initialized it using safe settings, and then you could go in and configure it to run at the speed you wanted using a feature called the CPU Softmenu. The Softmenu allowed you to change voltages and front side bus speed, not just the multiplier. You could even run your CPU at non-standard bus speeds like 75 or 83 MHz. Running a 166 MHz CPU at 83 MHz with a 2X multiplier actually ran faster than a 200 MHz CPU on a 66 MHz bus with a 3X multiplier. If you actually owned a 200 MHz processor, or a processor that overclocked well, you could run it at 83 MHz with a multiplier of 2.5, reach 208 MHz, and run rings around a CPU running on a 66 MHz bus with a 3x multiplier. It was the ultimate Socket 7 system at the time.&lt;/p&gt;
    &lt;p&gt;Overclockers loved the IT5H because they could easily test settings without looking up jumper settings and changing clumsy jumper blocks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Abit BP6: Dual CPUs on a budget&lt;/head&gt;
    &lt;p&gt;And then there was the legendary Abit BP6. Socket 370 era Celeron processors had a Pentium II core, but Intel disabled the ability to change the multiplier to discourage overclocking and they also disabled the ability to run them in multi-processor configurations. Enthusiasts figured out that if they wired the processors up a bit differently, they could restore the multiprocessor capability. With the BP6, Abit made that unnecessary. They just wired the board up so that you could drop a pair of cheap Celeron processors into it and have a very inexpensive dual CPU setup.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened to Abit to cause its demise?&lt;/head&gt;
    &lt;p&gt;One major problem for Abit was the quality of the capacitors they used was not as high as Asus. That meant Abit motherboards didn’t age as well as Asus boards did. Arguably, in the ’90s, that wasn’t as huge of a problem because enthusiasts would upgrade every 2 or 3 years. As long as the board lasted 3 years, nobody noticed. But as the century turned, people started expecting to be able to keep their computers a little bit longer. Abit’s propensity to go cheap on the capacitors left it extremely vulnerable when capacitor plague kicked in, and indeed, Abit was one of the hardest hit.&lt;/p&gt;
    &lt;p&gt;Starting in 2002, Abit started outsourcing production of some low end boards to Elite Computer Systems, a notorious cost-cutting manufacturer. You bought from companies like Abit to avoid accidentally buying a no-name board actually made by ECS. So this was problematic.&lt;/p&gt;
    &lt;p&gt;Abit suffered a major blow in March 2003 when Oscar Wu, the mastermind behind the CPU Softmenu and much of the hardware design, departed Abit for rival motherboard maker DFI.&lt;/p&gt;
    &lt;p&gt;But perhaps the biggest problem came in December 2004, when questionable accounting practices caused its stock to be delisted. Abit had been inflating its counts and potentially embezzling funds. It wasn’t quite Miniscribe or Media Vision, let alone Worldcom. But adding fraud and dishonesty to a reputation for declining quality isn’t a recipe for longevity.&lt;/p&gt;
    &lt;p&gt;On 25 January 2006, Abit sold itself to Universal Scientific Industrial. USI sold motherboards under the new brand name Universal Abit. But the venture wasn’t successful, and Universal Abit announced that it would close December 31, 2008, and officially cease to exist on January 1, 2009.&lt;/p&gt;
    &lt;head rend="h3"&gt;Abit’s legacy&lt;/head&gt;
    &lt;p&gt;Today, Abit motherboards are prized by collectors, but if you want to actually use them, you will need to replace the capacitors. That said, if you use high quality, brand name capacitors, the boards will perform. Likely they’ll do better than they did when they were new, since the classic-era Abit boards tended to be really well built. It’s unfortunate that Abit cheaped out on the capacitors.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dfarq.homeip.net/what-happened-to-abit-motherboards/"/><published>2025-12-30T14:58:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46434580</id><title>Show HN: Tidy Baby is a SET game but with words</title><updated>2025-12-30T20:12:13.719560+00:00</updated><content>&lt;doc fingerprint="1e8bfdcd154f754e"&gt;
  &lt;main&gt;
    &lt;p&gt;If you know how to play SET you basically know how to play Tidy Baby — the "dimensions" are just:&lt;/p&gt;
    &lt;p&gt;If you don't know how to play SET, we recommend checking out the How To Play page.&lt;/p&gt;
    &lt;p&gt;Make sets. Clean board.&lt;/p&gt;
    &lt;p&gt;No sets guessed yet...&lt;/p&gt;
    &lt;p&gt;Final Score:&lt;/p&gt;
    &lt;p&gt;Total Time:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tidy.baby"/><published>2025-12-30T15:57:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46435308</id><title>Show HN: 22 GB of Hacker News in SQLite</title><updated>2025-12-30T20:12:13.453074+00:00</updated><content>&lt;doc fingerprint="60e736960d5d7ecf"&gt;
  &lt;main&gt;
    &lt;p&gt;Hacker Book new | front | start | ask | show | jobs | query Someday, Month 00, 0000 &amp;lt; &amp;gt; ARCHIVE Loading… Y Combinator | Apply | Companies | Blog | Live HN | Contact &amp;lt; &amp;gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hackerbook.dosaygo.com"/><published>2025-12-30T17:01:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46435418</id><title>Toro: Deploy Applications as Unikernels</title><updated>2025-12-30T20:12:12.978375+00:00</updated><content>&lt;doc fingerprint="3069f0a7771d385"&gt;
  &lt;main&gt;
    &lt;p&gt;Toro is a unikernel dedicated to deploy applications as microVMs. Toro leverages on virtio-fs and virtio-vsocket to provide a minimalistic architecture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support x86-64 architecture&lt;/item&gt;
      &lt;item&gt;Support up to 512GB of RAM&lt;/item&gt;
      &lt;item&gt;Support QEMU-KVM microvm and Firecracker&lt;/item&gt;
      &lt;item&gt;Cooperative and I/O bound threading scheduler&lt;/item&gt;
      &lt;item&gt;Support virtio-vsocket for networking&lt;/item&gt;
      &lt;item&gt;Support virtio-fs for filesystem&lt;/item&gt;
      &lt;item&gt;Fast boot up&lt;/item&gt;
      &lt;item&gt;Tiny image&lt;/item&gt;
      &lt;item&gt;Built-in gdbstub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can try Toro by running the HelloWorld example using a Docker image that includes all the required tools. To do so, execute the following commands in a console (these steps require you to install before KVM and Docker):&lt;/p&gt;
    &lt;code&gt;wget https://raw.githubusercontent.com/torokernel/torokernel/master/ci/Dockerfile
sudo docker build -t torokernel-dev .
sudo docker run --privileged --rm -it torokernel-dev
cd examples/HelloWorld
python3 ../CloudIt.py -a HelloWorld&lt;/code&gt;
    &lt;p&gt;If these commands execute successfully, you will get the output of the HelloWorld example. You can also pull the image from dockerhub instead of building it:&lt;/p&gt;
    &lt;code&gt;sudo docker pull torokernel/torokernel-dev:latest
sudo docker run --privileged --rm -it torokernel/torokernel-dev:latest&lt;/code&gt;
    &lt;p&gt;You can share a directory from the host by running:&lt;/p&gt;
    &lt;code&gt;sudo docker run --privileged --rm --mount type=bind,source="$(pwd)",target=/root/torokernel -it torokernel/torokernel-dev:latest&lt;/code&gt;
    &lt;p&gt;You will find $pwd from host at &lt;code&gt;/root/torokernel&lt;/code&gt; in the container.&lt;/p&gt;
    &lt;p&gt;Execute the commands in &lt;code&gt;ci/Dockerfile&lt;/code&gt; to install the required components locally. Then, Go to &lt;code&gt;torokernel/examples&lt;/code&gt; and edit &lt;code&gt;CloudIt.py&lt;/code&gt; to set the correct paths to Qemu and fpc. Optionally, you can install vsock-socat from here and virtio-fs from here. You need to set the correct path to virtiofsd and socat.&lt;/p&gt;
    &lt;p&gt;Go to &lt;code&gt;examples/HelloWorld/&lt;/code&gt; and execute:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a HelloWorld&lt;/code&gt;
    &lt;p&gt;To run the StaticWebserver, you require virtiofsd and socat. To compile socat, execute the following commands:&lt;/p&gt;
    &lt;code&gt;git clone git@github.com:stefano-garzarella/socat-vsock.git
cd socat-vsock
autoreconf -fiv
./configure
make socat&lt;/code&gt;
    &lt;p&gt;Set the path to socat binary in CloudIt.py and then execute:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a StaticWebServer -r -d /path-to-directory/ -f 4000:80&lt;/code&gt;
    &lt;p&gt;You have to replace the &lt;code&gt;/path-to-directory/&lt;/code&gt; to a directory that containing the files, e.g., index.html. To try it, you can execute:&lt;/p&gt;
    &lt;code&gt;wget http://127.0.0.1:4000/index.html
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-f&lt;/code&gt; parameter indicates a forwarding of the 4000 port from the host to the 80 port in the guest using vsock.&lt;/p&gt;
    &lt;p&gt;This example shows how cores can communicate by using the VirtIOBus device. In this example, core #0 sends a packet to every core in the system with the ping string. Each core responds with a packet that contains the message pong. This example is configured to use three cores. To launch it, simply executes the following commands in the context of the container presented above:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a InterCoreComm&lt;/code&gt;
    &lt;p&gt;You will get the following output:&lt;/p&gt;
    &lt;p&gt;You have many ways to contribute to Toro. One of them is by joining the Google Group here. In addition, you can find more information here.&lt;/p&gt;
    &lt;p&gt;GPLv3&lt;/p&gt;
    &lt;p&gt;[0] A Dedicated Kernel named Toro. Matias Vara. FOSDEM 2015.&lt;/p&gt;
    &lt;p&gt;[1] Reducing CPU usage of a Toro Appliance. Matias Vara. FOSDEM 2018.&lt;/p&gt;
    &lt;p&gt;[2] Toro, a Dedicated Kernel for Microservices. Matias Vara and Cesar Bernardini. Open Source Summit Europe 2018.&lt;/p&gt;
    &lt;p&gt;[3] Speeding Up the Booting Time of a Toro Appliance. Matias Vara. FOSDEM 2019.&lt;/p&gt;
    &lt;p&gt;[4] Developing and Deploying Microservices with Toro Unikernel. Matias Vara. Open Source Summit Europe 2019.&lt;/p&gt;
    &lt;p&gt;[5] Leveraging Virtio-fs and Virtio-vsocket in Toro Unikernel. Matias Vara. DevConfCZ 2020.&lt;/p&gt;
    &lt;p&gt;[6] Building a Cloud Infrastructure to Deploy Microservices as Microvm Guests. Matias Vara. KVM Forum 2020.&lt;/p&gt;
    &lt;p&gt;[7] Running MPI applications on Toro unikernel. Matias Vara. FOSDEM 2023.&lt;/p&gt;
    &lt;p&gt;[8] Is Toro unikernel faster for MPI?. Matias Vara. FOSDEM 2024.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/torokernel/torokernel"/><published>2025-12-30T17:09:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46435614</id><title>A Vulnerability in Libsodium</title><updated>2025-12-30T20:12:12.258877+00:00</updated><content>&lt;doc fingerprint="1c17b053a8636291"&gt;
  &lt;main&gt;
    &lt;p&gt;Libsodium is now 13 years old!&lt;/p&gt;
    &lt;p&gt;I started that project to pursue Dan Bernsteinâs desire to make cryptography simple to use. That meant exposing a limited set of high-level functions and parameters, providing a simple API, and writing documentation for users, not cryptographers. Libsodiumâs goal was to expose APIs to perform operations, not low-level functions. Users shouldnât even have to know or care about what algorithms are used internally. This is how Iâve always viewed libsodium.&lt;/p&gt;
    &lt;p&gt;Never breaking the APIs is also something Iâm obsessed with. APIs may not be great, and if I could start over from scratch, I would have made them very different, but as a developer, the best APIs are not the most beautifully designed ones, but the ones that you donât have to worry about because they donât change and upgrades donât require any changes in your application either. Libsodium started from the NaCl API, and still adheres to it.&lt;/p&gt;
    &lt;p&gt;These APIs exposed high-level functions, but also some lower-level functions that high-level functions wrap or depend on. Over the years, people started using these low-level functions directly. Libsodium started to be used as a toolkit of algorithms and low-level primitives.&lt;/p&gt;
    &lt;p&gt;That made me sad, especially since it is clearly documented that only APIs from builds with &lt;code&gt;--enable-minimal&lt;/code&gt; are guaranteed to be tested and stable. But after all, it makes sense. When building custom protocols, having a single portable library with a consistent interface for different functions is far better than importing multiple dependencies, each with their own APIs and sometimes incompatibilities between them.&lt;/p&gt;
    &lt;p&gt;Thatâs a lot of code to maintain. It includes features and target platforms I donât use but try to support for the community. I also maintain a large number of other open source projects.&lt;/p&gt;
    &lt;p&gt;Still, the security track record of libsodium is pretty good, with zero CVEs in 13 years even though it has gotten a lot of scrutiny.&lt;/p&gt;
    &lt;p&gt;However, while recently experimenting with adding support for batch signatures, I noticed inconsistent results with code originally written in Zig. The culprit was a check that was present in a function in Zig, but that I forgot to add in libsodium.&lt;/p&gt;
    &lt;head rend="h2"&gt;The bug&lt;/head&gt;
    &lt;p&gt;The function &lt;code&gt;crypto_core_ed25519_is_valid_point()&lt;/code&gt;, a low-level function used to check if a given elliptic curve point is valid, was supposed to reject points that arenât in the main cryptographic group, but some points were slipping through.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this matter?&lt;/head&gt;
    &lt;p&gt;Edwards25519 is like a special mathematical playground where cryptographic operations happen.&lt;/p&gt;
    &lt;p&gt;It is used internally for Ed25519 signatures, and includes multiple subgroups of different sizes (order):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Order 1: just the identity (0, 1)&lt;/item&gt;
      &lt;item&gt;Order 2: identity + point (0, -1)&lt;/item&gt;
      &lt;item&gt;Order 4: 4 points&lt;/item&gt;
      &lt;item&gt;Order 8: 8 points&lt;/item&gt;
      &lt;item&gt;Order L: the âmain subgroupâ (L = ~2^252 points) where all operations are expected to happen&lt;/item&gt;
      &lt;item&gt;Order 2L, 4L, 8L: very large, but not prime order subgroups&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The validation function was designed to reject points not in the main subgroup. It properly rejected points in the small-order subgroups, but not points in the mixed-order subgroups.&lt;/p&gt;
    &lt;head rend="h2"&gt;What went wrong technically?&lt;/head&gt;
    &lt;p&gt;To check if a point is in the main subgroup (the one of order L), the function multiplies it by L. If the order is L, multiplying any point by L gives the identity point (the mathematical equivalent of zero). So, the code does the multiplication and checks that we ended up with the identity point.&lt;/p&gt;
    &lt;p&gt;Points are represented by coordinates. In the internal representation used here, there are three coordinates: X, Y, and Z. The identity point is represented internally with coordinates where X = 0 and Y = Z. Z can be anything depending on previous operations; it doesnât have to be 1.&lt;/p&gt;
    &lt;p&gt;The old code only checked X = 0. It forgot to verify Y = Z. This meant some invalid points (where X = 0 but Y â Z after the multiplication) were incorrectly accepted as valid.&lt;/p&gt;
    &lt;p&gt;Concretely: take any main-subgroup point Q (for example, the output of &lt;code&gt;crypto_core_ed25519_random&lt;/code&gt;) and add the order-2 point (0, -1), or equivalently negate both coordinates. Every such Q + (0, -1) would have passed validation before the fix, even though itâs not in the main subgroup.&lt;/p&gt;
    &lt;head rend="h2"&gt;The fix&lt;/head&gt;
    &lt;p&gt;The fix is trivial and adds the missing check:&lt;/p&gt;
    &lt;code&gt;// OLD:
return fe25519_iszero(pl.X);
&lt;/code&gt;
    &lt;code&gt;// NEW:
fe25519_sub(t, pl.Y, pl.Z);
return fe25519_iszero(pl.X) &amp;amp; fe25519_iszero(t);
&lt;/code&gt;
    &lt;p&gt;Now it properly verifies both conditions: X must be zero and Y must equal Z.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who is affected?&lt;/head&gt;
    &lt;p&gt;You may be affected if you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a point release &amp;lt;= &lt;code&gt;1.0.20&lt;/code&gt;or a version of&lt;code&gt;libsodium&lt;/code&gt;released before December 30, 2025.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;crypto_core_ed25519_is_valid_point()&lt;/code&gt;to validate points from untrusted sources&lt;/item&gt;
      &lt;item&gt;Implement custom cryptography using arithmetic over the Edwards25519 curve&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But donât panic. Most users are not affected.&lt;/p&gt;
    &lt;p&gt;None of the high-level APIs (&lt;code&gt;crypto_sign_*&lt;/code&gt;) are affected; they donât even use or need that function. Scalar multiplication using &lt;code&gt;crypto_scalarmult_ed25519&lt;/code&gt; wonât leak anything even if the public key is not on the main subgroup. And public keys created with the regular &lt;code&gt;crypto_sign_keypair&lt;/code&gt; and &lt;code&gt;crypto_sign_seed_keypair&lt;/code&gt; functions are guaranteed to be on the correct subgroup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommendation&lt;/head&gt;
    &lt;p&gt;Support for the Ristretto255 group was added to libsodium in 2019 specifically to solve cofactor-related issues. With Ristretto255, if a point decodes, itâs safe. No further validation is required.&lt;/p&gt;
    &lt;p&gt;If you implement custom cryptographic schemes doing arithmetic over a finite field group, using Ristretto255 is recommended. Itâs easier to use, and as a bonus, low-level operations will run faster than over Edwards25519.&lt;/p&gt;
    &lt;p&gt;If you canât update libsodium and need an application-level workaround, use the following function:&lt;/p&gt;
    &lt;code&gt;int is_on_main_subgroup(const unsigned char p[crypto_core_ed25519_BYTES])
{
    /* l - 1 (group order minus 1) */
    static const unsigned char L_1[crypto_core_ed25519_SCALARBYTES] = {
        0xec, 0xd3, 0xf5, 0x5c, 0x1a, 0x63, 0x12, 0x58,
        0xd6, 0x9c, 0xf7, 0xa2, 0xde, 0xf9, 0xde, 0x14,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x10
    };
    /* Identity point encoding: (x=0, y=1) */
    static const unsigned char ID[crypto_core_ed25519_BYTES] = {
        0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
    };
    unsigned char t[crypto_core_ed25519_BYTES];
    unsigned char r[crypto_core_ed25519_BYTES];
    if (crypto_scalarmult_ed25519_noclamp(t, L_1, p) != 0 ||
        crypto_core_ed25519_add(r, t, p) != 0) {
        return 0;
    }
    return sodium_memcmp(r, ID, sizeof ID) == 0;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Fixed packages&lt;/head&gt;
    &lt;p&gt;This issue was fixed immediately after discovery. All &lt;code&gt;stable&lt;/code&gt; packages released after December 30, 2025 include the fix:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;official tarballs&lt;/item&gt;
      &lt;item&gt;binaries for Visual Studio&lt;/item&gt;
      &lt;item&gt;binaries for MingW&lt;/item&gt;
      &lt;item&gt;NuGet packages for all architectures including Android&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;swift-sodium&lt;/code&gt;xcframework (but&lt;code&gt;swift-sodium&lt;/code&gt;doesnât expose low-level functions anyway)&lt;/item&gt;
      &lt;item&gt;Rust &lt;code&gt;libsodium-sys-stable&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;libsodium.js&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A new point release is also going to be tagged.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;libsodium&lt;/code&gt; is useful to you, please keep in mind that it is maintained by one person, for free, in time I could spend with my family or on other projects. The best way to help the project would be to consider sponsoring it, which helps me dedicate more time to improving it and making it great for everyone, for many more years to come.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://00f.net/2025/12/30/libsodium-vulnerability/"/><published>2025-12-30T17:24:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436127</id><title>Electrolysis can solve one of our biggest contamination problems</title><updated>2025-12-30T20:12:11.337327+00:00</updated><content>&lt;doc fingerprint="cb108501eca1c785"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Electrolysis can solve one of our biggest contamination problems&lt;/head&gt;
    &lt;p&gt;ETH Zurich researchers have developed a process that can be used on site to render environmental toxins such as DDT and lindane harmless and convert them into valuable chemicals – a breakthrough for the remediation of contaminated sites and a sustainable circular economy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read&lt;/item&gt;
      &lt;item&gt;Number of comments&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;In brief&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Persistent organic pollutants such as DDT and lindane still pollute the environment and affect humans decades after their use.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ETH researchers have developed a new electrochemical process that completely dehalogenates these long-lived toxins and converts them into valuable industrial chemicals.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The method uses cheap equipment, prevents side reactions and could be used on contaminated landfills, soils or sludge.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mobile systems could be used on site in the future – an important step towards the remediation of contaminated sites and the creation of a sustainable circular economy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They were once considered miracle workers – insecticides such as lindane or DDT were produced and used millions of times during the 20th century. But what was hailed as progress led to a global environmental catastrophe: persistent organic pollutants (POPs) are so chemically stable that they remain in soil, water and organisms for decades. They accumulate in the fatty tissue of animals and thus enter the human food chain. Many of these substances were banned long ago, but their traces can still be found today – even in human blood.&lt;/p&gt;
    &lt;p&gt;How to remediate such contaminated sites, be they soils, bodies of water or landfills, is one of the major unresolved questions of environmental protection. How can highly stable poisons be rendered harmless without creating new problems? Researchers at ETH Zurich, led by Bill Morandi, Professor of Synthetic Organic Chemistry, have now found a promising approach. Using an innovative electrochemical method, they are not only able to break down these long-lived pollutants but also to convert them into valuable raw materials for the chemical industry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Converting pollutants into raw materials&lt;/head&gt;
    &lt;p&gt;A key distinction between this and previous work is that the carbon skeleton of the pollutants is recycled and made reusable, while the halide component is sequestered as a harmless inorganic salt. “The previous methods were also energetically inefficient,” says Patrick Domke, a doctoral student in Morandi’s group. He explains: “The processes were expensive and still led to outcomes that were harmful to the environment.”&lt;/p&gt;
    &lt;p&gt;Together with electrochemistry specialist Alberto Garrido-Castro, a former postdoc in this group, Domke developed a process that renders the pollutants in question completely harmless. During this project, the two researchers were able to draw on the many years of experience of ETH professor Morandi, who has been working on the transformation of such compounds for years. “The key advance of this new technology is the use of alternating current to sequester the problematic halogen atoms as innocuous salts such as NaCl (table salt), while still generating valuable hydrocarbons,” says Morandi.&lt;/p&gt;
    &lt;head rend="h2"&gt;Using electricity to break down toxins&lt;/head&gt;
    &lt;p&gt;Electrolysis enables almost complete dehalogenation of pollutants under mild, environmentally friendly and cost-effective conditions. It cleaves the stable carbon-halogen bonds, leaving behind only harmless salts such as table salt and useful hydrocarbons such as benzene, diphenylethane or cyclododecatriene. These are actually sought-after intermediates in the chemical industry, for example, for plastics, varnishes, coatings and pharmaceutical applications. In this way, the technology not only contributes to the remediation of contaminated sites but also to the sustainable circular economy.&lt;/p&gt;
    &lt;p&gt;“What makes our process so special from a technical point of view is that we supply electricity using alternating current, similar to the electrical waveform delivered to households. It is one of the most cost-effective resources in chemistry,” explains Garrido-Castro. “Alternating current protects the electrodes from wear, which is why we can reuse them for many subsequent electrolysis cycles. In addition, the alternating current suppresses unwanted side reactions and the formation of poisonous chlorine gas, allowing the pollutant’s halogen atoms to be fully converted to inorganic salts.” The reactor used by the researchers consists of an undivided electrolysis cell in which dimethyl sulfoxide (DMSO) is used as a solvent – itself a by-product of the pulp process in paper production.&lt;/p&gt;
    &lt;head rend="h2"&gt;A fully thought-out circular economy&lt;/head&gt;
    &lt;p&gt;The process can be applied not only to pure substances but also to mixtures from contaminated soils. Soil or sludge can therefore be treated without pre-treatment or further separation processes. A prototype of the reactor has already been successfully tested on classic environmental toxins such as lindane and DDT. “Our system is mobile and can be assembled on site. This eliminates the need to transport these hazardous substances,” explains Domke.&lt;/p&gt;
    &lt;quote&gt;“Our motivation was to solve one of the biggest environmental problems of the last century. We cannot simply leave the pollution to future generations.”Alberto Garrido-Castro&lt;/quote&gt;
    &lt;head rend="h2"&gt;Spark Award 2025 – these projects have made it to the finals&lt;/head&gt;
    &lt;p&gt;On 27 November 2025 at ETH Zurich @ Open-i, ETH Zurich will award the Spark Award for the best invention of the year for the 14th time. The criteria for this award are originality, patent strength and market potential.&lt;/p&gt;
    &lt;p&gt;Click here to find all the Spark Award nominees of 2025.&lt;/p&gt;
    &lt;p&gt;Spark Award ceremony, Industry Day @ Open-i, Thursday, 27 November 2025, 1.30 p.m., Zurich Convention Center. Registration is required.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ethz.ch/en/news-and-events/eth-news/news/2025/11/electrolysis-can-solve-one-of-our-biggest-contamination-problems.html"/><published>2025-12-30T18:08:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436128</id><title>Show HN: I remade my website in the Sith Lord Theme and I hope it's fun</title><updated>2025-12-30T20:12:10.948703+00:00</updated><content>&lt;doc fingerprint="b9ae31118bfb75fd"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Latest Modified Projects&lt;/head&gt;
    &lt;head rend="h4"&gt;RogueBerry One&lt;/head&gt;
    &lt;p&gt; RogueBerry One - KDE Plasma Mobile based OS for the Hackberry Pi &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
    &lt;head rend="h4"&gt;Forensics Tools&lt;/head&gt;
    &lt;p&gt; Forensics Tools - Cookie's Forensics Tools &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
    &lt;head rend="h4"&gt;Gooey Framework&lt;/head&gt;
    &lt;p&gt; Gooey - Opinionated WebASM Bindings and Web Components Framework &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
    &lt;head rend="h4"&gt;ZIMdex&lt;/head&gt;
    &lt;p&gt; ZIMdex - Self-hostable Offline ZIM search and web spider &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
    &lt;head rend="h4"&gt;Gooey CLI&lt;/head&gt;
    &lt;p&gt; Gooey CLI - CLI Wizard for easier Gooey project creation and management &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cookie.engineer/index.html"/><published>2025-12-30T18:08:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436319</id><title>Funes.world</title><updated>2025-12-30T20:12:10.678575+00:00</updated><content>&lt;doc fingerprint="59ef33aded729ccf"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Preserve Our &lt;lb/&gt;Physical World &lt;lb/&gt;Together&lt;/head&gt;
    &lt;p&gt;From ruins to castles, we record what human hands have &lt;lb/&gt; shaped. Funes builds an ever-growing 3D archive — a &lt;lb/&gt; digital memory of our physical world.&lt;/p&gt;
    &lt;p&gt;Recent Models&lt;/p&gt;
    &lt;p&gt;Funes has collected — models &lt;lb/&gt;in — countries and areas.&lt;/p&gt;
    &lt;p&gt;in — countries and areas.&lt;/p&gt;
    &lt;p&gt;Get involved with Funes&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://funes.world"/><published>2025-12-30T18:28:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436409</id><title>A faster heart for F-Droid. Our new server is here</title><updated>2025-12-30T20:12:10.031520+00:00</updated><content>&lt;doc fingerprint="fdf7b599e0066ed7"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;A faster heart for F-Droid. Our new server is here!&lt;/head&gt;Posted on Dec 30, 2025 by F-Droid&lt;p&gt;Donations are a key part of what keeps F-Droid independent and reliable and our latest hardware update is a direct result of your support. Thanks to donations from our incredible community, F-Droid has replaced one of its most critical pieces of infrastructure, our core server hardware. It was overdue for a refresh, and now we are happy to give you an update on the new server and how it impacts the project.&lt;/p&gt;&lt;p&gt;This upgrade touches a core part of the infrastructure that builds and publishes apps for the main F-Droid repository. If the server is slow, everything downstream gets slower too. If it is healthy, the entire ecosystem benefits.&lt;/p&gt;&lt;head rend="h2"&gt;Why did we wait?&lt;/head&gt;&lt;p&gt;This server replacement took a bit longer than we would have liked. The biggest reason is that sourcing reliable parts right now is genuinely hard. Ongoing global trade tensions have made supply chains unpredictable, and that hit the specific components we needed. We had to wait for quotes, review, replan, and wait again when quotes turned out to have unexpected long waits, before we finally managed to receive hardware that met our requirements.&lt;/p&gt;&lt;p&gt;Even with the delays, the priority never changed. We were looking for the right server set up for F-Droid, built to last for the long haul.&lt;/p&gt;&lt;head rend="h2"&gt;A note about the host&lt;/head&gt;&lt;p&gt;Another important part of this story is where the server lives and how it is managed. F-Droid is not hosted in just any data center where commodity hardware is managed by some unknown staff. We worked out a special arrangement so that this server is physically held by a long time contributor with a proven track record of securely hosting services. We can control it remotely, we know exactly where it is, and we know who has access. That level of transparency and trust is not common in infrastructure, but it is central to how we think about resilience and stewardship.&lt;/p&gt;&lt;p&gt;This was not the easiest path, and it required careful coordination and negotiation. But we are glad we did it this way. It fits our values and our threat model, and it keeps the project grounded in real people rather than anonymous systems.&lt;/p&gt;&lt;head rend="h2"&gt;Old hardware, new momentum&lt;/head&gt;&lt;p&gt;The previous server was 12 year old hardware and had been running for about five years. In infrastructure terms, that is a lifetime. It served F-Droid well, but it was reaching the point where speed and maintenance overhead were becoming a daily burden.&lt;/p&gt;&lt;p&gt;The new system is already showing a huge improvement. Stats of the running cycles from the last two months suggest it can handle the full build and publish actions much faster than before. E.g. this year, between January and September, we published updates once every 3 or 4 days, that got down to once every 2 days in October, to every day in November and itâs reaching twice a day in December. (You can see this in the frequency of index publishing after October 18, 2025 in our f-droid.org transparency log). That extra capacity gives us more breathing room and helps shorten the gap between when apps are updated and when those updates reach users. We can now build all the auto-updated apps in the (UTC) morning in one cycle, and all the newly included apps, fixed apps and manually updated apps, through the day, in the evening cycle.&lt;/p&gt;&lt;p&gt;We are being careful here, because real world infrastructure always comes with surprises. But the performance gains are real, and they are exciting.&lt;/p&gt;&lt;head rend="h2"&gt;What donations make possible&lt;/head&gt;&lt;p&gt;This upgrade exists because of community support, pooled over time, turned into real infrastructure, benefiting everyone who relies on F-Droid.&lt;/p&gt;&lt;p&gt;A faster server does not just make our lives easier. It helps developers get timely builds. It reduces maintenance risk. It strengthens the health of the entire repository.&lt;/p&gt;&lt;p&gt;So thank you. Every donation, whether large or small, is part of how this project stays reliable, independent, and aligned with free software values.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://f-droid.org/2025/12/30/a-faster-heart-for-f-droid.html"/><published>2025-12-30T18:36:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436889</id><title>FediMeteo: A €4 FreeBSD VPS Became a Global Weather Service</title><updated>2025-12-30T20:12:09.409164+00:00</updated><content>&lt;doc fingerprint="b20d9871206db6ab"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Personal Introduction&lt;/head&gt;
    &lt;p&gt;Weather has always significantly influenced my life. When I was a young athlete, knowing the forecast in advance would have allowed me to better plan my training sessions. As I grew older, I could choose whether to go to school on my motorcycle or, for safety reasons, have my grandfather drive me. And it was him, my grandfather, who was my go-to meteorologist. He followed all weather patterns and forecasts, a remnant of his childhood in the countryside and his life on the move. It's to him that I dedicate FediMeteo.&lt;/p&gt;
    &lt;p&gt;The idea for FediMeteo started almost by chance while I was checking the holiday weather forecast to plan an outing. Suddenly, I thought how nice it would be to receive regular weather updates for my city directly in my timeline. After reflecting for a few minutes, I registered a domain and started planning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Principles&lt;/head&gt;
    &lt;p&gt;The choice of operating system was almost automatic. The idea was to separate instances by country, and FreeBSD jails are one of the most useful tools for this purpose.&lt;/p&gt;
    &lt;p&gt;I initially thought the project would generate little interest. I was wrong. After all, weather affects many of our lives, directly or indirectly. So I decided to structure everything in this way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I would use a test VPS to see how things would go. The VPS was a small VM on a German provider with 4 shared cores, 4GB of RAM, 120GB of SSD disk space, and a 1Gbit/sec internet connection and now is a 4 euro per month VPS in Milano, Italy - 4 shared cores, 8 GB RAM and 75GB disk space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would separate various countries into different instances, for both management and security reasons, as well as to have the possibility of relocating just some of them if needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weather data would come from a reliable and open-source friendly source. I narrowed it down to two options: wttr.in and Open-Meteo, two solutions I know and that have always given me reliable results.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would pay close attention to accessibility: forecasts would be in local languages, consultable via text browsers, with emojis to give an idea even to those who don't speak local languages, and everything would be accessible without JavaScript or other requirements. One's mother tongue is always more "familiar" than a second language, even if you're fluent.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would manage everything according to Unix philosophy: small pieces working together. The more years pass, the more I understand how valuable this approach is.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The software chosen to manage the instances is snac. Snac embodies my philosophy of minimal and effective software, perfect for this purpose. It provides clear web pages for those who want to consult via the web, "speaks" the ActivityPub protocol perfectly, produces RSS feeds for each user (i.e., city), has extremely low RAM and CPU consumption, compiles in seconds, and is stable. The developer is an extremely helpful and positive person, and in my opinion, this carries equal weight as everything else.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would do it for myself. If there was no interest, I would have kept it running anyway, without expanding it. So no anxiety or fear of failure.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Technical Implementation&lt;/head&gt;
    &lt;p&gt;I started setting up the first "pieces" during the days around Christmas 2024. The scheme was clear: each jail would handle everything internally. A Python script would download data, city by city, and produce markdown. The city coordinates would be calculated via the geopy library and passed to wttr.in and Open-Meteo. No data would be stored locally. This approach gives the ability to process all cities together. Just pass the city and country to the script, and the markdown would be served. At that point, snac comes into play: without the need to use external utilities, the "snac note" command allows posting from stdin by specifying the instance directory and the user to post from. No need to make API calls with external utilities, having to manage API keys, permissions, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Setting Up for Italy&lt;/head&gt;
    &lt;p&gt;To simplify things, I first structured the jail for Italy. I made a list of the main cities, normalizing them. For example, La Spezia became la_spezia. ForlÃ¬, with an accent, became forli - this for maximum compatibility since each city would be a snac user. I then created a script that takes this list and creates snac users via "snac adduser." At that point, after creating all the users, the script would modify the JSON of each user to convert the city name to uppercase, insert the bio (a standard text), activate the "bot" flag, and set the avatar, which was the same for all users at the time. This script is also able to add a new city: just run the script with the (normalized) name of the city, and it will add it - also adding it to the "cities.txt" file, so it will be updated in the next weather update cycle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core Application Development&lt;/head&gt;
    &lt;p&gt;I then created the heart of the service. A Python application (initially only in Italian, then multilingual, separating the operational part from the text) able to receive (via command line) the name of a city and a country code (corresponding to the file with texts in the local language). The script determines the coordinates and then, using API calls, requests the current weather conditions, those for the next 12 hours, and the next 7 days. I conducted experiments with both wttr.in and Open-Meteo, and both gave good results. However, I settled on Open-Meteo because, for my uses, it has always provided very reliable results. This application directly provides an output in Markdown since snac supports it, at least partially.&lt;/p&gt;
    &lt;p&gt;The cities.txt file is also crucial for updates. I created a script - post.sh, in pure sh, that scrolls through all cities, and for each one, launches the FediMeteo application and publishes its output using snac directly via command line. Once the job is finished, it makes a call to my instance of Uptime-Kuma, which keeps an eye on the situation. In case of failure, the monitoring will alert me that there have been no recent updates, and I can check.&lt;/p&gt;
    &lt;p&gt;At this point, the system cron takes care of launching post.sh every 6 hours. The requests are serialized, so the cities will update one at a time, and the posts will be sent to followers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Growth and Unexpected Success&lt;/head&gt;
    &lt;p&gt;After listing all Italian provincial capitals, I started testing everything. It worked perfectly. Of course, I had to make some adjustments at all levels. For example, one of the problems encountered was that snac did not set the language of the posts, and some users could have missed them. The developer was very quick and, as soon as I exposed the problem, immediately modified the program so that the post could keep the system language, set as an environment variable in the sh script.&lt;/p&gt;
    &lt;p&gt;After two days, I decided to start adding other countries and announce the project. And the announcement was unexpectedly well received: there were many boosts, and people started asking me to add their cities or countries. I tried to do what I could, within the limits of my physical condition, as in those days, I had the flu that kept me at home with a fever and illness for several days. I started adding many countries in the heart of Europe, translating the main indications into local languages but maintaining emojis so that everything would be understandable even to those who don't speak the local language. There were some small problems reported by some users. One of them: not all weather conditions had been translated, so sometimes they appeared in Italian - as well as errors. In bilingual countries, I tried to include all local languages. Sometimes, unfortunately, making mistakes as I encountered dynamics unknown to me or difficult to interpret. For example, in Ireland, forecasts were published in Irish, but it was pointed out to me that not everyone speaks it, so I modified and published in English.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Turning Point&lt;/head&gt;
    &lt;p&gt;The turning point was when FediFollows (@FediFollows@social.growyourown.services - who also manages the site Fedi Directory) started publishing the list of countries and cities, highlighting the project. Many people became aware of FediMeteo and started following the various accounts, the various cities. And from here came requests to add new countries and some new information, such as wind speed. Moreover, I was asked (rightly, to avoid flooding timelines) to publish posts as unlisted - this way, followers would see the posts, but they wouldn't fill local timelines. Snac didn't support this, but again, the snac dev came to my rescue in a few hours.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scaling Challenges&lt;/head&gt;
    &lt;p&gt;But with new countries came new challenges. For example, in my original implementation, all units of measurement were in metric/decimal/Celsius - and this doesn't adapt well to realities like the USA. Moreover, focusing on Europe, almost all countries were located in a single timezone, while for larger countries (such as Australia, USA, Canada, etc.), this is totally different. So I started developing a more complete and global version and, in the meantime, added almost all of Europe. The new version would have to be backward compatible, would have to take into account timezone differences for each city, different measurements (e.g., degrees C and F), as well as, initially more difficult part, being able to separate cities with the same name based on states or provinces. I had already seen a similar problem with the implementation of support for Germany, so it had to be addressed properly.&lt;/p&gt;
    &lt;p&gt;The original goal was to have a VPS for each continent, but I soon realized that thanks to the quality of snac's code and FreeBSD's efficient management, even keeping countries in separate jails, the load didn't increase much. So I decided to challenge myself and the limits of the economical 4 euros per month VPS. That is, to insert as much as possible until seeing what the limits were. Limits that, to date, I have not yet reached. I would also soon exhaust the available API calls for Open-Meteo's free accounts, so I tried to contact the team and explain everything. I was positively surprised to read that they appreciated the project and provided me with a dedicated API key.&lt;/p&gt;
    &lt;p&gt;Compatible with my free time, I managed to complete the richer and more complete version of my Python program. I'm not a professional dev, I'm more oriented towards systems, so the code is probably quite poor in the eyes of an expert dev. But, in the end, it just needs to take an input and give me an output. It's not a daemon, it's not a service that responds on the network. For that, snac takes care of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expansion to North America&lt;/head&gt;
    &lt;p&gt;So I decided to start with a very important launch: the USA and Canada. A non-trivial part was identifying the main cities in order to cover, state by state, all the territory. In the end, I identified more than 1200 cities. A number that, by itself, exceeded the sum of all other countries (at that time). And the program, now, is able to take an input with a separator (two underscores: __) between city and state. In this way, it's possible to perfectly understand the differences between city and state: new_york__new_york is an example I like to make, but there are many.&lt;/p&gt;
    &lt;p&gt;The launch of the USA was interesting: despite having had many previous requests, the reception was initially quite lukewarm, to my extreme surprise. The number of followers in Canada, in a few hours, far exceeded that of the USA. On the contrary, the country with the most followers (in a few days, more than 1000) was Germany. Followed by the UK - which I expected would have been the first.&lt;/p&gt;
    &lt;head rend="h2"&gt;System Performance&lt;/head&gt;
    &lt;p&gt;The VPS held up well. Except for the moments when FediFollows launched (after fixing some FreeBSD tuning, the service slowed slightly but didn't crash), the load remained extremely low. So I continued to expand: Japan, Australia, New Zealand, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Current Status&lt;/head&gt;
    &lt;p&gt;At the time of the last update of this article (30 December 2025), the supported countries are 38: Argentina, Australia, Austria, Belgium, Brazil, Bulgaria, Canada, Croatia, Czechia, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, India, Ireland, Italy, Japan, Latvia, Lithuania, Malta, Mexico, Netherlands, New Zealand, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, Switzerland, Taiwan, the United Kingdom, and the United States of America (with more regions coming soon!).&lt;/p&gt;
    &lt;p&gt;Direct followers in the Fediverse are around 7,707 and growing daily, excluding those who follow hashtags or cities via RSS, whose number I can't estimate. However, a quick look at the logs suggests there are many more.&lt;/p&gt;
    &lt;p&gt;The cities currently covered are 2937 - growing based on new countries and requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges Encountered&lt;/head&gt;
    &lt;p&gt;There have been some problems. The most serious, by my fault, was the API key leak: I had left a debug code active and, the first time Open-Meteo had problems, the error message also included the API call - including the API key. Some users reported it to me (others just mocked) and I fixed the code and immediately reported everything to the Open-Meteo team, who kindly gave me a new API Key and deactivated the old one.&lt;/p&gt;
    &lt;p&gt;A further problem was related to geopy. It makes a call to Nominatim to determine coordinates. One of the times Nominatim didn't respond, my program wasn't able to determine the position and went into error. I solved this by introducing coordinate caching: now the program, the first time it encounters a city, requests and saves the coordinates. If present, they will be used in the future without making a new request via geopy. This is both lighter on their servers and faster and safer for us.&lt;/p&gt;
    &lt;head rend="h2"&gt;Infrastructure Details&lt;/head&gt;
    &lt;p&gt;And the VPS? It has no problems and is surprisingly fast and effective. FreeBSD 14.3-RELEASE, BastilleBSD to manage the jails. Currently, there are 39 jails - one for haproxy, the FediMeteo website, so nginx, and the snac instance for FediMeteo announcements and support - the other 38 for the individual instances. Each of them, therefore, has its autonomous ZFS dataset. Every 15 minutes, there is a local snapshot of all datasets. Every hour, the homepage is regenerated: a small script calculates the number of followers (counting, instance by instance, the followers of individual cities, since I don't publish except in aggregate to avoid possible triangulations and privacy leaks of users). Every hour, moreover, an external backup is made via zfs-autobackup (on encrypted at rest dataset), and once a day, a further backup is made in my datacenter, on disks encrypted with geli. The occupied RAM is 501 MB (yes, exactly: 501 MB), which rises slightly when updates are in progress. Updates normally occur every 6 hours. I have tried, as much as possible, to space them out to avoid overloads in timelines (or on the server itself). Only for the USA, I added a sleep of 5 seconds between one city and another, to give snac the opportunity to better organize the sending of messages. It probably wouldn't be necessary, with the current numbers, but better safe than sorry. In this way, the USA is processed in about 2 and a half hours, but the other jails (thus countries) can work autonomously and send their updates.&lt;/p&gt;
    &lt;p&gt;The average load of the VPS (taking as reference both the last 24 hours and the last two weeks) is about 25%, as it rises to 70/75% when updates occur for larger instances (such as the USA), or when it is announced by FediFollows. Otherwise, it is on average less than 10%. So, the VPS still has huge margin, and new instances, with new nations, will still be inside it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This article, although in some parts very conversational, aims to demonstrate how it's possible to build solid, valid, and efficient solutions without the need to use expensive and complex services. Moreover, this is the demonstration of how it's possible to have your online presence without the need to put your data in the hands of third parties or without necessarily having to resort to complex stacks. Sometimes, less is more.&lt;/p&gt;
    &lt;p&gt;The success of this project demonstrates, once again, that my grandfather was right: weather forecasts interest everyone. He worried about my health and, thanks to his concerns, we spent time together. In the same way, I see many followers and friends talking to me or among themselves about the weather, their experiences, what happens. Again, in my life, weather forecasts have helped sociality and socialization.&lt;/p&gt;
    &lt;p&gt;Thank you, Grandpa.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://it-notes.dragas.net/2025/02/26/fedimeteo-how-a-tiny-freebsd-vps-became-a-global-weather-service-for-thousands/"/><published>2025-12-30T19:21:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436949</id><title>Foreign tech workers are avoiding travel to the US</title><updated>2025-12-30T20:12:09.095873+00:00</updated><content>&lt;doc fingerprint="3ec9435964e7c8a5"&gt;
  &lt;main&gt;
    &lt;p&gt;It's not just the Trump Administration’s H-1B visa overhaul that’s keeping people away from jobs and conferences in the US. Credit: Shutterstock I go to a lot of tech conferences — 13 in 2025 — and many of those I attend are outside the US; several are in London, one is in Amsterdam, another in Paris, and two in Tokyo. Wherever I went this past year, when we weren’t talking about AI, Linux, the cloud, or open-source software, the top non-tech topic for non-Americans involved the sweeping changes that have occurred since President Donald J. Trump returned to office last January. The conversations generally ended with something like this: “I’m not taking a job or going to a conference in the United States.” Honestly, who can blame them? Under Trump, America now has large “Keep Out!” and “No Trespassing!” signs effectively posted. I’ve known several top tech people who tried to come to the US for technology shows with proper visas and paperwork, but were still turned away at the border. Who wants to fly for 8+ hours for a conference, only to be refused entry at the last minute, and be forced to fly back? I know many of the leading trade show organizers, and it’s not just me who’s seeing this. They universally agree that getting people from outside the States to agree to come to the US is increasingly difficult. Many refuse even to try to come. As a result, show managers have begun to close US-based events and are seeking to replace them with shows in Europe, Canada, and Asia. Organizers of scientific meetings here are also reporting falling attendance from abroad. Why? PhysicsToday reports that scientists cite “visa woes and worries about being hassled, detained, or denied entry at the US border” as key reasons international researchers now skip American events. Foreign scholars are explicitly opting out of US conferences either out of protest or caution. They’re saying they fear not only delays and denials but also potential encounters with immigration enforcement while traveling. It’s not just the scientists and enterprise IT people I usually hang out with — it’s also happening on the consumer tech side of things. Chinese tech workers invited to CES in Las Vegas, which is only days away, report unusually high rates of US visa denials. Some advisers in China now warn that mentioning CES in an application can sharply increase the odds of rejection. CES organizers have acknowledged the issue and publicly urged Washington leaders to expedite business‑travel visa approvals. They’re calling the denials a setback for global industry exchange. Need I mention that their appeals are falling on deaf ears? It’s not just foreign trade show attendees and speakers who are steering clear. Foreign technologists and researchers are increasingly avoiding the US. They’re skipping both long‑term work opportunities as visas get harder to obtain and the political climate grows more hostile to international talent. The Trump administration’s new “Restriction on Entry of Certain Nonimmigrant Workers” demands that employers pay an annual fee of $100,000 per H‑1B application, which is a de facto deterrent to hiring foreign tech workers. Even the largest companies will be hesitant to hire talent given this fee. And as for smaller businesses? Forget about it! Even foreign workers already in the US are having second thoughts about living in a country that’s officially hostile to anyone who’s not of European descent. A recent report from Specialist Staffing Group found that 32% of US-based STEM pros said they were open to relocation. That’s bad news for US companies, which are already seeing projects delayed or disrupted. Major tech firms, including Amazon, Microsoft, and Google, have reportedly urged overseas staff to return to the US quickly while simultaneously warning them to limit dependents’ travel. At the same time, all these top tech companies, and many more, have been laying people off. Even if you can trace your ancestry back to the Mayflower, we’re living in a time of tech job insecurity. While the US raises barriers, rival tech hubs are pitching themselves as open and predictable. Canada, Europe, and parts of Asia are marketing fast‑track visas and remote‑work‑friendly policies aimed squarely at the engineers, founders, and researchers who now view the US as too much trouble for too little certainty. Once upon a time, everyone who was anyone in tech was willing to uproot their lives to come to the US. Here, they could make a good living. They could collaborate, publish, and build companies in jurisdictions that welcome them, and meet their peers at conferences. Now, they must run a gauntlet at the US border and neither a green card nor US citizenship guarantees they won’t be abused by the federal government. Trump’s America seems bound and determined to become a second-rate tech power. His administration can loosen all the restrictions it wants on AI, but without top global talent, US tech prowess will decline. That’s not good for America, the tech industry or the larger world. IT Skills and TrainingCareersH-1B VisasIT ManagementTechnology IndustryMarkets SUBSCRIBE TO OUR NEWSLETTER From our editors straight to your inbox Get started by entering your email address below. Please enter a valid email address Subscribe&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.computerworld.com/article/4110681/foreign-tech-workers-are-avoiding-travel-to-the-us.html"/><published>2025-12-30T19:27:59+00:00</published></entry></feed>