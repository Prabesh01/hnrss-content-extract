<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-04T23:36:54.865517+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46147540</id><title>Transparent leadership beats servant leadership</title><updated>2025-12-04T23:37:01.928455+00:00</updated><content>&lt;doc fingerprint="bce3d0111682d491"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Transparent Leadership Beats Servant Leadership&lt;/head&gt;
    &lt;p&gt;tl:dr: Parenting and leadership is similar. Teach a man to fish, etc.&lt;/p&gt;
    &lt;p&gt;I spent a couple of years managing a team, and I entered that role – like many – without knowing anything about how to do it. I tried to figure out how to be a good manager, and doing so I ended up reading a lot about servant leadership. It never quite sat right with me, though. Servant leadership seems to me a lot like curling parenting: the leader/parent anticipate problems and sweep the way for their direct reports/children.&lt;/p&gt;
    &lt;p&gt;To be clear, this probably feels very good (initially, anyway) for the direct reports/children. But the servant leader/curling parent quickly becomes an overworked single point of failure, and once they leave there is nobody else who knows how to handle the obstacles the leader moved out of the way for everyone. In the worst cases, they leave behind a group of people who have been completely isolated from the rest of the organisation, and has no idea what their purpose is and how to fit in with the rest of the world.&lt;/p&gt;
    &lt;p&gt;I would like to invent my own buzzword: transparent leadership. In my book, a good leader&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;coaches people,&lt;/item&gt;
      &lt;item&gt;connects people,&lt;/item&gt;
      &lt;item&gt;teaches people methodical problem solving,&lt;/item&gt;
      &lt;item&gt;explains values and principles embraced by the organisation to aid them in making aligned decisions on their own,&lt;/item&gt;
      &lt;item&gt;creates direct links between supply and demand (instead of deliberately making themselves a middle man),&lt;/item&gt;
      &lt;item&gt;allows their direct reports career growth by gradually taking over leadership responsibilities,&lt;/item&gt;
      &lt;item&gt;continuously trains their replacement, and&lt;/item&gt;
      &lt;item&gt;generally makes themselves redundant.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The middle manager that doesn’t perform any useful work is a fun stereotype, but I also think it’s a good target to aim for. The difference lies in what to do once one has rendered oneself redundant. A common response is to invent new work, ask for status reports, and add bureaucracy.&lt;/p&gt;
    &lt;p&gt;A better response is to go back to working on technical problems. This keeps the manager’s skills fresh and gets them more respect from their reports. The manager should turn into a high-powered spare worker, rather than a paper-shuffler.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/transparent-leadership-beats-servant-leadership"/><published>2025-12-04T13:40:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46148460</id><title>Show HN: Onlyrecipe 2.0 – I added all features HN requested – 4 years later</title><updated>2025-12-04T23:37:01.813817+00:00</updated><content>&lt;doc fingerprint="d37fffed7efd5e8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://onlyrecipeapp.com/?url=https://www.allrecipes.com/turkish-pasta-recipe-8754903"/><published>2025-12-04T15:06:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46148748</id><title>Microsoft drops AI sales targets in half after salespeople miss their quotas</title><updated>2025-12-04T23:37:01.440578+00:00</updated><content>&lt;doc fingerprint="af740a1914faa081"&gt;
  &lt;main&gt;
    &lt;p&gt;Microsoft has lowered sales growth targets for its AI agent products after many salespeople missed their quotas in the fiscal year ending in June, according to a report Wednesday from The Information. The adjustment is reportedly unusual for Microsoft, and it comes after the company missed a number of ambitious sales goals for its AI offerings.&lt;/p&gt;
    &lt;p&gt;AI agents are specialized implementations of AI language models designed to perform multistep tasks autonomously rather than simply responding to single prompts. So-called “agentic” features have been central to Microsoft’s 2025 sales pitch: At its Build conference in May, the company declared that it has entered “the era of AI agents.”&lt;/p&gt;
    &lt;p&gt;The company has promised customers that agents could automate complex tasks, such as generating dashboards from sales data or writing customer reports. At its Ignite conference in November, Microsoft announced new features like Word, Excel, and PowerPoint agents in Microsoft 365 Copilot, along with tools for building and deploying agents through Azure AI Foundry and Copilot Studio. But as the year draws to a close, that promise has proven harder to deliver than the company expected.&lt;/p&gt;
    &lt;p&gt;According to The Information, one US Azure sales unit set quotas for salespeople to increase customer spending on a product called Foundry, which helps customers develop AI applications, by 50 percent. Less than a fifth of salespeople in that unit met their Foundry sales growth targets. In July, Microsoft lowered those targets to roughly 25 percent growth for the current fiscal year. In another US Azure unit, most salespeople failed to meet an earlier quota to double Foundry sales, and Microsoft cut their quotas to 50 percent for the current fiscal year.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/ai/2025/12/microsoft-slashes-ai-sales-growth-targets-as-customers-resist-unproven-agents/"/><published>2025-12-04T15:31:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149066</id><title>Feynman vs. Computer</title><updated>2025-12-04T23:37:00.765586+00:00</updated><content>&lt;doc fingerprint="127ccb0343b51399"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Feynman vs. Computer&lt;/head&gt;
    &lt;p&gt;I read Burghelea’s article on the Feynman trick for integration. Well, I’m not good enough at analysis to follow along, but I tried reading it anyway because it’s fascinating.&lt;/p&gt;
    &lt;p&gt;For people who do not have experience with analysis, integration is counting the total size of very many, very small piles of things. Analytical integration, i.e. the process by which we can get an exact result, can be very difficult. It often takes knowledge of special tricks, strong pattern recognition, and plenty of trial and error. Fortunately, in all cases in my career when I’ve needed the value of an integral, an approximate answer has been good enough.&lt;/p&gt;
    &lt;p&gt;In practical terms, this means we could spend a lot of time learning integration tricks, practice using them, and then take half an hour out of our day to apply them to an integral in front of us … or, hear me out, or, we could write four lines of JavaScript that arrive at a relatively accurate answer in less than a second.&lt;/p&gt;
    &lt;head rend="h1"&gt;The approximating power of random numbers&lt;/head&gt;
    &lt;p&gt;If integration is summing many small piles, we have to figure out how big the piles are. Their height is usually given by a mathematical function, and our first example will be the same as in the Feynman trick article.&lt;/p&gt;
    &lt;p&gt;\[f(x) = \frac{x - 1}{\ln{x}}\]&lt;/p&gt;
    &lt;p&gt;This is to be integrated from zero to one, i.e. we want to know the size of the shaded area in the plot below. You can think of each column of shaded pixels as one pile, and we sum the size of all of them to get the total area.1 Of course, this is an svg image so there are no columns of pixels. Alternatively, the more we zoom in, the thinner the columns become – but the more of them there are. This is why we need integration: it’s dealing with the limit case of infinitely many, infinitely thin columns.&lt;/p&gt;
    &lt;p&gt;We could imagine drawing six random numbers between zero and one, and plotting piles of the corresponding height at those locations. Since there are six piles, their width is one sixth of the width of the area we are integrating.&lt;/p&gt;
    &lt;p&gt;Even though some of these piles overlap by chance, and even though there are some random gaps between them, the sum of their areas (0.66) comes very close to the actual shaded area determined analytically (0.69). If we draw more piles, we have to make them correspondingly thinner, but the agreement between their sum and the total size of the area improves.&lt;/p&gt;
    &lt;p&gt;These are 100× as many piles, and they’re 1/100th as thick to compensate. Their total area is 0.70 – very close to 0.69. If we draw even more piles, we’ll get even closer.&lt;/p&gt;
    &lt;p&gt;This illustrates a neat correspondence between integrals and expected values. In the simple case, we can frame it mathematically as&lt;/p&gt;
    &lt;p&gt;\[\int_a^b f(x) \mathrm{d}x = E(f(x))\]&lt;/p&gt;
    &lt;p&gt;In words, this says that integrating the function \(f\) between \(a\) and \(b\) is the same as taking the expected value of \(f(x)\) at uniformly distributed random points between \(a\) and \(b\).&lt;/p&gt;
    &lt;head rend="h1"&gt;Teaching the computer to do it&lt;/head&gt;
    &lt;p&gt;Here’s a JavaScript function that estimates the value of an integral in the most primitive way possible.&lt;/p&gt;
    &lt;quote&gt;I = (B, lo, hi, f) =&amp;gt; { // Generate B random values uniformly between lo and hi. let xs = Array.from({length: B}, _ =&amp;gt; lo + (hi - lo) * Math.random()); // Compute the value of f at each location. let ys = xs.map(f); // Return the total area of each corresponding pile. return (hi-lo)*ys.reduce((r, y) =&amp;gt; r + y, 0)/ys.length; }&lt;/quote&gt;
    &lt;p&gt;To compute an approximation to the value of the integral we’ve seen, we run&lt;/p&gt;
    &lt;quote&gt;I(10_000, 0, 1, x =&amp;gt; (x-1)/Math.log(x) );&lt;/quote&gt;
    &lt;quote&gt;0.6916867623261724&lt;/quote&gt;
    &lt;p&gt;This is fairly close to 0.69. And we got there in four lines of JavaScript, as promised.&lt;/p&gt;
    &lt;head rend="h1"&gt;Improved approximation through splittage&lt;/head&gt;
    &lt;p&gt;We can try this on the next example too. Now we’re asking about the integral&lt;/p&gt;
    &lt;p&gt;\[\int_0^{\frac{\pi}{2}} \frac{\ln{(1 - \sin{x})}}{\sin{x}} \mathrm{d}x\]&lt;/p&gt;
    &lt;p&gt;which, translated to JavaScript, becomes&lt;/p&gt;
    &lt;quote&gt;I(10_000, 0, Math.PI, x =&amp;gt; Math.log(1 - Math.sin(x))/Math.sin(x) );&lt;/quote&gt;
    &lt;quote&gt;-3.67&lt;/quote&gt;
    &lt;p&gt;This is again fairly close to the desired −3.7, but not quite there yet. The tricky shape of the function is the reason we aren’t getting as close as we want.&lt;/p&gt;
    &lt;p&gt;At the upper endpoint of the integration interval, this function goes to negative infinity. The random piles we draw come primarily from the well behaved region of the function, and thus don’t help the computer realise this behaviour.&lt;/p&gt;
    &lt;p&gt;There are clever ways to sample adaptively from the trickier parts of the function, but an easy solution is to just visually find a breakpoint, split the interval on that, and then estimate the sensible part separately from the crazy-looking part. Since the total area must be the sum of both areas, we can add their results together for a final estimation.&lt;/p&gt;
    &lt;p&gt;In this case, we might want to pick e.g. 1.5 as the breakpoint, so we combine the area estimations from 0–1.5 and then 1.5–\(\frac{\pi}{2}\). The result is&lt;/p&gt;
    &lt;quote&gt;I(2_000, 0, 1.5, x =&amp;gt; Math.log(1 - Math.sin(x))/Math.sin(x)) + I(8_000, 1.5, Math.PI/2, x =&amp;gt; Math.log(1 - Math.sin(x))/Math.sin(x));&lt;/quote&gt;
    &lt;quote&gt;-3.70&lt;/quote&gt;
    &lt;p&gt;which is indeed much closer to the actual value of −3.7.&lt;/p&gt;
    &lt;p&gt;Note that we aren’t taking more samples, we’re just sprinkling them more wisely over the number line. We spend 2,000 samples in the relatively well-behaved region where the function takes values from −1 to −6, and then we spend the other 8,000 samples in the small region that goes from −6 to negative infinity. Here it is graphically:&lt;/p&gt;
    &lt;p&gt;The reason this helps us is that this latter region contributes a lot to the value of the integral, but it is so small on the number line that we benefit from oversampling it compared to the other region. This is a form of sample unit engineering, which we have seen before in different contexts.&lt;/p&gt;
    &lt;head rend="h1"&gt;More evidence of sufficiency&lt;/head&gt;
    &lt;p&gt;We can continue with some more examples from the Feynman trick article. That gets us the following table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Integral&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;cell role="head"&gt;Estimation&lt;/cell&gt;
        &lt;cell role="head"&gt;Difference&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(\int_0^1 \frac{x-1}{\ln{x}} \mathrm{d}x\)&lt;/cell&gt;
        &lt;cell&gt;\(\ln{2}\)&lt;/cell&gt;
        &lt;cell&gt;0.6943&lt;/cell&gt;
        &lt;cell&gt;0.2 %&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(\int_0^{\frac{\pi}{2}} \frac{\ln{(1 - \sin{x})}}{\sin{x}} \mathrm{d}x\)&lt;/cell&gt;
        &lt;cell&gt;\(\frac{-3 \pi^2}{8}\)&lt;/cell&gt;
        &lt;cell&gt;-3.702&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 0.1 %&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(\int_0^1 \frac{\ln{(1 - x + x^2)}}{x - x^2} \mathrm{d}x\)&lt;/cell&gt;
        &lt;cell&gt;\(\frac{-\pi^2}{9}\)&lt;/cell&gt;
        &lt;cell&gt;-1.097&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 0.1 %&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(\int_0^{\frac{\pi}{2}} \frac{\arctan{(\sin{x})}}{\sin{x}} \mathrm{d}x\)&lt;/cell&gt;
        &lt;cell&gt;\(\frac{\pi}{2}\log{(1 + \sqrt{2})}\)&lt;/cell&gt;
        &lt;cell&gt;1.385&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 0.1 %&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(\int_0^\infty x^2 e^{-\left(4x^2 + \frac{9}{x^2}\right)} \mathrm{d}x\)&lt;/cell&gt;
        &lt;cell&gt;\(\frac{13 \sqrt{\pi}}{32 e^{12}}\)&lt;/cell&gt;
        &lt;cell&gt;0.000004414&lt;/cell&gt;
        &lt;cell&gt;0.2 %&lt;/cell&gt;
        &lt;cell&gt;(1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(\int_0^1 \frac{\ln{x}}{1 - x^2} \mathrm{d}x\)&lt;/cell&gt;
        &lt;cell&gt;\(\frac{-\pi^2}{8}\)&lt;/cell&gt;
        &lt;cell&gt;-1.227&lt;/cell&gt;
        &lt;cell&gt;0.5 %&lt;/cell&gt;
        &lt;cell&gt;(2)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(\int_0^\infty \frac{e^{-x^2}}{1 + x^2} \mathrm{d}x\)&lt;/cell&gt;
        &lt;cell&gt;\(\frac{\pi e}{2}\mathrm{erfc}(1)\)&lt;/cell&gt;
        &lt;cell&gt;0.6696&lt;/cell&gt;
        &lt;cell&gt;0.3 %&lt;/cell&gt;
        &lt;cell&gt;(3)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The integration is from zero to infinity, but the function practically only has a value between zero and three, so that’s the region we estimate over.&lt;/item&gt;
      &lt;item&gt;This is another case where the function goes to infinity near zero, so we split up the estimation into one for the range 0–0.1, and the other for 0.1–1.0. We have not increased the sample count, only reallocated the 10,000 samples.&lt;/item&gt;
      &lt;item&gt;Again, the integration is from zero to infinity, but the function practically only has a value between zero and three, so that’s the region we estimate over.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Finding the error without a ground truth&lt;/head&gt;
    &lt;p&gt;“Now,” the clever reader says, “this is all well and good when we have the actual value to compare to so we know the size of the error. What will we do if we’re evluating a brand new integral? What is the size of the error then, huh?”&lt;/p&gt;
    &lt;p&gt;This is why we sampled the function randomly. That means our approximation is a statistical average over samples, and for that we can compute the standard error of the mean. In the JavaScript implementation below, we use the quick variance computation, but we could perhaps more intuitively have used the spc inspired method.&lt;/p&gt;
    &lt;quote&gt;Ic = (B, lo, hi, f) =&amp;gt; { let xs = Array.from( {length: B}, _ =&amp;gt; lo + (hi - lo) * Math.random() ); let ys = xs.map(f); // Compute the variance of the ys from the sum and // the sum of squared ys. let s = ys.reduce((r, y) =&amp;gt; r + y, 0); let ssq = ys.reduce((r, y) =&amp;gt; r + y**2, 0); let v = (ssq - s**2/B)/(B-1); // Compute the mean and the standard error of the mean. let m = (hi-lo)*s/B; let se = (hi-lo)*Math.sqrt(v/B); // Compute the 90 % confidence interval of the value of // the integral. return { p05: m - 1.645*se, p95: m + 1.645*se, } }&lt;/quote&gt;
    &lt;p&gt;If we run this with the first integral as an example, we’ll learn that&lt;/p&gt;
    &lt;quote&gt;Ic(10_000, 0, 1, x =&amp;gt; (x-1)/Math.log(x) )&lt;/quote&gt;
    &lt;quote&gt;Object { p05: 0.6896 p95: 0.6963 }&lt;/quote&gt;
    &lt;p&gt;Not only is this range an illustration of the approximation error (small!), it is also very likely to capture the actual value of the integral. Here are some more examples from the same integrals as above:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;5 %&lt;/cell&gt;
        &lt;cell role="head"&gt;95 %&lt;/cell&gt;
        &lt;cell role="head"&gt;Actual&lt;/cell&gt;
        &lt;cell role="head"&gt;Contained?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;0.6904&lt;/cell&gt;
        &lt;cell&gt;0.6972&lt;/cell&gt;
        &lt;cell&gt;0.6931&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;-3.7673&lt;/cell&gt;
        &lt;cell&gt;-3.6787&lt;/cell&gt;
        &lt;cell&gt;-3.7011&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;-1.0975&lt;/cell&gt;
        &lt;cell&gt;-1.0960&lt;/cell&gt;
        &lt;cell&gt;-1.0966&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1.3832&lt;/cell&gt;
        &lt;cell&gt;1.3871&lt;/cell&gt;
        &lt;cell&gt;1.3845&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;0.4372&lt;/cell&gt;
        &lt;cell&gt;0.4651&lt;/cell&gt;
        &lt;cell&gt;0.4424&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;-1.2545&lt;/cell&gt;
        &lt;cell&gt;-1.2254&lt;/cell&gt;
        &lt;cell&gt;-1.2337&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;0.6619&lt;/cell&gt;
        &lt;cell&gt;0.6937&lt;/cell&gt;
        &lt;cell&gt;0.6716&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These are all built naïvely from 10,000 uniform samples. In other words, in none of the cases have the computation been split up to allocate samples more cleverly.&lt;/p&gt;
    &lt;p&gt;Again, we could spend a lot of time learning to integrate by hand … or we ask the computer for less than a second of its time first, and see if the accuracy it can do it with is appropriate for our use case. In my experience, it generally is.&lt;/p&gt;
    &lt;head rend="h1"&gt;Seeing the effect of sample unit engineering&lt;/head&gt;
    &lt;p&gt;What’s neat is we can still split up the computation like we did before, if we believe it will make the error smaller and the confidence interval narrower. Let’s use the following integral as an example.&lt;/p&gt;
    &lt;p&gt;\[\int_0^\infty \frac{\sin{x}}{x} \mathrm{d}x\]&lt;/p&gt;
    &lt;p&gt;This oscillates up and down quite a bit for small \(x\), and then decays but still provides significant contributions for larger \(x\). A naive evaluation would have a confidence interval of&lt;/p&gt;
    &lt;quote&gt;Ic(10_000, 0, 100, x =&amp;gt; Math.sin(x)/x)&lt;/quote&gt;
    &lt;quote&gt;Object { p05: 1.461 p95: 1.884 }&lt;/quote&gt;
    &lt;p&gt;and while this is certainly correct2 The actual value of the integral is half \(\pi\) or approximatey 1.571., we can do better. We’ll estimate the region of 0–6 separately from 6–100, using half the samples for each3 Why put the break point at 6? The period of sin is a full turn, which is roughly 6 radians. This ensures we get roughly symmetric contributions from both integrals. That’s not necessary for the technique to work, but it makes the illustration a little cleaner.:&lt;/p&gt;
    &lt;quote&gt;Ic(5_000, 0, 6, x =&amp;gt; Math.sin(x)/x)&lt;/quote&gt;
    &lt;quote&gt;Object { p05: 1.236 p95: 1.468 }&lt;/quote&gt;
    &lt;p&gt;This contains the bulk of the value of the integral, it seems. Let’s see what remains in the rest of it.&lt;/p&gt;
    &lt;quote&gt;Ic(5_000, 6, 100, x =&amp;gt; Math.sin(x)/x)&lt;/quote&gt;
    &lt;quote&gt;Object { p05: 0.080 p95: 0.198 }&lt;/quote&gt;
    &lt;p&gt;We can work backwards to what the standard errors must have been to produce these confidence intervals.4 The midpoint is the point estimation for each region, and the standard error is 1/1.645 times the distance between the 5 % point and the midpoint.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Region&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;cell role="head"&gt;Standard error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0–6&lt;/cell&gt;
        &lt;cell&gt;1.4067&lt;/cell&gt;
        &lt;cell&gt;0.0372&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;6–100&lt;/cell&gt;
        &lt;cell&gt;0.1390&lt;/cell&gt;
        &lt;cell&gt;0.0359&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The estimation of the total area would be the values summed, i.e. 1.5457. The estimation of the standard error of this we get through Pythagorean addition and it is approximately 0.05143. We convert it back to a confidence interval and compare with when we did not break it up into multiple components.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;5 %&lt;/cell&gt;
        &lt;cell role="head"&gt;95 %&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Single operation (10,000 samples)&lt;/cell&gt;
        &lt;cell&gt;1.461&lt;/cell&gt;
        &lt;cell&gt;1.884&lt;/cell&gt;
        &lt;cell&gt;0.423&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Two operations (5,000 samples × 2)&lt;/cell&gt;
        &lt;cell&gt;1.461&lt;/cell&gt;
        &lt;cell&gt;1.630&lt;/cell&gt;
        &lt;cell&gt;0.169&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Although in this case the two methods happen to share a lower bound, the upper bound has been dramatically reduced. The total range of the confidence interval is more than halved! This was because we allocated the samples more cleverly – concentrated them in the early parts of the function – rather than increased the number of samples.&lt;/p&gt;
    &lt;p&gt;That said, we’re at a computer, so we could try increasing the sample count. Or maybe both?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;5 %&lt;/cell&gt;
        &lt;cell role="head"&gt;95 %&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Single operation (10,000 samples)&lt;/cell&gt;
        &lt;cell&gt;1.461&lt;/cell&gt;
        &lt;cell&gt;1.884&lt;/cell&gt;
        &lt;cell&gt;0.423&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Two operations (5,000 samples × 2)&lt;/cell&gt;
        &lt;cell&gt;1.461&lt;/cell&gt;
        &lt;cell&gt;1.630&lt;/cell&gt;
        &lt;cell&gt;0.169&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Single operation (100,000 samples)&lt;/cell&gt;
        &lt;cell&gt;1.549&lt;/cell&gt;
        &lt;cell&gt;1.680&lt;/cell&gt;
        &lt;cell&gt;0.131&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Two operations (50,000 samples × 2)&lt;/cell&gt;
        &lt;cell&gt;1.524&lt;/cell&gt;
        &lt;cell&gt;1.578&lt;/cell&gt;
        &lt;cell&gt;0.054&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It seems like sampling more cleverly has almost the same effect as taking ten times as many samples.&lt;/p&gt;
    &lt;p&gt;We could play around with where to put the breakpoint, and how many samples to allocate to each side of it, and see which combination yields the lowest error. Then we can run that combination with a lot of samples to get the most accurate final result. That would take maybe 15 minutes of tooting about and exploring sensible-seeming alternatives, so it’s probably still quicker than integrating by hand.&lt;/p&gt;
    &lt;head rend="h1"&gt;When the computer is not enough&lt;/head&gt;
    &lt;p&gt;It should be said that there are times when numeric solutions aren’t great. I hear that in electronics and quantum dynamics, there are sometimes integrals whose value is not a number, but a function, and knowing that function is important in order to know how the thing it’s modeling behaves in interactions with other things.&lt;/p&gt;
    &lt;p&gt;Those are not my domains, though. And when that’s not the case, the computer beats Feynman any day of the week.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/feynman-vs-computer"/><published>2025-12-04T16:03:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149375</id><title>Autism should not be treated as a single condition</title><updated>2025-12-04T23:37:00.672558+00:00</updated><content/><link href="https://www.economist.com/science-and-technology/2025/12/03/why-autism-should-not-be-treated-as-a-single-condition"/><published>2025-12-04T16:25:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149727</id><title>Launch HN: Browser Buddy (YC W24) – A recommendation system for Internet writing</title><updated>2025-12-04T23:37:00.569829+00:00</updated><content>&lt;doc fingerprint="3bd993f7b8d51a27"&gt;
  &lt;main&gt;
    &lt;p&gt;A For-You page for writing Explore the best essays and blogs on the Internet with Browser Buddy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.browserbuddy.com/"/><published>2025-12-04T16:52:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149813</id><title>Multivox: Volumetric Display</title><updated>2025-12-04T23:36:59.946346+00:00</updated><content>&lt;doc fingerprint="87cd881f7921b9c9"&gt;
  &lt;main&gt;
    &lt;p&gt;This is the code I currently use to drive my volumetric displays.&lt;/p&gt;
    &lt;p&gt;It supports two closely related devices which are configured in the &lt;code&gt;src/driver/gadgets&lt;/code&gt; directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotovox is a 400mm Orb featuring two 128x64 panels arranged vertically side by side.&lt;/item&gt;
      &lt;item&gt;Vortex is a 300mm Orb featuring two 128x64 panels arranged horizontally, back to back.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rotovox has a higher vertical resolution and better horizontal density; Vortex is brighter and has a higher refresh rate.&lt;/p&gt;
    &lt;p&gt;The 3D printable parts for Vortex are available here.&lt;/p&gt;
    &lt;p&gt;This code was originally written for a single display, and the device specific code was later somewhat abstracted out to support a second similar gadget. There are assumptions about the hardware that are pretty well baked in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It consists of two HUB75 LED panels spinning around a vertical axis.&lt;/item&gt;
      &lt;item&gt;The panels use either ABCDE addressing or ABC shift register addressing.&lt;/item&gt;
      &lt;item&gt;It uses a single GPIO (a photodiode or similar) to sync to rotation - high for 180°, low for 180°.&lt;/item&gt;
      &lt;item&gt;It's running on a Raspberry Pi 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The GPIO mappings and panel layout are defined in &lt;code&gt;src/driver/gadgets/gadget_&amp;lt;name&amp;gt;.h&lt;/code&gt;. GPIO is via memory mapped
access - if you're using a different model of Pi you'll need to change &lt;code&gt;BCM_BASE&lt;/code&gt; in the GPIO code. I haven't tested
this, and you should probably assume it doesn't work.&lt;/p&gt;
    &lt;p&gt;Input is via a bluetooth gamepad - I've been using an Xbox controller, and the input system is based on the default mapping for that.&lt;/p&gt;
    &lt;p&gt;Audio out is also via bluetooth. I haven't had success with the higher quality codecs, but the headset protocol works.&lt;/p&gt;
    &lt;p&gt;There are two parts to this code - the driver, which creates a voxel buffer in shared memory and scans its contents out in sync with rotation, and the client code which generates content and writes it into the voxel buffer. Both driver and client code are designed to run on the same device, a Raspberry Pi embedded in the hardware and spinning at several hundred RPM. There is a demo included in the Python directory which streams point clouds from a PC over wifi to the device, but fundamentally it's designed as a self contained gadget, like an alternate timeline Vectrex. A bluetooth gamepad is used to control the demos.&lt;/p&gt;
    &lt;code&gt;├── src
│   ├── driver
│   │   ├── gadgets         -- the different volumetric display configurations
│   │   │   └──             
│   │   └── vortex.c        -- driver code - creates a voxel buffer in shared memory,
│   │                          and handles scanning it out to the led panels in sync with
│   │                          the rotation
│   ├── simulator
│   │   └── virtex.c        -- software simulator - presents the same voxel buffer as
│   │                          the driver would, but renders the contents into an X11 window
│   │
│   ├── multivox            -- front end / launcher for the various volumetric toys
│   │   └──
│   ├── platform            -- common client code
│   │   └──
│   └── toys                -- a collection of volumetric demos using the shared voxel buffer
│       ├── eighty          -- multiplayer light cycles
│       ├── fireworks.c     -- cheesy first demo
│       ├── flight.c        -- some kind of 70s scifi thing
│       ├── tesseract.c     -- a 4D cubube
│       ├── viewer.c        -- viewer for .obj and .png files
│       └── zander          -- lander/zarch/virus-esque
├── python  
│   ├── calibration.py      -
│   ├── grid.py             -- some pattern generators, useful when calibrating the device
│   ├── colourwheel.py      -
│   ├── obj2c.py            -- tool for embedding .obj models in a header file
│   ├── pointvision.py      -- receive point clouds streamed from vortexstream.py
│   └── vortexstream.py     -- stream point clouds to pointvision.py
└── README.md               -- you are here
&lt;/code&gt;
    &lt;p&gt;On the Raspberry Pi, clone the repository:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/AncientJames/multivox.git
&lt;/code&gt;
    &lt;p&gt;Configure the project for your hardware:&lt;/p&gt;
    &lt;code&gt;cd multivox
mkdir build
cd build
cmake -DMULTIVOX_GADGET=vortex ..
cmake --build .
&lt;/code&gt;
    &lt;p&gt;First, the driver has to be running:&lt;/p&gt;
    &lt;code&gt;sudo ./vortex
&lt;/code&gt;
    &lt;p&gt;When invoked from the command line it periodically outputs profiling information (frame rate, rotation rate), and accepts keyboard input for various diagnostics:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;esc&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;b&lt;/cell&gt;
        &lt;cell&gt;Bit depth - cycles through 1, 2 or 3 bits per channel. Higher bit depths result in lower refresh rates&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;u&lt;/cell&gt;
        &lt;cell&gt;Uniformity - cycles through different strategies for trading off brightness against uniformity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;Trails - adjusts how far back to accumulate skipped voxels when the rotation rate is too high for the refresh rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;l&lt;/cell&gt;
        &lt;cell&gt;Lock - whether to adjust the rotation sync to keep it facing one way&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;d D&lt;/cell&gt;
        &lt;cell&gt;Drift - rotisserie mode. Introduces some explicit drift to the rotation sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;Panel - selectively disable the panels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xyz&lt;/cell&gt;
        &lt;cell&gt;Axis - When the display isn't spinning, it shows an othographic view. This lets you choose the axis&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;While that's running, try one of the toys:&lt;/p&gt;
    &lt;code&gt;./tesseract
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;viewer&lt;/code&gt; takes a list of .obj and .png files as arguments. You can scale, rotate and so on using the gamepad, and it
also accepts keyboard input when run remotely from the command line.&lt;/p&gt;
    &lt;code&gt;./viewer ~/Multivox/models/*.obj
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;esc&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LB/RB&lt;/cell&gt;
        &lt;cell&gt;[ / ]&lt;/cell&gt;
        &lt;cell&gt;Cycle through models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Walkthrough / Orbit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;X&lt;/cell&gt;
        &lt;cell&gt;Zoom to fit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;Toggle wireframe&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you don't have a physical volumetric display, there's a simulator, &lt;code&gt;virtex&lt;/code&gt;, which you can run in place of &lt;code&gt;vortex&lt;/code&gt;. It exposes the same voxel buffer in shared memory, but renders the contents using OpenGL in an X11 window.&lt;/p&gt;
    &lt;p&gt;Run without command line arguments it creates a display compatible with the currently configured gadget, but there are some options to let you experiment with different geometries:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-s X&lt;/cell&gt;
        &lt;cell&gt;slice count - the number of vertical slices per revolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-o X X&lt;/cell&gt;
        &lt;cell&gt;offsets - distance the front and back screens are offset from the axis, as a fraction of screen radius&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-b X&lt;/cell&gt;
        &lt;cell&gt;bits per channel (1 - 3)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-w X Y&lt;/cell&gt;
        &lt;cell&gt;panel resolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-g X&lt;/cell&gt;
        &lt;cell&gt;scan geometry - radial or linear. Linear looks better, but it's a lot harder to build.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;An idealised device with linear scanning and 3 bits per channel can be invoked like this:&lt;/p&gt;
    &lt;code&gt;./virtex -g l -s 128 -w 1280 1280 -b 3
&lt;/code&gt;
    &lt;p&gt;The simulator is fill rate intensive; if you're running it on a Raspberry Pi you'll probably want to reduce the slice count.&lt;/p&gt;
    &lt;p&gt;If you want it to start up automatically on boot, you can install &lt;code&gt;vortex&lt;/code&gt; as a service, and set &lt;code&gt;multivox&lt;/code&gt; to run on startup.&lt;/p&gt;
    &lt;p&gt;First install everything to its default location &lt;code&gt;~/Multivox&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;make install&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This will build the executable files and copy them into the destination directory, as well as creating &lt;code&gt;.mct&lt;/code&gt; files in &lt;code&gt;~/Multivox/carts&lt;/code&gt; for the built in toys.&lt;/p&gt;
    &lt;p&gt;Create the driver service:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/lib/systemd/system/vortex.service
&lt;/code&gt;
    &lt;p&gt;and fill in the following information:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=Vortex Display Driver
After=multi-user.target

[Service]
ExecStart=/home/pi/Multivox/bin/vortex

[Install]
WantedBy=multi-user.target
&lt;/code&gt;
    &lt;p&gt;Then start it up:&lt;/p&gt;
    &lt;code&gt;sudo systemctl daemon-reload
sudo systemctl enable vortex.service
&lt;/code&gt;
    &lt;p&gt;The driver assigns itself to core 3 - you can add &lt;code&gt;isolcpus=3&lt;/code&gt; to the end of &lt;code&gt;/boot/cmdline.txt&lt;/code&gt; to ensure it's the only thing running on that core.&lt;/p&gt;
    &lt;p&gt;You'll also want the launcher to start up on boot:&lt;/p&gt;
    &lt;code&gt;crontab -e
&lt;/code&gt;
    &lt;p&gt;And add the line:&lt;/p&gt;
    &lt;code&gt;@reboot /home/pi/Multivox/bin/multivox
&lt;/code&gt;
    &lt;p&gt;If everything goes smoothly, when you turn on the device it will boot up into &lt;code&gt;Multivox&lt;/code&gt;. This is a fantasy console which
acts as a launcher for all the games and demos you run on the hardware. The bundled toys are automatically installed in
the &lt;code&gt;~/Multivox/carts/&lt;/code&gt; directory as &lt;code&gt;.mct&lt;/code&gt; files, and external apps can be launched by adding a &lt;code&gt;.mct&lt;/code&gt; file containing
its command, path and arguments.&lt;/p&gt;
    &lt;p&gt;Each &lt;code&gt;.mct&lt;/code&gt; file appears as a cartridge in the Multivox front end. They should each have a label on the side; at the moment
all you can do to distinguish between them is change their colour in the &lt;code&gt;.mct&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When you exit an app back to the launcher, it saves a snapshot of the voxel volume, and this gives a preview of what you'll see when you launch a cart. This means there are two competing representations of the same information, and any future work on the front end will probably start with overhauling the entire approach.&lt;/p&gt;
    &lt;p&gt;Some basic UI for controls such as changing bit depth, rebooting and so on would also be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LB/RB&lt;/cell&gt;
        &lt;cell&gt;Cycle through carts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Launch cart&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;⧉&lt;/cell&gt;
        &lt;cell&gt;Exit / resume running cart&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;△ ▽&lt;/cell&gt;
        &lt;cell&gt;Change bit depth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;☰ x5&lt;/cell&gt;
        &lt;cell&gt;Power off&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/AncientJames/multivox"/><published>2025-12-04T16:58:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149849</id><title>Converge (YC S23) is hiring a martech expert in NYC</title><updated>2025-12-04T23:36:59.715450+00:00</updated><content>&lt;doc fingerprint="ad21d2738937a54f"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Technical Customer Success Manager&lt;/head&gt;
    &lt;p&gt;Converge is building the definitive Growth OS: We help DTC Growth teams understand which marketing efforts drive profitable growth. We are the only platform combining best-in-class tracking with blended reporting and multi-touch attribution.&lt;/p&gt;
    &lt;p&gt;Our unique positioning has led to rapid growth in both number and size of customers. One of the secrets of our growth is that we invest heavily in customer success. Whereas our competitors see success as a cost center, we take pride in delivering expert martech and marketing reporting support throughout the entire customer lifecycle and we compensate accordingly.&lt;/p&gt;
    &lt;p&gt;Our strategy is paying off, with 200+ paying customers (including some of the most famous DTC brands) and strong investor backing. We are now looking for a senior Technical Customer Success Manager to help us scale to $10M+ ARR.&lt;/p&gt;
    &lt;head rend="h3"&gt;Responsibilities&lt;/head&gt;
    &lt;p&gt;Be a marketing measurement expert: Advise customers on attribution, conversion tracking, and reporting strategies, positioning yourself as a trusted technical partner.&lt;/p&gt;
    &lt;p&gt;Technical support: Investigate and resolve conversion tracking and attribution issues reported through all channels, including email, Slack and in-app.&lt;/p&gt;
    &lt;p&gt;Onboard new customers: Own the customer onboarding end-to-end, driving them from initial implementation to real and lasting success.&lt;/p&gt;
    &lt;p&gt;Drive renewals: Take full ownership of renewal conversations, mitigating churn risk and implementing proactive retention strategies.&lt;/p&gt;
    &lt;p&gt;Champion customer needs: Surface trends and insights from collected customer feedback to the team at large to inform product roadmap.&lt;/p&gt;
    &lt;p&gt;Activate: Maximize the adoption of our product features and provide proactive, regular recommendations to get more out of the platform.&lt;/p&gt;
    &lt;p&gt;Expand customer contracts: Identify and execute expansion opportunities to increase account value.&lt;/p&gt;
    &lt;p&gt;Lead strategic projects: Improve the support experience and feature adoption.&lt;/p&gt;
    &lt;head rend="h3"&gt;You will thrive in this role if you&lt;/head&gt;
    &lt;p&gt;Have strong martech experience: Google Tag Manager, Meta Events Manager, Google Consent Mode and other pieces of the martech stack have no secrets for you.&lt;/p&gt;
    &lt;p&gt;Are curious and technical: You love understanding complex products deeply. Bonus points if you already love JS debugging, sifting through network requests or reasoning over attribution logic.&lt;/p&gt;
    &lt;p&gt;Thrive in ambiguity: You enjoy building processes from scratch and figuring things out without a playbook.&lt;/p&gt;
    &lt;p&gt;Are commercially minded: You know how to uncover customer needs and tie solutions to real business value.&lt;/p&gt;
    &lt;p&gt;Have advertising experience: You speak the language of a growth team, and have experience with Ads Managers, attribution and creative strategy.&lt;/p&gt;
    &lt;head rend="h3"&gt;This role is not for you if you&lt;/head&gt;
    &lt;p&gt;Do not want to become an expert: Our customers choose us because we deeply understand their technical challenges.&lt;/p&gt;
    &lt;p&gt;Prefer certainty over upside: There are no rigid and limited responsibilities here - we grant a lot of agency and expect a lot of accountability.&lt;/p&gt;
    &lt;p&gt;Don't like working hard: This role demands more commitment and agency than a typical success role.&lt;/p&gt;
    &lt;p&gt;Prefer remote over in-person: We believe being in-person helps us move faster.&lt;/p&gt;
    &lt;head rend="h3"&gt;What we offer&lt;/head&gt;
    &lt;p&gt;Compensation: $155k - $217k + equity: 0.1% - 0.25%.&lt;/p&gt;
    &lt;p&gt;Career-defining opportunity to build the U.S. success function and work with the world's best DTC growth teams.&lt;/p&gt;
    &lt;p&gt;Private health, dental, and vision insurance.&lt;/p&gt;
    &lt;p&gt;Pension &amp;amp; 401k contributions.&lt;/p&gt;
    &lt;p&gt;Opportunity to work on a complex product that customers love - 35% of our users use us daily (!)&lt;/p&gt;
    &lt;head rend="h3"&gt;Interview process*&lt;/head&gt;
    &lt;p&gt;Application: We're looking to see how your skills and experience align with our needs.&lt;/p&gt;
    &lt;p&gt;Intro interview (30-min): Our goal is to learn more about what you are looking for in your next role, explore your motivations to join our team, why you would be a great fit, and answer questions about us.&lt;/p&gt;
    &lt;p&gt;Culture interview (45-min): We will walk through your experience and background in detail.&lt;/p&gt;
    &lt;p&gt;Case interview (1 hour): We will simulate a real customer situation.&lt;/p&gt;
    &lt;p&gt;Offer If everyoneâs aligned, weâll move quickly to make you an offer.&lt;/p&gt;
    &lt;p&gt;(*) can be done in 2 days, just flag to us that you want to do it fast.&lt;/p&gt;
    &lt;head rend="h2"&gt;We raised $5.7M from some of the best investors&lt;/head&gt;
    &lt;head rend="h3"&gt;James Hawkins&lt;/head&gt;
    &lt;head rend="h3"&gt;Nicolas Dessaigne&lt;/head&gt;
    &lt;head rend="h2"&gt;What makes Converge unique&lt;/head&gt;
    &lt;head rend="h3"&gt;Ridiculously lean&lt;/head&gt;
    &lt;p&gt;We operate a &amp;gt;$1M ARR business with &amp;gt;200 customers with a team of just 9 people.&lt;/p&gt;
    &lt;p&gt;Why you should care:&lt;/p&gt;
    &lt;p&gt;You will not find a startup with this level of product-market-fit where you can join as employee #10.&lt;/p&gt;
    &lt;head rend="h3"&gt;Huge product surface&lt;/head&gt;
    &lt;p&gt;We compete with Segment, Fivetran, Google Tag Manager, Rockerbox, Looker, just to name a few.&lt;/p&gt;
    &lt;p&gt;Why you should care:&lt;/p&gt;
    &lt;p&gt;Other startups give you ownership of a feature. At Converge, you get ownership over an entire product.&lt;/p&gt;
    &lt;head rend="h3"&gt;Customers rely on us&lt;/head&gt;
    &lt;p&gt;Converge sees 35% of its users daily, while this is only 13% for the average SaaS company.&lt;/p&gt;
    &lt;p&gt;Why you should care:&lt;/p&gt;
    &lt;p&gt;Our customers will be excited by every feature you ship, and your impact will be felt immediately.&lt;/p&gt;
    &lt;head rend="h3"&gt;Real scale&lt;/head&gt;
    &lt;p&gt;We collect around 20M customer interactions per day and process ~$3B in GMV annually.&lt;/p&gt;
    &lt;p&gt;Why you should care:&lt;/p&gt;
    &lt;p&gt;Even though you join early, this job comes with real engineering challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we started&lt;/head&gt;
    &lt;head rend="h3"&gt;Did you knowâ¦&lt;/head&gt;
    &lt;p&gt;All co-founders have written code that has run in production as part of Converge.&lt;/p&gt;
    &lt;p&gt;We closed our first publicly traded company during our YC batch from our living room in San Francisco.&lt;/p&gt;
    &lt;p&gt;Thomas and Tiago (Founding Engineer) worked together when Thomas was just an intern.&lt;/p&gt;
    &lt;p&gt;Michel (Customer Success) was responsible for most of the incoming Converge Support tickets in his previous job as a freelance tracking consultant.&lt;/p&gt;
    &lt;p&gt;Thomas and Jan were best friends in high school, and Jan and Jerome met in their first year of college.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.runconverge.com/careers/technical-customer-success-manager"/><published>2025-12-04T17:00:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46150447</id><title>PyTogether: Collaborative lightweight real-time Python IDE for teachers/learners</title><updated>2025-12-04T23:36:59.194612+00:00</updated><content>&lt;doc fingerprint="123b677d1dffb17a"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt; PyTogether&lt;lb/&gt; Google docs for Python. A fully browser-based collaborative Python IDE with real-time editing, chat, and visualization. &lt;lb/&gt; pytogether.org &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time Collaboration - Edit Python code together instantly using Y.js.&lt;/item&gt;
      &lt;item&gt;Secure Authentication - Log in manually or with Google OAuth.&lt;/item&gt;
      &lt;item&gt;Groups &amp;amp; Projects - Organize your work into teams and projects.&lt;/item&gt;
      &lt;item&gt;Share Links - Share your code snippets to others to edit or just run!&lt;/item&gt;
      &lt;item&gt;Live Drawings - Draw directly on the IDE to assist with note-taking or teaching.&lt;/item&gt;
      &lt;item&gt;Live Cursors/Selections - Google docs-like live selections for smoother collaboration.&lt;/item&gt;
      &lt;item&gt;Live Chat and Voice Calls - Real-time messaging, and Discord-like voice chats for each project.&lt;/item&gt;
      &lt;item&gt;Code Linting - Integrated CodeMirror linting for cleaner, error-free code.&lt;/item&gt;
      &lt;item&gt;Smart Autosave - Code is automatically saved every minute and on exit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When starting out in programming, many beginners find traditional IDEs overwhelming: full of plugins, extensions, configuration steps, paywalls, and complex UIs. PyTogether removes these barriers by offering a lightweight, distraction-free environment where you can focus on writing Python code right away.&lt;/p&gt;
    &lt;p&gt;The platform is designed for learning, teaching, and pair programming, making it ideal for classrooms, coding clubs, or quick collaborations.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: PyTogether is intended for educational purposes and beginner use. It is not optimized for large-scale production development.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While there are many online IDEs (Replit, Jupyter, Google Colab, etc.), PyTogether is built with a different goal: simplicity first.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;⚡Instant Setup⚡- No downloads, no pip installs, no hidden complexity. Just create a group, create a project, and bam!&lt;/item&gt;
      &lt;item&gt;Beginner Focused - No confusing menus, terminals, or configuration. Just code and run.&lt;/item&gt;
      &lt;item&gt;Real-Time Collaboration - Work together with classmates, friends, or mentors in the same editor.&lt;/item&gt;
      &lt;item&gt;Safe Learning Space - Limited features by design to reduce distractions and keep beginners focused.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unlike production-grade IDEs, PyTogether prioritizes ease of use and collaboration for learners rather than advanced features.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Backend: Django, Django REST Framework (DRF)&lt;/item&gt;
      &lt;item&gt;Real-Time: Y.js, WebSockets (Django Channels)&lt;/item&gt;
      &lt;item&gt;Async Processing: Celery&lt;/item&gt;
      &lt;item&gt;Data Store: PostgreSQL (via Supabase)&lt;/item&gt;
      &lt;item&gt;Caching, Broker, &amp;amp; Channel layers: Redis&lt;/item&gt;
      &lt;item&gt;Frontend: React, Tailwind CSS, CodeMirror (code linting)&lt;/item&gt;
      &lt;item&gt;Python Execution: Pyodide (via Web Worker)&lt;/item&gt;
      &lt;item&gt;Deployment: Vercel (Frontend), Docker on VPS (Backend), Nginx (reverse proxy)&lt;/item&gt;
      &lt;item&gt;CI/CD: GitHub Actions (deploy backend to VPS on push to main)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requirements: Docker, Node&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Running PyTogether locally is a simple two-step process. Run the following commands from the project root:&lt;/p&gt;
    &lt;code&gt;# 1. Install all dependencies (automatically does it for root and frontend)
npm install

# 2. Start the servers
npm run dev&lt;/code&gt;
    &lt;p&gt;This will install all required packages and run the backend container and start the frontend. It should take around 2-5 minutes on initial launch. The frontend will be live on http://localhost:5173. You can do CTRL+C to stop the program/containers.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note Two superusers are created automatically:&lt;/p&gt;&lt;item&gt;Email:&lt;/item&gt;&lt;code&gt;test1@gmail.com&lt;/code&gt;&lt;item&gt;Email:&lt;/item&gt;&lt;code&gt;test2@gmail.com&lt;/code&gt;&lt;p&gt;Both have the password&lt;/p&gt;&lt;code&gt;testtest&lt;/code&gt;. You can log in with them on the frontend.&lt;/quote&gt;
    &lt;p&gt;You may also adjust the settings in backend/backend/settings/dev.py&lt;/p&gt;
    &lt;p&gt;Jawad Rizvi&lt;/p&gt;
    &lt;p&gt;Applied Mathematics &amp;amp; Computer Engineering student at Queen's University.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/SJRiz/pytogether"/><published>2025-12-04T17:43:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46150715</id><title>Why are 38 percent of Stanford students saying they're disabled?</title><updated>2025-12-04T23:36:58.991313+00:00</updated><content>&lt;doc fingerprint="1e71405b38cb1c86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Are 38 Percent of Stanford Students Saying They're Disabled?&lt;/head&gt;
    &lt;head rend="h2"&gt;If you get into an elite college, you probably don't have a learning disability.&lt;/head&gt;
    &lt;p&gt;The students at America's elite universities are supposed to be the smartest, most promising young people in the country. And yet, shocking percentages of them are claiming academic accommodations designed for students with learning disabilities.&lt;/p&gt;
    &lt;p&gt;In an article published this week in The Atlantic, education reporter Rose Horowitch lays out some shocking numbers. At Brown and Harvard, 20 percent of undergraduate students are disabled. At Amherst College, that's 34 percent. At Stanford University, it's a galling 38 percent. Most of these students are claiming mental health conditions and learning disabilities, like anxiety, depression, and ADHD.&lt;/p&gt;
    &lt;p&gt;Obviously, something is off here. The idea that some of the most elite, selective universities in America—schools that require 99th percentile SATs and sterling essays—would be educating large numbers of genuinely learning disabled students is clearly bogus. A student with real cognitive struggles is much more likely to end up in community college, or not in higher education at all, right?&lt;/p&gt;
    &lt;p&gt;The professors Horowitz interviewed largely back up this theory. "You hear 'students with disabilities' and it's not kids in wheelchairs," one professor told Horowitch. "It's just not. It's rich kids getting extra time on tests." Talented students get to college, start struggling, and run for a diagnosis to avoid bad grades. Ironically, the very schools that cognitively challenged students are most likely to attend—community colleges—have far lower rates of disabled students, with only three to four percent of such students getting accommodations.&lt;/p&gt;
    &lt;p&gt;To be fair, some of the students receiving these accommodations do need them. But the current language of the Americans with Disabilities Act (ADA) allows students to get expansive accommodations with little more than a doctor's note.&lt;/p&gt;
    &lt;p&gt;While some students are no doubt seeking these accommodations as semi-conscious cheaters, I think most genuinely identify with the mental health condition they're using to get extra time on tests. Over the past few years, there's been a rising push to see mental health and neurodevelopmental conditions as not just a medical fact, but an identity marker. Will Lindstrom, the director of the Regents' Center for Learning Disorders at the University of Georgia, told Horowitch that he sees a growing number of students with this perspective. "It's almost like it's part of their identity," Lindstrom told her. "By the time we see them, they're convinced they have a neurodevelopmental disorder."&lt;/p&gt;
    &lt;p&gt;What's driving this trend? Well, the way conditions like ADHD, autism, and anxiety get talked about online—the place where most young people first learn about these conditions—is probably a contributing factor. Online creators tend to paint a very broad picture of the conditions they describe. A quick scroll of TikTok reveals creators labeling everything from always wearing headphones, to being bad at managing your time, to doodling in class as a sign that someone may have a diagnosable condition. According to these videos, who isn't disabled?&lt;/p&gt;
    &lt;p&gt;The result is a deeply distorted view of "normal." If ever struggling to focus or experiencing boredom is a sign you have ADHD, the implication is that a "normal," nondisabled person has essentially no problems. A "neurotypical" person, the thinking goes, can churn out a 15-page paper with no hint of procrastination, maintain perfect focus during a boring lecture, and never experience social anxiety or awkwardness. This view is buffeted by the current way many of these conditions are diagnosed. As Horowitch points out, when the latest issue of the DSM, the manual psychiatrists use to diagnose patients, was released in 2013, it significantly lowered the bar for an ADHD diagnosis. When the definition of these conditions is set so liberally, it's easy to imagine a highly intelligent Stanford student becoming convinced that any sign of academic struggle proves they're learning disabled, and any problems making friends are a sign they have autism.&lt;/p&gt;
    &lt;p&gt;Risk-aversion, too, seems like a compelling factor driving bright students to claim learning disabilities. Our nation's most promising students are also its least assured. So afraid of failure—of bad grades, of a poorly-received essay—they take any sign of struggle as a diagnosable condition. A few decades ago, a student who entered college and found the material harder to master and their time less easily managed than in high school would have been seen as relatively normal. Now, every time she picks up her phone, a barrage of influencers is clamoring to tell her this is a sign she has ADHD. Discomfort and difficulty are no longer perceived as typical parts of growing up.&lt;/p&gt;
    &lt;p&gt;In this context, it's easy to read the rise of academic accommodations among the nation's most intelligent students as yet another manifestation of the risk-aversion endemic in the striving children of the upper middle class. For most of the elite-college students who receive them, academic accommodations are a protection against failure and self-doubt. Unnecessary accommodations are a two-front form of cheating—they give you an unjust leg-up on your fellow students, but they also allow you to cheat yourself out of genuine intellectual growth. If you mask learning deficiencies with extra time on texts, soothe social anxiety by forgoing presentations, and neglect time management skills with deadline extensions, you might forge a path to better grades. But you'll also find yourself less capable of tackling the challenges of adult life.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://reason.com/2025/12/04/why-are-38-percent-of-stanford-students-saying-theyre-disabled/"/><published>2025-12-04T18:04:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46151335</id><title>Hammersmith Bridge – Where did 25,000 vehicles go?</title><updated>2025-12-04T23:36:58.944651+00:00</updated><content/><link href="https://nickmaini.substack.com/p/hammersmith-bridge"/><published>2025-12-04T18:52:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46151578</id><title>The RAM shortage comes for us all</title><updated>2025-12-04T23:36:58.832000+00:00</updated><content>&lt;doc fingerprint="be74b0991ca603be"&gt;
  &lt;main&gt;
    &lt;p&gt;Memory price inflation comes for us all, and if you're not affected yet, just wait.&lt;/p&gt;
    &lt;p&gt;I was building a new PC last month using some parts I had bought earlier this year. The 64 Gigabyte T-Create DDR5 memory kit I used cost $209 then. Today? The same kit costs $650!&lt;/p&gt;
    &lt;p&gt;Just in the past week, we found out Raspberry Pi's increasing their single board computer prices. Micron's killing the Crucial brand of RAM and storage devices completely, meaning there's gonna be one fewer consumer memory manufacturer. Samsung can't even buy RAM from themselves to build their own Smartphones, and small vendors like Libre Computer and Mono are seeing RAM prices double, triple, or even worse, and they're not even buying the latest RAM tech!&lt;/p&gt;
    &lt;p&gt;I think PC builders might be the first crowd to get impacted across the board—just look at these insane graphs from PC Parts Picker, showing RAM prices going from like $30 to $120 for DDR4, or like $150 to five hundred dollars for 64 gigs of DDR5.&lt;/p&gt;
    &lt;p&gt;But the impacts are only just starting to hit other markets.&lt;/p&gt;
    &lt;p&gt;Libre Computer mentioned on Twitter a single 4 gigabyte module of LPDDR4 memory costs $35. That's more expensive than every other component on one of their single board computers combined! You can't survive selling products at a loss, so once the current production batches are sold through, either prices will be increased, or certain product lines will go out of stock.&lt;/p&gt;
    &lt;p&gt;The smaller the company, the worse the price hit will be. Even Raspberry Pi, who I'm sure has a little more margin built in, already raised SBC prices (and introduced a 1 GB Pi 5—maybe a good excuse for developers to drop Javascript frameworks and program for lower memory requirements again?).&lt;/p&gt;
    &lt;p&gt;Cameras, gaming consoles, tablets, almost anything that has memory will get hit sooner or later.&lt;/p&gt;
    &lt;p&gt;I can't believe I'm saying this, but compared to the current market, Apple's insane memory upgrade pricing is... actually in line with the rest of the industry.&lt;/p&gt;
    &lt;p&gt;The reason for all this, of course, is AI datacenter buildouts. I have no clue if there's any price fixing going on like there was a few decades ago—that's something conspiracy theorists can debate—but the problem is there's only a few companies producing all the world's memory supplies.&lt;/p&gt;
    &lt;p&gt;And those companies all realized they can make billions more dollars making RAM just for AI datacenter products, and neglect the rest of the market.&lt;/p&gt;
    &lt;p&gt;So they're shutting down their consumer memory lines, and devoting all production to AI.&lt;/p&gt;
    &lt;p&gt;Even companies like GPU board manufacturers are getting shafted; Nvidia's not giving memory to them along with their chips like they used to, basically telling them "good luck, you're on your own for VRAM now!"&lt;/p&gt;
    &lt;p&gt;Which is especially rich, because Nvidia's profiting obscenely off of all this stuff.&lt;/p&gt;
    &lt;p&gt;That's all bad enough, but some people see a silver lining. I've seen some people say "well, once the AI bubble bursts, at least we'll have a ton of cheap hardware flooding the market!"&lt;/p&gt;
    &lt;p&gt;And yes, in past decades, that might be one outcome.&lt;/p&gt;
    &lt;p&gt;But the problem here is the RAM they're making, a ton of it is either integrated into specialized GPUs that won't run on normal computers, or being fitted into special types of memory modules that don't work on consumer PCs, either. (See: HBM).&lt;/p&gt;
    &lt;p&gt;That, and the GPUs and servers being deployed now don't even run on normal power and cooling, they're part of massive systems that would take a ton of effort to get running in even the most well-equipped homelabs. It's not like the classic Dell R720 that just needs some air and a wall outlet to run.&lt;/p&gt;
    &lt;p&gt;That is to say, we might be hitting a weird era where the PC building hobby is gutted, SBCs get prohibitively expensive, and anyone who didn't stockpile parts earlier this year is, pretty much, in a lurch.&lt;/p&gt;
    &lt;p&gt;Even Lenovo admits to stockpiling RAM, making this like the toilet paper situation back in 2020, except for massive corporations. Not enough supply, so companies who can afford to get some will buy it all up, hoping to stave off the shortages that will probably last longer, partly because of that stockpiling.&lt;/p&gt;
    &lt;p&gt;I don't think it's completely outlandish to think some companies will start scavenging memory chips (ala dosdude1) off other systems for stock, especially if RAM prices keep going up.&lt;/p&gt;
    &lt;p&gt;It's either that, or just stop making products. There are some echoes to the global chip shortages that hit in 2021-2022, and that really shook up the market for smaller companies.&lt;/p&gt;
    &lt;p&gt;I hate to see it happening again, but somehow, here we are a few years later, except this time, the AI bubble is to blame.&lt;/p&gt;
    &lt;p&gt;Sorry for not having a positive note to end this on, but I guess... maybe it's a good time to dig into that pile of old projects you never finished instead of buying something new this year.&lt;/p&gt;
    &lt;p&gt;How long will this last? That's anybody's guess. But I've already put off some projects I was gonna do for 2026, and I'm sure I'm not the only one.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Enterprise grade (i.e. large capacity, SAS) drives shortage is already here. Dell told us this morning the delivery time for 24 TB SAS drives is about 24 months out....&lt;/p&gt;
    &lt;p&gt;Oof, 24 months the universal term for "it will be anywhere from 2 years to never before you get that order"&lt;/p&gt;
    &lt;p&gt;Yup. Already leveraging old gear here.&lt;/p&gt;
    &lt;p&gt;Rebuilt my Jan-2019 8GB i3 NUC with current ubuntu LTS and am using it as a dockerized jellfin music server, after (re)ripping all my CDs (remember them ?). Gotta love local only and not needing to deal with Plex accounts/limitations/etc. silliness.&lt;/p&gt;
    &lt;p&gt;Ripped everything with xld on the mac mini to Apple Lossless format. Super good ripper, even corrects and notes when the CD is damaged or dirty. Re-ripping bad tracks after cleaning the CD. Highly recommend 'Finamp' as a great mobile platform player that can run in the background nicely while doing other things on the phone/tablet.&lt;/p&gt;
    &lt;p&gt;You might revisit and update your old post about ripping your DVDs from a few years ago if you get really bored....&lt;/p&gt;
    &lt;p&gt;It's been at least a decade that computing is no longer a hobby for everyone, you can't find entry level GPUs or SBC at cheap prices. On the other hand with AI less people feel the need to hack boards or pull juice from cheap boards... so it will become a lost art :(&lt;/p&gt;
    &lt;p&gt;Great timing for Microsoft to have put 50% of Windows 10 PCs on a 1 year clock to obsolescence. (Yes, I know there are many open source options on the GNU/Linux side, but some folks don't know about that yet)&lt;/p&gt;
    &lt;p&gt;It is just so sad growing up really loving the PC as a hobby, only to see it beaten up and spat on multiple times within the last decade.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/ram-shortage-comes-us-all"/><published>2025-12-04T19:16:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46152941</id><title>A Cozy Mk IV light aircraft crashed after 3D-printed part was weakened by heat</title><updated>2025-12-04T23:36:58.696769+00:00</updated><content>&lt;doc fingerprint="ac44743986358578"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Plane crashed after 3D-printed part collapsed&lt;/head&gt;
    &lt;p&gt;A plane crashed after a 3D-printed part softened and collapsed, causing its engine to lose power, a report has found.&lt;/p&gt;
    &lt;p&gt;The Cozy Mk IV light aircraft was destroyed after its plastic air induction elbow, bought at an air show in North America, collapsed.&lt;/p&gt;
    &lt;p&gt;The aircraft crashed into a landing aid system at Gloucestershire Airport in Staverton on 18 March at 13:04 GMT, after its engine lost power. The sole occupant was taken to hospital with minor injuries.&lt;/p&gt;
    &lt;p&gt;The Air Accidents Investigation Branch (AAIB) said in a report that the induction elbow was made of "inappropriate material" and safety actions will be taken in future regarding 3D printed parts.&lt;/p&gt;
    &lt;p&gt;Following an "uneventful local flight", the AAIB report said the pilot advanced the throttle on the final approach to the runway, and realised the engine had suffered a complete loss of power.&lt;/p&gt;
    &lt;p&gt;"He managed to fly over a road and a line of bushes on the airfield boundary, but landed short and struck the instrument landing system before coming to rest at the side of the structure," the report read.&lt;/p&gt;
    &lt;p&gt;It was revealed the part had been installed during a modification to the fuel system and collapsed due to its 3D-printed plastic material softening when exposed to heat from the engine.&lt;/p&gt;
    &lt;p&gt;The Light Aircraft Association (LAA) said it now intends to take safety actions in response to the accident, including a "LAA Alert" regarding the use of 3D-printed parts that will be sent to inspectors.&lt;/p&gt;
    &lt;p&gt;Follow BBC Gloucestershire on Facebook, X and Instagram. Send your story ideas to us on email or via WhatsApp on 0800 313 4630.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c1w932vqye0o"/><published>2025-12-04T20:56:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46153058</id><title>CUDA-l2: Surpassing cuBLAS performance for matrix multiplication through RL</title><updated>2025-12-04T23:36:58.132199+00:00</updated><content>&lt;doc fingerprint="df7c7dfdcca34d95"&gt;
  &lt;main&gt;
    &lt;p&gt;CUDA-L2 is a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art NVIDIA closed-source libraries (cuBLAS, cuBLASLt-heuristic, cuBLASLt-AutoTuning). Paper&lt;/p&gt;
    &lt;p&gt;Speedup of CUDA-L2 over torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning across 1000 (M,N,K) configurations on A100.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[Dec 2, 2025] Released A100 optimized HGEMM kernels across 1,000 configurations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Release HGEMM with 32-bit accumulator (SM80_16x8x16_F16F16F16F32 and F32F16F16F32 officially) for A100. Current version only support 16-bit accumulator (SM80_16x8x16_F16F16F16F16).&lt;/item&gt;
      &lt;item&gt;Support denser matrix configurations (more configurations).&lt;/item&gt;
      &lt;item&gt;Extend to more GPUs (Ada Lovelace, Hopper, Blackwell).&lt;/item&gt;
      &lt;item&gt;Easy deployment for open-source LLMs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Do A100 kernels apply to other machines like RTX 3090 or H100?&lt;/p&gt;
    &lt;p&gt;A: Ideally, kernels trained on A100 should only be used on A100 if you are targeting speedup. They might have speedup on other machines, but it's not guaranteed. We will progressively release kernels trained on different machines.&lt;/p&gt;
    &lt;p&gt;Q: What if I need matrix dimensions (M, N, K) not found in your configurations?&lt;/p&gt;
    &lt;p&gt;A: 1. You can find the nearest neighbor configuration (larger than yours) and pad with zeros. 2. Feel free to post your dimensions on GitHub issues. We are happy to release kernels for your configuration.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python: Ensure you have a working Python environment.&lt;/item&gt;
      &lt;item&gt;PyTorch: This project requires PyTorch version 2.6.0 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project depends on NVIDIA CUTLASS. You must clone specific tag &lt;code&gt;v4.2.1&lt;/code&gt; into a directory named &lt;code&gt;cutlass&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;git clone -b v4.2.1 https://github.com/NVIDIA/cutlass.git cutlass&lt;/code&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Warning: Please ensure you download the correct CUTLASS version (&lt;code&gt;v4.2.1&lt;/code&gt;) and set the&lt;code&gt;CUTLASS_DIR&lt;/code&gt;environment variable correctly. Incorrect CUTLASS setup may cause the project to fail silently or produce no results.&lt;/quote&gt;
    &lt;p&gt;Before building or running the project, you must configure the following environment variables:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CUTLASS_DIR&lt;/code&gt;: Points to the directory where you cloned CUTLASS.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TORCH_CUDA_ARCH_LIST&lt;/code&gt;: Specifies the target GPU architecture (e.g., "8.0" for NVIDIA Ampere / A100 / RTX 30 series).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run the following commands:&lt;/p&gt;
    &lt;code&gt;export CUTLASS_DIR=/path/to/your/cutlass
export TORCH_CUDA_ARCH_LIST="8.0"&lt;/code&gt;
    &lt;p&gt;To run the evaluation, use the &lt;code&gt;eval_one_file.sh&lt;/code&gt; script. Below is an example command for offline mode:&lt;/p&gt;
    &lt;code&gt;./eval_one_file.sh --mnk 64_4096_64 --warmup_seconds 5 --benchmark_seconds 10 --base_dir ./results --gpu_device_id 7 --mode offline&lt;/code&gt;
    &lt;p&gt;For server mode, you need to specify &lt;code&gt;--target_qps&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;./eval_one_file.sh --mnk 64_4096_64 --warmup_seconds 5 --benchmark_seconds 10 --base_dir ./results --gpu_device_id 7 --mode server --target_qps 100&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Argument&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mnk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Specifies the problem size (e.g., &lt;code&gt;64_4096_64&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--warmup_seconds&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Duration of warmup in seconds before timing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--benchmark_seconds&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Duration of benchmarking in seconds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--base_dir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Directory to save the compile and output results.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--gpu_device_id&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The ID of the GPU to use (e.g., &lt;code&gt;7&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mode&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execution mode. Options are:&lt;p&gt;•&lt;/p&gt;&lt;code&gt;offline&lt;/code&gt;: Runs the evaluation in offline/batch processing mode.&lt;p&gt;•&lt;/p&gt;&lt;code&gt;server&lt;/code&gt;: Runs the evaluation in server mode (simulating request-based scenarios).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--target_qps&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Target Queries Per Second (QPS) for server mode. Required if mode is &lt;code&gt;server&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you have any questions, please open a GitHub issue or reach out to us at jiwei_li@deep-reinforce.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/deepreinforce-ai/CUDA-L2"/><published>2025-12-04T21:04:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46153116</id><title>Django 6</title><updated>2025-12-04T23:36:57.979148+00:00</updated><content>&lt;doc fingerprint="3ab9991568719b8a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Django 6.0 release notes¶&lt;/head&gt;
    &lt;p&gt;December 3, 2025&lt;/p&gt;
    &lt;p&gt;Welcome to Django 6.0!&lt;/p&gt;
    &lt;p&gt;These release notes cover the new features, as well as some backwards incompatible changes you should be aware of when upgrading from Django 5.2 or earlier. We’ve begun the deprecation process for some features.&lt;/p&gt;
    &lt;p&gt;See the How to upgrade Django to a newer version guide if you’re updating an existing project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Python compatibility¶&lt;/head&gt;
    &lt;p&gt;Django 6.0 supports Python 3.12, 3.13, and 3.14. We highly recommend, and only officially support, the latest release of each series.&lt;/p&gt;
    &lt;p&gt;The Django 5.2.x series is the last to support Python 3.10 and 3.11.&lt;/p&gt;
    &lt;head rend="h2"&gt;Third-party library support for older versions of Django¶&lt;/head&gt;
    &lt;p&gt;Following the release of Django 6.0, we suggest that third-party app authors drop support for all versions of Django prior to 5.2. At that time, you should be able to run your package’s tests using &lt;code&gt;python -Wd&lt;/code&gt; so that deprecation
warnings appear. After making the deprecation warning fixes, your app should be
compatible with Django 6.0.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s new in Django 6.0¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Content Security Policy support¶&lt;/head&gt;
    &lt;p&gt;Built-in support for the Content Security Policy (CSP) standard is now available, making it easier to protect web applications against content injection attacks such as cross-site scripting (XSS). CSP allows declaring trusted sources of content by giving browsers strict rules about which scripts, styles, images, or other resources can be loaded.&lt;/p&gt;
    &lt;p&gt;CSP policies can now be enforced or monitored directly using built-in tools: headers are added via the &lt;code&gt;ContentSecurityPolicyMiddleware&lt;/code&gt;, nonces are
supported through the &lt;code&gt;csp()&lt;/code&gt; context
processor, and policies are configured using the &lt;code&gt;SECURE_CSP&lt;/code&gt; and
&lt;code&gt;SECURE_CSP_REPORT_ONLY&lt;/code&gt; settings.&lt;/p&gt;
    &lt;p&gt;These settings accept Python dictionaries and support Django-provided constants for clarity and safety. For example:&lt;/p&gt;
    &lt;code&gt;from django.utils.csp import CSP

SECURE_CSP = {
    "default-src": [CSP.SELF],
    "script-src": [CSP.SELF, CSP.NONCE],
    "img-src": [CSP.SELF, "https:"],
}
&lt;/code&gt;
    &lt;p&gt;The resulting &lt;code&gt;Content-Security-Policy&lt;/code&gt; header would be set to:&lt;/p&gt;
    &lt;code&gt;default-src 'self'; script-src 'self' 'nonce-SECRET'; img-src 'self' https:
&lt;/code&gt;
    &lt;p&gt;To get started, follow the CSP how-to guide. For in-depth guidance, see the CSP security overview and the reference docs, which include details about decorators to override or disable policies on a per-view basis.&lt;/p&gt;
    &lt;head rend="h3"&gt;Template Partials¶&lt;/head&gt;
    &lt;p&gt;The Django Template Language now supports template partials, making it easier to encapsulate and reuse small named fragments within a template file. The new tags &lt;code&gt;{% partialdef %}&lt;/code&gt; and &lt;code&gt;{% partial %}&lt;/code&gt;
define a partial and render it, respectively.&lt;/p&gt;
    &lt;p&gt;Partials can also be referenced using the &lt;code&gt;template_name#partial_name&lt;/code&gt; syntax
with &lt;code&gt;get_template()&lt;/code&gt;,
&lt;code&gt;render()&lt;/code&gt;, &lt;code&gt;{% include %}&lt;/code&gt;, and other
template-loading tools, enabling more modular and maintainable templates
without needing to split components into separate files.&lt;/p&gt;
    &lt;p&gt;A migration guide is available if you’re updating from the django-template-partials third-party package.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background Tasks¶&lt;/head&gt;
    &lt;p&gt;Django now includes a built-in Tasks framework for running code outside the HTTP request–response cycle. This enables offloading work, such as sending emails or processing data, to background workers.&lt;/p&gt;
    &lt;p&gt;The framework provides task definition, validation, queuing, and result handling. Django guarantees consistent behavior for creating and managing tasks, while the responsibility for running them continues to belong to external worker processes.&lt;/p&gt;
    &lt;p&gt;Tasks are defined using the &lt;code&gt;task()&lt;/code&gt; decorator:&lt;/p&gt;
    &lt;code&gt;from django.core.mail import send_mail
from django.tasks import task


@task
def email_users(emails, subject, message):
    return send_mail(subject, message, None, emails)
&lt;/code&gt;
    &lt;p&gt;Once defined, tasks can be enqueued through a configured backend:&lt;/p&gt;
    &lt;code&gt;email_users.enqueue(
    emails=["user@example.com"],
    subject="You have a message",
    message="Hello there!",
)
&lt;/code&gt;
    &lt;p&gt;Backends are configured via the &lt;code&gt;TASKS&lt;/code&gt; setting. The two
built-in backends included in this release are
primarily intended for development and testing.&lt;/p&gt;
    &lt;p&gt;Django handles task creation and queuing, but does not provide a worker mechanism to run tasks. Execution must be managed by external infrastructure, such as a separate process or service.&lt;/p&gt;
    &lt;p&gt;See Django’s Tasks framework for an overview and the Tasks reference for API details.&lt;/p&gt;
    &lt;head rend="h3"&gt;Adoption of Python’s modern email API¶&lt;/head&gt;
    &lt;p&gt;Email handling in Django now uses Python’s modern email API, introduced in Python 3.6. This API, centered around the &lt;code&gt;email.message.EmailMessage&lt;/code&gt; class, offers a cleaner and
Unicode-friendly interface for composing and sending emails. It replaces use of
Python’s older legacy (&lt;code&gt;Compat32&lt;/code&gt;) API, which relied on lower-level MIME
classes (from &lt;code&gt;email.mime&lt;/code&gt;) and required more manual handling of
message structure and encoding.&lt;/p&gt;
    &lt;p&gt;Notably, the return type of the &lt;code&gt;EmailMessage.message()&lt;/code&gt; method is now an instance of Python’s
&lt;code&gt;email.message.EmailMessage&lt;/code&gt;. This supports the same API as the
previous &lt;code&gt;SafeMIMEText&lt;/code&gt; and &lt;code&gt;SafeMIMEMultipart&lt;/code&gt; return types, but is not an
instance of those now-deprecated classes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Minor features¶&lt;/head&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.admin&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The Font Awesome Free icon set (version 6.7.2) is now used for the admin interface icons.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;AdminSite.password_change_form&lt;/code&gt;attribute allows customizing the form used in the admin site password change view.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Message levels&lt;/p&gt;&lt;code&gt;messages.DEBUG&lt;/code&gt;and&lt;code&gt;messages.INFO&lt;/code&gt;now have distinct icons and CSS styling. Previously, both levels shared the same appearance as&lt;code&gt;messages.SUCCESS&lt;/code&gt;. Given that&lt;code&gt;ModelAdmin.message_user()&lt;/code&gt;uses&lt;code&gt;messages.INFO&lt;/code&gt;by default, set the level to&lt;code&gt;messages.SUCCESS&lt;/code&gt;to keep the previous icon and styling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.auth&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The default iteration count for the PBKDF2 password hasher is increased from 1,000,000 to 1,200,000.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.gis&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;GEOSGeometry.hasm&lt;/code&gt;property checks whether the geometry has the M dimension.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;Rotate&lt;/code&gt;database function rotates a geometry by a specified angle around the origin or a specified point.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;BaseGeometryWidget.base_layer&lt;/code&gt;attribute allows specifying a JavaScript map base layer, enabling customization of map tile providers.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;coveredby&lt;/code&gt;and&lt;code&gt;isvalid&lt;/code&gt;lookups,&lt;code&gt;Collect&lt;/code&gt;aggregation, and&lt;code&gt;GeoHash&lt;/code&gt;and&lt;code&gt;IsValid&lt;/code&gt;database functions are now supported on MariaDB 12.0.1+.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;geom_type&lt;/code&gt;lookup and&lt;code&gt;GeometryType()&lt;/code&gt;database function allow filtering geometries by their types.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Widgets from&lt;/p&gt;&lt;code&gt;django.contrib.gis.forms.widgets&lt;/code&gt;now render without inline JavaScript in templates. If you have customized any geometry widgets or their templates, you may need to update them to match the new layout.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.postgres&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;Lexeme&lt;/code&gt;expression for full text search provides fine-grained control over search terms.&lt;code&gt;Lexeme&lt;/code&gt;objects automatically escape their input and support logical combination operators (&lt;code&gt;&amp;amp;&lt;/code&gt;,&lt;code&gt;|&lt;/code&gt;,&lt;code&gt;~&lt;/code&gt;), prefix matching, and term weighting.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Model fields, indexes, and constraints from&lt;/p&gt;&lt;code&gt;django.contrib.postgres&lt;/code&gt;now include system checks to verify that&lt;code&gt;django.contrib.postgres&lt;/code&gt;is an installed app.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;CreateExtension&lt;/code&gt;,&lt;code&gt;BloomExtension&lt;/code&gt;,&lt;code&gt;BtreeGinExtension&lt;/code&gt;,&lt;code&gt;BtreeGistExtension&lt;/code&gt;,&lt;code&gt;CITextExtension&lt;/code&gt;,&lt;code&gt;CryptoExtension&lt;/code&gt;,&lt;code&gt;HStoreExtension&lt;/code&gt;,&lt;code&gt;TrigramExtension&lt;/code&gt;, and&lt;code&gt;UnaccentExtension&lt;/code&gt;operations now support the optional&lt;code&gt;hints&lt;/code&gt;parameter. This allows providing database hints to database routers to assist them in making routing decisions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.staticfiles&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ManifestStaticFilesStorage&lt;/code&gt;now ensures consistent path ordering in manifest files, making them more reproducible and reducing unnecessary diffs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;collectstatic&lt;/code&gt;command now reports only a summary for skipped files (and for deleted files when using&lt;code&gt;--clear&lt;/code&gt;) at&lt;code&gt;--verbosity&lt;/code&gt;1. To see per-file details for either case, set&lt;code&gt;--verbosity&lt;/code&gt;to 2 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Email¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;policy&lt;/code&gt;argument for&lt;code&gt;EmailMessage.message()&lt;/code&gt;allows specifying the email policy, the set of rules for updating and serializing the representation of the message. Defaults to&lt;code&gt;email.policy.default&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EmailMessage.attach()&lt;/code&gt;now accepts a&lt;code&gt;MIMEPart&lt;/code&gt;object from Python’s modern email API.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Internationalization¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Added support and translations for the Haitian Creole language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Management Commands¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;startproject&lt;/code&gt;and&lt;code&gt;startapp&lt;/code&gt;commands now create the custom target directory if it doesn’t exist.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Common utilities, such as&lt;/p&gt;&lt;code&gt;django.conf.settings&lt;/code&gt;, are now automatically imported to the&lt;code&gt;shell&lt;/code&gt;by default.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Migrations¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Squashed migrations can now themselves be squashed before being transitioned to normal migrations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Migrations now support serialization of&lt;/p&gt;&lt;code&gt;zoneinfo.ZoneInfo&lt;/code&gt;instances.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Serialization of deconstructible objects now supports keyword arguments with names that are not valid Python identifiers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Models¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Constraints now implement a&lt;/p&gt;&lt;code&gt;check()&lt;/code&gt;method that is already registered with the check framework.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;order_by&lt;/code&gt;argument for&lt;code&gt;Aggregate&lt;/code&gt;allows specifying the ordering of the elements in the result.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;Aggregate.allow_order_by&lt;/code&gt;class attribute determines whether the aggregate function allows passing an&lt;code&gt;order_by&lt;/code&gt;keyword argument.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;StringAgg&lt;/code&gt;aggregate returns the input values concatenated into a string, separated by the&lt;code&gt;delimiter&lt;/code&gt;string. This aggregate was previously supported only for PostgreSQL.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;save()&lt;/code&gt;method now raises a specialized&lt;code&gt;Model.NotUpdated&lt;/code&gt;exception, when a forced update results in no affected rows, instead of a generic&lt;code&gt;django.db.DatabaseError&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;QuerySet.raw()&lt;/code&gt;now supports models with a&lt;code&gt;CompositePrimaryKey&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Subqueries returning a&lt;/p&gt;&lt;code&gt;CompositePrimaryKey&lt;/code&gt;can now be used as the target of lookups other than&lt;code&gt;__in&lt;/code&gt;, such as&lt;code&gt;__exact&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;JSONField&lt;/code&gt;now supports negative array indexing on SQLite.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;AnyValue&lt;/code&gt;aggregate returns an arbitrary value from the non-null input values. This is supported on SQLite, MySQL, Oracle, and PostgreSQL 16+.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GeneratedField&lt;/code&gt;s and fields assigned expressions are now refreshed from the database after&lt;code&gt;save()&lt;/code&gt;on backends that support the&lt;code&gt;RETURNING&lt;/code&gt;clause (SQLite, PostgreSQL, and Oracle). On backends that don’t support it (MySQL and MariaDB), the fields are marked as deferred to trigger a refresh on subsequent accesses.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Using a ForeignObject with multiple&lt;/p&gt;&lt;code&gt;from_fields&lt;/code&gt;in Model indexes, constraints, or&lt;code&gt;unique_together&lt;/code&gt;now emits a system check error.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Pagination¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;AsyncPaginator&lt;/code&gt;and&lt;code&gt;AsyncPage&lt;/code&gt;provide async implementations of&lt;code&gt;Paginator&lt;/code&gt;and&lt;code&gt;Page&lt;/code&gt;respectively.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Requests and Responses¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Multiple&lt;/p&gt;&lt;code&gt;Cookie&lt;/code&gt;headers are now supported for HTTP/2 requests when running with ASGI.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Templates¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new variable&lt;/p&gt;&lt;code&gt;forloop.length&lt;/code&gt;is now available within a&lt;code&gt;for&lt;/code&gt;loop.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;querystring&lt;/code&gt;template tag now consistently prefixes the returned query string with a&lt;code&gt;?&lt;/code&gt;, ensuring reliable link generation behavior.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;querystring&lt;/code&gt;template tag now accepts multiple positional arguments, which must be mappings, such as&lt;code&gt;QueryDict&lt;/code&gt;or&lt;code&gt;dict&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Tests¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;DiscoverRunner&lt;/code&gt;now supports parallel test execution on systems using the&lt;code&gt;forkserver&lt;/code&gt;&lt;code&gt;multiprocessing&lt;/code&gt;start method.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Backwards incompatible changes in 6.0¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Database backend API¶&lt;/head&gt;
    &lt;p&gt;This section describes changes that may be needed in third-party database backends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;BaseDatabaseSchemaEditor&lt;/code&gt;and PostgreSQL backends no longer use&lt;code&gt;CASCADE&lt;/code&gt;when dropping a column.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DatabaseOperations.return_insert_columns()&lt;/code&gt;and&lt;code&gt;DatabaseOperations.fetch_returned_insert_rows()&lt;/code&gt;methods are renamed to&lt;code&gt;returning_columns()&lt;/code&gt;and&lt;code&gt;fetch_returned_rows()&lt;/code&gt;, respectively, to denote they can be used in the context of&lt;code&gt;UPDATE … RETURNING&lt;/code&gt;statements as well as&lt;code&gt;INSERT … RETURNING&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;DatabaseOperations.fetch_returned_insert_columns()&lt;/code&gt;method is removed and the&lt;code&gt;fetch_returned_rows()&lt;/code&gt;method replacing&lt;code&gt;fetch_returned_insert_rows()&lt;/code&gt;expects both a&lt;code&gt;cursor&lt;/code&gt;and&lt;code&gt;returning_params&lt;/code&gt;to be provided, just like&lt;code&gt;fetch_returned_insert_columns()&lt;/code&gt;did.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If the database supports&lt;/p&gt;&lt;code&gt;UPDATE … RETURNING&lt;/code&gt;statements, backends can set&lt;code&gt;DatabaseFeatures.can_return_rows_from_update=True&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Dropped support for MariaDB 10.5¶&lt;/head&gt;
    &lt;p&gt;Upstream support for MariaDB 10.5 ends in June 2025. Django 6.0 supports MariaDB 10.6 and higher.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dropped support for Python &amp;lt; 3.12¶&lt;/head&gt;
    &lt;p&gt;Because Python 3.12 is now the minimum supported version for Django, any optional dependencies must also meet that requirement. The following versions of each library are the first to add or confirm compatibility with Python 3.12:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;aiosmtpd&lt;/code&gt;1.4.5&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;argon2-cffi&lt;/code&gt;23.1.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bcrypt&lt;/code&gt;4.1.1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;docutils&lt;/code&gt;0.22&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;geoip2&lt;/code&gt;4.8.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Pillow&lt;/code&gt;10.1.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mysqlclient&lt;/code&gt;2.2.1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;numpy&lt;/code&gt;1.26.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PyYAML&lt;/code&gt;6.0.2&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;psycopg&lt;/code&gt;3.1.12&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;psycopg2&lt;/code&gt;2.9.9&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;redis-py&lt;/code&gt;5.1.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;selenium&lt;/code&gt;4.23.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sqlparse&lt;/code&gt;0.5.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tblib&lt;/code&gt;3.0.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Email¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The undocumented&lt;/p&gt;&lt;code&gt;mixed_subtype&lt;/code&gt;and&lt;code&gt;alternative_subtype&lt;/code&gt;properties of&lt;code&gt;EmailMessage&lt;/code&gt;and&lt;code&gt;EmailMultiAlternatives&lt;/code&gt;are no longer supported.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The undocumented&lt;/p&gt;&lt;code&gt;encoding&lt;/code&gt;property of&lt;code&gt;EmailMessage&lt;/code&gt;no longer supports Python legacy&lt;code&gt;email.charset.Charset&lt;/code&gt;objects.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;As the internal implementations of&lt;/p&gt;&lt;code&gt;EmailMessage&lt;/code&gt;and&lt;code&gt;EmailMultiAlternatives&lt;/code&gt;have changed significantly, closely examine any custom subclasses that rely on overriding undocumented, internal underscore methods.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; setting now defaults to &lt;code&gt;BigAutoField&lt;/code&gt;¶&lt;/head&gt;
    &lt;p&gt;Since Django 3.2, when the &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; setting was added,
the default &lt;code&gt;startproject&lt;/code&gt; template’s &lt;code&gt;settings.py&lt;/code&gt; contained:&lt;/p&gt;
    &lt;code&gt;DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;and the default &lt;code&gt;startapp&lt;/code&gt; template’s &lt;code&gt;AppConfig&lt;/code&gt; contained:&lt;/p&gt;
    &lt;code&gt;default_auto_field = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;At that time, the default value of &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; remained
&lt;code&gt;django.db.models.AutoField&lt;/code&gt; for backwards compatibility.&lt;/p&gt;
    &lt;p&gt;In Django 6.0, &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; now defaults to
&lt;code&gt;django.db.models.BigAutoField&lt;/code&gt; and the aforementioned lines in the
project and app templates are removed.&lt;/p&gt;
    &lt;p&gt;Most projects shouldn’t be affected, since Django 3.2 has raised the system check warning models.W042 for projects that don’t set &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you haven’t dealt with this warning by now, add &lt;code&gt;DEFAULT_AUTO_FIELD = 'django.db.models.AutoField'&lt;/code&gt; to your project’s
settings, or &lt;code&gt;default_auto_field = 'django.db.models.AutoField'&lt;/code&gt; to an app’s
&lt;code&gt;AppConfig&lt;/code&gt;, as needed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Custom ORM expressions should return params as a tuple¶&lt;/head&gt;
    &lt;p&gt;Prior to Django 6.0, custom lookups and custom expressions implementing the &lt;code&gt;as_sql()&lt;/code&gt; method (and its supporting methods &lt;code&gt;process_lhs()&lt;/code&gt; and
&lt;code&gt;process_rhs()&lt;/code&gt;) were allowed to return a sequence of params in either a list
or a tuple. To address the interoperability problems that resulted, the second
return element of the &lt;code&gt;as_sql()&lt;/code&gt; method should now be a tuple:&lt;/p&gt;
    &lt;code&gt;def as_sql(self, compiler, connection) -&amp;gt; tuple[str, tuple]: ...
&lt;/code&gt;
    &lt;p&gt;If your custom expressions support multiple versions of Django, you should adjust any pre-processing of parameters to be resilient against either tuples or lists. For instance, prefer unpacking like this:&lt;/p&gt;
    &lt;code&gt;params = (*lhs_params, *rhs_params)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Miscellaneous¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The JSON serializer now writes a newline at the end of the output, even without the&lt;/p&gt;&lt;code&gt;indent&lt;/code&gt;option set.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The minimum supported version of&lt;/p&gt;&lt;code&gt;asgiref&lt;/code&gt;is increased from 3.8.1 to 3.9.1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Features deprecated in 6.0¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Positional arguments in &lt;code&gt;django.core.mail&lt;/code&gt; APIs¶&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;django.core.mail&lt;/code&gt; APIs now require keyword arguments for less commonly
used parameters. Using positional arguments for these now emits a deprecation
warning and will raise a &lt;code&gt;TypeError&lt;/code&gt; when the deprecation period ends:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;All optional parameters (&lt;/p&gt;&lt;code&gt;fail_silently&lt;/code&gt;and later) must be passed as keyword arguments to&lt;code&gt;get_connection()&lt;/code&gt;,&lt;code&gt;mail_admins()&lt;/code&gt;,&lt;code&gt;mail_managers()&lt;/code&gt;,&lt;code&gt;send_mail()&lt;/code&gt;, and&lt;code&gt;send_mass_mail()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;All parameters must be passed as keyword arguments when creating an&lt;/p&gt;&lt;code&gt;EmailMessage&lt;/code&gt;or&lt;code&gt;EmailMultiAlternatives&lt;/code&gt;instance, except for the first four (&lt;code&gt;subject&lt;/code&gt;,&lt;code&gt;body&lt;/code&gt;,&lt;code&gt;from_email&lt;/code&gt;, and&lt;code&gt;to&lt;/code&gt;), which may still be passed either as positional or keyword arguments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Miscellaneous¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;BaseDatabaseCreation.create_test_db(serialize)&lt;/code&gt;is deprecated. Use&lt;code&gt;serialize_db_to_string()&lt;/code&gt;instead.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The PostgreSQL&lt;/p&gt;&lt;code&gt;StringAgg&lt;/code&gt;class is deprecated in favor of the generally available&lt;code&gt;StringAgg&lt;/code&gt;class.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The PostgreSQL&lt;/p&gt;&lt;code&gt;OrderableAggMixin&lt;/code&gt;is deprecated in favor of the&lt;code&gt;order_by&lt;/code&gt;attribute now available on the&lt;code&gt;Aggregate&lt;/code&gt;class.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The default protocol in&lt;/p&gt;&lt;code&gt;urlize&lt;/code&gt;and&lt;code&gt;urlizetrunc&lt;/code&gt;will change from HTTP to HTTPS in Django 7.0. Set the transitional setting&lt;code&gt;URLIZE_ASSUME_HTTPS&lt;/code&gt;to&lt;code&gt;True&lt;/code&gt;to opt into assuming HTTPS during the Django 6.x release cycle.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;URLIZE_ASSUME_HTTPS&lt;/code&gt;transitional setting is deprecated.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Setting&lt;/p&gt;&lt;code&gt;ADMINS&lt;/code&gt;or&lt;code&gt;MANAGERS&lt;/code&gt;to a list of (name, address) tuples is deprecated. Set to a list of email address strings instead. Django never used the name portion. To include a name, format the address string as&lt;code&gt;'"Name" &amp;lt;address&amp;gt;'&lt;/code&gt;or use Python’s&lt;code&gt;email.utils.formataddr()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for the&lt;/p&gt;&lt;code&gt;orphans&lt;/code&gt;argument being larger than or equal to the&lt;code&gt;per_page&lt;/code&gt;argument of&lt;code&gt;django.core.paginator.Paginator&lt;/code&gt;and&lt;code&gt;django.core.paginator.AsyncPaginator&lt;/code&gt;is deprecated.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Using a percent sign in a column alias or annotation is deprecated.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for passing Python’s legacy email&lt;/p&gt;&lt;code&gt;MIMEBase&lt;/code&gt;object to&lt;code&gt;EmailMessage.attach()&lt;/code&gt;(or including one in the message’s&lt;code&gt;attachments&lt;/code&gt;list) is deprecated. For complex attachments requiring additional headers or parameters, switch to the modern email API’s&lt;code&gt;MIMEPart&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.core.mail.BadHeaderError&lt;/code&gt;exception is deprecated. Python’s modern email raises a&lt;code&gt;ValueError&lt;/code&gt;for email headers containing prohibited characters.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.core.mail.SafeMIMEText&lt;/code&gt;and&lt;code&gt;SafeMIMEMultipart&lt;/code&gt;classes are deprecated.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The undocumented&lt;/p&gt;&lt;code&gt;django.core.mail.forbid_multi_line_headers()&lt;/code&gt;and&lt;code&gt;django.core.mail.message.sanitize_address()&lt;/code&gt;functions are deprecated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Features removed in 6.0¶&lt;/head&gt;
    &lt;p&gt;These features have reached the end of their deprecation cycle and are removed in Django 6.0.&lt;/p&gt;
    &lt;p&gt;See Features deprecated in 5.0 for details on these changes, including how to remove usage of these features.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Support for passing positional arguments to&lt;/p&gt;&lt;code&gt;BaseConstraint&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;DjangoDivFormRenderer&lt;/code&gt;and&lt;code&gt;Jinja2DivFormRenderer&lt;/code&gt;transitional form renderers are removed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BaseDatabaseOperations.field_cast_sql()&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;request&lt;/code&gt;is required in the signature of&lt;code&gt;ModelAdmin.lookup_allowed()&lt;/code&gt;subclasses.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for calling&lt;/p&gt;&lt;code&gt;format_html()&lt;/code&gt;without passing args or kwargs is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The default scheme for&lt;/p&gt;&lt;code&gt;forms.URLField&lt;/code&gt;has changed from&lt;code&gt;"http"&lt;/code&gt;to&lt;code&gt;"https"&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;FORMS_URLFIELD_ASSUME_HTTPS&lt;/code&gt;transitional setting is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.db.models.sql.datastructures.Join&lt;/code&gt;no longer falls back to&lt;code&gt;get_joining_columns()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;get_joining_columns()&lt;/code&gt;method of&lt;code&gt;ForeignObject&lt;/code&gt;and&lt;code&gt;ForeignObjectRel&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;ForeignObject.get_reverse_joining_columns()&lt;/code&gt;method is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for&lt;/p&gt;&lt;code&gt;cx_Oracle&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;ChoicesMeta&lt;/code&gt;alias to&lt;code&gt;django.db.models.enums.ChoicesType&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;Prefetch.get_current_queryset()&lt;/code&gt;method is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;get_prefetch_queryset()&lt;/code&gt;method of related managers and descriptors is removed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_prefetcher()&lt;/code&gt;and&lt;code&gt;prefetch_related_objects()&lt;/code&gt;no longer fall back to&lt;code&gt;get_prefetch_queryset()&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See Features deprecated in 5.1 for details on these changes, including how to remove usage of these features.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;django.urls.register_converter()&lt;/code&gt;no longer allows overriding existing converters.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;ModelAdmin.log_deletion()&lt;/code&gt;and&lt;code&gt;LogEntryManager.log_action()&lt;/code&gt;methods are removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The undocumented&lt;/p&gt;&lt;code&gt;django.utils.itercompat.is_iterable()&lt;/code&gt;function and the&lt;code&gt;django.utils.itercompat&lt;/code&gt;module are removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.contrib.gis.geoip2.GeoIP2.coords()&lt;/code&gt;method is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.contrib.gis.geoip2.GeoIP2.open()&lt;/code&gt;method is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for passing positional arguments to&lt;/p&gt;&lt;code&gt;Model.save()&lt;/code&gt;and&lt;code&gt;Model.asave()&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The setter for&lt;/p&gt;&lt;code&gt;django.contrib.gis.gdal.OGRGeometry.coord_dim&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;check&lt;/code&gt;keyword argument of&lt;code&gt;CheckConstraint&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;get_cache_name()&lt;/code&gt;method of&lt;code&gt;FieldCacheMixin&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;OS_OPEN_FLAGS&lt;/code&gt;attribute of&lt;code&gt;FileSystemStorage&lt;/code&gt;is removed.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.djangoproject.com/en/6.0/releases/6.0/"/><published>2025-12-04T21:09:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46153332</id><title>Countdown until the AI bubble bursts</title><updated>2025-12-04T23:36:57.279314+00:00</updated><content>&lt;doc fingerprint="bed78b250e7f9bcb"&gt;
  &lt;main&gt;
    &lt;p&gt;Fetching latest analysis from Gemini...&lt;/p&gt;
    &lt;p&gt;No, this is a satirical project. We don't actually know if or when the "AI bubble" will burst. It's a thought experiment where an AI (Gemini) predicts its own potential industry decline based on web news.&lt;/p&gt;
    &lt;p&gt;Every night, a script runs that asks Gemini to search the web for the latest AI newsâboth hype and criticism. Based on the sentiment and economic indicators it finds, it updates the predicted "burst date" and explains its reasoning.&lt;/p&gt;
    &lt;p&gt;Not at all! We believe AI has incredible utility. However, we are critical of the massive hype cycle, the "AI in everything" trend, and the circular investment economy that currently dominates the sector.&lt;/p&gt;
    &lt;p&gt;You bet!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pop-the-bubble.xyz/"/><published>2025-12-04T21:27:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46153466</id><title>Thoughts on Go vs. Rust vs. Zig</title><updated>2025-12-04T23:36:56.860381+00:00</updated><content>&lt;doc fingerprint="f76818ddbee218fd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Thoughts on Go vs. Rust vs. Zig&lt;/head&gt;
    &lt;head rend="h4"&gt;Aug 09, 2025&lt;/head&gt;
    &lt;p&gt;I realized recently that rather than using “the right tool for the job” I’ve been using the tool at the job and that’s mostly determined the programming languages I know. So over the last couple months I’ve put a lot of time into experimenting with languages I don’t get to use at work. My goal hasn’t been proficiency; I’m more interested in forming an opinion on what each language is good for.&lt;/p&gt;
    &lt;p&gt;Programming languages differ along so many axes that it can be hard to compare them without defaulting to the obviously true but 1) entirely boring and 2) not-that-helpful conclusion that there are trade-offs. Of course there are trade-offs. The important question is, why did this language commit to this particular set of trade-offs?&lt;/p&gt;
    &lt;p&gt;That question is interesting to me because I don’t want to choose a language based on a list of features as if I were buying a humidifier. I care about building software and I care about my tools. In making the trade-offs they make, languages express a set of values. I’d like to find out which values resonate with me.&lt;/p&gt;
    &lt;p&gt;That question is also useful in clarifying the difference between languages that, at the end of the day, have feature sets that significantly overlap. If the number of questions online about “Go vs. Rust” or “Rust vs. Zig” is a reliable metric, people are confused. It’s hard to remember, say, that language X is better for writing web services because it has features a, b, and c whereas language Y only has features a and b. Easier, I think, to remember that language X is better for writing web services because language Y was designed by someone who hates the internet (let’s imagine) and believes we should unplug the whole thing.&lt;/p&gt;
    &lt;p&gt;I’ve collected here my impressions of the three languages I’ve experimented with lately: Go, Rust, and Zig. I’ve tried to synthesize my experience with each language into a sweeping verdict on what that language values and how well it executes on those values. This might be reductive, but, like, crystallizing a set of reductive prejudices is sort of what I’m trying to do here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Go&lt;/head&gt;
    &lt;p&gt;Go is distinguished by its minimalism. It has been described as “a modern C.” Go isn’t like C, because it is garbage-collected and has a real run-time, but it is like C in that you can fit the whole language in your head.&lt;/p&gt;
    &lt;p&gt;You can fit the whole language in your head because Go has so few features. For a long time, Go was notorious for not having generics. That was finally changed in Go 1.18, but that was only after 12 years of people begging for generics to be added to the language. Other features common in modern languages, like tagged unions or syntactic sugar for error-handling, have not been added to Go.&lt;/p&gt;
    &lt;p&gt;It seems the Go development team has a high bar for adding features to the language. The end result is a language that forces you to write a lot of boilerplate code to implement logic that could be more succinctly expressed in another language. But the result is also a language that is stable over time and easy to read.&lt;/p&gt;
    &lt;p&gt;To give you another example of Go’s minimalism, consider Go’s slice type. Both Rust and Zig have a slice type, but these are fat pointers and fat pointers only. In Go, a slice is a fat pointer to a contiguous sequence in memory, but a slice can also grow, meaning that it subsumes the functionality of Rust’s &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; type and Zig’s &lt;code&gt;ArrayList&lt;/code&gt;. Also, since Go is managing your memory for
you, Go will decide whether your slice’s backing memory lives on the stack or
the heap; in Rust or Zig, you have to think much harder about where your memory
lives.&lt;/p&gt;
    &lt;p&gt;Go’s origin myth, as I understand it, is basically this: Rob Pike was sick of waiting for C++ projects to compile and was sick of other programmers at Google making mistakes in those same C++ projects. Go is therefore simple where C++ is baroque. It is a language for the programming rank and file, designed to be sufficient for 90% of use cases while also being easy to understand, even (perhaps especially) when writing concurrent code.&lt;/p&gt;
    &lt;p&gt;I don’t use Go at work, but I think I should. Go is minimal in service of corporate collaboration. I don’t mean that as a slightâbuilding software in a corporate environment has its own challenges, which Go solves for.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rust&lt;/head&gt;
    &lt;p&gt;Where Go is minimalist, Rust is maximalist. A tagline often associated with Rust is “zero-cost abstractions.” I would amend that to read, “zero-cost abstractions, and lots of them!”&lt;/p&gt;
    &lt;p&gt;Rust has a reputation for being hard to learn. I agree with Jamie Brandon, who writes that it’s not lifetimes that make Rust difficult, it’s the number of concepts stuffed into the language. I’m not the first person to pick on this particular Github comment, but it perfectly illustrates the conceptual density of Rust:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The type&lt;/p&gt;&lt;code&gt;Pin&amp;lt;&amp;amp;LocalType&amp;gt;&lt;/code&gt;implements&lt;code&gt;Deref&amp;lt;Target = LocalType&amp;gt;&lt;/code&gt;but it doesnât implement&lt;code&gt;DerefMut&lt;/code&gt;. The types&lt;code&gt;Pin&lt;/code&gt;and&lt;code&gt;&amp;amp;&lt;/code&gt;are&lt;code&gt;#[fundamental]&lt;/code&gt;so that an&lt;code&gt;impl DerefMut&lt;/code&gt;for&lt;code&gt;Pin&amp;lt;&amp;amp;LocalType&amp;gt;&amp;gt;&lt;/code&gt;is possible. You can use&lt;code&gt;LocalType == SomeLocalStruct&lt;/code&gt;or&lt;code&gt;LocalType == dyn LocalTrait&lt;/code&gt;and you can coerce&lt;code&gt;Pin&amp;lt;Pin&amp;lt;&amp;amp;SomeLocalStruct&amp;gt;&amp;gt;&lt;/code&gt;into&lt;code&gt;Pin&amp;lt;Pin&amp;lt;&amp;amp;dyn LocalTrait&amp;gt;&amp;gt;&lt;/code&gt;. (Indeed, two layers of Pin!!) This allows creating a pair of âsmart pointers that implement&lt;code&gt;CoerceUnsized&lt;/code&gt;but have strange behaviorâ on stable (&lt;code&gt;Pin&amp;lt;&amp;amp;SomeLocalStruct&amp;gt;&lt;/code&gt;and&lt;code&gt;Pin&amp;lt;&amp;amp;dyn LocalTrait&amp;gt;&lt;/code&gt;become the smart pointers with âstrange behaviorâ and they already implement&lt;code&gt;CoerceUnsized&lt;/code&gt;).&lt;/quote&gt;
    &lt;p&gt;Of course, Rust isn’t trying to be maximalist the same way Go is trying to be minimalist. Rust is a complex language because what it’s trying to do is deliver on two goalsâsafety and performanceâthat are somewhat in tension.&lt;/p&gt;
    &lt;p&gt;The performance goal is self-explanatory. What “safety” means is less clear; at least it was to me, though maybe I’ve just been Python-brained for too long. “Safety” means “memory safety,” the idea that you shouldn’t be able to dereference an invalid pointer, or do a double-free, etc. But it also means more than that. A “safe” program avoids all undefined behavior (sometimes referred to as “UB”).&lt;/p&gt;
    &lt;p&gt;What is the dreaded UB? I think the best way to understand it is to remember that, for any running program, there are FATES WORSE THAN DEATH. If something goes wrong in your program, immediate termination is great actually! Because the alternative, if the error isn’t caught, is that your program crosses over into a twilight zone of unpredictability, where its behavior might be determined by which thread wins the next data race or by what garbage happens to be at a particular memory address. Now you have heisenbugs and security vulnerabilities. Very bad.&lt;/p&gt;
    &lt;p&gt;Rust tries to prevent UB without paying any run-time performance penalty by checking for it at compile-time. The Rust compiler is smart, but it’s not omniscient. For it to be able to check your code, it has to understand what your code will do at run-time. And so Rust has an expressive type system and a menagerie of traits that allow you to express, to the compiler, what in another language would just be the apparent run-time behavior of your code.&lt;/p&gt;
    &lt;p&gt;This makes Rust hard, because you can’t just do the thing! You have to find out Rust’s name for the thingâfind the trait or whatever you needâthen implement it as Rust expects you to. But if you do this, Rust can make guarantees about the behavior of your code that other languages cannot, which depending on your application might be crucial. It can also make guarantees about other people’s code, which makes consuming libraries easy in Rust and explains why Rust projects have almost as many dependencies as projects in the JavaScript ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Zig&lt;/head&gt;
    &lt;p&gt;Of the three languages, Zig is the newest and least mature. As of this writing, Zig is only on version 0.14. Its standard library has almost zero documentation and the best way to learn how to use it is to consult the source code directly.&lt;/p&gt;
    &lt;p&gt;Although I don’t know if this is true, I like to think of Zig as a reaction to both Go and Rust. Go is simple because it obscures details about how the computer actually works. Rust is safe because it forces you to jump through its many hoops. Zig will set you free! In Zig, you control the universe and nobody can tell you what to do.&lt;/p&gt;
    &lt;p&gt;In both Go and Rust, allocating an object on the heap is as easy as returning a pointer to a struct from a function. The allocation is implicit. In Zig, you allocate every byte yourself, explicitly. (Zig has manual memory management.) You have more control here than you have even in C: To allocate bytes, you have to call &lt;code&gt;alloc()&lt;/code&gt; on a specific kind of allocator, meaning you have to decide
on the best allocator implementation for your use case.&lt;/p&gt;
    &lt;p&gt;In Rust, creating a mutable global variable is so hard that there are long forum discussions on how to do it. In Zig, you can just create one, no problem.&lt;/p&gt;
    &lt;p&gt;Undefined behavior is still important in Zig. Zig calls it “illegal behavior.” It tries to detect it at run-time and crash the program when it occurs. For those who might worry about the performance cost of these checks, Zig offers four different “release modes” that you can choose from when you build your program. In some of these, the checks are disabled. The idea seems to be that you can run your program enough times in the checked release modes to have reasonable confidence that there will be no illegal behavior in the unchecked build of your program. That seems like a highly pragmatic design to me.&lt;/p&gt;
    &lt;p&gt;Another difference between Zig and the other two languages is Zig’s relationship to object-oriented programming. OOP has been out of favor for a while now and both Go and Rust eschew class inheritance. But Go and Rust have enough support for other object-oriented programming idioms that you could still construct your program as a graph of interacting objects if you wanted to. Zig has methods, but no private struct fields and no language feature implementing run-time polymorphism (AKA dynamic dispatch), even though &lt;code&gt;std.mem.Allocator&lt;/code&gt; is dying to be an interface. As best as I can tell, these
exclusions are intentional; Zig is a language for data-oriented
design.&lt;/p&gt;
    &lt;p&gt;One more thing I want to say about this, because I found it eye-opening: It might seem crazy to be building a programming language with manual memory management in 2025, especially when Rust has shown that you don’t even need garbage collection and can let the compiler do it for you. But this is a design choice very much related to the choice to exclude OOP features. In Go and Rust and so many other languages, you tend to allocate little bits of memory at a time for each object in your object graph. Your program has thousands of little hidden &lt;code&gt;malloc()&lt;/code&gt;s and &lt;code&gt;free()&lt;/code&gt;s, and therefore thousands of different
lifetimes. This is RAII. In Zig,
it might seem like manual memory management would require lots of tedious,
error-prone bookkeeping, but that’s only if you insist on tying memory
allocations to all your little objects. You could instead just allocate and
free big chunks of memory at certain sensible points in your program (like at
the start of each iteration of your event loop), and use that memory to hold
the data you need to operate on. It’s this approach that Zig encourages.&lt;/p&gt;
    &lt;p&gt;Many people seem confused about why Zig should exist if Rust does already. It’s not just that Zig is trying to be simpler. I think this difference is the more important one. Zig wants you to excise even more object-oriented thinking from your code.&lt;/p&gt;
    &lt;p&gt;Zig has a fun, subversive feel to it. It’s a language for smashing the corporate class hierarchy (of objects). It’s a language for megalomaniacs and anarchists. I like it. I hope it gets to a stable release soon, though the Zig team’s current priority seems to be rewriting all of their dependencies. It’s not impossible they try to rewrite the Linux kernel before we see Zig 1.0.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sinclairtarget.com/blog/2025/08/thoughts-on-go-vs.-rust-vs.-zig/"/><published>2025-12-04T21:40:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154022</id><title>State of AI: An Empirical 100T Token Study with OpenRouter</title><updated>2025-12-04T23:36:56.533141+00:00</updated><content>&lt;doc fingerprint="2d57a7fb8cbac4c9"&gt;
  &lt;main&gt;&lt;p&gt;An Empirical 100 Trillion Token Study with OpenRouter&lt;/p&gt;&lt;p&gt;The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.&lt;/p&gt;&lt;p&gt;Just a year ago, the landscape of large language models looked fundamentally different. Prior to late 2024, state-of-the-art systems were dominated by single-pass, autoregressive predictors optimized to continue text sequences. Several precursor efforts attempted to approximate reasoning through advanced instruction following and tool use. For instance, Anthropic's Sonnet 2.1 &amp;amp; 3 models excelled at sophisticated tool use and Retrieval-Augmented Generation (RAG), and Cohere's Command R models incorporated structured tool-planning tokens. Separately, open source projects like those done by Reflection explored supervised chain-of-thought and self-critique loops during training. Although these advanced techniques produced reasoning-like outputs and superior instruction following, the fundamental inference procedure remained based on a single forward pass, emitting a surface-level trace learned from data rather than performing iterative, internal computation.&lt;/p&gt;&lt;p&gt;This paradigm evolved on December 5, 2024, when OpenAI released the first full version of its o1 reasoning model (codenamed Strawberry) [4]. The preview released on September 12, 2024 had already indicated a departure from conventional autoregressive inference. Unlike prior systems, o1 employed an expanded inference-time computation process involving internal multi-step deliberation, latent planning, and iterative refinement before generating a final output. Empirically, this enabled systematic improvements in mathematical reasoning, logical consistency, and multi-step decision-making, reflecting a shift from pattern completion to structured internal cognition. In retrospect, last year marked the field's true inflection point: earlier approaches gestured toward reasoning, but o1 introduced the first generally-deployed architecture that performed reasoning through deliberate multi-stage computation rather than merely describing it [6, 7].&lt;/p&gt;&lt;p&gt;While recent advances in LLM capabilities have been widely documented, systematic evidence about how these models are actually used in practice remains limited [3, 5]. Existing accounts tend to emphasize qualitative demonstrations or benchmark performance rather than large-scale behavioral data. To bridge this gap, we undertake an empirical study of LLM usage, leveraging a 100 trillion token dataset from OpenRouter, a multi-model AI inference platform that serves as a hub for diverse LLM queries.&lt;/p&gt;&lt;p&gt;OpenRouter's vantage point provides a unique window into fine-grained usage patterns. Because it orchestrates requests across a wide array of models (spanning both closed source APIs and open-weight deployments), OpenRouter captures a representative cross-section of how developers and end-users actually invoke language models for various tasks. By analyzing this rich dataset, we can observe which models are chosen for which tasks, how usage varies across geographic regions and over time, and how external factors like pricing or new model launches influence behavior.&lt;/p&gt;&lt;p&gt;In this paper, we draw inspiration from prior empirical studies of AI adoption, including Anthropic's economic impact and usage analyses [1] and OpenAI's report How People Use ChatGPT [2], aiming for a neutral, evidence-driven discussion. We first describe our dataset and methodology, including how we categorize tasks and models. We then delve into a series of analyses that illuminate different facets of usage:&lt;/p&gt;&lt;p&gt;Finally, we discuss what these findings reveal about real-world LLM usage, highlighting unexpected patterns and correcting some myths.&lt;/p&gt;&lt;p&gt;Our analysis is based on metadata collected from the OpenRouter platform, a unified AI inference layer that connects users and developers to hundreds of large language models. Each user request on OpenRouter is executed against a user-selected model, and structured metadata describing the resulting "generation" event is logged. The dataset used in this study consists of anonymized request-level metadata for billions of prompt–completion pairs from a global user base, spanning approximately two years up to the time of writing. We do zoom in on the last year.&lt;/p&gt;&lt;p&gt;Crucially, we did not have access to the underlying text of prompts or completions. Our analysis relies entirely on metadata that capture the structure, timing, and context of each generation, without exposing user content. This privacy-preserving design enables large-scale behavioral analysis.&lt;/p&gt;&lt;p&gt;Each generation record includes information on timing, model and provider identifiers, token usage, and system performance metrics. Token counts encompass both prompt (input) and completion (output) tokens, allowing us to measure overall model workload and cost. Metadata also include fields related to geographic routing, latency, and usage context (for example, whether the request was streamed or cancelled, or whether tool-calling features were invoked). Together, these attributes provide a detailed but non-textual view of how models are used in practice.&lt;/p&gt;&lt;p&gt;All analyses, aggregations, and most visualizations based on this metadata were conducted using the Hex analytics platform, which provided a reproducible pipeline for versioned SQL queries, transformations, and final figure generation.&lt;/p&gt;&lt;p&gt;We emphasize that this dataset is observational: it reflects real-world activity on the OpenRouter platform, which itself is shaped by model availability, pricing, and user preferences. As of 2025, OpenRouter supports more than 300+ active models from over 60 providers and serves millions of developers and end-users, with over 50% of usage originating outside the United States. While certain usage patterns outside the platform are not captured, OpenRouter's global scale and diversity make it a representative lens on large-scale LLM usage dynamics.&lt;/p&gt;&lt;p&gt;No direct access to user prompts or model outputs was available for this study. Instead, OpenRouter performs internal categorization on a random sample comprising approximately 0.25% of all prompts and responses through a non-proprietary module GoogleTagClassifier. While this represents only a fraction of total activity, the underlying dataset remains substantial given the overall query volume processed by OpenRouter. GoogleTagClassifier interfaces with Google Cloud Natural Language's &lt;code&gt;classifyText&lt;/code&gt; content-classification API&lt;/p&gt;&lt;p&gt;The API applies a hierarchical, language-agnostic taxonomy to textual input, returning one or more category paths (e.g., &lt;code&gt;/Computers &amp;amp; Electronics/Programming&lt;/code&gt;, &lt;code&gt;/Arts &amp;amp; Entertainment/Roleplaying Games&lt;/code&gt;) with corresponding confidence scores in the range [0,1]. The classifier operates directly on prompt data (up to the first 1,000 characters). The classifier is deployed within OpenRouter's infrastructure, ensuring that classifications remain anonymous and are not linked to individual customers. Categories with confidence scores below the default threshold of 0.5 are excluded from further analysis. The classification system itself operates entirely within OpenRouter's infrastructure and was not part of this study; our analysis relied solely on the resulting categorical outputs (effectively metadata describing prompt classifications) rather than the underlying prompt content.&lt;/p&gt;&lt;p&gt;To make these fine-grained labels useful at scale, we map GoogleTagClassifier's taxonomy to a compact set of study-defined buckets and assign each request tags. Each tag rolls up to higher level category in one to one way. Representative mappings include:&lt;/p&gt;&lt;code&gt;/Computers &amp;amp; Electronics/Programming&lt;/code&gt; or &lt;code&gt;/Science/Computer Science/*&lt;/code&gt;&lt;code&gt;/Games/Roleplaying Games&lt;/code&gt; and creative dialogue leaves under &lt;code&gt;/Arts &amp;amp; Entertainment/*&lt;/code&gt;&lt;code&gt;/Reference/Language Resources/*&lt;/code&gt;&lt;code&gt;/Reference/General Reference/*&lt;/code&gt; and &lt;code&gt;/News/*&lt;/code&gt; when the intent appears to be factual lookup&lt;code&gt;/Computers &amp;amp; Electronics/Software/Business &amp;amp; Productivity Software&lt;/code&gt; or &lt;code&gt;/Business &amp;amp; Industrial/Business Services/Writing &amp;amp; Editing Services&lt;/code&gt;&lt;code&gt;/Jobs &amp;amp; Education/Education/*&lt;/code&gt;&lt;code&gt;/Books &amp;amp; Literature/*&lt;/code&gt; and narrative leaves under &lt;code&gt;/Arts &amp;amp; Entertainment/*&lt;/code&gt;&lt;code&gt;/Adult&lt;/code&gt;&lt;p&gt;There are inherent limitations to this approach, for instance, reliance on a predefined taxonomy constrains how novel or cross-domain behaviors are categorized, and certain interaction types may not yet fit neatly within existing classes. In practice, some prompts receive multiple category labels when their content spans overlapping domains. Nonetheless, the classifier-driven categorization provides us with a lens for downstream analyses. This enables us to quantify not just how much LLMs are used but what for.&lt;/p&gt;&lt;p&gt;A few variants are worth explicitly calling out:&lt;/p&gt;&lt;p&gt;Unless otherwise noted, token volume refers to the sum of prompt (input) and completion (output) tokens.&lt;/p&gt;&lt;p&gt;To understand regional patterns in LLM usage, we segment requests by user geography. Direct request metadata (like IP-based location) is typically imprecise or anonymized. Instead, we determine user region based on the billing location associated with each account. This provides a more reliable proxy for user geography, as billing data reflects the country or region linked to the user's payment method or account registration. We use this billing-based segmentation in our analysis of regional adoption and model preferences.&lt;/p&gt;&lt;p&gt;This method has limitations. Some users employ third-party billing or shared organizational accounts, which may not correspond to their actual location. Enterprise accounts may aggregate activity across multiple regions under one billing entity. Despite these imperfections, billing geography remains the most stable and interpretable indicator available for privacy-preserving geographic analysis given the metadata we had access to.&lt;/p&gt;&lt;p&gt;Our analyses primarily cover a rolling 13-month period ending on November, 2025, but not all underlying metadata spans this full window. Most model-level and pricing analyses were focused on November 3, 2024 – November 30, 2025 time frame. However, category-level analyses (especially those using the GoogleTagClassifier taxonomy) are based on a shorter interval beginning in May 2025, reflecting when consistent tagging became available on OpenRouter. In particular, detailed task classification fields (e.g., tags such as Programming, Roleplay, or Technology) were only added in mid-2025. Consequently, all findings in the Categories section should be interpreted as representative of mid-2025 usage rather than the entire prior year.&lt;/p&gt;&lt;p&gt;Unless otherwise specified, all time-series aggregates are computed on a weekly basis using UTC-normalized timestamps, summing prompt and completion tokens. This approach ensures comparability across model families and minimizes bias from transient spikes or regional time-zone effects.&lt;/p&gt;&lt;p&gt;A central question in the AI ecosystem is the balance between open-weight (that we abbreviate to OSS for simplicity) and proprietary models. The figures below illustrate how this balance has evolved on OpenRouter over the past year. While proprietary models, especially those from major North American providers, still serve the majority of tokens, OSS models have grown steadily, reaching approximately one-third of usage by late 2025.&lt;/p&gt;&lt;p&gt;This expansion is not incidental. Usage spikes align with major open-model releases such as DeepSeek V3 and Kimi K2 (indicated by vertical dashed lines in the first figure), suggesting that competitive OSS launches such as DeepSeek V3 [9] and GPT OSS models [8] are adopted rapidly and sustain their gains. Importantly, these increases persist beyond initial release weeks, implying genuine production use rather than short-term experimentation.&lt;/p&gt;&lt;p&gt;A significant share of this growth has come from Chinese-developed models. Starting from a negligible base in late 2024 (weekly share as low as 1.2%), Chinese OSS models steadily gained traction, reaching nearly 30% of total usage among all models in some weeks. Over the one-year window, they averaged approximately 13.0% of weekly token volume, with strong growth concentrated in the second half of 2025. For comparison, RoW OSS models averaged 13.7%, while proprietary RoW models retained the largest share (70% on average). The expansion of Chinese OSS reflects not only competitive quality, but also rapid iteration and dense release cycles. Models like Qwen and DeepSeek maintained regular model releases that enabled fast adaptation to emerging workloads. This pattern has materially reshaped the open source segment and progressed global competition across the LLM landscape.&lt;/p&gt;&lt;p&gt;These trends indicate a durable dual structure in the LLM ecosystem. Proprietary systems continue to define the upper bound of reliability and performance, particularly for regulated or enterprise workloads. OSS models, by contrast, offer cost efficiency, transparency, and customization, making them an attractive option for certain workloads. The equilibrium is currently reached at roughly 30%. These models are not mutually exclusive; rather, they complement each other within a multi-model stack that developers and infrastructure providers increasingly favor.&lt;/p&gt;&lt;p&gt;The table below ranks the top model families in our dataset by total token volume served. The landscape of OSS models has shifted significantly over the last year: while DeepSeek remains the single largest OSS contributor by volume, its dominance has waned as new entrants rapidly gain ground. Today, multiple open source families each sustain substantial usage, pointing to a diversified ecosystem.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Model Author&lt;/cell&gt;&lt;cell role="head"&gt;Total Tokens (Trillions)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;DeepSeek&lt;/cell&gt;&lt;cell&gt;14.37&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Qwen&lt;/cell&gt;&lt;cell&gt;5.59&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Meta LLaMA&lt;/cell&gt;&lt;cell&gt;3.96&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Mistral AI&lt;/cell&gt;&lt;cell&gt;2.92&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;OpenAI&lt;/cell&gt;&lt;cell&gt;1.65&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Minimax&lt;/cell&gt;&lt;cell&gt;1.26&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Z-AI&lt;/cell&gt;&lt;cell&gt;1.18&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;TNGTech&lt;/cell&gt;&lt;cell&gt;1.13&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;MoonshotAI&lt;/cell&gt;&lt;cell&gt;0.92&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0.82&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;This figure illustrates the dramatic evolution of market share among the top individual open source models week by week. Early in the period (late 2024), the market was highly consolidated: two models from the DeepSeek family (V3 and R1) consistently accounted for over half of all OSS token usage, forming the large, dark blue bands at the bottom of the chart.&lt;/p&gt;&lt;p&gt;This near-monopoly structure shattered following the Summer Inflection (mid-2025). The market has since become both broader and deeper, with usage diversifying significantly. New entrants like Qwen's models, Minimax's M2, MoonshotAI's Kimi K2, and OpenAI's GPT-OSS series all grew rapidly to serve significant portions of requests, often achieving production-scale adoption within weeks of release. This signals that the open source community and AI startups can achieve quick adoption by introducing models with novel capabilities or superior efficiency.&lt;/p&gt;&lt;p&gt;By late 2025, the competitive balance had shifted from near-monopoly to a pluralistic mix. No single model exceeds 25% of OSS tokens, and the token share is now distributed more evenly across five to seven models. The practical implication is that users are finding value in a wider array of options, rather than defaulting to one "best" choice. Although this figure visualizes relative share among OSS models (not absolute volume), the clear trend is a decisive shift toward market fragmentation and increased competition within the open source ecosystem.&lt;/p&gt;&lt;p&gt;Overall, the open source model ecosystem is now highly dynamic. Key insights include:&lt;/p&gt;&lt;p&gt;Today the open source LLM arena in 2025 resembles a competitive ecosystem where innovation cycles are rapid and leadership is not guaranteed. For model builders, this means that releasing an open model with state-of-the-art performance can yield immediate uptake, but maintaining usage share requires ongoing investment in further development. For users and application developers, the trend is positive: there is a richer selection of open models to choose from, often with comparable or sometimes superior capabilities to proprietary systems in specific areas (like roleplay).&lt;/p&gt;&lt;p&gt;A year ago, the open source model ecosystem was largely a story of trade-offs between two extremes: a vast number of small, fast models and a handful of powerful, large-scale models. However, a review of the past year reveals a significant maturation of the market and the emergence of a new, growing category: the medium-sized model. Please note that we categorize models by their parameter count as follows:&lt;/p&gt;&lt;p&gt;The data on developer and user behavior tells us a nuanced story. The figures show that while the number of models across all categories has grown, the usage has shifted notably. Small models are losing favor while medium and large models are capturing that value.&lt;/p&gt;&lt;p&gt;A deeper look at the models driving these trends reveals distinct market dynamics:&lt;/p&gt;&lt;code&gt;Google Gemma 3.12B&lt;/code&gt; (released August 2025) saw a rapid adoption but competes in a crowded field where users continually seek the next best alternative.&lt;code&gt;Qwen2.5 Coder 32B&lt;/code&gt; in November 2024, which effectively established this category. This segment then matured into a competitive ecosystem with the arrival of other strong contenders like &lt;code&gt;Mistral Small 3&lt;/code&gt; (January 2025) and &lt;code&gt;GPT-OSS 20B&lt;/code&gt; (August 2025), which carved out user mind share. This segment demonstrates that users are seeking a balance of capability and efficiency.&lt;code&gt;Qwen3 235B A22B Instruct&lt;/code&gt; (released in July 2025) and &lt;code&gt;Z.AI GLM 4.5 Air&lt;/code&gt; to &lt;code&gt;OpenAI: GPT-OSS-120B&lt;/code&gt; (August 5th): each capturing meaningful and sustained usage. This pluralism suggests users are actively benchmarking across multiple open large models rather than converging on a single standard.&lt;p&gt;The era of small models dominating the open source ecosystem might be behind. The market is now bifurcating, with users either gravitating toward a new, robust class of medium models, or consolidating their workloads onto the single most capable large model.&lt;/p&gt;&lt;p&gt;Open-source models today are employed for a remarkably broad range of tasks, spanning creative, technical, and informational domains. While proprietary models still dominate in structured business tasks, OSS models have carved out leadership in two particular areas: creative roleplay and programming assistance. Together, these categories account for the majority of OSS token usage.&lt;/p&gt;&lt;p&gt;The figure above highlights that more than half of all OSS model usage falls under Roleplay, with Programming being the second-largest category. This indicates that users turn to open models primarily for creative interactive dialogues (such as storytelling, character roleplay, and gaming scenarios) and for coding-related tasks. The dominance of roleplay (hovering at more than 50% of all OSS tokens) underscores a use case where open models have an edge: they can be utilized for creativity and are often less constrained by content filters, making them attractive for fantasy or entertainment applications. Roleplay tasks require flexible responses, context retention, and emotional nuance - attributes that open models can deliver effectively without being heavily restricted by commercial safety or moderation layers. This makes them particularly appealing for communities experimenting with character-driven experiences, fan fiction, interactive games, and simulation environments.&lt;/p&gt;&lt;p&gt;The figure above shows category breakdown over time if we zoom in on Chinese OSS models only. These models are no longer used primarily for creative tasks. Roleplay remains the largest category at around 33%, but programming and technology now account for a combined majority of usage (39%). This shift suggests that models like &lt;code&gt;Qwen&lt;/code&gt; and &lt;code&gt;DeepSeek&lt;/code&gt; are increasingly used for code generation and infrastructure-related workloads. While high-volume enterprise users may influence specific segments, the overall trend points to Chinese OSS models competing directly in technical and productivity domains.&lt;/p&gt;&lt;p&gt;If we zoom in just on the programming category, we observe that proprietary models still handle the bulk of coding assistance overall (the gray region), reflecting strong offerings like Anthropic's Claude. However, within the OSS portion, there was a notable transition: in mid-2025, Chinese OSS models (blue) delivered the majority of open source coding help (driven by early successes like &lt;code&gt;Qwen 3 Coder&lt;/code&gt;). By Q4 2025, Western OSS models (orange) such as Meta's LLaMA-2 Code and OpenAI's GPT-OSS series had surged, but decreased in overall share in recent weeks. This oscillation suggests a very competitive environment. The practical takeaway is that open source code assistant usage is dynamic and highly responsive to new model quality: developers are open to whichever OSS model currently provides the best coding support. As a limitation, this figure doesn't show absolute volumes: open source coding usage grew overall so a shrinking blue band doesn't mean Chinese OSS lost users, only relative share.&lt;/p&gt;&lt;p&gt;Now if we examine just the roleplay traffic, we see that it is now almost equally served by Rest-of-World OSS (orange, 43% in recent weeks) and Closed (gray, at ~42% most recently) models. This represents a significant shift from earlier in 2025, when the category was dominated by proprietary (gray) models, which held approximately 70% of the token share. At that time (May 2025), Western OSS models accounted for only ~22% of traffic, and Chinese OSS (blue) models held a small share of ~8%. Throughout the year, the proprietary share steadily eroded. By the end of October 2025, this trend accelerated as both Western and Chinese open source models gained significant ground.&lt;/p&gt;&lt;p&gt;The resulting convergence indicates a healthy competition; users have viable choices from both open and proprietary offerings for creative chats and storytelling. This reflects that developers recognize the demand for roleplay/chat models and have tailored their releases to that end (e.g., fine-tuning on dialogues, adding alignment for character consistency). A point to note is that "roleplay" covers a range of subgenres (from casual chatting to complex game scenarios). Yet from a macro perspective, it is clear OSS models have an edge in this creative arena.&lt;/p&gt;&lt;p&gt;Interpretation. Broadly, across the OSS ecosystem, the key use cases are: Roleplay and creative dialogue: the top category, likely because open models can be uncensored or more easily customized for fictional persona and story tasks. Programming assistance: second-largest, and growing, as open models become more competent at code. Many developers leverage OSS models locally for coding to avoid API costs. Translation and multilingual support: a steady use case, especially with strong bilingual models available (Chinese OSS models have an edge here). General knowledge Q&amp;amp;A and education: moderate usage; while open models can answer questions, users may prefer closed models like GPT-5 for highest factual accuracy.&lt;/p&gt;&lt;p&gt;It is worth noting that the OSS usage pattern (heavy on roleplay) mirrors what many might consider for "enthusiasts" or "indie developers" - areas where customization and cost-efficiency trump absolute accuracy. The lines are blurring, though: OSS models are rapidly improving in technical domains, and proprietary models are being used creatively too.&lt;/p&gt;&lt;p&gt;Building on the previous section's view of the evolving model landscape (open vs closed source), we now turn to the fundamental shape of LLM usage itself. A foundational shift is underway in how language models are used in production: from single-turn text completion toward multi-step, tool-integrated, and reasoning-intensive workflows. We refer to this shift as the rise of agentic inference, where models are deployed not just to generate text, but to act through planning, calling tools, or interacting across extended contexts. This section traces that shift through five proxies: the rise of reasoning models, the expansion of tool-calling behavior, the changing sequence length profile, and how programming use drives complexity.&lt;/p&gt;&lt;p&gt;As shown in the figure above, the share of total tokens routed through reasoning-optimized models climbed sharply in 2025. What was effectively a negligible slice of usage in early Q1 now exceeds fifty percent. This shift reflects both sides of the market. On the supply side, the release of higher-capability systems like GPT-5, Claude 4.5, and Gemini 3 expanded what users could expect from stepwise reasoning. On the demand side, users increasingly prefer models that can manage task state, follow multi-step logic, and support agent-style workflows rather than simply generate text.&lt;/p&gt;&lt;p&gt;The figure above shows the top models driving this shift. In the most recent data, xAI's Grok Code Fast 1 now drives the largest share of reasoning traffic (excluding free launch access), ahead of Google's Gemini 2.5 Pro and Gemini 2.5 Flash. This is a notable change from only a few weeks ago, when Gemini 2.5 Pro led the category and DeepSeek R1 and Qwen3 were also in the top tier. Grok Code Fast 1 and Grok 4 Fast have gained share quickly, supported by xAI's aggressive rollout, competitive pricing, and developer attention around its code-oriented variants. At the same time, the continued presence of open models like OpenAI's gpt-oss-120b underscores that developers still reach for OSS when possible. The mix overall highlights how dynamic the reasoning landscape has become, with rapid model turnover shaping which systems dominate real workloads.&lt;/p&gt;&lt;p&gt;The data points to a clear conclusion: reasoning-oriented models are becoming the default path for real workloads, and the share of tokens flowing through them is now a leading indicator of how users want to interact with AI systems.&lt;/p&gt;&lt;p&gt;In the figure above, we report the share of total tokens originating from requests whose finish reason was a Tool Call. This metric is normalized and captures only those interactions in which a tool was actually invoked.&lt;/p&gt;&lt;p&gt;This is in contrast to the Input Tool signal that records whether a tool was provided to the model during a request (regardless of invocation). Input Tool counts are, by definition, higher than Tool Call finish reasons, since provision is a superset of successful execution. Whereas the finish-reason metric measures realized tool use, Input Tool reflects potential availability rather than actual invocation. Because this metric was introduced only in September 2025, we are not reporting it in this paper.&lt;/p&gt;&lt;p&gt;The noticeable spike in May in the figure above was largely attributable to one sizable account whose activity briefly lifted overall volumes. Aside from this anomaly, tool adoption has shown a consistent upward trend throughout the year.&lt;/p&gt;&lt;p&gt;As shown in the figure above, tool invocation was initially concentrated among a small group of models: OpenAI's &lt;code&gt;gpt-4o-mini&lt;/code&gt; and Anthropic's Claude 3.5 and 3.7 series, which together accounted for most tool-enabled tokens in early 2025. By mid-year, however, a broader set of models began supporting tool provision, reflecting a more competitive and diversified ecosystem. From end of September onward, newer Claude 4.5 Sonnet model rapidly gained share. Meanwhile, newer entries like &lt;code&gt;Grok Code Fast&lt;/code&gt; and &lt;code&gt;GLM 4.5&lt;/code&gt; have made visible inroads, reflecting broader experimentation and diversification in tool-capable deployments.&lt;/p&gt;&lt;p&gt;For operators, the implication is clear: enabling tool use is on the rise for high-value workflows. Models without reliable tool formats risk falling behind in enterprise adoption and orchestration environments.&lt;/p&gt;&lt;p&gt;The shape of model workloads has evolved markedly over the past year. Both prompt (input) and completion (output) token volumes have risen sharply, though at different scales and rates. Average prompt tokens per request have increased roughly fourfold from around 1.5K to over 6K while completions have nearly tripled from about 150 to 400 tokens. The relative magnitude of growth highlights a decisive shift toward more complex, context-rich workloads.&lt;/p&gt;&lt;p&gt;This pattern reflects a new equilibrium in model usage. The typical request today is less about open-ended generation ("write me an essay") and more about reasoning over substantial user-provided material such as codebases, documents, transcripts, or long conversations, and producing concise, high-value insights. Models are increasingly acting as analytical engines rather than creative generators.&lt;/p&gt;&lt;p&gt;Category-level data (available only since Spring 2025) provides a more nuanced picture: programming workloads are the dominant driver of prompt token growth. Requests involving code understanding, debugging, and code generation routinely exceed 20K input tokens, while all other categories remain relatively flat and low-volume. This asymmetric contribution suggests that the recent expansion in prompt size is not a uniform trend across tasks but rather a concentrated surge tied to software development and technical reasoning use cases.&lt;/p&gt;&lt;p&gt;Sequence length is a proxy for task complexity and interaction depth. The figure above shows that average sequence length has more than tripled over the past 20 months from under 2,000 tokens in late 2023 to over 5,400 by late 2025. This growth reflects a structural shift toward longer context windows, deeper task history, and more elaborate completions.&lt;/p&gt;&lt;p&gt;As per previous section, the second figure adds further clarity: programming-related prompts now average 3–4 times the token length of general-purpose prompts. The divergence indicates that software development workflows are the primary driver of longer interactions. Long sequences are not just user verbosity: they are a signature of embedded, more sophisticated agentic workflows.&lt;/p&gt;&lt;p&gt;Together, these trends (rising reasoning share, expanded tool use, longer sequences, and programming's outsize complexity) suggest that the center of gravity in LLM usage has shifted. The median LLM request is no longer a simple question or isolated instruction. Instead, it is part of a structured, agent-like loop, invoking external tools, reasoning over state, and persisting across longer contexts.&lt;/p&gt;&lt;p&gt;For model providers, this raises the bar for default capabilities. Latency, tool handling, context support, and robustness to malformed or adversarial tool chains are increasingly critical. For infra operators, inference platforms must now manage not just stateless requests but long-running conversations, execution traces, and permission-sensitive tool integrations. Soon enough, if not already, agentic inference will be taking over the majority of the inference.&lt;/p&gt;&lt;p&gt;Understanding the distribution of tasks that users perform with LLMs is central to assessing real-world demand and model–market fit. As described in the Data and Methodology section, we categorized billions of model interactions into high-level application categories. In the Open vs. Closed Source Models section, we focused on open source models to see community-driven usage. Here, we broaden the lens to all LLM usage on OpenRouter (both closed and open models) to get a comprehensive picture of what people use LLMs for in practice.&lt;/p&gt;&lt;p&gt;Programming has become the most consistently expanding category across all models. The share of programming-related requests has grown steadily through 2025, paralleling the rise of LLM-assisted development environments and tool integrations. As shown in the figure above, programming queries accounted for roughly 11% of total token volume in early 2025 and exceeded 50% in recent weeks. This trend reflects a shift from exploratory or conversational use toward applied tasks such as code generation, debugging, and data scripting. As LLMs become embedded in developer workflows, their role as programming tools is being normalized. This evolution has implications for model development, including increased emphasis on code-centric training data, improved reasoning depth for multi-step programming tasks, and tighter feedback loops between models and integrated development environments.&lt;/p&gt;&lt;p&gt;This growing demand for programming support is reshaping competitive dynamics across model providers. As shown in the figure below, Anthropic's Claude series has consistently dominated the category, accounting for more than 60% of programming-related spend for most of the observed period. The landscape has nevertheless evolved meaningfully. During the week of November 17, Anthropic's share fell below the 60% threshold for the first time. Since July, OpenAI has expanded its share from roughly 2% to about 8% in recent weeks, likely reflecting a renewed emphasis on developer-centric workloads. Over the same interval, Google's share has remained stable at approximately 15%. The mid-tier segment is also in motion. Open source providers including Z.AI, Qwen, and Mistral AI are steadily gaining mindshare. MiniMax, in particular, has emerged as a fast-rising entrant, showing notable gains in recent weeks.&lt;/p&gt;&lt;p&gt;Overall, programming has become one of the most contested and strategically important model categories. It attracts sustained attention from top labs, and even modest changes in model quality or latency can shift share week to week. For infrastructure providers and developers, this highlights the need for continual benchmarking and evals, especially as the frontier is constantly evolving.&lt;/p&gt;&lt;p&gt;The figures above break down LLM usage across the twelve most common content categories, revealing the internal sub-topic structure of each. A key takeaway is that most categories are not evenly distributed: they are dominated by one or two recurring use patterns, often reflecting concentrated user intent or alignment with LLM strengths.&lt;/p&gt;&lt;p&gt;Among the highest-volume categories, roleplay stands out for its consistency and specialization. Nearly 60% of roleplay tokens fall under Games/Roleplaying Games, suggesting that users treat LLMs less as casual chatbots and more as structured roleplaying or character engines. This is further reinforced by the presence of Writers Resources (15.6%) and Adult content (15.4%), pointing to a blend of interactive fiction, scenario generation, and personal fantasy. Contrary to assumptions that roleplay is mostly informal dialogue, the data show a well-defined and replicable genre-based use case.&lt;/p&gt;&lt;p&gt;Programming is similarly skewed, with over two-thirds of traffic labeled as Programming/Other. This signals the broad and general-purpose nature of code-related prompts: users are not narrowly focused on specific tools or languages but are asking LLMs for everything from logic debugging to script drafting. That said, Development Tools (26.4%) and small shares from scripting languages indicate emerging specialization. This fragmentation highlights an opportunity for model builders to improve tagging or training around structured programming workflows.&lt;/p&gt;&lt;p&gt;Beyond the dominant categories of roleplay and programming, the remaining domains represent a diverse but lower-volume tail of LLM usage. While individually smaller, they reveal important patterns about how users interact with models across specialized and emerging tasks. For example, translation, science, and health show relatively flat internal structure. In translation, usage is nearly evenly split between Foreign Language Resources (51.1%) and Other, suggesting diffuse needs: multilingual lookup, rephrasing, light code-switching, rather than sustained document-level translation. Science is dominated by a single tag, Machine Learning &amp;amp; AI (80.4%), indicating that most scientific queries are meta-AI questions rather than general STEM topics like physics or biology. This reflects either user interest or model strengths skewed toward self-referential inquiry.&lt;/p&gt;&lt;p&gt;Health, in contrast, is the most fragmented of the top categories, with no sub-tag exceeding 25%. Tokens are spread across medical research, counseling services, treatment guidance, and diagnostic lookups. This diversity highlights the domain's complexity, but also the challenge of modeling it safely: LLMs must span high variance user intent, often in sensitive contexts, without clear concentration in a single use case.&lt;/p&gt;&lt;p&gt;What links these long-tail categories is their broadness: users turn to LLMs for exploratory, lightly structured, or assistance-seeking interactions, but without the focused workflows seen in programming or personal assistants. Taken together, these secondary categories may not dominate volume, but they hint at latent demand. They signal that LLMs are being used at the fringes of many fields from translation to medical guidance to AI introspection and that as models improve in domain robustness and tooling integration, we may see these scattered intents converge into clearer, higher-volume applications.&lt;/p&gt;&lt;p&gt;By contrast, finance, academia, and legal are much more diffuse. Finance spreads its volume across foreign exchange, socially responsible investing, and audit/accounting: no single tag breaks 20%. Legal shows similar entropy, with usage split between Government/Other (43.0%) and Legal/Other (17.8%). This fragmentation may reflect the complexity of these domains, or simply the lack of targeted LLM workflows for them compared to more mature categories like coding and chat.&lt;/p&gt;&lt;p&gt;The data suggest that real-world LLM usage is not uniformly exploratory: it clusters tightly around a small set of repeatable, high-volume tasks. Roleplay, programming, and personal assistance each exhibit clear structure and dominant tags. Science, health, and legal domains, by contrast, are more diffuse and likely under-optimized. These internal distributions can guide model design, domain-specific fine-tuning, and application-level interfaces particularly in tailoring LLMs to user goals.&lt;/p&gt;&lt;p&gt;Global LLM usage exhibits pronounced regional variation. By examining geographic breakdowns, we can infer how local usage and spend shape LLM usage patterns. While figures below reflect OpenRouter's user base, they offer one snapshot of regional engagements.&lt;/p&gt;&lt;p&gt;The distribution of spend, as shown in the figure below, underscores the increasingly global nature of AI inference market. North America, while still the single largest region, now accounts for less than half of total spend for most of the observed period. Europe shows a stable and durable contribution. Its relative share of weekly spend remains consistent throughout the timeline, typically occupying a band between the mid-teens and low twenties. A notable development is the rise of Asia not only as a producer of frontier models but also as a rapidly expanding consumer. In the earliest weeks of the dataset, Asia represented roughly thirteen percent of global spend. Over time, this share more than doubled, reaching approximately 31% in the most recent period.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Continent&lt;/cell&gt;&lt;cell role="head"&gt;Share (%)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;North America&lt;/cell&gt;&lt;cell&gt;47.22&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Asia&lt;/cell&gt;&lt;cell&gt;28.61&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Europe&lt;/cell&gt;&lt;cell&gt;21.32&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Oceania&lt;/cell&gt;&lt;cell&gt;1.18&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;South America&lt;/cell&gt;&lt;cell&gt;1.21&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Africa&lt;/cell&gt;&lt;cell&gt;0.46&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Country&lt;/cell&gt;&lt;cell role="head"&gt;Share (%)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;United States&lt;/cell&gt;&lt;cell&gt;47.17&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Singapore&lt;/cell&gt;&lt;cell&gt;9.21&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Germany&lt;/cell&gt;&lt;cell&gt;7.51&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;China&lt;/cell&gt;&lt;cell&gt;6.01&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;South Korea&lt;/cell&gt;&lt;cell&gt;2.88&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Netherlands&lt;/cell&gt;&lt;cell&gt;2.65&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;United Kingdom&lt;/cell&gt;&lt;cell&gt;2.52&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Canada&lt;/cell&gt;&lt;cell&gt;1.90&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Japan&lt;/cell&gt;&lt;cell&gt;1.77&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;India&lt;/cell&gt;&lt;cell&gt;1.62&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Others (60+ countries)&lt;/cell&gt;&lt;cell&gt;16.76&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Language&lt;/cell&gt;&lt;cell role="head"&gt;Token Share (%)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;English&lt;/cell&gt;&lt;cell&gt;82.87&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Chinese (Simplified)&lt;/cell&gt;&lt;cell&gt;4.95&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Russian&lt;/cell&gt;&lt;cell&gt;2.47&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Spanish&lt;/cell&gt;&lt;cell&gt;1.43&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Thai&lt;/cell&gt;&lt;cell&gt;1.03&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Other (combined)&lt;/cell&gt;&lt;cell&gt;7.25&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;As shown in the table above, English dominates usage, accounting for more than 80% of all tokens. This reflects both the prevalence of English-language models and the developer-centric skew of OpenRouter's user base. However, other languages particularly Chinese, Russian, and Spanish, make up a meaningful tail. Simplified Chinese alone accounts for nearly 5% of global tokens, suggesting sustained engagement by users in bilingual or Chinese-first environments, especially given the growth of Chinese OSS models like DeepSeek and Qwen.&lt;/p&gt;&lt;p&gt;For model builders and infrastructure operators, cross-regional usability, across languages, compliance regimes, and deployment settings, is becoming table stakes in a world where LLM adoption is simultaneously global and locally optimized.&lt;/p&gt;&lt;p&gt;Cohort Retention Rates. Retention is measured as activity retention, where users are counted if they return in subsequent months, even after periods of inactivity; as a result, curves may exhibit small non-monotonic bumps.&lt;/p&gt;&lt;p&gt;This collection of retention charts captures the dynamics of the LLM user market across leading models. At first glance, the data is dominated by high churn and rapid cohort decay. Yet beneath this volatility lies a subtler and more consequential signal: a small set of early user cohorts exhibits durable retention over time. We term these foundational cohorts.&lt;/p&gt;&lt;p&gt;These cohorts are not merely early adopters; they represent users whose workloads have achieved a deep and persistent workload–model fit. Once established, this fit creates both economic and cognitive inertia that resists substitution, even as newer models emerge.&lt;/p&gt;&lt;p&gt;We introduce the Cinderella Glass Slipper effect as a framework to describe this phenomenon. The hypothesis posits that in a rapidly evolving AI ecosystem, there exists a latent distribution of high-value workloads that remain unsolved across successive model generations. Each new frontier model is effectively "tried on" against these open problems. When a newly released model happens to match a previously unmet technical and economic constraint, it achieves the precise fit — the metaphorical "glass slipper."&lt;/p&gt;&lt;p&gt;For the developers or organizations whose workloads finally "fit," this alignment creates strong lock-in effects. Their systems, data pipelines, and user experiences become anchored to the model that solved their problem first. As costs decline and reliability increases, the incentive to re-platform diminishes sharply. Conversely, workloads that do not find such a fit remain exploratory, migrating from one model to another in search of their own solution.&lt;/p&gt;&lt;p&gt;Empirically, this pattern is observable in the June 2025 cohort of &lt;code&gt;Gemini 2.5 Pro&lt;/code&gt; and the May 2025 cohort of &lt;code&gt;Claude 4 Sonnet&lt;/code&gt;, which retain approximately 40% of users at Month 5, substantially higher than later cohorts. These cohorts appear to correspond to specific technical breakthroughs (e.g., reasoning fidelity or tool-use stability) that finally enabled previously impossible workloads.&lt;/p&gt;&lt;p&gt;In all, rapid capability shifts in foundation models necessitate a redefinition of user retention. Each new model generation introduces a brief opportunity to solve previously unmet workloads. When such alignment occurs, the affected users form foundational cohorts: segments whose retention trajectories remain stable despite subsequent model introductions.&lt;/p&gt;&lt;p&gt;The Dominant Launch Anomaly. The &lt;code&gt;OpenAI GPT-4o Mini&lt;/code&gt; chart shows this phenomenon in its extreme. A single foundational cohort (July 2024, orange line) established a dominant, sticky workload-model fit at launch. All subsequent cohorts, which arrived after this fit was established and the market had moved on, behave identically: they churn and cluster at the bottom. This suggests the window to establish this foundational fit is singular and occurs only at the moment a model is perceived as "frontier."&lt;/p&gt;&lt;p&gt;The Consequence of No-Fit. The &lt;code&gt;Gemini 2.0 Flash&lt;/code&gt; and &lt;code&gt;Llama 4 Maverick&lt;/code&gt; charts showcase a cautionary tale of what happens when this initial fit is never established. Unlike the other models, there is no high-performing foundational cohort. Every single cohort performs identically poorly. This suggests that the models were never perceived as a "frontier" for a high-value, sticky workload. It launched directly into the good enough market and thus failed to lock in any user base. Similarly, the chaotic charts for &lt;code&gt;DeepSeek&lt;/code&gt;, despite overwhelming success overall, struggle to establish a stable, foundational cohort.&lt;/p&gt;&lt;p&gt;Boomerang Effect. The DeepSeek models introduce a more complex pattern. Their retention curves display a highly unusual anomaly: resurrection jumps. Unlike typical, monotonically decreasing retention, several DeepSeek cohorts show a distinct rise in retention after an initial period of churn (e.g., DeepSeek R1's April 2025 cohort around Month 3, and DeepSeek Chat V3-0324's July 2025 cohort around Month 2). This indicates that some churned users are returning to the model. This "boomerang effect" suggests these users return to DeepSeek, after trying alternatives and confirming through competitive testing that DeepSeek provides an optimal, and often better fit for their specific workload due to a superior combination of specialized technical performance, cost-efficiency, or other unique features.&lt;/p&gt;&lt;p&gt;Implications. The Glass Slipper phenomenon reframes retention not as an outcome but as a lens for understanding capability breakthroughs. Foundational cohorts are the fingerprints of real technical progress: they mark where an AI model has crossed from novelty into necessity. For builders and investors alike, identifying these cohorts early may be the single most predictive signal of enduring model–market advantage.&lt;/p&gt;&lt;p&gt;The cost of using a model is a key factor influencing user behavior. In this section, we focus on how different AI workload categories distribute across the cost–usage landscape. By examining where categories cluster on log–log cost vs usage plots, we identify patterns in how workloads concentrate in low-cost, high-volume regions versus high-cost, specialized segments. We also reference similarities to the Jevon's paradox effects, in the sense that lower-cost categories often correspond to higher aggregate usage, though we do not attempt to formally analyze the paradox or causality.&lt;/p&gt;&lt;p&gt;The scatter plot above reveals a distinct segmentation of AI use cases, mapping them based on their aggregate usage volume (Total Tokens) against their unit cost (Cost per 1M Tokens). A critical preliminary observation is that both axes are logarithmic. This logarithmic scaling signifies that small visual distances on the chart correspond to substantial multiplicative differences in real-world volume and cost.&lt;/p&gt;&lt;p&gt;The chart is bisected by a vertical line at the median cost of $0.73 per 1M tokens, effectively creating a four-quadrant framework to simplify the AI market across categories.&lt;/p&gt;&lt;p&gt;Note that these end costs differ from advertised list prices. High-frequency workloads benefit from caching, which drives down realized spend and produces materially lower effective prices than those publicly listed. The cost metric shown reflects a blended rate across both prompt and completion tokens, providing a more accurate view of what users actually pay in aggregate. The dataset also excludes BYOK activity to isolate standardized, platform-mediated usage and avoid distortion from custom infrastructure setups.&lt;/p&gt;&lt;p&gt;Premium Workloads (Top-Right): This quadrant contains high-cost, high-usage applications, now including &lt;code&gt;technology&lt;/code&gt; and &lt;code&gt;science&lt;/code&gt;, positioned right at the intersection. These represent valuable and heavily-used professional workloads where users are willing to pay a premium for performance or specialized capabilities. &lt;code&gt;Technology&lt;/code&gt; is a significant outlier, being dramatically more expensive than any other category. This suggests that &lt;code&gt;technology&lt;/code&gt; as a use case (perhaps relating to complex system design or architecture) may require far more powerful and expensive models for inference, yet it maintains a high usage volume, indicating its essential nature.&lt;/p&gt;&lt;p&gt;Mass-Market Volume Drivers (Top-Left): This quadrant is defined by high usage and a low, at-or-below-average cost. This area is dominated by two massive use cases: &lt;code&gt;roleplay&lt;/code&gt;, &lt;code&gt;programming&lt;/code&gt; as well as &lt;code&gt;science&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;Programming&lt;/code&gt; stands out as the "killer professional" category, demonstrating the highest usage volume while having a highly optimized, median cost.&lt;code&gt;Roleplay&lt;/code&gt;'s usage volume is immense, nearly rivaling &lt;code&gt;programming&lt;/code&gt;. This is a striking insight: a consumer-facing roleplay application drives a volume of engagement on par with a top-tier professional one.&lt;p&gt;The sheer scale of these two categories confirms that both professional productivity and conversational entertainment are primary, massive drivers for AI. The cost sensitivity in this quadrant is where, as previously noted, open source models have found a significant edge.&lt;/p&gt;&lt;p&gt;Specialized Experts (Bottom-Right): This quadrant houses lower-volume, high-cost applications, including &lt;code&gt;finance&lt;/code&gt;, &lt;code&gt;academia&lt;/code&gt;, &lt;code&gt;health&lt;/code&gt;, and &lt;code&gt;marketing&lt;/code&gt;. These are high-stakes, niche professional domains. The lower aggregate volume is logical, as one might consult an AI for "health" or "finance" far less frequently than for "programming." Users are willing to pay a significant premium for these tasks, likely because the demand for accuracy, reliability, and domain-specific knowledge is extremely high.&lt;/p&gt;&lt;p&gt;Niche Utilities (Bottom-Left): This quadrant features low-cost, low-volume tasks, including &lt;code&gt;translation&lt;/code&gt;, &lt;code&gt;legal&lt;/code&gt;, and &lt;code&gt;trivia&lt;/code&gt;. These are functional, cost-optimized utilities. &lt;code&gt;Translation&lt;/code&gt; has the highest volume within this group, while &lt;code&gt;trivia&lt;/code&gt; has the lowest. Their low cost and relatively low volume suggest these tasks may be highly optimized, "solved," or commoditized, where good-enough alternative is available cheaply.&lt;/p&gt;&lt;p&gt;As noted, the most significant outlier on this chart is &lt;code&gt;technology&lt;/code&gt;. It commands the highest cost-per-token by a substantial margin while maintaining high usage. This strongly suggests a market segment with a high willingness-to-pay for high-value, complex answers (e.g., system architecture, advanced technical problem-solving). One key question is whether this high price is driven by high user value (a "demand-side" opportunity) or by a high cost-of-serving (a "supply-side" challenge), as these queries may require the most powerful frontier models. The "play" to be had in &lt;code&gt;technology&lt;/code&gt; is to service this high-value market. A provider who can serve this segment, perhaps through highly, optimized, specialist models, could potentially capture a market with higher margins.&lt;/p&gt;&lt;p&gt;The figure above maps model usage against cost per 1M tokens (log–log scale), revealing weak overall correlation. The x-axis maps out the nominal values for convenience. The trendline is nearly flat, indicating that demand is relatively price-inelastic; a 10% decrease in price corresponds to only about a 0.5–0.7% increase in usage. Yet the dispersion across the chart is substantial, reflecting strong market segmentation. Two distinct regimes appear: proprietary models from OpenAI and Anthropic occupy the high-cost, high-usage zone, while open models like DeepSeek, Mistral, and Qwen populate the low-cost, high-volume zone. This pattern supports a simple heuristic: closed source models capture high value tasks, while open source models capture high volume lower value tasks. The weak price elasticity indicates that even drastic cost differences do not fully shift demand; proprietary providers retain pricing power for mission-critical applications, while open ecosystems absorb volume from cost-sensitive users.&lt;/p&gt;&lt;table&gt;&lt;row span="5"&gt;&lt;cell role="head"&gt;Segment&lt;/cell&gt;&lt;cell role="head"&gt;Model&lt;/cell&gt;&lt;cell role="head"&gt;Price per 1M&lt;/cell&gt;&lt;cell role="head"&gt;Usage (log)&lt;/cell&gt;&lt;cell role="head"&gt;Takeaway&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Efficient giants&lt;/cell&gt;&lt;cell&gt;google/gemini-2.0-flash&lt;/cell&gt;&lt;cell&gt;$0.147&lt;/cell&gt;&lt;cell&gt;6.68&lt;/cell&gt;&lt;cell&gt;Low price and strong distribution make it a default high-volume workhorse&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Efficient giants&lt;/cell&gt;&lt;cell&gt;deepseek/deepseek-v3-0324&lt;/cell&gt;&lt;cell&gt;$0.394&lt;/cell&gt;&lt;cell&gt;6.55&lt;/cell&gt;&lt;cell&gt;Competitive quality at bargain cost drives massive adoption&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Premium leaders&lt;/cell&gt;&lt;cell&gt;anthropic/claude-3.7-sonnet&lt;/cell&gt;&lt;cell&gt;$1.963&lt;/cell&gt;&lt;cell&gt;6.87&lt;/cell&gt;&lt;cell&gt;Very high usage despite premium price, signaling preference for quality and reliability&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Premium leaders&lt;/cell&gt;&lt;cell&gt;anthropic/claude-sonnet-4&lt;/cell&gt;&lt;cell&gt;$1.937&lt;/cell&gt;&lt;cell&gt;6.84&lt;/cell&gt;&lt;cell&gt;Enterprise workloads appear price-inelastic for trusted frontier models&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Long tail&lt;/cell&gt;&lt;cell&gt;qwen/qwen-2-7b-instruct&lt;/cell&gt;&lt;cell&gt;$0.052&lt;/cell&gt;&lt;cell&gt;2.91&lt;/cell&gt;&lt;cell&gt;Rock-bottom pricing but limited reach, likely due to weaker model-market fit&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Long tail&lt;/cell&gt;&lt;cell&gt;ibm/granite-4.0-micro&lt;/cell&gt;&lt;cell&gt;$0.036&lt;/cell&gt;&lt;cell&gt;2.95&lt;/cell&gt;&lt;cell&gt;Cheap yet niche, used mainly in limited settings&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Premium specialists&lt;/cell&gt;&lt;cell&gt;openai/gpt-4&lt;/cell&gt;&lt;cell&gt;$34.068&lt;/cell&gt;&lt;cell&gt;3.53&lt;/cell&gt;&lt;cell&gt;High cost and moderate usage, reserved for the most demanding tasks&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Premium specialists&lt;/cell&gt;&lt;cell&gt;openai/gpt-5-pro&lt;/cell&gt;&lt;cell&gt;$34.965&lt;/cell&gt;&lt;cell&gt;3.42&lt;/cell&gt;&lt;cell&gt;Ultra-premium model with focused, high-stakes workloads. Still early in adoption given recent release.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The figure above is similar to the prior figure but displays the model authors. Four usage–cost archetypes emerge. Premium leaders, such as Anthropic's Claude 3.7 Sonnet and Claude Sonnet 4, command costs around $2 per 1M tokens and still reach high usage, suggesting users are willing to pay for superior reasoning and reliability at scale. Efficient giants, like Google's Gemini 2.0 Flash and DeepSeek V3 0324, pair strong performance with prices below $0.40 per 1M tokens and achieve similar usage levels, making them attractive defaults for high-volume or long-context workloads. Long tail models, including Qwen 2 7B Instruct and IBM Granite 4.0 Micro, are priced at just a few cents per 1M tokens yet sit around 10^2.9 in total usage, reflecting constraints from weaker performance, limited visibility, or fewer integrations. Finally, premium specialists, such as OpenAI's GPT-4 and GPT-5 Pro, occupy the high-cost, low-usage quadrant: at roughly $35 per 1M tokens and usage near 10^3.4, they are used sparingly for niche, high-stakes workloads where output quality matters far more than marginal token cost.&lt;/p&gt;&lt;p&gt;Overall, the scatterplot highlights that pricing power in the LLM market is not uniform. While cheaper models can drive scale through efficiency and integration, premium offerings still command strong demand where stakes are high. This fragmentation suggests that the market has not yet commoditized, and that differentiation, whether through latency, context length, or output quality, remains a source of strategic advantage.&lt;/p&gt;&lt;p&gt;These observations suggest the following:&lt;/p&gt;&lt;p&gt;From an operator's standpoint, several strategic patterns emerge. Providers like Google have leaned heavily into tiered offerings (most notably with Gemini Flash and Pro) explicitly trading off speed, cost, and capability. This tiering enables market segmentation by price sensitivity and task criticality: lightweight tasks are routed to cheaper, faster models; premium models serve complex or latency-tolerant workloads. Optimizing for use cases and reliability is often as impactful as "cutting" price. A faster, purpose-built model may be preferred over a cheaper but unpredictable one, especially in production settings. This shifts focus from cost-per-token to cost-per-successful-outcome. The relatively flat demand elasticity suggests LLMs are not yet a commodity—many users are willing to pay a premium for quality, capabilities, or stability. Differentiation still holds value, particularly when task outcomes matter more than marginal token savings.&lt;/p&gt;&lt;p&gt;This empirical study offers a data-driven perspective on how LLMs are actually being used, highlighting several themes that nuance the conventional wisdom about AI deployment:&lt;/p&gt;&lt;p&gt;1. A Multi-Model Ecosystem. Our analysis shows that no single model dominates all usage. Instead, we observe a rich multi-model ecosystem with both closed and open models capturing significant shares. For example, even though OpenAI and Anthropic models lead in many programming and knowledge tasks, open source models like DeepSeek and Qwen collectively served a large portion of total tokens (sometimes over 30%). This suggests the future of LLM usage is likely model-agnostic and heterogeneous. For developers, this means maintaining flexibility, integrating multiple models and choosing the best for each job, rather than betting everything on one model's supremacy. For model providers, it underscores that competition can come from unexpected places (e.g., a community model might erode part of your market unless you continuously improve and differentiate).&lt;/p&gt;&lt;p&gt;2. Usage Diversity Beyond Productivity. A surprising finding is the sheer volume of roleplay and entertainment-oriented usage. Over half of open source model usage was for roleplay and storytelling. Even on proprietary platforms, a non-trivial fraction of early ChatGPT use was casual and creative before professional use cases grew. This counters an assumption that LLMs are mostly used for writing code, emails, or summaries. In reality, many users engage with these models for companionship or exploration. This has important implications. It highlights a substantial opportunity for consumer-facing applications that merge narrative design, emotional engagement, and interactivity. It suggests new frontiers for personalization—agents that evolve personalities, remember preferences, or sustain long-form interactions. It also redefines model evaluation metrics: success may depend less on factual accuracy and more on consistency, coherence, and the ability to sustain engaging dialog. Finally, it opens a pathway for crossovers between AI and entertainment IP, with potential in interactive storytelling, gaming, and creator-driven virtual characters.&lt;/p&gt;&lt;p&gt;3. Agents vs Humans: The Rise of Agentic Inference. LLM usage is shifting from single-turn interactions to agentic inference, where models plan, reason, and execute across multiple steps. Rather than producing one-off responses, they now coordinate tool calls, access external data, and iteratively refine outputs to achieve a goal. Early evidence shows rising multi-step queries and chained tool use that we proxy to agentic use. As this paradigm expands, evaluation will move from language quality to task completion and efficiency. The next competitive frontier is how effectively models can perform sustained reasoning, a shift that may ultimately redefine what agentic inference at scale means in practice.&lt;/p&gt;&lt;p&gt;4. Geographic Outlook. LLM usage is becoming increasingly global and decentralized, with rapid growth beyond North America. Asia's share of total token demand has risen from about 13% to 31%, reflecting stronger enterprise adoption and innovation. Meanwhile, China has emerged as a major force, not only through domestic consumption but also by producing globally competitive models. The broader takeaway: LLMs must be globally useful performing well across languages, contexts, and markets. The next phase of competition will hinge on cultural adaptability and multilingual capability, not just model scale.&lt;/p&gt;&lt;p&gt;5. Cost vs. Usage Dynamics. The LLM market does not seem to behave like a commodity just yet: price alone explains little about usage. Users balance cost with reasoning quality, reliability, and breadth of capability. Closed models continue to capture high-value, revenue-linked workloads, while open models dominate lower-cost and high-volume tasks. This creates a dynamic equilibrium—one defined less by stability and more by constant pressure from below. Open source models continuously push the efficient frontier, especially in reasoning and coding domains (e.g. Kimi K2 Thinking) where rapid iteration and OSS innovations narrow the performance gap. Each improvement in open models compresses the pricing power of proprietary systems, forcing them to justify premiums through superior integration, consistency, and enterprise support. The resulting competition is fast-moving, asymmetric, and continuously shifting. Over time, as quality convergence accelerates, price elasticity is likely to increase, turning what was once a differentiated market into a more fluid one.&lt;/p&gt;&lt;p&gt;6. Retention and the Cinderella Glass Slipper Phenomenon. As foundation models advance in leaps, not steps, retention has become the true measure of defensibility. Each breakthrough creates a fleeting launch window where a model can "fit" a high-value workload perfectly (the Cinderella Glass Slipper moment) and once users find that fit, they stay. In this paradigm, product-market fit equals workload-model fit: being the first to solve a real pain point drives deep, sticky adoption as users build workflows and habits around that capability. Switching then becomes costly, both technically and behaviorally. For builders and investors, the signal to watch isn't growth but retention curves, namely, the formation of foundational cohorts who stay through model updates. In an increasingly fast-moving market, capturing these important unmet needs early determines who endures after the next capability leap.&lt;/p&gt;&lt;p&gt;Together, LLMs are becoming an essential computational substrate for reasoning-like tasks across domains, from programming to creative writing. As models continue to advance and deployment expands, having accurate insights on real-world usage dynamics will be crucial for making informed decisions. Ways in which people use LLMs do not always align with expectations and vary significantly country by country, state by state, use case by use case. By observing usage at scale, we can ground our understanding of LLM impact in reality, ensuring that subsequent developments, be they technical improvements, product features, or regulations, are aligned with actual usage patterns and needs. We hope this work serves as a foundation for more empirical studies and that it encourages the AI community to continuously measure and learn from real-world usage as we build the next generation of frontier models.&lt;/p&gt;&lt;p&gt;This study reflects patterns observed on a single platform, namely OpenRouter, and over a finite time window, offering only a partial view of the broader ecosystem. Certain dimensions, such as enterprise usage, locally hosted deployments, or closed internal systems, remain outside the scope of our data. Moreover, several of our data analyses rely on proxy measures: for instance, identifying agentic inference through multi-step or tool-invocation calls, or inferring user geography from billing rather than verified location data. As such, the results should be interpreted as indicative behavioral patterns rather than definitive measurements of underlying phenomena.&lt;/p&gt;&lt;p&gt;This study offers an empirical view of how large language models are becoming embedded in the world's computational infrastructure. They are now integral to workflows, applications, and agentic systems, transforming how information is generated, mediated, and consumed.&lt;/p&gt;&lt;p&gt;The past year catalyzed a step change in how the field conceives reasoning. The emergence of o1-class models normalized extended deliberation and tool use, shifting evaluation beyond single-shot benchmarks toward process-based metrics, latency-cost tradeoffs, and success-on-task under orchestration. Reasoning has become a measure of how effectively models can plan and verify to deliver more reliable outcomes.&lt;/p&gt;&lt;p&gt;The data show that the LLM ecosystem is structurally plural. No single model or provider dominates; instead, users select systems along multiple axes such as capability, latency, price, and trust depending on context. This heterogeneity is not a transient phase but a fundamental property of the market. It promotes rapid iteration and reduces systemic dependence on any one model or stack.&lt;/p&gt;&lt;p&gt;Inference itself is also changing. The rise of multi-step and tool-linked interactions signals a shift from static completion to dynamic orchestration. Users are chaining models, APIs, and tools to accomplish compound objectives, giving rise to what can be described as agentic inference. There are many reasons to believe that agentic inference will exceed, if it hasn't already, human inference.&lt;/p&gt;&lt;p&gt;Geographically, the landscape is becoming more distributed. Asian share of usage continues to expand, China specifically has emerged as both a model developer and exporter, illustrated by the rise of players like Moonshot AI, DeepSeek, and Qwen. The success of non-Western open-weight models shows that LLMs are truly global computational resource.&lt;/p&gt;&lt;p&gt;In effect, o1 did not end competition. Far from that. It expanded the design space. The field is moving toward systems thinking instead of monolithic bets, toward instrumentation instead of intuition, and toward empirical usage analytics instead of leaderboard deltas. If the past year demonstrated that agentic inference is viable at scale, the next will focus on operational excellence: measuring real task completion, reducing variance under distribution shifts, and aligning model behavior with the practical demands of production-scale workloads.&lt;/p&gt;&lt;p&gt;This work was made possible by the foundational platform, infrastructure, datasets, and technical vision developed by the OpenRouter team. In particular, Alex Atallah, Chris Clark, Louis Vichy provided the engineering groundwork and architectural direction that enabled the explorations undertaken in this study. Justin Summerville contributed fundamental support across implementation, testing, and experimental refinement. Additional contributions included launch support from Natwar Maheshwari and design edits from Julian Thayn.&lt;/p&gt;&lt;p&gt;Malika Aubakirova (a16z) served as the lead author, responsible for experiment design, implementation, data analysis, and full preparation of the paper. Anjney Midha provided strategic guidance and shaped the overarching framing and direction.&lt;/p&gt;&lt;p&gt;Early exploratory experimentation and system setup were supported by Abhi Desai during his internship at a16z. Rajko Radovanovic and Tyler Burkett, during their full-time tenure at a16z, provided targeted technical insights and practical assistance that strengthened several critical components of the work.&lt;/p&gt;&lt;p&gt;All contributors participated in discussions, provided feedback, and reviewed the final manuscript.&lt;/p&gt;&lt;p&gt;The figures below break down the internal sub-tag structure for the three major domains: roleplay, programming, and technology. Each domain exhibits distinct internal patterns that reveal how users interact with LLMs within these categories.&lt;/p&gt;&lt;p&gt;All three domains (roleplay, technology, programming) exhibit distinct internal patterns, reflecting how users engage with LLMs across different sub-categories within each major domain.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openrouter.ai/state-of-ai"/><published>2025-12-04T22:26:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154484</id><title>SMS phishers pivot to points, taxes, fake retailers</title><updated>2025-12-04T23:36:55.756835+00:00</updated><content>&lt;doc fingerprint="ba840d9209022784"&gt;
  &lt;main&gt;
    &lt;p&gt;China-based phishing groups blamed for non-stop scam SMS messages about a supposed wayward package or unpaid toll fee are promoting a new offering, just in time for the holiday shopping season: Phishing kits for mass-creating fake but convincing e-commerce websites that convert customer payment card data into mobile wallets from Apple and Google. Experts say these same phishing groups also are now using SMS lures that promise unclaimed tax refunds and mobile rewards points.&lt;/p&gt;
    &lt;p&gt;Over the past week, thousands of domain names were registered for scam websites that purport to offer T-Mobile customers the opportunity to claim a large number of rewards points. The phishing domains are being promoted by scam messages sent via Apple’s iMessage service or the functionally equivalent RCS messaging service built into Google phones.&lt;/p&gt;
    &lt;p&gt;The website scanning service urlscan.io shows thousands of these phishing domains have been deployed in just the past few days alone. The phishing websites will only load if the recipient visits with a mobile device, and they ask for the visitor’s name, address, phone number and payment card data to claim the points.&lt;/p&gt;
    &lt;p&gt;If card data is submitted, the site will then prompt the user to share a one-time code sent via SMS by their financial institution. In reality, the bank is sending the code because the fraudsters have just attempted to enroll the victim’s phished card details in a mobile wallet from Apple or Google. If the victim also provides that one-time code, the phishers can then link the victim’s card to a mobile device that they physically control.&lt;/p&gt;
    &lt;p&gt;Pivoting off these T-Mobile phishing domains in urlscan.io reveals a similar scam targeting AT&amp;amp;T customers:&lt;/p&gt;
    &lt;p&gt;Ford Merrill works in security research at SecAlliance, a CSIS Security Group company. Merrill said multiple China-based cybercriminal groups that sell phishing-as-a-service platforms have been using the mobile points lure for some time, but the scam has only recently been pointed at consumers in the United States.&lt;/p&gt;
    &lt;p&gt;“These points redemption schemes have not been very popular in the U.S., but have been in other geographies like EU and Asia for a while now,” Merrill said.&lt;/p&gt;
    &lt;p&gt;A review of other domains flagged by urlscan.io as tied to this Chinese SMS phishing syndicate shows they are also spoofing U.S. state tax authorities, telling recipients they have an unclaimed tax refund. Again, the goal is to phish the user’s payment card information and one-time code.&lt;/p&gt;
    &lt;head rend="h2"&gt;CAVEAT EMPTOR&lt;/head&gt;
    &lt;p&gt;Many SMS phishing or “smishing” domains are quickly flagged by browser makers as malicious. But Merrill said one burgeoning area of growth for these phishing kits — fake e-commerce shops — can be far harder to spot because they do not call attention to themselves by spamming the entire world.&lt;/p&gt;
    &lt;p&gt;Merrill said the same Chinese phishing kits used to blast out package redelivery message scams are equipped with modules that make it simple to quickly deploy a fleet of fake but convincing e-commerce storefronts. Those phony stores are typically advertised on Google and Facebook, and consumers usually end up at them by searching online for deals on specific products.&lt;/p&gt;
    &lt;p&gt;With these fake e-commerce stores, the customer is supplying their payment card and personal information as part of the normal check-out process, which is then punctuated by a request for a one-time code sent by your financial institution. The fake shopping site claims the code is required by the user’s bank to verify the transaction, but it is sent to the user because the scammers immediately attempt to enroll the supplied card data in a mobile wallet.&lt;/p&gt;
    &lt;p&gt;According to Merrill, it is only during the check-out process that these fake shops will fetch the malicious code that gives them away as fraudulent, which tends to make it difficult to locate these stores simply by mass-scanning the web. Also, most customers who pay for products through these sites don’t realize they’ve been snookered until weeks later when the purchased item fails to arrive.&lt;/p&gt;
    &lt;p&gt;“The fake e-commerce sites are tough because a lot of them can fly under the radar,” Merrill said. “They can go months without being shut down, they’re hard to discover, and they generally don’t get flagged by safe browsing tools.”&lt;/p&gt;
    &lt;p&gt;Happily, reporting these SMS phishing lures and websites is one of the fastest ways to get them properly identified and shut down. Raymond Dijkxhoorn is the CEO and a founding member of SURBL, a widely-used blocklist that flags domains and IP addresses known to be used in unsolicited messages, phishing and malware distribution. SURBL has created a website called smishreport.com that asks users to forward a screenshot of any smishing message(s) received.&lt;/p&gt;
    &lt;p&gt;“If [a domain is] unlisted, we can find and add the new pattern and kill the rest” of the matching domains, Dijkxhoorn said. “Just make a screenshot and upload. The tool does the rest.”&lt;/p&gt;
    &lt;p&gt;Merrill said the last few weeks of the calendar year typically see a big uptick in smishing — particularly package redelivery schemes that spoof the U.S. Postal Service or commercial shipping companies.&lt;/p&gt;
    &lt;p&gt;“Every holiday season there is an explosion in smishing activity,” he said. “Everyone is in a bigger hurry, frantically shopping online, paying less attention than they should, and they’re just in a better mindset to get phished.”&lt;/p&gt;
    &lt;head rend="h2"&gt;SHOP ONLINE LIKE A SECURITY PRO&lt;/head&gt;
    &lt;p&gt;As we can see, adopting a shopping strategy of simply buying from the online merchant with the lowest advertised prices can be a bit like playing Russian Roulette with your wallet. Even people who shop mainly at big-name online stores can get scammed if they’re not wary of too-good-to-be-true offers (think third-party sellers on these platforms).&lt;/p&gt;
    &lt;p&gt;If you don’t know much about the online merchant that has the item you wish to buy, take a few minutes to investigate its reputation. If you’re buying from an online store that is brand new, the risk that you will get scammed increases significantly. How do you know the lifespan of a site selling that must-have gadget at the lowest price? One easy way to get a quick idea is to run a basic WHOIS search on the site’s domain name. The more recent the site’s “created” date, the more likely it is a phantom store.&lt;/p&gt;
    &lt;p&gt;If you receive a message warning about a problem with an order or shipment, visit the e-commerce or shipping site directly, and avoid clicking on links or attachments — particularly missives that warn of some dire consequences unless you act quickly. Phishers and malware purveyors typically seize upon some kind of emergency to create a false alarm that often causes recipients to temporarily let their guard down.&lt;/p&gt;
    &lt;p&gt;But it’s not just outright scammers who can trip up your holiday shopping: Often times, items that are advertised at steeper discounts than other online stores make up for it by charging way more than normal for shipping and handling.&lt;/p&gt;
    &lt;p&gt;So be careful what you agree to: Check to make sure you know how long the item will take to be shipped, and that you understand the store’s return policies. Also, keep an eye out for hidden surcharges, and be wary of blithely clicking “ok” during the checkout process.&lt;/p&gt;
    &lt;p&gt;Most importantly, keep a close eye on your monthly statements. If I were a fraudster, I’d most definitely wait until the holidays to cram through a bunch of unauthorized charges on stolen cards, so that the bogus purchases would get buried amid a flurry of other legitimate transactions. That’s why it’s key to closely review your credit card bill and to quickly dispute any charges you didn’t authorize.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://krebsonsecurity.com/2025/12/sms-phishers-pivot-to-points-taxes-fake-retailers/"/><published>2025-12-04T23:08:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154491</id><title>We gave 5 LLMs $100K to trade stocks for 8 months</title><updated>2025-12-04T23:36:55.477008+00:00</updated><link href="https://www.aitradearena.com/research/we-ran-llms-for-8-months"/><published>2025-12-04T23:08:25+00:00</published></entry></feed>