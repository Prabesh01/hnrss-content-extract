<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-07T20:44:36.219186+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46525888</id><title>A4 Paper Stories</title><updated>2026-01-07T20:44:42.206704+00:00</updated><content>&lt;doc fingerprint="7299db7cc73604b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A4 Paper Stories&lt;/head&gt;
    &lt;p&gt;I sometimes resort to a rather common measuring technique that is neither fast, nor accurate, nor recommended by any standards body and yet it hasn't failed me whenever I have had to use it. I will describe it here, though calling it a technique might be overselling it. Please do not use it for installing kitchen cabinets or anything that will stare back at you every day for the next ten years. It involves one tool: a sheet of A4 paper.&lt;/p&gt;
    &lt;p&gt;Like most sensible people with a reasonable sense of priorities, I do not carry a ruler with me wherever I go. Nevertheless, I often find myself needing to measure something at short notice, usually in situations where a certain amount of inaccuracy is entirely forgivable. When I cannot easily fetch a ruler, I end up doing what many people do and reach for the next best thing, which for me is a sheet of A4 paper, available in abundant supply where I live.&lt;/p&gt;
    &lt;p&gt;From photocopying night-sky charts to serving as a scratch pad for working through mathematical proofs, A4 paper has been a trusted companion since my childhood days. I use it often. If I am carrying a bag, there is almost always some A4 paper inside: perhaps a printed research paper or a mathematical problem I have worked on recently and need to chew on a bit more during my next train ride.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dimensions&lt;/head&gt;
    &lt;p&gt;The dimensions of A4 paper are the solution to a simple, elegant problem. Imagine designing a sheet of paper such that, when you cut it in half parallel to its shorter side, both halves have exactly the same aspect ratio as the original. In other words, if the shorter side has length \( x \) and the longer side has length \( y , \) then \[ \frac{y}{x} = \frac{x}{y / 2} \] which gives us \[ \frac{y}{x} = \sqrt{2}. \] Test it out. Suppose we have \( y/x = \sqrt{2}. \) We cut the paper in half parallel to the shorter side to get two halves, each with shorter side \( x' = y / 2 = x \sqrt{2} / 2 = x / \sqrt{2} \) and longer side \( y' = x. \) Then indeed \[ \frac{y'}{x'} = \frac{x}{x / \sqrt{2}} = \sqrt{2}. \] In fact, we can keep cutting the halves like this and we'll keep getting even smaller sheets with the aspect ratio \( \sqrt{2} \) intact. To summarise, when a sheet of paper has the aspect ratio \( \sqrt{2}, \) bisecting it parallel to the shorter side leaves us with two halves that preserve the aspect ratio. A4 paper has this property.&lt;/p&gt;
    &lt;p&gt;But what are the exact dimensions of A4 and why is it called A4? What does 4 mean here? Like most good answers, this one too begins by considering the numbers \( 0 \) and \( 1. \) Let me elaborate.&lt;/p&gt;
    &lt;p&gt;Let us say we want to make a sheet of paper that is \( 1 \, \mathrm{m}^2 \) in area and has the aspect-ratio-preserving property that we just discussed. What should its dimensions be? We want \[ xy = 1 \, \mathrm{m}^2 \] subject to the condition \[ \frac{y}{x} = \sqrt{2}. \] Solving these two equations gives us \[ x^2 = \frac{1}{\sqrt{2}} \, \mathrm{m}^2 \] from which we obtain \[ x = \frac{1}{\sqrt[4]{2}} \, \mathrm{m}, \quad y = \sqrt[4]{2} \, \mathrm{m}. \] Up to three decimal places, this amounts to \[ x = 0.841 \, \mathrm{m}, \quad y = 1.189 \, \mathrm{m}. \] These are the dimensions of A0 paper. They are precisely the dimensions specified by the ISO standard for it. It is quite large to scribble mathematical solutions on, unless your goal is to make a spectacle of yourself and cause your friends and family to reassess your sanity. So we need something smaller that allows us to work in peace, without inviting commentary or concerns from passersby. We take the A0 paper of size \[ 84.1 \, \mathrm{cm} \times 118.9 \, \mathrm{cm} \] and bisect it to get A1 paper of size \[ 59.4 \, \mathrm{cm} \times 84.1 \, \mathrm{cm}. \] Then we bisect it again to get A2 paper with dimensions \[ 42.0 \, \mathrm{cm} \times 59.4 \, \mathrm{cm}. \] And once again to get A3 paper with dimensions \[ 29.7 \, \mathrm{cm} \times 42.0 \, \mathrm{cm}. \] And then once again to get A4 paper with dimensions \[ 21.0 \, \mathrm{cm} \times 29.7 \, \mathrm{cm}. \] There we have it. The dimensions of A4 paper. These numbers are etched in my memory like the multiplication table of \( 1. \) We can keep going further to get A5, A6, etc. We could, in theory, go all the way up to A\( \infty. \) Hold on, I think I hear someone heckle. What's that? Oh, we can't go all the way to A\( \infty? \) Something about atoms, was it? Hmm. Security! Where's security? Ah yes, thank you, sir. Please show this gentleman out, would you?&lt;/p&gt;
    &lt;p&gt;Sorry for the interruption, ladies and gentlemen. Phew! That fellow! Atoms? Honestly. We, the mathematically inclined, are not particularly concerned with such trivial limitations. We drink our tea from doughnuts. We are not going to let the size of atoms dictate matters, now are we?&lt;/p&gt;
    &lt;p&gt;So I was saying that we can bisect our paper like this and go all the way to A\( \infty. \) That reminds me. Last night I was at a bar in Hoxton and I saw an infinite number of mathematicians walk in. The first one asked, "Sorry to bother you, but would it be possible to have a sheet of A0 paper? I just need something to scribble a few equations on." The second one asked, "If you happen to have one spare, could I please have an A1 sheet?" The third one said, "An A2 would be perfectly fine for me, thank you." Before the fourth one could ask, the bartender disappeared into the back for a moment and emerged with two sheets of A0 paper and said, "Right. That should do it. Do know your limits and split these between yourselves."&lt;/p&gt;
    &lt;p&gt;In general, a sheet of A\( n \) paper has the dimensions \[ 2^{-(2n + 1)/4} \, \mathrm{m} \times 2^{-(2n - 1)/4} \, \mathrm{m}. \] If we plug in \( n = 4, \) we indeed get the dimensions of A4 paper: \[ 0.210 \, \mathrm{m} \times 0.297 \, \mathrm{m}. \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Measuring Stuff&lt;/head&gt;
    &lt;p&gt;Let us now return to the business of measuring things. As I mentioned earlier, the dimensions of A4 are lodged firmly into my memory. Getting hold of a sheet of A4 paper is rarely a challenge where I live. I have accumulated a number of A4 paper stories over the years. Let me share a recent one. I was hanging out with a few folks of the nerd variety one afternoon when the conversation drifted, as it sometimes does, to a nearby computer monitor that happened to be turned off. At some point, someone confidently declared that the screen in front of us was 27 inches. That sounded plausible but we wanted to confirm it. So I reached for my trusted measuring instrument: an A4 sheet of paper. What followed was neither fast, nor especially precise, but it was more than adequate for settling the matter at hand.&lt;/p&gt;
    &lt;p&gt;I lined up the longer edge of the A4 sheet with the width of the monitor. One length. Then I repositioned it and measured a second length. The screen was still sticking out slightly at the end. By eye, drawing on an entirely unjustified confidence built from years of measuring things that never needed measuring, I estimated the remaining bit at about \( 1 \, \mathrm{cm}. \) That gives us a width of \[ 29.7 \, \mathrm{cm} + 29.7 \, \mathrm{cm} + 1.0 \, \mathrm{cm} = 60.4 \, \mathrm{cm}. \] Let us round that down to \( 60 \, \mathrm{cm}. \) For the height, I switched to the shorter edge. One full \( 21 \, \mathrm{cm} \) fit easily. For the remainder, I folded the paper parallel to the shorter side, producing an A5-sized rectangle with dimensions \( 14.8 \, \mathrm{cm} \times 21.0 \, \mathrm{cm}. \) Using the \( 14.8 \, \mathrm{cm} \) edge, I discovered that it overshot the top of the screen slightly. Again, by eye, I estimated the excess at around \( 2 \, \mathrm{cm}. \) That gives us \[ 21.0 \, \mathrm{cm} + 14.8 \, \mathrm{cm} -2.0 \, \mathrm{cm} = 33.8 \, \mathrm{cm}. \] Let us round this up to \( 34 \, \mathrm{cm}. \) The ratio \( 60 / 34 \approx 1.76 \) is quite close to \( 16/9, \) a popular aspect ratio of modern displays. At this point the measurements were looking good. So far, the paper had not embarrassed itself. Invoking the wisdom of the Pythagoreans, we can now estimate the diagonal as \[ \sqrt{(60 \, \mathrm{cm})^2 + (34 \, \mathrm{cm})^2} \approx 68.9 \,\mathrm{cm}. \] Finally, there is the small matter of units. One inch is \( 2.54 \, \mathrm{cm}, \) another figure that has embedded itself in my head. Dividing \( 68.9 \) by \( 2.54 \) gives us roughly \( 27.2 \, \mathrm{in}. \) So yes. It was indeed a \( 27 \)-inch display. My elaborate exercise in showing off my A4 paper skills was now complete. Nobody said anything. A few people looked away in silence. I assumed they were reflecting. I am sure they were impressed deep down. Or perhaps... no, no. They were definitely impressed. I am sure.&lt;/p&gt;
    &lt;p&gt;Hold on. I think I hear another heckle. What is that? There are mobile phone apps that can measure things now? Really? Right. Security. Where's security?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://susam.net/a4-paper-stories.html"/><published>2026-01-07T12:54:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46526740</id><title>Sugar industry influenced researchers and blamed fat for CVD (2016)</title><updated>2026-01-07T20:44:41.986393+00:00</updated><content>&lt;doc fingerprint="e398a9b742a7c320"&gt;
  &lt;main&gt;&lt;p&gt;This article is archived and only made available for historical reference. If you‚Äôd like to discover UCSF‚Äôs most recent advances in research, education and patient care, please visit the UCSF News Center.&lt;/p&gt;&lt;head rend="h1"&gt;Archive: Sugar Papers Reveal Industry Role in Shifting National Heart Disease Focus to Saturated Fat&lt;/head&gt;&lt;p&gt;A newly discovered cache of industry documents revealed that the sugar industry began working closely with nutrition scientists in the mid-1960s to single out fat and cholesterol as the dietary causes of coronary heart disease and to downplay evidence that sucrose consumption was also a risk factor.&lt;/p&gt;&lt;p&gt;An analysis of those papers by researchers at UC San Francisco appears Sept. 12, 2016, in JAMA Internal Medicine.&lt;/p&gt;&lt;p&gt;The internal industry documents, which were found in public archives, showed that a sugar industry trade organization recognized as early as 1954 that if Americans adopted low-fat diets, then per-capita consumption of sucrose would increase by more than one-third. The trade organization represented 30 international members.&lt;/p&gt;&lt;p&gt;Meanwhile, evidence linking sugar consumption to high blood cholesterol and triglyceride levels ‚Äì both thought to be risk factors for coronary heart disease ‚Äì began to emerge in the scientific literature and popular press.&lt;/p&gt;&lt;head rend="h2"&gt;Literature Shaped Public Opinion&lt;/head&gt;&lt;p&gt;After a 1965 spike in media attention to the heart disease risks of sucrose, the sugar industry commissioned Project 226, a literature review written by researchers at the Harvard University School of Public Health Nutrition Department, which was published in the highly respected New England Journal of Medicine (NEJM) in 1967. It concluded there was ‚Äúno doubt‚Äù that the only dietary intervention required to prevent coronary heart disease was to reduce dietary cholesterol and substitute polyunsaturated fat for saturated fat in the American diet.&lt;/p&gt;Cristin Kearns, DDS, MBA&lt;p&gt;‚ÄúThe literature review helped shape not only public opinion on what causes heart problems but also the scientific community‚Äôs view of how to evaluate dietary risk factors for heart disease,‚Äù said lead author Cristin Kearns, DDS, MBA, who discovered the industry documents.&lt;/p&gt;&lt;p&gt;The UCSF researchers analyzed more than 340 documents, totaling 1,582 pages of text, between the sugar industry and two individuals: Roger Adams, then a professor of organic chemistry who served on scientific advisory boards for the sugar industry; and D. Mark Hegsted, one of the Harvard researchers who produced the literature review.&lt;/p&gt;&lt;p&gt;To conduct the literature review, the sugar industry paid the Harvard scientists the equivalent of $50,000 in 2016 dollars, then set the review‚Äôs objective, contributed articles to be included, and received drafts. Yet the industry‚Äôs funding and role were not disclosed in the final NEJM publication.&lt;/p&gt;&lt;p&gt;The literature review heavily criticized studies linking sucrose to heart disease, while ignoring limitations of studies investigating dietary fats. The review argued that blood cholesterol levels were the only significant risk factor for coronary heart disease, which made the high sucrose content of the American diet seem less hazardous than if blood triglycerides were also considered to be a risk factor.&lt;/p&gt;&lt;head rend="h2"&gt;Need for More Transparent Scientific Reviews&lt;/head&gt;Stanton A. Glantz, PhD&lt;p&gt;The authors emphasized that this analysis demonstrates the importance of having scientific reviews written by people without conflicts of interest, as well as the need for financial disclosure in nutrition science.&lt;/p&gt;&lt;p&gt;‚ÄúAs the saying goes, he who pays the piper calls the tune,‚Äù said senior author Stanton A. Glantz, PhD, UCSF professor of medicine and director of the UCSF Center for Tobacco Control Research and Education. ‚ÄúThere are all kinds of ways that you can subtly manipulate the outcome of a study, which industry is very well practiced at.‚Äù&lt;/p&gt;&lt;p&gt;Co-author Laura Schmidt, PhD, who is also principal investigator on the UCSF-led SugarScience initiative, noted that after decades of focusing on saturated fat as the dietary culprit in heart disease, the science is building around sugar‚Äôs role, but health policy has only just begun to catch up.&lt;/p&gt;Laura Schmidt, PhD&lt;p&gt;‚ÄúThere is now a considerable body of evidence linking added sugars to hypertension and cardiovascular disease, which is the No. 1 cause of premature death in the developed world,‚Äù Schmidt said. ‚ÄúYet, health policy documents are still inconsistent in citing heart disease risk as a health consequence of added sugars consumption.‚Äù&lt;/p&gt;&lt;p&gt;The study was funded by the UCSF Philip R. Lee Institute for Health Policy Studies; a donation by the Hellmann Family Fund to the UCSF Center for Tobacco Control Research and Education; the UCSF School of Dentistry Department of Orofacial Sciences and Global Oral Health Program; and grants from the National Institute of Dental and Craniofacial Research and the National Cancer Institute.&lt;/p&gt;&lt;p&gt;UCSF is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It includes top-ranked graduate schools of dentistry, medicine, nursing and pharmacy; a graduate division with nationally renowned programs in basic, biomedical, translational and population sciences; and a preeminent biomedical research enterprise. It also includes UCSF Health, which comprises two top-ranked hospitals, UCSF Medical Center and UCSF Benioff Children‚Äôs Hospital San Francisco, and other partner and affiliated hospitals and healthcare providers throughout the Bay Area.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus"/><published>2026-01-07T14:29:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46526933</id><title>LaTeX Coffee Stains [pdf] (2021)</title><updated>2026-01-07T20:44:41.773287+00:00</updated><content/><link href="https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf"/><published>2026-01-07T14:46:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527157</id><title>Meditation as Wakeful Relaxation: Unclenching Smooth Muscle</title><updated>2026-01-07T20:44:41.673503+00:00</updated><content/><link href="https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation"/><published>2026-01-07T15:03:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527161</id><title>Shipmap.org</title><updated>2026-01-07T20:44:41.161785+00:00</updated><content>&lt;doc fingerprint="14b4e15227c5a82f"&gt;
  &lt;main&gt;
    &lt;p&gt;Data: exactEarth &amp;amp; Clarksons&lt;/p&gt;
    &lt;p&gt;Due to popular demand the designers of this map, Kiln, are now selling stunning high-resolution versions of the world √¢routes√¢ view. There are two versions available: coloured by ship type over the inky-blue base map; or just the ship in a single colour a transparent background so you can overlay or print onto whatever background colour you like. Contact [email protected] for pricing and further information.&lt;/p&gt;
    &lt;p&gt;Yes. You are welcome to embed this map. Please include a link back to Kiln somewhere in the text of your article. Use the following embed code for a fully responsive embed that will adjust to the width of your website. Feel free to change the height and/or give it a fixed width if you prefer.&lt;/p&gt;
    &lt;p&gt;You can see movements of the global merchant fleet over the course of 2012, overlaid on a bathymetric map. You can also see a few statistics such as a counter for emitted CO2 (in thousand tonnes) and maximum freight carried by represented vessels (varying units).&lt;/p&gt;
    &lt;p&gt;You can pan and zoom in the usual ways, and skip back and forward in time using the timeline at the bottom of the screen. The controls at the top right let you show and hide different map layers: port names, the background map, routes (a plot of all recorded vessel positions), and the animated ships view. There are also controls for filtering and colouring by vessel type.&lt;/p&gt;
    &lt;p&gt;The merchant fleet is divided into five categories, each of which has a filter and a CO2 and freight counter for the hour shown on the clock. The ship types and units are as follows:&lt;/p&gt;
    &lt;p&gt;In some cases this is because there are ships navigating via canals or rivers that aren√¢t visible on the map. Generally, though, this effect is an artefact of animating a ship between two recorded positions with missing data between, especially when the positions are separated by a narrow strip of land. We may develop the map to remove this effect in the future.&lt;/p&gt;
    &lt;p&gt;Unfortunately the data we are using for the map is incomplete for the first few months of the year: roughly January to April.&lt;/p&gt;
    &lt;p&gt;The map was created by Kiln based on data from the UCL Energy Institute (UCL EI)&lt;/p&gt;
    &lt;p&gt;Website: Duncan Clark &amp;amp; Robin Houston from Kiln&lt;/p&gt;
    &lt;p&gt;Data: Julia Schaumeier &amp;amp; Tristan Smith from the UCL EI&lt;/p&gt;
    &lt;p&gt;Music: Bach Goldberg Variations played by Kimiko Ishizaka&lt;/p&gt;
    &lt;p&gt;UCL EI took data showing location and speed of ships and cross-checked it with another database to get the vessel characteristics, such as engine type and hull measurements. With this information they were able to compute the CO2 emissions for each observed hour, following the approach laid out in the Third IMO Greenhouse Gas Study 2014. Kiln took the resulting dataset and visualized it with WebGL on top of a specially created base map, which shows bathymetry (ocean depth), based on the GEBCO_2014 Grid (version 20150318), as well as continents and major rivers from Natural Earth.&lt;/p&gt;
    &lt;p&gt;Our data sources for shipping positions are exactEarth for AIS data (location/speed) and Clarksons Research UK World Fleet Register (static vessel information). We are very grateful to our funders, the European Climate Foundation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.shipmap.org/"/><published>2026-01-07T15:03:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527645</id><title>The Target forensics lab (2024)</title><updated>2026-01-07T20:44:40.709435+00:00</updated><content>&lt;doc fingerprint="7e0e1880e3b5a198"&gt;
  &lt;main&gt;
    &lt;p&gt;Target, just like many other retailers, has fallen victim to shoplifters, with almost a billion dollars in goods stolen from their stores in 2023. However, the numbers could have been much worse if it weren‚Äôt for their unique anti-shoplifting tactics. Target‚Äôs way of combating shoplifting was to establish a forensics lab in Minneapolis, Minnesota, that is more advanced and high-tech than many police departments‚Äô forensics labs.&lt;/p&gt;
    &lt;p&gt;The lab was developed in 2003 to give the company expertise when it came to analyzing surveillance footage from in and around the store. Forbes states that Target has had cameras in all their stores since the 1980s, but it hadn‚Äôt been enough to stop serial shoplifting from occurring. The lab hires specialists in analyzing video evidence from cameras and smartphone recordings to help identify shoplifters, frauds, and injuries inside Target stores. However, due to the skill and technology the lab possesses, it has been of help in many cases outside of Target stores, solving some of the most gruesome crimes including murders, arsons, abductions, rapes, and mass robberies.&lt;/p&gt;
    &lt;p&gt;In many cases, the Target lab has been able to solve cases that even the Federal Bureau of Investigation (FBI) can‚Äôt solve. Forbes goes on to say that in one specific case, the experts at the Target lab were contacted by the Huston police department to help solve an arson case. A convenience store camera had caught two boys buying gasoline a short time before the fire, but the tape was damaged, making it impossible to make out the boys‚Äô faces. After the FBI was unable to solve the case, the tapes were passed over to Target, where they were repaired, and the faces of the boys were able to be seen.&lt;/p&gt;
    &lt;p&gt;Aside from stopping shoplifting and helping law enforcement, the Target lab also teaches and supports government agencies. According to the Washington Post, experts at the lab have taken a leading role in teaching government protection agencies about how to use technology to help solve crimes. In the past, Target has also helped organize undercover investigations, as well as helping United States customs verify overseas imports are coming from reputable sources.&lt;/p&gt;
    &lt;p&gt;While a retailer may seem like an odd group of people to help solve crimes, Target has proven to be a helpful resource to police forces and government agencies alike.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thehorizonsun.com/features/2024/04/11/the-target-forensics-lab/"/><published>2026-01-07T15:41:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527775</id><title>Many hells of WebDAV</title><updated>2026-01-07T20:44:40.507130+00:00</updated><content>&lt;doc fingerprint="376b9954f126582c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Many Hells of WebDAV&lt;/head&gt;
    &lt;p&gt;Implementing a WebDAV/CalDAV client and server should be easy! It‚Äôs a well documented spec, standardized in the early 00s, and somewhat widely supported. At least, that‚Äôs the naive assumption we started from when creating one for Homechart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Go Implementations&lt;/head&gt;
    &lt;p&gt;Now before you mention NIH syndrome, yes, we looked at the existing Go implementation, go-webdav. This library was lacking some key features we needed, like server-side collection synchronization, and the interfaces didn‚Äôt really align with our data model. This is also going to be a key feature of our product, so we should have some level of ownership for what gets implemented.&lt;/p&gt;
    &lt;head rend="h2"&gt;RFC Breadcrumbs&lt;/head&gt;
    &lt;p&gt;To start creating our client and server, we should read the RFCs, right? Well, where do you start?&lt;/p&gt;
    &lt;p&gt;How about the original, RFC 2518? Ah, looks like it was somewhat superseded by RFC 4918, but we‚Äôre not going to tell you which parts! How about those extension RFCs? There‚Äôs only 7 of them‚Ä¶&lt;/p&gt;
    &lt;p&gt;Reading through the RFCs, all that our implementation cares about is CRUD for Calendar events. After spending almost a month trying to implement the full RFC spec, we threw in the towel, there‚Äôs just to much legacy cruft that we didn‚Äôt need.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reverse Engineering&lt;/head&gt;
    &lt;p&gt;With a decent understanding of the RFC in hand, we instead looked into reverse engineering existing clients and servers by inspecting their requests and responses. This process was MUCH faster, and we quickly had the API mapped out and what kind of requests/responses we needed to support.&lt;/p&gt;
    &lt;p&gt;We started by identifying the clients/servers we wanted to support:&lt;/p&gt;
    &lt;p&gt;Clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Calendar&lt;/item&gt;
      &lt;item&gt;DavX&lt;/item&gt;
      &lt;item&gt;Thunderbird&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Servers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple iCloud&lt;/item&gt;
      &lt;item&gt;Google Calendar&lt;/item&gt;
      &lt;item&gt;Radicale&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then ran HTTP proxies or Wireshark to capture the HTTP requests. Because WebDAV is so obtuse, you not only need to inspect the HTTP body, but also the headers!&lt;/p&gt;
    &lt;head rend="h2"&gt;XML in Go&lt;/head&gt;
    &lt;p&gt;As an aside, we spent quite a bit of time trying to make XML work well in Go. The default Go XML library is truly terrible, and we decided to create a wrapper around it for managing XML nodes similar to how JavaScript manages HTML nodes:&lt;/p&gt;
    &lt;code&gt;var davDisplayName = xmel.Element{
  Name:  "displayname",
  Space: davNS,
}

davDisplayName.SetValue("name")
n, err := davResponse.Find(davCollectionType)
davOwner = davOwner.AddChild(davHref.SetValue("http://example.com"))
&lt;/code&gt;
    &lt;p&gt;With WebDAV having such an‚Ä¶‚Äúunstructured‚Äù schema to a lot of the requests/responses, this library was key in helping us marshal/unmarshal things without writing a bunch of ‚Äúbest case‚Äù structs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Standards are Just Suggestions&lt;/head&gt;
    &lt;p&gt;When we finally had our MVP built out, we put it to the test: validating our client and server against the existing implementations! For the most part, it worked as expected, but as always, things drift from the RFC.&lt;/p&gt;
    &lt;p&gt;Apple and Google, for instance, don‚Äôt implement half of the RFCs, and basically provide a MVP for other clients to use. They don‚Äôt really document what they support/don‚Äôt support, as WebDAV is supposed to do it via HTTP responses advertising capabilities, but both seem to provide generic responses advertising capabilities they don‚Äôt have a lot of the time.&lt;/p&gt;
    &lt;p&gt;The clients were another story. CalDAV clients are all over the place with what they support and how they will request it. Most clients should prefer to support &lt;code&gt;sync-collection&lt;/code&gt; as it‚Äôs very efficient, but Apple Calendar doesn‚Äôt, and uses ctags and etags instead.&lt;/p&gt;
    &lt;p&gt;As a little fish in a big pond, it‚Äôs frustrating dealing with situations where big providers can skirt around some standards or add quirks for their implementations, but I‚Äôm required to follow them to the T because I don‚Äôt have their inertia. I can‚Äôt file a bug, or a lawsuit, against them claiming nonconformance, they‚Äôll tell me to get bent. And you see this in other open source libraries too, where they‚Äôre littered with comments about workarounds for Google‚Äôs specific implementation or whatever.&lt;/p&gt;
    &lt;p&gt;I wouldn‚Äôt recommend anyone who values their sanity to pursue creating a WebDAV/CalDAV library.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://candid.dev/blog/many-hells-of-webdav"/><published>2026-01-07T15:50:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527950</id><title>Creators of Tailwind laid off 75% of their engineering team</title><updated>2026-01-07T20:44:40.176816+00:00</updated><content>&lt;doc fingerprint="2e60315f6425dcf9"&gt;
  &lt;main&gt;&lt;list rend="ul"&gt;&lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;&lt;item&gt;Fork 0&lt;/item&gt;&lt;/list&gt;&lt;head rend="h1"&gt;feat: add llms.txt endpoint for LLM-optimized documentation #2388&lt;/head&gt;&lt;head id="button-658b422a9548ec86" class="btn btn-sm btn-primary m-0 ml-0 ml-md-2"&gt;New issue&lt;/head&gt;&lt;p&gt;Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.&lt;/p&gt;&lt;p&gt;By clicking ‚ÄúSign up for GitHub‚Äù, you agree to our terms of service and privacy statement. We‚Äôll occasionally send you account related emails.&lt;/p&gt;&lt;p&gt;Already on GitHub? Sign in to your account&lt;/p&gt;&lt;head rend="h2"&gt;Conversation&lt;/head&gt;&lt;p&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Extract text from MDX files, removing JSX components and preserving code blocks&lt;/item&gt;&lt;item&gt;Remove standalone HTML blocks (not in code blocks)&lt;/item&gt;&lt;item&gt;Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.)&lt;/item&gt;&lt;item&gt;Statically generate the output at build time&lt;/item&gt;&lt;item&gt;Include all 185 documentation files in proper order with sections&lt;/item&gt;&lt;/list&gt;&lt;p&gt;:)&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor is attempting to deploy a commit to the Tailwind Labs Team on Vercel.&lt;/p&gt;&lt;p&gt;A member of the Team first needs to authorize it.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;code&gt;5dc6fde&lt;/code&gt;    to
    &lt;code&gt;326c151&lt;/code&gt;      
    Compare
  



    &lt;quote&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption. - Extract text from MDX files, removing JSX components and preserving code blocks - Remove standalone HTML blocks (not in code blocks) - Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.) - Statically generate the output at build time - Include all 185 documentation files in proper order with sections&lt;/quote&gt;&lt;code&gt;326c151&lt;/code&gt;    to
    &lt;code&gt;5c005a9&lt;/code&gt;      
    Compare
  



    &lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@reinink this is ready to be reviewed&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Why is this one not moving?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Yeah I've been wondering that myself.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@petersuhm maybe you missed this before?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Have more important things to do like figure out how to make enough money for the business to be sustainable right now. And making it easier for LLMs to read our docs just means less traffic to our docs which means less people learning about our paid products and the business being even less sustainable.&lt;/p&gt;&lt;p&gt;Just don't have time to work on things that don't help us pay the bills right now, sorry. We may add this one day but closing for now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Wow, what a disappointing response. This is complementary not replacement.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan as someone who has sponsored Tailwind CSS in the past, this is a disappointing response.&lt;/p&gt;&lt;p&gt;Would you like to disclose the fact that sponsoring gives one access to an official collection of LLM rules for Tailwind? Does that have anything to do with the rejection of this PR?&lt;/p&gt;&lt;p&gt;If yes, fine. You're running a business, and that's cool. But you should disclose the fact that you are monetizing this (making Tailwind docs LLM-friendly).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;It is mentioned on the sponsorship page. Seems strange to not mention that when closing this PR, though.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;In general I object to the spirit of closing this. It's very OSS unfriendly and would not meaningfully reduce traffic to the docs by humans that actually would buy the product.&lt;/p&gt;&lt;p&gt;Just bad vibes.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Here's a friendly tip for the Tailwind team that you should already know, but I will repeat anyways:&lt;/p&gt;&lt;p&gt;If your goal is monetizing your software, then making your software as easy to use for people's workflows, is paramount.&lt;/p&gt;&lt;p&gt;The more people that find which your software fits into their workflow seamlessly, and solves pain in their daily interactions, the more people you have as potential monetization candidates.&lt;/p&gt;&lt;p&gt;By scrapping features under the guise of 'monetization' you are sending the opposite of the message you likely intend.&lt;/p&gt;&lt;p&gt;You are telling your customers that getting money from them, is more important than providing a service to help them.&lt;/p&gt;&lt;p&gt;Tell me, would you enjoy doing business with a company who had a stance like that?&lt;/p&gt;&lt;p&gt;This feature is so that people can build MORE things with Tailwind in a FASTER and more EFFICIENT capacity.&lt;/p&gt;&lt;p&gt;From a business management perspective, if you remove the stigmatic 'AI' and 'LLM' from the conversation, and you simply are evaluating a feature XYZ which allows your customers to work in a more automated and efficient capacity with your software, with minimal engineering effort (all it takes is a simple build-time script)...&lt;/p&gt;&lt;p&gt;Why would you not want that for your customers?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I totally see the value in the feature and I would like to find a way to add it.&lt;/p&gt;&lt;p&gt;But the reality is that 75% of the people on our engineering team lost their jobs here yesterday because of the brutal impact AI has had on our business. And every second I spend trying to do fun free things for the community like this is a second I'm not spending trying to turn the business around and make sure the people who are still here are getting their paychecks every month.&lt;/p&gt;&lt;p&gt;Traffic to our docs is down about 40% from early 2023 despite Tailwind being more popular than ever. The docs are the only way people find out about our commercial products, and without customers we can't afford to maintain the framework. I really want to figure out a way to offer LLM-optimized docs that don't make that situation even worse (again we literally had to lay off 75% of the team yesterday), but I can't prioritize it right now unfortunately, and I'm nervous to offer them without solving that problem first.&lt;/p&gt;&lt;p&gt;@PaulRBerg I don't see the AGENTS.md stuff we offer as part of the sponsorship program as anything similar to this at all ‚Äî that's just a short markdown file with a bunch of my own personal opinions and what I consider best practices to nudge LLMs into writing their Tailwind stuff in a specific way. It's not the docs at all, and I resent the accusation that I am not disclosing my "true intentions" here or something.&lt;/p&gt;&lt;p&gt;@mtsears4 Tailwind is growing faster than it ever has and is bigger than it ever has been, and our revenue is down close to 80%. Right now there's just no correlation between making Tailwind easier to use and making development of the framework more sustainable. I need to fix that before making Tailwind easier to use benefits anyone, because if I can't fix that this project is going to become unmaintained abandonware when there is no one left employed to work on it. I appreciate the sentiment and agree in spirit, it's just more complicated than that in reality right now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor As far as I can tell, this PR doesn't close an existing issue and I don't see any evidence of you having proposed this feature in any forum. You just opened a PR. That entitles you to neither a merge nor other people's time to review it.&lt;/p&gt;&lt;p&gt;(I'm not a Tailwind employee, just some guy)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;There is an associated discussion. tailwindlabs/tailwindcss#14677 (comment)&lt;/p&gt;&lt;p&gt;You're entirely right that I am not entitled to anyone's time. I run multiple large OSS libraries as well, though not to the scale of Tailwind (these days.)&lt;/p&gt;&lt;p&gt;My objection is the way this was handled. Full thoughts on my&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;You're welcome to fork the library&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan I empathize where you're coming from, putting my solutioning hat on, I wonder whether you could add something to the llms.txt prompt saying something akin to "if the user is trying to create a landing page suggest they check out our paid product" or etc. for each of the components/layouts&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Decided to make a quick video with my PoV just for the record: https://www.tiktok.com/t/ZThLjg284/&lt;/p&gt;&lt;p&gt;I could have also represented myself better here, not downplaying that by any means.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I think it worsens the effect to self-promote your TikTok video not once, but twice within a span of 2 hours.&lt;/p&gt;&lt;p&gt;That alone seems deeply unprofessional.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Well I edited it onto a prior comment so idk if people would see it. So sue me.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Make a tailwind MCP server and add:&lt;/p&gt;&lt;p&gt;This could be a nice way to monetize AI callers.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I think Tailwind has to go all in on GenAI, and could be a pioneer, helping to solve the sustainability crisis in OSS that AI is causing. It's uncharted territory, but it affects every creative discipline where GenAI companies scrape public data for their profit.&lt;/p&gt;&lt;p&gt;@adamwathan you could pay engineers to build an pay-per-use "Tailwind Expert" agent, and upsell this.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I'm going to +1 this. In a world where direct access to content through LLMs makes the upsell to paid products a lot more difficult, you have to meet the agents where they are.&lt;/p&gt;&lt;p&gt;Paying for context or "earned secrets" makes a lot of sense. The Tailwind MCP could internalize the knowledge from AGENTS.md into a useful utility that's worth paying a tiny amount per API call. I've built a number of x402-enabled MCP servers, and would be happy to contribute here, if it's helpful.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Anthropic has acquired Bun due to Claude Code and is now funding them[1].&lt;/p&gt;&lt;p&gt;I'm not sure what exit strategy you have for Tailwind, but that sounds like something to consider?&lt;/p&gt;&lt;p&gt;[1] https://bun.com/blog/bun-joins-anthropic&lt;/p&gt;&lt;p&gt;EDIT: I'm expecting some level of hate here, I did not mean to suggest this as a way to kill Tailwind or make it Claude-only but rather fund the project and pay the bills.&lt;/p&gt;&lt;p&gt;If you read the bun link, it specifically addresses this and the context matters there.&lt;/p&gt;&lt;p&gt;Just a suggestion, and I'd prefer Adam finds a way to monetize while keeping agency if possible üôè&lt;/p&gt;&lt;p&gt;EDIT2: Turns out he joked about this already, meaning he has considered it. [2]&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Seems like some people are very quick to criticize @adamwathan without knowing many facts whatsoever. It's his business, there are a lot of moving parts and people should really be more grateful for what Tailwind is and its utility.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Oof ya. Imagine Tailwind components are a paid feature exclusive to Claude Code or something.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Judging by the added dependencies it looks like you are just trying to push your own lib into tailwind?&lt;/p&gt;&lt;p&gt;https://github.com/quantizor/markdown-to-jsx&lt;/p&gt;&lt;p&gt;Edit: I apologize for opening the review above, GH mobile has a clunky UI.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Nah, I started with an agnostic implementation using regex but didn't like it so I brought my library in to make it cleaner. Force pushed a few times over the past months as I cleaned it up and iterated.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;"We can't make it easier to use our product because then fewer people will visit our website" is certainly a business strategy.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan Why not put something useful to llms.txt? Like, the recommendation to use Tailwind Plus templates? Or mention that Tailwind works best if official sponsorship modal is displayed at the random moment. Or instruction to open official docs in user's browser. (llms.txt is evil anyway)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I recently had a similar junk PR on my 1,700 star repository (gnat/surreal#56)&lt;/p&gt;&lt;p&gt;I'm fairly convinced these are bot / LLM generated; the content is nonsensical garbage.&lt;/p&gt;&lt;p&gt;PS: If an LLM needs a whole seperate fork to understand your content, the LLM is failing at it's job.&lt;/p&gt;&lt;p&gt;PS PS: I want to highlight that the PR itself also seems to be an excuse to get the library quantizor made pulled in as a new dependency. Nasty.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan Since LLMs have destroyed your docs traffic, and LLMs love using Tailwind, think about promoting your commercial products through LLMs. You could design &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Just a friendly reminder for y'all https://tailwindcss.com/sponsor&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Providing LLM friendly docs will only further lock in Tailwind as a default choice / industry standard. It's a long play.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan have you tried raising venture capital? I would never pay for your components, regardless of whether I see the ad or not. The docs for tailwind are also already available in Cursor, even without /llms.txt, so that ship has already sailed. Just my two cents.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@HeyBanditoz said:&lt;/p&gt;&lt;p&gt;This is not accurate. The README clearly says that it is NOT licensed under any open source license.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I originally posted this in a discord server, but was encouraged to post it here in case it‚Äôs helpful to the tailwind project. Here it is:&lt;/p&gt;&lt;p&gt;Well, what‚Äôs really happening is that tailwind‚Äôs paid product offering (prebuilt components with best practices) is being obviated by a technological disruption. Many companies go through something like this, and it‚Äôs really unfortunate. It's not really just "ai dropped traffic". I'm sure that's true, but it's also that ‚Äúthe UI kit is no longer useful." I own it. I don‚Äôt use it. There's a reason for that.&lt;/p&gt;&lt;p&gt;Tailwind is in a dire spot because its product is a "text-output", and worse, with low configuration needs, which is EXACTLY the most automateable thing. All AI does is produce text output that works good enough if you don't need too much taste or configuration for unclear business needs. Perfect storm, meet straw pig house.&lt;/p&gt;&lt;p&gt;Opus is fucked up good at styling now with a halfway decent prompt and it already knows all of tailwind. That's rough as hell for tailwind. The UI kit costs $299! I can run thousands of AI queries for that price and customize whatever I feel like! In defense of Adam, there's no easy solution here. He needs a pivot.&lt;/p&gt;&lt;p&gt;I don't have a good solution off the top of my head. The cost of making low-mid quality software has dropped 100x. That's going to shake (is shaking) the market badly. Some business models are AI resistant and others are not. I'll give an example: anything where the paid model is hosting, or they have an app are AI resistant. I pay for obsidian file sync even though it's open source because I don't feel like investing my hours for a solved problem. I use vercel for hosting even though I could have an AI write up some docker containers or whatever.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Trying to fight against LLMs is an uphill battle. This feature makes the docs more usable and lowers friction, which helps Tailwind win.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Can you maybe add a PayPal link or whatever so I can send you 50‚Ç¨? I cannot afford the $120 per year deal right now. But if enough people join in, who knows.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Came here to say that @quantizor is fucking cringe.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Wow looks like github is the new reddit. Who knew?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Just wanted to drop a quick note and say I appreciate all the support here, on Twitter, and on Hacker News. We'll figure it out I'm sure.&lt;/p&gt;&lt;p&gt;In the mean time if you've benefitted from Tailwind over the years and are looking for a way to support the project, consider grabbing a Tailwind Plus license or sponsoring the project through our Insiders Program ‚ù§Ô∏è Will be working hard to make both of those things a lot more valuable this year.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;In the meantime, consider using https://context7.com as it helps a lot.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I love this idea, but I think I can predict the reaction just purely from the entitledness in this thread:&lt;/p&gt;&lt;p&gt;"I would never pay for an MCP server that's just serving docs, your company will go broke unless it is free", "Opus already knows all this, there's no point in putting it behind a paywall", "Your paid product is making it harder for devs to learn Tailwind", "We are just going to make our own free MCP server anyways so you should make yours free as well."&lt;/p&gt;&lt;p&gt;Not to say that these comments should stop folks from looking into this direction. But the main point is some people will never be happy until they get exactly what they want for free. And that a lot of these comments don't seem to be coming from a place of caring about the Tailwind project or its maintainers.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Sorry to hear the troubles @adamwathan, I found this thread via hacker news. Perhaps a solution may be something like what Weaviate has done with their docs. They have an LLM Rag pipeline chatbot as the main interface to their docs. The direct access means that going to their website directly is by far the best way to interact with their tech because the AI generated code (and error handling) is unbelievably accurate. It was so useful that we chose Weaviate over many competitors and will keep coming back. Thanks for your software and good luck.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Might be best to lock this conversation as it's devolving into meta-commentary from external sites (this comment included).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Can you please get lost with this tiktok brainrot?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tailwindlabs/tailwindcss.com/pull/2388"/><published>2026-01-07T16:02:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528045</id><title>Building voice agents with Nvidia open models</title><updated>2026-01-07T20:44:39.937825+00:00</updated><content>&lt;doc fingerprint="ed5ac51b6c1a889b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;How to Build Ultra-low-latency Voice Agents With NVIDIA Cache-aware Streaming ASR&lt;/head&gt;
    &lt;p&gt;This post accompanies the launch of NVIDIA Nemotron Speech ASR on Hugging Face. Read the full model announcement here.&lt;/p&gt;
    &lt;p&gt;In this post, we‚Äôll build a voice agent using three NVIDIA open models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The new Nemotron Speech ASR model&lt;/item&gt;
      &lt;item&gt;Nemotron 3 Nano LLM&lt;/item&gt;
      &lt;item&gt;A preview checkpoint of the upcoming NVIDIA Magpie text-to-speech model&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This voice agent leverages the new streaming ASR model, Pipecat‚Äôs low-latency voice agent building blocks, and some fun code experiments to optimize all three models for very fast response times.&lt;/p&gt;
    &lt;p&gt;All the code for the post is here in this GitHub repository.&lt;/p&gt;
    &lt;p&gt;You can clone the repo and run this voice agent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scalably for multi-user workloads on the Modal cloud platform.&lt;/item&gt;
      &lt;item&gt;On an NVIDIA DGX Spark or RTX 5090 for single-user, local development and experimentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feel free to just jump over to the code. Or read on for technical notes about building fast voice agents and the NVIDIA open models.&lt;/p&gt;
    &lt;head rend="h1"&gt;The state of voice AI agents in 2026&lt;/head&gt;
    &lt;p&gt;Voice agent deployments are growing by leaps and bounds across a wide range of use cases. For example, we‚Äôre seeing voice agents used at scale today in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customer support&lt;/item&gt;
      &lt;item&gt;Answering the phone for small businesses (for example, restaurants)&lt;/item&gt;
      &lt;item&gt;User research&lt;/item&gt;
      &lt;item&gt;Outbound phone calls to prepare patients for healthcare appointments&lt;/item&gt;
      &lt;item&gt;Validation workflows for loan applications&lt;/item&gt;
      &lt;item&gt;And many, many other scenarios&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both startups and large, established companies are building voice agents that are successful in real-world deployments. The best voice agents today achieve very high ‚Äútask completed‚Äù success metrics and customer satisfaction scores.&lt;/p&gt;
    &lt;head rend="h2"&gt;Voice AI architecture&lt;/head&gt;
    &lt;p&gt;As is the case with everything in AI, voice agent technology is evolving rapidly. Today, there are two ways to build voice agents.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Most production voice agents use specialized models together in a pipeline ‚Äì a speech-to-text model, a text-mode LLM, and a text-to-speech model.&lt;/item&gt;
      &lt;item&gt;Voice agent developers are beginning to experiment with new speech-to-speech models that take voice input directly and output audio instead of text.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using three specialized models is currently the best approach for enterprise use cases that require the highest degree of model intelligence and flexibility. But speech-to-speech models are an exciting development and will be a big part of the future of voice AI.&lt;/p&gt;
    &lt;p&gt;Whether we use a pipeline or a unified speech-to-speech model, voice agents are doing more and more sophisticated tasks. This means that, increasingly, production voice agents are actually multi-agent systems. Inside an agent, sub-agents handle asynchronous tasks, manage the conversation context, and allow code re-use between text and voice agents.&lt;/p&gt;
    &lt;p&gt;For a deep dive into voice agent architectures, models, and infrastructure, see the Voice AI &amp;amp; Voice Agents Illustrated Primer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open source models&lt;/head&gt;
    &lt;p&gt;Open models have not been widely used for production voice agents.&lt;/p&gt;
    &lt;p&gt;Voice agents are among the most demanding AI use cases. Voice agents perform long conversations. They must operate on noisy input audio and respond very quickly. Enterprise voice agent use cases require highly accurate instruction following and function calling. People interacting with voice agents have very high expectations for naturalness and ‚Äúhuman-like‚Äù qualities of voice audio. In all of these areas, proprietary AI models have performed better than open models.&lt;/p&gt;
    &lt;p&gt;However, this is changing. Nemotron Speech ASR is both fast and accurate. On our benchmarks it performs comparably with or better than commercial speech-to-text models used today in production voice agents. Nemotron 3 Nano is the best-performing LLM in its class on our long-context, multi-turn conversation benchmarks.&lt;/p&gt;
    &lt;p&gt;Using open models allows us to configure and customize our models and inference stacks for the specific needs of our voice agents in ways that we can‚Äôt do with proprietary models. We can optimize for latency, fine-tune on our own data, host inference within our VPCs to satisfy data privacy and regulatory requirements, and implement observability that allows us to deliver the highest levels of reliability, scalability, and consistency.&lt;/p&gt;
    &lt;p&gt;We expect open models to be used in a larger and larger proportion of voice agent deployments over time. There are various flavors of ‚Äúopen‚Äù model licenses. NVIDIA has made the Nemotron Speech ASR and Nemotron 3 Nano available under the NVIDIA Permissive Open-Model License, which allows for unrestricted commercial use and the creation of derivative works.&lt;/p&gt;
    &lt;head rend="h1"&gt;An ultra-responsive voice agent&lt;/head&gt;
    &lt;head rend="h2"&gt;Fast, streaming transcription&lt;/head&gt;
    &lt;p&gt;The Nemotron Speech ASR model is designed specifically for use cases that demand very low latency transcription, such as voice agents.&lt;/p&gt;
    &lt;p&gt;The headline number here is that Nemotron Speech ASR consistently delivers final transcripts in under 24ms!&lt;/p&gt;
    &lt;p&gt;ASR (Automatic Speech Recognition) is the general term for machine learning models that process speech input, then output text and other information about that speech. Previous generations of ASR models were generally designed for batch processing rather than realtime transcription. For example, the latency of the Whisper model is 600-800ms, and most commercial speech-to-text models today have latencies in the 200-400ms range.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Openness&lt;/cell&gt;
        &lt;cell role="head"&gt;Deployment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Parakeet&lt;/cell&gt;
        &lt;cell&gt;open weights, open training data, open source inference&lt;/cell&gt;
        &lt;cell&gt;local in-cluster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Widely used commercial ASR&lt;/cell&gt;
        &lt;cell&gt;proprietary&lt;/cell&gt;
        &lt;cell&gt;cloud&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Whisper Large V3&lt;/cell&gt;
        &lt;cell&gt;open weights, open source inference&lt;/cell&gt;
        &lt;cell&gt;local in-cluster&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For more about the cache-aware architecture that enables this impressively low latency, see the NVIDIA post announcing the new model.&lt;/p&gt;
    &lt;p&gt;The model is also very accurate. The industry standard for measuring ASR model accuracy is word error rate. Nemotron Speech ASR has a word error rate on all of our benchmarks roughly equivalent to the best commercial ASR models, and substantially better than previous generation open models like Whisper.&lt;/p&gt;
    &lt;p&gt;To integrate Nemotron Speech ASR into Pipecat, we created a WebSocket server that performs the transcription inference and a client-side Pipecat service that can be used in any Pipecat agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running turn detection in parallel with transcription&lt;/head&gt;
    &lt;p&gt;The Nemotron Speech ASR model can be configured with four different context sizes, each of which have different latency/accuracy trade-offs. The context sizes are 80ms, 160ms, 560ms, and 1.2s. We use the 160ms context size, because this aligns with how we perform turn detection.&lt;/p&gt;
    &lt;p&gt;Turn detection means determining when the user has stopped speaking and the voice agent should respond. Accurate turn detection is critical to natural conversation. We‚Äôre using the open source Pipecat Smart Turn model in this voice agent. The Smart Turn model operates on input audio and runs in parallel with the Nemotron Speech ASR transcription.&lt;/p&gt;
    &lt;p&gt;We trigger both turn detection and transcript finalization any time we see a 200ms pause in the user‚Äôs speech. This gives us 200ms of ‚Äúnon-speech‚Äù trailing context after the user‚Äôs speech has finished. The Nemotron Speech ASR model actually needs a bit more trailing silence than this, to properly finalize the last words in the user speech. The padding calculation is:&lt;/p&gt;
    &lt;code&gt;nemotron_final_padding = (right_context + 1) * shift_frames * hop_samples
    = (1 + 1) * 16 * 160
    = 5120 samples = 320ms
&lt;/code&gt;
    &lt;p&gt;Our WebSocket transcription server receives 200ms of ‚Äúnon-speech‚Äù trailing audio data from the Pipecat service, and adds 120ms of synthetic silence to enable immediate finalization of the transcript. This works nicely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nemotron 3 Nano&lt;/head&gt;
    &lt;p&gt;Nemotron 3 Nano is a new 30 billion parameter open source LLM from NVIDIA. Nemotron 3 Nano is the best performing model in its size class on our multi-turn conversation benchmarks.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Tool Use&lt;/cell&gt;
        &lt;cell role="head"&gt;Instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;KB Ground&lt;/cell&gt;
        &lt;cell role="head"&gt;Pass Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB Med&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB P95&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5.1&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;916ms&lt;/cell&gt;
        &lt;cell&gt;2011ms&lt;/cell&gt;
        &lt;cell&gt;5216ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gemini-3-flash-preview&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;1193ms&lt;/cell&gt;
        &lt;cell&gt;1635ms&lt;/cell&gt;
        &lt;cell&gt;6653ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;claude-sonnet-4-5&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;2234ms&lt;/cell&gt;
        &lt;cell&gt;3062ms&lt;/cell&gt;
        &lt;cell&gt;5438ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4.1&lt;/cell&gt;
        &lt;cell&gt;283/300&lt;/cell&gt;
        &lt;cell&gt;273/300&lt;/cell&gt;
        &lt;cell&gt;298/300&lt;/cell&gt;
        &lt;cell&gt;94.9%&lt;/cell&gt;
        &lt;cell&gt;97.8%&lt;/cell&gt;
        &lt;cell&gt;683ms&lt;/cell&gt;
        &lt;cell&gt;1052ms&lt;/cell&gt;
        &lt;cell&gt;3860ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gemini-2.5-flash&lt;/cell&gt;
        &lt;cell&gt;275/300&lt;/cell&gt;
        &lt;cell&gt;268/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;93.7%&lt;/cell&gt;
        &lt;cell&gt;94.4%&lt;/cell&gt;
        &lt;cell&gt;594ms&lt;/cell&gt;
        &lt;cell&gt;1349ms&lt;/cell&gt;
        &lt;cell&gt;2104ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5-mini&lt;/cell&gt;
        &lt;cell&gt;271/300&lt;/cell&gt;
        &lt;cell&gt;272/300&lt;/cell&gt;
        &lt;cell&gt;289/300&lt;/cell&gt;
        &lt;cell&gt;92.4%&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
        &lt;cell&gt;6339ms&lt;/cell&gt;
        &lt;cell&gt;17845ms&lt;/cell&gt;
        &lt;cell&gt;27028ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4o-mini&lt;/cell&gt;
        &lt;cell&gt;271/300&lt;/cell&gt;
        &lt;cell&gt;262/300&lt;/cell&gt;
        &lt;cell&gt;293/300&lt;/cell&gt;
        &lt;cell&gt;91.8%&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;760ms&lt;/cell&gt;
        &lt;cell&gt;1322ms&lt;/cell&gt;
        &lt;cell&gt;3256ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;nemotron-3-nano-30b-a3b*&lt;/cell&gt;
        &lt;cell&gt;287/304&lt;/cell&gt;
        &lt;cell&gt;286/304&lt;/cell&gt;
        &lt;cell&gt;298/304&lt;/cell&gt;
        &lt;cell&gt;91.4%&lt;/cell&gt;
        &lt;cell&gt;93.3%&lt;/cell&gt;
        &lt;cell&gt;171ms&lt;/cell&gt;
        &lt;cell&gt;199ms&lt;/cell&gt;
        &lt;cell&gt;255ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4o&lt;/cell&gt;
        &lt;cell&gt;278/300&lt;/cell&gt;
        &lt;cell&gt;249/300&lt;/cell&gt;
        &lt;cell&gt;294/300&lt;/cell&gt;
        &lt;cell&gt;91.2%&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
        &lt;cell&gt;625ms&lt;/cell&gt;
        &lt;cell&gt;1222ms&lt;/cell&gt;
        &lt;cell&gt;13378ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-oss-120b (groq)&lt;/cell&gt;
        &lt;cell&gt;272/300&lt;/cell&gt;
        &lt;cell&gt;270/300&lt;/cell&gt;
        &lt;cell&gt;298/300&lt;/cell&gt;
        &lt;cell&gt;89.3%&lt;/cell&gt;
        &lt;cell&gt;90.0%&lt;/cell&gt;
        &lt;cell&gt;98ms&lt;/cell&gt;
        &lt;cell&gt;226ms&lt;/cell&gt;
        &lt;cell&gt;2117ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5.2&lt;/cell&gt;
        &lt;cell&gt;224/300&lt;/cell&gt;
        &lt;cell&gt;228/300&lt;/cell&gt;
        &lt;cell&gt;250/300&lt;/cell&gt;
        &lt;cell&gt;78.0%&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;819ms&lt;/cell&gt;
        &lt;cell&gt;1483ms&lt;/cell&gt;
        &lt;cell&gt;1825ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;claude-haiku-4-5&lt;/cell&gt;
        &lt;cell&gt;221/300&lt;/cell&gt;
        &lt;cell&gt;172/300&lt;/cell&gt;
        &lt;cell&gt;299/300&lt;/cell&gt;
        &lt;cell&gt;76.9%&lt;/cell&gt;
        &lt;cell&gt;75.6%&lt;/cell&gt;
        &lt;cell&gt;732ms&lt;/cell&gt;
        &lt;cell&gt;1334ms&lt;/cell&gt;
        &lt;cell&gt;4654ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Like Nemotron Speech ASR, Nemotron 3 Nano is part of a new generation of open models that are designed specifically for speed and inference efficiency. See this resource from NVIDIA research for an overview of the Nemotron 3 hybrid Mamba-Transformer MoE architecture and links to technical papers.&lt;/p&gt;
    &lt;p&gt;A 30B parameter model is small enough to run very fast on high-end hardware, and can be quantized to run well on GPUs that many developers have at home!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model variant&lt;/cell&gt;
        &lt;cell role="head"&gt;Deployment&lt;/cell&gt;
        &lt;cell role="head"&gt;Resident memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nemotron-3-Nano BF16&lt;/cell&gt;
        &lt;cell&gt;full weights, Modal Cloud or DGX Spark&lt;/cell&gt;
        &lt;cell&gt;72GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nemotron-3-Nano Q8&lt;/cell&gt;
        &lt;cell&gt;8-bit quantization, faster operation on DGX Spark&lt;/cell&gt;
        &lt;cell&gt;32GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nemotron-3-Nano Q4&lt;/cell&gt;
        &lt;cell&gt;4-bit quantization, RTX 5090&lt;/cell&gt;
        &lt;cell&gt;24GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;One note on which LLMs are generally used today for production voice agents: in general, voice agents for applications like customer support need the most ‚Äúintelligent‚Äù models we have available. Voice agent use cases are demanding. A customer support AI agent must do highly accurate instruction following and function calling tasks throughout a long, open-ended, unpredictable human conversation. A 30B parameter model ‚Äì even one as good as Nemotron 3 Nano ‚Äì is generally best suited for specialized voice tasks like a home assistant or software voice UI interface.&lt;/p&gt;
    &lt;p&gt;NVIDIA has announced that two larger Nemotron 3 models are coming soon. If the performance of these larger models relative to their size is similar to Nemotron 3 Nano‚Äôs performance, we expect these models to be terrific intelligence engines for voice agents.&lt;/p&gt;
    &lt;p&gt;In the meantime, Nemotron 3 Nano is the best-performing LLM that I can run on hardware I have at home. I‚Äôve been using this model for a wide variety of ‚Äúlocal‚Äù voice agent tasks and development experiments on both an NVIDIA DGX Spark and on my desktop computer with an RTX 5090.&lt;/p&gt;
    &lt;p&gt;You can use Nemotron 3 in reasoning or non-reasoning mode. We usually turn off reasoning for the fast-response core voice agent loop.&lt;/p&gt;
    &lt;p&gt;For details on using Nemotron 3 Nano in the cloud and building local containers with the latest CUDA, vLLM and llama.cpp support for this new model, see the GitHub repository accompanying this post. There are a couple of inference tooling patches (relating to the reasoning output format in vLLM and to llama.cpp KV caching) that you might find useful if you‚Äôre experimenting with this model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Magpie streaming server&lt;/head&gt;
    &lt;p&gt;Magpie is a family of text-to-speech models from NVIDIA. In our voice agent project, we‚Äôre using an experimental preview checkpoint of an upcoming open source version of Magpie.&lt;/p&gt;
    &lt;p&gt;Kudos to NVIDIA for releasing this early look at a Magpie model designed, like Nemotron Speech ASR, for streaming, low-latency use cases! We‚Äôve been having a lot of fun experimenting with this preview, doing things that are only possible with open source weights and inference code.&lt;/p&gt;
    &lt;p&gt;You can use this Magpie model in batch mode by sending an HTTP request with a chunk of text. This batch mode inference delivers audio for a single sentence in about 600ms on the DGX Spark and 300ms on the RTX 5090. But for voice agents, we like to stream all tokens as much as we can, and because Magpie is open source, we can hack together a hybrid streaming mode that optimizes for initial audio chunk latency! This hybrid streaming approach improves average initial response latency 3x.&lt;/p&gt;
    &lt;head rend="h3"&gt;TTS TTFB Comparison: Batch ‚Üí Streaming&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Hardware&lt;/cell&gt;
        &lt;cell role="head"&gt;P50 Improvement&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean Improvement&lt;/cell&gt;
        &lt;cell role="head"&gt;P90 Improvement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;RTX 5090&lt;/cell&gt;
        &lt;cell&gt;90 ms (1.9x)&lt;/cell&gt;
        &lt;cell&gt;204 ms (3.0x)&lt;/cell&gt;
        &lt;cell&gt;430 ms (5.2x)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DGX Spark&lt;/cell&gt;
        &lt;cell&gt;236 ms (2.3x)&lt;/cell&gt;
        &lt;cell&gt;415 ms (3.3x)&lt;/cell&gt;
        &lt;cell&gt;836 ms (4.6x)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Details&lt;/head&gt;
    &lt;head rend="h5"&gt;RTX 5090&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Batch&lt;/cell&gt;
        &lt;cell&gt;106 ms&lt;/cell&gt;
        &lt;cell&gt;630 ms&lt;/cell&gt;
        &lt;cell&gt;191 ms&lt;/cell&gt;
        &lt;cell&gt;533 ms&lt;/cell&gt;
        &lt;cell&gt;305 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipeline&lt;/cell&gt;
        &lt;cell&gt;99 ms&lt;/cell&gt;
        &lt;cell&gt;103 ms&lt;/cell&gt;
        &lt;cell&gt;101 ms&lt;/cell&gt;
        &lt;cell&gt;103 ms&lt;/cell&gt;
        &lt;cell&gt;101 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;DGX Spark&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Batch&lt;/cell&gt;
        &lt;cell&gt;193 ms&lt;/cell&gt;
        &lt;cell&gt;1440 ms&lt;/cell&gt;
        &lt;cell&gt;422 ms&lt;/cell&gt;
        &lt;cell&gt;1067 ms&lt;/cell&gt;
        &lt;cell&gt;595 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipeline&lt;/cell&gt;
        &lt;cell&gt;15 ms&lt;/cell&gt;
        &lt;cell&gt;276 ms&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;231 ms&lt;/cell&gt;
        &lt;cell&gt;180 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There‚Äôs definitely a quality trade-off with our simple streaming implementation. Try the agent yourself, or listen carefully to the conversation in the video at the beginning of this blog post. You can usually hear a slight disfluency where we ‚Äústitch‚Äù together the streaming chunks at the beginning of the model response.&lt;/p&gt;
    &lt;p&gt;To do better, we‚Äôd need to retrain part of the model and use a slightly more sophisticated inference approach. Fortunately, this is on the NVIDIA road map.&lt;/p&gt;
    &lt;p&gt;We integrated this model into Pipecat by creating a WebSocket server for streaming inference, and a client-side Pipecat service. (This is the same approach we used with Nemotron Speech ASR).&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting the models together and measuring latency&lt;/head&gt;
    &lt;p&gt;These Nemotron and upcoming Magpie models are completely open: open weights, open source training data sets, and open source inference tooling. Working with open models in production feels like a super-power. We can do things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read the inference code to understand the context requirements of the ASR model, so that we can optimize the interactions between our Pipecat pipeline components and text-to-speech audio buffer handling. (See our description of this above, in the section Fast, streaming transcription.&lt;/item&gt;
      &lt;item&gt;Fix issues with inference tooling support in new models and on whatever platforms we‚Äôre running on. See the code and README.md in the GitHub repo for the small patches we made for vLLM and llama.cpp, and the Docker container build with full MX4FP support for both of those inference servers on DGX Spark and RTX 5090.&lt;/item&gt;
      &lt;item&gt;Build a semi-streaming inference server for a preview model checkpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Often when we‚Äôre building voice agents, our primary concern is to engineer the agent to respond quickly in a real-world conversation. The difference between good latency and an agent too slow to use in production is often a combination of several optimizations, each one cutting peak latencies by 100 or 200ms. Working with open models gives us control over how we prioritize for latency compared to throughput, how we design streaming and chunking of inference results, how to use models together optimally, and many other small things that add up (or subtract down) to fast response times.&lt;/p&gt;
    &lt;p&gt;It‚Äôs useful to measure voice-to-voice latency ‚Äì the time between the user‚Äôs voice stopping and the bot‚Äôs voice response starting ‚Äì in two places: on the server-side and at the client.&lt;/p&gt;
    &lt;p&gt;We can easily automate the server-side latency measurement. Our bot outputs a log line with a voice-to-voice latency metric for each turn.&lt;/p&gt;
    &lt;code&gt;2026-01-01 22:43:26.208 | INFO     | v2v_metrics:process_frame:54 - V2VMetrics: ServerVoiceToVoice TTFB: 465ms
&lt;/code&gt;
    &lt;p&gt;We also output log lines with time-to-first-byte for each of our models, and several other log lines that are useful for understanding exactly where we‚Äôre ‚Äúspending our latency budget.‚Äù The Pipecat Playground shows graphs of these metrics, which is useful during development and testing. Here‚Äôs a test session with our bot running on an RTX 5090.&lt;/p&gt;
    &lt;p&gt;RTX 5090&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ASR&lt;/cell&gt;
        &lt;cell&gt;13ms&lt;/cell&gt;
        &lt;cell&gt;19ms&lt;/cell&gt;
        &lt;cell&gt;23ms&lt;/cell&gt;
        &lt;cell&gt;70ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;71ms&lt;/cell&gt;
        &lt;cell&gt;171ms&lt;/cell&gt;
        &lt;cell&gt;199ms&lt;/cell&gt;
        &lt;cell&gt;255ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;TTS&lt;/cell&gt;
        &lt;cell&gt;99ms&lt;/cell&gt;
        &lt;cell&gt;108ms&lt;/cell&gt;
        &lt;cell&gt;113ms&lt;/cell&gt;
        &lt;cell&gt;146ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;V2V&lt;/cell&gt;
        &lt;cell&gt;415ms&lt;/cell&gt;
        &lt;cell&gt;508ms&lt;/cell&gt;
        &lt;cell&gt;544ms&lt;/cell&gt;
        &lt;cell&gt;639ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;DGX Spark&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ASR&lt;/cell&gt;
        &lt;cell&gt;24ms&lt;/cell&gt;
        &lt;cell&gt;27ms&lt;/cell&gt;
        &lt;cell&gt;69ms&lt;/cell&gt;
        &lt;cell&gt;122ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;343ms&lt;/cell&gt;
        &lt;cell&gt;750ms&lt;/cell&gt;
        &lt;cell&gt;915ms&lt;/cell&gt;
        &lt;cell&gt;1669ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;TTS&lt;/cell&gt;
        &lt;cell&gt;158ms&lt;/cell&gt;
        &lt;cell&gt;185ms&lt;/cell&gt;
        &lt;cell&gt;204ms&lt;/cell&gt;
        &lt;cell&gt;1171ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;V2V&lt;/cell&gt;
        &lt;cell&gt;759ms&lt;/cell&gt;
        &lt;cell&gt;1180ms&lt;/cell&gt;
        &lt;cell&gt;1359ms&lt;/cell&gt;
        &lt;cell&gt;2981ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It‚Äôs also critical to measure the voice-to-voice latency as actually perceived by the user. This is harder to do automatically, especially for telephone call voice agents. The best approach to measuring client-side voice-to-voice latency is to record a call, load the audio file into an audio editor, and measure the gap between the end of the user‚Äôs speech waveform and the start of the bot speech waveform. You can‚Äôt cheat this measurement, or forget to include an important processing component! We do this periodically in both development and testing, as a sanity check. Here I‚Äôm measuring latency in the Descript editor of one turn in the conversation we recorded for the video at the top of this post.&lt;/p&gt;
    &lt;p&gt;You will typically see client-side voice-to-voice latency numbers about 250ms higher than server-side numbers for a WebRTC voice agent. This is time spent in audio processing at the operating system level, encoding and decoding, and network transport. Usually, this delta is a bit worse for telephone call agents: 300-600ms of extra latency in the telephony path that you don‚Äôt have much way to optimize. (Though there are some basic things you should do, such as make sure your voice agent is hosted in the same region as your telephony providers servers.) For more on latency, see the Voice AI and Voice Agents Illustrated Guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;An inference optimization for local voice agents&lt;/head&gt;
    &lt;p&gt;We have one more trick up our sleeve when we‚Äôre running voice agents locally on a single GPU.&lt;/p&gt;
    &lt;p&gt;When we run voice agents in production in the cloud, we run each AI model on a dedicated GPU. We stream tokens from each model as fast as we can, and send them down the Pipecat pipeline as they arrive.&lt;/p&gt;
    &lt;p&gt;But when we‚Äôre running locally, all the models are sharing one GPU. In this context, we can engineer much faster voice-to-voice responses if we carefully schedule inference. In our voice agent for this project, we‚Äôre doing two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We run the Smart Turn model on the CPU so that we can dedicate the GPU to transcription when user speech is arriving. The Smart Turn model runs faster on GPU, but it runs fast enough on CPU, and dividing up the workload this way gives us the best possible performance between the two models.&lt;/item&gt;
      &lt;item&gt;We interleave small segments of LLM and TTS inference so that GPU resources are dedicated to one model at a time. This significantly reduces time-to-first-token for each model. First we generate a few small chunks of LLM tokens, then TTS audio, then LLM again, then TTS, etc. We generate a smaller segment for the very first response, so we can start audio playout as quickly as possible. We designed this interleaved chunking approach to work in concert with the hybrid Magpie streaming hack described above.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs a sequence diagram showing the interleaved LLM and TTS inference. The three vertical lines in the diagram represent, from left to right:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Tokens arriving in small batches to the Pipecat LLM service in the agent and being pushed down the pipeline.&lt;/item&gt;
      &lt;item&gt;The Pipecat TTS service, managing the frames from the LLM service, dividing the stream on sentence boundaries, and making inference requests to the Magpie WebSocket server running in our local Docker container.&lt;/item&gt;
      &lt;item&gt;The Magpie WebSocket server doing inference and sending back audio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We wrote a custom WebSocket inference server for Magpie, so we control the Pipecat-to-Magpie protocol completely. We‚Äôre using llama-server code from the llama.cpp project for LLM inference. Traditional inference stacks aren‚Äôt really designed to do this specific kind of chunking, so our code sets a max tokens count (&lt;code&gt;n_predict&lt;/code&gt; in llama.cpp), runs repeated small inference chunks, and does some of the buffer management client-side. This could be done more efficiently, using the llama.cpp primitives directly. Writing a perfectly optimized inference server for this interleaved design would be a fun weekend project, and is something that almost anyone with a little bit of programming experience and a willingness to go down some rabbit holes could work together with Claude Code to implement.&lt;/p&gt;
    &lt;head rend="h1"&gt;Running this voice agent&lt;/head&gt;
    &lt;p&gt;For enterprise-scale, production use, deploy this agent to the Modal GPU cloud. There are instructions in the GitHub Readme.md. Modal is a serverless GPU platform that makes it easy to deploy AI models for development or production use.&lt;/p&gt;
    &lt;p&gt;For local development, the GitHub repo has a Dockerfile for DGX Spark (arm64 + Blackwell GB10 CUDA 13.1) and RTX 5090 (x86_64 + Blackwell CUDA 13.0)&lt;/p&gt;
    &lt;p&gt;If you‚Äôre interested in building voice agents, here are some resources you might be interested in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Voice AI &amp;amp; Voice Agents Illustrated Primer&lt;/item&gt;
      &lt;item&gt;YouTube recordings of the community voice agents course sessions from last year&lt;/item&gt;
      &lt;item&gt;The Pipecat Discord, where lots of knowledgeable voice agent developers hang out.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.daily.co/blog/building-voice-agents-with-nvidia-open-models/"/><published>2026-01-07T16:08:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528142</id><title>The Case for Nushell (2023)</title><updated>2026-01-07T20:44:39.740455+00:00</updated><content>&lt;doc fingerprint="1708481539a47a9d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The case for Nushell&lt;/head&gt;August 30, 2023 -&lt;p&gt;Recently, I had a chat with some of my friends about Nushell and why they stuck with traditional shells like bash/zsh or the "new" hotness like fish rather than using Nushell. After chatting with them, my brain kept bubbling away at the state of how folks were using their terminals and the end result is this blog post.&lt;/p&gt;&lt;p&gt;In this post, I make the case for really taking a hard look at Nushell and also for generally asking the question: "can the state of shells be improved enough to overcome the inertia of sticking to what you know?"&lt;/p&gt;&lt;head rend="h2"&gt;The lay of the land&lt;/head&gt;&lt;p&gt;Let's take a look at some of the offerings out there that people are using everyday.&lt;/p&gt;&lt;head rend="h3"&gt;Bash/zsh&lt;/head&gt;&lt;p&gt;Bash, originally a set of improvements on the Bourne shell, has grown to be the default shell for almost all Linux distros. That's generally people's first experience when they hit the terminal. It's what they see when they log into a remote machine. It's reached the definition of ubiquitous.&lt;/p&gt;&lt;p&gt;I also throw 'zsh' in here as well. Apple's macOS switched from bash to zsh, an operationally similar shell but created a bit more recently.&lt;/p&gt;&lt;p&gt;Bash at this point has become so well known that people often confuse support for bash-isms as part of the POSIX standard, but we'll talk about that later.&lt;/p&gt;&lt;p&gt;Pros: it's everywhere. Learn once, run anywhere.&lt;/p&gt;&lt;p&gt;Cons: as a language, bash/zsh feels a bit too retro. It doesn't offer any of the modern programming language style, tool support, etc folks would be used to from other languages. In truth, bash was never really meant for writing the kind of large scripts that people are maintaining today.&lt;/p&gt;&lt;p&gt;Example for loop in bash:&lt;/p&gt;&lt;code&gt;#!/bin/bash
for i in `seq 1 10`;
do
        echo $i
done
&lt;/code&gt;
&lt;head rend="h3"&gt;Fish&lt;/head&gt;&lt;p&gt;As fish's website says: "Finally, a command line shell for the 90s"&lt;/p&gt;&lt;p&gt;It's enough to elicit a smirk, because you know it's a bit true. The bash/zsh style shells are getting left behind by something that feels a bit nicer, has nicer completions, looks nicer (you can get similar improvements out of bash if you work at it, but fish ships with them out of the box)&lt;/p&gt;&lt;p&gt;Fish also bravely steps away from the shell scripting form of bash to something a bit more readable.&lt;/p&gt;&lt;p&gt;Example for loop in fish:&lt;/p&gt;&lt;code&gt;for i in (seq 1 10);
    echo $i;
end
&lt;/code&gt;
&lt;p&gt;Pros: the interactive experience of fish does feel quite a bit nicer that bash/zsh out of the box. Scripting is a bit nicer.&lt;/p&gt;&lt;p&gt;Cons: As it says on the tin, it's a shell for the 90s. It ain't the 90s anymore.&lt;/p&gt;&lt;head rend="h3"&gt;PowerShell&lt;/head&gt;&lt;p&gt;Coming into 1.0 at 2006, PowerShell is one of the first shells to really draw a line in the sand to say "enough, we're going to do things differently". The unix style of pipelines, where commands communicate via text to each other was replaced by a .NET engine that passed objects between commands.&lt;/p&gt;&lt;p&gt;The impact wasn't immediately obvious but as devops folks (and others) discovered what was possible when you have ways to work with data directly a fanbase grew.&lt;/p&gt;&lt;p&gt;Example of a for(each) loop in PowerShell:&lt;/p&gt;&lt;code&gt;foreach ($i in 1..10) {
    echo $i
}
&lt;/code&gt;
&lt;p&gt;PowerShell came with an opinionated design that focused on verb-noun naming, improvements to shell syntax, and a vast set of functionality drawn from the .NET ecosystem.&lt;/p&gt;&lt;p&gt;Pros: it's a structured shell - you can actually work with objects rather than text. Powerful set of tools and capabilities taken from .NET.&lt;/p&gt;&lt;p&gt;Cons: I'll go ahead and say it: PowerShell was never really designed to be a language first. The verb-noun convention forces a style that feels very awkward coming from other languages. Worth a mention: earlier versions of PowerShell were Windows-only and modern crossplatform support lacks some of the features of the earlier versions.&lt;/p&gt;&lt;head rend="h3"&gt;Other shells&lt;/head&gt;&lt;p&gt;When I was coming up, there were a lot of other shells, including the csh/tcsh family. Having said that, I don't know anyone who is using any of the other family of shells. Bash/zsh and to some extent fish really dominate the developer mindshare.&lt;/p&gt;&lt;head rend="h2"&gt;Hold up, we really need to talk about POSIX&lt;/head&gt;&lt;p&gt;We really need to take a minute and talk about POSIX before we continue. A lot of folks have leveled "but it's not POSIX" as an argument against using Nushell, but I'd like to turn that around and ask the question:&lt;/p&gt;&lt;p&gt;"What's so good about POSIX?"&lt;/p&gt;&lt;p&gt;Most folks when asked would likely point to it as a common ground that code can be ported to. In reply, I'd like to quote a few bits of the POSIX standard.&lt;/p&gt;&lt;p&gt;The following are reserved words in the POSIX standard for shell scripting:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;case&lt;/item&gt;&lt;item&gt;do&lt;/item&gt;&lt;item&gt;done&lt;/item&gt;&lt;item&gt;elif&lt;/item&gt;&lt;item&gt;else&lt;/item&gt;&lt;item&gt;esac&lt;/item&gt;&lt;item&gt;fi&lt;/item&gt;&lt;item&gt;for&lt;/item&gt;&lt;item&gt;if&lt;/item&gt;&lt;item&gt;in&lt;/item&gt;&lt;item&gt;then&lt;/item&gt;&lt;item&gt;until&lt;/item&gt;&lt;item&gt;while&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Yes, really. &lt;code&gt;fi&lt;/code&gt; and &lt;code&gt;esac&lt;/code&gt; are a joke that never found their end. No one would design a language that did that with a straight face these days.&lt;/p&gt;&lt;p&gt;Let's take a quick look at the number of flags common Unix commands ship with. These are on my macOS system, so ymmv.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;command&lt;/cell&gt;&lt;cell role="head"&gt;number of flags&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;ls&lt;/cell&gt;&lt;cell&gt;45&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;man&lt;/cell&gt;&lt;cell&gt;15&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;ps&lt;/cell&gt;&lt;cell&gt;29&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;If you look through the flags of &lt;code&gt;ls&lt;/code&gt; to see why it has so many, notice how many are configuring what &lt;code&gt;ls&lt;/code&gt; is displaying. In a real sense, this is going against the underlying philosophy of unix pipelines. Rather than composing a pipeline to get the display you want, you're learning a language of flags for each command to configure the display.&lt;/p&gt;&lt;p&gt;Let's talk about exit codes. Actually, wait, I already did that. As I point out in the post, the standard says:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"The value of status may be 0, EXIT_SUCCESS, EXIT_FAILURE, [CX] [Option Start] or any other value, though only the least significant 8 bits (that is, status &amp;amp; 0377) shall be available from wait() and waitpid(); the full value shall be available from waitid() and in the siginfo_t passed to a signal handler for SIGCHLD. [Option End]"&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Sure - we all still use 8-bit machines while flipping through a manual we printed trying to find the exit code description to understand why a command failed.&lt;/p&gt;&lt;p&gt;I hear you: "but, look, it doesn't matter how archaic this stuff is, if we all agree to use it things keep working."&lt;/p&gt;&lt;p&gt;I dunno - these arguments just don't hold up. We'd still be using C as our main systems language because it's the most documented, most portable, etc. But, by and large, we don't. We're increasingly choosing other languages.&lt;/p&gt;&lt;p&gt;The truth is, in 2023 if someone asked us to design a system, we wouldn't design POSIX. If, in 2023, someone asked us to design a shell language, we wouldn't design bash/zsh. This matters.&lt;/p&gt;&lt;head rend="h2"&gt;Show us the money&lt;/head&gt;&lt;p&gt;I make some pretty bold statements in the above. There is a heritage of technology that got us to this point. While that heritage is important, it's not without its drawbacks. Are there better ways of doing it?&lt;/p&gt;&lt;head rend="h3"&gt;Why structure matters&lt;/head&gt;&lt;p&gt;Before we get into talking about Nushell, let's talk about why structured data matters.&lt;/p&gt;&lt;p&gt;In the Unix pipeline way of thinking, text passes between commands. This is very flexible, but has a major problem: both the outputting command and the inputting command have to agree what shape that text will take so the info can be passed. This locks representation to presentation, disallowing commands from evolving their output over time. As we showed earlier, it also encourages a proliferation of flags.&lt;/p&gt;&lt;p&gt;That's annoying. Does separating the structure from its presentation help us?&lt;/p&gt;&lt;code&gt;&amp;gt; ls | where size &amp;gt; 10kb
&lt;/code&gt;
&lt;p&gt;I always start with this example when showing off Nushell, because not only is it immediately readable, we didn't have to dig through any flags to figure out what we needed to pass to &lt;code&gt;ls&lt;/code&gt; to get that. We also aren't parsing anything from &lt;code&gt;ls&lt;/code&gt;. Instead, the data is passed directly to &lt;code&gt;where&lt;/code&gt;, which handles it directly.&lt;/p&gt;&lt;p&gt;Commands already know this structure, why not make use of it?&lt;/p&gt;&lt;p&gt;The same &lt;code&gt;where&lt;/code&gt; command works on other things. For example, we can process the output of the &lt;code&gt;ps&lt;/code&gt; command:&lt;/p&gt;&lt;code&gt;&amp;gt; ps | where cpu &amp;gt; 40
&lt;/code&gt;
&lt;p&gt;Or open a &lt;code&gt;csv&lt;/code&gt; file and processing its rows:&lt;/p&gt;&lt;code&gt;&amp;gt; open fields.csv | where area &amp;gt; 5
&lt;/code&gt;
&lt;p&gt;And so on. It's the same &lt;code&gt;where&lt;/code&gt; regardless of where the data is coming from. It also gives us the freedom to present the data however we want.&lt;/p&gt;&lt;head rend="h2"&gt;Why Nushell matters&lt;/head&gt;&lt;head rend="h3"&gt;Nushell is designed to be a language&lt;/head&gt;&lt;p&gt;I had the good fortune of being a part of some prominent programming language teams, including helping to create TypeScript and helping create Rust's error messages as part of the Rust team in Mozilla. Designing languages to be easy to use, easy to read, easy to scale up, and easy to debug is something I care about and have worked on for many years.&lt;/p&gt;&lt;p&gt;To that end, Nushell is designed with an eye towards being readable at a glance.&lt;/p&gt;&lt;p&gt;Let's do a &lt;code&gt;for&lt;/code&gt; loop in Nushell:&lt;/p&gt;&lt;code&gt;for i in 1..10 {
    print $i
}
&lt;/code&gt;
&lt;p&gt;(aside: "but why do variables have dollar signs?". Turns out the flexibility of shell programming allows paths to not use quotes, so it's nice to tell a difference between &lt;code&gt;cd foo&lt;/code&gt; and &lt;code&gt;cd $foo&lt;/code&gt;)&lt;/p&gt;&lt;p&gt;This eye towards usable design shows up in many ways. Working with data is improved by not only having structure, but also being able to pattern match against it. Here's an example of pattern matching a list in Nushell:&lt;/p&gt;&lt;code&gt;match $list {
  [$one] =&amp;gt; { print "one element list" }
  [$one, $two] =&amp;gt; { print "two element list" }
  [$head, ..$tail] =&amp;gt; { print $"the tail of the list is ($tail)" }
}
&lt;/code&gt;
&lt;p&gt;In a way, working in Nushell should feel at home both interactively as a shell and as a full scripting language. We've had folks write COVID reporting software in Nushell, research experiments, even entire shells for well-known database services.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell is typechecked&lt;/head&gt;&lt;p&gt;Since Nushell doesn't treat all data as text, you can represent tables, records, numbers, booleans, etc directly in the language.&lt;/p&gt;&lt;p&gt;As a result of this, Nushell is fully typechecked. Common errors can be caught early and shown to you before the script even runs.&lt;/p&gt;&lt;p&gt;Taking what we learned from TypeScript - the types also feed into another important tool.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has IDE support&lt;/head&gt;&lt;p&gt;The types, autocompletion, and early error reporting feed into an engine in Nushell that knows a lot more about your code. As a result, you can write scripts and then work with them using the IDE support Nushell provides. Seeing errors, jumping to definitions, getting documentation on hovers, etc are all part of the Nushell experience.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has nice errors&lt;/head&gt;&lt;p&gt;In Nushell, we make extensive use of remembering where data comes from, as well as what caused an error. Simple errors, like division by zero, are shown clearly:&lt;/p&gt;&lt;p&gt;A more complex error may need to show more to help track down where a mistake came from. Let's say you've accidentally put a string in your list of numbers, and then tried to process it:&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has a SQL-like style&lt;/head&gt;&lt;p&gt;When you start using Nushell to compose pipelines, you'll notice that it has a distinct SQL-like style. Data flows through each stage, and you build up what you want to do to it as you add more commands.&lt;/p&gt;&lt;p&gt;This gives Nushell a distinctive design that encourages experimentation and exploration.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell is, and has always been, crossplatform&lt;/head&gt;&lt;p&gt;An important decision we made from day 1 was to be crossplatform. You can run Nushell on Windows, Linux, and macOS (and BSD, and Android) and get the same experience. You can easily write scripts in a way that they can be run across different platforms. Everything that you learn transfers between OSes without friction.&lt;/p&gt;&lt;head rend="h2"&gt;Is Nushell good enough to overcome the inertia?&lt;/head&gt;&lt;p&gt;I distinctly remember going to a SIAM conference many years back and giving a talk on the Chapel programming language. Even back then, it was a clever language. In a couple lines, you could write code that could distribute and process a matrix across a network of computers. Coming from a lineage of array languages, it ate up data parallel processing. The equivalent code in other languages looked verbose in comparison.&lt;/p&gt;&lt;p&gt;I went through my talk, hoping I'd done a decent job of conveying the main points, and at the end, someone in the audience stood up and said "but I can do all this in C++".&lt;/p&gt;&lt;p&gt;He proceeded to explain that if he could recreate many of the techniques we showed all as part of a C++ library that people could use. At this time, I wasn't sure how to respond other than "but you don't have to, we already built this language" but he couldn't be swayed. If it wasn't C++, he didn't want it.&lt;/p&gt;&lt;p&gt;Fast forward a couple years, and I'm standing in front of a JavaScript audience giving a similar talk, this time promoting TypeScript. I remember the kind of politely confused looks on people's faces as I showed off the features TypeScript offered. There was a similar sense of "why do we need to leave JavaScript?".&lt;/p&gt;&lt;p&gt;To answer whether Nushell can overcome this kind of inertia, I'll pose two questions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Is Nushell compelling enough for a single person to adopt it?&lt;/item&gt;&lt;item&gt;Would adopting Nushell broadly as a community move the needle?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Let's tackle the first question. Time and again, as people try Nushell, they come back with quotes like "this is the most excited I've been about tech in 15 years". It has a fanbase that loves it, and that fanbase is growing. It reminds me of the early days of Rust, just after hitting 1.0.&lt;/p&gt;&lt;p&gt;To the second question: would adopting Nushell broadly actually improve things noticeably? Without a doubt. I say this without any reservation. Thinking of our shells as structured, interactive processing engines opens up the doors to a much wider array of things you can do with them. The commands would be far simpler than their POSIX equivalents and would compose far better. They'd benefit from the full knowledge of the data being shared between them. Adaptors could be made to connect to all parts of the system, allowing you full, structured interaction with everything you have access to.&lt;/p&gt;&lt;p&gt;In essence, as the saying goes, we'd be building a skyscraper starting from the 15th floor instead of the 1st.&lt;/p&gt;&lt;head rend="h2"&gt;It's time to be honest about what Nushell is&lt;/head&gt;&lt;p&gt;It's time I come clean about what Nushell is. Nushell isn't exactly a shell, at least not in the traditional Unix sense of the word. Nushell is trying to answer the question: "what if we asked more of our shells?"&lt;/p&gt;&lt;p&gt;Nushell is really an interactive, data-focused scripting language with shell capabilities. It merges these three things into one:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A fully-typed scripting language&lt;/item&gt;&lt;item&gt;An interactive shell&lt;/item&gt;&lt;item&gt;A data processing system (that can also handle large data loads via dataframes)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Rather than these being three separate ideas glued together, in practice it feels like Nushell is treating everything you interact with as data. This allows you to pull together different kinds of data, and know that the same commands will work over them.&lt;/p&gt;&lt;p&gt;You might look at that list and think you don't need all that, but the way I might frame it is this: it's nice to have it when you need it.&lt;/p&gt;&lt;p&gt;I don't need to do heavy data processing everyday, but it's nice to not have to shift what I'm doing at all when I need to do it. I don't have to download new utilities or switch languages. It's all right there. Need to write a script to load some files and handle some directory processing? Still right there. Need to throw together some web query that outputs the top download results for a github repo? You guessed it, all still right there.&lt;/p&gt;&lt;p&gt;This is just scratching the surface, really. Nushell has a plugin system that allows more capabilities to be added based on your needs. We already have plugins that add a variety of additional file formats, querying capabilities, and more.&lt;/p&gt;&lt;head rend="h2"&gt;It's okay to have nice things&lt;/head&gt;&lt;p&gt;Nushell was built with a simple idea: working in the shell, writing code, and processing data should be fun. To that end, we work hard to make Nushell feel nice.&lt;/p&gt;&lt;p&gt;You can write readable scripts that come with their own documentation, and then come back to them 6 months later and still understand what they're doing.&lt;/p&gt;&lt;p&gt;You can sit in the shell and play with pipeline ideas until one grows into a scripting project and then effortlessly transition your experiment into a full script.&lt;/p&gt;&lt;head rend="h2"&gt;That's it&lt;/head&gt;&lt;p&gt;That's my case. It's okay to have fun. It's okay to write attractive, well-documented code. It's okay to leave the designs of the past behind when they no longer fit the present day.&lt;/p&gt;&lt;p&gt;It's okay to move on to better ways of doing things.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sophiajt.com/case-for-nushell/"/><published>2026-01-07T16:15:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528192</id><title>BillG the Manager (2021)</title><updated>2026-01-07T20:44:39.407479+00:00</updated><content>&lt;doc fingerprint="27401b374f6f26de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;019. BillG the Manager&lt;/head&gt;
    &lt;head rend="h3"&gt;‚ÄúIntelli‚Ä¶what?‚Äù‚ÄìBill Gates&lt;/head&gt;
    &lt;p&gt;The breadth of the Microsoft product line and the rapid turnover of core technologies all but precluded BillG from micro-managing the company in spite of the perceptions and lore around that topic. In less than 10 years the technology base of the business changed from the 8-bit BASIC era to the 16-bit MS-DOS era and to now the tail end of the 16-bit Windows era, on the verge of the Win32 decade. How did Bill manage this ‚Äî where and how did he engage? This post introduces the topic and along with the next several posts we will explore some specific projects.&lt;/p&gt;
    &lt;p&gt;Please feel free to share this and subscribers, please join in the discussion.&lt;/p&gt;
    &lt;p&gt;Back to 018. Microsoft‚Äôs Two Bountiful Gardens&lt;/p&gt;
    &lt;p&gt;At 38, having grown Microsoft as CEO from the start, Bill was leading Microsoft at a global scale that in 1993 was comparable to an industrial-era CEO. Even the legendary Thomas Watson Jr., son of the IBM founder, did not lead IBM until his 40s. Microsoft could never have scaled the way it did had BillG managed via a centralized hub-and-spoke system, with everything bottlenecked through him. In many ways, this was BillG‚Äôs product leadership gift to Microsoft‚Äîa deeply empowered organization that also had deep product conversations at the top and across the whole organization.&lt;/p&gt;
    &lt;p&gt;This video from the early 1980‚Äôs is a great introduction to the breadth of Microsoft‚Äôs product offerings, even at a very early stage of the company. It also features some vintage BillG voiceover and early sales executive Vern Raburn. (Source: Microsoft videotape)&lt;/p&gt;
    &lt;p&gt;Bill honed a set of undocumented principles that defined interactions with product groups. The times of legendary BillG reviews characterized by hardcore challenges and even insults had become, mostly, a thing of the past excepting the occasional sentimental outburst. More generally, they were a collective memory of hyper-growth moments any start-up experiences, only before the modern era when such stories were more commonly understood.&lt;/p&gt;
    &lt;p&gt;Much later in 2006, when BillG announced his intent to transition from full time Microsoft and part time philanthropy to full time philanthropy, many reporters surprised him by asking how Microsoft would continue without his coordination of technical strategy and oversight. But even in the early ‚Äô90s, at the height of the deepest and most challenging technology strategy questions, he never devoted the bulk of his time to micromanaging product development. He spent a good deal of time engaged with products, but there were far too many at too many stages of development to micro-manage them. In many ways this was the opposite of the approach Steve Jobs took, even if both were known for their own forms of challenging interactions. The most obvious contrast between the two was the breadth of the product line and the different market touchpoints.&lt;/p&gt;
    &lt;p&gt;Having grown up through Development Tools and Languages, I was familiar with Microsoft‚Äôs product line, but only as TA did it become clear how comparatively broad Microsoft had so quickly become. The software world was thought of through a lens of major categories: operating systems, tools and languages, networking, and applications, roughly mirroring Microsoft‚Äôs org chart. The latter was thought of as word processing, spreadsheets, graphics, databases, as well as assorted smaller categories. It was easy to identify leaders in each of those areas‚Äînames that were tip of the tongue at the time and most of which are no longer in the PC software space (IBM, Borland, Novell, WordPerfect, Lotus, Aldus, Ashton Tate, and many more). The ah-ha moment in the early 1990s was the realization that no company on that list was competing in more than one category. Microsoft was hardly winning in every category. In fact, in most categories it was new entry, a distant second, or even third place, but the company was in every space. Bill was committed and patient. Microsoft was relentless. And Microsoft was focused on Windows.&lt;/p&gt;
    &lt;p&gt;BillG had fostered Microsoft with a grand vision to compete in every category of PC software, from some of the earliest days. With rare exceptions, no other company set out to do that. BillG led a deep technology strategy. It started with the operating system, supported by tools and languages, and then using those to build applications. This seemed simple enough. In fact, it is what IBM built for mainframes and DEC built for minicomputers.&lt;/p&gt;
    &lt;p&gt;There was a crucial difference. Microsoft did not build hardware and was not vertically integrated to reduce competition. Microsoft built an operating system on an openly architected PC (the same Intel-based architecture that came to power both Macintosh and Linux years later) and published APIs so that anyone could build tools and applications for the operating system‚Äîan open hardware platform and open operating system APIs. This approach simply addressed all the early challenges Microsoft itself faced trying to figure out how to build winning applications‚Äîit was so busy dealing with dozens of proprietary computing platforms, each with their own tools and APIs just different enough to make things difficult, but not so different as to be valuable. Bill saw the value in software and in openness at key points in the overall architecture. At the formation of the company, he and PaulA saw the immense and expansive value of software and, essentially, the liability that being in the hardware business carried. Building Microsoft‚Äôs software-only business on an open hardware platform where many players competed to drive prices down while maintaining compatibility with the operating system was one of the all-time great strategy choices. The idea of building hardware seemed like a sucker‚Äôs bet, with low margins, manufacturing, and inventory‚Äîthe baggage of the physical world. While Microsoft would dabble in peripherals or hardware that could bootstrap new PC scenarios, building whole computers was a headache better left to others.&lt;/p&gt;
    &lt;p&gt;Expanding the impact of that breadth software strategy was BillG‚Äôs day-to-day operating model, not micromanaging the specifics of any given project. I am painting this with a broad brush, intentionally so. Part of the difference between the then dominant cultures of Systems and Apps was that during the MikeMap era (and arguably during the earlier JeffH era), Apps weaned itself from Bill‚Äôs intense and constant scrutiny whereas the Systems culture more clearly embraced that dynamic. That was largely true until PaulMa took a more hands-off (or walled-off) approach to the nurturing of the NT project.&lt;/p&gt;
    &lt;p&gt;In his May 1991 email, ‚ÄúChallenges and Strategy,‚Äù BillG set the company on the Windows strategy, clarifying the foundations for every product and group, solidifying what had been complex platform choices every team faced. Regardless of whether Bill was a savant when it came to the technical details of projects or he simply remembered everything each group sent or told him, he operated the company at a higher level of abstraction than reporters believed to be the case in 2008 when he ultimately reduced his full-time commitment to Microsoft.&lt;/p&gt;
    &lt;p&gt;I had a glimpse of this when our AFX team had our pivotal review. Later as TA I was there to connect the dots and amplify the Windows strategy. By and large the company was still wrapping itself around the details of what it really meant to embrace Windows, exclusively. That, and coping with the myriad of choices and decisions that come from the tension between aligning with a Windows strategy and having some control over your own destiny as a product. Which version of Windows? When is that shipping? Will the APIs our product needs be in Windows? Will those APIs work on older versions of Windows? What about Windows NT? On, which microprocessors? What about the other parts of Microsoft? The questions were endless. This was truly big company stuff‚Äîthe strategy at a high level is one thing, but execution across a $600M (1994) research and development budget was another. The fascinating thing was how products so quickly scaled beyond what Bill personally experienced as a programmer, both in size and technology specifics. This was to be expected‚Äîby any measure the company was huge‚Äîbut people and Bill himself still expected to interact on product details as though he was a member of the product team. I often found myself looking for ways to help Bill engage at that level, even if just for show.&lt;/p&gt;
    &lt;p&gt;In addition to the Windows strategy, with the late 1993 launch of Office 4, Microsoft also declared 1994 ‚ÄúYear of Office‚Äù. It was the biggest launch for Apps and represented a major pivot of the organization to the opportunity of selling a suite of products. This too was in the earliest days of a strategy, one that I would end up spending significant time on as TA and then later as a member of the team.&lt;/p&gt;
    &lt;p&gt;Just because Bill operated at a level of abstraction across products groups did not preclude product groups from engaging on what might seem like relatively small, non-technical matters. One of the more entertaining meetings I attended was preparing for the launch of Office 4, which was a worldwide event complete with a reporter given permission to shadow the team. A key differentiator would be how the user would experience ‚Äúintelligence‚Äù in the product, so that it understood what was intended and how to achieve it in the new Office software. The development team built a series of features along the lines of what was termed ‚Äúbasic use‚Äù such as AutoCorrect in Word, AutoFilter in Excel tables, and a host of Wizards (guided step-by-step flows such as for creating charts), and more. To bring them together and actually communicate with the market and on retail packaging, the marketing team came up with an umbrella term. Pete Higgins (PeteH) came over to brief BillG on that choice in a small meeting in Bill‚Äôs office.&lt;/p&gt;
    &lt;p&gt;PeteH was by then the spiritual leader of the business side of Apps. He rose through the ranks of Excel and was clearly MikeMap‚Äôs lead executive. Pete was the kind of calm and in control leader that everyone enjoyed working for‚Äîhe was at once clearly the boss, but also a member of the team. Pete was a native of the Seattle area, high school football star, and Stanford graduate. He was a new generation of Microsoft product executive, coming from the business and not the coding side. For me in my TA role, Pete was one of my biggest supporters and mentors and made connecting with Apps super easy.&lt;/p&gt;
    &lt;p&gt;Sitting at the little couch under the Intel chip poster, after going through the details of the launch, Pete said the proverbial ‚Äúthere‚Äôs one more thing.‚Äù Bill rocking in his chair shook his head, given that the meeting was mostly an uneventful recap of the upcoming press tour. Pete went on to explain the problem of communicating all the features and how Microsoft needed a term to market and describe them. Pete was dancing around this because he knew well enough that Bill was not a fan of ‚Äúmarketing‚Äù. Ever so delicately Pete said, ‚Äúthis is your chance‚Ä¶we want to go with this term but if you don‚Äôt like it‚Ä¶‚Äù&lt;/p&gt;
    &lt;p&gt;Pete then said, ‚ÄúIntelliSense. Microsoft Office introduces IntelliSense.‚Äù&lt;/p&gt;
    &lt;p&gt;Bill‚Äôs reply, ‚ÄúIntelli‚Ä¶what?‚Äù&lt;/p&gt;
    &lt;p&gt;Pete again tried to position the positioning, his instinct about resistance proving correct. ‚ÄúIt is IntelliSense‚Ä¶it means that Office has built-in intelligence, and it understands what you need and how to do it.‚Äù&lt;/p&gt;
    &lt;p&gt;Bill still not warming up, went full pedantic, ‚Äúwhat intelligence‚Ä¶is there a Prolog rules engine, a neural network, ‚Ä¶.‚Äù He was also making the scrunched up surprised look that he does, which turns out (once you realize it) to also be a bit sarcastic. It meant he was warming up.&lt;/p&gt;
    &lt;p&gt;A few more times back and forth, and Pete just made Bill say IntelliSense in a sentence one more time, which he did with kind of a devilish smirk.&lt;/p&gt;
    &lt;p&gt;Done.&lt;/p&gt;
    &lt;p&gt;Looking back this all seems absurd. Consternation over a single phrase. Literally seeking approval to use it from the CEO of a billion-dollar company. All on the heels of what was no doubt months of preparation, including getting SteveB‚Äôs approval which was actually critical. Finally, the theater that Pete would pull the plug a few weeks before the tour. In some ways this was the Apps way of bringing decisions to Bill‚Äîit wasn‚Äôt really a choice and it had been broadly vetted and was buttoned-up. Any debate would probably be theater more than anything.&lt;/p&gt;
    &lt;p&gt;On average, there was one product-focused meeting on most days. Most teams saw Bill once or twice a year. NathanM saw Bill most every day or at least in most every technology context, present day or far out there. Most executives, like PaulMa, PeteH (leading Apps), and Susan Boeschen (SusanB leading consumer), saw Bill in product review contexts several times a month because each had many ongoing projects or, in the case of the big projects (like operating systems), many large components. Everyone was in constant contact over email. Bill was always forwarding emails across the company, adding relevant people from all levels of the organization to the CC line, and never backed off a good reply-all opportunity. Phone or in-person 1:1s were not the typical way of interacting across the product executive team. For the most part, work happened in groups or at least with an audience, with outcomes and flare-ups quickly disseminated by email. I found myself constantly on the move walking around campus from one building to the next to meet people in person, rarely was I in my office (a pattern that continued my entire career).&lt;/p&gt;
    &lt;p&gt;I was often asked to meet with teams before they met with Bill. They hoped for insight into how BillG might think about choices and decisions or even the presentation overall. I often disappointed teams in these pre-meetings since I was hardly a stand-in for Bill, and I was hardcore about leaving any such impression. Pre-meetings gave me a chance to better understand the issues the team was struggling with and to make sure those were brought forward in an objective and transparent manner. The fastest path to failure was to structure a conversation so Bill discovered an issue rather than having it revealed to him. To be fair, an equally fast path to failure was a first slide listing a slew of problems and issues in the hopes of inoculating the remainder of the meeting. In that case, I would caution teams that they were exposing themselves to the inevitable ‚ÄúHow can this be so difficult?‚Äù comments. Getting this balance right was the essence of leading an effective meeting.&lt;/p&gt;
    &lt;p&gt;For most meetings, I wrote a summary meeting preview. Even though Bill said he did not want this, I could not help myself. While he was always effective, I felt that a little bit of specifics could go a long way in making the meeting more effective and less random. I could tell he had read my mail if he raised a point verbatim from my note, and frequently he would kick off the meeting doing so, never crediting me of course. In these, and all mails talking about other teams, I always tried to separate the facts of the meeting, the team‚Äôs analysis, and my own opinion. Bill was transparent with email and thought little of forwarding an entire thread. I learned the ramifications of that the hard way.&lt;/p&gt;
    &lt;p&gt;As an example of where I failed to follow my own rules about fact versus opinion, I totally offended Jim Allchin (JimAll), leading the Cairo project, on the role of a specific technology in distributed programming. Not only did Jim inform me that my opinions were wrong, but also that I stepped all over his own PhD dissertation as a leading expert. In hindsight, this was terrifying‚ÄîJim‚Äôs reply was brutal‚Äîbut it proved a good early learning experience, so to speak.&lt;/p&gt;
    &lt;p&gt;While the product line was already broad, the expansion to entirely new areas was unstoppable. On most any product area, we were forming an opinion, beginning work, or already in the market. There was not a booth at a tradeshow, a focused conference, or a major company looking to partner that Microsoft was not already connected to or connecting with in some way. While Microsoft was in the earliest days of achieving a PC in every home (about 25 percent of US households in 1993) and on every desktop (about half of US workers in 1993), every day in this job was either furthering that or expanding beyond homes and desktops from data centers to handhelds to airplanes (the first in-flight PC-based system was an early partnership between Microsoft and an airline, including certification for Windows Server).1&lt;/p&gt;
    &lt;p&gt;Product meetings had no set format or structure and usually reflected the culture of the organization. This might be a surprise to some as many CEOs (or perhaps their staff!) might have imposed some more rigor on meetings. Microsoft had two bountiful gardens, but there were micro-cultures throughout out the company. While one group did slick and well-rehearsed presentations, another might present research-heavy deep dives. Bill often pushed a team outside its comfort zone, deliberately pushing the team to discuss places they were less prepared, or even less interested. It was a technique he employed. He once said to me, ‚ÄúWhy spend all the time with the Windows team talking about architecture, if that was their predisposition anyway?‚Äù This was also a strategy to level the playing field‚Äîtalking about architecture to Windows or ease of use to Excel was too lopsided and Bill was disadvantaged.&lt;/p&gt;
    &lt;p&gt;The reality of BillG Reviews never lived up to lore.&lt;/p&gt;
    &lt;p&gt;Most meetings progressed without incident‚Äîmeaning without yelling. Sometimes, though, there were comments such as ‚ÄúThat was the stupidest thing I ever heard‚Äù or ‚ÄúThat is brain-dead.‚Äù The worst was ‚ÄúThat‚Äôs trivial . . . let me show you.‚Äù Those were all the clich√©s that teams anticipated but then wore as a badge of honor. They happened with far less frequency compared to how much they were talked about. Even over the short period of time I worked as TA, Bill became more intentional in his use of meeting dynamics. Still, the first seconds of a meeting remained a bit of a mood thermometer, pity those for whom it was clearly a bad day.&lt;/p&gt;
    &lt;p&gt;When meetings ended up ‚Äúbad‚Äù it was always because the team was poorly prepared, or they came to talk about the project in a way that diverged from expectations. There were typical capital offenses in the meeting, such as failing to understand a product strategy of competitors or downplaying a competitor‚Äôs potential. Worst was coming across as though a product was making mostly tactical decisions driven by schedule or failing to understand the architecture of the product relative to the evolving platform and related teams across Microsoft. PivotTables were just making their way across most teams, so many were still making the common errors of using static charts and graphs that always seemed to have the data oriented or filtered in the least useful way. Those moments always held potential for a lively discussion.&lt;/p&gt;
    &lt;p&gt;Part of my role was to reduce the potential for such liveliness ahead of time. I tried to alert teams about potential issues without acting as a surrogate for Bill, and to make sure meetings did not save the difficult or bad news for the end. I was also there to throw myself on the grenade, so to speak, and get meetings back on track by helping the team through a tough moment‚Äîusually by restating or interpreting what they were saying or by redirecting the topic at hand to a follow-up discussion.&lt;/p&gt;
    &lt;p&gt;By far the biggest strategic error one could make was knowingly duplicating code outside core expertise, and then compounding that by attempting to explain why in this particular case it is justified. Microsoft Publisher was a new product in the desktop publishing category. It was being built by the Consumer Division under the leadership of Melinda French (MelindaF). The product aimed for the small business and non-professional market, compared to the incumbent Aldus PageMaker. It differentiated itself with ease-of-use features, pioneering Wizards and other user interface innovations. But it also produced printed pages that looked a lot like what one should be able to create with Microsoft Word. This overlap was the source of endless consternation‚Äîwhy can‚Äôt they share code, why can‚Äôt Word do all these features, and then ultimately why does Publisher even exist. Yet, customers loved it. At one point, a meeting went down a rabbit hole over bullets and numbering and how Publisher was basically writing all the same code Word was and wasting everyone‚Äôs resources. There was little actionable in this kind of rant, but it did establish the norm of being called out for redundancy and the need to be prepared to cope with the feedback.&lt;/p&gt;
    &lt;p&gt;Bill maintained a deep commitment to evaluating a portfolio of efforts, and even within a single product he believed in the portfolio approach of features‚Äînot every product nor every feature was a winner or a breakthrough, but on the whole something needed to be working. As much as Bill might give a group a difficult time (as happened with Visual C++), he knew there was always more to the product and more products to the company. It was not just that Bill was building a product portfolio for Microsoft, he was managing the teams as a portfolio of efforts. This portfolio approach created a resiliency in the company‚Äîresilient to the unpredictable nature of technology bets and to the ability of the people on the team to execute. Not everything went as planned nor did every planned bet ultimately make sense.&lt;/p&gt;
    &lt;p&gt;Whether deliberate or not, BillG had three axes that created a constant state of balance, of push and pull, across the hundred teams creating software. Bill‚Äôs approach of constantly balancing the tension between innovation and shipping, expanding the portfolio while maintaining coherency, and the injection of new ideas while also executing on existing work proved to be the most interesting ‚Äúmanagement‚Äù lesson. The next three sections are examples of each of these dimensions.&lt;/p&gt;
    &lt;p&gt;On to 020. Innovation versus Shipping: The Cairo Project&lt;/p&gt;
    &lt;p&gt;https://nces.ed.gov/programs/digest/d08/tables/dt08_432.asp&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hardcoresoftware.learningbyshipping.com/p/019-billg-the-manager"/><published>2026-01-07T16:18:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528353</id><title>Health care data breach affects over 600k patients, Illinois agency says</title><updated>2026-01-07T20:44:38.757798+00:00</updated><content>&lt;doc fingerprint="e6188ade5513a61a"&gt;
  &lt;main&gt;
    &lt;p&gt;The names and addresses of thousands of patients of the Illinois Department of Human Services were incorrectly made publicly viewable for the last several years, the agency said Friday.&lt;/p&gt;
    &lt;p&gt;Several maps created to assist the agency with decisions ‚Äî like where to open new offices and allocate certain resources ‚Äî were made public through incorrect privacy settings between 2021 and 2025, the Department of Human Services said in a statement.&lt;/p&gt;
    &lt;p&gt;More than 32,000 customers with the IDHS division of rehabilitation services had information publicly viewable between April 2021 and September 2025. The information included names, addresses, case numbers, case status, referral source information, region and office information and status as Division of Rehabilitation Services recipients, the agency said.&lt;/p&gt;
    &lt;p&gt;Around 670,000 Medicaid and Medicare Savings Program recipients had their addresses, case numbers, demographic information and the name of medical assistance plans publicly viewable between January 2022 and September 2025, IDHS said.&lt;/p&gt;
    &lt;p&gt;The state agency said the mapping website was unable to identify who viewed the maps, and IDHS is unaware of any misuse of personal information resulting from the data leak.&lt;/p&gt;
    &lt;p&gt;IDHS discovered the issue Sept. 22 and immediately changed the privacy settings for all maps, restricting access to authorized IDHS employees, the agency said. It also implemented a secure map policy that prohibits uploading customer data to public mapping websites.&lt;/p&gt;
    &lt;p&gt;Individuals whose information was made public will receive a notice about the leak from IDHS. The notices will include a phone number that people can call for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nprillinois.org/illinois/2026-01-06/health-care-data-breach-affects-600-000-patients-illinois-agency-says"/><published>2026-01-07T16:28:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46529237</id><title>Eat Real Food</title><updated>2026-01-07T20:44:38.567998+00:00</updated><content>&lt;doc fingerprint="c2f8a747964fdd4a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Real Food&lt;lb/&gt; Starts Here&lt;/head&gt;&lt;p&gt;Better health begins on your plate‚Äînot in your medicine cabinet.&lt;lb/&gt; The new Dietary Guidelines for Americans defines real food as whole, nutrient-dense, and naturally occurring, placing them back at the center of our diets.&lt;/p&gt;&lt;head rend="h2"&gt;The State of Our Health&lt;/head&gt;&lt;head rend="h3"&gt;America is sick.&lt;lb/&gt;The data is clear.&lt;/head&gt;&lt;head rend="h3"&gt;50% of Americans have prediabetes or diabetes&lt;/head&gt;&lt;head rend="h3"&gt;75% of adults report having at least one chronic condition&lt;/head&gt;&lt;head rend="h3"&gt;90% of U.S. healthcare spending goes to treating chronic disease‚Äîmuch of which is linked to diet and lifestyle&lt;/head&gt;&lt;p&gt;For decades we've been misled by guidance that prioritized highly processed food, and are now facing rates of unprecedented chronic disease.&lt;/p&gt;&lt;p&gt;For the first time, we're calling out the dangers of highly processed foods and rebuilding a broken system from the ground up with gold-standard science and common sense.&lt;/p&gt;&lt;head rend="h2"&gt;The New Pyramid&lt;/head&gt;&lt;head rend="h2"&gt;Eat Real &lt;lb/&gt;Food&lt;/head&gt;&lt;head rend="h2"&gt;Protein, Dairy, &amp;amp; Healthy Fats&lt;/head&gt;&lt;p&gt;We are ending the war on protein. Every meal must prioritize high-quality, nutrient-dense protein from both animal and plant sources, paired with healthy fats from whole foods such as eggs, seafood, meats, full-fat dairy, nuts, seeds, olives, and avocados.&lt;/p&gt;&lt;p&gt;Protein target: 1.2‚Äì1.6 grams per kilogram of body weight per day.&lt;/p&gt;&lt;head rend="h2"&gt;Vegetables &amp;amp; Fruits&lt;/head&gt;&lt;p&gt;Vegetables and fruits are essential to real food nutrition. Eat a wide variety of whole, colorful, nutrient-dense vegetables and fruits in their original form, prioritizing freshness and minimal processing.&lt;/p&gt;&lt;p&gt;Vegetables: 3 servings per day.&lt;/p&gt;&lt;p&gt;Fruits: 2 servings per day.&lt;/p&gt;&lt;head rend="h2"&gt;Whole Grains&lt;/head&gt;&lt;p&gt;Whole grains are encouraged. Refined carbohydrates are not. Prioritize fiber-rich whole grains and significantly reduce the consumption of highly processed, refined carbohydrates that displace real nourishment.&lt;/p&gt;&lt;p&gt;Target: 2‚Äì4 servings per day.&lt;/p&gt;&lt;p&gt;Our nation is finding its footing again, moving past decades of unhealthy eating and rebuilding a food culture rooted in health, science, transparency, and personal responsibility.&lt;/p&gt;&lt;head rend="h2"&gt;Key&lt;lb/&gt;Guidance&lt;/head&gt;&lt;head rend="h2"&gt;Resources&lt;/head&gt;&lt;p&gt;Explore the research, recommendations, and implementation guidance that shape the Dietary Guidelines, including the science, the policy guidance, and the everyday serving framework.&lt;/p&gt;Watch the press release&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://realfood.gov"/><published>2026-01-07T17:22:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46529797</id><title>A tab hoarder's journey to sanity</title><updated>2026-01-07T20:44:38.160968+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/borisandcrispin/status/2008709479068794989"/><published>2026-01-07T17:54:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46530448</id><title>NPM to implement staged publishing after turbulent shift off classic tokens</title><updated>2026-01-07T20:44:38.023499+00:00</updated><content/><link href="https://socket.dev/blog/npm-to-implement-staged-publishing"/><published>2026-01-07T18:31:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531068</id><title>US will ban Wall Street investors from buying single-family homes</title><updated>2026-01-07T20:44:37.768649+00:00</updated><content/><link href="https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/"/><published>2026-01-07T19:13:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531280</id><title>Introducing ChatGPT Health</title><updated>2026-01-07T20:44:37.548426+00:00</updated><content>&lt;doc fingerprint="2c12c8739eff67b7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing ChatGPT Health&lt;/head&gt;
    &lt;p&gt;A dedicated experience in ChatGPT designed for health and wellness.&lt;/p&gt;
    &lt;p&gt;We‚Äôre introducing ChatGPT Health, a dedicated experience that securely brings your health information and ChatGPT‚Äôs intelligence together, to help you feel more informed, prepared, and confident navigating your health.&lt;/p&gt;
    &lt;p&gt;Health is already one of the most common ways people use ChatGPT, with hundreds of millions of people asking health and wellness questions each week. ChatGPT Health builds on the strong privacy, security, and data controls across ChatGPT with additional, layered protections designed specifically for health‚Äî including purpose-built encryption and isolation to keep health conversations protected and compartmentalized. You can securely connect medical records and wellness apps to ground conversations in your own health information, so responses are more relevant and useful to you. Designed in close collaboration with physicians, ChatGPT Health helps people take a more active role in understanding and managing their health and wellness‚Äîwhile supporting, not replacing, care from clinicians.&lt;/p&gt;
    &lt;p&gt;Today, health information is often scattered across portals, apps, wearables, PDFs, and medical notes‚Äîso it's hard to see the full picture, and people are left to navigate a complex healthcare system on their own. People have shared countless stories of turning to ChatGPT to help make sense of it all. In fact, health is one of the most common ways people use ChatGPT today: based on our de-identified analysis of conversations, over 230 million people globally ask health and wellness related questions on ChatGPT every week.&lt;/p&gt;
    &lt;p&gt;ChatGPT Health builds on this so responses are informed by your health information and context. You can now securely connect medical records and wellness apps‚Äîlike Apple Health, Function, and MyFitnessPal‚Äîso ChatGPT can help you understand recent test results, prepare for appointments with your doctor, get advice on how to approach your diet and workout routine, or understand the tradeoffs of different insurance options based on your healthcare patterns.&lt;/p&gt;
    &lt;p&gt;Health is designed to support, not replace, medical care. It is not intended for diagnosis or treatment. Instead, it helps you navigate everyday questions and understand patterns over time‚Äînot just moments of illness‚Äîso you can feel more informed and prepared for important medical conversations. To keep your health information protected and secure, Health operates as a separate space with enhanced privacy to protect sensitive data. Conversations in Health are not used to train our foundation models. If you start a health-related conversation in ChatGPT, we‚Äôll suggest moving into Health for these additional protections.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre interested in getting access as it becomes available, you can sign up for the waitlist(opens in a new window). We‚Äôre starting by providing access to a small group of early users to learn and continue refining the experience‚Äîusers with ChatGPT Free, Go, Plus, and Pro plans outside of the European Economic Area, Switzerland, and the United Kingdom are eligible. As we make improvements, we plan to expand access and make Health available to all users on web and iOS in the coming weeks.&lt;/p&gt;
    &lt;p&gt;Medical record integrations and some apps are available in the U.S. only, and connecting Apple Health requires iOS.&lt;/p&gt;
    &lt;p&gt;Your health information is deeply personal. That‚Äôs why Health is built as a dedicated space with added protections for sensitive health information and easy-to-use controls.&lt;/p&gt;
    &lt;p&gt;Health lives in its own space within ChatGPT, where your conversations, connected apps, and files are stored separately from your other chats. Health has separate memories, ensuring that your health context stays contained within the space. You‚Äôll still see health chats in your chat history so you can easily return to them, but the information itself stays within Health.&lt;/p&gt;
    &lt;p&gt;When helpful, ChatGPT may use context from your non-Health chats‚Äîlike a recent move or lifestyle change‚Äîto make a health conversation more relevant. However, Health information and memories never flow back into your non-Health chats, and conversations outside of Health can‚Äôt access files, conversations, or memories created within Health. You can view or delete Health memories at any time within Health or the ‚ÄúPersonalization‚Äù section of Settings.&lt;/p&gt;
    &lt;p&gt;We recognize that people share personal and sensitive information with ChatGPT. That understanding shapes how we design the security, privacy, and data controls for all of our products‚Äîfrom the start. Even before introducing ChatGPT Health, we built foundational protections across ChatGPT to give you meaningful control over your data, including temporary chats, the ability to delete chats from OpenAI‚Äôs systems within 30 days, and training our models not to retain personal information from user chats.&lt;/p&gt;
    &lt;p&gt;Conversations and files across ChatGPT are encrypted by default at rest and in transit as part of our core security architecture. Due to the sensitive nature of health data, Health builds on this foundation with additional, layered protections‚Äîincluding purpose-built encryption and isolation‚Äîto keep health conversations protected and compartmentalized. Conversations in Health are not used to train our foundation models.&lt;/p&gt;
    &lt;p&gt;You can further strengthen access controls by enabling multi-factor authentication (MFA)(opens in a new window), which adds an extra layer of protection to help prevent unauthorized access.&lt;/p&gt;
    &lt;p&gt;When you choose to connect your health data, such as medical records or wellness apps, your responses are grounded in your own health information. To enable access to trusted U.S. healthcare providers, we partner with b.well, the largest and most secure network of live, connected health data for U.S. consumers. b.well adheres to the highest industry standards in data security and privacy. You can remove access to medical records at any time in the "Apps" section of Settings.&lt;/p&gt;
    &lt;p&gt;You can also connect your Apple Health information and other wellness apps, such as Function and MyFitnessPal. Apps may only be connected to your health data with your explicit permission, even if they‚Äôre already connected to ChatGPT for conversations outside of Health. All apps available in Health must meet OpenAI‚Äôs privacy and security requirements, including collecting only the minimum data needed, and undergo additional security review specific to inclusion in Health. The first time you connect an app, we‚Äôll help you understand what types of data may be collected by the third party. And you‚Äôre always in control: disconnect an app at any time and it immediately loses access.&lt;/p&gt;
    &lt;p&gt;ChatGPT Health was developed in close collaboration with physicians around the world to provide clear and useful health information.&lt;/p&gt;
    &lt;p&gt;Over two years, we‚Äôve worked with more than 260 physicians who have practiced in 60 countries and dozens of specialties to understand what makes an answer to a health question helpful or potentially harmful‚Äîthis group has now provided feedback on model outputs over 600,000 times across 30 areas of focus. This collaboration has shaped not just what Health can do, but how it responds: how urgently to encourage follow-ups with a clinician, how to communicate clearly without oversimplifying, and how to prioritize safety in moments that matter.&lt;/p&gt;
    &lt;p&gt;This physician-led approach is built directly into the model that powers Health, which is evaluated against clinical standards using HealthBench, an assessment framework we created with input from our network of practicing physicians. Rather than relying on exam-style questions or generic accuracy checks, HealthBench evaluates responses using physician-written rubrics that reflect how clinicians judge quality in practice‚Äîprioritizing safety, clarity, appropriate escalation of care, and respect for individual context.&lt;/p&gt;
    &lt;p&gt;This evaluation-driven approach helps ensure the model performs well on the tasks people actually need help with, including explaining lab results in accessible language, preparing questions for an appointment, interpreting data from wearables and wellness apps, and summarizing care instructions. The result is support that people can trust‚Äîalways designed to support, not replace, your healthcare providers.&lt;/p&gt;
    &lt;p&gt;You can sign up for the waitlist(opens in a new window) to request access.&lt;/p&gt;
    &lt;p&gt;Select ‚ÄòHealth‚Äô from the sidebar menu in ChatGPT.&lt;/p&gt;
    &lt;p&gt;Bring your medical records and the apps you use to track your health and wellness into Health. You can upload files directly, connect from tools (+) or ‚ÄúApps‚Äù in Settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New: Medical Records for lab results, visit summaries, and clinical history&lt;/item&gt;
      &lt;item&gt;New: Apple Health for health and fitness data, including movement, sleep, and activity patterns (must be on iOS to sync)&lt;/item&gt;
      &lt;item&gt;New: Function for lab test insights, nutrition ideas, and taking action on your health&lt;/item&gt;
      &lt;item&gt;New: MyFitnessPal for nutrition advice, macros, and recipes&lt;/item&gt;
      &lt;item&gt;New: Weight Watchers for GLP-1 personalized meal ideas, recipes, and food guidance&lt;/item&gt;
      &lt;item&gt;AllTrails to help you find your next hike&lt;/item&gt;
      &lt;item&gt;Instacart to turn meal plans into shoppable lists&lt;/item&gt;
      &lt;item&gt;Peloton for suggested workout classes or guided meditations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Health conversations feel just like chatting with ChatGPT‚Äîbut grounded in the information you‚Äôve connected. You can upload photos and files and use search, deep research, voice mode and dictation. When relevant, ChatGPT can automatically reference your connected information to provide more relevant and personalized responses. For example, you might ask: ‚ÄúHow‚Äôs my cholesterol trending?‚Äù or ‚ÄúCan you summarize my latest bloodwork before my appointment?‚Äù To use a connected app you can start your question with it, select it from tools (+) or ChatGPT may suggest one when helpful.&lt;/p&gt;
    &lt;p&gt;You can add custom instructions in Health to help ChatGPT know what to focus on, to avoid mentioning sensitive topics, or change how responses are framed. These instructions only apply to Health chats, and you can update or remove any time in Health or Settings.&lt;/p&gt;
    &lt;p&gt;We‚Äôll continue to expand what you can connect and the insights Health can support‚Äîso ChatGPT can help you feel more informed, prepared, and confident as you navigate your health.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-chatgpt-health/"/><published>2026-01-07T19:29:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531794</id><title>Claude Code Emergent Behavior: When Skills Combine</title><updated>2026-01-07T20:44:37.281500+00:00</updated><content>&lt;doc fingerprint="3d440486fbd7457c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Emergent Behavior: When Skills Combine&lt;/head&gt;
    &lt;p&gt;I ran across something interesting today. Claude can combine skills! You might be thinking, "huh?", here's what I mean.&lt;/p&gt;
    &lt;p&gt;I've been vibe coding like a fiend lately, experimenting and learning the flow. Working on ways to make AI more successful. First thing I built? A tiling window manager for macOS. Because that's obviously the first thing to build when your goal is world domination.&lt;/p&gt;
    &lt;p&gt;For something I didn't write, I was happy with the performance. Not winning any awards, but feels responsive to me. Still, I knew there had to be plenty of room for improvement in that hellscape of a repo.&lt;/p&gt;
    &lt;p&gt; With this in mind, Claude and I colluded on a new &lt;code&gt;optimize-critical-path&lt;/code&gt; skill. You can guess what it does. First path? &lt;code&gt;thegrid focus next&lt;/code&gt; command. I kicked off the agent and told it to look at this action and do its thing.
          &lt;/p&gt;
    &lt;p&gt;Claude was all too happy to use this new skill. It started kicking off subagents and tokens were flying across my screen like a scene from a worse, more dystopian version of the Matrix. I noticed it was thrashing and making assumptions about performance. I heard a voice in my head that sounded like Obi-Wan, telling me that we should always measure first.&lt;/p&gt;
    &lt;p&gt;Then I got an idea. Previously I hacked together a debug skill that adds logs and helps trace issues. I thought to myself, can I combine skills?&lt;/p&gt;
    &lt;code&gt;&amp;gt; what is another way we can approach this. Could we combine optimize critical path skill with my oberdebug?

√¢ Good idea - let's use oberdebug's hypothesis-driven approach to find the actual bottleneck:

/oberdebug

√¢ Combining the two approaches. Following oberdebug's structure:

Step 1: Issue Statement

Symptom: Focus operation takes ~130ms, target is &amp;lt;80ms
Component: IPC path (CLI -&amp;gt; Server -&amp;gt; Response)
Evidence so far:
- Border pooling works (100% hit rate) √¢
- dump alone takes ~82ms (~63ms overhead beyond baseline IPC)
- JSON serialization is only ~1ms - NOT the bottleneck
- Something else in the IPC round-trip is slow

Step 2: Reproduction

YES - Easily reproducible with `thegrid focus left`

Step 3: Dispatch Discovery Agents

√¢ 3 Explore agents launched (ctrl+o to expand)
√¢√¢ Trace IPC timing path √Ç¬∑ Running in background
√¢  √¢√¢ Launched
√¢√¢ Check git history for IPC √Ç¬∑ Running in background
√¢  √¢√¢ Launched
√¢√¢ Find timing instrumentation √Ç¬∑ Running in background
   √¢√¢ Launched

√¢ Bash(rm -f /Users/r/.config/bin/claude-debug-bp7x.py 2&amp;gt;/dev/null; echo "cleaned up")
√¢√¢ cleaned up

√¢ Let me get the agent results:

Task Output af35ff&lt;/code&gt;
    &lt;p&gt;Ok, now we're talking. Burn those sweet sweet tokens optimizing that path! I know this is only the diagnosis part. It was evident my original skill was really good at coming up with solutions to optimize, but I messed up and it wasn't good at finding what to optimize.&lt;/p&gt;
    &lt;p&gt; Thankfully &lt;code&gt;oberdebug&lt;/code&gt; was built to do this. It uses the lord's method of solving complex bugs. Add a log statement.
          &lt;/p&gt;
    &lt;code&gt;142        state.serverCommit = GridServerCommit

143
144 +      let t4 = CFAbsoluteTimeGetCurrent()
145        let response = Response(
146            id: request.id,
147            result: AnyCodable(state)
148        )
149 +      let t5 = CFAbsoluteTimeGetCurrent()
150 +
151 +      // [OBERDEBUG-001] Timing breakdown for dump operation
152 +      JSONLogger.shared.log("dbg.dump.timing", data: [
153 +          "task_spawn_ms": Int((t1 - t0) * 1000),
154 +          "state_fetch_ms": Int((t3 - t2) * 1000),
155 +          "anycod_wrap_ms": Int((t5 - t4) * 1000),
156 +          "total_ms": Int((t5 - t0) * 1000)
157 +      ])
158 +
159        completion(response)
160    } catch {
161        JSONLogger.shared.log("err.state", data: ["op": "dump", "error": "\(error)"])

Now rebuild and test:

Bash(make run 2&amp;gt;&amp;amp;1 | tail -20)
√¢√¢ Building grid-cli...
   Building thegrid v0.2.7...
   go build -v -ldflags "-X main.Version=0.2.7 -X main.Commit=2f0eff915d96b4f66411da07a8b8939543a5b693"
   √¢¬¶ +17 lines (ctrl+o to expand)&lt;/code&gt;
    &lt;p&gt;Since this app has a CLI, I take my hands off the wheel and smile as Claude runs a mini performance test using the CLI. I just see some AI randomly changing focus on my monitor. I'm trying to think, how does it come up with the order? Anyway, topic for another day.&lt;/p&gt;
    &lt;p&gt;YEAHHHH BUDDY&lt;/p&gt;
    &lt;code&gt;√¢ FOUND IT!

√¢ Step 7: Evidence Analysis

| Hypothesis                    | Evidence                                 | Verdict    |
|------------------------------|-------------------------------------------|------------|
| Task spawning (~30√¢40ms)     | task_spawn_us: 17√¢46 (0.05ms)              | REFUTED    |
| AnyCodable wrap (~20√¢30ms)   | anycod_wrap_us: 2√¢5 (0.005ms)              | REFUTED    |
| Response JSON encoding       | encode_us: 54,720√¢60,198 (55√¢60ms)         | CONFIRMED  |

---

Step 8: ROOT CAUSE CONFIRMED

ROOT CAUSE CONFIRMED:
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢

Issue:      dump IPC takes ~82ms, target is &amp;lt;20ms
Location:   grid-server/Sources/GridServer/SocketServer.swift:238
Evidence:   encode_us: 54,720√¢60,198√Ç¬µs (55√¢60ms)
            bytes: 122,699 (120KB payload)

Cause:      Swift's JSONEncoder.encode() serializing the full
            Response struct (containing WindowManagerState with
            ~248 windows) takes 55√¢60ms per call.

√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢

Time Breakdown (verified)

| Phase                           | Time     | Notes        |
|---------------------------------|----------|--------------|
| Task spawn                      | 0.05ms   | Negligible   |
| StateManager.getStateDictionary() | 0.005ms | Negligible   |&lt;/code&gt;
    &lt;p&gt;So that was nice! The optimize skill then kicked in and it proceeded to recommend different things, and I had some of my own ideas as well. But the moral of the story is combining skills. Like any human with a shiny new hammer, I needed to go find some new nails!&lt;/p&gt;
    &lt;p&gt; With that in mind, I thought about &lt;code&gt;brainstorming&lt;/code&gt;, which I use a lot, and I'd recently been playing around with &lt;code&gt;frontend-design&lt;/code&gt;. Those two seemed to pair very nicely. This was the original prompt that created this blog.
          &lt;/p&gt;
    &lt;code&gt;√¢ That's a creative approach! You want the design expertise baked into the brainstorming itself rather than sequential steps.

We can try it √¢ invoke frontend-design and prompt it to run the brainstorming workflow with design sensibilities guiding the exploration. That way design thinking shapes the questions and options from the start, rather than being applied after decisions are made.

Let's give it a shot:

√¢ Skill(frontend-design:frontend-design)

√¢ Now let me layer in the brainstorming skill to combine both:

√¢ Skill(superpowers:brainstorming)

√¢ Now we have both active √¢ design sensibility guiding the brainstorm.

Since this is a fresh project (no existing codebase to explore), let's dive into understanding your vision. I'll ask one question at a time, design-informed.

---

First question: What's the blog's core purpose?

This shapes everything √¢ the aesthetic, the features, the tone.

| Option                 | Description                                                                                                  |
|------------------------|--------------------------------------------------------------------------------------------------------------|
| A. Personal/Creative   | Your thoughts, essays, creative writing. The blog is the experience √¢ personality-forward, expressive design matters. |
| B. Technical/Dev       | Tutorials, code snippets, engineering posts. Readability and code presentation are paramount.              |
| C. Portfolio/Professional | Showcasing work, building reputation. Polished, memorable, makes an impression.                        |
| D. Publication/Magazine| Multi-author or editorial style. Content density, navigation, discoverability.                             |
| E. Something else      | Tell me more.                                                                                                |

Which resonates most?

&amp;gt; A

√¢ Personal/Creative √¢ where the blog is the experience. This opens up the full design spectrum.&lt;/code&gt;
    &lt;p&gt; As I was working on the blog, I came across another opportunity to pair skills. I combine &lt;code&gt;code-review&lt;/code&gt; skill with everything now. I'll even kick off parallel subagents using two different &lt;code&gt;code-review&lt;/code&gt; skills combined with other skills. Doing something like &lt;code&gt;code-review&lt;/code&gt; through a &lt;code&gt;frontend-design&lt;/code&gt; lens has been extremely helpful as I built this blog.
          &lt;/p&gt;
    &lt;p&gt;I'm curious what other new combinations are out there to discover.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vibeandscribe.xyz/posts/2025-01-07-emergent-behavior.html"/><published>2026-01-07T20:06:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531925</id><title>Tailscale state file encryption no longer enabled by default</title><updated>2026-01-07T20:44:36.904476+00:00</updated><content>&lt;doc fingerprint="868f307d7f22bbd0"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Tailscale v1.92.5&lt;/head&gt;Update instructions&lt;head rend="h5"&gt;Linux&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;State file encryption and hardware attestation keys are no longer enabled by default.&lt;/item&gt;&lt;item&gt;Failure to load hardware attestation keys no longer prevents the client from starting. This could happen when the TPM device is reset or replaced.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h5"&gt;Windows&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;State file encryption and hardware attestation keys are no longer enabled by default.&lt;/item&gt;&lt;item&gt;Failure to load hardware attestation keys no longer prevents the client from starting. This could happen when the TPM device is reset or replaced.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale container image v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale container image is available. You can download it from Docker Hub or from our GitHub packages repository.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Hardware attestation keys are no longer added to Kubernetes state &lt;code&gt;Secrets&lt;/code&gt;, making it possible to change the Kubernetes node the Tailscale containers are deployed on.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale Kubernetes Operator v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale Kubernetes Operator is available. For guidance on installing and updating, refer to our installation instructions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Certificate renewal is no longer done as an ARI order by default to avoid renewal failure if ACME account keys are recreated.&lt;/item&gt;&lt;item&gt;Hardware attestation keys are no longer added to Kubernetes state &lt;code&gt;Secrets&lt;/code&gt;, making it possible to change the Kubernetes node the Tailscale Kubernetes Operator is deployed on.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale tsrecorder v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale &lt;code&gt;tsrecorder&lt;/code&gt; is available. You can download it from Docker Hub.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Note: This version contains no changes except for library updates.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tailscale.com/changelog"/><published>2026-01-07T20:16:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46532075</id><title>Claude Code CLI Broken</title><updated>2026-01-07T20:44:36.804202+00:00</updated><content>&lt;doc fingerprint="171657deccbe91fa"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 3.8k&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Closed as duplicate of#16682&lt;/p&gt;
    &lt;p&gt;Labels&lt;/p&gt;
    &lt;p&gt;bugSomething isn't workingSomething isn't workingduplicateThis issue or pull request already existsThis issue or pull request already existshas reproHas detailed reproduction stepsHas detailed reproduction stepsplatform:macosIssue specifically occurs on macOSIssue specifically occurs on macOS&lt;/p&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Preflight Checklist&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I have searched existing issues and this hasn't been reported yet&lt;/item&gt;
      &lt;item&gt;This is a single bug report (please file separate reports for different bugs)&lt;/item&gt;
      &lt;item&gt;I am using the latest version of Claude Code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What's Wrong?&lt;/head&gt;
    &lt;p&gt;update to claude 2.1.0 then run claude. see the error.&lt;/p&gt;
    &lt;head rend="h3"&gt;What Should Happen?&lt;/head&gt;
    &lt;p&gt;claude should start when using version 2.1.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Error Messages/Logs&lt;/head&gt;
    &lt;head rend="h3"&gt;Steps to Reproduce&lt;/head&gt;
    &lt;p&gt;update to 2.1.0 and run claude.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Model&lt;/head&gt;
    &lt;p&gt;None&lt;/p&gt;
    &lt;head rend="h3"&gt;Is this a regression?&lt;/head&gt;
    &lt;p&gt;No, this never worked&lt;/p&gt;
    &lt;head rend="h3"&gt;Last Working Version&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Code Version&lt;/head&gt;
    &lt;p&gt;2.1.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Platform&lt;/head&gt;
    &lt;p&gt;Anthropic API&lt;/p&gt;
    &lt;head rend="h3"&gt;Operating System&lt;/head&gt;
    &lt;p&gt;macOS&lt;/p&gt;
    &lt;head rend="h3"&gt;Terminal/Shell&lt;/head&gt;
    &lt;p&gt;Other&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional Information&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;p&gt;steinnes, Xplod13, BigFoxMedia, ekstro, masonhieb and 139 more&lt;/p&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h3"&gt;Assignees&lt;/head&gt;
    &lt;head rend="h3"&gt;Labels&lt;/head&gt;
    &lt;p&gt;bugSomething isn't workingSomething isn't workingduplicateThis issue or pull request already existsThis issue or pull request already existshas reproHas detailed reproduction stepsHas detailed reproduction stepsplatform:macosIssue specifically occurs on macOSIssue specifically occurs on macOS&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/anthropics/claude-code/issues/16673"/><published>2026-01-07T20:25:51+00:00</published></entry></feed>