<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-14T17:38:16.039676+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45924619</id><title>RegreSQL: Regression Testing for PostgreSQL Queries</title><updated>2025-11-14T17:38:26.018316+00:00</updated><content>&lt;doc fingerprint="6318b6147df597f4"&gt;
  &lt;main&gt;
    &lt;p&gt;TL;DR - RegreSQL brings PostgreSQL's regression testing methodology to your application queries, catching both correctness bugs and performance regressions before production.&lt;/p&gt;
    &lt;p&gt;As puzzling as it might seem, the common problem with production changes is the ever-present "AHA" moment when things start slowing down or crashing straight away. Testing isn't easy as it is, but there's a widespread practice gap when it comes to testing SQL queries. Some might pretend to "fix it" by using ORMs to abstract away the problem. Others treat SQL as "just glue code" that doesn't deserve systematic testing. Most settle for integration tests that verify the application layer works, never actually testing whether their queries will survive the next schema change or index modification.&lt;/p&gt;
    &lt;p&gt;For PostgreSQL development itself, the project has a robust regression test suite that has been preventing disasters in core development for decades. The database itself knows how to test SQL systematically - we just don't use those same techniques for our own queries. Enter RegreSQL, a tool originally created by Dimitri Fontaine for The Art of PostgreSQL book (which is excellent for understanding and mastering PostgreSQL as a database system), designed to bring the same regression testing framework to our application queries.&lt;/p&gt;
    &lt;p&gt;I've been trying to use it for some time, but due to missing features and limitations gave up several times. Until now. I decided to fork the project and spend the time needed to take it to the next level.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;The RegreSQL promise starts with the biggest strength and perceived weakness of SQL queries. They are just strings. And unless you use something like sqlc (for Go), PG'OCaml or Rust's SQLx toolkit giving you compile-time checking, your queries are validated only when they are executed. Which in better case mean either usually slow-ish test suite or integration tests, in worst scenario only when deployed. ORMs are another possibility - completely abstracting away SQL (but more on that later).&lt;/p&gt;
    &lt;p&gt;But even with compile-time checking, you are only checking for one class of problems: schema mismatches. What about behavior changes after schema migration or performance regressions? What about understanding whether your optimization actually made things faster or just moved the problem elsewhere?&lt;/p&gt;
    &lt;p&gt;This is where RegreSQL comes in. Rather than trying to turn SQL into something else, RegreSQL embraces "SQL as strings" reality and applies the same testing methodology PostgreSQL itself uses: regression testing. You write (or generate - continue reading) your SQL queries, provide input data, and RegreSQL verifies that future changes don't break those expectations.&lt;/p&gt;
    &lt;p&gt;The features don't stop there though - it tracks performance baselines, detects common query plan regressions (like sequential scans), and gives you framework for systematic experimentation with the schema changes and query change management.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic regression testing&lt;/head&gt;
    &lt;p&gt;Enough with theory. Let's jump in straight into the action and see what a sample run of RegreSQL looks like&lt;/p&gt;
    &lt;code&gt;$ regresql text
Connecting to 'postgres://radim:password123@192.168.139.28/cdstore_test'… ✓

Running regression tests...

✓ album-by-artist_list-albums-by-artist.1.json (0.00s)
✓ album-by-artist_list-albums-by-artist.2.json (0.00s)

✓ album-tracks_list-tracks-by-albumid.2.json (0.00s)
✓ album-tracks_list-tracks-by-albumid.1.json (0.00s)

✓ artist_top-artists-by-album.1.json (0.00s)

✓ genre-topn_genre-top-n.top-1.json (0.00s)
✓ genre-topn_genre-top-n.top-3.json (0.00s)

✓ genre-tracks_tracks-by-genre.json (0.00s)

Results: 8 passed, 0 failed, 8 skipped (0.00s)
&lt;/code&gt;
    &lt;p&gt;In this example based on Chinook database (as used originally in The Art of PostgreSQL book), RegreSQL scans the current directory (or one provided by &lt;code&gt;-C /path/to/project&lt;/code&gt;) for &lt;code&gt;*.sql&lt;/code&gt; files and attempts to run all queries against the configured PostgreSQL connection.&lt;/p&gt;
    &lt;p&gt;The individual files can contain either single or multiple sql queries. Like following example&lt;/p&gt;
    &lt;code&gt;-- name: top-artists-by-album
-- Get the list of the N artists with the most albums
SELECT
    artist.name,
    count(*) AS albums
FROM
    artist
    LEFT JOIN album USING (artist_id)
GROUP BY
    artist.name
ORDER BY
    albums DESC
LIMIT :n;
&lt;/code&gt;
    &lt;p&gt;The syntax for the queries supports both positional arguments (like &lt;code&gt;$1&lt;/code&gt; known from libpq library) or (preferred) &lt;code&gt;psql&lt;/code&gt; style variable (&lt;code&gt;:varname&lt;/code&gt;). The each identified query (not file) is then executed for 0..N times, based on number of predefined plans and verified to the expected results - validating the expected data matches the one returned. The support for SQL files handling is available separately with https://github.com/boringSQL/queries (Go version only for now).&lt;/p&gt;
    &lt;p&gt;This gives you what original RegreSQL tool has introduced - change your schema, refactor a query, run &lt;code&gt;regresql test&lt;/code&gt; and see immediately what broke. The test suite now has ability to catch regressions before they are committed / shipped. The current version built on top of it, giving you better console formatter instead of TAP style output, as well as jUnit, JSON and GitHub actions formatters for better integration into your CI/CD pipelines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance regression testing&lt;/head&gt;
    &lt;p&gt;Basic regression testing catches correctness issues - wrong results, broken queries, schema mismatches. But there's another class of production issues it misses. Performance regressions. No matter how unbelievable it might sound but queries get deployed without appropriate indexes, or they change over time. Simple fix - both for handwritten SQL or ORM code - can switch from milliseconds to seconds. You add index that helps one query, but tanks another. You modify conditionals and accidently force sequential scan of millions of rows. This is where it hurts.&lt;/p&gt;
    &lt;p&gt;RegreSQL addresses this by tracking performance baselines alongside correctness. Once baselines are generated&lt;/p&gt;
    &lt;code&gt;$ regresql baseline
Connecting to 'postgres://appuser:password123@192.168.139.28/cdstore_test'… ✓
Creating baselines directory: regresql/baselines
Creating directory 'regresql/baselines'

Creating baselines for queries:

  ./
  Created baseline: album-by-artist_list-albums-by-artist.1.json
  Created baseline: album-by-artist_list-albums-by-artist.2.json
  Created baseline: album-tracks_list-tracks-by-albumid.1.json
  Created baseline: album-tracks_list-tracks-by-albumid.2.json
  Created baseline: artist_top-artists-by-album.1.json
  Created baseline: genre-topn_genre-top-n.top-1.json
  Created baseline: genre-topn_genre-top-n.top-3.json
  Created baseline: genre-tracks_tracks-by-genre.json

Baselines have been created successfully!
Baseline files are stored in: regresql/baselines
&lt;/code&gt;
    &lt;p&gt;the test command not only tests the regressions to the captured times, but also detects the common bad patterns in query execution plans. For now it provides warnings for detection of sequential scans - both on their and/or with nested loops and multiple sort operations. I believe this alone might provide a valuable insights and reduce the mishaps in production. It's also a place where further development of RegreSQL will take place.&lt;/p&gt;
    &lt;p&gt;To demonstrate this, let's review the test output with the baselines.&lt;/p&gt;
    &lt;code&gt;Connecting to 'postgres://appuser:password123@192.168.139.28/cdstore_test'… ✓

Running regression tests...

✓ album-by-artist_list-albums-by-artist.1.json (0.00s)
✓ album-by-artist_list-albums-by-artist.2.json (0.00s)
✓ album-by-artist_list-albums-by-artist.1.cost (22.09 &amp;lt;= 22.09 * 110%) (0.00s)
  ⚠️  Sequential scan detected on table 'artist'
    Suggestion: Consider adding an index if this table is large or this query is frequently executed
  ⚠️  Nested loop join with sequential scan detected
    Suggestion: Add index on join column to avoid repeated sequential scans
✓ album-by-artist_list-albums-by-artist.2.cost (22.09 &amp;lt;= 22.09 * 110%) (0.00s)
  ⚠️  Sequential scan detected on table 'artist'
    Suggestion: Consider adding an index if this table is large or this query is frequently executed
  ⚠️  Nested loop join with sequential scan detected
    Suggestion: Add index on join column to avoid repeated sequential scans

✓ album-tracks_list-tracks-by-albumid.1.json (0.00s)
✓ album-tracks_list-tracks-by-albumid.2.json (0.00s)
✓ album-tracks_list-tracks-by-albumid.1.cost (8.23 &amp;lt;= 8.23 * 110%) (0.00s)
✓ album-tracks_list-tracks-by-albumid.2.cost (8.23 &amp;lt;= 8.23 * 110%) (0.00s)

✓ artist_top-artists-by-album.1.json (0.00s)
✓ artist_top-artists-by-album.1.cost (35.70 &amp;lt;= 35.70 * 110%) (0.00s)
  ⚠️  Multiple sequential scans detected on tables: album, artist
    Suggestion: Review query and consider adding indexes on filtered/joined columns

✓ genre-topn_genre-top-n.top-1.json (0.00s)
✓ genre-topn_genre-top-n.top-3.json (0.00s)
✓ genre-topn_genre-top-n.top-1.cost (6610.59 &amp;lt;= 6610.59 * 110%) (0.00s)
  ⚠️  Multiple sequential scans detected on tables: genre, artist
    Suggestion: Review query and consider adding indexes on filtered/joined columns
  ⚠️  Multiple sort operations detected (2 sorts)
    Suggestion: Consider composite indexes for ORDER BY clauses to avoid sorting
  ⚠️  Nested loop join with sequential scan detected
    Suggestion: Add index on join column to avoid repeated sequential scans
✓ genre-topn_genre-top-n.top-3.cost (6610.59 &amp;lt;= 6610.59 * 110%) (0.00s)
  ⚠️  Multiple sequential scans detected on tables: artist, genre
    Suggestion: Review query and consider adding indexes on filtered/joined columns
  ⚠️  Multiple sort operations detected (2 sorts)
    Suggestion: Consider composite indexes for ORDER BY clauses to avoid sorting
  ⚠️  Nested loop join with sequential scan detected
    Suggestion: Add index on join column to avoid repeated sequential scans

✓ genre-tracks_tracks-by-genre.json (0.00s)
✓ genre-tracks_tracks-by-genre.cost (37.99 &amp;lt;= 37.99 * 110%) (0.00s)
  ⚠️  Multiple sequential scans detected on tables: genre, track
    Suggestion: Review query and consider adding indexes on filtered/joined columns

Results: 16 passed (0.00s)
&lt;/code&gt;
    &lt;p&gt;As you can see, despite from not having baseline, RegreSQL is able to detect the basic bad patterns that should be addressed before queries can be considered "production ready".&lt;/p&gt;
    &lt;p&gt;In some cases, having the detection of sequential scans, or just tracking query costs baselines might be considered undesirable, which would lead to false positives. RegreSQL enables this to be addressed by query metadata as demonstrated below.&lt;/p&gt;
    &lt;code&gt;-- name: query_name
-- metadata: key1=value1, key2=value2
SELECT ...;
&lt;/code&gt;
    &lt;p&gt;At this point RegreSQL recognizes&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;notest&lt;/code&gt;to skip the query testing altogether (not just cost tracking)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;nobaseline&lt;/code&gt;to skip cost tracking&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;noseqscanwarn&lt;/code&gt;to keep cost tracking but disable sequential scan warnings&lt;/item&gt;
      &lt;item&gt;and &lt;code&gt;difffloattolerance&lt;/code&gt;to cost failure threshold (default 10% at the moment).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;-- name: query_name
-- regresql: notest, nobaseline
-- regresql: noseqscanwarn
-- regresql: difffloattolerance:0.25
-- query that can vary in cost by 20% without being considered a failure
SELECT ...;
&lt;/code&gt;
    &lt;head rend="h2"&gt;ORM enters the room&lt;/head&gt;
    &lt;p&gt;ORMs abstract away SQL, but they still generate it - and that generated SQL can have performance problems you won't catch until production. Consider this common scenario: you start with a simple SQLAlchemy query that works fine, then months later add eager loading for related data:&lt;/p&gt;
    &lt;code&gt;orders = (
    session.query(Order)
    .filter(Order.user_id == user_id)
    .options(
        joinedload(Order.user),
        joinedload(Order.shipping_address),
        selectinload(Order.items)  # NEW: Load order items
    )
    .all()
)
&lt;/code&gt;
    &lt;p&gt;That innocent &lt;code&gt;selectinload(Order.items)&lt;/code&gt; generates a separate query - and without an index on &lt;code&gt;order_items.order_id&lt;/code&gt;, it performs a sequential scan.&lt;/p&gt;
    &lt;p&gt;RegreSQL can catch this by intercepting ORM-generated SQL using SQLAlchemy's event system:&lt;/p&gt;
    &lt;code&gt;@event.listens_for(engine, "before_cursor_execute")
def capture_sql(conn, cursor, statement, *args):
    captured_queries.append(statement)
&lt;/code&gt;
    &lt;p&gt;Run your ORM code, capture the SQL, save it as a .sql file, and test it with RegreSQL. The performance baseline testing will flag the missing index before it hits production. This is currently experimental, but ORM integration is a key area for RegreSQL's future development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Test Data Management&lt;/head&gt;
    &lt;p&gt;Up until now we have covered how RegreSQL verifies query correctness and tracks performance regressions. But there's a critical prerequisite we've only skimmed through. Every regression test needs consistent, reproducible data. Change the data, change their cardinality, and your expected results become meaningless. Your performance baselines drift. Your tests become flaky.&lt;/p&gt;
    &lt;p&gt;Traditional approach to create test data might involve&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Database dumps become unmanageable - 500MB files you can't review, can't understand, that break with every schema migration, and whose data becomes stale as production evolves. Which version of the dump are your tests even using?&lt;/item&gt;
      &lt;item&gt;SQL scripts might be better than dumps, but still imperative and hard to maintain. You end up with INSERT statements scattered across multiple files, managing foreign keys manually, and debugging constraint violations.&lt;/item&gt;
      &lt;item&gt;Factories in application code might work great for integration tests, but we're testing SQL directly. Do you really want to maintain parallel data generation in your application language just for SQL tests?&lt;/item&gt;
      &lt;item&gt;Shared test database is the synonym for classic "works on my machine" problem. State leaks between tests. Parallel execution becomes impossible. Debugging is a nightmare.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What we need is something that's declarative (what data, not how to insert it), reproducible (similar data every time), composable (build complex scenarios from simple pieces), and scalable (from 10 rows to 100,000).&lt;/p&gt;
    &lt;p&gt;This is where next improvement in RegreSQL's fixture system comes in. Think of it as infrastructure-as-code for your test data. You describe the data you need in YAML files, and RegreSQL handles the rest - dependencies, cleanup, foreign keys, and even realistic data generation at scale.&lt;/p&gt;
    &lt;p&gt;RegreSQL's fixture system lets you define test data in YAML files stored in &lt;code&gt;regresql/fixtures/&lt;/code&gt;. Here's a simple example&lt;/p&gt;
    &lt;code&gt;  fixture: basic_users
  description: a handful of test users
  cleanup: rollback

  data:
    - table: users
      rows:
        - id: 1
          email: alice@example.com
          name: Alice Anderson
          created_at: 2024-01-15
        - id: 2
          email: bob@example.com
          name: Bob Builder
          created_at: 2024-02-20
&lt;/code&gt;
    &lt;p&gt;To use this fixture in your tests, reference it in the query's plan file (&lt;code&gt;regresql/plans/get-user.yaml&lt;/code&gt;) you can just reference the fixture&lt;/p&gt;
    &lt;code&gt;  fixtures:
    - basic_users

  "1":
    email: alice@example.com

  "2":
    email: bob@example.com
&lt;/code&gt;
    &lt;p&gt;And when you run &lt;code&gt;regresql test&lt;/code&gt;, the fixture is automatically loaded before the query executes, and cleaned up afterward. No manual setup scripts, no state leakage between tests. But it does not stop with static fixtures. When you want to test queries against realistic volumes you can use range of data generators including&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;sequences, random integer, decimal, string, uuid, email and name generators&lt;/item&gt;
      &lt;item&gt;date_between for generating random timestamps within a range&lt;/item&gt;
      &lt;item&gt;foreign key references to be able to reuse data from other table's fixtures&lt;/item&gt;
      &lt;item&gt;range to select value from predefined sources&lt;/item&gt;
      &lt;item&gt;Go template support&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; fixture: realistic_orders
  generate:
    - table: customers
      count: 1000
      columns:
        id:
          generator: sequence
          start: 1
        email:
          generator: email
          domain: shop.example.com
        name:
          generator: name
          type: full
        created_at:
          generator: date_between
          start: "2023-01-01"
          end: "2024-12-31"

    - table: orders
      count: 5000
      columns:
        id:
          generator: sequence
          start: 1
        customer_id:
          generator: int
          min: 1
          max: 1000
        amount:
          generator: decimal
          min: 10.00
          max: 999.99
          precision: 2
        order_date:
          generator: date_between
          start: "2023-01-01"
          end: "2024-12-31"
&lt;/code&gt;
    &lt;p&gt;This generates 1,000 customers and 5,000 orders with realistic-looking data - names, emails, dates, and amounts that feel production-like.&lt;/p&gt;
    &lt;p&gt;The fixtures are also stackable and can be build on top of each other. For example if you need to make sure users fixtures are created before orders fixtures, just declare the dependency (the already planned improvement is to include the support automatic foreign-key detection to avoid ID hard-coding). RegreSQL loads fixtures in dependency order and handles cleanup in reverse.&lt;/p&gt;
    &lt;code&gt;  fixture: orders_with_shipping
  depends_on:
    - basic_users

  data:
    - table: orders
      rows:
        - id: 101
          user_id: 1  # References Alice from basic_users
          total: 99.99
          status: shipped
&lt;/code&gt;
    &lt;p&gt;Should the available options for fixtures (manual data or data generators) not be enough, you always have options to use good old SQL based data generation.&lt;/p&gt;
    &lt;code&gt;  fixture: mixed_setup
  description: Combine SQL with YAML and generated data
  cleanup: rollback

  # SQL executes first (either as file or inline)
  sql:
    - file: sql/setup_schema.sql
    - inline: "INSERT INTO config (key, value) VALUES ('version', '1.0');"

  # followed YAML data
  data:
    - table: users
      rows:
        - id: 1
          email: admin@example.com

  # and finally generated data
  generate:
    - table: orders
      count: 100
      columns:
        id:
          generator: sequence
          start: 1
        user_id:
          generator: int
          min: 1
          max: 1
&lt;/code&gt;
    &lt;p&gt;RegreSQL provides commands to inspect and validate your fixtures&lt;/p&gt;
    &lt;code&gt;  # List all available fixtures
  regresql fixtures list

  # Show fixture details and dependencies
  regresql fixtures show realistic_orders

  # Validate fixture definitions
  regresql fixtures validate

  # Show dependency graph
  regresql fixtures deps

  # Apply fixture manually (for debugging)
  regresql fixtures apply basic_users
&lt;/code&gt;
    &lt;p&gt;The fixture system has been design to transforms test data from a maintenance burden into a documented, version-controlled process. Your YAML files become the single source of truth for what data your tests need, making it easy to understand test scenarios and maintain test data as the application evolves.&lt;/p&gt;
    &lt;head rend="h2"&gt;RegreSQL future&lt;/head&gt;
    &lt;p&gt;Introducing a new open source project is an ambitious goal, and RegreSQL is just starting up. Despite the fork being in works for almost 2 years. In coming weeks and months I plan further improvements, as well as better documentation and more tutorials. The project is maintained as part of my boringSQL brand, where it's vital component (together with pgTap) for building SQL Labs which (as I sincerely hope) will provide a foundation for its further development.&lt;/p&gt;
    &lt;p&gt;At the same time RegreSQL is an attempt to give back to welcoming PostgreSQL community, make developer user experience slightly better if possible and (just maybe) provide one more argument against the case that SQL queries are not testable.&lt;/p&gt;
    &lt;p&gt;RegreSQL is available at GitHub - feel free to open issue, or drop me email about the project at radim@boringsql.com or connect on LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://boringsql.com/posts/regresql-testing-queries/"/><published>2025-11-14T07:10:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45925890</id><title>Show HN: Encore – Type-safe back end framework that generates infra from code</title><updated>2025-11-14T17:38:25.290149+00:00</updated><content>&lt;doc fingerprint="f167ae19572d887a"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt; Open Source Framework for creating type-safe distributed systems with declarative infrastructure&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Backend Frameworks: Encore.ts and Encore.go simplify creating microservices and type-safe APIs, and provide an AI-ready declarative approach to define infrastructure in code.&lt;/item&gt;
      &lt;item&gt;Local Development: Encore's CLI automatically manages local infrastructure and provides a development dashboard with tracing, service catalog, and architecture diagrams.&lt;/item&gt;
      &lt;item&gt;Infrastructure Integration: Simplified integration with cloud infrastructure using the open source CLI (learn more), or using the optional Encore Cloud platform to automate DevOps and infrastructure provisioning in your own cloud on AWS and GCP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;⭐ Star this repository to help spread the word.&lt;/p&gt;
    &lt;p&gt;Install Encore:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: &lt;code&gt;brew install encoredev/tap/encore&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux: &lt;code&gt;curl -L https://encore.dev/install.sh | bash&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows: &lt;code&gt;iwr https://encore.dev/install.ps1 | iex&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create your first app:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TypeScript: &lt;code&gt;encore app create --example=ts/hello-world&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Go: &lt;code&gt;encore app create --example=hello-world&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add Encore LLM instructions to your app:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encore.ts: ts_llm_instructions.txt&lt;/item&gt;
      &lt;item&gt;Encore.go: go_llm_instructions.txt&lt;/item&gt;
      &lt;item&gt;How to use: &lt;list rend="ul"&gt;&lt;item&gt;Cursor: Rename the file to &lt;code&gt;.cursorrules&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;GitHub Copilot: Paste content in &lt;code&gt;.github/copilot-instructions.md&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;For other tools, place the file in your app root.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Cursor: Rename the file to &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore's open source backend frameworks Encore.ts and Encore.go enable you to define resources like services, databases, cron jobs, and Pub/Sub, as type-safe objects in your application code.&lt;/p&gt;
    &lt;p&gt;With the frameworks you only define infrastructure semantics — the things that matter to your application's behavior — not configuration for specific cloud services. Encore parses your application and builds a graph of both its logical architecture and its infrastructure requirements, it then automatically generates boilerplate and helps orchestrate the relevant infrastructure for each environment. This means the same application code can be used to run locally, test in preview environments, and deploy to cloud environments on e.g. AWS and GCP.&lt;/p&gt;
    &lt;p&gt;This often removes the need for separate infrastructure configuration like Terraform, increases standardization in both your codebase and infrastructure, and makes your application highly portable across cloud providers.&lt;/p&gt;
    &lt;p&gt;Encore is fully open source, there is no proprietary code running in your application.&lt;/p&gt;
    &lt;p&gt;Defining microservices and API endpoints is incredibly simple—with less than 10 lines of code, you can create a production-ready, deployable service.&lt;/p&gt;
    &lt;p&gt;Hello World in Encore.ts&lt;/p&gt;
    &lt;code&gt;import { api } from "encore.dev/api";

export const get = api(
  { expose: true, method: "GET", path: "/hello/:name" },
  async ({ name }: { name: string }): Promise&amp;lt;Response&amp;gt; =&amp;gt; {
    const msg = `Hello ${name}!`;
    return { message: msg };
  }
);

interface Response {
  message: string;
}&lt;/code&gt;
    &lt;p&gt;Hello World in Encore.go&lt;/p&gt;
    &lt;code&gt;package hello

//encore:api public path=/hello/:name
func World(ctx context.Context, name string) (*Response, error) {
	msg := fmt.Sprintf("Hello, %s!", name)
	return &amp;amp;Response{Message: msg}, nil
}

type Response struct {
	Message string
}&lt;/code&gt;
    &lt;p&gt;If you want a Pub/Sub Topic, you declare it directly in your application code and Encore will integrate the infrastructure and generate the boilerplate code necessary. Encore supports the following Pub/Sub infrastructure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NSQ for local environments (automatically provisioned by Encore's CLI)&lt;/item&gt;
      &lt;item&gt;GCP Pub/Sub for environments on GCP&lt;/item&gt;
      &lt;item&gt;SNS/SQS for environments on AWS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using Pub/Sub in Encore.ts&lt;/p&gt;
    &lt;code&gt;import { Topic } "encore.dev/pubsub"

export interface SignupEvent {
    userID: string;
}

export const signups = new Topic&amp;lt;SignupEvent&amp;gt;("signups", {
    deliveryGuarantee: "at-least-once",
});&lt;/code&gt;
    &lt;p&gt;Using Pub/Sub in Encore.go&lt;/p&gt;
    &lt;code&gt;import "encore.dev/pubsub"

type User struct { /* fields... */ }

var Signup = pubsub.NewTopic[*User]("signup", pubsub.TopicConfig{
  DeliveryGuarantee: pubsub.AtLeastOnce,
})

// Publish messages by calling a method
Signup.Publish(ctx, &amp;amp;User{...})&lt;/code&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For more info: See example apps: Example Apps Repo&lt;/p&gt;
    &lt;p&gt;See products being build with Encore: Showcase&lt;/p&gt;
    &lt;p&gt;Have questions? Join the friendly developer community on Discord.&lt;/p&gt;
    &lt;p&gt;Talk to a human: Book a 1:1 demo with one of our founders.&lt;/p&gt;
    &lt;p&gt;Watch the intro video for a quick introduction to Encore concepts &amp;amp; code examples.&lt;/p&gt;
    &lt;p&gt;Building scalable applications with cloud services is powerful but often frustrating. Developers face complex setups and repetitive tasks that slow them down.&lt;/p&gt;
    &lt;p&gt;Encore solves this with an all-in-one backend development toolkit, streamlining everything from local testing to cloud integration and DevOps.&lt;/p&gt;
    &lt;p&gt;See how to use the backend frameworks in the docs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Services: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;APIs: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Databases: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Cron Jobs: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Pub/Sub: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Object Storage: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Caching: Go / TypeScript (Coming soon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore provides purpose-built tooling for each step in the development process, from local development and testing, to cloud DevOps. Here we'll cover the key features for each part of the process.&lt;/p&gt;
    &lt;p&gt;When you run your app locally using the Encore CLI, Encore parses your code and automatically sets up the necessary local infrastructure on the fly. No more messing around with Docker Compose!&lt;/p&gt;
    &lt;p&gt;You also get built-in tools for an efficient workflow when creating distributed systems and event-driven applications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local environment matches cloud: Encore automatically handles the semantics of service communication and interfacing with different types of infrastructure services, so that the local environment is a 1:1 representation of your cloud environment.&lt;/item&gt;
      &lt;item&gt;Cross-service type-safety: When building microservices applications with Encore, you get type-safety and auto-complete in your IDE when making cross-service API calls.&lt;/item&gt;
      &lt;item&gt;Type-aware infrastructure: With Encore, infrastructure like Pub/Sub queues are type-aware objects in your program. This enables full end-to-end type-safety when building event-driven applications.&lt;/item&gt;
      &lt;item&gt;Secrets management: Built-in secrets management for all environments.&lt;/item&gt;
      &lt;item&gt;Tracing: The local development dashboard provides local tracing to help understand application behavior and find bugs.&lt;/item&gt;
      &lt;item&gt;Automatic API docs &amp;amp; clients: Encore generates API docs and API clients in Go, TypeScript, JavaScript, and OpenAPI specification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here's a video showing the local development dashboard:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;localdashvideo.2.mp4&lt;/head&gt;
    &lt;p&gt;Encore comes with several built-in tools to help with testing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built-in service/API mocking: Encore provides built-in support for mocking API calls, and interfaces for automatically generating mock objects for your services.&lt;/item&gt;
      &lt;item&gt;Local test infra: When running tests locally, Encore automatically provides dedicated test infrastructure to isolate individual tests.&lt;/item&gt;
      &lt;item&gt;Local test tracing: The Local Development Dashboard provides distributed tracing for tests, providing great visibility into what's happening and making it easier to understand why a test failed.&lt;/item&gt;
      &lt;item&gt;Preview Environments: When using Encore Cloud (optional), it automatically provisions a temporary cloud-based Preview Environment for each Pull Request, an effective tool when doing end-to-end testing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore Cloud is Encore's managed service offering for teams wanting to focus their engineering effort on their product development, avoiding investing time in platformization and DevOps.&lt;/p&gt;
    &lt;p&gt;Encore Cloud provides automatic infrastructure provisioning in your cloud on AWS &amp;amp; GCP. So instead of writing Terraform, YAML, or clicking in cloud consoles, you connect your cloud account and simply deploy your application. Since using Encore's open source backend frameworks means your application code is cloud agnostic and not tied to any specific infrastructure services, Encore Cloud enables you to change your infrastructure depending on your evolving needs, without needing to make code changes or manually update infrastructure config files.&lt;/p&gt;
    &lt;p&gt;When you deploy, Encore Cloud automatically provisions infrastructure using battle-tested cloud services on AWS and GCP, such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compute: GCP Cloud Run, AWS Fargate, Kubernetes (GKE and EKS)&lt;/item&gt;
      &lt;item&gt;Databases: GCP Cloud SQL, AWS RDS&lt;/item&gt;
      &lt;item&gt;Pub/Sub: GCP Pub/Sub, AWS SQS/SNS&lt;/item&gt;
      &lt;item&gt;Caches: GCP Memorystore, Amazon ElastiCache&lt;/item&gt;
      &lt;item&gt;Object Storage: GCS, Amazon S3&lt;/item&gt;
      &lt;item&gt;Secrets: GCP Secret Manager, AWS Secrets Manager&lt;/item&gt;
      &lt;item&gt;Etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore Cloud also includes cloud versions of Encore's built-in development tools:&lt;/p&gt;
    &lt;p&gt;Here's a video showing the Encore Cloud dashboard:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;envs.mp4&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Faster Development: Encore streamlines the development process with its backend frameworks, clear abstractions, and built-in local development tools.&lt;/item&gt;
      &lt;item&gt;Scalability &amp;amp; Performance: Encore simplifies building large-scale microservices applications that can handle growing user bases and demands, without the normal boilerplate and complexity.&lt;/item&gt;
      &lt;item&gt;Control &amp;amp; Standardization: Built-in tools like automated architecture diagrams, infrastructure tracking and approval workflows, make it easy for teams and leaders to get an overview of the entire application.&lt;/item&gt;
      &lt;item&gt;Security &amp;amp; Compliance: Encore Cloud helps ensure your application is secure and compliant by enforcing security standards like least privilege IAM, and provisioning infrastructure according to best practices for each cloud provider.&lt;/item&gt;
      &lt;item&gt;Reduced Costs: Encore Cloud's automatic infrastructure management minimizes wasteful cloud expenses and reduces DevOps workload, allowing you to work more efficiently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore is designed to give teams a productive and less complex experience when solving most backend use cases. Many teams use Encore to build things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High-performance B2B Platforms&lt;/item&gt;
      &lt;item&gt;Fintech &amp;amp; Consumer apps&lt;/item&gt;
      &lt;item&gt;Global E-commerce marketplaces&lt;/item&gt;
      &lt;item&gt;Microservices backends for SaaS applications and mobile apps&lt;/item&gt;
      &lt;item&gt;And much more...&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1. Install Encore: &lt;list rend="ul"&gt;&lt;item&gt;macOS: &lt;code&gt;brew install encoredev/tap/encore&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Linux: &lt;code&gt;curl -L https://encore.dev/install.sh | bash&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Windows: &lt;code&gt;iwr https://encore.dev/install.ps1 | iex&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;macOS: &lt;/item&gt;
      &lt;item&gt;2. Create your first app: &lt;list rend="ul"&gt;&lt;item&gt;TypeScript: &lt;code&gt;encore app create --example=ts/introduction&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Go: &lt;code&gt;encore app create --example=hello-world&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;TypeScript: &lt;/item&gt;
      &lt;item&gt;3. Star the project on GitHub to stay up-to-date&lt;/item&gt;
      &lt;item&gt;4. Explore the Documentation to learn more about Encore's features&lt;/item&gt;
      &lt;item&gt;5. Join Discord to ask questions and meet other Encore developers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything needed to develop and deploy Encore applications is Open Source, including the backend frameworks, parser, compiler, runtime, and CLI. This includes all code needed for local development and everything that runs in your application when it is deployed.&lt;/p&gt;
    &lt;p&gt;The Open Source CLI also provides a mechanism to generate a Docker images for your application, so you easily self-host your application. Learn more in the docs.&lt;/p&gt;
    &lt;p&gt;Developers building with Encore are forward-thinkers who want to focus on creative programming and building great software to solve meaningful problems. It's a friendly place, great for exchanging ideas and learning new things! Join the conversation on Discord.&lt;/p&gt;
    &lt;p&gt;We rely on your contributions and feedback to improve Encore for everyone who is using it. Here's how you can contribute:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;⭐ Star and watch this repository to help spread the word and stay up to date.&lt;/item&gt;
      &lt;item&gt;Meet fellow Encore developers and chat on Discord.&lt;/item&gt;
      &lt;item&gt;Follow Encore on Twitter.&lt;/item&gt;
      &lt;item&gt;Share feedback or ask questions via email.&lt;/item&gt;
      &lt;item&gt;Leave feedback on the Public Roadmap.&lt;/item&gt;
      &lt;item&gt;Send a pull request here on GitHub with your contribution.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intro: Encore concepts &amp;amp; features&lt;/item&gt;
      &lt;item&gt;Demo video: Getting started with Encore.ts&lt;/item&gt;
      &lt;item&gt;Demo: Building and deploying a simple Go service&lt;/item&gt;
      &lt;item&gt;Demo: Building an event-driven system in Go&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;Encore.example.1.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;indexvideo_5.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;index_1.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;envs.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;indexxxxx.mp4&lt;/head&gt;
    &lt;p&gt;Encore was founded by long-time backend engineers from Spotify, Google, and Monzo with over 50 years of collective experience. We’ve lived through the challenges of building complex distributed systems with thousands of services, and scaling to hundreds of millions of users.&lt;/p&gt;
    &lt;p&gt;Encore grew out of these experiences and is a solution to the frustrations that came with them: unnecessary crippling complexity and constant repetition of undifferentiated work that suffocates the developer’s creativity. With Encore, we want to set developers free to achieve their creative potential.&lt;/p&gt;
    &lt;p&gt;For individual developers building for the cloud, Encore provides a radically improved experience. With Encore you’re able to stay in the flow state and experience the joy and creativity of building.&lt;/p&gt;
    &lt;p&gt;For startup teams who need to build a scalable backend to support the growth of their product, Encore lets them get up and running in the cloud within minutes. It lets them focus on solving the needs of their users, instead of spending most of their time re-solving the everyday challenges of building distributed systems in the cloud.&lt;/p&gt;
    &lt;p&gt;For individual teams in large organizations that want to focus on innovating and building new features, Encore lets them stop spending time on operations and onboarding new team members. Using Encore for new feature development is easy, just spin up a new backend service in a few minutes.&lt;/p&gt;
    &lt;p&gt;Encore is the only tool that understands what you’re building. Encore uses static analysis to deeply understand the application you’re building. This enables a unique developer experience that helps you stay in the flow as you’re building. For instance, you don't need to bother with configuring and managing infrastructure, setting up environments and keeping them in sync, or writing documentation and drafting architecture diagrams. Encore does all of this automatically out of the box.&lt;/p&gt;
    &lt;p&gt;We've found that to meaningfully improve the developer experience, you have to operate across the full stack. Unless you understand how an application is deployed, there are a large number of things in the development process that you can't simplify. That's why so many other developer tools have such a limited impact. With Encore's more integrated approach, we're able to unlock a radically better experience for developers.&lt;/p&gt;
    &lt;p&gt;Encore is designed to let you go outside of the framework when you want to, and easily drop down in abstraction level when you need to, so you never run into any dead-ends.&lt;/p&gt;
    &lt;p&gt;Should you want to migrate away, it's straightforward and does not require a big rewrite. 99% of your code is regular Go or TypeScript.&lt;/p&gt;
    &lt;p&gt;Encore provides tools for self-hosting your application, by using the Open Source CLI to produce a standalone Docker image that can be deployed anywhere you'd like.&lt;/p&gt;
    &lt;p&gt;See CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/encoredev/encore"/><published>2025-11-14T11:41:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45925996</id><title>Show HN: European Tech News in 6 Languages</title><updated>2025-11-14T17:38:24.211362+00:00</updated><content>&lt;doc fingerprint="ed925a513922ac49"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;News&lt;/head&gt;
    &lt;p&gt;Daily digest of all European digital development news&lt;/p&gt;
    &lt;head rend="h3"&gt;Verifactu is driving half of Spain crazy, but there's good news: freelancers with simple invoices are exempt&lt;/head&gt;
    &lt;p&gt;Spanish businesses must implement Verifactu, a new digital invoicing system by January 1, 2026, to combat fraud. Freelancers using simple invoices created with programs like Word or Excel are exempt from the requirement.&lt;/p&gt;
    &lt;head rend="h3"&gt;Holo raises €1 million to bring personalised lab testing and daily-life health tracking to more users&lt;/head&gt;
    &lt;p&gt;Barcelona-based Holo secured €1 million in pre-Seed funding to launch its health app that combines AI, lab testing, and health tracking. The app integrates with devices like Apple Watch and Garmin to give users more control over their well-being.&lt;/p&gt;
    &lt;head rend="h3"&gt;Being able to talk on WhatsApp without using WhatsApp: it's the app's most important change and it's already underway&lt;/head&gt;
    &lt;p&gt;WhatsApp launches interoperability in Europe, allowing users to send and receive messages from other messaging apps. This move follows the EU's Digital Markets Act, forcing the platform to open up to third-party integrations.&lt;/p&gt;
    &lt;head rend="h3"&gt;WhatsApp in Europe can now chat with other applications&lt;/head&gt;
    &lt;p&gt;WhatsApp is rolling out third-party chats in Europe, allowing users to connect with individuals on other messaging services. This move follows new rules requiring interoperability.&lt;/p&gt;
    &lt;head rend="h3"&gt;With 300k add-to-cart events, Paris-based Dialog secures €3.7 million round to scale its AI shopping agent&lt;/head&gt;
    &lt;p&gt;Paris-based Dialog secured a €3. 7 million Seed round to scale its AI-powered shopping agent trained on brand data for e-commerce....&lt;/p&gt;
    &lt;head rend="h3"&gt;The EU already knows how to curb the dominance of Temu and Shein in e-commerce: small packages&lt;/head&gt;
    &lt;p&gt;Brussels aims to curtail the dominance of e-commerce giants like Temu and Shein by ending tax exemptions on small packages. The EU plans to eliminate the exemption for packages under €150, potentially by 2026, which previously cost the EU €1....&lt;/p&gt;
    &lt;head rend="h3"&gt;Rookoo, out of Belgium, raises €900k to help the global event industry escape administrative chaos&lt;/head&gt;
    &lt;p&gt;Belgian startup Rookoo secured €900k in funding to expand its digital platform for events and hospitality teams, aiming to automate administrative tasks. This investment follows a trend of nearly €79 million in funding for European hospitality tech, including AI-driven solutions, in 2025.&lt;/p&gt;
    &lt;head rend="h3"&gt;WhatsApp is launching third-party chat integration in Europe&lt;/head&gt;
    &lt;p&gt;Meta is set to launch third-party chat integration within WhatsApp across Europe, complying with the Digital Markets Act. Users will soon be able to message with apps like BirdyChat and Haiket, with end-to-end encryption maintained.&lt;/p&gt;
    &lt;head rend="h3"&gt;Internet consultation new cyber rules started&lt;/head&gt;
    &lt;p&gt;Brussels is launching a public consultation on new cybersecurity rules for the government, open to citizens, businesses, and experts. The consultation period will run from November 11, 2025, to December 23, 2025, as part of the Cyber Security Act.&lt;/p&gt;
    &lt;head rend="h3"&gt;New data-driven report highlights the AI opportunity for Public Sector Services&lt;/head&gt;
    &lt;p&gt;The EU-funded report highlights how AI can transform public services and create opportunities for startups across the EU. Between 2021 and 2024, venture capital investment in AI-powered public sector technologies soared, with nearly 50% of deals in 2024 involving AI-first GovTech startups.&lt;/p&gt;
    &lt;head rend="h3"&gt;Merz: Huawei and Co. are banned from the outset in 6G networks&lt;/head&gt;
    &lt;p&gt;Germany will ban Chinese suppliers like Huawei from its future 6G networks, ensuring they have no place in the country’s telecom infrastructure. This decision aims to establish planning security for the next generation of mobile networks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Digital Sovereignty: Think Tank Recommends More Investment in Big Tech Alternatives&lt;/head&gt;
    &lt;p&gt;A think tank urges the German government to boost investment in open-source platforms as an alternative to Big Tech. The call for action aims to shift open networks, like the Fediverse, from niche status to mainstream adoption, promoting digital sovereignty.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trade Republic launches Crypto Wallet, its crypto wallet: 4 things to know&lt;/head&gt;
    &lt;p&gt;Trade Republic launches its "Crypto Wallet," offering access to nearly 50 cryptocurrencies with regulated banking protection. The move expands digital asset access for over 10 million users across 18 countries.&lt;/p&gt;
    &lt;head rend="h3"&gt;Irish sales planning and intelligence platform Lative raises $7.5m&lt;/head&gt;
    &lt;p&gt;Irish sales planning platform Lative secured $7. 5M to boost its product development and expand its market reach....&lt;/p&gt;
    &lt;head rend="h3"&gt;With users reporting 24% productivity gains, Lative secures €6.4 million to scale its AI-driven sales planning tool&lt;/head&gt;
    &lt;p&gt;Dublin-based Lative secured €6. 4 million to scale its AI-driven sales planning platform, boosting product development and go-to-market efforts....&lt;/p&gt;
    &lt;head rend="h3"&gt;Swiss startup Forgis raises €3.8 million to automate industrial machines as Europe confronts Asia’s manufacturing lead&lt;/head&gt;
    &lt;p&gt;Forgis, a Swiss startup, secured €3. 8 million to automate industrial machines using software in the automotive and manufacturing sectors....&lt;/p&gt;
    &lt;head rend="h3"&gt;They stole almost 23,000 euros with the SIM swapping scam. Now Vodafone and Ibercaja will have to return it&lt;/head&gt;
    &lt;p&gt;Vodafone and Ibercaja were ordered to refund nearly €23,000 to a customer scammed via SIM swapping. The court ruled the companies were liable after fraudsters duplicated the victim's SIM and stole the money.&lt;/p&gt;
    &lt;head rend="h3"&gt;The anti-abuse bracelets were going to be a technical solution to a social problem. They are generating a chaos of incidents&lt;/head&gt;
    &lt;p&gt;The Spanish Cometa system, managing anti-abuse bracelets, suffered a technical incident causing service overload and triggering an emergency protocol. The issue affected the safety of approximately 4,500 women using the devices, with roughly 10% of alerts causing system failures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Digital Markets Act: EU Commission Accuses Google of Discrimination Against News Sites&lt;/head&gt;
    &lt;p&gt;Brussels launched a new investigation into Google's parent company, Alphabet, for potential Digital Markets Act violations. The EU suspects Google's search results may discriminate against news websites, impacting their visibility.&lt;/p&gt;
    &lt;head rend="h3"&gt;FMC raises €100M as it unveils new class of memory chips for the AI era&lt;/head&gt;
    &lt;p&gt;FMC secured €100 million to revolutionize memory chips, aiming for faster, more energy-efficient AI data centers. The funding, led by HV Capital and DTCF, will help the company compete with global chip manufacturers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://europedigital.cloud/en/news"/><published>2025-11-14T12:00:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926037</id><title>Magit manuals are available online again</title><updated>2025-11-14T17:38:23.198862+00:00</updated><content>&lt;doc fingerprint="b7accebf398fa351"&gt;
  &lt;main&gt;
    &lt;p&gt;You signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert&lt;/p&gt;
    &lt;p&gt;I'm just getting started with Magit, and it works in my Emacs, but I wanted to go over some tutorials from the webpage. magit.vc does not work currently.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/magit/magit/issues/5472"/><published>2025-11-14T12:09:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926224</id><title>Winamp for OS/X</title><updated>2025-11-14T17:38:22.667096+00:00</updated><content>&lt;doc fingerprint="6cc6844dbf363dae"&gt;
  &lt;main&gt;
    &lt;p&gt;A native macOS application that recreates the classic Winamp experience for playing MP3 and FLAC audio files.&lt;/p&gt;
    &lt;p&gt;If you enjoy using Winamp macOS and would like to support its development, consider buying me a coffee:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;🎵 MP3 and FLAC playback support&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🎨 Winamp-inspired UI&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;📝 Playlist management / M3U&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;⏯️ Full playback controls (play, pause, stop, next, previous)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;📊 Spectrum analyzer visualization&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🎚️ 10-band equalizer&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🔍 File browser with drag-and-drop support&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple oscilliscope visualizations&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Milkdrop (click on the icon in the main app) - supports fullscreen mode&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lyrics overlay in Milkdrop&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 or later&lt;/item&gt;
      &lt;item&gt;Xcode 15.0 or later&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open &lt;code&gt;Winamp.xcodeproj&lt;/code&gt;in Xcode&lt;/item&gt;
      &lt;item&gt;Select the Winamp scheme&lt;/item&gt;
      &lt;item&gt;Build and run (⌘R)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;alternatively&lt;/p&gt;
    &lt;code&gt;./build.sh --run&lt;/code&gt;
    &lt;code&gt;swift build
swift run&lt;/code&gt;
    &lt;p&gt;alternatively&lt;/p&gt;
    &lt;code&gt;./build.sh --release&lt;/code&gt;
    &lt;p&gt;MIT License - Feel free to use and modify as needed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mgreenwood1001/winamp"/><published>2025-11-14T12:44:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926263</id><title>EDE: Small and Fast Desktop Environment (2014)</title><updated>2025-11-14T17:38:21.377673+00:00</updated><content>&lt;doc fingerprint="36df547a55af023e"&gt;
  &lt;main&gt;
    &lt;p&gt;small and fast desktop environment&lt;/p&gt;
    &lt;p&gt;EDE is small desktop environment built to be responsive, light in resource usage and to have familiar look and feel. It runs on Linux, *BSD, Solaris, Minix, Zaurus and even on XBox.&lt;/p&gt;
    &lt;p&gt;or you can browse for older releases.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://edeproject.org/"/><published>2025-11-14T12:51:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926350</id><title>Google Releases CodeWiki</title><updated>2025-11-14T17:38:20.983662+00:00</updated><link href="https://codewiki.google/"/><published>2025-11-14T13:05:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926383</id><title>Backblaze Drive Stats for Q3 2025</title><updated>2025-11-14T17:38:20.488827+00:00</updated><content>&lt;doc fingerprint="5654bed345af2d9d"&gt;
  &lt;main&gt;
    &lt;p&gt;Every quarter, Drive Stats gives us the numbers. This quarter, it gave us a crisis of meaning. What does it really mean for a hard drive to fail? Is it the moment the lights go out, or the moment we decide they have? Philosophers might call that an ontological gray area. We just call it Q3.&lt;/p&gt;
    &lt;p&gt;As of June 30, 2025, we had 332,915 drives under management. Of that total, there were 3,970 boot drives and 328,348 data drives. Let’s dig into our stats, then talk about the meaning of failure.&lt;/p&gt;
    &lt;head rend="h4"&gt;This quarter, we have more to talk about (Stats-wise)&lt;/head&gt;
    &lt;p&gt;Drive Stats was the beginning. Want to see more of the full picture? Check out the Stats Lab webinar, bringing together content from all of our Stats articles. We’re going to chat about all things Backblaze (and beyond)—by the numbers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Drive Stats: The digest version&lt;/head&gt;
    &lt;head rend="h2"&gt;Q3 2025 hard drive failure rates&lt;/head&gt;
    &lt;p&gt;During Q3 2025, we were tracking 328,348 storage drives. Here are the numbers:&lt;/p&gt;
    &lt;p&gt;Backblaze Hard Drive Failure Rates for Q3 2025&lt;/p&gt;
    &lt;p&gt;Reporting period July 1, 2025–September 30, 2025 inclusive&lt;lb/&gt;Drive models with drive count &amp;gt; 100 as of July 1, 2025 and drive days &amp;gt; 10,000 in Q3 2025&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes and observations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The failure rate has increased: The failure rate has changed, and by quite a bit. As a reminder, last quarter’s AFR was 1.36% compared with this quarter’s 1.55%. (Interestingly, the 2024 yearly AFR was 1.57%.)&lt;/item&gt;
      &lt;item&gt;That new drive energy: Say hello to the 24TB Toshiba MG11ACA24TE, joining the drive pool with 2,400 drives and 24,148 drive days. That means that we’ve hit the thresholds for the quarterly stats, but not the lifetime.&lt;/item&gt;
      &lt;item&gt;The zero failure club: It was a big month for the zero failure club, with four drives making the cut: &lt;list rend="ul"&gt;&lt;item&gt;Seagate HMS5C4040BLE640 (4TB)&lt;/item&gt;&lt;item&gt;Seagate ST8000NM000A (8TB)&lt;/item&gt;&lt;item&gt;Toshiba MG09ACA16TE (16TB)&lt;/item&gt;&lt;item&gt;Toshiba MG11ACA24TE (24TB)—and yes, that’s the new drive.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For those of you tracking the stats closely, you’ll notice that the Seagate ST8000NM000A (8TB) is a frequent flier on this list. The last time it had a failure was in Q3 2024—and it was just a single failure for the whole quarter!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The highest AFRs were really high: The high end was so high that this month, it inspired us to run an outlier analysis using the standard quartile analysis (Tukey method). Based on that information, any drive with a quarterly AFR higher than 5.88% is an outlier, and there are three: &lt;list rend="ul"&gt;&lt;item&gt;Seagate ST10000NM0086 (10TB): 7.97%&lt;/item&gt;&lt;item&gt;Seagate ST14000NM0138 (14TB): 6.86%&lt;/item&gt;&lt;item&gt;Toshiba MG08ACA16TEY (16TB): 16.95%&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s going on there? Great question, and we’ll get into that after the lifetime failure rates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lifetime hard drive failure rates&lt;/head&gt;
    &lt;p&gt;To be considered for the lifetime review, a drive model was required to have 500 or more drives as of the end of Q2 2025 and have over 100,000 accumulated drive days during their lifetime. When we removed those drive models which did not meet the lifetime criteria, we had drives grouped into 27 models remaining for analysis as shown in the table below.&lt;/p&gt;
    &lt;p&gt;Backblaze Hard Drive Failure Rates for Q2 2025&lt;/p&gt;
    &lt;p&gt;Reporting period ending September 30, 2025&lt;lb/&gt;Drive models &amp;gt; 500 drives and &amp;gt; 100,000 lifetime drive days&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes and observations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;That lifetime AFR is pretty consistent, isn’t it? The lifetime AFR is 1.31%. Last quarter we reported that it was 1.30%, and the quarter before that, it was 1.31%.&lt;/item&gt;
      &lt;item&gt;The 4TB average age hasn’t shifted: As we’ve reported on previously, the 4TB drives are being decommissioned over time. Now, we’re down to just a handful left—just 11 of the ALE models and 187 of the BLE models. But, because their lifetime populations are so comparatively large, the additional drive days aren’t enough to move the needle on the average age in months. So, no ghosts in the machine here, and decommissioning is proceeding as planned.&lt;/item&gt;
      &lt;item&gt;Steady uptick in higher capacity drives: Of the 20TB+ drives that meet our lifetime data parameters, we’ve added 7,936 since last quarter. And, don’t forget that our newest entrée to the cohort, the Toshiba MG11ACA24TE (24TB), hasn’t made its way to this table yet—that adds an additional 2,400 drive models. All together, the 20TB+ club represents 67,939 drives, or about 21% of the drive pool.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Defining a failure—from a technical perspective&lt;/head&gt;
    &lt;p&gt;A question that’s come up a few times when we’re hosting a webinar or chatting in the comments section is how we define a failure. While it may seem intuitive, it’s actually something of a meaty conundrum, and something we haven’t addressed since the early days of this series. Tracking down the answer to this question touches internal drive fleet monitoring tools (via SMART stats), the actual Drive Stats collection program, and our data engineering layer. I’ll dig into each of these in detail, then we’ll take a look at the outliers for this quarter.&lt;/p&gt;
    &lt;head rend="h3"&gt;SMART stats reporting&lt;/head&gt;
    &lt;p&gt;We use Smartmontools to collect the SMART attributes of drives, and another monitoring tool called drive sentinel to flag read/write errors that exceed a certain threshold as well as some other anomalies.&lt;/p&gt;
    &lt;p&gt;The main indicator we use for determining if a drive should be replaced is when it responds to reads with uncorrectable medium errors. When a drive reads the data from the disk, but the data fails its integrity check, the drive will try to reconstruct the data using internal error correction codes. If it is unable to reconstruct the data, it notifies the host by reporting it as an uncorrectable error and marks that part of the disk as pending reallocation, which shows up in SMART under an attribute like &lt;code&gt;Current_Pending_Sector&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;On Storage Pods that control drives through SATA links, the drive sentinel will count the number of these uncorrectable errors a drive reports and if it exceeds a threshold, access to the drive will be removed. This is important in the classic Backblaze Storage Pods where five drives share a single SATA link and errors by one drive will affect all drives on the link.&lt;/p&gt;
    &lt;p&gt;On Dell and SMCI pods that use a SAS topology to connect drives, drive sentinel doesn’t remove access to drives because the errors are reported differently; but, that’s also not as critical since SAS minimizes the impact that a problem disk can have on others.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Drive Stats program&lt;/head&gt;
    &lt;p&gt;We’ve talked about the custom program we use to collect Drive Stats in the past, and here’s a quick recap:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The podstats generator runs on every Storage Pod, what we call any host that holds customer data, every few minutes. It’s a C++ program that collects SMART stats and a few other attributes, then converts them into an .xml file (“podstats”). Those are then pushed to a central host in each datacenter and bundled. Once the data leaves these central hosts, it has entered the domain of what we will call Drive Stats.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For this program, the logic is relatively simple: A failure in Drive Stats occurs when a drive vanishes out of the reporting population. It is considered “failed” until it shows up again. Drives are tracked by serial number and we report daily logs on a per-drive basis, so truly, we can get pretty granular here.&lt;/p&gt;
    &lt;head rend="h3"&gt;The data engineering layer&lt;/head&gt;
    &lt;p&gt;To recap, we’ve collected our SMART stats and compiled them with the podstats program. Now we’ve got all the information, and data intelligence needs to add the context. A drive may go offline for a day or so (not return a response to those tools that collect daily logs of SMART stats), but it could be something as simple as a loose cable. So, time-wise, if a drive reappears after one day or 30, at what point in that period of time do we classify it as an official failure?&lt;/p&gt;
    &lt;p&gt;Previously, we manually cross-referenced data center work tickets, but these days, we’ve automated that process. On the backend, it’s a SQL query, but in human speak, this is what it comes down to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If a drive logs data on the last day of the selection period (which in this case is a quarter) then it has not failed.&lt;/item&gt;
      &lt;item&gt;There are three human-curated tables that the query cross references. If a drive serial number appears on one of them, it tells us whether there’s a failure or not (depending on the table’s function).&lt;/item&gt;
      &lt;item&gt;If the drive serial number is the primary serial number in a drive replacement Jira ticket then it has failed. (Jira is where we track our data center work tickets.)&lt;/item&gt;
      &lt;item&gt;If the drive serial number is the target serial number in a clone Jira ticket or a (temp) replacement ticket, then it has not failed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Basically, when we go to write the Drive Stats reports at the end of the quarter, if a drive has either appeared in one of our various work trackers or hasn’t re-entered the population, then it’s considered failed.&lt;/p&gt;
    &lt;p&gt;In rare instances, that can mean that we have so-called “cosmetic” failures when we have some work we’re doing on a drive model that lasts more than that quarterly collection period. And, spoiler, we have one of those instances that showed up in the data this month—our outlier Toshiba drive with the 16.9% failure rate. We’ll dig in in just a minute; but first, some context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Connecting drive failure to overall picture of the drive pool&lt;/head&gt;
    &lt;p&gt;As we mentioned above, certain drives in the pool had such high swings in AFR that we ended up running an outlier analysis using the quartile method. (It’s also worth mentioning that a cluster analysis could potentially be a better fit, but we can save that for another day.) Based on that analysis, anything that has above a 5.88% failure rate is an outlier.&lt;/p&gt;
    &lt;p&gt;The primary motivation was inspired by an attempt to visualize the relationship between the age in months of a drive versus this quarter’s AFRs.&lt;/p&gt;
    &lt;p&gt;And yes, we’re fully aware that that’s a… super unreadable scatter plot. Removing the labels, this is a bit better:&lt;/p&gt;
    &lt;p&gt;We’re interested, really, in the shape of the relationship. If we posit that the older drives get, the higher their failure rates, you’d expect a larger concentration in the top right quadrant. But, our data follows a much more interesting pattern than that, with most of our data points concentrated in the lowest regions of the graph regardless of age—something you’d expect from a set of data that reflects a bunch of smart folks actively working towards the goal of a healthy drive population. And yet, we have some data points that break the mold.&lt;/p&gt;
    &lt;p&gt;As is pretty intuitive to my business intelligence folks in the audience, the process of identifying outliers is actionable data as well. Just like all press is good press; in our world, more data is more better. So, let’s take a closer look at those outliers. As a reminder, that’s these three drive models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seagate ST10000NM0086 (10TB): 7.97%&lt;/item&gt;
      &lt;item&gt;Seagate ST14000NM0138 (14TB): 6.86%&lt;/item&gt;
      &lt;item&gt;Toshiba MG08ACA16TEY (16TB): 16.95%&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Seagate ST10000NM0086 (10TB)&lt;/head&gt;
    &lt;p&gt;This drive has some pretty explainable factors for the high failure rate. It’s well over seven years old (92.35 months). And, since it only has 1,018 drive models in operation, single failures hold a lot of weight compared with the average drive count per model—which comes in at 10,952 if you use the mean of this quarterly data and 6,177 if you use the median.&lt;/p&gt;
    &lt;p&gt;And, you can see that borne out in the trend in the last year of data:&lt;/p&gt;
    &lt;head rend="h3"&gt;Seagate ST14000NM0138 (14TB)&lt;/head&gt;
    &lt;p&gt;This drive is nearing five years in age (56.57 months) and, again, has a lower drive count at 1,286. More importantly, this particular drive model has had historically high failure rates. In parallel with above, here’s the last year of quarterly failure rates:&lt;/p&gt;
    &lt;head rend="h3"&gt;Toshiba MG08ACA16TEY (16TB)&lt;/head&gt;
    &lt;p&gt;Finally, our Toshiba model is the most interesting of all. It’s less than four years old (44.61 months), and has 5,145 drives in the pool. And, this quarter is clearly a change from its normal, decent, AFRs.&lt;/p&gt;
    &lt;p&gt;When we see deviations like this one, it’s usually an indication that there’s something afoot.&lt;/p&gt;
    &lt;p&gt;Never fear, Drive Stats fans; this was a known quantity before we went on this journey. This past quarter, working with Toshiba, we deployed some firmware updates they provided to optimize performance on these drives. Because we needed to pull drives to achieve this in some cases, we had an abnormal number of “failed” drives in this population.&lt;/p&gt;
    &lt;p&gt;What that means for this drive is that it’s actually not a bad drive model; and, given the ways we and Toshiba have worked together on a fix, we should see failure rates normalizing in the near future. And, this also goes back to our conversation of defining a failure—in this case, while the drives “failed,” the failure wasn’t mechanical and was based on something that we’ll be able to fix without replacing the drives. In short, don’t sweat the spike and pay attention to the long arc of performance on this population. We expect to see those drives happy and spinning for years to come (and with better performance, too).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hard Drive dataset (and beyond)&lt;/head&gt;
    &lt;p&gt;Thank you, as always, for making it through ~2,500 or so words to examine the fun side of data. Here’s our standard fine print:&lt;/p&gt;
    &lt;p&gt;The complete dataset used to create the tables and charts in this report is available on our Hard Drive Test Data page. You can download and use this data for free for your own purpose. All we ask are three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You cite Backblaze as the source if you use the data;&lt;/item&gt;
      &lt;item&gt;You accept that you are solely responsible for how you use the data, and;&lt;/item&gt;
      &lt;item&gt;You do not sell this data itself to anyone; it is free.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re a new Drive Stats fan, consider signing up for the newsletter. If you’re not ready for that kind of commitment, sound off in the comments section below or reach out directly to us to let us know what you’re working on. Happy investigating!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2025/"/><published>2025-11-14T13:10:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926439</id><title>Nvidia is gearing up to sell servers instead of just GPUs and components</title><updated>2025-11-14T17:38:20.132529+00:00</updated><content>&lt;doc fingerprint="b7f8d4517b86a56a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;JP Morgan says Nvidia is gearing up to sell entire AI servers instead of just AI GPUs and components — Jensen's master plan of vertical integration will boost Nvidia profits, purportedly starting with Vera Rubin&lt;/head&gt;
    &lt;p&gt;The launch of Nvidia's Vera Rubin platform for AI and HPC next year could mark significant changes in the AI hardware supply chain as Nvidia plans to ship its partners fully assembled Level-10 (L10) VR200 compute trays with all compute hardware, cooling systems, and interfaces pre-installed, according to J.P. Morgan (via @Jukanlosreve). The move would leave major ODMs with very little design or integration work, making their lives easier, but would also trim their margins in favor of Nvidia's. The information remains unofficial at this stage.&lt;/p&gt;
    &lt;p&gt;Starting with the VR200 platform, Nvidia is reportedly preparing to take over production of fully built L10 compute trays with a pre-installed Vera CPU, Rubin GPUs, and a cooling system instead of allowing hyperscalers and ODM partners to build their own motherboards and cooling solutions. This would not be the first time the company has supplied its partners with a partially integrated server sub-assembly: it did so with its GB200 platform when it supplied the whole Bianca board with key components pre-installed. However, at the time, this could be considered as L7 – L8 integration, whereas now the company is reportedly considering going all the way to L10, selling the whole tray assembly — including accelerators, CPU, memory, NICs, power-delivery hardware, midplane interfaces, and liquid-cooling cold plates — as a pre-built, tested module.&lt;/p&gt;
    &lt;p&gt;If the information is correct, and Nvidia will indeed ship its partners L10 compute trays (which probably account for 90% of the cost of a server), then Nvidia will only leave its partners with rack-level integration rather than server design. They would still build the outer chassis, integrate power supplies depending on requirements, install sidecars or CDUs for rack-level cooling, add their own BMC and management stack, and perform final assembly and testing. These tasks matter operationally, but they do not differentiate hardware in a meaningful way.&lt;/p&gt;
    &lt;p&gt;This move promises to shorten the ramp for VR200 as Nvidia's partners will not have to design everything in-house and could lower production costs due to the volume of scale ensured by a direct contract between Nvidia and an EMS (most likely Foxconn as the primary supplier and then Quanta and Wistron, but that is speculation). For example, a Vera Rubin Superchip board recently demonstrated by Jensen Huang uses a very complex design, a very thick PCB, and only solid-state components. Designing such a board takes time and costs a lot of money, so using select EMS provider(s) to build it makes a lot of sense.&lt;/p&gt;
    &lt;p&gt;J.P. Morgan reportedly mentions the increase in power consumption of one Rubin GPU from 1.4 kW (Blackwell Ultra) to 1.8 kW (R200) and even 2.3 kW (a previously unannounced TDP for an allegedly unannounced SKU (Nvidia declined a Tom's Hardware request for comment on the matter) and increased cooling requirements as one of the motivations for moving to supply the whole tray instead of individual components. However, we know from reported supply chain sources that various OEMs and ODMs, as well as hyperscalers like Microsoft, are experimenting with very advanced cooling systems, including immersion and embedded cooling, which underscores their experience.&lt;/p&gt;
    &lt;p&gt;However, Nvidia's partners will shift from being system designers to becoming system integrators, installers, and support providers. They are going to keep enterprise features, service contracts, firmware ecosystem work, and deployment logistics, but the 'heart' of the server — the compute engine — is now fixed, standardized, and produced by Nvidia rather than by OEMs or ODMs themselves.&lt;/p&gt;
    &lt;p&gt;Also, we can only wonder what will happen with Nvidia's Kyber NVL576 rack-scale solution based on the Rubin Ultra platform, which is set to launch alongside the emergence of 800V data center architecture meant to enable megawatt-class racks and beyond. Now the only question is whether Nvidia further increases its share in the supply chain to, say, rack-level integration?&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Anton Shilov is a contributing writer at Tom’s Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;DS426&lt;/header&gt;There are definitely advantages of such levels of integration, but ultimately, it only bolsters Nvidia's monopolistic powers. When the AI world desperately needs a more open ecosystem, Nvidia is sure to double-down on forcing the standard for everyone.Reply&lt;lb/&gt;Hard as heck to change the status quo sometimes, ain't it!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;JTWrenn&lt;/header&gt;This makes me wonder if the stock slide is actually just heavy money moving around in the background at lower rates to try to free up capital to do this. Wonder how much this will tank Super MicroReply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;edzieba&lt;/header&gt;"Gearing up"? They've been selling not just servers but entire racks for years now (starting with DGX-1 back in 2016, so nearly a decade). If anything, this is unbundling those DGX boxes from the DGX pods to be sold as individual RUs for OEMS to integrate.Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/artificial-intelligence/jp-morgan-says-nvidia-is-gearing-up-to-sell-entire-ai-servers-instead-of-just-ai-gpus-and-componentry-jensens-master-plan-of-vertical-integration-will-boost-profits-purportedly-starting-with-vera-rubin"/><published>2025-11-14T13:18:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926469</id><title>AGI fantasy is a blocker to actual engineering</title><updated>2025-11-14T17:38:19.888325+00:00</updated><content>&lt;doc fingerprint="6cff1130952e1f98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AGI fantasy is a blocker to actual engineering&lt;/head&gt;
    &lt;p&gt;Reading Empire of AI by Karen Hao, I was struck by how people associated with OpenAI believe in AGI. They really do think someone, perhaps them, will build AGI, and that it will lead to either the flourishing or destruction of humanity.&lt;/p&gt;
    &lt;p&gt;Elon Musk founded OpenAI because he thought Demis Hassabis was an evil genius who would build AGI first:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…Musk would regularly characterise Hassabis as a supervillain who needed to be stopped. Musk would make unequivocally clear that OpenAI was the good to DeepMind’s evil. … “He literally made a video game where an evil genius tries to create AI to take over the world,” Musk shouted [at an OpenAI off-site], referring to Hassabis’s 2004 title Evil Genius, “and fucking people don’t see it. Fucking people don’t see it! And Larry [Page]? Larry thinks he controls Demis but he’s too busy fucking windsurfing to realize that Demis is gathering the power.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;OpenAI’s co-founder and chief scientist Ilya Sutskever regularly told audiences and employees to “feel the AGI”. At a company off-site in Yosemite in September 2022, employees gathered around a firepit:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In the pit, [Sutskever] had placed a wooden effigy that he’d commissioned from a local artist, and began a dramatic performance. This effigy, he explained represented a good, aligned AGI that OpenAI had built, only to discover it was actually lying and deceitful. OpenAI’s duty, he said, was to destroy it. … Sutskever doused the effigy in lighter fluid and lit on fire.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think it’s remarkable that what was until recently sci-fi fantasy has become a mainstream view in Silicon Valley.&lt;/p&gt;
    &lt;p&gt;Hao writes that GPT-2 was a bet on the “pure language” hypothesis, that asserts that since we communicate through language, then AGI should emerge from training a model solely on language. This is contrast to the “grounding” hypothesis, that asserts an AGI needs to perceive the world. Successfully scaling GPT to GPT-2 convinced enough people at OpenAI that the pure language hypothesis was valid. They just needed more data, more model parameters, and more compute.&lt;/p&gt;
    &lt;p&gt;So the belief in AGI, plus the recent results from LLMs, necessitates scaling, and justifies building data centres that consume hundreds of litres of water a second, run on polluting gas generators because the grid can’t supply the power (and might use as much power as entire cities), driving up CO2 emissions from manufacture and operation of new hardware, and exploits and traumatises data workers to make sure ChatGPT doesn’t generate outputs like child sexual abuse material and hate speech or encourage users to self-harm. (The thirst for data is so great that they stopped curating training data and instead consume the internet, warts and all, and manage the model output using RLHF.)&lt;/p&gt;
    &lt;p&gt;And this is all fine, because they’re going to make AGI and the expected value (EV) of it will be huge! (Briefly, the argument goes that if there is a 0.001% chance of AGI delivering an extremely large amount of value, and 99.999% chance of much less or zero value, then the EV is still extremely large because &lt;code&gt;(0.001% * very_large_value) + (99.999% * small_value) = very_large_value&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;But AGI arguments based on EV are nonsensical because the values and probabilities are made up and unfalsifiable. They also ignore externalities like environmental damage, which in contrast to AGI, have known negative value and certain probability: costs borne by everyone else right now.&lt;/p&gt;
    &lt;p&gt;As a technologist I want to solve problems effectively (by bringing about the desired, correct result), efficiently (with minimal waste) and without harm (to people or the environment).&lt;/p&gt;
    &lt;p&gt;LLMs-as-AGI fail on all three fronts. The computational profligacy of LLMs-as-AGI is dissatisfying, and the exploitation of data workers and the environment unacceptable. Instead, if we drop the AGI fantasy, we can evaluate LLMs and other generative models as solutions for specific problems, rather than all problems, with proper cost benefit analysis. For example, by using smaller purpose-built generative models, or even discriminative (non-generative) models. In other words, make trade-offs and actually do engineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomwphillips.co.uk/2025/11/agi-fantasy-is-a-blocker-to-actual-engineering/"/><published>2025-11-14T13:21:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926779</id><title>I think nobody wants AI in Firefox, Mozilla</title><updated>2025-11-14T17:38:19.753028+00:00</updated><content>&lt;doc fingerprint="f7ee53523cb1437d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I think nobody wants AI in Firefox, Mozilla&lt;/head&gt;
    &lt;p&gt;Mozilla is developing a built‑in AI assistant for Firefox that will be offered as a third browsing mode alongside Normal and Private tabs. They’re calling it “Window AI.”&lt;/p&gt;
    &lt;p&gt;Details are still scarce. Based on Mozilla’s official announcement on Thursday (13th), it looks like a deeper implementation than the existing sidebar that gives access to third‑party chatbots (ChatGPT, Gemini, Copilot, etc.). The post stresses the feature will be opt-in and that the user “is in control.”&lt;/p&gt;
    &lt;p&gt;There’s a waitlist to try the feature and a Mozilla forum thread inviting people to “help shape” the initiative.&lt;/p&gt;
    &lt;p&gt;It’s safe to say that the people who volunteered to “shape” the initiative want it dead and buried. Of the 52 responses at the time of writing, *all* rejected the idea and asked Mozilla to stop shoving AI features into Firefox.&lt;/p&gt;
    &lt;p&gt;I don’t know whether the negative reactions reflect the majority of Firefox users or are just a noisy minority. Mozilla, after all, likely has a clearer view of the whole user base.&lt;/p&gt;
    &lt;p&gt;What strikes me as odd is the decision to position itself as just another AI‑enabled web browser, picking a fight with big techs and better‑funded startups whose users are less hostile (and sometimes enthusiastic) about adding AI to web browsing.&lt;/p&gt;
    &lt;p&gt;Mozilla seems to be trying to wedge itself between those who reject AI and those who want generative‑AI features in the browser — trying to please everyone — as this excerpt from the post shows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We see a lot of promise in AI browser features making your online experience smoother, more helpful, and free from the everyday disruptions that break your flow. But browsers made by AI companies ask you to make a hard choice — either use AI all the time or don’t use it at all.&lt;/p&gt;
      &lt;p&gt;We’re focused on making the best browser, which means recognizing that everyone has different needs. For some, AI is part of everyday life. For others, it’s useful only occasionally. And many are simply curious about what it can offer, but unsure where to start.&lt;/p&gt;
      &lt;p&gt;Regardless of your choice, with Firefox, you’re in control.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Those unhappy have another option: use an AI‑free Firefox fork such as LibreWolf, Waterfox, or Zen Browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://manualdousuario.net/en/mozilla-firefox-window-ai/"/><published>2025-11-14T14:05:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45927042</id><title>What Comes After Science?</title><updated>2025-11-14T17:38:19.513456+00:00</updated><content/><link href="https://www.science.org/doi/10.1126/science.aec7650"/><published>2025-11-14T14:32:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45927210</id><title>Linear Algebra Explains Why Some Words Are Effectively Untranslatable</title><updated>2025-11-14T17:38:19.179721+00:00</updated><content>&lt;doc fingerprint="7519dbbe0167b35d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linear Algebra Explains Why Some Words Are Effectively Untranslatable&lt;/head&gt;
    &lt;head rend="h2"&gt;A modest mathematical framing of language&lt;/head&gt;
    &lt;p&gt;Marco Giancotti,&lt;/p&gt;
    &lt;p&gt;Marco Giancotti,&lt;/p&gt;
    &lt;p&gt;Cover image:&lt;/p&gt;
    &lt;p&gt;Image by vackground.com, Unsplash&lt;/p&gt;
    &lt;p&gt;A part of me still hasn't recovered from learning that some people believe there is no such thing as an untranslatable word. I've written about why I disagree before, but that explanation didn't satisfy me completely. There was a stronger argument to be made, I thought, but I couldn't put it into words. Now I remember, though: you need to see language as (a little bit) like math. Call me crazy, but I think that language translation is like a change of basis in linear algebra.&lt;/p&gt;
    &lt;p&gt;Me making weird connections like this might simply be an occupational hazard. Both my PhD research and my first job had to do with controlling the position and orientation of spacecraft and rocks in space, which means that I spent years juggling vectors, matrix multiplications, and reference frames almost daily. Still, I think it is simple enough to be understood by anyone, so hear me out.&lt;/p&gt;
    &lt;p&gt;(You might remember linear algebra from high school. It's that subfield where you write about stuff like this:&lt;/p&gt;
    &lt;p&gt;If the mere sight of the above is like a punch in the face for you, don't worry. I'm not going to math you to death in what follows. I will only remind you of a tiny basic part of it that I think relates to languages.)&lt;/p&gt;
    &lt;p&gt;During those same mathematical days, I was also learning Japanese. The language fascinated me for many reasons, like its beautiful dissociation between written and spoken words and its many unique quirks, but I was also struck early on by something a bit more meta: how hard it is to translate things to and from it.&lt;/p&gt;
    &lt;p&gt;These two concurrent interests made it hard for me not to see a connection. For almost a decade now, I've held it in a corner of my mind without telling anyone, perhaps because I thought it would be seen as too outrageous, but hey! Now LLMs are popular and they literally handle words and concepts as vectors with linear algebra operations, so maybe my analogy isn't that out there. Let me give it a try.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Case of Vectors&lt;/head&gt;
    &lt;p&gt;Contrary to popular belief, a vector is not "a list of numbers" but an abstract object with no predefined way to express it.&lt;/p&gt;
    &lt;p&gt;But a vector is not very useful in this abstract state. We need a way to write it down so that we can manipulate it with algebra and communicate it to others. We do this by choosing a frame of reference—or, more accurately, a set of vectors to use as "basis" to quantify all others.&lt;/p&gt;
    &lt;p&gt;This is where numbers come into play. By projecting the vectors against the basis vectors, you can assign them lists of numbers (two numbers each, in this two-dimensional case):&lt;/p&gt;
    &lt;p&gt;Those numbers are the coordinates of the vectors in that basis. For instance, the vector can be read as "half as long as the vector along the direction of , and times as long as the vector along the direction of ."&lt;/p&gt;
    &lt;p&gt;The important thing I want to convey here (and the last mathy thing to remember) is that if you were to choose a different basis, the same vector would have different coordinates.&lt;/p&gt;
    &lt;p&gt;In this new basis, the two vectors are written as:&lt;/p&gt;
    &lt;p&gt;In short, change the basis (e.g. from " &amp;amp; " to " &amp;amp; ") and the same abstract object (vector) will be represented with different numbers. You can do all the same operations with them, like changing the vector's length and direction, calculating the angle between the vectors, and so on, and you'll get the same results in both bases, because they are operations on the same objects. The choice of basis is merely cosmetic from this point of view.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Case of Language&lt;/head&gt;
    &lt;p&gt;Now let's turn to language and see the parallel.&lt;/p&gt;
    &lt;p&gt;Contrary to popular belief, a concept is not a word or a group of words but an abstract object in your mind.&lt;/p&gt;
    &lt;p&gt;But a concept is not very useful in this abstract state. We need a way to write it down so that we can manipulate it with grammar and communicate it to others. We do this by choosing a language that is shared with the receiver of the message.&lt;/p&gt;
    &lt;p&gt;For the concept in my head right now, and choosing the English language to represent it, you get this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Going to Tokyo"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is where words come into play. By projecting the abstract idea onto the standard English vocabulary and grammar, you can assign it a list of words:&lt;/p&gt;
    &lt;p&gt;Those words are the equivalent of the coordinates of the vectors in linear algebra. The way I just wrote it is not accurate, though, because it makes it look as if English only had 3 dimensions (3 words), just like the 2-element vectors were 2-dimensional. English actually has hundreds of thousands of words, so we would need a vector that long to fully represent it, with blank spaces for all the words that aren't involved in this case.&lt;/p&gt;
    &lt;p&gt;The English language offers its speakers many other words, like camel, frolic, and or, but in this case none of them was necessary to express the idea that was in my mind, so they remain empty () and absent from my utterance of "going to Tokyo".&lt;/p&gt;
    &lt;p&gt;Of course, similar to vectors with bases, you can express the same concept in a different language. If I chose Italian as the "basis", you'd get:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Andare a Tokio"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The concept is the same, but now its representation (its "word coordinates") is different:&lt;/p&gt;
    &lt;p&gt;You can, in theory, say those two different sequences of sounds in those two languages, and obtain the same effect in the mind of the receiver. The only requirement is that all people involved know both languages.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cosmetics Matter&lt;/head&gt;
    &lt;p&gt;Alright, the parallel seems plain enough. Sadly, expressing all language in that vector-like format would use up a lot of ink and is not really practical for everyday use. (LLMs kind of achieve that feat, but in a more convoluted and definitely not human-readable way.)&lt;/p&gt;
    &lt;p&gt;Why do I think it is interesting, then? Because "untranslatable" words exist.&lt;/p&gt;
    &lt;p&gt;The words that people sometimes call "untranslatable" are terms that have a clear and widely understood meaning in one language, but no equivalent in another one. I gave some examples from Japanese before, and the web is awash with blog posts enumerating curious words like that from many other languages. The key takeaway is that what only takes a single word in Language 1 can only be expressed with some accuracy in Language 2 if you use many words to explain them in all of their facets.&lt;/p&gt;
    &lt;p&gt;I wrote that the choice of basis in linear algebra is "cosmetic", because the result of an operation on vectors does not change depending on the basis. But that is only the ideal, mathematical way to look at it. We humans are not so ideal. We are weak and fallible and sometimes we even stink.&lt;/p&gt;
    &lt;p&gt;Which vector representation do you like better between these two?&lt;/p&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;p&gt;Both represent the same vector under different bases, because I chose a base that has the vector as one of the basis vectors. Here is what the two cases look like graphically:&lt;/p&gt;
    &lt;p&gt;In theory, both representations are exactly equivalent to each other. A computer wouldn't have any preference. But the second representation, the clean one with a simple one and zero, is not only easier to remember and grasp for a person, but it is also easier to handle mathematically. Things simplify easily with it, everything that is multiplied by zero becomes zero, and similarly for the multiplications by one. The calculations proceed faster and with fewer errors.&lt;/p&gt;
    &lt;p&gt;This means that, at least for our feeble organic minds, the choice of basis does matter.&lt;/p&gt;
    &lt;p&gt;The same holds for language. A concept that took several words in English...&lt;/p&gt;
    &lt;p&gt;in Japanese is a single word: joukyou (上京)&lt;/p&gt;
    &lt;p&gt;Looking at it in the other direction, a native word in one language is like one of its "basis vectors"—simple and straightforward—but the underlying concept might need to be "spread out" onto several words when applying a different language (i.e. "basis").&lt;/p&gt;
    &lt;p&gt;Arguably, having a compact word makes it easier not only to express the concept, but also to think about it. This is the Sapir-Whorf debate, but I'll leave that for another day. Instead, I want to show you what this means for untranslatability, because there are people who vehemently deny their existence.&lt;/p&gt;
    &lt;head rend="h3"&gt;Losing in Translation&lt;/head&gt;
    &lt;p&gt;I think there are two ways in which this analogy makes it quite obvious that "practical untranslatability" is a thing.&lt;/p&gt;
    &lt;p&gt;First, communication is costly, and we don't have infinite time and space to put in all the words that are needed. Even if the word could in theory be explained in the other language, usually it's not worth it.&lt;/p&gt;
    &lt;p&gt;For example, the Japanese term mono no aware (物の哀れ) could be rather accurately translated as&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a gentle, poignant sadness or pathos felt in response to the transient nature of all things, a deep awareness of their impermanence that evokes a subtle, bittersweet sorrow and a profound, quiet empathy for their passing.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In a dictionary, perhaps that's okay. But a dictionary is not translation. Translation is about conveying the meaning of full texts, and you can't do that kind of multi-line expansion for every word.&lt;/p&gt;
    &lt;p&gt;And so the translator simplifies it to the gist, e.g. as the pathos of all things. This conveys the majority of the meaning and it is usually enough, but it does lose a lot in the process.&lt;/p&gt;
    &lt;p&gt;This is exactly analogous to the data analysis technique called Principal Component Analysis (PCA), where one simplifies a vector by picking only its largest coordinates and disregarding all others. This means choosing a subset of the basis vectors that are more closely aligned with the vectors of interest, and ignoring the existence of the other basis vectors, effectively reducing the dimensions of the data. Translators use a version of PCA every time they (begrudgingly) accept to leave the finer nuances of a concept unsaid for the sake of space.&lt;/p&gt;
    &lt;p&gt;But, even assuming you do have time to explicate, using many more words increases the risk of introducing unintended nuances that come bundled with those extra words. This is like doing PCA but selecting inappropriate basis vectors, which introduce lots of small errors in the calculation of the vector's coordinates. Is "sorrow" too emotionally charged in that long translation of mono no aware? Does the use of "passing" unnecessarily remind English speakers of people dying?&lt;/p&gt;
    &lt;p&gt;You eventually hit diminishing returns: using more words confuses the reader instead of clarifying things further.&lt;/p&gt;
    &lt;p&gt;The second problem with translation is precision. Even if you can use many words, and even if none of those are misleading, words are still finite in number. Unlike ideal numerical coordinates, which can take any value down to the finest detail, you only have a small selection of words to convey a given bit of meaning.&lt;/p&gt;
    &lt;p&gt;Suppose you realize that the word "subtle" is not accurate enough in the "...evokes a subtle, bittersweet" part of the translation above. Maybe you do want to convey subtlety, but feel that the simple word "subtle" feels too strong in this case. You might try to soften it with an adverb, like "somewhat subtle" or "slightly subtle", but there aren't many other options out there. What if none of them is perfect for your current needs?&lt;/p&gt;
    &lt;p&gt;In this sense, language is "quantized": you can jump from one level of intensity of some meaning to the next, but you can't express anything in between.&lt;/p&gt;
    &lt;p&gt;This is a problem shared by all computers. Unlike ideal numbers, the numbers in a processor necessarily have a finite number of decimal places. So when you want to work with the ideal vector&lt;/p&gt;
    &lt;p&gt;the storage limitations of your computer might mean that you have to content yourself with this truncated version:&lt;/p&gt;
    &lt;p&gt;(Modern computers can usually handle many more digits than that, but you get the idea.)&lt;/p&gt;
    &lt;p&gt;Graphically, it looks something like this:&lt;/p&gt;
    &lt;p&gt;(Incidentally, AI engineers sometimes intentionally quantize large language models to make them take up less memory, but this tends to make them dumber, because they lose nuance.)&lt;/p&gt;
    &lt;p&gt;With language, like with computers, we're forced to "lower the resolution" of our concepts whenever we put them into words. This happens twice in translated text: once when the author first writes down their thoughts, then once more when the translator transports that already-degraded concept into a different language with different "quantization steps".&lt;/p&gt;
    &lt;head rend="h3"&gt;Between the Lines&lt;/head&gt;
    &lt;p&gt;I hope these rather unorthodox leaps between linguistics and mathematics helped make it almost obvious that some words and ideas are untranslatable in practice. I also hope you don't take the analogy too seriously, because it won't go much further than this. You might be tempted to begin talking about "word matrices" and whatnot, but I doubt it would help clarify things. That kind of advanced linear algebra with concepts might work for LLMs, but it doesn't seem to map to anything intelligible for a human being, not to mention make you any wiser.&lt;/p&gt;
    &lt;p&gt;Besides, language has something going for it that doesn't seem to have a mathematical equivalent: what does it mean to "read between the lines"?&lt;/p&gt;
    &lt;p&gt;It's hard to pin down, but I think it has to do with the structure and context of the words communicating something that is not contained in any of the words themselves. Perhaps it is the clever use of those "negligible coordinates"—the fringe nuances—of words scattered around the text to produce a collective effect on the reader.&lt;/p&gt;
    &lt;p&gt;A good translator might not be able to exactly translate a given word or sentence, but they might be able to "write between the lines" so that it doesn't matter very much. ●&lt;/p&gt;
    &lt;p&gt;Cover image:&lt;/p&gt;
    &lt;p&gt;Image by vackground.com, Unsplash&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aethermug.com/posts/linear-algebra-explains-why-some-words-are-effectively-untranslatable"/><published>2025-11-14T14:46:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45927435</id><title>Oracle hit hard in Wall Street's tech sell-off over its AI bet</title><updated>2025-11-14T17:38:18.713896+00:00</updated><content>&lt;doc fingerprint="9c7f1c515ada93e9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;&lt;quote&gt;Oracle hit hard in Wall Street’s tech sell-off over its huge AI bet&lt;/quote&gt;&lt;/head&gt;&lt;head rend="h2"&gt;Try unlimited access&lt;/head&gt;Only $1 for 4 weeks&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel anytime during your trial.&lt;/p&gt;&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;FT Edit&lt;/head&gt;&lt;p&gt;Access to eight surprising articles a day, hand-picked by FT editors. For seamless reading, access content via the FT Edit page on FT.com and receive the FT Edit newsletter.&lt;/p&gt;&lt;head rend="h3"&gt;Standard Digital&lt;/head&gt;&lt;p&gt;Essential digital access to quality FT journalism on any device. Pay a year upfront and save 20%.&lt;/p&gt;&lt;head rend="h3"&gt;Premium Digital&lt;/head&gt;&lt;p&gt;Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ft.com/content/583e9391-bdd0-433e-91e0-b1b93038d51e"/><published>2025-11-14T15:04:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45928321</id><title>The American Tradition of Trying to Address Anxiety with Parks</title><updated>2025-11-14T17:38:18.542656+00:00</updated><content>&lt;doc fingerprint="29e2d03e7495450d"&gt;
  &lt;main&gt;
    &lt;p&gt;As summer approaches, America’s national parks are bracing for an influx of visitors, even as deep federal cuts to park services likely mean fewer camp employees, closed campgrounds, long lines, and cancelled programs. Travelers have been warned away from some national parks by experts, urged to reschedule for next year.&lt;/p&gt;
    &lt;p&gt;But millions are still opting to go. Last summer, a record 332 million people visited America’s 63 national parks. Based on yearly upward trends, the estimates for this summer are even higher. In a “hold-your-breath year” for national park tourism, Americans are still turning en masse to the natural environment as respite from the stresses of modern life.&lt;/p&gt;
    &lt;p&gt;The frenzy shouldn’t surprise us. With festering worries related to economic uncertainty, inflated costs, and federal policy whiplash, the popularity of park vacations is no coincidence. Rather, the rush to escape to these beautiful sanctuaries echoes a long history of Americans turning to nature for relief from anxiety, particularly during moments of sudden and widely felt changes.&lt;/p&gt;
    &lt;p&gt;In the 1870s, the United States was in the midst of the most spectacular transformations yet in its history. The end of the American Civil War brought an end to slavery and the emancipation of some 4 million Black people, while a slew of new innovations brought irreversible changes to the day-to-day lives of all Americans.&lt;/p&gt;
    &lt;p&gt;Read More: When History Tourism Puts Profit Before the Past&lt;/p&gt;
    &lt;p&gt;New machinery brought advanced manufacturing, jobs, speedier production of goods, and lower costs for consumers. Hundreds of thousands of miles of telegraph cable delivered information at break-neck speed, forever reshaping how Americans accessed news, communicated, conducted business, and envisioned the world. And the completion of a continent-crossing railroad in 1869 revolutionized travel, making it possible to move people and cargo across vast distances in hours, rather than weeks or months.&lt;/p&gt;
    &lt;p&gt;Spurred by monumental developments in technology, industry, and travel, more Americans than ever before—including new immigrants—made their way to growing cities, seeking work, education, entertainment, and exposure to new people, ideas, and possibilities.&lt;/p&gt;
    &lt;p&gt;Sudden and rapid change fired up excitement about the future. But it also stirred anxieties.&lt;/p&gt;
    &lt;p&gt;During this time, American doctors noticed more and more seemingly healthy patients with a range of complaints about hard-to-explain medical issues, including digestive problems, hair loss, sexual dysfunction, aches and pains without identifiable injuries, and profound exhaustion without obvious cause.&lt;/p&gt;
    &lt;p&gt;In response, a widely respected neurologist named George Miller Beard offered a theory. Americans, he said, were suffering from a malady called “neurasthenia.” Writing in The Boston Medical and Surgical Journal, Beard borrowed an old term used to describe “weakness of the nerves” and reintroduced it to the medical community as a “morbid condition” afflicting Americans at a worrisome rate. In his 1881 book American Nervousness, Beard also pinpointed the key culprit: modern change.&lt;/p&gt;
    &lt;p&gt;For instance, new communication technology delivered shocking news of faraway crime, disaster, and war; mechanization in industry brought extreme economic volatility and labor strife; speedy railroad travel introduced the real possibility of horrific accidents involving “wholesale killings.” Even the invention of the pocket watch, a simple hand-held timepiece, fostered a maniacal obsession with punctuality. Americans were “under constant strain,” Beard warned, “to get somewhere or to do something at some definite moment.”&lt;/p&gt;
    &lt;p&gt;Constant strain was a big problem, according to Beard and his contemporaries. Victorian-era neurologists theorized that the body functioned like an electrical machine, powered by energy distributed through the nervous system. When Americans spent too much energy navigating the extreme shifts and new worries in their modern lives, they experienced aches, pains, exhaustion, irritability, and malaise. Doctors also theorized that urban life only made such conditions worse by further taxing and weakening the body.&lt;/p&gt;
    &lt;p&gt;In response, a range of popular remedies and medical treatments for neurasthenia emerged. Some doctors recommended that women suffering symptoms should halt all physical and intellectual activity. Colloquially known as the “rest cure,” this treatment—famously recounted in “The Yellow Wallpaper,” a horror novella written by Charlotte Perkins Gilman—involved isolation in the home, bed rest for weeks, and an embargo on reading, writing, drawing, socializing, and exercising.&lt;/p&gt;
    &lt;p&gt;Women patients and doctors, including New York City physician Grace Peckham, successfully argued that the rest cure was not only quack medicine but more harmful to patients than the nervous sickness itself. Thus, it didn’t stick.&lt;/p&gt;
    &lt;p&gt;What did catch on was the “West cure,” a different kind of treatment originally reserved for men. Neurologists worried that the urban environment, factory work and office jobs, and other modern pressures were making men tired, indecisive, and physically weak. On doctor’s orders, male patients ventured into the western wilderness, where, it was thought, the natural environment would inspire the mind and reinvigorate the body. Prescriptions emphasized physical exercise, including hiking and horseback riding.&lt;/p&gt;
    &lt;p&gt;The legacies of this are notable. In the 1880s, Theodore Roosevelt, a young, well-to-do New Yorker at the time, suffered from a range of neurasthenic conditions including asthma, and he sought treatment. Roosevelt was so inspired by his own privileged experience of the West cure, and its restorative outcomes, that later, as president, he built upon state park preservation and forest protection acts to dramatically expand federal support for public access to park lands, including National Parks. Most famously, in 1903, Roosevelt partnered with naturalist John Muir—also diagnosed as neurasthenic—to expand federal protection for Yosemite in the Sierra Nevada mountain range in California.&lt;/p&gt;
    &lt;p&gt;Initially, it was urban elite white men, like Roosevelt, who were most likely to have the means to travel and to pay for the therapy of riding horses, hunting game, and sleeping under the stars. But the notion of the natural world as an antidote for the stresses of modern life appealed broadly, across lines of class, race, and gender.&lt;/p&gt;
    &lt;p&gt;Although few Americans had access to medical care in the late 19th and early 20th centuries, the idea that the body could be recharged through outdoor physical activity caught on thanks to the low-cost medical pamphlets, ads for over-the-counter remedies, advice columns, and simple word of mouth. The media-fueled desire to fend off neurasthenia drove a booming market in exercise equipment, including bicycles, and participation in cheap outdoor sports, like baseball and pedestrianism, a competitive walking trend.&lt;/p&gt;
    &lt;p&gt;Read More: As Grand Canyon National Park Turns 100, Its Chief Ranger Plans for the Next Century&lt;/p&gt;
    &lt;p&gt;By the end of the 19th century, city planners, imagining more healthful, walkable, livable urban environments, also incorporated green spaces for urban residents to enjoy for free. From small picnic areas and playgrounds to sprawling urban parks designed to feel like the bucolic countryside, American cities began providing West cure benefits without the steep price tag or the need to travel.&lt;/p&gt;
    &lt;p&gt;Camping became another popular, and more affordable, option for vacations from modernity. Working people could purchase a simple tent, one-burner stove, and a few other provisions, load up the horse and buggy and head to a park or campground just outside the city. This cheap and accessible alternative to West cure travel ballooned in popularity in the early 20th century, with the proliferation of camping guides and camping clubs, the growth of the National Park Service, and the introduction of the car. Enthusiasm for camping and national park tourism as affordable restorative activities endured through the 20th century. And they remain as popular as ever today.&lt;/p&gt;
    &lt;p&gt;Neurasthenia as a diagnostic category, has not endured. It disappeared in the early 20th century, thanks mainly to the rise of psychoanalysis and expanding knowledge about mental health and conditions like chronic fatigue, anxiety disorders, phobias, and depression.&lt;/p&gt;
    &lt;p&gt;But its most popular remedy—particularly exercise, outdoor recreation, and reflection in nature—has proved truly beneficial for both mental and physical health.&lt;/p&gt;
    &lt;p&gt;Amid unsettling changes, Americans touted the curative powers of the natural world, fueling the call for outdoor exercise and recreation, and laying the groundwork for the astounding growth of national and state park tourism. Today, with so much to worry about, it is important to remember how national and state parks, and the workers who run and sustain them, have long played a healing role in American society. As we head off to America’s many majestic park destinations—our favorite “mental health escapes” and “calmcation” getaways—may this history reinforce the need to preserve, protect, and invest in them, especially in uncertain times.&lt;/p&gt;
    &lt;p&gt;Felicia Angeja Viator is associate professor of history at San Francisco State University, a culture writer, and curator for the GRAMMY Museum.&lt;/p&gt;
    &lt;p&gt;Made by History takes readers beyond the headlines with articles written and edited by professional historians. Learn more about Made by History at TIME here. Opinions expressed do not necessarily reflect the views of TIME editors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://time.com/7288956/american-tradition-anxiety-parks/"/><published>2025-11-14T16:17:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45928492</id><title>'No One Lives Forever' Turns 25 and You Still Can't Buy It Legitimately</title><updated>2025-11-14T17:38:18.337413+00:00</updated><content>&lt;doc fingerprint="3f72d83d73abbdf1"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Maybe we’ll be able to play this game legitimately by the time Bobby Bonilla stops making his million and change per year.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ahh, a refreshing bit of optimism in these dark times!&lt;/p&gt;
    &lt;p&gt;One of my favorite things in all of professional sports is the unofficial holiday referred to as “Bobby Bonilla Day.” The short version of it is that Bonilla played for the New York Mets decades ago and eventually bought out his contract in 2000 when they decided they were done with him. Rather than pay the $5.9 million buyout of the contract up front, the team instead made the bonkers decision to negotiate a deferred payment schedule for that amount with 8% interest over the course of 25 years. The result is that the Mets will be paying Bonilla $1.2 million per year every July 1st, starting in 2011 and ending in 2035. And if you can’t make sense of the math on that one, it’s because you aren’t aware that the Mets ownership was one of Bernie Madoff’s many victims, which is why they had to defer the payments.&lt;/p&gt;
    &lt;p&gt;November 10th is not Bobby Bonilla Day. But it should be named “Let Us Play No One Lives Forever, You Assholes Day.” The classic spy-shooter turned 25 on that date and, for the exact same reasons we’ve detailed for a god damned decade now, you still can’t buy the game.&lt;/p&gt;
    &lt;p&gt;Here’s the short of it. Due to a series of mergers, closures, and rights purchases, the IP rights for No One Lives Forever and its sequel have been potentially split into three pieces between Warner Bros., Activision, and 20th Century Fox, like it was some kind of fucking horcrux. I say potentially because nobody really knows who owns what, if anything, when it comes to these games. When one company, Nightdive Studios, attempted to remaster and re-release the game as they’ve done with other titles, along with securing trademark rights to the game which hasn’t been sold in over a decade, all three companies complained that they may have rights to it and may sue over it.&lt;/p&gt;
    &lt;p&gt;All of those qualifiers are, again, because even these companies themselves don’t know what rights they actually have. And why is that? Well, because the gaming rights deals were inked before digital storage was widely used for this sort of thing and, well, nobody seems to be able to locate the actual paperwork denoting who owns what. Here’s an example of an exchange Nightdive had with Activision.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“So we went back to Activision and, [after] numerous correspondence going back and forth, they replied that they thought they might have some rights, but that any records predated digital storage. So we’re talking about a contract in a box someplace.” Kuperman laughed. “The image I get is the end of Indiana Jones… somewhere in a box, maybe in the bowels of Activision, maybe it was shipped off to Iron Mountain or somewhere. And they confessed, they didn’t have [their] hands on it. And they weren’t sure that they even had any of those rights.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Which didn’t keep Activision from warning Nightdive that it might totally sue if it moved forward with remastering the game. The other companies made similar noises.&lt;/p&gt;
    &lt;p&gt;So what’s a person to do if they want to play this game? You can’t buy it legitimately currently. It’s not even for sale anywhere. And a situation like that, which I’ve stated before, completely breaks the copyright bargain. The only option is, as Kotaku of all places notes, to download it for free from somewhere.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Downloading games that are available for sale is piracy. It’s illegal, and it’s not supportive of developers and their art. But when companies have gone out of their way to refuse to take your money for a game for the better part of two decades, it’s a very different situation. Look, I’m not your real mom and dad, and I can’t tell you what to do. But if you were to click on this link (link removed by Techdirt due to us not knowing where it takes you) and download both games (as well as spin-off Contract Jack), you’d end up with modernized versions of these classic games, with mods that allow them to work on Windows 10 and 11, and in widescreen. And what better time to do (or not do) this than on the first game’s 25th anniversary?&lt;/p&gt;
      &lt;p&gt;At this point (as indeed it was over eight years ago, the last time I suggested just downloading it, to no negative response at all) we have to consider No One Lives Forever to be abandonware. No one is willing to take ownership of it, although those that could do so sometimes mindlessly threaten to intervene should anyone else try to rebuild it for sale. Nightdive were scared off a decade ago, and it’s been sitting on GOG’s Dreamlist since that launched earlier this year (with 87,171 people saying they’d pay for it if they could). It’s far too small of a concern for any of the megacorps who might own it to spend the time and money to work out if they do, but it’s far too big of a concern within gaming history to be allowed to just disappear. Thank goodness for the anonymous heroes running NOLF Revival. I thank them for their service.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It’s the only option the public has to play this game and enjoy this small piece of our collective culture. The real answer here is some sort of copyright reform that makes this situation not a thing. If a company, or group of companies, won’t offer a piece of work for sale, can’t be bothered to understand what they own of it, if anything, and have no plans to figure any of that out… then how can this be copyright infringement?&lt;/p&gt;
    &lt;p&gt;So happy “Let Us Play No One Lives Forever, You Assholes” Day. Maybe we’ll be able to play this game legitimately by the time Bobby Bonilla stops making his million and change per year.&lt;/p&gt;
    &lt;p&gt; Filed Under: copyright, ip rights, no one lives forever, video games &lt;lb/&gt; Companies: 20th century fox, activision, microsoft, nightdive, warner bros. &lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Maybe we’ll be able to play this game legitimately by the time Bobby Bonilla stops making his million and change per year.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ahh, a refreshing bit of optimism in these dark times!&lt;/p&gt;
    &lt;p&gt;IANAL, but I’d say just release and let them sue.&lt;lb/&gt; If they can’t find the documentation during discovery, case dismissed. Maybe even counter-sue for legal fees.&lt;/p&gt;
    &lt;p&gt;But then they have an incentive to look for the paperwork. they don’t have any particular reason to go looking right now but if they see money and a lawsuit they’ll assign someone to start digging.&lt;/p&gt;
    &lt;p&gt;You could be in for a really big payout to them.&lt;/p&gt;
    &lt;p&gt;So it would be a gamble, and the potential payoff is probably not worth the risk you would be taking.&lt;/p&gt;
    &lt;p&gt;I am baffled why we make intellectual property as legally upheld as physical property, and yet we still end up with these areas that are so untouchably gray area nobody can do anything.&lt;/p&gt;
    &lt;p&gt;In terms of physical property gray areas, there are certainly disputed areas, but almost nowhere on Earth do people avoid due to the ambiguity. The only real example I know is Bir Tawil.&lt;/p&gt;
    &lt;p&gt;Those NOLF IP “feeling unproductive, might litigate later… idk &amp;lt;3&amp;lt;3&amp;lt;3” hogging assholes can burn in… molten lava? Very nice.&lt;/p&gt;
    &lt;p&gt;The fact that the link in the article is indeed a valid, working dedicated site for hosting “pirated” copies of these games and it hasn’t been taken down in nearly a decade says a lot about how much the potential “rightsholders” genuinely do not give a damn about this series.&lt;/p&gt;
    &lt;p&gt;Feels like copyright law should have stipulations for cases like this, especially considering how long terms are right now – corporations shouldn’t be sitting on copyrights doing absolutely nothing with them like dragon hoards.&lt;/p&gt;
    &lt;p&gt;No-one plays…forever. Well, legally anyway.&lt;lb/&gt; The only games are being played by the corporations, and played so poorly that EVERYONE loses.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.techdirt.com/2025/11/13/no-one-lives-forever-turns-25-you-still-cant-buy-it-legitimately/"/><published>2025-11-14T16:31:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45928822</id><title>RetailReady (YC W24) Is Hiring</title><updated>2025-11-14T17:38:17.758062+00:00</updated><content>&lt;doc fingerprint="8990dec1641c8fda"&gt;
  &lt;main&gt;
    &lt;p&gt;An AI-powered supply chain compliance engine&lt;/p&gt;
    &lt;p&gt;San Francisco - In Person&lt;/p&gt;
    &lt;p&gt;We’re RetailReady (YC W24), an AI-powered supply chain compliance engine shaking up an antiquated (and yes, unsexy) industry. Since YC, we raised a $3.3M seed round and signed over 15 enterprise customers… we’re officially in scaling mode.&lt;/p&gt;
    &lt;p&gt;RetailReady is the first AI-powered compliance engine designed for retail supply chains. We automate the messy web of compliance requirements between brands, warehouses, and retailers, turning weeks of manual work into minutes of automation. Our platform integrates deeply with warehouse operations and connects with customer systems via EDI, APIs, and flat files.&lt;/p&gt;
    &lt;p&gt;We don’t just build dashboards. We build the nervous system of compliance inside a warehouse.&lt;/p&gt;
    &lt;p&gt;We’re hiring a Support Engineer to be the first line of defense when issues arise. You’ll troubleshoot customer questions, diagnose technical issues, and flag deeper bugs for our on-call engineering team, keeping operations smooth across our growing customer base.&lt;/p&gt;
    &lt;p&gt;You’ll also help us scale smarter by documenting common issues and writing internal + customer-facing help articles to train our AI support model.&lt;/p&gt;
    &lt;p&gt;This is an in-person role in San Francisco with early hours (5 a.m. – 2 p.m.) to support our East Coast customers in real time.&lt;/p&gt;
    &lt;p&gt;If you’re an early riser who loves solving problems, thrives in fast-moving environments, and wants to help power the next generation of supply chain tech - we’d love to hear from you.&lt;/p&gt;
    &lt;p&gt;RetailReady is building an AI-powered supply chain compliance engine. Supply chains are still heavily reliant on paper processes and tribal knowledge, causing costly shipping mistakes that jeopardize the longevity of businesses. RetailReady is the first-to-market with our retail compliance packing software, leveraging camera vision to direct warehouses to ship orders without error. We are positioning our compliance data models to become the operating system that will power the next wave of warehouse robotics and automation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/retailready/jobs/kGHAith-support-engineer"/><published>2025-11-14T17:00:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45928873</id><title>Moving Back to a Tiling WM – XMonad</title><updated>2025-11-14T17:38:17.406385+00:00</updated><content>&lt;doc fingerprint="12c6cc7807b53b9c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Moving Back to a Tiling WM - XMonad&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Published on&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Authors&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;list rend="dl"&gt;
              &lt;item rend="dt-2"&gt;Author&lt;/item&gt;
              &lt;item rend="dd-2"&gt;Manas&lt;/item&gt;
              &lt;item rend="dt-3"&gt;Mastodon&lt;/item&gt;
              &lt;item rend="dd-3"&gt;mastodon @weirdsmiley&lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are my dotfiles.&lt;/p&gt;
    &lt;p&gt;When I was still using Manjaro Linux back in 2019, I got a nudge to try i3wm. It was my first experience with any window manager. And I spent nearly 5 years with it, enjoying the absolute control over my workflow. Nearing the end of 2023, when I finally decided to leave Manjaro (for good), I had a bunch of options on my hand. Fedora looked really promising at that time. But even then, I wasnât sure I was going to be using any tiling window manager. I happily switched to Gnome in Fedora 40. I ran it along with XOrg so that I could make my &lt;code&gt;capslock&lt;/code&gt; key act as a &lt;code&gt;ctrl&lt;/code&gt; when held and as an &lt;code&gt;escape&lt;/code&gt; when pressed
once, using &lt;code&gt;setxkbmap&lt;/code&gt; and &lt;code&gt;xcape&lt;/code&gt;. But only after spending a few months there,
I realized I missed that finer control at my fingertips. So, I resumed searching
for a newer tiling window manager. I was also learning Haskell at that time, so
picking up XMonad was natural.&lt;/p&gt;
    &lt;p&gt;There are a lot of things that I like about XMonad apart from its standard tiling manager features. I enjoy writing the configuration in Haskell. Where ever possible, I try to leverage the benefits of Haskellâs strong type system. Defining keybindings with a strong type system ensures that I cannot go very wrong with it. Using stack for building my configuration allows me to port the entire configuration easily to other systems, which are my various virtual machines. I have split configuration in various modules.&lt;/p&gt;
    &lt;code&gt;src
âââ Keybindings.hs
âââ Layout.hs
âââ Plugins
âÂ Â  âââ Bluetooth.hs
âÂ Â  âââ Pomodoro.hs
âÂ Â  âââ Soundtrack.hs
âââ Preferences.hs
âââ Theme
âÂ Â  âââ Dmenu.hs
âÂ Â  âââ Font.hs
âÂ Â  âââ Theme.hs
âÂ Â  âââ Xresources.hs
âââ Workspaces.hs
âââ xmobar.hs
âââ xmonad.hs

3 directories, 13 files&lt;/code&gt;
    &lt;p&gt;If you want to poke around the config according to your needs, go through Preferences.hs. It contains lots of variables which can be customized like terminal emulator, browser, scratchpads, window gap size etc. It also contains a list of applications which you would like to start automatically at boot.&lt;/p&gt;
    &lt;p&gt;Overall, the modularization has turned out to be pretty in terms of categorizing things. I tried writing a few xmobar plugins for my own needs. The guide for writing them was straightforward to begin with. I also wrote my entire xmobar configuration in Haskell itself, keeping this executable in the same project. In the end, the project itself became a one-shot way for an entire desktop environment which I can easily clone, compile and install on any system.&lt;/p&gt;
    &lt;head rend="h1"&gt;1. Setup&lt;/head&gt;
    &lt;p&gt;I will go briefly over the stack-based setup. The only thing needed is to have a &lt;code&gt;build&lt;/code&gt; script at the root of your xmonad project. Everything else is simply a
normal stack project with modules and a few executables. I have 2 executables in
my project: xmonad and xmobar.&lt;/p&gt;
    &lt;p&gt;A detailed description and example build files can be found here. My build script is simple enough.&lt;/p&gt;
    &lt;code&gt;#!/bin/sh

SRC_DIR=$HOME/.config/xmonad
WM=xmonad

unset STACK_YAML
FAIL=0

cd $SRC_DIR
stack install 2&amp;gt;.log || FAIL=1

ln -f -T $(stack exec -- which $WM) $1 2&amp;gt;.log || FAIL=2&lt;/code&gt;
    &lt;head rend="h1"&gt;2. Installation&lt;/head&gt;
    &lt;p&gt;Stack is a package manager for Haskell projects and it will be used to compile the package. Install stack either via GHCup or your distributionâs package manager.&lt;/p&gt;
    &lt;code&gt;mkdir -p $HOME/.config/xmonad
git clone --branch release https://github.com/weirdsmiley/xmonad $HOME/.config/xmonad/
cd $HOME/.config/xmonad
./install.sh&lt;/code&gt;
    &lt;p&gt;The installation script will install a few fonts and other tools which are default for this setup. It will also write &lt;code&gt;.xinitrc&lt;/code&gt; and &lt;code&gt;.Xresources&lt;/code&gt; files.&lt;/p&gt;
    &lt;p&gt;After the installation is complete, and you are logged into xmonad, pressing &lt;code&gt;alt+shift+/&lt;/code&gt; or &lt;code&gt;alt+?&lt;/code&gt; will open up a dialog box containing all available
keybindings.&lt;/p&gt;
    &lt;head rend="h1"&gt;3. Diving in&lt;/head&gt;
    &lt;head rend="h2"&gt;3.1. Layouts and per-workspace layouts&lt;/head&gt;
    &lt;p&gt;XMonad provides a very easy way to describe various layouts that workspaces can follow. I found it useful to constrain only a few layouts on each workspace. I used PerWorkspace for this. This allows me to only switch between specified set of layouts. So for example, my workspace 2 is my writing workspace, in which I have 3 applications. A browser, a pdf reader and a terminal with a tmux session attached to it. This can simply be arranged as a three column layout. But sometimes certain pdfs may have smaller font size which can be tough to read in a column. If I zoom in the pdf it spills sideways, and I have to use arrow keys or h,l to move left and right.&lt;/p&gt;
    &lt;p&gt;To tackle this, I have another layout with added magnification on top of the three column layout. It magnifies the focused window by a certain limit. And having only these two layouts in my layout set helps me in easily cycling between layouts. I donât have to skip through 4 different layouts which I would never use in this workspace.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.2. Topbar modification&lt;/head&gt;
    &lt;p&gt;By default, XMonad adds a border to the tiled window which is in focus. I took this idea from here. This adds a title bar with formatted colors. This looks nicer that having a border surrounding the window. The focused window is colored blue while unfocused is colored black. Also, having the title names in topbar looks nice, and in a way removes the need of using XMonadLogâs application names in xmobar itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.3. Type safety in keybindings&lt;/head&gt;
    &lt;p&gt;This is something which I truly adore about XMonad and writing its configuration in Haskell. I can write my keybindings in a functional manner and leverage Haskellâs type system to ensure safety. Arranging keybindings in this way, seems more fruitful than having them represented via strings.&lt;/p&gt;
    &lt;code&gt;myKeys :: XConfig Layout -&amp;gt; M.Map (KeyMask, KeySym) (X ())
myKeys conf@XConfig {XMonad.modMask = modm} =
  M.fromList -- list of keybindings
    $ [
      -- Restart XMonad
      ( (modm, xK_q), safeSpawn "xmonad" ["--restart"])
      -- Toggle fullscreen
      , ((modm, xK_f), sendMessage $ Toggle NBFULL)
      -- Lock screen
      , ((modm, xK_l), unGrab *&amp;gt; safeSpawn "env" myLockscreen)
      ]&lt;/code&gt;
    &lt;p&gt;Each keybinding is comprised of two values of types: &lt;code&gt;KeyMask&lt;/code&gt; and &lt;code&gt;KeySym&lt;/code&gt;,
followed by an &lt;code&gt;X ()&lt;/code&gt; action. If you donât want to set a keymask simply pass a &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;noModMask&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.4. Submap keybindings and makeChords&lt;/head&gt;
    &lt;p&gt;Using submaps in xmonad-contrib, I can write a utility function to easily generate a set of keybindings with an added description.&lt;/p&gt;
    &lt;code&gt;import XMonad.Actions.Submap
import qualified Data.Map as M

makeChords :: a -&amp;gt; [((KeyMask, KeySym), String, X ())] -&amp;gt; [(a, X ())]
makeChords majorKey subKeys =
  (majorKey, submap . M.fromList $ map ((k, _, a) -&amp;gt; (k, a)) subKeys)
    : [ ( majorKey
        , visualSubmap myVisualSubmapDef
            $ M.fromList
            $ map ((k, d, a) -&amp;gt; (k, (d, a))) subKeys)
      ]

soundChords modm =
  makeChords
    (modm, xK_a)
    [ ( (0, xK_a), "open alsamixer"
      , spawn $ myNamedTerminal "alsamixer" ++ " -e alsamixer")
    , ( (0, xK_m), "toggle music playing"
      , getRunningPlayer' &amp;gt;&amp;gt;= player -&amp;gt;
          spawn $ myMusicCtrl ++ " -p "" ++ player ++ "" play-pause")
    ]

myKeys conf@XConfig {XMonad.modMask = modm} =
  M.fromList $ [] ++ soundChords modm&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;makeChords&lt;/code&gt; adds two distinct sets of keybindings, one normal set and
another a visual set, which creates a dialog box when you press the main submap
key. In the example above, the &lt;code&gt;soundChords&lt;/code&gt; submap is enabled with &lt;code&gt;alt+a&lt;/code&gt;,
then you can see a dialog box containing two keybindings with their
descriptions. Pressing either &lt;code&gt;a&lt;/code&gt; or &lt;code&gt;m&lt;/code&gt; will launch the first or the second
action. The documentation also contains an example which you can read to see the actual code that will be
appended to your myKeys.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.5. Xmobar configuration in Haskell&lt;/head&gt;
    &lt;p&gt;Writing the Xmobar configuration inside the same project really allows me to keep everything in one place. I create another executable alongside the xmonad executable, in my &lt;code&gt;package.yaml&lt;/code&gt;. And then xmonad launches xmobar in the
startup apps section.&lt;/p&gt;
    &lt;code&gt;executables:
  xmonad:
    main: xmonad.hs
    dependencies:
      - xmonad
      - xmonad-contrib
      - containers
  xmobar:
    main: xmobar.hs
    dependencies:
      - xmobar
    ghc-options: -rtsopts -threaded -with-rtsopts=-N&lt;/code&gt;
    &lt;p&gt;You may have noticed a small icon beside my layout icons on the left side of xmobar. The represent the current layout in a visual form. Try switching layouts with &lt;code&gt;alt+space&lt;/code&gt; and see the icons change.&lt;/p&gt;
    &lt;code&gt;myXmobarPP =
  def
    { ppLayout =
        case
          "Columns" -&amp;gt; "&amp;lt;icon=Columns.xpm/&amp;gt;"
          "MagnifiedColumns" -&amp;gt; "&amp;lt;icon=MagnifiedColumns.xpm/&amp;gt;"
          "Full" -&amp;gt; "&amp;lt;icon=Full.xpm/&amp;gt;"
          "Tall" -&amp;gt; "&amp;lt;icon=Tall.xpm/&amp;gt;"
          "ThreeCol" -&amp;gt; "&amp;lt;icon=MagnifiedColumns.xpm/&amp;gt;"
          "2-by-3 (left)" -&amp;gt; "&amp;lt;icon=TwoByThreeLeft.xpm/&amp;gt;"
          "2-by-3 (right)" -&amp;gt; "&amp;lt;icon=TwoByThreeRight.xpm/&amp;gt;"
          "2x3 LT" -&amp;gt; "&amp;lt;icon=TwoByThreeLeftWithTabs.xpm/&amp;gt;"
          "2x3 RT" -&amp;gt; "&amp;lt;icon=TwoByThreeRightWithTabs.xpm/&amp;gt;"
          "Tiled" -&amp;gt; "&amp;lt;icon=Tiled.xpm/&amp;gt;"
          _ -&amp;gt; "&amp;lt;icon=Unknown.xpm/&amp;gt;"
    }

main =
  xmonad
    . withEasySB (statusBarProp "xmobar" (pure myXmobarPP)) defToggleStrutsKey
    $ myConfig&lt;/code&gt;
    &lt;head rend="h2"&gt;3.6. Scratchpads in action&lt;/head&gt;
    &lt;p&gt;I am using 4 scratchpads in total. Each scratchpad is mapped to a keybinding.&lt;/p&gt;
    &lt;code&gt;  [
  -- Open Scratchpad
  ((modm, xK_Return), namedScratchpadAction myScratchpads "terminal")
  -- Open Kanboard session
  , ((modm, xK_x), namedScratchpadAction myScratchpads "Kanboard")
  -- Open CalibreWeb
  , ((modm, xK_z), namedScratchpadAction myScratchpads "CalibreWeb")
  -- Open Anki
  , ((modm, xK_m), namedScratchpadAction myScratchpads "Anki")
  ]

myScratchpads
 =
  [ NS "terminal" spawnTerm findTerm manageTerm
  , NS "Kanboard" spawnKanboard (className =? "Kanboard") doFullFloat
  , NS "Anki" spawnAnki (className =? "Anki") doFullFloat
  , NS "CalibreWeb" spawnCalibreWeb (className =? "CalibreWeb") doFullFloat
  ]
  ...&lt;/code&gt;
    &lt;p&gt;I realized that I donât really open new terminals that often because I use tmux (with tmux-resurrect and tmux-continuum). So I remapped &lt;code&gt;alt+enter&lt;/code&gt; with showing the terminal scratchpad, instead of the usual, open a new terminal.&lt;/p&gt;
    &lt;p&gt;I can open up the calibre-web instance with &lt;code&gt;alt+z&lt;/code&gt;, and immediately resume
whatever I was reading.&lt;/p&gt;
    &lt;p&gt;If you have any questions for me, head over to this discussion page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wssite.vercel.app/blog/moving-back-to-a-tiling-wm-xmonad"/><published>2025-11-14T17:04:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45928912</id><title>You misunderstand what it means to be poor</title><updated>2025-11-14T17:38:16.901014+00:00</updated><content>&lt;doc fingerprint="bf40ba5146f6accd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You misunderstand what it means to be poor&lt;/head&gt;
    &lt;p&gt;The more I speak about being poor, the more I realize how fundamentally other folks misunderstand what it means to be poor versus being broke. The advice folks will give comes from a good place. However, I would like to give some examples to help everyone understand most advice from non-poor folks isn’t helpful.&lt;/p&gt;
    &lt;p&gt;Everyone has experienced being broke. Being broke sucks. You are watching every dollar spent, finding wayys to trim or make things stretch until the next payday. The difference is when you are broke you have some money. You can afford to put gas in your car, but not enough to do that repair. Money is tight, but you can get the basics at the grocery store. You can’t afford to go to the movies, but will stay home and watch what’s new on Netflix. Being broke sucks. You are watching every dollar spent, finding ways to trim or make things stretch until the next payday.&lt;/p&gt;
    &lt;p&gt;When you are poor that next payday brings no relief. It is like an endless runner game. No matter how fast you run or how high you jump you can never see the finish line. No matter how tired you are the ground keeps moving. There is no room for errors as the punishment for mistakes is astronomical. When you hit an obstacle you don’t restart from the last checkpoint, you go back to the beginning.&lt;/p&gt;
    &lt;p&gt;There is this mindset from folks that poor people must not be smart. I mean, you’re smart and you’re not poor! The problem must be a skill issue! Learn the skills to do it yourself and you’ll be able to pull yourself up.&lt;/p&gt;
    &lt;p&gt;The other mindset is poor people are lazy. Quit complaining and do it yourself! Just get a better job! Get a second job! There’s money out there, you just have to go get it.&lt;/p&gt;
    &lt;p&gt;The last is these folks think they understand what it is like to be poor. Hey, I was a broke college student and I get being poor! I had a rough patch, it will pass.&lt;/p&gt;
    &lt;p&gt;Let’s start at the top.&lt;/p&gt;
    &lt;p&gt;I have a van that is falling apart. It needs a lot of work that we cannot afford to do. In the mindset that poor people are unskilled, it appears that I should watch some YouTube videos, get the parts, and do it myself. The misunderstanding is that being poor means you have tons and tons of skills. You have to fix everything yourself. There is never, I mean never, a time you can pay someone to fix it for you.&lt;/p&gt;
    &lt;p&gt;In this example folks think, “If the repair at the shop costs $1,000, but the parts cost $300, you can save a lot of money doing it yourself.” You are absolutely correct. Yet, I still need to be able to afford the $300 in parts.&lt;/p&gt;
    &lt;p&gt;Do you see the misunderstanding? I can’t make $300 appear out of thin air. I have the skills, I’ve had to fix all my cars myself. I’ve done complete engine rebuilds, I’ve replaced transmissions, I do all my own regular maintenance. The problem isn’t skills, its money. When you are broke, spending $300 instead of $1,000 sounds like a win because you can’t afford the $1,000. When you’re poor $300 might as well be $1,000 or $10,000, you will never afford it.&lt;/p&gt;
    &lt;p&gt;This is not a matter of time, either. I can’t put aside money each month and then get it. There is never money to put aside. I can’t put it on the credit card as I know I will never be able to pay it. I’ll just have this $300 debt looming over me, increasing with interest every month, mocking how much of a loser I am.&lt;/p&gt;
    &lt;p&gt;The second mindset: Being lazy.&lt;/p&gt;
    &lt;p&gt;How do I have the time to work multiple jobs when I’m doing all this extra work? How do I have the time when in my extra time I’m fixing cars, appliances, the roof, and cooking every meal from scratch?&lt;/p&gt;
    &lt;p&gt;Should I work a second job and never see my wife? My kids? Should I never have any personal time? Should my entire life revolve around money? Should I kill myself for capitalism?&lt;/p&gt;
    &lt;p&gt;Folks who say things like this have only ever experienced being broke. It is a temporary situation and a few months of extra income will solve the problem. Being poor is not missing $1,000 or $10,000 in the short term. It’s missing $40,000 a year, every year, forever. There is no short term relief. This isn’t a rough patch. It is The Pit in The Dark Knight Rises.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“There’s a reason why this prison is the worst hell on earth… Hope. Every man who has rotted here over the centuries has looked up to the light and imagined climbing to freedom. So easy… So simple… And like shipwrecked men turning to sea water from uncontrollable thirst, many have died trying. I learned here that there can be no true despair without hope.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, it is possible to escape. Hell, two people have done it! Why can’t you do it! But you are completely ignoring how many people have fallen to their death trying.&lt;/p&gt;
    &lt;p&gt;All of the general guidance to escape being poor is actually advice for getting through being broke.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cancel Netflix&lt;/item&gt;
      &lt;item&gt;Make food at home&lt;/item&gt;
      &lt;item&gt;Stop going to Starbucks&lt;/item&gt;
      &lt;item&gt;Fix it yourself&lt;/item&gt;
      &lt;item&gt;Don’t upgrade your phone&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are all things that will help you temporarily. It will help you get through a short period of being broke. Or it will help you get your spending under control. You have enough money, you just don’t spend it wisely.&lt;/p&gt;
    &lt;p&gt;Being poor is you already did all those things. You cancelled all your streaming services years ago. You make all your food from scratch all the time. You never go to fucking Starbucks. You fix everything yourself. You already stretch everything to the limit. That is how you have to live every day of your life, for eternity, with no relief in sight.&lt;/p&gt;
    &lt;p&gt;Last example and is pertinent to our times in the US. A lot of poor folks are having to stand in line for hours and hours to get food at a food bank due to goverment ineptitude. The advice to simply cook at home doesn’t fix that there isn’t any food at home.&lt;/p&gt;
    &lt;p&gt;Do you honestly think people standing in line at a food bank could fix their situation if they stopped getting DoorDash? Going to Starbucks? Fuck off. They weren’t doing that already.&lt;/p&gt;
    &lt;p&gt;How are they to get another job or put in extra hours if they have to stand in line for 3 hours to get food? Should they go without food until they get that job and the paycheck?&lt;/p&gt;
    &lt;p&gt;You need to step aside and think about the differences between being broke and being poor.&lt;/p&gt;
    &lt;p&gt;- - - - -&lt;/p&gt;
    &lt;p&gt;Thank you for reading! If you would like to comment on this post you can start a conversation on the Fediverse. Message me on Mastodon at @cinimodev@masto.ctms.me. Or, you may email me at blog.discourse904@8alias.com. This is an intentionally masked email address that will be forwarded to the correct inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.ctms.me/posts/2025-11-14-being-poor-or-being-broke/"/><published>2025-11-14T17:08:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45929052</id><title>Germany to Ban Huawei from Future 6G Network in Sovereignty Push</title><updated>2025-11-14T17:38:16.515261+00:00</updated><content>&lt;doc fingerprint="fcf60ee1a3011678"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Germany to Ban Huawei From Future 6G Network in Sovereignty Push&lt;/head&gt;
    &lt;p&gt;German Chancellor Friedrich Merz said Chinese suppliers such as Huawei Technologies Co. will be excluded from the country’s future telecommunication networks on security grounds as he pushes for more digital sovereignty.&lt;/p&gt;
    &lt;p&gt;“We have decided within the government that everywhere it’s possible we’ll replace components, for example in the 5G network, with components we have produced ourselves,” Merz told a business conference in Berlin on Thursday. “And we won’t allow any components from China in the 6G network.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bloomberg.com/news/articles/2025-11-13/germany-to-ban-huawei-from-future-6g-network-in-sovereignty-push"/><published>2025-11-14T17:19:08+00:00</published></entry></feed>