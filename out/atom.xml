<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-15T22:35:39.795784+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45249915</id><title>PayPal to support Ethereum and Bitcoin</title><updated>2025-09-15T22:35:49.130669+00:00</updated><content>&lt;doc fingerprint="bc09acb02f2e3d8"&gt;
  &lt;main&gt;
    &lt;p&gt;PayPal Ushers in a New Era of Peer-to-Peer Payments, Reimagining How Money Moves to Anyone, Anywhere&lt;/p&gt;
    &lt;p&gt;Send and receive money as easily as sending a text, across apps, borders, and currencies &lt;/p&gt;
    &lt;div&gt;
      &lt;p&gt;SAN JOSE, Calif., Sept. 15, 2025 /PRNewswire/ -- On the heels of the PayPal World announcement, a global platform connecting the world's largest digital payment systems and wallets, PayPal today introduced PayPal links, a new way to send and receive money through a personalized, one-time link that can be shared in any conversation.&lt;/p&gt;
      &lt;p&gt;&lt;lb/&gt; Creating personalized payment links | Click to Enlarge&lt;/p&gt;
      &lt;p&gt;PayPal users in the U.S. can begin creating personalized payment links today, with international expansion to the UK, Italy, and other markets starting later this month. By making payments this simple and universal, PayPal links helps drive new customer acquisition and brings more users into the PayPal ecosystem.&lt;/p&gt;
      &lt;p&gt;The peer-to-peer (P2P) experience is about to go even further. Crypto will soon be directly integrated into PayPal's new P2P payment flow in the app. This will make it more convenient for PayPal users in the U.S. to send Bitcoin, Ethereum, PYUSD, and more, to PayPal, Venmo, as well a rapidly growing number of digital wallets across the world that support crypto and stablecoins.&lt;/p&gt;
      &lt;p&gt;Expanding what people can do with PayPal also comes with reassurance around how personal payments are handled. As always, friends-and-family transfers through Venmo and PayPal are exempt from 1099-K reporting. Users won't receive tax forms for gifts, reimbursements, or splitting expenses, helping ensure that personal payments stay personal.&lt;/p&gt;
      &lt;p&gt;"For 25 years, PayPal has revolutionized how money moves between people. Now, we're taking the next major step," said Diego Scotti, General Manager, Consumer Group at PayPal. "Whether you're texting, messaging, or emailing, now your money follows your conversations. Combined with PayPal World, it's an unbeatable value proposition, showing up where people connect, making it effortless to pay your friends and family, no matter where they are or what app they're using."&lt;/p&gt;
      &lt;p&gt;P2P is a cornerstone of PayPal's consumer experience, driving engagement and bringing more users into the ecosystem. P2P and other consumer total payment volume saw solid growth in the second quarter, increasing 10% year-over-year as the company focused on improving the experience and increasing user discoverability to make it easier than ever to move money globally. Plus, Venmo saw its highest TPV growth in three years. With PayPal World unlocking seamless interoperability, P2P is poised for even greater momentum in the future as PayPal and Venmo connect to billions of wallets worldwide.&lt;/p&gt;
      &lt;p&gt;How PayPal links work:&lt;/p&gt;
      &lt;list type="disc" rend="ul"&gt;
        &lt;item&gt;Create a personalized link – Open the PayPal app, enter the details of your payment or request, and generate a unique, one-time link to share.&lt;/item&gt;
        &lt;item&gt;Always the right person – Each link is private, one-time use, and created for a specific transaction.&lt;/item&gt;
        &lt;item&gt;Drop it anywhere – Send your link in a text, DM, email, or chat. Add a note, emoji, or payment note.&lt;/item&gt;
        &lt;item&gt;Manage payment activity: Unclaimed links expire after 10 days. Users can send a reminder or even cancel the payment or request before the link is claimed with the PayPal app.&lt;/item&gt;
        &lt;item&gt;Tap and done – The recipient taps the link and either completes or accepts the payment within the PayPal App with their PayPal account.&lt;/item&gt;
        &lt;item&gt;Funds are instant – the recipient will get immediate access to their funds with a PayPal Balance account once accepted.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;About PayPal&lt;lb/&gt; PayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy. For more information, visit https://www.paypal.com, https://about.pypl.com/ and https://investor.pypl.com/.&lt;/p&gt;
      &lt;p&gt;About PayPal USD (PYUSD) &lt;lb/&gt; PayPal USD is issued by Paxos Trust Company, LLC, a fully chartered limited purpose trust company. Paxos is licensed to engage in Virtual Currency Business Activity by the New York State Department of Financial Services. Reserves for PayPal USD are fully backed by U.S. dollar deposits, U.S. Treasuries and similar cash equivalents, and PayPal USD can be bought or sold through PayPal and Venmo at a rate of $1.00 per PayPal USD. &lt;lb/&gt; PayPal, Inc. (NMLS ID #: 910457) is licensed to engage in Virtual Currency Business Activity by the New York State Department of Financial Services. &lt;/p&gt;
      &lt;p&gt;Media contact&lt;lb/&gt; Gideon Anstey&lt;lb/&gt; gbanstey@paypal.com&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://newsroom.paypal-corp.com/2025-09-15-PayPal-Ushers-in-a-New-Era-of-Peer-to-Peer-Payments,-Reimagining-How-Money-Moves-to-Anyone,-Anywhere"/><published>2025-09-15T14:04:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45250202</id><title>How to self-host a web font from Google Fonts</title><updated>2025-09-15T22:35:49.011804+00:00</updated><content/><link href="https://blog.velocifyer.com/Posts/3,0,0,2025-8-13,+how+to+self+host+a+font+from+google+fonts.html"/><published>2025-09-15T14:33:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45250328</id><title>GuitarPie: Electric Guitar Fretboard Pie Menus</title><updated>2025-09-15T22:35:48.564871+00:00</updated><content>&lt;doc fingerprint="be9bdb1a0d168a50"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;&lt;div&gt;Video coming soon&lt;/div&gt;ACM UIST 2025&lt;lb/&gt;Frank Heyen, Marius Labudda, Michael Sedlmair, Andreas Fender&lt;p&gt;Nowadays, electric guitars are often used together with digital interfaces. For instance, tablature applications can support guitar practice by rendering and playing back the tabs of individual instrument tracks of a song (guitar, drums, etc.). However, those interfaces are typically controlled via mouse and keyboard or via touch input. This means that controlling and configuring playback during practice can lead to high switching costs, as learners often need to switch between playing and interface control. In this paper, we explore the use of audio input from an unmodified electric guitar to enable interface control without letting go of the guitar. We present GuitarPie, an audio-based pie menu interaction method. GuitarPie utilizes the grid-like structure of a fretboard to spatially represent audio-controlled operations, avoiding the need to memorize note sequences. Furthermore, we implemented TabCtrl, a tablature interface that uses GuitarPie and other audio-based interaction methods for interface control.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM UIST 2024&lt;lb/&gt;Andreas Fender, Mohamed Kari&lt;p&gt;Digital pen input devices based on absolute pen position sensing, such as a Wacom pen, support high-fidelity pen input. However, they require specialized sensing surfaces like drawing tablets, which can have a large desk footprint, constrain the possible input area, and limit mobility. In contrast, digital pens with integrated relative sensing enable mobile use on passive surfaces, but suffer from motion artifacts or require surface contact at all times, deviating from natural pen affordances. We present OptiBasePen, a device for mobile pen input on ordinary surfaces. Our prototype consists of two parts: the ``base'' on which the hand rests and the pen for fine-grained input. The base features a high-precision mouse sensor to sense its own relative motion, and two infrared image sensors to track the absolute pen tip position within the base's frame of reference. This enables pen input on ordinary surfaces without external cameras while also avoiding drift from pen micro-movements. In this work, we present our prototype as well as the general base+pen concept, which combines relative and absolute sensing.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM UIST 2023&lt;lb/&gt;Andreas Fender, Derek Alexander Witzig, Max Moebus, Christian Holz&lt;p&gt;When learning to play an instrument, it is crucial for the learner's muscles to be in a relaxed state when practicing. Identifying, which parts of a song lead to increased muscle tension requires self-awareness during an already cognitively demanding task. In this work, we investigate unobtrusive pressure sensing for estimating muscle tension while practicing songs with the guitar. First, we collected data from twelve guitarists. Our apparatus consisted of three pressure sensors (one on each side of the guitar pick and one on the guitar neck) to determine the sensor that is most suitable for automatically estimating muscle tension. Second, we extracted features from the pressure time series that are indicative of muscle tension. Third, we present the hardware and software design of our PressurePick prototype, which is directly informed by the data collection and subsequent analysis.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM CHI 2023&lt;lb/&gt;Andreas Fender, Thomas Roberts, Tiffany Luong, Christian Holz&lt;p&gt;Digital painting interfaces require an input fidelity that preserves the artistic expression of the user. Drawing tablets allow for precise and low-latency sensing of pen motions and other parameters like pressure to convert them to fully digitized strokes. A drawback is that those interfaces are rigid. While soft brushes can be simulated in software, the haptic sensation of the rigid pen input device is different compared to using a soft wet brush on paper. We present InfinitePaint, a system that supports digital painting in Virtual Reality on real paper with a real wet brush. We use special paper that turns black wherever it comes into contact with water and turns blank again upon drying. A single camera captures those temporary strokes and digitizes them while applying properties like color or other digital effects. We tested our system with artists and compared the subjective experience with a drawing tablet.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM UIST 2022&lt;lb/&gt;Guy Luethi, Andreas Fender, Christian Holz&lt;p&gt;We present DeltaPen, a pen device that operates on passive surfaces without the need for external tracking systems or active sensing surfaces. DeltaPen integrates two adjacent lens-less optical flow sensors at its tip, from which it reconstructs accurate directional motion as well as yaw rotation. DeltaPen also supports tilt interaction using a built-in inertial sensor. A pressure sensor and high-fidelity haptic actuator complements our pen device while retaining a compact form factor that supports mobile use on uninstrumented surfaces. We present a processing pipeline that reliably extracts fine-grained pen translations and rotations from the two optical flow sensors. To asses the accuracy of our translation and angle estimation pipeline, we conducted a technical evaluation in which we compared our approach with ground-truth measurements of participants' pen movements during typical pen interactions. We conclude with several example applications that leverage our device's capabilities. Taken together, we demonstrate novel input dimensions with DeltaPen that have so far only existed in systems that require active sensing surfaces or external tracking.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM CHI 2022&lt;lb/&gt;(Best paper award)&lt;lb/&gt;Andreas Fender, Christian Holz&lt;lb/&gt; Additional links: [Full scenario] [Conference presentation] [Explainer video]&lt;p&gt;Mixed Reality is gaining interest as a platform for collaboration and focused work to a point where it may supersede current office settings in future workplaces. At the same time, we expect that interaction with physical objects and face-to-face communication will remain crucial for future work environments, which is a particular challenge in fully immersive Virtual Reality. In this work, we reconcile those requirements through a user's individual Asynchronous Reality, which enables seamless physical interaction across time. When a user is unavailable, e.g., focused on a task or in a call, our approach captures co-located or remote physical events in real-time, constructs a causality graph of co-dependent events, and lets immersed users revisit them at a suitable time in a causally accurate way. Enabled by our system AsyncReality, we present a workplace scenario that includes walk-in interruptions during a person's focused work, physical deliveries, and transient spoken messages. We then generalize our approach to a use-case agnostic concept and system architecture. We conclude by discussing the implications of an Asynchronous Reality for future offices.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM CHI 2021&lt;lb/&gt;Andreas Fender, Diego Martinez Plasencia, Sriram Subramanian&lt;p&gt;Acoustic levitation is gaining popularity as an approach to create physicalized mid-air content by levitating different types of levitation primitives. Such primitives can be independent particles or particles that are physically connected via threads or pieces of cloth to form shapes in mid-air. However, initialization (i.e., placement of such primitives in their mid-air target locations) currently relies on either manual placement or specialized ad-hoc implementations, which limits their practical usage. We present ArticuLev, an integrated pipeline that deals with the identification, assembly and mid-air placement of levitated shape primitives. We designed ArticuLev with the physical properties of commonly used levitation primitives in mind. It enables experiences that seamlessly combine different primitives into meaningful structures (including fully articulated animated shapes) and supports various levitation display approaches (e.g., particles moving at high speed). In this paper, we describe our pipeline and demonstrate it with heterogeneous combinations of levitation primitives.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;&lt;div&gt;Storyboard of video by Christian Holz&lt;/div&gt;IEEE VR 2021&lt;lb/&gt;Manuel Meier, Paul Streli, Andreas Fender, Christian Holz&lt;p&gt;In this paper, we bring rapid touch interaction on surfaces to Virtual Reality. Current systems capture input with cameras, for which touch detection remains a core challenge, often leaving free-hand mid-air interaction and controllers as viable alternatives for input. We present TapID, a wrist-based system that complements optical hand tracking with inertial sensing to detect touch events on surfaces - the input modality that users have grown used to on phones and tablets. TapID embeds a pair of inertial sensors in a flexible strap, one at either side of the wrist; from the combination of registered signals, TapID reliably detects surface touch events and, more importantly, identifies the finger used for touch, which we fuse with optically tracked hand poses to trigger input in VR. We evaluated TapID in a series of user studies on event-detection accuracy (F1 = 0.997) and finger-identification accuracy (within-user: F1 = 0.93; cross-user: F1 = 0.91 after 10 refinement taps and F1 = 0.87 with no refinement) in a seated table scenario. We conclude with a series of applications that complement hand tracking with touch input, including UI control, rapid typing, and surface gestures.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;&lt;div&gt;Video edited by Hugo Romat&lt;/div&gt;IEEE VR 2021&lt;lb/&gt;Hugo Romat, Andreas Fender, Manuel Meier, Christian Holz&lt;p&gt;Digital pen interaction has become a first-class input modality for precision tasks such as writing, annotating, and drawing. In Virtual Reality, however, input is largely detected using cameras which does not nearly reach the fidelity we achieve with analog handwriting or the spatial resolution required to enable fine-grained on-surface input.&lt;lb/&gt;We present FlashPen, a digital pen for VR whose sensing principle affords accurately digitizing hand-writing and fine-grained 2D input for manipulation. We combine absolute camera tracking with relative motion sensing from an optical flow sensor. In this paper, we describe our prototype, a user study and several application prototypes.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM ISS 2019&lt;lb/&gt;(Best application paper award)&lt;lb/&gt;Joao Belo, Andreas Fender, Tiare Feuchtner, Kaj Groenbaek&lt;p&gt;We present a digital assistance approach for applied metrology on near-symmetrical objects. In manufacturing, systematically measuring products for quality assurance is often a manual task, where the primary challenge for the workers lies in accurately identifying positions to measure and correctly documenting these measurements. This paper focuses on a use-case, which involves metrology of small near-symmetrical objects, such as LEGO bricks. We aim to support this task through situated visual measurement guides. Aligning these guides poses a major challenge, since fine grained details, such as embossed logos, serve as the only feature by which to retrieve an object's unique orientation. We present a two-step approach, which consists of (1) locating and orienting the object based on its shape, and then (2) disambiguating the object's rotational symmetry based on small visual features. We apply and compare different deep learning approaches and discuss our guidance system in the context of our use case.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM ISS 2019&lt;lb/&gt;Andreas Fender, Joerg Mueller&lt;p&gt;We present SpaceState, a system for designing spatial user interfaces that react to changes of the physical layout of a room. SpaceState uses depth cameras to measure the physical environment and allows designers to interactively define global and local states of the room. After designers defined states, SpaceState can identify the current state of the physical environment in real-time. This allows applications to adapt the content to room states and to react to transitions between states. Other scenarios include analysis and optimizations of work flows in physical environments. We demonstrate SpaceState by showcasing various example states and interactions. Lastly, we implemented an example application: A projection mapping based tele-presence application, which projects a remote user in the local physical space according to the current layout of the space.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM ISS 2018&lt;lb/&gt;Andreas Fender, Joerg Mueller&lt;p&gt;We present Velt, a flexible framework for multi RGB-D camera systems. Velt supports modular real-time streaming and processing of multiple RGB, depth and skeleton streams in a camera network. RGB-D data from multiple devices can be combined into 3D data like point clouds. Furthermore, we present an integrated GUI, which enables viewing and controlling all streams, as well as debugging and profiling performance. The node-based GUI provides access to everything from high level parameters like frame rate to low level properties of each individual device. Velt supports modular preprocessing operations like downsampling and cropping of streaming data. Furthermore, streams can be recorded and played back. This paper presents the architecture and implementation of Velt.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM CHI 2018&lt;lb/&gt;Andreas Fender, Philipp Herholz, Marc Alexa, Joerg Mueller&lt;p&gt;We present OptiSpace, a system for the automated placement of perspectively corrected projection mapping content. We analyze the geometry of physical surfaces and the viewing behavior of users over time using depth cameras. Our system measures user view behavior and simulates a virtual projection mapping scene users would see if content were placed in a particular way. OptiSpace evaluates the simulated scene according to perceptual criteria, including visibility and visual quality of virtual content. Finally, based on these evaluations, it optimizes content placement, using a two-phase procedure involving adaptive sampling and the covariance matrix adaptation algorithm. With our proposed architecture, projection mapping applications are developed without any knowledge of the physical layouts of the target environments. Applications can be deployed in different uncontrolled environments, such as living rooms and office spaces.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM UIST 2017&lt;lb/&gt;Andreas Fender, David Lindlbauer, Philipp Herholz, Marc Alexa, Joerg Mueller&lt;p&gt;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;ACM ISS 2017&lt;lb/&gt;(Best application paper award)&lt;lb/&gt;Andreas Fender, Hrvoje Benko, Andy Wilson&lt;p&gt;MeetAlive combines multiple depth cameras and projectors to create a room-scale omni-directional display surface designed to support collaborative face-to-face group meetings. With MeetAlive, all participants may simultaneously display and share content from their personal laptop wirelessly anywhere in the room. MeetAlive gives each participant complete control over displayed content in the room. This is achieved by a perspective corrected mouse cursor that transcends the boundary of the laptop screen to position, resize, and edit their own and others' shared content. MeetAlive includes features to replicate content views to ensure that all participants may see the actions of other participants even as they are seated around a conference table. We report on observing six groups of three participants who worked on a collaborative task with minimal assistance. Participants' feedback highlighted the value of MeetAlive features for multi-user engagement in meetings involving brainstorming and content creation.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;&lt;div&gt;Video: shot and edited by Ines Ben Said&lt;/div&gt;ACM SUI 2015&lt;lb/&gt;Andreas Fender, Joerg Mueller, David Lindlbauer&lt;p&gt;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;TVCG 2024&lt;lb/&gt;Lara Lenz, Andreas Fender, Julia Chatain, Christian Holz&lt;p&gt;Asynchronous digital communication is a widely applied and well-known form of information exchange. Most pieces of technology make use of some variation of asynchronous communication systems, be it messaging or email applications. This allows recipients to process digital messages immediately (synchronous) or whenever they have time (asynchronous), meaning that purely digital interruptions can be mitigated easily. Mixed Reality systems have the potential to not only handle digital interruptions but also interruptions in physical space, e.g., caused by co-workers in workspaces or learning environments. However, the benefits of such systems previously remained untested in the context of Mixed Reality. We conducted a user study (N=26) to investigate the impact that the timing of task delivery has on the participants' performance, workflow, and emotional state. Participants had to perform several cognitively demanding tasks in a Mixed Reality workspace. Inside the virtual workspace, we simulated in-person task delivery either during tasks (i.e., interrupting the participant) or between tasks (i.e., delaying the interruption). Our results show that delaying interruptions has a significant impact on subjective metrics like the perceived performance and workload.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;TVCG 2023&lt;lb/&gt;Tiffany Luong, Yi Fei Cheng, Max Moebus, Andreas Fender, Christian Holz&lt;p&gt;Virtual Reality (VR) systems have traditionally required users to operate the user interface with controllers in mid-air. More recent VR systems, however, integrate cameras to track the headset's position inside the environment as well as the user's hands when possible. This allows users to directly interact with virtual content in mid-air just by reaching out, thus discarding the need for hand-held physical controllers. However, it is unclear which of these two modalities—controller-based or free-hand interaction—is more suitable for efficient input, accurate interaction, and long-term use under reliable tracking conditions. While interacting with hand-held controllers introduces weight, it also requires less finger movement to invoke actions (e.g., pressing a button) and allows users to hold on to a physical object during virtual interaction. In this paper, we investigate the effect of VR input modality (controller vs. free-hand interaction) on physical exertion, agency, task performance, and motor behavior across two mid-air interaction techniques (touch, raycast) and tasks (selection, trajectory-tracing). Participants reported less physical exertion, felt more in control, and were faster and more accurate when using VR controllers compared to free-hand interaction in the raycast setting. Regarding personal preference, participants chose VR controllers for raycast but free-hand interaction for mid-air touch. Our correlation analysis revealed that participants' physical exertion increased with selection speed, quantity of arm motion, variation in motion speed, and bad postures, following ergonomics metrics such as consumed endurance and rapid upper limb assessment. We also found a negative correlation between physical exertion and the participant's sense of agency, and between physical exertion and task accuracy.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;IEEE ISMAR 2022&lt;lb/&gt;Yi Fei Cheng, Tiffany Luong, Andreas Fender, Paul Streli, Christian Holz&lt;p&gt;Real-world work-spaces typically revolve around tables, which enable knowledge workers to comfortably perform tasks over an extended period of time during productivity tasks. Tables afford more ergonomic postures and provide opportunities for rest, which raises the question of whether they may also benefit prolonged interaction in Virtual Reality (VR). In this paper, we investigate the effects of tabletop surface presence in situated VR settings on task performance, behavior, and subjective experience. In an empirical study, 24 participants performed two tasks (selection, docking) on virtual interfaces placed at two distances and two orientations. Our results show that a physical tabletop inside VR improves comfort, agency, and task performance while decreasing physical exertion and strain of the neck, shoulder, elbow, and wrist, assessed through objective metrics and subjective reporting. Notably, we found that these benefits apply when the UI is placed on and aligned with the table itself as well as when it is positioned vertically in mid-air above it. Our experiment therefore provides empirical evidence for integrating physical table surfaces into VR scenarios to enable and support prolonged interaction. We conclude by discussing the effective usage of surfaces in situated VR experiences and provide initial guidelines.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;&lt;div&gt;Video edited by Paul Streli&lt;/div&gt;ECCV 2022&lt;lb/&gt;Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, Christian Holz&lt;p&gt;Today's Mixed Reality head-mounted displays track the user's head pose in world space as well as the user's hands for interaction in both Augmented Reality and Virtual Reality scenarios. While this is adequate to support user input, it unfortunately limits users' virtual representations to just their upper bodies. Current systems thus resort to floating avatars, whose limitation is particularly evident in collaborative settings. To estimate full-body poses from the sparse input sources, prior work has incorporated additional trackers and sensors at the pelvis or lower body, which increases setup complexity and limits practical application in mobile settings. In this paper, we present AvatarPoser, the first learning-based method that predicts full-body poses in world coordinates using only motion input from the user's head and hands. Our method builds on a Transformer encoder to extract deep features from the input signals and decouples global motion from the learned local joint orientations to guide pose estimation. To obtain accurate full-body motions that resemble motion capture animations, we refine the arm joints' positions using an optimization routine with inverse kinematics to match the original tracking input. In our evaluation, AvatarPoser achieved new state-of-the-art results in evaluations on large motion capture datasets (AMASS). At the same time, our method's inference speed supports real-time operation, providing a practical interface to support holistic avatar control and representation for Metaverse applications.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;&lt;div&gt;Video co-edited with Paul Streli&lt;/div&gt;ACM CHI 2022&lt;lb/&gt;Paul Streli, Jiaxi Jiang, Andreas Fender, Manuel Meier, Hugo Romat, Christian Holz&lt;p&gt;Despite the advent of touchscreens, typing on physical keyboards remains most efficient for entering text, because users can leverage all fingers across a full-size keyboard for convenient typing. As users increasingly type on the go, text input on mobile and wearable devices has had to compromise on full-size typing. In this paper, we present TapType, a mobile text entry system for full-size typing on passive surfaces—without an actual keyboard. From the inertial sensors inside a band on either wrist, TapType decodes and relates surface taps to a traditional QWERTY keyboard layout. The key novelty of our method is to predict the most likely character sequences by fusing the finger probabilities from our Bayesian neural network classifier with the characters’ prior probabilities from an n-gram language model. In our online evaluation, participants on average typed 19 words per minute with a character error rate of 0.6% after 30 minutes of training. Expert typists thereby consistently achieved more than 25 WPM at a similar error rate. We demonstrate applications of TapType in mobile use around smartphones and tablets, as a complement to interaction in situated Mixed Reality outside visual control, and as an eyes-free mobile text input method using an audio feedback-only interface.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;&lt;div&gt;Video fully created by Mohamed Kari&lt;/div&gt;IEEE ISMAR 2021&lt;lb/&gt;Mohamed Kari, Tobias Grosse-Puppendahl, Luis Falconeri Coelho, Andreas Fender, David Bethge, Reinhard Schütte, Christian Holz&lt;p&gt;Despite the advances in machine perception, semantic scene understanding is still a limiting factor in mixed reality scene composition. In this paper, we present TransforMR, a video see-through mixed reality system for mobile devices that performs 3D-pose-aware object substitution to create meaningful mixed reality scenes. In real-time and for previously unseen and unprepared real-world environments, TransforMR composes mixed reality scenes so that virtual objects assume behavioral and environment-contextual properties of replaced real-world objects. This yields meaningful, coherent, and humaninterpretable scenes, not yet demonstrated by today’s augmentation techniques. TransforMR creates these experiences through our novel pose-aware object substitution method building on different 3D object pose estimators, instance segmentation, video inpainting, and pose-aware object rendering. TransforMR is designed for use in the real-world, supporting the substitution of humans and vehicles in everyday scenes, and runs on mobile devices using just their monocular RGB camera feed as input. We evaluated TransforMR with eight participants in an uncontrolled city environment employing different transformation themes. Applications of TransforMR include real-time character animation analogous to motion capturing in professional film making, however without the need for preparation of either the scene or the actor, as well as narrative-driven experiences that allow users to explore fictional parallel universes in mixed reality.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell class="pubdesc"&gt;&lt;div&gt;Video fully created by Soeren Qvist Jensen&lt;/div&gt;ACM CHI 2018&lt;lb/&gt;Soeren Qvist Jensen, Andreas Fender, Joerg Mueller&lt;p&gt;We present Inpher, a virtual reality system for setting physical properties of virtual objects using mid-air interaction. Users simply grasp virtual objects and mimic their desired physical movement. The physical properties required to fulfill that movement will then be inferred directly from that motion. We provide a 3D user interface that does not require users to have an abstract model of physical properties. Our approach leverages users' real world experiences with physics. We conducted a bodystorming to investigate users' mental model of physics. Based on our iterative design process, we implemented techniques for inferring mass, bounciness and friction. We conducted a case study with 15 participants with varying levels of physics education. The results indicate that users are capable of demonstrating the required interactions and achieve satisfying results.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andreasfender.com/publications.php"/><published>2025-09-15T14:45:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45250720</id><title>Launch HN: Trigger.dev (YC W23) – Open-source platform to build reliable AI apps</title><updated>2025-09-15T22:35:48.364436+00:00</updated><content>&lt;doc fingerprint="bf77954a6d078e42"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, I’m Eric, CTO at Trigger.dev (&lt;/p&gt;https://trigger.dev&lt;p&gt;). We’re a developer platform for building and running AI agents and workflows, open-source under the Apache 2.0 license (&lt;/p&gt;https://github.com/triggerdotdev/trigger.dev&lt;p&gt;).&lt;/p&gt;&lt;p&gt;We provide everything needed to create production-grade agents in your codebase and deploy, run, monitor, and debug them. You can use just our primitives or combine with tools like Mastra, LangChain and Vercel AI SDK. You can self-host or use our cloud, where we take care of scaling for you. Here’s a quick demo: (https://youtu.be/kFCzKE89LD8).&lt;/p&gt;&lt;p&gt;We started in 2023 as a way to reliably run async background jobs/workflows in TypeScript (https://news.ycombinator.com/item?id=34610686). Initially we didn’t deploy your code, we just orchestrated it. But we found that most developers struggled to write reliable code with implicit determinism, found breaking their work into small “steps” tricky, and they wanted to install any system packages they needed. Serverless timeouts made this even more painful.&lt;/p&gt;&lt;p&gt;We also wanted to allow you to wait for things to happen: on external events, other tasks finishing, or just time passing. Those waits can take minutes, hours, or forever in the case of events, so you can’t just keep a server running.&lt;/p&gt;&lt;p&gt;The solution was to build and operate our own serverless cloud infrastructure. The key breakthrough that enabled this was realizing we could snapshot the CPU and memory state. This allowed us to pause running code, store the snapshot, then restore it later on a different physical server. We currently use Checkpoint Restore In Userspace (CRIU) which Google has been using at scale inside Borg since 2018.&lt;/p&gt;&lt;p&gt;Since then, our adoption has really taken off especially because of AI agents/workflows. This has opened up a ton of new use cases like compute-heavy tasks such as generating videos using AI (Icon.com), real-time computer use (Scrapybara), AI enrichment pipelines (Pallet, Centralize), and vibe coding tools (Hero UI, Magic Patterns, Capy.ai).&lt;/p&gt;&lt;p&gt;You can get started with Trigger.dev cloud (https://cloud.trigger.dev), self-hosting (https://trigger.dev/docs/self-hosting/overview), or read the docs (https://trigger.dev/docs).&lt;/p&gt;&lt;p&gt;Here’s a sneak peek at some upcoming changes: 1) warm starts for self-hosting 2) switching to MicroVMs for execution – this will be open source, self-hostable, and will include checkpoint/restoring.&lt;/p&gt;&lt;p&gt;We’re excited to be sharing this with HN and are open to all feedback!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45250720"/><published>2025-09-15T15:20:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45251093</id><title>Boring work needs tension</title><updated>2025-09-15T22:35:48.194808+00:00</updated><content>&lt;doc fingerprint="89b89d7ba9f84476"&gt;
  &lt;main&gt;
    &lt;p&gt;We are all moved by great movies, cinematography, and stories. Watching them is fun because you can imagine yourself resonating with a character. You are thrilled by the tension the story creates and curious how it will be resolved.&lt;/p&gt;
    &lt;p&gt;Many find software development a dull job where you have to write exactly what your PM or client asks for. It’s exciting at first, but it can become boring after a few iterations.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Whatever doesn’t excite you, change it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When we, as developers, push ourselves to be protagonists, we discover many problems to solve — a lot of tension to resolve. Here are a few good problems for everyday devs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your CI/CD takes a huge amount of time because you forgot to leverage caching.&lt;/item&gt;
      &lt;item&gt;You forgot to add connection pooling and your service bombarded the database, causing too many open connections.&lt;/item&gt;
      &lt;item&gt;You misconfigured the garbage collector and now you have a memory leak that keeps growing.&lt;/item&gt;
      &lt;item&gt;If it takes you more than 3 seconds to understand what you wrote last week, it’s poorly written.&lt;/item&gt;
      &lt;item&gt;Latency is high for your users in Mumbai because your servers are in Singapore.&lt;/item&gt;
      &lt;item&gt;The database becomes very slow when you start dumping data in batches.&lt;/item&gt;
      &lt;item&gt;You want consistent API responses for read operations for users in both Mumbai and Singapore.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not trivial problems; they happen every day. These are our villains — irritating, unwanted, and surprising. We should eliminate them.&lt;/p&gt;
    &lt;p&gt;Pick your fight. This is one way to make your day exciting. If you can’t tackle these at work, do it in your personal projects.&lt;/p&gt;
    &lt;p&gt;If you chase the right tension, a story will follow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://iaziz786.com/blog/boring-work-needs-tension/"/><published>2025-09-15T15:44:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45251375</id><title>Asciinema CLI 3.0 rewritten in Rust, adds live streaming, upgrades file format</title><updated>2025-09-15T22:35:47.456675+00:00</updated><content>&lt;doc fingerprint="e8da08d941a42ea2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;3.0 Published on 15 Sep 2025 by Marcin Kulik&lt;/head&gt;
    &lt;p&gt;I’m happy to announce the release of asciinema CLI 3.0!&lt;/p&gt;
    &lt;p&gt;This is a complete rewrite of asciinema in Rust, upgrading the recording file format, introducing terminal live streaming, and bringing numerous improvements across the board.&lt;/p&gt;
    &lt;p&gt;In this post, I’ll go over the highlights of the release. For a deeper overview of new features and improvements, see the release notes and the detailed changelog.&lt;/p&gt;
    &lt;p&gt;First, let’s get the Rust rewrite topic out of the way. I did it because I felt like it. But seriously, I felt like it because I prefer working with Rust 100x more than with Python these days. And this type of code, with syscalls and concurrency, is way easier to deal with in Rust than in Python. That’s my experience, YMMV. Anyway, in addition to making me enjoy working with this component of asciinema again, the rewrite resulted in faster startup, easier installation (a static binary), and made many new features possible by integrating asciinema virtual terminal (also Rust) into the CLI.&lt;/p&gt;
    &lt;p&gt;Let’s look at what’s cool and new now.&lt;/p&gt;
    &lt;head rend="h2"&gt;asciicast v3 file format&lt;/head&gt;
    &lt;p&gt;The new asciicast v3 file format is an evolution of the good old asciicast v2. It addresses several shortcomings of the previous format that were discovered over the years.&lt;/p&gt;
    &lt;p&gt;The major change in the new format is the use of intervals (deltas) for timing session events. v2 used absolute timestamps (measured since session start), which had its own pros and cons. One often-brought-up issue was the difficulty of editing the recordings - timestamps of all following events had to be adjusted when adding/removing/updating events.&lt;/p&gt;
    &lt;p&gt;Other than timing, the header has been restructured, grouping related things together, e.g. all terminal-related metadata is now under &lt;code&gt;term&lt;/code&gt;. There’s also
support for the new &lt;code&gt;"x"&lt;/code&gt; (exit) event type, for storing the session exit
status. Finally, line comments are allowed by using the &lt;code&gt;#&lt;/code&gt; character as the
first character on a line.&lt;/p&gt;
    &lt;p&gt;Here’s an example of a short recording in asciicast v3 format:&lt;/p&gt;
    &lt;code&gt;{"version": 3, "term": {"cols": 80, "rows": 24, "type": "xterm-256color"}, "timestamp": 1504467315, "title": "Demo", "env": {"SHELL": "/bin/zsh"}}
# event stream follows the header
[0.248848, "o", "Hey Dougal...\n"]
[0.248848, "o", "Yes Ted?\n"]
[1.001376, "o", "Is there anything on your mind?\n"]
[3.500000, "m", ""]
[0.143733, "o", "No."]
# terminal window resized to 90 cols and 30 rows
[2.050000, "r", "90x30"]
[1.541828, "o", "Bye!"]
[0.8870, "x", "0"]
&lt;/code&gt;
    &lt;p&gt;The new format is already supported by asciinema server and asciinema player.&lt;/p&gt;
    &lt;head rend="h2"&gt;Live terminal streaming&lt;/head&gt;
    &lt;p&gt;The new CLI allows for live streaming of terminal sessions, and provides two modes for doing so.&lt;/p&gt;
    &lt;p&gt;Local mode uses built-in HTTP server, allowing people to view the stream on trusted networks (e.g. a LAN). In this mode no data is sent anywhere, except to the viewers’ browsers, which may require opening a firewall port. The CLI bundles the latest version of asciinema player, and uses it to connect to the stream from the page served by the built-in server.&lt;/p&gt;
    &lt;code&gt;$ asciinema stream --local
::: asciinema session started
::: Live streaming at http://127.0.0.1:37881
::: Press &amp;lt;ctrl+d&amp;gt; or type 'exit' to end
$ _
&lt;/code&gt;
    &lt;p&gt;Remote mode publishes the stream through an asciinema server (either asciinema.org or a self-hosted one), which acts as a relay, delivering the stream to the viewers at a shareable URL.&lt;/p&gt;
    &lt;code&gt;$ asciinema stream --remote
::: asciinema session started
::: Live streaming at https://asciinema.org/s/TQGS82DwiBS1bYAY
::: Press &amp;lt;ctrl+d&amp;gt; or type 'exit' to end
$ _
&lt;/code&gt;
    &lt;p&gt;The two modes can be used together as well.&lt;/p&gt;
    &lt;p&gt;Here’s a live stream of &lt;code&gt;btop&lt;/code&gt; running on one of the asciinema.org servers:&lt;/p&gt;
    &lt;p&gt;You can also watch it directly on asciinema.org at asciinema.org/s/olesiD03BIFH6Yz1.&lt;/p&gt;
    &lt;p&gt;Read more about the streaming architecture and supported protocols here.&lt;/p&gt;
    &lt;p&gt;asciinema player (seen above) supports all the described protocols. To make the viewing experience smooth and glitch-free, it implements an adaptive buffering mechanism. It measures network latency in real-time and adjusts the buffer size constantly, aiming for a good balance between low latency and buffer-underrun protection.&lt;/p&gt;
    &lt;p&gt;asciinema server can now record every live stream and turn it into a regular recording. At the moment, asciinema server running at asciinema.org has stream recording disabled and a concurrent live stream limit of 1, but you can self-host the server where recording is enabled and there’s no concurrent stream limit by default. The limits on asciinema.org may change. I’d like to first see how the streaming feature affects resource usage (btw, shout-out to Brightbox, which provides cloud services for asciinema.org).&lt;/p&gt;
    &lt;head rend="h2"&gt;Local-first&lt;/head&gt;
    &lt;p&gt;In the early versions of asciinema, &lt;code&gt;asciinema rec&lt;/code&gt; didn’t support saving to a
file - the recording was saved to a tmp file, uploaded to asciinema.org, and
the tmp file was removed. Later on, the CLI got the ability to specify a
filename, which allowed you to save the result of a recording session to a file
in asciicast v1 format and decide whether you want to keep it local only or
publish.&lt;/p&gt;
    &lt;p&gt;Although optional, the filename argument had long been available. However, many, many tutorials on the internet (probably including asciinema’s own docs) showed examples of recording and publishing in one go with &lt;code&gt;asciinema rec&lt;/code&gt;.
That was fine - many people loved this short path from recording to sharing.&lt;/p&gt;
    &lt;p&gt;Over the years, I started seeing two problems with this. The first one is that lots of people still think you must upload to asciinema.org, which is not true. You can save locally and nothing leaves your machine. The second one is that the optionality of the filename made it possible to unintentionally publish a recording, and potentially leak sensitive data. And it’s a completely valid concern!&lt;/p&gt;
    &lt;p&gt;Because of that, on several occasions I’ve seen negative comments saying “asciinema is shady” /m\. It was never shady. It’s just a historical thing. I just kept the original behavior for backward compatibility. asciinema.org is not a commercial product - it’s an instance of asciinema server, which is meant to give users an easy way to share, and to give a taste of what you get when you self-host the server. In fact, I encourage everyone to self-host it, as the recordings uploaded to asciinema.org are a liability for me (while being a good promotion of the project :)).&lt;/p&gt;
    &lt;p&gt;I hope this clears up any confusion and suspicion.&lt;/p&gt;
    &lt;p&gt;Anyway, many things have changed since the original behavior of &lt;code&gt;asciinema rec&lt;/code&gt; was implemented, including my approach to sharing my data with cloud
services. These days I self-host lots of services on a server at home, and I
try to avoid cloud services if I can (I’m pragmatic about it though).&lt;/p&gt;
    &lt;p&gt;The streaming feature was built from the ground up to support the local mode, which came first, and the remote mode followed.&lt;/p&gt;
    &lt;p&gt;In asciinema CLI 2.4, released 2 years ago, I made the &lt;code&gt;upload&lt;/code&gt; command show a
prompt where you have to explicitly make a decision on what to do with the
recording. It looked like this:&lt;/p&gt;
    &lt;code&gt;$ asciinema rec
asciinema: recording asciicast to /tmp/tmpo8_612f8-ascii.cast
asciinema: press &amp;lt;ctrl-d&amp;gt; or type "exit" when you're done
$ echo hello
hello
$ exit 
asciinema: recording finished
(s)ave locally, (u)pload to asciinema.org, (d)iscard
[s,u,d]? _
&lt;/code&gt;
    &lt;p&gt;It was a stopgap and a way to prepare users for further changes that are coming now.&lt;/p&gt;
    &lt;p&gt;In 3.0, the filename is always required, and the &lt;code&gt;rec&lt;/code&gt; command no longer has
upload capability. To publish a recording to asciinema.org or a self-hosted
asciinema server, use the explicit &lt;code&gt;asciinema upload &amp;lt;filename&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;More self-hosting-friendly&lt;/head&gt;
    &lt;p&gt;A related improvement introduced in this release is the new server URL prompt.&lt;/p&gt;
    &lt;p&gt;When using a command that integrates with asciinema server (&lt;code&gt;upload&lt;/code&gt;, &lt;code&gt;stream&lt;/code&gt;,
&lt;code&gt;auth&lt;/code&gt;) for the first time, a prompt is shown, pre-filled with
https://asciinema.org (for convenience). This lets you choose an asciinema
server instance explicitly and intentionally. The choice is saved for future
invocations.&lt;/p&gt;
    &lt;p&gt;It was always possible to point the CLI to another asciinema server with a config file or environment variable, but this new prompt should come in handy especially when running the CLI in a non-workstation/non-laptop yet interactive environment, such as a fresh VM or a dev container.&lt;/p&gt;
    &lt;p&gt;This change should make it easier to use the CLI with your own asciinema server, and at the same time it doubles as an additional guard preventing unintended data leaks (to asciinema.org).&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;I’m really excited about this release. It’s been in the making for a while, but it’s out now, and I’m looking forward to seeing what new use-cases and workflows people will discover with it.&lt;/p&gt;
    &lt;p&gt;It’s going to take a moment until 3.0 shows up in package repositories for all supported platforms/distros. Meanwhile, you can download prebuilt binaries for GNU/Linux and macOS from the GitHub release, or build it from source.&lt;/p&gt;
    &lt;p&gt;Thanks for reading to this point!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.asciinema.org/post/three-point-o/"/><published>2025-09-15T16:06:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45251690</id><title>Wanted to spy on my dog, ended up spying on TP-Link</title><updated>2025-09-15T22:35:46.140578+00:00</updated><content>&lt;doc fingerprint="b430811b1b0232ef"&gt;
  &lt;main&gt;
    &lt;p&gt;I recently bought a cheap Tapo indoor camera to see what my dog gets up to when I am out of the house.&lt;/p&gt;
    &lt;p&gt;What actually followed? I ended up reverse-engineering onboarding flows, decompiling an APK, MITMing TLS sessions, and writing cryptographic scripts.&lt;/p&gt;
    &lt;p&gt;My main motivation for this project really stemmed from the fact that the camera annoyed me from day one. Setting the camera up in frigate was quite painful, no one really seemed to know how these cameras worked online.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;SIDENOTE: If you want 2 way audio to work in frigate you must use the&lt;/p&gt;&lt;code&gt;tapo://&lt;/code&gt;go2rtc configuration for your main stream instead of the usual&lt;code&gt;rtsp://&lt;/code&gt;. TP-Link are lazy and only implement 2 way audio on their own proprietary API.&lt;/quote&gt;
    &lt;p&gt;One undocumented behavior that tripped me up was that the device’s API is supposed to accept credentials &lt;code&gt;admin&lt;/code&gt;:&lt;code&gt;&amp;lt;your-tapo-cloud-password&amp;gt;&lt;/code&gt; after onboarding. However after banging my head against a wall for a few hours I later discovered that if you change your cloud password after onboarding, paired devices don’t get the memo ð.&lt;/p&gt;
    &lt;p&gt;This implied a few things to me that started the cogs turning:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There must be a call made during on-boarding that syncs the device password with the cloud password&lt;/item&gt;
      &lt;item&gt;The device must either allow unauthenticated calls before this step or have some sort of default password.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So considering my onboarding woes and the fact that I was starting to recoil every time the tapo app tried to jam a “Tapo Care” subscription down my throat, a cloudless onboarding solution for the device was beginning to look more and more desirable.&lt;/p&gt;
    &lt;p&gt;The first step to cracking this egg was to be be able to snoop on what the app and the camera are saying to each other during onboarding. E.g, establish a man in the middle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Man in the middle&lt;/head&gt;
    &lt;p&gt;To man in the middle a phone app, you must be able to route all http(s) traffic via a proxy server you control. Historically this has been quite simple to achieve, simply spin up a proxy on a computer, add the proxy’s self-signed certificate to the phone’s truststore, and configure the phone to point at the proxy.&lt;/p&gt;
    &lt;p&gt;However, modern phone apps can use a few nasty tricks to render this approach ineffective. Namely they will blatantly ignore proxies, throw the system truststore to the wind and make liberal use of certificate pinning.&lt;/p&gt;
    &lt;p&gt;The most full-proof technique for generically MITMing an app has therefore become dynamic instrumentation via tools like &lt;code&gt;frida&lt;/code&gt;. What this allows us to do is force an app to use the proxies and certificates that we tell it to whilst batting aside it’s attempts to do things like certificate pinning.&lt;/p&gt;
    &lt;p&gt;So the setup ended up looking like this (full setup guide here ):&lt;/p&gt;
    &lt;quote&gt;--- config: theme: 'base' themeVariables: primaryColor: '#00000000' primaryTextColor: '#fff' primaryBorderColor: '#ffffff8e' lineColor: '#fff' secondaryColor: '#fff' tertiaryColor: '#fff' --- sequenceDiagram participant A as Tapo App &amp;lt;br&amp;gt;(with frida hooks) participant L as Laptop &amp;lt;br&amp;gt;(mitmproxy) participant C as Tapo Camera A-&amp;gt;&amp;gt;L: Request L-&amp;gt;&amp;gt;L: Record request L-&amp;gt;&amp;gt;C: Forward request C--&amp;gt;&amp;gt;L: Response L-&amp;gt;&amp;gt;L: Record response L--&amp;gt;&amp;gt;A: Forward response&lt;/quote&gt;
    &lt;p&gt;After spinning up &lt;code&gt;mitmproxy&lt;/code&gt;, injecting the frida scripts
, and onboarding the camera, we finally see an initial login flow â before the admin password ever gets changed:&lt;/p&gt;
    &lt;p&gt;However, subsequent requests look like this:&lt;/p&gt;
    &lt;code&gt;{
  "method": "securePassthrough",
  "params": {
    "request": "bAhdgihJ9j6PrrknnbXWATBohGTZK5llv3MEzRcmoAmcxexmlVNz3OUX2r0h9a9EG/3X0tBpPi654T2+BjqVEOn2D178kokBpf8RQj01AvBZLYD5S5sFeaCXWiRXA7MgQUppROV4AbrU4f+GOM37KgPqT59qgLVja2slw6CzrKjPzOrG4Ho6Mu6wBa1xepcj"
  }
}
&lt;/code&gt;
    &lt;p&gt;And responses look like this:&lt;/p&gt;
    &lt;code&gt;{
  "seq": 584,
  "result": {
    "response": "Gqz1wbXAig/3wL+kXzY2Ig3hq+JSYasYI7FXdMNZR5PyH8bpLX+GJqQbImUtby9IEj5HQDhxqcTa+dUqQjI0GaGCxuGHqmrgQ0FeyCTQjBiW5gslAPQG33wj44OOkAep"
  },
  "error_code": 0
}
&lt;/code&gt;
    &lt;p&gt;So from this initial dive we have learned that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tapo 100% has a default password due to the fact that it performs a full login before it knows anything about the cloud password.&lt;/item&gt;
      &lt;item&gt;Tapo has an encrypted &lt;code&gt;securePassthrough&lt;/code&gt;channel for its API calls to prevent peeping toms such as myself from spilling the beans.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The JADX dive&lt;/head&gt;
    &lt;p&gt;The next logical step is to decompile the apk in JADX and start rummaging around for a default password.&lt;/p&gt;
    &lt;p&gt;The initial login call that we captured references an &lt;code&gt;admin&lt;/code&gt; username:&lt;/p&gt;
    &lt;code&gt;{
  "method": "login",
  "params": {
    "cnonce": "AD0E189F6E1BA335",
    "encrypt_type": "3",
    "username": "admin"
  }
}
&lt;/code&gt;
    &lt;p&gt;Searching for &lt;code&gt;"admin"&lt;/code&gt; in JADX gives us many hits but there are a few concentrated in a &lt;code&gt;CameraOnboardingViewModel&lt;/code&gt; class that look interesting:&lt;/p&gt;
    &lt;p&gt;The function &lt;code&gt;m98131y2&lt;/code&gt; appears to be returning a password that is then passed to the &lt;code&gt;new Account()&lt;/code&gt; call. Following this function up the chain, we hit gold:&lt;/p&gt;
    &lt;p&gt;We already know that the device is using &lt;code&gt;encrypt_type: 3&lt;/code&gt;, so that means our default password is:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;TPL075526460603&lt;/code&gt;
    &lt;/p&gt;
    &lt;head rend="h2"&gt;Teaching mitmproxy new tricks&lt;/head&gt;
    &lt;p&gt;With the default password now revealed, we have the cards in our hand to derive session keys and decode the &lt;code&gt;securePassthrough&lt;/code&gt; messages.&lt;/p&gt;
    &lt;p&gt;The only thing that would help us further is if we had a reference implementation for the authentication flow. This is where PyTapo really came in handy.&lt;/p&gt;
    &lt;p&gt;Using PyTapo as a reference, we could dump the session state and encrypted messages from mitmproxy and write a script to do some static analysis on the decrypted requests and responses, but a really cool feature of &lt;code&gt;mitmproxy&lt;/code&gt; is that it supports scripting itself.&lt;/p&gt;
    &lt;p&gt;What this means is that we can pass a python script to mitmproxy, and have it directly decrypt request and response payloads inline whilst running a capture.&lt;/p&gt;
    &lt;p&gt;So I wrote &lt;code&gt;tapo_decrypt_pretty.py&lt;/code&gt; which:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Watches for the login handshake (&lt;code&gt;cnonce&lt;/code&gt;,&lt;code&gt;nonce&lt;/code&gt;,&lt;code&gt;device_confirm&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Derives &lt;code&gt;lsk&lt;/code&gt;/&lt;code&gt;ivb&lt;/code&gt;session keys from it&lt;/item&gt;
      &lt;item&gt;Transparently decrypts subsequent API calls&lt;/item&gt;
      &lt;item&gt;Pretty-prints them inline in mitmproxyâs UI in &lt;code&gt;request_decrypted&lt;/code&gt;and&lt;code&gt;response_decrypted&lt;/code&gt;fields&lt;/item&gt;
      &lt;item&gt;Dumps them to JSON files for later analysis&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Analysing the results&lt;/head&gt;
    &lt;p&gt;The complete list of calls made by the Tapo app during onboarding were:&lt;/p&gt;
    &lt;code&gt;getAppComponentList
setLanguage
scanApList
bindToCloud
changeAdminPassword
setTimezone
setRecordPlan
setDeviceLocation
connectAp
getConnectStatus
setAccountEnabled
changeThirdAccount
&lt;/code&gt;
    &lt;p&gt;This boiled down to just four important calls:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;scanApList&lt;/code&gt;â list Wi-Fi access points&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setAccountEnabled&lt;/code&gt;+&lt;code&gt;changeThirdAccount&lt;/code&gt;â enable RTSP/ONVIF account&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;changeAdminPassword&lt;/code&gt;â change from default password to the cloud password&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;connectAp&lt;/code&gt;â join the selected Wi-Fi access point&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything else was fluff: timezones, record plans, binding to cloud.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;In the end, the prize for all this nonsense was a scrappy little Bash script, &lt;code&gt;tapo_onboard.sh&lt;/code&gt;
, which:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Logs in with the default admin password,&lt;/item&gt;
      &lt;item&gt;Scans and selects a Wifi access point&lt;/item&gt;
      &lt;item&gt;Switches off the obnoxious OSD logo on the camera feed,&lt;/item&gt;
      &lt;item&gt;Enables RTSP/ONVIF capabilities&lt;/item&gt;
      &lt;item&gt;Changes the admin password,&lt;/item&gt;
      &lt;item&gt;And finally joins the Wi-Fi.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Peeling this onion left me with a few observations on Tapoâs firmware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Some endpoints use SHA-256 for hashing, while others cling to MD5 like itâs 2003.&lt;/item&gt;
      &lt;item&gt;There are two public keys used to send passwords to the device â one that is shared with the client and another super secret one that’s hardcoded in the app. The easiest way to figure out which one to use is to flip a coin.&lt;/item&gt;
      &lt;item&gt;Password syncing between the app and its managed devices is strictly vibe-based.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The whole thing feels like it was cobbled together by a consortium of couch-cryptographers. But then again, it was the cheapest indoor camera on amazon, so what did I expect?&lt;/p&gt;
    &lt;p&gt;And with all this said I did finally manage to figure out what the dog does when I am away.&lt;/p&gt;
    &lt;p&gt;She sleeps. On the sofa. Sometimes even in her bed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kennedn.com/blog/posts/tapo/"/><published>2025-09-15T16:28:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45252301</id><title>GPT-5-Codex</title><updated>2025-09-15T22:35:45.937639+00:00</updated><content>&lt;doc fingerprint="ba44655ec680c89c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing upgrades to Codex&lt;/head&gt;
    &lt;p&gt;Codex just got faster, more reliable, and better at real-time collaboration and tackling tasks independently anywhere you develop—whether via the terminal, IDE, web, or even your phone.&lt;/p&gt;
    &lt;p&gt;Today, we’re releasing GPT‑5-Codex—a version of GPT‑5 further optimized for agentic coding in Codex. GPT‑5-Codex was trained with a focus on real-world software engineering work; it’s equally proficient at quick, interactive sessions and at independently powering through long, complex tasks. Its code review capability can catch critical bugs before they ship. GPT‑5-Codex is available everywhere you use Codex—it’s the default for cloud tasks and code review, and developers can choose to use it for local tasks via Codex CLI and the IDE extension.&lt;/p&gt;
    &lt;p&gt;Since we first launched Codex CLI(opens in a new window) in April and Codex web in May, Codex has steadily evolved into a more effective coding collaborator. Two weeks ago, we unified Codex into a single product experience connected by your ChatGPT account, enabling you to move work seamlessly between your local environment and the cloud without losing context. Codex now works where you develop—in your terminal or IDE, on the web, in GitHub, and even in the ChatGPT iOS app. Codex is included with ChatGPT Plus, Pro, Business, Edu, and Enterprise plans.&lt;/p&gt;
    &lt;p&gt;With these updates, Codex moves closer to what we’ve been building toward all along—a teammate that understands your context, works alongside you, and reliably takes on work for your team.&lt;/p&gt;
    &lt;p&gt;GPT‑5-Codex is a version of GPT‑5 further optimized for agentic software engineering in Codex. It’s trained on complex, real-world engineering tasks such as building full projects from scratch, adding features and tests, debugging, performing large-scale refactors, and conducting code reviews. It’s more steerable, adheres better to AGENTS.md(opens in a new window) instructions, and produces higher-quality code—just tell it what you need without writing long instructions on style or code cleanliness.&lt;/p&gt;
    &lt;p&gt;GPT‑5-Codex adapts how much time it spends thinking more dynamically based on the complexity of the task. The model combines two essential skills for a coding agent: pairing with developers in interactive sessions, and persistent, independent execution on longer tasks. That means Codex will feel snappier on small, well-defined requests or while you are chatting with it, and will work for longer on complex tasks like big refactors. During testing, we've seen GPT‑5-Codex work independently for more than 7 hours at a time on large, complex tasks, iterating on its implementation, fixing test failures, and ultimately delivering a successful implementation.&lt;/p&gt;
    &lt;p&gt;On OpenAI employee traffic, we see that for the bottom 10% of user turns sorted by model-generated tokens (including hidden reasoning and final output), GPT‑5-Codex uses 93.7% fewer tokens than GPT‑5. Conversely, for the top 10%, GPT‑5-Codex thinks more, spending twice as long reasoning, editing and testing code, and iterating.&lt;/p&gt;
    &lt;p&gt;GPT‑5-Codex has been trained specifically for conducting code reviews and finding critical flaws. When reviewing, it navigates your codebase, reasons through dependencies, and runs your code and tests in order to validate correctness. We evaluated code review performance on recent commits from popular open-source repositories. For each commit, experienced software engineers evaluated review comments for correctness and importance. We find that comments by GPT‑5-Codex are less likely to be incorrect or unimportant, reserving more user attention for critical issues.&lt;/p&gt;
    &lt;p&gt;GPT‑5-Codex is a reliable partner on front-end tasks. In addition to creating aesthetic desktop apps, GPT‑5-Codex also shows significant improvements in human preference evaluations when creating mobile websites. When working in the cloud, it can look at images or screenshots you provide as input, visually inspect its progress, and display screenshots of its work to you.&lt;/p&gt;
    &lt;p&gt;GPT‑5-Codex was purpose-built for Codex CLI, the Codex IDE extension, the Codex cloud environment, and working in GitHub, and also supports versatile tool use. Unlike GPT‑5, which is a general-purpose model, we recommend using GPT‑5-Codex only for agentic coding tasks in Codex or Codex-like environments.&lt;/p&gt;
    &lt;p&gt;We also recently made some updates to make Codex a better pair programmer, with a revamped Codex CLI and the new Codex IDE extension.&lt;/p&gt;
    &lt;p&gt;Codex CLI is open-source, and community feedback over the last few months has been invaluable in shaping its evolution. With this feedback, we’ve rebuilt Codex CLI around agentic coding workflows to harness our models into more capable and reliable partners. You can now attach and share images—screenshots, wireframes, and diagrams—right in the CLI to build shared context on design decisions and get exactly what you want. When doing more complex work, Codex now tracks progress with a to-do list, and includes tools like web search and MCP for connecting to external systems, with more accurate tool use overall.&lt;/p&gt;
    &lt;p&gt;The terminal UI has also been upgraded: tool calls and diffs are better formatted and easier to follow. Approval modes are simplified to three levels: read-only with explicit approvals, auto with full workspace access but requiring approvals outside the workspace, and full access with the ability to read files anywhere and run commands with network access. It also supports compacting conversation state to make longer sessions easier to manage.&lt;/p&gt;
    &lt;p&gt;Check out the Codex CLI quickstart(opens in a new window) to learn more.&lt;/p&gt;
    &lt;p&gt;Codex meets you where you already work, including in your IDE. The IDE extension brings the Codex agent into VS Code, Cursor, and other VS Code forks, so that you can seamlessly preview local changes and edit code with Codex. When you use Codex in your IDE, you can write shorter prompts and get faster results because Codex can use context like the files you’ve opened or the code you’ve selected.&lt;/p&gt;
    &lt;p&gt;The Codex IDE extension also lets you move work smoothly between the cloud and your local environment. You can create new cloud tasks, track in‑progress work, and review completed tasks without leaving your editor. To make finishing touches, you can open cloud tasks in your IDE, and Codex maintains context. Learn more on how to get the most out of the IDE extension in the quickstart(opens in a new window).&lt;/p&gt;
    &lt;p&gt;In addition to Codex CLI, the new IDE extension and GitHub integration bring the Codex cloud agent closer to developer workflows so you can now delegate tasks without switching away from your editor or GitHub.&lt;/p&gt;
    &lt;p&gt;Behind the scenes, we’ve also been continuously improving cloud infrastructure performance. By caching containers, we’ve slashed the median completion time for new tasks and follow-ups by 90%. Codex also now automatically sets up its own environment by scanning for common setup scripts and executing them, and with configurable internet access can run commands like pip install to fetch dependencies as needed at runtime.&lt;/p&gt;
    &lt;p&gt;Like in the CLI and IDE extension, you can now use images to share frontend design specs or explain UI bugs. As it builds for you, Codex can spin up its own browser, look at what it built, iterate, and attach a screenshot of the result to the task and GitHub PR. For more details, check out the docs(opens in a new window).&lt;/p&gt;
    &lt;p&gt;Codex also now includes code review capabilities trained to catch critical flaws. Unlike static analysis tools, it matches the stated intent of a PR to the actual diff, reasons over the entire codebase and dependencies, and executes code and tests to validate behavior. Only the most thorough human reviewers put this level of effort into every PR they review, so Codex fills the gap—helping teams find problems earlier, reduce reviewer load, and ship with more confidence.&lt;/p&gt;
    &lt;p&gt;Once turned on for a GitHub repo, Codex automatically reviews PRs as they move from draft to ready, posting its analysis on the PR. If it recommends edits, you can stay in the same thread and ask Codex to implement them. You can also explicitly ask for a review by mentioning “@codex review” in a PR, and give it extra guidance like “@codex review for security vulnerabilities” or “@codex review for outdated dependencies”. Check out the quickstart(opens in a new window) to learn how to set up code review for your repos.&lt;/p&gt;
    &lt;p&gt;At OpenAI, Codex now reviews the vast majority of our PRs, catching hundreds of issues every day—often before a human review begins. It’s been key to letting the Codex team move fast with greater confidence.&lt;/p&gt;
    &lt;head rend="h3"&gt;How developers are using Codex&lt;/head&gt;
    &lt;quote&gt;“I needed to update a codebase owned by another team for a feature release. [...] With Codex, I offloaded the refactoring and test generation while focusing on other priorities. It produced high-quality, fully tested code that I could quickly hand back — keeping the feature on schedule without adding risk.”&lt;/quote&gt;
    &lt;p&gt;We’re building Codex with a focus on protecting code and data from exfiltration, and guarding against misuse. By default, Codex runs in a sandboxed environment with network access(opens in a new window) disabled, whether locally or in the cloud. This helps ensure Codex can’t take harmful actions on your computer, and reduces the risk of prompt injections from untrusted sources.&lt;/p&gt;
    &lt;p&gt;Codex can ask for permission before potentially dangerous actions and is trained to run commands to verify its outputs. Developers can customize security settings to match their risk tolerance. In the cloud, you can limit network access to trusted domains. In the CLI and IDE extension, developers can approve commands to run with full access or allow the agent to use web search and connect to MCP servers. This can expand capabilities of the agent while increasing the risks–learn more about how to securely operate and manage Codex here(opens in a new window).&lt;/p&gt;
    &lt;p&gt;We always encourage developers to review the agent’s work before making changes or deploying to production. Codex provides citations, terminal logs, and test results with each task to help with this. While Codex code reviews help reduce the risk of dangerous issues being deployed to production, whether created by humans or agents, we always recommend using Codex as an additional reviewer—not a replacement for human reviews.&lt;/p&gt;
    &lt;p&gt;Consistent with our approach to GPT‑5, we have decided to treat GPT‑5-Codex as High capability in the Biological and Chemical domains, and have implemented safeguards to minimize the associated risks. Read more about our assessments and robust safety approach in the system card addendum.&lt;/p&gt;
    &lt;p&gt;Codex is included with ChatGPT Plus, Pro, Business, Edu, and Enterprise plans. Usage scales with your plan: Plus, Edu and Business seats can cover a few focused coding sessions each week, while Pro can support a full workweek across multiple projects.&lt;/p&gt;
    &lt;p&gt;Business plans can purchase credits to enable developers to go beyond their included limits, while Enterprise plans provide a shared credit pool so you only pay for what your developers use. Learn more about usage limits in ChatGPT here(opens in a new window).&lt;/p&gt;
    &lt;p&gt;For developers using Codex CLI via API key, we plan to make GPT‑5-Codex available in the API soon.&lt;/p&gt;
    &lt;p&gt;Codex is becoming the coding partner we’ve always envisioned—one that’s faster, more reliable, and deeply integrated into the tools you already use. We’re excited to see what you build with it and will keep improving Codex to make it an even better teammate for your most ambitious projects.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-upgrades-to-codex/"/><published>2025-09-15T17:10:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45252378</id><title>macOS Tahoe</title><updated>2025-09-15T22:35:45.581193+00:00</updated><content>&lt;doc fingerprint="a4b4eb2d8b62abe1"&gt;
  &lt;main&gt;
    &lt;p&gt;A new design with Liquid Glass. Beautiful, delightful, and instantly familiar.&lt;/p&gt;
    &lt;p&gt;Now with the Phone app1 and Live Activities2 from iPhone for next‑level Continuity.&lt;/p&gt;
    &lt;p&gt;Take hundreds of actions in Spotlight without lifting your hands off the keyboard.&lt;/p&gt;
    &lt;p&gt;Create more powerful shortcuts than ever with Apple Intelligence.3&lt;/p&gt;
    &lt;p&gt;Design&lt;/p&gt;
    &lt;p&gt;More you. Shines through.&lt;/p&gt;
    &lt;p&gt;Reimagined with Liquid Glass, macOS Tahoe is at once fresh and familiar. Apps bring more focus to your content. You can personalize your Mac like never before. And everything just flows into place.&lt;/p&gt;
    &lt;p&gt;Delightful new design.&lt;/p&gt;
    &lt;p&gt;Liquid Glass refracts and reflects content in real time, bringing even more clarity to navigation and controls — and even more vitality to everything you do.&lt;/p&gt;
    &lt;p&gt;Updated app icons.&lt;/p&gt;
    &lt;p&gt;Personalize your Mac with new options including updated light or dark appearances, new color-tinted icons, or a stunning clear look.&lt;/p&gt;
    &lt;p&gt;Personalized controls and menu bar.&lt;/p&gt;
    &lt;p&gt;Your display feels even larger with the transparent menu bar. And you have more ways to customize the controls and layout in the menu bar and Control Center, even those from third parties.&lt;/p&gt;
    &lt;p&gt;Refreshed apps.&lt;/p&gt;
    &lt;p&gt;Sidebars and toolbars in apps reflect the depth of your workspace and offer a subtle hint of the content within reach as you scroll.&lt;/p&gt;
    &lt;p&gt;Apple Intelligence&lt;/p&gt;
    &lt;p&gt;Get more done, from even more places.&lt;/p&gt;
    &lt;p&gt;Now integrated into even more apps and experiences, Apple Intelligence helps you get things done effortlessly and communicate across languages.3&lt;/p&gt;
    &lt;p&gt;Live Translation.&lt;/p&gt;
    &lt;p&gt;Automatically translate texts in Messages,4 display live translated captions in FaceTime, and get spoken translations for calls in the Phone app.5&lt;/p&gt;
    &lt;p&gt;Accelerate your workflows.&lt;/p&gt;
    &lt;p&gt;Intelligent actions in Shortcuts can summarize text, create images, or tap directly into Apple Intelligence models to provide responses that feed into your shortcut.&lt;/p&gt;
    &lt;p&gt;More ways to express yourself with images.&lt;/p&gt;
    &lt;p&gt;Mix emoji and descriptions to make something brand-new. In Image Playground, discover additional ChatGPT styles. And have even more control when making images inspired by family and friends using Genmoji and Image Playground.6&lt;/p&gt;
    &lt;p&gt;Continuity&lt;/p&gt;
    &lt;p&gt;Together they’re better than ever.&lt;/p&gt;
    &lt;p&gt;Continuity helps you work seamlessly across Apple devices. And with the Phone app and Live Activities coming to Mac, it’s even easier to stay on top of things happening in real time.&lt;/p&gt;
    &lt;p&gt;Live Activities on Mac.&lt;/p&gt;
    &lt;p&gt;The menu bar now features the Live Activities from your iPhone. And when you click one, the app opens in iPhone Mirroring so you can take action.2&lt;/p&gt;
    &lt;p&gt;New Phone app for Mac.&lt;/p&gt;
    &lt;p&gt;Make and take calls with a click. Conveniently access your synced content like Recents, Contacts, and Voicemail — and enjoy the familiar features from iPhone.1&lt;/p&gt;
    &lt;p&gt;Manage unwanted calls.&lt;/p&gt;
    &lt;p&gt;For unknown numbers, Call Screening finds out who’s calling and why. Once the caller shares their name and the reason for their call, your phone rings and you can decide if you want to pick up.7&lt;/p&gt;
    &lt;p&gt;Stay productive while on hold.&lt;/p&gt;
    &lt;p&gt;Hold Assist keeps your spot in line while you wait for a live agent and notifies you when they’re ready.8&lt;/p&gt;
    &lt;p&gt;Productivity&lt;/p&gt;
    &lt;p&gt;Calm in the brainstorm.&lt;/p&gt;
    &lt;p&gt;Make quick work of everyday tasks, jump into your favorite activities, and turbocharge pro workflows — all with a whole lot less effort.&lt;/p&gt;
    &lt;p&gt;Biggest Spotlight update ever.&lt;/p&gt;
    &lt;p&gt;Spotlight lets you take hundreds of actions without lifting your hands off the keyboard. And new quick keys help you perform actions even faster.&lt;/p&gt;
    &lt;p&gt;A faster way to browse.&lt;/p&gt;
    &lt;p&gt;You can now keep all your apps and most accessed files within easy reach, including intelligent suggestions based on your routines.&lt;/p&gt;
    &lt;p&gt;Smart, effortless automation.&lt;/p&gt;
    &lt;p&gt;Now you can run shortcuts automatically — at a specific time of day or when you take specific actions, like saving a file to a particular folder or connecting to a display.&lt;/p&gt;
    &lt;p&gt;Magnifier lets you zoom in on your surroundings using a connected camera. Accessibility Reader provides a systemwide, customized reading and listening experience. Braille Access creates an all-new interface for braille displays.9 And Vehicle Motion Cues help reduce motion sickness in moving vehicles.&lt;/p&gt;
    &lt;p&gt;Family.&lt;/p&gt;
    &lt;p&gt;Parents can take advantage of a wide set of parental controls designed to keep children safe. These include new enhancements across Communication Limits, Communication Safety, and the App Store.&lt;/p&gt;
    &lt;p&gt;Journal.&lt;/p&gt;
    &lt;p&gt;Now on Mac for the most comfortable writing experience, Journal makes it easy to capture and write about everyday moments and special events using photos, videos, audio recordings, places, and more.&lt;/p&gt;
    &lt;p&gt;Photos.&lt;/p&gt;
    &lt;p&gt;An updated design lets you quickly access filtering and sorting options and customize the size of Collections tiles so you can view your library just how you like. And with Pinned Collections, you can keep your most-visited ones right at your fingertips.&lt;/p&gt;
    &lt;p&gt;FaceTime.&lt;/p&gt;
    &lt;p&gt;Celebrate the people who matter most with a new tiled design that features beautiful and personalized Contact Posters.&lt;/p&gt;
    &lt;p&gt;Reminders.&lt;/p&gt;
    &lt;p&gt;With Apple Intelligence, Reminders can suggest tasks, grocery items, and follow-ups based on emails or other text on your device. It can also automatically categorize related reminders into sections within a list.&lt;/p&gt;
    &lt;p&gt;Games.&lt;/p&gt;
    &lt;p&gt;The new Games app brings together all the games you have on your Mac. In the Game Overlay, you can adjust system settings, chat with friends, or invite them to play — all without leaving the game. And for developers, Metal 4 brings even more advanced graphics and rendering technologies, like MetalFX Frame Interpolation and Denoising.&lt;/p&gt;
    &lt;p&gt;Messages.&lt;/p&gt;
    &lt;p&gt;Create polls and personalize conversations with backgrounds. Redesigned conversation details feature designated sections for contact info, photos, links, location, and more. Typing indicators in groups let you know exactly who is about to chime in. Screening tools detect spam and give you control. And the Add Contact button now appears next to an unknown number in a group.&lt;/p&gt;
    &lt;p&gt;Passwords.&lt;/p&gt;
    &lt;p&gt;Easily refer to changes you’ve made to your accounts. Find previous versions of passwords, along with when they were changed.&lt;/p&gt;
    &lt;p&gt;Notes.&lt;/p&gt;
    &lt;p&gt;Capture conversations in the Phone app as audio recordings with transcriptions.10 You can also export a note into a Markdown file.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.apple.com/os/macos/"/><published>2025-09-15T17:16:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45252715</id><title>React is winning by default and slowing innovation</title><updated>2025-09-15T22:35:45.364050+00:00</updated><content>&lt;doc fingerprint="b7e9953b6b12f9da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;React Won by Default – And It's Killing Frontend Innovation&lt;/head&gt;
    &lt;p&gt;React-by-default has hidden costs. Here's a case for making deliberate choices to select the right framework for the job.&lt;/p&gt;
    &lt;head rend="h1"&gt;React Won by Default – And It’s Killing Frontend Innovation&lt;/head&gt;
    &lt;p&gt;React is no longer winning by technical merit. Today it is winning by default. That default is now slowing innovation across the frontend ecosystem.&lt;/p&gt;
    &lt;p&gt;When teams need a new frontend, the conversation rarely starts with “What are the constraints and which tool best fits them?” It often starts with “Let’s use React; everyone knows React.” That reflex creates a self-perpetuating cycle where network effects, rather than technical fit, decide architecture.&lt;/p&gt;
    &lt;p&gt;Meanwhile, frameworks with real innovations struggle for adoption. Svelte compiles away framework overhead. Solid delivers fine-grained reactivity without virtual-DOM tax. Qwik achieves instant startup via resumability. These approaches can outperform React’s model in common scenarios, but they rarely get a fair evaluation because React is chosen by default.&lt;/p&gt;
    &lt;p&gt;React is excellent at many things. The problem isn’t React itself, it’s the React-by-default mindset.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Innovation Ceiling&lt;/head&gt;
    &lt;p&gt;React’s technical foundations explain some of today’s friction. The virtual DOM was a clever solution for 2013’s problems, but as Rich Harris outlined in “Virtual DOM is pure overhead”, it introduces work modern compilers can often avoid.&lt;/p&gt;
    &lt;p&gt;Hooks addressed class component pain but introduced new kinds of complexity: dependency arrays, stale closures, and misused effects. Even React’s own docs emphasize restraint: “You Might Not Need an Effect”. Server Components improve time-to-first-byte, but add architectural complexity and new failure modes.&lt;/p&gt;
    &lt;p&gt;The React Compiler is a smart solution that automates patterns like &lt;code&gt;useMemo&lt;/code&gt;/&lt;code&gt;useCallback&lt;/code&gt;. Its existence is also a signal: we’re optimizing around constraints baked into the model.&lt;/p&gt;
    &lt;p&gt;Contrast this with alternative approaches: Svelte 5’s Runes simplify reactivity at compile time; Solid’s fine-grained reactivity updates exactly what changed; Qwik’s resumability eliminates traditional hydration. These aren’t incremental tweaks to React’s model—they’re different models with different ceilings.&lt;/p&gt;
    &lt;p&gt;Innovation without adoption doesn’t change outcomes. Adoption can’t happen when the choice is made by reflex.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Technical Debt We’re All Carrying&lt;/head&gt;
    &lt;p&gt;Defaulting to React often ships a runtime and reconciliation cost we no longer question. Even when it’s fast enough, the ceiling is lower than compile-time or fine-grained models. Developer time is spent managing re-renders, effect dependencies, and hydration boundaries instead of shipping value. The broader lesson from performance research is consistent: JavaScript is expensive on the critical path (The Cost of JavaScript).&lt;/p&gt;
    &lt;p&gt;We’ve centered mental models around “React patterns” instead of web fundamentals, reducing portability of skills and making architectural inertia more likely.&lt;/p&gt;
    &lt;p&gt;The loss isn’t just performance, it’s opportunity cost when better-fit alternatives are never evaluated. For instance, benchmarks like the JS Framework Benchmark show alternatives like Solid achieving up to 2-3x faster updates in reactivity-heavy scenarios compared to React.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Frameworks Being Suffocated&lt;/head&gt;
    &lt;head rend="h3"&gt;Svelte: The Compiler Revolution&lt;/head&gt;
    &lt;p&gt;Svelte shifts work to compile time: no virtual DOM, minimal runtime. Components become targeted DOM operations. The mental model aligns with web fundamentals.&lt;/p&gt;
    &lt;p&gt;But “not enough jobs” keeps Svelte adoption artificially low despite its technical superiority for most use cases. Real-world examples, like The Guardian’s adoption of Svelte for their frontend, demonstrate measurable gains in performance and developer productivity, with reported reductions in bundle sizes and faster load times. For instance, as detailed in Wired’s article on Svelte, developer Shawn Wang (@swyx on X/Twitter) reduced his site’s size from 187KB in React to just 9KB in Svelte by leveraging its compile-time optimizations, which shift framework overhead away from runtime. This leads to faster, more efficient apps especially on slow connections.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solid: The Reactive Primitive Approach&lt;/head&gt;
    &lt;p&gt;Solid delivers fine-grained reactivity with JSX familiarity. Updates flow through signals directly to affected DOM nodes, bypassing reconciliation bottlenecks. Strong performance characteristics, limited mindshare. As outlined in Solid’s comparison guide, this approach enables more efficient updates than React’s virtual DOM, with precise reactivity that minimizes unnecessary work and improves developer experience through simpler state management.&lt;/p&gt;
    &lt;p&gt;While prominent case studies are scarcer than for more established frameworks, this is largely due to Solid’s lower adoption. Yet anecdotal reports from early adopters suggest similar transformative gains in update efficiency and code simplicity, waiting to be scaled and shared as more teams experiment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Qwik: The Resumability Innovation&lt;/head&gt;
    &lt;p&gt;Qwik uses resumability instead of hydration, enabling instant startup by loading only what the current interaction needs. Ideal for large sites, long sessions, or slow networks. According to Qwik’s Think Qwik guide, this is achieved through progressive loading and serializing both state and code. Apps can thus resume execution instantly without heavy client-side bootstrapping, resulting in superior scalability and reduced initial load times compared to traditional frameworks.&lt;/p&gt;
    &lt;p&gt;Success stories for Qwik may be less visible simply because fewer teams have broken from defaults to try it. But those who have report dramatic improvements in startup times and resource efficiency, indicating a wealth of untapped potential if adoption grows.&lt;/p&gt;
    &lt;p&gt;All three under-adopted not for lack of merit, but because the default choice blocks trying them out.&lt;/p&gt;
    &lt;p&gt;Furthermore, React’s API surface area is notably larger and more complex than its alternatives, encompassing concepts like hooks, context, reducers, and memoization patterns that require careful management to avoid pitfalls. This expansive API contributes to higher cognitive load for developers, often leading to bugs from misunderstood dependencies or over-engineering. For example, in Cloudflare’s September 12, 2025 outage, a useEffect hook with a problematic dependency array triggered repeated API calls, overwhelming their Tenant Service and causing widespread failures. In contrast, frameworks like Svelte, Solid, and Qwik feature smaller, more focused APIs that emphasize simplicity and web fundamentals, reducing the mental overhead and making them easier to master and maintain.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Network Effect Prison&lt;/head&gt;
    &lt;p&gt;React’s dominance creates self-reinforcing barriers. Job postings ask for “React developers” rather than “frontend engineers,” limiting skill diversity. Component libraries and team muscle memory create institutional inertia.&lt;/p&gt;
    &lt;p&gt;Risk-averse leaders choose the “safe” option. Schools teach what jobs ask for. The cycle continues independent of technical merit.&lt;/p&gt;
    &lt;p&gt;That’s not healthy competition; it’s ecosystem capture by default.&lt;/p&gt;
    &lt;head rend="h2"&gt;Breaking the Network Effect&lt;/head&gt;
    &lt;p&gt;Escaping requires deliberate action at multiple levels. Technical leaders should choose based on constraints and merits, not momentum. Companies can allocate a small innovation budget to trying alternatives. Developers can upskill beyond a single mental model.&lt;/p&gt;
    &lt;p&gt;Educators can teach framework-agnostic concepts alongside specific tools. Open source contributors can help alternative ecosystems mature.&lt;/p&gt;
    &lt;p&gt;Change won’t happen automatically. It requires conscious choice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Framework Evaluation Checklist&lt;/head&gt;
    &lt;p&gt;To make deliberate choices, use this simple checklist when starting a new project:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Assess Performance Needs: Evaluate metrics like startup time, update efficiency, and bundle size. Prioritize frameworks with compile-time optimizations if speed is critical.&lt;/item&gt;
      &lt;item&gt;Team Skills and Learning Curve: Consider existing expertise but factor in migration paths; many alternatives offer gentle ramps (e.g., Solid’s JSX compatibility with React).&lt;/item&gt;
      &lt;item&gt;Scaling and Cost of Ownership: Calculate long-term costs, including maintenance, dependency management, and tech debt. Alternatives often reduce runtime overhead, lowering hosting costs and improving scalability.&lt;/item&gt;
      &lt;item&gt;Ecosystem Fit: Balance maturity with innovation; pilot in non-critical areas to test migration feasibility and ROI.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Standard Counter‑Arguments&lt;/head&gt;
    &lt;p&gt;“But ecosystem maturity!” Maturity is valuable, and can also entrench inertia. Age isn’t the same as fitness for today’s constraints.&lt;/p&gt;
    &lt;p&gt;Additionally, a mature ecosystem often means heavy reliance on third-party packages, which can introduce maintenance burdens like keeping dependencies up-to-date, dealing with security vulnerabilities, and bloating bundles with unused code. While essential in some cases, this flexibility can lead to over-dependence; custom solutions tailored to specific needs are often leaner and more maintainable in the long run. Smaller ecosystems in alternative frameworks encourage building from fundamentals, fostering deeper understanding and less technical debt. Moreover, with AI coding assistants now able to generate precise, custom functions on demand, the barrier to creating bespoke utilities has lowered dramatically. This makes it feasible to avoid generic libraries like lodash or date libraries like Moment or date-fns entirely in favor of lightweight, app-specific implementations.&lt;/p&gt;
    &lt;p&gt;“But hiring!” Hiring follows demand. You can de‑risk by piloting alternatives in non‑critical paths, then hiring for fundamentals plus on‑the‑job training.&lt;/p&gt;
    &lt;p&gt;“But component libraries!” Framework‑agnostic design systems and Web Components reduce lock-in while preserving velocity.&lt;/p&gt;
    &lt;p&gt;“But stability!” React’s evolution from classes to hooks to Server Components demonstrates constant churn, not stability. Alternative frameworks often provide more consistent APIs.&lt;/p&gt;
    &lt;p&gt;“But proven at scale!” jQuery was proven at scale too. Past success doesn’t guarantee future relevance.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Broader Ecosystem Harm&lt;/head&gt;
    &lt;p&gt;Monoculture slows web evolution when one framework’s constraints become de facto limits. Talent spends cycles solving framework-specific issues rather than pushing the platform forward. Investment follows incumbents regardless of technical merit.&lt;/p&gt;
    &lt;p&gt;Curricula optimize for immediate employability over fundamentals, creating framework-specific rather than transferable skills. Platform improvements get delayed because “React can handle it” becomes a default answer.&lt;/p&gt;
    &lt;p&gt;The entire ecosystem suffers when diversity disappears.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Garden We Could Grow&lt;/head&gt;
    &lt;p&gt;Healthy ecosystems require diversity, not monocultures. Innovation emerges when different approaches compete and cross-pollinate. Developers grow by learning multiple mental models. The platform improves when several frameworks push different boundaries.&lt;/p&gt;
    &lt;p&gt;Betting everything on one model creates a single point of failure. What happens if it hits hard limits? What opportunities are we missing by not exploring alternatives?&lt;/p&gt;
    &lt;p&gt;It’s time to choose frameworks based on constraints and merit rather than momentum. Your next project deserves better than React-by-default. The ecosystem deserves the innovation only diversity can provide.&lt;/p&gt;
    &lt;p&gt;Stop planting the same seed by default. The garden we could cultivate through diverse framework exploration would be more resilient and more innovative than the monoculture we’ve drifted into.&lt;/p&gt;
    &lt;p&gt;The choice is ours to make.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.lorenstew.art/blog/react-won-by-default/"/><published>2025-09-15T17:46:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45252817</id><title>Hosting a website on a disposable vape</title><updated>2025-09-15T22:35:45.265246+00:00</updated><content>&lt;doc fingerprint="26b5c1054b922ae9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Hosting a WebSite on a Disposable Vape&lt;/head&gt;
    &lt;head rend="h1"&gt;Preface#&lt;/head&gt;
    &lt;p&gt;This article is NOT served from a web server running on a disposable vape. If you want to see the real deal, click here. The content is otherwise identical.&lt;/p&gt;
    &lt;head rend="h1"&gt;Background#&lt;/head&gt;
    &lt;p&gt;For a couple of years now, I have been collecting disposable vapes from friends and family. Initially, I only salvaged the batteries for “future” projects (It’s not hoarding, I promise), but recently, disposable vapes have gotten more advanced. I wouldn’t want to be the lawyer who one day will have to argue how a device with USB C and a rechargeable battery can be classified as “disposable”. Thankfully, I don’t plan on pursuing law anytime soon.&lt;/p&gt;
    &lt;p&gt;Last year, I was tearing apart some of these fancier pacifiers for adults when I noticed something that caught my eye, instead of the expected black blob of goo hiding some ASIC (Application Specific Integrated Circuit) I see a little integrated circuit inscribed “PUYA”. I don’t blame you if this name doesn’t excite you as much it does me, most people have never heard of them. They are most well known for their flash chips, but I first came across them after reading Jay Carlson’s blog post about the cheapest flash microcontroller you can buy. They are quite capable little ARM Cortex-M0+ micros.&lt;/p&gt;
    &lt;p&gt;Over the past year I have collected quite a few of these PY32 based vapes, all of them from different models of vape from the same manufacturer. It’s not my place to do free advertising for big tobacco, so I won’t mention the brand I got it from, but if anyone who worked on designing them reads this, thanks for labeling the debug pins!&lt;/p&gt;
    &lt;head rend="h1"&gt;What are we working with#&lt;/head&gt;
    &lt;p&gt;The chip is marked &lt;code&gt;PUYA C642F15&lt;/code&gt;, which wasn’t very helpful. I was pretty sure it was a &lt;code&gt;PY32F002A&lt;/code&gt;, but after poking around with pyOCD, I noticed that the flash was 24k and we have 3k of RAM. The extra flash meant that it was more likely a &lt;code&gt;PY32F002B&lt;/code&gt;, which is actually a very different chip.1&lt;/p&gt;
    &lt;p&gt;So here are the specs of a microcontroller so bad, it’s basically disposable:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;24MHz Coretex M0+&lt;/item&gt;
      &lt;item&gt;24KiB of Flash Storage&lt;/item&gt;
      &lt;item&gt;3KiB of Static RAM&lt;/item&gt;
      &lt;item&gt;a few peripherals, none of which we will use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You may look at those specs and think that it’s not much to work with. I don’t blame you, a 10y old phone can barely load google, and this is about 100x slower. I on the other hand see a blazingly fast web server.&lt;/p&gt;
    &lt;head rend="h1"&gt;Getting online#&lt;/head&gt;
    &lt;p&gt;The idea of hosting a web server on a vape didn’t come to me instantly. In fact, I have been playing around with them for a while, but after writing my post on semihosting, the penny dropped.&lt;/p&gt;
    &lt;p&gt;If you don’t feel like reading that article, semihosting is basically syscalls for embedded ARM microcontrollers. You throw some values/pointers into some registers and call a breakpoint instruction. An attached debugger interprets the values in the registers and performs certain actions. Most people just use this to get some logs printed from the microcontroller, but they are actually bi-directional.&lt;/p&gt;
    &lt;p&gt;If you are older than me, you might remember a time before Wi-Fi and Ethernet, the dark ages, when you had to use dial-up modems to get online. You might also know that the ghosts of those modems still linger all around us. Almost all USB serial devices actually emulate those modems: a 56k modem is just 57600 baud serial device. Data between some of these modems was transmitted using a protocol called SLIP (Serial Line Internet Protocol).2&lt;/p&gt;
    &lt;p&gt;This may not come as a surprise, but Linux (and with some tweaking even macOS) supports SLIP. The &lt;code&gt;slattach&lt;/code&gt; utility can make any &lt;code&gt;/dev/tty*&lt;/code&gt; send and receive IP packets. All we have to do is put the data down the wire in the right format and provide a virtual tty.
This is actually easier than you might imagine, pyOCD can forward all semihosting though a telnet port. Then, we use &lt;code&gt;socat&lt;/code&gt; to link that port to a virtual tty:&lt;/p&gt;
    &lt;code&gt;pyocd gdb -S -O semihost_console_type=telnet -T $(PORT) $(PYOCDFLAGS) &amp;amp;
socat PTY,link=$(TTY),raw,echo=0 TCP:localhost:$(PORT),nodelay &amp;amp;
sudo slattach -L -p slip -s 115200 $(TTY) &amp;amp;
sudo ip addr add 192.168.190.1 peer 192.168.190.2/24 dev sl0
sudo ip link set mtu 1500 up dev sl0
&lt;/code&gt;
    &lt;p&gt;Ok, so we have a “modem”, but that’s hardly a web server. To actually talk TCP/IP, we need an IP stack. There are many choices, but I went with uIP because it’s pretty small, doesn’t require an RTOS, and it’s easy to port to other platforms. It also, helpfully, comes with a very minimal HTTP server example.&lt;/p&gt;
    &lt;p&gt;After porting the SLIP code to use semihosting, I had a working web server&amp;amp;mldr;half of the time. As with most highly optimised libraries, uIP was designed for 8 and 16-bit machines, which rarely have memory alignment requirements. On ARM however, if you dereference a &lt;code&gt;u16 *&lt;/code&gt;, you better hope that address is even, or you’ll get an exception. The &lt;code&gt;uip_chksum&lt;/code&gt; assumed &lt;code&gt;u16&lt;/code&gt; alignment, but the script that creates the filesystem didn’t.
I actually decided to modify a bit the structure of the filesystem to make it a bit more portable.
This was my first time working with &lt;code&gt;perl&lt;/code&gt; and I have to say, it’s quite well suited to this kind of task.&lt;/p&gt;
    &lt;head rend="h1"&gt;Blazingly fast#&lt;/head&gt;
    &lt;p&gt;So how fast is a web server running on a disposable microcontroller. Well, initially, not very fast. Pings took ~1.5s with 50% packet loss and a simple page took over 20s to load. That’s so bad, it’s actually funny, and I kind of wanted to leave it there.&lt;/p&gt;
    &lt;p&gt;However, the problem was actually between the seat and the steering wheel the whole time. The first implementation read and wrote a single character at a time, which had a massive overhead associated with it. I previously benchmarked semihosting on this device, and I was getting ~20KiB/s, but uIP’s SLIP implementation was designed for very low memory devices, so it was serialising the data byte by byte. We have a whopping 3kiB of RAM to play with, so I added a ring buffer to cache reads from the host and feed them into the SLIP poll function. I also split writes in batches to allow for escaping.&lt;/p&gt;
    &lt;p&gt;Now this is what I call blazingly fast! Pings now take 20ms, no packet loss and a full page loads in about 160ms. This was using using almost all of the RAM, but I could also dial down the sizes of the buffer to have more than enough headroom to run other tasks. The project repo has everything set to a nice balance latency and RAM usage:&lt;/p&gt;
    &lt;code&gt;Memory region         Used Size  Region Size  %age Used
           FLASH:        5116 B        24 KB     20.82%
             RAM:        1380 B         3 KB     44.92%
&lt;/code&gt;
    &lt;p&gt;For this blog however, I paid for none of the RAM, so I’ll use all of the RAM.&lt;/p&gt;
    &lt;p&gt;As you may have noticed, we have just under 20kiB (80%) of storage space. That may not be enough to ship all of React, but as you can see, it’s more than enough to host this entire blog post. And this is not just a static page server, you can run any server-side code you want, if you know C that is.&lt;/p&gt;
    &lt;p&gt;Just for fun, I added a json api endpoint to get the number of requests to the main page (since the last crash) and the unique ID of the microcontroller.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bogdanthegeek.github.io/blog/projects/vapeserver/"/><published>2025-09-15T17:53:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45253458</id><title>Addendum to GPT-5 system card: GPT-5-Codex</title><updated>2025-09-15T22:35:44.600944+00:00</updated><content>&lt;doc fingerprint="9c432defe1bb46cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Addendum to GPT-5 system card: GPT-5-Codex&lt;/head&gt;
    &lt;p&gt;GPT‑5-Codex is a version of GPT‑5 optimized for agentic coding in Codex. Like its predecessor, codex-1, this model was trained using reinforcement learning on real-world coding tasks in a variety of environments to generate code that closely mirrors human style and PR preferences, adhere precisely to instructions, and iteratively run tests until passing results are achieved.&lt;/p&gt;
    &lt;p&gt;This model is available locally in the terminal or IDE through Codex CLI and IDE extension, and on the cloud via the Codex web, GitHub, and the ChatGPT mobile app.&lt;/p&gt;
    &lt;p&gt;This addendum outlines the comprehensive safety measures implemented for GPT‑5-Codex. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/"/><published>2025-09-15T18:45:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45253775</id><title>How People Use ChatGPT [pdf]</title><updated>2025-09-15T22:35:44.251154+00:00</updated><content/><link href="https://cdn.openai.com/pdf/a253471f-8260-40c6-a2cc-aa93fe9f142e/economic-research-chatgpt-usage-paper.pdf"/><published>2025-09-15T19:14:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45253807</id><title>GPT‑5-Codex and upgrades to Codex</title><updated>2025-09-15T22:35:44.117113+00:00</updated><content>&lt;doc fingerprint="bc415d9890a8cc88"&gt;
  &lt;main&gt;
    &lt;p&gt;GPT‑5-Codex and upgrades to Codex. OpenAI half-released a new model today: GPT‑5-Codex, a fine-tuned GPT-5 variant explicitly designed for their various AI-assisted programming tools.&lt;/p&gt;
    &lt;p&gt;I say half-released because it's not yet available via their API, but they "plan to make GPT‑5-Codex available in the API soon".&lt;/p&gt;
    &lt;p&gt;I wrote about the confusing array of OpenAI products that share the name Codex a few months ago. This new model adds yet another, though at least "GPT-5-Codex" (using two hyphens) is unambiguous enough not to add to much more to the confusion.&lt;/p&gt;
    &lt;p&gt;At this point it's best to think of Codex as OpenAI's brand name for their coding family of models and tools.&lt;/p&gt;
    &lt;p&gt;The new model is already integrated into their VS Code extension, the Codex CLI and their Codex Cloud asynchronous coding agent. I'd been calling that last one "Codex Web" but I think Codex Cloud is a better name since it can also be accessed directly from their iPhone app.&lt;/p&gt;
    &lt;p&gt;Codex Cloud also a new feature: you can configure it to automatically run code review against specific GitHub repositories (I found that option on chatgpt.com/codex/settings/code-review) and it will create a temporary container to use as part of those reviews. Here's the relevant documentation.&lt;/p&gt;
    &lt;p&gt;Some documented features of the new GPT-5-Codex model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Specifically trained for code review, which directly supports their new code review feature.&lt;/item&gt;
      &lt;item&gt;"GPT‑5-Codex adapts how much time it spends thinking more dynamically based on the complexity of the task." Simple tasks (like "list files in this directory") should run faster. Large, complex tasks should use run for much longer - OpenAI report Codex crunching for seven hours in some cases!&lt;/item&gt;
      &lt;item&gt;Increased score on their proprietary "code refactoring evaluation" from 33.9% for GPT-5 (high) to 51.3% for GPT-5-Codex (high). It's hard to evaluate this without seeing the details of the eval but it does at least illustrate that refactoring performance is something they've focused on here.&lt;/item&gt;
      &lt;item&gt;"GPT‑5-Codex also shows significant improvements in human preference evaluations when creating mobile websites" - in the past I've habitually prompted models to "make it mobile-friendly", maybe I don't need to do that any more.&lt;/item&gt;
      &lt;item&gt;"We find that comments by GPT‑5-Codex are less likely to be incorrect or unimportant" - less unimportant comments in code is definitely an improvement!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The system prompt for GPT-5-Codex in Codex CLI is worth a read. It's notably shorter than the system prompt for other models - here's a diff.&lt;/p&gt;
    &lt;p&gt;Theo Browne has a video review of the model and accompanying features. He was generally impressed but noted that it was surprisingly bad at using the Codex CLI search tool to navigate code. Hopefully that's something that can fix with a system prompt update.&lt;/p&gt;
    &lt;p&gt;Finally, can it drew a pelican riding a bicycle? Without API access I instead got Codex Cloud to have a go by prompting:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;Generate an SVG of a pelican riding a bicycle, save as pelican.svg&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here's the result:&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My review of Claude's new Code Interpreter, released under a very confusing name - 9th September 2025&lt;/item&gt;
      &lt;item&gt;Recreating the Apollo AI adoption rate chart with GPT-5, Python and Pyodide - 9th September 2025&lt;/item&gt;
      &lt;item&gt;GPT-5 Thinking in ChatGPT (aka Research Goblin) is shockingly good at search - 6th September 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2025/Sep/15/gpt-5-codex/"/><published>2025-09-15T19:17:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45254330</id><title>Scryer Prolog Meetup 2025</title><updated>2025-09-15T22:35:43.390007+00:00</updated><content>&lt;doc fingerprint="e81bf7079ab60a95"&gt;
  &lt;main&gt;
    &lt;p&gt;Menü&lt;/p&gt;
    &lt;p&gt;Konferenz&lt;lb/&gt;Scryer Prolog Meetup 2025&lt;/p&gt;
    &lt;p&gt;Do, 13.11.2025&lt;lb/&gt;↳ bis bis Fr, 14.11.2025&lt;/p&gt;
    &lt;p&gt;Hochschule Düsseldorf&lt;lb/&gt; Gebäude 6&lt;/p&gt;
    &lt;p&gt;The 3rd Scryer Prolog Meetup will take place on Nov. 13th and 14th 2025 at the Hochschule Düsseldorf in Düsseldorf, Germany.&lt;/p&gt;
    &lt;p&gt;This meetup is an excellent opportunity to learn more about the latest developments and applications of Scryer Prolog, a modern free ISO compliant Prolog system.&lt;/p&gt;
    &lt;p&gt;Attendance is free, registration is not required.&lt;/p&gt;
    &lt;p&gt;If you want to give a talk, please contact Prof. Christian Jendreiko and Markus Triska.&lt;/p&gt;
    &lt;p&gt;We hope to see you at the meetup!&lt;/p&gt;
    &lt;p&gt;More information:&lt;lb/&gt; Scryer Prolog&lt;lb/&gt; Scryer Prolog Meetup 2023&lt;lb/&gt; Scryer Prolog Meetup 2024&lt;/p&gt;
    &lt;p&gt;↓&lt;lb/&gt; Thursday, Nov. 13th 2025&lt;/p&gt;
    &lt;p&gt;10.00–11.00&lt;lb/&gt; Mark Thom: Recent progress in Scryer Prolog and current developments&lt;/p&gt;
    &lt;p&gt;11.00–12.00&lt;lb/&gt; Kauê Hunnicutt Bazilli: The Rust, C and Wasm embedding APIs of Scryer Prolog&lt;/p&gt;
    &lt;p&gt;12.00–13.30&lt;lb/&gt; Lunch break&lt;/p&gt;
    &lt;p&gt;13.30–14.30&lt;lb/&gt; David C. Norris: The DEDUCTION Programme – Dose Escalation Designs in Universal Context of Titration for Oncology Drug Development&lt;/p&gt;
    &lt;p&gt;14.30–15.00&lt;lb/&gt; Coffee break&lt;/p&gt;
    &lt;p&gt;15.00–15.30&lt;lb/&gt; Jonathan McHugh: Guix OS and Scryer – Prolog With Added Func&lt;/p&gt;
    &lt;p&gt;15.30–17.00&lt;lb/&gt; Ulrich Neumerkel: Current developments in the Prolog ISO standard, systematic testing of Prolog implementations&lt;/p&gt;
    &lt;p&gt;Starting at 19.00&lt;lb/&gt; Dinner&lt;/p&gt;
    &lt;p&gt;↓&lt;lb/&gt; Friday, Nov. 14th 2025&lt;/p&gt;
    &lt;p&gt;10.00–11.00&lt;lb/&gt; Christian Jendreiko and Björn Lellmann: An update on recent applications of Scryer Prolog in Quantum Mechanics and Music Theory&lt;/p&gt;
    &lt;p&gt;11.00–12.00&lt;lb/&gt; Kauê Hunnicutt Bazilli and Bryan-Elliott Tam: Bakage, a package manager for Prolog systems&lt;/p&gt;
    &lt;p&gt;12.00–13.30&lt;lb/&gt; Lunch break&lt;/p&gt;
    &lt;p&gt;13:30–14:00&lt;lb/&gt; Daniel K. Hashimoto&lt;lb/&gt; Towards an Implementation-Independent Interface for Reasoning about Semantic Web in Prolog&lt;/p&gt;
    &lt;p&gt;14.00–14.30&lt;lb/&gt; Barnabás Zahorán and Bennet Bleßmann: plwm – An X11 window manager written in Prolog&lt;/p&gt;
    &lt;p&gt;14.30–15.00&lt;lb/&gt; Coffee break&lt;/p&gt;
    &lt;p&gt;15.00–16.00&lt;lb/&gt; Michael Leuschel: Using Prolog to Translate B and Set Theory to Answer Set Programming&lt;/p&gt;
    &lt;p&gt;16.00–17.00&lt;lb/&gt; James J. Tolton: TBD&lt;/p&gt;
    &lt;p&gt;Starting at 19.00&lt;lb/&gt; Dinner&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hsd-pbsa.de/veranstaltung/scryer-prolog-meetup-2025/"/><published>2025-09-15T20:07:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45255132</id><title>Imperial Tyranny, Korean Humiliation</title><updated>2025-09-15T22:35:42.538932+00:00</updated><content>&lt;doc fingerprint="7f40128d1ae1d474"&gt;
  &lt;main&gt;
    &lt;p&gt;By Park Hyun, editorial writer&lt;/p&gt;
    &lt;p&gt;While many would have you believe that the fiasco involving over 300 Korean workers being arrested and detained by US immigration authorities has been resolved by allowing the workers to “voluntarily depart” back to Korea, this is far from the truth. Korea as a nation was deeply shocked to witness our workers, who had traveled to the US to work at the request of American investors, shackled at their hands and feet with chains. This barbaric incident will leave a lasting stain on Korea-US relations.&lt;/p&gt;
    &lt;p&gt;The mass arrests are undoubtedly a wake-up call, a major rupture that opens our eyes to what is happening in the US at this moment. We must heed this warning to close the loopholes and traps in our investment projects with the US. Otherwise, we may eventually face even greater calamity.&lt;/p&gt;
    &lt;p&gt;To understand what’s at the core of this situation, we must revisit the “Make America Great Again” movement championed by US President Donald Trump. MAGA represents a reactionary movement by white evangelical forces seeking to revert America to a time before the civil rights movement in the 1960s. Its core supporters are low-income, poorly educated white Americans and evangelical Protestants. Trump has become a voice amplifying the anxieties of these groups, whose societal status has been shaken by job losses due to globalization, deepening economic polarization, and a surge in immigration. Trump has incited them to channel their anger toward the established elite and “outsiders,” such as people of color, undocumented immigrants, and Muslims.&lt;/p&gt;
    &lt;p&gt;Trump is fundamentally a populist and white supremacist. His slogan of “Make America Great Again” would be phrased more accurately as “Make White America Great Again.” His insistence on imposing a 50% tariff specifically on steel and aluminum stems from the fact that white, Protestant populations are concentrated in the American Rust Belt.&lt;/p&gt;
    &lt;p&gt;The massive crackdown on allegedly undocumented immigrants at the Hyundai-LG Energy Solution plant in Georgia must also be understood within this context. The sight of our workers being led out in chains resembles images of African slaves in the 18th and 19th centuries being dragged out by their owners. The Department of Homeland Security boasted that the raid was “the largest single-site enforcement operation in [its] history,” and Immigration and Customs Enforcement even brazenly released footage of the operation — which clearly risks human rights violations — as if to flaunt these arrests as their achievement.&lt;/p&gt;
    &lt;p&gt;Far-right white Americans may have rejoiced inwardly at this incident. Even politicians like the governor of Georgia and local lawmakers, who had previously been enthusiastic about hosting the factory, have abruptly changed their stance and joined the chorus of discontent. This shift is likely because it’s difficult to ignore the anti-immigrant sentiment among Americans born into citizenship. The US is in the grip of an irrational frenzy, reminiscent of the McCarthyism that swept through American society in the 1950s. The US could have resolved this visa issue diplomatically by giving Korea advance notice as an ally. Yet, the crackdown — complete with helicopters and armored vehicles, as if to gleefully flaunt their power — can only be explained as a political performance.&lt;/p&gt;
    &lt;p&gt;The Trump administration’s plan to revive manufacturing in America is a strategy deeply influenced by political calculations rather than economic logic. Having stoked the discontent of the white working class in the Rust Belt to win his presidency, Trump has a strong motive to continue exploiting them politically. While he, as a political leader, may attempt to pursue such policies, these efforts amount to little more than wishful thinking. Historically, such attempts have rarely succeeded. If they had, why did the British Empire, once called a territory where the sun never sets, crumble over time?&lt;/p&gt;
    &lt;p&gt;Declining industries inevitably relocate to emerging nations over time. Even in Korea, we face difficulties in reviving such industries. How much more challenging would it be for the US, where production costs are at least 30% higher than ours, and over two decades of hollowing-out in manufacturing have collapsed the industrial ecosystem? Trump is dreaming a delusional fantasy of using imperial might to forcibly mobilize allies and reverse this trend. To make matters worse, treating allied workers who are trying their best to assist in realizing this pipe dream as if they are slaves of a vassal state will jeopardize even those projects that had some potential. We’re currently watching the US foolishly shoot itself in the foot.&lt;/p&gt;
    &lt;p&gt;This incident should prompt us in Korea to comprehensively reassess our investment projects in the US. We agreed to a tariff deal during a transitional period when President Lee Jae Myung had just taken office. Trump’s aggressive tactics forced us to follow Japan’s lead, but we must now take a cold, hard look at the specifics of what we agreed to. The US made outrageous demands on Japan: execute US$550 billion in investments within Trump’s term, provide funds within 45 days if Trump orders it, and hand over 50%-90% of profits to the US. Reports say the US is now making identical demands of us.&lt;/p&gt;
    &lt;p&gt;Following the footsteps of Japan, the world’s third-largest economy and a quasi-reserve currency nation, could poison our own economy. Rather than yielding to America’s unreasonable demands to avert an immediate crisis, the government must clearly distinguish what we can and cannot do and negotiate with the US. We must bear in mind that even if the manufacturing revival fails, the US, as the world’s largest economy and reserve currency holder, will likely suffer little harm. Our economy, on the other hand, could be severely shaken by a major shock.&lt;/p&gt;
    &lt;p&gt;Please direct questions or comments to [english@hani.co.kr]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://english.hani.co.kr/arti/english_edition/english_editorials/1218475.html"/><published>2025-09-15T21:27:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45255137</id><title>William Gibson Reads Neuromancer</title><updated>2025-09-15T22:35:42.445314+00:00</updated><content>&lt;doc fingerprint="7a59d0cbefbe40c0"&gt;
  &lt;main&gt;
    &lt;p&gt;The author Ray Bradbury is one of the early science fiction authors that moved science fiction into a literary form. As a writer Bradbury constructs beautifully written stories and novels. Bradbury's writing is in stark contrast to Bradbury as a speaker. The first time I heard Ray Bradbury speak was at the Association for Computing Machinery (ACM) yearly conference in Los Angeles in the 1980s. Hearing Bradbury speak is an almost painful experience. The pictures that Bradbury can paint with the written word seem to be entirely missing when Bradbury speaks. He is halting, awkward and does not seem to know where he wants to go in his talk.&lt;/p&gt;
    &lt;p&gt;In contrast to Bradbury, listenting to William Gibson has the feel of his written work. The same complex world view and sentence structure is there, although not as finely edited. An example of this can be found in the documentary made about William Gibson, No Maps for these Territories. This documentary includes extensive interviews with William Gibson. No Maps also provides a glimpse of the way Gibson looks at the interconnections and relationships in the world around us. This view of Gibson's mind shows us his genius.&lt;/p&gt;
    &lt;p&gt;The mirror between William Gibson's spoken voice and his written voice gives special force to his readings of his work. Early in his career Gibson did an abridged reading of Neuromancer, his first novel and the work that made him famous. It was in this novel that Gibson coined the term cyberspace. This reading was only published on audio-tape and is now out of print.&lt;/p&gt;
    &lt;p&gt;I hate the idea that Gibson's wonderful reading of Neuromancer should be lost or inaccessable. I was only able to hear it because the Mountain View (California) Library had a copy. Fortunately I've been able to find an MP3 copy of these audio tapes. They can be downloaded below.&lt;/p&gt;
    &lt;p&gt;I am only providing these MP3s because the original has been out of print for years. As a software engineer I believe that I should be paid for my work. If I hold this view then it is only reasonable that I should also believe that artist should be paid for their work. All of the software and music I own I have paid for (or is open source). I would prefer that the publisher re-issue the audio-tape of William Gibson's reading in a more modern format (perhaps CD) and that William Gibson collect royalties on this work. Gibson's reading has been out of print so long that I can only assume that this is unlikely to happen.&lt;/p&gt;
    &lt;p&gt;If you're a fan of William Gibson I hope that others will mirror these files as well so that they will never be lost.&lt;/p&gt;
    &lt;p&gt;This reading was published on four magnetic tape audio cassetts. These have been re-recorded in MP3 format:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Neuromancer (abridged) read by William Gibson&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 1, side 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 1, side 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 2, side 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 2, side 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 3, side 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 3, side 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 4, side 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 4, side 2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;An on-line copy of William Gibson's Neuromancer&lt;/p&gt;
    &lt;p&gt;Neuromancer is one of the few books that I've read many times. All of Gibson's books are good (well, except for The Difference Engine, but that's Bruce Sterling's fault). Neuromancer is still in print, so you should go out an buy a copy if you want to read it. Writers pay their bills from the royalties from book sales. I've included the link above in case you want to get a feel for the book before you buy it (even paperback books are not cheap these days).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://bearcave.com/bookrev/neuromancer/neuromancer_audio.html"/><published>2025-09-15T21:28:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45255155</id><title>The Revised Report on Scheme or An UnCommon Lisp (1985) [pdf]</title><updated>2025-09-15T22:35:41.192436+00:00</updated><content/><link href="https://dspace.mit.edu/bitstream/handle/1721.1/5600/AIM-848.pdf"/><published>2025-09-15T21:29:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45255337</id><title>Show HN: Pooshit – sync local code to remote Docker containers</title><updated>2025-09-15T22:35:41.065124+00:00</updated><content>&lt;doc fingerprint="66f8a2505587ef69"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Pronounced Push-It....&lt;/p&gt;
      &lt;p&gt;I'm a lazy developer for the most part, so this is for people like me. Sometimes I just want my local code running in live remote containers quickly, without building images and syncing to cloud docker repos or setting up git workflows or any of the other draining ways to get your code running remotely.&lt;/p&gt;
      &lt;p&gt;With pooshit (and a simple config file), you can simply push your local dev files to a remote folder on a VM then automatically remove relevant running containers, then build and run an updated container with one command line call.&lt;/p&gt;
      &lt;p&gt;It works well with reverse proxies like nginx or caddy as you can specify the docker run arguments in the pooshit_config files.&lt;/p&gt;
      &lt;p&gt;https://github.com/marktolson/pooshit&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45255337"/><published>2025-09-15T21:46:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45255400</id><title>Massive Attack Turns Concert into Facial Recognition Surveillance Experiment</title><updated>2025-09-15T22:35:40.411729+00:00</updated><content>&lt;doc fingerprint="53081b6012282d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Imagine you’re vibing to “Teardrop” when suddenly your face appears on the massive LED screen behind the band. Not as a fun crowd shot—as processed data in Massive Attack’s real-time facial recognition system. Welcome to the most uncomfortable concert experience of 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;When Your Face Becomes the Show&lt;/head&gt;
    &lt;p&gt;The band deployed live facial recognition technology that captured and analyzed attendees during their recent performance.&lt;/p&gt;
    &lt;p&gt;During their latest tour stop, Massive Attack shocked fans by integrating facial recognition into the show itself. Live video feeds captured audience faces, processing them through recognition software and projecting the results as part of the visual experience. This wasn’t subtle venue security—your biometric data became part of the artistic statement, whether you consented or not.&lt;/p&gt;
    &lt;p&gt;Social media erupted with bewildered reactions from attendees. Some praised the band for forcing a conversation about surveillance that most people avoid, while others expressed discomfort with the unexpected data capture. The split reactions confirmed the band’s provocative intent had landed exactly as designed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Art Meets Digital Resistance&lt;/head&gt;
    &lt;p&gt;This stunt aligns with the band’s decades-long critique of surveillance culture and digital control systems.&lt;/p&gt;
    &lt;p&gt;This provocation fits Massive Attack’s DNA perfectly. The Bristol collective has spent years weaving political commentary into their performances, particularly around themes of surveillance and control. Their collaboration with filmmaker Adam Curtis and consistent engagement with privacy issues positioned them as natural provocateurs for this moment.&lt;/p&gt;
    &lt;p&gt;Unlike typical concert technology that enhances your experience, this facial recognition system explicitly confronted attendees with the reality of data capture. The band made visible what usually happens invisibly—your face being recorded, analyzed, and potentially stored by systems you never explicitly agreed to interact with.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Consent Question Nobody Asked&lt;/head&gt;
    &lt;p&gt;Details about data storage and participant consent remain unclear, adding to both artistic ambiguity and ethical concerns.&lt;/p&gt;
    &lt;p&gt;Here’s where things get murky. Massive Attack hasn’t released official details about what happened to the captured biometric data or whether permanent records were kept. This opacity intensifies the artistic statement while raising legitimate privacy questions about conducting surveillance to critique surveillance.&lt;/p&gt;
    &lt;p&gt;The audience split predictably along ideological lines. Privacy advocates called it a boundary violation disguised as art. Others viewed it as necessary shock therapy for our sleepwalking acceptance of facial recognition in everyday spaces. Both reactions prove the intervention achieved its disruptive goal.&lt;/p&gt;
    &lt;p&gt;Your relationship with facial recognition technology just got more complicated. Every venue, every event, every public space potentially captures your likeness. Massive Attack simply made the invisible visible—and deeply uncomfortable. The question now isn’t whether this was art or privacy violation, but whether you’re ready to confront how normalized surveillance has become in your daily life.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gadgetreview.com/massive-attack-turns-concert-into-facial-recognition-surveillance-experiment"/><published>2025-09-15T21:51:55+00:00</published></entry></feed>