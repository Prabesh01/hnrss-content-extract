<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-26T15:18:39.757851+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46758644</id><title>LED lighting undermines visual performance unless supplemented by wider spectra</title><updated>2026-01-26T15:18:47.993817+00:00</updated><content>&lt;doc fingerprint="541a12230e7947d4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Life evolved under broad spectrum sunlight, from ultraviolet to infrared (300‚Äì2500 nm). This spectrally balanced light sculpted life‚Äôs physiology and metabolism. But modern lighting has recently become dominated by restricted spectrum light emitting diodes (350‚Äì650 nm LEDs). Absence of longer wavelengths in LEDs and their short wavelength dominance impacts physiology, undermining normal mitochondrial respiration that regulates metabolism, disease and ageing. Mitochondria are light sensitive. The 420‚Äì450 nm dominant in LEDs suppresses respiration while deep red/infrared (670‚Äì900 nm) increases respiration in aging and some diseases including in blood sugar regulation. Here we supplement LED light with broad spectrum lighting (400‚Äì1500 nm+) for 2 weeks and test colour contrast sensitivity. We show significant improvement in this metric that last for 2 months after the supplemental lighting is removed. Mitochondria communicate across the body with systemic impacts following regional light exposure. This likely involves shifting patterns of serum cytokine expression, raising the possibility of wider negative impacts of LEDs on human health particularly, in the elderly or in the clinical environment where individuals are debilitated. Changing the lighting in these environments could be a highly economic route to improved public health.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Ambient light impacts on human health. Sunlight, under which life evolved, extends over approximately 300‚Äì2500 nm. Older incandescent lighting common until recently has a similar spectral range. But because our visual sensitivity is limited to 400‚Äì700 nm we are unaware of infrared light (approximately 700‚Äì2500 nm). However, light in the built environment is now driven by light emitting diodes (LEDs), whose restricted spectrum (approximately 350‚Äì650 nm) is designed around our visual sensitivity and consequently is economic1.&lt;/p&gt;
    &lt;p&gt;Typical LED lighting produces strong elements in the shorter blue wavelengths (420‚Äì450 nm) with a second yellow peak which drops swiftly above 650 nm, with little light above 700 nm1. Short wavelength exposure in animals in the range of 420‚Äì450 nm reduces mitochondrial function, which provides the energy for physiological performance in the form of adenosine triphosphate (ATP). This short wavelength light reduces mitochondrial complex activity and ATP production, in a highly conserved manner. Hoh Kam et al. showed a significant decrease in mitochondrial enzymatic activity in fruit flies for complexes I-IV under 420 nm light2. Kaynezhad et al. used broadband near infrared spectroscopy (bNIRS) imaging the mouse retina and reported significant instability of deoxygenated haemoglobin and oxidised cytochrome-c-oxidase after exposure to 420 nm light. This instability remained significant through a 1 h recovery period when the light was withdrawn3. Short wavelength light (420 and 450 nm) also results in increased body weight. Hussaini et al. demonstrated that mice exposed to these wavelengths gaining weight rapidly compared to controls over the course of eight weeks4. Shorter wavelengths in similar ranges are also associated with reduced lifespan. Nash et al. revealed a 50% drop in the median lifespan of fruit flies exposed to unfiltered white LED light relative to those kept in darkness, but only 4% drop if this LED light was passed through a yellow filter, blocking the shorter wavelength light5. This negative influence is likely due to mitochondrial absorption by porphyrin that may increase proinflammatory oxygen singlet production reducing mitochondrial function as proposed by Kaynezhad et al.3.&lt;/p&gt;
    &lt;p&gt;Longer wavelengths (700 nm+) penetrate deeply and those in sunlight can be measured passing through the human body6. These are absent from standard LEDs but present in sunlight and incandescent lighting. Their presence increases mitochondrial performance and ATP production, particularly when challenged by age or disease. Gkotsi et al. demonstrated significantly increased ATP production in the retina, cortex, and thalamus of mice following exposure to 670 nm light7. Calaza et al. revealed a 50% increase in ATP in eight-month-old complement factor H knock out mice that have a mitochondrial deficit and are used as a murine model of macular degeneration8.&lt;/p&gt;
    &lt;p&gt;Increased mitochondrial performance is associated with increased lifespan and enhanced mobility. Begum et al. demonstrated using fruit flies that exposure to 670 nm resulted in a positive divergence of ageing survival rates of 10% at 4 weeks of age and up to nearly 180% by 8 weeks of age. The older animals also displayed an almost doubling of mobility against controls9. Neonicotinoid insecticides specifically target mitochondrial respiration inducing Parkinson like symptoms of immobility resulting in death. Here 670 exposure reversed damaged ATP levels to normal and corrects mobility and lifespan issues10.&lt;/p&gt;
    &lt;p&gt;Increased mitochondrial activity should result in reduced blood sugars and increased oxygen consumption as mitochondria use both in respiration. Powner and Jeffery found both in bumble bees exposed to 670 nm light11. The same authors translated this to humans showing again, reduced blood sugars and increased oxygen consumption in a standard glucose tolerance test following 15 min of 670 nm exposure. Here the spike in blood glucose was reduced significantly by around 27%12.&lt;/p&gt;
    &lt;p&gt;Changes in physiology produced by longer wavelengths translate to improved function. Shinhmar et al. revealed improved colour contrast sensitivity in humans after 3 min of morning exposure to 670 nm light13. Hence, exposure to different ends of the spectrum that impact differentially on mitochondria can translate into changes in key physiological metrics.&lt;/p&gt;
    &lt;p&gt;Similar changes are found at the population level. Those spending more time in sunlight generally have improved health including reduced incidents of cardiovascular disease and the incidence of cancer. They also have lower rates of type 2 diabetes14,15.&lt;/p&gt;
    &lt;p&gt;In this study we confront the impact of LED lighting on human visual performance by measuring colour contrast detection in an LED illuminated working environment that is then supplemented with incandescent lighting. The hypothesis is that LED lighting suppresses mitochondrial function in the retina and that this can be corrected by introduction of wide spectrum incandescent lights. The results highlight the potential damaging influence of LED lighting on human performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methods&lt;/head&gt;
    &lt;p&gt;The Subjects and their environment: The study was conducted in accordance with the Declaration of Helsinki and approved by University College London research ethics committee (16547/001). It was undertaken in University College London buildings in October to December. In October local daylight hours were approximately 10.37 with 75% cloud cover. In November local daylight hours were approximately 8.45 with 55% cloud cover. In December local daylight hours were approximately 7.5 with 90% cloud cover. Local sunset time in October is approximately 18.30. In November it is approximately 16.15 and in December it is approximately 16.00. Consequently, many subjects would be returning home after sunset in November and December. Subjects worked approximately 8 h a day 5 days a week and travel to and from work via public transport that was illuminated by LED devices. Most subjects did not leave the building in which they worked during the working day in these months. For those that did it was commonly for less than 15 min at lunch time. Within the work environment subjects were free to move around. Here the internal lighting they experienced was consistently LED based. Hence, natural daylight exposure during this latter part of the year was limited. We could not control for weekend exposure, however subjects homes were consistently illuminated with LEDs and because the weather in the UK at this time of year is inclement, their time outside buildings can be expected to be limited.&lt;/p&gt;
    &lt;p&gt;Each participant provided written informed consent prior to testing and data generated was anonymised. Subjects (N = 22) were of both sexes and between the ages of 23 and 65 years. Prior to the experiment all subjects were asked to confirm normal corrected visual function and general good systemic health. This was undertaken in a simple interview prior to their inclusion in the study. All were healthy without visual or other health problems. Experimental subjects (N = 11) worked exclusively under LED lighting in the back of the Here East building on the north side, &amp;gt; 50 m from what little light did manage to penetrate the entrance doors when open. The LED lighting delivered an illuminance of 1000 lx at working height, with a correlated colour temperature (CCT) of 4000 K, and a TM-30 average colour fidelity index, Rf, of 91. The infrared light that was introduced was provided by tungsten desk lamps placed around the working space was non-uniform. The visible component of the 60 W tungsten bulbs was small when compared with the 1000 lx of LED. The test subjects were not expected to use these as task lamps. The LED lighting delivered an irradiance of 3.7 W/m2 on the horizontal working plane.&lt;/p&gt;
    &lt;p&gt;Control subjects (N = 11) worked in similar environments under LED lighting without direct sunlight. The LED lighting delivered an illuminance of 900 lx at working height, with a CCT of 3000 K and a Rf of 85. The colour contrast tests were performed in a darkened room where the only light came from the test itself. There were no requirements restricting other light exposure patterns during the study.&lt;/p&gt;
    &lt;p&gt;The experimental location: Subjects worked at UCL Here East, a media and innovation complex located in East London (London E15 2GW), originally built as a press and broadcast centre for the London 2012 Olympics and subsequently repurposed as a campus. UCL Here East occupies part of the Broadcast Centre, taking up the ground and first floor of unit B. The footprint of the building is deep, with daylight only able to enter through the glazing at the front of the building. This glazing uses an infrared blocking film, which can be revealed using infrared photography.&lt;/p&gt;
    &lt;p&gt;A Canon 500D digital camera was modified to replace the infrared blocking layer with clear glass that passes infrared wavelengths. This was used in conjunction with filters that block visible and infrared wavelengths to explore the presence and absence of infrared light. Spectral measurements were made with two spectrophotometers (Ocean Optics SR-6XR250-50 and FLAME-NIR) with optic fibre and cosine correctors used to collect the incandescent spectra in the shorter and longer wavelengths.&lt;/p&gt;
    &lt;p&gt;Incandescent desk lighting was introduced into the work environment using desk lamps with 60 W clear Edison bulbs (Polaris UK) placed on work benches. All subjects had worked in this LED-lit environment for more than 2 years. Desk lamps with incandescent bulbs were introduced onto the benches where experimental subjects spent the majority of their time. They were given the incandescent lighting for 2 weeks and, while they spend the majority of their time working near these lights, they were free to move around and leave their desks as they wished. The introduced light showed a high degree of reflectance from the work surfaces.&lt;/p&gt;
    &lt;p&gt;Colour contrast testing: All subjects were tested for colour contrast ability using ChromaTest prior to the introduction of incandescent lighting and then again 2 weeks later. This test must be carried out in a darkened room, so a nearby windowless room was set aside for this purpose. The incandescent lighting was then removed and subjects retested at 4 and 6 weeks. Hence, this element of the experiment was a before and after design which avoids between subject variability. However, there was also a separate control group (N = 11) composed of subjects that worked under LED lighting similar to those in the experimental group.&lt;/p&gt;
    &lt;p&gt;ChromaTests is a sensitive measure of colour contrast detection of letters presented in a random order against a noisy visual background in either tritan (blue) or protan (red) visual axes13. If subjects correctly identify a letter its contrast is reduced in the next presentation of a letter. Likewise, if they fail to correctly identify the letter, the contrast is increased. This is repeated until thresholds are determined in 5 identical repeated trials. This normally involved around 70‚Äì100 separate presentations in total. Subjects were given an initial trial before testing to avoid a learning effect. Initial presentations were at high colour contrast. No learning effects were noted in the study.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Light assessment in the experimental environment&lt;/head&gt;
    &lt;p&gt;Figure 1 shows the front exterior of the Here East building using infrared imaging at ground level where experiments were undertaken. The windows are completely infrared reflective due to their blocking film and hence mirror-like. Figure 2 is an infrared image from inside the building looking out through the open doorway. Only infrared light coming through the open door and its reflectance can be seen, not light coming through adjacent windows. Hence, the building is relatively impervious to infrared light.&lt;/p&gt;
    &lt;p&gt;Figure 3 shows the working environment in Here East in visible light in which the experiment took place. Images in infrared were completely black. The distance between the work environment and the front door was &amp;gt; 50 m with multiple doors between.&lt;/p&gt;
    &lt;p&gt;The internal lighting throughout Here East was provided by arrays of ceiling mounted standard LED units. Spectral profiles of the lighting within the building are shown in Fig. 4. against incandescent lighting in black and red.&lt;/p&gt;
    &lt;p&gt;The blue curve shows the spectral profile of the light delivered to the horizontal working plane. As the work environment was deep in the building and lit only with LED lighting, it received no daylight and was devoid of any infrared illumination. The LED units delivered 1000 lx on the horizontal working plane with a correlated colour temperature (CCT) of 4000 K and a TM-30 colour fidelity index of 91. The irradiance of this LED light was 3.6 W/m2.&lt;/p&gt;
    &lt;p&gt;The specific spectra and energy levels were mapped over the workspace at fixed locations. This is shown in Fig. 5a and b. Here there is a plan of the space and measurements made at 9 locations of energy given in W/cm‚àí 2 and lux, which is a metric corrected for the human eye. Also provided are the spectra at each location. Changes in brightness at different locations were largely not detectable by the human eye and were gradual. There were no differences in spectral profiles across the area only their relative intensity.&lt;/p&gt;
    &lt;p&gt;Light assessment was also undertaken at bench level where individual subjects worked. This is shown in Fig. 6. This confirmed the absence of any part of the infrared spectrum in the work environment and how this changed with the addition of the incandescent lamps.&lt;/p&gt;
    &lt;p&gt;In this study, responses to lighting were measured in test subjects both before and also after the lighting had been changed. However, there was also an independent control group that comprised individuals under similar LED lighting condition without daylight. A comparison of the lighting conditions in the two groups is shown in Fig. 7. Critically, the LEDs in both groups had very similar profiles with no infrared components. The overall brightness in the control group was slightly less than in the test group, although this was not apparent to the human eye. As in the test group, subjects were free to move around.&lt;/p&gt;
    &lt;head rend="h3"&gt;Visual responses to shifts in spectral lighting&lt;/head&gt;
    &lt;p&gt;Exposure to 60 W incandescent luminaires, which have a wider spectrum than LEDs extending into the infra-red1, resulted in significant improvements in visual performance in all experimental subjects across both the protan and tritan visual ranges. Improvements in both tritan and protan were of the order of 25%. Hence, significant improvements were uniform across visual ranges (Fig. 8). This is unlike experiments where specific red/infrared ranges have been used in LED devices, for example via 670 nm, where visual improvements have been biased towards tritan function13.&lt;/p&gt;
    &lt;p&gt;Figure 8 shows the results of both individual subjects on the left and also changes in the groups on the right. In spite of the universal improvement in visual function, in both tritan and protan range there was considerable variability between subjects. This variability validates the inclusion of a repeated measures design and the use of a sign test in the analysis. In all cases protan thresholds were lower than tritan consistent with previous studies13.&lt;/p&gt;
    &lt;p&gt;At the end of the 2 week period the incandescent luminaires were removed and the subjects returned to an exclusively LED dominated light working environment. They were then retested at 4 and 6 weeks. In previous experiments where 670 nm alone has been used, rather than the wide spectrum infrared produced by incandescent lighting, visual improvements decline in approximately a week13. However, following incandescent light exposure improvements remain unchanged across both visual domains at both 4 and 6 weeks. Hence, the impact of broad-spectrum incandescent light not only resulted in balanced improvements in colour contrast but also these improvements lasted much longer than previous interventions with restricted red/infrared ranges13.&lt;/p&gt;
    &lt;p&gt;An independent control group was used in addition to a before and after experimental design. Again, data between individuals was varied on both visual metrics. However, over a 2 week period there were no significant changes in proton or tritan visual thresholds (Fig. 9).&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;We demonstrate that the visual performance of those working under standard LED is significantly improved by exposure to incandescent lighting that has a spectrum similar to daylight with an extensive infrared component. These data are consistent with the hypothesis that LED lighting undermines human visual performance. This result is consistent with laboratory experiments where specific red/infrared wavelength ranges generated by LEDs have been used to improve visual function in animals and humans in a conserved manner13,16,17. But there are three critical differences from these earlier studies. First, we have simply changed environmental lighting in a free moving work environment. Second, we have obtained significant balanced improvements in both the protan and tritan range. Previously, exposure to restricted experimental 670 nm resulted in improvements biased strongly in favour of only tritan function13. Hence, exposure to full spectrum lighting results in a balanced pattern of improvement in visual performance. Third, we have shown that improvements in visual function following incandescent light exposure are sustained for up to 6 weeks, and possibly beyond, whereas benefits from single LED restricted range red light were confined to around 5 days13. These three features change the way in which long wavelength light may be applied to improve human physiology by delivery in normal environments with lasting balance effects. These results are novel and may have public health implications.&lt;/p&gt;
    &lt;p&gt;Our study used 22 subjects but was statistically significant using both a before and after metric and also against an independent control group. They are also similar to group sizes in aspects of Shinhmar et al.13 (Figs. 2, 3, 4 and 5). However, future studies would clearly benefit from inclusion of a larger number of subjects.&lt;/p&gt;
    &lt;p&gt;The evolution of life on earth extends over 4 billion years, and that of humans over approximately 4‚Äì5 million years from the last common primate ancestor. This has all taken place under sunlight that has a spectral range of approximately 300‚Äì2500 nm+, within which there has been an invariant balance between short and longer wavelengths. Human adoption of fire 1‚Äì2 million years ago supplemented sunlight as they moved out of Africa as its spectrum is similar having a large infrared component. Likewise, development of the Edison filament luminaire, common until approximately the year 2000 had a spectrum similar to sunlight. However, around 2010 LED lighting with its highly restricted spectrum (350‚Äì650 nm) and energy saving characteristics became common, resulting in a loss of infrared light in the built environment1.&lt;/p&gt;
    &lt;p&gt;The physiology of life forms are adapted to natural environmental light in a highly conserved pattern across species. Light impacts on mitochondrial function, which is a key regulator of metabolism and ageing in animals. When the balance of short and long wavelengths is shifted there are consequences for mitochondria. When shorter wavelength exposure is dominant, as in LED lighting, mitochondrial function declines. Mitochondrial complex proteins are reduced and there is reduced ATP production2,3. With reduced mitochondrial demand for glucose there is increased body weight and disruptions to serum cytokines4. Consequently, consistent with the mitochondrial theory of ageing there is an increased probability of cell/organism ageing and death18. It is suggested that this is partly due to 420‚Äì450 nm light, dominant in LEDs, being absorbed by porphyrin and the subsequent production of oxygen singlets driving inflammation3.&lt;/p&gt;
    &lt;p&gt;Conversely, exposure to longer wavelengths is associated with increased mitochondrial membrane potential and increased concentration of mitochondrial complex proteins that have declined with ageing and disease. This in turn is associated with elevated ATP, reduced inflammation and extended average lifespan7,9,10,19. The experimental use of longer wavelengths in such situations is commonly referred to as photobiomodulation.&lt;/p&gt;
    &lt;p&gt;The retina has the greatest metabolic rate in the body and a high mitochondrial concentration20. Retinal metabolism declines with age, but this can be partly corrected with long wavelength light across species16,21. In humans a single 3 min 670 nm exposure improves colour vision within 3 h, which is sustained for almost a week13. But what the authors of this study did not appreciate was that this was within a population who worked and lived mainly under LED lighting that may have undermined their baseline measurements. Here, we made no attempt to control light exposures or subject movements as would occur in laboratory-based experiments. Rather, our aim was to introduce wide spectrum long wavelengths into a work environment to improving human performance via mitochondrial manipulation in a translational step.&lt;/p&gt;
    &lt;p&gt;There is considerable evidence that introduction of longer wavelengths impact systemically. Durieux et al.22 stated in relation to experiments in C.elegans that ‚Äú We find that mitochondrial perturbation in one tissue is perceived and acted upon by the mitochondrial stress response pathway in distal tissue‚Äù. In mice there are significant distinct changes serum cytokine expression to exposures of both short and long wavelength light4,23. Similarly, long wavelength exposures to the surface of the human body excluding the eyes significantly reduces blood glucose levels and increases oxygen consumption in humans. This is likely because mitochondrial upregulation will increase carbohydrate demand to support increased ATP production12. Other systemic impacts can be found and are clear in experimentally induced Parkinson‚Äôs in primates. Light targeted by implants focusing on the substantia nigra are effective in reducing symptoms24, but so also are those that are directed at distal locations25.&lt;/p&gt;
    &lt;p&gt;Single 3 min 670 nm exposures remain effective for about 5 days13. But we show that with a wider spectrum they remain effective for 6 weeks, although we did not find the end of the effect. Here it is worth considering potential mechanisms of action which remain subject of debate. Historically, improvements with red light were thought to be due to light absorption by cytochrome C in the respiratory chain26. However, positive effects are found in vitro in the absence of this. Consequently, it has been suggested that longer wavelengths reduce water viscosity around rotary ATP pumps allowing the rotor to increase speed27. This cannot explain the sustained impacts of light exposure as this effect should be relatively transitory as viscosity would increase rapidly following light withdrawal. However, a key feature of long wavelength light absorption is increased respiratory chain protein synthesis. These proteins are in flux throughout the day28 and complex IV is upregulated following red light exposure19. Hence, while red light may initially increase rotor pump speed there rapidly follows an increase in protein synthesis which may establish greater respiratory chain capacity. The life of these proteins could then determine the length of effect.&lt;/p&gt;
    &lt;p&gt;Only thirteen polypeptides are made in mitochondrial protein synthesis. This probably slows with age and likely contributes to aged mitochondrial decline18. But critically, we do not know the speed of mitochondrial protein synthesis, the life of such proteins or the pace of their decline. We suggest that these may be key events in the length of the effects from light exposure.&lt;/p&gt;
    &lt;p&gt;LED lighting clearly has the ability to undermine visual performance probably via reduced mitochondrial function. As light induced changes in mitochondrial ability have been shown to have systemic impacts4,15,22,23,25, the effects of LEDs revealed here may be wider than initially anticipated. Given the prevalence of LEDs, this may represent an important issue in public health and clinical environments where changing lighting patterns in appreciation of this point can have significant positive outcomes29.&lt;/p&gt;
    &lt;p&gt;Given our results, it is important to ask what solutions may be found to improve health in terms of lighting in the built environment. Incandescent lights that we reveal here to have significant positive impact over standard LEDs are being phased out universally for reasons of energy efficiency, where focus is only on the visible light produced.&lt;/p&gt;
    &lt;p&gt;A solution may be found in creating lighting units with multiple longer wavelength LEDs to cover a wider span of the near infrared. However, our attempts here have had limited success. Multiple closely associated spectral peaks do not produce a smooth spectral output as found in incandescent lights and sunlight, which is problematic in improving function and has yet to deliver. This possibly may be overcome using a greater number of spectral peaks with tighter spacing. But this raises a different series of problems regarding cost and increased energy consumption making this solution no better than retention of incandescent sources in terms of environmental sustainability.&lt;/p&gt;
    &lt;p&gt;Key to this issue is the question of how much infrared is needed to sustain improved function? Infrared has relatively few absorbers in the built environment and in current studies relatively little has to be added to the environment for effect. However, a viable option is to run an incandescent light at a lower temperature which results in both energy savings and increased life of the unit and also shifts the peak spectral output towards longer wavelengths.&lt;/p&gt;
    &lt;p&gt;If this is done with a halogen bulb, which is a type of incandescent tungsten bulb, the filament lasts for a longer period as evaporated tungsten is redeposited on the filament rather than blackening the bulb glass. Hence, using a halogen bulb at lower voltage is a realistic alternative in terms of health and energy consumption.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data availability&lt;/head&gt;
    &lt;p&gt;The data sets used and/or analysed during the current study are available from the corresponding author on reasonable request.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Ratto, G. E., Videla, F. A. &amp;amp; Martinez Valiviezd, J. H. Artificial light: traditional and new sources, their potential impact on health, and coping strategies: preliminary spectral analysis. Proc. SPIE Conf. 11814. San Diego California. (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hoh Kam, J., Hogg, C., Fosbury, R., Shinhmar, H. &amp;amp; Jeffery, G. Mitochondria are specifically vulnerable to 420nm light in drosophila which undermines their function and is associated with reduced fly mobility. Plos one. Sep 3;16(9):e0257149. (2021). https://pubmed.ncbi.nlm.nih.gov/34478469. PMID: 34478469.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kaynezhad, P. et al. Near infrared spectroscopy reveals instability in retinal mitochondrial metabolism and haemodynamics with blue light exposure at environmental levels. J.Biophotonics. ;15(4):e202100283. (2022). https://pubmed.ncbi.nlm.nih.gov/35020273/. PMID: 35020273.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Al-Hussaini, H. et al. Impact of short wavelength light exposure on body weight, mobility, anxiety like behaviour and cytokine expression. Sci. Rep. ;15(1):5927. (2025). https://pubmed.ncbi.nlm.nih.gov/39966413/. PMID: 39966413.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nash, T. R. et al. Daily blue-light exposure shortens lifespan and causes brain neurodegeneration in Drosophila. Aging. Mech. Dis 2019 Oct 17:5:8. https://pubmed.ncbi.nlm.nih.gov/31636947/. PMID: 31636947.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jeffery, G. et al. Longer wavelengths in sunlight pass through the human body and have a systemic impact which improves vision. Sci. Rep. 2025 July;15(1);24435. https://doi.org/10.1038/s41598-025-09785-3&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gkotsi, D. et al. Recharging mitochondrial batteries in old eyes. Near infra-red increases ATP. Exp.Eye Res. 2014 May:122:50 ‚Äì 3. https://pubmed.ncbi.nlm.nih.gov/24631333/ PMID: 24631333.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Calaza, K. C., Hoh Kam, J., Hogg, C. &amp;amp; Jeffery, G. Mitochondrial decline precedes phenotype development in the complement factor H mouse model of retinal degeneration but can be corrected by near infrared light. Neurobiol. Aging. ;36(10):2869-76. (2015). https://pubmed.ncbi.nlm.nih.gov/26149919/ PMID: 26149919.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Begum, R. et al. Near-infrared light increases ATP, extends lifespan and improves mobility in aged Drosophila melanogaster. Biol.Lett. ;11(3):20150073. (2015). https://pubmed.ncbi.nlm.nih.gov/25788488/ PMID: 25788488.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Powner, P. MB, SaltTE, Hogg, C. &amp;amp; Jeffery, G. Improving mitochondrial function protects bumblebees from neonicotinoid pesticides. Plos One. 11 (11), e0166531 (2016). https://pubmed.ncbi.nlm.nih.gov/27846310/ PMID: 27846310.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Powner, P. MB &amp;amp; Jeffery, G. Systemic glucose levels are modulated by specific wavelengths in the solar light spectrum that shift mitochondrial metabolism. Plos One. 17 (11), e0276937 (2022). https://pubmed.ncbi.nlm.nih.gov/36327250/ PMID: 36327250.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Powner, M. B. &amp;amp; Jeffery, G. Light stimulation of mitochondria reduces blood glucose levels. J.Biophotonics. ;17(5):e202300521. (2024). https://pubmed.ncbi.nlm.nih.gov/38378043/. PMID: 38378043.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shinhmar, H., Hoog, C., Neveu, M. &amp;amp; Jeffery, G. Weeklong improved colour contrasts sensitivity after single 670 nm exposures associated with enhanced mitochondrial function. Sci. Rep. ;11(1):22872. (2021). https://pubmed.ncbi.nlm.nih.gov/34819619/. PMID: 34819619.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weller, R. B. &amp;amp; Sunlight Time for a Rethink? J.Invest. Dermatol. ;144(8):1724‚Äì1732. (2024). https://pubmed.ncbi.nlm.nih.gov/38661623/ PMID: 38661623.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shore-Lorenti, C. et al. Shining the light on Sunshine: a systematic review of the influence of sun exposure on type 2 diabetes mellitus-related outcomes. Clin. Endocrinol. ;81(6):799‚Äì811. (2014). https://pubmed.ncbi.nlm.nih.gov/25066830/ PMID: 25066830.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weinrich, T. W., Coyne, A., Salt, T. E., Hogg, C. &amp;amp; Jeffery, G. Improving mitochondrial function significantly reduces metabolic, visual, motor and cognitive decline in aged Drosophila melanogaster. Neurobiol. Aging. 2017 Dec:60:34‚Äì43. doi: 10.1016. https://pubmed.ncbi.nlm.nih.gov/28917665/ PMID: 28917665.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sivapathasuntharam, C., Sivaprasad, S., Hogg, C. &amp;amp; Jeffery, G. Aging retinal function is improved by near infrared light (670 nm) that is associated with corrected mitochondrial decline. Neurbiol. Aging 2017 Apr:52:66‚Äì70. https://pubmed.ncbi.nlm.nih.gov/28129566/ PMID: 28129566.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lopez-Otin, C., Blasco, M. A., Partridge, L., Serrano, M. &amp;amp; Kroemer, G. The hallmarks of aging. Cell 153 (6), 1194‚Äì1217 (2013). https://pubmed.ncbi.nlm.nih.gov/23746838/ PMID: 23746838.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Begum, R., Powner, P. MB, Hudson, N., Hogg, C. &amp;amp; Jeffery, G. Treatment with 670 Nm light up regulates cytochrome C oxidase expression and reduces inflammation in an Age-Related macular degeneration model. Plos One. 8 (2), e57828 (2013). https://pubmed.ncbi.nlm.nih.gov/23469078/ PMID: 23469078.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kocherlakota, S., Hurley, J. B., Shu, D. Y. &amp;amp; Editorial Retinal metabolism in health and disease. Front Ophthalmol (Lausanne). 2024 Jul 17:4:1459318. https://pubmed.ncbi.nlm.nih.gov/39086994/ PMID: 39086994.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hoh Kam, J. et al. Mitochondrial decline in the ageing old world primate retina: Little evidence for difference between the centre and periphery. Plos One. ;18(5):e0273882. (2023). https://pubmed.ncbi.nlm.nih.gov/37130143/ PMID: 37130143.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Durieux, J., Wolff, S. &amp;amp; Dillin, A. The cell-non-autonomous nature of electron transport chain-mediated longevity. Cell 144 (1), 79‚Äì91 (2011). https://pubmed.ncbi.nlm.nih.gov/21215371/ PMID: 21215371.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shinhmar, H., Hogg, C. &amp;amp; Jeffery, G. Exposure to long wavelength light that improves aged mitochondrial function shifts acute cytokine expression in serum and the retina. Plos One. 18 (7), e0284172 (2023). https://pubmed.ncbi.nlm.nih.gov/37478072/ PMID: 37478072.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Darlot, F. et al. Near-infrared light is neuroprotective in a monkey model of Parkinson disease. Ann. Neurol. ;79(1):59‚Äì75. (2016). https://pubmed.ncbi.nlm.nih.gov/26456231/ PMID: 26456231.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gordon, L. C. et al. Remote photobiomodulation targeted at the abdomen or legs provides effective neuroprotection against parkinsonian MPTP insult. Eur. J. Neurosci. ;57(9):1611‚Äì1624. (2023). https://pubmed.ncbi.nlm.nih.gov/36949610/. PMID: 36949610.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Salehpour, F., Mahmoudi, J., Kamari, F., Sadigh-Eteghad, Rasta, S. H. &amp;amp; Hamblin, M. R. Brain Photobiomodulation Therapy: a Narrative Review. Mol. Neurobiol. ;55(8):6601‚Äì6636. (2018). https://pubmed.ncbi.nlm.nih.gov/29327206/ PMID: 29327206.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sommer, A. P. Mitochondrial cytochrome c oxidase is not the primary acceptor for near infrared light-it is mitochondrial bound water: the principles of low-level light therapy. Ann. Transl. Med. ;7(Suppl 1):S13. (2019). https://pubmed.ncbi.nlm.nih.gov/31032294/. PMID: 31032294.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weinrich, T. et al. A day in the life of mitochondria reveals shifting workloads. Sci. Rep. ;9(1):13898. (2019). https://pubmed.ncbi.nlm.nih.gov/31554906/ PMID: 31554906.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Neto, R. P. M. et al. Photobiomodulation therapy (red/NIR LEDs) reduced the length of stay in intensive care unit and improved muscle function: A randomized, triple-blind, and sham-controlled trial. J.Biophotonics. ;17(5):e202300501. (2024). https://pubmed.ncbi.nlm.nih.gov/38262071/ PMID: 38262071.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We thank Chris Hogg for assistance with Chromatest, and Mandana Khanie for use of the Ocean Optics spectrophotometers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Funding&lt;/head&gt;
    &lt;p&gt;This research did not receive funding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Author information&lt;/head&gt;
    &lt;head rend="h3"&gt;Authors and Affiliations&lt;/head&gt;
    &lt;head rend="h3"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;GJ and EB designed the experiments undertook all the experimental work and wrote the manuscript.&lt;/p&gt;
    &lt;head rend="h3"&gt;Corresponding author&lt;/head&gt;
    &lt;head rend="h2"&gt;Ethics declarations&lt;/head&gt;
    &lt;head rend="h3"&gt;Competing interests&lt;/head&gt;
    &lt;p&gt;The authors declare no competing interests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional information&lt;/head&gt;
    &lt;head rend="h3"&gt;Publisher‚Äôs note&lt;/head&gt;
    &lt;p&gt;Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rights and permissions&lt;/head&gt;
    &lt;p&gt;Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.&lt;/p&gt;
    &lt;head rend="h2"&gt;About this article&lt;/head&gt;
    &lt;head rend="h3"&gt;Cite this article&lt;/head&gt;
    &lt;p&gt;Barrett, E.M., Jeffery, G. LED lighting (350-650nm) undermines human visual performance unless supplemented by wider spectra (400-1500nm+) like daylight. Sci Rep 16, 3061 (2026). https://doi.org/10.1038/s41598-026-35389-6&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Received:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Accepted:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Published:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Version of record:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DOI: https://doi.org/10.1038/s41598-026-35389-6&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nature.com/articles/s41598-026-35389-6"/><published>2026-01-25T21:44:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46759063</id><title>The future of software engineering is SRE</title><updated>2026-01-26T15:18:47.859327+00:00</updated><content>&lt;doc fingerprint="bd4e911dfc4564fd"&gt;
  &lt;main&gt;
    &lt;p&gt;When code gets cheap operational excellence wins. Anyone can build a greenfield demo, but it takes engineering to run a service.&lt;/p&gt;
    &lt;p&gt;You may be wondering: With all the hype about agentic coding, will we even need software engineers anymore? Yes! We'll need more.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;SRE about to become the most hired job in engineering&lt;/p&gt;‚Äî Swizec Teller (@Swizec) January 13, 2026&lt;lb/&gt;Everybody wants to write a greenfield demo.&lt;lb/&gt;Nobody wants to run a service. https://t.co/THl9rBJ9rk&lt;/quote&gt;
    &lt;p&gt;Writing code was always the easy part of this job. The hard part was keeping your code running for the long time. Software engineering is programming over time. It's about how systems change.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons from the no-code and spreadsheets era&lt;/head&gt;
    &lt;p&gt;Let's take no-code and spreadsheets as an example of the kind of software people say is the future ‚Äì custom-built, throwaway, built by non-experts to solve specific problems.&lt;/p&gt;
    &lt;p&gt;Joe Schmoe from accounting takes 10 hours to do a thing. He's does this every week and it feels repetitive, mechanical, and boring. Joe could do the work in his sleep.&lt;/p&gt;
    &lt;p&gt;But he can't get engineering resources to build a tool. The engineers are busy building the product. No worries, Joe is a smart dude. With a little Googling, a few no-code tools, and good old spreadsheet macros he builds a tool.&lt;/p&gt;
    &lt;p&gt;Amazing.&lt;/p&gt;
    &lt;p&gt;Joe's tool is a little janky but his 10 hour weekly task now takes 1 hour! üéâ Sure, he finds a new edge case every every week and there's constant tinkering, but he's having a lot more fun.&lt;/p&gt;
    &lt;p&gt;Time passes, the business changes, accounting rules are in constant flux, and let's never talk about timezones or daylight savings ever again. Joe is sick of this bullshit.&lt;/p&gt;
    &lt;p&gt;All he wanted was to make his job easier and now he's shackled to this stupid system. He can't go on vacation, he can't train anyone else to run this thing successfully, and it never fucking works right.&lt;/p&gt;
    &lt;p&gt;Joe can't remember the last time running his code didn't fill him with dread. He spends hours carefully making sure it all worked.&lt;/p&gt;
    &lt;head rend="h2"&gt;The computer disease&lt;/head&gt;
    &lt;p&gt;Feynman called this the computer disease.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Feynman called this The Computer Disease pic.twitter.com/Zv4Bu4ftv1&lt;/p&gt;‚Äî Swizec Teller (@Swizec) December 26, 2025&lt;/quote&gt;
    &lt;p&gt;The problem with computers is that you tinker. Automating things is fun! You might forget you don't need to üòÜ&lt;/p&gt;
    &lt;p&gt;The part that's not fun is running things. Providing a service. Reliably, at scale, for years on end. A service that people will hire to do their jobs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why operational excellence is the future&lt;/head&gt;
    &lt;p&gt;People don't buy software, they hire a service.&lt;/p&gt;
    &lt;p&gt;You don't care how iCloud works, you just want your photos to magically show up across devices every time. You don't care about Word or Notion or gDocs, you just want to write what's on your mind, share it with others, and see their changes. And you definitely don't care how a payments network point of sale terminal and your bank talk to each other, you just want your $7 matcha latte to get you through the week.&lt;/p&gt;
    &lt;p&gt;Good software is invisible.&lt;/p&gt;
    &lt;p&gt;And that takes work. A lot of work. Because the first 90% to get a working demo is easy. It's the other 190% that matters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What's your uptime?&lt;/item&gt;
      &lt;item&gt;Defect rate?&lt;/item&gt;
      &lt;item&gt;How quickly do you recover from defects?&lt;/item&gt;
      &lt;item&gt;Do I have to reach out or will you know before me?&lt;/item&gt;
      &lt;item&gt;Can you own upstream dependencies?&lt;/item&gt;
      &lt;item&gt;When a vendor misbehaves, will you notice or wait until your users complain?&lt;/item&gt;
      &lt;item&gt;When users share ideas, how long does it take?&lt;/item&gt;
      &lt;item&gt;How do you keep engineers from breaking each other's systems?&lt;/item&gt;
      &lt;item&gt;Do you have systems to keep engineers moving without turning your app into a disjointed mess?&lt;/item&gt;
      &lt;item&gt;Can you build software bigger than fits in 1 person's brain?&lt;/item&gt;
      &lt;item&gt;When I'm in a 12 hour different timezone, your engineers are asleep, and there's a big issue ... will it be fixed before I give up?&lt;/item&gt;
      &lt;item&gt;Can you recover from failures, yours and upstream, or does important data get lost?&lt;/item&gt;
      &lt;item&gt;Are you keeping up with security updates?&lt;/item&gt;
      &lt;item&gt;Will you leak all my data?&lt;/item&gt;
      &lt;item&gt;Do I trust you?&lt;/item&gt;
      &lt;item&gt;Can I rely on you?&lt;/item&gt;
      &lt;item&gt;How can you be so sure?&lt;/item&gt;
      &lt;item&gt;Will you sign a legally binding guarantee that your software works when I need it? üòâ&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Those are the ~~fun~~ hard engineering challenges. Writing code is easy.&lt;/p&gt;
    &lt;p&gt;Cheers,&lt;lb/&gt; ~Swizec&lt;/p&gt;
    &lt;head rend="h3"&gt;Scaling Fast book free preview&lt;/head&gt;
    &lt;p&gt;Enter your email to receive a sample chapter of Scaling Fast: Software Engineering Through the Hockeystick and learn how to navigate hypergrowth without burning out your team.&lt;/p&gt;
    &lt;p&gt;Have a burning question that you think I can answer? Hit me up on twitter and I'll do my best.&lt;/p&gt;
    &lt;p&gt;Who am I and who do I help? I'm Swizec Teller and I turn coders into engineers with "Raw and honest from the heart!" writing. No bullshit. Real insights into the career and skills of a modern software engineer.&lt;/p&gt;
    &lt;p&gt;Want to become a true senior engineer? Take ownership, have autonomy, and be a force multiplier on your team. The Senior Engineer Mindset ebook can help üëâ swizec.com/senior-mindset. These are the shifts in mindset that unlocked my career.&lt;/p&gt;
    &lt;p&gt;Curious about Serverless and the modern backend? Check out Serverless Handbook, for frontend engineers üëâ ServerlessHandbook.dev&lt;/p&gt;
    &lt;p&gt;Want to Stop copy pasting D3 examples and create data visualizations of your own? Learn how to build scalable dataviz React components your whole team can understand with React for Data Visualization&lt;/p&gt;
    &lt;p&gt;Want to get my best emails on JavaScript, React, Serverless, Fullstack Web, or Indie Hacking? Check out swizec.com/collections&lt;/p&gt;
    &lt;p&gt;Did someone amazing share this letter with you? Wonderful! You can sign up for my weekly letters for software engineers on their path to greatness, here: swizec.com/blog&lt;/p&gt;
    &lt;p&gt;Want to brush up on your modern JavaScript syntax? Check out my interactive cheatsheet: es6cheatsheet.com&lt;/p&gt;
    &lt;p&gt;By the way, just in case no one has told you it yet today: I love and appreciate you for who you are ‚ù§Ô∏è&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://swizec.com/blog/the-future-of-software-engineering-is-sre/"/><published>2026-01-25T22:18:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46760099</id><title>Scientists identify brain waves that define the limits of 'you'</title><updated>2026-01-26T15:18:47.719935+00:00</updated><content>&lt;doc fingerprint="b103d32cb0e38d8a"&gt;
  &lt;main&gt;
    &lt;p&gt;At what point do "you" end and the outside world begins?&lt;/p&gt;
    &lt;p&gt;It might feel like a weird question with an obvious answer, but your brain has to work surprisingly hard to judge that boundary. Now, scientists have linked a specific set of brain waves in a certain part of the brain to a sense of body ownership.&lt;/p&gt;
    &lt;p&gt;In a series of new experiments, researchers from Sweden and France put 106 participants through what's called the rubber hand illusion, monitoring and stimulating their brain activity to see what effect it had.&lt;/p&gt;
    &lt;p&gt;Related: Octopuses Fall For The Classic Fake Arm Trick ‚Äì Just Like We Do&lt;/p&gt;
    &lt;p&gt;This classic illusion involves hiding one of a participant's hands from their view and replacing it with a rubber one instead. When both their real and fake hands are repeatedly touched at the same time, it can evoke the eerie sensation that the rubber hand is part of the person's body.&lt;/p&gt;
    &lt;p&gt;The tests, which in one experiment involved EEG (electroencephalography) readings of brain activity, revealed that a sense of body ownership seems to arise from the frequency of alpha waves in the parietal cortex, a brain region responsible for mapping the body, processing sensory input and building a sense of self.&lt;/p&gt;
    &lt;p&gt;"We have identified a fundamental brain process that shapes our continuous experience of being embodied," says lead author Mariano D'Angelo, a neuroscientist at Karolinska Institute in Sweden.&lt;/p&gt;
    &lt;p&gt;"The findings may provide new insights into psychiatric conditions such as schizophrenia, where the sense of self is disturbed."&lt;/p&gt;
    &lt;p&gt;In the first batch of experiments, participants had a robotic arm tap the index finger of their real and fake hands, either at the exact same time or with a delay of up to 500 milliseconds between each tap.&lt;/p&gt;
    &lt;p&gt;As expected, participants reported feeling that the fake hand was part of their body more strongly if the taps were synchronized, and the feeling steadily weakened as the gap widened between what they felt and what they saw.&lt;/p&gt;
    &lt;p&gt;The EEG readings from the second experiment added more detail to the story. The frequency of alpha waves in the parietal cortex seemed to correlate with how well participants could detect the time delay between taps.&lt;/p&gt;
    &lt;p&gt;Those with faster alpha waves appeared to rule out fake hands even with a tiny gap in taps, while those with slower waves were more likely to feel the fake hand as their own, even if the taps were farther apart.&lt;/p&gt;
    &lt;p&gt;Finally, the researchers investigated whether the frequency of these brain waves actually controls the sensation of body ownership, or if they were perhaps both effects of some other factor.&lt;/p&gt;
    &lt;p&gt;With a third group of participants, they used a non-invasive technique called transcranial alternating current stimulation to speed up or slow down the frequency of a person's alpha waves. And sure enough, this seemed to correlate with how real a fake hand felt.&lt;/p&gt;
    &lt;p&gt;Speeding up someone's alpha waves gave them a tighter sense of body ownership, making them more sensitive to small timing discrepancies. Slowing down the waves had the opposite effect, making it harder for people to tell the difference between their own body and the outside world.&lt;/p&gt;
    &lt;p&gt;"Our findings help explain how the brain solves the challenge of integrating signals from the body to create a coherent sense of self," says Henrik Ehrsson, neuroscientist at Karolinska.&lt;/p&gt;
    &lt;p&gt;The researchers say that the findings could lead to new understanding of or treatments for conditions where the brain's body maps have gone askew, such as schizophrenia or the sensation of 'phantom limbs' experienced by amputees.&lt;/p&gt;
    &lt;p&gt;It could also help make for more realistic prosthetic limbs or even virtual reality tools.&lt;/p&gt;
    &lt;p&gt;The research was published in the journal Nature Communications.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sciencealert.com/scientists-identify-brain-waves-that-define-the-limits-of-you"/><published>2026-01-26T00:10:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46761761</id><title>Running the Stupid Cricut Software on Linux</title><updated>2026-01-26T15:18:47.350193+00:00</updated><content>&lt;doc fingerprint="171f99183bc7b5f9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Why The Hell Would You do This?&lt;/head&gt;
    &lt;p&gt;I‚Äôm more than happy building vector designs in Inkscape. It‚Äôs the most proficient vector designer app that is free, open source, and runs natively under Linux. However, simply having a quality SVG is only part of the process when it comes to using a plotter like the Cricut.&lt;/p&gt;
    &lt;p&gt;On my own, I would not recommend the Cricut brand, but this is a machine that was bought years ago, and I want to get the most usage out of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Install Cricut Design Space&lt;/head&gt;
    &lt;p&gt;As far as I can tell, almost everything works in the Cricut Design Space application under Linux. The only bug I‚Äôve noticed is the application becomes invisible when going full screen. This could easily be a Wayland bug too. I have no idea. Ideally, Cricut could Easily make a Linux Application with WINE. I could see this as a Flatpak install. I understand why they don‚Äôt, as that comes with the expectation of support, and supporting another operating system costs money.&lt;/p&gt;
    &lt;p&gt;This is a multistep process that you only really have to do once. Because of the complexity, I decided to lay out the instructions, along with the ‚Äòwhy‚Äô of the situation. The more you understand how it works, the more armed you are if anything goes wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the Software&lt;/head&gt;
    &lt;p&gt;I HIGHTLY recommend grabbing a fresh build of WINE. I‚Äôm a Debian user, so depending on the build of Linux you‚Äôre using it might vary, but I start on the Wine HQ Git. I know a lot of people might want to run a platform like Wine Bottles, but in my experience a more granular project like this is just easier under regular WINE.&lt;/p&gt;
    &lt;head rend="h3"&gt;OS Detection on the Website&lt;/head&gt;
    &lt;p&gt;When you visit the Design Space Download Page, it seems that the developer chose to use OS detection on the website. If it sees your user agent listed as Linux, for some weird reason it defaults to Mac. Logically, it would make more sense to default to the Windows build, but I think there was some corner cutting on this.&lt;/p&gt;
    &lt;p&gt;I recommend the open source UserAgent Switcher for this, as it has a build for Firefox and Chromium based browsers. (And of course, is free and open source.) After setting to ‚ÄúWindows 10‚Äù mode and refreshing the page:&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing the Software&lt;/head&gt;
    &lt;p&gt;As of this tutorial, the latest build of Design Space is &lt;code&gt;CricutDesignSpace-Install-v9.47.92.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Make sure you‚Äôve setup your install of WINE first. run &lt;code&gt;winecfg&lt;/code&gt; and make sure everything is working.&lt;/p&gt;
    &lt;code&gt;wine CricutDesignSpace-Install-v9.47.92.exe
&lt;/code&gt;
    &lt;p&gt;This will launch the installer. Just install like you normally would under Windows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Logging in, the Stupid Parts‚Ä¶&lt;/head&gt;
    &lt;p&gt;First we need to find the binary with the &lt;code&gt;where&lt;/code&gt; command. Here‚Äôs how it looked on my machine.&lt;/p&gt;
    &lt;code&gt;where cricut
cricut: aliased to wine '/home/art/.wine/drive_c/users/art/AppData/Local/Programs/Cricut Design Space/Cricut Design Space.exe'
&lt;/code&gt;
    &lt;p&gt;Then we need to open two terminals (or two sessions, I use tmux for that).&lt;/p&gt;
    &lt;p&gt;Terminal #1&lt;/p&gt;
    &lt;code&gt;wine Cricut\ Design\ Space.exe
&lt;/code&gt;
    &lt;p&gt;This will show the login panel and will want to launch your default browser. On my machine, that‚Äôs my native Firefox. Login there, and you‚Äôll see a url asking for the Cricut software. Because your native browser and the wine wrapped cricut software can‚Äôt see each other, you gotta extract the part of the URL that features &lt;code&gt;code=&lt;/code&gt;. In the second terminal:&lt;/p&gt;
    &lt;p&gt;Terminal #2&lt;/p&gt;
    &lt;code&gt;wine Cricut\ Design\ Space.exe "cricut://?code=XXXXXXXXXXXXXXXXXXX
&lt;/code&gt;
    &lt;p&gt;You should be logged in!&lt;/p&gt;
    &lt;p&gt;Now you can start uploading your designs and colaborating with other through their locked down, proprietary platform. You can even waste money on stock images.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arthur.pizza/2025/12/running-stupid-cricut-software-under-linux/"/><published>2026-01-26T04:05:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46762150</id><title>The browser is the sandbox</title><updated>2026-01-26T15:18:47.173738+00:00</updated><content>&lt;doc fingerprint="5daa04924fbda384"&gt;
  &lt;main&gt;
    &lt;p&gt;the browser is the sandbox. Paul Kinlan is a web platform developer advocate at Google and recently turned his attention to coding agents. He quickly identified the importance of a robust sandbox for agents to operate in and put together these detailed notes on how the web browser can help:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This got me thinking about the browser. Over the last 30 years, we have built a sandbox specifically designed to run incredibly hostile, untrusted code from anywhere on the web, the instant a user taps a URL. [...]&lt;/p&gt;
      &lt;p&gt;Could you build something like Cowork in the browser? Maybe. To find out, I built a demo called Co-do that tests this hypothesis. In this post I want to discuss the research I've done to see how far we can get, and determine if the browser's ability to run untrusted code is useful (and good enough) for enabling software to do more for us directly on our computer.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Paul then describes how the three key aspects of a sandbox - filesystem, network access and safe code execution - can be handled by browser technologies: the File System Access API (still Chrome-only as far as I can tell), CSP headers with &lt;code&gt;&amp;lt;iframe sandbox&amp;gt;&lt;/code&gt; and WebAssembly in Web Workers.&lt;/p&gt;
    &lt;p&gt;Co-do is a very interesting demo that illustrates all of these ideas in a single application:&lt;/p&gt;
    &lt;p&gt;You select a folder full of files and configure an LLM provider and set an API key, Co-do then uses CSP-approved API calls to interact with that provider and provides a chat interface with tools for interacting with those files. It does indeed feel similar to Claude Cowork but without running a multi-GB local container to provide the sandbox.&lt;/p&gt;
    &lt;p&gt;My biggest complaint about &lt;code&gt;&amp;lt;iframe sandbox&amp;gt;&lt;/code&gt; remains how thinly documented it is, especially across different browsers. Paul's post has all sorts of useful details on that which I've not encountered elsewhere, including a complex double-iframe technique to help apply network rules to the inner of the two frames.&lt;/p&gt;
    &lt;p&gt;Thanks to this post I also learned about the &lt;code&gt;&amp;lt;input type="file" webkitdirectory&amp;gt;&lt;/code&gt; tag which turns out to work on Firefox, Safari and Chrome and allows a browser read-only access to a full directory of files at once. I had Claude knock up a webkitdirectory demo to try it out and I'll certainly be using it for projects in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wilson Lin on FastRender: a browser built by thousands of parallel agents - 23rd January 2026&lt;/item&gt;
      &lt;item&gt;First impressions of Claude Cowork, Anthropic's general agent - 12th January 2026&lt;/item&gt;
      &lt;item&gt;My answers to the questions I posed about porting open source code with LLMs - 11th January 2026&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/"/><published>2026-01-26T05:23:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46762882</id><title>The Holy Grail of Linux Binary Compatibility: Musl and Dlopen</title><updated>2026-01-26T15:18:46.582738+00:00</updated><content>&lt;doc fingerprint="adda3afb1a14b6a8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Holy Grail of Linux Binary Compatibility: musl + dlopen #242&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I guess using Go + Godot to build native &amp;amp; installable Android &amp;amp; iOS binaries (without any proprietary SDKs) was too easy. So it's time for a real challenge...&lt;/p&gt;
          &lt;head&gt;Linux Binary Compatibility&lt;/head&gt;
          &lt;p&gt;(some background reading: https://jangafx.com/insights/linux-binary-compatibility)&lt;/p&gt;
          &lt;p&gt;For a while now, it's been very easy to reliably ship command line software &amp;amp; servers for Linux, just run &lt;/p&gt;
          &lt;p&gt;The problems begin to creep in when you want access to hardware accelerated graphics. All the GPU drivers on Linux require accessing dynamic libraries via the C ABI. These C libraries are built against a particular libc, which is most commonly &lt;/p&gt;
          &lt;p&gt;In fact, I've directly experienced this, as I recently replaced the OS on my personal computer with the &lt;/p&gt;
          &lt;p&gt;That's a problem, firstly because this is my distro now, I need to be able to build graphics.gd projects! Secondly, in theory, &lt;/p&gt;
          &lt;head&gt;Supporting &lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 2 comments&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I use the dlopen/dlsym technique in Linux and loadlibrary/getprocaddress in my language's VM.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Doesn't work for me:&lt;/p&gt;
          &lt;code&gt;Thread 1 "dodge_the_creep" received signal SIGSEGV, Segmentation fault.
0x0000000002b13d16 in foreign_tramp ()
(gdb) bt
#0  0x0000000002b13d16 in foreign_tramp ()
#1  0x00007fffaa55a5a0 in ?? ()
#2  0x00007ffff75e4558 in ?? ()
#3  0x00007fff5ffec436 in ?? ()
#4  0x00007fffaa2ed710 in ?? ()
#5  0x00007fffffffc0d0 in ?? ()
#6  0x00007fff5ffe5ecc in ?? ()
#7  0x00007fffffffc0d0 in ?? ()
#8  0x00007fff5f9cc488 in ?? ()
#9  0x00007fffaa559a40 in ?? ()
#10 0x0000000000000000 in ?? ()
(gdb)&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/quaadgras/graphics.gd/discussions/242"/><published>2026-01-26T07:41:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46763864</id><title>MapLibre Tile: a modern and efficient vector tile format</title><updated>2026-01-26T15:18:46.510055+00:00</updated><content>&lt;doc fingerprint="4787f953dcbb25f4"&gt;
  &lt;main&gt;
    &lt;p&gt;Jan 23, 2026&lt;/p&gt;
    &lt;p&gt;Today we are happy to announce MapLibre Tile (MLT), a new modern and efficient vector tile format.&lt;/p&gt;
    &lt;p&gt;MapLibre Tile (MLT) is a succesor to Mapbox Vector Tile (MVT). It has been redesigned from the ground up to address the challenges of rapidly growing geospatial data volumes and complex next-generation geospatial source formats, as well as to leverage the capabilities of modern hardware and APIs.&lt;/p&gt;
    &lt;p&gt;MLT is specifically designed for modern and next-generation graphics APIs to enable high-performance processing and rendering of large (planet-scale) 2D and 2.5 basemaps. This current implementation offers feature parity with MVT1 while delivering on the following:&lt;/p&gt;
    &lt;p&gt;In addition, MLT was designed to support the following use cases in the future:&lt;/p&gt;
    &lt;p&gt;As with any MapLibre project, the future of MLT is decided by the needs of the community. There are a lot of exciting ideas for other future extensions and we welcome contributions to the project.&lt;/p&gt;
    &lt;p&gt;For a more in-depth exploration of MLT have a look at the following slides, watch this talk or read this publication by MLT inventor Markus Tremmel.&lt;/p&gt;
    &lt;p&gt;For the adventurous, the answer is: today. Both MapLibre GL JS and MapLibre Native now support MLT sources. You can use the new &lt;code&gt;encoding&lt;/code&gt; property on sources in your style JSON with a value of &lt;code&gt;mlt&lt;/code&gt; for MLT vector tile sources.&lt;/p&gt;
    &lt;p&gt;To try out MLT, you have the following options:&lt;/p&gt;
    &lt;p&gt;Refer to this page for a complete and up-to-date list of integrations and implementations. If you are an integrator working on supporting MLT, feel free to add your own project there.&lt;/p&gt;
    &lt;p&gt;We would love to hear your experience with using MLT! Join the &lt;code&gt;#maplibre-tile-format&lt;/code&gt; channel on our Slack or create an Issue or Discussion on the tile spec repo.&lt;/p&gt;
    &lt;p&gt;MapLibre Tile came to be thanks to a multi-year collaboration between academia, open source and enterprise. Thank you to everyone who was involved! We are very proud that our community can innovate like this.&lt;/p&gt;
    &lt;p&gt;Special thanks go to Markus Tremmel for inventing the format, Yuri Astrakhan for spearheading the project, Tim Sylvester for the C++ implementation, Harel Mazor, Benedikt Vogl and Niklas Greindl for working on the JavaScript implementation.&lt;/p&gt;
    &lt;p&gt;Also thanks to Microsoft and AWS for financing work on MLT.&lt;/p&gt;
    &lt;p&gt;One exception: unlike MVT, MLT does not support layers where a value in a column changes type from feature to feature. ‚Ü©&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maplibre.org/news/2026-01-23-mlt-release/"/><published>2026-01-26T10:19:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46764170</id><title>Show HN: Only 1 LLM can fly a drone</title><updated>2026-01-26T15:18:46.017185+00:00</updated><content>&lt;doc fingerprint="4fd783f63903440d"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Inspired by Pok√©mon Snap (1999). VLM pilots a drone through 3D world to locate and identify creatures.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;%%{init: {'theme': 'base', 'themeVariables': { 'background': '#ffffff', 'primaryColor': '#ffffff'}}}%%
flowchart LR
    subgraph Controller["**Controller** (Rust)"]
        C[Orchestration]
    end

    subgraph VLM["**VLM** (OpenRouter)"]
        V[Vision-Language Model]
    end

    subgraph Simulation["**Simulation** (Zig/raylib)"]
        S[Game State]
    end

    C --&amp;gt;|"screenshot + prompt"| V
    C &amp;lt;--&amp;gt;|"cmds + state&amp;lt;br&amp;gt;**UDP:9999**"| S

    style Controller fill:#8B5A2B,stroke:#5C3A1A,color:#fff
    style VLM fill:#87CEEB,stroke:#5BA3C6,color:#1a1a1a
    style Simulation fill:#4A7C23,stroke:#2D5A10,color:#fff
    style C fill:#B8864A,stroke:#8B5A2B,color:#fff
    style V fill:#B5E0F7,stroke:#87CEEB,color:#1a1a1a
    style S fill:#6BA33A,stroke:#4A7C23,color:#fff
&lt;/code&gt;
    &lt;p&gt;The simulation generates procedural terrain and spawns creatures (cat, dog, pig, sheep) for the drone to discover. It handles drone physics and collision detection, accepting 8 movement commands plus &lt;code&gt;identify&lt;/code&gt; and &lt;code&gt;screenshot&lt;/code&gt;. The Rust controller captures frames from the simulation, constructs prompts enriched with position and state data, then parses VLM responses into executable command sequences. The objective: locate and successfully identify 3 creatures, where &lt;code&gt;identify&lt;/code&gt; succeeds when the drone is within 5 units of a target.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;demo_3x.mov&lt;/head&gt;
    &lt;p&gt;I gave 7 frontier LLMs a simple task: pilot a drone through a 3D voxel world and find 3 creatures.&lt;/p&gt;
    &lt;p&gt;Only one could do it.&lt;/p&gt;
    &lt;p&gt;Is this a rigorous benchmark? No. However, it's a reasonably fair comparison - same prompt, same seeds, same iteration limits. I'm sure with enough refinement you could coax better results out of each model. But that's kind of the point: out of the box, with zero hand-holding, only one model figured out how to actually fly.&lt;/p&gt;
    &lt;p&gt;The core differentiator wasn't intelligence - it was altitude control. Creatures sit on the ground. To identify them, you need to descend.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemini Flash: Actively adjusts altitude, descends to creature level, identifies&lt;/item&gt;
      &lt;item&gt;GPT-5.2-chat: Gets close horizontally but never lowers&lt;/item&gt;
      &lt;item&gt;Claude Opus: Attempts identification 160+ times, never succeeds - approaching at wrong angles&lt;/item&gt;
      &lt;item&gt;Others: Wander randomly or get stuck&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This left me puzzled. Claude Opus is arguably the most capable model in the lineup. It knows it needs to identify creatures. It tries - aggressively. But it never adjusts its approach angle.&lt;/p&gt;
    &lt;p&gt;Run 13 (seed 72) was the only run where any model found 2 creatures. Why? They happened to spawn near each other. Gemini Flash found one, turned around, and spotted the second.&lt;/p&gt;
    &lt;p&gt;In most other runs, Flash found one creature quickly but ran out of iterations searching for the others. The world is big. 50 iterations isn't a lot of time.&lt;/p&gt;
    &lt;p&gt;This was the most surprising finding. I expected:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Opus 4.5 (most expensive) to dominate&lt;/item&gt;
      &lt;item&gt;Gemini 3 Pro to outperform Gemini 3 Flash (same family, more capability)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead, the cheapest model beat models costing 10x more.&lt;/p&gt;
    &lt;p&gt;What's going on here? A few theories:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spatial reasoning doesn't scale with model size - at least not yet&lt;/item&gt;
      &lt;item&gt;Flash was trained differently - maybe more robotics data, more embodied scenarios?&lt;/item&gt;
      &lt;item&gt;Smaller models follow instructions more literally - "go down" means go down, not "consider the optimal trajectory"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I genuinely don't know. But if you're building an LLM-powered agent that needs to navigate physical or virtual space, the most expensive model might not be your best choice.&lt;/p&gt;
    &lt;p&gt;Anecdotally, creatures with higher contrast (gray sheep, pink pigs) seemed easier to spot than brown-ish creatures that blended into the terrain. A future version might normalize creature visibility. Or maybe that's the point - real-world object detection isn't normalized either.&lt;/p&gt;
    &lt;p&gt;Before this, I tried having LLMs pilot a real DJI Tello drone.&lt;/p&gt;
    &lt;p&gt;Results: it flew straight up, hit the ceiling, and did donuts until I caught it. (I was using Haiku 4.5, which in hindsight explains a lot.)&lt;/p&gt;
    &lt;p&gt;The Tello is now broken. I've ordered a BetaFPV and might get another Tello since they're so easy to program. Now that I know Gemini Flash can actually navigate, a real-world follow-up might be worth revisiting.&lt;/p&gt;
    &lt;p&gt;This is half-serious research, half "let's see what happens."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The simulation has rough edges (it's a side project, not a polished benchmark suite)&lt;/item&gt;
      &lt;item&gt;One blanket prompt is used for all models - model-specific tuning would likely improve results&lt;/item&gt;
      &lt;item&gt;The feedback loop is basic (position, screenshot, recent commands) - there's room to get creative with what information gets passed back&lt;/item&gt;
      &lt;item&gt;Iteration limits (50) may artificially cap models that are slower but would eventually succeed&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;Install&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Zig&lt;/cell&gt;
        &lt;cell&gt;‚â•0.15.2&lt;/cell&gt;
        &lt;cell&gt;ziglang.org/download&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;stable (2024 edition)&lt;/cell&gt;
        &lt;cell&gt;rust-lang.org/tools/install&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;‚â•3.11&lt;/cell&gt;
        &lt;cell&gt;python.org&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;uv&lt;/cell&gt;
        &lt;cell&gt;latest&lt;/cell&gt;
        &lt;cell&gt;docs.astral.sh/uv&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You'll also need an OpenRouter API key.&lt;/p&gt;
    &lt;code&gt;gh repo clone kxzk/snapbench
cd snapbench

# set your API key
export OPENROUTER_API_KEY="sk-or-..."&lt;/code&gt;
    &lt;code&gt;# terminal 1: start the simulation (with optional seed)
zig build run -Doptimize=ReleaseFast -- 42
# or
make sim

# terminal 2: start the drone controller
cargo run --release --manifest-path llm_drone/Cargo.toml -- --model google/gemini-3-flash-preview
# or
make drone&lt;/code&gt;
    &lt;code&gt;# runs all models defined in bench/models.toml
uv run bench/bench_runner.py
# or
make bench&lt;/code&gt;
    &lt;p&gt;Results get saved to &lt;code&gt;data/run_&amp;lt;id&amp;gt;.csv&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model-specific prompts: Tune instructions to each model's strengths&lt;/item&gt;
      &lt;item&gt;Richer feedback: Pass more spatial context (distance readings, compass, minimap?)&lt;/item&gt;
      &lt;item&gt;Multi-agent runs: What if you gave each model a drone and made them compete?&lt;/item&gt;
      &lt;item&gt;Extended iterations: Let slow models run longer to isolate reasoning from speed&lt;/item&gt;
      &lt;item&gt;Real drone benchmark: Gemini Flash vs. the BetaFPV&lt;/item&gt;
      &lt;item&gt;Pok√©mon assets: Found low-poly Pok√©mon models on Poly Pizza‚Äîleaning into the Pok√©mon Snap inspiration&lt;/item&gt;
      &lt;item&gt;World improvements: Larger terrain, better visuals, performance optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Drone by NateGazzard CC-BY via Poly Pizza&lt;/item&gt;
      &lt;item&gt;Cube World Kit by Quaternius via Poly Pizza&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Donated to Poly Pizza to support the platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/kxzk/snapbench"/><published>2026-01-26T11:00:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46764223</id><title>TSMC Risk</title><updated>2026-01-26T15:18:45.744665+00:00</updated><content>&lt;doc fingerprint="6380d4514daed7ef"&gt;
  &lt;main&gt;
    &lt;p&gt;Listen to this post:&lt;/p&gt;
    &lt;p&gt;You probably think, given this title, you know what this Article is about. The most advanced semiconductors are made by TSMC in Taiwan,1 and Taiwan is claimed by China, which has not and will not take reunification-by-force off of the table.&lt;/p&gt;
    &lt;p&gt;Relatedly, AI obviously has significant national security implications; at Davos, Anthropic CEO Dario Amodei reiterated his objection to the U.S. allowing the sale of Nvidia chips to China. From Bloomberg:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Anthropic Chief Executive Officer Dario Amodei said selling advanced artificial intelligence chips to China is a blunder with ‚Äúincredible national security implications‚Äù as the US moves to allow Nvidia Corp. to sell its H200 processors to Beijing. ‚ÄúIt would be a big mistake to ship these chips,‚Äù Amodei said in an interview with Bloomberg Editor-in-Chief John Micklethwait at the World Economic Forum in Davos, Switzerland. ‚ÄúI think this is crazy. It‚Äôs a bit like selling nuclear weapons to North Korea.‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The nuclear weapon analogy is an interesting one: a lot of game theory was developed to manage the risk of nuclear weapons, particularly once the U.S.S.R. gained/stole nuclear capability, ending the U.S.‚Äôs brief monopoly on the technology. Before that happened, however, the U.S. had a dominant military position, given we had nuclear weapons and no one else did. Perhaps Amodei believes the U.S. should have advanced AI and China should not, giving us a dominant military position?&lt;/p&gt;
    &lt;p&gt;The problem with that reality, however, is Taiwan, as I explained in AI Promise and Chip Precariousness. AI, in contrast to nuclear weapons, has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion; if we got to a situation where only the U.S. had the sort of AI that would give us an unassailable advantage militarily, then the optimal strategy for China would change to taking TSMC off of the board.&lt;/p&gt;
    &lt;p&gt;Given this dependency, my recommendations in the Article run counter to Amodei: I want China dependent on not just U.S. chips but also on TSMC directly, which is why I argued in favor of selling Nvidia chips to China, and further believe that Huawei and other Chinese companies ought to be able to source from TSMC (on the flip side, I would ban the sale of semiconductor manufacturing equipment to Chinese fabs). I think it‚Äôs a good thing the Trump administration moved on the first point, at least.&lt;/p&gt;
    &lt;p&gt;However, this risk is not what this Article is about: there is another TSMC risk facing the entire AI industry in particular; moreover, it‚Äôs a risk the downside of which is already being realized.&lt;/p&gt;
    &lt;head rend="h3"&gt;The TSMC Brake&lt;/head&gt;
    &lt;p&gt;There was one refrain that was common across Big Tech earnings last quarter: demand for AI exceeds supply. Here was Amazon CEO Andy Jassy on the company‚Äôs earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You‚Äôre going to see us continue to be very aggressive investing in capacity because we see the demand. As fast as we‚Äôre adding capacity right now, we‚Äôre monetizing it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here was Microsoft CFO Amy Hood on the company‚Äôs earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Azure AI services revenue was generally in line with expectations, and this quarter, demand again exceeded supply across workloads, even as we brought more capacity online.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here was Google CFO Anat Ashkenazi on the company‚Äôs earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In GCP, we see strong demand for enterprise AI infrastructure, including TPUs and GPUs, enterprise AI solutions driven by demand for Gemini 2.5 and our other AI models, and core GCP infrastructure and other services such as cybersecurity and data analytics. As I‚Äôve mentioned on previous earnings calls, while we have been working hard to increase capacity and have improved the pace of server deployments and data center construction, we still expect to remain in a tight demand-supply environment in Q4 and 2026.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here was Meta CEO Mark Zuckerberg on the company‚Äôs earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To date, we keep on seeing this pattern where we build some amount of infrastructure to what we think is an aggressive assumption. And then we keep on having more demand to be able to use more compute, especially in the core business in ways that we think would be quite profitable than we end up having compute for.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Earlier this month, TSMC CEO C.C. Wei admitted that the shortage was a lack of chips, not power; from the company‚Äôs earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Talking about to build a lot of AI data center all over the world, I use one of my customers‚Äô customers‚Äô answer. I asked the same question. They told me that they planned this one, 5-6 years ago already. So, as I said, those cloud service providers are smart, very smart. So, they say that they work on the power supply 5-6 years ago. So, today, their message to me is: silicon from TSMC is a bottleneck, and asked me not to pay attention to all others, because they have to solve the silicon bottleneck first. But indeed, we do get the power supply, all over the world, especially in the US. Not only that, but we also look at, who support those kind of a power supply, like a turbine, like, what, nuclear power plant, the plan or those kinds of things. We also look at the supply of the rack. We also look at the supply of the cooling system. Everything, so far, so good. So we have to work hard to narrow the gap between the demand and supply from TSMC.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The cause of that gap is obvious if you look at TSMC‚Äôs financials, specifically the company‚Äôs annual capital expenditures:&lt;/p&gt;
    &lt;p&gt;After a big increase in CapEx in 2021, driven by the COVID shortages and a belief in 5G, TSMC‚Äôs annual CapEx in the following years was basically flat ‚Äî it actually declined on a year-over-year basis in both 2023 and 2024. Note those dates! ChatGPT was released in November 2022; that kicked off a massive increase in CapEx amongst the hyperscalers in particular, but it sure seems like TSMC didn‚Äôt buy the hype.&lt;/p&gt;
    &lt;p&gt;That lack of increased investment earlier this decade is why there is a shortage today, and is why TSMC has been a de facto brake on the AI buildout/bubble; I wrote last quarter:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To put it another way, if Altman and OpenAI are the ones pushing to accelerate the AI infrastructure buildout, it‚Äôs Wei and TSMC that are the brakes. The extent to which all of Altman‚Äôs deals actually materialize is dependent on how much TSMC invests in capacity now, and while they haven‚Äôt shown their hand yet, the company is saying all of the right things about AI being a huge trend without having yet committed to a commensurate level of investment, at least relative to OpenAI‚Äôs goals.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That Update was about the future, but it‚Äôs important to note that the TSMC brake has ‚Äî if all of those CEO and CFO comments above are to be believed ‚Äî already cost the biggest tech companies a lot of money. That‚Äôs the implication of not having enough supply to satisfy demand: there was revenue to be made that wasn‚Äôt, because TSMC didn‚Äôt buy the AI hype at the same time everyone else did.&lt;/p&gt;
    &lt;head rend="h3"&gt;TSMC‚Äôs CapEx Plans&lt;/head&gt;
    &lt;p&gt;TSMC is, finally, starting to invest more. Last year‚Äôs CapEx increased 37% to $41 billion, and there‚Äôs another increase in store for this year to $52‚Äì$56 billion; if we take the midpoint, that represents an increase of 32%, a bit less than last year:&lt;/p&gt;
    &lt;p&gt;Make no mistake, $54 billion is a big number, one that Wei admitted made him nervous:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You essentially try to ask whether the AI demand is real or not. I‚Äôm also very nervous about it. Yeah, you bet, because we have to invest about USD52 billion to USD56 billion for the CapEx, right? If we did not do it carefully, that will be a big disaster to TSMC for sure. So, of course, I spent a lot of time in the last three-four months talking to my customers and then customers‚Äô customers. I want to make sure that my customers‚Äô demands are real.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Wei made clear that he was worried about the market several years down the line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you build a new fab, it takes two and three year, two to three years to build a new fab. So even we start to spend $52 billion to $56 billion, the contribution to this year is almost none, and 2027, a little bit. So we actually, we are looking for 2028-2029 supply, and we hope it‚Äôs a time that the gap will be narrow‚Ä¶So 2026-2027 for the short-term, we are looking to improve our productivity. 2028 to 2029, yes, we start to increase our capacity significantly. And it will continue this way if the AI demand megatrend as we expected.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;First off, this delayed impact explains why TSMC‚Äôs lack of CapEx increase a few years ago is resulting in supply-demand imbalance today. Secondly, notice how this year‚Äôs planned increase ‚Äî which again, won‚Äôt really have an impact until 2028 ‚Äî pales in comparison to the CapEx growth of the hyperscalers (2025 numbers are estimates; note that Amazon‚Äôs CapEx includes Amazon.com):&lt;/p&gt;
    &lt;p&gt;Remember, a significant portion of this CapEx growth is for chips that are supported by TSMC‚Äôs stagnant CapEx growth from a few years ago. It‚Äôs notable, then, that TSMC‚Äôs current and projected CapEx growth is still less than the hyperscalers: how much less is it going to be than the hyperscalers‚Äô growth in 2028, when the fabs being built today start actually producing chips?&lt;/p&gt;
    &lt;p&gt;In short, the TSMC brake isn‚Äôt going anywhere ‚Äî if anything, it‚Äôs being pressed harder than ever.&lt;/p&gt;
    &lt;head rend="h3"&gt;TSMC Risk&lt;/head&gt;
    &lt;p&gt;TSMC is, to be clear, being extremely rational. CapEx is inherently risky: you are spending money now in anticipation of demand that may or may not materialize. Moreover, the risk for a foundry is higher than basically any other business model: nearly all of a foundry‚Äôs costs are CapEx, which means that if demand fails to materialize, costs ‚Äî in the form of depreciation ‚Äî don‚Äôt go down as they might with a business model with a higher percentage of marginal costs. This is exacerbated by the huge dollar figures entailed in building fabs: $52‚Äì$56 billion may drive revenues with big margins, but those big margins can easily flip to being huge losses and years of diminished pricing power thanks to excess capacity. Therefore, it‚Äôs understandable that TSMC is trying to manage its risks. Sure, the company may be foregoing some upside in 2028, but what is top of Wei‚Äôs mind is avoiding ‚Äúa big disaster.‚Äù&lt;/p&gt;
    &lt;p&gt;What is important to note, however, is that the risk TSMC is managing doesn‚Äôt simply go away: rather, it‚Äôs being offloaded to the hyperscalers in particular. Specifically, if we get to 2028, and TSMC still isn‚Äôt producing enough chips to satisfy demand, then that means the hyperscalers will be forgoing billions of dollars in revenue ‚Äî even more than they are already forgoing today. Yes, that risk is harder to see than the risk TSMC is avoiding, because the hyperscalers aren‚Äôt going to be bankrupt for a lack of chips to satisfy demand. Still, the potential money not made ‚Äî particularly when the number is potentially in the hundreds of billions of dollars ‚Äî is very much a risk that the hyperscalers are incurring because of TSMC‚Äôs conservatism.&lt;/p&gt;
    &lt;p&gt;What the hyperscalers need to understand is that simply begging TSMC to make more isn‚Äôt going to fix this problem, because begging TSMC to make more is to basically ask TSMC to take back the risk TSMC is offloading to the hyperscalers ‚Äî they already declined! Rather, the only thing that will truly motivate TSMC to take on more risk is competition. If TSMC were worried about not just forgoing its own extra revenue, but actually losing business to a competitor, then the company would invest more. Moreover, that extra investment would be stacked on top of the investment made by said competitor, which means the world would suddenly have dramatically more fab capacity.&lt;/p&gt;
    &lt;head rend="h3"&gt;If You Want a Bubble&lt;/head&gt;
    &lt;p&gt;In short, the only way to truly get an AI bubble, with all of the potential benefits that entails, or, in the optimistic case, to actually meet demand in 2028 and beyond, is to have competition in the foundry space. That, by extension, means Samsung or Intel ‚Äî or both ‚Äî actually being viable options.&lt;/p&gt;
    &lt;p&gt;Remember, however, the number one challenge facing those foundries: a lack of demand from the exact companies whom TSMC has deputized to take on their risk. I wrote in U.S. Intel:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our mythical startup, however, doesn‚Äôt exist in a vacuum: it exists in the same world as TSMC, the company who has defined the modern pure play foundry. TSMC has put in the years, and they‚Äôve put in the money; TSMC has the unparalleled customer service approach that created the entire fabless chip industry; and, critically, TSMC, just as they did in the mobile era, is aggressively investing to meet the AI moment. If you‚Äôre an Nvidia, or an Apple in smartphones, or an AMD or a Qualcomm, why would you take the chance of fabricating your chips anywhere else? Sure, TSMC is raising prices in the face of massive demand, but the overall cost of a chip in a system is still quite small; is it worth risking your entire business to save a few dollars for worse performance with a worse customer experience that costs you time to market and potentially catastrophic product failures?&lt;/p&gt;
      &lt;p&gt;We know our mythical startup would face these challenges because they are the exact challenges Intel faces. Intel may need ‚Äúa meaningful external customer to drive acceptable returns on [its] deployed capital‚Äù, but Intel‚Äôs needs do not drive the decision-making of those external customers, despite the fact that Intel, while not fully caught up to TSMC, is at least in the ballpark, something no startup could hope to achieve for decades.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Becoming a meaningful customer of Samsung or Intel is very risky: it takes years to get a chip working on a new process, which hardly seems worth it if that process might not be as good, and if the company offering the process definitely isn‚Äôt as customer service-centric as TSMC. I understand why everyone sticks with TSMC.&lt;/p&gt;
    &lt;p&gt;The reality that hyperscalers and fabless chip companies need to wake up to, however, is that avoiding the risk of working with someone other than TSMC incurs new risks that are both harder to see and also much more substantial. Except again, we can see the harms already: foregone revenue today as demand outstrips supply. Today‚Äôs shortages, however, may prove to be peanuts: if AI has the potential these companies claim it does, future foregone revenue at the end of the decade is going to cost exponentially more ‚Äî surely a lot more than whatever expense is necessary to make Samsung and/or Intel into viable competitors for TSMC.&lt;/p&gt;
    &lt;p&gt;This, incidentally, is how the geographic risk issue will be fixed, if it ever is. It‚Äôs hard to get companies to pay for insurance for geopolitical risks that may never materialize. What is much more likely is that TSMC‚Äôs customers realize that their biggest risk isn‚Äôt that TSMC gets blown up by China, but that TSMC‚Äôs monopoly and reasonable reluctance to risk a rate of investment that matches the rest of the industry means that the rest of the industry fails to fully capture the value of AI.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, there are chips made in Arizona, but only a portion, and they need to be sent back to Taiwan for packaging and testing. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stratechery.com/2026/tsmc-risk/"/><published>2026-01-26T11:07:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46765092</id><title>Water 'Bankruptcy' Era Has Begun for Billions, Scientists Say</title><updated>2026-01-26T15:18:45.561173+00:00</updated><content/><link href="https://www.bloomberg.com/news/articles/2026-01-20/water-bankruptcy-era-has-begun-for-billions-scientists-say"/><published>2026-01-26T12:57:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46765120</id><title>Vibe coding kills open source</title><updated>2026-01-26T15:18:45.468211+00:00</updated><content>&lt;doc fingerprint="a49e240a44ff128c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Economics &amp;gt; General Economics&lt;/head&gt;&lt;p&gt; [Submitted on 21 Jan 2026]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Vibe Coding Kills Open Source&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Generative AI is changing how software is produced and used. In vibe coding, an AI agent builds software by selecting and assembling open-source software (OSS), often without users directly reading documentation, reporting bugs, or otherwise engaging with maintainers. We study the equilibrium effects of vibe coding on the OSS ecosystem. We develop a model with endogenous entry and heterogeneous project quality in which OSS is a scalable input into producing more software. Users choose whether to use OSS directly or through vibe coding. Vibe coding raises productivity by lowering the cost of using and building on existing code, but it also weakens the user engagement through which many maintainers earn returns. When OSS is monetized only through direct user engagement, greater adoption of vibe coding lowers entry and sharing, reduces the availability and quality of OSS, and reduces welfare despite higher productivity. Sustaining OSS at its current scale under widespread vibe coding requires major changes in how maintainers are paid.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;econ.GN&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2601.15494"/><published>2026-01-26T13:01:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46765273</id><title>Transfering Files with gRPC</title><updated>2026-01-26T15:18:45.220596+00:00</updated><content>&lt;doc fingerprint="a5bbb94665a38ca0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Transfering files with gRPC&lt;/head&gt;
    &lt;p&gt;Is transfering files with gRPC a good idea? Or should that be handled by a separate REST API endpoint? In this post, we will implement a file transfer service in both, use Kreya to test those APIs, and finally compare the performance to see which one is better.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges when doing file transfers&lt;/head&gt;
    &lt;p&gt;When handling large files, it is important to stream the file from one place to another. This might sound obvious, but many developers (accidentally) buffer the whole file in memory, potentially leading to out-of-memory errors. For a web server that provides files for download, a correct implementation would stream the files directly from the file system into the HTTP response.&lt;/p&gt;
    &lt;p&gt;Another problem with very large files are network failures. Imagine you are downloading a 10 GB file on a slow connection, but your connection is interrupted for a second after downloading 90% of it. With REST, this could be solved by sending a HTTP &lt;code&gt;Range&lt;/code&gt; header, requesting the rest of the file content without download the first 9 GB again.
For the simplicity of the blogpost and since something similar is possible with gRPC, we are going to ignore this problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transfering files with REST&lt;/head&gt;
    &lt;p&gt;Handling file transfers with REST (more correctly plain HTTP) is pretty straight forward in most languages and frameworks. In C# or rather ASP.NET Core, an example endpoint offering a file for downloading could look like this:&lt;/p&gt;
    &lt;code&gt;[HttpGet("api/files/pdf")]&lt;/code&gt;
    &lt;p&gt;We are effectively telling the framework to stream the file &lt;code&gt;/files/test-file.pdf&lt;/code&gt; as the response.
Internally, the framework repeatedly reads a small chunk (usually a few KB) from the file and writes it to the response.&lt;/p&gt;
    &lt;p&gt;The whole response body will consist of the file content and Kreya, our API client, automatically renders it as a PDF. Other information about the file, such as content type or file name, will have to be sent via HTTP headers.&lt;/p&gt;
    &lt;p&gt;This is important. If you have a JSON REST API and try to send additional information in the response body like this:&lt;/p&gt;
    &lt;code&gt;{&lt;/code&gt;
    &lt;p&gt;This is bad! The whole file content will be Base64-encoded and takes up 30% more space than the file size itself. In most languages/frameworks (without additional workarounds), this would also buffer the whole file in memory since the Base64-encoding process is usually not streamed while creating the JSON response. If the file itself is also buffered in memory, you could see memory usage over twice the size of the file. This may be fine if your files are only a few KB in size. But even then, if multiple requests are concurrently hitting this endpoint, you may notice quite a lot of memory usage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transfering files with gRPC&lt;/head&gt;
    &lt;p&gt;While the REST implementation was straight forward, this is not the case with gRPC. The design of gRPC is based on protobuf messages. There is no concept of "streaming" the content of a message. Instead, gRPC is designed to buffer a message fully in memory. This is the reason why individual gRPC messages should be kept small. The default maximum size is set at 4 MB. So how do we send large files bigger than that?&lt;/p&gt;
    &lt;p&gt;While gRPC cannot stream the content of a message, it allows streaming multiple messages. The solution is to break up the file into small chunks (usually around 32 KB) and then send these chunks until the file is transferred completely. The protobuf definition for a file download service could look like this:&lt;/p&gt;
    &lt;code&gt;edition = "2023";&lt;/code&gt;
    &lt;p&gt;This defines the &lt;code&gt;FileService.DownloadFile&lt;/code&gt; server-streaming method, which means that the method accepts a single (empty) request and returns multiple responses.
While we could send the file metadata via gRPC metadata (=HTTP headers or trailers), I think it's nicer to define it explicitly via a message.
The server should send the metadata first, as it contains important information, such as the file size.&lt;/p&gt;
    &lt;p&gt;A naive server implementation in C# could look like this:&lt;/p&gt;
    &lt;code&gt;private const int ChunkSize = 32 * 1024;&lt;/code&gt;
    &lt;p&gt;This works, but has some issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A new byte array buffer is created for each request&lt;/item&gt;
      &lt;item&gt;The buffer is copied each time to create a &lt;code&gt;ByteString&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first point is easily solved by using a buffer from the shared pool, potentially re-using the same buffer for subsequent requests.&lt;/p&gt;
    &lt;p&gt;The second point happens due to the gRPC implementation in practically all languages. Since the implementation wants to guarantee that the bytes are not modified while sending them, it performs a copy first. This is a design decision which favors stability over performance. Luckily, there is a workaround by using a "unsafe" method, which is perfectly safe in our scenario and improves performance:&lt;/p&gt;
    &lt;code&gt;private const int ChunkSize = 32 * 1024;&lt;/code&gt;
    &lt;p&gt;Let's try this out. After importing the protobuf definition, we call the gRPC method with Kreya:&lt;/p&gt;
    &lt;p&gt;This works, but where is our PDF? Since we are sending individual chunks, we need to put them back together manually.&lt;/p&gt;
    &lt;p&gt;To achieve this, we simply need to append each chunk to a file. In Kreya, this is done via Scripting:&lt;/p&gt;
    &lt;code&gt;import { writeFile, appendFile } from 'fs/promises';&lt;/code&gt;
    &lt;p&gt;This allows us to view the PDF:&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparison&lt;/head&gt;
    &lt;p&gt;Great! So transfering files with gRPC is definitely possible. But how do these two technologies compare against each other? Which one is faster and has less overhead?&lt;/p&gt;
    &lt;head rend="h3"&gt;Total bytes transferred&lt;/head&gt;
    &lt;p&gt;The total amount of bytes transferred on the wire is actually a pretty difficult topic. It depends on a lot of factors, such as the HTTP protocol (HTTP/1.1, HTTP/2 or HTTP/3), the package size of TCP/IP, whether TLS is being used etc. We are going to take a look how this applies to REST and gRPC.&lt;/p&gt;
    &lt;p&gt;Streaming files over a gRPC connection generates overhead, although not much. Since gRPC uses HTTP/2 under the hood, each individual chunk message has a few bytes overhead due to the HTTP/2 DATA frame information needed. Additionally, each chunk needs a few bytes to describe the content of the gRPC message. You are looking at roughly 15 bytes per message chunk if it fits into one HTTP/2 DATA frame. Transfering 4 GB of data with a chunk size of 16 KB would need around 250,000 messages to transfer the file completely, incurring an overhead of ~3.7 MB. This may or may not be negilible depending on the use case.&lt;/p&gt;
    &lt;p&gt;Up- or downloading huge files with REST over HTTP/1.1 has less overhead. Since the bytes of the file are sent as the response/request body, there is not much else that takes up space. In case of uploads to a server, HTTP multipart requests incur a small overhead cost to define the multipart boundary. Additionally, HTTP headers and everything else that is needed to send the request over the wire take up space, but this is the case for all HTTP-based protocols. Downloading files, whether small or large, have roughly the same amount of bytes overhead with HTTP/1.1. Depending on the count and size of HTTP headers, this is around a few hundred bytes.&lt;/p&gt;
    &lt;p&gt;Funnily enough, transfering files with REST over HTTP/2 incurs a bigger overhead. HTTP/2 splits the payload into individual DATA frames, very similar to our custom gRPC solution. Each frame, often with a maximum size of 16 KB, has an overhead of 9 bytes. For a file 4 GB in size, this amounts to ~2.2 MB overhead. While HTTP/2 has many performance advantages, transfering a single, large file over HTTP/1.1 has less overhead.&lt;/p&gt;
    &lt;p&gt;In these examples, we omitted the overhead created by TCP/IP, TLS and the lower network layers, which both HTTP/1.1 and HTTP/2 share. Comparing it with HTTP/3, which uses UDP, would make everything even more complicated, so we leave this as exercise for the reader :)&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance and memory usage&lt;/head&gt;
    &lt;p&gt;I spun up the local server plus client and took a look at the CPU and memory usage. Please note that this is not an accurate benchmark, which would require a more complex setup. Nevertheless, it provides some insight into the differences between the approaches. The example file used was 4 GB in size.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;gRPC (naive)&lt;/cell&gt;
        &lt;cell role="head"&gt;gRPC (optimized)&lt;/cell&gt;
        &lt;cell role="head"&gt;REST (HTTP/1.1)&lt;/cell&gt;
        &lt;cell role="head"&gt;REST (HTTP/2)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Duration&lt;/cell&gt;
        &lt;cell&gt;24s&lt;/cell&gt;
        &lt;cell&gt;22s&lt;/cell&gt;
        &lt;cell&gt;20s&lt;/cell&gt;
        &lt;cell&gt;28s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Max memory usage&lt;/cell&gt;
        &lt;cell&gt;36 MB&lt;/cell&gt;
        &lt;cell&gt;35 MB&lt;/cell&gt;
        &lt;cell&gt;32 MB&lt;/cell&gt;
        &lt;cell&gt;35 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total memory allocation&lt;/cell&gt;
        &lt;cell&gt;4465 MB&lt;/cell&gt;
        &lt;cell&gt;165 MB&lt;/cell&gt;
        &lt;cell&gt;38 MB&lt;/cell&gt;
        &lt;cell&gt;137 MB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If done right, memory usage is no problem for streaming very large files. And as expected, the naive gRPC implementation which copies a lot of &lt;code&gt;ByteString&lt;/code&gt;s around allocates a lot of memory!
It needs constant garbage collections to clean up the mess.
The maximum memory usage however stays low for all approaches.&lt;/p&gt;
    &lt;p&gt;What really surprised me was the bad performance of HTTP/2 in comparison to HTTP/1.1. It was even slower than gRPC, which builds on top of it! I cannot really explain this huge difference, especially since the code is exactly the same, both on the client and the server. I was running these tests on .NET 10 on Windows 11.&lt;/p&gt;
    &lt;p&gt;The optimized gRPC version performs pretty well, but is still slower than REST via HTTP/1.1. Since it has to do more work, it takes longer and uses more memory (and CPU). Optimizing the gRPC code was very important, as the naive implementation allocates so much memory!&lt;/p&gt;
    &lt;p&gt;I also tested HTTP/1.1 with TLS disabled, but it did not really make a difference.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;A REST endpoint for up- and downloading files is still the best option. If you are forced to use gRPC or simply too lazy to add REST endpoints in addition to your gRPC services, transfering files via gRPC is not too bad!&lt;/p&gt;
    &lt;p&gt;If you use some kind of S3 compatible storage backend, the best options is to generate a presigned URL. Then, download your files directly from the S3 storage instead of piping it through your backend.&lt;/p&gt;
    &lt;p&gt;There are lots of points to consider when implementing file transfer APIs. For example, if your users may have slow networks, it may be useful to compress the data before sending it over the wire.&lt;/p&gt;
    &lt;p&gt;Should you need resumable up- or downloads, instead of rolling your own, you could use https://tus.io/. This open-source protocol has implementations in various languages.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kreya.app/blog/transfering-files-with-grpc/"/><published>2026-01-26T13:17:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46765460</id><title>After two years of vibecoding, I'm back to writing by hand</title><updated>2026-01-26T15:18:45.143796+00:00</updated><content/><link href="https://atmoio.substack.com/p/after-two-years-of-vibecoding-im"/><published>2026-01-26T13:36:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46765694</id><title>Porting 100k lines from TypeScript to Rust using Claude Code in a month</title><updated>2026-01-26T15:18:44.398756+00:00</updated><content>&lt;doc fingerprint="23280a7e3da5c6e8"&gt;
  &lt;main&gt;
    &lt;p&gt;I read this post ‚ÄúOur strategy is to combine AI and Algorithms to rewrite Microsoft‚Äôs largest codebases [from C++ to Rust]. Our North Star is ‚Äò1 engineer, 1 month, 1 million lines of code.‚Äù and it got me curious, how difficult is it really?&lt;/p&gt;
    &lt;p&gt;I've long wanted to build a competitive Pokemon battle AI after watching a lot of WolfeyVGC and following the Pok√©Agent challenge at NeurIPS. Thankfully there's an open source project called "Pokemon Showdown" that implements all the rules but it's written in JavaScript which is quite slow to run in a training loop. So my holiday project came to life: let's convert it to Rust using Claude!&lt;/p&gt;
    &lt;head rend="h2"&gt;Escaping the sandbox&lt;/head&gt;
    &lt;p&gt;Having the AI able to run arbitrary code on your machine is dangerous, so there's a lot of safeguards put in place. But... at the same time, this is what I want to do in this case. So let me walk through the ways I escaped the various sandboxes.&lt;/p&gt;
    &lt;head rend="h3"&gt;git push&lt;/head&gt;
    &lt;p&gt;Claude runs in a sandbox that limits some operations like ssh access. You need ssh access in order to publish to GitHub. This is very important as I want to be able to check how the AI is doing from my phone while I do some other activities üòâ&lt;/p&gt;
    &lt;p&gt;What I realized is that I can run the code on my terminal but Claude cannot do it from its own terminal. So what I did was to ask Claude to write a nodejs script that opens an http server on a local port that executes the git commands from the url. Now I just need to keep a tab open on my terminal with this server active and ask Claude to write instructions in Claude.md for it to interact with it.&lt;/p&gt;
    &lt;head rend="h3"&gt;rustc&lt;/head&gt;
    &lt;p&gt;There's an antivirus on my computer that requires a human interaction when an unknown binary is being ran. Since every time we compile it's a new unknown binary, this wasn't going to work.&lt;/p&gt;
    &lt;p&gt;What I found is that I can setup a local docker instance and compile + run the code inside of docker which doesn't trigger the antivirus. Again, I asked Claude to generate the right instructions in Claude.md and problem solved.&lt;/p&gt;
    &lt;p&gt;The next hurdle was to figure out how to let Claude Code for hours without any human intervention.&lt;/p&gt;
    &lt;head rend="h3"&gt;--yes&lt;/head&gt;
    &lt;p&gt;Claude keeps asking for permission to do things. I tried adding a bunch of things to the allowed commands file and &lt;code&gt;--allow-dangerously-skip-permissions --dangerously-skip-permissions&lt;/code&gt;was disabled in my environment (it has now been resolved).&lt;/p&gt;
    &lt;p&gt;I realized that I could run an AppleScript that presses enter every few seconds in another tab. This way it's going to say Yes to everything Claude asks to do. So far it hasn't decided to hack my computer...&lt;/p&gt;
    &lt;code&gt;#!/bin/bash

osascript -e \
'tell application "System Events"
    repeat
        delay 5
        key code 36
    end repeat
end tell'&lt;/code&gt;
    &lt;head rend="h3"&gt;Never give up&lt;/head&gt;
    &lt;p&gt;Claude after working for some time seem to always stop to recap things. I tried prompting it to never do, even threatening it to no avail.&lt;/p&gt;
    &lt;p&gt;I tried using the Ralph Wiggum loop but it couldn't get it to work and apparently I'm not alone.&lt;/p&gt;
    &lt;p&gt;What ended up working is to copy in my clipboard the task I wanted it to do and to tweak the script above to hit the keys "cmd-v" after pressing enter. This way in case it asks a question the "enter" is being used and in case it's not it's queuing the prompt for when Claude is giving back control.&lt;/p&gt;
    &lt;head rend="h3"&gt;Auto-updates&lt;/head&gt;
    &lt;p&gt;There are programs on the computer like software updater that can steal the focus from the terminal window, for example showing a modal. Once that happens, then the cmd-v / enter are no longer sent to the terminal and the execution stops.&lt;/p&gt;
    &lt;p&gt;I used my trusty Auto Clicker by MurGaa from Minecraft days to simulate a left click every few seconds. I place my terminal on the edge of the screen and same for my mouse so that when a modal appears in the middle, it refocuses the terminal correctly.&lt;/p&gt;
    &lt;p&gt;It also prevents the computer from going to sleep so that it can run even when I'm not using the laptop or at night.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bugs üêú&lt;/head&gt;
    &lt;p&gt;Reliability when running things for a long period of time is paramount. Overall it's been a pretty smooth ride but I ran into this specific error during a handful of nights which stopped the process. I hope they get to the bottom of it and solve it as I'm not the only one to report it!&lt;/p&gt;
    &lt;p&gt;This setup is far from optimal but has worked so far. Hopefully this gets streamlined in the future!&lt;/p&gt;
    &lt;head rend="h2"&gt;Porting Pokemon&lt;/head&gt;
    &lt;head rend="h3"&gt;One Shot&lt;/head&gt;
    &lt;p&gt;At the very beginning, I started with a simple prompt asking Claude to port the codebase and make sure that things are done line by line. At first it felt extremely impressive, it generated thousands of lines of Rust that was compiling.&lt;/p&gt;
    &lt;p&gt;Sadly it was only an appearance as it took a lot of shortcuts. For example, it created two different structures for what a move is in two different files so that they would both compile independently but didn't work when integrated together. It ported all the functions very loosely where anything that was remotely complicated would not be ported but instead "simplified".&lt;/p&gt;
    &lt;p&gt;I didn't realize it yet, I got the loop working to have it port more and more code. The issue is that it created wrong abstractions all over the place and kept adding hardcoded code to make whatever it was supposed to fix work. This wasn't going to go anywhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;Giving it structure&lt;/head&gt;
    &lt;p&gt;At this point I knew that I needed to be a lot more prescriptive for what I wanted out of it. Taking a step back, the end result should have every JavaScript file and every method inside to have a Rust equivalent.&lt;/p&gt;
    &lt;p&gt;So I asked Claude to write a script that takes all the files and methods in the JavaScript codebase and put comments in the rust codebase with the JavaScript source, next to the Rust methods.&lt;/p&gt;
    &lt;p&gt;It was really important for it to be a script as even when instructed to copy code over, it would mistranslate JavaScript code. Being deterministic here greatly increased the odds of getting the right results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Litte Islands&lt;/head&gt;
    &lt;p&gt;The next challenge is that the original files were thousands of lines long, double it with source comments we got to files more than 10k lines long. This causes a ton of issues with the context window where Claude straight up refuses to open the file. So it started reading the file in chunks but without a ton of precision. Also the context grew a lot quicker and compaction became way more frequent.&lt;/p&gt;
    &lt;p&gt;So I went ahead and split every method into its own file for the Rust version. This dramatically improved the results. For maximal efficiency I would need to do the same for the JavaScript codebase as well but I was too afraid to do it and accidentally change the behavior so decided not to.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cleanup&lt;/head&gt;
    &lt;p&gt;The process of porting went through two repeating phases. I would give a large task to Claude to do in a loop that would churn on it for a day, and then I would need to spend time cleaning up the places where it went into the wrong direction.&lt;/p&gt;
    &lt;p&gt;For the cleanup, I still used Claude but gave a lot more specific recommendations. For example, I noticed that it would hardcode moves/abilities/items/... behaviors everywhere in the code when left unchecked, even after explicitly telling it not to. So I would manually look for all these and tell it to move them into the right places.&lt;/p&gt;
    &lt;p&gt;This is where engineering skills come into play, all my experience building software let me figure out what went wrong and how to fix it. The good part is that I didn't have to do the cleanup myself, Claude was able to do it just fine when directed to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Integration&lt;/head&gt;
    &lt;head rend="h3"&gt;Build everything before testing&lt;/head&gt;
    &lt;p&gt;So far, I just made sure that the code compiled, but have never actually put all the pieces together to ensure it actually worked. What Claude really wanted was to do a traditional software building strategy where you make "simple" implementations of all of the pieces and then build them up as time goes.&lt;/p&gt;
    &lt;p&gt;But in our case, all this iteration has already happened for 10 years on the pokemon-showdown codebase. It's counter productive to try and re-learn all these lessons and will unlikely converge the same way. What works better is to port everything at once, and then do the integration at the end once.&lt;/p&gt;
    &lt;p&gt;I've learned this strategy from working on Skip, a compiler. For years all the building blocks were built independently and then it all came together with nothing to show for but within a month at the end it all worked. I was so shocked.&lt;/p&gt;
    &lt;head rend="h3"&gt;End-to-end test&lt;/head&gt;
    &lt;p&gt;Once most of the codebase was ported one to one, I started putting it all together. The good thing is that we can run and edit the code in JavaScript and in Rust, and the input/output is very simple and standardized: list of pokemons with their options (moves, items, nature, iv/ev spread...) and then the list of actions at each step (moves and switches). Given the same random sequence, it'll advance the state the same way.&lt;/p&gt;
    &lt;p&gt;Now I can let Claude generate this testing harness and go through all the issues one by one. Impressively, it was able to figure out all issues and fix them.&lt;/p&gt;
    &lt;p&gt;Over the course of 3 weeks it averaged fixing one issue every 20 minutes or so. It fixed hundreds of issues on its own. I never intervened, it was only a matter of time before it fixed every issue that it encountered.&lt;/p&gt;
    &lt;head rend="h3"&gt;Giving it structure&lt;/head&gt;
    &lt;p&gt;At the beginning, this process was extremely slow. Every time a compaction happened, Claude became "dumb" again and reinvented the wheel, writing down tons of markdown files and test scripts along the way. Or Claude decided to take the easy way out and just generate tons of tests but never actually making them match with JavaScript.&lt;/p&gt;
    &lt;p&gt;So, I started looking at what it did well and encoding it. For example, it added a lot of debugging around the PRNG steps and what actions happened at every turn with all the debugging metadata. So I asked it to create a single test script to print down this information for a single step and to print stack traces. Then add instruction to the Claude.md file. This way every investigation started right away.&lt;/p&gt;
    &lt;head rend="h3"&gt;The long slog&lt;/head&gt;
    &lt;p&gt;I built used the existing random number generator to generate battles and could put in a number as a seed. This let me generate consistent battles at an increasing size.&lt;/p&gt;
    &lt;p&gt;I started fixing the first 100 battles, then 1000, 10k, 100k and I'm almost done solving all the issues for the first 2.4 million battles! I'm not sure how many more issues there are but the good thing is that they are getting smaller and smaller as the batch size increases.&lt;/p&gt;
    &lt;head rend="h3"&gt;Types of issues&lt;/head&gt;
    &lt;p&gt;There are two broad classes of issues that were fixed. The first one that I expected is that Rust has different constraints than JavaScript which need to be taken into account and lead to bugs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust has the "borrow checker" where a mutable variable cannot be passed in two different contexts at once. The problem is that "Pokemon" and "Battle" have references to each others. So there's a lot of workarounds like doing copies, passing indices instead of the object, providing functions with mutable object as callback...&lt;/item&gt;
      &lt;item&gt;The JavaScript codebase uses dynamism heavily where some function return '', undefined, null, 0, 1, 5.2, Pokemon... which all are handled with different behaviors. At first the rust port started using Option&amp;lt;&amp;gt; to handle many of them but then moved to structs with all these variants.&lt;/item&gt;
      &lt;item&gt;Rust doesn't support optional arguments so every argument has to be spelled out literally.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the second one are due to itself... Claude Code is like a smart student that is trying to find every opportunity to avoid doing the hard work and take the easy way out if it thinks it can get away with it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If a fix requires changing more than one or two files, this is a "significant infrastructure" and Claude Code will refuse to do it unless explicitly prompted and will put in whatever hacks it can to make the specific test work.&lt;/item&gt;
      &lt;item&gt;Along the same lines, it is going to implement "simplified" versions of things. For some methods, it was better to delete everything and asking it to port it over from scratch than trying to fix all the existing code it created.&lt;/item&gt;
      &lt;item&gt;The JavaScript comments are supposed to be the source of truth. But Claude is not above changing the original code if it feels like this is the way to solve the problem...&lt;/item&gt;
      &lt;item&gt;If given a list of tasks, it's going to avoid doing the ones that seem difficult until it is absolutely forced to. This is inefficient if not careful as it's going to keep spending time investigating and then skipping all the "hard" ones. Compaction is basically wiping all its memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Prompts&lt;/head&gt;
    &lt;p&gt;I didn't write a single line of code myself in this project. I alternated between "co-op" where I work with Claude interactively during the day and creating a job for it to run overnight. I'll focus on the night ones for this section.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conversion&lt;/head&gt;
    &lt;p&gt;For the first phase of the project, I mostly used variations of this one. Asking it to go through all the big files one by one and implement them faithfully (it didn't really follow instructions as we've seen later...)&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Open BATTLE_TODO.md to get the list of all the methods in both battle*.rs.&lt;/p&gt;&lt;lb/&gt;Inspect every single one of them and make sure that they are a direct translation the JavaScript file. If there's a method with the same name, the JavaScript definition will be in the comment.&lt;lb/&gt;If there's no JavaScript definition, question whether this method should be there in the rust version. Our goal is to follow as closely as possible the JavaScript version to avoid any bugs in translation. If you notice that the implementation doesn't match, do all the refactoring needed to match 1 to 1.&lt;lb/&gt;This will be a complex project. You need to go through all the methods one by one, IN ORDER. YOU CANNOT skip a method because it is too hard or would requiring building new infrastructure. We will call this in a loop so spend as much time as you need building the proper infrastructure to make it 1 to 1 with the JavaScript equivalent. Do not give up.&lt;lb/&gt;Update BATTLE_TODO.md and do a git commit after each unit of work.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Todos&lt;/head&gt;
    &lt;p&gt;Claude Code while porting the methods one by one often decided to write a "simplified" version or add a "TODO" for later. I also found it to be useful when generating work to add the instructions in the codebase itself via a TODO comment, so I don't need to wish that it's going to be read from the context.&lt;/p&gt;
    &lt;p&gt;The master md file in practice didn't really work, it quickly became too big to be useful and Claude started creating a bunch more littering the repo with them. Instead I gave it a deterministic way to go through then by calling grep on the codebase, so it knew when to find them.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;We want to fix every TODO in the codebase.&lt;/p&gt;&lt;code&gt;TODO&lt;/code&gt;or&lt;code&gt;simplif&lt;/code&gt;in pokemon-showdown-rs/.&lt;lb/&gt;There are hundreds of them, so go diligently one by one. Do not skip them even if they are difficult. I will call this prompt again and again so you don't need to worry about taking too long on any single one.&lt;lb/&gt;The port must be exactly one to one. If the infrastructure doesn't exist, please implement it. Do not invent anything.&lt;lb/&gt;Make sure it still compiles after each addition and commit and push to git.&lt;/quote&gt;
    &lt;p&gt;At some point the context was poisoned where a TODO was inside of the original js codebase so it changed it to something else which made sense. But then it did the same for all the subsequent TODOs which didn't... Thankfully I could just revert all these commits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fixing&lt;/head&gt;
    &lt;p&gt;I put in all the instructions to debug in Claude.md and a script to run all the tests which outputs a txt file with progress report. This way Claude was able to just keep going fixing issues after issues.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We want to fix all the divergences in battles. Please look at 500-seeds-results.txt and fix them one by one. The only way you can fix is by making sure that the differences between javascript and rust are explained by language differences and not logic. Every line between the two must match one by one. If you fixed something specific, it's probably a larger issue, spend the time to figure out if other similar things are broken and do the work to do the larger infrastructure fixes. Make sure it still compiles after each addition and commit and push to git. Check if there are other parts of the codebase that make this mistake.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is really useful to have this txt file diff committed to GitHub to get a sense of progress on the go!&lt;/p&gt;
    &lt;head rend="h2"&gt;Epilogue&lt;/head&gt;
    &lt;head rend="h3"&gt;It works ü§Ø&lt;/head&gt;
    &lt;p&gt;I didn't quite know what to expect coming into this project. They usually tend to die due to the sheer amount of work needed to get anywhere close to something complete. But not this time!&lt;/p&gt;
    &lt;p&gt;We have a complete implementation of Pokemon battle system that produces the same results as the existing JavaScript codebase*. This was done through 5000 commits in 4 weeks and the Rust codebase is around 100k lines of code.&lt;/p&gt;
    &lt;p&gt;*I wish we had 0 divergences but right now there are 80 out of the first 2.4 million seeds or 0.003%. I need to run it for longer to solve these.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is it fast?&lt;/head&gt;
    &lt;p&gt;The whole point of the project was for it to be faster than the initial JavaScript implementation. Only towards the end of the project where we had a sizable amount of battles running perfectly I felt like it would be a fair time to do a performance comparison.&lt;/p&gt;
    &lt;p&gt;I asked Claude Code to parallelize both implementations and was relieved by the results, the Rust port is actually significantly faster, I didn't spend all this time for nothing!&lt;/p&gt;
    &lt;p&gt;I've tried asking Claude to optimize it further, it created a plan that looks reasonable (I've never interacted with Rust in my life) and it spent a day building many of these optimizations but at the end of the day, none of them actually improved the runtime and some even made it way worse.&lt;/p&gt;
    &lt;p&gt;This is a good example of how experience and expertise is still very required in order to get the best out of LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This is pretty wild that I was able to port a ~100k lines codebase from JavaScript to Rust in two weeks on my own with Claude Code running 24 hours a day for a month creating 5k commits! I have never written any line of Rust before in my life.&lt;/p&gt;
    &lt;p&gt;LLM-based coding agents are such a great new tool for engineers, there's no way I would have been able to do that without Claude Code. That said, it still feels like a tool that requires my engineering expertise and constant babysitting to produce these results.&lt;/p&gt;
    &lt;p&gt;Sadly I didn't get to build the Pokemon Battle AI and the winter break is over, so if anybody wants to do it, please have fun with the codebase!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.vjeux.com/2026/analysis/porting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html"/><published>2026-01-26T13:58:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46765819</id><title>Apple introduces new AirTag with longer range and improved findability</title><updated>2026-01-26T15:18:44.138736+00:00</updated><content>&lt;doc fingerprint="bc88ce9ed13616d8"&gt;
  &lt;main&gt;
    &lt;p&gt; UPDATE January 26, 2026 &lt;/p&gt;
    &lt;head rend="h1"&gt; Apple introduces new &lt;lb/&gt;AirTag with expanded &lt;lb/&gt;connectivity range &lt;lb/&gt;and improved findability &lt;/head&gt;
    &lt;p&gt; The next generation of AirTag ‚Äî the bestselling item finder ‚Äî is even easier to locate with more powerful Precision Finding, a longer Bluetooth range, and a louder speaker &lt;/p&gt;
    &lt;p&gt;Apple today unveiled the new AirTag, a powerful accessory that helps users keep track of and find the items that matter most with Apple‚Äôs Find My app ‚Äî now with an expanded finding range and a louder speaker. Powered by the strength of the Find My network, AirTag allows users to keep tabs on their belongings every single day. Since the launch of AirTag in 2021, users from around the world have shared stories of being reunited with lost luggage, keys, bicycles, bags, and more. With the help of AirTag placed inside an instrument case, a musician was able to locate their lost instrument and perform that evening, while another user was able to find lost luggage that contained a lifesaving medication. AirTag is designed exclusively for tracking objects and offers industry-leading protections against unwanted tracking. It is available today for the same price as its predecessor: $29 for a single AirTag and $99 for a four-pack, with free personalized engraving available on apple.com and the Apple Store app. &lt;/p&gt;
    &lt;head rend="h2"&gt;Enhanced Range and Findability&lt;/head&gt;
    &lt;p&gt;Apple‚Äôs second-generation Ultra Wideband chip ‚Äî the same chip found in the iPhone 17 lineup, iPhone Air, Apple Watch Ultra 3, and Apple Watch Series 11 ‚Äî powers the new AirTag, making it easier to locate than ever before. Using haptic, visual, and audio feedback, Precision Finding guides users to their lost items from up to 50 percent farther away than the previous generation.1 And an upgraded Bluetooth chip expands the range at which items can be located. For the first time, users can use Precision Finding on Apple Watch Series 9 or later, or Apple Watch Ultra 2 or later, to find their AirTag, bringing a powerful experience to the wrist. &lt;/p&gt;
    &lt;p&gt;With its updated internal design, the new AirTag is 50 percent louder than the previous generation, enabling users to hear their AirTag from up to 2x farther than before. Paired with its enhanced Precision Finding capabilities and distinctive new chime, AirTag now makes it easier for users to find their important items, such as keys hidden deep in between couch cushions or a wallet as they head out the door. &lt;/p&gt;
    &lt;head rend="h2"&gt;The Find My Network and Share Item Location&lt;/head&gt;
    &lt;p&gt;Find My makes it easy to locate AirTag, Apple devices, and compatible third-party devices, as well as keep up with friends and family, all while protecting user privacy. If AirTag is out of range of its paired iPhone, the Find My network can help track it down. The Find My network is a crowdsourced network of Apple devices that use Bluetooth technology to detect the location of an accessory or device, and report their approximate location back to the owner. &lt;/p&gt;
    &lt;p&gt;The new AirTag integrates seamlessly with Share Item Location, an iOS feature designed to help users recover a misplaced item by temporarily and securely sharing its location with trusted third parties, such as airlines, so they can assist in recovering delayed luggage or other lost items. Apple has partnered directly with more than 50 airlines to privately and securely accept Share Item Location links. &lt;/p&gt;
    &lt;p&gt;With Share Item Location, users can share the location of a misplaced item with a participating airline‚Äôs customer service team. According to SITA, a leading IT provider for airlines, carriers report that using Share Item Location has reduced baggage delays by 26 percent and reduced incidences of ‚Äútruly lost‚Äù or unrecoverable luggage by 90 percent. Access is granted only to authorized personnel via secure Apple Account or partner authentication. The shared location will be disabled as soon as a user is reunited with their item, can be stopped by the owner at any time, and will automatically expire after seven days. &lt;/p&gt;
    &lt;head rend="h2"&gt;Industry-Leading Security Features&lt;/head&gt;
    &lt;p&gt;The new AirTag is designed from the ground up to keep location data private and secure. AirTag doesn‚Äôt physically store location data or history on device, and end-to-end encryption protects all communication with the Find My network, ensuring that only the owner of a device can access its location data. No one, including Apple, knows the identity or location of any device that helped find it. Designed exclusively for tracking objects, and not people or pets, the new AirTag incorporates a suite of industry-first protections against unwanted tracking, including cross-platform alerts and unique Bluetooth identifiers that change frequently. &lt;/p&gt;
    &lt;head rend="h2"&gt;Environmental Responsibility and Accessory Compatibility&lt;/head&gt;
    &lt;p&gt;Apple 2030 is the company‚Äôs ambitious plan to be carbon neutral across its entire footprint by the end of this decade by reducing product emissions from their three biggest sources: materials, electricity, and transportation. The new AirTag is designed with the environment in mind, with 85 percent recycled plastic in the enclosure, 100 percent recycled rare earth elements in all magnets, and 100 percent recycled gold plating in all Apple-designed printed circuit boards. The paper packaging is 100 percent fiber-based and can be easily recycled. Maintaining the same form factor as the original, the new AirTag is compatible with all existing AirTag accessories, including the FineWoven Key Ring, which is made from 68 percent recycled content and available in five beautiful colors. &lt;/p&gt;
    &lt;p&gt;Pricing and Availability &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The new AirTag is available to order on apple.com and in the Apple Store app today, and will be available at Apple Store locations later this week. The new AirTag will also be available at Apple Authorized Resellers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customers can add a personalized engraving to the new AirTag for free during checkout on apple.com and the Apple Store app.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AirTag is available in one- and four-packs for $29 (U.S.) and $99 (U.S.), respectively.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Apple-designed AirTag FineWoven Key Ring is available in fox orange, midnight purple, navy, moss, and black for $35 (U.S.).2&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The new AirTag requires a compatible iPhone with iOS 26 or later, or iPad with iPadOS 26 or later. Customers must have an Apple Account and be signed into their iCloud account. Certain features require Find My to be enabled in iCloud settings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Precision Finding on Apple Watch requires Apple Watch Series 9 or later, or Apple Ultra 2 or later, with watchOS 26.2.1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Share article&lt;/p&gt;
    &lt;head rend="h2"&gt;Media&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Text of this article&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Images in this article&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Precision Finding is not available in countries and regions where Ultra Wideband technology is restricted. For more information, visit apple.com/uwb.&lt;/item&gt;
      &lt;item&gt;AirTag accessories, including the AirTag FineWoven Key Ring, are sold separately.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.apple.com/newsroom/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/"/><published>2026-01-26T14:10:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46766031</id><title>Google AI Overviews cite YouTube more than any medical site for health queries</title><updated>2026-01-26T15:18:44.030897+00:00</updated><content>&lt;doc fingerprint="bdca1812fc9b2573"&gt;
  &lt;main&gt;
    &lt;p&gt;Google‚Äôs search feature AI Overviews cites YouTube more than any medical website when answering queries about health conditions, according to research that raises fresh questions about a tool seen by 2 billion people each month.&lt;/p&gt;
    &lt;p&gt;The company has said its AI summaries, which appear at the top of search results and use generative AI to answer questions from users, are ‚Äúreliable‚Äù and cite reputable medical sources such as the Centers for Disease Control and Prevention and the Mayo Clinic.&lt;/p&gt;
    &lt;p&gt;However, a study that analysed responses to more than 50,000 health queries, captured using Google searches from Berlin, found the top cited source was YouTube. The video-sharing platform is the world‚Äôs second most visited website, after Google itself, and is owned by Google.&lt;/p&gt;
    &lt;p&gt;Researchers at SE Ranking, a search engine optimisation platform, found YouTube made up 4.43% of all AI Overview citations. No hospital network, government health portal, medical association or academic institution came close to that number, they said.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis matters because YouTube is not a medical publisher,‚Äù the researchers wrote. ‚ÄúIt is a general-purpose video platform. Anyone can upload content there (eg board-certified physicians, hospital channels, but also wellness influencers, life coaches, and creators with no medical training at all).‚Äù&lt;/p&gt;
    &lt;p&gt;Google told the Guardian that AI Overviews was designed to surface high-quality content from reputable sources, regardless of format, and a variety of credible health authorities and licensed medical professionals created content on YouTube. The study‚Äôs findings could not be extrapolated to other regions as it was conducted using German-language queries in Germany, it said.&lt;/p&gt;
    &lt;p&gt;The research comes after a Guardian investigation found people were being put at risk of harm by false and misleading health information in Google AI Overviews responses.&lt;/p&gt;
    &lt;p&gt;In one case that experts said was ‚Äúdangerous‚Äù and ‚Äúalarming‚Äù, Google provided bogus information about crucial liver function tests that could have left people with serious liver disease wrongly thinking they were healthy. The company later removed AI Overviews for some but not all medical searches.&lt;/p&gt;
    &lt;p&gt;The SE Ranking study analysed 50,807 healthcare-related prompts and keywords to see which sources AI Overviews relied on when generating answers.&lt;/p&gt;
    &lt;p&gt;They chose Germany because its healthcare system is strictly regulated by a mix of German and EU directives, standards and safety regulations. ‚ÄúIf AI systems rely heavily on non-medical or non-authoritative sources even in such an environment, it suggests the issue may extend beyond any single country,‚Äù they wrote.&lt;/p&gt;
    &lt;p&gt;AI Overviews surfaced on more than 82% of health searches, the researchers said. When they looked at which sources AI Overviews relied on most often for health-related answers, one result stood out immediately, they said. The single most cited domain was YouTube with 20,621 citations out of a total of 465,823.&lt;/p&gt;
    &lt;p&gt;The next most cited source was NDR.de, with 14,158 citations (3.04%). The German public broadcaster produces health-related content alongside news, documentaries and entertainment. In third place was a medical reference site, Msdmanuals.com with 9,711 citations (2.08%).&lt;/p&gt;
    &lt;p&gt;The fourth most cited source was Germany‚Äôs largest consumer health portal, Netdoktor.de, with 7,519 citations (1.61%). The fifth most cited source was a career platform for doctors, Praktischarzt.de, with 7,145 citations (1.53%).&lt;/p&gt;
    &lt;p&gt;The researchers acknowledged limitations to their study. It was conducted as a one-time snapshot in December 2025, using German-language queries that reflected how users in Germany typically search for health information.&lt;/p&gt;
    &lt;p&gt;Results could vary over time, by region, and by the phrasing of questions. However, even with those caveats, the findings still prompted alarm.&lt;/p&gt;
    &lt;p&gt;Hannah van Kolfschooten, a researcher specialising in AI, health and law at the University of Basel who was not involved with the research, said: ‚ÄúThis study provides empirical evidence that the risks posed by AI Overviews for health are structural, not anecdotal. It becomes difficult for Google to argue that misleading or harmful health outputs are rare cases.&lt;/p&gt;
    &lt;p&gt;‚ÄúInstead, the findings show that these risks are embedded in the way AI Overviews are designed. In particular, the heavy reliance on YouTube rather than on public health authorities or medical institutions suggests that visibility and popularity, rather than medical reliability, is the central driver for health knowledge.‚Äù&lt;/p&gt;
    &lt;p&gt;A Google spokesperson said: ‚ÄúThe implication that AI Overviews provide unreliable information is refuted by the report‚Äôs own data, which shows that the most cited domains in AI Overviews are reputable websites. And from what we‚Äôve seen in the published findings, AI Overviews cite expert YouTube content from hospitals and clinics.‚Äù&lt;/p&gt;
    &lt;p&gt;Google said the study showed that of the 25 most cited YouTube videos, 96% were from medical channels. However, the researchers cautioned that these videos represented fewer than 1% of all the YouTube links cited by AI Overviews on health.&lt;/p&gt;
    &lt;p&gt;‚ÄúMost of them (24 out of 25) come from medical-related channels like hospitals, clinics and health organisations,‚Äù the researchers wrote. ‚ÄúOn top of that, 21 of the 25 videos clearly note that the content was created by a licensed or trusted source.&lt;/p&gt;
    &lt;p&gt;‚ÄúSo at first glance it looks pretty reassuring. But it‚Äôs important to remember that these 25 videos are just a tiny slice (less than 1% of all YouTube links AI Overviews actually cite). With the rest of the videos, the situation could be very different.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/technology/2026/jan/24/google-ai-overviews-youtube-medical-citations-study"/><published>2026-01-26T14:27:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46766229</id><title>Exactitude in Science ‚Äì Borges (1946) [pdf]</title><updated>2026-01-26T15:18:43.138344+00:00</updated><content/><link href="https://kwarc.info/teaching/TDM/Borges.pdf"/><published>2026-01-26T14:44:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46766493</id><title>AI will not replace software engineers (hopefully)</title><updated>2026-01-26T15:18:43.019222+00:00</updated><content>&lt;doc fingerprint="691d9551a9ef249d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI will not replace software engineers (hopefully)&lt;/head&gt;
    &lt;p&gt;The title is not a statement, it‚Äôs a wish. I‚Äôm writing this blog not because I have some profound knowledge about what the future holds, but to gather my thoughts around the subject, because everywhere I look, people are talking about it. Whatever app I open, people are talking about the same thing, they‚Äôre either worried that AI will replace them, they‚Äôre sharing ‚Äúsuccess‚Äù stories about how they built apps with only AI and no engineering background or that AI is still too dumb to replace anyone.&lt;/p&gt;
    &lt;p&gt;I‚Äôm in the first category, I am afraid that I will end up being replaced, that I‚Äôll find myself without a job and that my only option will be applying to work as a cashier at McDonalds. There‚Äôs one thing that gives me hope though, and that is human drive and a little bit of greed.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs imagine a future where AI is so good at writing software that you can one shot any app. You want to edit photos?&lt;lb/&gt;‚ÄúGenerate a photoshop clone.‚Äù&lt;lb/&gt;You‚Äôre not satisfied with you current browser?&lt;lb/&gt;‚ÄúGenerate a Chrome clone that will allow me to block all ads‚Äù.&lt;lb/&gt;If anyone can write any application they want, what‚Äôs the endgame? Will software companies just cease to exist? All that‚Äôs gonna be left are 3 big companies competing to provide models to everyone?&lt;/p&gt;
    &lt;p&gt;And this is where the drive and the greed makes an appearance. Perhaps it‚Äôs not always money that motivates people, but one thing is for certain, people will always want to build new products, build new companies, disrupt markets and earn a lot of money. But if anyone can do anything with AI, what‚Äôs going to be the competitive advantage? Why should I use someone else's software when I can build my own? And if your product starts to be slightly successful, what‚Äôs stoping me from prompting an AI to build a clone of your product so that I can compete with you? And that, I naively believe will always be people, not because they can write better software than AI, but because people are innovative and unpredictable, and they will come up with a way to be competitive.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://medium.com/@sig.segv/ai-will-not-replace-software-engineers-hopefully-84c4f8fc94c0"/><published>2026-01-26T15:05:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46766526</id><title>Windows 11's Patch Tuesday nightmare gets worse</title><updated>2026-01-26T15:18:42.777992+00:00</updated><content>&lt;doc fingerprint="86b6e812902674d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Windows 11‚Äôs Patch Tuesday nightmare gets worse ‚Äî Microsoft says some PCs might not boot&lt;/head&gt;
    &lt;p&gt;Microsoft has posted an online bulletin confirming that the company is investigating reports that state Windows 11's latest security update has rendered some PCs unbootable.&lt;/p&gt;
    &lt;p&gt;Updated 3 PM ET on January 25, 2026: For those experiencing issues with boot failures, we had to put together a whole new guide on how to fix it. While we wanted this out earlier, it takes a few hours to research and write it all up to make sure it's accurate versus just getting the news out, so we appreicate you patience, especially since it's a Sunday and most of our staff are off. ‚Äî Daniel Rubino, Editor-in-Chief&lt;/p&gt;
    &lt;p&gt;Microsoft has confirmed that some users might find their PC unable to boot after installing the January 2026 security update released on January 13. This is on top of the plethora of other issues that have been reported since Microsoft's disastrous Patch Tuesday updates arrived.&lt;/p&gt;
    &lt;p&gt;So far, the company has released two emergency out-of-band updates for Windows 11 to address major bugs that were introduced with this month's security updates, but this latest issue that is causing PCs to fail to boot has not yet been addressed.&lt;/p&gt;
    &lt;p&gt;"Microsoft has received a limited number of reports of an issue in which devices are failing to boot with stop code ‚ÄúUNMOUNTABLE_BOOT_VOLUME‚Äù, after installing the January 2026 Windows security update released January 13, 2026, and later updates," the company has confirmed in an online bulletin (via AskWoody.) "Affected devices show a black screen with the message ‚ÄúYour device ran into a problem and needs a restart. You can restart.‚Äù At this stage, the device cannot complete startup and requires manual recovery steps."&lt;/p&gt;
    &lt;p&gt;Microsoft says this issue is likely to impact users running Windows 11 version 24H2 and 25H2 on physical machines, and that it is exploring potential fixes and workarounds. In the meantime, if you do encounter this problem, you will need to manually recover your PC by entering the Windows Recovery Environment and uninstalling the latest January 2026 security patch.&lt;/p&gt;
    &lt;p&gt;It's unclear how common this issue is, as most users have not reported their PC unable to boot. The company says it has received a limited number of reports, but has not provided an explanation as to what is causing the unbootable state, or whether it can be avoided.&lt;/p&gt;
    &lt;p&gt;This is the latest in a long line of issues that were introduced with this month's Patch Tuesday updates. First, users reported that PCs running version 23H2 were unable to shutdown or hibernate, and PCs running version 24H2 and 25H2 were unable to sign-in when using Remote Desktop.&lt;/p&gt;
    &lt;p&gt;A few days later, reports came in confirming an issue that rendered cloud-backed apps like Outlook, Dropbox, and OneDrive inoperable, forcing Microsoft to issue two emergency updates to address these showstopping bugs. Now, with reports that some PCs are unable to boot, it's likely the company will need to issue a third out of band update to fix this problem too.&lt;/p&gt;
    &lt;p&gt;All the latest news, reviews, and guides for Windows and Xbox diehards.&lt;/p&gt;
    &lt;p&gt;It's unclear why January's security update for Windows 11 has been so disastrous. Whatever the reason, Microsoft needs to step back and reevaluate how it developers Windows, as the current quality bar might be at the lowest it's ever been.&lt;/p&gt;
    &lt;p&gt;via Neowin&lt;/p&gt;
    &lt;p&gt;Follow Windows Central on Google News to keep our latest news, insights, and features at the top of your feeds!&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.windowscentral.com/microsoft/windows-11/windows-11s-botched-patch-tuesday-update-nightmare-continues-as-microsoft-confirms-some-pcs-might-fail-to-boot"/><published>2026-01-26T15:07:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46766560</id><title>Cop-assisted extortion of DWI arrestees in New Mexico include getting them drunk</title><updated>2026-01-26T15:18:42.373790+00:00</updated><content>&lt;doc fingerprint="6cf598bd2960e9e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cop-Assisted Extortion of DWI Arrestees in New Mexico Included Getting Them Drunk&lt;/head&gt;
    &lt;head rend="h2"&gt;A recent guilty plea reveals a new wrinkle in a long-running corruption scheme that involved bribing police officers to make drunk driving cases disappear.&lt;/head&gt;
    &lt;p&gt;For three decades, Albuquerque defense attorney Thomas Clear bribed police officers to make drunk driving cases against his clients disappear. The cops typically did that by deliberately failing to show up at administrative hearings, pretrial interviews, or judicial proceedings, allowing Clear to move for dismissal of the charges. The latest guilty plea in that wide-ranging corruption case reveals a new wrinkle: Sometimes Clear's paralegal, Rick Mendez, or his associates would "orchestrate" DWI arrests by getting people drunk and arranging for a corrupt cop to nab them after they hit the road.&lt;/p&gt;
    &lt;p&gt;Lt. Justin Hunt, who was employed by the Albuquerque Police Department (APD) for more than 20 years, retired in February 2024 amid the FBI's investigation of the bribery scheme that federal prosecutors have dubbed the "DWI Enterprise." This week, he pleaded guilty to extortion "under color of official right," meaning he used his position to help Clear extract money from clients eager to avoid the consequences of a DWI arrest‚Äîprofits that Clear would then share with Hunt.&lt;/p&gt;
    &lt;p&gt;Hunt is part of the biggest law enforcement scandal in New Mexico's history. He is one of two dozen people‚Äîincluding Albuquerque police officers, Bernalillo County sheriff's deputies, and a New Mexico State Police sergeant known as "the face of DWI enforcement" because he was featured in a state ad campaign against drunk driving‚Äîwho have been implicated in the bribery scheme so far. Half of them have pleaded guilty, including Clear and Mendez.&lt;/p&gt;
    &lt;p&gt;"For many of the cases on which I accepted payment in exchange for not performing my official duties, I conducted the DWI arrest in conformity with APD policies and procedures," Hunt says in his plea agreement. "The illegal conduct‚Äîthat being me receiving benefits or payments from CLEAR and MENDEZ‚Äîwould occur after I conducted the otherwise legitimate DWI arrest." But he adds that he, Clear, and Mendez "also developed another method of operating the scheme."&lt;/p&gt;
    &lt;p&gt;Here is how Hunt says that other method worked. Mendez "would orchestrate the traffic stop, thereby allowing me to conduct the DWI arrest, with the expectation that I would then be paid or receive a benefit to not appear as required." Mendez and "other co-conspirators" would "go out drinking with a particular target." After the target "had consumed alcohol and was heavily intoxicated," Hunt says, "I would then conduct a traffic stop on the target's vehicle and arrest them for DWI."&lt;/p&gt;
    &lt;p&gt;In May 2014, for example, Mendez took a man identified as "C.F." out for his birthday. Mendez, C.F., "and others" ended up at a strip club, where Mendez bought C.F. drinks. Once C.F. "was preparing to drive after consuming a large amount of alcohol," Mendez alerted Hunt, who stopped C.F.'s car after he left the strip club's parking lot. Hunt arrested C.F. for DWI, and C.F. subsequently hired Clear to represent him.&lt;/p&gt;
    &lt;p&gt;In his police report, Hunt "purposefully omitted the information" he had received from Mendez. He also did not mention that Mendez was in the car when C.F. was arrested. After C.F. was charged with DWI, Hunt "intentionally failed to appear at required criminal settings and the [Motor Vehicle Division] hearing," thereby "guaranteeing dismissal of the criminal case" and enabling C.F. to get back his driver's license, which had been revoked following his arrest.&lt;/p&gt;
    &lt;p&gt;In return for his assistance, Hunt received "wheels, tires, and a lift kit" for his Jeep. On other occasions, Hunt was paid in cash for helping Clear drum up business.&lt;/p&gt;
    &lt;p&gt;Clear and Mendez, in short, decided to stop waiting for people to drive drunk on their own initiative. Whatever points you might give them for creativity, Mendez, who apparently thought a DWI arrest was a fine birthday gift, does not seem like a very good friend. And the description of his efforts to manufacture DWI cases adds a new facet to a story that was already troubling in several ways.&lt;/p&gt;
    &lt;p&gt;We already knew that corrupt cops like Hunt were willing to let drunk drivers off the hook in exchange for money, potentially endangering public safety by protecting arrestees from penalties that might have encouraged them to think twice before driving while intoxicated. We also knew that prosecutors ended up dropping hundreds of DWI cases because they involved officers who were deemed untrustworthy after they were implicated in the scandal, which likewise surely did not make the roads any safer. There was also evidence that some of the allegedly intoxicated drivers nabbed by bribe-hungry cops were actually sober, which if true was unjust as well as counterproductive. Hunt's account compounds the outrage, revealing that Clear's operation created new hazards by directly fostering drunk driving.&lt;/p&gt;
    &lt;p&gt;Hunt's plea agreement suggests how this racket persisted for so long. Hunt, who joined the APD as a sworn officer in 2003, served in the department's DWI unit, where corruption was endemic, from 2011 to 2014. After he left the unit, Hunt continued to help Clear by tipping off Mendez about potential threats to Clear's lucrative business strategy.&lt;/p&gt;
    &lt;p&gt;In November 2023, for example, Albuquerque's Civilian Police Oversight Agency received a letter from a local court official who said Officer Honorio Alba Jr., a member of the DWI unit who would later admit that he took bribes from Clear, reportedly had pulled over a speeding, flagrantly drunk driver and, instead of filing charges, referred him to a specific local defense attorney. Hunt says he discussed the complaint with Mendez "in an attempt to assist ALBA from having adverse action taken against him."&lt;/p&gt;
    &lt;p&gt;At that point, the FBI was already looking into corruption within the DWI unit. The previous month, the FBI had discussed the investigation with Albuquerque Police Chief Harold Medina, who says that was his first clue to the rampant corruption. But intervention by supervisors like Hunt apparently helped keep the "DWI Enterprise" going before it was finally revealed by the FBI.&lt;/p&gt;
    &lt;p&gt;Hunt was a lieutenant assigned to southeast Albuquerque. The senior Albuquerque police officers implicated in the scandal also include Commander Mark Landavazo, who was placed on administrative leave in February 2024 and fired the following August; Deputy Commander Gustavo Gomez, who was placed on administrative leave in October 2024 and fired last March; Lt. Kyle Curtis, who retired in January 2025 after he was placed on administrative leave; and Lt. Matthew Chavez, who was placed on administrative leave the same day as Curtis and terminated last March.&lt;/p&gt;
    &lt;p&gt;Landavazo, Gomez, Chavez, and Curtis, like Hunt, had previously served in the DWI unit. Prosecutors say former members of the unit helped protect Clear's operation as they moved up in the department. Landavazo was especially well positioned to do that because he was in charge of the APD's internal affairs division. Gomez was Landavazo's internal affairs deputy.&lt;/p&gt;
    &lt;p&gt;Medina, who joined the APD in 1995 and has run or helped run the department since 2017, promoted both men. He says he had no inkling of the pervasive and persistent corruption until he was briefed on the FBI's investigation in October 2023.&lt;/p&gt;
    &lt;p&gt;Despite his avowed cluelessness, Medina, who has repeatedly promised to "make sure that we get to the bottom of this," wants to take credit for revealing the bribery that was happening under his nose for decades. "We uncovered the DWI scandal," he told a local TV station month, saying it was "a low point" to see "people I worked with for 20 years" were "involved in that." He expressed "disappointment" that "individuals that I believed in, I worked with" had "given up their integrity and lost everything, all the honor of their career," simply "for money."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://reason.com/2026/01/23/cop-assisted-extortion-of-dwi-arrestees-in-new-mexico-included-getting-them-drunk/"/><published>2026-01-26T15:09:47+00:00</published></entry></feed>