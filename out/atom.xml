<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-29T16:13:34.913002+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45739499</id><title>Tinkering is a way to acquire good taste</title><updated>2025-10-29T16:13:43.836127+00:00</updated><content>&lt;doc fingerprint="7426e95752a204d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If you don't tinker, you don't have taste&lt;/head&gt;
    &lt;head rend="h2"&gt;tinÂ·ker&lt;/head&gt;
    &lt;head rend="h4"&gt;/ËtiNGkÉr/&lt;/head&gt;
    &lt;head rend="h4"&gt;to make small changes to something, especially in an attempt to repair or improve it.&lt;/head&gt;
    &lt;head rend="h1"&gt;In Hindsight&lt;/head&gt;
    &lt;p&gt;Growing up, I never stuck to a single thing, be it guitar lessons, art school, martial arts â I tried them all. when it came to programming, though, I never really tinkered. I was always amazed with video games and wondered how they were made but I never pursued that curiosity.&lt;/p&gt;
    &lt;p&gt;My tinkering habits picked up very late, and now I cannot go by without picking up new things in one form or another. Itâs how I learn. I wish I did it sooner. Itâs a major part of my learning process now, and I would never be the &lt;del&gt;programmer&lt;/del&gt; person I am today.&lt;/p&gt;
    &lt;head rend="h1"&gt;What the hell is tinkering?&lt;/head&gt;
    &lt;p&gt;Have you ever spent hours tweaking the mouse sensitivity in your favorite FPS game?&lt;/p&gt;
    &lt;p&gt;Have you ever installed a Linux distro, spent days configuring window managers, not because you had to, but purely because it gave you satisfaction and made your workflow exactly yours?&lt;/p&gt;
    &lt;p&gt;Ever pulled apart your mechanical keyboard, swapped keycaps, tested switches, and lubed stabilizers just for more thock?&lt;/p&gt;
    &lt;p&gt;That is what I mean.&lt;/p&gt;
    &lt;p&gt;I have come to understand that there are two kinds of people, those who do things only if it helps them achieve a goal, and those who do things just because. The ideal, of course, is to be a mix of both.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;when you tinker and throw away, thatâs practice, and practice should inherently be ephemeral, exploratory, and be frequent - @ludwigABAP&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;My approach to tinkering&lt;/head&gt;
    &lt;p&gt;There are plenty of people who still use the VSCode terminal as their default terminal, do not know what vim bindings are, GitHub desktop rather than the cli (at the very least). Iâm not saying these are bad things necessarily, just that this should be the minimum, not the median.&lt;/p&gt;
    &lt;p&gt;This does not mean I spend every waking hour fiddling with my neovim config. In fact, the last meaningful change to my config was 6 months ago. Finding that balance is where most people fail.&lt;/p&gt;
    &lt;p&gt;Over the years I have done so many things that in hindsight have made me appreciate programming more but were completely âunnecessaryâ in the strict sense.&lt;/p&gt;
    &lt;p&gt;In the past week I have, for the first time, written a glsl fragment shader, a rust procedural macro, template c++, a swift app, furthered my hatred for windows development (this is not new), and started using the helix editor more (mainly for good defaults + speed). I didnât have to do these things, but I did, for fun! And I know more about these things now.&lt;/p&gt;
    &lt;p&gt;No time spent learning, is time wasted.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why taste matters, especially now&lt;/head&gt;
    &lt;p&gt;Acquiring good taste comes through using various things, discarding the ones you donât like and keeping the ones you do. if you never try various things, you will not acquire good taste.&lt;/p&gt;
    &lt;p&gt;And what I mean by taste here is simply the honed ability to distinguish mediocrity from excellence. This will be highly subjective, and not everyoneâs taste will be the same, but that is the point, you should NOT have the same taste as someone else.&lt;/p&gt;
    &lt;p&gt;Question the status quo, experiment, break things, do this several times, do this everyday and keep doing it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://seated.ro/blog/tinkering-a-lost-art"/><published>2025-10-28T21:31:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742419</id><title>Tips for stroke-surviving software engineers</title><updated>2025-10-29T16:13:43.258659+00:00</updated><content>&lt;doc fingerprint="f62a9b5785b1fadc"&gt;
  &lt;main&gt;
    &lt;p&gt;2025-10-29&lt;/p&gt;
    &lt;p&gt;This is a pretty niche topic; I don't imagine there are many of us out there.&lt;/p&gt;
    &lt;p&gt;Actually, to be strict, I'd say this advice is tailored to people who've had hemorrhagic stroke in the parietal lobe with residual epilepsy...&lt;/p&gt;
    &lt;p&gt;I was 29 and around 12 years into my career when it all happened, and in the six years since then I've had time to learn a bit more about my new self.&lt;/p&gt;
    &lt;p&gt;The first tip is to just stop. Fatigue, fuzziness, nausea, or affected-sided weird sensations are non-negotiable stop signals. So go lie down, hydrate, reset. Close your eyes and think about the cottage or lonely mountain you want to retire to. Escape the overwhelming mental or physical space.&lt;/p&gt;
    &lt;p&gt;HEADPHONES, blinders, and 'No'. Eliminate unwanted inputs at the earliest point of entry. Work from home or environments where you can control most variables. Routes of escape and rest are important.&lt;/p&gt;
    &lt;p&gt;Health above performance every single time. Metrics and productivity be damned. Self-advocate, and all that. Reject with directness any demands made of you that cross the threshold.&lt;/p&gt;
    &lt;p&gt;Laws. Use them. You don't have to rely on good behaviour and kindness. You are, depending on your location, usually protected by all types of anti-discrimination legislation, implicit and explicit. Use your employee assistance programs too.&lt;/p&gt;
    &lt;p&gt;Single-thread it all! Less context switching. Batch your work, finish one thing, then move to the next. Externalize working-memory. Use notebooks, whiteboards, and lists instead of juggling state in your head. I am not good at this, and over-stretch my brain, leading to auras, overwhelm, and general sickness. Terrible idea.&lt;/p&gt;
    &lt;p&gt;Related: Sssh to the AI naysayers. Use it as your help and scratchpad. Let it hold state so your brain can judge rather than store and needlessly cogitate on stuff. You don't have to do this alone out of some purity fetishism. You, too, have a limited context window. Sorry!&lt;/p&gt;
    &lt;p&gt;Do the heavy thinking in your peak window (for me, that's the morning); push everything else to later. Spend your time more carefully than your money.&lt;/p&gt;
    &lt;p&gt;Pick the route of least attention. Attention is expensive, and rarely needed as much as we think it is. It's a heavy toll to pay. Unless you're in an ops or monitoring role, you don't need to be synchronously active. DISABLE NOTIFICATIONS.&lt;/p&gt;
    &lt;p&gt;AVOID long meetings. Emails are good. Oh god am I bad at this? YES, I like people so I like some meetings, but communicating is so so expensive. Being polite is also expensive; It's not nice to have to tell people they're draining you.&lt;/p&gt;
    &lt;p&gt;I think that's mostly it. I'm still working on this stuff. And would probably grade myself pretty poorly. One day I'll be better at saying no, at advocating for myself, and knowing how to navigate the disappointment of others.&lt;/p&gt;
    &lt;p&gt;Footnote &amp;amp; some casual research: If you're into this, here's some stuff I found out related to my specific injury location and how it might apply to my work. This was gathered with help from gemini when I was struggling with left-arm and eye prodromes after long coding sessions:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Frontal and parietal cortices form a flexible control system that holds goals, routes attention, and updates task sets; this "multiple-demand" network scales with task complexity and underpins how we store, manipulate, and decide on information during work[1][2][3]. Superior parietal cortex is especially taxed when we transform or reorganize information in working memory rather than simply maintain it, which is why mental navigations, refactors, and other transformations feel costly[4][5]. Frequent context switches recruit lateral prefrontal and parietal regions and increase control load, so hopping between threads repeatedly spikes demand on this same circuitry[6][7]. After AVM resection (what I had!) or stroke generally, tissue near the lesion can remain hyperexcitable with impaired neurovascular coupling; heavy cognitive load lowers seizure threshold and can produce somatosensory auras and body-image distortions from parietal cortex[8][9][10].&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thanks for reading :) Tonnes of love to all the stroke survivors out there &amp;lt;3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.j11y.io/2025-10-29_stroke_tips_for_engineers/"/><published>2025-10-29T03:51:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742446</id><title>uBlock Origin Lite Apple App Store</title><updated>2025-10-29T16:13:43.039984+00:00</updated><content>&lt;doc fingerprint="b62ad8dff7b5baa7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;uBlock Origin Lite 4+&lt;/head&gt;
    &lt;head rend="h2"&gt;An efficient content blocker&lt;/head&gt;
    &lt;head rend="h2"&gt;Raymond Hill&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Free&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Screenshots&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;uBO Lite (uBOL) is a reliable and efficient content blocker.&lt;lb/&gt;The default ruleset corresponds to uBlock Origin's default filterset:&lt;lb/&gt;- uBlock Origin's built-in filter lists&lt;lb/&gt;- EasyList&lt;lb/&gt;- EasyPrivacy&lt;lb/&gt;- Peter Loweâs Ad and tracking server list&lt;lb/&gt;You can enable more rulesets by visiting the options page -- click the _Cogs_ icon in the popup panel.&lt;lb/&gt;uBOL is entirely declarative, meaning there is no need for a permanent uBOL process for the filtering to occur, and CSS/JS injection-based content filtering is performed reliably by the browser itself rather than by the extension. This means that uBOL itself does not consume CPU/memory resources while content blocking is ongoing -- uBOL's service worker process is required _only_ when you interact with the popup panel or the option pages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whatâs New&lt;/head&gt;
    &lt;p&gt;Version 2025.1019.1656&lt;/p&gt;
    &lt;p&gt;â¢ Automatically select optimal for newly allowed hosts&lt;lb/&gt;â¢ Updated filter lists&lt;/p&gt;
    &lt;head rend="h2"&gt;Ratings and Reviews&lt;/head&gt;
    &lt;head rend="h3"&gt;The best content blocker is finally on iPadOS!!&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Itâs was a really long wait, but finally we are able to use it directly on the iPad. The first TestFlight version had a big battery drain, but itâs better now on the official release. The only limitation is that we canât add our own lists, but I am fine with the default lists ans it works perfect.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;I was looking for this long time finally itâs here.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;I love block. I do not want to use chrome. Itâs perfect. I can use this in safari..&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Finally ð¤©&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Waiting for many years. Added in all apple devices. Working properly. ðð&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;App Privacy&lt;/head&gt;
    &lt;p&gt;The developer, Raymond Hill, indicated that the appâs privacy practices may include handling of data as described below. For more information, see the developerâs privacy policy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Data Not Collected&lt;/head&gt;
    &lt;p&gt;The developer does not collect any data from this app.&lt;/p&gt;
    &lt;p&gt;Privacy practices may vary based on, for example, the features you use or your age. LearnÂ More&lt;/p&gt;
    &lt;head rend="h2"&gt;Information&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Provider&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Raymond Hill&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Size&lt;/item&gt;
      &lt;item rend="dd-2"&gt;6 MB&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Category&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Utilities&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Compatibility&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-5"&gt;iPhone&lt;/item&gt;
          &lt;item rend="dd-5"&gt;Requires iOS 18.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-6"&gt;iPad&lt;/item&gt;
          &lt;item rend="dd-6"&gt;Requires iPadOS 18.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-7"&gt;Mac&lt;/item&gt;
          &lt;item rend="dd-7"&gt;Requires macOS 13.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-8"&gt;Apple Vision&lt;/item&gt;
          &lt;item rend="dd-8"&gt;Requires visionOS 2.5 or later.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-9"&gt;Languages&lt;/item&gt;
      &lt;item rend="dd-9"&gt;
        &lt;p&gt;English&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-10"&gt;Age Rating&lt;/item&gt;
      &lt;item rend="dd-11"&gt;Learn More&lt;/item&gt;
      &lt;item rend="dt-12"&gt;Copyright&lt;/item&gt;
      &lt;item rend="dd-12"&gt;Â© Raymond Hill 2025&lt;/item&gt;
      &lt;item rend="dt-13"&gt;Price&lt;/item&gt;
      &lt;item rend="dd-13"&gt;Free&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apps.apple.com/in/app/ublock-origin-lite/id6745342698"/><published>2025-10-29T03:57:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742488</id><title>Keep Android Open</title><updated>2025-10-29T16:13:42.879582+00:00</updated><content>&lt;doc fingerprint="9da6d2e399ba52e9"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Keep Android Open&lt;/head&gt;
      &lt;p&gt;In August 2025, Google announced that starting next year, it will no longer be possible to develop apps for the Android platform without first registering centrally with Google.&lt;/p&gt;
      &lt;p&gt;This registration will involve:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Paying a fee to Google&lt;/item&gt;
        &lt;item&gt;Agreeing to Google’s Terms and Conditions&lt;/item&gt;
        &lt;item&gt;Providing government identification&lt;/item&gt;
        &lt;item&gt;Uploading evidence of an app’s private signing key&lt;/item&gt;
        &lt;item&gt;Listing all current and future application identifiers&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Some actions you can take to help oppose the enactment of this policy are:&lt;/p&gt;
      &lt;head rend="h2"&gt;Sign the Open Letter&lt;/head&gt;
      &lt;head rend="h2"&gt;European Union&lt;/head&gt;
      &lt;head rend="h2"&gt;United States&lt;/head&gt;
      &lt;head rend="h2"&gt;United Kingdom&lt;/head&gt;
      &lt;head rend="h2"&gt;Brazil&lt;/head&gt;
      &lt;head rend="h2"&gt;Other&lt;/head&gt;
      &lt;head rend="h2"&gt;References&lt;/head&gt;
      &lt;head rend="h3"&gt;Overview&lt;/head&gt;
      &lt;head rend="h3"&gt;Editorials and Blogs&lt;/head&gt;
      &lt;head rend="h3"&gt;Press Reactions&lt;/head&gt;
      &lt;head rend="h3"&gt;Video Responses&lt;/head&gt;
      &lt;head rend="h3"&gt;Discussions&lt;/head&gt;
      &lt;head rend="h3"&gt;Official Documentation&lt;/head&gt;
      &lt;head rend="h3"&gt;Miscellaneous&lt;/head&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://keepandroidopen.org/"/><published>2025-10-29T04:03:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742907</id><title>Who needs Graphviz when you can build it yourself?</title><updated>2025-10-29T16:13:42.561615+00:00</updated><content>&lt;doc fingerprint="af4c3a680161d9ab"&gt;
  &lt;main&gt;
    &lt;p&gt;We recently overhauled our internal tools for visualizing the compilation of JavaScript and WebAssembly. When SpiderMonkey’s optimizing compiler, Ion, is active, we can now produce interactive graphs showing exactly how functions are processed and optimized.&lt;/p&gt;
    &lt;p&gt;You can play with these graphs right here on this page. Simply write some JavaScript code in the &lt;code&gt;test&lt;/code&gt; function and see what graph is produced. You can click and drag to navigate, ctrl-scroll to zoom, and drag the slider at the bottom to scrub through the optimization process.&lt;/p&gt;
    &lt;p&gt;As you experiment, take note of how stable the graph layout is, even as the sizes of blocks change or new structures are added. Try clicking a block's title to select it, then drag the slider and watch the graph change while the block remains in place. Or, click an instruction's number to highlight it so you can keep an eye on it across passes.&lt;/p&gt;
    &lt;p&gt;We are not the first to visualize our compiler’s internal graphs, of course, nor the first to make them interactive. But I was not satisfied with the output of common tools like Graphviz or Mermaid, so I decided to create a layout algorithm specifically tailored to our needs. The resulting algorithm is simple, fast, produces surprisingly high-quality output, and can be implemented in less than a thousand lines of code. The purpose of this article is to walk you through this algorithm and the design concepts behind it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;As readers of this blog already know, SpiderMonkey has several tiers of execution for JavaScript and WebAssembly code. The highest tier is known as Ion, an optimizing SSA compiler that takes the most time to compile but produces the highest-quality output.&lt;/p&gt;
    &lt;p&gt;Working with Ion frequently requires us to visualize and debug the SSA graph. Since 2011 we have used a tool for this purpose called iongraph, built by Sean Stangl. It is a simple Python script that takes a JSON dump of our compiler graphs and uses Graphviz to produce a PDF. It is perfectly adequate, and very much the status quo for compiler authors, but unfortunately the Graphviz output has many problems that make our work tedious and frustrating.&lt;/p&gt;
    &lt;p&gt;The first problem is that the Graphviz output rarely bears any resemblance to the source code that produced it. Graphviz will place nodes wherever it feels will minimize error, resulting in a graph that snakes left and right seemingly at random. There is no visual intuition for how deeply nested a block of code is, nor is it easy to determine which blocks are inside or outside of loops. Consider the following function, and its Graphviz graph:&lt;/p&gt;
    &lt;code&gt;function foo(n) {
  let result = 0;
  for (let i = 0; i &amp;lt; n; i++) {
    if (!!(i % 2)) {
      result = 0x600DBEEF;
    } else {
      result = 0xBADBEEF;
    }
  }

  return result;
}
&lt;/code&gt;
    &lt;p&gt;Counterintuitively, the &lt;code&gt;return&lt;/code&gt; appears before the two assignments in the body of the loop. Since this graph mirrors JavaScript control flow, we’d expect to see the return at the bottom. This problem only gets worse as graphs grow larger and more complex.&lt;/p&gt;
    &lt;p&gt;The second, related problem is that Graphviz’s output is unstable. Small changes to the input can result in large changes to the output. As you page through the graphs of each pass within Ion, nodes will jump left and right, true and false branches will swap, loops will run up the right side instead of the left, and so on. This makes it very hard to understand the actual effect of any given pass. Consider the following before and after, and notice how the second graph is almost—but not quite—a mirror image of the first, despite very minimal changes to the graph’s structure:&lt;/p&gt;
    &lt;p&gt;None of this felt right to me. Control flow graphs should be able to follow the structure of the program that produced them. After all, a control flow graph has many restrictions that a general-purpose tool would not be aware of: they have very few cycles, all of which are well-defined because they come from loops; furthermore, both JavaScript and WebAssembly have reducible control flow, meaning all loops have only one entry, and it is not possible to jump directly into the middle of a loop. This information could be used to our advantage.&lt;/p&gt;
    &lt;p&gt;Beyond that, a static PDF is far from ideal when exploring complicated graphs. Finding the inputs or uses of a given instruction is a tedious and frustrating exercise, as is following arrows from block to block. Even just zooming in and out is difficult. I eventually concluded that we ought to just build an interactive tool to overcome these limitations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How hard could layout be?&lt;/head&gt;
    &lt;p&gt;I had one false start with graph layout, with an algorithm that attempted to sort blocks into vertical “tracks”. This broke down quickly on a variety of programs and I was forced to go back to the drawing board—in fact, back to the source of the very tool I was trying to replace.&lt;/p&gt;
    &lt;p&gt;The algorithm used by &lt;code&gt;dot&lt;/code&gt;, the typical hierarchical layout mode for Graphviz, is known as the Sugiyama layout algorithm, from a 1981 paper by Sugiyama et al. As introduction, I found a short series of lectures that broke down the Sugiyama algorithm into 5 steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cycle breaking, where the direction of some edges are flipped in order to produce a DAG.&lt;/item&gt;
      &lt;item&gt;Leveling, where vertices are assigned into horizontal layers according to their depth in the graph, and dummy vertices are added to any edge that crosses multiple layers.&lt;/item&gt;
      &lt;item&gt;Crossing minimization, where vertices on a layer are reordered in order to minimize the number of edge crossings.&lt;/item&gt;
      &lt;item&gt;Vertex positioning, where vertices are horizontally positioned in order to make the edges as straight as possible.&lt;/item&gt;
      &lt;item&gt;Drawing, where the final graph is rendered to the screen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These steps struck me as surprisingly straightforward, and provided useful opportunities to insert our own knowledge of the problem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cycle breaking would be trivial for us, since the only cycles in our data are loops, and loop backedges are explicitly labeled. We could simply ignore backedges when laying out the graph.&lt;/item&gt;
      &lt;item&gt;Leveling would be straightforward, and could easily be modified to better mimic the source code. Specifically, any blocks coming after a loop in the source code could be artificially pushed down in the layout, solving the confusing early-exit problem.&lt;/item&gt;
      &lt;item&gt;Permuting vertices to reduce edge crossings was actually just a bad idea, since our goal was stability from graph to graph. The true and false branches of a condition should always appear in the same order, for example, and a few edge crossings is a small price to pay for this stability.&lt;/item&gt;
      &lt;item&gt;Since reducible control flow ensures that a program’s loops form a tree, vertex positioning could ensure that loops are always well-nested in the final graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Taken all together, these simplifications resulted in a remarkably straightforward algorithm, with the initial implementation being just 1000 lines of JavaScript. (See this demo for what it looked like at the time.) It also proved to be very efficient, since it avoided the most computationally complex parts of the Sugiyama algorithm.&lt;/p&gt;
    &lt;head rend="h2"&gt;iongraph from start to finish&lt;/head&gt;
    &lt;p&gt;We will now go through the entire iongraph layout algorithm. Each section contains explanatory diagrams, in which rectangles are basic blocks and circles are dummy nodes. Loop header blocks (the single entry point to each loop) are additionally colored green.&lt;/p&gt;
    &lt;p&gt;Be aware that the block positions in these diagrams are not representative of the actual computed layout position at each point in the process. For example, vertical positions are not calculated until the very end, but it would be hard to communicate what the algorithm was doing if all blocks were drawn on a single line!&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 1: Layering&lt;/head&gt;
    &lt;p&gt;We first sort the basic blocks into horizontal tracks called “layers”. This is very simple; we just start at layer 0 and recursively walk the graph, incrementing the layer number as we go. As we go, we track the “height” of each loop, not in pixels, but in layers.&lt;/p&gt;
    &lt;p&gt;We also take this opportunity to vertically position nodes “inside” and “outside” of loops. Whenever we see an edge that exits a loop, we defer the layering of the destination block until we are done layering the loop contents, at which point we know the loop’s height.&lt;/p&gt;
    &lt;p&gt;A note on implementation: nodes are visited multiple times throughout the process, not just once. This can produce a quadratic explosion for large graphs, but I find that an early-out is sufficient to avoid this problem in practice.&lt;/p&gt;
    &lt;p&gt;The animation below shows the layering algorithm in action. Notice how the final block in the graph is visited twice, once after each loop that branches to it, and in each case, the block is deferred until the entire loop has been layered, rather than processed immediately after its predecessor block. The final position of the block is below the entirety of both loops, rather than directly below one of its predecessors as Graphviz would do. (Remember, horizontal and vertical positions have not yet been computed; the positions of the blocks in this diagram are hardcoded for demonstration purposes.)&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=layering*/function layerBlock(block, layer = 0) {
  // Omitted for clarity: special handling of our "backedge blocks"

  // Early out if the block would not be updated
  if (layer &amp;lt;= block.layer) {
    return;
  }

  // Update the layer of the current block
  block.layer = Math.max(block.layer, layer);

  // Update the heights of all loops containing the current block
  let header = block.loopHeader;
  while (header) {
    header.loopHeight = Math.max(header.loopHeight, block.layer - header.layer + 1);
    header = header.parentLoopHeader;
  }

  // Recursively layer successors
  for (const succ of block.successors) {
    if (succ.loopDepth &amp;lt; block.loopDepth) {
      // Outgoing edges from the current loop will be layered later
      block.loopHeader.outgoingEdges.push(succ);
    } else {
      layerBlock(succ, layer + 1);
    }
  }

  // Layer any outgoing edges only after the contents of the loop have
  // been processed
  if (block.isLoopHeader()) {
    for (const succ of block.outgoingEdges) {
      layerBlock(succ, layer + block.loopHeight);
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 2: Create dummy nodes&lt;/head&gt;
    &lt;p&gt;Any time an edge crosses a layer, we create a dummy node. This allows edges to be routed across layers without overlapping any blocks. Unlike in traditional Sugiyama, we always put downward dummies on the left and upward dummies on the right, producing a consistent “counter-clockwise” flow. This also makes it easy to read long vertical edges, whose direction would otherwise be ambiguous. (Recall how the loop backedge flipped from the right to the left in the “unstable layout” Graphviz example from before.)&lt;/p&gt;
    &lt;p&gt;In addition, we coalesce any edges that are going to the same destination by merging their dummy nodes. This heavily reduces visual noise.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 3: Straighten edges&lt;/head&gt;
    &lt;p&gt;This is the fuzziest and most ad-hoc part of the process. Basically, we run lots of small passes that walk up and down the graph, aligning layout nodes with each other. Our edge-straightening passes include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pushing nodes to the right of their loop header to “indent” them.&lt;/item&gt;
      &lt;item&gt;Walking a layer left to right, moving children to the right to line up with their parents. If any nodes overlap as a result, they are pushed further to the right.&lt;/item&gt;
      &lt;item&gt;Walking a layer right to left, moving parents to the right to line up with their children. This version is more conservative and will not move a node if it would overlap with another. This cleans up most issues from the first pass.&lt;/item&gt;
      &lt;item&gt;Straightening runs of dummy nodes so we have clean vertical lines.&lt;/item&gt;
      &lt;item&gt;“Sucking in” dummy runs on the left side of the graph if there is room for them to move to the right.&lt;/item&gt;
      &lt;item&gt;Straighten out any edges that are “nearly straight”, according to a chosen threshold. This makes the graph appear less wobbly. We do this by repeatedly “combing” the graph upward and downward, aligning parents with children, then children with parents, and so on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is important to note that dummy nodes participate fully in this system. If for example you have two side-by-side loops, straightening the left loop’s backedge will push the right loop to the side, avoiding overlaps and preserving the graph’s visual structure.&lt;/p&gt;
    &lt;p&gt;We do not reach a fixed point with this strategy, nor do we attempt to. I find that if you continue to repeatedly apply these particular layout passes, nodes will wander to the right forever. Instead, the layout passes are hand-tuned to produce decent-looking results for most of the graphs we look at on a regular basis. That said, this could certainly be improved, especially for larger graphs which do benefit from more iterations.&lt;/p&gt;
    &lt;p&gt;At the end of this step, all nodes have a fixed X-coordinate and will not be modified further.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 4: Track horizontal edges&lt;/head&gt;
    &lt;p&gt;Edges may overlap visually as they run horizontally between layers. To resolve this, we sort edges into parallel “tracks”, giving each a vertical offset. After tracking all the edges, we record the total height of the tracks and store it on the preceding layer as its “track height”. This allows us to leave room for the edges in the final layout step.&lt;/p&gt;
    &lt;p&gt;We first sort edges by their starting position, left to right. This produces a consistent arrangement of edges that has few vertical crossings in practice. Edges are then placed into tracks from the “outside in”, stacking rightward edges on top and leftward edges on the bottom, creating a new track if the edge would overlap with or cross any other edge.&lt;/p&gt;
    &lt;p&gt;The diagram below is interactive. Click and drag the blocks to see how the horizontal edges get assigned to tracks.&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=tracks*/function trackHorizontalEdges(layer) {
  const TRACK_SPACING = 20;

  // Gather all edges on the layer, and sort left to right by starting coordinate
  const layerEdges = [];
  for (const node of layer.nodes) {
    for (const edge of node.edges) {
      layerEdges.push(edge);
    }
  }
  layerEdges.sort((a, b) =&amp;gt; a.startX - b.startX);

  // Assign edges to "tracks" based on whether they overlap horizontally with
  // each other. We walk the tracks from the outside in and stop if we ever
  // overlap with any other edge.
  const rightwardTracks = []; // [][]Edge
  const leftwardTracks = [];  // [][]Edge
  nextEdge:
  for (const edge of layerEdges) {
    const trackSet = edge.endX - edge.startX &amp;gt;= 0 ? rightwardTracks : leftwardTracks;
    let lastValidTrack = null; // []Edge | null

    // Iterate through the tracks in reverse order (outside in)
    for (let i = trackSet.length - 1; i &amp;gt;= 0; i--) {
      const track = trackSet[i];
      let overlapsWithAnyInThisTrack = false;
      for (const otherEdge of track) {
        if (edge.dst === otherEdge.dst) {
          // Assign the edge to this track to merge arrows
          track.push(edge);
          continue nextEdge;
        }

        const al = Math.min(edge.startX, edge.endX);
        const ar = Math.max(edge.startX, edge.endX);
        const bl = Math.min(otherEdge.startX, otherEdge.endX);
        const br = Math.max(otherEdge.startX, otherEdge.endX);
        const overlaps = ar &amp;gt;= bl &amp;amp;&amp;amp; al &amp;lt;= br;
        if (overlaps) {
          overlapsWithAnyInThisTrack = true;
          break;
        }
      }

      if (overlapsWithAnyInThisTrack) {
        break;
      } else {
        lastValidTrack = track;
      }
    }

    if (lastValidTrack) {
      lastValidTrack.push(edge);
    } else {
      trackSet.push([edge]);
    }
  }

  // Use track info to apply offsets to each edge for rendering.
  const tracksHeight = TRACK_SPACING * Math.max(
    0,
    rightwardTracks.length + leftwardTracks.length - 1,
  );
  let trackOffset = -tracksHeight / 2;
  for (const track of [...rightwardTracks.toReversed(), ...leftwardTracks]) {
    for (const edge of track) {
      edge.offset = trackOffset;
    }
    trackOffset += TRACK_SPACING;
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 5: Verticalize&lt;/head&gt;
    &lt;p&gt;Finally, we assign each node a Y-coordinate. Starting at a Y-coordinate of zero, we iterate through the layers, repeatedly adding the layer’s height and its track height, where the layer height is the maximum height of any node in the layer. All nodes within a layer receive the same Y-coordinate; this is simple and easier to read than Graphviz’s default of vertically centering nodes within a layer.&lt;/p&gt;
    &lt;p&gt;Now that every node has both an X and Y coordinate, the layout process is complete.&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=verticalize*/function verticalize(layers) {
  let layerY = 0;
  for (const layer of layers) {
    let layerHeight = 0;
    for (const node of layer.nodes) {
      node.y = layerY;
      layerHeight = Math.max(layerHeight, node.height);
    }
    layerY += layerHeight;
    layerY += layer.trackHeight;
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 6: Render&lt;/head&gt;
    &lt;p&gt;The details of rendering are out of scope for this article, and depend on the specific application. However, I wish to highlight a stylistic decision that I feel makes our graphs more readable.&lt;/p&gt;
    &lt;p&gt;When rendering edges, we use a style inspired by railroad diagrams. These have many advantages over the Bézier curves employed by Graphviz. First, straight lines feel more organized and are easier to follow when scrolling up and down. Second, they are easy to route (vertical when crossing layers, horizontal between layers). Third, they are easy to coalesce when they share a destination, and the junctions provide a clear indication of the edge’s direction. Fourth, they always cross at right angles, improving clarity and reducing the need to avoid edge crossings in the first place.&lt;/p&gt;
    &lt;p&gt;Consider the following example. There are several edge crossings that may traditionally be considered undesirable—yet the edges and their directions remain clear. Of particular note is the vertical junction highlighted in red on the left: not only is it immediately clear that these edges share a destination, but the junction itself signals that the edges are flowing downward. I find this much more pleasant than the “rat’s nest” that Graphviz tends to produce.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this work?&lt;/head&gt;
    &lt;p&gt;It may seem surprising that such a simple (and stupid) layout algorithm could produce such readable graphs, when more sophisticated layout algorithms struggle. However, I feel that the algorithm succeeds because of its simplicity.&lt;/p&gt;
    &lt;p&gt;Most graph layout algorithms are optimization problems, where error is minimized on some chosen metrics. However, these metrics seem to correlate poorly to readability in practice. For example, it seems good in theory to rearrange nodes to minimize edge crossings. But a predictable order of nodes seems to produce more sensible results overall, and simple rules for edge routing are sufficient to keep things tidy. (As a bonus, this also gives us layout stability from pass to pass.) Similarly, layout rules like “align parents with their children” produce more readable results than “minimize the lengths of edges”.&lt;/p&gt;
    &lt;p&gt;Furthermore, by rejecting the optimization problem, a human author gains more control over the layout. We are able to position nodes “inside” of loops, and push post-loop content down in the graph, because we reject this global constraint-solver approach. Minimizing “error” is meaningless compared to a human maximizing meaning through thoughtful design.&lt;/p&gt;
    &lt;p&gt;And finally, the resulting algorithm is simply more efficient. All the layout passes in iongraph are easy to program and scale gracefully to large graphs because they run in roughly linear time. It is better, in my view, to run a fixed number of layout iterations according to your graph complexity and time budget, rather than to run a complex constraint solver until it is “done”.&lt;/p&gt;
    &lt;p&gt;By following this philosophy, even the worst graphs become tractable. Below is a screenshot of a zlib function, compiled to WebAssembly, and rendered using the old tool.&lt;/p&gt;
    &lt;p&gt;It took about ten minutes for Graphviz to produce this spaghetti nightmare. By comparison, iongraph can now lay out this function in 20 milliseconds. The result is still not particularly beautiful, but it renders thousands of times faster and is much easier to navigate.&lt;/p&gt;
    &lt;p&gt;Perhaps programmers ought to put less trust into magic optimizing systems, especially when a human-friendly result is the goal. Simple (and stupid) algorithms can be very effective when applied with discretion and taste.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;We have already integrated iongraph into the Firefox profiler, making it easy for us to view the graphs of the most expensive or impactful functions we find in our performance work. Unfortunately, this is only available in specific builds of the SpiderMonkey shell, and is not available in full browser builds. This is due to architectural differences in how profiling data is captured and the flags with which the browser and shell are built. I would love for Firefox users to someday be able to view these graphs themselves, but at the moment we have no plans to expose this to the browser. However, one bug tracking some related work can be found here.&lt;/p&gt;
    &lt;p&gt;We will continue to sporadically update iongraph with more features to aid us in our work. We have several ideas for new features, including richer navigation, search, and visualization of register allocation info. However, we have no explicit roadmap for when these features may be released.&lt;/p&gt;
    &lt;p&gt;To experiment with iongraph locally, you can run a debug build of the SpiderMonkey shell with &lt;code&gt;IONFLAGS=logs&lt;/code&gt;; this will dump information to &lt;code&gt;/tmp/ion.json&lt;/code&gt;. This file can then be loaded into the standalone deployment of iongraph. Please be aware that the user experience is rough and unpolished in its current state.&lt;/p&gt;
    &lt;p&gt;The source code for iongraph can be found on GitHub. If this subject interests you, we would welcome contributions to iongraph and its integration into the browser. The best place to reach us is our Matrix chat.&lt;/p&gt;
    &lt;p&gt;Thanks to Matthew Gaudet, Asaf Gartner, and Colin Davidson for their feedback on this article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spidermonkey.dev/blog/2025/10/28/iongraph-web.html"/><published>2025-10-29T05:17:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45744395</id><title>SpiderMonkey Garbage Collector</title><updated>2025-10-29T16:13:42.370251+00:00</updated><content>&lt;doc fingerprint="b7f969cd84f696d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SpiderMonkey garbage collector&lt;/head&gt;
    &lt;p&gt;The SpiderMonkey garbage collector is responsible for allocating memory representing JavaScript data structures and deallocating them when they are no longer in use. It aims to collect as much data as possible in as little time as possible. As well as JavaScript data it is also used to allocate some internal SpiderMonkey data structures.&lt;/p&gt;
    &lt;p&gt;The garbage collector is a hybrid tracing collector, and has the following features:&lt;/p&gt;
    &lt;p&gt;For an overview of garbage collection see: https://en.wikipedia.org/wiki/Tracing_garbage_collection&lt;/p&gt;
    &lt;head rend="h2"&gt;Description of features&lt;/head&gt;
    &lt;head rend="h3"&gt;Precise collection&lt;/head&gt;
    &lt;p&gt;The GC is ‘precise’ in that it knows the layout of allocations (which is used to determine reachable children) and also the location of all stack roots. This means it does not need to resort to conservative techniques that may cause garbage to be retained unnecessarily.&lt;/p&gt;
    &lt;p&gt;Knowledge of the stack is achieved with C++ wrapper classes that must be used for stack roots and handles (pointers) to them. This is enforced by the SpiderMonkey API (which operates in terms of these types) and checked by a static analysis that reports places when unrooted GC pointers can be present when a GC could occur.&lt;/p&gt;
    &lt;p&gt;For details of stack rooting, see: https://github.com/mozilla-spidermonkey/spidermonkey-embedding-examples/blob/esr78/docs/GC%20Rooting%20Guide.md&lt;/p&gt;
    &lt;p&gt;We also have a static analysis for detecting errors in rooting. It can be run locally or in CI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Incremental collection&lt;/head&gt;
    &lt;p&gt;‘Stop the world’ collectors run a whole collection in one go, which can result in unacceptable pauses for users. An incremental collector breaks its execution into a number of small slices, reducing user impact.&lt;/p&gt;
    &lt;p&gt;As far as possible the SpiderMonkey collector runs incrementally. Not all parts of a collection can be performed incrementally however as there are some operations that need to complete atomically with respect to the rest of the program.&lt;/p&gt;
    &lt;p&gt;Currently, most of the collection is performed incrementally. Root marking, compacting, and an initial part of sweeping are not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Generational collection&lt;/head&gt;
    &lt;p&gt;Most real world allocations either die very quickly or live for a long time. This suggests an approach to collection where allocations are moved between ‘generations’ (separate heaps) depending on how long they have survived. Generations containing young allocations are fast to collect and can be collected more frequently; older generations are collected less often.&lt;/p&gt;
    &lt;p&gt;The SpiderMonkey collector implements a single young generation (the nursery) and a single old generation (the tenured heap). Collecting the nursery is known as a minor GC as opposed to a major GC that collects the whole heap (including the nursery).&lt;/p&gt;
    &lt;head rend="h3"&gt;Concurrent collection&lt;/head&gt;
    &lt;p&gt;Many systems have more than one CPU and therefore can benefit from offloading GC work to another core. In GC terms ‘concurrent’ usually refers to GC work happening while the main program continues to run.&lt;/p&gt;
    &lt;p&gt;The SpiderMonkey collector currently only uses concurrency in limited phases.&lt;/p&gt;
    &lt;p&gt;This includes most finalization work (there are some restrictions as not all finalization code can tolerate this) and some other aspects such as allocating and decommitting blocks of memory.&lt;/p&gt;
    &lt;p&gt;Performing marking work concurrently is currently being investigated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallel collection&lt;/head&gt;
    &lt;p&gt;In GC terms ‘parallel’ usually means work performed in parallel while the collector is running, as opposed to the main program itself. The SpiderMonkey collector performs work within GC slices in parallel wherever possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compacting collection&lt;/head&gt;
    &lt;p&gt;The collector allocates data with the same type and size in ‘arenas’ (often know as slabs). After many allocations have died this can leave many arenas containing free space (external fragmentation). Compacting remedies this by moving allocations between arenas to free up as much memory as possible.&lt;/p&gt;
    &lt;p&gt;Compacting involves tracing the entire heap to update pointers to moved data and is not incremental so it only happens rarely, or in response to memory pressure notifications.&lt;/p&gt;
    &lt;head rend="h3"&gt;Partitioned heap&lt;/head&gt;
    &lt;p&gt;The collector has the concept of ‘zones’ which are separate heaps which can be collected independently. Objects in different zones can refer to each other however.&lt;/p&gt;
    &lt;p&gt;Zones are also used to help incrementalize parts of the collection. For example, compacting is not fully incremental but can be performed one zone at a time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other documentation&lt;/head&gt;
    &lt;p&gt;More details about the Garbage Collector (GC) can be found by looking for the [SMDOC] Garbage Collector comment in the sources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://firefox-source-docs.mozilla.org/js/gc.html"/><published>2025-10-29T09:06:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745281</id><title>AWS to bare metal two years later: Answering your questions about leaving AWS</title><updated>2025-10-29T16:13:41.685019+00:00</updated><content>&lt;doc fingerprint="ff74527664b5afab"&gt;
  &lt;main&gt;
    &lt;p&gt;When we published How moving from AWS to Bare-Metal saved us $230,000 /yr. in 2023, the story travelled far beyond our usual readership. The discussion threads on Hacker News and Reddit were packed with sharp questions: did we skip Reserved Instances, how do we fail over a single rack, what about the people cost, and when is cloud still the better answer? This follow-up is our long-form reply.&lt;/p&gt;
    &lt;p&gt;Over the last twenty-four months we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ran the MicroK8s + Ceph stack in production for 730+ days with 99.993% measured availability.&lt;/item&gt;
      &lt;item&gt;Added a second rack in Frankfurt, joined to our primary Paris cage over redundant DWDM, to kill the “single rack” concern.&lt;/item&gt;
      &lt;item&gt;Cut average customer-facing latency by 19% thanks to local NVMe and eliminating noisy neighbours.&lt;/item&gt;
      &lt;item&gt;Reinvested the savings into buying bare metal AI servers to expand LLM-based alert / incident summarisation and auto code fixes based on log / traces and metrics in OneUptime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Below we tackle the recurring themes from the community feedback, complete with the numbers we use internally.&lt;/p&gt;
    &lt;head rend="h2"&gt;$230,000 / yr savings? That is just an engineers salary.&lt;/head&gt;
    &lt;p&gt;In the US, it is. In the rest of the world. That's 2-5x engineers salary. We used to save $230,000 / yr but now the savings have exponentially grown. We now save over $1.2M / yr and we expect this to grow, as we grow as a business.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Why not just buy Savings Plans or Reserved Instances?”&lt;/head&gt;
    &lt;p&gt;We tried. Long answer: the maths still favoured bare metal once we priced everything in. We see a savings of over 76% if you compare our bare metal setup to AWS.&lt;/p&gt;
    &lt;p&gt;A few clarifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Savings Plans do not reduce S3, egress, or Direct Connect. 37% off instances still leaves you paying list price for bandwidth, which was 22% of our AWS bill.&lt;/item&gt;
      &lt;item&gt;EKS had an extra $1,260/month control-plane fee plus $600/month for NAT gateways. Those costs disappear once you run Kubernetes yourself.&lt;/item&gt;
      &lt;item&gt;Our workload is 24/7 steady. We were already at &amp;gt;90% reservation coverage; there was no idle burst capacity to “right size” away. If we had the kind of bursty compute profile many commenters referenced, the choice would be different.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;“How much did migration and ongoing ops really cost?”&lt;/head&gt;
    &lt;p&gt;We spent a week of engineers time (and that is the worst case estimate) on the initial migration, spread across SRE, platform, and database owners. Most of that time was work we needed anyway—formalising infrastructure-as-code, smoke testing charts, tightening backup policies. The incremental work that existed purely because of bare metal was roughly one week.&lt;/p&gt;
    &lt;p&gt;Ongoing run-cost looks like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hands-on keyboard: ~24 engineer-hours/quarter across the entire platform team, including routine patching and firmware updates. That is comparable to the AWS time we used to burn on cost optimisation, IAM policy churn, and chasing deprecations and updating our VM's on AWS.&lt;/item&gt;
      &lt;item&gt;Remote hands: 2 interventions in 24 months (mainly disks). Mean response time: 27 minutes. We do not staff an on-site team. We rely on co-location provider to physically manage our rack. This means no traditional hardware admins.&lt;/item&gt;
      &lt;item&gt;Automation: We're now moving to Talos. We PXE boot with Tinkerbell, image with Talos, manage configs through Flux and Terraform, and run conformance suites before each Kubernetes upgrade. All of those tools also hardened our AWS estate, so they were not net-new effort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The opportunity cost question from is fair. We track it the same way we track feature velocity: did the infra team ship less? The answer was “no”—our release cadence increased because we reclaimed few hours/month we used to spend in AWS “cost council” meetings.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Isn’t a single rack a single point of failure?”&lt;/head&gt;
    &lt;p&gt;We have multiple racks across two different DC / providers. We:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leased a secondary quarter rack in Frankfurt with a different provider and power utility.&lt;/item&gt;
      &lt;item&gt;Currently: Deployed a second MicroK8s control plane, mirrored Ceph pools with asynchronous replication. Future: We're moving to Talos. Nothing against Microk8s, but we like the Talos way of managing the k8s cluster.&lt;/item&gt;
      &lt;item&gt;Added isolated out-of-band management paths (4G / satellite) so we can reach the gear even during metro fibre events.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AWS failover cluster we mentioned in 2023 still exists. We rehearse a full cutover quarterly using the same Helm releases we ship to customers. DNS failover remains the slowest leg (resolver caches can ignore TTL), so we added Anycast ingress via BGP with our transit provider to cut traffic shifting to sub-minute.&lt;/p&gt;
    &lt;head rend="h2"&gt;“What about hardware lifecycle and surprise CapEx?”&lt;/head&gt;
    &lt;p&gt;We amortise servers over five years, but we sized them with 2 × AMD EPYC 9654 CPUs, 1 TB RAM, and NVMe sleds. At our current growth rate the boxes will hit CPU saturation before we hit year five. When that happens, the plan is to cascade the older gear into our regional analytics cluster (we use Posthog + Metabase for this) and buy a new batch. Thanks to the savings delta, we can refresh 40% of the fleet every 24 months and still spend less annually than the optimised AWS bill above.&lt;/p&gt;
    &lt;p&gt;We also buy extended warranties from the OEM (Supermicro) and keep three cold spares in the cage. The hardware lasts 7-8 years and not 5, but we wtill count it as 5 to be very conservative.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Are you reinventing managed services?”&lt;/head&gt;
    &lt;p&gt;Another strong Reddit critique: why rebuild services AWS already offers? Three reasons we are comfortable with the trade:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Portability is part of our product promise. OneUptime customers self-host in their own environments. Running the same open stack we ship (Postgres, Redis, ClickHouse, etc.) keeps us honest. We eun on Kubernetes and self-hosted customers run on Kubernetes as well.&lt;/item&gt;
      &lt;item&gt;Tooling maturity. Two years ago we relied on Terraform + EKS + RDS. Today we run MicroK8s (Talos in the future), Argo Rollouts, OpenTelemetry Collector, and Ceph dashboards. None of that is bespoke. We do not maintain a fork of anything.&lt;/item&gt;
      &lt;item&gt;Selective cloud use. We still pay AWS for Glacier backups, CloudFront for edge caching, and short-lived burst capacity for load tests. Cloud makes sense when elasticity matters; bare metal wins when baseload dominates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Managed services are phenomenal when you are short on expertise or need features beyond commodity compute. If we were all-in on DynamoDB streams or Step Functions we would almost certainly still be on AWS.&lt;/p&gt;
    &lt;head rend="h2"&gt;“How do bandwidth and DoS scenarios work now?”&lt;/head&gt;
    &lt;p&gt;We committed to 5 Gbps 95th percentile across two carriers. The same traffic on AWS egress would be 8x expensive in eu-west-1. For DDoS protection we front our ingress with Cloudflare.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Has reliability suffered?”&lt;/head&gt;
    &lt;p&gt;Short answer: No. Infact it was better than AWS (compared to recent AWS downtimes)&lt;/p&gt;
    &lt;p&gt;We have 730+ days with 99.993% measured availability and we also escaped AWS region wide downtime that happened a week ago.&lt;/p&gt;
    &lt;head rend="h2"&gt;“How do audits and compliance work off-cloud now?”&lt;/head&gt;
    &lt;p&gt;We stayed SOC 2 Type II and ISO 27001 certified through the transition. The biggest deltas auditors cared about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physical controls: We provide badge logs from the colo, camera footage on request, and quarterly access reviews. The colo already meets Tier III redundancy, so their reports roll into ours.&lt;/item&gt;
      &lt;item&gt;Change management: Terraform plans, and now Talos machine configs give us immutable evidence of change. Auditors liked that more than AWS Console screenshots.&lt;/item&gt;
      &lt;item&gt;Business continuity: We prove failover by moving workload to other DC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are in a regulated space (HIPAA for instance), expect the paperwork to grow a little. We worked it in by leaning on the colo providers’ standard compliance packets—they slotted straight into our risk register.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Why not stay in the cloud but switch providers?”&lt;/head&gt;
    &lt;p&gt;We priced Hetzner, OVH, Leaseweb, Equinix Metal, and AWS Outposts. The short version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hyperscaler alternatives were cheaper on compute but still expensive on egress once you hit petabytes/month. Outposts also carried minimum commits that exceeded our needs.&lt;/item&gt;
      &lt;item&gt;European dedicated hosts (Hetzner, OVH) are fantastic for lab clusters. The challenge was multi-100 TB Ceph clusters with redundant uplinks and smart-hands SLAs. Once we priced that tier, the savings narrowed.&lt;/item&gt;
      &lt;item&gt;Equinix Metal got the closest, but bare metal on-demand still carried a 25-30% premium over our CapEx plan. Their global footprint is tempting; we may still use them for short-lived expansion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Owning the hardware also let us plan power density (we run 15 kW racks) and reuse components. For our steady-state footprint, colocation won by a long shot.&lt;/p&gt;
    &lt;head rend="h2"&gt;“What does day-to-day toil look like now?”&lt;/head&gt;
    &lt;p&gt;We put real numbers to it because Reddit kept us honest:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Weekly: Kernel and firmware patches (Talos makes this a redeploy), Ceph health checks, Total time averages 1 hour/week on average over months.&lt;/item&gt;
      &lt;item&gt;Monthly: Kubernetes control plane upgrades in canary fashion. About 2 engineer-hours. We expect this to reduce when Talos kicks in.&lt;/item&gt;
      &lt;item&gt;Quarterly: Disaster recovery drills, capacity planning, and contract audits with carriers. Roughly 12 hours across three engineers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total toil is ~14 engineer-hours/month, including prep. The AWS era had us spending similar time but on different work: chasing cost anomalies, expanding Security Hub exceptions, and mapping breaking changes in managed services. The toil moved; it did not multiply.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Do you still use the cloud for anything substantial?”&lt;/head&gt;
    &lt;p&gt;Absolutely. Cloud still solves problems we would rather not own:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Glacier keeps long-term log archives at a price point local object storage cannot match.&lt;/item&gt;
      &lt;item&gt;CloudFront handles 14 edge PoPs we do not want to build. We terminate TLS at the edge for marketing assets and docs. We will soon move this to Cloudflare as they are cheaper.&lt;/item&gt;
      &lt;item&gt;We spin up short-lived AWS environments for load testing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So yes, we left AWS for the base workload, but we still swipe the corporate card when elasticity or geography outweighs fixed-cost savings.&lt;/p&gt;
    &lt;head rend="h2"&gt;When the cloud is still the right answer&lt;/head&gt;
    &lt;p&gt;It depends on your workload. We still recommend staying put if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your usage pattern is spiky or seasonal and you can auto-scale to near zero between peaks.&lt;/item&gt;
      &lt;item&gt;You lean heavily on managed services (Aurora Serverless, Kinesis, Step Functions) where the operational load is the value prop.&lt;/item&gt;
      &lt;item&gt;You do not have the appetite to build a platform team comfortable with Kubernetes, Ceph, observability, and incident response.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cloud-first was the right call for our first five years. Bare metal became the right call once our compute footprint, data gravity, and independence requirements stabilised.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is next&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are working on a detailed runbook + Terraform module to help teams do capex forecasting for colo moves. Expect that on the blog later this year.&lt;/item&gt;
      &lt;item&gt;A deep dive on Talos is in the queue, as requested by multiple folks in the HN thread.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Questions we did not cover? Let us know in the discussion threads—we are happy to keep sharing the gritty details.&lt;/p&gt;
    &lt;p&gt;Related Reading:&lt;/p&gt;
    &lt;head rend="h3"&gt;Neel Patel&lt;/head&gt;
    &lt;p&gt;@devneelpatel • Oct 29, 2025 •&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view"/><published>2025-10-29T11:14:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745566</id><title>Show HN: Learn German with Games</title><updated>2025-10-29T16:13:40.869399+00:00</updated><content>&lt;doc fingerprint="2ce7b8cb946d6548"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Choose Your Learning Adventure&lt;/head&gt;
    &lt;p&gt;Select a game below and start mastering German in an engaging, interactive way!&lt;/p&gt;
    &lt;head rend="h3"&gt;Numbers to Words Game&lt;/head&gt;
    &lt;p&gt;See a number and type the German word - perfect for learning German number vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;Words to Numbers Game&lt;/head&gt;
    &lt;p&gt;Practice recognizing German number words and converting them to digits&lt;/p&gt;
    &lt;head rend="h3"&gt;German Time Game&lt;/head&gt;
    &lt;p&gt;Learn to tell time in German by reading analog clocks and typing time expressions&lt;/p&gt;
    &lt;head rend="h3"&gt;Time Short Form Game&lt;/head&gt;
    &lt;p&gt;Practice German time with short forms: nach, vor, halb, viertel, and punkt&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Artikel&lt;/head&gt;
    &lt;p&gt;Master German artikels (der, die, das) by guessing the correct artikel for each noun&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Word&lt;/head&gt;
    &lt;p&gt;Translate German nouns to English - see a German word with its article and type the English meaning&lt;/p&gt;
    &lt;head rend="h3"&gt;English Nouns to German&lt;/head&gt;
    &lt;p&gt;See an English word and type the German translation with its artikel&lt;/p&gt;
    &lt;head rend="h3"&gt;Verb Conjugation&lt;/head&gt;
    &lt;p&gt;Practice conjugating German verbs in present tense for all persons - ich, du, er/sie/es, wir, ihr, sie/Sie&lt;/p&gt;
    &lt;head rend="h3"&gt;German Verbs to English&lt;/head&gt;
    &lt;p&gt;See a German verb and type its English meaning - perfect for building vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;English Verbs to German&lt;/head&gt;
    &lt;p&gt;See an English verb meaning and type the German infinitive form - reverse translation practice&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.learngermanwithgames.com/"/><published>2025-10-29T11:50:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745995</id><title>Berkeley Out-of-Order RISC-V Processor (Boom) (2020)</title><updated>2025-10-29T16:13:40.562902+00:00</updated><content>&lt;doc fingerprint="f35afe91c75d4326"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Berkeley Out-of-Order Machine (BOOM)Â¶&lt;/head&gt;
    &lt;p&gt;The Berkeley Out-of-Order Machine (BOOM) is heavily inspired by the MIPS R10000 [1] and the Alpha 21264 [2] outâofâorder processors. Like the MIPS R10000 and the Alpha 21264, BOOM is a unified physical register file design (also known as âexplicit register renamingâ).&lt;/p&gt;
    &lt;p&gt;BOOM implements the open-source RISC-V ISA and utilizes the Chisel hardware construction language to construct generator for the core. A generator can be thought of a generialized RTL design. A standard RTL design can be viewed as a single instance of a generator design. Thus, BOOM is a family of out-of-order designs rather than a single instance of a core. Additionally, to build an SoC with a BOOM core, BOOM utilizes the Rocket Chip SoC generator as a library to reuse different micro-architecture structures (TLBs, PTWs, etc).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;Yeager, Kenneth C. âThe MIPS R10000 superscalar microprocessor.â IEEE micro 16.2 (1996): 28-41.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[2]&lt;/cell&gt;
        &lt;cell&gt;Kessler, Richard E. âThe alpha 21264 microprocessor.â IEEE micro 19.2 (1999): 24-36.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.boom-core.org/en/latest/sections/intro-overview/boom.html"/><published>2025-10-29T12:33:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746401</id><title>Grammarly rebrands to 'Superhuman,' launches a new AI assistant</title><updated>2025-10-29T16:13:40.373894+00:00</updated><content>&lt;doc fingerprint="be5e8c1381fae8ec"&gt;
  &lt;main&gt;
    &lt;p&gt;Typically, when a company acquires another, it will absorb the new company’s branding or integrate it with its own identity. Grammarly is doing something different: After acquiring email client Superhuman in July, the company is renaming itself “Superhuman.”&lt;/p&gt;
    &lt;p&gt;Despite the branding change, Grammarly, the product, will continue to be known as it has. However, the company says it is thinking about rebranding products like Coda, a productivity platform it acquired last year, in the long run.&lt;/p&gt;
    &lt;p&gt;The company is also launching an AI assistant called Superhuman Go that’s built into Grammarly’s existing extension. The assistant can provide writing suggestions, give feedback on emails, and you can even connect it with other apps like Jira, Gmail, Google Drive and Google Calendar to arm it with more context. The assistant can use these connections to do tasks like logging tickets or fetching your availability when you’re scheduling a meeting.&lt;/p&gt;
    &lt;p&gt;Superhuman said it plans to add functionality to enable the assistant to fetch data from sources like CRMs and internal systems to suggest changes to your emails.&lt;/p&gt;
    &lt;p&gt;Users can try Superhuman Go by turning on a toggle in the Grammarly extension, which will let them connect it to different apps. Users can also try out different agents in the company’s agent store, which include a plagiarism checker and a proofreader, launched in August.&lt;/p&gt;
    &lt;p&gt;All Grammarly users can try out Superhuman Go right now, though the company is also selling product bundles. Its Pro subscription plan will cost $12 per month (billed annually) and will enable grammar and tone support in multiple languages. The Business plan will cost $33 per month (billed annually) and will give users access to Superhuman Mail.&lt;/p&gt;
    &lt;p&gt;Superhuman said it also wants to add more AI-powered features to the Coda document suite and Superhuman email clients, such as fetching details from external and internal sources to create additional details in documents and email drafts automatically.&lt;/p&gt;
    &lt;head rend="h3"&gt;TechCrunch Disrupt is live!&lt;/head&gt;
    &lt;head rend="h4"&gt;Join Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla — some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don’t miss 300+ showcasing startups in all sectors.&lt;lb/&gt;Register now and save 50% on your pass.&lt;/head&gt;
    &lt;head rend="h3"&gt;2-FOR-1 DISCOUNT: Bring a +1 and save 60%&lt;/head&gt;
    &lt;head rend="h4"&gt;Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla — some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don’t miss 300+ showcasing startups in all sectors. Bring a +1 and save 60% on their pass, or get your pass by Oct 27 to save up to $444.&lt;/head&gt;
    &lt;p&gt;Grammarly has for the past few years made a concerted effort to increase its viability as a productivity suite, exemplified through its acquisitions of Coda and Superhuman. With this AI assistant, the company is positioning itself to compete better with the likes of Notion, ClickUp and Google Workspace, which have launched multiple AI-powered features in the past few years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2025/10/29/grammarly-rebrands-to-superhuman-launches-a-new-ai-assistant/"/><published>2025-10-29T13:12:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746478</id><title>From VS Code to Helix</title><updated>2025-10-29T16:13:39.775687+00:00</updated><content>&lt;doc fingerprint="7708325733a03a9d"&gt;
  &lt;main&gt;
    &lt;p&gt;I created the website you’re reading with VS Code. Behind the scenes I use Astro, a static site generator that gets out of the way while providing nice conveniences.&lt;/p&gt;
    &lt;p&gt;Using VS Code was a no-brainer: everyone in the industry seems to at least be familiar with it, every project can be opened with it, and most projects can get enhancements and syntactic helpers in a few clicks. In short: VS Code is free, easy to use, and widely adopted.&lt;/p&gt;
    &lt;p&gt;A Rustacean colleague kept singing Helix’s praises. I discarded it because he’s much smarter than I am, and I only ever use vim when I need to fiddle with files on a server. I like when things “Just Work” and didn’t want to bother learning how to use Helix nor how to configure it.&lt;/p&gt;
    &lt;p&gt;Today it has become my daily driver. Why did I change my mind? What was preventing me from using it before? And how difficult was it to get there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Automation is a double-edged sword&lt;/head&gt;
    &lt;p&gt;Automation and technology make work easier, this is why we produce technology in the first place. But it also means you grow more dependent on the tech you use. If the tech is produced transparently by an international team or a team you trust, it’s fine. But if it’s produced by a single large entity that can screw you over, it’s dangerous.&lt;/p&gt;
    &lt;p&gt;VS Code might be open source, but in practice it’s produced by Microsoft. Microsoft has a problematic relationship to consent and is shoving AI products down everyone’s throat. I’d rather use tools that respect me and my decisions, and I’d rather not get my tools produced by already monopolistic organizations.&lt;/p&gt;
    &lt;p&gt;Microsoft is also based in the USA, and the political climate over there makes me want to depend as little as possible on American tools. I know that’s a long, uphill battle, but we have to start somewhere.&lt;/p&gt;
    &lt;p&gt;I’m not advocating for a ban against American tech in general, but for more balance in our supply chain. I’m also not advocating for European tech either: I’d rather get open source tools from international teams competing in a race to the top, rather than from teams in a single jurisdiction. What is happening in the USA could happen in Europe too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I feared using Helix&lt;/head&gt;
    &lt;p&gt;I’ve never found vim particularly pleasant to use but it’s everywhere, so I figured I might just get used to it. But one of the things I never liked about vim is the number of moving pieces. By default, vim and neovim are very bare bones. They can be extended and completely modified with plugins, but I really don’t like the idea of having extremely customize tools.&lt;/p&gt;
    &lt;p&gt;I’d rather have the same editor as everyone else, with a few knobs for minor preferences. I am subject to choice paralysis, so making me configure an editor before I’ve even started editing is the best way to tank my productivity.&lt;/p&gt;
    &lt;p&gt;When my colleague told me about Helix, two things struck me as improvements over vim.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Helix’s philosophy is that everything should work out of the box. There are a few configs and themes, but everything should work similarly from one Helix to another. All the language-specific logic is handled in Language Servers that implement the Language Server Protocol standard.&lt;/item&gt;
      &lt;item&gt;In Helix, first you select text, and then you perform operations onto it. So you can visually tell what is going to be changed before you apply the change. It fits my mental model much better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But there are major drawbacks to Helix too:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;After decades of vim, I was scared to re-learn everything. In practice this wasn’t a problem at all because of the very visual way Helix works.&lt;/item&gt;
      &lt;item&gt;VS Code “Just Works”, and Helix sounded like more work than the few clicks from VS Code’s extension store. This is true, but not as bad as I had anticipated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After a single week of usage, Helix was already very comfortable to navigate. After a few weeks, most of the wrinkles have been ironed out and I use it as my primary editor. So how did I overcome those fears?&lt;/p&gt;
    &lt;head rend="h2"&gt;What Helped&lt;/head&gt;
    &lt;head rend="h3"&gt;Just Do It&lt;/head&gt;
    &lt;p&gt;I tried Helix. It can sound silly, but the very first step to get into Helix was not to overthink it. I just installed it on my mac with &lt;code&gt;brew install helix&lt;/code&gt; and gave it a go. I was not too familiar with it, so I looked up the official documentation and noticed there was a tutorial.&lt;/p&gt;
    &lt;p&gt;This tutorial alone is what convinced me to try harder. It’s an interactive and well written way to learn how to move and perform basic operations in Helix. I quickly learned how to move around, select things, surround them with braces or parenthesis. I could see what I was about to do before doing it. This has been epiphany. Helix just worked the way I wanted.&lt;/p&gt;
    &lt;p&gt;Better: I could get things done faster than in VS Code after a few minutes of learning. Being a lazy person, I never bothered looking up VS Code shortcuts. Because the learning curve for Helix is slightly steeper, you have to learn those shortcuts that make moving around feel so easy.&lt;/p&gt;
    &lt;p&gt;Not only did I quickly get used to Helix key bindings: my vim muscle-memory didn’t get in the way at all!&lt;/p&gt;
    &lt;head rend="h3"&gt;Better docs&lt;/head&gt;
    &lt;p&gt;The built-in tutorial is a very pragmatic way to get started. You get results fast, you learn hands on, and it’s not that long. But if you want to go further, you have to look for docs. Helix has officials docs. They seem to be fairly complete, but they’re also impenetrable as a new user. They focus on what the editor supports and not on what I will want to do with it.&lt;/p&gt;
    &lt;p&gt;After a bit of browsing online, I’ve stumbled upon this third-party documentation website. The domain didn’t inspire me a lot of confidence, but the docs are really good. They are clearly laid out, use-case oriented, and they make the most of Astro Starlight to provide a great reading experience. The author tried to upstream these docs, but that won’t happen. It looks like they are upstreaming their docs to the current website. I hope this will improve the quality of upstream docs eventually.&lt;/p&gt;
    &lt;p&gt;After learning the basics and finding my way through the docs, it was time to ensure Helix was set up to help me where I needed it most.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the most of Markdown and Astro in Helix&lt;/head&gt;
    &lt;p&gt;In my free time, I mostly use my editor for three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write notes in markdown&lt;/item&gt;
      &lt;item&gt;Tweak my website with Astro&lt;/item&gt;
      &lt;item&gt;Edit yaml to faff around my Kubernetes cluster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Helix is a “stupid” text editor. It doesn’t know much about what you’re typing. But it supports Language Servers that implement the Language Server Protocol. Language Servers understand the document you’re editing. They explain to Helix what you’re editing, whether you’re in a TypeScript function, typing a markdown link, etc. With that information, Helix and the Language Server can provide code completion hints, errors &amp;amp; warnings, and easier navigation in your code.&lt;/p&gt;
    &lt;p&gt;In addition to Language Servers, Helix also supports plugging code formatters. Those are pieces of software that will read the document and ensure that it is consistently formatted. It will check that all indentations use spaces and not tabs, that there is a consistent number of space when indenting, that brackets are on the same line as the function, etc. In short: it will make the code pretty.&lt;/p&gt;
    &lt;head rend="h3"&gt;Markdown&lt;/head&gt;
    &lt;p&gt;Markdown is not really a programming language, so it might seem surprising to configure a Language Server for it. But if you remember what we said earlier, Language Servers can provide code completion, which is useful when creating links for example. Marksman does exactly that!&lt;/p&gt;
    &lt;p&gt;Since Helix is pre-configured to use marksman for markdown files we only need to install marksman and make sure it’s in our &lt;code&gt;PATH&lt;/code&gt;. Installing it with homebrew is enough.&lt;/p&gt;
    &lt;p&gt;We can check that Helix is happy with it with the following command&lt;/p&gt;
    &lt;p&gt;But Language Servers can also help Helix display errors and warnings, and “code suggestions” to help fix the issues. It means Language Servers are a perfect fit for… grammar checkers! Several grammar checkers exist. The most notable are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LTEX+, the Language Server used by Language Tool. It supports several languages must is quite resource hungry.&lt;/item&gt;
      &lt;item&gt;Harper, a grammar checker Language Server developed by Automattic, the people behind WordPress, Tumblr, WooCommerce, Beeper and more. Harper only support English and its variants, but they intend to support more languages in the future.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I mostly write in English and want to keep a minimalistic setup. Automattic is well funded, and I’m confident they will keep working on Harper to improve it. Since grammar checker LSPs can easily be changed, I’ve decided to go with Harper for now.&lt;/p&gt;
    &lt;p&gt;To install it, homebrew does the job as always:&lt;/p&gt;
    &lt;p&gt;Then I edited my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to add Harper as a secondary Language Server in addition to marksman&lt;/p&gt;
    &lt;p&gt;Finally I can add a markdown linter to ensure my markdown is formatted properly. Several options exist, and markdownlint is one of the most popular. My colleagues recommended the new kid on the block, a Blazing Fast equivalent: rumdl.&lt;/p&gt;
    &lt;p&gt;Installing rumdl was pretty simple on my mac. I only had to add the repository of the maintainer, and install rumdl from it.&lt;/p&gt;
    &lt;p&gt;After that I added a new &lt;code&gt;language-server&lt;/code&gt; to my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; and added it to the language servers to use for the markdown &lt;code&gt;language&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since my website already contained a &lt;code&gt;.markdownlint.yaml&lt;/code&gt; I could import it to the rumdl format with&lt;/p&gt;
    &lt;p&gt;You might have noticed that I’ve added a little quality of life improvement: soft-wrap at 80 characters.&lt;/p&gt;
    &lt;p&gt;Now if you add this to your own &lt;code&gt;config.toml&lt;/code&gt; you will notice that the text is completely left aligned. This is not a problem on small screens, but it rapidly gets annoying on wider screens.&lt;/p&gt;
    &lt;p&gt;Helix doesn’t support centering the editor. There is a PR tackling the problem but it has been stale for most of the year. The maintainers are overwhelmed by the number of PRs making it their way, and it’s not clear if or when this PR will be merged.&lt;/p&gt;
    &lt;p&gt;In the meantime, a workaround exists, with a few caveats. It is possible to add spaces to the left gutter (the column with the line numbers) so it pushes the content towards the center of the screen.&lt;/p&gt;
    &lt;p&gt;To figure out how many spaces are needed, you need to get your terminal width with &lt;code&gt;stty&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;In my case, when in full screen, my terminal is 243 characters wide. I need to remove the content column with from it, and divide everything by 2 to get the space needed on each side. In my case for a 243 character wide terminal with a text width of 80 characters:&lt;/p&gt;
    &lt;p&gt;As is, I would add 203 spaces to my left gutter to push the rest of the gutter and the content to the right. But the gutter itself has a width of 4 characters, that I need to remove from the total. So I need to subtract them from the total, which leaves me with &lt;code&gt;76&lt;/code&gt; characters to add.&lt;/p&gt;
    &lt;p&gt;I can open my &lt;code&gt;~/.config/helix/config.toml&lt;/code&gt; to add a new key binding that will automatically add or remove those spaces from the left gutter when needed, to shift the content towards the center.&lt;/p&gt;
    &lt;p&gt;Now when in normal mode, pressing Space then t then z will add/remove the spaces. Of course this workaround only works when the terminal runs in full screen mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Astro&lt;/head&gt;
    &lt;p&gt;Astro works like a charm in VS Code. The team behind it provides a Language Server and a TypeScript plugin to enable code completion and syntax highlighting.&lt;/p&gt;
    &lt;p&gt;I only had to install those globally with&lt;/p&gt;
    &lt;p&gt;Now we need to add a few lines to our &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to tell it how to use the language server&lt;/p&gt;
    &lt;p&gt;We can check that the Astro Language Server can be used by helix with&lt;/p&gt;
    &lt;p&gt;I also like to get a formatter to automatically make my code consistent and pretty for me when I save a file. One of the most popular code formaters out there is Prettier. I’ve decided to go with the fast and easy formatter dprint instead.&lt;/p&gt;
    &lt;p&gt;I installed it with&lt;/p&gt;
    &lt;p&gt;Then in the projects I want to use dprint in, I do&lt;/p&gt;
    &lt;p&gt;I might edit the &lt;code&gt;dprint.json&lt;/code&gt; file to my liking. Finally, I configure Helix to use dprint globally for all Astro projects by appending a few lines in my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;One final check, and I can see that Helix is ready to use the formatter as well&lt;/p&gt;
    &lt;head rend="h3"&gt;YAML&lt;/head&gt;
    &lt;p&gt;For yaml, it’s simple and straightforward: Helix is preconfigured to use &lt;code&gt;yaml-language-server&lt;/code&gt; as soon as it’s in the PATH. I just need to install it with&lt;/p&gt;
    &lt;head rend="h2"&gt;Is it worth it?&lt;/head&gt;
    &lt;p&gt;Helix really grew on me. I find it particularly easy and fast to edit code with it. It takes a tiny bit more work to get the language support than it does in VS Code, but it’s nothing insurmountable. There is a slightly steeper learning curve than for VS Code, but I consider it to be a good thing. It forced me to learn how to move around and edit efficiently, because there is no way to do it inefficiently. Helix remains intuitive once you’ve learned the basics.&lt;/p&gt;
    &lt;p&gt;I am a GNOME enthusiast, and I adhere to the same principles: I like when my apps work out of the box, and when I have little to do to configure them. This is a strong stance that often attracts a vocal opposition. I like products that follow those principles better than those who don’t.&lt;/p&gt;
    &lt;p&gt;With that said, Helix sometimes feels like it is maintained by one or two people who have a strong vision, but who struggle to onboard more maintainers. As of writing, Helix has more than 350 PRs open. Quite a few bring interesting features, but the maintainers don’t have enough time to review them.&lt;/p&gt;
    &lt;p&gt;Those 350 PRs mean there is a lot of energy and goodwill around the project. People are willing to contribute. Right now, all that energy is gated, resulting in frustration both from the contributors who feel like they’re working in the void, and the maintainers who feel like there at the receiving end of a fire hose.&lt;/p&gt;
    &lt;p&gt;A solution to make everyone happier without sacrificing the quality of the project would be to work on a Contributor Ladder. CHAOSS’ Dr Dawn Foster published a blog post about it, listing interesting resources at the end.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ergaster.org/posts/2025/10/29-vscode-to-helix/"/><published>2025-10-29T13:19:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746726</id><title>Recreating a Homebrew Game System from 1987</title><updated>2025-10-29T16:13:39.645863+00:00</updated><content>&lt;doc fingerprint="345a297f4904c8c5"&gt;
  &lt;main&gt;
    &lt;p&gt;The specifications are as follows:&lt;/p&gt;
    &lt;p&gt;Controller input and audio output are handled by either an Intel 8255 or Zilog Z80PIO I/O controller. There are two sockets on the PCB for either controller, depending on which is easier for you to obtain. These two ICs have to be controlled slightly differently by software, but it's possible to write games that are compatible with both, as demonstrated by the games written by Inufuto.&lt;/p&gt;
    &lt;p&gt;The controller interface is designed for two-button Sega Master System controllers and will also work with Mega Drive/Genesis controllers. Standard one-button joysticks will also work, aside from the lack of a second button.&lt;/p&gt;
    &lt;p&gt;The composite sync signal is generated with an EPROM, an unconventional method of simplifying the circuitry. Different ROMs have different data access times, so you may need to experiment with one or two models of ROM before you'll find one that produces a glitchless video signal, due to the high speed at which the raster generator steps through the ROM's address bus.&lt;/p&gt;
    &lt;p&gt;Fortunately, any size of ROM between 4KB (2732) and 64KB (27512) can be used, so long as the 4KB binary data file (available for download further down this page) is written to the upper 4KB of higher capacity ROMs. During testing, I found that a 150ns ROM worked well, while a 450ns ROM was too slow.&lt;/p&gt;
    &lt;p&gt;If the prospect of making a lot of cartridges doesn't appeal to you, I've designed a multi-cartridge that holds sixteen 32KB games on one 27C040 ROM. Game selection on the multi-cartridge is performed with DIP switches.&lt;/p&gt;
    &lt;p&gt;The maximum file size for games is 32KB, but I've designed an experimental bank-switching cartridge PCB (not tested yet!) that should allow games of up to 256KB to be accessed through two configurable 16KB page registers on the cartridge.&lt;/p&gt;
    &lt;p&gt;8255:&lt;/p&gt;
    &lt;p&gt;There's a selection of tools available for programming the Z80 TV Game in C:&lt;/p&gt;
    &lt;p&gt; Schematic - Console &lt;lb/&gt; PDF document, 941 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - Console &lt;lb/&gt; ZIP archive, 744 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - Console &lt;lb/&gt; ZIP archive, 1.33 MB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - 32KB ROM Cartridge &lt;lb/&gt; HTML document, 338 KB &lt;/p&gt;
    &lt;p&gt; Schematic - 32KB ROM Cartridge &lt;lb/&gt; PDF document, 127 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - 32KB ROM Cartridge &lt;lb/&gt; ZIP archive, 170 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - 32KB ROM Cartridge &lt;lb/&gt; ZIP archive, 532 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - 32KB x 16 Multi-Cartridge &lt;lb/&gt; HTML document, 350 KB &lt;/p&gt;
    &lt;p&gt; Schematic - 32KB x 16 Multi-Cartridge &lt;lb/&gt; PDF document, 151 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - 32KB x 16 Multi-Cartridge &lt;lb/&gt; ZIP archive, 191 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - 32KB x 16 Multi-Cartridge &lt;lb/&gt; ZIP archive, 564 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - Experimental 256KB ROM Cartridge &lt;lb/&gt; HTML document, 316 KB &lt;/p&gt;
    &lt;p&gt; Schematic - Experimental 256KB ROM Cartridge &lt;lb/&gt; PDF document, 221 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - Experimental 256KB ROM Cartridge &lt;lb/&gt; ZIP archive, 209 KB - Please note that the 256KB cartridge hasn't yet been tested! &lt;/p&gt;
    &lt;p&gt; KiCad Files - Experimental 256KB ROM Cartridge &lt;lb/&gt; ZIP archive, 603 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Custom Fonts &lt;lb/&gt; ZIP archive, 8.90 MB - Custom fonts used for the KiCad files. Only needed if you want to modify these files. &lt;/p&gt;
    &lt;p&gt; Original Schematics &lt;lb/&gt; ZIP archive, 1.14 MB - Mr. Isizu's original schematics for the Z80 TV Game, with the 74LS122 timing circuit corrected. Includes the 1980's hand-drawn schematic, which has a different memory map to the 2000's CAD schematic that this PCB, emulators, C devtools, etc. are based on. &lt;/p&gt;
    &lt;p&gt; Game ROMs &lt;lb/&gt; ZIP archive, 922 KB - All the games I know to exist for the Z80 TV Game thus far. Includes two combined ROMs for those who would rather have all 26 games on 2 multi-cartridges. If you know of any games that aren't mentioned on this page (or you've written a new game), please let me know! My email address is on the home page. &lt;/p&gt;
    &lt;p&gt; 32KB Cartridge Dimensions &lt;lb/&gt; PDF document, 61.3 KB - Useful for designing a 3D printed cartridge enclosure. Note that the standard PCB thickness used by most manufacturers is 1.6mm. &lt;/p&gt;
    &lt;p&gt; 32KB x 16 Multi-Cartridge Dimensions &lt;lb/&gt; PDF document, 67.1 KB - Useful for designing a 3D printed cartridge enclosure. Note that the standard PCB thickness used by most manufacturers is 1.6mm. &lt;/p&gt;
    &lt;p&gt; Z80 TV Game Logo (1920 x 846) (Variant 1) &lt;lb/&gt; PNG image, 1.21 MB - The logo seen at the top of the page in full resolution. &lt;/p&gt;
    &lt;p&gt; Z80 TV Game Logo (1920 x 846) (Variant 2) &lt;lb/&gt; PNG image, 1.08 MB - The logo seen at the top of the page in full resolution. &lt;/p&gt;
    &lt;p&gt;Inufuto: Developer of Cate, a multi-platform compiler that can generate software for the Z80 TV Game. All 20 of the games he has created with it thus far have Z80 TV Game versions. Inufuto has also designed a PCB version of the Z80 TV Game that outputs VGA video via a Raspberry Pi Pico.&lt;/p&gt;
    &lt;p&gt;Takeda Toshiya: Developer of eZ80TVGAME, a Z80 TV Game emulator for Windows.&lt;/p&gt;
    &lt;p&gt;lsluk: Developer of vdmgr, a multi-platform emulator for Windows that supports the Z80 TV Game.&lt;/p&gt;
    &lt;p&gt;Last updated on Oct 26, 2025. &lt;lb/&gt;This page was first uploaded on Oct 26, 2025. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alex-j-lowry.github.io/z80tvg.html"/><published>2025-10-29T13:42:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746753</id><title>New attacks are diluting secure enclave defenses from Nvidia, AMD, and Intel</title><updated>2025-10-29T16:13:39.336786+00:00</updated><content>&lt;doc fingerprint="b2b31958f9265ad6"&gt;
  &lt;main&gt;
    &lt;p&gt;Trusted execution environments, or TEEs, are everywhere—in blockchain architectures, virtually every cloud service, and computing involving AI, finance, and defense contractors. It’s hard to overstate the reliance that entire industries have on three TEEs in particular: Confidential Compute from Nvidia, SEV-SNP from AMD, and SGX and TDX from Intel. All three come with assurances that confidential data and sensitive computing can’t be viewed or altered, even if a server has suffered a complete compromise of the operating kernel.&lt;/p&gt;
    &lt;p&gt;A trio of novel physical attacks raises new questions about the true security offered by these TEES and the exaggerated promises and misconceptions coming from the big and small players using them.&lt;/p&gt;
    &lt;p&gt;The most recent attack, released Tuesday, is known as TEE.fail. It defeats the latest TEE protections from all three chipmakers. The low-cost, low-complexity attack works by placing a small piece of hardware between a single physical memory chip and the motherboard slot it plugs into. It also requires the attacker to compromise the operating system kernel. Once this three-minute attack is completed, Confidential Compute, SEV-SNP, and TDX/SDX can no longer be trusted. Unlike the Battering RAM and Wiretap attacks from last month—which worked only against CPUs using DDR4 memory—TEE.fail works against DDR5, allowing them to work against the latest TEEs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some terms apply&lt;/head&gt;
    &lt;p&gt;All three chipmakers exclude physical attacks from threat models for their TEEs, also known as secure enclaves. Instead, assurances are limited to protecting data and execution from viewing or tampering, even when the kernel OS running the processor has been compromised. None of the chipmakers make these carveouts prominent, and they sometimes provide confusing statements about the TEE protections offered.&lt;/p&gt;
    &lt;p&gt;Many users of these TEEs make public assertions about the protections that are flat-out wrong, misleading, or unclear. All three chipmakers and many TEE users focus on the suitability of the enclaves for protecting servers on a network edge, which are often located in remote locations, where physical access is a top threat.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/security/2025/10/new-physical-attacks-are-quickly-diluting-secure-enclave-defenses-from-nvidia-amd-and-intel/"/><published>2025-10-29T13:44:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747018</id><title>Kafka is Fast – I'll use Postgres</title><updated>2025-10-29T16:13:38.520981+00:00</updated><content>&lt;doc fingerprint="6f5090de6e0009c6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intro&lt;/head&gt;
    &lt;p&gt;I feel like the tech world lives in two camps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;One camp chases buzzwords.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp tends to adopt whatever’s popular without thinking hard about whether it’s appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.&lt;/p&gt;
    &lt;p&gt;You see this everywhere in the Kafka world: Streaming Lakehouse™️, Kappa™️ Architecture, Streaming AI Agents1.&lt;/p&gt;
    &lt;p&gt;This phenomenon is sometimes known as resume-driven design. Modern practices actively encourage this. Consultants push “innovative architectures” stuffed with vendor tech via “insight” reports2. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you’re interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack™️, not for being resourceful.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The other camp chases common sense&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp is far more pragmatic. They strip away unnecessary complexity and steer clear of overengineered solutions. They reason from first principles before making technology choices. They resist marketing hype and approach vendor claims with healthy skepticism.&lt;/p&gt;
    &lt;p&gt;Historically, it has felt like Camp 1 definitively held the upper hand in sheer numbers and noise. Today, it feels like the pendulum may be beginning to swing back, at least a tiny bit. Two recent trends are on the side of Camp 2:&lt;/p&gt;
    &lt;p&gt;Trend 1 - the “Small Data” movement. People are realizing two things - their data isn’t that big and their computers are becoming big too. You can rent a 128-core, 4 TB of RAM instance from AWS. AMD just released 192-core CPUs this summer. That ought to be enough for anybody.3&lt;/p&gt;
    &lt;p&gt;Trend 2 - the Postgres Renaissance. The space is seeing incredible growth and investment4. In the last 2 years, the phrase “Just Use Postgres (for everything)” has gained a ton of popularity. The basic premise is that you shouldn’t complicate things with new tech when you don’t need to, and that Postgres alone solves most problems pretty well. Postgres competes with purpose-built solutions like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Elasticsearch (functionality supported by Postgres’ &lt;code&gt;tsvector&lt;/code&gt;/&lt;code&gt;tsquery&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;MongoDB (&lt;code&gt;jsonb&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Redis (&lt;code&gt;CREATE UNLOGGED TABLE&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;AI Vector Databases (&lt;code&gt;pgvector&lt;/code&gt;,&lt;code&gt;pgai&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Snowflake (&lt;code&gt;pg_mooncake&lt;/code&gt;,&lt;code&gt;pg_duckdb&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and… Kafka (this blog).&lt;/p&gt;
    &lt;p&gt;The claim isn’t that Postgres is functionally equivalent to any of these specialized systems. The claim is that it handles 80%+ of their use cases with 20% of the development effort. (Pareto Principle)&lt;/p&gt;
    &lt;p&gt;When you combine the two trends, the appeal becomes obvious. Postgres is a battle-tested, well-known system that is simple, scalable and reliable. Pair it with today’s powerful hardware and you quickly begin to realize that, more often than not, you do not need the state-of-the-art highly optimized and complex distributed system in order to handle your organization’s scale.&lt;/p&gt;
    &lt;p&gt;Despite being somebody who is biased towards Kafka, I tend to agree. Kafka is similar to Postgres in that it’s stable, mature, battle-tested and boasts a strong community. It also scales a lot further. Despite that, I don’t think it’s the right choice for a lot of cases. Very often I see it get adopted where it doesn’t make sense.&lt;/p&gt;
    &lt;p&gt;A 500 KB/s workload should not use Kafka. There is a scalability cargo cult in tech that always wants to choose “the best possible” tech for a problem - but this misses the forest for the trees. The “best possible” solution frequently isn’t a technical question - it’s a practical one. Adriano makes an airtight case for why you should opt for simple tech in his PG as Queue blog (2023) that originally inspired me to write this.&lt;/p&gt;
    &lt;p&gt;Enough background. In this article, we will do three simple things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for pub/sub messaging - # PG as a Pub/Sub&lt;/item&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for queueing - # PG as a Queue&lt;/item&gt;
      &lt;item&gt;Concisely touch upon when Postgres can be a fit for these use cases - # Should You Use Postgres?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I am not aiming for an exhaustive in-depth evaluation. Benchmarks are messy af. Rather, my goal is to publish some reasonable data points which can start a discussion.&lt;/p&gt;
    &lt;p&gt;(while this article is for Postgres, feel free to replace it with your database of choice)&lt;/p&gt;
    &lt;head rend="h1"&gt;Results TL;DR&lt;/head&gt;
    &lt;p&gt;If you’d like to skip straight to the results, here they are:&lt;/p&gt;
    &lt;head&gt;🔥 The Benchmark Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;✍️ Write&lt;/cell&gt;
        &lt;cell role="head"&gt;📖 Read&lt;/cell&gt;
        &lt;cell role="head"&gt;🔭 e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1× c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;4.8 MiB/s&lt;p&gt;5036 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.6 MiB/s&lt;p&gt;25 183 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;60 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3× c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;4.9 MiB/s&lt;p&gt;5015 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.5 MiB/s&lt;p&gt;25 073 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;~65 % CPU; cross-AZ RF≈2.5; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1× c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;238 MiB/s&lt;p&gt;243,000 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.16 GiB/s&lt;p&gt;1,200,000 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;853 ms&lt;/cell&gt;
        &lt;cell&gt;~10 % CPU (idle); 30 partitions&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Queue Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;📬 Throughput (read + write)&lt;/cell&gt;
        &lt;cell role="head"&gt;🔭 e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1× c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;2.81 MiB/s&lt;p&gt;2885 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;17.7 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; read-client bottleneck&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3× c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;2.34 MiB/s&lt;p&gt;2397 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;920 ms ⚠️6&lt;/cell&gt;
        &lt;cell&gt;replication lag inflated E2E latency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1× c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;19.7 MiB/s&lt;p&gt;20,144 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;930 ms ⚠️6&lt;/cell&gt;
        &lt;cell&gt;~50 % CPU; single-table bottleneck&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Make sure to at least read the last section of the article where we philosophize - # Should You Use Postgres?&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Pub/Sub&lt;/head&gt;
    &lt;p&gt;There are dozens of blogs out there using Postgres as a queue, but interestingly enough I haven’t seen one use it as a pub-sub messaging system.&lt;/p&gt;
    &lt;p&gt;A quick distinction between the two because I often see them get confused:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Queues are meant for point-to-point communication. They’re widely used for asynchronous background jobs: worker apps (clients) process a task in the queue like sending an e-mail or pushing a notification. The event is consumed once and it’s done with. A message is immediately deleted (popped) off the queue once it’s consumed. Queues do not have strict ordering guarantees7.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pub-sub messaging differs from the queue in that it is meant for one-to-many communication. This inherently means there is a large read fanout - more than one reader client is interested in any given message. Good pub-sub systems decouple readers from writers by storing data on disks. This allows them to not impose a max queue depth limit - something in-memory queues need to do in order to prevent them from going OOM.&lt;/p&gt;
        &lt;p&gt;There is also a general expectation that there is strict order - events should be read in the same order that they arrived in the system.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres’ main competitor here is Kafka, which is the standard in pub-sub today. Various (mostly-proprietary) alternatives exist.8&lt;/p&gt;
    &lt;p&gt;Kafka uses the Log data structure to hold messages. You’ll see my benchmark basically reconstructs a log from Postgres primitives.&lt;/p&gt;
    &lt;p&gt;Postgres doesn’t seem to have any popular libraries for pub-sub9 use cases, so I had to write my own. The Kafka-inspired workflow I opted for is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Writers produce batches of messages per statement10 (&lt;code&gt;INSERT INTO&lt;/code&gt;). Each transaction carries one batch insert and targets a single&lt;code&gt;topicpartition&lt;/code&gt;table11&lt;/item&gt;
      &lt;item&gt;Each writer is sticky to one table, but in aggregate they produce to multiple tables.&lt;/item&gt;
      &lt;item&gt;Each message has a unique monotonically-increasing offset number. A specific row in a special &lt;code&gt;log_counter&lt;/code&gt;table denotes the latest offset for a given&lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Write transactions atomically update both the &lt;code&gt;topicpartition&lt;/code&gt;data and the&lt;code&gt;log_counter&lt;/code&gt;row. This ensures consistent offset tracking across concurrent writers.&lt;/item&gt;
      &lt;item&gt;Readers poll for new messages. They consume the &lt;code&gt;topicpartition&lt;/code&gt;table(s) sequentially, starting from the lowest offset and progressively reading up.&lt;/item&gt;
      &lt;item&gt;Readers are split into consumer groups. Each group performs separate, independent reads and makes progress on the &lt;code&gt;topicpartition&lt;/code&gt;tables.&lt;/item&gt;
      &lt;item&gt;Each group contains 1 reader per &lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Readers store their progress in a &lt;code&gt;consumer_offsets&lt;/code&gt;table, with a row for each&lt;code&gt;topicpartition,group&lt;/code&gt;pair.&lt;/item&gt;
      &lt;item&gt;Each reader updates the latest processed offset (claiming the records), selects the records and processes them inside a single transaction.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This ensures Kafka-like semantics - gapless, monotonically-increasing offsets and at-least-once/at-most-once processing. This test in particular uses at-least-once semantics, but neither choice should impact the benchmark results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;The benchmark runs &lt;code&gt;N&lt;/code&gt; writer goroutines. These represent writer clients.
Each one loops and atomically inserts &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records while updating the latest offset:&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;The benchmark also runs &lt;code&gt;N&lt;/code&gt; reader goroutines. Each reader is assigned a particular consumer group and partition. The group as a whole reads all partitions while each reader in the group reads only one partition at a time.&lt;/p&gt;
    &lt;p&gt;The reader loops, opens a transaction, optimistically claims &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records (by advancing the offset mark beyond them), selects them and processes the records.
If successful, it commits the transaction and through that advances the offset for the group.&lt;/p&gt;
    &lt;p&gt;It is a pull-based read (just like Kafka), rather than push-based. If the reader has no records to poll, it sleeps for a bit.&lt;/p&gt;
    &lt;p&gt;First it opens a transaction:&lt;/p&gt;
    &lt;p&gt;Then it claims the offsets:&lt;/p&gt;
    &lt;p&gt;Followed by selecting the claimed records:&lt;/p&gt;
    &lt;p&gt;Finally, the data gets processed by the business logic (no-op in our benchmark) and the transaction is closed:&lt;/p&gt;
    &lt;p&gt;If you’re wondering “why no &lt;code&gt;NOTIFY/LISTEN&lt;/code&gt;?” - my understanding of that feature is that it’s an optimization and cannot be fully relied upon, so polling is required either way12. Given that, I just copied Kafka’s relatively simple design.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;p&gt;The full code and detailed results are all published on GitHub at stanislavkozlovski/pg-queue-pubsub-benchmark. I ran three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;mostly default Postgres settings (synchronous commit, fsync); &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5036 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.8 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 38.7ms p99 / 6.2ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,183 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.6 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 27.3ms p99 (varied 8.9ms-47ms b/w runs); 4.67ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 60ms p99 / 10.6ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~60% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are pretty good results. It’s funny to think that the majority of people run a complex distributed system like Kafka for similar workloads13.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;Now, a replicated setup to more accurately mimic the durability and availability guarantees of Kafka.&lt;/p&gt;
    &lt;p&gt;The average of two 5-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5015 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.9 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 153.45ms p99 / 6.8ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,073 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.5 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 57ms p99; 4.91ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 186ms p99 / 12ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~65% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now these are astonishing results! Throughput was not impacted at all. Latency increased but not extremely. Our p99 e2e latency 3x’d (60ms vs 185ms), but the p95 barely moved from 10.6ms to 12ms.&lt;/p&gt;
    &lt;p&gt;This shows that a simple 3-node Postgres cluster can pretty easily sustain what is a very common Kafka workload - 5 MB/s ingest and 25 MB/s egress. Not only that, but for a cheap cost too. Just $11,514 per year.16&lt;/p&gt;
    &lt;p&gt;Typically, you’d expect Postgres to run more expensive than Kafka at a certain scale, simply because it wasn’t designed to be efficient for this use case. Not here though. Running Kafka yourself would cost the same. Running the same workload through a Kafka vendor will cost you at least $50,000 a year. 🤯&lt;/p&gt;
    &lt;p&gt;By the way, in Kafka it’s customary to apply client-side compression on your data. If we assume your messages were 5 KB in size and your clients applied a pretty regular compression ratio of 4x17 - Postgres is actually handling 20 MB/s ingress and 100 MB/s egress.&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;Ok, let’s see how far Postgres will go.&lt;/p&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge (96 vCPU, 192 GiB RAM) Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;30 topicpartition tables&lt;/item&gt;
      &lt;item&gt;100 writers (~3.33 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;150 reader clients total (5 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 200 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 243,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 238 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 138ms p99 / 47ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 1,200,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 1.16 GiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 24.6ms p99&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 853ms p99 / 242ms p95 / 23.4ms p50&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~10% CPU (basically idle);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;bottleneck: The bottleneck was the write rate per partition. It seems like the test wasn’t able to write at a higher rate than 8 MiB/s (8k msg/s) per table with this design. I didn’t push further, but I do wonder now as I write this - how far would writes have scaled?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reads were trivial to scale. Adding more consumer groups was trivial - I tried with 10x fanout and still ran at low CPU. I didn’t include it because I didn’t feel the need to push to an unrealistic read-fanout extreme.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;240 MiB/s ingress and 1.16 GiB/s egress are pretty good! The 96 vCPU machine was overkill for this test - it could have done a lot more, or we could have simply opted for a smaller machine. For what it’s worth, I do think it’s worth it to deploy a separate Kafka cluster at this scale. Kafka can save you a lot of money here because it can be more efficient in how it handles cross-zone network traffic with features like Diskless Kafka.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pub-Sub Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here → 👉 stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;These tests seem to show that Postgres is pretty competitive with Kafka at low scale.&lt;/p&gt;
    &lt;p&gt;You may have noticed none of these tests were particularly long-running. From my understanding, the value in longer-running tests is to test table vacuuming in Postgres, as that can have negative performance effects. In the pub-sub section, vacuuming doesn’t apply because the tables are append-only. My other reasoning for running shorter tests was to keep costs in check and not spend too much time18.&lt;/p&gt;
    &lt;p&gt;In any case, no benchmark is perfect. My goal wasn’t to indisputably prove &lt;code&gt;$MY_CLAIM&lt;/code&gt;. Rather, I want to start a discussion by showing that what’s possible is likely larger than what most people assume. I certainly didn’t assume I’d get such good numbers, especially with the pub-sub part.&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Queue&lt;/head&gt;
    &lt;p&gt;In Postgres, a queue can be implemented with &lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;. This command selects an unlocked row and locks it. It also skips reading already-locked rows. That’s how mutual exclusion is achieved - a worker can’t get other workers’ jobs.&lt;/p&gt;
    &lt;p&gt;Postgres has a very popular pgmq library that offers a slick queue API. To keep it simple and understand the end-to-end flow better, I decided to write my own queue. The basic version of it is very easy. My workflow is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;add job (&lt;code&gt;INSERT&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;lock row &amp;amp; take job (&lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;process job (&lt;code&gt;{your business logic}&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;mark job as “done” (&lt;code&gt;UPDATE&lt;/code&gt;a field or&lt;code&gt;DELETE &amp;amp; INSERT&lt;/code&gt;the row into a separate table)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres competes with RabbitMQ, AWS SQS, NATS, Redis19 and to an extent Kafka20 here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;p&gt;We use a simple &lt;code&gt;queue&lt;/code&gt; table. When an element is processed off the queue, it’s moved into the archive table.&lt;/p&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;N&lt;/code&gt; writer client goroutines.
Each one simply loops and sequentially inserts a single random item into the table:&lt;/p&gt;
    &lt;p&gt;It only inserts one message per statement, which is pretty inefficient at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;M&lt;/code&gt; reader client goroutines. Each reader loops and processes one message.
The processing is done inside a database transaction.&lt;/p&gt;
    &lt;p&gt;Each reader again only works with one message at a time per transaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Results&lt;/head&gt;
    &lt;p&gt;I again ran the same three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of two 15-minute tests. I also ran three 2-minute tests. They all performed similarly. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;all default Postgres settings21&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2885 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.81 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 2.46ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 4.2ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 17.72ms p99&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What I found Postgres wasn’t good at was handling client count. The bottleneck in this setup was the read clients. Each client could not read more than ~192 messages a second because of its median read latency and sequential read nature.&lt;/p&gt;
    &lt;p&gt;Increasing client count boosted throughput but violated my ~60% CPU target. Trying to run 50 write and 50 read clients got to 4000 msg/s without increasing the queue depth but pegged the server’s CPU to 100%. I wanted to keep the benchmarks realistic for what you may run in production, rather than maxing out what a machine can do. This would be easily alleviated with a connection pooler (standard across all prod PG deployments) or a larger machine.&lt;/p&gt;
    &lt;p&gt;Another thing worth mentioning is that the workload could sustain a lot more writes than reads. If I didn’t throttle the benchmark, it would write at 12,000 msg/s and read at 2,800 msg/s. In the spirit of simplicity, I didn’t debug further and instead throttled my writes to see at what point I could get a stable 1:1 workload.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;A single 10-minute test. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2397 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.34 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 3.3ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 7.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 920ms p99 ⚠️6; 536ms p95; 7ms p50&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As expected, throughput and latency were impacted somewhat. But not that much. It’s still over 2000 messages a second, which is pretty good for an HA queue!&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;100 writer clients, 200 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 20,144 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 19.67 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 9.42ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 22.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency: 930ms p99 ⚠️6; 709ms p95; 12.6ms p50&lt;/item&gt;
      &lt;item&gt;server at 40-60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This run wasn’t that impressive. There is some bottleneck in the single-table queue approach at this scale which I didn’t bother figuring out. I figured that it wasn’t important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one. The 96 vCPU instance would likely scale far further were we to run a few separate queue tables in parallel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Queue Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here → 👉 stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;Even a modest Postgres node can durably push thousands of queue ops/sec, which already covers the scale 99% of companies ever hit with a single queue. As I said earlier, the last 2 years have seen the Just Use Postgres slogan become mainstream. The &lt;code&gt;pgmq&lt;/code&gt; library’s star history captures this trend perfectly:
&lt;/p&gt;
    &lt;head rend="h1"&gt;Should You Use Postgres?&lt;/head&gt;
    &lt;p&gt;Most of the time - yes. You should always default to Postgres until the constraints prove you wrong.&lt;/p&gt;
    &lt;p&gt;Kafka is obviously better optimized for pub-sub workloads. Queue systems are obviously better optimized for queue workloads. The point is that picking a technology based on technical optimization alone is a flawed approach. To throw an analogy:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a Formula One car is optimized to drive faster, but I still use a sedan to go to work. I am way more comfortable driving my sedan than an F1 car.&lt;/p&gt;
      &lt;p&gt;(seriously, see the steering wheel on these things)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Postgres sedan comes with many quality-of-life comforts that the F1 Kafka does not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ability to debug messages with regular SQL&lt;/item&gt;
      &lt;item&gt;ability to delete, re-order or edit messages in place&lt;/item&gt;
      &lt;item&gt;ability to join pub-sub data with regular tables&lt;/item&gt;
      &lt;item&gt;ability to trivially read specific data via rich SQL queries (&lt;code&gt;ID=54&lt;/code&gt;,&lt;code&gt;name="John"&lt;/code&gt;,&lt;code&gt;cost&amp;gt;1000&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Giving up these comforts is a justified sacrifice for your F1 car to go at 378 kmh (235 mph), but masochistic if you plan on driving at 25kmh (15 mph).&lt;/p&gt;
    &lt;p&gt;Donald Knuth warned us in 1974 - premature optimization is the root of all evil. Deploying Kafka at small scale is premature optimization. The point of this article is to show you that this “small scale” number has grown further than what people remember it to be - it can comfortably mean many megabytes per second.&lt;/p&gt;
    &lt;p&gt;We are in a Postgres Renaissance for a reason: Postgres is frequently good enough. Modern NVMEs and cheap RAM allow it to scale absurdly high.&lt;/p&gt;
    &lt;p&gt;What’s the alternative?&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Solutions for Everything?&lt;/head&gt;
    &lt;p&gt;Naive engineers tend to adopt a specialized technology at the slightest hint of a need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Need a cache? Redis, of course!&lt;/item&gt;
      &lt;item&gt;Search? Let’s deploy Elasticsearch!&lt;/item&gt;
      &lt;item&gt;Offline data analysis? BigQuery or Snowflake - that’s what our data analysts used at their last job.&lt;/item&gt;
      &lt;item&gt;No schemas? We need a NoSQL database like MongoDB.&lt;/item&gt;
      &lt;item&gt;Have to crunch some numbers on S3? Let’s use Spark!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A good engineer thinks through the bigger picture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Does this new technology move the needle?&lt;/item&gt;
      &lt;item&gt;Is shaving a few milliseconds off our query worth the extra organizational complexity introduced with the change?&lt;/item&gt;
      &lt;item&gt;Will our users notice?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At small scale, these systems hurt you more than they benefit you. Distributed systems - both in terms of node count and system cardinality - should be respected, feared, avoided and employed only as a weapon of last resort against particularly gnarly problems. Everything with a distributed system becomes more challenging and time-consuming.&lt;/p&gt;
    &lt;p&gt;The problem is the organizational overhead. The organizational overhead of adopting a new system, learning its nuances, configs, establishing monitoring, establishing processes around deployments and upgrades, attaining operational expertise on how to manage it, creating runbooks, testing it, debugging it, adopting its clients and API, using its UI, keeping up with its ecosystem, etc.&lt;/p&gt;
    &lt;p&gt;All of these are real organizational costs that can take months to get right, even if the system in question isn’t difficult (a lot are). Managed SaaS offerings trade off some of the organizational overhead for greater financial costs - but they still don’t remove it all. And until you reach the scale where the technology is necessary, you pay these extra {financial, organizational} costs for zero significant gain.&lt;/p&gt;
    &lt;p&gt;If the same can be done with tech for which you’ve already paid the organizational costs for (e.g Postgres), adopting something else prematurely is most definitely an anti-pattern. You don’t need web-scale technologies when you don’t have web-scale problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;MVI (a better alternative)&lt;/head&gt;
    &lt;p&gt;What I think is a better approach is to search for the minimum viable infrastructure (MVI): build the smallest amount of system while still providing value.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;choose good-enough technology your org is already familiar with &lt;list rend="ul"&gt;&lt;item&gt;good-enough == meets your users’ needs without being too slow/expensive/insecure&lt;/item&gt;&lt;item&gt;familiar == your org has prior experience, has runbooks/ops setups, monitoring, UI, etc&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;solve a real problem with it&lt;/item&gt;
      &lt;item&gt;use the minimum set of features &lt;list rend="ul"&gt;&lt;item&gt;the fewer features you use, the more flexibility you have to move off the infra in question in the future (e.g if locked in with a vendor)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bonus points if that technology:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;is widely adopted so finding good engineers for it is trivial (Postgres - check)&lt;/item&gt;
      &lt;item&gt;has a strong and growing network effect (Postgres - check)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MVI approach reduces the surface area of your infra. The fewer moving parts you have, the fewer failure modes you worry about and the less glue code you have to maintain.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it’s human nature to go against this. Just like startups suffer due to MVP bloat (one more feature!), infra teams suffer due to MVI bloat (one more system!)&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are we like this?&lt;/head&gt;
    &lt;p&gt;I won’t pretend to be able to map out the exact path-dependent outcome, but my guess is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;the zero interest rate era gave us abundant speculative money that was invested in any company that could grow fast&lt;/item&gt;
      &lt;item&gt;a lot of viral internet companies were growing at speeds that led old infra to become obsolete fast&lt;/item&gt;
      &lt;item&gt;this prompted the next wave of ZIRP investment - specialized data infrastructure companies (in a gold rush, sell shovels!); some of these data infra startups spun off directly from the high-growth companies themselves&lt;/item&gt;
      &lt;item&gt;each well-funded data infra vendor is financially motivated to evangelize their product and have you adopt it even when you don’t need to (Everyone is Talking Their Book). They had deep pockets for marketing and used them.&lt;/item&gt;
      &lt;item&gt;innovative infrastructure software got engineered. It was exciting - so engineers got nerd-sniped into it&lt;/item&gt;
      &lt;item&gt;a web-scale craze/cargo cult developed, where everybody believed they need to be able to scale from zero to millions of RPS because they may go viral any day.&lt;/item&gt;
      &lt;item&gt;a trend developed to copy whatever solutions the most successful, largest digital-native companies were using (Amazon, Google, Uber, etc.)&lt;/item&gt;
      &lt;item&gt;the trend became a self-perpetuating prophecy: these technologies became a sought-after skill on resumes &lt;list rend="ul"&gt;&lt;item&gt;system design interview questions were adapted to test for knowledge of these systems&lt;/item&gt;&lt;item&gt;within an organization, engineers (knowingly or not) pushed for projects that are exciting and helped build their resumes;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This trend continues to grow while there is no strong competing force that is sufficiently motivated to push the opposite view. Even engineers inside a company, who ought to be motivated to keep things simple, have strong incentives to pursue extra complexity. It benefits their career by giving them a project to use as ammo for their next promotion and improves their resume (cool tech/story on there) for their next job-hop. Plus it’s simply more fun.&lt;/p&gt;
    &lt;p&gt;This is why I think we, as an industry, don’t always use the simplest solution available.&lt;/p&gt;
    &lt;p&gt;In most cases, Postgres is that simplest solution that is available.&lt;/p&gt;
    &lt;head rend="h2"&gt;But It Won’t Scale!&lt;/head&gt;
    &lt;p&gt;I want to wrap this article up, but one rebuttal I can’t miss addressing is the “it won’t scale argument”.&lt;/p&gt;
    &lt;p&gt;The argument goes something like this: “in today’s age we can go viral at a moment’s notice; these viral moments are very valuable for our business so we need to aggressively design in a way that keeps our app stable under traffic spikes”&lt;/p&gt;
    &lt;p&gt;I have three arguments against this:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Postgres Scales&lt;/head&gt;
    &lt;p&gt;As of 2025, OpenAI still uses an unsharded Postgres architecture with only one primary instance for writes22. OpenAI is the poster-child of rapid viral growth. They hold the record for the fastest startup to reach 100 million users.&lt;/p&gt;
    &lt;p&gt;Bohan Zhang, a member of OpenAI’s infrastructure team and co-founder of OtterTune (a Postgres tuning service), can be quoted as saying23:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“At OpenAI, we utilize an unsharded architecture with one writer and multiple readers, demonstrating that PostgreSQL can scale gracefully under massive read loads.”&lt;/p&gt;
      &lt;p&gt;“The main message of my talk was that if you are not too write heavy, you can scale Postgres to a very high read throughput with read replicas using only a single master! That is exactly the message that needs to be spelled out as that covers the vast majority of apps.”&lt;/p&gt;
      &lt;p&gt;“Postgres is probably the default choice for developers right now. You can use Postgres for a very long time. If you are building a startup with read-heavy workloads, just start with Postgres. If you hit a scalability issue, increase the instance size. You can scale it to a very large scale. If in the future the database becomes a bottleneck, congratulations. You have built a successful startup. It’s a good problem to have.”&lt;/p&gt;
      &lt;p&gt;(slightly edited for clarity and grammar)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite their rapid growth to a user base of more than 800 million, OpenAI has still NOT opted for a web-scale distributed database. If they haven’t… why does your unproven project need to?&lt;/p&gt;
    &lt;head rend="h3"&gt;2. You Have More Time To Scale Than You Think&lt;/head&gt;
    &lt;p&gt;Let’s say it’s a good principle to design/test for ~10x your scale. Here are the years of consistent growth rate it takes to get to 10x your current scale:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;annual growth&lt;/cell&gt;
        &lt;cell role="head"&gt;years to hit 10× scale&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10 %&lt;/cell&gt;
        &lt;cell&gt;24.16 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25 %&lt;/cell&gt;
        &lt;cell&gt;10.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50 %&lt;/cell&gt;
        &lt;cell&gt;5.68 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;75 %&lt;/cell&gt;
        &lt;cell&gt;4.11 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;100 %&lt;/cell&gt;
        &lt;cell&gt;3.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;150 %&lt;/cell&gt;
        &lt;cell&gt;2.51 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;200 %&lt;/cell&gt;
        &lt;cell&gt;2.10 y&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It goes to show that even at extreme growth levels, you still have years to migrate between solutions. The majority of developers, though, work at companies in the 0-50% growth rate. They are more likely to have moved on to another job by the time the solution needs to change (if ever).&lt;/p&gt;
    &lt;head rend="h3"&gt;3. It’s Overdesign&lt;/head&gt;
    &lt;p&gt;In an ideal world, you would build for scale and any other future problem you may hit in 10 years.&lt;/p&gt;
    &lt;p&gt;In the real world, you have finite bandwidth, so you have to build for the most immediate, highest ROI problem.&lt;/p&gt;
    &lt;p&gt;Commenter snej on lobste.rs captured it well:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Planning your infrastructure around being able to handle that is sort of like buying a huge Marshall stack as your first guitar amp because your garage band might get invited to open for Coldplay.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Just use Postgres until it breaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Title inspiration comes from a great recent piece - “Redis is fast - I’ll cache in Postgres”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m a complete Postgres noob. You may see a lot of dumb mistakes here. Feel free to call me out on them - I’m happy to learn. I used AI to help a lot with some of the PG tools to use. This both shows how inexperienced I am in the context and how easy it is to start. I am generally skeptical of AI’s promise (in the short-term), but there’s no denying it has made a large dent in democratizing niche/low-level knowledge.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’d like to reach out to me, you can find me on LinkedIn or X (Twitter).&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Don’t worry if you don’t fully understand these terms. I work full-time in the industry that spews these things and I don’t have a great grasp either. It’s marketing slop. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gartner and others push embarrassing recommendations that aren’t tech driven. It’s frequently the opposite - they’re profit driven. Gartner makes $6.72B purely off a consulting service that charges organizations $50k per seat solely for access to reports that recommend these slop architectures. It’s not crazy to believe, hence many people are converging with the idea that it is a pay-to-win racket model. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seriously, the improvement in hardware is something I find most senior engineers haven’t properly appreciated. Newest gen AMD CPUs boast 192 cores. Modern SSDs can do 5.5 million random reads a second, or ~28GB/s sequential reads. Both are a 10-20x improvement over the last 10 years alone. Single nodes are more powerful than ever. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Just in the last 6 months - Snowflake acquired Crunchy Data for ~$250M, Databricks acquired Neon for ~$1 billion; In the last 12 months, Supabase more than 5x’d its valuation from ($900M to $5B), raising $380M across three series (!!!). Within a single year! ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;End-to-end latency here is defined as&lt;/p&gt;&lt;code&gt;now() - event_create_time&lt;/code&gt;; In essence, it tracks how long a brand new persisted event takes to get consumed. It helps show cases where queue lag spikes like when consumers temporarily fall behind the write rate. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some queue tests showed higher E2E latencies which I believe was due to a bug. In the pub-sub tests, I ensured readers start before the writers via a 1000ms sleep. For the queue tests, though, I didn’t do this. The result is that queue tests immediately spike queue depth at startup because the writers manage to get a head start before the readers. I believe the E2E latency is artificially high because of this flaw in the test. ↩ ↩2 ↩3 ↩4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actually, things are ordered in the happy path. Only during retries can you get out of order processing. e.g at t=0, client A reads task N; At t=1, client B reads task N+1 and processes it successfully; At t=2, A fails and is unable to process task N; At t=3, client B takes the next available task - which is N. B therefore executes the tasks in order [N+1, N], whereas proper order would have been [N, N+1] ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open-source projects include Apache Pulsar (open source), RedPanda (source-available), AutoMQ (a fork of Kafka) and a lot of proprietary offerings - AWS Kinesis, Google Pub/Sub, Azure Event Hubs, Confluent Kora, Confluent WarpStream, Bufstream to name a few. What’s common in 90% of these projects is that they all implement the Apache Kafka API, making Kafka undoubtedly the protocol standard in the space. There’s also an open-source project which exposes a Kafka API on top of a pluggable Postgres or S3 backend - Tansu (Rust, btw :] ) ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The most popular library I could find is pg-pubsub with 106 stars as of writing (Oct 2025). Its last commit was 3 months ago. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Batching messages per client is very important for scalability here. It is one of Kafka’s least-talked-about performance “tricks”. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;These tables act as different log data structures. You can see them as separate topics, or partitions of one topic (shards). ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Postgres stores all&lt;/p&gt;&lt;code&gt;NOTIFY&lt;/code&gt;events in a single, global queue. If this queue becomes full, transactions calling&lt;code&gt;NOTIFY&lt;/code&gt;will fail when committing. (src) ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A report by RedPanda found that ~55% of respondents use Kafka for &amp;lt; 1 MB/s. Kafka-vendor Aiven similarly shared that 50% of their Kafka deployments have an ingest rate of below 10 MB/s. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This replication is equivalent to RF=2 in Kafka with one extra non-synchronous replica. Call it RF=2.5. The client receives a response when the one&lt;/p&gt;&lt;code&gt;sync&lt;/code&gt;replica confirms the change. The other&lt;code&gt;potential&lt;/code&gt;replica is replicating asynchronously without blocking the write path. It will become promoted to&lt;code&gt;sync&lt;/code&gt;if the other one was to die. ↩ ↩2&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The tests didn’t direct any read traffic to the standbys. This caused extra load on the primary - most production workloads would read from the standbys. Despite that, the results were still good! In my tests, I found that the extra read workload didn’t seem to have a negative effect on the database - it seems such tail reads were served exclusively from cache. ↩ ↩2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The node and its disk cost $1826 per year. Since we run three of those, it’s $5478/yr. The networking in AWS costs $0.02/GB and our setup is replicating 4.9MB/s across two instances - that results in 294.74TB cross-zone networking per year. That’s $6036 per year in replication networking. Assuming your clients are in the same zone as the database they’re writing to / reading from, that networking is free. That results in an annual cost of $11,514. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can realistically achieve a 10x+ compression ratio if operating on compressible data like logs (something Kafka is used for frequently). The only gotcha is that we need to compress larger batches - eg 25KB+ - so that requires a bit of a re-design in the pub-sub data model. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I had already spent enough business days working on this benchmark and re-running tests numerous, numerous times as I iterated on the benchmark and the methodology. On the larger instances, the cost accumulates fast and running longer tests at high MB/s rates requires deploying much larger and more expensive disks in order to store all the accumulated data. The effort spent matches the goal I have with the article. If any Postgres vendor wants to sponsor a more thorough investigation - let me know! ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Surprisingly (to me), Redis is a very popular queue-like backend choice for background jobs. Most popular open-source libraries use it. Although I’m sure Postgres can do just as good a job, many devs will prefer to use an established library rather than build one from scratch or use something less well-maintained. I do think PG-backed libraries should get developed though! ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kafka has historically never been a queue. To use it as one, you had to develop some difficult workarounds. Today, however, it is in the middle of implementing a first-class Queue-like interface (currently in Preview) ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most importantly, synchronous commit and fsync are both on. This means every write is durably persisted to disk. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The optimizations they did to support this scale are cool, but not novel. See these two talks at a) PGConf.dev 2025 (my transcript) and b) POSETTE (my transcript) ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the talks PGConf.dev 2025 (my transcript) and POSETTE (my transcript) ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks"/><published>2025-10-29T14:06:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747112</id><title>I made a 10¢ MCU Talk</title><updated>2025-10-29T16:13:38.120148+00:00</updated><content>&lt;doc fingerprint="a50cd3b339220607"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;TLDR: Yes, you can fit about 7 seconds of audio into 16K of flash and still have room for code. And you can even play LPC encoded audio on a 10 cent MCU.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There’s quite a lot more detail in this video (and of course you can hear the audio!).&lt;/p&gt;
    &lt;p&gt;In the previous project, I had this ultra-cheap CH32V003 microcontroller playing simple tunes on a tiny SMD buzzer. It was just toggling a GPIO pin at musical note frequencies – 1-bit audio output – and it sounded surprisingly decent. That was a fun start, but now it’s time to push this little $0.10 MCU even further: can we make it actually talk?&lt;/p&gt;
    &lt;p&gt;Spoiler: Yes, we can! (well, there wouldn’t be much of a blog post if we couldn’t) This 8-pin RISC-V chip is now producing sampled audio data and spoken words. We’re really stretching the limits of what you can fit in 16 KB of flash.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Beeps to Actual Audio&lt;/head&gt;
    &lt;p&gt;Moving from simple beeps to real audio meant using the microcontroller’s PWM output as a rudimentary DAC. Instead of just on/off beeping, I’m driving a waveform at an 8 kHz sample rate using a high-frequency PWM on the output pin. The hardware is the same tiny board as before – but I’ve swapped the small SMD buzzer for a small speaker. The buzer works too, but it’s quieter and very tinny.&lt;/p&gt;
    &lt;p&gt;The sample I wanted to test with is just over 6 seconds in length - it’s the iconic “Open the pod bay doors HAL…” sequence from 2001.&lt;/p&gt;
    &lt;p&gt;If we keep this audio at 16-bit PCM, 8kHZ, we’d need about 96KB – way beyond our 16 KB flash! And remember, that 16 KB has to hold both the audio data and our playback code. Clearly some aggressive compression is required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Bits/Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Fits in 16KB?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CD Quality&lt;/cell&gt;
        &lt;cell&gt;44.1 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;529 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 33× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Phone Quality&lt;/cell&gt;
        &lt;cell&gt;16 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;192 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 12× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Basic PCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;8-bit&lt;/cell&gt;
        &lt;cell&gt;48 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 3× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;4-bit ADPCM (IMA)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;4-bit&lt;/cell&gt;
        &lt;cell&gt;24 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 1.5× too big&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;QOA (Quite OK Audio)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;3.2-bit&lt;/cell&gt;
        &lt;cell&gt;19 KB&lt;/cell&gt;
        &lt;cell&gt;❌ Still too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2-bit ADPCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;2-bit&lt;/cell&gt;
        &lt;cell&gt;12 KB&lt;/cell&gt;
        &lt;cell&gt;✅ Fits!&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I considered a few encoding options for compressing the audio.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8-bit PCM: Simply using 8-bit samples at 8 kHz cuts size in half (to ~47 KB for 6s), but that’s still about 3× too large for our flash.&lt;/item&gt;
      &lt;item&gt;4-bit ADPCM: Adaptive Differential PCM is a simple lossy compression that could quarter the size. In theory 6 seconds would be ~24 KB – much closer to fitting,&lt;/item&gt;
      &lt;item&gt;“Quite OK Audio” (QOA): This is nice codec that packs audio into about 3.2 bits per sample (roughly 1/5 the size of 16-bit PCM)&lt;/item&gt;
      &lt;item&gt;2-bit ADPCM: Going even further with ADPCM, using only 2 bits per sample gives a 4:1 compression relative to 8-bit audio – that’s 75% storage savings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2-bit ADPCM is definitely the winner here. Our 6-second clip shrinks to under 12 KB, which comfortably fits in flash with room for code. This looked like the winner, provided the audio quality was acceptable. The decoder for 2-bit ADPCM is also very lightweight (my implementation compiled to under just over 1K of code - 1340 bytes!). It’s definitely low quality - but it actually sounds surprisingly ok.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does 2-bit ADPCM work?&lt;/head&gt;
    &lt;p&gt;It’s actually a very simple algorithm. Both the encoder and decoder maintain a predicted signal value and a step size index into a predefined table. Each 2-bit code tells the decoder how to adjust the current prediction and the step size index. In essence, we’re coding the difference between the real audio and our prediction, with only four possible levels (since 2 bits gives 4 values). After each sample, the algorithm adapts: if the prediction error was large, we move to a bigger step size (to allow larger changes); if the error was small, we use a smaller step size for finer resolution. This adaptive step is what makes it ADPCM (Adaptive Differential PCM).&lt;/p&gt;
    &lt;p&gt;Our codes are as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;00&lt;/code&gt;(0): Go down by 1 step - subtract the step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01&lt;/code&gt;(1): Go up by 1 step - add the step size to our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;10&lt;/code&gt;(2): Go down by 2 steps - subtract the 2 x step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;11&lt;/code&gt;(3): Go up by 2 steps - add the 2 x step size to our current prediction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with this very high level of compression, the predicted waveform manages to track the original audio surprisingly well. The above graph shows a small snippet of the audio: the blue line is the original waveform and the yellow line is the ADPCM decoder’s output.&lt;/p&gt;
    &lt;p&gt;They’re not identical (and we wouldn’t expect them to be), but the general shape is preserved. When you play it back through the little speaker, it’s recognizable and surprisingly good.&lt;/p&gt;
    &lt;p&gt;To make my life easier, I built a quick conversion tool to encode WAV files into this 2-bit ADPCM format. The tool lets me drag-and-drop a WAV, and it gives you the files with the data that can ve dropped into the firmware code.&lt;/p&gt;
    &lt;head rend="h2"&gt;LPC Speech Synthesis&lt;/head&gt;
    &lt;p&gt;Six seconds of audio is cool, but what about longer phrases or even arbitrary speech? Storing anything much longer with raw or ADPCM audio would quickly fill the 16K of flash.&lt;/p&gt;
    &lt;p&gt;For my second experiment, I tried something different: instead of recorded waveform audio, I used an old-school speech synthesis approach. This leverages the fact that spoken language can be encoded very compactly by modeling the human voice, rather than storing the raw sound. Specifically, I integrated a library called Talkie.&lt;/p&gt;
    &lt;p&gt;Talkie is a software implementation of the Texas Instruments LPC speech synthesis architecture from the late 1970s. This was implemented in a variety of chips, most commonly the TMS5220 and TMS5100 speech chips.&lt;/p&gt;
    &lt;p&gt;These were used in things like the original Speak &amp;amp; Spell, arcade games like early Star Wars, and speech add-ons for home computers (e.g. the BBC Micro).&lt;/p&gt;
    &lt;p&gt;The Talkie library (originally by Peter Knight, later added to by Adafruit) comes with a big set of examples and vocabulary. It’s also possible to extract examples from old ROMs from arcade games.&lt;/p&gt;
    &lt;p&gt;Each phrase or word only takes a few hundred bytes or even less, so you can fit quite a lot of speech into a few kilobytes of flash. The trade-off is that the voice has a very computer-esque timbre – think of the Speak &amp;amp; Spell’s voice. It’s clearly synthetic, but still understandable.&lt;/p&gt;
    &lt;p&gt;To say custom sentences not in the library, you either concatenate the available words/phonemes (which can be clunky), or you need to generate new LPC data. The original tools for this are a bit obscure – there’s BlueWizard (a classic Mac app) and PythonWizard (a command-line tool with TK GUI) which can analyze WAV files and produce LPC data.&lt;/p&gt;
    &lt;p&gt;I gave both a try with some success (and a few headaches setting them up). In the end, I cheated a bit and used an AI coding assistant to help me create a streamlined online tool for this.&lt;/p&gt;
    &lt;p&gt;The result is a little web app where I can upload a recording of, say, my own voice, and it outputs the LPC data. It even lets me play back the synthesized voice in-browser to check it.&lt;/p&gt;
    &lt;p&gt;So there we have it – our 10¢ microcontroller now has a voice! By using 2-bit ADPCM compression, we can store short audio clips (up to around 8 seconds) even in 16 KB of flash, and play them back via PWM with decent fidelity.&lt;/p&gt;
    &lt;p&gt;And with the Talkie LPC speech synthesis, we can make the device “speak” lots of words and phrases with only a tiny memory footprint.&lt;/p&gt;
    &lt;p&gt;If you want to hear it for yourself, check out the video demo linked at the top of this post. In the video, you’ll see (and hear) the WarGames clip and the Star Wars quotes running on the hardware. It’s honestly amazing what these cheap little MCUs can do. We’re really pushing the boundaries of cheap hardware here.&lt;/p&gt;
    &lt;p&gt;You can find all my code on GitHub in this repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.atomic14.com/2025/10/29/CH32V003-talking"/><published>2025-10-29T14:12:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747402</id><title>Show HN: HUD-like live annotation and sketching app for macOS</title><updated>2025-10-29T16:13:37.678596+00:00</updated><content>&lt;doc fingerprint="354456b7533f15ed"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Annotate, explain, create - anywhere on your screen&lt;/head&gt;&lt;p&gt;Draw Over It gives presenters, trainers, and any professional an always-ready overlay for live markups. Open it with a hotkey, sketch directly on top of any app, and jump back to work without leaving a trace.&lt;/p&gt;Mac App Store&lt;head rend="h2"&gt;Always-on overlay for live sessions&lt;/head&gt;&lt;p&gt;Stay in flow while you mark up anything on screen. Draw Over It floats above your desktop and hides with a single hotkey.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Overlay the canvas on any app instantly with the menu bar controls or just ââ¥âD combination.&lt;/item&gt;&lt;item&gt;Access the HUD toolkit easily with a right-click.&lt;/item&gt;&lt;item&gt;With just a click, blur everything else to keep attention on your key points.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Tools built for clear annotations&lt;/head&gt;&lt;p&gt;Summon the SwiftUI HUD with a right-click and reach pens, highlighters, shapes, blur, and session controls without leaving the overlay.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Swap between pens, highlighters, rectangles, and circles with one click.&lt;/item&gt;&lt;item&gt;Configure pen widths from 1â64 pt, opacity, fill modes, and color presets.&lt;/item&gt;&lt;item&gt;Use quick presets for Pen sizes, highlight box, and highlighter to stay in rhythm.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Capture your ideas close to the source&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Save the canvas at any time so you can get back to that state quickly.&lt;/item&gt;&lt;item&gt;Export the canvas state as transparent PNGs ready for sharing.&lt;/item&gt;&lt;item&gt;The entire application is available in 14 languages.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Switch tools, tweak width or opacity, and pick presets in one floating palette.&lt;/p&gt;&lt;p&gt;Easily annotate any document, presentation or codebase&lt;/p&gt;&lt;p&gt;Export your canvas to a PNG with a single click&lt;/p&gt;&lt;p&gt;Visualise your ideas close to the source&lt;/p&gt;&lt;p&gt;Easy to use tooling always at your fingertips&lt;/p&gt;&lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;&lt;head rend="h3"&gt;How do I toggle the overlay?&lt;/head&gt;&lt;p&gt;Use ââ¥âD or the status bar menu. The overlay floats above every window until you hide it.&lt;/p&gt;&lt;head rend="h3"&gt;How do I open the tool HUD?&lt;/head&gt;&lt;p&gt;Right-click the overlay or press ââ¥âH. The HUD appears near your pointer for quick adjustments.&lt;/p&gt;&lt;head rend="h3"&gt;Can I erase or undo mistakes?&lt;/head&gt;&lt;p&gt;Hold Option to switch to the eraser or press ââ¥âZ to undo. Redo lives in the HUD and app menus.&lt;/p&gt;&lt;head rend="h3"&gt;Where are exports saved?&lt;/head&gt;&lt;p&gt;Transparent PNG exports live in ~/Library/Application Support/DrawOverIt/Exports inside the app sandbox.&lt;/p&gt;&lt;head rend="h3"&gt;Does the app remember my annotations?&lt;/head&gt;&lt;p&gt;Yes. Sessions persist between launches, and you can save or restore snapshots manually. Enable Ephemeral Canvas if you prefer a fresh slate every time.&lt;/p&gt;&lt;head rend="h3"&gt;Which macOS versions are supported?&lt;/head&gt;&lt;p&gt;Draw Over It targets macOS 13.5 or later with universal binaries for Apple Silicon and Intel.&lt;/p&gt;&lt;head rend="h2"&gt;Ready to draw over anything?&lt;/head&gt;&lt;p&gt;Try it yourself and see how easy visualising your ideas becomes!&lt;/p&gt;Mac App Store&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://draw.wrobele.com/"/><published>2025-10-29T14:36:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747578</id><title>Tether is now the 17th largest holder of US debt</title><updated>2025-10-29T16:13:37.147710+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/paoloardoino/status/1983455972636111011"/><published>2025-10-29T14:50:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747605</id><title>Oracle has adopted BOOLEAN in 23ai and PostgreSQL had it forever</title><updated>2025-10-29T16:13:36.075162+00:00</updated><content>&lt;doc fingerprint="731da2abf15693f9"&gt;
  &lt;main&gt;
    &lt;p&gt;Oracle has finally introduced support for the Boolean data type in the release 23ai. Many thanks to the Engineers at Oracle for implementing this data type for an optimal performance. PL/SQL had BOOLEAN for decades, but developers were not able to declare native boolean type for columns of tables. For this reason, developers returned VARCHAR2/NUMBER from functions instead of BOOLEAN. Interestingly, PostgreSQL, an open-source relational database that has been widely used for many years and a migration target for Oracle, has had support for Boolean data for more than the past two decades. In this article, we will discuss about the workarounds used by developers before Oracle adopted boolean, and how it works in PostgreSQL.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Hidden Cost of Simulating True and False&lt;/head&gt;
    &lt;p&gt;For database engineers and architects designing schemas, it required workarounds to represent "true" or "false" values as ‘Y’/’N’, ‘T’/’F’, or 1/0. While these may function adequately on the surface, they may hinder performance. The lack of a native Boolean data type can be a limitation in database design and impact storage efficiency.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Workarounds We Lived With&lt;/head&gt;
    &lt;p&gt;Before Oracle’s recent version 23ai introduced BOOLEAN as the supported datatypes, following were some of the options.&lt;/p&gt;
    &lt;p&gt;Each of them may add redundant conversions and conditions in application code and PL/SQL functions, leading to more CPU work and larger indexes.&lt;/p&gt;
    &lt;p&gt;While Oracle took 2 decades to bring this up, PostgreSQL had it forever. Postgres stores Boolean values efficiently (internally as a single byte) and allows direct logical operations as follows.&lt;/p&gt;
    &lt;code&gt;SELECT * FROM employees WHERE is_active;&lt;/code&gt;
    &lt;code&gt;UPDATE orders SET is_verified = TRUE WHERE id = 1001;&lt;/code&gt;
    &lt;p&gt;What this clearly means is that it requires – No conversions, no string comparisons, just clean, logical semantics. This approach results in simpler queries, smaller indexes, and faster filtering, especially in analytical workloads with millions of rows.&lt;/p&gt;
    &lt;p&gt;Following chart demonstrates us the table structure in both Oracle and PostgreSQL, where we can avoid the additional checks in the case of PostgreSQL but not in Oracle.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Oracle Table Structure before 23ai&lt;/cell&gt;
        &lt;cell role="head"&gt;Equivalent PostgreSQL Table Structure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;CREATE TABLE user_flags ( user_id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, is_active CHAR(1) CHECK (is_active IN ('Y', 'N')), is_verified NUMBER(1) CHECK (is_verified IN (0, 1)) );&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;CREATE TABLE user_flags ( user_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, is_active BOOLEAN, is_verified BOOLEAN );&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;lb/&gt; Insert Statements&lt;/p&gt;
    &lt;p&gt;You can directly insert TRUE and FALSE if boolean is supported natively, as seen in the case of PostgreSQL.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Oracle Compatible Insert before 23ai&lt;/cell&gt;
        &lt;cell role="head"&gt;PostgreSQL Compatible Insert&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;INSERT INTO user_flags (is_active, is_verified) VALUES ('Y', 1); INSERT INTO user_flags (is_active, is_verified) VALUES ('N', 0);&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;INSERT INTO user_flags (is_active, is_verified) VALUES (TRUE, TRUE); INSERT INTO user_flags (is_active, is_verified) VALUES (FALSE, FALSE);&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;lb/&gt; Select Statements&lt;/p&gt;
    &lt;p&gt;You can see simplified and performant select statements as seen in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Oracle&lt;/cell&gt;
        &lt;cell role="head"&gt;PostgreSQL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;SQL&amp;gt; SELECT * FROM genericmini_cdc.user_flags WHERE is_active = 'Y' AND is_verified = 1; USER_ID | IS_ACTIVE | IS_VERIFIED --------+-----------+------------ 1 | Y | 1 2 | Y | 1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;postgres=# SELECT * FROM user_flags WHERE is_active AND is_verified; user_id | is_active | is_verified --------+-----------+------------ 1 | t | t (1 row)&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In a nutshell, we can see boolean as the optimal data type over other relevant alternatives. Some of such benefits are listed in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Aspect&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;BOOLEAN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;&lt;code&gt;CHAR(1)&lt;/code&gt; / &lt;code&gt;CHAR(5)&lt;/code&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;&lt;code&gt;SMALLINT&lt;/code&gt; / &lt;code&gt;NUMBER(1)&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Meaning&lt;/cell&gt;
        &lt;cell&gt;Explicit logical type with &lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;, and &lt;code&gt;NULL&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Textual convention (e.g., &lt;code&gt;'Y'&lt;/code&gt;, &lt;code&gt;'N'&lt;/code&gt;, &lt;code&gt;'YES'&lt;/code&gt;, &lt;code&gt;'NO'&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Numeric convention (&lt;code&gt;1&lt;/code&gt; = true, &lt;code&gt;0&lt;/code&gt; = false)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;Typically 1 byte or bit&lt;/cell&gt;
        &lt;cell&gt;1–5 bytes depending on length and encoding&lt;/cell&gt;
        &lt;cell&gt;1–2 bytes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Type Safety&lt;/cell&gt;
        &lt;cell&gt;Accepts only logical truth values&lt;/cell&gt;
        &lt;cell&gt;May store invalid text (e.g., &lt;code&gt;'A'&lt;/code&gt;, &lt;code&gt;'?'&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;May store non-logical numbers (e.g., &lt;code&gt;2&lt;/code&gt;, &lt;code&gt;-1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Query Simplicity&lt;/cell&gt;
        &lt;cell&gt;Supports direct logical operations (&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;NOT&lt;/code&gt;, &lt;code&gt;IS TRUE&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Requires explicit comparison (&lt;code&gt;='Y'&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Requires explicit comparison (&lt;code&gt;=1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Value Interpretation&lt;/cell&gt;
        &lt;cell&gt;Automatically interprets &lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;, &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;F&lt;/code&gt;, &lt;code&gt;YES&lt;/code&gt;, &lt;code&gt;NO&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt; (case-insensitive)&lt;/cell&gt;
        &lt;cell&gt;Interpretation depends on convention and case sensitivity&lt;/cell&gt;
        &lt;cell&gt;Limited to numeric values; cannot represent textual forms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Readability&lt;/cell&gt;
        &lt;cell&gt;Self-explanatory and standardized&lt;/cell&gt;
        &lt;cell&gt;Convention-dependent and less portable&lt;/cell&gt;
        &lt;cell&gt;Convention-dependent and less intuitive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Language Integration&lt;/cell&gt;
        &lt;cell&gt;Maps directly to native Boolean types&lt;/cell&gt;
        &lt;cell&gt;Requires conversion logic&lt;/cell&gt;
        &lt;cell&gt;Requires conversion logic&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To simplify end-to-end database migrations from Oracle to PostgreSQL, we have announced our tool called Hexarocket. One of the advantages of using this tool is that – during data migration from Oracle to PostgreSQL, it can automatically map CHAR(1) and NUMBER(1) of Oracle to BOOLEAN in PostgreSQL.&lt;/p&gt;
    &lt;p&gt;Are you looking to migrate from Oracle to PostgreSQL or SQL Server to PostgreSQL? We are here to support with a simple and seamless migration experience within few clicks using HexaRocket. Request us for a demo on HexaRocket today: Schedule a demo.&lt;/p&gt;
    &lt;p&gt;Contact Us Today!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hexacluster.ai/blog/postgresql/oracles-adoption-of-native-boolean-data-type-vs-postgresql/"/><published>2025-10-29T14:51:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747804</id><title>Collins Aerospace: Sending text messages to the cockpit with test:test</title><updated>2025-10-29T16:13:35.397127+00:00</updated><content>&lt;doc fingerprint="ef98aafde223bee7"&gt;
  &lt;main&gt;
    &lt;p&gt;Informed parties: RTX Corporation and Department of Defense Cyber Crime Center (on September 21, 2025)&lt;/p&gt;
    &lt;p&gt;Using the credentials test:test, it was possible to log in at the ARINC OpCenter Message Browser (Screenshot) as U.S. Navy Fleet Logistics Support Wing.&lt;/p&gt;
    &lt;p&gt;Via this web service, text messages can be sent to the cockpit. For obvious reasons, we did not try that. Sent messages could be viewed.&lt;/p&gt;
    &lt;p&gt;Unfortunately, RTX did not respond to our vulnerability report. The account was disabled.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ccc.de/en/disclosure/collins-aerospace-mit-test-test-textnachrichten-bis-ins-cockpit-senden"/><published>2025-10-29T15:07:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748195</id><title>The end of the rip-off economy: consumers use LLMs against information asymmetry</title><updated>2025-10-29T16:13:35.276987+00:00</updated><content/><link href="https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy"/><published>2025-10-29T15:32:46+00:00</published></entry></feed>