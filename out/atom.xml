<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-16T09:11:46.166059+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45592271</id><title>F5 says hackers stole undisclosed BIG-IP flaws, source code</title><updated>2025-10-16T09:11:54.719045+00:00</updated><content>&lt;doc fingerprint="a46e185ad817f7ce"&gt;
  &lt;main&gt;
    &lt;p&gt;U.S. cybersecurity company F5 disclosed that nation-state hackers breached its systems and stole undisclosed BIG-IP security vulnerabilities and source code.&lt;/p&gt;
    &lt;p&gt;The company states that it first became aware of the breach on August 9, 2025, with its investigations revealing that the attackers had gained long-term access to its system, including the company's BIG-IP product development environment and engineering knowledge management platform.&lt;/p&gt;
    &lt;p&gt;F5 is a Fortune 500 tech giant specializing in cybersecurity, cloud management, and application delivery networking (ADN) applications. The company has 23,000 customers in 170 countries, and 48 of the Fortune 50 entities use its products.&lt;/p&gt;
    &lt;p&gt;BIG-IP is the firm's flagship product used for application delivery and traffic management by many large enterprises worldwide.&lt;/p&gt;
    &lt;head rend="h3"&gt;No supply-chain risk&lt;/head&gt;
    &lt;p&gt;It’s unclear how long the hackers maintained access, but the company confirmed that they stole source code, vulnerability data, and some configuration and implementation details for a limited number of customers.&lt;/p&gt;
    &lt;p&gt;"Through this access, certain files were exfiltrated, some of which contained certain portions of the Company's BIG-IP source code and information about undisclosed vulnerabilities that it was working on in BIG-IP," the company states.&lt;/p&gt;
    &lt;p&gt;Despite this critical exposure of undisclosed flaws, F5 says there's no evidence that the attackers leveraged the information in actual attacks, such as exploiting the undisclosed flaw against systems. The company also states that it has not seen evidence that the private information has been disclosed.&lt;/p&gt;
    &lt;p&gt;F5 claims that the threat actors' access to the BIG-IP environment did not compromise its software supply chain or result in any suspicious code modifications.&lt;/p&gt;
    &lt;p&gt;This includes its platforms that contain customer data, such as its CRM, financial, support case management, or iHealth systems. Furthermore, other products and platforms managed by the company are not compromised, including NGINX, F5 Distributed Cloud Services, or Silverline systems' source code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Response to the breach&lt;/head&gt;
    &lt;p&gt;After discovering the intrusion, F5 took remediation action by tightening access to its systems, and improving its overall threat monitoring, detection, and response capabilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotated credentials and strengthened access controls across our systems.&lt;/item&gt;
      &lt;item&gt;Deployed improved inventory and patch management automation, as well as additional tooling to better monitor, detect, and respond to threats.&lt;/item&gt;
      &lt;item&gt;Implemented enhancements to our network security architecture.&lt;/item&gt;
      &lt;item&gt;Hardened our product development environment, including strengthening security controls and monitoring of all software development platforms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, the company also focuses on the security of its products through source code reviews and security assessements with support from NCC Group and IOActive.&lt;/p&gt;
    &lt;p&gt;NCC Group's assessment covered security reviews of critical software components in BIG-IP and portions of the development pipeline in an effort that involved 76 consultants.&lt;/p&gt;
    &lt;p&gt;IOActive's expertise was called in after the security breach and the engagement is still in progress. The results so far show no evidence of the threat actor introducing vulnerablities in critical F5 software source code or the software development build pipeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;Customers should take action&lt;/head&gt;
    &lt;p&gt;F5 is still reviewing which customers had their configuration or implementation details stolen and will contact them with guidance.&lt;/p&gt;
    &lt;p&gt;To help customers secure their F5 environments against risks stemming from the breach, the company released updates for BIG-IP, F5OS, BIG-IP Next for Kubernetes, BIG-IQ, and APM clients.&lt;/p&gt;
    &lt;p&gt;Despite any evidence "of undisclosed critical or remote code execution vulnerabilities," the company urges customers to prioritize installing the new BIG-IP software updates.&lt;/p&gt;
    &lt;p&gt;F5 confirmed that today's updates address the potential impact stemming from the stolen undisclosed vulnerabilities.&lt;/p&gt;
    &lt;p&gt;Furthermore, F5 support makes available a threat hunting guide for customers to improve detection and monitoring in their environment.&lt;/p&gt;
    &lt;p&gt;New best practices for hardening F5 systems now include automated checks to the F5 iHealth Diagnostic Tool, which can now flag security risks, vulnerabilities, prioritize actions, and provide remediation guidance.&lt;/p&gt;
    &lt;p&gt;Another recommendation is to enable BIG-IP event streaming to SIEM and configure the systems to log to a remote syslog server and monitor for login attempts.&lt;/p&gt;
    &lt;p&gt;"Our global support team is available to assist. You can open a MyF5 support case or contact F5 support directly for help updating your BIG-IP software, implementing any of these steps, or to address any questions you may have" - F5&lt;/p&gt;
    &lt;p&gt;The company added that it has validated the safety of BIG-IP releases through multiple independent reviews by leading cybersecurity firms, including CrowdStrike and Mandiant.&lt;/p&gt;
    &lt;p&gt;On Monday, F5 announced that it rotated the cryptographic certcertificates and keys used for signing its digital products. The change affects installing BIG-IP and BIG-IQ TMOS software images while ISO image signature verification is enabled, and installing BIG-IP F5OS tenant images on host systems running F5OS.&lt;/p&gt;
    &lt;p&gt;Additional guidance for F5 customers comes from UK's National Cyber Security Centre (NCSC) and the U.S. Cybersecurity and Infrastructure Security Agency (CISA).&lt;/p&gt;
    &lt;p&gt;Both agencies recommmend identifying all F5 products (hardware, software, and virtualized) and making sure that no management interface is exposed on the public web. If an exposed interface is discovered, companies should make compromise assessment.&lt;/p&gt;
    &lt;p&gt;F5 notes that it delayed the public disclosure of the incident at the U.S. government's request, presumably to allow enough time to secure critical systems.&lt;/p&gt;
    &lt;p&gt;"On September 12, 2025, the U.S. Department of Justice determined that a delay in public disclosure was warranted pursuant to Item 1.05(c) of Form 8-K. F5 is now filing this report in a timely manner," explains F5.&lt;/p&gt;
    &lt;p&gt;F5 states that the incident has no material impact on its operations. All services remain available and are considered safe, based on the latest available evidence.&lt;/p&gt;
    &lt;p&gt;BleepingComputer has contacted F5 to request more details about the incident, and we will update this post when we receive a response.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Security Validation Event of the Year: The Picus BAS Summit&lt;/head&gt;
    &lt;p&gt;Join the Breach and Attack Simulation Summit and experience the future of security validation. Hear from top experts and see how AI-powered BAS is transforming breach and attack simulation.&lt;/p&gt;
    &lt;p&gt;Don't miss the event that will shape the future of your security strategy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bleepingcomputer.com/news/security/f5-says-hackers-stole-undisclosed-big-ip-flaws-source-code/"/><published>2025-10-15T13:33:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45592401</id><title>Pwning the Nix ecosystem</title><updated>2025-10-16T09:11:54.422668+00:00</updated><content>&lt;doc fingerprint="4f2c49954c2797ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pwning the Entire Nix Ecosystem&lt;/head&gt;
    &lt;p&gt;last year at nixcon, me and my friend lexi gave a lightning talk about how we found a vulnerability in nixpkgs that would have allowed us to pwn pretty much the entire nix ecosystem and inject malicious code into nixpkgs. it only took us about a day from starting our search to reporting it and getting it fixed. since i unfortunately was too sick to attend this years nixcon, i thought it might be a good time to write up what we found and how we did it.&lt;/p&gt;
    &lt;head rend="h2"&gt;github actions: the easy target #&lt;/head&gt;
    &lt;p&gt;github actions is a ci/cd system by github that can do pretty much anything in a repo. it’s an easy target for attackers because if you have access to a workflow, you can just commit code without authorization and then you have a supply chain attack. plus, it’s all written in yaml 🇳🇴, which was NEVER meant to be executed !!&lt;/p&gt;
    &lt;code&gt;name: learn-github-actions
on: [push]
jobs:
  check-bats-version:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm install -g bats
      - run: bats -v
&lt;/code&gt;
    &lt;p&gt;this is a simple example of a github action. nothing fancy, just running some commands when code is pushed.&lt;/p&gt;
    &lt;head rend="h2"&gt;the dangerous pull_request_target #&lt;/head&gt;
    &lt;p&gt;actions run when a trigger activates them. there are a bunch of different triggers like pushes, commits, or pull requests. but there’s a special one called &lt;code&gt;pull_request_target&lt;/code&gt; that has a few critical differences from regular &lt;code&gt;pull_request&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;crucially, unlike &lt;code&gt;pull_request&lt;/code&gt;, &lt;code&gt;pull_request_target&lt;/code&gt; has read/write and secret access by default, even on pull requests from forks. this isn’t vulnerable by itself, but things go south when you start trusting user input from those PRs.&lt;/p&gt;
    &lt;p&gt;github even warns about this in their docs:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Warning: For workflows that are triggered by the&lt;/p&gt;&lt;code&gt;pull_request_target&lt;/code&gt;event, the&lt;code&gt;GITHUB_TOKEN&lt;/code&gt;is granted read/write repository permission unless the&lt;code&gt;permissions&lt;/code&gt;key is specified and the workflow can access secrets, even when it is triggered from a fork.&lt;/quote&gt;
    &lt;p&gt;so we started looking for workflows in nixpkgs that use &lt;code&gt;pull_request_target&lt;/code&gt; and found 14 files. some of them were secure, like this labeler example:&lt;/p&gt;
    &lt;code&gt;name: "Label PR"
on:
  pull_request_target:
jobs:
  labels:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/labeler@8558fd74291d67161a8a
        with:
          repo-token: $
&lt;/code&gt;
    &lt;p&gt;this is safe because it just passes the token to a trusted action. but then we found some more interesting ones…&lt;/p&gt;
    &lt;head rend="h2"&gt;the editorconfig vulnerability #&lt;/head&gt;
    &lt;p&gt;the first vulnerable workflow we found was for checking editorconfig rules. here’s a simplified version of what it was doing:&lt;/p&gt;
    &lt;code&gt;steps:
  - name: Get list of changed files from PR
    run: gh api [...] | jq [ ... ] &amp;gt; "$HOME/changed_files"
  - uses: actions/checkout@eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871
    with:
      ref: refs/pull/$/merge
  - name: Checking EditorConfig
    run: cat "$HOME/changed_files" | xargs -r editorconfig-checker
&lt;/code&gt;
    &lt;p&gt;the workflow would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;get a list of files changed in the PR&lt;/item&gt;
      &lt;item&gt;checkout the PR code&lt;/item&gt;
      &lt;item&gt;run editorconfig-checker on those files&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;the problem? it was using &lt;code&gt;xargs&lt;/code&gt; to pass the filenames to editorconfig-checker. if you’ve read the man page for xargs, you’ll see this warning:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is not possible for xargs to be used securely&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;basically, we could create a file with a name that’s actually a command line argument. for example, if we added a file called &lt;code&gt;--help&lt;/code&gt; to our PR, when the workflow ran &lt;code&gt;cat "$HOME/changed_files" | xargs -r editorconfig-checker&lt;/code&gt;, the filename would be passed as an argument to editorconfig-checker, causing it to print its help message instead of checking files.&lt;/p&gt;
    &lt;p&gt;this is a classic command injection vulnerability. we didn’t take it further to try to execute arbitrary code since editorconfig-checker is written in go and we’d need to audit it more deeply, but it’s most likely possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;the code owners vulnerability: local file inclusion #&lt;/head&gt;
    &lt;p&gt;the second vulnerable workflow we found was even more serious. it was checking the CODEOWNERS file in PRs:&lt;/p&gt;
    &lt;code&gt;steps:
  - uses: actions/checkout@eef61447b9ff4aafe5dcd4e0bbf
    with:
      ref: refs/pull/$/merge
      path: pr
  - run: nix-build base/ci -A codeownersValidator
  - run: result/bin/codeowners-validator
    env:
      OWNERS_FILE: pr/ci/OWNERS
&lt;/code&gt;
    &lt;p&gt;the workflow would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;checkout the PR code&lt;/item&gt;
      &lt;item&gt;build the codeowners validator&lt;/item&gt;
      &lt;item&gt;run the validator on the OWNERS file from the PR&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;the validator would echo the contents of the OWNERS file if there was an error. this meant we could put whatever we wanted in that file and it would get printed in the logs.&lt;/p&gt;
    &lt;p&gt;but it gets worse. since the workflow was checking out our PR code, we could replace the OWNERS file with a symbolic link to ANY file on the runner. like, say, the github actions credentials file:&lt;/p&gt;
    &lt;code&gt;$ rm OWNERS
$ ln -s /home/runner/runners/2.320.0/.credentials OWNERS
&lt;/code&gt;
    &lt;p&gt;when the validator ran, it would try to read our symlinked file and helpfully print out an error message containing the first line:&lt;/p&gt;
    &lt;p&gt;and just like that, we had a github actions token with read/write access to nixpkgs. this would let us push directly to nixpkgs, bypassing all the normal review processes.&lt;/p&gt;
    &lt;head rend="h2"&gt;the fix #&lt;/head&gt;
    &lt;p&gt;after we found these vulnerabilities, we reported them to the nixpkgs maintainers, in this case infinisil, who immediately took action:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;they disabled the vulnerable workflows in the repos action settings&lt;/item&gt;
      &lt;item&gt;they fixed the vulnerabilities by properly separating untrusted data from privileged operations&lt;/item&gt;
      &lt;item&gt;they renamed the fixed workflows after the security fixes, this is because of another pitfall with &lt;code&gt;pull_request_target&lt;/code&gt;allowing you to target any branch the action is on, even if it’s 5 or 10 years old as long as it hasn’t been disabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;the key lessons from this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;avoid mixing untrusted data and secrets, or be very careful with them&lt;/item&gt;
      &lt;item&gt;only allow the permissions you really need&lt;/item&gt;
      &lt;item&gt;read the docs about permissions very carefully&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;if you think your org has vulnerable github actions, you can use the panic button too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;go to your org at https://github.com/[org]&lt;/item&gt;
      &lt;item&gt;go to the “Settings” tab&lt;/item&gt;
      &lt;item&gt;go to “Actions” → “General” section&lt;/item&gt;
      &lt;item&gt;under “Policies”, switch “All repositories” to “Disable”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;conclusion #&lt;/head&gt;
    &lt;p&gt;it only took us about a day to find, report, and help fix a vulnerability that could have compromised the entire nix ecosystem. this shows how important it is to be careful with github actions, especially when dealing with &lt;code&gt;pull_request_target&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;big thanks to intrigus and everyone at KITCTF (intrigus gave a talk about exactly these issues that taught us how this works), and thanks to infinisil for fixing this on the same day we reported it.&lt;/p&gt;
    &lt;p&gt;if you want to learn more, check out these resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://kitctf.de/talks/2023-10-26-insecure-github-actions/insecure-github-actions.pdf&lt;/item&gt;
      &lt;item&gt;https://securitylab.github.com/resources/github-actions-preventing-pwn-requests/&lt;/item&gt;
      &lt;item&gt;https://github.com/NixOS/nixpkgs/pull/351446&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;also, if you’re curious, you can watch our original lightning talk from nixcon&lt;/p&gt;
    &lt;p&gt;anyway that’s all. stay safe with your github actions. meow :3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ptrpa.ws/nixpkgs-actions-abuse"/><published>2025-10-15T13:41:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45594920</id><title>Zed is now available on Windows</title><updated>2025-10-16T09:11:54.159807+00:00</updated><content>&lt;doc fingerprint="1d8ec95fb92666b6"&gt;
  &lt;main&gt;
    &lt;p&gt;Zed is now available on Windows. You can download the stable release here. Or if you prefer to live on the bleeding edge, you can use the preview release, which receives new features one week earlier.&lt;/p&gt;
    &lt;p&gt;Windows is now a fully supported platform for Zed. We'll be shipping updates every week, like we do with Mac and Linux. Several Zed engineers use Windows as their daily driver, and we will maintain a full-time Windows team, including @localcc, our Windows platform lead.&lt;/p&gt;
    &lt;p&gt;Read on to learn about the key Windows features.&lt;/p&gt;
    &lt;head rend="h2"&gt;Windows Platform Integration&lt;/head&gt;
    &lt;p&gt;Zed isn't an Electron app; we integrate directly with the underlying platform for maximal control. The Windows build uses DirectX 11 for rendering, and DirectWrite for text rendering, to match the Windows look and feel.&lt;/p&gt;
    &lt;head rend="h2"&gt;WSL and SSH Remoting&lt;/head&gt;
    &lt;p&gt;Zed integrates directly with Windows Subsystem for Linux (WSL). From the WSL terminal, you can open a folder in Zed using the &lt;code&gt;zed&lt;/code&gt; command-line script. And from within Zed, you can open a folder in any of your WSL distros by clicking &lt;code&gt;File &amp;gt; Open Remote&lt;/code&gt; (or running &lt;code&gt;project: open remote&lt;/code&gt; from the command palette) and selecting &lt;code&gt;Add WSL Distro&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similarly, if you're connecting to a remote Linux machine, select &lt;code&gt;Connect New Server&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Under the hood, when editing under WSL or SSH, Zed runs a lightweight "remote server" process under &lt;code&gt;wsl.exe&lt;/code&gt; / &lt;code&gt;ssh.exe&lt;/code&gt;, and all I/O operations are routed through that process. Most features in Zed are designed to work with remote editing: loading and saving files, git integration, terminals, tasks, language servers, and debuggers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extension Compatibility&lt;/head&gt;
    &lt;p&gt;Zed extensions work on Windows; no special steps, no caveats. You can install them from the Extensions panel and get back to coding. And if you want to create a new extension, you can do so without any Windows-specific workarounds.&lt;/p&gt;
    &lt;p&gt;Zed extensions are WebAssembly Components, and they have sandboxed access to the file system via the WebAssembly System Interface (WASI). Zed manages the conversions of file system paths as they are passed into and out of extensions, so that extension authors don't need to worry about the differences between Windows and Unix paths.&lt;/p&gt;
    &lt;head rend="h2"&gt;Agentic Coding on Windows&lt;/head&gt;
    &lt;p&gt;All of Zed’s AI features, including edit predictions and ACP-powered agents, are fully supported on Windows, and in combination with WSL/SSH remoting. Leverage Claude Code directly in Zed through ACP, trial Zed Pro for free for 14 days, or bring your own keys.&lt;/p&gt;
    &lt;head rend="h2"&gt;Use It Today&lt;/head&gt;
    &lt;p&gt;Thank you to everyone who participated in our Alpha &amp;amp; Beta testing, reporting issues on GitHub and Discord. We've fixed a lot of bugs, but we know the work is not over. If you find something amiss, please let us know. We’re especially looking for feedback on WSL workflows, IME and keyboard layouts, multi-monitor setups, and 120–144 Hz displays.&lt;/p&gt;
    &lt;p&gt;Your reports will shape the next set of fixes, features, and polish. Download Zed for Windows, take it for a spin, and tell us what to build next.&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking for a better editor?&lt;/head&gt;
    &lt;p&gt;You can try Zed today on macOS, Windows, or Linux. Download now!&lt;/p&gt;
    &lt;head rend="h3"&gt;We are hiring!&lt;/head&gt;
    &lt;p&gt;If you're passionate about the topics we cover on our blog, please consider joining our team to help us ship the future of software development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://zed.dev/blog/zed-for-windows-is-here"/><published>2025-10-15T16:24:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45595403</id><title>Claude Haiku 4.5</title><updated>2025-10-16T09:11:53.912795+00:00</updated><content>&lt;doc fingerprint="172b3fff2916ca6c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Claude Haiku 4.5&lt;/head&gt;
    &lt;p&gt;Claude Haiku 4.5, our latest small model, is available today to all users.&lt;/p&gt;
    &lt;p&gt;What was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4.5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.&lt;/p&gt;
    &lt;p&gt;Claude Haiku 4.5 even surpasses Claude Sonnet 4 at certain tasks, like using computers. These advances make applications like Claude for Chrome faster and more useful than ever before.&lt;/p&gt;
    &lt;p&gt;Users who rely on AI for real-time, low-latency tasks like chat assistants, customer service agents, or pair programming will appreciate Haiku 4.5’s combination of high intelligence and remarkable speed. And users of Claude Code will find that Haiku 4.5 makes the coding experience—from multiple-agent projects to rapid prototyping—markedly more responsive.&lt;/p&gt;
    &lt;p&gt;Claude Sonnet 4.5, released two weeks ago, remains our frontier model and the best coding model in the world. Claude Haiku 4.5 gives users a new option for when they want near-frontier performance with much greater cost-efficiency. It also opens up new ways of using our models together. For example, Sonnet 4.5 can break down a complex problem into multi-step plans, then orchestrate a team of multiple Haiku 4.5s to complete subtasks in parallel.&lt;/p&gt;
    &lt;p&gt;Claude Haiku 4.5 is available everywhere today. If you’re a developer, simply use claude-haiku-4-5 via the Claude API. Pricing is now $1/$5 per million input and output tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Benchmarks&lt;/head&gt;
    &lt;quote&gt;Claude Haiku 4.5 hit a sweet spot we didn't think was possible: near-frontier coding quality with blazing speed and cost efficiency. In Augment's agentic coding evaluation, it achieves 90% of Sonnet 4.5's performance, matching much larger models. We're excited to offer it to our users.&lt;/quote&gt;
    &lt;quote&gt;Claude Haiku 4.5 is a leap forward for agentic coding, particularly for sub-agent orchestration and computer use tasks. The responsiveness makes AI-assisted development in Warp feel instantaneous.&lt;/quote&gt;
    &lt;quote&gt;Historically models have sacrificed speed and cost for quality. Claude Haiku 4.5 is blurring the lines on this trade off: it's a fast frontier model that keeps costs efficient and signals where this class of models is headed.&lt;/quote&gt;
    &lt;quote&gt;Claude Haiku 4.5 delivers intelligence without sacrificing speed, enabling us to build AI applications that utilize both deep reasoning and real-time responsiveness.&lt;/quote&gt;
    &lt;quote&gt;Claude Haiku 4.5 is remarkably capable—just six months ago, this level of performance would have been state-of-the-art on our internal benchmarks. Now it runs up to 4-5 times faster than Sonnet 4.5 at a fraction of the cost, unlocking an entirely new set of use cases.&lt;/quote&gt;
    &lt;quote&gt;Speed is the new frontier for AI agents operating in feedback loops. Haiku 4.5 proves you can have both intelligence and rapid output. It handles complex workflows reliably, self-corrects in real-time, and maintains momentum without latency overhead. For most development tasks, it's the ideal performance balance.&lt;/quote&gt;
    &lt;quote&gt;Claude Haiku 4.5 outperformed our current models on instruction-following for slide text generation, achieving 65% accuracy versus 44% from our premium tier model—that's a game-changer for our unit economics.&lt;/quote&gt;
    &lt;quote&gt;Our early testing shows that Claude Haiku 4.5 brings efficient code generation to GitHub Copilot with comparable quality to Sonnet 4 but at faster speed. Already we're seeing it as an excellent choice for Copilot users who value speed and responsiveness in their AI-powered development workflows.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Safety evaluations&lt;/head&gt;
    &lt;p&gt;We ran a detailed series of safety and alignment evaluations on Claude Haiku 4.5. The model showed low rates of concerning behaviors, and was substantially more aligned than its predecessor, Claude Haiku 3.5. In our automated alignment assessment, Claude Haiku 4.5 also showed a statistically significantly lower overall rate of misaligned behaviors than both Claude Sonnet 4.5 and Claude Opus 4.1—making Claude Haiku 4.5, by this metric, our safest model yet.&lt;/p&gt;
    &lt;p&gt;Our safety testing also showed that Claude Haiku 4.5 poses only limited risks in terms of the production of chemical, biological, radiological, and nuclear (CBRN) weapons. For that reason, we’ve released it under the AI Safety Level 2 (ASL-2) standard—compared to the more restrictive ASL-3 for Sonnet 4.5 and Opus 4.1. You can read the full reasoning behind the model’s ASL-2 classification, as well as details on all our other safety tests, in the Claude Haiku 4.5 system card.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further information&lt;/head&gt;
    &lt;p&gt;Claude Haiku 4.5 is available now on Claude Code and our apps. Its efficiency means you can accomplish more within your usage limits while maintaining premium model performance.&lt;/p&gt;
    &lt;p&gt;Developers can use Claude Haiku 4.5 on our API, Amazon Bedrock, and Google Cloud’s Vertex AI, where it serves as a drop-in replacement for both Haiku 3.5 and Sonnet 4 at our most economical price point.&lt;/p&gt;
    &lt;p&gt;For complete technical details and evaluation results, see our system card, model page, and documentation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SWE-bench Verified: All Claude results were reported using a simple scaffold with two tools—bash and file editing via string replacements. We report 73.3%, which was averaged over 50 trials, no test-time compute, 128K thinking budget, and default sampling parameters (temperature, top_p) on the full 500-problem SWE-bench Verified dataset.&lt;list rend="ul"&gt;&lt;item&gt;The score reported uses a minor prompt addition: "You should use tools as much as possible, ideally more than 100 times. You should also implement your own tests first before attempting the problem."&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Terminal-Bench: All scores reported use the default agent framework (Terminus 2), with XML parser, averaging 11 runs (6 without thinking (40.21% score), 5 with 32K thinking budget (41.75% score)) with n-attempts=1.&lt;/item&gt;
      &lt;item&gt;τ2-bench: Scores were achieved averaging over 10 runs using extended thinking (128k thinking budget) and default sampling parameters (temperature, top_p) with tool use and a prompt addendum to the Airline and Telecom Agent Policy instructing Claude to better target its known failure modes when using the vanilla prompt. A prompt addendum was also added to the Telecom User prompt to avoid failure modes from the user ending the interaction incorrectly.&lt;/item&gt;
      &lt;item&gt;AIME: Haiku 4.5 score reported as the average over 10 independent runs that each calculate pass@1 over 16 trials with default sampling parameters (temperature, top_p) and 128K thinking budget.&lt;/item&gt;
      &lt;item&gt;OSWorld: All scores reported use the official OSWorld-Verified framework with 100 max steps, averaged across 4 runs with 128K total thinking budget and 2K thinking budget per-step configured.&lt;/item&gt;
      &lt;item&gt;MMMLU: All scores reported are the average of 10 runs over 14 non-English languages with a 128K thinking budget.&lt;/item&gt;
      &lt;item&gt;All other scores were averaged over 10 runs with default sampling parameters (temperature, top_p) and 128K thinking budget.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All OpenAI scores reported from their GPT-5 post, GPT-5 for developers post, GPT-5 system card (SWE-bench Verified reported using n=500), and Terminal Bench leaderboard (using Terminus 2). All Gemini scores reported from their model web page, and Terminal Bench leaderboard (using Terminus 1).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-haiku-4-5"/><published>2025-10-15T16:55:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45595724</id><title>Are hard drives getting better?</title><updated>2025-10-16T09:11:53.409100+00:00</updated><content>&lt;doc fingerprint="d2543893cca2059d"&gt;
  &lt;main&gt;
    &lt;p&gt;If you’ve hung around Backblaze for a while (and especially if you’re a Drive Stats fan), you may have heard us talking about the bathtub curve. In Drive Failure Over Time: The Bathtub Curve Is Leaking, we challenged one of reliability engineering’s oldest ideas—the notion that drive failures trace a predictable U-shaped curve over time.&lt;/p&gt;
    &lt;p&gt;But, the data didn’t agree. Our fleet showed dips, spikes, and plateaus that refused to behave. Now, after 13 years of continuous data, the picture is clearer—and stranger.&lt;/p&gt;
    &lt;p&gt;The bathtub curve isn’t just leaking, and the shape of reliability might look more like an ankle-high wall at the entrance to a walk-in shower. The neat story of early failures, calm middle age, and gentle decline no longer fits the world our drives inhabit. Drives are getting better—or, more precisely, the Drive Stats dataset says that our drives are performing better in data center environments.&lt;/p&gt;
    &lt;p&gt;So, let’s talk about what our current “bathtub curve” looks like, and how it compares to earlier generations of the analysis.&lt;/p&gt;
    &lt;p&gt;The TL;DR: Hard drives are getting better, and lasting longer.&lt;/p&gt;
    &lt;head rend="h2"&gt;The intro: Let’s talk bathtub curve&lt;/head&gt;
    &lt;p&gt;If you’ve spent any time around hardware reliability, you’ve seen it: a smooth U-shaped line called the bathtub curve. It promises order in the chaos of failure—a story where devices begin life with a burst of defects, settle into steady performance, and finally wear out in predictable decline. And, this is what it looks like:&lt;/p&gt;
    &lt;p&gt;For decades, it’s been engineering shorthand for how things die. But as our dataset has grown—more than a decade of drive telemetry and millions of drive-days—the data is clear: Our real drive population is more complicated.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the bathtub curve looked like then&lt;/head&gt;
    &lt;p&gt;The first time we ran this analysis was in 2013, and when we updated the article in 2021, we shared this chart:&lt;/p&gt;
    &lt;p&gt;It shows the annualized failure rate (AFR) of the full drive pool over time (in years) at two different look-back points—2013 and 2021. At that time, you could already see that the bathtub curve was starting to, as the venerable Andy Klein put it, “leak.” The 2013 data looks the closest to a true bathtub curve, while the 2021 data shows fewer early failures and a lower failure rate for more years. We also see the average longevity of drives goes up by about two years before spiking into the failure zone.&lt;/p&gt;
    &lt;head rend="h3"&gt;Numbers can both define and obscure reality&lt;/head&gt;
    &lt;p&gt;Now, there are some very interesting factors that come into play when comparing hard drive reliability over time. For example, our usual caveats about how we use drives vs. how consumers use drives, how our workloads have changed over time, etc. More importantly, though, because we’re comparing averages, it’s easy to lose track of the context around our dataset—how many hard drives are we talking about in 2013 vs. 2021?&lt;/p&gt;
    &lt;p&gt;When we did this analysis in 2013, Backblaze had been open for six years, but we’d only been publishing the Drive Stats dataset since 2013. So, arriving at presenting a look-back at the data (i.e., this is how many drives failed when they were between zero and one years old) was a bit of a math problem compared to our usual data reporting. We were talking about drives that entered the drive pool in 2007, and those were ones we hadn’t shared complete daily logs about, even if the drive was still in service in 2013 (which, as you can tell from the data, was unlikely). We achieved that by looking at failures vs. logged on hours, and when we re-created the analysis recently, we used this SQL query:&lt;/p&gt;
    &lt;quote&gt;CREATE VIEW introduction_dates AS&lt;lb/&gt;-- Calculate the introduction date of drives that were already in service on 2013-04-10&lt;lb/&gt;SELECT serial_number, date(date_add('hour', -1 * smart_9_raw, TIMESTAMP '2013-04-10 00:00:00')) AS introduced&lt;lb/&gt;FROM drivestats&lt;lb/&gt;WHERE date = DATE '2013-04-10'&lt;lb/&gt;UNION&lt;lb/&gt;-- Use the minimum date for drives that entered service after after 2013-04-10&lt;lb/&gt;SELECT serial_number, MIN(date) as introduced&lt;lb/&gt;FROM drivestats&lt;lb/&gt;WHERE serial_number NOT IN (&lt;lb/&gt;SELECT serial_number&lt;lb/&gt;FROM drivestats&lt;lb/&gt;WHERE date = DATE '2013-04-10'&lt;lb/&gt;)&lt;lb/&gt;GROUP BY serial_number;&lt;lb/&gt;SELECT&lt;lb/&gt;date_diff('day', d2.introduced, d1.date) / 91 AS age_in_quarters,&lt;lb/&gt;100 * 365 * (cast(SUM(d1.failure) AS DOUBLE) / COUNT(*)) AS afr&lt;lb/&gt;FROM drivestats AS d1&lt;lb/&gt;INNER JOIN introduction_dates AS d2&lt;lb/&gt;ON d1.serial_number = d2.serial_number&lt;lb/&gt;GROUP BY 1&lt;lb/&gt;ORDER BY 1;&lt;/quote&gt;
    &lt;p&gt;Our drive pool looked a lot different in 2013 as well. Not only was it smaller (~35,000 drives and over 100PB of data were live as of September 2014), but it also was made up of “consumer” drives. While we didn’t see much of a difference between the two when we actually tested them in the environment, we did a lot of drive farming in those days, a process that included actually “shelling” the drives and removing them from their housings—which means that our drive pool had a lot more potential to get some bumps along the way. Hard drives are pretty resilient and we were careful, but it’s worth noting.&lt;/p&gt;
    &lt;p&gt;By the time we were doing this analysis in 2021, we had a lot more data and a lot more storage drives—206,928 or so. Between 2013 and 2021, we had added capacity to our Sacramento data center; expanded our data center regions with locations in Phoenix and Amsterdam, with more on the way in 2022; we launched Backblaze B2 Cloud Storage; and, we went public.&lt;/p&gt;
    &lt;p&gt;All those things are cool from a historical perspective, but the more impactful thing to pay attention to is that any time you have less data (read: a smaller number of total drives), each individual data point has more impact on the whole. In the bathtub curve, you naturally reduce the number of drives as they get older—every drive has a day one, but not every drive has a day 1,462 (or, in lay people’s terms: four years, one day). With fewer drives, more spikes. So, if you start off with more drives, your numbers are likely to be more steady—unless there’s a real problem, or you’re entering your true drive pool failure zone.&lt;/p&gt;
    &lt;p&gt;And, since we’ve transitioned to buying more drives, and decommissioning drives in a different way—well, that all affects what the end result is. More on our drive hygiene habits later; for now, let’s get into our current data.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the bathtub curve looks like now&lt;/head&gt;
    &lt;p&gt;Without further ado, let’s look at the failure rates in our current Backblaze drive pool:&lt;/p&gt;
    &lt;p&gt;That’s a pretty solid deviation in both age of drive failure and the high point of AFR from the last two times we’ve run the analyses. When we ran our 2025 numbers (at the close of Q2 2025), we reported on 317,230 drives. Take that as an approximate raw number given the normal drive exclusions in each Drive Stats report, but it gets you in the ballpark.&lt;/p&gt;
    &lt;p&gt;For consistency’s sake, here’s 2013:&lt;/p&gt;
    &lt;p&gt;And here’s 2021:&lt;/p&gt;
    &lt;p&gt;What’s missing, and a bit difficult to visualize, is the scale on both the x axis (time in years) and the y axis (annualized failure rate expressed in percentage). Let’s put all three on the same chart:&lt;/p&gt;
    &lt;p&gt;Note that both the 2013 data and the 2021 data have high failure percentage peaks at some point near the end of their drive lifetimes. In 2013, it was 13.73% at about 3 years, 3 months (and 13.30% at 3 years, 9 months). In 2021, it’s 14.24%, with that peak hitting at 7 years, 9 months.&lt;/p&gt;
    &lt;p&gt;Now, compare that with the 2025 data: Our peak is 4.25% at 10 years, 3 months (woah). Not only is that a significant improvement in drive longevity, it’s also the first time we’ve seen the peak drive failure rate at the hairy end of the drive curve. And, it’s about a third of each of the other failure peaks.&lt;/p&gt;
    &lt;p&gt;Meanwhile, we see that the drive failure rates on the front end of the curve are also incredibly low—when a drive is between zero and one years old, we barely crack 1.30% AFR. For reference, the most recent quarterly AFR is 1.36%.&lt;/p&gt;
    &lt;p&gt;Still, if we take a look at the trendlines, we can see that the 2021 and the 2025 data isn’t too far off, shape-wise. That is, we see a pretty even failure rate through the significant majority of the drives’ lives, then a fairly steep spike once we get into drive failure territory.&lt;/p&gt;
    &lt;p&gt;What does that mean? Well, drives are getting better, and lasting longer. And, given that our trendlines are about the same shape from 2021 to 2025, we should likely check back in when 2029 rolls around to see if our failure peak has pushed out even further.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hey, what about that data contextualization you did above?&lt;/head&gt;
    &lt;p&gt;Good point—there are significant things that have changed about our dataset that may be affecting our numbers. We’ve already tackled the consumer vs. enterprise drive debate, and while we don’t have updated testing on that front, there are other things about buying drives at scale that may have an effect on the data.&lt;/p&gt;
    &lt;p&gt;For instance, because we buy drives in bulk, that means that a big chunk of drives enter our data pool at the same time. Given that we, over the years, have really only seen model-by-model variation, this means that if you get a lemon of a drive and you’ve added a lot of them, you may have a chunk of drives failing all at once.&lt;/p&gt;
    &lt;p&gt;Also, we have a different process for decommissioning drives these days. There are lots of things that go into that strategy, but you can simplify it all to risk management and our ability to grow our storage footprint over time. From a practical perspective, that means sometimes there are drives that are still performing well that we decide to take out of service anyway—and that means they get taken out of the fleet without ever having failed. Since our analyses above are based on annualized failure rate vs. age of drive, you can see a big drop in drive population without the expected failure rate spike.&lt;/p&gt;
    &lt;p&gt;Finally, we have different standards for new drives. Some of them just have to do with the industry at large—drives are getting bigger, and storage patterns are changing. But, compared with 2013, when a natural disaster forced us to innovate in unexpected ways, we’ve got more flexibility to consider our purchases, and to do so in a way that’s specific to our environment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was the bathtub curve just wrong?&lt;/head&gt;
    &lt;p&gt;The issue isn’t that the bathtub curve is wrong—it’s that it’s incomplete. It treats time as the only dimension of reliability, ignoring workload, manufacturing variation, firmware updates, and operational churn. And, it rests on a set of assumptions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Devices are identical and operate under the same conditions.&lt;/item&gt;
      &lt;item&gt;Failures happen independently, driven mostly by time.&lt;/item&gt;
      &lt;item&gt;The environment stays constant across a product’s life.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The good news: When it comes to data centers, most of these are as true as they can be in a real-world environment. Data centers environments attempt to be as consistent as possible to be able to reduce power consumption, and to be able to properly anticipate and plan data workloads. Basically, consistency = a happy data center.&lt;/p&gt;
    &lt;p&gt;That said, conditions can’t ever be perfect. Our numbers have always and will always reflect both good planning and the unforeseen aspects of reality. Understanding whether drives are “good” or “bad” is always a conversation between what you theorize (in this case, the bathtub curve) and what happens (the Drive Stats dataset).&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next?&lt;/head&gt;
    &lt;p&gt;Why does all this talk of numbers matter? Well, as we’ve expanded our drive pool over time, in some ways, we’ve increased confidence in the results we’re seeing, both on day one and day 1,462. Even if we had the exact same drives models and drive pool make up (by percentage) from 2013 that we did in 2021, having more of them would give us better results. But, now we have a greater diversity of drives and more of them.&lt;/p&gt;
    &lt;p&gt;That doesn’t mean we’re the be-all, end-all of drive reliability, but it does give us some more footing to slice and dice the data and bring it back to you. As always, you can find the full Drive Stats dataset on our website, which means you can repeat this experiment, or use the data in any way you can imagine. Stay tuned for our quarterly reports and more articles from the Drive Stats extended universe—and feel free to sign up for the Drive Stats newsletter if you want to stay up-to-date.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.backblaze.com/blog/are-hard-drives-getting-better-lets-revisit-the-bathtub-curve/"/><published>2025-10-15T17:18:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45597006</id><title>A Gemma model helped discover a new potential cancer therapy pathway</title><updated>2025-10-16T09:11:53.102365+00:00</updated><content>&lt;doc fingerprint="95c2d199ccbb819e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How a Gemma model helped discover a new potential cancer therapy pathway&lt;/head&gt;
    &lt;p&gt;Today, as part of our research collaboration with Yale University, we’re releasing Cell2Sentence-Scale 27B (C2S-Scale), a new 27 billion parameter foundation model designed to understand the language of individual cells. Built on the Gemma family of open models, C2S-Scale represents a new frontier in single-cell analysis.&lt;/p&gt;
    &lt;p&gt;This announcement marks a milestone for AI in science. C2S-Scale generated a novel hypothesis about cancer cellular behavior and we have since confirmed its prediction with experimental validation in living cells. This discovery reveals a promising new pathway for developing therapies to fight cancer.&lt;/p&gt;
    &lt;p&gt;This launch builds upon our work from earlier this year, where we demonstrated that biological models follow clear scaling laws — just like with natural language, larger models perform better on biology. This work raised a critical question: Does a larger model just get better at existing tasks, or can it acquire entirely new capabilities? The true promise of scaling lies in the creation of new ideas, and the discovery of the unknown.&lt;/p&gt;
    &lt;head rend="h3"&gt;How C2S-Scale 27B works&lt;/head&gt;
    &lt;p&gt;A major challenge in cancer immunotherapy is that many tumors are “cold” — invisible to the body's immune system. A key strategy to make them “hot” is to force them to display immune-triggering signals through a process called antigen presentation.&lt;/p&gt;
    &lt;p&gt;We gave our new C2S-Scale 27B model a task: Find a drug that acts as a conditional amplifier, one that would boost the immune signal only in a specific “immune-context-positive” environment where low levels of interferon (a key immune-signaling protein) were already present, but inadequate to induce antigen presentation on their own. This required a level of conditional reasoning that appeared to be an emergent capability of scale; our smaller models could not resolve this context-dependent effect.&lt;/p&gt;
    &lt;p&gt;To accomplish that, we designed a dual-context virtual screen to find this specific synergistic effect. The virtual screen involved two stages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Immune-Context-Positive: We provided the model with real-world patient samples with intact tumor-immune interactions and low-level interferon signaling.&lt;/item&gt;
      &lt;item&gt;Immune-Context-Neutral: We provided the model with isolated cell line data with no immune context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We then simulated the effect of over 4,000 drugs across both contexts and asked the model to predict which drugs would only boost antigen presentation in the first context, to bias the screen towards the patient-relevant setting. Out of the many drug candidates highlighted by the model, a fraction (10-30%) of drug hits are already known in prior literature, while the remaining drugs are surprising hits with no prior known link to the screen.&lt;/p&gt;
    &lt;head rend="h2"&gt;From prediction to experimental validation&lt;/head&gt;
    &lt;p&gt;The model's predictions were clear. It identified a striking “context split” for the kinase CK2 inhibitor called silmitasertib (CX-4945). The model predicted a strong increase in antigen presentation when silmitasertib was applied in the “immune-context-positive” setting, but little to no effect in the “immune-context-neutral” one. What made this prediction so exciting was that it was a novel idea. Although CK2 has been implicated in many cellular functions, including as a modulator of the immune system, inhibiting CK2 via silmitasertib has not been reported in the literature to explicitly enhance MHC-I expression or antigen presentation. This highlights that the model was generating a new, testable hypothesis, and not just repeating known facts.&lt;/p&gt;
    &lt;p&gt;A prediction, however, is only valuable if it can be validated in clinical application. The real test is first in the lab, and eventually, in the clinic.&lt;/p&gt;
    &lt;p&gt;For the next phase of our project, we took this hypothesis to the lab bench and tested it in human neuroendocrine cell models — a cell type that was completely unseen by the model during training. The experiments demonstrated:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treating the cells with silmitasertib alone had no effect on antigen presentation (MHC-I).&lt;/item&gt;
      &lt;item&gt;Treating the cells with a low dose of interferon alone had a modest effect.&lt;/item&gt;
      &lt;item&gt;Treating the cells with both silmitasertib and low-dose interferon produced a marked, synergistic amplification of antigen presentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Remarkably, in our lab tests the combination of silmitasertib and low-dose interferon resulted in a roughly 50% increase in antigen presentation, which would make the tumor more visible to the immune system.&lt;/p&gt;
    &lt;p&gt;The model’s in silico prediction was confirmed multiple times in vitro. C2S-Scale had successfully identified a novel, interferon-conditional amplifier, revealing a new potential pathway to make “cold” tumors “hot,” and potentially more responsive to immunotherapy. While this is an early first step, it provides a powerful, experimentally-validated lead for developing new combination therapies, which use multiple drugs in concert to achieve a more robust effect.&lt;/p&gt;
    &lt;p&gt;This result also provides a blueprint for a new kind of biological discovery. It demonstrates that by following the scaling laws and building larger models like C2S-Scale 27B, we can create predictive models of cellular behavior that are powerful enough to run high-throughput virtual screens, discover context-conditioned biology, and generate biologically-grounded hypotheses.&lt;/p&gt;
    &lt;p&gt;Teams at Yale are now exploring the mechanism uncovered here and testing additional AI-generated predictions in other immune contexts. With further preclinical and clinical validation, such hypotheses may be able to ultimately accelerate the path to new therapies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started with C2S-Scale 27B&lt;/head&gt;
    &lt;p&gt;The new C2S-Scale 27B model and its resources are available today for the research community. We invite you to explore these tools, build on our work and help us continue to translate the language of life.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read the full scientific preprint on bioRxiv.&lt;/item&gt;
      &lt;item&gt;Explore the model and resources on Hugging Face.&lt;/item&gt;
      &lt;item&gt;Access the code on GitHub.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"/><published>2025-10-15T19:04:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45598590</id><title>Next Steps for the Caddy Project Maintainership</title><updated>2025-10-16T09:11:52.657358+00:00</updated><content>&lt;doc fingerprint="3904d3597accc7fe"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;tldr: I won’t personally see all comments/issues/PRs anymore; maintainer team is being granted tag+release privileges; community will be more involved with leadership; increase current bus factor of 1; unblock the project where I am the bottleneck; help the project scale better.&lt;/p&gt;
      &lt;p&gt;Caddy is now about 11 years old, and the project has changed a lot over that time, and grown hugely popular! To shed some perspective…&lt;/p&gt;
      &lt;head rend="h1"&gt;What it used to be like&lt;/head&gt;
      &lt;p&gt;For years, my daily-ish routine involved checking my GitHub notifications – usually around 1-3 – triaging them and responding to each one of them personally. Most issues were obvious: bugs that needed urgent fixing, features that were a clear yes/no for the project, or questions that had easy answers.&lt;/p&gt;
      &lt;p&gt;Even after the launch of v2, the project was still new and developing, most other people didn’t have a lot of experience with it, and my vision was clear, so it was pretty easy to answer questions, make decisions, review the trickle of pull requests, etc. I wrote most of the code and was familiar with it.&lt;/p&gt;
      &lt;p&gt;My notification inbox essentially became my TODO list, and it was fairly easy to keep under 1 page (or about 25 notifications). At any given time, Caddy almost never had more than 100 open issues or 25 open PRs.&lt;/p&gt;
      &lt;p&gt;Later, we set up a forum, which I’d check multiple times per day and reply to questions there. Usually about 1-3 posts per day. No problem keeping up with it all. I read every single topic for years, and answered many of them myself to help educate others and be aware of user experiences, etc.&lt;/p&gt;
      &lt;p&gt;I tagged and published every single release. Sometimes multiple per day (oops). Over 100 now.&lt;/p&gt;
      &lt;head rend="h1"&gt;How it changed over time&lt;/head&gt;
      &lt;p&gt;As the project grew, the docs improved substantially via contributions. More nits and edge cases were covered. Examples were added (and more to come, I’m sure).&lt;/p&gt;
      &lt;p&gt;Knowledge began to accumulate in the community, meaning that people could answer more questions by search results, and help others find answers to their questions, which tended to grow more niche since the general questions were answered. (This is precisely the outcome I’d hoped for over years with a public forum.)&lt;/p&gt;
      &lt;p&gt;You may recognize some of these people who stuck around as they gained experience, and have helped others in our community and with code maintenance (in no particular order): @Whitestrake , @francislavoie , @elcore , @abiosoft , @Mohammed90 , @WeidiDeng , @tobya , @timelordx , @elee , @hairyhenderson , and many others who have contributed their time and skills to help out. I am very appreciative! As are thousands of lurkers. &lt;/p&gt;
      &lt;head rend="h1"&gt;What it’s like now&lt;/head&gt;
      &lt;p&gt;Forum activity is up about 2-5x. Where we used to get 1 topic per day, sometimes it’s up to 10 (it fluctuates, but the average is about 3-5). And posts average around 5-15. It can be higher when there’s people actively helping answer questions. This is not huge, but it’s a lot for just myself and our little community. Our forum gets about 50,000 page views per day!&lt;/p&gt;
      &lt;p&gt;Many of the questions now are either so niche that I don’t have the skills/expertise to answer them (many, many questions are less about Caddy specifically these days, and more about external system configurations, third-party software integrations, etc.), OR they are trivial/routine enough that others who have a bit of experience can easily answer them (i.e. I don’t have to be the one to respond, since the knowledge is shared by many now).&lt;/p&gt;
      &lt;p&gt;On GitHub, my notification inbox is almost out of control: I have just under 200 in the inbox, or about 8 pages – and that’s my TODO list that I work through each day. Caddy has almost 200 open issues and over 50 open PRs. I wake up to about 10-25 new notifications per day now, instead of 1-3. Again, this is still quite good for a project of our size, but it’s more than just the backlog…&lt;/p&gt;
      &lt;p&gt;The issues are also more obscure and less obvious. For example, bugs used to be pretty obvious and easy to reproduce. Most could be fixed in a few minutes or a day. Now, the project is so stable and mature that most bugs require extensive explaining and troubleshooting, and very specific configurations, to reproduce. Many are related to subtle interactions with the Go standard library or upstream dependencies, or even OS kernels. They take longer, and require more specific expertise, than Ye Olde Bugs of Yore. And most of them are very edge-casey anyway. Few people hit these bugs, and rarely. (This is right where we want to be!) Special thank-you to @WeidiDeng for taking care of so many transport-related issues (weird quirks with different HTTP versions), and @hairyhenderson with metrics, and @Mohammed90 for CI issues, and @francislavoie for a lot of the Caddyfile and config things. I cannot imagine having to figure out all that stuff myself.&lt;/p&gt;
      &lt;p&gt;Feature requests are also more nuanced than before. Caddy 2 has more or less achieved my vision of the web server I started in 2014. To clarify, it’s not done… there is plenty more to do; we will continue to evolve and adapt the project to a changing Internet landscape. But many of the big and obvious features have mostly shipped. And the plugin architecture is powerful enough that nearly all new features can be implemented as separate plugins before being added to our code base. (Plugins can be added to our repository, but these days most need to be proven outside of it first.)&lt;/p&gt;
      &lt;p&gt;All this means that I have started falling behind, for the last couple years, to personally keep up with every single:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Comment&lt;/item&gt;
        &lt;item&gt;New issue&lt;/item&gt;
        &lt;item&gt;New PR&lt;/item&gt;
        &lt;item&gt;Code review&lt;/item&gt;
        &lt;item&gt;Requested review&lt;/item&gt;
        &lt;item&gt;Dependency update&lt;/item&gt;
        &lt;item&gt;Forum topic&lt;/item&gt;
        &lt;item&gt;Forum reply&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;in the Caddy org on GitHub, and these forums. I can’t close issues, answer questions, and merge PRs as quickly and easily now because the nature of their complexity is changing. I have started to become a bottleneck in the project’s growth and development.&lt;/p&gt;
      &lt;head rend="h1"&gt;Next steps&lt;/head&gt;
      &lt;p&gt;The stress of such a huge and growing backlog – combined with the increasing nuance/specificity of issues, feature requests, and questions – has strained my mental health and work habits, and added strain on my family life. So after talking with my wise and wonderful wife, I am making the decision to turn off most notifications on GitHub and the forum, so that I can prioritize work that only I can do (or am the most qualified to do), and my family.&lt;/p&gt;
      &lt;p&gt;In other words, new activity of all kinds (listed above ) won’t automatically add itself to my TODO list. I won’t see every comment and issue like I do today. I don’t need to, either, it’s kind of getting bad for my mental health to try to keep track of the hundreds of discussions.&lt;/p&gt;
      &lt;p&gt;To clarify, I’ll still be very actively engaged with the project. I’ll still be notified of specific events, and I will still be checking GitHub and the forums ~daily, and replying to issues and questions as I have time for them.&lt;/p&gt;
      &lt;p&gt;I will also be clearing out my existing TODO list. It will be manually curated instead. 200 issues in my backlog… that’s a disservice to everyone who is contributing. You’ll get lost in there. It’s time for me to let the community take another step up as a mature project.&lt;/p&gt;
      &lt;p&gt;All this time, I have been the only one with the key to tag and publish releases. I will be granting privileges to our maintainer team to tag new releases going forward. Any new release should require approval from at least 2 maintainers.&lt;/p&gt;
      &lt;p&gt;We’ll also be looking to grow our maintainer team. The best way to join is to start reviewing PRs and submit patches for reported bugs. You can also help improve our documentation/website, help with CI/dependencies, etc. We’ll send out maintainer invites to people who show consistent patterns of making valuable contributions and an understanding of our project’s values.&lt;/p&gt;
      &lt;p&gt;We may also add more collaborators to the project, to help get PRs merged, but with less privileges than maintainers. Again, to be invited, get involved and demonstrate patterns of valuable contributions.&lt;/p&gt;
      &lt;p&gt;A consensus from the maintainer team will be sufficient to add new maintainers and collaborators, and two or more can remove those who are inactive for an extended period of time. We’ll strive to enforce best security practices when it comes to access to the project. (We already require 2FA, for example.)&lt;/p&gt;
      &lt;p&gt;This should help increase the current bus factor of 1, and unblock the project where I’ve been the bottleneck. And lower my stress and improve my mental health and ability to deliver quality work.&lt;/p&gt;
      &lt;head rend="h1"&gt;Big thank you&lt;/head&gt;
      &lt;p&gt;Huge thank you to everyone who contributes and helps in any way – we value your participation, and hope you will continue to do so, and if interested, become a collaborator or maintainer with our project!&lt;/p&gt;
      &lt;p&gt;Also, the only reason this project has survived so long is because of our sponsors – thank you for making it what it is! Without you I would have had to pack up shop years ago and let the project kind of… I dunno, mold? Whatever stale open source projects do. So thank you for continuing to sponsor. I look forward to continuing to serve and support you for years to come.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://caddy.community/t/next-steps-for-the-caddy-project-maintainership/33076"/><published>2025-10-15T21:32:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45599084</id><title>ImapGoose</title><updated>2025-10-16T09:11:51.603676+00:00</updated><content>&lt;doc fingerprint="7c8ce5dfac29a4cb"&gt;
  &lt;main&gt;
    &lt;p&gt;ImapGoose is a small program to keep local mailboxes in sync with an IMAP server. The wording “keep [â¦] in sync” implies that it does so continuously, rather than a one-time sync. ImapGoose is designed as a daemon, monitoring both the IMAP server and the local filesystem, and immediately synchronising changes. When the IMAP server receives an email, it shows up in the filesystem within a second. When an email is deleted on another email client, it is removed1 from the filesystem within a second.&lt;/p&gt;
    &lt;p&gt;ImapGoose is highly optimised to reduce the amount of network traffic and tasks performed. To do so, it relies on a few modern IMAP extensions and only supports modern email servers. “Modern servers” in the context of email means servers which support extensions which were standardised between 2005 and 2009.&lt;/p&gt;
    &lt;p&gt;ImapGoose uses the CONDSTORE extension (standardised in 2006), which basically allows it to tell the server “I last saw this mailbox when it was in state XYZ, please tell me what’s new”. This avoids the need to download an entire message list (which can be tens of thousands of emails), making incremental syncs much more efficient. It also uses the QRESYNC extension (standardised in 2008) so that the server includes a list of deleted messages too (i.e. &lt;code&gt;VANISHED&lt;/code&gt;). Finally, ImapGoose uses the NOTIFY extension
(standardised in 2009), which allows an IMAP client to tell the server
“please let me know when there are changes to these mailboxes”, and then leave a
connection open. &lt;code&gt;NOTIFY&lt;/code&gt; has two nice consequences: (1) the client doesn’t need
to ask the server if there have been any changes at regular intervals, and (2)
the client is informed of any changes immediately, so they can be processed
without delay. Unlike the older IDLE extension (from 1996), NOTIFY (from 2009)
allows monitoring multiple mailboxes per connection, rather than just one.&lt;/p&gt;
    &lt;p&gt;In this article, I’ll cover some of the general design details, inner workings and other development details.&lt;/p&gt;
    &lt;head rend="h1"&gt;General mode of operation&lt;/head&gt;
    &lt;p&gt;First off, ImapGoose keeps a small status database with some minor metadata about the last-seen status of both the server and local Maildirs. This includes the mapping between server UIDs and filesystem filenames. Its general design is strongly inspired by how OfflineIMAP works.&lt;/p&gt;
    &lt;p&gt;At start-up, ImapGoose lists all mailboxes in the server and in the local filesystem. It then starts monitoring them (the server via NOTIFY, the client via inotify/kqueue), so we receive notifications of any changes that may happen after our initial listing. This ensures that, for example, if we receive a new email while performing the initial sync, we get a notification for it.&lt;/p&gt;
    &lt;p&gt;Once monitoring is set up, ImapGoose queues a task to perform a full sync of each mailbox. Initially, we determine if this is the first time we see this mailbox by its absence in the status database. If this mailbox has not been seen before, then we request all messages. The server returns all of these along with a &lt;code&gt;HIGHESTMODSEQ&lt;/code&gt;, which we store in the status database. This &lt;code&gt;HIGHESTMODSEQ&lt;/code&gt;
is a numeric property of each mailbox and increases every time a change occurs
inside that mailbox. If a mailbox has been seen before, then we can ask the
server for changes since that &lt;code&gt;HIGHESTMODSEQ&lt;/code&gt;, which delivers only the minimal
amount of data which we need, and nothing else about all the other thousands of
unchanged messages.&lt;/p&gt;
    &lt;p&gt;When a message is present in the server and absent in the filesystem (or vice versa), we need to determine whether it is a new message, or if it is a message that was previously present in both and deleted from the local filesystem. To determine this, we use the status database and apply the exact same algorithm as offlineimap. It’s simple and well tested.&lt;/p&gt;
    &lt;p&gt;At times, ImapGoose may disconnect from the server (for example, due to a laptop disconnecting from Wi-Fi, or going into sleep mode). It will try to re-connect automatically using an exponential back-off: after 1 second, then after 2 seconds, 4 seconds, 8 seconds, 16 seconds, 32 seconds,â¦ all the way up to 17 minutes. Then it will continue retrying every 17 minutes. This means users don’t really have to worry about ImapGoose’s current state, whether it’s still working, etc. It knows how to back-off when there’s no network and how to get back to work when it is feasible again.&lt;/p&gt;
    &lt;p&gt;As mentioned above, ImapGoose “queues” sync tasks. Internally, it uses a task queue; when changes are detected on the server, a task to sync that entire mailbox is queued. A worker picks this up from the queue, asks for changes in that mailbox, and synchronises them. When changes are detected in the filesystem, a task to sync that particular message is queued. It may happen that multiple messages arrive in quick succession for the same mailbox. In this case, we don’t want to trigger multiple syncs of the same mailbox, and we especially don’t want two workers to sync the same mailbox concurrently: this would quickly lead to duplicate emails.&lt;/p&gt;
    &lt;p&gt;To work around concurrent syncs and redundant mailbox updates, ImapGoose uses a “dispatcher”, which hands off sync tasks to workers. When a task to sync a specific mailbox is handed to a worker, that mailbox is marked as “busy”, and we don’t process other tasks for that queue until that worker notifies that it has finished its work on that mailbox. While a worker is synchronising a mailbox, we may receive several notifications that changes have happened to that mailbox. These changes could be the result of the changes made by the worker, or they could be new emails being delivered, so we have to queue another task to sync that mailbox. These tasks are kept in queue until the worker frees up the mailbox, and the dispatcher additionally de-duplicates them: synchronising a mailbox just once after the last change notification is enough to synchronise the changes in all the notifications.&lt;/p&gt;
    &lt;p&gt;When a message changes in the filesystem, ImapGoose receives an inotify event. This doesn’t trigger a sync of the full mailbox, but instead a “targeted” sync, which focuses only on that email message. We know that a single message has changed, so there’s no point in re-scanning the thousands of messages in the mailbox. These targeted syncs are taken into account in deduplication; they only get de-duplicated if the path for them is the same.&lt;/p&gt;
    &lt;p&gt;While the connection which is listening for changes from the server is kept alive by sending periodic NOOP commands, the connections for workers are allowed to time out. If no activity is happening, these connections simply time out, but a connection is re-established once a worker needs it again. Great care has been taken to avoid unnecessary churn in all possible aspects.&lt;/p&gt;
    &lt;head rend="h1"&gt;Prior art&lt;/head&gt;
    &lt;p&gt;Before developing ImapGoose, I studied prior art in the field. In particular, offlineimap does a great job at synchronising mailboxes. However, it doesn’t “keep in sync” in the same way; offlineimap needs to execute periodic syncs, doesn’t rely on modern extensions, and tends to “hang” when there are network time-outs. ImapGoose is new and has no existing users, so it can just require modern extensions or declare other scenarios as unsupported. Existing tools have to maintain compatibility for existing users, which might rely on some legacy email server. If I couldn’t rely on NOTIFY, implementing ImapGoose in such a clean efficient way would not have been possible. If I couldn’t rely on &lt;code&gt;CONDSTORE&lt;/code&gt; and &lt;code&gt;QRESYNC&lt;/code&gt;, I would have had to download lists of thousands of
emails each time even a single one changes. Thanks to &lt;code&gt;UIDPLUS&lt;/code&gt;, the server
returns the UID of a newly uploaded message, and we donât need any ugly
workarounds to retrieve it.&lt;/p&gt;
    &lt;p&gt;If someone needs to sync data from legacy servers, plenty of tools are still out there, providing the best experience which those servers can offer.&lt;/p&gt;
    &lt;head rend="h1"&gt;Development&lt;/head&gt;
    &lt;p&gt;When working on ImapGoose, I focused exactly on my needs for my particular use case: keep my local mailboxes in sync with an IMAP server. There’s no other supported scenario, there’s no fallback for legacy servers, and there’s no support for alternative email backends. All these constraints allowed me to focus on making a tool that’s great for a single use case: it does one thing and does it well.&lt;/p&gt;
    &lt;p&gt;I strongly believe that my keeping tight constraints (e.g.: focusing on just one use case, ignoring support for legacy servers, keeping things as simple as possible) helped develop this much faster and with much cleaner results.&lt;/p&gt;
    &lt;p&gt;I started with a very clear picture of how the whole thing would work. I was also familiar with go-imap, and knew it to be a well designed and well implemented IMAP library. My immense appreciation goes to emersion and the contributors who’ve worked on it. I didn’t need to worry about the inner details of talking to an IMAP server, parsing responses, tracking connection state, etc. go-imap provides a simple idiomatic Go interface for IMAP commands and their responses.&lt;/p&gt;
    &lt;p&gt;go-imap was lacking two features which I needed: support for the NOTIFY command and for VANISHED (rfc5162). While still standing on the shoulders of giants, I implemented both of these and sent patches for both of them (NOTIFY, VANISHED). Until those are merged, ImapGoose is built using my own (temporary) fork which has those two patches applied.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuration&lt;/head&gt;
    &lt;p&gt;For configuration, I opted for the very simple and straightforward scfg configuration format. The configuration file looks something like:&lt;/p&gt;
    &lt;code&gt;account example {
    server imap.example.com:993
    username hugo@example.com
    password-cmd pass show email/example
    local-path ~/mail/example
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Naming&lt;/head&gt;
    &lt;p&gt;I wanted something easy to remember, easy to pronounce and that won’t yield thousands of unrelated search engine results. There’s also room for an obvious mascot/logo: a goose wearing a postman’s hat carrying an envelope, using the colour palette from the Go ecosystem. Please reach out if you are an illustrator willing to contribute with artwork.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open source&lt;/head&gt;
    &lt;p&gt;ImapGoose is open source and distributed under the terms of the ISC licence. The source code is available via git. Feedback is welcome, including bug reports.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Typically, another client moves a message to Trash, and ImapGoose replicates the same operation, but the general idea still stands. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://whynothugo.nl/journal/2025/10/15/introducing-imapgoose/"/><published>2025-10-15T22:28:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45599567</id><title>IRS open sources its fact graph</title><updated>2025-10-16T09:11:50.850394+00:00</updated><content>&lt;doc fingerprint="b607e05ca421b6a4"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;No Endorsement or Warranty&lt;/p&gt;
      &lt;p&gt;The Internal Revenue Service (IRS) does not endorse, maintain, or guarantee the accuracy, completeness, or functionality of the code in this repository. The IRS assumes no responsibility or liability for any use of the code by external parties, including individuals, developers, or organizations. This includes—but is not limited to—any tax consequences, computation errors, data loss, or other outcomes resulting from the use or modification of this code.&lt;/p&gt;
      &lt;p&gt;Use of the code in this repository is at your own risk. Users of this repository are responsible for complying with any open source or third-party licenses.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Fact Graph is a production-ready knowledge graph for modeling, among other things, the United States Internal Revenue Code and related tax law. It can be used in JavaScript as well as any JVM language (Java, Kotlin, Scala, Clojure, etc.).&lt;/p&gt;
    &lt;p&gt;See ONBOARDING.md for environment/developer setup.&lt;/p&gt;
    &lt;p&gt;See the Fact Graph 3.1 ADR for more information about the fact graph and how it has been changed since early 2025 See here for a brief description of changes between the older versions of the Fact Graph and the current v3.1 in this repository&lt;/p&gt;
    &lt;p&gt;See CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;p&gt;This repository is updated frequently. Development occurs in a private repository and approved changes to &lt;code&gt;main&lt;/code&gt; are pushed to this repository in real-time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/IRS-Public/fact-graph"/><published>2025-10-15T23:24:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45599727</id><title>Writing an LLM from scratch, part 22 – training our LLM</title><updated>2025-10-16T09:11:50.394999+00:00</updated><content>&lt;doc fingerprint="a65d905599330fba"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Writing an LLM from scratch, part 22 -- finally training our LLM!&lt;/head&gt;
    &lt;p&gt;This post wraps up my notes on chapter 5 of Sebastian Raschka's book "Build a Large Language Model (from Scratch)". Understanding cross entropy loss and perplexity were the hard bits for me in this chapter -- the remaining 28 pages were more a case of plugging bits together and running the code, to see what happens.&lt;/p&gt;
    &lt;p&gt;The shortness of this post almost feels like a damp squib. After writing so much in the last 22 posts, there's really not all that much to say -- but that hides the fact that this part of the book is probably the most exciting to work through. All these pieces developed with such care, and with so much to learn, over the preceding 140 pages, with not all that much to show -- and suddenly, we have a codebase that we can let rip on a training set -- and our model starts talking to us!&lt;/p&gt;
    &lt;p&gt;I trained my model on the sample dataset that we use in the book, the 20,000 characters of "The Verdict" by Edith Wharton, and then ran it to predict next tokens after "Every effort moves you". I got:&lt;/p&gt;
    &lt;code&gt;Every effort moves you in," was down surprise a was one of lo "I quote.
&lt;/code&gt;
    &lt;p&gt;Not bad for a model trained on such a small amount of data (in just over ten seconds).&lt;/p&gt;
    &lt;p&gt;The next step was to download the weights for the original 124M-parameter version of GPT-2 from OpenAI, following the instructions in the book, and then to load them into my model. With those weights, against the same prompt, I got this:&lt;/p&gt;
    &lt;code&gt;Every effort moves you as far as the hand can go until the end of your turn unless something interrupts your control flow. As you may observe I
&lt;/code&gt;
    &lt;p&gt;That's amazingly cool. Coherent enough that you could believe it's part of the instructions for a game.&lt;/p&gt;
    &lt;p&gt;Now, I won't go through the remainder of the chapter in detail -- as I said, it's essentially just plugging together the various bits that we've gone through so far, even though the results are brilliant. In this post I'm just going to make a few brief notes on the things that I found interesting.&lt;/p&gt;
    &lt;head rend="h3"&gt;Randomness and seeding&lt;/head&gt;
    &lt;p&gt;One thing I really do recommend to anyone working through the book is that you type in all of the code, and run it yourself -- it really will help you remember how stuff fits together.&lt;/p&gt;
    &lt;p&gt;There is one slight issue I found with that, however: the book has a number of examples where you get output from code that uses randomness -- for example, where you take a look at the loss it has on some sample text before you start training, or make it generate samples during the train.&lt;/p&gt;
    &lt;p&gt;Now, in theory, because Raschka puts &lt;code&gt;torch.manual_seed&lt;/code&gt; calls before all of these,
the results you get should be exactly the same as the outputs in the book.  However,
the amount of code we're working with at this stage is quite large -- we have various
helper functions that were created in earlier sections, for example.  And some of these
use randomness.&lt;/p&gt;
    &lt;p&gt;That means that to get the same results as the ones in the book, you would need to ensure that all of the code that uses randomness was running in exactly the same order as it was when Raschka did it for the book. That turns out to be surprisingly hard!&lt;/p&gt;
    &lt;p&gt;My instinct is that it doesn't actually matter all that much. So long as the loss numbers that you see are in the same ballpark as the ones in the book, and the outputs you see are roughly equally incoherent (before training) and become more coherent at what feels like the same kind of rate, you're fine. Probably the most important one to look out for is when the training run starts -- you should see loss on the training set decreasing steadily, just like in the book, and likewise as in the book, the validation loss should plateau out pretty early.&lt;/p&gt;
    &lt;head rend="h3"&gt;Optimisers&lt;/head&gt;
    &lt;p&gt;When I have built simple backpropagation through neural networks in the past, I've generally updated parameters by multiplying the gradients by a small number, the learning rate, and then subtracting them from their respective parameters to get updated ones -- classic stochastic gradient descent.&lt;/p&gt;
    &lt;p&gt;Non-trivial ML uses optimisers; I'd come across them while fine-tuning LLMs, and also used one in the RNN code I wrote last week. Instead of updating the parameters yourself, you ask the optimiser to do it for you, by calling its &lt;code&gt;step&lt;/code&gt; function.  AdamW appears to be the default optimiser in most textbooks,
though Muon seems to be the most popular
in use, if my AI X/Twitter feed is to be believed.&lt;/p&gt;
    &lt;p&gt;I don't understand how optimisers work in any detail, and I'm going to have to dig into that in the future. However, my high-level simplified picture right now is that they dynamically adjust the learning rate over time, so that it's easier to take big "jumps" downwards on the gradients when you start, and then smaller ones later. I believe they can also sometimes avoid local minima in the loss landscape -- a nice metaphor I read somewhere (lost the source, sadly) was that simple gradient descent was like rolling a ball down a hill, but (some?) optimisers give the ball a bit of momentum so that it can coast over a small uphill portion, so long as the general slope is downwards.&lt;/p&gt;
    &lt;p&gt;Anyway, more investigation needed later.&lt;/p&gt;
    &lt;p&gt;In practice, with AdamW, you initialise it at the start of your training loop, with a learning rate (which I imagine is similar to the one my older code used, a scaling factor for gradients) and a weight decay (:shrug:). You also provide it with the parameters it's going to be managing.&lt;/p&gt;
    &lt;p&gt;In the training loop, at the start of each input batch, you tell it to zero out the gradients it's managing with &lt;code&gt;optimizer.zero_grad()&lt;/code&gt;, run the data through your model and calculate your loss, and then after
calling &lt;code&gt;loss.backward()&lt;/code&gt; to get your gradients,
you just call &lt;code&gt;optimizer.step()&lt;/code&gt;, and that does the parameter update.&lt;/p&gt;
    &lt;p&gt;Again, I want to dig into how optimisers work in more detail in the future. But for now, I think that's all I need to know.&lt;/p&gt;
    &lt;head rend="h3"&gt;Speed, and the cost of training&lt;/head&gt;
    &lt;p&gt;The book tells you how to train on a public domain book, "The Verdict" by Edith Wharton. Full training on the hardware that people are likely to have to hand would be extremely expensive, so we just train on that short example, then later on learn how to download and use the weights that OpenAI made available for their GPT-2 models.&lt;/p&gt;
    &lt;p&gt;But there was something that surprised me a little. When talking about the training run on "The Verdict", Raschka says that it takes "about 5 minutes to complete on a MacBook Air".&lt;/p&gt;
    &lt;p&gt;On my machine using CUDA on an RTX 3090, it took just less than eleven seconds.&lt;/p&gt;
    &lt;p&gt;This makes perfect sense, of course -- there's a really good reason why AI training is normally done on GPUs or custom hardware, and the MacBook Air would presumably be training on the CPU. But I was a little surprised at how huge the difference was in this simple example!&lt;/p&gt;
    &lt;p&gt;Now, while the book mentions that Llama 2 probably cost hundreds of thousands of dollars to train, I must admit that I do wonder how much it really would cost to train a 124M parameter model on my own hardware -- or, indeed, on the machines with 8x 80GiB A100 GPUs that I rented from Lambda Labs during my fine-tuning experiments.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy was able to train a 124M GPT-2 model for $20, using his hand-written C/CUDA LLM system &lt;code&gt;llm.c&lt;/code&gt;.  That is undoubtedly more efficient than the
PyTorch code that we're working on in this book.  But it really would be interesting
to find out whether it would be doable for me at all!  The training data he used
is the 10B-token version of the FineWeb collection, which
is freely available. 1&lt;/p&gt;
    &lt;p&gt;I think I have a good candidate for a next project when I've finished the book; see how many tokens/second I can train on locally -- that will allow me to estimate how long it would take to train one epoch over the whole training set. I imagine that will be longer than I'm willing to leave my desktop machine tied up doing this, but then I can try mixing in the lessons I learned doing fine-tuning, and see if I can get it up and running on Lambda Labs. If the cost is in the tens of dollars, or even a hundred or so, I really think it would be worthwhile!&lt;/p&gt;
    &lt;head rend="h3"&gt;"Memorisation", temperature and top-k sampling&lt;/head&gt;
    &lt;p&gt;One thing I found a little confusing in this chapter -- and this is very much a nit -- was the section on preventing "memorisation"; I think this was due to a mismatch in the meaning I attach to the word, and the way it's used here.&lt;/p&gt;
    &lt;p&gt;To me, memorisation is something that the model does during training -- if you keep training a 124M-parameter model on a 20,000-character file, as we're doing here, then whatever happens the model is going to memorise it -- it's unavoidable. The only way to reduce memorisation in this sense would be to increase the amount of training data (and even then, as the findings in the lawsuit by the New York Times against OpenAI show, some stuff would be memorised).&lt;/p&gt;
    &lt;p&gt;In the book, "memorisation" is being used to mean something more like what I'd call "parroting" -- issues with the model just repeating the stuff that it has memorised, because it was always choosing the most-probable next word. Avoiding this is super-important, of course! It's just the framing that confused me a little.&lt;/p&gt;
    &lt;p&gt;The techniques are nifty, anyway. The first cut -- just use the softmaxed logits as a probability distribution and sample from it -- is obvious enough. Temperature is a clever trick on top of that -- just divide the logits by some number greater than one before softmax, and you can make the distribution that comes out flatter (or you can make it more "pointy" by dividing by a number less than 1). The graphs in the book showing how that works are great, but I asked Claude to knock together a temperature playground website, which I found made things even clearer to me.&lt;/p&gt;
    &lt;p&gt;And finally, the top-k technique -- only consider the k most probable tokens, and then do the temperature/softmax calculations -- was a sensible addition to add on top of that. The code is clever: identify the top k logits, get the value of the lowest one of them, and then replace every logit less than that with minus infinity. When you run that through softmax, you get zeros for the ones that were replaced, and the probability distribution is based on the remainder.&lt;/p&gt;
    &lt;p&gt;So: excellent stuff, and very well explained in the book -- it just didn't feel like preventing "memorisation" specifically was what it was doing, at least based on what I take the word to mean.&lt;/p&gt;
    &lt;head rend="h3"&gt;Downloading the OpenAI weights&lt;/head&gt;
    &lt;p&gt;At the end of the chapter, we download the weights for the original GPT-2 model that OpenAI produced from their site, and load them into our own model.&lt;/p&gt;
    &lt;p&gt;The code to download weights is (thankfully) something that you don't need to type in, as it's downloadable from GitHub. And in one specific related case, I'll also contradict what I said earlier about typing stuff in yourself -- I definitely recommend that you copy the &lt;code&gt;load_weights_into_gpt&lt;/code&gt; that copies the downloaded weights into our own model
from GitHub too.  I did actually type it all in and I don't think I gained anything
from doing that.&lt;/p&gt;
    &lt;p&gt;One thing I did notice while going through that section was that I'd been making a mistake as I wrote up this series; I'd thought that all GPT-2 models had 768 embedding dimensions. It turns out that this is only true of the 124M model in that series, and the larger ones have more. That makes a lot of sense -- and I've updated the older posts to reflect it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;That's all I really have to add to what is in the rest of chapter 5. Like I said at the start, it feels almost like a let-down to be writing so little about a section of the book that has such amazing results! But now we have a working LLM, and at least the foundations that might allow us to train our own from scratch if we had the resources.&lt;/p&gt;
    &lt;p&gt;Next up: using it to classify text. Will this be quick and easy? Or will it lead down another fascinating rabbit hole? Time will tell...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gilesthomas.com/2025/10/llm-from-scratch-22-finally-training-our-llm"/><published>2025-10-15T23:42:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45600263</id><title>Who's Submitting AI-Tainted Filings in Court?</title><updated>2025-10-16T09:11:50.055357+00:00</updated><content>&lt;doc fingerprint="f53c9a1097f4a79a"&gt;
  &lt;main&gt;
    &lt;p&gt;It seems like every day brings another news story about a lawyer caught unwittingly submitting a court filing that cites nonexistent cases hallucinated by AI. The problem persists despite courts’ standing orders on the use of AI, formal opinions and continuing legal education (CLE) courses on ethical use of AI in law practice, and revelations that AI-powered legal research tools are more fallible than they purport to be.&lt;/p&gt;
    &lt;p&gt;Who are the attorneys submitting AI-tainted briefs? A recent 404 Media article about lawyers’ use of AI drew my attention to a database of AI Hallucination Cases compiled and maintained by Damien Charlotin, a French lawyer and scholar. Charlotin classifies the nature of the incident by various types of inaccuracies: fabricated cases, false quotes from or misrepresentations of real cases, or outdated invocations of cases that have been overturned. Besides helping the public understand how lawyers are getting tripped up by AI, Charlotin’s database also enables a better view of who is getting tripped up by AI.&lt;/p&gt;
    &lt;p&gt;Using the database, I analyzed 114 cases from U.S. courts where, according to either opposing counsel and/or the court’s own investigation, an attorney’s filing included inaccuracies that were suspected or shown to have been caused by the use of AI. I find that the vast majority of the law firms involved – 90% – are either solo practices or small firms. What’s more, in 56% of the cases, the AI hallucinations were attributed to the plaintiff’s counsel, compared with 31% to the defense. And, while most cases in the sample did not specify the AI tool used, of those that did, fully half involved some version of ChatGPT.&lt;/p&gt;
    &lt;p&gt;Methodology&lt;/p&gt;
    &lt;p&gt;I based my analysis on cases I downloaded in a .csv file from Charlotin’s database on October 9, 2025. The time period covers court orders issued from June 2023 (the month of the landmark order in Mata v. Avianca) through October 7, 2025.&lt;/p&gt;
    &lt;p&gt;[Note: October 9 was a Thursday; by the following Monday, when I began drafting this write-up, Charlotin had added three new matters involving pro se litigants (which I exclude from my analysis) as well as two updates on cases that were already in the database (and thus already in my sample), plus there was news coverage of an oral argument where an attorney was grilled about hallucinations in his briefing. I did not add that last matter, which had not yet yielded a written opinion at the time I wrote this, to my sample. This is all to give the reader some idea of just how frequently these incidents are happening and consequently to highlight that my sample should not be considered comprehensive – the data became outdated almost immediately.]&lt;/p&gt;
    &lt;p&gt;Charlotin has helpfully coded the data by a number of selectors including country. I restricted my download to the USA only. After importing the .csv file into Google Sheets, I then filtered by Party to include all cases involving a Federal Defender, Lawyer, and/or Paralegal. (Prosecutor is also an option in the database, but there were zero such cases in the USA for that time period.) This resulted in 117 cases, which fell to 114 after I excluded three cases from the sample (two cases that actually involved pro se litigants rather than lawyers and one duplicate case).&lt;/p&gt;
    &lt;p&gt;I reviewed the court orders that Charlotin included for each database entry in order to determine which party was accused of submitting hallucinated citations and the name and law firm affiliation of the attorney(s) for that party. If that information was unavailable in the court order, I looked up the case docket in federal or state court records.&lt;/p&gt;
    &lt;p&gt;Most of the cases in the sample are typical adversarial matters where the parties can be classified as either plaintiff or defendant. For matters that fall outside that usual structure (such as bankruptcy cases), I created an “other” category. Where the court order came from an appellate case, I tried to classify the party as plaintiff or defendant as per the parties’ trial-court posture.&lt;/p&gt;
    &lt;p&gt;My data for the number of attorneys at each firm came from either the firm’s website or some other authoritative source (such as the NALP, Vault, or Law.com). I sometimes had to guess that an attorney was solo, typically where the attorney does not have a dedicated website and the firm name listed in court records, if any, is indicative of a solo practice (e.g., “Law Office of Jane Smith”).&lt;/p&gt;
    &lt;p&gt;For firm size, I have used the following bands: solo; 2-25; 26-100; 101-200; 201-500; 501-700; 701-1000; 1001+. These are the bands the NALP uses for its Directory of Legal Employers, except that it uses “1-25” as a band. I chose to split out solo attorneys as a separate category because I believe solo attorneys deserve recognition as a standalone group with unique characteristics that differentiate their practices from firms of 10 or 20 lawyers. I added a “government” category for the rare cases involving government attorneys (two: a public defender and attorneys for a county), but did not attempt to count how many attorneys were part of that particular government unit.&lt;/p&gt;
    &lt;p&gt;There may be errors in my data, thanks to having to guess about some things (such as whether someone is a solo practitioner) or relying on inaccurate or outdated sources (for example, third-party reporting on firm size). If you find an error, please email me (riana at stanford dot edu) and I’ll fix it and update this post.&lt;/p&gt;
    &lt;p&gt;The Party Submitting Hallucination-Tainted Filings Is Usually the Plaintiff&lt;/p&gt;
    &lt;p&gt;The plaintiff is more commonly the party allegedly responsible for submitting filings containing AI hallucinations. Out of 114 cases, 64 were attributed to the plaintiff (56.1%), compared with 35 to the defendant (30.7%). There were 15 “other” cases (13.2%): bankruptcy, family, probate, and tax court matters, agency matters, a habeas petition, and an attorney disciplinary proceeding. (The lawyer allegedly submitted filings with AI hallucinations during that disciplinary proceeding, not in an underlying case involving that lawyer like other disciplinary proceedings in the sample. Where the attorney was facing discipline for misusing AI while representing a client, I classified the lawyer according to the party they were representing in the underlying case.)&lt;/p&gt;
    &lt;p&gt;AI Hallucination Cases Overwhelmingly Involve Solo or Small Firms&lt;/p&gt;
    &lt;p&gt;Some of the 114 cases in the sample involved attorneys from more than one firm – for example, local counsel filing briefs drafted by a different firm. I counted each firm separately, except that if the court’s order faulted only one firm’s attorney, I did not count the other firm(s). The total number of firms (including government entities) was 129.&lt;/p&gt;
    &lt;p&gt;Solo practices and small firms represent the overwhelming majority of that number. Solos account for half (50.4%) and small firms of 2-25 lawyers for another 39.5%. Of the remaining 10% of firms, 3.1% are firms of 26-100 lawyers; 2.3% are firms of 201-500 lawyers; 1.6% are firms of 1001+ attorneys; 1.6% are government entities; and firms of either 101-200 or 501-700 lawyers each represent less than 1%. There were no cases involving firms of 701-1000 lawyers.&lt;/p&gt;
    &lt;p&gt;The number of firms in the sample with more than 25 lawyers is small enough to count on two AI-generated hands. Four have up to 100 lawyers: Ellis George, Hagens Berman Sobol Shapiro, Merlin Law Group, and Williams Kastner. Five have 101-700 lawyers: Butler Snow, Goldberg Segalla, Morrison Mahoney, Quintairos Prieto Wood &amp;amp; Boyer, and Spencer Fane. Two have more than 1000 attorneys: K&amp;amp;L Gates and Morgan &amp;amp; Morgan.&lt;/p&gt;
    &lt;p&gt;Five lawyers are implicated in more than one case in the sample. All are either solo practitioners or small-firm lawyers: solo Maren Miller Bam of Salus Law; Jane Watson of Watson &amp;amp; Norris (who was only admitted to the bar in 2024); Chris Kachouroff of McSweeney Cynkar &amp;amp; Kachouroff (who gained notoriety for appearing pantsless at a Zoom court hearing); solo Tyrone Blackburn (who got arrested for assault in June in connection with a different case of his); and William Panichi, a family-court attorney. While the first four allegedly misused AI in two separate cases, Panichi was called out in an astonishing four cases in one 30-day period; he has supposedly begun winding down his law practice and surrendering his license.&lt;/p&gt;
    &lt;p&gt;ChatGPT Was the Most Commonly Used AI Tool&lt;/p&gt;
    &lt;p&gt;Of the 114 cases in the sample, only 34 (30%) identified the specific AI tool(s) used by the attorneys. Some cases involved the use of more than one AI tool. OpenAI’s ChatGPT (any version, including in-house versions and the ChatGPT-powered app Ghostwriter Legal) was far and away the most common: it was implicated in fully half (18) of the 34 cases that specified a tool. Coming in a distant second were AI tools offered by Westlaw, followed by Anthropic’s Claude (any version), Microsoft Copilot, Google Gemini, and LexisNexis’s AI tools.&lt;/p&gt;
    &lt;p&gt;Discussion&lt;/p&gt;
    &lt;p&gt;This analysis confirms what many lawyers and judges may have suspected: that the archetype of misplaced reliance on AI in drafting court filings is a small or solo law practice using ChatGPT in a plaintiff’s-side representation.&lt;/p&gt;
    &lt;p&gt;Ultimately, the buck stops with the attorney to make sure that she can stand behind every word of every brief filed over her signature. But the 404 Media article that led me to Charlotin’s database paints a picture of how hard it is to live up to that obligation, particularly for solo or small-firm attorneys. Lawyers struggle with busy caseloads, the trustworthiness of their co-counsel, junior attorneys, and support staff, and personal issues (health problems, caregiving obligations, etc.) that compete with work for their time and attention. Of course, that was already true long before AI. Lawyers, even very good ones, have always made the occasional mistake or oversight in their work. AI tools have merely provided a new way to make those errors – while also promising a way out of the underlying issues that contribute to them, like time crunches and insufficient support. As the 404 Media article observed, “the legal industry is under great pressure to use AI.” To overworked attorneys at small law offices, these tools must seem like a godsend.&lt;/p&gt;
    &lt;p&gt;However, as the lawyers in this analysis learned the hard way, these tools are not reliable for their core purpose of accurate, comprehensive legal research results. Several of my Stanford colleagues are coauthors on a recent paper that investigated AI legal tools’ claims to be “hallucination-free” or to “eliminate” or “avoid” hallucinations. To the contrary, they found disturbingly high levels of hallucinations in all the tools they studied: OpenAI’s GPT-4, Lexis+ AI (offered by LexisNexis), Westlaw’s AI-Assisted Research, and Ask Practical Law AI (which, like Westlaw, is owned by Thomson Reuters). All of those companies are represented in the 34 cases analyzed above.&lt;/p&gt;
    &lt;p&gt;The incidents in Charlotin’s database illustrate the real-world impact of AI legal tools’ shortcomings – and not just on the lawyers, who end up humiliated and sanctioned for relying on tools they thought were reliable. AI-tainted legal briefs negatively affect those lawyers’ clients, who depend on them for high-quality representation, including in incredibly high-stakes matters such as criminal prosecutions or the termination of parental rights. They affect opposing counsel, who must waste their time tracking down nonexistent case citations. And they affect the courts, which are busy enough already without also having to police this new form of attorney ethics violations and take care not to let nonexistent cases cited by counsel creep into court opinions.&lt;/p&gt;
    &lt;p&gt;What Is To Be Done?&lt;/p&gt;
    &lt;p&gt;These cases keep happening at an alarming pace. Dozens of cases have been added to Charlotin’s database since the American Bar Association (ABA) issued its formal opinion warning about generative AI tools in July 2024. For all the news stories about lawyers caught flat-footed by these tools, clearly there are lawyers who never read them and subsequently become the headline of the next one. It may be that nothing will sufficiently penetrate lawyers’ consciousness about the pitfalls of relying on AI tools until every practicing lawyer is personally confronted with that knowledge through some combination of (1) every single type of court – federal, state, tribal, agency; civil, criminal, bankruptcy, family, probate, you name it – requiring every lawyer who appears in every case to file a declaration attesting that they understand and acknowledge the fallibility of AI tools and have educated all their staff as well, and (2) every single state bar (including D.C. and U.S. territories) imposing CLE requirements specifically about AI tools for legal research, like they now do for topics like substance abuse and elimination of bias.&lt;/p&gt;
    &lt;p&gt;Even then, there will be failures. Inevitably, some lawyers will dutifully certify that they understand that AI tools are unreliable, then file an AI-tainted brief anyway. But perhaps the incidence of lawyers sanctioned for unwittingly misusing AI will slacken with time and more pervasive awareness of AI’s perils. And hopefully AI legal research tools themselves will improve over time (as their paying customers surely expect them to) – though it is as unreasonable to expect perfection from them as from humans. “Trust, but verify” must remain the watchword.&lt;/p&gt;
    &lt;p&gt;With all that said, no amount of CLE courses and state bar ethics opinions will fix the problem I haven’t discussed until now: use of AI by pro se litigants. I wanted to figure out which lawyers were getting tripped up by AI, so I only analyzed U.S. cases involving lawyers or paralegals, for a sample of 114 cases. But in the .csv file I downloaded from Charlotin’s database, there are 160 cases involving a pro se litigant. That is: Pro se litigants account for the majority of the cases in the United States where a party submitted a court filing containing AI hallucinations. In a country where legal representation is unaffordable for most people, it is no wonder that pro se litigants are depending on free or low-cost AI tools. But it is a scandal that so many have been betrayed by them, to the detriment of the cases they are litigating all on their own.&lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;This analysis speaks to both the urgent need for high-quality legal research tools in a legal profession dominated by small and solo practices, and the yawning gap between current AI tools’ actual and perceived reliability. In many cases in the analysis, the attorney had not understood that AI tools may produce inaccurate results. True, lawyers are ethically obligated to ensure the accuracy of their work product. But it is also incumbent upon the companies offering AI tools, especially those tailored specifically for legal research, not to oversell them or hide their shortcomings; that is, their marketing shouldn’t outgun their disclaimers. So long as these tools remain flawed without lawyers understanding that, AI tools for legal research threaten to be, not a timesaver, but a source of unnecessary extra work for lawyers and the courts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cyberlaw.stanford.edu/whos-submitting-ai-tainted-filings-in-court/"/><published>2025-10-16T00:50:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45600338</id><title>Retiring Windows 10 and Microsoft's move towards a surveillance state</title><updated>2025-10-16T09:11:49.606823+00:00</updated><content>&lt;doc fingerprint="b51f295fc9aca084"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently, the Secure Resilient Future Foundation released a newsletter calling for Microsoft to extend Windows 10 support past the October 14th deadline.&lt;/p&gt;
    &lt;p&gt;With the release of Windows 11, the threat to data privacy is the worst it’s ever been. In my recent article, “Looking back at my transition from Windows to Linux in an anti-customer age”, I wrote about my switch to Linux and how it saved me from having to sacrifice my freedom in the name of convenience.&lt;/p&gt;
    &lt;p&gt;Whether you’re a business or a home user, I’m here to tell you that in many cases, Linux is a real alternative to Windows. So instead of pushing the goal post back from the brink of an Orwellian nightmare. I’m suggesting all of us consider switching Linux now.&lt;/p&gt;
    &lt;p&gt;Microsoft’s design of Windows 11 is a concern because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Computer manufacturers, due to pressure from Microsoft, are designing new computers with artificial limitations like TPM and Secure Boot. These unnecessary add-ins push consumers to unnecessary hardware upgrades1.&lt;/item&gt;
      &lt;item&gt;In the setup of newly purchased consumer-grade computers, there is obfuscation in the installation language. Many of the default choices are aimed at confusing customers into selecting options that share data with vendors: &lt;list rend="ul"&gt;&lt;item&gt;The process of setting up OneDrive to act as a backup of data. Without consent, the setup of this configuration moves all customers’ data to the cloud service, re-points all the user folders to a cloud-specific OneDrive folder that’s very difficult to revert.&lt;/item&gt;&lt;item&gt;The process of selecting a browser is obfuscated by Microsoft’s Edge Browser setup&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The AI tool Co-pilot is installed and enabled without consent. Removal is difficult or nonexistent.&lt;/item&gt;
      &lt;item&gt;The history tracking tool “Recall” that is due to be released, sometime in the future, saves snapshots of your user experience into Microsoft’s OneDrive cloud. It looks great on paper, but in reality, this feature, along with others, will be used to move forward a surveillance state.&lt;/item&gt;
      &lt;item&gt;Windows 11 prevents the complete uninstall of many of its built-in features. They can be removed from one user account, but they can be reinstalled during an update, or if you upgrade your computer, without your consent.&lt;/item&gt;
      &lt;item&gt;Microsoft Edge is forced on users as a replacement by obfuscating choice in various ways.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Due to these concerns, I will be recommending Linux as a replacement for new computers I build for my customers. You can still request Windows if Linux doesn’t work for you.&lt;/p&gt;
    &lt;p&gt;Linux Distribution Replacements for Windows 1. Zorin OS: A Windows-like Linux experience, requires modern hardware 2. PopOS: Built for gamers out of the box 3. Ubuntu: All-around desktop, requires modern hardware 4. Elementary OS: For minimalist users 5. MX Linux: For 10+ years, hardware&lt;/p&gt;
    &lt;p&gt;If you currently have a computer with Windows installed that you are unhappy with, contact me about migrating to Linux. It’s never been a better time for freedom in Linux.&lt;/p&gt;
    &lt;head rend="h2"&gt;Caveats&lt;/head&gt;
    &lt;p&gt;Linux is a different desktop environment from Windows, which requires different programs to make use of your data. Please note that if you are a power user or a gamer, due to the way developers use vendor lock-in with their software products, certain software or games might not work, or will need to be replaced by alternatives. Below is an incomplete list of typical situations that will not work at this time. If you have any questions about these concerns, contact me to schedule a consultation to further talk about your specific use-case and the costs involved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adobe Cloud Products - See some alternatives&lt;/item&gt;
      &lt;item&gt;Most anti-cheat specific games&lt;/item&gt;
      &lt;item&gt;Microsoft Office and Outlook - Alternative for Microsoft Office: LibreOffice, Alternative for Outlook: Thunderbird (Does not handle Office 365 services very well; in this case, I suggest migrating your contacts, calendars, and email to an IMAP-hosted mail provider)&lt;/item&gt;
      &lt;item&gt;QuickBooks - Requires an Online Hosted alternative&lt;/item&gt;
      &lt;item&gt;Turbotax - Requires an Online Hosted alternative&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Microsoft uses its monopolistic position in the PC market to push hardware manufacturers to adopt changes that lock customers into anti-consumer software products via security hardware like TPM 2.0, which prevents freedom of choice. Based on my direct observations, I outline what these changes mean to the future of technology. ^&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.scottrlarson.com/publications/publication-windows-move-towards-surveillance/"/><published>2025-10-16T01:00:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45601177</id><title>Free applicatives, the handle pattern, and remote systems</title><updated>2025-10-16T09:11:49.311384+00:00</updated><content>&lt;doc fingerprint="721ee56b6f99d084"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Free applicatives, the handle pattern, and remote systems&lt;/head&gt;
    &lt;p&gt;We recently refactored some gnarly code that manipulated customer and order records in our enterprise resource planning (ERP) system. That system had a few idiosyncrasies which complicated this task:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating new records required referencing other entities by internal ID, so we had to do a number of lookups before issuing “create record” requests;&lt;/item&gt;
      &lt;item&gt;For some entity types, we found it easiest to issue “search” API calls and extract the required IDs from the returned search results. This necessitated an extra parsing step between “we have a successful response” and “we have the ID we’re looking for”; and&lt;/item&gt;
      &lt;item&gt;Requests are often slow, but the marginal cost of additional requests in a batch was quite low. This meant that we could expect some good results from batching related requests together.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The benefits of batching led us to seek a solution that permitted static analysis. Applicative functors have a completely static control flow, and cannot express computations where one step depends on the result of a previous step. A well-chosen applicative would let us analyse the requests we need to send without executing any of them, batch queries together without worrying about data dependencies, and then route relevant results to each individual query to parse (if necessary). Our library users could ignore batching details but still gain the efficiency benefits of a batch query API.&lt;/p&gt;
    &lt;p&gt;In this post, we’ll look how we’ve been using handles, what “free structures” are, how free applicatives elegantly solved some of our problems interfacing with a remote system, and how they interacted especially well with the “handle pattern”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Handles, as Bellroy uses them&lt;/head&gt;
    &lt;p&gt;The “handle pattern” is a Haskell idiom that is similar to dependency injection in mainstream languages. Instead of directly writing in the side effects we want our code to perform, we accept a record of functions that we call a “handle”. (In an object-oriented language, we’d probably accept an object that implements an abstract interface instead of a record.) These handles can group related functions into a single record but often only contain one:&lt;/p&gt;
    &lt;code&gt;newtype Handle e m = Handle {
    performRequest :: ERP.Request -&amp;gt; m (Either e Aeson.Value)

   }
-- Plus some other handle-making functions e.g. for testing.
newHandle ::
MonadIO m =&amp;gt; ERP.Credentials -&amp;gt; m (Handle ERP.Error m)   &lt;/code&gt;
    &lt;p&gt;Functions that consume handles generally look like this:&lt;/p&gt;
    &lt;code&gt;someFunction ::
-- When all side effects come from handles,
   -- we rarely need anything stronger than `Monad`.
   Monad m =&amp;gt;
   -- First: Any handles we need
   FooHandle m -&amp;gt; BarHandle m -&amp;gt;
   -- Second: Other "normal" arguments
   Argument1 -&amp;gt; .. -&amp;gt; ArgumentN -&amp;gt;
   Result   m &lt;/code&gt;
    &lt;p&gt;This idiom is a simpler, library-free alternative to effect system libraries like &lt;code&gt;effectful&lt;/code&gt;,
&lt;code&gt;bluefin&lt;/code&gt;,
&lt;code&gt;heftia&lt;/code&gt; and
&lt;code&gt;polysemy&lt;/code&gt;. We
previously wrote about an
experiment with &lt;code&gt;effectful&lt;/code&gt;,
but we have still not committed to an effect system. Instead, we are
refactoring towards handles as a way to encapsulate our side effects,
and because it should be easy to convert handle-using code to an
effect system if and when we choose one.&lt;/p&gt;
    &lt;p&gt;Because we have code written against other idioms (e.g. MTL-style classes), and because we often find it convenient to introduce an &lt;code&gt;ExceptT e&lt;/code&gt; or &lt;code&gt;MaybeT&lt;/code&gt; monad transformer in the body of our
functions, we sometimes need to change the monad of a handle that
we’ve been given. We do this by providing a &lt;code&gt;hoistHandle&lt;/code&gt; function:&lt;/p&gt;
    &lt;code&gt;hoistHandle :: (forall x . f x -&amp;gt; g x) -&amp;gt; Handle f -&amp;gt; Handle g
Handle {performRequest} =
 hoistHandle f Handle {performRequest = f . performRequest}   &lt;/code&gt;
    &lt;p&gt;That first argument, &lt;code&gt;forall x . f x -&amp;gt; g x&lt;/code&gt;, is worth commenting
on. A &lt;code&gt;forall&lt;/code&gt; in a type signature explicitly introduces a type
variable that is provided by the function’s caller. For a simpler
example of how &lt;code&gt;forall&lt;/code&gt; works here, let’s look at the &lt;code&gt;map&lt;/code&gt; function
on lists, but with explicit &lt;code&gt;forall&lt;/code&gt;s:&lt;/p&gt;
    &lt;code&gt;map :: forall a b . (a -&amp;gt; b) -&amp;gt; [a] -&amp;gt; [b]&lt;/code&gt;
    &lt;p&gt;The caller of &lt;code&gt;map&lt;/code&gt; gets to choose the types of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, and GHC
is often smart enough to figure this out automatically:&lt;/p&gt;
    &lt;code&gt;-- GHC concludes that it needs to call
-- `map` with `Int` for `a` and `String` for `b`.
&amp;gt; map show [1, 2, 3]
 ghci"1","2","3"] [&lt;/code&gt;
    &lt;p&gt;In our &lt;code&gt;hoistHandle&lt;/code&gt; function, we let the caller choose &lt;code&gt;f&lt;/code&gt; and
&lt;code&gt;g&lt;/code&gt;, but they must provide us a function where we are allowed to
choose &lt;code&gt;x&lt;/code&gt;. The types force this function to convert &lt;code&gt;f x&lt;/code&gt; into &lt;code&gt;g x&lt;/code&gt;
in a way that’s blind to what &lt;code&gt;x&lt;/code&gt; actually is — guaranteeing that
the conversion only changes structure, not wrapped values. It also
ensures that we can write &lt;code&gt;hoistHandle&lt;/code&gt; for a handle containing
multiple functions, because we can choose a different &lt;code&gt;x&lt;/code&gt; for each
one.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building our applicative&lt;/head&gt;
    &lt;p&gt;We want to build a structure that is essentially a syntax tree of the operations we want to perform. This means it needs to hold the requests we want to send, and because we want it to be an applicative, we’ll add constructors to represent &lt;code&gt;pure&lt;/code&gt; and &lt;code&gt;(&amp;lt;*&amp;gt;)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;data Query a where
QueryAndParse ::
   FromJSON x =&amp;gt;
     ERP.Request -&amp;gt; (x -&amp;gt; Either Text a) -&amp;gt; Query a
     
-- Extra constructors to hold applicative structure
   Pure :: a -&amp;gt; Query a
   Ap :: Query (a -&amp;gt; b) -&amp;gt; Query a -&amp;gt; Query b
   
deriving stock instance Functor Query

instance Applicative Query where
pure = Pure
   
Pure f &amp;lt;*&amp;gt; Pure x = Pure $ f x
   QueryAndParse req f &amp;lt;*&amp;gt; Pure a =
   QueryAndParse req $ fmap ($ a) . f
     -- Plus another seven cases, being careful that
   -- each case obeys the applicative laws.   &lt;/code&gt;
    &lt;p&gt;&lt;code&gt;QueryAndParse&lt;/code&gt; is the only data constructor directly relevant to our
problem. It captures the request we want to make against the ERP, a
&lt;code&gt;FromJSON x&lt;/code&gt; constraint so we can parse the raw response into some
intermediate type representing an API response, and a function &lt;code&gt;x -&amp;gt; Either Text a&lt;/code&gt; to extract just the data we want from that API
response.&lt;/p&gt;
    &lt;p&gt;This design could work, but it’s a fair amount of boilerplate, and the next time we want an applicative like this we’d need to repeat most of it. In the next section, we’ll use a free applicative to separate the general “applicative” code from the specific “query and parse” code.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a “free structure”?&lt;/head&gt;
    &lt;p&gt;To understand how free applicatives help us with this problem, we need to have some idea what “freeness” means in this context. The Haskell community usually talks about taking “the free &lt;code&gt;$class&lt;/code&gt; over &lt;code&gt;$type&lt;/code&gt;”
as a way to make &lt;code&gt;$type&lt;/code&gt; an instance of &lt;code&gt;$class&lt;/code&gt;, by adding just
enough structure to construct a lawful instance of &lt;code&gt;$class&lt;/code&gt;. Packages
like &lt;code&gt;free&lt;/code&gt; provide
wrapping types that hold values of &lt;code&gt;$type&lt;/code&gt; and provide instances of
&lt;code&gt;$class&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let’s pare our &lt;code&gt;Query&lt;/code&gt; type back to something much smaller: a type
representing a single request against our ERP:&lt;/p&gt;
    &lt;code&gt;data OneQuery a where
QueryAndParse ::
   FromJSON x =&amp;gt;
     ERP.Request -&amp;gt; (x -&amp;gt; Either Text a) -&amp;gt; OneQuery a     &lt;/code&gt;
    &lt;p&gt;We will now re-write &lt;code&gt;Query&lt;/code&gt; as the free &lt;code&gt;Applicative&lt;/code&gt; over
&lt;code&gt;OneQuery&lt;/code&gt;. To make &lt;code&gt;OneQuery&lt;/code&gt; into an &lt;code&gt;Applicative&lt;/code&gt;, we’ll use the
&lt;code&gt;Ap&lt;/code&gt; wrapper from
&lt;code&gt;Control.Applicative.Free&lt;/code&gt;.
Here is its interface:&lt;/p&gt;
    &lt;code&gt;-- `Ap f` is the free applicative over `f`. We never use its
-- constructors directly; instead, we use `liftAp` and the
-- `Applicative` interface (`pure`, `(&amp;lt;*&amp;gt;)`, `liftA2`, etc.)
data Ap f a

-- For *any* `f`, `Ap f` is an applicative.
instance Applicative (Ap f)

-- We can turn any `f a` into an `Ap f a`.
liftAp :: f a -&amp;gt; Ap f a

-- If we can turn our `f` into some applicative `g`, then we can turn
-- `Ap f a` into `g a` in a way that respects the Applicative laws:
--
-- runAp _ (pure x) = pure x
-- runAp f (x &amp;lt;*&amp;gt; y) = (runAp f x) &amp;lt;*&amp;gt; (runAp f y)
--
-- Similar to the `forall x. f x -&amp;gt; g x` in `hoistHandle` above,
-- this lets us turn each `f x` stored in the `Ap f a` into a
-- corresponding `g x`, while remaining ignorant of the specific
-- type `x`.
runAp :: Applicative g =&amp;gt; (forall x. f x -&amp;gt; g x) -&amp;gt; Ap f a -&amp;gt; g a&lt;/code&gt;
    &lt;p&gt;We’ll skip the implementations because we won’t ever manually recurse through an &lt;code&gt;Ap f a&lt;/code&gt; value; from a modularity perspective, we are only
interested in the abstract interface. We declare &lt;code&gt;Query&lt;/code&gt; as the free
applicative over &lt;code&gt;OneQuery&lt;/code&gt;, make it a &lt;code&gt;newtype&lt;/code&gt; to establish an
abstraction boundary between the query library and its callers, and
use &lt;code&gt;deriving newtype&lt;/code&gt; to avoid writing any applicative structure
ourselves:&lt;/p&gt;
    &lt;code&gt;newtype Query a = Query (Free.Ap OneQuery a)
deriving stock Functor
   deriving newtype Applicative
   
-- Helper functions to avoid building `Query` values by hand.

query ::
FromJSON a =&amp;gt; ERP.Request -&amp;gt; Query a
   =
 query _ req Query . Free.liftAp $ QueryAndParse req Right
   
queryAndParse ::
FromJSON a =&amp;gt; ERP.Request -&amp;gt; (a -&amp;gt; Either Text b) -&amp;gt; Query b
   =
 queryAndParse req f Query . Free.liftAp $ QueryAndParse req f   &lt;/code&gt;
    &lt;head rend="h2"&gt;Building a &lt;code&gt;Query&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;From this infrastructure, we can write functions representing individual queries. These are direct applications of the &lt;code&gt;query&lt;/code&gt; and
&lt;code&gt;queryAndParse&lt;/code&gt; helpers:&lt;/p&gt;
    &lt;code&gt;queryLocationId ::
ERP.Location.Name -&amp;gt; Query ERP.Location.Id
   =
 queryLocationId locationName $ ERP.lookupLocation locationName
   query 
queryOrderId ::
ERP.Order.Name -&amp;gt; Query ERP.Order.Id
   =
 queryOrderId orderName 
   queryAndParse$ \case -&amp;gt;
     (ERP.searchOrders orderName) -&amp;gt; Right order
       [order] :_) -&amp;gt; Left "Multiple Orders in response"
       (_-&amp;gt; Left "No Orders in response"       [] &lt;/code&gt;
    &lt;p&gt;From these functions we can build up complex queries using applicative operations:&lt;/p&gt;
    &lt;code&gt;queryOrderAndLocation ::
ERP.Order.Name -&amp;gt; ERP.Location.Name -&amp;gt;
   Query (ERP.Order.Id, ERP.Location.Id)
   =
 queryOrderAndLocation orderName locationName    liftA2 (,) (queryOrderId orderName) (queryLocationId locationName)&lt;/code&gt;
    &lt;head rend="h2"&gt;Running a &lt;code&gt;Query&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;We can run a &lt;code&gt;Query&lt;/code&gt; by using
&lt;code&gt;runAp&lt;/code&gt;.
Because we’re in an applicative context and we’re making requests that
don’t alter the remote system, we can run every request and use a
&lt;code&gt;Validation&lt;/code&gt;
to collect all failures:&lt;/p&gt;
    &lt;code&gt;data RunQueryError e = RequestError e | JsonError Text | ParseResultError Text
type RunQueryErrors e = NonEmpty (RunQueryError e)

runQuery :: forall e m a.
Monad m =&amp;gt;
   ERP.Handle e m -&amp;gt;
   Query a -&amp;gt;
   Validation (RunQueryErrors e) a)
   m (ERP.Handle{performRequest} (Query q) =
 runQuery $ Free.runAp (Compose . go) q
   getCompose where
       go :: OneQuery x -&amp;gt; m (Validation (RunQueryErrors e) x)
QueryAndParse req parse) = performRequest req &amp;lt;&amp;amp;&amp;gt; \case
     go (Left reqErr -&amp;gt; Failure . NonEmpty.singleton $ RequestError reqErr
       Right value -&amp;gt; case Aeson.parseEither Aeson.parseJSON value of
       Left jsonErr -&amp;gt; Failure . NonEmpty.singleton . JsonError $ Text.pack jsonErr
         Right x -&amp;gt; case parse x of
         Left parseErr -&amp;gt; Failure . NonEmpty.singleton $ ParseResultError parseErr
           Right a -&amp;gt; Success a           &lt;/code&gt;
    &lt;p&gt;The implementation can be mostly derived by following the types, but we’ll highlight some specifics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Validation e a&lt;/code&gt;is a type that’s structurally isomorphic to&lt;code&gt;Either e a&lt;/code&gt;, but provides an&lt;code&gt;Applicative&lt;/code&gt;instance that accumulates errors:&lt;code&gt;-- From the validation-selective package. instance Semigroup e =&amp;gt; Applicative (Validation e) where pure = Success -- This asymmetric way of writing &amp;lt;*&amp;gt; maximises laziness. Failure e1 &amp;lt;*&amp;gt; b = Failure $ case b of Failure e2 -&amp;gt; e1 &amp;lt;&amp;gt; e2 Success _ -&amp;gt; e1 Success _ &amp;lt;*&amp;gt; Failure e = Failure e Success f &amp;lt;*&amp;gt; Success a = Success (f a)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Since the composition of any two applicatives is itself an applicative,&lt;/p&gt;&lt;code&gt;Data.Functor.Compose&lt;/code&gt;lets us combine the&lt;code&gt;m&lt;/code&gt;and&lt;code&gt;Validation e&lt;/code&gt;applicatives into&lt;code&gt;Compose m (Validation e)&lt;/code&gt;, which executes actions in&lt;code&gt;m&lt;/code&gt;and accumulates errors — exactly what we want.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Since we use the&lt;/p&gt;&lt;code&gt;Compose&lt;/code&gt;constructor to wrap the result of&lt;code&gt;go&lt;/code&gt;,&lt;code&gt;Free.runAp&lt;/code&gt;will return a&lt;code&gt;Compose m (Validation e) a&lt;/code&gt;which we must unwrap with&lt;code&gt;getCompose&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;go&lt;/code&gt;function processes a single request held in a&lt;code&gt;OneQuery x&lt;/code&gt;, and&lt;code&gt;Free.runAp&lt;/code&gt;uses it to build up the applicative combination of each result.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We accept a handle telling us how to contact the ERP. This is the key location where the handle pattern and the free applicative interact, giving the library user a lot of power: the handle parameter frees us from being coupled to any particular monad and makes it easier to write tests for this code. We’ll see another way to construct a&lt;/p&gt;&lt;code&gt;ERP.Handle&lt;/code&gt;very soon.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The caller of the&lt;/p&gt;&lt;code&gt;Query&lt;/code&gt;interface has no idea that we’re building and consuming free structures under the hood. It’s an implementation detail that doesn’t distort the abstraction boundary at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Extracting requests&lt;/head&gt;
    &lt;p&gt;Now that we can execute queries, let’s explore the main benefit of free applicatives: the ability to analyse the applicative program without running it. We can extract a monoidal summary of any free applicative’s structure via &lt;code&gt;runAp_&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;runAp_ :: Monoid m =&amp;gt; (forall x . f x -&amp;gt; m) -&amp;gt; Ap f a -&amp;gt; m&lt;/code&gt;
    &lt;p&gt;For an intuition why this is true, consider that the constant functor &lt;code&gt;Const r&lt;/code&gt;
has an &lt;code&gt;Applicative&lt;/code&gt; instance whenever &lt;code&gt;r&lt;/code&gt; is a monoid, because &lt;code&gt;pure&lt;/code&gt;
stores a &lt;code&gt;mempty&lt;/code&gt; value and &lt;code&gt;(&amp;lt;*&amp;gt;)&lt;/code&gt; combines the held values with
&lt;code&gt;(&amp;lt;&amp;gt;)&lt;/code&gt;. For a fun exercise, implement &lt;code&gt;runAp_&lt;/code&gt; in terms of &lt;code&gt;runAp&lt;/code&gt; and
&lt;code&gt;Const&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We can use &lt;code&gt;runAp_&lt;/code&gt; to extract a list of every request a &lt;code&gt;Query a&lt;/code&gt;
will send:&lt;/p&gt;
    &lt;code&gt;allRequests :: Query a -&amp;gt; [Request]
Query q) = ordNub $ Free.runAp_ go q
 allRequests (where
       go :: OneQuery x -&amp;gt; [Request]
QueryAndParse req _) = [req]     go (&lt;/code&gt;
    &lt;p&gt;Once we have the list of requests, we can look for ways to optimise them. De-duplicating the requests with &lt;code&gt;ordNub&lt;/code&gt;
is an easy optimisation, but if the remote API supports it, we could
do more advanced optimisations like using a batch request API.&lt;/p&gt;
    &lt;p&gt;As a simple demonstration, we can perform all the lookup requests in advance and construct a &lt;code&gt;Map Request Aeson.Value&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;type SavedRequests = Map Request Aeson.Value

saveRequests ::
forall e m.
   Monad m =&amp;gt;
   Handle e m -&amp;gt; [Request] -&amp;gt; m (Either e SavedRequests)
   Handle{performRequest} requests =
 saveRequests $ Map.fromList &amp;lt;$&amp;gt; traverse go requests
   runExceptT where
       go :: Request -&amp;gt; ExceptT e m (Request, Aeson.Value)
= (req,) &amp;lt;$&amp;gt; ExceptT $ performRequest req     go req &lt;/code&gt;
    &lt;p&gt;Using a collection of saved results, we can construct a handle that returns the saved responses instead of performing real requests:&lt;/p&gt;
    &lt;code&gt;newtype UnsavedRequestError = UnsavedRequestError Request

newHandleFromSavedRequests ::
Applicative m) =&amp;gt; SavedRequests -&amp;gt; Handle UnsavedRequestError m
   (=
 newHandleFromSavedRequests requests Handle
   = \req -&amp;gt;
     { performRequest pure . maybe (Left (UnsavedRequestError req)) Right $
         
           Map.lookup req requests     }&lt;/code&gt;
    &lt;p&gt;This gives us a great story for testing. Since our &lt;code&gt;runQuery&lt;/code&gt; works
with any handle, we can capture some real requests to a file, redact
any sensitive information, and create a pure handle built from saved
requests. We can then use this handle to write test cases that run
real code without performing side-effects.&lt;/p&gt;
    &lt;p&gt;If this example moved too quickly, or you want to see another application of free structures, Justin Le has a spectacular post on matching regular expressions using the free &lt;code&gt;Alternative&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Payoffs and limitations&lt;/head&gt;
    &lt;p&gt;What have we achieved? We decided that we wanted an applicative to describe queries against our remote system. Instead of inventing a complicated data structure to represent the syntax tree of &lt;code&gt;pure&lt;/code&gt; and
&lt;code&gt;(&amp;lt;*&amp;gt;)&lt;/code&gt; calls, we defined a type just to hold one request and took the
free applicative over it. We also used the handle pattern to ask for
only the side-effects that we needed. Both patterns are reasonably
easy to implement, and in exchange we got some pretty neat benefits
that would’ve been harder to realise with either technique alone:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;We can analyse a&lt;/p&gt;&lt;code&gt;Query&lt;/code&gt;without running it, and use the&lt;code&gt;Query&lt;/code&gt;to inform the handle we do eventually use;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As a special case of (1), library users can code against a convenient interface and request individual records, but we can inspect the set of queries before we begin execution and issue optimised, parallelised, de-duplicated and batched requests in their place;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We don’t have to abort at the first failed request — we can collect and report every problem with a&lt;/p&gt;&lt;code&gt;Query&lt;/code&gt;; and&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We can record and replay requests, giving us a great testing story in the style of Ruby’s&lt;/p&gt;&lt;code&gt;vcr&lt;/code&gt;library.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s not all roses, though. We lose a significant amount of expressive power by giving up the monadic interface. For APIs where we need to interleave pure queries and side-effecting requests, losing the &lt;code&gt;Monad&lt;/code&gt; instance might be a bridge too far. Chris Penner suggests
that &lt;code&gt;Selective&lt;/code&gt; functors could be closer to the sweet spot,
but then you lose the nice ergonomics of &lt;code&gt;-XApplicativeDo&lt;/code&gt;.
Chris Done identifies an “Applicative-wired Monad” pattern
which uses a monad only to plumb together applicative values.&lt;/p&gt;
    &lt;p&gt;So where does this leave us? The handle pattern has been working well for us and we plan to continue refactoring code to use handles for the foreseeable future. In narrow contexts where we want to take advantage of static analysis, a well-chosen free applicative has given us a surprising amount of modularity, testability and opportunities for automatic optimisation. In the function that “runs” the free applicative, these two idioms interacted in a very satisfying way: the handle parameter gave us a lot of flexibility without asking library users to write a lot of boilerplate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://exploring-better-ways.bellroy.com/free-applicatives-the-handle-pattern-and-remote-systems.html"/><published>2025-10-16T03:33:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45601230</id><title>TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task</title><updated>2025-10-16T09:11:49.032098+00:00</updated><content>&lt;doc fingerprint="c9c34285afd808f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 22 Jul 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Can AI file your taxes? Not yet. Calculating US personal income taxes is a task that requires building an understanding of vast amounts of English text and using that knowledge to carefully compute results. We propose TaxCalcBench, a benchmark for determining models' abilities to calculate personal income tax returns given all of the necessary information. Our experiment shows that state-of-the-art models succeed in calculating less than a third of federal income tax returns even on this simplified sample set. Our analysis concludes that models consistently misuse tax tables, make errors in tax calculation, and incorrectly determine eligibility. Our findings point to the need for additional infrastructure to apply LLMs to the personal income tax calculation task.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2507.16126"/><published>2025-10-16T03:45:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45601750</id><title>TurboTax’s 20-year fight to stop Americans from filing taxes for free (2019)</title><updated>2025-10-16T09:11:48.603462+00:00</updated><content>&lt;doc fingerprint="2f18c95a29b9b569"&gt;
  &lt;main&gt;
    &lt;p&gt;ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up for ProPublica’s Big Story newsletter to receive stories like this one in your inbox as soon as they are published.&lt;/p&gt;
    &lt;p&gt;Last fall, Intuit’s longtime CEO Brad Smith embarked on a farewell tour of the company’s offices around the world. Smith had presided over 11 years of explosive growth, a period when Intuit had secured its place in the Silicon Valley pantheon, and the tour was like a long party.&lt;/p&gt;
    &lt;p&gt;In Ontario, employees wore T-shirts with Smith’s quasi-spiritual sayings: “Do whatever makes your heart beat fastest” and “Repetition doesn’t ruin the prayer.” In Bangalore, India, workers put on Smith face masks as they posed for selfies with the man himself. Fittingly, the tour culminated in San Diego, the home of TurboTax, the software that transformed the company’s fortunes. There, Smith arrived at his party in a DeLorean, and as he walked a red carpet, cheering employees waved “Brad is Rad” signs. To Smith’s delight, his favorite rock star, Gene Simmons of Kiss, emerged. The two posed for pictures, Simmons clad in black and the beaming CEO flashing the “rock on” hand sign.&lt;/p&gt;
    &lt;p&gt;Intuit began in the 1980s as an accounting software company focused on helping people with their bookkeeping. Over time, the company, like the other giants of Big Tech, cultivated an image of being not just good at what it did, but good, period. In a recent Super Bowl ad, Intuit portrayed itself as a gentle robot that liberates small-business owners from paperwork. The company stresses values above all, urging employees to “deliver awesome” and pursue “integrity without compromise.”&lt;/p&gt;
    &lt;p&gt;Intuit’s QuickBooks accounting product remains a steady moneymaker, but in the past two decades TurboTax, its tax preparation product, has driven the company’s steadily growing profits and made it a Wall Street phenom. When Smith took over in 2008, TurboTax was a market leader, but only a small portion of Americans filed their taxes online. By 2019, nearly 40% of U.S. taxpayers filed online and some 40 million of them did so with TurboTax, far more than with any other product.&lt;/p&gt;
    &lt;p&gt;But the success of TurboTax rests on a shaky foundation, one that could collapse overnight if the U.S. government did what most wealthy countries did long ago and made tax filing simple and free for most citizens.&lt;/p&gt;
    &lt;p&gt;For more than 20 years, Intuit has waged a sophisticated, sometimes covert war to prevent the government from doing just that, according to internal company and IRS documents and interviews with insiders. The company unleashed a battalion of lobbyists and hired top officials from the agency that regulates it. From the beginning, Intuit recognized that its success depended on two parallel missions: stoking innovation in Silicon Valley while stifling it in Washington. Indeed, employees ruefully joke that the company’s motto should actually be “compromise without integrity.”&lt;/p&gt;
    &lt;p&gt;Internal presentations lay out company tactics for fighting “encroachment,” Intuit’s catchall term for any government initiative to make filing taxes easier — such as creating a free government filing system or pre-filling people’s returns with payroll or other data the IRS already has. “For a decade proposals have sought to create IRS tax software or a ReturnFree Tax System; All were stopped,” reads a confidential 2007 PowerPoint presentation from an Intuit board of directors meeting. The company’s 2014-15 plan included manufacturing “3rd-party grass roots” support. “Buy ads for op-eds/editorials/stories in African American and Latino media,” one internal PowerPoint slide states.&lt;/p&gt;
    &lt;p&gt;The centerpiece of Intuit’s anti-encroachment strategy has been the Free File program, hatched 17 years ago in a moment of crisis for the company. Under the terms of an agreement with the federal government, Intuit and other commercial tax prep companies promised to provide free online filing to tens of millions of lower-income taxpayers. In exchange, the IRS pledged not to create a government-run system.&lt;/p&gt;
    &lt;p&gt;Since Free File’s launch, Intuit has done everything it could to limit the program’s reach while making sure the government stuck to its end of the deal. As ProPublica has reported, Intuit added code to the Free File landing page of TurboTax that hid it from search engines like Google, making it harder for would-be users to find.&lt;/p&gt;
    &lt;p&gt;Twelve years ago, Intuit launched its own “free” product: the similarly named “Free Edition” of TurboTax. But unlike the government program, this one comes with traps that can push customers lured with the promise of “free” into paying, some more than $200. Free Edition was a smash hit for Intuit and its pitch for “free” prep remains core to the company’s growth. Recently, it launched a “free, free free free” ad campaign for the Free Edition, including a crossword puzzle in The New York Times in which the answer to every clue was “f-r-e-e.”&lt;/p&gt;
    &lt;p&gt;Intuit knows it’s deceiving its customers, internal company documents obtained by ProPublica show. “The website lists Free, Free, Free and the customers are assuming their return will be free,” said a company PowerPoint presentation that reported the results of an analysis of customer calls this year. “Customers are getting upset.”&lt;/p&gt;
    &lt;p&gt;Intuit also continues to use “dark patterns” — design tricks to get users of its website to do things they don’t necessarily mean to do — to ensure that as many customers as possible pay, former employees say. A marketing concept frequently invoked at Intuit, which goes by the acronym “FUD,” seeks to tap into Americans’ fear, uncertainty and doubt about the tax filing process.&lt;/p&gt;
    &lt;p&gt;An Intuit spokesman declined to answer ProPublica’s detailed questions about its efforts to fend off a government filing system, but he provided a statement.&lt;/p&gt;
    &lt;p&gt;“We empower our customers to take control of their financial lives, which includes being in charge of their own tax preparation,” he said, adding that a “government-run pre-filled tax preparation system that makes the tax collector (who is also the investigator, auditor and enforcer) the tax preparer is fraught with conflicts of interest.”&lt;/p&gt;
    &lt;p&gt;The IRS is seemingly the biggest threat to Intuit and other commercial tax prep businesses, but it has more frequently acted as the industry’s ally, defending the Free File program even in the face of critical internal reviews. The IRS declined to comment for this article.&lt;/p&gt;
    &lt;p&gt;The consequences of Intuit’s efforts affect a huge proportion of the taxpaying public. Americans spend an estimated 1.7 billion hours and $31 billion doing their taxes each year. Just 2.8 million participated in the Free File program this year, down from 5.1 million at the program’s peak in 2005.&lt;/p&gt;
    &lt;p&gt;Intuit’s success has made the men who run the company rich. Smith, the CEO who stepped down last year and is now executive board chair, had a stake worth $20 million when he became chief executive. It ballooned to $220 million by last year. Co-founder Scott Cook is now among the country’s wealthiest people, his fortune soaring to $3.3 billion.&lt;/p&gt;
    &lt;p&gt;This year, Intuit was close to realizing a long-held goal: enshrining the Free File program in law, effectively closing the door on the IRS ever creating a free tax filing system. But an outcry followed ProPublica’s reporting on the matter and Intuit’s treatment of its customers, prompting the provision to be dropped and state and federal investigations into Intuit’s practices.&lt;/p&gt;
    &lt;p&gt;Yet even after this setback, the company remained steadfastly confident that its clout in Washington would win the day.&lt;/p&gt;
    &lt;p&gt;“What we’re not gonna do is fight this publicly because that is exactly what they want us to do,” said Sasan Goodarzi, the new CEO, in a video released to staff this May and obtained by ProPublica. “We are actually working with the IRS and members of the Congress to ensure that the facts are very clear.”&lt;/p&gt;
    &lt;p&gt;Intuit has dominated the tax software market since 1993, when for $225 million, it bought Chipsoft, the San Diego-based company that had created TurboTax. Even then, TurboTax was the most popular option, but Intuit pursued a plan of aggressive growth. The product necessarily came on a disk, and by the end of the 1990s TurboTax boxes were nearly ubiquitous, on shelves in office supply stores across America.&lt;/p&gt;
    &lt;p&gt;As internet speeds increased and dot-com mania took hold, it became apparent that Intuit’s future was not in a box on a shelf. It was online.&lt;/p&gt;
    &lt;p&gt;The prospect of TurboTax’s growth was vast for another reason. As late as 2001, around 45 million Americans still filled out their tax forms on paper. For Intuit, those were all potential customers.&lt;/p&gt;
    &lt;p&gt;But Intuit wasn’t alone in seeing possibilities in the spread of high-speed internet. In Washington, lawmakers began pushing the IRS to modernize and get more taxpayers to file electronically. It was a no-brainer: Filing taxes online would be easier, and the IRS would save staff costs on processing paper returns.&lt;/p&gt;
    &lt;p&gt;The danger to Intuit’s growing business was obvious. If the government succeeded in creating a system that allowed the vast majority of taxpayers to file online for free, TurboTax profits would plummet. Intuit recognized that the notion of “return-free filing” was not going away on its own.&lt;/p&gt;
    &lt;p&gt;And so in 1998, the company hired Bernie McKay, a onetime Carter administration aide and a senior lobbyist at AT&amp;amp;T, to be its vice president for corporate affairs. Intuit executives like to talk about having a “customer obsession” in developing their products. McKay’s obsession is stopping government encroachment. Known to physically bang the table to drive home a point, McKay’s style is “aggressive to the point of offense,” said one fellow tax prep lobbyist. An Intuit spokesman said, “This mischaracterization of Mr. McKay is pure fiction.”&lt;/p&gt;
    &lt;p&gt;McKay, for his part, when asked at a recent tax industry conference which Star Wars character he is, responded, “Darth Vader.”&lt;/p&gt;
    &lt;p&gt;The year McKay was hired, Congress passed a major overhaul of the IRS. The bill, reflecting Intuit’s lobbying, said that the IRS “should cooperate with and encourage the private sector” to increase electronic filing.&lt;/p&gt;
    &lt;p&gt;While McKay came through in his first big test, in 2002, the company found itself up against an unexpected foe, the George W. Bush administration. The threat came from a broad administration initiative to upgrade government technology. One of the proposals called for the IRS to develop “an easy, no-cost option for taxpayers to file their tax return online.”&lt;/p&gt;
    &lt;p&gt;Without such an option, taxpayers were stuck either filing on paper or, to file electronically, paying a tax professional or software company like TurboTax. Providing an alternative would be an obvious improvement, said Mark Forman, an official at the Office of Management and Budget who led the “e-government” program. The technology wasn’t all that complicated, and creating a free, automated filing system would help tens of millions of Americans. “This was seen as a low-cost, high-payoff initiative,” Forman recalled in a recent interview with ProPublica. Standing in the way, he said, was an industry “that lives off the complexity of the tax code.”&lt;/p&gt;
    &lt;p&gt;Intuit revved its new lobbying machine. Even before the OMB report was publicly released, a group of Republican lawmakers, led by TurboTax’s hometown congressman, wrote to the agency arguing that there was no reason for the government to “compete” with the “well-established” private tax prep companies. Intuit’s lobbyists also went above the OMB and pressed their case directly to the White House, Forman recalled.&lt;/p&gt;
    &lt;p&gt;At the IRS, “all hell broke loose,” remembered Terry Lutes, who was then the head of electronic filing at the agency. Intuit’s clout on the Hill meant that lawmakers were soon accusing the IRS of making “secret plans to undercut the industry,” Lutes said. The agency ran the risk of seeing its funding cut if it were to pursue the Bush plan.&lt;/p&gt;
    &lt;p&gt;The IRS commissioner at the time, Charles Rossotti, also opposed the idea. The IRS’ customer service staff, already too thin to respond adequately to Americans’ questions about the tax code, would have to grow substantially to handle millions of software queries. Congress “will never give you sufficient funding,” Rossotti told ProPublica.&lt;/p&gt;
    &lt;p&gt;So the IRS felt caught in the middle. The question became, Lutes said, “Is there some way to come out of this with something for taxpayers that addresses the administration’s objective and at the same time is acceptable to industry?”&lt;/p&gt;
    &lt;p&gt;Intuit, it turned out, did have a way. Since 1999, as part of the company’s strategy to head off encroachment, TurboTax had been offering free tax prep to the poorest filers. It was a program that served to bolster the company’s arguments that government intervention was unnecessary.&lt;/p&gt;
    &lt;p&gt;This became the basis for a deal. The industry would offer free tax prep to a larger portion of taxpayers. In exchange, the IRS would promise not to develop its own system.&lt;/p&gt;
    &lt;p&gt;Intuit organized a coalition of tax prep companies under the name the Free File Alliance, and after negotiations with the IRS, the group agreed to provide free federal filing to 60% of taxpayers, or about 78 million people at the time. Government officials touted the solution as a marvel of public and private cooperation. Americans would get free tax prep, and it would cost the government almost nothing.&lt;/p&gt;
    &lt;p&gt;For Intuit, it was the culmination of years of lobbying. The IRS had signed a contract that said it “will not compete with the [Free File Alliance] in providing free, online tax return preparation and filing services to taxpayers.”&lt;/p&gt;
    &lt;p&gt;What’s more, “free” wasn’t as unprofitable as it sounded. The alliance, guided by a lawyer who was also an Intuit lobbyist, won a series of concessions that made the program palatable to industry. Free File only required the companies to offer free federal returns. They could charge for other products. The state return was the most common, but they could also pitch loans, “audit defense” or even products that had nothing to do with taxes.&lt;/p&gt;
    &lt;p&gt;Free File had another bright side: The companies could tailor their Free File offers so that they didn’t cut into their base of paying customers. The agreement said the industry had to offer free federal services to at least 60% of taxpayers, but each company individually only had to cover 10% of taxpayers. Intuit and the others were free to limit their offers of free tax prep by age, income or state.&lt;/p&gt;
    &lt;p&gt;There was little incentive for the companies to publicize a free alternative to their paid products, and the IRS agreed that the Free File offers need only be listed on a special page of the agency’s website.&lt;/p&gt;
    &lt;p&gt;For Intuit, it was a major victory in the war against encroachment. The company could now focus on turning whatever new customers it acquired through the program into paying users, both that year and in the future.&lt;/p&gt;
    &lt;p&gt;The first year of Free File was 2003, and for Intuit, things went well. On paper, the Free File Alliance was a collection of 17 companies, all of them vying to serve the American taxpayer. But in reality, it was a group made up of two giants and a bunch of gnats. Intuit’s only significant competitor was H&amp;amp;R Block, and even it was a distant second. The rest of the alliance consisted mostly of tiny companies with names like Free1040TaxReturns.com. As a result, Intuit could tailor its Free File offer just the way it wanted.&lt;/p&gt;
    &lt;p&gt;But the next year, Intuit began to lose control of its creation. A scrappy competitor, TaxAct, decided to use Free File to stand out. The company decided it would try to pick up as many new customers as possible and then charge them for ancillary services. Instead of following Intuit’s lead and constraining its offer to a subset of low-income taxpayers, TaxAct went the opposite direction.&lt;/p&gt;
    &lt;p&gt;“Why not go for an offer that’s much simpler to understand?” is how Lance Dunn, the president of the maker of TaxAct, described the strategy in a later court hearing. It began advertising a pitch for “free federal online tax preparation and e-filing for all taxpayers. No restrictions. Everyone qualifies.”&lt;/p&gt;
    &lt;p&gt;TurboTax’s offer on the Free File page, meanwhile, was more difficult to parse: “if you are eligible for EIC, are age 22 or younger, age 62 or older, or active Military with a W2.” (EIC stood for the Earned Income Tax Credit.)&lt;/p&gt;
    &lt;p&gt;TaxAct’s ploy was a smashing success. The company’s volume exploded.&lt;/p&gt;
    &lt;p&gt;Alarmed, Intuit tried to get the other companies not to offer their products for free to too many potential customers, according to Dunn. Such a request could be collusion, a violation of antitrust law, Dunn said. “Intuit asked the Free File Alliance members that we should restrict offers, which I believe is probably not legal for that group to restrain trade,” he said.&lt;/p&gt;
    &lt;p&gt;ProPublica asked Intuit about Dunn’s accusation, but the company did not respond.&lt;/p&gt;
    &lt;p&gt;Dunn, who declined to speak with ProPublica, made these remarks during sworn testimony in 2011. The hearing was part of an antitrust case by the Justice Department against H&amp;amp;R Block after it tried to buy TaxAct. The U.S. argued that, by eliminating a competitor, the merger would create a duopoly of Intuit and H&amp;amp;R Block. Although the Justice Department ultimately blocked that takeover, the market has grown even more consolidated in recent years. In 2019, according to a ProPublica analysis of IRS data, the two giants accounted for 81% of all individual returns filed using tax prep software.&lt;/p&gt;
    &lt;p&gt;On the defensive, Intuit and H&amp;amp;R Block matched TaxAct’s “no restrictions” offer on Free File. Americans rushed to file for free, and in 2005, 5 million people filed their taxes through the program. Free File had become the most popular way to file taxes online.&lt;/p&gt;
    &lt;p&gt;Intuit viewed the popularity of Free File as a serious threat and took its case to Congress. That year, Brad Smith, then a senior vice president at the company and head of TurboTax, told a House committee that “the current Free File Alliance program has drifted very far from its original public service purpose and objective,” as he put it. The program wasn’t supposed to be for everyone, he said: It was for the “disadvantaged, underprivileged and underserved taxpayer populations.”&lt;/p&gt;
    &lt;p&gt;Intuit’s arguments quickly gained traction at the IRS. Already, in March 2005, the IRS had written to the Justice Department for legal advice on modifying the Free File program. The agency wanted to know: Would it run afoul of antitrust laws if the IRS barred companies in the Free File Alliance from offering a free product to everyone?&lt;/p&gt;
    &lt;p&gt;The Justice Department responded in a May 2005 letter. Clearly, wrote Renata Hesse, an antitrust section chief at the department, “any agreement among Alliance members to restrict such free service is likely a form of price fixing” and thus illegal. But there was still a way for Intuit to get what it wanted. She wrote that if the IRS itself were to impose such a restriction, it would be legal.&lt;/p&gt;
    &lt;p&gt;The IRS swooped in to beat back Intuit’s competition, doing for Intuit what the company could not on its own. Despite just 5 million Americans using a program that was purportedly available to 80 million, the IRS agreed that Free File needed to be reined in.&lt;/p&gt;
    &lt;p&gt;The agency made its reasoning clear in a previously unreported letter sent to the Free File Alliance the following year. Bert DuMars, then head of electronic filing at the IRS, wrote that there’d been a huge jump in people using Free File in 2005, but no corresponding boom in people paying for tax prep. “If this trend continued, the IRS was concerned that it could cause many vendors to go out of business,” he wrote. Stock market analysts, he pointed out, had said Free File “represented a threat to future revenues and profits of the publicly traded company participants.” The IRS decided to remove this threat.&lt;/p&gt;
    &lt;p&gt;The new agreement, struck between the IRS and the alliance in 2005, gave Intuit what it had sought. Companies were now expressly barred from offering free tax prep to everyone through the program. Instead, only taxpayers under an income cap, then $50,000 a year, would be eligible.&lt;/p&gt;
    &lt;p&gt;On paper, the program’s eligibility had actually increased to 70% of taxpayers, or about 93 million households, up from the previous 78 million. But in practice, because broad, easy-to-understand offers were now barred, it was clear the program’s use would decline.&lt;/p&gt;
    &lt;p&gt;Intuit had again bent the power of the federal government in its favor. After 2005, the Free File program was never again as popular, eventually falling to about half that year’s level.&lt;/p&gt;
    &lt;p&gt;With the threat of government encroachment on ice and high-speed internet access proliferating in the mid-2000s, Intuit looked forward to steady growth and big profits. The upside of the online software business was huge, with the cost of producing each additional unit approaching zero. And TurboTax was hardly a niche product: Intuit executives still excitedly talk about the TAM, total available market, of TurboTax as every single tax filer in the country, now over 150 million households.&lt;/p&gt;
    &lt;p&gt;But TaxAct’s Free File gambit had forever transformed the industry. Advertising “free” was a great lure, so TaxAct took the battle to a different venue. Barred from making a free offer to everyone through Free File on the IRS’ website, TaxAct decided to make the offer on its own website in 2006. Intuit recognized a credible challenge from the upstart and countered the next year, launching TurboTax Free Edition on its website.&lt;/p&gt;
    &lt;p&gt;Confusingly, there were now two distinct options: the government-sponsored Free File and the commercial free editions.&lt;/p&gt;
    &lt;p&gt;For customers who managed to qualify, the new commercial options offered by these companies were similar to what they could get on the IRS’ Free File website: The underlying software was the same, only the federal return was free, and the companies expected to make money on each customer through charging for a state tax return or other services.&lt;/p&gt;
    &lt;p&gt;But for the companies, there was a clear benefit to winning customers directly, rather than through the IRS program. The companies had complete control over how they handled customers from start to finish.&lt;/p&gt;
    &lt;p&gt;Intuit poured ad dollars into its Free Edition. Not only did the new product effectively meet TaxAct’s challenge, it quickly became the major driver of TurboTax’s customer growth.&lt;/p&gt;
    &lt;p&gt;That growth posed a challenge: how to, as internal company documents put it, “monetize free.” Over successive tax seasons, Intuit unleashed teams of designers, engineers, marketers and data scientists on that problem, working at its headquarters in Mountain View and TurboTax’s main offices in San Diego.&lt;/p&gt;
    &lt;p&gt;Part of the solution was to pitch users side products like loans or “Audit Defense.” But it also meant misleading customers. Frequently “free” didn’t mean free at all. Many who started in TurboTax Free Edition found that if their return required certain commonplace tax forms, they would have to upgrade to a paid edition in order to file.&lt;/p&gt;
    &lt;p&gt;The company came to a key insight: Americans’ anxiety around tax filing is so powerful that it usually trumps any frustration with the TurboTax product, according to three former Intuit staffers. So even if customers click on “free” and are ultimately asked to pay, they will usually do it rather than start the entire process anew. Intuit capitalized on this tendency by making sure the paywall popped up only when the taxpayer was deep into the filing process.&lt;/p&gt;
    &lt;p&gt;“There’s a lot of desperation — people will agree, will click, will do anything to file,” said a former longtime software developer.&lt;/p&gt;
    &lt;p&gt;Every fall before tax season, the company puts every aspect of the TurboTax homepage and filing process through rigorous user testing. Design decisions down to color, word choice and other features are picked to maximize how many customers pay, regardless if they are eligible for the free product. “Dark patterns are something that are spoken of with pride and encouraged in design all hands” meetings, said one former designer. In the design world, “dark patterns” are tactics to get users to do something they don’t necessarily mean to do. (ProPublica previously documented dark patterns encountered by people trying to file their taxes for free.)&lt;/p&gt;
    &lt;p&gt;On TurboTax’s homepage, for example, the company carefully chooses how it describes the different editions. Prominently featured next to Deluxe Edition, which costs around $100, is the phrase “maximize your deductions.”&lt;/p&gt;
    &lt;p&gt;If users initially click on the Deluxe software, they are never offered the choice to go to the Free Edition even if the no-cost option would produce the same return. “Maximize your deductions” was legendary at Intuit for its effectiveness in steering customers eligible for free filing to buy the paid product, according to a former marketing staffer.&lt;/p&gt;
    &lt;p&gt;Another celebrated feature, former staffers said, were the animations that appear as TurboTax users prepare their returns. One shows icons representing different tax deductions scrolling by, while another, at the end of the process, shows paper tax forms being scanned line-by-line and the phrase “Let’s comb through your returns.” What users are not told is that these cartoons reflect no actual processing or calculations; rather, Intuit’s designers deliberately added these delays to both reinforce and ease users’ “Fear, Uncertainty, and Doubt.” The animations emphasize that taxes are complicated but also reassure users that the technological wizardry of TurboTax will protect them from mistakes.&lt;/p&gt;
    &lt;p&gt;In a statement, the Intuit spokesman said, “The process of completing a tax return often has at least some level of stress and anxiety associated with it. … To offset these feelings, we use a variety of design elements — content, animation, movement, etc. — to ensure our customers’ peace of mind.”&lt;/p&gt;
    &lt;p&gt;The 2007 launch of Free Edition started a period of rapid growth for TurboTax. Within two years, use of its web products had almost doubled, and over the past decade, its website has grown each year by an average of 2 million more customers. The company reported this year that TurboTax online had handled 32 million returns. In a statement, it said around a third of that number used Free Edition.&lt;/p&gt;
    &lt;p&gt;The government’s Free File program, meanwhile, has mostly faded into the background, drowned out by Intuit’s and other companies’ “free” offers. The IRS did try advertising campaigns, spending around $2 million some years to spread the word. But compared with the reach of Intuit, this was a pittance: The company reported this year that it spent $800 million on advertising. With its budget slashed by Congress, the IRS has spent no money at all to advertise the program in recent years.&lt;/p&gt;
    &lt;p&gt;Amid its success, Intuit has sometimes had to put down insurgents bent on reforming the tax filing system. In 2007, the same year Intuit launched its Free Edition, Barack Obama, then a candidate for president, took aim at the tax prep industry. In a speech to an audience of tax wonks in Washington, he promised that the IRS would establish a simple return system. “This means no more worry, no more waste of time, no more extra expense for a tax preparer,” he declared.&lt;/p&gt;
    &lt;p&gt;But the Obama administration, as Bush’s had before, found that it was no match for Intuit.&lt;/p&gt;
    &lt;p&gt;Again, Bernie McKay, the lobbyist who had joined Intuit in the late 1990s and outlasted multiple CEOs, led the company’s campaign. In response to the Obama threat, McKay and Intuit’s small army of outside lobbyists turned to Congress, where lawmakers friendly to the company introduced a series of bills that would elevate Free File from a temporary deal with the IRS to the law of the land.&lt;/p&gt;
    &lt;p&gt;Republicans have historically been the company’s most reliable supporters, but some Democrats joined them. Rep. Zoe Lofgren, the California Democrat whose district includes part of Silicon Valley, has introduced or co-sponsored five bills over the years that would codify the Free File program, with names like the Free File Permanence Act. Lofgren’s spokesperson told ProPublica that the congresswoman believes the IRS, because of its role as tax collector, should not also be the tax preparer.&lt;/p&gt;
    &lt;p&gt;Hedging its bets, the company also sought to make sure the IRS could not spend a single dollar creating a public filing system. One internal document says Intuit would “advance legislative language in House Appropriations for ‘No Funds’ restriction on IRS spending” on such a system. It worked. Within a few years, Congress passed a 3,000-page appropriations bill that included a single sentence crucial to Intuit’s financial future: “No funds,” the law decreed, could be used “to provide to any person a proposed final return or statement.”&lt;/p&gt;
    &lt;p&gt;Another important aspect of Intuit’s influence strategy during the Obama years was covertly enlisting minority and women’s groups to press its case.&lt;/p&gt;
    &lt;p&gt;The internal 2014-15 “encroachment strategy” document discloses plans to “leverage trade groups to support House/Senate Free File bills.” It goes on to list the groups Women Impacting Public Policy, The Latino Coalition and the National Black Chamber of Commerce.&lt;/p&gt;
    &lt;p&gt;Intuit has given money to all of those groups over the years. All have signed letters urging Congress to make the Free File deal permanent. “The Free File program has been a clear success,” said one letter signed by The Latino Coalition and the Hispanic Leadership Fund.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Women Impacting Public Policy said it has received $70,000 from Intuit. The amounts given to the other groups are unknown, and they did not respond to requests for comment.&lt;/p&gt;
    &lt;p&gt;Company documents also outline plans to “mobilize” a “coalition” that included think tanks and academics, who published op-eds.&lt;/p&gt;
    &lt;p&gt;Will Marshall, president of the pro-business Progressive Policy Institute, opposed return-free filing in an op-ed in The Hill because doing one’s taxes is “a teachable moment [that] prompts us to review our financial circumstances.”&lt;/p&gt;
    &lt;p&gt;Anti-tax activist Grover Norquist, the most consistent champion of Intuit’s policy positions, warned that “big spenders in Washington, D.C. want to socialize all tax preparation in America.”&lt;/p&gt;
    &lt;p&gt;It is unclear whether they were paid by Intuit or the Free File Alliance. Norquist didn’t respond to a request for comment, and a Progressive Policy Institute spokesman declined to say whether Intuit gave the group money.&lt;/p&gt;
    &lt;p&gt;Whatever external challenges to the status quo Intuit has faced, the company has been able to rely on the IRS’ continuing enthusiastic support of the Free File program. Every few years, the IRS and the industry got together to renew the deal.&lt;/p&gt;
    &lt;p&gt;In part, that was due to the relationships Intuit had developed with high-ranking IRS officials. One, Dave Williams, served as the agency’s top negotiator on the Free File program for several years and “was very commercially sensitive,” said Mark Ernst, the CEO of H&amp;amp;R Block until 2007. Ernst, who later held a senior role at the IRS, told ProPublica that Williams “didn’t want to offend the industry,” noting that “he was particularly open to having sidebar conversations with key people where he could imagine himself landing some day.”&lt;/p&gt;
    &lt;p&gt;Today, Williams works at Intuit, where he’s held the title of chief tax officer since 2013. He is one of several former IRS employees who have gone on to work there. In a statement, Williams told ProPublica he did not have discussions about future employment with Intuit or other companies until after he left the IRS. He added that his career in government was focused on “what is best for the taxpayer” and that he “joined Intuit for the same reason: to help the American taxpayer.”&lt;/p&gt;
    &lt;p&gt;Despite Free File’s declining use, the IRS often claimed that the program was nevertheless meeting one of its original goals: driving more people to file electronically instead of on paper. Ernst, who served as a senior official at the IRS from 2009 to 2010, didn’t believe that a program used by so few people was having any such effect. “It was a talking point that got trotted out all the time to justify the Free File Alliance,” he said.&lt;/p&gt;
    &lt;p&gt;Internally, IRS managers have also argued that the program is, in a way, a success, because it created “a free marketplace,” as one internal management report in 2017 put it. Apparently, customers weren’t the only ones taken in by the word “free.”&lt;/p&gt;
    &lt;p&gt;In 2018, Intuit faced rare scrutiny from inside the IRS. The agency asked its Advisory Council, a group of outside experts, to take stock of Free File. To the company’s alarm, it soon became apparent that the council’s report might be sharply critical.&lt;/p&gt;
    &lt;p&gt;That July, council chair and University of California, Davis, law professor Dennis Ventry wrote two pieces criticizing an Intuit-backed bill in Congress that would make the program permanent. His op-ed in The Hill was called, “Free File providers scam taxpayers; Congress shouldn’t be fooled.”&lt;/p&gt;
    &lt;p&gt;In response, the IRS again rose to Intuit’s aid. It rushed to assure the company that Ventry’s power to affect the program was limited, according to emails to the Free File Alliance obtained through a public records request.&lt;/p&gt;
    &lt;p&gt;“The Commissioner has met directly with Mr. Ventry,” IRS official Ken Corbin wrote to Steve Ryan, a lobbyist for Intuit who also represented the alliance. “Mr. Ventry will recuse himself from participating or contributing to the topic of Free File.”&lt;/p&gt;
    &lt;p&gt;Corbin heads the IRS division that processes most Americans’ tax returns and negotiates the Free File deal with Intuit and the industry.&lt;/p&gt;
    &lt;p&gt;A few days later, Ryan arrived at the IRS’ Constitution Avenue headquarters in Washington to mount a defense of the program. A former Democratic Senate aide turned lawyer-lobbyist, Ryan is known on Capitol Hill for taking on politically fraught clients, including Trump attorney Michael Cohen and the government of Qatar. He helped create Free File in the early 2000s, and it was now his job to secure its future.&lt;/p&gt;
    &lt;p&gt;Ryan’s PowerPoint presentation at the IRS rehashed arguments that the company had been making for the past 15 years. It also highlighted a 2013 study by Brown University professor John Friedman, a former Obama National Economic Council official, to make the point that the program had been successful in generating “Free Tax Returns Outside of Free File.” The presentation did not mention that Friedman’s study was paid for by the Free File companies and was not published in an academic journal. Friedman declined to say what he was paid but told ProPublica he “wrote the piece based on my analysis of the issues, which I stand by.”&lt;/p&gt;
    &lt;p&gt;Ventry, who attended the meeting, got a call the next day alerting him that a California public records request had been filed for his emails — they were subject to such a request because he’s an employee of a state university. It came from the Free File Alliance, as The New York Times later reported. The request, Ventry believes, was designed to “freak me out.”&lt;/p&gt;
    &lt;p&gt;In early October, the council sent a version of its final report, which included a harsh appraisal of the Free File program, to the IRS to seek responses before releasing it publicly the following month.&lt;/p&gt;
    &lt;p&gt;But in mid-October, just weeks before the report saw the light of day, the Free File industry group fired off an “urgent” request to meet with IRS officials. The goal was to re-sign and “improve” the memorandum of understanding that governed the Free File program, according to the emails. The current agreement wasn’t expiring for another two years, but Ryan cited the “time urgency to make changes that will benefit taxpayers” in the coming tax season, adding, “I have not darkened your door in 2018 and need your … attention to this opportunity.”&lt;/p&gt;
    &lt;p&gt;The IRS’ Corbin signed the new deal on Oct. 31. Two weeks later, the Advisory Council report was released, with a damning indictment of the program: “The IRS’s deficient oversight and performance standards for the Free File program put vulnerable taxpayers at risk,” the report found.&lt;/p&gt;
    &lt;p&gt;The expert body recommended that the IRS negotiate a series of new provisions designed to increase the use and oversight of the program, including mandating advertising by the companies. But it was too late. A new deal had already been signed with modest changes. As it had in the past, Intuit and the alliance had effectively insulated the program from reform. Members of the council, Ventry said, were “pissed off.”&lt;/p&gt;
    &lt;p&gt;A spokesman for the Free File Alliance said the group had pushed to renegotiate the deal in 2018 because of the looming 2020 presidential campaign. “The reason for the timing of the extension of the agreement was the political season,” he said. The group had not seen the report before its release, he added.&lt;/p&gt;
    &lt;p&gt;(In August, ProPublica sued the IRS to get more correspondence between the agency and Intuit’s lobbyists. In response to our Freedom of Information Act requests, the agency has withheld over 100 pages. The case is ongoing.)&lt;/p&gt;
    &lt;p&gt;The new deal included rules that barred Free File companies from offering extra products to the relatively small number of users who access the program. This makes it much more difficult to convert those users into paying customers.&lt;/p&gt;
    &lt;p&gt;At around the same time, the industry took steps to make the program more difficult to find. Both Intuit and H&amp;amp;R Block added code to their Free File websites that shielded them from search engines such as Google. The Intuit spokesman said the company increased paid search advertising for Free File “by nearly 80 percent” over the last year and has data showing more people found the program through online search this year than last year, but he declined to provide specific figures.&lt;/p&gt;
    &lt;p&gt;What is clear is that Intuit’s business relies on keeping the use of Free File low. The company has repeatedly declined to say how many of its paying customers are eligible for the program, which is currently open to anyone who makes under $66,000. But based on publicly available data and statements by Intuit executives, ProPublica estimates that roughly 15 million paying TurboTax customers could have filed for free if they found Free File. That represents more than $1.5 billion in estimated revenue, or more than half the total that TurboTax generates. Those affected include retirees, students, people on disability and minimum-wage workers.&lt;/p&gt;
    &lt;p&gt;Customers, meanwhile, remain confused by Intuit’s myriad uses of “free,” and internal documents show the company knows it. Over just a two-week period this past filing season, Intuit received nearly 7,000 TurboTax customer calls in which the phrase “supposed to be free” was uttered, according to a company analysis. One customer complained that Intuit charged him even though “it says ‘free free free’ on the commercial.” The TurboTax representative responded: “That ad has been the bane of my existence.”&lt;/p&gt;
    &lt;p&gt;Even as TurboTax’s business thrived, 2019 has been a rocky year for Intuit’s long-running war against government encroachment. In April, the company was close to finally succeeding in its long-held goal to make Free File permanent. A bill called the Taxpayer First Act was sailing toward almost unanimous approval in Congress. But after ProPublica published a series of stories about the program, including a story showing that military families and students were particularly affected by Intuit’s business tactics, the bill stalled. Congress ultimately removed the provision that would have enshrined Free File in law.&lt;/p&gt;
    &lt;p&gt;After having enabled Intuit for so long, the IRS finally responded to the pressure. It hired a contractor to review the Free File program. But the contractor had previously argued against the IRS offering its own tax prep option, and the review did not recommend major changes. The agency has not yet announced its plans for the future of the program.&lt;/p&gt;
    &lt;p&gt;The agency’s inspector general also launched an audit, which is ongoing. Other investigations and litigation followed, ranging from class-action complaints, alleging that consumers had been deceived by Intuit’s tactics, to investigations and lawsuits by regulators and prosecutors in New York and California. Intuit has denied wrongdoing, saying it “has at all times been clear and fair with its customers.”&lt;/p&gt;
    &lt;p&gt;Despite the scrutiny, Wall Street has continued to embrace the company’s business model. The company recently announced it made $1.5 billion in profits for its fiscal year. It expects its TurboTax unit to grow by 10% next year. Last year the CEO was paid $20 million. The share price hit an all-time record.&lt;/p&gt;
    &lt;p&gt;The company has returned to its old strategy: stay the course and take its case directly to the IRS and Congress. Its allies in the Senate have again advanced an appropriations bill that would bar the IRS from developing its own tax filing system. In the spring, Sasan Goodarzi, a former head of the TurboTax unit who took over as CEO of the entire company in January, sought to reassure employees.&lt;/p&gt;
    &lt;p&gt;“Our view is this will be in the press until there is a resolution with the IRS,” he said, according to the video obtained by ProPublica. “And we’re working with them and we feel very good about where this will end.”&lt;/p&gt;
    &lt;p&gt;Doris Burke contributed research to this story.&lt;/p&gt;
    &lt;p&gt;Do you have information about Intuit, the IRS or tax prep? We want to hear from you. Fill out our questionnaire or contact Justin at [email protected] or via Signal at 774-826-6240.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.propublica.org/article/inside-turbotax-20-year-fight-to-stop-americans-from-filing-their-taxes-for-free"/><published>2025-10-16T05:31:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45601834</id><title>New coding models and integrations</title><updated>2025-10-16T09:11:48.365361+00:00</updated><content>&lt;doc fingerprint="3d49ccd3b51d4a3e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New coding models &amp;amp; integrations&lt;/head&gt;
    &lt;head rend="h2"&gt;October 16, 2025&lt;/head&gt;
    &lt;p&gt;GLM-4.6 and Qwen3-coder-480B are available on Ollama’s cloud service with easy integrations to the tools you are familiar with. Qwen3-Coder-30B has been updated for faster, more reliable tool calling in Ollama’s new engine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started&lt;/head&gt;
    &lt;p&gt;GLM-4.6&lt;/p&gt;
    &lt;code&gt;ollama run glm-4.6:cloud
&lt;/code&gt;
    &lt;p&gt;Qwen3-Coder-480B&lt;/p&gt;
    &lt;code&gt;ollama run qwen3-coder:480b-cloud
&lt;/code&gt;
    &lt;p&gt;For users with more than 300GB of VRAM, &lt;code&gt;qwen3-coder:480b&lt;/code&gt; is also available locally.&lt;/p&gt;
    &lt;p&gt;Qwen3-Coder-30B&lt;/p&gt;
    &lt;code&gt;ollama run qwen3-coder:30b
&lt;/code&gt;
    &lt;head rend="h3"&gt;Example prompts&lt;/head&gt;
    &lt;code&gt;Create a single-page app in a single HTML file with the following requirements:

Name: Ollama's Adventure 
Goal: Jump over obstacles to survive as long as possible.
Features: Increasing speed, high score tracking, retry button, and funny sounds for actions and events.

The UI should be colorful, with parallax scrolling backgrounds.
The characters should look cartoonish, related to alpacas and be fun to watch.
The game should be enjoyable for everyone.
&lt;/code&gt;
    &lt;p&gt;Example code by GLM-4.6 in a single prompt&lt;/p&gt;
    &lt;head rend="h2"&gt;Usage with VS Code&lt;/head&gt;
    &lt;p&gt;First, pull the coding models so they can be accessed via VS Code:&lt;/p&gt;
    &lt;code&gt;ollama pull glm-4.6:cloud
ollama pull qwen3-coder:480b-cloud
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open the copilot chat sidebar&lt;/item&gt;
      &lt;item&gt;Select the model dropdown → Manage models&lt;/item&gt;
      &lt;item&gt;Click on Ollama under Provider Dropdown, then select desired models&lt;/item&gt;
      &lt;item&gt;Select the model dropdown → and choose the model (e.g. &lt;code&gt;glm-4.6&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Usage with Zed&lt;/head&gt;
    &lt;p&gt;First pull the coding models so they can be accessed via Zed:&lt;/p&gt;
    &lt;code&gt;ollama pull glm-4.6:cloud
ollama pull qwen3-coder:480b-cloud
&lt;/code&gt;
    &lt;p&gt;Then, open Zed (now available for Windows!)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Click on the agent panel button (glittering stars)&lt;/item&gt;
      &lt;item&gt;Click on the model dropdown → Configure&lt;/item&gt;
      &lt;item&gt;Select LLM providers → Ollama&lt;/item&gt;
      &lt;item&gt;Confirm the Host URL is &lt;code&gt;http://localhost:11434&lt;/code&gt;, then click Connect&lt;/item&gt;
      &lt;item&gt;Select a model under Ollama&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Usage with Droid&lt;/head&gt;
    &lt;p&gt;First, install Droid:&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://app.factory.ai/cli | sh
&lt;/code&gt;
    &lt;p&gt;Add the following configuration to &lt;code&gt;~/.factory/config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "custom_models": [
    {
      "model_display_name": "GLM-4.6",
      "model": "glm-4.6:cloud",
      "base_url": "http://localhost:11434/v1",
      "api_key": "not-needed",
      "provider": "generic-chat-completion-api",
      "max_tokens": 16384
    },
    {
      "model_display_name": "Qwen3-Coder-480B",
      "model": "qwen3-coder:480b-cloud",
      "base_url": "http://localhost:11434/v1",
      "api_key": "not-needed",
      "provider": "generic-chat-completion-api",
      "max_tokens": 16384
    }
  ]
}
&lt;/code&gt;
    &lt;p&gt;Then run Droid and type &lt;code&gt;/model&lt;/code&gt; to change to the model:&lt;/p&gt;
    &lt;code&gt;╭──────────────────────────────────────────────────╮
│ &amp;gt; GLM-4.6 [current]                              │
│   Qwen3-Coder-480B                               │
│                                                  │
│ ↑/↓ to navigate, Enter to select, ESC to go back │
╰──────────────────────────────────────────────────╯
&lt;/code&gt;
    &lt;head rend="h2"&gt;Integrations&lt;/head&gt;
    &lt;p&gt;Ollama’s documentation now includes sections on using Ollama with popular coding tools:&lt;/p&gt;
    &lt;head rend="h2"&gt;Cloud API access&lt;/head&gt;
    &lt;p&gt;Cloud models such as &lt;code&gt;glm-4.6&lt;/code&gt; and &lt;code&gt;qwen3-coder:480b&lt;/code&gt; can also be accessed directly via ollama.com’s cloud API:&lt;/p&gt;
    &lt;p&gt;First, create an API key, and set it in your environment&lt;/p&gt;
    &lt;code&gt;export OLLAMA_API_KEY="your_api_key_here"
&lt;/code&gt;
    &lt;p&gt;Then, call ollama.com’s API&lt;/p&gt;
    &lt;code&gt;curl https://ollama.com/api/chat \
    -H "Authorization: Bearer $OLLAMA_API_KEY" \
    -d '{
    "model": "glm-4.6",
    "messages": [{
      "role": "user",
      "content": "Write a snake game in HTML."
    }]
}'
&lt;/code&gt;
    &lt;p&gt;For more information see the Ollama’s API documentation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ollama.com/blog/coding-models"/><published>2025-10-16T05:46:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45601982</id><title>Upcoming Rust language features for kernel development</title><updated>2025-10-16T09:11:47.999723+00:00</updated><content>&lt;doc fingerprint="9fa9d83718a13eea"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Upcoming Rust language features for kernel development&lt;/head&gt;
    &lt;quote&gt;Benefits for LWN subscribers&lt;p&gt;The primary benefit from subscribing to LWN is helping to keep us publishing, but, beyond that, subscribers get immediate access to all site content and access to a number of extra site features. Please sign up today!&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;The Rust for Linux project has been good for Rust, Tyler Mandry, one of the co-leads of Rust's language-design team, said. He gave a talk at Kangrejos 2025 covering upcoming Rust language features and thanking the Rust for Linux developers for helping drive them forward. Afterward, Benno Lossin and Xiangfei Ding went into more detail about their work on the three most important language features for kernel development: field projections, in-place initialization, and arbitrary self types.&lt;/p&gt;
    &lt;p&gt; Many people have remarked that the development of new language features in Rust can be quite slow, Mandry said. Partly, that can be attributed to the care the Rust language team takes to avoid enshrining bad designs. But the biggest reason is "&lt;quote&gt;alignment in attention&lt;/quote&gt;". The Rust project is driven by volunteers, which means that if there are not people focusing on pushing a given feature or group of related features forward, they languish. The Rust for Linux project has actually been really helpful for addressing that, Mandry explained, because it is something that a lot of people are excited about, and that focuses effort onto the few specific things that the Linux kernel needs. &lt;/p&gt;
    &lt;p&gt; Mandry then went through a whirlwind list of upcoming language features, including types without known size information, reference-counting improvements, user-defined function modifiers of the same kind as const, and more. At the end, he asked which of those were most important to Rust for Linux, and how the assembled kernel developers would prioritize them. Beyond the three features to be discussed later, Lossin said that the project definitely wanted the ability to write functions that can be evaluated at compile time (called const functions in Rust) in trait definitions. Danilo Krummrich asked for specialization, which immediately prompted an "&lt;quote&gt;Oh no!&lt;/quote&gt;" from Lossin, due to the feature's nearly decade-long history of causing problems for Rust's type system. Specialization would allow two overlapping implementations for a single trait to exist, with the compiler picking the more specific one. Matthew Maurer asked for some ability to control what the compiler does on integer overflow. &lt;/p&gt;
    &lt;p&gt;Ultimately, Miguel Ojeda told Mandry that the priority should be on stabilizing the unstable language features that Rust for Linux currently uses, followed by language features that would change how the project structures its code, followed by everything else. The next two talks went into much more detail about the current status and future plans for some of those key language features.&lt;/p&gt;
    &lt;head rend="h4"&gt;Field projections&lt;/head&gt;
    &lt;p&gt; Field projection refers to the idea of taking a pointer to a structure, and turning it into a pointer to a field of the structure. Rust does already have this for the built-in reference and pointer types, but it can't always be made to work for user-defined smart-pointer types. Since the Rust for Linux developers would like to have custom smart pointers to handle untrusted data, reference counting, external locking, and related kernel complications, they would benefit from a general language feature allowing field projections for all pointer types using the same syntax. Lossin spoke about his work on the problem, which has been ongoing since Kangrejos 2022. There has been "&lt;quote&gt;lots of progress&lt;/quote&gt;" so far, but the work is still in the design stage, with a few details left to work out. &lt;/p&gt;
    &lt;p&gt;The built-in field projections all have the same kind of type signature, Lossin explained. For example, the code for converting a reference to an object into a reference to one of its fields and the code for converting a raw pointer to an object into a raw pointer to one of its fields look different, but have similar signatures:&lt;/p&gt;
    &lt;quote&gt;fn project_reference(r: &amp;amp;MyStruct) -&amp;gt; &amp;amp;Field { &amp;amp;r.field } unsafe fn project_pointer(r: *mut MyStruct) -&amp;gt; *mut Field { unsafe { &amp;amp;raw mut (*r).field } } // The equivalent C code would look like this: struct field *project(struct my *r) { return &amp;amp;(r-&amp;gt;field); }&lt;/quote&gt;
    &lt;p&gt;This example uses the relatively recent raw borrow syntax.&lt;/p&gt;
    &lt;p&gt; The Pin type throws a bit of a wrench into things. The Rust compiler is, by default, free to move structures around for performance reasons. That doesn't work when the structure is being referenced from the C side, so the Pin type is used to mark structures that shouldn't be moved. Projecting a &lt;del&gt;Pin&amp;lt;MyStruct&amp;gt;&lt;/del&gt; Pin&amp;lt;&amp;amp;mut MyStruct&amp;gt; [Lossin sent LWN a correction: Pin is always used to wrap a pointer type, not a structure directly] might produce either a Pin&amp;lt;&amp;amp;mut Field&amp;gt; or a plain &amp;amp;mut Field depending on whether the field is also of a type that shouldn't be moved or not. So the most general possible signature for the field projection operation would be something like this, Lossin said: &lt;/p&gt;
    &lt;quote&gt;Container&amp;lt;'a, Struct&amp;gt; -&amp;gt; Output&amp;lt;'a, Field&amp;gt;&lt;/quote&gt;
    &lt;p&gt;That is, given some pointer type that wraps a structure and must be valid for lifetime a, projecting a field gives a (possibly different) output pointer type wrapping a field of that structure, valid for the same lifetime. Lossin then gave an example of how supporting this could make fully implementing read-copy-update (RCU) support in the kernel's Rust bindings a lot easier.&lt;/p&gt;
    &lt;p&gt;The RCU mechanism protects readers from concurrent writers, he explained, but it doesn't protect writers from each other. It's somewhat common in the kernel, therefore, to have a mutex protecting some data, with a frequently accessed field of that data being protected by RCU. That way, readers rely on the RCU lock (which is cheap), and writers synchronize with each other using the mutex. Translating that interface to Rust poses problems: Rust doesn't allow any access to the content inside a Mutex without locking it first, so the straightforward translation of this pattern wouldn't work. It would force Rust readers to lock the mutex in order to read the RCU field, which would be an unacceptable performance hit.&lt;/p&gt;
    &lt;p&gt;With generalized field projection in the language, though, the Rust for Linux developers could write bindings that permit projecting a &amp;amp;Mutex&amp;lt;MyStruct&amp;gt; into an &amp;amp;Rcu&amp;lt;Field&amp;gt; without holding the lock. In driver code, attempting to read from the RCU-protected field would look like a normal access, the same way it is in C — but the compiler would still check that the other, non-RCU-protected data isn't touched without holding the mutex.&lt;/p&gt;
    &lt;p&gt;Lossin ended by asking the assembled developers to keep an eye on the tracking issue for the feature, and provide feedback on it. Daniel Almeida asked whether testing the feature outside the mainline kernel was really helpful; Ojeda affirmed that it was, because that makes it easier to go to the Rust team and make a case to stabilize the feature. The Rust for Linux project is trying not to use any new unstable features (and to compile with a version of Rust equal to or older than the version packaged on Debian stable), so the feature needs to be completed and make it into Debian 14 (expected in 2027) before it will be widely usable in kernel code.&lt;/p&gt;
    &lt;p&gt; Andreas Hindborg asked: "&lt;quote&gt;Can we have this yesterday, please?&lt;/quote&gt;", to general amusement. The kernel's Rust bindings already feature a plethora of custom pointers encoding various invariants; this feature, whenever it becomes available to kernel code, may make them a good deal easier to use in driver code. &lt;/p&gt;
    &lt;head rend="h4"&gt;Arbitrary self types&lt;/head&gt;
    &lt;p&gt;Ding gave an update immediately afterward about another ergonomic language feature for custom pointers: arbitrary self types. In Rust, a method on a type can have a first argument that is an object of the type or that is a reference to one. Such a method can be called with the .method() syntax, instead of the more general Type::function() syntax. But the proliferation of smart pointers in kernel Rust code means that the programmer frequently does not have a plain reference; often, they instead have a Pin, an Arc, or some other smart-pointer type.&lt;/p&gt;
    &lt;p&gt;The arbitrary self types proposal that Ding has been working on would let programmers write methods that take smart pointers, instead of normal references:&lt;/p&gt;
    &lt;quote&gt;impl MyStruct { fn method(self: Pin&amp;lt;&amp;amp;mut MyStruct&amp;gt;) {} }&lt;/quote&gt;
    &lt;p&gt;Unfortunately, adding this to the compiler has not proved to be straightforward. The interaction with Rust's existing Deref trait, which makes custom smart pointers possible in the first place, complicates the implementation because not all of the type information is available while searching for matching methods. Currently, if the user has a Pin&amp;lt;&amp;amp;mut MyStruct&amp;gt; and they call a method on it, the compiler will first look for a matching method for Pin. If one isn't found, it will try to dereference the type, producing a &amp;amp;mut MyStruct. That type is checked for matching methods, and then is dereferenced one final time, producing a MyStruct. That type will finally have a matching method, or else the compiler will emit a type error.&lt;/p&gt;
    &lt;p&gt;By the time that procedure begins checking functions associated with MyStruct, it will have already discarded information about the wrapping types, which an implementation of arbitrary self types needs. Ding spent a few minutes explaining the approaches for rectifying the problem that he had tried and discarded, before focusing on the current approach. He has added another trait — tentatively called Receiver — that is used to mark types that can be used with arbitrary self types. Then the compiler can try following the chain of Receiver implementations before following the chain of Deref implementations. That does mean that a pointer type will have to opt into being used as an arbitrary self type, but Ding didn't see that as a downside. Letting the author of a pointer type decide when it should support the new feature eliminates a lot of concerns around accidentally introducing backward compatibility problems. For the kernel, it doesn't really impose a barrier, because the Rust developers can just add Receiver implementations as they run across cases that require them.&lt;/p&gt;
    &lt;p&gt;Ojeda asked how long Ding thought it would take to finalize the arbitrary self types feature; in particular, would it be ready within a year? Ding agreed that a year was possible, although he would need support from the Rust language team in order to make that happen. He wants to run Crater, the tool that the Rust community uses to check whether compiler changes break any published Rust libraries, against his change before submitting the code. Ojeda offered help with obtaining a large build machine to do that, since Ding has had trouble previously with the memory requirements to compile some packages during a Crater run.&lt;/p&gt;
    &lt;head rend="h4"&gt;In-place initialization&lt;/head&gt;
    &lt;p&gt;The other topic that Ding wanted to cover was his work on in-place initialization. Like the other new language features being discussed, this doesn't really enable new use cases, but it does make common kernel code cleaner. Currently, Rust code in the kernel uses the pin_init!() macro to create structures that are fixed in place after initialization (by being wrapped in Pin).&lt;/p&gt;
    &lt;p&gt; There's nothing wrong with pin_init!(): "&lt;quote&gt;We love pin_init!()! We want to make a language feature out of it.&lt;/quote&gt;" Adopting a language feature for in-place initialization would also help with a handful of sharp edges outside kernel code; it could make creating large Future values on the heap more ergonomic, and let some traits become dyn-compatible. The exact design of this language feature was more up in the air; Ding covered three different proposals for how it could work. &lt;/p&gt;
    &lt;p&gt;The simplest, proposed by Alice Ryhl and Lossin, would be to add a new keyword, init, before a structure-initializing expression in order to ask the compiler to automatically write an implementation of the kernel's PinInit trait. That has the nice benefit of being a fairly minimal change to the language, although it would lock in the use of the PinInit trait in its current form.&lt;/p&gt;
    &lt;p&gt;Another solution, proposed by Taylor Cramer, would introduce a new type of reference into the language. Rust's existing references can either be read from (&amp;amp;) or read from and written to (&amp;amp;mut). This proposal would add a third type, &amp;amp;out, that can only be written to, not read from. The only way to use an &amp;amp;out reference would be to either write to it, or use projection to break it apart into multiple &amp;amp;out references to various fields. Under this scheme, in-place initialization would look like allocating space on the heap, and then returning an &amp;amp;out reference. The calling code could then fill it in however it wants to, potentially passing off sub-parts to other functions. The compiler would track that the &amp;amp;out references are all used before allowing the code to obtain a normal &amp;amp;mut reference to the heap allocation.&lt;/p&gt;
    &lt;p&gt;That proposal was considerably less polished than Ryhl and Lossin's approach, however. Ding later told me that he, Mandry, and other compiler contributors at Kangrejos were actually working on figuring out how it would interact with some of the Rust compiler's internals in between talks that day. By the end of the conference, they had a rough idea of how it could be implemented, so a more detailed version of the out-pointer proposal may be forthcoming shortly.&lt;/p&gt;
    &lt;p&gt;The final design, taking inspiration from C++, would be a form of guaranteed optimization, where constructing a new value and then immediately moving it to the heap causes it to be constructed on the heap in the first place. Ding was less sure about the details of the final proposal; he suggested that the best way forward might be to implement both the PinInit proposal and the out-reference proposal, and see how well each approach works in practice.&lt;/p&gt;
    &lt;p&gt;Regardless of which approach ends up being chosen, it seems clear that Mandry's point about the Rust for Linux project driving language improvement is correct. While these features are in the early stages, adopting them could significantly simplify code involving user-defined smart pointers, both within and outside the kernel.&lt;/p&gt;
    &lt;p&gt;Update: Since the talks described in this article, the work on field projection has received an update. Lossin wrote in to inform LWN that all fields of all structures are now considered structurally pinned, so projecting a Pin will now always produce a Pin&amp;lt;&amp;amp;mut Field&amp;gt; or similar value.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Development tools/Rust&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Conference&lt;/cell&gt;
        &lt;cell&gt;Kangrejos/2025&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Oct 8, 2025 17:39 UTC (Wed) by NYKevin (subscriber, #129325) [Link] (2 responses) To the best of my understanding, this is true but incomplete. Rust generally assumes that it is safe to allow *the developer* to move structures around (for whatever reason they see fit). But there are some structures where doing this would invalidate some invariant or cause other problems. Normally, that would be the developer's problem, but in some cases, those invariants are required to avoid UB, so safe Rust must not allow a move in that case. FFI, as described in the article, is a good example of this. Another is self-referential structures (structures that hold pointers into themselves). The Pin type imposes an exception on this general rule - anything* behind a pinned pointer (a Pin&amp;lt;Ptr&amp;gt; where Ptr: Deref) is considered immovable. Aside for C++ developers: Because Rust always implements move by memcpy, this is nearly equivalent to saying that all Rust types are trivially movable unless protected by Pin. However, Rust does not run the destructor on the moved-from value (unlike C++, which does), so the vast majority of RAII types can be trivially movable in Rust but not in C++. Pin is only needed for the rare case where the move constructor would do something other than transferring ownership of a resource to the new instance. What makes Pin especially weird is the fact that it is not a language-level feature at all. It is a standard library feature, implemented entirely in terms of Rust's visibility and borrow-checking rules. In order to accomplish that, it wraps the pointer and makes it unsafe to obtain &amp;amp;mut T (where T is the pointee type), only allowing safe access to &amp;amp;T. Blocking &amp;amp;mut T is necessary because of functions like std::mem::swap() (roughly equivalent to C++'s std::swap()). But that would prevent safe Rust from mutating the pointee at all, so now you have to write a bunch of boilerplate to individually allow specific field mutations (even if the field is private, you probably still want a pub(crate) mutator for internal use). This boilerplate frequently contains a large amount of unsafe Rust to selectively unwrap the Pin, and that is generally considered problematic. If I'm reading between the lines correctly, this field projection proposal would let you put all of that unsafe boilerplate inside of your smart pointer implementation, so you don't have to keep rewriting it for each new pinnable (non-Unpin) type. That sounds like a pretty big win to me, but I imagine the RCU benefits are probably a bigger deal, since you can macro-generate the Pin boilerplate if you really want to. *** * Technically, there's an exception to the exception - (pointee) types that implement Unpin are immune to pinning and can be moved regardless. But Unpin is mostly just a convenience trait for generics (it allows them to use pinning pointers without having to worry about whether that's needlessly restrictive for any given pointee type), so for the purposes of this discussion, I'm going to ignore it. Posted Oct 8, 2025 17:51 UTC (Wed) by daroc (editor, #160859) [Link] In any case, I believe you're correct. If the pin-projection proposal is adopted, a lot of boiler-plate around accessing fields of pinned objects could go away. Instead you'd just use the same field-projection operator as with references, pointers, reference-counted pointers, etc. Posted Oct 9, 2025 7:19 UTC (Thu) by lossin (subscriber, #177724) [Link] Indeed, the field projection proposal is to have an operator that makes all the unsafe boilerplate obsolete. The wins around Pin are very big, as almost every type in the kernel will end up being pinned. That's because pinning is infectious -- as soon as one of the (transitive) fields of your struct requires it, the entire struct needs to be pinned. Field projections will also improve ergonomics for raw pointers, handling uninitialized data as well as untrusted data and many more user declared types. The RCU abstractions are a different beast, as without field projections, they probably aren't possible in a safe, ergonomic and performant way. Without it, we'd probably need two different APIs, one unsafe and performant, the other safe &amp;amp; ergonomic. But do note that we could also macro-generate the unsafe boilerplate similar to how it's possible with Pin. The issue is, that means yet another derive macro that one needs to think about (and rather bad ergonomics). &amp;gt; * Technically, there's an exception to the exception - (pointee) types that implement Unpin are immune to pinning and can be moved regardless. But Unpin is mostly just a convenience trait for generics (it allows them to use pinning pointers without having to worry about whether that's needlessly restrictive for any given pointee type), so for the purposes of this discussion, I'm going to ignore it. In the current version of the pin-ergonomics proposal, Unpin is going to play a more important role. It will allow coercions between Pin&amp;lt;&amp;amp;mut T&amp;gt; and &amp;amp;mut T if T: Unpin. Then we can just always allow pin projections, so going from Pin&amp;lt;&amp;amp;mut Struct&amp;gt; to Pin&amp;lt;&amp;amp;mut Field&amp;gt; and let the coercions &amp;amp; types handle the not structurally pinned fields. We still need field projections for pin projections in the kernel: our mutex guard is pinning the data and thus it also needs to provide this kind of projection. Posted Oct 8, 2025 18:48 UTC (Wed) by prittner (subscriber, #110534) [Link] (1 responses) This is a sorely needed feature for folks who work with large types. The cost of having to reserve lots of stack space, initialize the object, and then copy it to the heap can be significant; in some cases, it's impossible to construct the type on the stack at all due to its size (unless you also blow up your stack size). Inlining can help, but once you have to use dynamic dispatch that goes out the window. Posted Oct 9, 2025 7:30 UTC (Thu) by lossin (subscriber, #177724) [Link] Very much so. The Asahi GPU driver developed a similar macro-based solution to this problem around the same time we added the pin-init [1] crate that we use to this day. They regularly had structs with hundreds of fields that obviously overflowed the small kernel stack. IIRC, their use-case also wasn't inlined (at least not everywhere), which made it rather painful. This use-case is part of the core motivation for developing the language feature. &lt;head&gt;Pin is a bit more complicated than described&lt;/head&gt;&lt;head&gt;Pin is a bit more complicated than described&lt;/head&gt;&lt;head&gt;Pin is a bit more complicated than described&lt;/head&gt;&lt;lb/&gt; &amp;gt; &lt;lb/&gt; &amp;gt; If I'm reading between the lines correctly, this field projection proposal would let you put all of that unsafe boilerplate inside of your smart pointer implementation, so you don't have to keep rewriting it for each new pinnable (non-Unpin) type. That sounds like a pretty big win to me, but I imagine the RCU benefits are probably a bigger deal, since you can macro-generate the Pin boilerplate if you really want to.&lt;head&gt;Heap initialization is important&lt;/head&gt;&lt;head&gt;Heap initialization is important&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/Articles/1039073/"/><published>2025-10-16T06:12:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45602124</id><title>Steve Jobs and Cray-1 to be featured on 2026 American Innovations $1 coin</title><updated>2025-10-16T09:11:47.767550+00:00</updated><content/><link href="https://www.usmint.gov/news/press-releases/united-states-mint-releases-2026-american-innovation-one-dollar-coin-program-designs"/><published>2025-10-16T06:39:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45602179</id><title>Journalists turn in access badges, exit Pentagon rather than agreeing new rules</title><updated>2025-10-16T09:11:47.372642+00:00</updated><content>&lt;doc fingerprint="9655c112931089c1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Journalists turn in access badges, exit Pentagon rather than agree to new reporting rules&lt;/head&gt;
    &lt;head rend="h2"&gt;Journalists turn in access badges, exit Pentagon rather than agree to new reporting rules&lt;/head&gt;
    &lt;p&gt;NEW YORK (AP) — Dozens of reporters turned in access badges and exited the Pentagon on Wednesday rather than agree to government-imposed restrictions on their work, pushing journalists who cover the American military further from the seat of its power. The nation’s leadership called the new rules “common sense” to help regulate a “very disruptive” press.&lt;/p&gt;
    &lt;p&gt;News outlets were nearly unanimous in rejecting new rules imposed by Defense Secretary Pete Hegseth that would leave journalists vulnerable to expulsion if they sought to report on information — classified or otherwise — that had not been approved by Hegseth for release.&lt;/p&gt;
    &lt;p&gt;Many of the reporters waited to leave together at a 4 p.m. deadline set by the Defense Department to get out of the building. As the hour approached, boxes of documents lined a Pentagon corridor and reporters carried chairs, a copying machine, books and old photos to the parking lot from suddenly abandoned workspaces. Shortly after 4, about 40 to 50 journalists left together after handing in badges.&lt;/p&gt;
    &lt;p&gt;“It’s sad, but I’m also really proud of the press corps that we stuck together,” said Nancy Youssef, a reporter for The Atlantic who has had a desk at the Pentagon since 2007. She took a map of the Middle East out to her car.&lt;/p&gt;
    &lt;p&gt;It is unclear what practical impact the new rules will have, though news organizations vowed they’d continue robust coverage of the military no matter the vantage point.&lt;/p&gt;
    &lt;p&gt;Images of reporters effectively demonstrating against barriers to their work are unlikely to move supporters of President Donald Trump, many of whom resent journalists and cheer his efforts to make their jobs harder. Trump has been involved in court fights against The New York Times, CBS News, ABC News, the Wall Street Journal and The Associated Press in the past year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trump supports the new rules&lt;/head&gt;
    &lt;p&gt;Speaking to reporters at the White House on Tuesday, Trump backed his defense secretary’s new rules. “I think he finds the press to be very disruptive in terms of world peace,” Trump said. “The press is very dishonest.”&lt;/p&gt;
    &lt;p&gt;Even before issuing his new press policy, Hegseth, a former Fox News Channel host, has systematically choked off the flow of information. He’s held only two formal press briefings, banned reporters from accessing many parts of the sprawling Pentagon without an escort and launched investigations into leaks to the media.&lt;/p&gt;
    &lt;p&gt;He has called his new rules “common sense” and said the requirement that journalists sign a document outlining the rules means they acknowledge the new rules, not necessarily agree to them. Journalists see that as a distinction without a difference.&lt;/p&gt;
    &lt;p&gt;“What they’re really doing, they want to spoon-feed information to the journalist, and that would be their story. That’s not journalism,” said Jack Keane, a retired U.S. Army general and Fox News analyst, said on Hegseth’s former network.&lt;/p&gt;
    &lt;p&gt;When he served, Keane said he required new brigadier generals to take a class on the role of the media in a democracy so they wouldn’t be intimidated and also see reporters as a conduit to the American public. “There were times when stories were done that made me flinch a little bit,” he said. “But that’s usually because we had done something that wasn’t as good as we should have done it.”&lt;/p&gt;
    &lt;p&gt;Youssef said it made no sense to sign on to rules that said reporters should not solicit military officials for information. “To agree to not solicit information is to agree to not be a journalist,” she said. “Our whole goal is soliciting information.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Reporting on US military affairs will continue — from a greater distance&lt;/head&gt;
    &lt;p&gt;Several reporters posted on social media when they turned in their press badges.&lt;/p&gt;
    &lt;p&gt;“It’s such a tiny thing, but I was really proud to see my picture up on the wall of Pentagon correspondents,” wrote Heather Mongilio, a reporter for USNINews, which covers the Navy. “Today, I’ll hand in my badge. The reporting will continue.”&lt;/p&gt;
    &lt;p&gt;Mongilio, Youssef and others emphasized that they’ll continue to do their jobs no matter where their desks are. Some sources will continue to speak with them, although they say some in the military have been chilled by threats from Pentagon leadership.&lt;/p&gt;
    &lt;p&gt;In an essay, NPR reporter Tom Bowman noted the many times he’d been tipped off by people he knew from the Pentagon and while embedded in the military about what was happening, even if it contradicted official lines put out by leadership. Many understand the media’s role.&lt;/p&gt;
    &lt;p&gt;“They knew the American public deserved to know what’s going on,” Bowman wrote. “With no reporters able to ask questions, it seems the Pentagon leadership will continue to rely on slick social media posts, carefully orchestrated short videos and interviews with partisan commentators and podcasters. No one should think that’s good enough.”&lt;/p&gt;
    &lt;p&gt;The Pentagon Press Association, whose 101 members represent 56 news outlets, has spoken out against the rules. Organizations from across the media spectrum, from legacy organizations like The Associated Press and The New York Times to outlets like Fox and the conservative Newsmax, told their reporters to leave instead of signing the new rules.&lt;/p&gt;
    &lt;p&gt;Only the conservative One America News Network signed on. Its management likely believes it will have greater access to Trump administration officials by showing its support, Gabrielle Cuccia, a former Pentagon reporter who was fired by OANN earlier this year for writing an online column criticizing Hegseth’s media policies, told the AP in an interview.&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;Associated Press reporter Laurie Kellman in London contributed to this report. David Bauder writes about the intersection of media and entertainment for the AP. Follow him at http://x.com/dbauder and https://bsky.app/profile/dbauder.bsky.social&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apnews.com/article/pentagon-press-access-hegseth-trump-restrictions-5d9c2a63e4e03b91fc1546bb09ffbf12"/><published>2025-10-16T06:51:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45602676</id><title>Liquibase continues to advertise itself as "open source" despite license switch</title><updated>2025-10-16T09:11:46.654925+00:00</updated><content>&lt;doc fingerprint="ad5d765d4f0017b6"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 1.9k&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Open&lt;/p&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Search first&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I searched and no similar issues were found&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Description&lt;/head&gt;
    &lt;p&gt;Liquibase has migrated to the Functional Source License, which is not an open source license, as Liquibase itself acknowledges. But this repository continues to misleadingly characterize Liquibase as an open source project, particularly in various places in the file README.md.&lt;/p&gt;
    &lt;head rend="h3"&gt;Steps To Reproduce&lt;/head&gt;
    &lt;p&gt;View github.com/liquibase/liquibase, or the contents of the README.md file, to find out about the liquibase community project.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expected/Desired Behavior&lt;/head&gt;
    &lt;p&gt;The README.md file and any similar project documentation will no longer misleadingly suggest that liquibase is still an open source project.&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquibase Version&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Database Vendor &amp;amp; Version&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquibase Integration&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquibase Extensions&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;OS and/or Infrastructure Type/Provider&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional Context&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Are you willing to submit a PR?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I'm willing to submit a PR (Thank you!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;gedejong, ssddanbrown, nucatus, whyman10x, jalberto and 5 moreGregOriol&lt;/p&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h3"&gt;Assignees&lt;/head&gt;
    &lt;head rend="h3"&gt;Labels&lt;/head&gt;
    &lt;p&gt;No labels&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/liquibase/liquibase/issues/7374"/><published>2025-10-16T08:02:52+00:00</published></entry></feed>