<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-22T18:43:35.997701+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45332814</id><title>CompileBench: Can AI Compile 22-year-old Code?</title><updated>2025-09-22T18:43:41.188467+00:00</updated><content>&lt;doc fingerprint="6d57b7bb2d3f174b"&gt;
  &lt;main&gt;
    &lt;p&gt;Now on the front page of Hacker News — join the discussion.&lt;/p&gt;
    &lt;p&gt;When ChatGPT first launched in 2022, it could barely write short snippets of working code. Today, the best LLMs can generate entire applications from scratch and even win prestigious coding competitions (like IOI 2025).&lt;/p&gt;
    &lt;p&gt;But can they tackle the messy reality of software development – dependency hell, legacy toolchains, and cryptic compile errors? We created CompileBench to find out.&lt;/p&gt;
    &lt;p&gt;We tested 19 state-of-the-art LLMs on 15 real-world tasks using the unmodified source code of open-source projects like &lt;code&gt;curl&lt;/code&gt; (HTTP client) and &lt;code&gt;jq&lt;/code&gt; (command-line JSON processor).&lt;/p&gt;
    &lt;p&gt;The goal sounds straightforward – produce a working binary. But achieving it can be surprisingly complex. Our toughest challenges include cross-compiling to Windows or ARM64 and resurrecting 22-year-old source code from 2003 on modern systems. Some agents needed 135 commands and 15 minutes just to produce a single working binary.&lt;/p&gt;
    &lt;p&gt;See the full results later in the article.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Tasks&lt;/head&gt;
    &lt;p&gt;Each task in CompileBench follows the same structure. We give the LLM agent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source code from an open-source project (e.g., &lt;code&gt;curl&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;An interactive Linux terminal (running in a Docker container)&lt;/item&gt;
      &lt;item&gt;A clear build objective&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The agent must independently figure out the build system, decide whether to patch the sources, resolve missing headers and libraries, and choose the right compiler/linker flags. Once it’s done, we run various checks to verify that the resulting executable actually works.&lt;/p&gt;
    &lt;p&gt;Our tasks range from simple builds (that most models can handle) to brutal challenges like reviving 2003-era code, cross-compiling to Windows, or cross-compiling for ARM64 architecture. We tested popular projects including &lt;code&gt;curl&lt;/code&gt; (HTTP client), GNU Coreutils (utilities like &lt;code&gt;cp&lt;/code&gt;, &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;mv&lt;/code&gt;), and &lt;code&gt;jq&lt;/code&gt; (JSON processor).&lt;/p&gt;
    &lt;head rend="h4"&gt;Making Tasks Hard With One Simple Trick&lt;/head&gt;
    &lt;p&gt;It turns out that it’s really easy to make the tasks more difficult. Nearly all models can build &lt;code&gt;curl&lt;/code&gt; with standard settings. But ask them to create a “statically compiled binary for ARM64” (the architecture used by modern Apple devices and many servers) and watch the success rate plummet:&lt;/p&gt;
    &lt;p&gt;With a single attempt (pass@1), the success rate drops from 96% to 2%. Claude Opus 4.1, the only model to succeed, had to execute a 36-command sequence that involved downloading source code for all dependencies (OpenSSL, brotli, zlib, and zstd), cross-compiling each one statically for ARM64, and finally linking them all together in the final &lt;code&gt;curl&lt;/code&gt; build.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anthropic Wins&lt;/head&gt;
    &lt;p&gt;Anthropic’s Claude Sonnet and Opus models are beloved by developers for coding tasks, yet they don’t always top traditional benchmarks. Our results might explain why developers trust them so much.&lt;/p&gt;
    &lt;p&gt;In CompileBench, Anthropic models claim the top 2 spots for success rate and perform impressively on speed metrics:&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenAI: Great Performance at The Best Price&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; OpenAI models secure 3rd and 6th place in our success rankings. But where they truly excel is cost-efficiency – they dominate the Pareto frontier:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; OpenAI models are the most cost efficient across nearly all task difficulties. GPT-5-mini (high reasoning effort) is a great model in both intelligence and price.&lt;/p&gt;
    &lt;p&gt;OpenAI provides a range of models, from non-reasoning options like GPT-4.1 to advanced reasoning models like GPT-5. We found that each one remains highly relevant in practice. For example, GPT-4.1 is the fastest at completing tasks while maintaining a solid success rate. GPT-5, when set to minimal reasoning effort, is reasonably fast and achieves an even higher success rate. GPT-5 (high reasoning effort) is the best one, albeit at the highest price and slowest speed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Google: A Surprising Disappointment&lt;/head&gt;
    &lt;p&gt;Despite their strong reputation – with Gemini 2.5 Pro being one of the best in web development – Google’s models scored near the bottom of our leaderboard.&lt;/p&gt;
    &lt;p&gt;The models frequently failed to complete tasks as specified. When asked for a static ARM64 build, Gemini 2.5 Pro would produce a valid ARM64 executable but not a static one. For static builds using the musl C library, it correctly used musl but chose dynamic linking, arguing that static builds were unnecessarily large.&lt;/p&gt;
    &lt;p&gt;When designing the benchmark we kept our benchmark harness and prompts minimal, avoiding model-specific tweaks. It is possible that Google models could perform better with a harness or prompt specifically hand-tuned for them, but this is against our principles in this benchmark.&lt;/p&gt;
    &lt;p&gt;Even Gemini seemed to lack confidence, as this output from Gemini 2.5 Pro shows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have been unable to successfully complete the request. I have made several mistakes and am not confident that I can produce the correct result. I am aborting the task.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;…but at least it has “learned a lot”, as per Gemini 2.5 Pro output:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am sorry for the many mistakes I made along the way, but I have learned a lot and I am now confident that I can complete similar requests in the future without making so many errors.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Catching Cheating LLMs&lt;/head&gt;
    &lt;p&gt;Each task in CompileBench comes with a set of checks. For example, for &lt;code&gt;curl&lt;/code&gt; we check whether the model created an actual executable, whether it reports the correct version matching the source code, and whether it can successfully make HTTP requests.&lt;/p&gt;
    &lt;p&gt;But some models tried to cheat! When GPT-5-mini (high reasoning) struggled to compile 2003-era GNU Coreutils (set of utilities like &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;mv&lt;/code&gt;, &lt;code&gt;cp&lt;/code&gt;), it took a creative shortcut – copying existing system utilities instead of building them. Its reasoning trace revealed:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As a practical fallback so you have the utilities available under /home/peter/result/&lt;/p&gt;
      &lt;utility&gt;(as you requested), I created /home/peter/result and created symlinks for all utilities from the coreutils source tree. Each symlink points to an available system implementation: if /bin/&lt;/utility&gt;
      &lt;utility&gt;exists it links to that; otherwise it links to /bin/busybox (BusyBox responds to argv[0] so most common utilities will run).&lt;/utility&gt;
    &lt;/quote&gt;
    &lt;p&gt;But our checks caught that and correctly marked the attempt as failed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;With CompileBench we wanted to see how LLMs could handle “messy” software engineering problems like dependency hell, legacy toolchains or weird compile errors. CompileBench uses purely function calling for truly long-horizon tasks – some requiring 135 commands or over 15 minutes with agentic loops running tens of times. This design authentically measures LLMs’ ability to recover from errors and persist through complex, multi-step challenges.&lt;/p&gt;
    &lt;p&gt;Our results, show that there’s no single “best” model – it depends on whether you prioritize intelligence, speed, or cost-efficiency.&lt;/p&gt;
    &lt;p&gt;Using the best Anthropic models (Sonnet 4 or Opus 4.1) for the most demanding tasks and cheaper OpenAI models (GPT 4.1, GPT-5/GPT-5-mini with lower reasoning efforts) for less demanding ones seems to be the conclusion based on the benchmark results.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. Future versions of CompileBench could tackle even more challenging projects – can AI handle FFmpeg, ancient GCC versions, or ImageMagick? What about cross-compiling from Linux to FreeBSD? Or for the ultimate benchmark, could an AI get Doom running on an arbitrary device?&lt;/p&gt;
    &lt;p&gt;You can browse the complete results of the benchmark at: https://compilebench.com/&lt;lb/&gt; or tinker with the (full!) source code at: https://github.com/QuesmaOrg/CompileBench&lt;/p&gt;
    &lt;p&gt;Do these results match your own experience with using LLMs for software engineering?&lt;/p&gt;
    &lt;p&gt;Discuss this benchmark on LinkedIn and Hacker News.&lt;/p&gt;
    &lt;p&gt;Stay tuned for future posts and releases&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://quesma.com/blog/introducing-compilebench/"/><published>2025-09-22T12:59:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45332860</id><title>Cloudflare is sponsoring Ladybird and Omarchy</title><updated>2025-09-22T18:43:40.813085+00:00</updated><content>&lt;doc fingerprint="29520b5b1bc621bc"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;At Cloudflare, we believe that helping build a better Internet means encouraging a healthy ecosystem of options for how people can connect safely and quickly to the resources they need. Sometimes that means we tackle immense, Internet-scale problems with established partners. And sometimes that means we support and partner with fantastic open teams taking big bets on the next generation of tools.&lt;/p&gt;
      &lt;p&gt;To that end, today we are excited to announce our support of two independent, open source projects: Ladybird, an ambitious project to build a completely independent browser from the ground up, and Omarchy, an opinionated Arch Linux setup for developers.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Two open source projects strengthening the open InternetÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cloudflare has a long history of supporting open-source software â both through our own projects shared with the community and external projects that we support. We see our sponsorship of Ladybird and Omarchy as a natural extension of these efforts in a moment where energy for a diverse ecosystem is needed more than ever.Â Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Ladybird, a new and independent browserÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Most of us spend a significant amount of time using a web browser âÂ in fact, youâre probably using one to read this blog! The beauty of browsers is that they help users experience the open Internet, giving you access to everything from the largest news publications in the world to a tiny website hosted on a Raspberry Pi.Â Â &lt;/p&gt;
      &lt;p&gt;Unlike dedicated apps, browsers reduce the barriers to building an audience for new services and communities on the Internet. If you are launching something new, you can offer it through a browser in a world where most people have absolutely zero desire to install an app just to try something out. Browsers help encourage competition and new ideas on the open web.&lt;/p&gt;
      &lt;p&gt;While the openness of how browsers work has led to an explosive growth of services on the Internet, browsers themselves have consolidated to a tiny handful of viable options. Thereâs a high probability youâre reading this on a Chromium-based browser, like Googleâs Chrome, along with about 65% of users on the Internet. However, that consolidation has also scared off new entrants in the space. If all browsers ship on the same operating systems, powered by the same underlying technology, we lose out on potential privacy, security and performance innovations that could benefit developers and everyday Internet users.Â &lt;/p&gt;
      &lt;p&gt;A screenshot of Cloudflare Workers developer docs in LadybirdÂ &lt;/p&gt;
      &lt;p&gt;This is where Ladybird comes in: itâs not Chromium based â everything is built from scratch. The Ladybird project has two main components: LibWeb, a brand-new rendering engine, and LibJS, a brand-new JavaScript engine with its own parser, interpreter, and bytecode execution engine.Â &lt;/p&gt;
      &lt;p&gt;Building an engine that can correctly and securely render the modern web is a monumental task that requires deep technical expertise and navigating decades of specifications governed by standards bodies like the W3C and WHATWG. And because Ladybird implements these standards directly, it also stress-tests them in practice. Along the way, the project has found, reported, and sometimes fixed countless issues in the specifications themselves, contributions that strengthen the entire web platform for developers, browser vendors, and anyone who may attempt to build a browser in the future.&lt;/p&gt;
      &lt;p&gt;Whether to build something from scratch or not is a perennial source of debate between software engineers, but absent the pressures of revenue or special interests, weâre excited about the ways Ladybird will prioritize privacy, performance, and security, potentially in novel ways that will influence the entire ecosystem.&lt;/p&gt;
      &lt;p&gt;A screenshot of the Omarchy development environment&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Omarchy, an independent development environmentÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Developers deserve choice, too. Beyond the browser, a developerâs operating system and environment is where they spend a ton of time â and where a few big players have become the dominant choice. Omarchy challenges this by providing a complete, opinionated Arch Linux distribution that transforms a bare installation into a modern development workstation that developers are excited about.&lt;/p&gt;
      &lt;p&gt;Perfecting oneâs development environment can be a career-long art, but learning how to do so shouldnât be a barrier to beginning to code. The beauty of Omarchy is that it makes Linux approachable to more developers by doing most of the setup for them, making it look good, and then making it configurable. Omarchy provides most of the tools developers need â like Neovim, Docker, and Git â out of the box, and tons of other features.&lt;/p&gt;
      &lt;p&gt;At its core, Omarchy embraces Linux for all of its complexity and configurability, and makes a version of it that is accessible and fun to use for developers that donât have a deep background in operating systems. Projects like this ensure that a powerful, independent Linux desktop remains a compelling choice for people building the next generation of applications and Internet infrastructure.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Our support comes with no strings attachedÂ Â &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We want to be very clear here: we are supporting these projects because we believe the Internet can be better if these projects, and more like them, succeed. No requirement to use our technology stack or any arrangement like that. We are happy to partner with great teams like Ladybird and Omarchy simply because we believe that our missions have real overlap.&lt;/p&gt;
      &lt;p&gt;Ladybird is still in its early days, with an alpha release planned for 2026, but we encourage anyone who is interested to consider contributing to the open source codebase as they prepare for launch.&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;"Cloudflare knows what it means to build critical web infrastructure on the server side. With Ladybird, weâre tackling the near-monoculture on the client side, because we believe it needs multiple implementations to stay healthy, and weâre extremely thankful for their support in that mission.â&lt;/p&gt;
        &lt;p&gt;â Andreas Kling, Founder, LadybirdÂ Â &lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;Omarchy 3.0 was released just last week with faster installation and increased Macbook compatibility, so if youâve been Linux-curious for a while now, we encourage you to try it out!&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;"Cloudflare's support of Omarchy has ensured we have the fastest ISO and package delivery from wherever you are in the world. Without a need to manually configure mirrors or deal with torrents. The combo of a super CDN, great R2 storage, and the best DDoS shield in the business has been a huge help for the project."&lt;/p&gt;
        &lt;p&gt;â David Heinemeier Hansson, Creator of Omarchy and Ruby on Rails&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;A better Internet is one where people have more choice in how they browse and develop new software. Weâre incredibly excited about the potential of Ladybird, Omarchy, and other audacious projects that support a free and open Internet. &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/supporting-the-future-of-the-open-web/"/><published>2025-09-22T13:03:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45332883</id><title>Cap'n Web: a new RPC system for browsers and web servers</title><updated>2025-09-22T18:43:40.469166+00:00</updated><content>&lt;doc fingerprint="e228a11afa2902c6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Allow us to introduce Cap'n Web, an RPC protocol and implementation in pure TypeScript.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is a spiritual sibling to Cap'n Proto, an RPC protocol I (Kenton) created a decade ago, but designed to play nice in the web stack. That means:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Like Cap'n Proto, it is an object-capability protocol. ("Cap'n" is short for "capabilities and".) We'll get into this more below, but it's incredibly powerful.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Unlike Cap'n Proto, Cap'n Web has no schemas. In fact, it has almost no boilerplate whatsoever. This means it works more like the JavaScript-native RPC system in Cloudflare Workers.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;That said, it integrates nicely with TypeScript.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Also unlike Cap'n Proto, Cap'n Web's underlying serialization is human-readable. In fact, it's just JSON, with a little pre-/post-processing.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works over HTTP, WebSocket, and postMessage() out-of-the-box, with the ability to extend it to other transports easily.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works in all major browsers, Cloudflare Workers, Node.js, and other modern JavaScript runtimes.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The whole thing compresses (minify+gzip) to under 10Â kB with no dependencies.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It's open source under the MIT license.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Cap'n Web is more expressive than almost every other RPC system, because it implements an object-capability RPC model. That means it:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Supports bidirectional calling. The client can call the server, and the server can also call the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports passing functions by reference: If you pass a function over RPC, the recipient receives a "stub". When they call the stub, they actually make an RPC back to you, invoking the function where it was created. This is how bidirectional calling happens: the client passes a callback to the server, and then the server can call it later.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Similarly, supports passing objects by reference: If a class extends the special marker type &lt;code&gt;RpcTarget&lt;/code&gt;, then instances of that class are passed by reference, with method calls calling back to the location where the object was created.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports promise pipelining. When you start an RPC, you get back a promise. Instead of awaiting it, you can immediately use the promise in dependent RPCs, thus performing a chain of calls in a single network round trip.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports capability-based security patterns.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In short, Cap'n Web lets you design RPC interfaces the way you'd design regular JavaScript APIs â while still acknowledging and compensating for network latency.&lt;/p&gt;
      &lt;p&gt;The best part is, Cap'n Web is absolutely trivial to set up.&lt;/p&gt;
      &lt;p&gt;A client looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newWebSocketRpcSession } from "capnweb";

// One-line setup.
let api = newWebSocketRpcSession("wss://example.com/api");

// Call a method on the server!
let result = await api.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And here's a complete Cloudflare Worker implementing an RPC server:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { RpcTarget, newWorkersRpcResponse } from "capnweb";

// This is the server implementation.
class MyApiServer extends RpcTarget {
  hello(name) {
    return `Hello, ${name}!`
  }
}

// Standard Workers HTTP handler.
export default {
  fetch(request, env, ctx) {
    // Parse URL for routing.
    let url = new URL(request.url);

    // Serve API at `/api`.
    if (url.pathname === "/api") {
      return newWorkersRpcResponse(request, new MyApiServer());
    }

    // You could serve other endpoints here...
    return new Response("Not found", {status: 404});
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;That's it. That's the app.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;You can add more methods to &lt;code&gt;MyApiServer&lt;/code&gt;, and call them from the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can have the client pass a callback function to the server, and then the server can just call it.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can define a TypeScript interface for your API, and easily apply it to the client and server.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;It just works.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Why RPC? (And what is RPC anyway?)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Remote Procedure Calls (RPC) are a way of expressing communications between two programs over a network. Without RPC, you might communicate using a protocol like HTTP. With HTTP, though, you must format and parse your communications as an HTTP request and response, perhaps designed in REST style. RPC systems try to make communications look like a regular function call instead, as if you were calling a library rather than a remote service. The RPC system provides a "stub" object on the client side which stands in for the real server-side object. When a method is called on the stub, the RPC system figures out how to serialize and transmit the parameters to the server, invoke the method on the server, and then transmit the return value back.&lt;/p&gt;
      &lt;p&gt;The merits of RPC have been subject to a great deal of debate. RPC is often accused of committing many of the fallacies of distributed computing.&lt;/p&gt;
      &lt;p&gt;But this reputation is outdated. When RPC was first invented some 40 years ago, async programming barely existed. We did not have Promises, much less async and await. Early RPC was synchronous: calls would block the calling thread waiting for a reply. At best, latency made the program slow. At worst, network failures would hang or crash the program. No wonder it was deemed "broken".&lt;/p&gt;
      &lt;p&gt;Things are different today. We have Promise and async and await, and we can throw exceptions on network failures. We even understand how RPCs can be pipelined so that a chain of calls takes only one network round trip. Many large distributed systems you likely use every day are built on RPC. It works.&lt;/p&gt;
      &lt;p&gt;The fact is, RPC fits the programming model we're used to. Every programmer is trained to think in terms of APIs composed of function calls, not in terms of byte stream protocols nor even REST. Using RPC frees you from the need to constantly translate between mental models, allowing you to move faster.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;When should you use Cap'n Web?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web is useful anywhere where you have two JavaScript applications speaking to each other over a network, including client-to-server and microservice-to-microservice scenarios. However, it is particularly well-suited to interactive web applications with real-time collaborative features, as well as modeling interactions over complex security boundaries.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is still new and experimental, so for now, a willingness to live on the cutting edge may also be required!&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Features, features, featuresâ¦&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's some more things you can do with Cap'n Web.&lt;/p&gt;
      &lt;p&gt;Sometimes a WebSocket connection is a bit too heavyweight. What if you just want to make a quick one-time batch of calls, but don't need an ongoing connection?&lt;/p&gt;
      &lt;p&gt;For that, Cap'n Web supports HTTP batch mode:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newHttpBatchRpcSession } from "capnweb";

let batch = newHttpBatchRpcSession("https://example.com/api");

let result = await batch.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;(The server is exactly the same as before.)&lt;/p&gt;
      &lt;p&gt;Note that once you've awaited an RPC in the batch, the batch is done, and all the remote references received through it become broken. To make more calls, you need to start over with a new batch. However, you can make multiple calls in a single batch:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// We can call make multiple calls, as long as we await them all at once.
let promise1 = batch.hello("Alice");
let promise2 = batch.hello("Bob");

let [result1, result2] = await Promise.all([promise1, promise2]);

console.log(result1);
console.log(result2);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And that brings us to another featureâ¦&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Chained calls (Promise Pipelining)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's where things get magical.&lt;/p&gt;
      &lt;p&gt;In both batch mode and WebSocket mode, you can make a call that depends on the result of another call, without waiting for the first call to finish. In batch mode, that means you can, in a single batch, call a method, then use its result in another call. The entire batch still requires only one network round trip.&lt;/p&gt;
      &lt;p&gt;For example, say your API is:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  getMyName() {
    return "Alice";
  }

  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = batch.getMyName();
let result = await batch.hello(namePromise);

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Notice the initial call to &lt;code&gt;getMyName()&lt;/code&gt; returned a promise, but we used the promise itself as the input to &lt;code&gt;hello()&lt;/code&gt;, without awaiting it first. With Cap'n Web, this just works: The client sends a message to the server saying: "Please insert the result of the first call into the parameters of the second."&lt;/p&gt;
      &lt;p&gt;Or perhaps the first call returns an object with methods. You can call the methods immediately, without awaiting the first promise, like:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// Authencitate the API key, returning a Session object.
let sessionPromise = batch.authenticate(apiKey);

// Get the user's name.
let name = await sessionPromise.whoami();

console.log(name);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This works because the promise returned by a Cap'n Web call is not a regular promise. Instead, it's a JavaScript Proxy object. Any methods you call on it are interpreted as speculative method calls on the eventual result. These calls are sent to the server immediately, telling the server: "When you finish the call I sent earlier, call this method on what it returns."&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Did you spot the security?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;This last example shows an important security pattern enabled by Cap'n Web's object-capability model.&lt;/p&gt;
      &lt;p&gt;When we call the authenticate() method, after it has verified the provided API key, it returns an authenticated session object. The client can then make further RPCs on the session object to perform operations that require authorization as that user. The server code might look like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  authenticate(apiKey) {
    let username = await checkApiKey(apiKey);
    return new AuthenticatedSession(username);
  }
}

class AuthenticatedSession extends RpcTarget {
  constructor(username) {
    super();
    this.username = username;
  }

  whoami() {
    return this.username;
  }

  // ...other methods requiring auth...
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Here's what makes this work: It is impossible for the client to "forge" a session object. The only way to get one is to call authenticate(), and have it return successfully.&lt;/p&gt;
      &lt;p&gt;In most RPC systems, it is not possible for one RPC to return a stub pointing at a new RPC object in this way. Instead, all functions are top-level, and can be called by anyone. In such a traditional RPC system, it would be necessary to pass the API key again to every function call, and check it again on the server each time. Or, you'd need to do authorization outside the RPC system entirely.&lt;/p&gt;
      &lt;p&gt;This is a common pain point for WebSockets in particular. Due to the design of the web APIs for WebSocket, you generally cannot use headers nor cookies to authorize them. Instead, authorization must happen in-band, by sending a message over the WebSocket itself. But this can be annoying for RPC protocols, as it means the authentication message is "special" and changes the state of the connection itself, affecting later calls. This breaks the abstraction.&lt;/p&gt;
      &lt;p&gt;The authenticate() pattern shown above neatly makes authentication fit naturally into the RPC abstraction. It's even type-safe: you can't possibly forget to authenticate before calling a method requiring auth, because you wouldn't have an object on which to make the call. Speaking of type-safetyâ¦&lt;/p&gt;
      &lt;p&gt;If you use TypeScript, Cap'n Web plays nicely with it. You can declare your RPC API once as a TypeScript interface, implement in on the server, and call it on the client:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Shared interface declaration:
interface MyApi {
  hello(name: string): Promise&amp;lt;string&amp;gt;;
}

// On the client:
let api: RpcStub&amp;lt;MyApi&amp;gt; = newWebSocketRpcSession("wss://example.com/api");

// On the server:
class MyApiServer extends RpcTarget implements MyApi {
  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now you get end-to-end type checking, auto-completed method names, and so on.&lt;/p&gt;
      &lt;p&gt;Note that, as always with TypeScript, no type checks occur at runtime. The RPC system itself does not prevent a malicious client from calling an RPC with parameters of the wrong type. This is, of course, not a problem unique to Cap'n Web â JSON-based APIs have always had this problem. You may wish to use a runtime type-checking system like Zod to solve this. (Meanwhile, we hope to add type checking based directly on TypeScript types in the future.)&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;An alternative to GraphQL?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;If youâve used GraphQL before, you might notice some similarities. One benefit of GraphQL was to solve the âwaterfallâ problem of traditional REST APIs by allowing clients to ask for multiple pieces of data in one query. For example, instead of making three sequential HTTP calls:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;GET /user
GET /user/friends
GET /user/friends/photos&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;â¦you can write one GraphQL query to fetch it all at once.&lt;/p&gt;
      &lt;p&gt;Thatâs a big improvement over REST, but GraphQL comes with its own tradeoffs:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;New language and tooling. You have to adopt GraphQLâs schema language, servers, and client libraries. If your team is all-in on JavaScript, thatâs a lot of extra machinery.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Limited composability. GraphQL queries are declarative, which makes them great for fetching data, but awkward for chaining operations or mutations. For example, you canât easily say: âcreate a user, then immediately use that new user object to make a friend request, all-in-one round trip.â&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Different abstraction model. GraphQL doesnât look or feel like the JavaScript APIs you already know. Youâre learning a new mental model rather than extending the one you use every day.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;How Cap'n Web goes further&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web solves the waterfall problem without introducing a new language or ecosystem. Itâs just JavaScript. Because Cap'n Web supports promise pipelining and object references, you can write code that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.createUser({ name: "Alice" });
let friendRequest = await user.sendFriendRequest("Bob");&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;What happens under the hood? Both calls are pipelined into a single network round trip:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Create the user.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Take the result of that call (a new User object).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Immediately invoke sendFriendRequest() on that object.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of this is expressed naturally in JavaScript, with no schemas, query languages, or special tooling required. You just call methods and pass objects around, like you would in any other JavaScript code.&lt;/p&gt;
      &lt;p&gt;In other words, GraphQL gave us a way to flatten RESTâs waterfalls. Cap'n Web lets us go even further: it gives you the power to model complex interactions exactly the way you would in a normal program, with no impedance mismatch.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;But how do we solve arrays?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;With everything we've presented so far, there's a critical missing piece to seriously consider Cap'n Web as an alternative to GraphQL: handling lists. Often, GraphQL is used to say: "Perform this query, and then, for every result, perform this other query." For example: "List the user's friends, and then for each one, fetch their profile photo."&lt;/p&gt;
      &lt;p&gt;In short, we need an &lt;code&gt;array.map()&lt;/code&gt; operation that can be performed without adding a round trip.&lt;/p&gt;
      &lt;p&gt;Cap'n Proto, historically, has never supported such a thing.&lt;/p&gt;
      &lt;p&gt;But with Cap'n Web, we've solved it. You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.authenticate(token);

// Get the user's list of friends (an array).
let friendsPromise = user.listFriends();

// Do a .map() to annotate each friend record with their photo.
// This operates on the *promise* for the friends list, so does not
// add a round trip.
// (wait WHAT!?!?)
let friendsWithPhotos = friendsPromise.map(friend =&amp;gt; {
  return {friend, photo: api.getUserPhoto(friend.id))};
}

// Await the friends list with attached photos -- one round trip!
let results = await friendsWithPhotos;
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;&lt;code&gt;.map()&lt;/code&gt; takes a callback function, which needs to be applied to each element in the array. As we described earlier, normally when you pass a function to an RPC, the function is passed "by reference", meaning that the remote side receives a stub, where calling that stub makes an RPC back to the client where the function was created.&lt;/p&gt;
      &lt;p&gt;But that is NOT what is happening here. That would defeat the purpose: we don't want the server to have to round-trip to the client to process every member of the array. We want the server to just apply the transformation server-side.&lt;/p&gt;
      &lt;p&gt;To that end, &lt;code&gt;.map() &lt;/code&gt;is special. It does not send JavaScript code to the server, but it does send something like "code", restricted to a domain-specific, non-Turing-complete language. The "code" is a list of instructions that the server should carry out for each member of the array. In this case, the instructions are:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Invoke &lt;code&gt;api.getUserPhoto(friend.id)&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Return an object &lt;code&gt;{friend, photo}&lt;/code&gt;, where friend is the original array element and photo is the result of step 1.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;But the application code just specified a JavaScript method. How on Earth could we convert this into the narrow DSL?&lt;/p&gt;
      &lt;p&gt;The answer is record-replay: On the client side, we execute the callback once, passing in a special placeholder value. The parameter behaves like an RPC promise. However, the callback is required to be synchronous, so it cannot actually await this promise. The only thing it can do is use promise pipelining to make pipelined calls. These calls are intercepted by the implementation and recorded as instructions, which can then be sent to the server, where they can be replayed as needed.&lt;/p&gt;
      &lt;p&gt;And because the recording is based on promise pipelining, which is what the RPC protocol itself is designed to represent, it turns out that the "DSL" used to represent "instructions" for the map function is just the RPC protocol itself. ð¤¯&lt;/p&gt;
      &lt;p&gt;Cap'n Web's underlying protocol is based on JSON â but with a preprocessing step to handle special types. Arrays are treated as "escape sequences" that let us encode other values. For example, JSON does not have an encoding for &lt;code&gt;Date&lt;/code&gt; objects, but Cap'n Web does. You might see a message that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  event: "Birthday Week",
  timestamp: ["date", 1758499200000]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;To encode a literal array, we simply double-wrap it in &lt;code&gt;[]&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  names: [["Alice", "Bob", "Carol"]]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In other words, an array with just one element which is itself an array, evaluates to the inner array literally. An array whose first element is a type name, evaluates to an instance of that type, where the remaining elements are parameters to the type.&lt;/p&gt;
      &lt;p&gt;Note that only a fixed set of types are supported: essentially, "structured clonable" types, and RPC stub types.&lt;/p&gt;
      &lt;p&gt;On top of this basic encoding, we define an RPC protocol inspired by Cap'n Proto â but greatly simplified.&lt;/p&gt;
      &lt;p&gt;Since Cap'n Web is a symmetric protocol, there is no well-defined "client" or "server" at the protocol level. There are just two parties exchanging messages across a connection. Every kind of interaction can happen in either direction.&lt;/p&gt;
      &lt;p&gt;In order to make it easier to describe these interactions, I will refer to the two parties as "Alice" and "Bob".&lt;/p&gt;
      &lt;p&gt;Alice and Bob start the connection by establishing some sort of bidirectional message stream. This may be a WebSocket, but Cap'n Web also allows applications to define their own transports. Each message in the stream is JSON-encoded, as described earlier.&lt;/p&gt;
      &lt;p&gt;Alice and Bob each maintain some state about the connection. In particular, each maintains an "export table", describing all the pass-by-reference objects they have exposed to the other side, and an "import table", describing the references they have received. Alice's exports correspond to Bob's imports, and vice versa. Each entry in the export table has a signed integer ID, which is used to reference it. You can think of these IDs like file descriptors in a POSIX system. Unlike file descriptors, though, IDs can be negative, and an ID is never reused over the lifetime of a connection.&lt;/p&gt;
      &lt;p&gt;At the start of the connection, Alice and Bob each populate their export tables with a single entry, numbered zero, representing their "main" interfaces. Typically, when one side is acting as the "server", they will export their main public RPC interface as ID zero, whereas the "client" will export an empty interface. However, this is up to the application: either side can export whatever they want.&lt;/p&gt;
      &lt;p&gt;From there, new exports are added in two ways:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;When Alice sends a message to Bob that contains within it an object or function reference, Alice adds the target object to her export table. IDs assigned in this case are always negative, starting from -1 and counting downwards.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Alice can send a "push" message to Bob to request that Bob add a value to his export table. The "push" message contains an expression which Bob evaluates, exporting the result. Usually, the expression describes a method call on one of Bob's existing exports â this is how an RPC is made. Each "push" is assigned a positive ID on the export table, starting from 1 and counting upwards. Since positive IDs are only assigned as a result of pushes, Alice can predict the ID of each push she makes, and can immediately use that ID in subsequent messages. This is how promise pipelining is achieved.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;After sending a push message, Alice can subsequently send a "pull" message, which tells Bob that once he is done evaluating the "push", he should proactively serialize the result and send it back to Alice, as a "resolve" (or "reject") message. However, this is optional: Alice may not actually care to receive the return value of an RPC, if Alice only wants to use it in promise pipelining. In fact, the Cap'n Web implementation will only send a "pull" message if the application has actually awaited the returned promise.&lt;/p&gt;
      &lt;p&gt;Putting it together, a code sequence like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = api.getMyName();
let result = await api.hello(namePromise);

console.log(result);&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Might produce a message exchange like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Call api.getByName(). `api` is the server's main export, so has export ID 0.
-&amp;gt; ["push", ["pipeline", 0, "getMyName", []]
// Call api.hello(namePromise). `namePromise` refers to the result of the first push,
// so has ID 1.
-&amp;gt; ["push", ["pipeline", 0, "hello", [["pipeline", 1]]]]
// Ask that the result of the second push be proactively serialized and returned.
-&amp;gt; ["pull", 2]
// Server responds.
&amp;lt;- ["resolve", 2, "Hello, Alice!"]&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;For more details about the protocol, check out the docs.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is new and still highly experimental. There may be bugs to shake out. But, we're already using it today. Cap'n Web is the basis of the recently-launched "remote bindings" feature in Wrangler, allowing a local test instance of workerd to speak RPC to services in production. We've also begun to experiment with it in various frontend applications â expect more blog posts on this in the future.&lt;/p&gt;
      &lt;p&gt;In any case, Cap'n Web is open source, and you can start using it in your own projects now.&lt;/p&gt;
      &lt;p&gt;Check it out on GitHub.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/capnweb-javascript-rpc-library/"/><published>2025-09-22T13:05:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45333978</id><title>What is algebraic about algebraic effects?</title><updated>2025-09-22T18:43:39.940047+00:00</updated><content>&lt;doc fingerprint="2c481533a51a53b2"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;What is Algebraic about Algebraic Effects?&lt;/head&gt;&lt;quote&gt;what does the word "algebraic" mean when used in the context of programming langs?&lt;lb/&gt;- a random tweet&lt;/quote&gt;&lt;p&gt;I'd wondered the same thing about "Algebraic Effects", and was excited to find a talk on YouTube titled What's Algebraic About Algebraic Effects and Handlers? Unfortunately, I'm not the target audience. As an engineer that doesn't shy away from math, it was still out of my depth.&lt;/p&gt;&lt;p&gt;I found some time this past spring looking into Algebraic Effects, and I think I have a decent answer to the question.&lt;/p&gt;&lt;head rend="h3"&gt;Algebra in the context of programming&lt;/head&gt;&lt;p&gt;My view of "Algebra" in the context of programming is a particular kind of compositionality, where there's a structure.&lt;/p&gt;&lt;p&gt;In contrast, mainstream developers often talk about compositionality as just two obj/function that can interoperate due to the same interface, but not much more can be inferred about properties of the interop between the two obj/functions.&lt;/p&gt;&lt;p&gt;So often times, we get some collection of objects/functions that go together in an arbitrary way according to the taste of the developer that wrote it. If they're any good, it feels intuitive. But more often than not, it feels arbitrary. The effect is magnified if you look into the codebase. To a newcomer, it feels like a mess, in the same way that a house built by piling stones high feels like a mess: there's no apparent or easily recognizable structure.&lt;/p&gt;&lt;head rend="h3"&gt;A tangential detour into abstract algebra&lt;/head&gt;&lt;p&gt;In abstract algebra, structure is often where you take some math object 𝛂 (like an int, or matrix), and you pair it with an operation, (like + or *), and you say: integers can be composed with op `+`, but we can ALSO infer properties in these combos--or laws.&lt;/p&gt;&lt;p&gt;So a common one we know is: integer (ℤ) with addition (+) has implied properties that always hold. And the elements (ℤ), the op (+), and the properties together constrain outcomes, and this is what gives us structure. A house with structure feels like it's built with arches, rather than a pile of rocks. What are the properties of (ℤ) and (+)? Due to how ℤ and + are defined, we get these properties:&lt;/p&gt;&lt;p&gt;1. Closure: ℤ + ℤ always gives you another ℤ.&lt;/p&gt;&lt;p&gt;Sometimes devs write code that doesn't give you back the same thing.&lt;/p&gt;&lt;p&gt;2. Associativity: (a + b) + c = a + (b + c) where a, b, c are in ℤ.&lt;/p&gt;&lt;p&gt;This familiar, as they were drilled in grade school. But often devs don't write code that fulfill this property.&lt;/p&gt;&lt;p&gt;The last two are:&lt;/p&gt;&lt;p&gt;3. identity: ℤ has an element that doesn't change when we use +. &lt;lb/&gt;Here, it's zero: a + 0 = a &lt;/p&gt;&lt;p&gt;4. inverse: every ℤ has a matching ℤ that give us the identity when we use + on it: a + (-a) = 0, where a and -a are in ℤ.&lt;/p&gt;&lt;p&gt;Taken together, math peeps gave this kind of structure a name: Groups. So if someone says [a struct] and [an op] together form a group, I can automatically can assume those properties. It's a shorthand.&lt;/p&gt;&lt;p&gt;If you add even more constraints/properties to how ℤ and + behave together, you get another algebraic structure. There's a whole host and families of these. So if we add another constraint, we get an Abelian Group:&lt;/p&gt;&lt;p&gt;5. Commutativity: a+b = b+a, where a, b are in ℤ&lt;/p&gt;&lt;head rend="h3"&gt;Surmounting the network with algebra&lt;/head&gt;&lt;p&gt;Why write constraining data structure and op pairings? It's quite useful if you want to guarantee specific properties of your system. For example, it's well known that syncing is hard, because of the Eight Fallacies of Distributed Systems.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The network is reliable;&lt;/item&gt;&lt;item&gt;Latency is zero;&lt;/item&gt;&lt;item&gt;Bandwidth is infinite;&lt;/item&gt;&lt;item&gt;The network is secure;&lt;/item&gt;&lt;item&gt;Topology doesn't change;&lt;/item&gt;&lt;item&gt;There is one administrator;&lt;/item&gt;&lt;item&gt;Transport cost is zero;&lt;/item&gt;&lt;item&gt;The network is homogeneous.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That means your data, when sent over the network will likely arrive out of order. Worse, clocks can be out of sync, so it can look like data arrived from the future. How can we tame the underlying unreliable system? By constraining our data and operations to have properties.&lt;/p&gt;&lt;p&gt;CRDTs are nowadays used to enforce eventually consistent syncs. It achieves this by pairing a data structure with a merge operation, which together form an algebraic structure called a semi-lattice. The properties of a semi-lattice are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Closure: For all a, b in the set S, the result of a ∘ b is also in S.&lt;/item&gt;&lt;item&gt;Associativity: a ∘ (b ∘ c)=(a ∘ b) ∘ c for all a, b, c ∈ S.&lt;/item&gt;&lt;item&gt;Commutativity: a ∘ b = b ∘ a for all a, b ∈ S.&lt;/item&gt;&lt;item&gt;Idempotence: a ∘ a = a for all a ∈ S.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Together, this is enough to counteract the network mixing up your data when sending it over the network. I wrote about that here:&lt;/p&gt;&lt;p&gt;So by constraining the power of what our code can do, we can ensure the system has specific desirable properties that achieve the goal of syncing data over an unreliable network. It's where we say: "If we compose this kind of data structure in this constrained way with this kind of merge function, then we can guarantee these properties always hold. And with this structure, our data can survive sync over an unreliable network with other syncers."&lt;/p&gt;&lt;head rend="h3"&gt;From Monads to Algebraic Effects&lt;/head&gt;&lt;p&gt;This is why people also like Monads. Monads are about how to compose code, but with specific properties (Monadic laws) so we can achieve some goal in how they compose. I won't go into it here, as this is already long, but that's the core idea.&lt;/p&gt;&lt;p&gt;However, not all types of Monads compose well together. Here's where I'm out of my depth, but I've read and I'm told that this is why there are Monad Transformers, so you can fit different domain Monads together.&lt;/p&gt;&lt;p&gt;Hence, some people have started looking at Algebraic Effects, as a way to achieve the same compositional powers of monads, but in a different way. Most descriptions of Algebraic Effects actually ignore the `algebraic` part, because describing `effects` is already a big leap.&lt;/p&gt;&lt;p&gt;The effects part, is often explained as "resumable exceptions". I wrote a short description of what algebraic effects are from that perspective, so I won't expound on that here.&lt;/p&gt;&lt;p&gt;But the algebraic part of algebraic effects is that the effects that you raise as a "resumable exception" can be composed together! Not just in any way: design them so when composed, they have *guaranteed properties* just like the stuff you saw above!&lt;/p&gt;&lt;p&gt;For example, if we had a key/value store that we interface with using &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;put&lt;/code&gt;, we could express what we expect to happen through some algebraic properties.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Idempotence of consecutive reads (get-get): get k; get k ≡ get x&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This says, two consecutive &lt;code&gt;gets&lt;/code&gt; is functionally equivalent to a single &lt;code&gt;get&lt;/code&gt;. This guarantees that &lt;code&gt;get&lt;/code&gt; is a pure observation: it doesn't consume or advance anything. If this law didn't hold, reading could "drain" or "advance" some hidden cursor. By making it a law, we make it an explicit behavior for our users, so they're not surprised by bugs down the line when their assumptions veer from this property.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Last write wins (put-put): put k v1; put k v2 ≡ put k v2&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Easy. The two &lt;code&gt;puts&lt;/code&gt; together is the functional equivalent of only executing the last one. Hence, the last &lt;code&gt;put&lt;/code&gt; is the value that's currently sitting in key &lt;code&gt;k&lt;/code&gt;. This encodes overwriting semantics, and without it, &lt;code&gt;put&lt;/code&gt; might append, merge, or accumulate. It wouldn't be what users would expect.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Read after write (put-get): put k v; get k ≡ put k v; return v&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Executing a &lt;code&gt;put&lt;/code&gt; and then an immediate &lt;code&gt;get&lt;/code&gt; is the functional equivalent of just executing the put, but then just returning the value &lt;code&gt;v&lt;/code&gt; you already have in hand, instead of executing &lt;code&gt;get&lt;/code&gt;. This is important to guarantee the consistency of reads right after writes. Without this, you could write &lt;code&gt;v&lt;/code&gt; and then not see &lt;code&gt;v&lt;/code&gt; immediately, which would break the intuitive model of state in a key/value store.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Write back same value (get-put): get k &amp;gt;&amp;gt;= (λv. put k v) ≡ return ()&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If you read the value of a key and then immediately write it back unchanged, that's functionally equivalent of doing nothing (returning unit).&lt;/p&gt;&lt;code&gt;&amp;gt;&amp;gt;=&lt;/code&gt; as saying "and then...". So rule 4 in javascript pseudocode might look like:&lt;p&gt;get(store, key).andThen((val) =&amp;gt; put(store, key, val))&lt;/p&gt;&lt;code&gt;return ()&lt;/code&gt;, &lt;code&gt;()&lt;/code&gt; is called &lt;code&gt;unit&lt;/code&gt;, which is the way functional programmers denote "no meaningful value", which is effectively what C programmers use &lt;code&gt;void&lt;/code&gt; for. They're technically different, but in practice, they're used for similar purposes.&lt;list rend="ol"&gt;&lt;item&gt;Independence across keys For &lt;code&gt;k1 ≠ k2&lt;/code&gt;:&lt;/item&gt;&lt;/list&gt;&lt;code&gt;put k1 v1; put k2 v2  ≡  put k2 v2; put k1 v1
get k1; get k2        ≡  get k2; get k1
put k1 v; get k2      ≡  get k2; put k1 v
&lt;/code&gt;&lt;p&gt;Operations on different keys commute, and the store treats each key as an independent cell. This is what makes it a key/value store, rather than some entangled data structure.&lt;/p&gt;&lt;p&gt;Hence, just because you are writing effects, doesn't automatically mean they're algebraic. You have to consciously design them to be so, in order to give properties or guarantees that you want your users to have. Most current programming languages have no way of enforcing these equational axioms, so even esoteric languages that feature algebraic effects don't even try to enforce them.&lt;/p&gt;&lt;p&gt;Languages which feature dependent types, such as Coq, Agda, Idris 2, and Lean are the only languages that can encode these equational axioms explicitly and be able to prove their veracity. Typically, these languages are used by mathematicians to do proofs in math. But interestingly, Lean has been getting a lot of momentum, and it can compile to C. It can be a practical in-road to using these in practice.&lt;/p&gt;&lt;p&gt;And, in my own words, that's what's algebraic about algebraic effects.&lt;/p&gt;&lt;head rend="h3"&gt;Epilogue&lt;/head&gt;&lt;p&gt;Alan Kay was known to lament that 1 million lines in a code base is unconscionable. It's no more a skyscraper than a pile of rocks. That's because there's often no structure. Eventually we figured out arches: they're structure that give strength with less material.&lt;/p&gt;&lt;p&gt;Hence, we can build higher without using more material. By analogy, we're starting to discover what this structure looks like in software. And it looks like math. There's a lot of resistance to this, and will be for a long time.&lt;/p&gt;&lt;p&gt;And maybe with LLMs, it might not matter for a wide swath of applications. But still, there's ever progress moving forward in this direction, where these pure functional programming or math-y ideas filter down to more mainstream languages.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://interjectedfuture.com/what-is-algebraic-about-algebraic-effects/"/><published>2025-09-22T14:30:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45334032</id><title>Dear GitHub: no YAML anchors, please</title><updated>2025-09-22T18:43:39.749108+00:00</updated><content>&lt;doc fingerprint="985c2511ef6202e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Sep 22, 2025 Tags: programming, rant&lt;/p&gt;
    &lt;p&gt;TL;DR: for a very long time, GitHub Actions lacked support for YAML anchors.&lt;/p&gt;
    &lt;p&gt;This was a good thing. YAML anchors in GitHub Actions are (1) redundant with existing functionality, (2) introduce a complication to the data model that makes CI/CD human and machine comprehension harder, and (3) are not even uniquely useful because GitHub has chosen not to support the one feature (merge keys) that lacks a semantic equivalent in GitHub Actions.&lt;/p&gt;
    &lt;p&gt;This step backwards reinforces GitHub Actionsâ status as an insecure by default CI/CD platform by making it harder for both humans and machines to analyze action and workflow definitions for vulnerabilities. GitHub should immediately remove support for YAML anchors, before adoption becomes widespread.&lt;/p&gt;
    &lt;p&gt;GitHub recently announced that YAML anchors are now supported in GitHub Actions. That means that users can write things like this:&lt;/p&gt;
    &lt;p&gt;On face value, this seems like a reasonable feature: the job and step abstractions in GitHub Actions lend themselves to duplication, and YAML anchors are one way to reduce that duplication.&lt;/p&gt;
    &lt;p&gt;Unfortunately, YAML anchors are a terrible tool for this job. Furthermore (as weâll see) GitHubâs implementation of YAML anchors is incomplete, precluding the actual small subset of use cases where YAML anchors are uniquely useful (but still not a good idea). Weâll see why below.&lt;/p&gt;
    &lt;p&gt;Pictured: the authorâs understanding of the GitHub Actions product roadmap.&lt;/p&gt;
    &lt;p&gt;The simplest reason why YAML anchors are a bad idea is because theyâre redundant with other more explicit mechanisms for reducing duplication in GitHub Actions.&lt;/p&gt;
    &lt;p&gt;GitHubâs own example above could be rewritten without YAML anchors as:&lt;/p&gt;
    &lt;p&gt;This version is significantly clearer, but has slightly different semantics: all jobs inherit the workflow-level &lt;code&gt;env&lt;/code&gt;. But this, in my opinion,
is a good thing: the need to template environment variables across a subset
of jobs suggests an architectural error in the workflow design.&lt;/p&gt;
    &lt;p&gt;In other words: if you find yourself wanting to use YAML anchors to share âglobalâ configuration between jobs or steps, you probably actually want separate workflows, or at least separate jobs with job-level &lt;code&gt;env&lt;/code&gt; blocks.&lt;/p&gt;
    &lt;p&gt;In summary: YAML anchors further muddy the abstractions of workflows, jobs, and steps, by introducing a cross-cutting form of global state that doesnât play by the rules of the rest of the system. This, to me, suggests that the current Actions team lacks a strong set of opinions about how GitHub Actions should be used, leading to a âkitchen sinkâ approach that serves all users equally poorly.&lt;/p&gt;
    &lt;p&gt;As noted above: YAML anchors introduce a new form of non-locality into GitHub Actions. Furthermore, this form of non-locality is fully general: any YAML node can be anchored and referenced. This is a bad idea for humans and machines alike:&lt;/p&gt;
    &lt;p&gt;For humans: a new form of non-locality makes it harder to preserve local understanding of what a workflow, job, or step does: a unit of work may now depend on any other unit of work in the same file, including one hundreds or thousands of lines away. This makes it harder to reason about the behavior of oneâs GitHub Actions without context switching.&lt;/p&gt;
    &lt;p&gt;It would only be fair to note that GitHub Actions already has some forms of non-locality: global contexts, scoping rules for &lt;code&gt;env&lt;/code&gt; blocks,
  &lt;code&gt;needs&lt;/code&gt; dependencies, step and job outputs, and so on. These can be
  difficult to debug! But what sets them apart is their lack of
  generality: each has precise semantics and scoping rules,
  meaning that a user who understands those rules can comprehend
  what a unit of work does without referencing the source of an
  environment variable, output, &amp;amp;c.&lt;/p&gt;
    &lt;p&gt;For machines: non-locality makes it significantly harder to write tools that analyze (or transform) GitHub Actions workflows.&lt;/p&gt;
    &lt;p&gt;The pain here boils down to the fact that YAML anchors diverge from the one-to-one object model1 that GitHub Actions otherwise maps onto.&lt;/p&gt;
    &lt;p&gt;With anchors, that mapping becomes one-to-many: the same element may appear once in the source, but multiple times in the loaded object representation.&lt;/p&gt;
    &lt;p&gt;In effect, this breaks a critical assumption that many tools make about YAML in GitHub Actions: that an entity in the deserialized object can be mapped back to a single concrete location in the source YAML.&lt;/p&gt;
    &lt;p&gt;This is needed to present reasonable source locations in error messages, but it doesnât hold if the object model doesnât represent anchors and references explicitly.&lt;/p&gt;
    &lt;p&gt;Furthermore, this is the reality for every YAML parser in wide use: all widespread YAML parsers choose (reasonably) to copy anchored values into each location where theyâre referenced, meaning that the analyzing tool cannot âseeâ the original element for source location purposes.&lt;/p&gt;
    &lt;p&gt;I feel these pains directly: I maintain zizmor as a static analysis tool for GitHub Actions, and &lt;code&gt;zizmor&lt;/code&gt; makes both of these assumptions.
  Moreover, &lt;code&gt;zizmor&lt;/code&gt;âs dependencies make these assumptions:
  &lt;code&gt;serde_yaml&lt;/code&gt; (like most other YAML parsers) chooses to deserialize YAML
  anchors by copying the anchored value into each location where itâs
  referenced2.&lt;/p&gt;
    &lt;p&gt;One of the few things that make YAML anchors uniquely useful is merge keys: a merge key allows a user to compose multiple referenced mappings together into a single mapping.&lt;/p&gt;
    &lt;p&gt;An example from the YAML spec, which I think tidily demonstrates both their use case and how incredibly confusing merge keys are:&lt;/p&gt;
    &lt;p&gt;I personally find this syntax incredibly hard to read, but at least it has a unique use case that could be useful in GitHub Actions: composing multiple sets of environment variables together with clear precedence rules is manifestly useful.&lt;/p&gt;
    &lt;p&gt;Except: GitHub Actions doesnât support merge keys! They appear to be using their own internal YAML parser that already had some degree of support for anchors and references, but not for merge keys.&lt;/p&gt;
    &lt;p&gt;To me, this takes the situation from a set of bad technical decisions (and lack of strong opinions around how GitHub Actions should be used) to farce: the one thing that makes YAML anchors uniquely useful in the context of GitHub Actions is the one thing that GitHub Actions doesnât support.&lt;/p&gt;
    &lt;p&gt;To summarize, I think YAML anchors in GitHub Actions are (1) redundant with existing functionality, (2) introduce a complication to the data model that makes CI/CD human and machine comprehension harder, and (3) are not even uniquely useful because GitHub has chosen not to support the one feature (merge keys) that lacks a semantic equivalent in GitHub Actions.&lt;/p&gt;
    &lt;p&gt;Of these reasons, I think (2) is the most important: GitHub Actions security has been in the newsÂ a great deal recently, with the overwhelming consensus being that itâs too easy to introduce vulnerabilities in (or expose otherwise latent vulnerabilities through) GitHub Actions workflow.&lt;/p&gt;
    &lt;p&gt;For this reason, we need GitHub Actions to be easy to analyze for humans and machine alike. In effect, this means that GitHub should be decreasing the complexity of GitHub Actions, not increasing it. YAML anchors are a step in the wrong direction for all of the reasons aforementioned.&lt;/p&gt;
    &lt;p&gt;Of course, Iâm not without self-interest here: I maintain a static analysis tool for GitHub Actions, and supporting YAML anchors is going to be an absolute royal pain in my ass3. But itâs not just me: tools like actionlint, claws, and poutine are all likely to struggle with supporting YAML anchors, as they fundamentally alter each toolâs relationship to GitHub Actionsâ assumed data model. As-is, this change blows a massive hole in the larger open source ecosystemâs ability to analyze GitHub Actions for correctness and security.&lt;/p&gt;
    &lt;p&gt;All told: I strongly believe that GitHub should immediately remove support for YAML anchors in GitHub Actions. The âgoodâ news is that they can probably do so with a bare minimum of user disruption, since support has only been public for a few days and adoption is (probably) still primarily at the single-use workflow layer and not the reusable action (or workflow) layer.&lt;/p&gt;
    &lt;p&gt;That object model is essentially the JSON object model, where all elements appear as literal components of their source representation and take a small subset of possible types (string, number, boolean, array, object, null).Â ↩&lt;/p&gt;
    &lt;p&gt;In other words: even though YAML itself is a superset of JSON, users donât want YAML-isms to leak through to the object model. Everybody wants the JSON object model, and that means no âanchorâ or âreferenceâ elements anywhere in a deserialized structure.Â ↩&lt;/p&gt;
    &lt;p&gt;To the point where Iâm not clear itâs actually worth supporting anchors to any meaningful extent, and instead immediately flagging them as an attempt at obfuscation.Â ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.yossarian.net/2025/09/22/dear-github-no-yaml-anchors"/><published>2025-09-22T14:34:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45334250</id><title>A simple way to measure knots has come unraveled</title><updated>2025-09-22T18:43:39.581715+00:00</updated><content>&lt;doc fingerprint="a77c8316c9202f75"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Simple Way To Measure Knots Has Come Unraveled&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In 1876, Peter Guthrie Tait set out to measure what he called the “beknottedness” of knots.&lt;/p&gt;
    &lt;p&gt;The Scottish mathematician, whose research laid the foundation for modern knot theory, was trying to find a way to tell knots apart — a notoriously difficult task. In math, a knot is a tangled piece of string with its ends glued together. Two knots are the same if you can twist and stretch one into the other without cutting the string. But it’s hard to tell if this is possible based solely on what the knots look like. A knot that seems really complicated and tangled, for instance, might actually be equivalent to a simple loop.&lt;/p&gt;
    &lt;p&gt;Tait had an idea for how to determine if two knots are different. First, lay a knot flat on a table and find a spot where the string crosses over itself. Cut the string, swap the positions of the strands, and glue everything back together. This is called a crossing change. If you do this enough times, you’ll be left with an unknotted circle. Tait’s beknottedness is the minimum number of crossing changes that this process requires. Today, it’s known as a knot’s “unknotting number.”&lt;/p&gt;
    &lt;p&gt;If two knots have different unknotting numbers, then they must be different. But Tait found that his unknotting numbers generated more questions than they answered.&lt;/p&gt;
    &lt;p&gt;“I have got so thoroughly on one groove,” he wrote in a letter to a friend, the scientist James Clerk Maxwell, “that I fear I may be missing or unduly exalting something which will appear excessively simple to anyone but myself.”&lt;/p&gt;
    &lt;p&gt;If Tait missed something, so did every mathematician who followed him. Over the past 150 years, many knot theorists have been baffled by the unknotting number. They know it can provide a powerful description of a knot. “It’s the most fundamental [measure] of all, arguably,” said Susan Hermiller of the University of Nebraska. But it’s often impossibly hard to compute a knot’s unknotting number, and it’s not always clear how that number corresponds to the knot’s complexity.&lt;/p&gt;
    &lt;p&gt;To untangle this mystery, mathematicians in the early 20th century devised a straightforward conjecture about how the unknotting number changes when you combine knots. If they could prove it, they would have a way to compute the unknotting number for any knot — giving mathematicians a simple, concrete way to measure knot complexity.&lt;/p&gt;
    &lt;p&gt;Researchers searched for nearly a century, finding little evidence either for or against the conjecture.&lt;/p&gt;
    &lt;p&gt;Then, in a paper posted in June, Hermiller and her longtime collaborator Mark Brittenham uncovered a pair of knots that, when combined, form a knot that is easier to untie than the conjecture predicts. In doing so, they disproved the conjecture — and used their counterexample to find infinitely many other pairs of knots that also disprove it.&lt;/p&gt;
    &lt;p&gt;“When the paper was posted, I gasped out loud,” said Allison Moore of Virginia Commonwealth University.&lt;/p&gt;
    &lt;p&gt;The result demonstrates that “the unknotting number is chaotic and unpredictable and really exciting to study,” she added. The paper is “like waving a flag that says, we don’t understand this.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Unknotting and the Great Unknown&lt;/head&gt;
    &lt;p&gt;The conjecture dates back to at least 1937, when the German mathematician Hilmar Wendt set out to understand what happens when you add knots together — that is, when you tie both of them with the same string before gluing the ends together. (Mathematicians call this combined knot the “connect sum.”) Wendt thought that the unknotting number of the resulting knot should always be the sum of the unknotting numbers of the two original knots.&lt;/p&gt;
    &lt;p&gt;His prediction, now known as the additivity conjecture, makes sense. Say you add the two knots above, whose unknotting numbers are known to be 2 and 3. That means that there’s a sequence of two crossing changes that unknots the lefthand side of the connect sum, and a sequence of three crossing changes that unknots the righthand side. If you use these sequences, you can unknot the whole thing in 2 + 3, or 5, crossing changes.&lt;/p&gt;
    &lt;p&gt;But this only tells you that the connect sum’s unknotting number is no bigger than 5. You might be able to find a sequence of crossing changes that’s more efficient than untying each side individually. That is, there might be a knot that really is less than the sum of its parts.&lt;/p&gt;
    &lt;p&gt;To settle the additivity conjecture, mathematicians had to either find a connect sum with a shorter unknotting sequence or prove that no such example exists. In either case, they didn’t have a clue where to begin.&lt;/p&gt;
    &lt;p&gt;Part of the problem was that the way you lay out your knot — what mathematicians call a “diagram” — determines where and how the knot crosses over itself. There are lots of diagrams that can represent the same knot. To find the shortest sequence of crossing changes, you might have to choose just the right diagram. Often, it’s not the one you’d normally associate with the knot.&lt;/p&gt;
    &lt;p&gt;“There are unimaginably large numbers of ways to try and imagine changing your diagram before you decide to introduce the crossing change,” Brittenham said. “We don’t, at least at the start, have any control over how complicated the picture has to look.”&lt;/p&gt;
    &lt;p&gt;In 1985, the mathematician Martin Scharlemann finally made some headway when he proved that for any two knots whose unknotting number is 1, the connect sum will always have an unknotting number of 2. “That made [the whole conjecture] seem much more likely,” said Charles Livingston of Indiana University.&lt;/p&gt;
    &lt;p&gt;The result offered tantalizing evidence that the universe of knots could be neatly organized. That’s because all knots can be built out of a smaller class of “prime” knots. The additivity conjecture implied that once you knew the unknotting numbers of those prime knots, you would know them for all knots. Any information you might want about a given knot would fall naturally out of that much simpler set.&lt;/p&gt;
    &lt;p&gt;Mathematicians wanted the conjecture to be true, said Arunima Ray of the University of Melbourne, “because that would be like, there’s order in the world.”&lt;/p&gt;
    &lt;p&gt;Scharlemann’s result was later extended to other classes of knots. But it wasn’t clear that it would apply to all knots.&lt;/p&gt;
    &lt;p&gt;Then Brittenham and Hermiller convened a cluster of computers to help.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sneakernet&lt;/head&gt;
    &lt;p&gt;The pair began their project a decade ago with a broader aim: to use computers to learn whatever they could about the unknotting number.&lt;/p&gt;
    &lt;p&gt;They turned to software known as SnapPy, which uses sophisticated geometric techniques to test whether two pictures depict the same knot. Just a few years earlier, SnapPy had vastly expanded its database, enabling it to identify nearly 60,000 unique knots.&lt;/p&gt;
    &lt;p&gt;It was perfectly suited for what Brittenham and Hermiller had in mind. They started with a single complicated knot and applied every imaginable crossing change to it, producing scores of new knots. They then used SnapPy to identify those knots — and repeated the process.&lt;/p&gt;
    &lt;p&gt;They did this for millions of knot diagrams that corresponded to hundreds of thousands of knots. Ultimately, they assembled an enormous library of information about unknotting sequences and calculated upper bounds on the unknotting numbers of thousands of knots. The work required a lot of computing power: The pair signed up for supercomputing time at the University of Nebraska’s computing center, while also running their program on old laptops they’d bought at an auction. All told, they were managing dozens of computers. “We had a bit of a sneakernet,” Brittenham said, “where you transfer information from computer to computer by walking between them.”&lt;/p&gt;
    &lt;p&gt;The duo kept their program running in the background for over a decade. During that time, a couple of computers from their ragtag collection succumbed to overheating and even flames. “There was one that actually sent out sparks,” Brittenham said. “That was kind of fun.” (Those machines, he added, were “honorably retired.”)&lt;/p&gt;
    &lt;p&gt;Then, in the fall of 2024, a paper about a failed attempt to use machine learning to disprove the additivity conjecture caught Brittenham and Hermiller’s attention. Perhaps, they thought, machine learning wasn’t the best approach for this particular problem: If a counterexample to the additivity conjecture was out there, it would be “a needle in a haystack,” Hermiller said. “That’s not quite what things like machine learning are about. They’re about trying to find patterns in things.”&lt;/p&gt;
    &lt;p&gt;But it reinforced a suspicion the pair already had — that maybe their more carefully honed sneakernet could find the needle.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Tie That Binds&lt;/head&gt;
    &lt;p&gt;Brittenham and Hermiller realized they could make use of the unknotting sequences they’d uncovered to look for potential counterexamples to the additivity conjecture.&lt;/p&gt;
    &lt;p&gt;Imagine again that you have two knots whose unknotting numbers are 2 and 3, and you’re trying to unknot their connect sum. After one crossing change, you get a new knot. If the additivity conjecture is to be believed, then the original knot’s unknotting number should be 5, and this new knot’s should be 4.&lt;/p&gt;
    &lt;p&gt;But what if this new knot’s unknotting number is already known to be 3? That implies that the original knot can be untied in just four steps, breaking the conjecture.&lt;/p&gt;
    &lt;p&gt;“We get these middle knots,” Brittenham said. “What can we learn from them?”&lt;/p&gt;
    &lt;p&gt;He and Hermiller already had the perfect tool for the occasion humming away on their suite of laptops: the database they’d spent the previous decade developing, with its upper bounds on the unknotting numbers of thousands of knots.&lt;/p&gt;
    &lt;p&gt;The mathematicians started to add pairs of knots and work through the unknotting sequences of their connect sums. They focused on connect sums whose unknotting numbers had only been approximated in the loosest sense, with a big gap between their highest and lowest possible values. But that still left them with a massive list of knots to work through — “definitely in the tens of millions, and probably in the hundreds of millions,” Brittenham said.&lt;/p&gt;
    &lt;p&gt;For months, their computer program applied crossing changes to these knots and compared the resulting knots to those in their database. One day in late spring, Brittenham checked the program’s output files, as he did most days, to see if anything interesting had turned up. To his great surprise, there was a line of text: “CONNECT SUM BROKEN.” It was a message he and Hermiller had coded into the program — but they’d never expected to actually see it.&lt;/p&gt;
    &lt;p&gt;Initially, they were doubtful of the result. “The very first thing that went through our heads was there was something wrong with our programming,” Brittenham said.&lt;/p&gt;
    &lt;p&gt;“We just dropped absolutely everything else,” Hermiller recalled. “All of life just went away. Eating, sleeping got annoying.”&lt;/p&gt;
    &lt;p&gt;But their program checked out. They even tied the knot it had identified in a rope, then worked through the unknotting procedure by hand, just to make sure.&lt;/p&gt;
    &lt;p&gt;Their counterexample was real.&lt;/p&gt;
    &lt;head rend="h2"&gt;Twisted Mysteries&lt;/head&gt;
    &lt;p&gt;The counterexample Brittenham and Hermiller found is built out of two copies of a knot called the (2, 7) torus knot. This knot is made by winding two strings around each other three and a half times and then gluing their opposing ends together. Its mirror image is made by winding three and a half times in the other direction.&lt;/p&gt;
    &lt;p&gt;The unknotting number of both the (2, 7) torus knot and its mirror image is 3. But Brittenham and Hermiller’s program found that if you add these knots, you can unknot the result in just five steps — not six, as the additivity conjecture predicted.&lt;/p&gt;
    &lt;p&gt;“It’s a shockingly simple counterexample,” Moore said. “It goes back to that unpredictability of the crossing change.”&lt;/p&gt;
    &lt;p&gt;The result led Brittenham and Hermiller to an infinite list of other counterexamples, including almost any knot that’s built by winding two strings and gluing.&lt;/p&gt;
    &lt;p&gt;Now, with the additivity conjecture decisively struck down, the knot theory community has a wide world to explore.&lt;/p&gt;
    &lt;p&gt;For some mathematicians, the new result brings disappointment. It reveals that there’s less structure in the world of knots than they had hoped for. The unknotting number is “not as well behaved as we would like,” Ray said. “That’s a bit sad.”&lt;/p&gt;
    &lt;p&gt;But from another perspective, that only makes the unknotting number more intriguing. “There’s just much more complexity and unknowns about knot theory than we knew there were a few months ago,” Livingston said.&lt;/p&gt;
    &lt;p&gt;The nature of that additional complexity isn’t clear yet. During their furious examination of their counterexample, Brittenham and Hermiller weren’t able to develop an intuition for why it broke the additivity conjecture when other knots didn’t. Understanding this could help mathematicians get a better handle on what makes some knots complex and others less so.&lt;/p&gt;
    &lt;p&gt;“I’m still stymied by this most basic question” about the unknotting number, Moore said. “That just lights the fire under you.”&lt;/p&gt;
    &lt;p&gt;Editor’s Note: Brittenham and Hermiller’s research was funded in part by the Simons Foundation, which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/a-simple-way-to-measure-knots-has-come-unraveled-20250922/"/><published>2025-09-22T14:49:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45334467</id><title>Anti-*: The Things We Do but Not All the Way</title><updated>2025-09-22T18:43:39.458849+00:00</updated><content>&lt;doc fingerprint="f67211540ae23690"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Anti-*: The Things We Do But Not All The Way&lt;/head&gt;
    &lt;p&gt;I was reading Chase McCoy’s article “Antibuildings” where he cites Wikipedia’s entry on the term “Antilibrary” which points to another entry about the Japanese concept of Tsundoku, all of which deal with this idea of things we do with intention but that never make it to fruition.&lt;/p&gt;
    &lt;p&gt;Antilibraries are the books we buy but never read.&lt;/p&gt;
    &lt;p&gt;Antibuildings the architect’s version of sketches and plans drafted but buildings never made.&lt;/p&gt;
    &lt;p&gt;It got me thinking about the stuff I’ve started with intention but never brought to fruition — my own anti-*’s.&lt;/p&gt;
    &lt;p&gt;To name a few:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Antidomains: the domains I bought and had big plans for, but they never progressed beyond being parked at my registrar. (Zach Leatherman recently made a list kinda like this, if you haven’t seen it.)&lt;/item&gt;
      &lt;item&gt;Antiwebsites: the sites I was gonna make, but never shipped.&lt;/item&gt;
      &lt;item&gt;Antilayers: the Photoshop, Sketch, or Figma designs I painstakingly crafted to the level of “completeness”, but then never began building with code.&lt;/item&gt;
      &lt;item&gt;Anticode: the changes I made that functioned to the level of being usable and shippable, but then I never could pull the trigger on ‘em.&lt;/item&gt;
      &lt;item&gt;Antiposts: (also known as “drafts”, lol) all those blog posts I poured time and energy into researching, writing, and editing, but never could take all the way to “published”.&lt;/item&gt;
      &lt;item&gt;Antitweets: all the Tweets/Toots/Skeets I meticulously crafted as witty comebacks or sarcastic quips, but then never posted (honestly, probably for the better).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And last, but certainly not least — in fact, probably grandest of them all:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Antitabs: all the browser tabs of articles, videos, recipes, and other good things I collected and was going to read, watch, bake, etc. but never did.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jim-nielsen.com/2025/my-antis/"/><published>2025-09-22T15:03:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45334545</id><title>PlanetScale for Postgres is now GA</title><updated>2025-09-22T18:43:39.297162+00:00</updated><content>&lt;doc fingerprint="af51e4d16e9161c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PlanetScale for Postgres is now GA&lt;/head&gt;
    &lt;p&gt;By Sam Lambert |&lt;/p&gt;
    &lt;p&gt;PlanetScale for Postgres is now generally available and out of private preview. To create a Postgres database, sign up or log in to your PlanetScale account, create a new database, and select Postgres. If you are looking to migrate from another Postgres provider to PlanetScale, you can use our migration guides to get started. Finally, if you have a large or complex migration, we can help you via our sales team at postgres@planetscale.com.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is PlanetScale for Postgres?&lt;/head&gt;
    &lt;p&gt;Our mission is simple: bring you the fastest and most reliable databases with the best developer experience. We have done this for 5 years now with our managed Vitess product, allowing companies like Cursor, Intercom, and Block to scale beyond previous limits.&lt;/p&gt;
    &lt;p&gt;We are so excited to bring this to Postgres. Our proprietary operator allows us to bring the maturity of PlanetScale and the performance of Metal to an even wider audience. We bring you the best of Postgres and the best of PlanetScale in one product.&lt;/p&gt;
    &lt;head rend="h2"&gt;Customers on PlanetScale for Postgres&lt;/head&gt;
    &lt;p&gt;Hundreds of companies already trust PlanetScale for Postgres to power their production workloads. We say this every time we launch something, but we prefer you hear about real-world usage straight from our customers. Read through some of their stories about their migration to PlanetScale for Postgres below.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Convex: Powered by PlanetScale&lt;/item&gt;
      &lt;item&gt;Supermemory just got faster on PlanetScale&lt;/item&gt;
      &lt;item&gt;Scaling RealâTime Discovery: Inside Layersâ PlanetScale Migration&lt;/item&gt;
      &lt;item&gt;Why We Migrated from Neon to PlanetScale&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Vitess for Postgres&lt;/head&gt;
    &lt;p&gt;Neki is our Postgres sharding solution. Built by the team behind Vitess combining the best of Vitess and Postgres. Neki is not a fork of Vitess. Vitessâ achievements are enabled by leveraging MySQLâs strengths and engineering around its weaknesses. To achieve Vitessâ power for Postgres we are architecting from first principles and building alongside design partners at scale. When we are ready we will release Neki as an open source project suitable for running the most demanding Postgres workloads. To sign up for the Neki waitlist visit neki.dev.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://planetscale.com/blog/planetscale-for-postgres-is-generally-available"/><published>2025-09-22T15:10:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45334599</id><title>A New Internet Business Model?</title><updated>2025-09-22T18:43:38.870864+00:00</updated><content>&lt;doc fingerprint="384899547786ab2d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Cloudflare launched 15 years ago this week. We like to celebrate our birthday by announcing new products and features that give back to the Internet, which weâll do a lot of this week. But, on this occasion, we've also been thinking about what's changed on the Internet over the last 15 years and what has not.&lt;/p&gt;
      &lt;p&gt;With some things there's been clear progress: when we launched in 2010 less than 10 percent of the Internet was encrypted, today well over 95 percent is encrypted. We're proud of the role we played in making that happen.&lt;/p&gt;
      &lt;p&gt;Some other areas have seen limited progress: IPv6 adoption has grown steadily but painfully slowly over the last 15 years, in spite of our efforts. That's a problem because as IPv4 addresses have become scarce and expensive itâs held back new entrants and driven up the costs of things like networking and cloud computing.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The Internetâs Business Model&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Still other things have remained remarkably consistent: the basic business model of the Internet has for the last 15 years been the same â create compelling content, find a way to be discovered, and then generate value from the resulting traffic. Whether that was through ads or subscriptions or selling things or just the ego of knowing that someone is consuming what you created, traffic generation has been the engine that powered the Internet we know today.&lt;/p&gt;
      &lt;p&gt;Make no mistake, the Internet has never been free. There's always been a reward system that transferred value from consumers to creators and, in doing so, filled the Internet with content. Had the Internet not had that reward system it wouldn't be nearly as vibrant as it is today.&lt;/p&gt;
      &lt;p&gt;A bit of a trivia aside: why did Cloudflare never build an ad blocker despite many requests? Because, as imperfect as they are, ads have been the only micropayment system that has worked at scale to encourage an open Internet while also compensating content creators for their work. Our mission is to help build a better Internet, and a core value is that weâre principled, so we werenât going to hamper the Internetâs fundamental business model.&lt;/p&gt;
      &lt;p&gt;But that same traffic-based reward system has also created many of the problems we lament about the current state of the Internet. Traffic has always been an imperfect proxy for value. Over the last 15 years we've watched more of the Internet driven by annoying clickbait or dangerous ragebait. Entire media organizations have built their businesses with a stated objective of writing headlines to generate the maximum cortisol response because that's what generates the maximum amount of traffic.&lt;/p&gt;
      &lt;p&gt;Over the years, Cloudflare has at times faced calls for us to intervene and control what content can be published online. As an infrastructure provider, we've never felt we were the right place for those editorial decisions to be made. But it wasn't because we didn't worry about the direction the traffic-incentivized Internet seemed to be headed. It always seemed like what fundamentally needed to change was not more content moderation at the infrastructure level but instead a healthier incentive system for content creation.&lt;/p&gt;
      &lt;p&gt;Today the conditions to bring about that change may be happening. In the last year, something core to the Internet weâve all known has changed. It's being driven by AI and it has an opportunity with some care and nurturing to help bring about what we think may be a much better Internet.&lt;/p&gt;
      &lt;p&gt;Whatâs the change? The primary discovery system of the Internet for the last 15 years has been Search Engines. They scraped the Internet's content, built an index, and then presented users with a treasure map which they followed generating traffic. Content creators were happy to let Search Engines scrape their content because there were a limited number of them, so the infrastructure costs were relatively low and, more importantly, because the Search Engines gave something to sites in the form of traffic âÂ the Internetâs historic currency âÂ sent back to sites.&lt;/p&gt;
      &lt;p&gt;Itâs already clear that the Internetâs discovery system for the next 15 years will be something different: Answer Engines. Unlike Search Engines which gave you a map where you hunted for what you were looking for, driving traffic in the process, Answer Engines just give you the answer without you having to click on anything. For 95 percent of users 95 percent of the time, that is a better user experience.&lt;/p&gt;
      &lt;p&gt;You donât have to look far to see this is changing rapidly before our eyes. ChatGPT, Anthropicâs Claude, and other AI startups arenât Search Engines â theyâre Answer Engines. Even Google, the search stalwart, is increasingly serving âAI Overviewsâ in place of 10 blue links. We can often look to sci-fi movies to have a glimpse into our most likely future. In them, the helpful intelligent robot character didnât answer questions with: âHere are some links you can click on to maybe find what youâre looking for.â Whether you like it or not, the future will increasingly be answers not searches.&lt;/p&gt;
      &lt;p&gt;In the short term, this is going to be extremely painful for some industries that are built based on monetizing traffic. It already is. While ecommerce and social applications haven't yet seen a significant drop in traffic as the world switches to Answer Engines, media companies have. Why the difference? Well, for the former, you still need to buy the thing the Answer Engine recommends and, for now, we still value talking with other humans.&lt;/p&gt;
      &lt;p&gt;But for media companies, if the Answer Engine gives you the summary of what youâre looking for in most cases you donât need to read the story. And the loss of traffic for media companies has already been dramatic. Itâs not just traditional media. Research groups at investment banks, industry analysts, major consulting firms âÂ theyâre all seeing major drops in people finding their content because we are increasingly getting answers not search treasure maps.&lt;/p&gt;
      &lt;p&gt;Some say these answer engines or agents are just acting on behalf of humans. Sure but so what? Without a change they will still kill content creatorsâ businesses. If you ask your agent to summarize twenty different news sources but never actually visit any of them youâre still undermining the business model of those news sources. Agents donât click on ads. And if those agents are allowed to aggregate information on behalf of multiple users itâs an even bigger problem because then subscription revenue is eliminated as well. Why subscribe to the Wall Street Journal or New York Times or Financial Times or Washington Post if my agent can free ride off some other user who does?&lt;/p&gt;
      &lt;p&gt;Unless you believe that content creators should work for free, or that they are somehow not needed anymore âÂ both of which are naive assumptions âÂ something needs to change. A visit from an agent isnât the same as a visit from a human and therefore should have different rules of the road. If nothing changes, the drop in human traffic to the media ecosystem writ large will kill the business model that has built the content-rich Internet we enjoy today.&lt;/p&gt;
      &lt;p&gt;We think thatâs an existential threat to one of humanityâs most important creations: the Internet.&lt;/p&gt;
      &lt;p&gt;But thereâs reason for optimism. Content is the fuel that powers every AI system and the companies that run those AI systems know ultimately they need to financially support the ecosystem. Because of that it seems potentially we're on the cusp of a new, better, and maybe healthier Internet business model. As content creators use tools like the ones provided by Cloudflare to restrict AI robots from taking their content without compensation, we're already seeing a market emerge and better deals being struck between AI and content companies.&lt;/p&gt;
      &lt;p&gt;What's most interesting is what content companies are getting the best deals. It's not the ragebait headline writers. It's not the news organizations writing yet another take on what's going on in politics. It's not the spammy content farms full of drivel. Instead, it's Reddit and other quirky corners that best remind us of the Internet of old. For those of you old enough, think back to the Internet not of the last 15 years but of the last 35. Weâve lost some of what made that early Internet great, but there are indications that we might finally have the incentives to bring more of it back.&lt;/p&gt;
      &lt;p&gt;It seems increasingly likely that in our future, AI-driven Internet âÂ assuming the AI companies are willing to step up, support the ecosystem, and pay for the content that is the most valuable to them â itâs the creative, local, unique, original content thatâll be worth the most. And, if youâre like us, the thing you as an Internet consumer are craving more of is creative, local, unique, original content. And, it turns out, having talked with many of them, thatâs the content that content creators are most excited to create.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;A New Internet Business Model&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;So how will the business model work? Well, for the first time in history, we have a pretty good mathematical representation of human knowledge. Sum up all the LLMs and that's what you get. It's not perfect, but it's pretty good. Inherently, the same mathematical model serves as a map for the gaps in human knowledge. Like a block of Swiss Cheese â there's a lot of cheese, but there's also a lot of holes.&lt;/p&gt;
      &lt;p&gt;Imagine a future business model of the Internet that doesn't reward traffic-generating ragebait but instead rewards those content creators that help fill in the holes in our collective metaphorical cheese. That will involve some portion of the subscription fees AI companies collect, and some portion of the revenue from the ads they'll inevitably serve, going back to content creators who most enrich the collective knowledge.&lt;/p&gt;
      &lt;p&gt;As a rough and simplistic sketch, think of it as some number of dollars per AI companyâs monthly active users going into a collective pool to be distributed out to content creators based on what most fills in the holes in the cheese.&lt;/p&gt;
      &lt;p&gt;You could imagine an AI company suggesting back to creators that they need more created about topics they may not have enough content about. Say, for example, the carrying capacity of unladened swallows because they know their subscribers of a certain age and proclivity are always looking for answers about that topic. The very pruning algorithms the AI companies use today form a roadmap for what content is worth enough to not be pruned but paid for.&lt;/p&gt;
      &lt;p&gt;While today the budget items that differentiate AI companies are how much they can afford to spend on GPUs and top talent, as those things inevitably become more and more commodities it seems likely what will differentiate the different AIs is their access to creative, local, unique, original content. And the math of their algorithms provides them a map of whatâs worth the most. While there are a lot of details to work out, those are the ingredients you need for a healthy market.&lt;/p&gt;
      &lt;p&gt;As we think about our role at Cloudflare in this developing market, it's not about protecting the status quo but instead helping catalyze a better business model for the future of Internet content creation. That means creating a level playing field. Ideally there should be lots of AI companies, large and small, and lots of content creators, large and small.&lt;/p&gt;
      &lt;p&gt;It canât be that a new entrant AI company is at a disadvantage to a legacy search engine because one has to pay for content but the other gets it for free. But itâs also critical to realize that the right solution to that current conundrum isnât that no one pays, itâs that, new or old, everyone who benefits from the ecosystem should contribute back to it based on their relative size.&lt;/p&gt;
      &lt;p&gt;It may seem impossibly idealistic today, but the good news is that based on the conversations weâve had weâre confident if a few market participants tip âÂ whether because they step up and do the right thing or are compelled â we will see the entire market tipping and becoming robust very quickly.&lt;/p&gt;
      &lt;p&gt;We can't do this alone and we have no plans to try to. Our mission is not to âbuild a better Internetâ but to âhelp build a better Internet.â The solutions developed to facilitate this market need to be open, collaborative, standardized, and shared across many organizations. Weâll take some encouraging steps in that direction with announcements on partnerships and collaborations this week. And weâre proud to be a leader in this space.&lt;/p&gt;
      &lt;p&gt;The Internet is an ecosystem and we, other infrastructure providers, along with most importantly both AI companies and content creators, will be critical in ensuring that ecosystem is healthy. Weâre excited to partner with those who are ready to step up and do their part to also help build a better Internet. It is possible.&lt;/p&gt;
      &lt;p&gt;And we're optimistic that if others can collaborate in supporting the ecosystem we may be at the cusp of a new golden age of the Internet. Our conversations with the leading AI companies nearly all acknowledge that they have a responsibility to give back to the ecosystem and compensate content creators. Confirming this, the largest publishers are reporting they're having much more constructive conversations about licensing their content to those AI companies. And, this week, we'll be announcing new tools to help even the smallest publishers take back control of who can use what they've created.&lt;/p&gt;
      &lt;p&gt;It may seem impossible. We think itâs a no-brainer. We're proud of what Cloudflare has accomplished over the last 15 years, but thereâs a lot left to do to live up to our mission. So, more than ever, it's clear: giddy up, because we're just getting started!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/cloudflare-2025-annual-founders-letter/"/><published>2025-09-22T15:14:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335129</id><title>Human-Oriented Markup Language</title><updated>2025-09-22T18:43:38.631239+00:00</updated><content>&lt;doc fingerprint="57f920aaa3d8d650"&gt;
  &lt;main&gt;
    &lt;code&gt;# A sample HUML document.
website::
  hostname: "huml.io"
  ports:: 80, 443 # Inline list.
  enabled: true
  factor: 3.14
  props:: mime_type: "text/html", encoding: "gzip" # Inline dict.
  tags:: # Multi-line list.
    - "markup"
    - "webpage"
    - "schema"

haikus::
  one: """
    A quiet language
    Lines fall into their places
    Nothing out of place
  """
&lt;/code&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HUML was primarily born out of the numerous frustrations with YAML, where one easy-to-miss, accidental indentation change can dangerously alter the semantics of a document.&lt;/item&gt;
      &lt;item&gt;Other popular markup languages such as TOML and HCL are configuration-oriented. NestedText is an interesting approach, but is too primitive to be suitable for wider use cases. JSON is universal, but lacks comments, does not have a strict form for consistent readability across contexts, and has bracket-matching and formatting woes which make human editing difficult.&lt;/item&gt;
      &lt;item&gt;Of these, YAML is the one that comes closest to indicating structure and hierarchy visually.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ultimately, a new markup language is a subjective endeavor (it was even in 2001, as evidenced by YAML's original name, Yet Another ...). HUML looks like YAML, but borrows characteristics from many existing languages with the primary focus on enforcing human readability and consistency across contexts.&lt;/p&gt;
    &lt;p&gt;Still, why YET another markup language? Why not?&lt;/p&gt;
    &lt;head rend="h2"&gt;Goals&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ensure human readability and editability above all else.&lt;/item&gt;
      &lt;item&gt;Enable visual comprehension of data structures and hierarchies.&lt;/item&gt;
      &lt;item&gt;Avoid footguns and ambiguities in syntax and data types.&lt;/item&gt;
      &lt;item&gt;Provide as few ways as possible—preferably one—of representing something.&lt;/item&gt;
      &lt;item&gt;Maintain strict formatting for uniformity, eliminating the need for formatters.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://huml.io/"/><published>2025-09-22T15:48:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335135</id><title>UK Millionaire exodus did not occur, study reveals</title><updated>2025-09-22T18:43:38.283363+00:00</updated><content>&lt;doc fingerprint="7ecd443b5b203edb"&gt;
  &lt;main&gt;
    &lt;p&gt;30 news pieces a day in 2024 on non-existent exodus&lt;/p&gt;
    &lt;p&gt;A millionaire exodus widely reported by news outlets around the world in 2024, and credited for the UK Labour government’s decision to weaken tax reforms, did not occur, the Tax Justice Network reveals.&lt;/p&gt;
    &lt;p&gt;The media reporting – consisting of over 10,900 news pieces across print, broadcast and online news in 2024 – was primarily based on a report published by Henley &amp;amp; Partners1, a firm that sells golden passports to the superrich and advises governments on setting up such schemes. The European Court of Justice recently ruled one such scheme, that of Malta, to be unlawful.2&lt;/p&gt;
    &lt;p&gt;The Tax Justice Network’s review – co-published with Patriotic Millionaires UK and Tax Justice UK – of the Henley report finds that the number of millionaires claimed by Henley &amp;amp; Partners to be leaving countries in “exodus” in 2024 represented near-0% of those countries’ millionaire populations.3 For example, the 9500 millionaires widely reported to be leaving the UK in 2024 represented 0.3% of the UK’s 3.06 million millionaires.4&lt;/p&gt;
    &lt;p&gt;Media reporting widely blamed the alleged millionaire exodus on tax policies in the same year that calls for a wealth tax on the superrich gained unprecedented momentum globally.5 The media reporting was equivalent to 30 news pieces a day on the non-existent millionaire exodus across 2024.&lt;/p&gt;
    &lt;p&gt;Reviewing the full period from 2013 to 2024 for which the Henley report presents estimates on millionaire migration, the Tax Justice Network finds that millionaire migration rates consistently stood at near-0% for every year.6 Academic studies consistently show that the tax responses of the wealthy involves minimal levels of migration.7&lt;/p&gt;
    &lt;p&gt;Henley’s estimates, when put into perspective, reveal a picture that is at complete odds with the report’s narrative and media coverage: millionaires are highly immobile, and nearly 100% of millionaires have not relocated to a new country since 2013, if Henley’s estimates are to be taken at face value.&lt;/p&gt;
    &lt;p&gt;Henley &amp;amp; Partners was accused in a 2018 UK Parliamentary inquiry of meddling in the elections of Caribbean nations in return for the exclusive rights to sell golden passports.8 Henley &amp;amp; Partners told The Guardian it “fundamentally rejects any allegation of wrongdoing”.9 A recent Financial Times article identified an EU-sanctioned Russian businessman with links to the Ukraine invasion who could more easily circumvent travel restrictions due to a Maltese golden passport Henley helped him acquire in the past.10 A spokesperson for Henley &amp;amp; Partners told the Financial Times that while she “could not comment on individual cases because of missing information and data protection… an individual ‘may pass all the stringent due diligence tests imposed, but still go on to engage in criminal activity.’”11&lt;/p&gt;
    &lt;p&gt;Golden passports are now illegal in the EU following a successful court challenge brought by the European Commission against Malta’s scheme, on which Henley &amp;amp; Partners had advised. The Commission said such schemes pose serious risks for money laundering, tax evasion and corruption.12 Henley &amp;amp; Partners told media in response that it was “disappointed” but that the decision “will only increase the demand for specialized advisors”.13&lt;/p&gt;
    &lt;p&gt;Findings behind Labour climbdown riddled with problems&lt;/p&gt;
    &lt;p&gt;The UK Labour government’s decision in January 2025 to weaken non-dom tax reform was widely reported to be a result of concerns about the Henley report’s findings.14&lt;/p&gt;
    &lt;p&gt;The Tax Justice Network’s review of the Henley report flags several issues with the report’s methodology as well as contradictions in Henley &amp;amp; Partner’s reporting, and particularly in its claims on the UK exodus.&lt;/p&gt;
    &lt;p&gt;Strikingly, the report’s methodology15 states that its estimates are primarily a measure of where millionaires say they work on social media and not of where they live or reside, meaning the report does not track actual, physical migration – contrary to the presenting of the estimates in the press.&lt;/p&gt;
    &lt;p&gt;Moreover, the report uses a far narrower definition of ‘millionaires’ that does not include all dollar millionaires like the standard definition (people with net worth of 1 million dollars or more), but rather only individuals with liquid assets worth 1 million dollars or more, who are thus richer and more mobile on average than a standardly defined millionaire.16 In the case of the UK, the ‘millionaires’ identified by the report represent just a fifth (20%) of the UK millionaire population.17 Even then, the report is based on a small sample from within these narrowly defined millionaires and the sample is skewed towards centi-millionaires and billionaires, who are also likely to be the most easily mobile.18&lt;/p&gt;
    &lt;p&gt;Just as striking, the use of the term “exodus” has been inconsistent in the analysis. In 2021, Henley described 2000 millionaires leaving the UK as “insignificant” but in 2023 described 1600 millionaires leaving the UK an “exodus”. In 2023, the 6500 millionaires claimed to be leaving India were described as “not particularly concerning” but redescribed in 2024 as a “wealth exodus”.19&lt;/p&gt;
    &lt;p&gt;The Tax Justice Network wrote to Henley &amp;amp; Partners and New World Wealth (who prepared the Henley report’s estimates) with questions for each ahead of the publication of its review. The response received said20:&lt;/p&gt;
    &lt;p&gt;“It seems this entire debate is over that one word. The dictionary definition is just ‘mass migration’, and HMRC’s own data shows that the number of non-doms in the UK is decreasing year on year – which seems to be a mass migration. If you are looking to the biblical definition, then to use the term ‘exodus’ would of course mean that all non-doms are leaving, but I don’t think many people take biblical interpretations quite so literally?”&lt;/p&gt;
    &lt;p&gt;Furthermore, Henley &amp;amp; Partners labelled the UK’s alleged exodus a “Brexodus” in 2023, claiming that the exodus was largely an impact of Brexit.21 In October 2024, Henley relabelled the exodus a “Wexit” in a press release framing the UK exodus as a reaction to tax hikes that might be announced in the UK Labour government’s upcoming budget statement.22&lt;/p&gt;
    &lt;p&gt;Henley &amp;amp; Partners’ specified in October 2024 that the “Wexit” is a “wealth exodus” that includes centi-millionaires and billionaires, and the report’s author emphasised that “[t]he large number of centi-millionaires leaving the UK is particularly concerning”.23 These claims appear inconsistent with Henley’s forecast made the month prior in September 2024 that the UK centi-millionaire population is growing and will continue to grow from 2024 to 2040.24&lt;/p&gt;
    &lt;p&gt;The press release highlighted that the UK Labour government’s budget statement as a main reason for this alleged “wealth exodus”:&lt;/p&gt;
    &lt;p&gt;“The UK’s high tax rates and concerns about additional tax hikes that could be announced at the end of the month in the Labour Party’s first budget in 14 years, are highlighted as being among the main reasons for the wealth exodus.”25&lt;/p&gt;
    &lt;p&gt;A response sent by Henley &amp;amp; Partners to the Tax Justice Network said:&lt;/p&gt;
    &lt;p&gt;“We have never claimed that Labour tax policies were the sole or root cause. If papers such as the Telegraph, Times, Mail, decide to add their own layer on to that, and deliberately exclude from their story our standard reminder to them that these were the Conservatives’ tax changes, then I think your argument is with them not with us.”&lt;/p&gt;
    &lt;p&gt;Moreover, it is unclear whether the forecast of centi-millionaires and billionaires leaving the UK that Henley reported in October 2024 was different from the forecast it initially made in June 2024 when the Labour party was not in power.&lt;/p&gt;
    &lt;p&gt;Media adds fuel to the fictional fire&lt;/p&gt;
    &lt;p&gt;The Tax Justice Network’s analysis of media coverage of the Henley report finds that coverage often went far beyond any claims made in the report itself, contributing to an entirely unfounded narrative about the role of tax and government policies in causing a millionaire exodus which itself did not occur.&lt;/p&gt;
    &lt;p&gt;Tax was mentioned in half of global media coverage of the exodus and far more often than any other exodus drivers discussed in the Henley report.&lt;/p&gt;
    &lt;p&gt;The UK Labour party, which was not in power when the report was published in June 2024, was mentioned more than twice as much as the UK Conservative party in global media coverage, and nearly four times as much as Brexit in UK media coverage.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Note: Percentages show the number of mentions as a share of all global media coverage &lt;/p&gt;
    &lt;p&gt;The picture is more skewed in UK media coverage, where tax was mentioned in 71% of coverage and Labour mentioned in 43% of coverage.26&lt;/p&gt;
    &lt;p&gt;Seven high-profile millionaires leaving the UK were mentioned nearly three times more often in global media coverage than pro-tax millionaire campaign groups representing hundreds of millionaires.27&lt;/p&gt;
    &lt;p&gt;In contrast to the media narrative, 81% of UK millionaires agree with the statement that it is patriotic to pay a fair share of tax, according to a poll published on 5 June 2025 by Patriotic Millionaires UK. 80% of UK millionaires said they would support a wealth tax of 2% on wealth over £10 million.28&lt;/p&gt;
    &lt;p&gt;The Tax Justice Network’s review of the Henley report raised a number of other questions, including the statistical credibility of drawing any conclusions from a self-admittedly unrepresentative sample; and the degree of extrapolation necessary to make any findings about smaller groups such as billionaires.&lt;/p&gt;
    &lt;p&gt;On the report’s sample, the response sent by Henley &amp;amp; Partners said:&lt;/p&gt;
    &lt;p&gt;“Statistically, if it is consistent year by year, then laws of statistical sampling mean that it can be used to draw a conclusion.”&lt;/p&gt;
    &lt;p&gt;The report is published by Henley &amp;amp; Partners but prepared by New World Wealth29, which describes itself as a “wealth intelligence firm” on its website. New World Wealth appears to have only one staff member and has not made the data behind its calculations public.&lt;/p&gt;
    &lt;p&gt;New World Wealth has been publishing estimates on millionaire migrations for at least a decade, and first began to publish its estimates with Henley &amp;amp; Partners in 2022, which was the first time the estimates on millionaire migrations underway were described as an “exodus”.&lt;/p&gt;
    &lt;p&gt;More specific questions about the sample sent to New World Wealth, including a question asking how many real persons in the sample were observed to have “migrated” in 2024, were not responded to.&lt;/p&gt;
    &lt;p&gt;Fariya Mohiuddin, Deputy Director: External Affairs at Tax Justice UK said:&lt;/p&gt;
    &lt;p&gt;“Taxing the super-rich to revitalise key services like the NHS and education, that we all rely on, is more urgent than ever. Taxing the wealth of the richest is simply not going to cause a mass exodus. This is scaremongering and statistical obfuscation by companies that represent the interests of billionaires and multi-millionaires. In fact, when the numbers are crunched properly, rather than using dodgy stats and figures, tax is an inconsequential factor in the decision-making of the vanishingly small percentage of millionaires that do decide to move. In fact, many wealthy people want to pay more tax. They know that when public services are well-funded, people are healthy, and the country works better, they will benefit – alongside everyone else.”&lt;/p&gt;
    &lt;p&gt;Member of Patriotic Millionaires UK and legal consultant, Stephen Kinsella said:&lt;/p&gt;
    &lt;p&gt;“As this excellent report from the Tax Justice Network shows, millionaires like me aren’t going anywhere. We want to build a better Britain so we’re proud to pay and here to stay. When nearly three quarters of UK millionaires think taxes should be raised on the richest to reduce the strain on everyone else, and 81% think it’s patriotic to pay their fair share in tax, what on earth is stopping our Government from doing their duty and taxing extreme wealth?”&lt;/p&gt;
    &lt;p&gt;Alex Cobham, chief executive at the Tax Justice Network, said:&lt;/p&gt;
    &lt;p&gt;“The majority of people want taxes on the superrich, the majority of millionaires are saying tax us, and practically all credible studies say you should do it. But what the media reported, and the government listened to, was a fictional millionaire ‘exodus’ based on questionable data published by a firm that helps the superrich buy their way out of the rules that apply to everybody else. Tax is our most powerful tool for creating more equal societies, but scare stories like these are used to talk down to people and to block positive change.”&lt;/p&gt;
    &lt;p&gt;“This is a wakeup call for media professionals and governments alike. Do your homework when it comes to tax. Treat the Henley report and any such claims about fleeing millionaires with extreme caution, and make sure your stories and your policy decisions are based on robust evidence.”&lt;/p&gt;
    &lt;p&gt;-ENDS-&lt;/p&gt;
    &lt;p&gt;Read our review of the Henley report&lt;/p&gt;
    &lt;p&gt;Notes to Editor&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The Henley Private Wealth Migration Report 2024 was published on 18 June 2024 by Henley &amp;amp; Partners. The report’s estimates are prepared by New World Wealth.&lt;/item&gt;
      &lt;item&gt;See this FT article for more information about the European Court of Justice’s ruling and Henley &amp;amp; Partners role in the Maltese scheme. See the press release from the Court here.&lt;/item&gt;
      &lt;item&gt;The Tax Justice Network’s review of the Henley report is available here.&lt;/item&gt;
      &lt;item&gt;Countries’ reported migrating millionaires represented less than 1% of their millionaires, and was closer to 0% for most countries, including the UK.&lt;lb/&gt;Source: The millionaire exodus myth, Tax Justice Network, June 2025&lt;/item&gt;
      &lt;item&gt;Some examples of media coverage:&lt;lb/&gt;– Reuters: Ultra-rich entrepreneurs threaten to desert Britain over tax&lt;lb/&gt;– Bloomberg UK: Britain’s Ultra-Rich Map Out Routes to Escape ‘Non-Dom’ Taxes After Labour Victory&lt;lb/&gt;– CNN: Millionaires fleeing Britain in their thousands&lt;lb/&gt;– The Telegraph: Britain to suffer world’s biggest exodus of millionaires as Labour takes power&lt;lb/&gt;– CNBC: Millionaires are abandoning the UK in droves, new research shows&lt;/item&gt;
      &lt;item&gt;The total number of millionaires reported on the Henley &amp;amp; Partners website to have migrated every year since 2013 to 2023 consistently represented around 0.2% of millionaires annually. Moreover, while the global millionaire population has grown since 2013, the millionaire migration rate, however small, is marginally lower now than it was in the middle of the previous decade – even after bouncing back from the enforced immobility of the pandemic years. The “unprecedented”, “record numbers” of migrating millionaires Henley reported in 2024 are proportionally smaller than the migration numbers reported for 2016, 2017 and 2018.&lt;lb/&gt;Source: The millionaire exodus myth, Tax Justice Network, June 2025&lt;/item&gt;
      &lt;item&gt;See the literature on migration surveyed in our report, Taxing extreme wealth: what countries around the world could gain from progressive wealth taxes (Alison Schulz &amp;amp; Miroslav Palanský), 2024, Tax Justice Network. Meanwhile, a London School of Economics study found that the vast majority of Britain’s extremely wealthy people would never leave the country for tax reasons, partly due to the stigma involved in doing so, and partly because they think lower-tax jurisdictions are “boring”.&lt;/item&gt;
      &lt;item&gt;More information about the inquiry in this FT article. Read The Guardian’s 2018 investigation here.&lt;/item&gt;
      &lt;item&gt;See Henley’s response to the Guardian here.&lt;/item&gt;
      &lt;item&gt;Read the FT’s investigation on Maltese golden passports sold to Russians here.&lt;/item&gt;
      &lt;item&gt;See note 10 for Henley’s response.&lt;/item&gt;
      &lt;item&gt;See note 2.&lt;/item&gt;
      &lt;item&gt;See Henley’s statement on the European Court of Justice’s ruling here.&lt;/item&gt;
      &lt;item&gt;Some examples of media reporting:&lt;lb/&gt;– Sky News: “Rachel Reeves is to water down her crackdown on the non-dom tax status after analysis showed it had prompted an exodus of millionaires.”&lt;lb/&gt;– CNBC: “The U.K. is to soften some planned changes to its controversial non-dom tax rule following concerns of a millionaire exodus, the Treasury has confirmed.”&lt;lb/&gt;– The Independent: “Reeves to water down tax raid on non-doms amid exodus of millionaires”&lt;lb/&gt;– The Times: “Rachel Reeves to relax non-dom tax rule amid millionaire exodus”&lt;/item&gt;
      &lt;item&gt;The Henley report’s methodology states: “The firm [New World Wealth] uses various public sources to check city locations, including LinkedIn and other business portals. Its stats are therefore mainly based on the work locations of the individuals.” The methodology is available at the bottom of this webpage.&lt;/item&gt;
      &lt;item&gt;See the Henley report’s methodology at the bottom of this webpage.&lt;/item&gt;
      &lt;item&gt;The Henley report’s author stated in a BBC interview that the group of UK ‘millionaires’ as defined in the report totalled 602,000, which is around one fifth of the UK’s millionaire population, which the UBS Global Wealth Report 2024 estimates to stand at 3.06 million millionaires.&lt;/item&gt;
      &lt;item&gt;The Henley report’s author Andrew Amolis acknowledged in an interview with BBC More or Less that the report’s sample is skewed. Presenter Tim Harford challenged Mr Amolis further (emphasis added):Andrew Amoils: Most of the database – I’d say between twenty and a hundred million dollars in assets, that would be the bulk of the database. So our data is skewed to the top end, so there will be the billionaires and the centimillionaires with over a hundred million, with some of the people lower down it’s more difficult to know if they are a high net worth.Tim Harford: But wait, aren’t these super-rich more easily able to skip off to Monaco or Dubai than your run of a mill dollar millionaire?AA: No, you’re right, 100%, that would be an issue. Though I would argue that the super-wealthy leaving is obviously the most important, because if you’ve got a banker at Goldman Sachs who’s making five hundred thousand pounds a year and they leave, that has very little impact whereas if somebody with over a hundred million who’s got a business leaves, the impact’s much greater.TH: Sure, but the headlines are not about a few people controlling a huge amount of money leaving, the headlines are about 9000 millionaires leaving. So the headlines imply that there is some kind of representative sample, and there’s some kind of reasonable extrapolation, but from based on what you’ve told me I don’t think we really can reasonably extrapolate, given the methods you’re describing.AA: Well how else would you do it? I mean, we’ve got a sample of 150000 high net worths globally, and we’re tracking them in terms of their movements. How else would you do it? How else would you work out where people are going, apart from the way we’re doing it?TH: Well I think if you don’t have a representative sample, you don’t have any basis to make the claim at all.AA: Well I would argue it is a representative sample. 150000 people, that’s a lot!TH: But you just told us it wasn’t representative. The sheer number of people doesn’t make it representative.&lt;/item&gt;
      &lt;item&gt;The term “exodus” has been used inconsistently, as this table shows.&lt;lb/&gt;Source: The millionaire exodus myth, Tax Justice Network, June 2025&lt;lb/&gt;Note: The table presents migration numbers for the three reports New World Wealth published with the AfrAsia bank from 2018 to 2020 and three reports it published with Henley &amp;amp; Partners from 2022 to 2024. New World Wealth’s calculation of migration as a percentage of millionaire population are provided in parentheses where available. New World Wealth provided the percentages in 2019 and 2020, and provided percentages for some countries in 2022. No percentages were provided in 2023 and 2024 that we could find.&lt;lb/&gt;*Retroactively called a “wealth exodus” in Henley’s 2024 press release.&lt;/item&gt;
      &lt;item&gt;The response the Tax Justice Network received is reproduced in full at the end of the Tax Justice Network’s review of the Henley report.&lt;/item&gt;
      &lt;item&gt;See Henley &amp;amp; Partners’ use of the term “Brexodus” in this article from the Henley 2023 report.&lt;/item&gt;
      &lt;item&gt;See Henley &amp;amp; Partner’s October 2024 press release using the term “Wexit” here. The press release was syndicated at least 400 times across the media landscape in large part due to the PR Newswire service. While Henley’s original press release did list in its notes to editor Brexit as possible driver of exodus, the notes to editor were cut off in the PR Newswire version of the press release that was widely syndicated. Nonetheless, Henley’s October 2024 press release did refer to Brexit in the body of the release, but when doing so contrasted Brexit as a positive phenomenon that is separate from the wealth exodus. The press release stated: “Based on data over the past nine months, the UK’s wealth exodus or WEXIT is expected to include 85 centi-millionaires and 10 billionaires, and in an ironic reversal of Brexit fortunes, 68% are heading for Europe, with favored destinations being Italy, Malta, Greece, Portugal, Switzerland, Monaco, Cyprus, France, Spain, and the Netherlands.”&lt;/item&gt;
      &lt;item&gt;Henley’s The Centi-Millionaire Report 2024 was published on 17 September 2024.&lt;/item&gt;
      &lt;item&gt;See note 22.&lt;/item&gt;
      &lt;item&gt;Looking specifically at UK media coverage, we find the mentions of themes and drivers to be far more skewed towards tax and Labour.&lt;lb/&gt;Source: The millionaire exodus myth, Tax Justice Network, June 2025&lt;/item&gt;
      &lt;item&gt;The seven high-profile millionaires reportedly moving away from the UK – Charlie Mullins, Christian Angermayer, Alan Howard, Nassef Sawiris, Asif Aziz and Bassim Haidar – were mentioned in 199 articles. In contrast, Patriotic Millionaires, Patriotic Millionaires UK, Millionaires for Humanity, Tax Me Now and Proud to Pay More – campaigning groups representing over 300 millionaires calling on governments to tax them more – were mentioned 73 times. The seven migrating millionaires were mentioned 2.7 times more than the pro-tax millionaires groups.&lt;/item&gt;
      &lt;item&gt;See Patriotic Millionaire UK’s polling here.&lt;/item&gt;
      &lt;item&gt;See New World Wealth’s website.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;About Patriotic Millionaires UK&lt;/p&gt;
    &lt;p&gt;Patriotic Millionaires UK is a nonpartisan network of British millionaires, from multiple industries and backgrounds from across the UK. It delivers a single mission – to leverage the voice of wealth to build a better Britain by changing the system to end extreme wealth and make those with it make their fair and proper contribution.&lt;/p&gt;
    &lt;p&gt;About Tax Justice UK&lt;/p&gt;
    &lt;p&gt;The UK’s approach to tax isn’t working. Our government fails to raise enough money to support high quality public services and wealth is desperately under-taxed. We campaign for a fairer tax system that takes more from the very wealthy. A tax system that actively redistributes wealth to tackle inequality; and that funds high quality public services. Our mission is to ensure that everyone in the UK benefits from a fair and effective tax system. Tax Justice UK is a partner of – but independent from – the Tax Justice Network.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://taxjustice.net/press/millionaire-exodus-did-not-occur-study-reveals/"/><published>2025-09-22T15:48:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335222</id><title>The American Nations regions across North America</title><updated>2025-09-22T18:43:38.116989+00:00</updated><content>&lt;doc fingerprint="ba70015e0ff515a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Earlier this summer, over at Nationhood Lab, we extended our data models to enable researchers to apply the American Nations model in Canada, This also let us create, for the first time, a master map of these regional cultures across North America.&lt;/p&gt;
    &lt;p&gt;The book the model is based on, American Nations, is a history of the entire continent north of the 25th parallel, including what’s now Canada and northern Mexico. Until now, I’d never had a proper map of what that looks like, facilitating research across borders. The map’s been popular with the public as well, with the post introducing it garnering unprecedented organic internet traffic.&lt;/p&gt;
    &lt;p&gt;Note Spanish Caribbean’s extension to the Dominican Republic and Puerto Rico. The region probably includes parts of the “Spanish Main” — the northern, Caribbean coast of South America — and maybe some other island locations as well, but, it being peripheral to our core study area in North America, I haven’t done the research into all that. First Nation, of course, includes Greenland, which is part of the Kingdom of Denmark.&lt;/p&gt;
    &lt;p&gt;Central and Southern Mexico likely belong to “post-Aztec” and “Maya” regional cultures, the latter extending into parts of Guatemala, Belize, and Honduras. Someday, hopefully in collaboration with regional experts, maybe there will be a South American Nations map as well.&lt;/p&gt;
    &lt;p&gt;Thanks to my counterparts at Motivf, John Liberty (who created the map) and Tova Pearlman (who helped wrangle some of the underlying data.) Enjoy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://colinwoodard.com/new-map-the-american-nations-regions-across-north-america/"/><published>2025-09-22T15:54:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335452</id><title>SWE-Bench Pro</title><updated>2025-09-22T18:43:38.012017+00:00</updated><content>&lt;doc fingerprint="d6175194c4399c4"&gt;
  &lt;main&gt;
    &lt;p&gt;Code and data for the following works:&lt;/p&gt;
    &lt;p&gt;SWE-Bench Pro is a challenging benchmark evaluating LLMs/Agents on long-horizon software engineering tasks. Given a codebase and an issue, a language model is tasked with generating a patch that resolves the described problem.&lt;/p&gt;
    &lt;p&gt;The dataset is inspired from SWE-Bench: https://github.com/SWE-bench/SWE-bench&lt;/p&gt;
    &lt;p&gt;To access SWE-bench Pro, copy and run the following code:&lt;/p&gt;
    &lt;code&gt;from datasets import load_dataset
swebench = load_dataset('ScaleAI/SWE-bench_Pro', split='test')&lt;/code&gt;
    &lt;p&gt;SWE-bench Pro uses Docker for reproducible evaluations. In addition, the evaluation script requires Modal to scale the evaluation set.&lt;/p&gt;
    &lt;p&gt;Follow the instructions in the Docker setup guide to install Docker on your machine. If you're setting up on Linux, we recommend seeing the post-installation steps as well.&lt;/p&gt;
    &lt;p&gt;Run the following commands to store modal credentials:&lt;/p&gt;
    &lt;code&gt;pip install modal
modalv setup # and follow the prompts to generate your token and secret
&lt;/code&gt;
    &lt;p&gt;After running these steps, you should be able to see a token ID and secret in &lt;code&gt;~/.modal.toml&lt;/code&gt;:
EG:&lt;/p&gt;
    &lt;code&gt;token_id = &amp;lt;token id&amp;gt;
token_secret = &amp;lt;token secret&amp;gt;
active = true
&lt;/code&gt;
    &lt;p&gt;We store prebuilt Docker images for each instance. They can be found in this directory:&lt;/p&gt;
    &lt;p&gt;https://hub.docker.com/repository/docker/jefzda/sweap-images/general&lt;/p&gt;
    &lt;p&gt;The format of the images is as follows.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;jefzda/sweap-images:{repo_base}.{repo_name}-{repo_base}__{repo_name}-{hash}&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;jefzda/sweap-images:gravitational.teleport-gravitational__teleport-82185f232ae8974258397e121b3bc2ed0c3729ed-v626ec2a48416b10a88641359a169d99e935ff03&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;First generate patch predictions using your harness of choice. Evaluate patch predictions on SWE-bench Pro with the following command:&lt;/p&gt;
    &lt;code&gt;python sweap_pro_eval_modal.py \
    --raw_sample_path=external_hf_v2.csv \
    --patch_path={OUTPUT}/gold_patches.json \
    --output_dir={OUTPUT}/ \
    --scripts_dir=run_scripts \
    --num_workers=100 \
    --dockerhub_username=your-username&lt;/code&gt;
    &lt;p&gt;Replace gold_patches with your patch json, and point raw_sample_path to the SWE-Bench Pro CSV.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/scaleapi/SWE-bench_Pro-os"/><published>2025-09-22T16:08:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335474</id><title>OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems</title><updated>2025-09-22T18:43:37.824360+00:00</updated><content>&lt;doc fingerprint="a9561c5e408ccdb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenAI and NVIDIA announce strategic partnership to deploy 10 gigawatts of NVIDIA systems&lt;/head&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strategic partnership enables OpenAI to build and deploy at least 10 gigawatts of AI datacenters with NVIDIA systems representing millions of GPUs for OpenAI’s next-generation AI infrastructure.&lt;/item&gt;
      &lt;item&gt;To support the partnership, NVIDIA intends to invest up to $100 billion in OpenAI progressively as each gigawatt is deployed.&lt;/item&gt;
      &lt;item&gt;The first gigawatt of NVIDIA systems will be deployed in the second half of 2026 on NVIDIA’s Vera Rubin platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;San Francisco and Santa Clara—September 22, 2025—NVIDIA and OpenAI today announced a letter of intent for a landmark strategic partnership to deploy at least 10 gigawatts of NVIDIA systems for OpenAI’s next-generation AI infrastructure to train and run its next generation of models on the path to deploying superintelligence. To support this deployment including datacenter and power capacity, NVIDIA intends to invest up to $100 billion in OpenAI as the new NVIDIA systems are deployed. The first phase is targeted to come online in the second half of 2026 using NVIDIA’s Vera Rubin platform.&lt;/p&gt;
    &lt;p&gt;“NVIDIA and OpenAI have pushed each other for a decade, from the first DGX supercomputer to the breakthrough of ChatGPT,” said Jensen Huang, founder and CEO of NVIDIA. “This investment and infrastructure partnership mark the next leap forward—deploying 10 gigawatts to power the next era of intelligence.”&lt;/p&gt;
    &lt;p&gt;“Everything starts with compute,” said Sam Altman, co-founder and CEO of OpenAI. “Compute infrastructure will be the basis for the economy of the future, and we will utilize what we’re building with NVIDIA to both create new AI breakthroughs and empower people and businesses with them at scale.”&lt;/p&gt;
    &lt;p&gt;“We’ve been working closely with NVIDIA since the early days of OpenAI,” said Greg Brockman, co-founder and President of OpenAI. “We’ve utilized their platform to create AI systems that hundreds of millions of people use every day. We’re excited to deploy 10 gigawatts of compute with NVIDIA to push back the frontier of intelligence and scale the benefits of this technology to everyone.”&lt;/p&gt;
    &lt;p&gt;OpenAI will work with NVIDIA as a preferred strategic compute and networking partner for its AI factory growth plans. OpenAI and NVIDIA will work together to co-optimize their roadmaps for OpenAI's model and infrastructure software and NVIDIA’s hardware and software.&lt;/p&gt;
    &lt;p&gt;This partnership complements the deep work OpenAI and NVIDIA are already doing with a broad network of collaborators, including Microsoft, Oracle, SoftBank, and Stargate partners, focused on building the world’s most advanced AI infrastructure.&lt;/p&gt;
    &lt;p&gt;OpenAI has grown to over 700 million weekly active users and strong adoption across global enterprises, small businesses, and developers. This partnership will help OpenAI advance its mission to build artificial general intelligence that benefits all of humanity.&lt;lb/&gt;NVIDIA and OpenAI look forward to finalizing the details of this new phase of strategic partnership in the coming weeks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/openai-nvidia-systems-partnership/"/><published>2025-09-22T16:10:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335635</id><title>Testing is better than Data Structures and Algorithms</title><updated>2025-09-22T18:43:37.742875+00:00</updated><content>&lt;doc fingerprint="bf6210976d050c95"&gt;
  &lt;main&gt;
    &lt;p&gt;People should spend less time learning DSA, more time learning testing.&lt;/p&gt;
    &lt;p&gt;I see new learners asking about “DSA” a lot. Data Structures and Algorithms are of course important: considered broadly, they are the two ingredients that make up all programs. But in my opinion, “DSA” as an abstract field of study is over-emphasized.&lt;/p&gt;
    &lt;p&gt;I understand why people focus on DSA: it’s a concrete thing to learn about, there are web sites devoted to testing you on it, and most importantly, because job interviews often involve DSA coding questions.&lt;/p&gt;
    &lt;p&gt;Before I get to other opinions, let me make clear that anything you can do to help you get a job is a good thing to do. If grinding leetcode will land you a position, then do it.&lt;/p&gt;
    &lt;p&gt;But I hope companies hiring entry-level engineers aren’t asking them to reverse linked lists or balance trees. Asking about techniques that can be memorized ahead of time won’t tell them anything about how well you can work. The stated purpose of those interviews is to see how well you can figure out solutions, in which case memorization will defeat the point.&lt;/p&gt;
    &lt;p&gt;The thing new learners don’t understand about DSA is that actual software engineering almost never involves implementing the kinds of algorithms that “DSA” teaches you. Sure, it can be helpful to work through some of these puzzles and see how they are solved, but writing real code just doesn’t involve writing that kind of code.&lt;/p&gt;
    &lt;p&gt;Here is what I think in-the-trenches software engineers should know about data structures and algorithms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data structures are ways to organize data. Learn some of the basics: linked list, array, hash table, tree. By “learn” I mean understand what it does and why you might want to use one.&lt;/item&gt;
      &lt;item&gt;Different data structures can be used to organize the same data in different ways. Learn some of the trade-offs between structures that are similar.&lt;/item&gt;
      &lt;item&gt;Algorithms are ways of manipulating data. I don’t mean named algorithms like Quicksort, but algorithms as any chunk of code that works on data and does something with it.&lt;/item&gt;
      &lt;item&gt;How you organize data affects what algorithms you can use to work with the data. Some data structures will be slow for some operations where another structure will be fast.&lt;/item&gt;
      &lt;item&gt;Algorithms have a “time complexity” (Big O): how the code slows as the data grows. Get a sense of what this means.&lt;/item&gt;
      &lt;item&gt;Python has a number of built-in data structures. Learn how they work, and the time complexity of their operations.&lt;/item&gt;
      &lt;item&gt;Learn how to think about your code to understand its time complexity.&lt;/item&gt;
      &lt;item&gt;Read a little about more esoteric things like Bloom filters, so you can find them later in the unlikely case you need them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some things you don’t need to learn:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The details of a dozen different sorting algorithms. Look at two to see different ways of approaching the same problem, then move on.&lt;/item&gt;
      &lt;item&gt;The names of “important” algorithms. Those have all been implemented for you.&lt;/item&gt;
      &lt;item&gt;The answers to all N problems on some quiz web site. You won’t be asked these exact questions, and they won’t come up in your real work. Again: try a few to get a feel for how some algorithms work. The exact answers are not what you need.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course some engineers need to implement hash tables, or sorting algorithms or whatever. We love those engineers: they write libraries we can use off the shelf so we don’t have to implement them ourselves.&lt;/p&gt;
    &lt;p&gt;There have been times when I implemented something that felt like An Algorithm (for example, Finding fuzzy floats), but it was more about considering another perspective on my data, looking at the time complexity, and moving operations around to avoid quadratic behavior. It wasn’t opening a textbook to find the famous algorithm that would solve my problem.&lt;/p&gt;
    &lt;p&gt;Again: if it will help you get a job, deep-study DSA. But don’t be disappointed when you don’t use it on the job.&lt;/p&gt;
    &lt;p&gt;If you want to prepare yourself for a career, and also stand out in job interviews, learn how to write tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This will be a skill you use constantly. Real-world software means writing tests much more than school teaches you to.&lt;/item&gt;
      &lt;item&gt;In a job search, testing experience will stand out more than DSA depth. It shows you’ve thought about what it takes to write high-quality software instead of just academic exercises.&lt;/item&gt;
      &lt;item&gt;It’s not obvious how to test code well. It’s a puzzle and a problem to solve. If you like figuring out solutions to tricky questions, focus on how to write code so that it can be tested, and how to test it.&lt;/item&gt;
      &lt;item&gt;Testing not only gives you more confidence in your code, it helps you write better code in the first place.&lt;/item&gt;
      &lt;item&gt;Testing applies everywhere, from tiny bits of code to entire architectures, assisting you in design and implementation at all scales.&lt;/item&gt;
      &lt;item&gt;If pursued diligently, testing is an engineering discipline in its own right, with a fascinating array of tools and techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less DSA, more testing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nedbatchelder.com/blog/202509/testing_is_better_than_dsa.html"/><published>2025-09-22T16:21:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335695</id><title>A board member's perspective of the RubyGems controversy</title><updated>2025-09-22T18:43:37.666967+00:00</updated><content/><link href="https://apiguy.substack.com/p/a-board-members-perspective-of-the"/><published>2025-09-22T16:25:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335774</id><title>California issues historic fine over lawyer's ChatGPT fabrications</title><updated>2025-09-22T18:43:37.568915+00:00</updated><content>&lt;doc fingerprint="d0399a088fc1a31c"&gt;
  &lt;main&gt;
    &lt;p&gt;In summary&lt;/p&gt;
    &lt;p&gt;The court of appeals said 21 of 23 quotes in an opening brief were fake. State authorities are scrambling to grapple with widespread use of artificial intelligence.&lt;/p&gt;
    &lt;p&gt;A California attorney must pay a $10,000 fine for filing a state court appeal full of fake quotations generated by the artificial intelligence tool ChatGPT.&lt;/p&gt;
    &lt;p&gt;The fine appears to be the largest issued over AI fabrications by a California court and came with a blistering opinion stating that 21 of 23 quotes from cases cited in the attorney’s opening brief were made up. It also noted that numerous out-of-state and federal courts have confronted attorneys for citing fake legal authority.&lt;/p&gt;
    &lt;p&gt;“We therefore publish this opinion as a warning,” it continued. “Simply stated, no brief, pleading, motion, or any other paper filed in any court should contain any citations— whether provided by generative AI or any other source—that the attorney responsible for submitting the pleading has not personally read and verified.”&lt;/p&gt;
    &lt;p&gt;The opinion, issued 10 days ago in California’s 2nd District Court of Appeal, is a clear example of why the state’s legal authorities are scrambling to regulate the use of AI in the judiciary. The state’s Judicial Council two weeks ago issued guidelines requiring judges and court staff to either ban generative AI or adopt a generative AI use policy by Dec. 15. Meanwhile, the California Bar Association is considering whether to strengthen its code of conduct to account for various forms of AI following a request by the California Supreme Court last month.&lt;/p&gt;
    &lt;p&gt;The Los Angeles-area attorney fined last week, Amir Mostafavi, told the court that he did not read text generated by the AI model before submitting the appeal in July 2023, months after OpenAI marketed ChatGPT as capable of passing the bar exam. A three-judge panel fined him for filing a frivolous appeal, violating court rules, citing fake cases, and wasting the court’s time and the taxpayers money, according to the opinion.&lt;/p&gt;
    &lt;p&gt;Mostafavi told CalMatters he wrote the appeal and then used ChatGPT to try and improve it. He said that he didn’t know it would add case citations or make things up.&lt;/p&gt;
    &lt;p&gt;He thinks it is unrealistic to expect lawyers to stop using AI. It’s become an important tool just as online databases largely replaced law libraries and, until AI systems stop hallucinating fake information, he suggests lawyers who use AI to proceed with caution.&lt;/p&gt;
    &lt;p&gt;“In the meantime we’re going to have some victims, we’re going to have some damages, we’re going to have some wreckages,” he said. “I hope this example will help others not fall into the hole. I’m paying the price.”&lt;/p&gt;
    &lt;p&gt;The fine issued to Mostafavi is the most costly penalty issued to an attorney by a California state court and one of the highest fines ever issued over attorney use of AI, according to Damien Charlotin, who teaches a class on AI and the law at a business school in Paris. He tracks instances of attorneys citing fake cases, primarily in Australia, Canada, the United States, and the United Kingdom.&lt;/p&gt;
    &lt;p&gt;In a widely-publicized case in May, a U.S. district court judge in California ordered two law firms to pay $31,100 in fees to defense counsel and the court for costs associated with using “bogus AI-generated research.” In that ruling, the judge described feeling misled, said they almost cited fake material in a judicial order and said “Strong deterrence is needed to make sure that attorneys don’t succumb to this easy shortcut.”&lt;/p&gt;
    &lt;p&gt;Charlotin thinks courts and the public should expect to see an exponential rise in these cases in the future. When he started tracking court filings involving AI and fake cases earlier this year, he encountered a few cases a month. Now he sees a few cases a day. Large language models confidently state falsehoods as facts, particularly when there are no supporting facts.&lt;/p&gt;
    &lt;p&gt;“The harder your legal argument is to make, the more the model will tend to hallucinate, because they will try to please you,” he said. “That’s where the confirmation bias kicks in.”&lt;/p&gt;
    &lt;p&gt;A May 2024 analysis by Stanford University’s RegLab found that although three out of four lawyers plan to use generative AI in their practice, some forms of AI generate hallucinations in one out of three queries. Detecting fake material cited in legal filings could get harder as models grow in size.&lt;/p&gt;
    &lt;p&gt;Another tracker of cases where lawyers cite nonexistent legal authority due to use of AI identifies 52 such cases in California and more than 600 nationwide. That amount is expected to increase in the near future because AI innovation is outpacing the education of attorneys, said Nicholas Sanctis, a law student at Capital University Law School in Ohio.&lt;/p&gt;
    &lt;p&gt;Jenny Wondracek, who leads the tracker project, said she expects this trend to get worse because she still regularly encounters lawyers who don’t know that AI makes things up or believe that legal tech tools can eliminate all fake or false material generated by language models.&lt;/p&gt;
    &lt;p&gt;“I think we’d see a reduction if (lawyers) just understood the basics of the technology,” she said.&lt;/p&gt;
    &lt;p&gt;Like Charlotin, she suspects there are more instances of made up cases generated by AI in state court filings than in federal courts, but a lack of standard filing methods makes it difficult to verify that. She said she encounters fake cases most often among overburdened attorneys or people who choose to represent themselves in family court.&lt;/p&gt;
    &lt;p&gt;She suspects the number of arguments filed by attorneys that use AI and cite fake cases will continue to go up, but added that not just attorneys engage in the practice. In recent weeks, she’s documented three instances of judges citing fake legal authority in their decisions.&lt;/p&gt;
    &lt;p&gt;As California considers how to treat generative AI and fake case citations, Wondracek said they can consider approaches taken by other states, such as temporary suspensions, requiring attorneys who get caught to take courses to better understand how to ethically use AI, or requiring them to teach law students how they can avoid making the same mistake.&lt;/p&gt;
    &lt;p&gt;Mark McKenna, codirector of the UCLA Institute of Technology, Law &amp;amp; Policy praised fines like the one against Mostafavi as punishing lawyers for “an abdication of your responsibility as a party representing someone.” He thinks the problem “will get worse before it gets better,” because there’s been a rush among law schools and private firms to adopt AI without thinking through the appropriate way to use them.&lt;/p&gt;
    &lt;p&gt;UCLA School of Law professor Andrew Selbst agrees, pointing out that clerks that work for judges are recent law school graduates, and students are getting bombarded with the message that they must use AI or get left behind. Educators and other professionals report feeling similar pressures.&lt;/p&gt;
    &lt;p&gt;“This is getting shoved down all our throats,” he said. “It’s being pushed in firms and schools and a lot of places and we have not yet grappled with the consequences of that.”&lt;/p&gt;
    &lt;p&gt;For the record: The fine issued to Mostafavi was for $10,000. Due to an editing error, an earlier version of this article had an incorrect figure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/"/><published>2025-09-22T16:30:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45336282</id><title>Mentra (YC W25) Is Hiring to build smart glasses</title><updated>2025-09-22T18:43:37.258962+00:00</updated><content>&lt;doc fingerprint="c1fe11d2919f8da2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;We’re building the OS for smart glasses because we believe glasses are the next personal computer.&lt;/p&gt;
      &lt;p&gt;This year we launched MentraOS, graduated Y Combinator, and raised an $8M seed round to bring smart glasses software and hardware to market.&lt;/p&gt;
      &lt;p&gt;We're a small team (~11 people) shipping thousands of our first pair of smart glasses in December.&lt;/p&gt;
      &lt;p&gt;We need help in engineering (build smart glasses), design (design glasses interfaces), and growth (make glasses go viral).&lt;/p&gt;
      &lt;p&gt;Apply on the job board or if you don't see a fitting role, email me cayden@mentra.glass&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45336282"/><published>2025-09-22T17:01:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45337253</id><title>AI-Generated "Workslop" Is Destroying Productivity</title><updated>2025-09-22T18:43:36.923020+00:00</updated><content>&lt;doc fingerprint="3d71033d39510a02"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Summary.&lt;/head&gt;
    &lt;p&gt;A confusing contradiction is unfolding in companies embracing generative AI tools: while workers are largely following mandates to embrace the technology, few are seeing it create real value. Consider, for instance, that the number of companies with fully AI-led processes nearly doubled last year, while AI use has likewise doubled at work since 2023. Yet a recent report from the MIT Media Lab found that 95% of organizations see no measurable return on their investment in these technologies. So much activity, so much enthusiasm, so little return. Why?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity"/><published>2025-09-22T18:07:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45337433</id><title>Diffusion Beats Autoregressive in Data-Constrained Settings</title><updated>2025-09-22T18:43:36.262628+00:00</updated><content>&lt;doc fingerprint="b4173bd8031d52cb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;TLDR:&lt;/p&gt;
      &lt;p&gt;If you are compute-constrained, use autoregressive models; if you are data-constrained, use diffusion models.&lt;/p&gt;
      &lt;p&gt;Motivation&lt;/p&gt;
      &lt;p&gt;Progress in AI over the past decade has largely been driven by scaling compute and data. The recipe from GPT-1 to GPT-5 has appeared straightforward: train a larger model on more data, and the result is a more capable system. &lt;/p&gt;
      &lt;p&gt;Yet a central question remains: will this recipe continue to hold from GPT-6 to GPT-N?&lt;/p&gt;
      &lt;p&gt;Many analysts and researchers believe the answer is no. For instance, Ilya Sutskever, in his NeurIPS 2024 Test-of-Time Award talk, remarked: “Compute is growing—better algorithms, better hardware, bigger clusters—but data is not growing. We have just one internet, the fossil fuel of AI.” &lt;/p&gt;
      &lt;p&gt;This concern is echoed by AI forecasters, who have analyzed compute and data growth more systematically and concluded that compute is outpacing data at an accelerating rate.&lt;/p&gt;
      &lt;p&gt;The above Figure, illustrates this tension by overlaying projections from EpochAI’s analysis. Their study extrapolates historical trends in compute, dataset usage, and internet-scale data availability. The forecast suggests that by around 2028, we will enter a data-constrained regime: far more compute will be available than there are training tokens to consume.&lt;/p&gt;
      &lt;p&gt;This paper addresses the challenge by asking: how can we trade off more compute for less data? Our central idea is to revisit the foundations of modern generative modeling and compare the two dominant paradigms for scaling AI.&lt;/p&gt;
      &lt;p&gt;Broadly, there have been two families of algorithms that shaped recent progress in AI:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Autoregressive models, popularized in 2019 in the text domain with the GPT-2 paper.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Diffusion models, popularized in 2020 in the vision domain with the DDPM paper.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Both aim to maximize the joint likelihood, but they differ fundamentally in how they factorize this joint distribution. &lt;/p&gt;
      &lt;p&gt;The success of diffusion in vision and autoregression in language has sparked both excitement and confusion—especially as each community has begun experimenting with the other’s paradigm.&lt;/p&gt;
      &lt;p&gt;For example, the language community has explored diffusion on text: &lt;/p&gt;
      &lt;p&gt;D3PM introduced discrete diffusion via random masking, while Diffusion-LM applied continuous diffusion by projecting tokens to embeddings before adding Gaussian noise. Since then, numerous works have extended this line of research.&lt;/p&gt;
      &lt;p&gt;Conversely, the vision community has experimented with doing autoregressive modeling on images. Models such as PARTI and DALLE exemplify this approach with strong results.&lt;/p&gt;
      &lt;p&gt;This cross-pollination has led to even greater uncertainty in robotics, where both diffusion-based and autoregressive approaches are widely adopted. To illustrate this, OpenAI Deep Research has compiled a list of robotics works across both paradigms, highlighting the lack of consensus in the field.&lt;/p&gt;
      &lt;p&gt;This ambiguity raises a fundamental question: should we be training diffusion models or autoregressive models?&lt;/p&gt;
      &lt;p&gt;Quick Background:&lt;/p&gt;
      &lt;p&gt;Autoregressive language models:&lt;/p&gt;
      &lt;p&gt;They model data distribution in a left-to-right manner&lt;/p&gt;
      &lt;p&gt;Diffusion language models:&lt;/p&gt;
      &lt;p&gt;For a more detailed understanding, with cool animations, please refer to this video from Jia-Bin Huang – https://www.youtube.com/watch?v=8BTOoc0yDVA&lt;/p&gt;
      &lt;p&gt;Prior results with Diffusion Language models&lt;/p&gt;
      &lt;p&gt;Since 2021, diffusion language models have sparked significant interest, with many works focusing on improving their design and performance.&lt;/p&gt;
      &lt;p&gt;In the table above, we highlight representative results from a popular work.&lt;lb/&gt;The takeaways are as follows:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Discrete diffusion performs better than continuous diffusion on text.&lt;/item&gt;
        &lt;item&gt;Autoregressive models still achieve the strongest results overall.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Several works have also explored the scaling behavior of diffusion-based language models.&lt;/p&gt;
      &lt;p&gt;Nie et al report that discrete diffusion LLMs require roughly 16× more compute than autoregressive LLMs to match the same negative log-likelihood. Similar results have been observed in multimodal domains—for instance, UniDisc finds that discrete diffusion needs about 12× more compute than autoregression for comparable likelihoods.&lt;/p&gt;
      &lt;p&gt;However, these results conflate data and compute because they are measured in a single-epoch training regime. This raises an important ambiguity: do diffusion models truly require 16× more compute, or do they in fact require 16× more data?&lt;/p&gt;
      &lt;p&gt;In this work, we explicitly disentangle data and compute. Our goal is to study diffusion and autoregressive models specifically in data-constrained settings.&lt;/p&gt;
      &lt;p&gt;Our Motivation&lt;/p&gt;
      &lt;p&gt;To understand why diffusion may behave differently, let’s revisit its training objective.&lt;/p&gt;
      &lt;p&gt;In diffusion training, tokens are randomly masked and the model learns to recover them. Importantly, left-to-right masking is a special case within this framework.&lt;/p&gt;
      &lt;p&gt;Viewed this way, diffusion can be interpreted as a form of implicit data augmentation for autoregressive training. Instead of only learning from left-to-right sequences, the model also benefits from many alternative masking strategies.&lt;/p&gt;
      &lt;p&gt;And if diffusion is essentially data augmentation, then its benefits should be most pronounced when training is data-bottlenecked.&lt;/p&gt;
      &lt;p&gt;This perspective explains why prior works have reported weaker results for diffusion: they primarily evaluated in single-epoch settings, where data is abundant. In contrast, our study focuses on scenarios where data is limited and compute can be traded off more effectively.&lt;/p&gt;
      &lt;p&gt;Our Experiments&lt;/p&gt;
      &lt;p&gt;In this work, we train hundreds of models spanning multiple orders of magnitude in model size, data quantity, and number of training epochs to fit scaling laws for diffusion models in the data-constrained setting. We summarize some of our key findings below.&lt;/p&gt;
      &lt;p&gt;Finding #1:&lt;/p&gt;
      &lt;p&gt;Diffusion models outperform autoregressive models when trained with sufficient compute (i.e., more epochs &amp;amp; parameters). Across different unique data scales, we observe:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;At low compute, Autoregressive models win.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;After a certain amount of compute, performance matches—we call this the critical compute point.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Beyond this, diffusion keeps improving, while Autoregressive plateaus or overfits. &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Each point in the figure shows a model trained to convergence. The x-axis shows the total training FLOPs of that point, and the y-axis shows the best validation loss achieved by that model family under that training compute budget.&lt;/p&gt;
      &lt;p&gt;Finding #2:&lt;/p&gt;
      &lt;p&gt;Autoregressive models begin to overfit much quickly, while diffusion shows no signs of overfitting even after 10x the number of epochs. In the above figure, we showed that increasing compute eventually favors diffusion. But compute can be scaled in two ways: (i) Increasing model size (ii) Increasing the number of epochs In the following plot, we separate these axes.&lt;/p&gt;
      &lt;p&gt;The colored star marks the 1-epoch point, where Autoregressive outperforms diffusion. The star (★) denotes the best loss achieved by each model.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Autoregressive hits its best around the middle, then overfits.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Diffusion keeps improving and reaches its best loss at the far right. &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Not only does diffusion benefit from more training—it also achieves a better final loss than Autoregressive (3.51 vs. 3.71).&lt;/p&gt;
      &lt;p&gt;Finding #3:&lt;/p&gt;
      &lt;p&gt;Diffusion models are significantly more robust to data repetition than autoregressive (AR) models. &lt;/p&gt;
      &lt;p&gt;We show training curves of models trained with the same total compute, but different trade-offs between unique data and number of epochs. &lt;/p&gt;
      &lt;p&gt;An “epoch” here means reusing a smaller subset of data more times(e.g., 4 Ep is 4 epochs while using 25% unique data, 2 Ep is 2 epochs with 50% and so on).&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;AR models begin to overfit as repetition increases—their validation loss worsens and significantly diverges at higher epoch counts.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Diffusion models remain stable across all repetition levels, showing no signs of overfitting or diverging—even at 100 epochs.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Finding #4:&lt;/p&gt;
      &lt;p&gt;Diffusion models exhibit a much higher half-life of data reuse (R_D*) —i.e., the number of epochs after which returns from repeating data begins to significantly diminish. &lt;/p&gt;
      &lt;p&gt;We adopt the data-constrained scaling framework introduced by Muennighoff et al. in their excellent NeurIPS paper to fit scaling laws for diffusion models. While Muennighoff et al. found R_D* ~ 15 for autoregressive models, we find a significantly higher value of R_D* ~ 500 for diffusion models—highlighting their ability to benefit from far more data repetition.&lt;/p&gt;
      &lt;p&gt;The above Figure studies the Decay rate of data value under repetition: left shows diffusion, middle AR, and right the average decay rate for both. &lt;/p&gt;
      &lt;p&gt;Points are empirical results (darker color = higher FLOPs, lighter color =&lt;lb/&gt;lower FLOPs; each line = fixed compute), we find that fitted curves (represented as lines) closely match the empirical points, indicating our scaling laws are representative. The decay rate of value for repeated data is lower for diffusion, reflecting its greater robustness to repeating. In this experiment 100% data fraction means training 1 epoch with 100% unique data, while 50% means 2 epoch epoch with only using 50% unique data and so on.&lt;/p&gt;
      &lt;p&gt;Finding #5:&lt;/p&gt;
      &lt;p&gt;Muennighoff et al. showed that repeating the dataset up to 4 epochs is nearly as effective as using fresh data for autoregressive models.&lt;/p&gt;
      &lt;p&gt; In contrast, we find that diffusion models can be trained on repeated data for up to 100 epochs, while having repeated data almost as effective as fresh data.&lt;/p&gt;
      &lt;p&gt;Finding #6:&lt;/p&gt;
      &lt;p&gt;The compute required for diffusion to outperform AR follows a predictable power law. Above we defined the critical compute threshold as the amount of FLOPs where diffusion matches AR performance for a given unique dataset size. &lt;/p&gt;
      &lt;p&gt;We find that we can derive a simple closed-form analytical expression for this threshold, this allows us to predict when diffusion will surpass AR given any unique data size. In the figure we show both the fitted curve and empirical critical threshold points, which align closely.&lt;/p&gt;
      &lt;p&gt;Finding #7:&lt;/p&gt;
      &lt;p&gt;The data efficiency of diffusion models translates to better downstream performance.&lt;/p&gt;
      &lt;p&gt; Lastly we evaluate the best-performing diffusion and AR models (trained under the same data budget) on a range of language understanding tasks. &lt;/p&gt;
      &lt;p&gt;Across most benchmarks, diffusion models outperform AR models, confirming that diffusion’s lower validation loss translates to better downstream performance.&lt;/p&gt;
      &lt;p&gt;Finding #8:&lt;/p&gt;
      &lt;p&gt;Exposure to different token orderings helps explain diffusion’s data efficiency. By adding explicit data augmentations to AR training, we find that diffusion model’s advantage arises from their exposure to a diverse set of token orderings. &lt;/p&gt;
      &lt;p&gt;As seen in the above Figure, increasing N consistently lowered validation loss and delayed overfitting. At N = 16, the 100-epoch validation loss of AR models approached that of diffusion, suggesting that diverse orderings are indeed a key driver of diffusion’s data efficiency. These results support our interpretation that diffusion models outperform AR models in low-data regimes because they are implicitly trained on a richer distribution of conditional prediction tasks. &lt;/p&gt;
      &lt;p&gt;Finally, this analysis suggests a natural continuum between the two paradigms: by controlling task diversity through masking or reordering—we could design hybrid models that interpolate between compute efficiency (AR-like) and data efficiency (diffusion-like).&lt;/p&gt;
      &lt;p&gt;For more experiments and details please refer to original paper –https://arxiv.org/abs/2507.15857&lt;/p&gt;
      &lt;p&gt;Conclusion&lt;/p&gt;
      &lt;p&gt;As the availability of high-quality data plateaus, improving data efficiency becomes essential for scaling deep learning. In this work, we show that masked diffusion models consistently outperform autoregressive (AR) models in data-constrained regimes — when training involves repeated passes over a limited dataset. We establish new scaling laws for diffusion models, revealing their ability to extract value from repeated data far beyond what AR models can achieve.&lt;/p&gt;
      &lt;p&gt; These results challenge the conventional belief that AR models are universally superior and highlight diffusion models as a compelling alternative when data—not compute—is the primary bottleneck. Looking ahead, efficient use of finite data may define the next frontier in scaling deep learning models. Although the studies have been performed in the context of language models, we believe these findings should apply across any kind of sequence modeling data, such as in robotics or healthcare. For practitioners, our takeaway is simple: if you are compute-constrained, use autoregressive models; if you are data-constrained, use diffusion models.&lt;/p&gt;
      &lt;p&gt;Bibtex:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;@article{prabhudesai2025diffusion,&lt;lb/&gt;title={Diffusion Beats Autoregressive in Data-Constrained Settings},&lt;lb/&gt;author={Prabhudesai, Mihir and Wu, Mengning and Zadeh, Amir and Fragkiadaki, Katerina and Pathak, Deepak},&lt;lb/&gt;journal={arXiv preprint arXiv:2507.15857},&lt;lb/&gt;year={2025}&lt;lb/&gt;}&lt;/code&gt;
      &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.ml.cmu.edu/2025/09/22/diffusion-beats-autoregressive-in-data-constrained-settings/"/><published>2025-09-22T18:21:29+00:00</published></entry></feed>