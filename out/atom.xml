<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-05T18:12:14.659612+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45478780</id><title>Ambigr.am</title><updated>2025-10-05T18:12:22.629686+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ambigr.am/hall-of-fame"/><published>2025-10-05T04:11:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45479006</id><title>Managing context on the Claude Developer Platform</title><updated>2025-10-05T18:12:22.436876+00:00</updated><content>&lt;doc fingerprint="bf43ca3f38eb8046"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Managing context on the Claude Developer Platform&lt;/head&gt;
    &lt;p&gt;Today, we’re introducing new capabilities for managing your agents’ context on the Claude Developer Platform: context editing and the memory tool.&lt;/p&gt;
    &lt;p&gt;With our latest model, Claude Sonnet 4.5, these capabilities enable developers to build AI agents capable of handling long-running tasks at higher performance and without hitting context limits or losing critical information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context windows have limits, but real work doesn’t&lt;/head&gt;
    &lt;p&gt;As production agents handle more complex tasks and generate more tool results, they often exhaust their effective context windows—leaving developers stuck choosing between cutting agent transcripts or degrading performance. Context management solves this in two ways, helping developers ensure only relevant data stays in context and valuable insights get preserved across sessions.&lt;/p&gt;
    &lt;p&gt;Context editing automatically clears stale tool calls and results from within the context window when approaching token limits. As your agent executes tasks and accumulates tool results, context editing removes stale content while preserving the conversation flow, effectively extending how long agents can run without manual intervention. This also increases the effective model performance as Claude focuses only on relevant context.&lt;/p&gt;
    &lt;p&gt;The memory tool enables Claude to store and consult information outside the context window through a file-based system. Claude can create, read, update, and delete files in a dedicated memory directory stored in your infrastructure that persists across conversations. This allows agents to build up knowledge bases over time, maintain project state across sessions, and reference previous learnings without having to keep everything in context.&lt;/p&gt;
    &lt;p&gt;The memory tool operates entirely client-side through tool calls. Developers manage the storage backend, giving them complete control over where the data is stored and how it’s persisted.&lt;/p&gt;
    &lt;p&gt;Claude Sonnet 4.5 enhances both capabilities with built-in context awareness—tracking available tokens throughout conversations to manage context more effectively.&lt;/p&gt;
    &lt;p&gt;Together, these updates create a system that improves agent performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable longer conversations by automatically removing stale tool results from context&lt;/item&gt;
      &lt;item&gt;Boost accuracy by saving critical information to memory—and bring that learning across successive agentic sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Building long-running agents&lt;/head&gt;
    &lt;p&gt;Claude Sonnet 4.5 is the best model in the world for building agents. These features unlock new possibilities for long-running agents—processing entire codebases, analyzing hundreds of documents, or maintaining extensive tool interaction histories. Context management builds on this foundation, ensuring agents can leverage this expanded capacity efficiently while still handling workflows that extend beyond any fixed limit. Use cases include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coding: Context editing clears old file reads and test results while memory preserves debugging insights and architectural decisions, enabling agents to work on large codebases without losing progress.&lt;/item&gt;
      &lt;item&gt;Research: Memory stores key findings while context editing removes old search results, building knowledge bases that improve performance over time.&lt;/item&gt;
      &lt;item&gt;Data processing: Agents store intermediate results in memory while context editing clears raw data, handling workflows that would otherwise exceed token limits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance improvements with context management&lt;/head&gt;
    &lt;p&gt;On an internal evaluation set for agentic search, we tested how context management improves agent performance on complex, multi-step tasks. The results demonstrate significant gains: combining the memory tool with context editing improved performance by 39% over baseline. Context editing alone delivered a 29% improvement.&lt;/p&gt;
    &lt;p&gt;In a 100-turn web search evaluation, context editing enabled agents to complete workflows that would otherwise fail due to context exhaustion—while reducing token consumption by 84%.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;These capabilities are available today in public beta on the Claude Developer Platform, natively and in Amazon Bedrock and Google Cloud’s Vertex AI. Explore the documentation for context editing and the memory tool, or visit our cookbook to learn more.&lt;/p&gt;
    &lt;p&gt;Anthropic is not affiliated with, endorsed by, or sponsored by CATAN GmbH or CATAN Studio. The CATAN trademark and game are the property of CATAN GmbH.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/context-management"/><published>2025-10-05T05:20:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45479165</id><title>Social Cooling (2017)</title><updated>2025-10-05T18:12:22.099687+00:00</updated><content>&lt;doc fingerprint="3ab8db3d68f3a42d"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;LIKE OIL LEADS TO GLOBAL WARMING...&lt;/head&gt;
    &lt;head rend="h2"&gt;DATA LEADS TO SOCIAL COOLING&lt;/head&gt;
    &lt;head rend="h2"&gt;If you feel you are being watched, you change your behavior.&lt;/head&gt;
    &lt;head rend="h2"&gt;Big Data is supercharging this effect.&lt;/head&gt;
    &lt;head rend="h2"&gt;This could limit your desire to take risks or exercise free speech.&lt;/head&gt;
    &lt;head rend="h2"&gt;Over the long term these 'chilling effects' could 'cool down' society.&lt;/head&gt;
    &lt;head rend="h1"&gt;Your data is turned into thousands of different scores.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;There are stars behind the cloud:&lt;/p&gt;
    &lt;p&gt;Databrokers compare your data to the data of people they know more about. By comparing the patterns they try to guess the likelihood of thousands of details that you may never have disclosed. These are actual examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Religion&lt;/item&gt;
      &lt;item&gt;Rape victim&lt;/item&gt;
      &lt;item&gt;Into dieting&lt;/item&gt;
      &lt;item&gt;Into gardening&lt;/item&gt;
      &lt;item&gt;Number of online friends&lt;/item&gt;
      &lt;item&gt;Number of real friends&lt;/item&gt;
      &lt;item&gt;IQ&lt;/item&gt;
      &lt;item&gt;Political views&lt;/item&gt;
      &lt;item&gt;Had abortion&lt;/item&gt;
      &lt;item&gt;Gullibility&lt;/item&gt;
      &lt;item&gt;Projected sexual orientation&lt;/item&gt;
      &lt;item&gt;Real sexual orientation&lt;/item&gt;
      &lt;item&gt;Reads magazines on travel&lt;/item&gt;
      &lt;item&gt;Reads books on travel&lt;/item&gt;
      &lt;item&gt;Planning to have a baby&lt;/item&gt;
      &lt;item&gt;Communication device preference&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Has house plants&lt;/item&gt;
      &lt;item&gt;Neuroticism&lt;/item&gt;
      &lt;item&gt;Openness&lt;/item&gt;
      &lt;item&gt;Date of Birth&lt;/item&gt;
      &lt;item&gt;Into Fashion&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parents divorced before the age of 21&lt;/item&gt;
      &lt;item&gt;Economic stability&lt;/item&gt;
      &lt;item&gt;Potential inheritor&lt;/item&gt;
      &lt;item&gt;Extraversion&lt;/item&gt;
      &lt;item&gt;Agreeableness&lt;/item&gt;
      &lt;item&gt;Year house built&lt;/item&gt;
      &lt;item&gt;Smoker in the household&lt;/item&gt;
      &lt;item&gt;Has 'senior needs'&lt;/item&gt;
      &lt;item&gt;Has 'diabetic focus'&lt;/item&gt;
      &lt;item&gt;Easily addictable&lt;/item&gt;
      &lt;item&gt;Physical frailty&lt;/item&gt;
      &lt;item&gt;Gun owner&lt;/item&gt;
      &lt;item&gt;Adult 'empty nester'&lt;/item&gt;
      &lt;item&gt;Education level&lt;/item&gt;
      &lt;item&gt;Runs marathons&lt;/item&gt;
      &lt;item&gt;Into Elvis Memorabilia&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;People are starting to realize that this 'digital reputation' could limit their opportunities.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;(And that these algorithms are often biased, and built on bad data.)&lt;/p&gt;
    &lt;head rend="h3"&gt;In the news&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;You may not get that dream job if your data suggests you're not a very positive person.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you are a woman you may see fewer ads for high paying jobs.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you have "bad friends" on social media you might pay more for your loan.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Tinder's algorithms might not show you attractive people if you are not desirable yourself.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Cambridge Analytica created psychological profiles on all Americans to try and dissuade people from voting.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you return goods to the store often this will be used against you.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;What you post on social media may influence your odds of getting a tax audit.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Your health insurer may collect intimate data about your lifestyle, race and more.&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;People are changing their behavior to get better scores.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;This has good and bad sides.&lt;/p&gt;
    &lt;head rend="h2"&gt;Social Cooling is a name for the long-term negative side effects of living in a reputation economy:&lt;/head&gt;
    &lt;head rend="h3"&gt;1. A culture of conformity&lt;/head&gt;
    &lt;p&gt;Have you ever hesitated to click on a link because you thought your visit might be logged, and it could look bad?&lt;/p&gt;
    &lt;p&gt;More and more people feel this pressure, and they are starting to apply self-censorship.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. A culture of risk-aversion&lt;/head&gt;
    &lt;p&gt;When doctors in New York were given scores this had unexpected results.&lt;lb/&gt; Doctors that tried to help advanced cancer patients had a higher mortality rate, which translated into a lower score.&lt;/p&gt;
    &lt;p&gt;Doctors that didn't try to help were rewarded with high scores, even though their patients died prematurely.&lt;/p&gt;
    &lt;p&gt;Rating systems can create unwanted incentives, and increase pressure to conform to a bureaucratic average.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Increased social rigidity&lt;/head&gt;
    &lt;p&gt;Digital reputation systems are limiting our ability and our will to protest injustice.&lt;/p&gt;
    &lt;p&gt;In China each adult citizen is getting a government mandated "social credit score". This represents how well behaved they are, and is based on crime records, what they say on social media, what they buy, and even the scores of their friends.&lt;/p&gt;
    &lt;p&gt;If you have a low score you can't get a government job, visa, cheap loan, or even a nice online date.&lt;/p&gt;
    &lt;p&gt;Social pressure is the most powerful and most subtle form of control.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; As our weaknesses are mapped..&lt;/p&gt;
    &lt;head rend="h1"&gt;We are becoming too transparent.&lt;/head&gt;
    &lt;head rend="h1"&gt;This is breeding a society where self-censorship and risk-aversion are the new normal.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; Yes, we've had credit ratings before. But this is a whole new scale, with an incredible level of automation, integration and accessibility.&lt;/p&gt;
    &lt;p&gt;The solution?&lt;/p&gt;
    &lt;head rend="h1"&gt;We should compare this problem to Global Warming.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Social Cooling is subtle&lt;/head&gt;The pollution of our social environment is invisible to most people, just like air pollution was at first.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Social Cooling is complex&lt;/head&gt;It cannot be solved by politicians, citizens, entrepreneurs or scientists on their own.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Public awareness is still very low.&lt;/head&gt;
    &lt;p&gt;It took 40 years to get the problems with oil on the agenda, and 80 years to get to where we are now.&lt;lb/&gt; We can't take that long with Social Cooling.&lt;/p&gt;
    &lt;head rend="h2"&gt;In the next 10 years we will need to spread a more mature and nuanced perception of data and privacy.&lt;/head&gt;
    &lt;head rend="h2"&gt;As pressure to be perfect rises we will learn what privacy really is:&lt;/head&gt;
    &lt;p/&gt;
    &lt;p&gt;&lt;lb/&gt;Can we still forgive and forget?&lt;/p&gt;
    &lt;head rend="h2"&gt;When algorithms judge everything we do, we need to protect the right to make mistakes.&lt;/head&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt; When everything is remembered as big data, we need the right to have our mistakes forgotten.&lt;/head&gt;
    &lt;p&gt;In our data driven world..&lt;/p&gt;
    &lt;head rend="h2"&gt;Help spread the word&lt;/head&gt;
    &lt;p&gt;These are privacy-friendly sharing buttons.&lt;/p&gt;
    &lt;p&gt;Site by Tijmen Schep - Technology critic, privacy designer and public speaker.&lt;/p&gt;
    &lt;head rend="h2"&gt;Like this? Then also visit Mathwashing.com, HowNormalAmI.eu or cloakingcompany.com.&lt;/head&gt;
    &lt;p&gt;Feel free to re-use content, it's all under a CC-BY 4.0 License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.socialcooling.com/"/><published>2025-10-05T06:01:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45479820</id><title>Benefits of choosing email over messaging</title><updated>2025-10-05T18:12:21.047427+00:00</updated><content>&lt;doc fingerprint="6ed2ac1a2f24f8dc"&gt;
  &lt;main&gt;&lt;p&gt;My colleagues and friends know that I prefer to communicate with them via email rather than chat messaging. There are many benefits in such a choice. You may want to consider them and adopt the same stance.&lt;/p&gt;&lt;p&gt;My messages arrive in a single program, where I can process and tag them. With messaging programs Iâd have to iterate through Teams, Signal, WhatsApp, Slack, Viber, FaceTime, LinkedIn, Messenger, Google Meet, Discord, Mattermost, Instagram, WebEx, and possibly others, to collect and process the messages sent on each platform.&lt;/p&gt;&lt;p&gt;Similarly, if I want to find a past message I have exactly one place to search: my email archive.&lt;/p&gt;&lt;p&gt;Companies get out of business or become acquired and services can easily be discontinued; for a reminder have a look at the 64 services Google has discontinued. If you ever exchanged messages on ICQ, AIM, MSN Messenger, Skype, Yahoo! Messenger, Google Hangouts, GChat, BlackBerry Messenger, or Campfire your messages are now gone. With email and local message storage you control the lifetime of your messages (provided you perform regular backups). My email archive contains the messages I have sent and received from 1986 onward.&lt;/p&gt;&lt;p&gt;Email clients offer rich functionality. In the Thunderbird email client, I use the following features:&lt;/p&gt;&lt;p&gt;Some messaging systems offer some of these features, but all features are certainly not universally available.&lt;/p&gt;&lt;p&gt;Having a single messaging interface allows me to invest in becoming maximally productive in the email client application Iâm using. I can learn its features in-depth, I can tailor it with plug-ins, and I can extend it to fit my needs. When using it (many hours a day) my mind and muscles memorize how to perform common actions. With messaging platforms Iâd only be able to dabble in each.&lt;/p&gt;&lt;p&gt;Rather than having flow and concentration interrupted by incoming message notifications, with email I can easily decide when to fetch and process messages.&lt;/p&gt;&lt;p&gt;Some âfreeâ messaging services serve together with the messages ads or addictive content, such as short-form videos. Email clients will only display email messages.&lt;/p&gt;&lt;p&gt;Depending on the email provider I choose, I can obtain strong guarantees on who reads my email messages. Some, like Proton Mail are explicitly targeting people who want to protect their privacy. In contrast, many messaging platform will scan my messages to send me targeted ads or train their AI systems on them.&lt;/p&gt;&lt;p&gt;Email is transported with open protocols (SMTP, IMAP), which means I can use any email client and operating system I want and obtain any functionality I need, without depending on the business model or whims of the company controlling a proprietary messaging platform. I can even develop my own clients, something I have often done to automate the sending of multi-part email messages to students or conference committee members.&lt;/p&gt;&lt;p&gt;My messages are stored as plain text files in the super-simple Mbox file format, which means I can easily process them with other tools, reliably create backup copies, and move them from one email client to another.&lt;/p&gt;&lt;p&gt;For example, I have a small script that removes all attachments from old email messages, allowing me to keep my email archive in a manageable size. In other cases Iâve run on my message files scripts to analyze the messages I send and receive, and Iâve opened them in my editor to fix hardware-induced corruption.&lt;/p&gt;&lt;p&gt;In short, email can be an amazingly open and reliable environment that fosters exceptional productivity. We shouldnât settle for anything less.&lt;/p&gt;Comments Post Toot! Tweet&lt;p&gt;Last modified: Saturday, September 27, 2025 11:07 pm&lt;/p&gt;&lt;p&gt;Unless otherwise expressly stated, all original material on this page created by Diomidis Spinellis is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.spinellis.gr/blog/20250926/?li"/><published>2025-10-05T08:12:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480106</id><title>Personal data storage is an idea whose time has come</title><updated>2025-10-05T18:12:20.661092+00:00</updated><content>&lt;doc fingerprint="f5359ed38cb08d8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Personal data storage is an idea whose time has come&lt;/head&gt;
    &lt;p&gt;Back in 2009 Tim Berners-Lee drafted a web-specification for "Socially Aware Cloud Storage":&lt;/p&gt;
    &lt;quote&gt;There is an architecture in which a few existing or Web protocols are gathered together with some glue to make a world wide system in which applications (desktop or web application) can work on top of a layer of commodity read-write storage.&lt;lb/&gt;Crucial design issues are that principals (users) and groups are identifies by URIs, and so are global in scope, and that elements of storage are access controlled using those global identifiers. The result is that storage becomes a commodity, independent of the application running on it.&lt;/quote&gt;
    &lt;p&gt;Several of these ideas were going around in the late 2000s, shortly after the explosive growth of "web2" monoliths like Facebook.&lt;/p&gt;
    &lt;p&gt;Another spiritually similar idea being championed at the time came from the Opera browser folks who wanted to put "a web server in your browser".&lt;/p&gt;
    &lt;p&gt;While 'Opera Unite' never fully materialized, Tim's spec got significant traction some years down the road as one privacy crisis after another made the case for stronger web agency self-evident.&lt;/p&gt;
    &lt;p&gt;In 2015 Tim &amp;amp; co. secured some funding for the Solid Protocol.&lt;/p&gt;
    &lt;quote&gt;Right now we have the worst of both worlds, in which people not only cannot control their data, but also can’t really use it, due to it being spread across a number of different silo-ed websites. Our goal is to develop a web architecture that gives users ownership over their data, including the freedom to switch to new applications in search of better features, pricing, and policies.”&lt;/quote&gt;
    &lt;quote&gt;On the better web Berners-Lee envisions, users control where their data is stored and how it's accessed. For example, social networks would still run in the cloud. But you could store your data locally. Alternately, you could choose a different cloud server run by a company or community you trust.&lt;lb/&gt;You might have different servers for different types of information—for health and fitness data, say—that is completely separate from the one you use for financial records.&lt;/quote&gt;
    &lt;p&gt;To this day, Tim continues to eloquently champion the virtues of the Solid vision.&lt;/p&gt;
    &lt;quote&gt;We have the technical capability to give that power back to the individual. Solid is an open-source interoperable standard that I and my team developed at MIT more than a decade ago. Apps running on Solid don’t implicitly own your data – they have to request it from you and you choose whether to agree, or not. Rather than being in countless separate places on the internet in the hands of whomever it had been resold to, your data is in one place, controlled by you.&lt;lb/&gt;Sharing your information in a smart way can also liberate it. Why is your smartwatch writing your biological data to one silo in one format? Why is your credit card writing your financial data to a second silo in a different format? Why are your YouTube comments, Reddit posts, Facebook updates and tweets all stored in different places? Why is the default expectation that you aren’t supposed to be able to look at any of this stuff? You generate all this data – your actions, your choices, your body, your preferences, your decisions. You should own it. You should be empowered by it.&lt;/quote&gt;
    &lt;p&gt;The Solid Protocol remains an excellent idea and has even culminated in an official web specification, but Solid has not yet amounted to any mainstream adoption on the web. Its primary financial sponsor Inrupt (of which Tim is co-founder &amp;amp; CTO) has focused on the enterprise market as a path to sustainability; it remains to be seen what resources will be directed towards web-scale adoption of Solid.&lt;/p&gt;
    &lt;p&gt;Thankfully those of us who want data ownership and agency in our web applications now don't have to wait. AT Protocol was ushered in by the folks at Bluesky, now with a network of over 30M people strong and increasingly spread across multiple federated platforms/communities like Blacksky or Tangled.&lt;/p&gt;
    &lt;p&gt;While the respective architectures of the Solid and AT protocols are quite different, they're pointing to the same Open Social Web, re-built on the principles of user-sovereign data storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Personal Data Storage&lt;/head&gt;
    &lt;p&gt;What web-user sovereignty looks like in practice, from the vantage point of atproto, has been expertly illustrated by danabra.mov&lt;/p&gt;
    &lt;quote&gt;Notice that Alice’s handle is now&lt;code&gt;@alice.com&lt;/code&gt;. It is not allocated by a social media company [like facebook.com/alice]. Rather, her handle is the universal “internet handle”, i.e. a domain. Alice owns the&lt;code&gt;alice.com&lt;/code&gt;domain, so she can use it as a handle on any open social app. (On most open social apps, she goes by&lt;code&gt;@alice.com&lt;/code&gt;, but for others she wants a distinct disconnected identity, so she owns another handle she’d rather not share.)&lt;lb/&gt;Bob owns a domain too, even though he isn’t technical. He might not even know what a “domain” is. Bob just thinks of&lt;code&gt;@bob.com&lt;/code&gt;as his “internet handle”. Some open social apps will offer you a free subdomain on registration, just like Gmail gives you a free Gmail address, or may offer an extra flow for buying a domain. You’re not locked into your first choice, and can swap to a different domain later.&lt;lb/&gt;(...) With open social, Alice’s data—her posts, likes, follows, etc—is hosted on the web itself. Alongside her personal site, Alice now has a personal repository of her data.&lt;/quote&gt;
    &lt;p&gt;This new paradigm is made technically possible by what the AT protocol refers to as a Personal Data Server or PDS for short (what Solid calls a Pod).&lt;/p&gt;
    &lt;p&gt;The notion of a 'PDS' quickly comes off as something very technical and nerdy which is why it's not mentioned once in Dan's explainer, even though it's still targeted at an audience of web nerds. But really the only obscure word here is the Server, which in this context is interchangeable with Storage, as in Personal Data Storage.&lt;/p&gt;
    &lt;p&gt;Even regular internet users have some mental model of what personalized data storage entails, especially with the complementary framing of collectively owned and operated data storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data-banking Coops&lt;/head&gt;
    &lt;p&gt;If you're a regular internet user the PDS paradigm won't move your data from the cloud to your personal computer. Most people will still rely on an institutional cloud service, but instead of data-banking with a shareholder-controlled corporation most people’s data can be entrusted to the equivalent of member-owned credit unions for data storage.&lt;/p&gt;
    &lt;p&gt;One in every three US adults banks with a Credit Union. Achieving similar or better numbers for data storage is far from inconceivable considering how much our collective experience with Big Banking mirrors that of Big Tech/Social.&lt;/p&gt;
    &lt;p&gt;The concept of data cooperatives has already gained a lot of traction in the fediverse with several providers like social.coop, data.coop and cosocial.ca being operational for many years and still going strong. Soon the AT network will have a similarly co-owned institution in Northsky.&lt;/p&gt;
    &lt;p&gt;Whether these providers are strictly cooperatives in the formal sense isn't what's most important here though; any suffuciently transparent, democratic and community-oriented data bank (like the aforementioned Blacksky, or the forthcoming Eurosky) is a valid steward and co-creator of an Open Social.&lt;/p&gt;
    &lt;p&gt;Data Ownership as a conversation changes when data resides primarily with people-governed institutions rather than corporations. Rather than arguing for what kinds of data we ought to be able to download from the corporate silos, the platforms should be asking us what kinds of data they may copy from our servers, and only with strictly temporary allowances.&lt;/p&gt;
    &lt;p&gt;And while the separation of user data and social platform is most fully realized today in the AT network, there are exciting signs of cross-pollination happening in the ongoing development of atproto’s predecessor ActivityPub. I hope to see similar openness towards technological convergence in Solid for a more pluralistic social web.&lt;/p&gt;
    &lt;p&gt;Personal Data Storage has long since escaped containment as a concept pertaining to any specific protocol. Some implementations of it will be more mainstream than others, but pragmatic data coops can be protocol-agnostic and storage formats are transmutable.&lt;/p&gt;
    &lt;p&gt;As long as we have sufficient control of our own data there will always be a way to restart our social graph and digital presence elsewhere in the event of platform collapse. Let’s make the web personal again.&lt;/p&gt;
    &lt;p&gt;See also:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.muni.town/personal-data-storage-idea/"/><published>2025-10-05T09:07:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480317</id><title>Self hosting 10TB in S3 on a framework laptop and disks</title><updated>2025-10-05T18:12:20.461587+00:00</updated><content>&lt;doc fingerprint="108fdf7d4527a9d6"&gt;
  &lt;main&gt;
    &lt;p&gt;About 5 months ago I made the decision to start self hosting my own S3. I was working on AppGoblin’s SDK tracking of the top 100k Android and iOS apps so was wanting a lot of space, but for cheap.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;I got really lucky with getting a second hand Framework laptop. The laptop was missing it’s screen, and was one of the older ones, so it was perfect for a home server. In addition I bought a “just a bunch of disks” JBOD. The framework laptop is running ZFS + garage S3. &lt;/p&gt;
    &lt;head rend="h2"&gt;I’m happy to report I haven’t thought about this laptop for months&lt;/head&gt;
    &lt;p&gt;I’ve been away, I’ve been working, I’ve been busy, and I’ve definitely been using my S3. But I hadn’t thought about the laptop in 4 months. When I finally logged in, I saw I’ve used 10TB of space and it was patiently waiting for a restart for some upgrades. I nervously restarted, and was so relieved to see everything come right back up.&lt;/p&gt;
    &lt;head rend="h2"&gt;I updated garage s3 with no issues as well&lt;/head&gt;
    &lt;p&gt;I also saw a pending upgrade for garage v1 to v2. This went along without a hitch too. Feels like it’s been a good weekend.&lt;/p&gt;
    &lt;head rend="h2"&gt;I’ve been warned…&lt;/head&gt;
    &lt;p&gt;Just so you know, I understand my use case for ZFS is possibly a bit non standard as I’m using a USB to connect the laptop and JBOD. This initially caused me issues with ZFS when garage was heavily reading and writing (the initial setup had the SQLite metadata also stored on the JBOD/ZFS).&lt;/p&gt;
    &lt;p&gt;I moved my metadata to the laptop, which has so far resolved any ZFS issues again.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jamesoclaire.com/2025/10/05/self-hosting-10tb-in-s3-on-a-framework-laptop-disks/"/><published>2025-10-05T09:51:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480506</id><title>Beginner Guide to VPS Hetzner and Coolify</title><updated>2025-10-05T18:12:20.069082+00:00</updated><content>&lt;doc fingerprint="fe08e21335a407ea"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VPS Setup and Security Checklist: Complete Self-Hosting Guide for 2025&lt;/head&gt;
    &lt;p&gt;I set up my own VPS, documented every step, and ended up with a repeatable deployment pipeline. This is both a checklist for my future self and a guide for anyone curious about self-hosting. Along the way I'll explain why I picked Hetzner and Coolify, and how they compare with other options like DigitalOcean, AWS, Render, or Fly.io.&lt;/p&gt;
    &lt;p&gt;This comprehensive checklist covers every essential step for setting up a secure, production-ready VPS. Each section includes commands, verification steps, and troubleshooting tips based on real-world experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pre-Setup Checklist&lt;/head&gt;
    &lt;p&gt;Before You Begin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Choose your VPS provider (Hetzner recommended for price/performance)&lt;/item&gt;
      &lt;item&gt;Select server specifications (minimum 1GB RAM, 20GB storage)&lt;/item&gt;
      &lt;item&gt;Note down server IP address and root credentials&lt;/item&gt;
      &lt;item&gt;Prepare your local machine with SSH client&lt;/item&gt;
      &lt;item&gt;Have a strong password generator ready&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Picking the VPS provider&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chose Hetzner Cloud (cheap, fast, reliable in Europe)&lt;/item&gt;
      &lt;item&gt;Alternatives I considered: &lt;list rend="ul"&gt;&lt;item&gt;DigitalOcean → smoother onboarding, great docs, slightly more expensive&lt;/item&gt;&lt;item&gt;AWS Lightsail → decent for small apps, but tied to AWS ecosystem (complex for beginners)&lt;/item&gt;&lt;item&gt;Linode → reliable, but Hetzner wins on price/performance&lt;/item&gt;&lt;item&gt;Render/Fly.io → easier PaaS, but more opinionated and costly at scale&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why Hetzner?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2–3x cheaper for the same specs compared to DO/AWS&lt;/item&gt;
      &lt;item&gt;Strong European datacenter presence (latency advantage for my use case)&lt;/item&gt;
      &lt;item&gt;Transparent pricing and no surprise bills&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Initial Server Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;First Login and System Updates&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Initial login as root&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh root@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Update package lists and upgrade system&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;apt update &amp;amp;&amp;amp; apt upgrade -y&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify system information&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;uname -a cat /etc/os-release&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Root Account Security&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change root password&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;passwd&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Use strong password with mixed case, numbers, symbols
- Store securely in password manager
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create secondary user account&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;adduser your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Choose descriptive username (not 'admin' or 'user')
- Set strong password
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add user to sudo group&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;usermod -aG sudo your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify user groups&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;groups your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show: `your-username : your-username sudo`
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test sudo access&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;su - your-username sudo whoami&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should return: `root`
&lt;/code&gt;
    &lt;head rend="h4"&gt;SSH Key Authentication Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generate SSH keys on LOCAL machine (not server)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#### Ed25519 (recommended) ssh-keygen -t ed25519 -C "your-email@example.com" ##### Or RSA if Ed25519 not supported ssh-keygen -t rsa -b 4096 -C "your-email@example.com"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Display public key on local machine&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;cat ~/.ssh/id_ed25519.pub #### or cat ~/.ssh/id_rsa.pub&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Copy public key to clipboard&lt;/item&gt;
      &lt;item&gt;Create .ssh directory on server (as your user, not root)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;mkdir -p ~/.ssh chmod 700 ~/.ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create authorized_keys file&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;nano ~/.ssh/authorized_keys&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Paste your public key
- Save and exit
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set correct permissions&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;chmod 600 ~/.ssh/authorized_keys&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH key login (from local machine)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh your-username@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should login without password prompt
&lt;/code&gt;
    &lt;head rend="h4"&gt;Disable Password Authentication&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edit SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modify these settings:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;PasswordAuthentication no PubkeyAuthentication yes&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check cloud-init config if exists&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config.d/50-cloud-init.conf&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Set `PasswordAuthentication no` here too if file exists
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo sshd -t&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show no errors
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart SSH service&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart ssh #### or sudo service ssh restart&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify service status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show active (running) with green dot
&lt;/code&gt;
    &lt;head rend="h4"&gt;Disable Root Login&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edit SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change root login setting&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;PermitRootLogin no&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart SSH service&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test root login is blocked (from another terminal)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh root@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should get "Permission denied"
&lt;/code&gt;
    &lt;head rend="h2"&gt;Firewall Configuration Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;UFW (Uncomplicated Firewall) Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check UFW status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set default policies&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw default deny incoming sudo ufw default allow outgoing&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow SSH before enabling firewall&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow ssh #### or if you changed SSH port: sudo ufw allow 2022/tcp&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow HTTP and HTTPS for web apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow 80/tcp sudo ufw allow 443/tcp&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable firewall&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw enable&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Type 'y' when prompted
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify firewall rules&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status verbose&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Advanced Firewall Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restrict SSH to your IP (optional but recommended)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow from YOUR_IP_ADDRESS to any port 22 sudo ufw delete allow ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change default SSH port (optional security through obscurity)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Change `Port 22` to `Port 2022` (or your chosen port)
- Update firewall: `sudo ufw allow 2022/tcp`
- Remove old rule: `sudo ufw delete allow 22/tcp`
- Restart SSH: `sudo systemctl restart ssh`
&lt;/code&gt;
    &lt;head rend="h2"&gt;Automatic Updates Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Unattended Upgrades Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install unattended-upgrades&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install unattended-upgrades apt-listchanges&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable automatic updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo dpkg-reconfigure unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Select "Yes" in the dialog
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure update settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/apt/apt.conf.d/50unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uncomment security updates line&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;"${distro_id}:${distro_codename}-security";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure email notifications (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;Unattended-Upgrade::Mail "your-email@example.com";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable automatic reboots if needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;Unattended-Upgrade::Automatic-Reboot "true"; Unattended-Upgrade::Automatic-Reboot-Time "02:00";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo unattended-upgrades --dry-run&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check service status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Production Application Deployment Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Node.js Production Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Node.js LTS&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash - sudo apt-get install -y nodejs&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify installation&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;node --version npm --version&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install PM2 globally&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo npm install -g pm2&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Upload your application files&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;scp -r ./your-app your-username@your-server-ip:~/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;cd ~/your-app npm install --production&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create production build&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;npm run build&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Process Manager Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start application with PM2&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;NODE_ENV=production pm2 start app.js --name "your-app"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure PM2 for clustering (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 start app.js -i max --name "your-app-cluster"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Save PM2 configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 save&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable PM2 startup&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 startup #### Run the command it outputs&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test application restart&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 restart all pm2 status&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Reverse Proxy Setup (Nginx)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Nginx&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create site configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/nginx/sites-available/your-app&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Basic Nginx configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;server { listen 80; server_name your-domain.com; location / { proxy_pass http://localhost:3000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_cache_bypass $http_upgrade; } }&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable site&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ln -s /etc/nginx/sites-available/your-app /etc/nginx/sites-enabled/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test Nginx configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nginx -t&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart Nginx&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;SSL Certificate Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Let's Encrypt with Certbot&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Certbot&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install certbot python3-certbot-nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Obtain SSL certificate&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo certbot --nginx -d your-domain.com&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test automatic renewal&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo certbot renew --dry-run&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify SSL grade&lt;/item&gt;
      &lt;item&gt;Visit: https://www.ssllabs.com/ssltest/&lt;/item&gt;
      &lt;item&gt;Should get A or A+ rating&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Monitoring and Maintenance Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Basic Monitoring Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install monitoring tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install htop iotop netstat-nat&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check system resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop df -h free -h&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo tail -f /var/log/syslog sudo tail -f /var/log/auth.log&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up log rotation&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/logrotate.d/your-app&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Backup Strategy&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create backup script&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;nano ~/backup.sh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sample backup script&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#!/bin/bash DATE=$(date +%Y%m%d_%H%M%S) tar -czf ~/backups/app_backup_$DATE.tar.gz ~/your-app #### Add database backup commands if needed&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make script executable&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;chmod +x ~/backup.sh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up automated backups&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;crontab -e&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Add: `0 2 * * * /home/username/backup.sh`
&lt;/code&gt;
    &lt;head rend="h2"&gt;Troubleshooting Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Common Issues and Solutions&lt;/head&gt;
    &lt;p&gt;SSH Connection Problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check firewall rules: &lt;code&gt;sudo ufw status&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Verify SSH service: &lt;code&gt;sudo systemctl status ssh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check SSH logs: &lt;code&gt;sudo tail -f /var/log/auth.log&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Test from different network&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Permission Denied Errors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check file permissions: &lt;code&gt;ls -la&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Verify user groups: &lt;code&gt;groups username&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check sudo configuration: &lt;code&gt;sudo -l&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Service Not Starting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check service status: &lt;code&gt;sudo systemctl status service-name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;View service logs: &lt;code&gt;sudo journalctl -u service-name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check configuration files syntax&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High Resource Usage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identify processes: &lt;code&gt;htop&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check disk usage: &lt;code&gt;df -h&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monitor network: &lt;code&gt;netstat -tulpn&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Review application logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Final Verification Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Security Verification&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH key authentication works&lt;/item&gt;
      &lt;item&gt;Verify password authentication is disabled&lt;/item&gt;
      &lt;item&gt;Confirm root login is blocked&lt;/item&gt;
      &lt;item&gt;Check firewall is active and configured&lt;/item&gt;
      &lt;item&gt;Verify automatic updates are working&lt;/item&gt;
      &lt;item&gt;Test application runs in production mode&lt;/item&gt;
      &lt;item&gt;Confirm SSL certificate is valid&lt;/item&gt;
      &lt;item&gt;Verify backups are being created&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Performance Testing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run basic load test&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#### Install Apache Bench sudo apt install apache2-utils #### Test with 100 requests, 10 concurrent ab -n 100 -c 10 http://your-domain.com/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor resource usage during load&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check application logs for errors&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 logs&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Quick Reference Commands&lt;/head&gt;
    &lt;p&gt;System Information:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop # System monitor df -h # Disk usage free -h # Memory usage uname -a # System info&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Process Management:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 status # PM2 process status pm2 restart all # Restart all processes pm2 logs # View logs pm2 monit # Real-time monitoring&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Security:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status # Firewall status sudo fail2ban-client status # Fail2ban status sudo lynis audit system # Security audit&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Services:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status nginx # Service status sudo systemctl restart nginx # Restart service sudo journalctl -u nginx # Service logs&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;This checklist provides a complete approach to VPS setup and management. This isn’t just about saving money. It’s about control and understanding. By self-hosting with Hetzner + Coolify, I built muscle memory for devops that paid off in confidence and freedom.&lt;/p&gt;
    &lt;p&gt;If you’ve been meaning to try VPS hosting, consider this a nudge.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bhargav.dev/blog/VPS_Setup_and_Security_Checklist_A_Complete_Self_Hosting_Guide"/><published>2025-10-05T10:39:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480622</id><title>The deadline isn't when AI outsmarts us – it's when we stop using our own minds</title><updated>2025-10-05T18:12:19.837468+00:00</updated><content>&lt;doc fingerprint="3f37101d29eb8c85"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;“You have 18 months”&lt;/head&gt;
    &lt;head rend="h3"&gt;The real deadline isn’t when AI outsmarts us — it’s when we stop using our own minds.&lt;/head&gt;
    &lt;p&gt;In fitness, there is a concept called “time under tension.” Take a simple squat, where you hold a weight and lower your hips from a standing position. With the same weight, a person can do a squat in two seconds or 10 seconds. The latter is harder, but it also builds more muscle. More time is more tension; more pain is more gain.&lt;/p&gt;
    &lt;p&gt;Thinking benefits from a similar principle of “time under tension.” It is the ability to sit patiently with a group of barely connected or disconnected ideas that allows a thinker to braid them together into something that is combinatorially new. It’s very difficult to defend this idea by describing other people’s thought processes, so I’ll describe my own.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, The Argument Editor-in-Chief Jerusalem Demsas asked me to write an essay about the claim that AI systems would take all of our jobs within 18 months. My initial reaction was … no?&lt;/p&gt;
    &lt;p&gt;The prediction is so stupendously aggressive and almost certainly wrong, so my instinct was there was really nothing more to say on the subject. Certainly not 1,799 words more. But as I sat with the prompt, several pieces of a puzzle began to slide together: a Financial Times essay I’d read, an Atlantic article I liked, a National Assessment of Educational Progress study I’d saved in a tab, an interview with Cal Newport I’d recorded, a Walter Ong book I was encouraged to read, a stray thought I’d had in the gym recently while trying out eccentric pullups for the first time about how time multiplies both pain and gain in fitness settings. The contours of a framework came into view.&lt;/p&gt;
    &lt;p&gt;The problem of the next 18 months isn’t AI disemploying all workers, or students losing competition after competition to nonhuman agents. The problem is whether we will degrade our own capabilities in the presence of new machines. We are so fixated on how technology will outskill us that we miss the many ways that we can deskill ourselves.&lt;/p&gt;
    &lt;p&gt;You have 18 months.&lt;/p&gt;
    &lt;p&gt;That’s the message from several leading AI executives and thinkers about how long people will retain their advantage over artificial intelligence in the workforce. By the summer of 2027, the story goes, AI’s explosion in capabilities will leave carbon-based life forms in the dust. Up to “half of all entry-level white-collar jobs” will be wiped out, and even Nobel Prize-worthy minds will cower in fear that AI’s architects will have built a “country of geniuses in a datacenter.”&lt;/p&gt;
    &lt;p&gt;This doomsday clock seems true enough to many people, because the question I’ve fielded more than any other from parents in the last few months is some version of: “If AI is about to be better than us at everything, what should my children do?” If generative AI is better at coding, diagnosing, and problem-solving than any software programmer, radiologist, or mathematician, then even the traditionally “safe” majors like computer science, medicine, and math could be anything but safe.&lt;/p&gt;
    &lt;p&gt;I understand the anxiety behind the question, but rather than try to forecast the future as it might turn out, I’d prefer to describe reality as it already exists. While we have no idea how AI might make working people obsolete at some imaginary date, we can already see how technology is affecting our capacity to think deeply right now. And I am much more concerned about the decline of thinking people than I am about the rise of thinking machines.&lt;/p&gt;
    &lt;head rend="h3"&gt;The end of writing, the end of reading&lt;/head&gt;
    &lt;p&gt;In March, New York Magazine published the sort of cover story that goes instantly viral, not because of its shock value, but, quite the opposite, because it loudly proclaimed what most people were already thinking: Everybody is using AI to cheat in school.&lt;/p&gt;
    &lt;p&gt;By allowing high-school and college students to summon into existence any essay on any topic, large language models have created an existential crisis for teachers trying to evaluate their students’ ability to actually write. “College is just how well I can use ChatGPT at this point,” one student told New York Magazine. “Massive numbers of students are going to emerge from university with degrees, and into the workforce, who are essentially illiterate,” a professor echoed.&lt;/p&gt;
    &lt;p&gt;The demise of writing matters because writing is not a second thing that happens after thinking. The act of writing is an act of thinking. This is as true for professionals as it is for students. In “Writing is thinking,” an editorial in Nature, the authors argued that “outsourcing the entire writing process to LLMs” deprives scientists of the important work of understanding what they’ve discovered and why it matters.&lt;/p&gt;
    &lt;p&gt;Students, scientists, and anyone else who lets AI do the writing for them will find their screens full of words and their minds emptied of thought.&lt;/p&gt;
    &lt;p&gt;As writing skills have declined, reading has declined even more. “Most of our students are functionally illiterate,” a pseudonymous college professor using the name Hilarius Bookbinder wrote in a March Substack essay on the state of college campuses. “This is not a joke.” Nor is it hyperbole.&lt;/p&gt;
    &lt;p&gt;Achievement scores in literacy and numeracy are declining across the West for the first time in decades, leading the Financial Times reporter John Burn-Murdoch to wonder if humans have “passed peak brain power” at the very moment that we are building machines to think for us. In the U.S., the so-called Nation’s Report Card, published by the NAEP, recently found that average reading scores hit a 32-year low in 2024 — which is troubling, since the data series only goes back 32 years.&lt;/p&gt;
    &lt;p&gt;Of course, Americans are reading words all the time: email, texts, social media newsfeeds, subtitles on Netflix shows. But these words live in writing fragments that hardly require any kind of sustained focus necessary to make sense of a larger text. Indeed, Americans in the digital age don’t seem interested in or capable of sitting with anything longer than a tweet. The share of Americans overall who say they read books for leisure has declined by nearly 40% since the 2000s.&lt;/p&gt;
    &lt;p&gt;Even America’s highest-performing students have essentially stopped reading anything longer than a paragraph. Last year, The Atlantic’s Rose Horowitch reported that students are matriculating into America’s most-elite colleges without having ever read a full book for school. “Daniel Shore, the chair of Georgetown’s English department, told me that his students have trouble staying focused on even a sonnet,” Horowitch wrote.&lt;/p&gt;
    &lt;p&gt;Nat Malkus, an education researcher at the American Enterprise Institute, suggested to me that high schools have chunkified books to prepare students for the reading-comprehension sections of standardized exams. By optimizing the assessment of reading skills, the U.S. education system appears to have accidentally killed book reading.&lt;/p&gt;
    &lt;p&gt;The decline of writing and reading matters because writing and reading are the twin pillars of deep thinking, according to Cal Newport, a computer science professor and the author of several bestselling books, including Deep Work. The modern economy prizes the sort of symbolic logic and systems thinking for which deep reading and writing are the best practice.&lt;/p&gt;
    &lt;p&gt;AI is “the latest in multiple heavyweight entrances into the prize fight against our ability to actually think,” Newport said. The rise of TV corresponded with the decline in per capita newspaper subscriptions and a slow demise of reading for pleasure. Then along came the internet, followed by social media, the smartphone, and streaming TV.&lt;/p&gt;
    &lt;p&gt;“The one-two punch of reading and writing is like the serum we have to take in a superhero comic book to gain the superpower of deep symbolic thinking,” Newport said. “And so I have been ringing this alarm bell that we have to keep taking the serum.”&lt;/p&gt;
    &lt;p&gt;Newport’s warning echoes an observation made by the scholar Walter Ong in his book “Orality and Literacy.” According to Ong, literacy is no passing skill. It was a means of restructuring human thought and knowledge to create space for complex ideas.&lt;/p&gt;
    &lt;p&gt;Stories can be memorized by people who cannot read or write. But nothing as advanced as, say, Newton’s “Principia” could be passed down from generation to generation without the ability to write down calculus formulas. Oral dialects commonly have only a few thousand words, while “the grapholect known as standard English has … at least a million and a half words,” Ong wrote. If reading and writing “rewired” the logic engine of the human brain, the decline of reading and writing are unwiring our cognitive superpower at the very moment that a greater machine appears to be on the horizon.&lt;/p&gt;
    &lt;p&gt;So what should our children study in an age of thinking machines? While I don’t know what field any particular student should major in, I do feel strongly about what skill they should value: It’s the very same skill that I see in decline. It’s the patience to read long and complex texts; to hold conflicting ideas in our heads and enjoy their dissonance; to engage in hand-to-hand combat at the sentence level within a piece of writing — and to value these things at a time when valuing them is a choice, because video entertainment is replacing reading and ChatGPT essays are replacing writing. As AI becomes abundant, there is a clear and present threat that deep human thinking will become scarce.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theargumentmag.com/p/you-have-18-months"/><published>2025-10-05T11:08:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45481008</id><title>86 GB/s bitpacking with ARM SIMD (single thread)</title><updated>2025-10-05T18:12:19.470443+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ashtonsix/perf-portfolio/tree/main/bytepack"/><published>2025-10-05T12:27:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45481298</id><title>Show HN: Pyscn – Python code quality analyzer for vibe coders</title><updated>2025-10-05T18:12:18.905635+00:00</updated><content>&lt;doc fingerprint="9e863d005f1db92f"&gt;
  &lt;main&gt;
    &lt;p&gt;Building with Cursor, Claude, or ChatGPT? pyscn performs structural analysis to keep your codebase maintainable.&lt;/p&gt;
    &lt;code&gt;# Run analysis without installation
uvx pyscn analyze .
# or
pipx run pyscn analyze .&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;pyscn_20251005.mov&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔍 CFG-based dead code detection – Find unreachable code after exhaustive if-elif-else chains&lt;/item&gt;
      &lt;item&gt;📋 Clone detection with APTED + LSH – Identify refactoring opportunities with tree edit distance&lt;/item&gt;
      &lt;item&gt;🔗 Coupling metrics (CBO) – Track architecture quality and module dependencies&lt;/item&gt;
      &lt;item&gt;📊 Cyclomatic complexity analysis – Spot functions that need breaking down&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;100,000+ lines/sec • Built with Go + tree-sitter&lt;/p&gt;
    &lt;p&gt;Run comprehensive analysis with HTML report&lt;/p&gt;
    &lt;code&gt;pyscn analyze .                              # All analyses with HTML report
pyscn analyze --json .                       # Generate JSON report
pyscn analyze --select complexity .          # Only complexity analysis
pyscn analyze --select deps .                # Only dependency analysis
pyscn analyze --select complexity,deps,deadcode . # Multiple analyses&lt;/code&gt;
    &lt;p&gt;Fast CI-friendly quality gate&lt;/p&gt;
    &lt;code&gt;pyscn check .                      # Quick pass/fail check
pyscn check --max-complexity 15 .  # Custom thresholds&lt;/code&gt;
    &lt;p&gt;Create configuration file&lt;/p&gt;
    &lt;code&gt;pyscn init                         # Generate .pyscn.toml&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;💡 Run&lt;/p&gt;&lt;code&gt;pyscn --help&lt;/code&gt;or&lt;code&gt;pyscn &amp;lt;command&amp;gt; --help&lt;/code&gt;for complete options&lt;/quote&gt;
    &lt;p&gt;Create a &lt;code&gt;.pyscn.toml&lt;/code&gt; file or add &lt;code&gt;[tool.pyscn]&lt;/code&gt; to your &lt;code&gt;pyproject.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# .pyscn.toml
[complexity]
max_complexity = 15

[dead_code]
min_severity = "warning"

[output]
directory = "reports"&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;⚙️ Run&lt;/p&gt;&lt;code&gt;pyscn init&lt;/code&gt;to generate a full configuration file with all available options&lt;/quote&gt;
    &lt;code&gt;# Install with pipx (recommended)
pipx install pyscn

# Or run directly with uvx
uvx pyscn&lt;/code&gt;
    &lt;head&gt;Alternative installation methods&lt;/head&gt;
    &lt;code&gt;git clone https://github.com/ludo-technologies/pyscn.git
cd pyscn
make build&lt;/code&gt;
    &lt;code&gt;go install github.com/ludo-technologies/pyscn/cmd/pyscn@latest&lt;/code&gt;
    &lt;code&gt;# .github/workflows/code-quality.yml
name: Code Quality
on: [push, pull_request]

jobs:
  quality-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: pip install pyscn
      - name: Quick quality check
        run: pyscn check .
      - name: Generate detailed report
        run: pyscn analyze --json --select complexity,deadcode,deps src/
      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-report
          path: .pyscn/reports/&lt;/code&gt;
    &lt;p&gt;📚 Development Guide • Architecture • Testing&lt;/p&gt;
    &lt;p&gt;MIT License — see LICENSE&lt;/p&gt;
    &lt;p&gt;Built with ❤️ using Go and tree-sitter&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ludo-technologies/pyscn"/><published>2025-10-05T13:22:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45481609</id><title>Retiring Test-Ipv6.com</title><updated>2025-10-05T18:12:18.731449+00:00</updated><content>&lt;doc fingerprint="3e399bddbaeb15e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Retiring test-ipv6.com&lt;/head&gt;
    &lt;p&gt;TL;DR: I will retire test-ipv6.com in December 2025.&lt;/p&gt;
    &lt;p&gt;I have provided test-ipv6.com to the public since 2010. I've sunk significant resources - engineering, support, equipment, and hosting fees - into what is a revenue-free product.&lt;/p&gt;
    &lt;p&gt;Without going into details: I feel now is the time for me to refocus my resources within the family.&lt;lb/&gt; I hope people will understand, and respect this decision.&lt;/p&gt;
    &lt;p&gt;I am shutting the site down, with a target of "during winter break" (December) 2025.&lt;/p&gt;
    &lt;p&gt;Mirror operators: Should you wish to keep your mirrors up, they will stop getting updates in December.&lt;/p&gt;
    &lt;p&gt;Service providers: If you have runbooks for your support team based on this site, or based on RIPE-631, you'll need to update those.&lt;/p&gt;
    &lt;p&gt;FAQ:&lt;/p&gt;
    &lt;p&gt;Q: Will I (jfesler) transfer the source?&lt;/p&gt;
    &lt;p&gt;A: These portions are already public.&lt;/p&gt;
    &lt;p&gt;These are already public.&lt;lb/&gt; http://github.com/falling-sky/source&lt;lb/&gt; https://github.com/falling-sky/fsbuilder - used to build what's in source&lt;lb/&gt; https://github.com/falling-sky/mod_ip - the /ip/ handler for Apache&lt;lb/&gt; https://github.com/falling-sky/mtu1280d - the synthetic MTU180 netfilter daemon.&lt;/p&gt;
    &lt;p&gt;The remaining parts, such as geolocation and service provider lookups, I am contractually unable to release. Please do not ask.&lt;/p&gt;
    &lt;p&gt;Q: Will I (jfesler) transfer the domain?&lt;/p&gt;
    &lt;p&gt;A: I’d consider a reputable RIR or NIC organization serving the public interest taking things over.&lt;/p&gt;
    &lt;p&gt;Q: Should mirrors be retired?&lt;/p&gt;
    &lt;p&gt;A: I would suggest it. Once the primary site is retired, I will stop monitoring the functionality of your mirror, and stop providing geolocation and service provider lookups.&lt;/p&gt;
    &lt;p&gt;Q: I have more questions or comments!&lt;/p&gt;
    &lt;p&gt;A: If we ever meet for coffee or beer, ask me then.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://retire.test-ipv6.com/"/><published>2025-10-05T14:11:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45481892</id><title>The QNX Operating System</title><updated>2025-10-05T18:12:18.203818+00:00</updated><content>&lt;doc fingerprint="ff9e891d51a325c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Gordon Bell and Dan Dodge were finishing their time at the University of Waterloo in Ontario in 1979. In pursuit of their masters degrees, they’d worked on a system called Thoth in their real-time operating systems course. Thoth was interesting not only for having been real-time and having featured synchronous message passing, but also for originally having been written in the B programming langue. It was then rewritten in the UW-native Eh language (fitting for a Canadian university), and then finally rewritten in Zed. It is this last, Zed-written, version of Thoth to which Bell and Dodge would have been exposed. Having always been written in a high-level language, the system was portable, and programs were the same regardless of the underlying hardware. Both by convention and by design, Thoth strongly encouraged programs to be structured as networks of communicating processes. As the final project for the RTOS course, students were expected to implement a real-time system of their own. This experience was likely pivotal to their next adventure.&lt;/p&gt;
    &lt;p&gt;The duo’s first year after graduation was a busy one. They moved to Kanata, went to work for Bell-Northern Research (now Nortel), and on the 30th of March in 1980, they founded Quantum Software Systems. To continue their research and experimentation with operating systems, they assembled a microcomputer built around a Motorola 6809. With the release of the IBM PC in September of 1981, Quantum’s efforts shifted to that target. Their goal was to produce a real-time operating system that would enable the PC’s use in factories, communication systems, and anywhere else that emphasized reliability.&lt;/p&gt;
    &lt;p&gt;The first version of Bell and Dodge’s operating system was QUNIX 0.1 (the Q could have been for Quantum, or for Quick, I’ve seen both from former Quantum employees), and it was running on that early, hand-assembled, 8bit microcomputer. This earliest creation was never released outside of Quantum Software as far as I know. QUNIX was a vaguely UNIX-like, microkernel, real-time operating system. I say that it was vaguely UNIX-like because in these early versions, there were some serious differences. In QUNIX, there were CP/M-like things too. Each disk had a drive number prefix, non-disk device files’ names were reserved, and the commands were a bit different from those in UNIX, often simplified to the point of being more CP/M-like than UNIX-like. Another major difference was the directory hierarchy. On a traditional UNIX system, binaries were stored in &lt;code&gt;/bin&lt;/code&gt; or &lt;code&gt;/usr/bin&lt;/code&gt;, configurations in &lt;code&gt;/etc&lt;/code&gt;, and user directories in &lt;code&gt;/home&lt;/code&gt;. On QUNIX, this wasn’t the case. Commands included in the path variable were in &lt;code&gt;/cmds&lt;/code&gt;, configuration files were in &lt;code&gt;/config&lt;/code&gt;, the OS binaries were in &lt;code&gt;/sys&lt;/code&gt;, user directories were &lt;code&gt;/user&lt;/code&gt;, drivers were in &lt;code&gt;/drivers&lt;/code&gt;, and utilities were in &lt;code&gt;/util&lt;/code&gt;. Then, the &lt;code&gt;man&lt;/code&gt; command did not exist, and &lt;code&gt;help&lt;/code&gt; was used instead. Instead of &lt;code&gt;ps&lt;/code&gt;, the system had &lt;code&gt;task&lt;/code&gt; with the labels of father, son, and brother to denote parent and child processes. The first version of QUNIX for the IBM PC was made before the end of 1981, and released either in December of 1981 or January of 1982, making QUNIX the first known microkernel operating system for the PC platform.&lt;/p&gt;
    &lt;p&gt;A fun note from Paul N. Leroux, the bar chart on the monitor in the back left was physically glued to that monitor for another press image. It wasn’t meant to be in this image, but as photo editing tools were essentially non-existent at the time, fixing this would have required them to reshoot. They chose to go to press with bar chart present.&lt;/p&gt;
    &lt;p&gt;With QUNIX 0.4.33 in 1982, QUNIX became the first operating system for the IBM PC to support a hard disk, and in particular, it supported a 5MB Davong HDD. Given that a 10MB disk in 1982 could cost around $3000, it makes sense that the company’s first target was a bit more modest. At this point, however, QUNIX would not boot from an HDD. All of the floppy contents could be copied to a hard disk, but the user would still need to boot from a floppy disk.&lt;/p&gt;
    &lt;p&gt;Even in these early stages of development, the system began getting recognition, and this became a small problem. The name QUNIX was a bit too close to the name UNIX for AT&amp;amp;T. The name of the system was changed to QNX in late 1982 following a Cease and Desist by AT&amp;amp;T. The first official QNX version was released the following year. At the time of the name change the kernel consisted of around 10K line of C, and it handled task scheduling, message passing, and task priority. Everything else was implemented in services that used the microkernel’s message passing to communicate with each other (even drivers, filesystems, and networking). As an important feature, message queues were network transparent so a task on one physical machine could communicate with a task on a separate physical machine on the same network as easily as if it were local. This inherently multitasked and multiuser system allowed 250 simultaneous tasks from 4 to 16 simultaneous users. The system would make extensive use of the 8087 if it was available, and required a minimum of 96K RAM. Loading up the C compiler would require an additional 32K. It’s impressive what the small company achieved on the 8088, even if, for the time, the RAM requirements were quite high. QNX release version 1.0, in March of 1983, running on an IBM PC achieved 29% to 47% the speed of a DEC VAX 11/780 depending upon the task at hand when tested by Rao Mikkilineni at Bell Labs. Sadly, I’ve been unable to find his original write-up of his testing, which was apparently in the publication Personna. If you have information about it, I’d love to get some details. While RV1 was limited to just C and x86 assembly language, the company was hard at work on BASIC, FORTRAN, and Pascal compilers that would utilize common code generators allowing for the mixed-use of languages without losing optimization. With the introduction of GUIs on the Apple Lisa, Xerox systems, and VisiCorp’s Visi-On, Quantum also had plans for windowing as well. According to Quantum’s president Syd Geraghty in InfoWorld on the 21st of March in 1983, the majority of customers were high-end system developers at large corporations. Version 1.0 cost $650 in 1983 (around $2100 in 2025), and that included a C compiler, full-screen editor, the ability to read MS-DOS disks, and full networking support. I haven’t found much information about versions 1.1 through 1.14, but I did find some information about 1.20 released on the 15th of November in 1984. This version brought pattern matching on filenames in the current directory, expanded shell programming, &lt;code&gt;login&lt;/code&gt; was now a separate task with fast user switching and login stacking, &lt;code&gt;TCAP&lt;/code&gt; (think terminfo), &lt;code&gt;ed&lt;/code&gt; was rewritten and supported full-screen visual mode (think Vi), and support for the IBM AT (real-mode) was added. The price of QNX had also fallen to $450.&lt;/p&gt;
    &lt;p&gt;In June of 1981, the Ontario Ministry of Education identified computing as being important for the future, and they wanted to bring computing into their schools. They were also quite aware that some teachers had taken the initiative to bring microcomputers into their classrooms already, and the Commodore PET was the most common for programming courses, while the Apple II was the most common for other educational programs. Targeting many computers would have meant that they’d have rather high software development costs in any attempt to achieve standardization, and it was therefore decided that they’d need a single computer. In 1983, it was found by the ministry and the Canadian Advanced Technology Alliance that no existing computer would fully satisfy the goals of their educational computer. By March that year, some requirements had been drafted: all-in-one PET-like design, headphone output for voice and sound, a trackball, an 80186 CPU, a multitasking operating system, color graphics, voice synthesis, keyboard with accented characters, and networked storage (no physical disk in the computer itself). This machine as described had the sobriquet “bionic beaver.”&lt;/p&gt;
    &lt;p&gt;With the specifications in hand, Robert Arn at CATA created CEMCORP (Canadian Educational Microprocessor Corporation) and won a contract from the ministry for $10 million to develop the initial machines. This resulted in the ICON having been chosen. This machine was initially manufactured by Microtel and it ran QNX from Quantum Software Systems. The first machines were delivered in 1984. Later machines were produced, sold, and supported by Burroughs Canada, and after the merger with Sperry in 1986, by Unisys.&lt;/p&gt;
    &lt;p&gt;The ICON was built around an Intel 80186 clocked at 7.16MHz and 512K RAM. It lacked any local storage having neither a hard disk drive nor floppy disk drive. At boot, the computer grabbed QNX from a local LexICON file server over a 2.5Mbps ARCNET connection, and loaded the OS into RAM. Once loaded, the user logged into the system and his/her home directory was on the file server. Up to 32 of these machines could be on a single LAN. Saving any work to a floppy, meant putting the floppy into the file server, and then copying the file from the LexICON hard disk (early models were 10MB, later models were 64MB) to that floppy. The cost of these machines was high at $2500, but any school need only have paid $495 with the government covering the rest. One incredibly forward thinking feature was the lessonware. This would have been a hypertext system in which educators could have written pages that linked to others building an extensive corpus overtime. Even applications could have been run by simply clicking a link. This model was rejected by the ministry before the ICON shipped, and was replaced by a top-down system with ministry making lesson decisions. This also resulted in the ICON having shipped a QNX CLI with the CEMCORP text editor in the earliest models.&lt;/p&gt;
    &lt;p&gt;The ICON was a project hated by many and loved by many. For detractors, it was seen as expensive and wasteful while not exposing students to industry currents. For supporters, it accomplished all of its goals. It was excellent for programming, and it was excellent at multitasking, networking, and running educational software. The software was also quite reliable. It was QNX doing what QNX does best.&lt;/p&gt;
    &lt;p&gt;From students who used ICONs, we know that it did have educational games, text editors, compilers, word processors, spread sheets, circuit design and simulation software, and CAD software. Of course, being networked machines, some unconventional students figured out ways to hack into other machines over the network, print stuff to other students’ screens, and generally cause some chaos. Combined with audio capabilities (later models even included MIDI support), this apparently got a bit out of hand from time to time.&lt;/p&gt;
    &lt;p&gt;I normally wouldn’t show so many ads, but here is a development that is rather interesting. OS/2 had been announced on the 2nd of April in 1987, and Quantum perceived the OS as a real threat. The comparisons to UNIX were now joined by comparisons to OS/2, and QNX wanted to be certain that people understood QNX to be superior. This advertisement also shows us that QNX had responded to OS/2’s ability to run DOS software by adding that feature to QNX with the QDOS II (invoked as &lt;code&gt;QDOS&lt;/code&gt;) emulator, or by running a DOS application as a task via &lt;code&gt;RUNDOS&lt;/code&gt;. QNX had been ported to the IBM PS/2 as well. This was QNX version 2.&lt;/p&gt;
    &lt;p&gt;As far as I can tell, the release of QNX version 2 was announced on the release date of version 1.2. The release of this version appears to have been quite late, and it occurred in autumn of 1987 (two years after the initial release date given). This release brought protected-mode support for the IBM AT, full LAN support with some networking enhancements ported from BSD, support for files of up to one terabyte in size, up to 32 serial ports in one machine, and a somewhat primitive GUI called House about which I can find nothing but the name.&lt;/p&gt;
    &lt;p&gt;While I couldn’t find anything about the House graphical environment, QNX Windows running the Open Look Window Manager (OLWM) is available.&lt;/p&gt;
    &lt;p&gt;In June of 1987, Quantum Software Systems ceased renting their office space, and they moved into a building they’d had built just for them. Following this, the company would expand the building three times, and finally add another building. So, the company moved from 215 Stafford Road to 175 Terrance Mathews Crescent.&lt;/p&gt;
    &lt;p&gt;As late as 1990, QNX advertisements still mentioned performance on the 80286. This seems more as though Quantum didn’t spend much on marketing rather than not having progressed. In Dan Hildebrand’s An Architectural Overview of QNX from April of 1992, we find that the company had developed QNX versions up to 3.15, and articles about operating systems in the tech press had mentioned QNX as one of the systems that took advantage of features in the 80386.&lt;/p&gt;
    &lt;p&gt;In 1989, Quantum Software Systems began work on a dramatic overhaul of the operating system. This new version would be fully POSIX-compliant and increase performance over the prior generation of QNX operating systems. This version, 4.0, was released in 1991. The kernel now had just 14 calls associated with IPC, network, scheduling, and interrupts, and the kernel weighed in at just 7K (605 LOC), allowing the entire kernel to fit in CPU caches of the time. Unlike earlier versions, messages were no longer queued. Instead, they were copied from process to process. Being POSIX-compliant allowed for the easier porting of software, and it also meant that the directory hierarchy was decidedly more familiar to UNIX veterans. Beyond source compatibility, Quantum was actively working on becoming binary compatible with UNIX as of 1992. In 1994, beyond POSIX and performance, QNX 4.1 introduced the QNX Photon microGUI. This system was developed by Patrick Hayden and Robin Burgener. Much like the underlying system, it was built around a microkernel (around 20K), and it was network transparent. A Photon application could have its interface beamed to another QNX 4 machine at any point in time, or it could be dragged from one device to another just as easily. Photon likewise allowed remote monitoring or control of the user interface. This worked regardless of the device class (desktop, laptop, handheld, server). For those who needed it, the X Windows System (X11R5, Motif Window Manager) was also available, though Photon did implement a binary interface library that was X compatible. Being so lightweight allowed the company to release a demo disk that combined networking, a web browser, web server, graphical environment, file manager, text editor, a vector animation demo, and Towers of Hanoi game onto a single 1.44MB floppy. Unlike prior QNX versions, version 4 required at least an Intel 80386 and VGA graphics card. No 16bit systems were supported.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;KANATA, ONTARIO, September, 1994—QNX Software Systems Ltd., developers of the QNX realtime operating system, announced a unique window system targeted for handheld and embedded applications.&lt;/p&gt;&lt;lb/&gt;According to Rob Oakley, Corporate Communications and Product Management, "the Photon Window System is the first of its kind—a GUI built around a graphical microkernel."&lt;lb/&gt;QNX Software Systems designed the Photon Window System as a graphical microkernel and a team of cooperating processes, basing this design on the company's QNX OS, a microkernel network-distributed system.&lt;lb/&gt;Photon's cooperating processes provide the functionality to scale the system up into a full-featured windowing system or down to fit into resource-constrained environments, like handheld personal computers (HPCs) and compact embedded systems.&lt;lb/&gt;Photon provides a rich widget library that operates much like the X Window System widget set, with an X-inspired API. A Motif-like window manager and a code-generating, visual application builder are also available.&lt;lb/&gt;"Photon is extremely light and fast. It runs in only 256K, yet provides enormous GUI functionality," Oakley said.&lt;lb/&gt;Like the QNX OS itself, Photon is network transparent—an HPC running Photon and QNX, equipped with a wireless LAN interface, becomes a transparent extension of the LAN, able to use all the LAN's resources as if they were integrated directly into the HPC. The power this brings to the HPC user is difficult to appreciate—imagine having the power of 100 Pentiums in the palm of your hand!&lt;lb/&gt;According to Dan Dodge, Vice President R&amp;amp;D, "Photon applications are very network distributed. From the application's perspective, all the resources of all the nodes on the LAN look like a single, logical machine. The environment is so transparent that a user can drag applications from one physical screen to another."&lt;lb/&gt;For example, a user in a factory control environment could walk up to a computer and drag an application from the control screen onto an HPC, and then walk out onto the factory floor with it and interact with the live application.&lt;lb/&gt;Although Photon is aimed at compact environments, its dynamic range is extensive. "Photon's API and rich widget library can support high-performance GUI applications with enough functionality to enter the domain of X, while consuming only a fraction of the resources," said Dodge.&lt;lb/&gt;The QNX operating system is a POSIX-certified realtime OS for Intel and AMD processors. Scalable and modular, QNX fits a wide range of environments, from compact embedded controllers to resource-rich X-based development systems, to distributed realtime systems running hundreds of CPUs.&lt;/quote&gt;
    &lt;p&gt;Versions 4.2, 4.22, and 4.24 all released in 1995. The final version 4 release was 4.25 in 1997. At least one QNX 4 installation ran for over 20 years without a reboot at the ESA. This was possible because peripherals could be hotswapped, drivers could be changed, and network nodes could be added or removed without bringing the system down.&lt;/p&gt;
    &lt;p&gt;Notably, we see that in 1994, Quantum renamed itself to QNX Software Systems Limited. And with a new name and a new version of their operating system, the company won some major installs. From POS systems at FasFax that allowed for real-time sales figures from geographically disparate locations, to video conferencing systems at Georgia State University, to factories, power plants, hospitals, set-top boxes, phone systems, trains, jets, the Space Shuttle, ISPs, and even traffic lights. The price for a single license dropped to around $285 at this time, and by 1995, QNX was the leading real-time OS for x86 systems. The majority of the company’s revenue was from large enterprises.&lt;/p&gt;
    &lt;p&gt;Of course, change was coming in the 1990s, and QSSL knew it. The company took the QNX kernel from version 4.24 and forked it. They had multiple goals with this fork. The system needed to be SMP capable, support POSIX, and be more portable to new hardware. The kernel handled only IPC, message passing, interrupts, and timing. Threading became the minimal unit of scheduling. The new Process Manager then used a loader thread that copied a process’s image into memory freeing the Manager to service other requests while a program continued to load. Naturally, being a real-time system, priority levels were used when scheduling any time-critical process, and new processes inherited the priority of their parent by default. The Process Manager weighed in at 32K (same size as the kernel itself) but added memory allocation, process contexts, resource-manager namespaces, and so on. In this new QNX version, the Process Manager ran inside the microkernel’s address space, but was the only element of the OS to do so. Much of the network stack for this version came from NetBSD, and with that came the ability to use NetBSD network drivers. There was another major change that came from the wider UNIX world, GCC. This naturally meant that language support was quite broadened to include not just C, C++ but all of the other languages supported by the GNU Compiler Collection. This became QNX Neutrino 1.0 released in 1996.&lt;/p&gt;
    &lt;p&gt;On the 19th of October in 1998, QSSL announced QNX Neutrino 2.0 which featured UPM (Universal Process Model). In the words of CTO Dan Dodge:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The premise of UPM is simple. Go beyond the limited MMU protection provided by the other major embedded OSs - where only applications are prevented from corrupting memory - and extend that protection down to services at the kernel level. The result? For the first time, MIPS and PowerPC-based embedded systems can intelligently recover from software faults in drivers, protocol stacks, and custom OS extensions - typically without rebooting.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;QNX was branching into non x86 platforms, and this included PowerPC processors: 401, 403GC, 603e, 821, 823, 860; MIPS processors R4000 and R5000; and naturally all x86 CPUs from the 80386 onward. At this stage, however, the development environment was restricted to QNX 4 and Windows 95/98/NT.&lt;/p&gt;
    &lt;p&gt;This announcement was followed by another about a partnership with Amiga:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Cologne, Germany, November 13 - Amiga Inc. today announced a partnership with QNX Software Systems Ltd. to utilize the QNX realtime operating system (RTOS) as the foundation for the next-generation Amiga architecture. The announcement was made at Computer '98 in Cologne.&lt;/p&gt;
      &lt;p&gt;"The Amiga shook the industry in the 80s with world-leading multimedia architecture," said Jeff Schindler, general manager of Amiga Inc. "QNX's RTOS resembles many of Amiga's unique qualities. It provides the foundation in reaching our vision for the rebirth of Amiga in the new millenium."&lt;/p&gt;
      &lt;p&gt;"We see this partnership as a powerful combination of superior OS technologies, common corporate cultures, and shared business vision," said Dan Dodge, Chief Technology Officer and Cofounder of QNX Software Systems Ltd.&lt;/p&gt;
      &lt;p&gt;About Amiga&lt;/p&gt;
      &lt;p&gt;Amiga Inc. is a technology company targeting the next generation of Amiga architecture with a continued focus on multimedia and the Internet. Since the introduction of the Amiga A1000 in 1985, Amiga has represented the embodiment of the efficient use of memory and hard drive capacity, while pioneering industry developments in multimedia, 32-bit multitasking, and autoconfiguration. Amiga led the industry in combining computer graphics, animation, and film sequences with stereo sound known today as multimedia. Visit http://www.amiga.com and http://www.amiga.de.&lt;/p&gt;
      &lt;p&gt;About QSSL&lt;/p&gt;
      &lt;p&gt;Founded in 1980, QNX Software Systems is one of the top three realtime operating-system vendors in the world, with products licensed in more than a million systems worldwide. The company has established a strong customer base in a variety of industries, including aerospace, telecommunications, medical instrumentation, process control, point-of-sale, consumer electronics, finance, and telephony. With products distributed in over 100 countries, the company is headquartered in Ottawa, Canada.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Amiga port should have been somewhat straightforward considering that Amiga accelerators had been using PowerPC chips, and those chips were now supported by QNX. Gateway’s Amiga team was working closely with QSSL to build a new Amiga (Amiga NG) around the PowerPC G3 and G4 chips running QNX, and these were apparently prototyped as single, dual, and quad processor machines. During alpha testing, Gateway PowerPC boards apparently had some issues, and the two parties blamed one another. By the middle of 1999, Gateway, QSSL, and to some extent Motorola, had poured a hefty sum into the project, and Gateway began insisting on a solid date for the availability of a QNX Neutrino port. Evidently they weren’t satisfied, and I do not believe communication between the two teams, which had one been quite good, was solid by this point. At noon on the 8th of July in 1999, Dan Dodge announced the QNX Developers Network for Amigans. This was followed by another announcement at 15:15 the following day:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Eight months ago we were chosen by Amiga as their foundation OS partner. Our development group was thrilled to be part of the rebirth of such an innovative product. To meet the challenge we knew it would take a tremendous effort on our part. We had a team of people in place working on our part of the Amiga NG soon after the alliance was announced. Over the next few months we involved more and more of our engineering resouces towards making QNX an advanced multi-media platform. Our investment so far has been significant. These are costs we have borne ourselves.&lt;/p&gt;
      &lt;p&gt;It is clear today from Jim's letter that we were not chosen for the next generation Amiga. Naturally we're disappointed. So, where do we stand now? It is not our intent to confuse the Amiga community. We are proud of what we have accomplished and want to include Amigans in what we've achieved. I did make a promise to deliver an operating system and I intend on keeping that promise. I don't want to split the community, nor do I wish to engage in a war of words. I don't ask you to "trust" me or to take me at my word. Both QNX and Amiga have promised to deliver technology into your hands in the very near future. I ask only that your assessment of QNX be based on what we do and what we deliver.&lt;/p&gt;
      &lt;p&gt;Thanks for the overwhelming support we have received so far.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That letter by Jim Collas read, in part:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Dear Amigans,&lt;/p&gt;&lt;p&gt;After months of research and in-depth discussions with all of our technology partners we have decided to use Linux as the primary OS kernel for the new Amiga Operating Environment (OE). I know this decision is a shock to many of you given the previous announcements and activities relative to QNX. This was a very complicated and difficult decision to make and I assure you that I didn't make this decision without a significant amount of research and deliberation. We have been researching Linux since February but didn't finalized our decision until several weeks ago. We were planning to communicate it to the Amiga community in the technology brief that will be released in the next few days.&lt;/p&gt;&lt;p&gt;I am pressed to communicate the Linux decision before the technology brief because of information released by QNX in the last few days. This information had not been reviewed or approved for release by Amiga. In light of our Linux decision, this information is confusing and misleading so I would like to take the time to clarify the situation. I can't disclose any details of the Amiga/QNX discussions because of legally binding confidentiality agreements but I can talk to you about our decision to use the Linux kernel. I think that you will agree that this is the right decision once you understand the reasons for this decision.&lt;/p&gt;&lt;p&gt;Before I continue, I should mention that our technology decision does not reflect negatively on QNX. I believe that QNX is a good company with great technology. I just believe that Linux gives us a better chance of executing our plans successfully. The decision to use QNX as our OS partner on our next generation multimedia convergence computer (MCC) was made late last year. When I took over as president of Amiga in February of this year, I initiated an in-depth review of existing Amiga plans and decisions. As president of Amiga I had to make sure that we were defining a strategy and an execution plan that would allow Amiga and the Amiga community to be successful. We reviewed our strategy, architecture decisions, technology partners, and execution plans. During this review period we also added a number of very talented and experienced people to help us finalize our technology and product decisions. I am confident that we now have a solid and exciting plan that people can have confidence in.&lt;/p&gt;&lt;p&gt;Linux has been picking up substantial momentum over the past year as a viable, open OS alternative in the marketplace. This momentum, the growing commitment to Linux applications from a wide variety of software vendors, and the growing availability of Linux device drivers from hardware vendors, makes it a compelling candidate. Additionally, with all of the significant component suppliers putting resources on writing drivers for Linux it was difficult to get them to port to yet another operating system. Using the Linux OS as a foundation for our Amiga OE allows us to leverage a significant amount of available software drivers and utilities. This allows us to quickly support multiple graphics cards and other peripherals.&lt;/p&gt;&lt;p&gt;Given the above-mentioned advantages, we decided to do an in-depth technical analysis of Linux to determine if it was a suitable OS kernel for our new Amiga operating environment (OE). As we ported parts of our higher level operating environment and AmigaObject architecture to Linux, we discovered some significant performance advantages in the Linux kernel in areas such as distributed object messaging across a network (up to 10X the performance of Windows NT).&lt;/p&gt;&lt;p&gt;Does this mean that the next generation Amiga will not be unique? Absolutely not! Remember that the OS kernel is only one component of the new Amiga OE and the hardware is unique. The revolutionary nature of the Amiga OE is in the way it extends the traditional operating system to provide a host environment for a new class of portable applications - applications that exist in a pervasive networked computing environment. We will be integrating multiple technologies including an efficient windowing environment and a unique user interface. In summary, we decided to use Linux because of the incredible momentum and the fact that it is solid technology and a good foundation for our new Amiga OE.&lt;/p&gt;&lt;p&gt;Additionally, the Linux community is an impressive force that we should be aligned with. We share many common values and objectives with the Linux community. Using Linux as our OS kernel allows us to build a unique and revolutionary operating environment while leveraging the enormous momentum of Linux. The soon to be released technology brief will further explain our architecture and plans for integrating all of the selected technology. Once you read it, I am confident that you will understand the revolutionary nature of the next generation Amiga. I assure you that Amiga and the Amiga community will be a driving force behind the next computer&lt;/p&gt;&lt;lb/&gt;revolution.&lt;/quote&gt;
    &lt;p&gt;As a person using Linux at the time, I believe this to have been the wrong decision. Despite the momentum that Linux had, it wasn’t (still isn’t) as stable, as reliable, or as efficient as QNX. If network performance were a serious consideration, one of the BSDs would have been the better choice. Linux’s hardware support also wasn’t that great in reality. While it could run on quite a bit of kit, it didn’t always support that hardware well, and it didn’t always support all features. Plus, QNX was doing the work to build drivers for the new Amiga. Of course, none of this really mattered. Gateway chose to divest itself of Amiga entirely. The new Amiga Inc. then turned to AmigaOne Partners for Amiga OS 4.&lt;/p&gt;
    &lt;p&gt;QNX Neutrino 2.1 was released in 1999 with support for Java, the Glide API, a wide array of microcontrollers, ARM, StrongARM, and Hitachi SH-4. Interestingly, this release had beta packages including RealPlayer and X in Photon, and it had experimental packages that included Quake 3 Arena and Doom.&lt;/p&gt;
    &lt;p&gt;On the 14th of September in 1999, QNX made an announcement that would shape the future of QNX. The company was partnering with Motorola to develop automobile driver information systems that included in-vehicle navigation, internet access, natural language processing, car audio, multimedia, and vehicle information dashboards. While the Motorola unit responsible for mobileGT wouldn’t last and the unit at IBM working on Java wouldn’t last, QNX would survive and thrive in that segment.&lt;/p&gt;
    &lt;p&gt;QNX version 6 was released on the 18th of January in 2001. The new version was focused on multimedia with streaming video and audio as well as hardware accelerated MPEG encode/decode. The new system included a web based package manager greatly easing the installation of available software. Thankfully, all supported architectures could now be used for developing QNX native software too. Version 6.1 was mostly a patch release and followed later the same year. QSSL was a founding member of the Eclipse Foundation, and QNX software development got quite a bit better with the release of the Momentics Tool Suite on the 4th of June in 2002 (along with QNX 6.2). This was largely the Eclipse IDE combined with a series of plugins that were QNX and Photon oriented.&lt;/p&gt;
    &lt;p&gt;The last release of QNX by QSSL was version 6.3 on the 3rd of June in 2004. This version was visually slightly different, and Voyager was replaced by the Mozilla Suite. The development environment was improved and now offered a clustering framework for the development of networked applications utilizing distributed processing. Among the highlights for this release were SCTP support, IP filter and NAT support, IPv6 support, 2D and 3D graphics layering/compositing, full UTF8 support in Photon, USB2 host support, and support for up to 64GB of RAM on x86 and PPC, up to 1TB on MIPS.&lt;/p&gt;
    &lt;p&gt;On the 27th of October in 2004, QNX Software Systems Limited was purchased by Harman International Industries. Harman specifically wanted to focus on QNX Neutrino in the embedded market, and within that market, specifically on automotive applications where Harman had found a market in audio. Under Harman’s ownership, QNX operated as a separate division led by Dan Dodge as CEO. While QNX did continue to serve networking, medical, and industrial markets, the direction was clear. What had begun with the Motorola partnership in automotive would become the primary market.&lt;/p&gt;
    &lt;p&gt;QNX development continued with 6.3 SP1, SP2, and SP3. Version 6.3.2 was released on the 16th of August in 2006, 6.4 on the 30th of October in 2008, and 6.4.1 in May of 2009. Throughout that time period, QNX had introduced support for Adobe Flash and developed the QNX CAR platform winning a trophy from Adobe for their efforts. This platform was built of modular components allowing manufacturers to mix and match based upon the market segment. QNX was chosen by companies like BMW, Mercedes, Dodge, Toyota, Volkswagen, and Audi. When QNX demoed their automotive systems in the 2007-2009 timeframe, they had concept dashboards. These all ran QNX Neutrino on ARM CPUs (often Freescale i.MX6 or TI Sitara [Cortex A8]) with the EtherCAT motion library, and many demo units had UIs created in Qt5 and QML while a few had hardware accelerated OpenGL interfaces. From 2008 to 2010, QNX had been licensed for use in more than 17 million in-vehicle systems representing an increase of around 130% over those two years. By March of 2010, more than 200 vehicles were already shipping with QNX, and the QNX CAR platform had more than 60 participants. Those participants included 17 auto makers and 26 automotive suppliers.&lt;/p&gt;
    &lt;p&gt;On the 9th of April in 2010, Research in Motion announced their acquisition of QSS from Harman for $200 million:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“RIM is excited about the planned acquisition of QNX Software Systems and we look forward to ongoing collaboration between Harman, QNX and RIM to further integrate and enhance the user experience between smartphones and in-vehicle audio and infotainment systems," said Mike Lazaridis, President and Co-CEO at RIM. "In addition to our interests in expanding the opportunities for QNX in the automotive sector and other markets, we believe the planned acquisition of QNX will also bring other value to RIM in terms of supporting certain unannounced product plans for intelligent peripherals, adding valuable intellectual property to RIM's portfolio and providing long-term synergies for the companies based on the significant and complementary OS expertise that exists within the RIM and QNX teams today."&lt;/p&gt;
      &lt;p&gt;"We welcome the opportunities that a strengthened relationship with RIM will create, as two innovation leaders collaborate to bring new connectivity solutions to the industry," said Dinesh C. Paliwal, Harman's Chairman, President and CEO. "We expect to maintain our close association with QNX and the cutting-edge software solutions it provides to Harman and our customers. We believe our leading customers will fully endorse this move and see it as a major step in advancing seamless connectivity and integration among intelligent devices."&lt;/p&gt;
      &lt;p&gt;"Like Harman, RIM shares our passion for innovation and reliability, so we are absolutely thrilled with this opportunity," said Dan Dodge, CEO, QNX Software Systems. "Moreover, RIM will give us the best of all possible mandates: to continue on our innovation path and to increase investment in our core products, professional services, and go-to-market channels. This is a great time to be a QNX customer, as we focus on collaborating with RIM to create an even more exciting platform for the next generation of connected and embedded devices."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Also in 2010, QNX gained the QNX Safety kernel variant. This was a version of Neutrino that was security hardened specifically for mission critical applications. This variant continues to this day with the most recent version (8.0) having been independently certified by TÜV Rheinland to meet several standards including ISO 26262 ASIL D, IEC 61508 SIL3, IEC 62304 Class C, and ISO/SAE 21434. Aside from security hardening, the QNX Safety variant is still fully compatible with Neutrino’s native APIs and POSIX.&lt;/p&gt;
    &lt;p&gt;In July of 2010, QNX Neutrino 6.5 was released. This version brought performance improvements to the kernel when systems were seeing high memory utilization, the kernel gained zombie reaping, and it gained address space randomization. SMP support was increased, and CPU support was extended to ARMv7 Cortex A-9. The Photon microGUI saw some refinements. As one would expect, version updates were present for everything imported from BSD, Linux, and GNU. This version could make use of the NetBSD’s Pkgsrc tool.&lt;/p&gt;
    &lt;p&gt;Version 6.5 was forked to create both the BlackBerry Tablet OS and BBX shortly after its creation. The first device to see a QNX-derived operating system from RIM was the PlayBook, which featured an OMAP 4430 SoC (1.5 GHz dual-core A9), PowerVR SGX540 GPU, 1GB of RAM, 16GB of eMMC flash, a 1024 by 600 seven inch LCD, Bluetooth, 802.11n, USB2, micro HDMI, a 5MP rear camera, and a 3MP front camera. It measured 5.1 inches by 7.6 inches, was about 2/5 of inch thick, and it weighed just under a pound.&lt;/p&gt;
    &lt;p&gt;The PlayBook was released on the 19th of April in 2011 to mixed reviews. While many loved the webkit browser, user interface, HDMI output, and multitasking, many loathed the requirement of a BlackBerry to get certain apps working. Additionally, there was a dearth of third party applications. This latter complaint did get ameliorated. While at launch there weren’t too many applications, this grew to over 24,000 by the same time the following year. Around 2,465,000 PlayBooks had been sold by June of 2013.&lt;/p&gt;
    &lt;p&gt;The BlackBerry Z10 was released on the 31st of January in 2013 running BBX (officially BlackBerry 10 due to a trademark dispute, and at the launch event for BBX, Research in Motion announced that they were changing their name to BlackBerry Limited). The Z10 was built around a Qualcomm Snapdragon S4 Plus SoC (dual core 1.5GHz Krait CPU, Adreno 225 GPU) for LTE units, or around the TI OMAP 4470 for non-LTE units. The shell was plastic wrapped around a stainless steel inner frame, and the on/off, voice command, and volume buttons on the right side were of metal. While it didn’t have quite the premium feel of an iPhone, it did feel good in the hand. In its dimensions it was 5.1 inches by 2.6 inches, and just over an 1/3 of an inch thick (or just slightly larger than an iPhone 5). It was a slick piece of kit with a high price for the time at $599. The display, however, was excellent. It was a 4.2 inch LCD with a resolution of 1280 by 768 at 355ppi (the iPhone 5 was 326ppi). The device had a 2MP font camera, and an 8MP rear camera capable of HDR, panorama, and 1080p video at 30fps. Wi-Fi was dual band 802.11n, and the device featured Bluetooth, GPS, and NFC. Of course, connectivity didn’t stop there. This device had physical ports: micro USB2, micro HDMI, and 3.5mm audio.&lt;/p&gt;
    &lt;p&gt;BBX made heavy use of gestures with a swipe up from the bottom taking the user to the Home Screen, a swipe to right to hit the App Library, and a swipe to the left going to the BlackBerry Hub. The Hub was a combination of SMS/MMS, email, social media, chat, notifications, and calls in a single unified location. BBX was QNX Neutrino, but it did differ. Multitasking was limited to 8 applications at any one time which I believe to have been done due to the application frameworks. A developer could choose to use C/C++ and the Cascades UI framework, or WebWorks which utilized HTML5 with Zepto.js (JQuery API, but 8.4k compressed), or WebKit, or Adobe AIR (Flash), or Android runtime. With so many different application types, decisions would have had to have been made around resource management, and a best guess at when performance would become unacceptable.&lt;/p&gt;
    &lt;p&gt;BlackBerry had been unable to compete against the iPhone and Android, and BBX was their last, best hope. By 2014, BBX was in the number four spot behind Windows Phone. By 2017, it was clear that they weren’t going to survive in the mobile market. Due to the extreme devotion of their fans, they kept BBX on life support until 2022. Being an amazing OS running on good hardware, why did BBX fail? Likely, the most pressing problem was application support. While BBX could run some Android applications, support was limited. The platform likewise failed to grab many developers as the existing install bases for iOS and Android were enormous. What applications were made for BBX were often of quite low effort. Finally, moving to a touch screen angered BlackBerry’s existing fanbase. For those individuals hanging on to the BlackBerry ecosystem, the keyboard was one of the main reasons why. Removing the physical keyboard made many of those fans feel betrayed. When BlackBerry Limited did release another phone with a physical keyboard, it was a bit too late.&lt;/p&gt;
    &lt;p&gt;On the 20th of September in 2013, BlackBerry Limited announced a 4500 person staff reduction and $1 billion (CAD) loss. On the 23rd, they announced an acquisition by Fairfax Financial Holdings for $9/share. This deal was canceled in November. Instead, John Chen became CEO and initiated a turn around that focused on QNX’s former markets of healthcare, finance, law, and mission critical systems. This focus allowed the company to pick up Ford Motor Company as a QNX customer on the 11th of December in 2014 (Ford had previously used Microsoft Auto).&lt;/p&gt;
    &lt;p&gt;On the 28th of February in 2014, BlackBerry released QNX 6.6. The supported platforms were now the expected x86 and ARM CPUs with no mention of any others. This was a major change despite being a point release. Photon support was removed in favor of the Screen Graphics Subsystem. Screen operates as a lower-level service, and this has the benefit of supporting off-screen rendering and compositing of various image sources, and as QNX software had been increasingly using Qt, HTML5, or OpenGL rather than the toolkits supplied with Photon, this made logical sense.&lt;/p&gt;
    &lt;p&gt;QNX version 7 was released on the 4th of January in 2017 for ARM v7, ARM v8, x86, and AMD64. This release featured a rewritten PCI server with APIs moved out of libc and into libpci, rewritten virtual memory manager, fewer synchronization objects with increased limits, and filesystem encryption was moved into the Encrypted Filesystem package available from QNX Software Centry.&lt;/p&gt;
    &lt;p&gt;By June of 2023, QNX was in over 255 million vehicles around the world, and this would explain why the BlackBerry blog featured a rather large section on automobiles:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The automotive evolution to SDVs and “connected cars” requires an OS capable of speed, safety, and security — while unlocking the power to innovate.&lt;/p&gt;
      &lt;p&gt;"With more than 300 million vehicles capable of over-the-air software updates expected to be on the road globally by 2032, automakers are clamoring for better tools to help them develop compelling technology features in the software-defined vehicle," says Alex Oyler, director of North America at SBD Automotive, a leading global automotive technology research and consulting firm.&lt;/p&gt;
      &lt;p&gt;“Both automakers and suppliers rely on validated software and well-integrated development tools to help them more efficiently build and maintain differentiating software for their fleets,” Oyler adds. "A secured-by-design operating system such as the next generation QNX OS — that seamlessly integrates with other software components on a high-performance system-on-chip — represents the foundation of a safe, secure, and seamless experience for drivers.”&lt;/p&gt;
      &lt;p&gt;In addition, early reviews of the new QNX SDP 8.0 give automotive industry leaders a glimpse into what’s possible.&lt;/p&gt;
      &lt;p&gt;“The combination of our DRIVE Thor centralized computer and the new QNX OS will serve as a powerful foundation on which OEMs can build next-generation automotive systems that offer the highest levels of safety and security,” says Ali Kani, vice president and general manager of automotive at NVIDIA. “This represents another major milestone in a nearly 20-year collaboration with BlackBerry QNX that has helped both companies move to the forefront of the automotive industry.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;QNX 8.0 was officially announced in December of 2023, and the release was made in January of 2024. Version 8.0 was quickly discontinued with 8.0.1 taking its place. Version 8.0.3 was made available on the 21st of March in 2024. This latest release is available for a variety of Aarch64 platforms including the Raspberry Pi, and is also available for AMD64. QNX 8 supports SoCs with up to 64 cores and has near linear performance scaling. The network stack is now based upon FreeBSD 13.2, Wi-Fi 6 support is present with WPA3 and TLS 1.3, Screen can operate fully headless and now supports Vulkan 1.3 and OpenCL 3, and Screen now supports Wayland 1.21. Developers are now encouraged to use LLVM and libc++ 16 though GCC is still available with libstdc++ 12.2. Python 3.11, valgrind, libasan (address sanitizer), libubsan (undefined behavior detection), and libunwind are all available. For the UNIX user land, Toybox has replaced many common GNU utilities.&lt;/p&gt;
    &lt;p&gt;If the Raspberry Pi port caught your attention, this is available free for non-commercial use via QNX Everywhere. The image requires a Raspberry Pi 4 with at least 2GB of RAM and an 8GB or greater MicroSD card.&lt;/p&gt;
    &lt;p&gt;On the 2nd of January in 2025, it was announced that BlackBerry IoT would now be known as QNX. This decision was made largely by BlackBerry responding to their customers who recognized and desired the QNX brand. QNX CEO John Giamatteo stated:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Relaunching the QNX brand is an important step in BlackBerry’s broader strategy to increase our visibility and fortify our leadership within the automotive and embedded industries, with a view to better positioning us for sustained growth and success. The values that QNX stands for have always been a cornerstone for our customers and this brand relaunch honors that strong history while setting the stage for the division to fire on all cylinders and drive smarter, safer, and faster innovation through precision-engineered performance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;QNX is a fascinating operating system. It was extremely well designed from the start, and while it has been rewritten, the core ideas that allowed it survive for 45 years persist to this day. While I am sad that Photon was deprecated, the reasoning is sound. Most vendors using QNX either do not require a GUI, or they implement their own. For example, while Boston Dynamics uses QNX in their robots, they don’t really need Photon, and neither do SpaceX’s Falcon rockets. While cars certainly have displays, most vehicle makers desire their screen interfaces to have a unique look and feel. Of course, just stating these use cases of robots, rockets, and cars speaks to the incredible reliability and versatility of QNX. Better operating systems are possible, and QNX proves it.&lt;/p&gt;
    &lt;p&gt;My dear readers, many of you worked at, ran, or even founded the companies I cover here on ARF, and some of you were present at those companies for the time periods I cover. A few of you have been mentioned by name. All corrections to the record are sincerely welcome, and I would love any additional insights, corrections, or feedback. Please feel free to leave a comment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.abortretry.fail/p/the-qnx-operating-system"/><published>2025-10-05T14:47:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482106</id><title>The Demonization of DeepSeek: How NIST Turned Open Science into a Security Scare</title><updated>2025-10-05T18:12:18.065651+00:00</updated><content/><link href="https://erichartford.com/the-demonization-of-deepseek"/><published>2025-10-05T15:12:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482333</id><title>Show HN: ASCII Drawing Board</title><updated>2025-10-05T18:12:17.799903+00:00</updated><content>&lt;doc fingerprint="f65ec2ddcce25e37"&gt;
  &lt;main&gt;
    &lt;p&gt;Use the List of Unicode characters as a source of characters for your brush ✦ ◒ ▜ █▓▒░ Unfortunately not all of them will work due to font limitations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.delopsu.com/draw.html"/><published>2025-10-05T15:36:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482467</id><title>NFS at 40 – Remembering the Sun Microsystems Network File System</title><updated>2025-10-05T18:12:16.042177+00:00</updated><content>&lt;doc fingerprint="2f90b3e32bf72b79"&gt;
  &lt;main&gt;
    &lt;p&gt;This website gathers material related to the Sun Microsystems Network File System, a project that began in 1983 and remains a fundamental technology for today’s distributed computer systems.&lt;/p&gt;
    &lt;p&gt;The occasion which prompted this project was the ~40th anniversary of NFS, celebrated in September 2025 at the MSST Conference in Santa Clara, CA.&lt;/p&gt;
    &lt;p&gt;The core of the collection is design documents, white papers, engineering specifications, conference and journal papers, and standards material. However it also covers marketing materials, trade press, advertising, books, “swag”, and personal ephemera. We’re always looking for new contributions.&lt;/p&gt;
    &lt;p&gt;We’ve organized the material in four sections:&lt;/p&gt;
    &lt;p&gt;Unless otherwise noted, everything is downloadable from this site.&lt;/p&gt;
    &lt;p&gt;A full list of the Internet RFCs related to NFS can be found here.&lt;/p&gt;
    &lt;p&gt;There is also a site, nfsv4bat.org, which seems to include a variety of materials related to NFS after 1995, especially Connectathons. However, be careful: the site is insecure, load times are insanely slow, and it is unclear whether it’s still being maintained.&lt;/p&gt;
    &lt;p&gt;This website was created with the help of (alphabetically) Russel Berg, Russ Cox, Steve Kleiman, Bob Lyon, Tom Lyon, Joseph Moran, Brian Pawlowski, David Rosenthal, and Kate Stout. Please send any comments or suggestions to me, Geoff Arnold, via email. Last updated .&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nfs40.online/"/><published>2025-10-05T15:49:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482484</id><title>If the University of Chicago won't defend the humanities, who will?</title><updated>2025-10-05T18:12:15.913488+00:00</updated><content>&lt;doc fingerprint="76417213ac8b1428"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If the University of Chicago Won’t Defend the Humanities, Who Will?&lt;/head&gt;
    &lt;p&gt;Why it matters that the University of Chicago is pausing admissions to doctoral programs in literature, the arts, and languages&lt;/p&gt;
    &lt;p&gt;Listen to more stories on the Noa app.&lt;/p&gt;
    &lt;p&gt;Updated at 1:15 p.m. ET on September 12, 2025&lt;/p&gt;
    &lt;p&gt;The Rockefeller Center Christmas Tree was lit, COVID-19 was still a mysterious respiratory illness in Wuhan, and I was a Ph.D. candidate in a dying field: comparative literature. I was getting ready to Zoom interview for a tenure-track job near Boston that I almost certainly wouldn’t get (and didn’t). Sardined with me in a Greenwich Village coffee shop in December 2019, one of my faculty mentors talked me through, for the thousandth time, the questions I should expect the hiring committee to ask me and dispensed advice about how I should answer them. Then we walked back to his office, lined in handsome foreign-language editions of various novels and works of philosophy, where I would sit for the interview. There, he offered a final piece of wisdom: “Don’t be nervous. It’s just Harvard,” he said, grinning. “It’s not like it’s Chicago.”&lt;/p&gt;
    &lt;p&gt;A joke, but not entirely. For as long as I can remember, and certainly much longer than that, the University of Chicago has been widely viewed as the destination for humanities students and scholars. Some other elite schools might have the coveted Ivy League branding, or a few more famous faculty members, or a couple more dollars to tack onto the salaries of its professors and graduate students. But perhaps nowhere is the study of literature, philosophy, the arts, and languages more valued, their spirit more authentically preserved, their frontiers more doggedly pursued, than at Chicago. The university has had several household names on its humanities faculty, including the firebrand critic Allan Bloom, the novelist Saul Bellow, and the ethicist Martha Nussbaum, as well as scholars who may be less well known to the general public but whose work has been deeply influential in their fields, including the brilliant literary critic Sianne Ngai and Fred Donner, the pathbreaking and Guggenheim-winning historian of early Islam. In short, Chicago is a place for scholars’ scholars. At least, that’s the reputation. And Chicago’s reputation is no doubt why, when the university announced recently that it was reducing Ph.D. admissions for seven departments—among them art history and English language and literature—and outright freezing admissions to others, including classics, the decision was met, in some quarters, with fury and disbelief. “Chicago!” as one stunned academic friend put it in a text to me.&lt;/p&gt;
    &lt;p&gt;In an August 12 email to faculty, Deborah Nelson, Chicago’s arts and humanities dean, said that the changes were necessitated by “this moment of uncertainty” and “evolving fiscal realities.” These bits of bureaucratese appear to be allusions to both the Trump administration’s war on higher education and Chicago’s homegrown financial troubles, which include an eye-popping $6.3 billion in debt. “To be anything but cautious at this moment,” the dean’s email continued, “would be irresponsible.”&lt;/p&gt;
    &lt;p&gt;Chicago’s social-sciences division has also announced doctoral-admissions pauses, primarily in humanistic-leaning programs such as anthropology and social thought, where towering figures including the philosopher Hannah Arendt once taught. What’s happening at Chicago is a particular gut-punch to the humanities, not just at the university itself, but nationally and even globally. The school is, as the classics professor Catherine Kearns put it in a message to me, “a singular center for the pursuit of humanistic knowledge and intellectual growth.” Of the nearly 30 Chicago humanities professors I spoke with for this article, many emphasized that the stakes are much higher than the fate of prospective graduate students or the professors who might teach them. Chicago has long helped to keep alive tiny fields and esoteric areas of humanistic study, particularly in the languages. Without the university’s support, and the continued training of graduate students who can keep these bodies of knowledge going, entire spheres of human learning might eventually blink out.&lt;/p&gt;
    &lt;p&gt;Of course, some might view these comments as self-serving complaints. But the primary fears of the people I spoke with were not about their own careers or futures, but instead about their fields—about knowledge that, once lost, cannot be easily regained. “If you allow a field to die, there’s a loss to something like humanity,” Clifford Ando, a Chicago classicist who has been outspoken about the administration’s maneuvers, told me. “There’s also a real practical risk that a field simply cannot be re-created just because you have books.” I heard this sentiment echoed over and over. “If we stop producing people who are trained or educated to help undergraduates understand the most important things thought or written or painted in human history,” the renowned philosopher Robert Pippin said, “we might not be able to recover that.” Elaine Hadley, an emerita professor of English, told me, “Part of what we do is we’re conservators, keeping a body of knowledge going. We want to innovate and we want to think new things about it, and, you know, we want to make it relevant to the present day, but we’re also trying to keep this knowledge alive.”&lt;/p&gt;
    &lt;p&gt;These responses emphasize the cultural costs of shrinking the number of people trained in humanities fields, rather than focusing on the question of whether universities should be calibrating the production of Ph.D.s to the academic job market. No one I spoke to was insensitive to the pressures their grad students face when confronting the vanishing opportunities for tenure-track employment. But the professors also seemed reluctant to define the success of a program by how many professors it creates—after all, most humanities PhD students at Chicago do not pay tuition and receive stipends to cover their living costs, and getting paid to learn and read is not the worst fate.&lt;/p&gt;
    &lt;p&gt;These faculty perspectives also stood in stark contrast with the reigning image of elite higher educators in right-wing media outlets: that humanities professors are “woke” activists whose primary concern is the political indoctrination of “the youth.” Most of the Chicago faculty I spoke with saw—and defended—their disciplines in terms that were, if anything, conservative. Implicit in their impassioned defenses was the belief that the role of a humanist is to preserve knowledge, safeguard learning from the market and the tides of popular interest, and ward off coarse appeals to economic utility.&lt;/p&gt;
    &lt;p&gt;Depending on whom I asked, the move to scale back humanities doctoral programs is either a prudent acknowledgment of the cratered job market for tenure-track professorships and a wise attempt to protect the university’s humanities division from looming financial and political risks, or it is a cynical effort, under cover of the Trump administration’s assaults, to transfer resources away from “impractical,” unprofitable, and largely jobless fields (such as, say, comparative literature) and toward areas that the university’s senior leadership seems to care about (such as, say, STEM and “innovation”). One faculty member I spoke with mentioned a consulting firm that was brought on to help Chicago as it considers changes to its humanities division, including possibly consolidating the departments from 15 down to eight. Many professors worried that the move to impose uneven changes—reducing admissions in some while halting them in others—may be an attempt to create circumstances that will ultimately make it easier to dissolve the paused programs. “Let no good crisis go unleveraged,” Holly Shissler, an associate professor in the Middle Eastern Studies department, said with a dark laugh. “You engineer a situation in which there are no students, and then you turn around and say, ‘Why are we supporting all these departments and faculty when they have no students?’”&lt;/p&gt;
    &lt;p&gt;When I emailed Nelson and asked whether the changes were part of a plan to kill off the paused departments, she said, “A one-year pause is exactly that—a discrete decision that applies merely to a single admissions cycle.” She seemed to acknowledge, however, that a divisional reorganization could happen. “My goal is to sustain the full scope of our faculty’s research and teaching,” she said. “To do so, we must be open to new ideas and structures.” She added, “There’s no magic number of departments in the arts and humanities.” In the meantime, Chicago’s humanities professors appear largely determined to resist being evaluated in terms of expediency. In a meeting with Nelson a few days after the announcement, 14 out of 15 chairs in the humanities division told the dean that she should pause enrollment in all of their departments or none of them. Targeting some and not others was unacceptable, they argued, because it sent the message that some fields matter and others do not.&lt;/p&gt;
    &lt;p&gt;The department chairs’ wager seems to be that acting as a unified bloc will make reorganizing the division and cutting programs more difficult, even if the division-wide pause causes short-term pain for the next academic year. As anyone who has served on a faculty anywhere can tell you, this degree of cross-department solidarity and willingness to sacrifice for less-favored colleagues is remarkable, and even moving. Last Wednesday afternoon, the dean announced that the chairs had gotten their wish: With the exception of philosophy and music composition (owing to previous pauses in those programs), doctoral admissions will be frozen across the humanities for the 2026–27 academic year.&lt;/p&gt;
    &lt;p&gt;It’s a bittersweet victory, of course, one that will result in fewer doctoral students in the short term and is not guaranteed to strengthen the division in the long term. And it does not settle the most pressing question raised by all this turmoil. If even Chicago is not willing to support and protect American arts and letters, who will? One Chicago administrator, in an attempt to defend the university’s admissions pauses, pointed out that other prestigious peer institutions were expected to make similar announcements about their Ph.D. admissions in the coming weeks, and noted that Harvard is cutting nearly $2 million from its own humanities division. I would like to think that my (and others’) alarm about the future of the humanities is overblown. But the evidence doesn’t give me much hope.&lt;/p&gt;
    &lt;p&gt;The subheading of this article originally incorrectly stated that philosophy was one of the University of Chicago doctoral programs whose graduate admissions were paused.&lt;/p&gt;
    &lt;p&gt;This article originally stated that the University of Chicago’s investments in cryptocurrency are part of its financial troubles. The university maintains that it has not lost money on its cryptocurrency investments.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theatlantic.com/culture/archive/2025/08/university-chicago-humanities-doctorate/684004/"/><published>2025-10-05T15:51:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482516</id><title>BYD Builds World's Fastest Car</title><updated>2025-10-05T18:12:15.796195+00:00</updated><content/><link href="https://www.autotrader.co.uk/content/news/byd-builds-world-s-fastest-car"/><published>2025-10-05T15:54:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482719</id><title>Link Khan: Activision-Blizzard buyout is 'harming both gamers and developers'</title><updated>2025-10-05T18:12:15.659982+00:00</updated><content>&lt;doc fingerprint="227429327ee674b3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;As Microsoft lays off thousands and jacks up Game Pass prices, former FTC chair says I told you so: The Activision-Blizzard buyout is 'harming both gamers and developers'&lt;/head&gt;
    &lt;p&gt;When you're right, you're right.&lt;/p&gt;
    &lt;p&gt;As Microsoft slashes jobs and raises prices, former US Federal Trade Commission chair Lina Khan has taken to X to say that the company's actions since completing its acquisition of Activision Blizzard in 2023 is pretty much what the FTC warned would happen when it opposed the deal.&lt;/p&gt;
    &lt;p&gt;Khan, you may recall, was head of the FTC when it challenged Microsoft's proposed acquisition of Activision Blizzard, a convoluted process that didn't formally end until May of 2025—almost two years after the deal closed.&lt;/p&gt;
    &lt;p&gt;"Microsoft’s acquisition of Activision has been followed by significant price hikes and layoffs, harming both gamers and developers," Khan wrote on X. "As we’ve seen across sectors, increasing market consolidation and increasing prices often go hand-in-hand.&lt;/p&gt;
    &lt;p&gt;"As dominant firms become too-big-to-care, they can make things worse for their customers without having to worry about the consequences."&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Microsoft’s acquisition of Activision has been followed by significant price hikes and layoffs, harming both gamers and developers. As we’ve seen across sectors, increasing market consolidation and increasing prices often go hand-in-hand. As dominant firms become… https://t.co/FoI50tlEsLOctober 3, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Well, when you're right, you're right, and it's hard to argue that Khan wasn't right on this one. The FTC filed a lawsuit to block the deal in 2022 over concerns that the impact of the proposed acquisition was "reasonably likely to substantially lessen competition and/or tend to create a monopoly in both well-developed and new, burgeoning markets" if it was allowed to go through.&lt;/p&gt;
    &lt;p&gt;Microsoft and Activision, of course, insisted otherwise: Bobby Kotick, then the CEO of Activision Blizzard, said in a July 2023 statement that the merger "will benefit consumers and workers," and also "enable competition rather than allow entrenched market leaders to continue to dominate our rapidly growing industry."&lt;/p&gt;
    &lt;p&gt;The deal was closed in October 2023, even though the FTC's legal action against it was still pending, and it's been one shitty thing after another since then. Just a few months after the deal was sealed, Microsoft laid off 1,900 workers at Activision Blizzard and Xbox, and cancelled the studio's long-awaited survival game; then in September 2024, another 650 people were shown the door. That was followed by the layoff of 9,000 more employees across Microsoft in July 2025, a spot of unpleasantness that also saw multiple game cancellations, the closure of The Initiative, and knock-on impacts on other studios, even as Xbox boss Phil Spencer said the company's gaming business "never looked stronger."&lt;/p&gt;
    &lt;p&gt;Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.&lt;/p&gt;
    &lt;p&gt;Meanwhile, in case you hadn't heard, the cost of Game Pass Ultimate and PC Game Pass also jumped significantly this week. Which is actually the second price hike for Game Pass since the Activision Blizzard deal was concluded: The FTC had some harsh words for the previous (and, ironically, much smaller) price increase in July 2024.&lt;/p&gt;
    &lt;p&gt;Khan was replaced as chair of the FTC in January 2025 by incoming president Donald Trump, so her comments on X don't carry any regulatory weight. But even if this is a hollow I-told-you-so, I'd say it's a well-earned one.&lt;/p&gt;
    &lt;p&gt;2025 games: This year's upcoming releases&lt;lb/&gt;Best PC games: Our all-time favorites&lt;lb/&gt;Free PC games: Freebie fest&lt;lb/&gt;Best FPS games: Finest gunplay&lt;lb/&gt;Best RPGs: Grand adventures&lt;lb/&gt;Best co-op games: Better together&lt;/p&gt;
    &lt;p&gt;Andy has been gaming on PCs from the very beginning, starting as a youngster with text adventures and primitive action games on a cassette-based TRS80. From there he graduated to the glory days of Sierra Online adventures and Microprose sims, ran a local BBS, learned how to build PCs, and developed a longstanding love of RPGs, immersive sims, and shooters. He began writing videogame news in 2007 for The Escapist and somehow managed to avoid getting fired until 2014, when he joined the storied ranks of PC Gamer. He covers all aspects of the industry, from new game announcements and patch notes to legal disputes, Twitch beefs, esports, and Henry Cavill. Lots of Henry Cavill.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pcgamer.com/gaming-industry/as-microsoft-lays-off-thousands-and-jacks-up-game-pass-prices-former-ftc-chair-says-i-told-you-so-the-activision-blizzard-buyout-is-harming-both-gamers-and-developers/"/><published>2025-10-05T16:13:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45483205</id><title>Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR</title><updated>2025-10-05T18:12:15.407649+00:00</updated><content>&lt;doc fingerprint="6c9c320bddd310bf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 2 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at this https URL.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.02522"/><published>2025-10-05T17:01:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45483275</id><title>Focus Is Saying No</title><updated>2025-10-05T18:12:15.221958+00:00</updated><content>&lt;doc fingerprint="7c5a597f265626ee"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Focus Is Saying No&lt;/head&gt;
    &lt;head rend="h2"&gt;Software Modernization Projects Dilemma (Part 2)&lt;/head&gt;
    &lt;p&gt;This is the second part of a two-part series. Reading the first part is not required, but it is recommended.&lt;/p&gt;
    &lt;p&gt;After working with the same team for two years, I joined another tier 0 product team as a senior software engineer — one of the company’s most critical services handling massive traffic, where any downtime directly impacts revenue.&lt;/p&gt;
    &lt;p&gt;The new team focused primarily on product requirement delivery, and they were working with a legacy codebase that had significant tech debt: deprecated libraries, a cumbersome deployment process, and unstable integration tests etc.&lt;/p&gt;
    &lt;p&gt;Having spent all my time on feature work in my previous team, I worried I wouldn’t grow by doing more of the same. (Looking back now, I realize this assumption was incorrect — working on product features actually contributed significantly to my growth; I’ll unpack that in a future post).&lt;/p&gt;
    &lt;p&gt;The tech debt issues seemed like the perfect opportunity. As a result, I decided to focus on software modernization tasks for my career growth.&lt;/p&gt;
    &lt;p&gt;Two years flew by after joining the new team. I had successfully delivered challenging software modernization tasks — from removing outdated library updates to building a new deployment pipeline. Feeling confident about my contributions, I decided it was time to speak with my manager about promotion.&lt;/p&gt;
    &lt;p&gt;“I want to talk about promotion,” I said confidently in a one-on-one meeting.&lt;/p&gt;
    &lt;p&gt;“I think I’ve made some good progress for the team. Deployment is way simpler and faster now. Our test cases are finally stable. And we’ve basically eliminated all the critical security vulnerabilities.”&lt;/p&gt;
    &lt;p&gt;“So… Does this seem like staff-level work to you?”&lt;/p&gt;
    &lt;p&gt;“You’re doing great work,” my manager replied calmly.&lt;/p&gt;
    &lt;p&gt;“but I have to stack-rank the team, and those tasks aren’t staff-level.”&lt;/p&gt;
    &lt;p&gt;“Because… some lack business value. These tasks aren’t business priorities and had no impact on customers and other teams”&lt;/p&gt;
    &lt;p&gt;“Also, at the staff level, you need to work across teams, influence broader decisions, and build visibility beyond just our team.”&lt;/p&gt;
    &lt;p&gt;The conversation went nowhere. Don’t get me wrong — my manager at the time is a great manager. He is the role model of the team, and helps everyone on the team innovate and deliver results. His feedback was honest and fair.&lt;/p&gt;
    &lt;p&gt;Reflecting on that one-on-one conversation and everything I had done over those two years, I realized how much I had learned. One of the most important lessons is to focus on building a great product.&lt;/p&gt;
    &lt;p&gt;Building a great product doesn’t mean building a perfect system with zero tech debt. Instead, it’s about aligning the team’s efforts toward tasks that drives measurable business impact. And to do that, it takes courage to say no to other tasks.&lt;/p&gt;
    &lt;p&gt;Like Steve Jobs said in a conference talk:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When you think about focusing, you think focusing is about saying yes. No. Focusing is about saying no. The result of that focus is going to be some great products where the total is much greater than the sum of the parts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Looking back, I realized I had worked on a lot of low-impact projects — tasks that made no impact on users and no impact on the team, like updating outdated libraries. The old library worked fine without any updates. Updating it took weeks of my time but delivered zero value to the team or business. I did it simply because my manager told me to.&lt;/p&gt;
    &lt;p&gt;Early in my career, I said “yes” often. As I got more experience, I learned when to say “no.”&lt;/p&gt;
    &lt;p&gt;Now if my manager asks me to do tasks that I believe add no value to the team or business, I’ll politely say no. Why? Because I’m focusing on building the great product.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://medium.com/@HobokenDays/software-modernization-projects-dilemma-part-2-7f6002c4b6f1"/><published>2025-10-05T17:08:19+00:00</published></entry></feed>