<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-04T20:33:38.229432+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45466086</id><title>PEP 810 – Explicit lazy imports</title><updated>2025-10-04T20:33:50.262810+00:00</updated><content>&lt;doc fingerprint="d0324610caa1e4dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PEP 810 – Explicit lazy imports&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Author:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Pablo Galindo &amp;lt;pablogsal at python.org&amp;gt;, Germán Méndez Bravo &amp;lt;german.mb at gmail.com&amp;gt;, Thomas Wouters &amp;lt;thomas at python.org&amp;gt;, Dino Viehland &amp;lt;dinoviehland at gmail.com&amp;gt;, Brittany Reynoso &amp;lt;brittanyrey at gmail.com&amp;gt;, Noah Kim &amp;lt;noahbkim at gmail.com&amp;gt;, Tim Stumbaugh &amp;lt;me at tjstum.com&amp;gt;&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Discussions-To:&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Discourse thread&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Status:&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Draft&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Type:&lt;/item&gt;
      &lt;item rend="dd-4"&gt;Standards Track&lt;/item&gt;
      &lt;item rend="dt-5"&gt;Created:&lt;/item&gt;
      &lt;item rend="dd-5"&gt;02-Oct-2025&lt;/item&gt;
      &lt;item rend="dt-6"&gt;Python-Version:&lt;/item&gt;
      &lt;item rend="dd-6"&gt;3.15&lt;/item&gt;
      &lt;item rend="dt-7"&gt;Post-History:&lt;/item&gt;
      &lt;item rend="dd-7"&gt;03-Oct-2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;This PEP introduces syntax for lazy imports as an explicit language feature:&lt;/p&gt;
    &lt;code&gt;lazy import json
lazy from json import dumps
&lt;/code&gt;
    &lt;p&gt;Lazy imports defer the loading and execution of a module until the first time the imported name is used, in contrast to ‘normal’ imports, which eagerly load and execute a module at the point of the import statement.&lt;/p&gt;
    &lt;p&gt;By allowing developers to mark individual imports as lazy with explicit syntax, Python programs can reduce startup time, memory usage, and unnecessary work. This is particularly beneficial for command-line tools, test suites, and applications with large dependency graphs.&lt;/p&gt;
    &lt;p&gt;This proposal preserves full backwards compatibility: normal import statements remain unchanged, and lazy imports are enabled only where explicitly requested.&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;The dominant convention in Python code is to place all imports at the module level, typically at the beginning of the file. This avoids repetition, makes import dependencies clear and minimizes runtime overhead by only evaluating an import statement once per module.&lt;/p&gt;
    &lt;p&gt;A major drawback with this approach is that importing the first module for an execution of Python (the “main” module) often triggers an immediate cascade of imports, and optimistically loads many dependencies that may never be used. The effect is especially costly for command-line tools with multiple subcommands, where even running the command with &lt;code&gt;--help&lt;/code&gt; can load dozens of
unnecessary modules and take several seconds. This basic example demonstrates
what must be loaded just to get helpful feedback to the user on how to run the
program at all. Inefficiently, the user incurs this overhead again when they
figure out the command they want and invoke the program “for real.”&lt;/p&gt;
    &lt;p&gt;A somewhat common way to delay imports is to move the imports into functions (inline imports), but this practice requires more work to implement and maintain, and can be subverted by a single inadvertent top-level import. Additionally, it obfuscates the full set of dependencies for a module. Analysis of the Python standard library shows that approximately 17% of all imports outside tests (nearly 3500 total imports across 730 files) are already placed inside functions or methods specifically to defer their execution. This demonstrates that developers are already manually implementing lazy imports in performance-sensitive code, but doing so requires scattering imports throughout the codebase and makes the full dependency graph harder to understand at a glance.&lt;/p&gt;
    &lt;p&gt;The standard library provides the &lt;code&gt;LazyLoader&lt;/code&gt; class to
solve some of these inefficiency problems. It permits imports at the module
level to work mostly like inline imports do. Many scientific Python
libraries have adopted a similar pattern, formalized in
SPEC 1.
There’s also the third-party lazy_loader package, yet another
implementation of lazy imports. Imports used solely for static type checking
are another source of potentially unneeded imports, and there are similarly
disparate approaches to minimizing the overhead. The various approaches used
here to defer or remove eager imports do not cover all potential use-cases for
a general lazy import mechanism. There is no clear standard, and there are
several drawbacks including runtime overhead in unexpected places, or worse
runtime introspection.&lt;/p&gt;
    &lt;p&gt;This proposal introduces syntax for lazy imports with a design that is local, explicit, controlled, and granular. Each of these qualities is essential to making the feature predictable and safe to use in practice.&lt;/p&gt;
    &lt;p&gt;The behavior is local: laziness applies only to the specific import marked with the &lt;code&gt;lazy&lt;/code&gt; keyword, and it does not cascade recursively into other
imports. This ensures that developers can reason about the effect of laziness
by looking only at the line of code in front of them, without worrying about
whether imported modules will themselves behave differently. A &lt;code&gt;lazy import&lt;/code&gt;
is an isolated decision each time it is used, not a global shift in semantics.&lt;/p&gt;
    &lt;p&gt;The semantics are explicit. When a name is imported lazily, the binding is created in the importing module immediately, but the target module is not loaded until the first time the name is accessed. After this point, the binding is indistinguishable from one created by a normal import. This clarity reduces surprises and makes the feature accessible to developers who may not be deeply familiar with Python’s import machinery.&lt;/p&gt;
    &lt;p&gt;Lazy imports are controlled, in the sense that deferred loading is only triggered by the importing code itself. In the general case, a library will only experience lazy imports if its own authors choose to mark them as such. This avoids shifting responsibility onto downstream users and prevents accidental surprises in library behavior. Since library authors typically manage their own import subgraphs, they retain predictable control over when and how laziness is applied.&lt;/p&gt;
    &lt;p&gt;The mechanism is also granular. It is introduced through explicit syntax on individual imports, rather than a global flag or implicit setting. This allows developers to adopt it incrementally, starting with the most performance-sensitive areas of a codebase. As this feature is introduced to the community, we want to make the experience of onboarding optional, progressive, and adaptable to the needs of each project.&lt;/p&gt;
    &lt;p&gt;Lazy imports provide several concrete advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Command-line tools are often invoked directly by a user, so latency – in particular startup latency – is quite noticeable. These programs are also typically short-lived processes (contrasted with, e.g., a web server). With lazy imports, only the code paths actually reached will import a module. This can reduce startup time by 50-70% in practice, providing a significant improvement to a common user experience and improving Python’s competitiveness in domains where fast startup matters most.&lt;/item&gt;
      &lt;item&gt;Type annotations frequently require imports that are never used at runtime. The common workaround is to wrap them in &lt;code&gt;if TYPE_CHECKING:&lt;/code&gt;blocks [1]. With lazy imports, annotation-only imports impose no runtime penalty, eliminating the need for such guards and making annotated codebases cleaner.&lt;/item&gt;
      &lt;item&gt;Large applications often import thousands of modules, and each module creates function and type objects, incurring memory costs. In long-lived processes, this noticeably raises baseline memory usage. Lazy imports defer these costs until a module is needed, keeping unused subsystems unloaded. Memory savings of 30-40% have been observed in real workloads.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Rationale&lt;/head&gt;
    &lt;p&gt;The design of this proposal is centered on clarity, predictability, and ease of adoption. Each decision was made to ensure that lazy imports provide tangible benefits without introducing unnecessary complexity into the language or its runtime.&lt;/p&gt;
    &lt;p&gt;It is also worth noting that while this PEP outlines one specific approach, we list alternate implementation strategies for some of the core aspects and semantics of the proposal. If the community expresses a strong preference for a different technical path that still preserves the same core semantics or there is fundamental disagreement over the specific option, we have included the brainstorming we have already completed in preparation for this proposal as reference.&lt;/p&gt;
    &lt;p&gt;The choice to introduce a new &lt;code&gt;lazy&lt;/code&gt; keyword reflects the need for explicit
syntax. Import behavior is too fundamental to be left implicit or hidden
behind global flags or environment variables. By marking laziness directly at
the import site, the intent is immediately visible to both readers and tools.
This avoids surprises, reduces the cognitive burden of reasoning about
imports, and keeps lazy import semantics in line with Python’s tradition of
explicitness.&lt;/p&gt;
    &lt;p&gt;Another important decision is to represent lazy imports with proxy objects in the module’s namespace, rather than by modifying dictionary lookup. Earlier approaches experimented with embedding laziness into dictionaries, but this blurred abstractions and risked affecting unrelated parts of the runtime. The dictionary is a fundamental data structure in Python – literally every object is built on top of dicts – and adding hooks to dictionaries would prevent critical optimizations and complicate the entire runtime. The proxy approach is simpler: it behaves like a placeholder until first use, at which point it resolves the import and rebinds the name. From then on, the binding is indistinguishable from a normal import. This makes the mechanism easy to explain and keeps the rest of the interpreter unchanged.&lt;/p&gt;
    &lt;p&gt;Compatibility for library authors was also a key concern. Many maintainers need a migration path that allows them to support both new and old versions of Python at once. For this reason, the proposal includes the &lt;code&gt;__lazy_modules__&lt;/code&gt; global as a transitional mechanism. A module can
declare which imports should be treated as lazy (by listing the module names
as strings), and on Python 3.15 or later those imports will become lazy
automatically, as if they were imported with the &lt;code&gt;lazy&lt;/code&gt; keyword. On earlier
versions the declaration is ignored, leaving imports eager. This gives authors
a practical bridge until they can rely on the keyword as the canonical syntax.&lt;/p&gt;
    &lt;p&gt;Finally, the feature is designed to be adopted incrementally. Nothing changes unless a developer explicitly opts in, and adoption can begin with just a few imports in performance-sensitive areas. This mirrors the experience of gradual typing in Python: a mechanism that can be introduced progressively, without forcing projects to commit globally from day one. Notably, the adoption can also be done from the “outside in”, permitting CLI authors to introduce lazy imports and speed up user-facing tools, without requiring changes to every library the tool might use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other design decisions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The scope of laziness is deliberately local and non-recursive. A lazy import only affects the specific statement where it appears; it does not cascade into other modules or submodules. This choice is crucial for predictability. When developers read code, they can reason about import behavior line by line, without worrying about hidden laziness deeper in the dependency graph. The result is a feature that is powerful but still easy to understand in context.&lt;/item&gt;
      &lt;item&gt;In addition, it is useful to provide a mechanism to activate or deactivate lazy imports at a global level. While the primary design centers on explicit syntax, there are scenarios – such as large applications, testing environments, or frameworks – where enabling laziness consistently across many modules provides the most benefit. A global switch makes it easy to experiment with or enforce consistent behavior, while still working in combination with the filtering API to respect exclusions or tool-specific configuration. This ensures that global adoption can be practical without reducing flexibility or control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Specification&lt;/head&gt;
    &lt;head rend="h3"&gt;Grammar&lt;/head&gt;
    &lt;p&gt;A new soft keyword &lt;code&gt;lazy&lt;/code&gt; is added. A soft keyword is a context-sensitive
keyword that only has special meaning in specific grammatical contexts;
elsewhere it can be used as a regular identifier (e.g., as a variable name).
The &lt;code&gt;lazy&lt;/code&gt; keyword only has special meaning when it appears before import
statements:&lt;/p&gt;
    &lt;code&gt;import_name:
    | 'lazy'? 'import' dotted_as_names

import_from:
    | 'lazy'? 'from' ('.' | '...')* dotted_name 'import' import_from_targets
    | 'lazy'? 'from' ('.' | '...')+ 'import' import_from_targets
&lt;/code&gt;
    &lt;head rend="h4"&gt;Syntax restrictions&lt;/head&gt;
    &lt;p&gt;The soft keyword is only allowed at the global (module) level, not inside functions, class bodies, with &lt;code&gt;try&lt;/code&gt;/&lt;code&gt;with&lt;/code&gt; blocks, or &lt;code&gt;import *&lt;/code&gt;. Import
statements that use the soft keyword are potentially lazy. Imports that
can’t be lazy are unaffected by the global lazy imports flag, and instead are
always eager.&lt;/p&gt;
    &lt;p&gt;Examples of syntax errors:&lt;/p&gt;
    &lt;code&gt;# SyntaxError: lazy import not allowed inside functions
def foo():
    lazy import json

# SyntaxError: lazy import not allowed inside classes
class Bar:
    lazy import json

# SyntaxError: lazy import not allowed inside try/except blocks
try:
    lazy import json
except ImportError:
    pass

# SyntaxError: lazy import not allowed inside with blocks
with suppress(ImportError):
    lazy import json

# SyntaxError: lazy from ... import * is not allowed
lazy from json import *
&lt;/code&gt;
    &lt;head rend="h3"&gt;Semantics&lt;/head&gt;
    &lt;p&gt;When the &lt;code&gt;lazy&lt;/code&gt; keyword is used, the import becomes potentially lazy.
Unless lazy imports are disabled or suppressed (see below), the module is not
loaded immediately at the import statement; instead, a lazy proxy object is
created and bound to the name. The actual module is loaded on first use of
that name.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;import sys

lazy import json

print('json' in sys.modules)  # False - module not loaded yet

# First use triggers loading
result = json.dumps({"hello": "world"})

print('json' in sys.modules)  # True - now loaded
&lt;/code&gt;
    &lt;p&gt;A module may contain a &lt;code&gt;__lazy_modules__&lt;/code&gt; attribute, which is a
sequence of fully qualified module names (strings) to make potentially lazy
(as if the &lt;code&gt;lazy&lt;/code&gt; keyword was used). This attribute is checked on each
&lt;code&gt;import&lt;/code&gt; statement to determine whether the import should be made
potentially lazy. When a module is made lazy this way, from-imports using
that module are also lazy, but not necessarily imports of sub-modules.&lt;/p&gt;
    &lt;p&gt;The normal (non-lazy) import statement will check the global lazy imports flag. If it is “enabled”, all imports are potentially lazy (except for imports that can’t be lazy, as mentioned above.)&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;__lazy_modules__ = ["json"]
import json
print('json' in sys.modules)  # False
result = json.dumps({"hello": "world"})
print('json' in sys.modules)  # True
&lt;/code&gt;
    &lt;p&gt;If the global lazy imports flag is set to “disabled”, no potentially lazy import is ever imported lazily, and the behavior is equivalent to a regular import statement: the import is eager (as if the lazy keyword was not used).&lt;/p&gt;
    &lt;p&gt;For a potentially lazy import, the lazy imports filter (if set) is called with the name of the module doing the import, the name of the module being imported, and (if applicable) the fromlist. If the lazy import filter returns &lt;code&gt;True&lt;/code&gt;, the potentially lazy import becomes a lazy import. Otherwise, the
import is not lazy, and the normal (eager) import continues.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lazy import mechanism&lt;/head&gt;
    &lt;p&gt;When an import is lazy, &lt;code&gt;__lazy_import__&lt;/code&gt; is called instead of
&lt;code&gt;__import__&lt;/code&gt;. &lt;code&gt;__lazy_import__&lt;/code&gt; has the same function signature as
&lt;code&gt;__import__&lt;/code&gt;. It adds the module name to &lt;code&gt;sys.lazy_modules&lt;/code&gt;, a set of
fully-qualified module names which have been lazily imported at some point
(primarily for diagnostics and introspection), and returns a “lazy module
object.”&lt;/p&gt;
    &lt;p&gt;The implementation of &lt;code&gt;from ... import&lt;/code&gt; (the &lt;code&gt;IMPORT_FROM&lt;/code&gt; bytecode
implementation) checks if the module it’s fetching from is a lazy module
object, and if so, returns a lazy object for each name instead.&lt;/p&gt;
    &lt;p&gt;The end result of this process is that lazy imports (regardless of how they are enabled) result in lazy objects being assigned to global variables.&lt;/p&gt;
    &lt;p&gt;Lazy module objects do not appear in &lt;code&gt;sys.modules&lt;/code&gt;, they’re just listed in
the &lt;code&gt;sys.lazy_modules&lt;/code&gt; set. Under normal operation lazy objects should only
end up stored in global variables, and the common ways to access those
variables (regular variable access, module attributes) will resolve lazy
imports (“reify”) and replace them when they’re accessed.&lt;/p&gt;
    &lt;p&gt;It is still possible to expose lazy objects through other means, like debuggers. This is not considered a problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reification&lt;/head&gt;
    &lt;p&gt;When a lazy object is first used, it needs to be reified. This means resolving the import at that point in the program and replacing the lazy object with the concrete one. Reification imports the module in the same way as it would have been if it had been imported eagerly, barring intervening changes to the import system (e.g. to &lt;code&gt;sys.path&lt;/code&gt;, &lt;code&gt;sys.meta_path&lt;/code&gt;, &lt;code&gt;sys.path_hooks&lt;/code&gt; or
&lt;code&gt;__import__&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Reification still calls &lt;code&gt;__import__&lt;/code&gt; to resolve the import. When the module
is first reified, it’s removed from &lt;code&gt;sys.lazy_modules&lt;/code&gt; (even if there are
still other unreified lazy references to it). When a package is reified and
submodules in the package were also previously lazily imported, those
submodules are not automatically reified but they are added to the reified
package’s globals (unless the package already assigned something else to the
name of the submodule).&lt;/p&gt;
    &lt;p&gt;If reification fails (e.g., due to an &lt;code&gt;ImportError&lt;/code&gt;), the exception is
enhanced with chaining to show both where the lazy import was defined and
where it was first accessed (even though it propagates from the code that
triggered reification). This provides clear debugging information:&lt;/p&gt;
    &lt;code&gt;# app.py - has a typo in the import
lazy from json import dumsp  # Typo: should be 'dumps'

print("App started successfully")
print("Processing data...")

# Error occurs here on first use
result = dumsp({"key": "value"})
&lt;/code&gt;
    &lt;p&gt;The traceback shows both locations:&lt;/p&gt;
    &lt;code&gt;App started successfully
Processing data...
Traceback (most recent call last):
  File "app.py", line 2, in &amp;lt;module&amp;gt;
    lazy from json import dumsp
ImportError: deferred import of 'json.dumsp' raised an exception during resolution

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "app.py", line 8, in &amp;lt;module&amp;gt;
    result = dumsp({"key": "value"})
             ^^^^^
ImportError: cannot import name 'dumsp' from 'json'. Did you mean: 'dump'?
&lt;/code&gt;
    &lt;p&gt;This exception chaining clearly shows: (1) where the lazy import was defined, (2) that it was deferred, and (3) where the actual access happened that triggered the error.&lt;/p&gt;
    &lt;p&gt;Reification does not automatically occur when a module that was previously lazily imported is subsequently eagerly imported. Reification does not immediately resolve all lazy objects (e.g. &lt;code&gt;lazy from&lt;/code&gt; statements) that
referenced the module. It only resolves the lazy object being accessed.&lt;/p&gt;
    &lt;p&gt;Accessing a lazy object (from a global variable or a module attribute) reifies the object. Accessing a module’s &lt;code&gt;__dict__&lt;/code&gt; reifies all lazy objects in
that module. Operations that indirectly access &lt;code&gt;__dict__&lt;/code&gt; (such as
&lt;code&gt;dir()&lt;/code&gt;) also trigger this behavior.&lt;/p&gt;
    &lt;p&gt;Example using &lt;code&gt;__dict__&lt;/code&gt; from external code:&lt;/p&gt;
    &lt;code&gt;# my_module.py
import sys
lazy import json

print('json' in sys.modules)  # False - still lazy

# main.py
import sys
import my_module

# Accessing __dict__ from external code DOES reify all lazy imports
d = my_module.__dict__

print('json' in sys.modules)  # True - reified by __dict__ access
print(type(d['json']))  # &amp;lt;class 'module'&amp;gt;
&lt;/code&gt;
    &lt;p&gt;However, calling &lt;code&gt;globals()&lt;/code&gt; does not trigger reification – it returns
the module’s dictionary, and accessing lazy objects through that dictionary
still returns lazy proxy objects that need to be manually reified upon use. A
lazy object can be resolved explicitly by calling the &lt;code&gt;get&lt;/code&gt; method. Other,
more indirect ways of accessing arbitrary globals (e.g. inspecting
&lt;code&gt;frame.f_globals&lt;/code&gt;) also do not reify all the objects.&lt;/p&gt;
    &lt;p&gt;Example using &lt;code&gt;globals()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import sys
lazy import json

# Calling globals() does NOT trigger reification
g = globals()

print('json' in sys.modules)  # False - still lazy
print(type(g['json']))  # &amp;lt;class 'lazy_import'&amp;gt;

# Explicitly reify using the get() method
resolved = g['json'].get()

print(type(resolved))  # &amp;lt;class 'module'&amp;gt;
print('json' in sys.modules)  # True - now loaded
&lt;/code&gt;
    &lt;head rend="h2"&gt;Reference Implementation&lt;/head&gt;
    &lt;p&gt;A reference implementation is available at: https://github.com/LazyImportsCabal/cpython/tree/lazy&lt;/p&gt;
    &lt;head rend="h3"&gt;Bytecode and adaptive specialization&lt;/head&gt;
    &lt;p&gt;Lazy imports are implemented through modifications to four bytecode instructions: &lt;code&gt;IMPORT_NAME&lt;/code&gt;, &lt;code&gt;IMPORT_FROM&lt;/code&gt;, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;, and
&lt;code&gt;LOAD_NAME&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;lazy&lt;/code&gt; syntax sets a flag in the &lt;code&gt;IMPORT_NAME&lt;/code&gt; instruction’s oparg
(&lt;code&gt;oparg &amp;amp; 0x01&lt;/code&gt;). The interpreter checks this flag and calls
&lt;code&gt;_PyEval_LazyImportName()&lt;/code&gt; instead of &lt;code&gt;_PyEval_ImportName()&lt;/code&gt;, creating a
lazy import object rather than executing the import immediately. The
&lt;code&gt;IMPORT_FROM&lt;/code&gt; instruction checks whether its source is a lazy import
(&lt;code&gt;PyLazyImport_CheckExact()&lt;/code&gt;) and creates a lazy object for the attribute
rather than accessing it immediately.&lt;/p&gt;
    &lt;p&gt;When a lazy object is accessed, it must be reified. The &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;
instruction (used in function scopes) and &lt;code&gt;LOAD_NAME&lt;/code&gt; instruction (used at
module and class level) both check whether the object being loaded is a lazy
import. If so, they call &lt;code&gt;_PyImport_LoadLazyImportTstate()&lt;/code&gt; to perform the
actual import and store the module in &lt;code&gt;sys.modules&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This check incurs a very small cost on each access. However, Python’s adaptive interpreter can specialize &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; after observing that a lazy import
has been reified. After several executions, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; becomes
&lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;, which accesses the module dictionary directly without
checking for lazy imports.&lt;/p&gt;
    &lt;p&gt;Examples of the bytecode generated:&lt;/p&gt;
    &lt;code&gt;lazy import json  # IMPORT_NAME with flag set
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;IMPORT_NAME              1 (json + lazy)
&lt;/code&gt;
    &lt;code&gt;lazy from json import dumps  # IMPORT_NAME + IMPORT_FROM
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;IMPORT_NAME              1 (json + lazy)
IMPORT_FROM              1 (dumps)
&lt;/code&gt;
    &lt;code&gt;lazy import json
x = json  # Module-level access
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;LOAD_NAME                0 (json)
&lt;/code&gt;
    &lt;code&gt;lazy import json

def use_json():
    return json.dumps({})  # Function scope
&lt;/code&gt;
    &lt;p&gt;Before any calls:&lt;/p&gt;
    &lt;code&gt;LOAD_GLOBAL              0 (json)
LOAD_ATTR                2 (dumps)
&lt;/code&gt;
    &lt;p&gt;After several calls, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; specializes to &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;LOAD_GLOBAL_MODULE       0 (json)
LOAD_ATTR_MODULE         2 (dumps)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Lazy imports filter&lt;/head&gt;
    &lt;p&gt;This PEP adds two new functions to the &lt;code&gt;sys&lt;/code&gt; module to manage the lazy
imports filter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;sys.set_lazy_imports_filter(func)&lt;/code&gt;- Sets the filter function. The&lt;code&gt;func&lt;/code&gt;parameter must have the signature:&lt;code&gt;func(importer: str, name: str, fromlist: tuple[str, ...] | None) -&amp;gt; bool&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sys.get_lazy_imports_filter()&lt;/code&gt;- Returns the currently installed filter function, or&lt;code&gt;None&lt;/code&gt;if no filter is set.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The filter function is called for every potentially lazy import, and must return &lt;code&gt;True&lt;/code&gt; if the import should be lazy. This allows for fine-grained
control over which imports should be lazy, useful for excluding modules with
known side-effect dependencies or registration patterns.&lt;/p&gt;
    &lt;p&gt;The filter mechanism serves as a foundation that tools, debuggers, linters, and other ecosystem utilities can leverage to provide better lazy import experiences. For example, static analysis tools could detect modules with side effects and automatically configure appropriate filters. In the future (out of scope for this PEP), this foundation may enable better ways to declaratively specify which modules are safe for lazy importing, such as package metadata, type stubs with lazy-safety annotations, or configuration files. The current filter API is designed to be flexible enough to accommodate such future enhancements without requiring changes to the core language specification.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;import sys

def exclude_side_effect_modules(importer, name, fromlist):
    """
    Filter function to exclude modules with import-time side effects.

    Args:
        importer: Name of the module doing the import
        name: Name of the module being imported
        fromlist: Tuple of names being imported (for 'from' imports), or None

    Returns:
        True to allow lazy import, False to force eager import
    """
    # Modules known to have important import-time side effects
    side_effect_modules = {'legacy_plugin_system', 'metrics_collector'}

    if name in side_effect_modules:
        return False  # Force eager import

    return True  # Allow lazy import

# Install the filter
sys.set_lazy_imports_filter(exclude_side_effect_modules)

# These imports are checked by the filter
lazy import data_processor        # Filter returns True -&amp;gt; stays lazy
lazy import legacy_plugin_system  # Filter returns False -&amp;gt; imported eagerly

print('data_processor' in sys.modules)       # False - still lazy
print('legacy_plugin_system' in sys.modules) # True - loaded eagerly

# First use of data_processor triggers loading
result = data_processor.transform(data)
print('data_processor' in sys.modules)       # True - now loaded
&lt;/code&gt;
    &lt;head rend="h3"&gt;Global lazy imports control&lt;/head&gt;
    &lt;p&gt;The global lazy imports flag can be controlled through:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;-X lazy_imports=&amp;lt;mode&amp;gt;&lt;/code&gt;command-line option&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;PYTHON_LAZY_IMPORTS=&amp;lt;mode&amp;gt;&lt;/code&gt;environment variable&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;sys.set_lazy_imports(mode)&lt;/code&gt;function (primarily for testing)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Where &lt;code&gt;&amp;lt;mode&amp;gt;&lt;/code&gt; can be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;"default"&lt;/code&gt;(or unset): Only explicitly marked lazy imports are lazy&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;"enabled"&lt;/code&gt;: All module-level imports (except in&lt;code&gt;try&lt;/code&gt;or&lt;code&gt;with&lt;/code&gt;blocks and&lt;code&gt;import *&lt;/code&gt;) become potentially lazy&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;"disabled"&lt;/code&gt;: No imports are lazy, even those explicitly marked with&lt;code&gt;lazy&lt;/code&gt;keyword&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the global flag is set to &lt;code&gt;"enabled"&lt;/code&gt;, all imports at the global level
of all modules are potentially lazy except for those inside a &lt;code&gt;try&lt;/code&gt; or
&lt;code&gt;with&lt;/code&gt; block or any wild card (&lt;code&gt;from ... import *&lt;/code&gt;) import.&lt;/p&gt;
    &lt;p&gt;If the global lazy imports flag is set to &lt;code&gt;"disabled"&lt;/code&gt;, no potentially
lazy import is ever imported lazily, the import filter is never called, and
the behavior is equivalent to a regular &lt;code&gt;import&lt;/code&gt; statement: the import is
eager (as if the lazy keyword was not used).&lt;/p&gt;
    &lt;head rend="h2"&gt;Backwards Compatibility&lt;/head&gt;
    &lt;p&gt;Lazy imports are opt-in. Existing programs continue to run unchanged unless a project explicitly enables laziness (via &lt;code&gt;lazy&lt;/code&gt; syntax,
&lt;code&gt;__lazy_modules__&lt;/code&gt;, or an interpreter-wide switch).&lt;/p&gt;
    &lt;head rend="h3"&gt;Unchanged semantics&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Regular &lt;code&gt;import&lt;/code&gt;and&lt;code&gt;from ... import ...&lt;/code&gt;statements remain eager unless explicitly made potentially lazy by the local or global mechanisms provided.&lt;/item&gt;
      &lt;item&gt;Dynamic import APIs remain eager and unchanged: &lt;code&gt;__import__()&lt;/code&gt;and&lt;code&gt;importlib.import_module()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Import hooks and loaders continue to run under the standard import protocol when a lazy object is reified.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Observable behavioral shifts (opt-in only)&lt;/head&gt;
    &lt;p&gt;These changes are limited to bindings explicitly made lazy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Error timing. Exceptions that would have occurred during an eager import (for example &lt;code&gt;ImportError&lt;/code&gt;or&lt;code&gt;AttributeError&lt;/code&gt;for a missing member) now occur at the first use of the lazy name.&lt;quote&gt;# With eager import - error at import statement import broken_module # ImportError raised here # With lazy import - error deferred lazy import broken_module print("Import succeeded") broken_module.foo() # ImportError raised here on first use&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Side-effect timing. Import-time side effects in lazily imported modules occur at first use of the binding, not at module import time.&lt;/item&gt;
      &lt;item&gt;Import order. Because modules are imported on first use, the order in which modules are imported may differ from how they appear in code.&lt;/item&gt;
      &lt;item&gt;Presence in ``sys.modules``. A lazily imported module does not appear in &lt;code&gt;sys.modules&lt;/code&gt;until first use. After reification, it must appear in&lt;code&gt;sys.modules&lt;/code&gt;. If some other code eagerly imports the same module before first use, the lazy binding resolves to that existing (lazy) module object when it is first used.&lt;/item&gt;
      &lt;item&gt;Proxy visibility. Before first use, the bound name refers to a lazy proxy. Indirect introspection that touches the value may observe a proxy lazy object representation. After first use, the name is rebound to the real object and becomes indistinguishable from an eager import.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Thread-safety and reification&lt;/head&gt;
    &lt;p&gt;First use of a lazy binding follows the existing import-lock discipline. Exactly one thread performs the import and atomically rebinds the importing module’s global to the resolved object. Concurrent readers thereafter observe the real object.&lt;/p&gt;
    &lt;p&gt;Lazy imports are thread-safe and have no special considerations for free-threading. A module that would normally be imported in the main thread may be imported in a different thread if that thread triggers the first access to the lazy import. This is not a problem: the import lock ensures thread safety regardless of which thread performs the import.&lt;/p&gt;
    &lt;p&gt;Subinterpreters are supported. Each subinterpreter maintains its own &lt;code&gt;sys.lazy_modules&lt;/code&gt; and import state, so lazy imports in one subinterpreter
do not affect others.&lt;/p&gt;
    &lt;head rend="h3"&gt;Typing and tools&lt;/head&gt;
    &lt;p&gt;Type checkers and static analyzers may treat &lt;code&gt;lazy&lt;/code&gt; imports as ordinary
imports for name resolution. At runtime, annotation-only imports can be marked
&lt;code&gt;lazy&lt;/code&gt; to avoid startup overhead. IDEs and debuggers should be prepared to
display lazy proxies before first use and the real objects thereafter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security Implications&lt;/head&gt;
    &lt;p&gt;There are no known security vulnerabilities introduced by lazy imports.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Teach This&lt;/head&gt;
    &lt;p&gt;The new &lt;code&gt;lazy&lt;/code&gt; keyword will be documented as part of the language standard.&lt;/p&gt;
    &lt;p&gt;As this feature is opt-in, new Python users should be able to continue using the language as they are used to. For experienced developers, we expect them to leverage lazy imports for the variety of benefits listed above (decreased latency, decreased memory usage, etc) on a case-by-case basis. Developers interested in the performance of their Python binary will likely leverage profiling to understand the import time overhead in their codebase and mark the necessary imports as &lt;code&gt;lazy&lt;/code&gt;. In addition, developers can mark imports
that will only be used for type annotations as &lt;code&gt;lazy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Below is guidance on how to best take advantage of lazy imports and how to avoid incompatibilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When adopting lazy imports, users should be aware that eliding an import until it is used will result in side effects not being executed. In turn, users should be wary of modules that rely on import time side effects. Perhaps the most common reliance on import side effects is the registry pattern, where population of some external registry happens implicitly during the importing of modules, often via decorators but sometimes implemented via metaclasses or &lt;code&gt;__init_subclass__&lt;/code&gt;. Instead, registries of objects should be constructed via explicit discovery processes (e.g. a well-known function to call).&lt;quote&gt;# Problematic: Plugin registers itself on import # my_plugin.py from plugin_registry import register_plugin @register_plugin("MyPlugin") class MyPlugin: pass # In main code: lazy import my_plugin # Plugin NOT registered yet - module not loaded! # Better: Explicit discovery # plugin_registry.py def discover_plugins(): from my_plugin import MyPlugin register_plugin(MyPlugin) # In main code: plugin_registry.discover_plugins() # Explicit loading&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Always import needed submodules explicitly. It is not enough to rely on a different import to ensure a module has its submodules as attributes. Plainly, unless there is an explicit &lt;code&gt;from . import bar&lt;/code&gt;in&lt;code&gt;foo/__init__.py&lt;/code&gt;, always use&lt;code&gt;import foo.bar; foo.bar.Baz&lt;/code&gt;, not&lt;code&gt;import foo; foo.bar.Baz&lt;/code&gt;. The latter only works (unreliably) because the attribute&lt;code&gt;foo.bar&lt;/code&gt;is added as a side effect of&lt;code&gt;foo.bar&lt;/code&gt;being imported somewhere else.&lt;/item&gt;
      &lt;item&gt;Users who are moving imports into functions to improve startup time, should instead consider keeping them where they are but adding the &lt;code&gt;lazy&lt;/code&gt;keyword. This allows them to keep dependencies clear and avoid the overhead of repeatedly re-resolving the import but will still speed up the program.&lt;quote&gt;# Before: Inline import (repeated overhead) def process_data(data): import json # Re-resolved on every call return json.dumps(data) # After: Lazy import at module level lazy import json def process_data(data): return json.dumps(data) # Loaded once on first call&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Avoid using wild card (star) imports, as those are always eager.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Q: How does this differ from the rejected PEP 690?&lt;/p&gt;
    &lt;p&gt;A: PEP 810 takes an explicit, opt-in approach instead of PEP 690’s implicit global approach. The key differences are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Explicit syntax: &lt;code&gt;lazy import foo&lt;/code&gt;clearly marks which imports are lazy.&lt;/item&gt;
      &lt;item&gt;Local scope: Laziness only affects the specific import statement, not cascading to dependencies.&lt;/item&gt;
      &lt;item&gt;Simpler implementation: Uses proxy objects instead of modifying core dictionary behavior.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: What happens when lazy imports encounter errors?&lt;/p&gt;
    &lt;p&gt;A: Import errors (&lt;code&gt;ImportError&lt;/code&gt;, &lt;code&gt;ModuleNotFoundError&lt;/code&gt;, syntax errors) are
deferred until first use of the lazy name. This is similar to moving an import
into a function. The error will occur with a clear traceback pointing to the
first access of the lazy object.&lt;/p&gt;
    &lt;p&gt;The implementation provides enhanced error reporting through exception chaining. When a lazy import fails during reification, the original exception is preserved and chained, showing both where the import was defined and where it was first used:&lt;/p&gt;
    &lt;code&gt;Traceback (most recent call last):
  File "test.py", line 1, in &amp;lt;module&amp;gt;
    lazy import broken_module
ImportError: deferred import of 'broken_module' raised an exception during resolution

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "test.py", line 3, in &amp;lt;module&amp;gt;
    broken_module.foo()
    ^^^^^^^^^^^^^
  File "broken_module.py", line 2, in &amp;lt;module&amp;gt;
    1/0
ZeroDivisionError: division by zero
&lt;/code&gt;
    &lt;p&gt;Q: How do lazy imports affect modules with import-time side effects?&lt;/p&gt;
    &lt;p&gt;A: Side effects are deferred until first use. This is generally desirable for performance, but may require code changes for modules that rely on import-time registration patterns. We recommend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use explicit initialization functions instead of import-time side effects&lt;/item&gt;
      &lt;item&gt;Call initialization functions explicitly when needed&lt;/item&gt;
      &lt;item&gt;Avoid relying on import order for side effects&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Can I use lazy imports with &lt;code&gt;from ... import ...&lt;/code&gt; statements?&lt;/p&gt;
    &lt;p&gt;A: Yes, as long as you don’t use &lt;code&gt;from ... import *&lt;/code&gt;. Both &lt;code&gt;lazy import
foo&lt;/code&gt; and &lt;code&gt;lazy from foo import bar&lt;/code&gt; are supported. The &lt;code&gt;bar&lt;/code&gt; name will be
bound to a lazy object that resolves to &lt;code&gt;foo.bar&lt;/code&gt; on first use.&lt;/p&gt;
    &lt;p&gt;Q: Does &lt;code&gt;lazy from module import Class&lt;/code&gt; load the entire module or just
the class?&lt;/p&gt;
    &lt;p&gt;A: It loads the entire module, not just the class. This is because Python’s import system always executes the complete module file – there’s no mechanism to execute only part of a &lt;code&gt;.py&lt;/code&gt; file. When you first access
&lt;code&gt;Class&lt;/code&gt;, Python:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Loads and executes the entire &lt;code&gt;module.py&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;Extracts the &lt;code&gt;Class&lt;/code&gt;attribute from the resulting module object&lt;/item&gt;
      &lt;item&gt;Binds &lt;code&gt;Class&lt;/code&gt;to the name in your namespace&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is identical to eager &lt;code&gt;from module import Class&lt;/code&gt; behavior. The only
difference with lazy imports is that steps 1-3 happen on first use instead of
at the import statement.&lt;/p&gt;
    &lt;code&gt;# heavy_module.py
print("Loading heavy_module")  # This ALWAYS runs when module loads

class MyClass:
    pass

class UnusedClass:
    pass  # Also gets defined, even though we don't import it

# app.py
lazy from heavy_module import MyClass

print("Import statement done")  # heavy_module not loaded yet
obj = MyClass()                  # NOW "Loading heavy_module" prints
                                 # (and UnusedClass gets defined too)
&lt;/code&gt;
    &lt;p&gt;Key point: Lazy imports defer when a module loads, not what gets loaded. You cannot selectively load only parts of a module – Python’s import system doesn’t support partial module execution.&lt;/p&gt;
    &lt;p&gt;Q: What about type annotations and &lt;code&gt;TYPE_CHECKING&lt;/code&gt; imports?&lt;/p&gt;
    &lt;p&gt;A: Lazy imports eliminate the common need for &lt;code&gt;TYPE_CHECKING&lt;/code&gt; guards. You
can write:&lt;/p&gt;
    &lt;code&gt;lazy from collections.abc import Sequence, Mapping  # No runtime cost

def process(items: Sequence[str]) -&amp;gt; Mapping[str, int]:
    ...
&lt;/code&gt;
    &lt;p&gt;Instead of:&lt;/p&gt;
    &lt;code&gt;from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from collections.abc import Sequence, Mapping

def process(items: Sequence[str]) -&amp;gt; Mapping[str, int]:
    ...
&lt;/code&gt;
    &lt;p&gt;Q: What’s the performance overhead of lazy imports?&lt;/p&gt;
    &lt;p&gt;A: The overhead is minimal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero overhead after first use thanks to the adaptive interpreter optimizing the slow path away.&lt;/item&gt;
      &lt;item&gt;Small one-time cost to create the proxy object.&lt;/item&gt;
      &lt;item&gt;Reification (first use) has the same cost as a regular import.&lt;/item&gt;
      &lt;item&gt;No ongoing performance penalty unlike &lt;code&gt;importlib.util.LazyLoader&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarking with the pyperformance suite shows the implementation is performance neutral when lazy imports are not used.&lt;/p&gt;
    &lt;p&gt;Q: Can I mix lazy and eager imports of the same module?&lt;/p&gt;
    &lt;p&gt;A: Yes. If module &lt;code&gt;foo&lt;/code&gt; is imported both lazily and eagerly in the same
program, the eager import takes precedence and both bindings resolve to the
same module object.&lt;/p&gt;
    &lt;p&gt;Q: How do I migrate existing code to use lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Migration is incremental:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Identify slow-loading modules using profiling tools.&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;lazy&lt;/code&gt;keyword to imports that aren’t needed immediately.&lt;/item&gt;
      &lt;item&gt;Test that side-effect timing changes don’t break functionality.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;__lazy_modules__&lt;/code&gt;for compatibility with older Python versions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: What about star imports (&lt;code&gt;from module import *&lt;/code&gt;)?&lt;/p&gt;
    &lt;p&gt;A: Wild card (star) imports cannot be lazy - they remain eager. This is because the set of names being imported cannot be determined without loading the module. Using the &lt;code&gt;lazy&lt;/code&gt; keyword with star imports will be a syntax
error. If lazy imports are globally enabled, star imports will still be eager.&lt;/p&gt;
    &lt;p&gt;Q: How do lazy imports interact with import hooks and custom loaders?&lt;/p&gt;
    &lt;p&gt;A: Import hooks and loaders work normally. When a lazy object is first used, the standard import protocol runs, including any custom hooks or loaders that were in place at reification time.&lt;/p&gt;
    &lt;p&gt;Q: What happens in multi-threaded environments?&lt;/p&gt;
    &lt;p&gt;A: Lazy import reification is thread-safe. Only one thread will perform the actual import, and the binding is atomically updated. Other threads will see either the lazy proxy or the final resolved object.&lt;/p&gt;
    &lt;p&gt;Q: Can I force reification of a lazy import without using it?&lt;/p&gt;
    &lt;p&gt;A: Yes, accessing a module’s &lt;code&gt;__dict__&lt;/code&gt; will reify all lazy objects in that
module. Individual lazy objects can be resolved by calling their &lt;code&gt;get()&lt;/code&gt;
method.&lt;/p&gt;
    &lt;p&gt;Q: What’s the difference between &lt;code&gt;globals()&lt;/code&gt; and &lt;code&gt;mod.__dict__&lt;/code&gt; for lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Calling &lt;code&gt;globals()&lt;/code&gt; returns the module’s dictionary without reifying lazy
imports – you’ll see lazy proxy objects when accessing them through the
returned dictionary. However, accessing &lt;code&gt;mod.__dict__&lt;/code&gt; from external code
reifies all lazy imports in that module first. This design ensures:&lt;/p&gt;
    &lt;code&gt;# In your module:
lazy import json

g = globals()
print(type(g['json']))  # &amp;lt;class 'lazy_import'&amp;gt; - your problem

# From external code:
import sys
mod = sys.modules['your_module']
d = mod.__dict__
print(type(d['json']))  # &amp;lt;class 'module'&amp;gt; - reified for external access
&lt;/code&gt;
    &lt;p&gt;This distinction means adding lazy imports and calling &lt;code&gt;globals()&lt;/code&gt; is your
responsibility to manage, while external code accessing &lt;code&gt;mod.__dict__&lt;/code&gt;
always sees fully loaded modules.&lt;/p&gt;
    &lt;p&gt;Q: Why not use &lt;code&gt;importlib.util.LazyLoader&lt;/code&gt; instead?&lt;/p&gt;
    &lt;p&gt;A: &lt;code&gt;LazyLoader&lt;/code&gt; has significant limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires verbose setup code for each lazy import.&lt;/item&gt;
      &lt;item&gt;Has ongoing performance overhead on every attribute access.&lt;/item&gt;
      &lt;item&gt;Doesn’t work well with &lt;code&gt;from ... import&lt;/code&gt;statements.&lt;/item&gt;
      &lt;item&gt;Less clear and standard than dedicated syntax.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Will this break tools like &lt;code&gt;isort&lt;/code&gt; or &lt;code&gt;black&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;A: Tools will need updates to recognize the &lt;code&gt;lazy&lt;/code&gt; keyword, but the changes
should be minimal since the import structure remains the same. The keyword
appears at the beginning, making it easy to parse.&lt;/p&gt;
    &lt;p&gt;Q: How do I know if a library is compatible with lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Most libraries should work fine with lazy imports. Libraries that might have issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Those with essential import-time side effects (registration, monkey-patching).&lt;/item&gt;
      &lt;item&gt;Those that expect specific import ordering.&lt;/item&gt;
      &lt;item&gt;Those that modify global state during import.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When in doubt, test lazy imports with your specific use cases.&lt;/p&gt;
    &lt;p&gt;Q: What happens if I globally enable lazy imports mode and a library doesn’t work correctly?&lt;/p&gt;
    &lt;p&gt;A: Note: This is an advanced feature. You can use the lazy imports filter to exclude specific modules that are known to have problematic side effects:&lt;/p&gt;
    &lt;code&gt;import sys

def my_filter(importer, name, fromlist):
    # Don't lazily import modules known to have side effects
    if name in {'problematic_module', 'another_module'}:
        return False  # Import eagerly
    return True  # Allow lazy import

sys.set_lazy_imports_filter(my_filter)
&lt;/code&gt;
    &lt;p&gt;The filter function receives the importer module name, the module being imported, and the fromlist (if using &lt;code&gt;from ... import&lt;/code&gt;). Returning &lt;code&gt;False&lt;/code&gt;
forces an eager import.&lt;/p&gt;
    &lt;p&gt;Alternatively, set the global mode to &lt;code&gt;"disabled"&lt;/code&gt; via &lt;code&gt;-X
lazy_imports=disabled&lt;/code&gt; to turn off all lazy imports for debugging.&lt;/p&gt;
    &lt;p&gt;Q: Can I use lazy imports inside functions?&lt;/p&gt;
    &lt;p&gt;A: No, the &lt;code&gt;lazy&lt;/code&gt; keyword is only allowed at module level. For
function-level lazy loading, use traditional inline imports or move the import
to module level with &lt;code&gt;lazy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Q: What about forwards compatibility with older Python versions?&lt;/p&gt;
    &lt;p&gt;A: Use the &lt;code&gt;__lazy_modules__&lt;/code&gt; global for compatibility:&lt;/p&gt;
    &lt;code&gt;# Works on Python 3.15+ as lazy, eager on older versions
__lazy_modules__ = ['expensive_module', 'expensive_module_2']
import expensive_module
from expensive_module_2 import MyClass
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;__lazy_modules__&lt;/code&gt; attribute is a list of module name strings. When
an import statement is executed, Python checks if the module name being
imported appears in &lt;code&gt;__lazy_modules__&lt;/code&gt;. If it does, the import is
treated as if it had the &lt;code&gt;lazy&lt;/code&gt; keyword (becoming potentially lazy). On
Python versions before 3.15 that don’t support lazy imports, the
&lt;code&gt;__lazy_modules__&lt;/code&gt; attribute is simply ignored and imports proceed
eagerly as normal.&lt;/p&gt;
    &lt;p&gt;This provides a migration path until you can rely on the &lt;code&gt;lazy&lt;/code&gt; keyword. For
maximum predictability, it’s recommended to define &lt;code&gt;__lazy_modules__&lt;/code&gt;
once, before any imports. But as it is checked on each import, it can be
modified between &lt;code&gt;import&lt;/code&gt; statements.&lt;/p&gt;
    &lt;p&gt;Q: How do explicit lazy imports interact with PEP-649/PEP-749&lt;/p&gt;
    &lt;p&gt;A: If an annotation is not stringified, it is an expression that is evaluated at a later time. It will only be resolved if the annotation is accessed. In the example below, the &lt;code&gt;fake_typing&lt;/code&gt; module is only loaded when the user
inspects the &lt;code&gt;__annotations__&lt;/code&gt; dictionary. The &lt;code&gt;fake_typing&lt;/code&gt; module would
also be loaded if the user uses &lt;code&gt;annotationlib.get_annotations()&lt;/code&gt; or
&lt;code&gt;getattr&lt;/code&gt; to access the annotations.&lt;/p&gt;
    &lt;code&gt;lazy from fake_typing import MyFakeType
def foo(x: MyFakeType):
  pass
print(foo.__annotations__)  # Triggers loading the fake_typing module
&lt;/code&gt;
    &lt;p&gt;Q: How do lazy imports interact with &lt;code&gt;dir()&lt;/code&gt;, &lt;code&gt;getattr()&lt;/code&gt;, and
module introspection?&lt;/p&gt;
    &lt;p&gt;A: Accessing lazy imports through normal attribute access or &lt;code&gt;getattr()&lt;/code&gt;
will trigger reification. Calling &lt;code&gt;dir()&lt;/code&gt; on a module will reify all lazy
imports in that module to ensure the directory listing is complete. This is
similar to accessing &lt;code&gt;mod.__dict__&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;lazy import json

# Before any access
# json not in sys.modules

# Any of these trigger reification:
dumps_func = json.dumps
dumps_func = getattr(json, 'dumps')
dir(json)
# Now json is in sys.modules
&lt;/code&gt;
    &lt;p&gt;Q: Do lazy imports work with circular imports?&lt;/p&gt;
    &lt;p&gt;A: Lazy imports don’t automatically solve circular import problems. If two modules have a circular dependency, making the imports lazy might help only if the circular reference isn’t accessed during module initialization. However, if either module accesses the other during import time, you’ll still get an error.&lt;/p&gt;
    &lt;p&gt;Example that works (deferred access in functions):&lt;/p&gt;
    &lt;code&gt;# user_model.py
lazy import post_model

class User:
    def get_posts(self):
        # OK - post_model accessed inside function, not during import
        return post_model.Post.get_by_user(self.name)

# post_model.py
lazy import user_model

class Post:
    @staticmethod
    def get_by_user(username):
        return f"Posts by {username}"
&lt;/code&gt;
    &lt;p&gt;This works because neither module accesses the other at module level – the access happens later when &lt;code&gt;get_posts()&lt;/code&gt; is called.&lt;/p&gt;
    &lt;p&gt;Example that fails (access during import):&lt;/p&gt;
    &lt;code&gt;# module_a.py
lazy import module_b

result = module_b.get_value()  # Error! Accessing during import

def func():
    return "A"

# module_b.py
lazy import module_a

result = module_a.func()  # Circular dependency error here

def get_value():
    return "B"
&lt;/code&gt;
    &lt;p&gt;This fails because &lt;code&gt;module_a&lt;/code&gt; tries to access &lt;code&gt;module_b&lt;/code&gt; at import time,
which then tries to access &lt;code&gt;module_a&lt;/code&gt; before it’s fully initialized.&lt;/p&gt;
    &lt;p&gt;The best practice is still to avoid circular imports in your code design.&lt;/p&gt;
    &lt;p&gt;Q: Will lazy imports affect the performance of my hot paths?&lt;/p&gt;
    &lt;p&gt;A: After first use, lazy imports have zero overhead thanks to the adaptive interpreter. The interpreter specializes the bytecode (e.g., &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;
becomes &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;) which eliminates the lazy check on subsequent
accesses. This means once a lazy import is reified, accessing it is just as
fast as a normal import.&lt;/p&gt;
    &lt;code&gt;lazy import json

def use_json():
    return json.dumps({"test": 1})

# First call triggers reification
use_json()

# After 2-3 calls, bytecode is specialized
use_json()
use_json()
&lt;/code&gt;
    &lt;p&gt;You can observe the specialization using &lt;code&gt;dis.dis(use_json, adaptive=True)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;=== Before specialization ===
LOAD_GLOBAL              0 (json)
LOAD_ATTR                2 (dumps)

=== After 3 calls (specialized) ===
LOAD_GLOBAL_MODULE       0 (json)
LOAD_ATTR_MODULE         2 (dumps)
&lt;/code&gt;
    &lt;p&gt;The specialized &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt; and &lt;code&gt;LOAD_ATTR_MODULE&lt;/code&gt; instructions
are optimized fast paths with no overhead for checking lazy imports.&lt;/p&gt;
    &lt;p&gt;Q: What about &lt;code&gt;sys.modules&lt;/code&gt;? When does a lazy import appear there?&lt;/p&gt;
    &lt;p&gt;A: A lazily imported module does not appear in &lt;code&gt;sys.modules&lt;/code&gt; until it’s
reified (first used). Once reified, it appears in &lt;code&gt;sys.modules&lt;/code&gt; just like
any eager import.&lt;/p&gt;
    &lt;code&gt;import sys
lazy import json

print('json' in sys.modules)  # False

result = json.dumps({"key": "value"})  # First use

print('json' in sys.modules)  # True
&lt;/code&gt;
    &lt;p&gt;Q: Why you chose ``lazy`` as the keyword name?&lt;/p&gt;
    &lt;p&gt;A: Not “why”… memorize! :)&lt;/p&gt;
    &lt;head rend="h2"&gt;Alternate Implementation Ideas&lt;/head&gt;
    &lt;p&gt;Here are some alternative design decisions that were considered during the development of this PEP. While the current proposal represents what we believe to be the best balance of simplicity, performance, and maintainability, these alternatives offer different trade-offs that may be valuable for implementers to consider or for future refinements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leveraging a subclass of dict&lt;/head&gt;
    &lt;p&gt;Instead of updating the internal dict object to directly add the fields needed to support lazy imports, we could create a subclass of the dict object to be used specifically for Lazy Import enablement. This would still be a leaky abstraction though - methods can be called directly such as &lt;code&gt;dict.__getitem__&lt;/code&gt; and it would impact the performance of globals lookup in
the interpreter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternate keyword names&lt;/head&gt;
    &lt;p&gt;For this PEP, we decided to propose &lt;code&gt;lazy&lt;/code&gt; for the explicit keyword as it
felt the most familar to those already focused on optimizing import overhead.
We also considered a variety of other options to support explicit lazy
imports. The most compelling alternates were &lt;code&gt;defer&lt;/code&gt; and &lt;code&gt;delay&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rejected Ideas&lt;/head&gt;
    &lt;head rend="h3"&gt;Modification of the dict object&lt;/head&gt;
    &lt;p&gt;The initial PEP for lazy imports (PEP 690) relied heavily on the modification of the internal dict object to support lazy imports. We recognize that this data structure is highly tuned, heavily used across the codebase, and very performance sensitive. Because of the importance of this data structure and the desire to keep the implementation of lazy imports encapsulated from users who may have no interest in the feature, we’ve decided to invest in an alternate approach.&lt;/p&gt;
    &lt;p&gt;The dictionary is the foundational data structure in Python. Every object’s attributes are stored in a dict, and dicts are used throughout the runtime for namespaces, keyword arguments, and more. Adding any kind of hook or special behavior to dicts to support lazy imports would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prevent critical interpreter optimizations including future JIT compilation.&lt;/item&gt;
      &lt;item&gt;Add complexity to a data structure that must remain simple and fast.&lt;/item&gt;
      &lt;item&gt;Affect every part of Python, not just import behavior.&lt;/item&gt;
      &lt;item&gt;Violate separation of concerns – the hash table shouldn’t know about the import system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Past decisions that violated this principle of keeping core abstractions clean have caused significant pain in the CPython ecosystem, making optimization difficult and introducing subtle bugs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Placing the &lt;code&gt;lazy&lt;/code&gt; keyword in the middle of from imports&lt;/head&gt;
    &lt;p&gt;While we found &lt;code&gt;from foo lazy import bar&lt;/code&gt; to be a really intuitive placement
for the new explicit syntax, we quickly learned that placing the &lt;code&gt;lazy&lt;/code&gt;
keyword here is already syntactically allowed in Python. This is because
&lt;code&gt;from . lazy import bar&lt;/code&gt; is legal syntax (because whitespace does not
matter.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Placing the &lt;code&gt;lazy&lt;/code&gt; keyword at the end of import statements&lt;/head&gt;
    &lt;p&gt;We discussed appending lazy to the end of import statements like such &lt;code&gt;import
foo lazy&lt;/code&gt; or &lt;code&gt;from foo import bar, baz lazy&lt;/code&gt; but ultimately decided that
this approach provided less clarity. For example, if multiple modules are
imported in a single statement, it is unclear if the lazy binding applies to
all of the imported objects or just a subset of the items.&lt;/p&gt;
    &lt;head rend="h3"&gt;Returning a proxy dict from &lt;code&gt;globals()&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;An alternative to reifying on &lt;code&gt;globals()&lt;/code&gt; or exposing lazy objects would be
to return a proxy dictionary that automatically reifies lazy objects when
they’re accessed through the proxy. This would seemingly give the best of both
worlds: &lt;code&gt;globals()&lt;/code&gt; returns immediately without reification cost, but
accessing items through the result would automatically resolve lazy imports.&lt;/p&gt;
    &lt;p&gt;However, this approach is fundamentally incompatible with how &lt;code&gt;globals()&lt;/code&gt; is
used in practice. Many standard library functions and built-ins expect
&lt;code&gt;globals()&lt;/code&gt; to return a real &lt;code&gt;dict&lt;/code&gt; object, not a proxy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;exec(code, globals())&lt;/code&gt;requires a real dict.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;eval(expr, globals())&lt;/code&gt;requires a real dict.&lt;/item&gt;
      &lt;item&gt;Functions that check &lt;code&gt;type(globals()) is dict&lt;/code&gt;would break.&lt;/item&gt;
      &lt;item&gt;Dictionary methods like &lt;code&gt;.update()&lt;/code&gt;would need special handling.&lt;/item&gt;
      &lt;item&gt;Performance would suffer from the indirection on every access.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The proxy would need to be so transparent that it would be indistinguishable from a real dict in almost all cases, which is extremely difficult to achieve correctly. Any deviation from true dict behavior would be a source of subtle bugs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reifying lazy imports when &lt;code&gt;globals()&lt;/code&gt; is called&lt;/head&gt;
    &lt;p&gt;Calling &lt;code&gt;globals()&lt;/code&gt; returns the module’s namespace dictionary without
triggering reification of lazy imports. Accessing lazy objects through the
returned dictionary yields the lazy proxy objects themselves. This is an
intentional design decision for several reasons:&lt;/p&gt;
    &lt;p&gt;The key distinction: Adding a lazy import and calling &lt;code&gt;globals()&lt;/code&gt; is the
module author’s concern and under their control. However, accessing
&lt;code&gt;mod.__dict__&lt;/code&gt; from external code is a different scenario – it crosses
module boundaries and affects someone else’s code. Therefore, &lt;code&gt;mod.__dict__&lt;/code&gt;
access reifies all lazy imports to ensure external code sees fully realized
modules, while &lt;code&gt;globals()&lt;/code&gt; preserves lazy objects for the module’s own
introspection needs.&lt;/p&gt;
    &lt;p&gt;Technical challenges: It is impossible to safely reify on-demand when &lt;code&gt;globals()&lt;/code&gt; is called because we cannot return a proxy dictionary – this
would break common usages like passing the result to &lt;code&gt;exec()&lt;/code&gt; or other
built-ins that expect a real dictionary. The only alternative would be to
eagerly reify all lazy imports whenever &lt;code&gt;globals()&lt;/code&gt; is called, but this
behavior would be surprising and potentially expensive.&lt;/p&gt;
    &lt;p&gt;Performance concerns: It is impractical to cache whether a reification scan has been performed with just the globals dictionary reference, whereas module attribute access (the primary use case) can efficiently cache reification state in the module object itself.&lt;/p&gt;
    &lt;p&gt;Use case rationale: The chosen design makes sense precisely because of this distinction: adding a lazy import and calling &lt;code&gt;globals()&lt;/code&gt; is your
problem to manage, while having lazy imports visible in &lt;code&gt;mod.__dict__&lt;/code&gt;
becomes someone else’s problem. By reifying on &lt;code&gt;__dict__&lt;/code&gt; access but not on
&lt;code&gt;globals()&lt;/code&gt;, we ensure external code always sees fully loaded modules while
giving module authors control over their own introspection.&lt;/p&gt;
    &lt;p&gt;Note that three options were considered:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;or&lt;code&gt;mod.__dict__&lt;/code&gt;traverses and resolves all lazy objects before returning.&lt;/item&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;or&lt;code&gt;mod.__dict__&lt;/code&gt;returns the dictionary with lazy objects present.&lt;/item&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;returns the dictionary with lazy objects, but&lt;code&gt;mod.__dict__&lt;/code&gt;reifies everything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We chose the third option because it properly delineates responsibility: if you add lazy imports to your module and call &lt;code&gt;globals()&lt;/code&gt;, you’re responsible
for handling the lazy objects. But external code accessing your module’s
&lt;code&gt;__dict__&lt;/code&gt; shouldn’t need to know about your lazy imports – it gets fully
resolved modules.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We would like to thank Paul Ganssle, Yury Selivanov, Łukasz Langa, Lysandros Nikolaou, Pradyun Gedam, Mark Shannon, Hana Joo and the Python Google team, the Python team(s) @ Meta, the Python @ HRT team, the Bloomberg Python team, the Scientific Python community, everyone who participated in the initial discussion of PEP 690, and many others who provided valuable feedback and insights that helped shape this PEP.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;head rend="h2"&gt;Copyright&lt;/head&gt;
    &lt;p&gt;This document is placed in the public domain or under the CC0-1.0-Universal license, whichever is more permissive.&lt;/p&gt;
    &lt;p&gt;Source: https://github.com/python/peps/blob/main/peps/pep-0810.rst&lt;/p&gt;
    &lt;p&gt;Last modified: 2025-10-03 20:29:13 GMT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pep-previews--4622.org.readthedocs.build/pep-0810/"/><published>2025-10-03T18:24:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45467500</id><title>Offline card payments should be possible no later than 1 July 2026</title><updated>2025-10-04T20:33:49.881906+00:00</updated><content>&lt;doc fingerprint="cb8cfa25092dfffd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Offline card payments should be possible no later than 1 July 2026&lt;/head&gt;
    &lt;p&gt;Press release The Riksbank and representatives from the payment market have today reached an agreement to increase the possibility to make offline card payments for essential goods. The agreement is an important step in the work to strengthen Sweden's payment preparedness and increase resilience to disruptions in the digital payments system. The goal is for the measures to be in place no later than 1 July 2026.&lt;/p&gt;
    &lt;p&gt;“In Sweden, we pay digitally to a large degree and the use of cash is low. The general public being able to pay by card for example for food and medicines even in the event of a serious breakdown in data communication, that is offline, is a milestone in our intensified efforts to strengthen emergency preparedness”, says Governor Erik Thedéen.&lt;/p&gt;
    &lt;p&gt;The agreement describes the measures that participants in Swedish card payments – card issuers, card networks, card acquirers, the retail sector and the Riksbank – will implement to increase the possibility of offline payments by card. For instance, financial agents will adapt their regulatory frameworks, and the retail trade will introduce technological solutions. The Riksbank is leading this work and is responsible for monitoring its implementation.&lt;/p&gt;
    &lt;p&gt;“We are very pleased that all participants involved are taking responsibility for strengthening Sweden's payment readiness. Some are covered by the Riksbank's regulations, but far from all. We regard the fact that so many are nevertheless choosing to contribute as very positive for Sweden's overall civil preparedness”, concludes Erik Thedéen.&lt;/p&gt;
    &lt;p&gt;The online function shall apply to physical payment cards and accompanying PIN code when purchasing essential goods such as food, medicine and fuel. The Riksbank will continue its work on enabling offline payments for other payment methods after 1 July 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.riksbank.se/en-gb/press-and-published/notices-and-press-releases/press-releases/2025/offline-card-payments-should-be-possible-no-later-than-1-july-2026/"/><published>2025-10-03T20:36:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45468698</id><title>Zig builds are getting faster</title><updated>2025-10-04T20:33:49.807048+00:00</updated><content>&lt;doc fingerprint="adc8b06e29012720"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;Zig Builds Are Getting Faster&lt;/head&gt;
    &lt;p&gt;Andrew Kelley famously (or infamously, depending on your views) said "the compiler is too damn slow, that's why we have bugs."1&lt;/p&gt;
    &lt;p&gt;As a result, one of the primary stated goals of Zig for years has been faster compile times. The Zig team has been working on extremely hard problems to make this a reality (such as yeeting LLVM, writing their own code generation backends, building their own linkers, and marching towards incremental compilation in general).2&lt;/p&gt;
    &lt;p&gt;The fruits of this multi-year labor are finally starting to show with Zig 0.15.1. The Ghostty project just completed upgrading to Zig 0.15.1, and I'd like to share some real-world build times.3&lt;/p&gt;
    &lt;head rend="h2"&gt;Build Script Compilation&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 7sec 167ms&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 1sec 702ms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to build the &lt;code&gt;build.zig&lt;/code&gt; script itself. The
times above were measured by running &lt;code&gt;zig build --help&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A well-written build script should only rebuild itself rarely. However, this is a cost every new uncached source build will pay (e.g. a user downloading the project to build from source one time). As such, it directly impacts the time to build a usable binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Full Uncached Ghostty Binary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 41sec&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 32sec&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This includes the time to build the build script itself. Given the prior results, Zig 0.15 is building everything else ~2 seconds faster. But, you can still see in wall time the change in this initial build time.&lt;/p&gt;
    &lt;p&gt;Important: most of this is still using LLVM. Ghostty still can't fully build and link using the self-hosted x86_64 backend, since the backend still has bugs. So, this just shows the general improvements in the Zig compiler itself, even with LLVM in the picture.&lt;/p&gt;
    &lt;p&gt;Once Ghostty can use the self-hosted x86_64 backend completely, I expect this time to plummet to around 25 seconds or less, fully half the time it would take with Zig 0.14.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incremental Build (Ghostty Executable)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 19sec&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 16sec&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to rebuild Ghostty after a one-line change to the most core terminal emulation code (adding a log function call to the escape sequence parser).&lt;/p&gt;
    &lt;p&gt;This build has a fully cached build script and dependency graph, so it is only rebuilding what it needs to. Incremental compilation in Zig isn't functional yet, so this still recompiles a considerable amount of code. Additionally, as with the prior section, this is still using LLVM. By simply dropping LLVM out of the picture, I expect this time to drop to around 12 seconds or so (less the time LLVM is emitting).&lt;/p&gt;
    &lt;p&gt;Going further, once Zig supports incremental compilation, I expect we'll be able to measure incremental builds like this within milliseconds at worst. But, let's wait and see when that is reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incremental Build (libghostty-vt)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 2sec 884ms&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 975ms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to rebuild only libghostty-vt after a one-line change. Unlike the Ghostty executable, &lt;code&gt;libghostty-vt&lt;/code&gt;
is fully functional with the self-hosted x86_64 backend, so this
shows the differences in build times without LLVM in the picture.&lt;/p&gt;
    &lt;p&gt;Similar to the Ghostty executable, this is still rebuilding the full Zig module for &lt;code&gt;libghostty-vt&lt;/code&gt;, since incremental compilation isn't
fully functional yet. I expect this to also drop to single-digit milliseconds
at worst once incremental compilation is a reality.&lt;/p&gt;
    &lt;p&gt;But still, a sub-second build time for a non-trivial library is amazing. This is the library I'm spending most of my time working on right now, and even in a few short days since upgrading to Zig 0.15.1, I've felt a huge difference in my workflow. Previously, I might tab out to read an email between builds or tests, but now its so fast I can stay in flow in my terminal.&lt;/p&gt;
    &lt;p&gt;This improvement is most indicative of what's to come in the short term. The self-hosted x86_64 backend is already stable enough to build all debug builds by default and the aarch64 backend is getting there, too. We aren't able to build the full Ghostty executable yet, but I bet this will get ironed out within months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Faster Builds Are Here&lt;/head&gt;
    &lt;p&gt;As you can see, building Ghostty with Zig 0.15.1 is faster in every single scenario, despite the fact that a lot of Ghostty still can't even take advantage of the self-hosted backend! And despite the fact that incremental compilation isn't functional yet!&lt;/p&gt;
    &lt;p&gt;I've loved betting on Zig for Ghostty, and I love that they're focusing on compile times. These improvements are real, and they're here now. And I suspect in the next couple years, the results posted today will look downright slow. 😜&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Timestamped link: https://youtu.be/5eL_LcxwwHg?t=565 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This ignores an astronomical amount of work that has gone into making every aspect of the Zig compiler faster, more parallelizable, etc. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;All measurements done on the same x86_64 Linux machine. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/zig-builds-getting-faster"/><published>2025-10-03T22:45:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45469579</id><title>New antibiotic targets IBD and AI predicted how it would work</title><updated>2025-10-04T20:33:49.573598+00:00</updated><content/><link href="https://healthsci.mcmaster.ca/new-antibiotic-targets-ibd-and-ai-predicted-how-it-would-work-before-scientists-could-prove-it/"/><published>2025-10-04T01:09:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45471136</id><title>Alibaba cloud FPGA: the $200 Kintex UltraScale+</title><updated>2025-10-04T20:33:49.330958+00:00</updated><content>&lt;doc fingerprint="bd18a05c4875d6f6"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction#&lt;/head&gt;
    &lt;p&gt;I was recently in the market for a new FPGA to start building my upcoming projects on.&lt;/p&gt;
    &lt;p&gt;Due to the scale of my upcoming projects a Xilinx series 7 UltraScale+ FPGA of the Virtex family would be perfect, but a Kintex series FPGA will be sufficient for early prototyping. Due to not wanting to part ways with the eye watering amounts of money that is required for an Vivado enterprise edition license my choice was effectively narrowed to the FPGA chips available under the WebPack version of Vivado.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly Xilinx are well aware of how top of the range the Virtex series are, and doesn’t offer any Virtex UltraScale+ chips with the webpack license. That said, they do offer support for two very respectable Kintex UltraScale+ FPGA models, the &lt;code&gt;XCKU3P&lt;/code&gt; and the &lt;code&gt;XCKU5P&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;These two chips are far from being small hobbyist toys, with the smaller &lt;code&gt;XCUK3P&lt;/code&gt; already boasting +162K LUTs and
16 GTY transceivers, capable, depending on the physical constraints imposed by the chip packaging of
operating at up to 32.75Gb/s.&lt;/p&gt;
    &lt;p&gt;Now that the chip selection has been narrowed down I set out to look for a dev board.&lt;/p&gt;
    &lt;p&gt;My requirements for the board where that it featured :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;at least 2 SFP+ or 1 QSFP connector&lt;/item&gt;
      &lt;item&gt;a JTAG interface&lt;/item&gt;
      &lt;item&gt;a PCIe interface at least x8 wide&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As to where to get the board from, my options where :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Design the board myself&lt;/item&gt;
      &lt;item&gt;Get the AXKU5 or AXKU3 from Alinx&lt;/item&gt;
      &lt;item&gt;See what I could unearth on the second hand market&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Although option &lt;code&gt;1&lt;/code&gt; could have been very interesting, designing a
dev board with both a high speed PCIe and ethernet interface was not the goal of
today’s project.&lt;/p&gt;
    &lt;p&gt;As for option &lt;code&gt;2&lt;/code&gt;,
Alinx is newer vendor that is still building up its credibility in the west,
their technical documentation is a bit sparse, but the feedback seems to be positive with no major issues being reported.
Most importantly, Alinx provided very fairly priced development boards
in the 900 to 1050 dollar range ( +150$ for the HPC FMC SFP+ extension board ).
Although these are not cheap by any metric, compared to the competitions
price point, they are the best value.&lt;/p&gt;
    &lt;p&gt;Option &lt;code&gt;2&lt;/code&gt; was coming up ahead until I stumbled upon this ebay listing :&lt;/p&gt;
    &lt;p&gt;For 200$ this board featured a &lt;code&gt;XCKU3P-FFVB676&lt;/code&gt;, 2 SPF+ connector and a x8 PCIe interface.
On the flip side it came with no documentation whatsoever, no guaranty it worked, and the
faint promise in the listing that there was a JTAG interface.
A sane person would likely have dismissed this as an interesting internet oddity, a remanence
of what happens when a generation of accelerator cards gets phased out in favor of the next,
or maybe just an expensive paperweight.&lt;/p&gt;
    &lt;p&gt;But I like a challenge, and the appeal of unlocking the 200$ Kintex UltraScale+ development board was too great to ignore.&lt;/p&gt;
    &lt;p&gt;As such, I aim for this article to become the documentation paving the way to though this mirage.&lt;/p&gt;
    &lt;head rend="h2"&gt;The debugger challenge#&lt;/head&gt;
    &lt;p&gt;Xilinx’s UG908 Programming and Debugging User Guide (Appendix D) specifies their blessed JTAG probe ecosystem for FPGA configuration and debug. Rather than dropping $100+ on yet another proprietary dongle that’ll collect dust after the project ends, I’m exploring alternatives. The obvious tradeoff: abandoning Xilinx’s toolchain means losing ILA integration. However, the ILA fundamentally just captures samples and streams them via JTAG USER registers, there’s nothing preventing us from building our own logic analyzer with equivalent functionality and a custom host interface.&lt;/p&gt;
    &lt;p&gt;Enter OpenOCD. While primarily targeting ARM/RISC-V SoCs, it maintains an impressive database of supported probe hardware and provides granular control over JTAG operations. More importantly, it natively supports SVF (Serial Vector Format), a vendor-neutral bitstream format that Vivado can export.&lt;/p&gt;
    &lt;p&gt;The documentation landscape is admittedly sparse for anything beyond 7-series FPGAs, and the most recent OpenOCD documentation I could unearth was focused on Zynq ARM core debugging rather than fabric configuration. But the fundamentals remain sound: JTAG is JTAG, SVF is standardized, and the boundary scan architecture hasn’t fundamentally changed.&lt;/p&gt;
    &lt;p&gt;The approach should be straightforward: generate SVF from Vivado, feed it through OpenOCD with a commodity JTAG adapter, and validate the configuration. Worst case, we’ll need to patch some adapter-specific quirks or boundary scan chain register addresses. Time to find out if this theory holds up in practice.&lt;/p&gt;
    &lt;head rend="h2"&gt;The plan#&lt;/head&gt;
    &lt;p&gt;So, to resume, the current plan is to buy a second hand hardware accelerator of eBay at a too good to be true price, and try to configure it with an unofficial probe using open source software without any clear official support.&lt;lb/&gt;The answer to the obvious question you are thinking if you, like me, have been around the block a few times is: many things.&lt;/p&gt;
    &lt;p&gt;As such, we need a plan for approaching this. The goal of this plan is to outline incremental steps that will build upon themselves with the end goal of being able to use this as a dev board.&lt;/p&gt;
    &lt;head rend="h3"&gt;1 - Confirming the board works#&lt;/head&gt;
    &lt;p&gt;First order of business will be to confirm the board is showing signs of working as intended.&lt;/p&gt;
    &lt;p&gt;There is a high probability that the flash wasn’t wiped before this board was sold off, as such the previous bitstream should still be in the flash. Given this board was used as an accelerator, we should be able to use that to confirm the board is working by either checking if the board is presenting itself as a PCIe endpoint or if the SFP’s are sending the ethernet PHY idle sequence.&lt;/p&gt;
    &lt;head rend="h3"&gt;2 - Connecting a debugger to it#&lt;/head&gt;
    &lt;p&gt;The next step is going to be to try and connect the debugger. The eBay listing advertised there is a JTAG interface, but the picture is grainy enough that where that JTAG is and what pins are available is unclear.&lt;/p&gt;
    &lt;p&gt;Additionally, we have no indication of what devices are daisy chained together onto the JTAG scan chain. This is an essential question for flashing over JTAG, so it will need to be figured out.&lt;/p&gt;
    &lt;p&gt;At this point, it would also be strategic to try and do some more probing into the FPGA via JTAG. Xilinx FPGAs exposes a handful of useful system registers accessible over JTAG. The most well known of these interfaces is the SYSMON, which allows us, among other things, to get real time temperature and voltage reading from inside the chip. Although openOCD doesn’t have SYSMON support out of the box it would be worth while to build it, to :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Familiarise myself with openOCD scripting, this might come in handy when building my ILA replacement down the line&lt;/item&gt;
      &lt;item&gt;Having an easy side channel to monitor FPGA operating parameters&lt;/item&gt;
      &lt;item&gt;Make a contribution to openOCD as it have support for the interfacing with XADC but not SYSMON&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;3 - Figuring out the Pinout#&lt;/head&gt;
    &lt;p&gt;The hardest part will be figuring out the FPGA’s pinout and my clock sources. The questions that need answering are :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;what external clocks sources do I have, what are there frequencies and which pins are they connected to&lt;/item&gt;
      &lt;item&gt;which transceivers are the SFPs connected to&lt;/item&gt;
      &lt;item&gt;which transceivers is the PCIe connected to&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;4 - Writing a bitstream#&lt;/head&gt;
    &lt;p&gt;For now I will be focusing on writing a temporary configurations over JTAG to the CCLs and not re-writing the flash.&lt;/p&gt;
    &lt;p&gt;That plan is to trying writing either the bitstream directly though openOCD’s &lt;code&gt;virtex2&lt;/code&gt; + &lt;code&gt;pld&lt;/code&gt; drivers, or by replaying the
SVF generated by Vivado.&lt;/p&gt;
    &lt;p&gt;Since I believe a low iteration time is paramount to project velocity and getting big things done, I also want automatize all of the Vivado flow from taking the rtl to the SVF generation.&lt;/p&gt;
    &lt;p&gt;Simple enough ?&lt;/p&gt;
    &lt;head rend="h2"&gt;Liveness test#&lt;/head&gt;
    &lt;p&gt;A few days later my prize arrived via express mail.&lt;/p&gt;
    &lt;p&gt;Unexpectedly it even came with a free 25G SFP28 Huawei transceiver rated for a 300m distance and a single 1m long OS2 fiber patch cable. This was likely not intentional as the transceiver was jammed in the SFP cage, but it was still very generous of them to include the fiber patch cable.&lt;/p&gt;
    &lt;p&gt;The board also came with a travel case and half of a PCIe to USB adapter and a 12V power supply that one could use to power the board as a standalone device. Although this standalone configuration will not be of any use to me, for those looking to develop just networking interfaces without any PCIe interface, this could come in handy.&lt;/p&gt;
    &lt;p&gt;Overall the board looked a little worn, but both the transceiver cages and PCIe connectors didn’t look to be damaged.&lt;/p&gt;
    &lt;head rend="h3"&gt;Standalone configuration#&lt;/head&gt;
    &lt;p&gt;Before real testing could start I first did a small power-up test using the PCIe to USB adapter that the seller provided. I was able to do a quick check using the LEDs and the FPGAs dissipated heat that the board seemed to be powering up at a surface level (pun intended).&lt;/p&gt;
    &lt;head rend="h3"&gt;PCIe interface#&lt;/head&gt;
    &lt;p&gt;Since I didn’t want to directly plug mystery hardware into my prized build server, I decided to use a Raspberry Pi 5 as my sacrificial test device and got myself an external PCIe adapter.&lt;/p&gt;
    &lt;p&gt;It just so happened that the latest Raspberry Pi version, the Pi 5, now features an external PCIe Gen 2.0 x1 interface. Though our FPGA can handle up to a PCIe Gen 3.0 and the board had a x8 wide interface, since PCIe standard is backwards compatible and the number of lanes on the interface can be downgraded, plugging our FPGA with this Raspberry Pi will work.&lt;/p&gt;
    &lt;p&gt;After both the Raspberry and the FPGA were booted, I SSHed into my rpi and started looking for the PCIe enumeration sequence logged from the Linux PCIe core subsystem.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;dmesg&lt;/code&gt; log :&lt;/p&gt;
    &lt;code&gt;[    0.388790] pci 0000:00:00.0: [14e4:2712] type 01 class 0x060400
[    0.388817] pci 0000:00:00.0: PME# supported from D0 D3hot
[    0.389752] pci 0000:00:00.0: bridge configuration invalid ([bus 00-00]), reconfiguring
[    0.495733] brcm-pcie 1000110000.pcie: link up, 5.0 GT/s PCIe x1 (!SSC)
[    0.495759] pci 0000:01:00.0: [dabc:1017] type 00 class 0x020000
&lt;/code&gt;
    &lt;head rend="h4"&gt;Background information#&lt;/head&gt;
    &lt;p&gt;Since most people might not be intimately as familiar with PCIe terminology, allow me to quickly document what is going on here.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;0000:00:00.0&lt;/code&gt;: is the identifier of a specific PCIe device connected through the PCIe network
to the kernel, it read as &lt;code&gt;domain&lt;/code&gt;:&lt;code&gt;bus&lt;/code&gt;:&lt;code&gt;device&lt;/code&gt;.&lt;code&gt;function&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;[14e4:2712]&lt;/code&gt;: is the device’s &lt;code&gt;[vendor id:device id]&lt;/code&gt;, these vendor id identifiers are
assigned by the PCI standard body to hardware vendors. Vendors are then free to define there
own vendor id’s.&lt;/p&gt;
    &lt;p&gt;The full list of official vendor id’s and released device id can be found : https://admin.pci-ids.ucw.cz/read/PC/14e4 or in the linux kernel code : https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;type 01&lt;/code&gt;: PCIe has two types of devices, bridges allowing the connection of multiple downstream devices to an
upstream device, and endpoints are the leafs.
Bridges are of type &lt;code&gt;01&lt;/code&gt; and endpoints of type &lt;code&gt;00&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;class 0x60400&lt;/code&gt;: is the PCIe device class, it categorizes the kind of function the device performs. It
uses the following format &lt;code&gt;0x[Base Class (8 bits)][Sub Class (8 bits)][Programming Interface (8 bits)]&lt;/code&gt;,
( note : the sub class field might be unused ).&lt;/p&gt;
    &lt;p&gt;A list of class and sub class identifiers can be found: https://admin.pci-ids.ucw.cz/read/PD or again in the linux codebase : https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158&lt;/p&gt;
    &lt;head rend="h4"&gt;Dmesg log#&lt;/head&gt;
    &lt;p&gt;The two most interesting lines of the &lt;code&gt;dmesg&lt;/code&gt; log are :&lt;/p&gt;
    &lt;code&gt;[    0.388790] pci 0000:00:00.0: [14e4:2712] type 01 class 0x060400
[    0.495759] pci 0000:01:00.0: [dabc:1017] type 00 class 0x020000
&lt;/code&gt;
    &lt;p&gt;Firstly the PCIe subsystem logs that at &lt;code&gt;0000:00:00.0&lt;/code&gt; it has discovered a Broadcom BCM2712 PCIe Bridge ( vendor id &lt;code&gt;14e4&lt;/code&gt;, device id &lt;code&gt;0x2712&lt;/code&gt; ).This bridge (type &lt;code&gt;01&lt;/code&gt;) class &lt;code&gt;0x0604xx&lt;/code&gt; tells us it is a PCI-to-PCI bridge, meaning it is essentially creating additional PCIe lanes downstream for endpoint devices or additional bridges.&lt;/p&gt;
    &lt;p&gt;The subsystem then discovers a second device at &lt;code&gt;0000:01:00.0&lt;/code&gt;, this is an endpoint (type &lt;code&gt;00&lt;/code&gt;), and class &lt;code&gt;0x02000&lt;/code&gt; tells us it is an ethernet networking equipment.&lt;lb/&gt;Of note &lt;code&gt;dabc&lt;/code&gt; doesn’t correspond to a known vendor id.
When designing a PCIe interface in hardware these
are parameters we can configured. Additionally, among the different ways Linux uses to identify which driver to load for a PCIe device
the vendor id and device id can be used for matching. Supposing we are implementing custom logic, in order to prevent any bug where the wrong driver
might be loaded, it is best to use a separate vendor id.
This also helps identify your custom accelerator at a glance and use it to load your custom driver.&lt;/p&gt;
    &lt;p&gt;As such, it is not surprising to see an unknown vendor id appear for an FPGA, this with the class as an ethernet networking device is a strong hint this is our board.&lt;/p&gt;
    &lt;head rend="h4"&gt;Full PCIe device status#&lt;/head&gt;
    &lt;p&gt;Dmesg logs have already given us a good indication that our FPGA board and its PCIe interface was working but to confirm with certainty that the device with vendor id &lt;code&gt;dabc&lt;/code&gt; is our FPGA we now turn to &lt;code&gt;lspci&lt;/code&gt;.
&lt;code&gt;lspci -vvv&lt;/code&gt; is the most verbose output and gives us a full overview of the detected PCIe devices capabilities and current configurations.&lt;/p&gt;
    &lt;p&gt;Broadcom bridge:&lt;/p&gt;
    &lt;code&gt;0000:00:00.0 PCI bridge: Broadcom Inc. and subsidiaries BCM2712 PCIe Bridge (rev 21) (prog-if 00 [Normal decode])
        Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;gt;SERR- &amp;lt;PERR- INTx-
        Latency: 0
        Interrupt: pin A routed to IRQ 38
        Bus: primary=00, secondary=01, subordinate=01, sec-latency=0
        Memory behind bridge: [disabled] [32-bit]
        Prefetchable memory behind bridge: 1800000000-182fffffff [size=768M] [32-bit]
        Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;lt;SERR- &amp;lt;PERR-
        BridgeCtl: Parity- SERR- NoISA- VGA- VGA16- MAbort- &amp;gt;Reset- FastB2B-
                PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
        Capabilities: [48] Power Management version 3
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold-)
                Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=1 PME-
        Capabilities: [ac] Express (v2) Root Port (Slot-), MSI 00
                DevCap: MaxPayload 512 bytes, PhantFunc 0
                        ExtTag- RBE+
                DevCtl: CorrErr- NonFatalErr- FatalErr- UnsupReq-
                        RlxdOrd+ ExtTag- PhantFunc- AuxPwr+ NoSnoop+
                        MaxPayload 512 bytes, MaxReadReq 512 bytes
                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 5GT/s, Width x1, ASPM L0s L1, Exit Latency L0s &amp;lt;2us, L1 &amp;lt;4us
                        ClockPM+ Surprise- LLActRep- BwNot+ ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s, Width x1
                        TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt+
                RootCap: CRSVisible+
                RootCtl: ErrCorrectable- ErrNon-Fatal- ErrFatal- PMEIntEna+ CRSVisible+
                RootSta: PME ReqID 0000, PMEStatus- PMEPending-
                DevCap2: Completion Timeout: Range ABCD, TimeoutDis+ NROPrPrP- LTR+
                         10BitTagComp- 10BitTagReq- OBFF Via WAKE#, ExtFmt- EETLPPrefix-
                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
                         FRS- LN System CLS Not Supported, TPHComp- ExtTPHComp- ARIFwd+
                         AtomicOpsCap: Routing- 32bit- 64bit- 128bitCAS-
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled, ARIFwd-
                         AtomicOpsCtl: ReqEn- EgressBlck-
                LnkCap2: Supported Link Speeds: 2.5-5GT/s, Crosslink- Retimer- 2Retimers- DRS+
                LnkCtl2: Target Link Speed: 5GT/s, EnterCompliance- SpeedDis-
                         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
                         Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete- EqualizationPhase1-
                         EqualizationPhase2- EqualizationPhase3- LinkEqualizationRequest-
                         Retimer- 2Retimers- CrosslinkRes: unsupported, DRS-
                         DownstreamComp: Link Up - Present
        Capabilities: [100 v1] Advanced Error Reporting
                UESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
                CESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
                CEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
                AERCap: First Error Pointer: 00, ECRCGenCap+ ECRCGenEn- ECRCChkCap+ ECRCChkEn-
                        MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
                HeaderLog: 00000000 00000000 00000000 00000000
                RootCmd: CERptEn+ NFERptEn+ FERptEn+
                RootSta: CERcvd- MultCERcvd- UERcvd- MultUERcvd-
                         FirstFatal- NonFatalMsg- FatalMsg- IntMsg 0
                ErrorSrc: ERR_COR: 0000 ERR_FATAL/NONFATAL: 0000
        Capabilities: [160 v1] Virtual Channel
                Caps:   LPEVC=0 RefClk=100ns PATEntryBits=1
                Arb:    Fixed- WRR32- WRR64- WRR128-
                Ctrl:   ArbSelect=Fixed
                Status: InProgress-
                VC0:    Caps:   PATOffset=00 MaxTimeSlots=1 RejSnoopTrans-
                        Arb:    Fixed- WRR32- WRR64- WRR128- TWRR128- WRR256-
                        Ctrl:   Enable+ ID=0 ArbSelect=Fixed TC/VC=ff
                        Status: NegoPending- InProgress-
        Capabilities: [180 v1] Vendor Specific Information: ID=0000 Rev=0 Len=028 &amp;lt;?&amp;gt;
        Capabilities: [240 v1] L1 PM Substates
                L1SubCap: PCI-PM_L1.2+ PCI-PM_L1.1+ ASPM_L1.2+ ASPM_L1.1+ L1_PM_Substates+
                          PortCommonModeRestoreTime=8us PortTPowerOnTime=10us
                L1SubCtl1: PCI-PM_L1.2- PCI-PM_L1.1- ASPM_L1.2- ASPM_L1.1-
                           T_CommonMode=1us LTR1.2_Threshold=0ns
                L1SubCtl2: T_PwrOn=10us
        Capabilities: [300 v1] Secondary PCI Express
                LnkCtl3: LnkEquIntrruptEn- PerformEqu-
                LaneErrStat: 0
        Kernel driver in use: pcieport
&lt;/code&gt;
    &lt;p&gt;FPGA board:&lt;/p&gt;
    &lt;code&gt;0000:01:00.0 Ethernet controller: Device dabc:1017
        Subsystem: Red Hat, Inc. Device a001
        Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;gt;SERR- &amp;lt;PERR- INTx-
        Region 0: Memory at 1820000000 (64-bit, prefetchable) [disabled] [size=2K]
        Region 2: Memory at 1800000000 (64-bit, prefetchable) [disabled] [size=512M]
        Capabilities: [40] Power Management version 3
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)
                Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=0 PME-
        Capabilities: [70] Express (v2) Endpoint, MSI 00
                DevCap: MaxPayload 1024 bytes, PhantFunc 0, Latency L0s &amp;lt;64ns, L1 &amp;lt;1us
                        ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset- SlotPowerLimit 0W
                DevCtl: CorrErr+ NonFatalErr+ FatalErr+ UnsupReq+
                        RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+
                        MaxPayload 512 bytes, MaxReadReq 512 bytes
                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s (downgraded), Width x1 (downgraded)
                        TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-
                DevCap2: Completion Timeout: Range BC, TimeoutDis+ NROPrPrP- LTR-
                         10BitTagComp- 10BitTagReq- OBFF Not Supported, ExtFmt- EETLPPrefix-
                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
                         FRS- TPHComp- ExtTPHComp-
                         AtomicOpsCap: 32bit- 64bit- 128bitCAS-
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled,
                         AtomicOpsCtl: ReqEn-
                LnkCap2: Supported Link Speeds: 2.5-8GT/s, Crosslink- Retimer- 2Retimers- DRS-
                LnkCtl2: Target Link Speed: 8GT/s, EnterCompliance- SpeedDis-
                         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
                         Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete- EqualizationPhase1-
                         EqualizationPhase2- EqualizationPhase3- LinkEqualizationRequest-
                         Retimer- 2Retimers- CrosslinkRes: unsupported
        Capabilities: [100 v1] Advanced Error Reporting
                UESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
                CESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
                CEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
                AERCap: First Error Pointer: 00, ECRCGenCap- ECRCGenEn- ECRCChkCap- ECRCChkEn-
                        MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
                HeaderLog: 00000000 00000000 00000000 00000000
        Capabilities: [1c0 v1] Secondary PCI Express
                LnkCtl3: LnkEquIntrruptEn- PerformEqu-
                LaneErrStat: 0
&lt;/code&gt;
    &lt;p&gt;For our board, the following lines are particularly interesting:&lt;/p&gt;
    &lt;code&gt;                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s (downgraded), Width x1 (downgraded)0x060400
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;LnkCap&lt;/code&gt; tells us about the full capabilities of this PCIe device, here we can see that
the current design supports PCIe Gen 3.0 x8.
The &lt;code&gt;LnkSta&lt;/code&gt; tells us the current configuration, here we have been downgraded to PCIe Gen 2.0 at 5GT/s with a width of only x1.&lt;/p&gt;
    &lt;p&gt;During startup of when a new PCIe device is plugged, PCIe performs a link speed and width negotiation where it tries to reach the highest supported stable configuration for the current system. In our current system, though our FPGA is capable of 8GT/s, as it is located downstream of the Broadcom bridge with a maximum link capacity of Gen 2.0 ( 5GT/s ), the FPGA has been downgraded to 5GT/s.&lt;/p&gt;
    &lt;p&gt;As for the width of x1, that is expected since the Broadcom bridge is also only x1 wide, and our board’s other 7 PCIe lanes are literally hanging over the side.&lt;/p&gt;
    &lt;p&gt;Thus, we can finally confirm that this is our board and that the PCIe interface is working. We can now proceed to establishing the JTAG connection.&lt;/p&gt;
    &lt;head rend="h2"&gt;JTAG interface#&lt;/head&gt;
    &lt;p&gt;Xilinx FPGAs can be configured by writing a bitstream to their internal CMOS Configuration Latches (CCL). CCL is SRAM memory and volatile, thus the configuration is re-done on every power cycle. For devices in the field this bitstream would be read from an external SPI memory during initialization, or written from an external device, such as an embedded controller. But for development purposes overwriting the contents of the CCLs over JTAG is acceptable.&lt;/p&gt;
    &lt;p&gt;This configuration is done by shifting in the entire FPGA bitstream into the device’s configuration logic over the JTAG bus.&lt;/p&gt;
    &lt;head rend="h3"&gt;FPGA board JTAG interface#&lt;/head&gt;
    &lt;p&gt;As promised by the original eBay listing the board did come with an accessible JTAG interface, and gloriously enough, this time there wasn’t even the need for any additional soldering.&lt;/p&gt;
    &lt;p&gt;In addition to a power reference, and ground, conformely to the Xilinx JTAG interface it featured the four mandatory signals comprising the JTAG TAP :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TCK Test Clock&lt;/item&gt;
      &lt;item&gt;TMS Test Mode Select&lt;/item&gt;
      &lt;item&gt;TDI Test Data Input&lt;/item&gt;
      &lt;item&gt;TDO Test Data Output&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of note, the JTAG interface can also come with an independent reset signal. But since Xilinx JTAG interfaces do not have this independent reset signal, we be using the JTAG FSM reset state for our reset signal.&lt;/p&gt;
    &lt;p&gt;This interface layout doesn’t follow a standard layout so I cannot just plug in one of my debug probes, it requires some re-wiring.&lt;/p&gt;
    &lt;head rend="h3"&gt;Segger JLINK :heart:#&lt;/head&gt;
    &lt;p&gt;I do not own an AMD approved JTAG programmer.&lt;/p&gt;
    &lt;p&gt;Traditionally speaking, the Segger JLink is used for debugging embedded CPUs let them be standalone or in a Zynq, and not for configuring FPGAs.&lt;/p&gt;
    &lt;p&gt;That said, all we need to do is use JTAG to shift in a bitstream to the CCLs, so technically speaking any programmable device with 4 sufficiently fast GPIOs can be used as a JTAG programmer. Additionally, the JLink is well supported by OpenOCD, the JLink’s libraries are open source, and I happened to own one.&lt;/p&gt;
    &lt;head rend="h4"&gt;Wiring#&lt;/head&gt;
    &lt;p&gt;Rewiring :&lt;/p&gt;
    &lt;p&gt;JTAG is a parallel protocol where &lt;code&gt;TDI&lt;/code&gt; and &lt;code&gt;TMS&lt;/code&gt; will be captured according to &lt;code&gt;TCK&lt;/code&gt;.
Because of this, good JTAG PCB trace length matching is advised in order to minimize skew.&lt;/p&gt;
    &lt;p&gt;Ideally a custom connector with length matched traces to work as an interface between the JLink’s probe and a board specific connector would be used.&lt;/p&gt;
    &lt;p&gt;Yet, here we are shoving breadboard wires between our debugger and the board. Since OpenOCD allows us to easily control the debugger clock speed, we can increase the skew tolerance by slowing down the TCK clock signal. As such there is no immediate need for a custom connector but we will not be able to reach the maximum JTAG speeds.&lt;/p&gt;
    &lt;p&gt;No issues were encountered at these speeds.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenOCD#&lt;/head&gt;
    &lt;p&gt;OpenOCD is a free and open source on-chip debugger software that aims to be compatible with as many probes, boards and chips as possible.&lt;/p&gt;
    &lt;p&gt;Since OpenOCD has support for the standard SVF file format, my plan for my flashing flow will be to use Vivado to generate the SVF and have OpenOCD flash it. Now, some of you might be starting to notice that I am diverging quite far from the well lit path of officially supported tools. Not only am I using a not officially supported debug probe, but I am also using some obscure open source software with questionable support for interfacing with Xilinx UltraScale+ FPGAs. You might be wondering, given that the officially supported tools can already prove themselves to be a headache to get working properly, why am I seemingly making my life even harder?&lt;/p&gt;
    &lt;p&gt;The reason is quite simple: when things inevitably start going wrong, as they will, having an entirely open toolchain, allows me to have more visibility as to what is going on and the ability to fix it. I cannot delve into a black box.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building OpenOCD#&lt;/head&gt;
    &lt;p&gt;By default the version of OpenOCD that I got on my server via the official packet manager was outdated and missing features I will need.&lt;/p&gt;
    &lt;p&gt;Also, since saving the ability to modify OpenOCD’s source code could come in handy, I decided to re-build it from source.&lt;/p&gt;
    &lt;p&gt;Thus, in the following logs, I will be running OpenOCD version &lt;code&gt;0.12.0+dev-02170-gfcff4b712&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note : I have also re-build the JLink libs from source.&lt;/p&gt;
    &lt;head rend="h3"&gt;Determining the scan chain#&lt;/head&gt;
    &lt;p&gt;Since I do not have the schematics for the board I do not know how many devices are daisy-chainned on the board JTAG bus. Also, I want to confirm if the FPGA on the ebay listing is actually the one on the board. In JTAG, each chained device exposes an accessible &lt;code&gt;IDCODE&lt;/code&gt; register used to identify the manufacturer, device type, and revision number.&lt;/p&gt;
    &lt;p&gt;When setting up the JTAG server, we typically define the scan chain by specifying the expected &lt;code&gt;IDCODE&lt;/code&gt; for each TAP and the corresponding instruction register length, so that instructions can be correctly aligned and routed to the intended device.
Given this is an undocumented board off Ebay, I do not know what the chain looks like.
Fortunately, OpenOCD has an autoprobing functionality, to do a blind interrogation in an attempt to discover
the available devices.&lt;/p&gt;
    &lt;p&gt;Thus, my first order of business was doing this autoprobing.&lt;/p&gt;
    &lt;p&gt;In OpenOCD the autoprobing is done when the configuration does not specify any taps.&lt;/p&gt;
    &lt;code&gt;source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED

reset_config none
&lt;/code&gt;
    &lt;p&gt;The blind interrogation successfully discovered a single device on the chain with an &lt;code&gt;IDCODE&lt;/code&gt; of &lt;code&gt;0x04a63093&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test/autoprob$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
none separate
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Warn : There are no enabled taps.  AUTO PROBING MIGHT NOT WORK!!
Info : JTAG tap: auto0.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : AUTO auto0.tap - use "jtag newtap auto0 tap -irlen 2 -expected-id 0x04a63093"
Error: IR capture error at bit 2, saw 0x3ffffffffffffff5 not 0x...3
Warn : Bypassing JTAG setup events due to errors
Warn : gdb services need one or more targets defined
&lt;/code&gt;
    &lt;p&gt;Comparing against the &lt;code&gt;UltraScale Architecture Configuration User Guide (UG570)&lt;/code&gt; we see that this &lt;code&gt;IDCODE&lt;/code&gt; matches up
precisely with the expected value for the &lt;code&gt;KU3P&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;By default OpenOCD assumes a JTAG IR length of 2 bits, while our FPGA has an IR length of 6 bits. This is the cause behind the IR capture error encountered during autoprobing. By updating the script with an IR length of 6 bits we can re-detect the FPGA with no errors.&lt;/p&gt;
    &lt;code&gt;source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED

reset_config none

jtag newtap auto_detect tap -irlen 6
&lt;/code&gt;
    &lt;p&gt;Output :&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test/autoprob$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Info : JTAG tap: auto_detect.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
&lt;/code&gt;
    &lt;p&gt;Based on the probing, this is the JTAG scan chain for our board :&lt;/p&gt;
    &lt;head rend="h3"&gt;System Monitor Registers#&lt;/head&gt;
    &lt;p&gt;Previous generations of Xilinx FPGA had a system called the XADC that, among other features, allowed you to acquire chip temperature and voltage readings. The newer UltraScale and UltraScale+ family have deprecated this XADC module in favor of the SYSMON (and SYSMON4) which allows you to also get these temperature readings, just better.&lt;/p&gt;
    &lt;p&gt;Unfortunately, openOCD didn’t have support for reading the SYSMON over JTAG out of the box, so I will be adding it.&lt;/p&gt;
    &lt;p&gt;To be more precise, the Kintex UltraScale+ has a SYSMON4 and not a SYSMON. For full context, there are 3 flavors of SYSMON:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SYSMON1&lt;/code&gt;used in the Kintex and Virtex UltraScale series&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SYSMON4&lt;/code&gt;used in the Kintex, Virtex and in the Zynq programmable logic for the UltraScale+ series&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SYSMON&lt;/code&gt;used in the Zynq in the processing system of the UltraScale+ series.&lt;lb/&gt;Yes, you read that correctly the Zynq of the UltraScale+ series features not one, but at least two unique SYSMON instances.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the purpose of this article, all these instances are similar enough that I will be using the terms SYSMON4 and SYSMON interchangeably.&lt;/p&gt;
    &lt;p&gt;In order for the JTAG to interact with the SYSMON, we first need to write the &lt;code&gt;SYSMON_DRP&lt;/code&gt; command to the
JTAG Instruction Register (IR).
Based on the documentation, we see that this command has a value of &lt;code&gt;0x37&lt;/code&gt;, which funnily enough,
is the same command code as the XADC, solidifying the SYSMON as the XADC’s descendant.&lt;/p&gt;
    &lt;p&gt;The SYSMON offers a lot more additional functionalities than just being used to read voltage and temperature, but for today’s use case we will not be using any of that. Rather, we will focus only on reading a subset of the SYSMON status registers.&lt;/p&gt;
    &lt;p&gt;These status registers are located at addresses &lt;code&gt;(00h-3Fh, 80h-BFh)&lt;/code&gt;,
and contain the measurement results of the analog-to-digital conversions, the flag registers,
and the calibration coefficients. We can select which address we wish to read by writing the
address to the Data Register (DR) over JTAG and the data will be read out of &lt;code&gt;TDO&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# SPDX-License-Identifier: GPL-2.0-or-later

# Xilinx SYSMON4 support
#
# Based on UG580, used for UltraScale+ Xilinx FPGA
# This code implements access through the JTAG TAP.
#
# build a 32 bit DRP command for the SYSMON DRP
proc sysmon_cmd {cmd addr data} {
	array set cmds {
		NOP 0x00
		READ 0x01
		WRITE 0x02
	}
	return [expr {($cmds($cmd) &amp;lt;&amp;lt; 26) | ($addr &amp;lt;&amp;lt; 16) | ($data &amp;lt;&amp;lt; 0)}]
}

# Status register addresses
# Some addresses (status registers 0-3) have special function when written to.
proc SYSMON {key} {
	array set addrs {
		TEMP 0x00
		VCCINT 0x01
		VCCAUX 0x02
		VPVN 0x03
		VREFP 0x04
		VREFN 0x05
		VCCBRAM 0x06
		SUPAOFFS 0x08
		ADCAOFFS 0x09
		ADCAGAIN 0x0a
		VCCPINTLP 0x0d
		VCCPINTFP 0x0e
		VCCPAUX 0x0f
		VAUX0 0x10
		VAUX1 0x11
		VAUX2 0x12
		VAUX3 0x13
		VAUX4 0x14
		VAUX5 0x15
		VAUX6 0x16
		VAUX7 0x17
		VAUX8 0x18
		VAUX9 0x19
		VAUX10 0x1a
		VAUX11 0x1b
		VAUX12 0x1c
		VAUX13 0x1d
		VAUX14 0x1e
		VAUX15 0x1f
		MAXTEMP 0x20
		MAXVCC 0x21
		MAXVCCAUX 0x22
	}
	return $addrs($key)
}

# transfer
proc sysmon_xfer {tap cmd addr data} {
	set ret [drscan $tap 32 [sysmon_cmd $cmd $addr $data]]
	runtest 10
	return [expr "0x$ret"]
}

# sysmon register write
proc sysmon_write {tap addr data} {
	sysmon_xfer $tap WRITE $addr $data
}

# sysmon register read, non-pipelined
proc sysmon_read {tap addr} {
	sysmon_xfer $tap READ $addr 0
	return [sysmon_xfer $tap NOP 0 0]
}


# Select the sysmon DR, SYSMON_DRP has the same binary code value as the XADC
proc sysmon_select {tap} {
	set SYSMON_IR 0x37
	irscan $tap $SYSMON_IR
	runtest 10
}

# convert 16 bit temperature measurement to Celsius
proc sysmon_temp_internal {code} {
	return [expr {$code * 509.314/(1 &amp;lt;&amp;lt; 16) - 280.23}]
}

# convert 16 bit supply voltage measurments to Volt
proc sysmon_sup {code} {
	return [expr {$code * 3./(1 &amp;lt;&amp;lt; 16)}]
}

# measure all internal voltages
proc sysmon_report {tap} {
	puts "Sysmon status report :"
	sysmon_select $tap
	foreach ch [list TEMP MAXTEMP] {
		echo "$ch [format %.2f [sysmon_temp_internal [sysmon_read $tap [SYSMON $ch]]]] C"
	}
	foreach ch [list VCCINT MAXVCC VCCAUX MAXVCCAUX] {
		echo "$ch [format %.3f [sysmon_sup [sysmon_read $tap [SYSMON $ch]]]] V"	
	}
}
&lt;/code&gt;
    &lt;p&gt;I added a report that reads the current chip temperature, internal and external voltages as well as the maximum values for these recorded since FPGA power cycle, to my flashing script output:&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-20:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
set chipname XCKU3P
Read temperature sysmon 4
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.819 V
Info : clock speed 1 kHz
Info : JTAG tap: XCKU3P.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
--------------------
Sysmon status report :
TEMP 31.12 C
MAXTEMP 34.62 C
VCCINT 0.852 V
MAXVCC 0.855 V
VCCAUX 1.805 V
MAXVCCAUX 1.807 V
&lt;/code&gt;
    &lt;head rend="h2"&gt;Pinout#&lt;/head&gt;
    &lt;p&gt;To my indescribable joy I happened to stumble onto this gold mine, in which we get the board pinout. This most likely fell off a truck: https://blog.csdn.net/qq_37650251/article/details/145716953&lt;/p&gt;
    &lt;p&gt;So far this pinout looks correct.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Pin Index&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;IO Standard&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
        &lt;cell role="head"&gt;Bank&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;diff_100mhz_clk_p&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;E18&lt;/cell&gt;
        &lt;cell&gt;BANK67&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;diff_100mhz_clk_n&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;D18&lt;/cell&gt;
        &lt;cell&gt;BANK67&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;sfp_mgt_clk_p&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;K7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;sfp_mgt_clk_n&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;K6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;sfp_1_txn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;sfp_1_txp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;sfp_1_rxn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;A3&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;sfp_1_rxp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;A4&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;sfp_2_txn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;D6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;sfp_2_txp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;D7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;sfp_2_rxn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B1&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;sfp_2_rxp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B2&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;SFP_1_MOD_DEF_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;SFP_1_TX_FAULT&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;SFP_1_LOS&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;SFP_1_LED&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;SFP_2_MOD_DEF_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;E11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;SFP_2_TX_FAULT&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;F9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;SFP_2_LOS&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;E10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;SFP_2_LED&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_SFP_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_SFP_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_SFP_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_SFP_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_EEPROM_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;G10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_EEPROM_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;G9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_EEPROM_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;J15&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_EEPROM_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;J14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_R&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;29&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_G&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_H&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;31&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_3&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;34&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_4&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;pcie_mgt_clkn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T6&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell&gt;pcie_mgt_clkp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T7&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;pcie_tx0_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;R4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;pcie_tx1_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;U4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;pcie_tx2_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;W4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;pcie_tx3_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AA4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;41&lt;/cell&gt;
        &lt;cell&gt;pcie_tx4_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AC4&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;pcie_tx5_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD6&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;pcie_tx6_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE8&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;pcie_tx7_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF6&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;pcie_rx0_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;P1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;46&lt;/cell&gt;
        &lt;cell&gt;pcie_rx1_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell&gt;pcie_rx2_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;V1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;pcie_rx3_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;Y1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;49&lt;/cell&gt;
        &lt;cell&gt;pcie_rx4_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AB1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;pcie_rx5_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;51&lt;/cell&gt;
        &lt;cell&gt;pcie_rx6_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE3&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;52&lt;/cell&gt;
        &lt;cell&gt;pcie_rx7_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;53&lt;/cell&gt;
        &lt;cell&gt;pcie_tx0_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;R5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;54&lt;/cell&gt;
        &lt;cell&gt;pcie_tx1_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;U5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;55&lt;/cell&gt;
        &lt;cell&gt;pcie_tx2_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;W5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;56&lt;/cell&gt;
        &lt;cell&gt;pcie_tx3_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AA5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;57&lt;/cell&gt;
        &lt;cell&gt;pcie_tx4_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AC5&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;58&lt;/cell&gt;
        &lt;cell&gt;pcie_tx5_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD7&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;59&lt;/cell&gt;
        &lt;cell&gt;pcie_tx6_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE9&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;60&lt;/cell&gt;
        &lt;cell&gt;pcie_tx7_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF7&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;61&lt;/cell&gt;
        &lt;cell&gt;pcie_rx0_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;P2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;62&lt;/cell&gt;
        &lt;cell&gt;pcie_rx1_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;63&lt;/cell&gt;
        &lt;cell&gt;pcie_rx2_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;V2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;pcie_rx3_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;Y2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;65&lt;/cell&gt;
        &lt;cell&gt;pcie_rx4_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AB2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;66&lt;/cell&gt;
        &lt;cell&gt;pcie_rx5_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;67&lt;/cell&gt;
        &lt;cell&gt;pcie_rx6_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE4&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;68&lt;/cell&gt;
        &lt;cell&gt;pcie_rx7_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;69&lt;/cell&gt;
        &lt;cell&gt;pcie_perstn_rst&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Global clock#&lt;/head&gt;
    &lt;p&gt;On high end FPGAs like the UltraScale+ family, high-speed global clocks are typically driven from external sources using differential pairs for better signal integrity.&lt;/p&gt;
    &lt;p&gt;According to the pinout we have two such differential pairs.&lt;/p&gt;
    &lt;p&gt;First I must determine the nature of these external reference clocks to see how I can use them to drive my clocks.&lt;/p&gt;
    &lt;p&gt;These differential pairs are provided over the following pins:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;100MHz : {E18, D18}&lt;/item&gt;
      &lt;item&gt;156.25MHz : {K7, K6}&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Judging by the naming and the frequencies, the 156.25MHz clock is likely my SFP reference clock, and the 100MHz can be used as my global clock.&lt;/p&gt;
    &lt;p&gt;We can confirm by querying the pin properties.&lt;/p&gt;
    &lt;p&gt;K6 properties :&lt;/p&gt;
    &lt;code&gt;Vivado% report_property [get_package_pins K6]
Property                Type    Read-only  Value
BANK                    string  true       227
BUFIO_2_REGION          string  true       TR
CLASS                   string  true       package_pin
DIFF_PAIR_PIN           string  true       K7
IS_BONDED               bool    true       1
IS_DIFFERENTIAL         bool    true       1
IS_GENERAL_PURPOSE      bool    true       0
IS_GLOBAL_CLK           bool    true       0
IS_LOW_CAP              bool    true       0
IS_MASTER               bool    true       0
IS_VREF                 bool    true       0
IS_VRN                  bool    true       0
IS_VRP                  bool    true       0
MAX_DELAY               int     true       38764
MIN_DELAY               int     true       38378
NAME                    string  true       K6
PIN_FUNC                enum    true       MGTREFCLK0N_227
PIN_FUNC_COUNT          int     true       1
PKGPIN_BYTEGROUP_INDEX  int     true       0
PKGPIN_NIBBLE_INDEX     int     true       0
&lt;/code&gt;
    &lt;p&gt;E18 properties :&lt;/p&gt;
    &lt;code&gt;Vivado% report_property [get_package_pins E18]
Property                Type    Read-only  Value
BANK                    string  true       67
BUFIO_2_REGION          string  true       TL
CLASS                   string  true       package_pin
DIFF_PAIR_PIN           string  true       D18
IS_BONDED               bool    true       1
IS_DIFFERENTIAL         bool    true       1
IS_GENERAL_PURPOSE      bool    true       1
IS_GLOBAL_CLK           bool    true       1
IS_LOW_CAP              bool    true       0
IS_MASTER               bool    true       1
IS_VREF                 bool    true       0
IS_VRN                  bool    true       0
IS_VRP                  bool    true       0
MAX_DELAY               int     true       87126
MIN_DELAY               int     true       86259
NAME                    string  true       E18
PIN_FUNC                enum    true       IO_L11P_T1U_N8_GC_67
PIN_FUNC_COUNT          int     true       2
PKGPIN_BYTEGROUP_INDEX  int     true       8
PKGPIN_NIBBLE_INDEX     int     true       2
&lt;/code&gt;
    &lt;p&gt;This tells us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The differential pairings are correct: {K6, K7}, {E18, D18}&lt;/item&gt;
      &lt;item&gt;We can easily use the 100MHz as a source to drive our global clocking network&lt;/item&gt;
      &lt;item&gt;The 156.25MHz clock is to be used as the reference clock for our GTY transceivers and lands on bank 227 as indicated by the &lt;code&gt;PIN_FUNC&lt;/code&gt;property&lt;code&gt;MGTREFCLK0N_227&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;We cannot directly use the 156.25MHz clock to drive our global clock network&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all this we have sufficient information to write a constraint file (&lt;code&gt;xdc&lt;/code&gt;) for this board.&lt;/p&gt;
    &lt;head rend="h2"&gt;Test design#&lt;/head&gt;
    &lt;p&gt;Further sections will be using the following design files.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;top.v&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;module top (
    input wire Clk_100mhz_p_i, 
    input wire Clk_100mhz_n_i,

    output wire [3:0] Led_o 
);
    wire        clk_ibuf;
    reg  [28:0] ctr_q; 
    reg         unused_ctr_q;


    IBUFDS #(
        .DIFF_TERM("TRUE"),
        .IOSTANDARD("LVDS")
    ) m_ibufds (
        .I(Clk_100mhz_p_i),
        .IB(Clk_100mhz_n_i),
        .O(clk_ibuf)
    );

    BUFG m_bufg (
        .I(clk_ibuf),
        .O(clk)
    );

    always @(posedge clk)
        { unused_ctr_q, ctr_q } &amp;lt;= ctr_q + 29'b1;    
    
    assign Led_o = ctr_q[28:25];
endmodule
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;alibaba_cloud.xdc&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;# Global clock signal 
set_property -dict {LOC E18 IOSTANDARD LVDS} [get_ports Clk_100mhz_p_i]
set_property -dict {LOC D18 IOSTANDARD LVDS} [get_ports Clk_100mhz_n_i]
create_clock -period 10 -name clk_100mhz [get_ports Clk_100mhz_p_i]

# LEDS
set_property -dict {LOC B11 IOSTANDARD LVCMOS18} [get_ports { Led_o[0]}]
set_property -dict {LOC C11 IOSTANDARD LVCMOS18} [get_ports { Led_o[1]}]
set_property -dict {LOC A10 IOSTANDARD LVCMOS18} [get_ports { Led_o[2]}]
set_property -dict {LOC B10 IOSTANDARD LVCMOS18} [get_ports { Led_o[3]}]
&lt;/code&gt;
    &lt;head rend="h2"&gt;Writing the bitstream#&lt;/head&gt;
    &lt;p&gt;My personal belief is that one of the most important contributors to design quality is iteration cost. The lower your iteration cost, the higher your design quality is going to be.&lt;/p&gt;
    &lt;p&gt;As such I will invest the small upfront cost to have the workflow be as streamlined as efficiently feasible.&lt;/p&gt;
    &lt;p&gt;Thus, my workflow evolved into doing practically everything over the command line interfaces and only interacting with the tools, Vivado in this case, through tcl scripts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vivado flow#&lt;/head&gt;
    &lt;p&gt;The goal of this flow is to, given a few verilog design and constraint files produce a SVF file. Our steps are :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;creat the Vivado project &lt;code&gt;setup.tcl&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;run the implementation &lt;code&gt;build.tcl&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;generate the bitstream and the SVF &lt;code&gt;gen.tcl&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I will be using &lt;code&gt;make&lt;/code&gt; to kick off and manage the dependencies between the different steps, though I recognise this isn’t a widespread practice for hardware projects. &lt;code&gt;make&lt;/code&gt; is a highly flexible, reliable and powerful tool and I believe its ability to tie together any type of workflow makes it a prime tool for this use case.&lt;/p&gt;
    &lt;p&gt;We will be invoking Vivado in batch mode, this allows us to provide a tcl script alongside script arguments, the format is as following :&lt;/p&gt;
    &lt;code&gt;vivado -mode batch &amp;lt;path to tcl script&amp;gt; -tclargs &amp;lt;script args&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Though this allows us to easily break down our flow into incremental stages, invoking a single script in batch mode has the drawback of restarting Vivado and needing to re-load the project or the project checkpoint on each invocation.&lt;/p&gt;
    &lt;p&gt;As the project size grows so will the project load time, so segmenting the flow into a large number of independent scripts comes at an increasing cost.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Makefile&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;SHELL := /bin/bash

VIVADO_PRJ_DIR=prj
VIVADO_PRJ_NAME=$(VIVADO_PRJ_DIR)
VIVADO_PRJ_PATH=$(VIVADO_PRJ_DIR)/$(VIVADO_PRJ_NAME).xpr
VIVADO_CHECKPOINT_PATH=$(VIVADO_PRJ_DIR)/$(VIVADO_PRJ_NAME)_checkpoint.dcp

VIVADO_CMD=vivado -mode batch -source

SRC_PATH=src
OUT_DIR=out


all: setup build gen

$(VIVADO_PRJ_PATH):  
    mkdir -p $(VIVADO_PRJ_DIR)
    $(VIVADO_CMD) setup.tcl -tclargs $(VIVADO_PRJ_DIR) $(VIVADO_PRJ_NAME)

setup: $(VIVADO_PRJ_PATH) 

$(VIVADO_CHECKPOINT_PATH): $(VIVADO_PRJ_PATH) $(wildcard $(SRC_PATH)/*.xdc) $(wildcard $(SRC_PATH)/*.v)
    $(VIVADO_CMD) build.tcl -tclargs $(VIVADO_PRJ_PATH) $(SRC_PATH) $(VIVADO_CHECKPOINT_PATH)

build: $(VIVADO_CHECKPOINT_PATH)

$(OUT_DIR)/$(VIVADO_PRJ_NAME).svf: $(VIVADO_CHECKPOINT_PATH) 
    mkdir -p $(OUT_DIR)
    $(VIVADO_CMD) gen.tcl -tclargs $(VIVADO_CHECKPOINT_PATH) $(OUT_DIR)

gen: $(OUT_DIR)/$(VIVADO_PRJ_NAME).svf

flash: $(OUT_DIR)/$(VIVADO_PRJ_NAME).svf
    openocd	

clean: 
    rm -rf $(VIVADO_PRJ_DIR)
    rm -rf $(OUT_DIR)
    rm -f vivado*{log,jou}
    rm -f webtalk*{log,jou}
    rm -f usage_statistics_webtalk*{html,xml}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;setup.tcl&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set project_dir [lindex $argv 0]
set project_name [lindex $argv 1]

puts "Creating project $project_name at path [pwd]/$project_dir"
create_project -part xcku3p-ffvb676-2-e -force $project_name $project_dir

close_project
exit 0
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;build.tcl&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set project_path [lindex $argv 0]
set src_path [lindex $argv 1]
set checkpoint_path [lindex $argv 2]
puts "Implementation script called with project path $project_path and src path $src_path, generating checkpoint at $checkpoint_path"

open_project $project_path 

# load src
read_verilog [glob -directory $src_path *.v]
read_xdc [glob -directory $src_path *.xdc]


# synth
synth_design -top top

# implement
opt_design
place_design
route_design
phys_opt_design

write_checkpoint $checkpoint_path -force 
close_project
exit 0
&lt;/code&gt;
    &lt;head rend="h4"&gt;Generating the SVF file#&lt;/head&gt;
    &lt;p&gt;The SVF for Serial Vector Format is a human readable, vendor agnostic specification used to specify JTAG bus operations.&lt;/p&gt;
    &lt;p&gt;Example SVF file, test program:&lt;/p&gt;
    &lt;code&gt;! Initialize UUT
STATE RESET;
! End IR scans in DRPAUSE
ENDIR DRPAUSE;
! End DR scans in DRPAUSE
ENDDR DRPAUSE;
! 24 bit IR header
HIR 24 TDI (FFFFFF);
! 3 bit DR header
HDR 3 TDI (7);
! 16 bit IR trailer
TIR 16 TDI (FFFF);
! 2 bit DR trailer
TDR 2 TDI (3);
! 8 bit IR scan, load BIST opcode
SIR 8 TDI (41) TDO (81) MASK (FF);
! 16 bit DR scan, load BIST seed
SDR 16 TDI (ABCD);
! RUNBIST for 95 TCK Clocks
RUNTEST 95 TCK ENDSTATE IRPAUSE;
! 16 bit DR scan, check BIST status
SDR 16 TDI (0000) TDO(1234) MASK(FFFF);
! Enter Test-Logic-Reset
STATE RESET;
! End Test Program
&lt;/code&gt;
    &lt;p&gt;Vivado can generate a hardware aware SVF file containing the configuration sequence for an FPGA board, allowing us to write a bitstream.&lt;/p&gt;
    &lt;p&gt;Given the SVF file literally contains the bitstream written in clear hexademical, in the file, our first step is to generate our design’s bitstream.&lt;/p&gt;
    &lt;p&gt;Vivado proper isn’t the software that generates the SVF file, this task is done by the hardware manager which handles all of the configuration.&lt;/p&gt;
    &lt;p&gt;We can launch a new instance &lt;code&gt;open_hw_manager&lt;/code&gt; and connect to it &lt;code&gt;connect_hw_server&lt;/code&gt;.
Since JTAG is a daisy chained bus, and given the SVF file is just a standardised way of specifying
JTAG bus operations, in order to generate a correct JTAG configuration sequence, we must inform the hardware manger
of our scan chain.&lt;/p&gt;
    &lt;p&gt;During our earlier probing of the scan chain, we have established that our FPGA is the only device on the chain. We inform the hardware manager of this by creating a new device configuration ( the term “device” refers to the “board” here ) and add our fpga to the chain using the &lt;code&gt;create_hw_device -part &amp;lt;device name&amp;gt;&lt;/code&gt;.When we have multiple
devices we should register them following the order in which they appear on the chain.&lt;/p&gt;
    &lt;p&gt;Finally to generate the SVF file, we must select the device we wish to program with &lt;code&gt;program_hw_device &amp;lt;hw_device&amp;gt;&lt;/code&gt;,
then write out the SVF to the file using &lt;code&gt;write_hw_svf &amp;lt;path to svf file&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;gen.tcl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;set checkpoint_path [lindex $argv 0]
set out_dir [lindex $argv 1]
puts "SVF generation script called with checkpoint path $checkpoint_path, generating to $out_dir"

open_checkpoint $checkpoint_path

# defines
set hw_target "alibaba_board_svf_target"
set fpga_device "xcku3p"
set bin_path "$out_dir/[current_project]"

write_bitstream "$bin_path.bit" -force

open_hw_manager

# connect to hw server with default config
connect_hw_server
puts "connected to hw server at [current_hw_server]"

create_hw_target $hw_target
puts "current hw target [current_hw_target]"

open_hw_target

# single device on scan chain
create_hw_device -part $fpga_device
puts "scan chain : [get_hw_devices]"

set_property PROGRAM.FILE "$bin_path.bit" [get_hw_device]

#select device to program
program_hw_device [get_hw_device]

# generate svf file
write_hw_svf -force "$bin_path.svf"

close_hw_manager
exit 0
&lt;/code&gt;
    &lt;head rend="h3"&gt;Configuring the FPGA using OpenOCD#&lt;/head&gt;
    &lt;p&gt;Although not widespread openOCD has a very nice &lt;code&gt;svf&lt;/code&gt; execution command :&lt;/p&gt;
    &lt;quote&gt;&lt;head&gt;18.1 SVF: Serial Vector Format#&lt;/head&gt;&lt;p&gt;The Serial Vector Format, better known as SVF, is a way to represent JTAG test patterns in text files. In a debug session using JTAG for its transport protocol, OpenOCD supports running such test files.&lt;/p&gt;&lt;code&gt;[Command]svf filename [-tap tapname] [[-]quiet] [[-]nil] [[-]progress] [[-]ignore_error]&lt;/code&gt;&lt;p&gt;This issues a JTAG reset (Test-Logic-Reset) and then runs the SVF script from filename. Arguments can be specified in any order; the optional dash doesn’t affect their se- mantics.&lt;/p&gt;&lt;p&gt;Command options:&lt;/p&gt;&lt;code&gt;-tap&lt;/code&gt;tapname ignore IR and DR headers and footers specified by the SVF file with HIR, TIR, HDR and TDR commands; instead, calculate them automatically according to the current JTAG chain configuration, targeting tapname;&lt;code&gt;[-]quiet&lt;/code&gt;do not log every command before execution;&lt;code&gt;[-]nil&lt;/code&gt;“dry run”, i.e., do not perform any operations on the real interface;&lt;code&gt;[-]progress&lt;/code&gt;enable progress indication;&lt;code&gt;[-]ignore&lt;/code&gt;_error continue execution despite TDO check errors.&lt;/quote&gt;
    &lt;p&gt;We invoke it in our openOCD script using the &lt;code&gt;-progress&lt;/code&gt; option for additional logging.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;openocd&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set svf_path "out/project_prj_checkpoint.svf"

source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED 
reset_config none

# jlink config

set CHIPNAME XCKU3P
set CHIP $CHIPNAME
puts "set chipname "$CHIP

source [find ../openocd/tcl/cpld/xilinx-xcu.cfg]

source [find ../openocd/tcl/fpga/xilinx-sysmon.cfg]

init 

puts "--------------------"

sysmon_report $CHIP.tap

puts "--------------------"

# program
if {![file exists $svf_path]} {
    puts "Svf path not found : $svf_path"
    exit
}

svf $svf_path -progress 
 
exit 
&lt;/code&gt;
    &lt;p&gt;Flashing sequence log :&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
set chipname XCKU3P
Read temperature sysmon 4
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Info : JTAG tap: XCKU3P.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
--------------------
Sysmon status report :
TEMP 50.46 C
MAXTEMP 52.79 C
VCCINT 0.846 V
MAXVCC 0.860 V
VCCAUX 1.799 V
MAXVCCAUX 1.809 V
--------------------
svf processing file: "out/project_prj_checkpoint.svf"
  0%  TRST OFF;
  0%  ENDIR IDLE;
  0%  ENDDR IDLE;
  0%  STATE RESET;
  0%  STATE IDLE;
  0%  FREQUENCY 1.00E+07 HZ;
adapter speed: 10000 kHz
  0%  HIR 0 ;
  0%  TIR 0 ;
  0%  HDR 0 ;
  0%  TDR 0 ;
  0%  SIR 6 TDI (09) ;
  0%  SDR 32 TDI (00000000) TDO (04a63093) MASK (0fffffff) ;
  0%  STATE RESET;
  0%  STATE IDLE;
  0%  SIR 6 TDI (0b) ;
  0%  SIR 6 TDI (14) ;
  0%  RUNTEST 0.100000 SEC;
  0%  RUNTEST 10000 TCK;
  0%  SIR 6 TDI (14) TDO (11) MASK (31) ;
  0%  SIR 6 TDI (05) ;
 95%  ffffffffffff) ;
 95%  SIR 6 TDI (09) TDO (31) MASK (11) ;
 95%  STATE RESET;
 95%  RUNTEST 5 TCK;
 95%  SIR 6 TDI (05) ;
 95%  SDR 160 TDI (0000000400000004800700140000000466aa9955) ;
 95%  SIR 6 TDI (04) ;
 95%  SDR 32 TDI (00000000) TDO (3f5e0d40) MASK (08000000) ;
 95%  STATE RESET;
 95%  RUNTEST 5 TCK;
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
&lt;/code&gt;
    &lt;p&gt;Resulting in a successfully configured our FPGA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;For $200 we got a fully working decommissioned Alibaba Cloud accelerator featuring a Kintex UltraScale+ FPGA with an easily accessible debugging/programming interface and enough pinout information to define our own constraint files.&lt;/p&gt;
    &lt;p&gt;We also have a fully automated Vivado workflow to implement our designs and the ability to write the bitstream, and interface with the FPGA’s internal JTAG accessible registers using an open source programming tool without the need for an official Xilinx programmer.&lt;/p&gt;
    &lt;p&gt;In the end, this project delivered an at least 5x cost savings over commercial boards (compared to the lowest cost $900-1050 Alinx alternatives), making this perhaps the most cost effective entry point for a Kintex UltraScale+ board.&lt;/p&gt;
    &lt;head rend="h2"&gt;External ressources#&lt;/head&gt;
    &lt;p&gt;Xilinx Vivado Supported Devices : https://docs.amd.com/r/en-US/ug973-vivado-release-notes-install-license/Supported-Devices&lt;/p&gt;
    &lt;p&gt;Official Xilinx dev board : https://www.amd.com/en/products/adaptive-socs-and-fpgas/evaluation-boards/ek-u1-kcu116-g.html&lt;/p&gt;
    &lt;p&gt;Alinx Kintex UltraScale+ dev boards : https://www.en.alinx.com/Product/FPGA-Development-Boards/Kintex-UltraScale-plus.html&lt;/p&gt;
    &lt;p&gt;UltraScale Architecture Configuration User Guide (UG570) : https://docs.amd.com/r/en-US/ug570-ultrascale-configuration/Device-Resources-and-Configuration-Bitstream-Lengths?section=gyn1703168518425__table_vyh_4hs_szb&lt;/p&gt;
    &lt;p&gt;UltraScale Architecture System Monitor User Guide (UG580): https://docs.amd.com/v/u/en-US/ug580-ultrascale-sysmon&lt;/p&gt;
    &lt;p&gt;Vivado Design Suite Tcl Command Reference Guide (UG835): https://docs.amd.com/r/en-US/ug835-vivado-tcl-commands/Tcl-Initialization-Scripts&lt;/p&gt;
    &lt;p&gt;PCI vendor/device ID database: https://admin.pci-ids.ucw.cz/read/PC/14e4&lt;/p&gt;
    &lt;p&gt;PCI device classes: https://admin.pci-ids.ucw.cz/read/PD&lt;/p&gt;
    &lt;p&gt;Linux kernel PCI IDs: https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256&lt;/p&gt;
    &lt;p&gt;Linux kernel PCI classes: https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158&lt;/p&gt;
    &lt;p&gt;Truck-kun pinout: https://blog.csdn.net/qq_37650251/article/details/145716953&lt;/p&gt;
    &lt;p&gt;Ebay listing: https://www.ebay.com/itm/167626831054?_trksid=p4375194.c101800.m5481&lt;/p&gt;
    &lt;p&gt;OpenOCD documentation: https://openocd.org/doc-release/pdf/openocd.pdf&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://essenceia.github.io/projects/alibaba_cloud_fpga/"/><published>2025-10-04T06:49:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45472319</id><title>Paged Out Issue #7 [pdf]</title><updated>2025-10-04T20:33:45.859952+00:00</updated><content/><link href="https://pagedout.institute/download/PagedOut_007.pdf"/><published>2025-10-04T10:38:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45472678</id><title>The Buchstabenmuseum Berlin is closing</title><updated>2025-10-04T20:33:43.743763+00:00</updated><content>&lt;doc fingerprint="7e81930193d6e0f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;After 20 Years:&lt;/head&gt;
    &lt;head rend="h3"&gt;The Buchstabenmuseum Berlin is closing!&lt;/head&gt;
    &lt;p&gt;Until 5 October 2025, you can visit our museum every Thursday to Sunday from 1 to 5 pm.&lt;lb/&gt; A visit outside opening hours is possible with a guided tour.&lt;lb/&gt; visit@buchstabenmuseum.de&lt;/p&gt;
    &lt;p&gt;We are still looking for long-term storage for our collection.&lt;lb/&gt; For this we need support:&lt;lb/&gt; &amp;gt;&amp;gt; betterplace&lt;/p&gt;
    &lt;head rend="h4"&gt;We look forward to your visit!&lt;/head&gt;
    &lt;p&gt;______________________________&lt;/p&gt;
    &lt;head rend="h1"&gt;20 Years of the Buchstabenmuseum!&lt;/head&gt;
    &lt;head rend="h4"&gt;On Sat 14 June 2025 from 3 to 9 pm we want to celebrate our birthday and toast with you!&lt;lb/&gt; You and all your friends and family are cordially invited.&lt;/head&gt;
    &lt;p&gt;20 years is a very long time to look back on.&lt;lb/&gt; That’s why we’ve put together a colourful brochure that we’ll be presenting on Saturday.&lt;/p&gt;
    &lt;p&gt;In addition, Sabrina Hauck (student at TU Berlin / architect at gkks) will give a talk on the use,&lt;lb/&gt; vacancy and potential of Berlin’s S-Bahn arches at 18:00.&lt;/p&gt;
    &lt;head rend="h1"&gt;FINAL SALE – FROM DEPARTMENT STORES’ TO MUSEUM &lt;/head&gt;
    &lt;p&gt;Extended Term until Autumn 2025!&lt;/p&gt;
    &lt;p&gt;AN EXHIBITION OF FORMER DEPARTMENT STORES FROM 1980 TO TODAY&lt;/p&gt;
    &lt;p&gt;Horten, Quelle, Hertie, Kaufhof and Karstadt – corporate names that are disappearing from German city centres. Galeria Karstadt Kaufhof is currently struggling with closure in instalments. With the creeping loss of the corporations, the distinctive lettering of the department stores’ chains is also being lost.&lt;lb/&gt; “FINAL SALE” tells the typographic and urban-historical stories of the letters and shows the former significance of the department stores and department stores with their architecture.&lt;lb/&gt; We invite you to discover the typographic department stores’ icons and to buy selected items in the exhibition: “Final Sale – from department stores’ to museum” in the Buchstabenmuseum in the Stadtbahnbögen in the Hansaviertel.&lt;/p&gt;
    &lt;head rend="h4"&gt;COOPERATION WITH THE STAATSBIBLIOTHEK BERLIN&lt;/head&gt;
    &lt;p&gt;In the »Staatsbibliothek Berlin«, Unter den Linden, selected Ks from the collection of the Buchstabenmuseum point the way to the in-house museum "Kulturwerk".&lt;/p&gt;
    &lt;head rend="h4"&gt;INDIVIDUAL GUIDED TOURS&lt;/head&gt;
    &lt;p&gt;Discover our unique collection and learn the exciting background stories to our letters.&lt;/p&gt;
    &lt;head rend="h4"&gt;NEON CLASSES: BENDING BASICS&lt;/head&gt;
    &lt;p&gt;The art of neon and the bending of neon tubes is a fascinating craft! Learn the basics of neon and glass bending.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE BUCHSTABENMUSEUM&lt;/head&gt;
    &lt;p&gt;Preservation and documentation of letters&lt;lb/&gt; The Buchstabenmuseum is the first museum in the world to collect letterforms from public spaces and display them as part of urban history. We preserve and document three-dimensional letters and signage, and their history, as well as providing information about their origins and construction. Our collection has captured the imagination of visitors from all around the world for over 10 years. Hundreds of letters have been saved from being battered by the elements or ending up on the scrap heap. A selection of what we offer can be found under » COLLECTION&lt;/p&gt;
    &lt;head rend="h2"&gt;COLLECTION&lt;/head&gt;
    &lt;head rend="h2"&gt;SPECIALS&lt;/head&gt;
    &lt;p&gt;— BECOME A MEMBER —&lt;/p&gt;
    &lt;p&gt;Become a member of the club and actively support our museum.&lt;lb/&gt; We are always in need of help with the rescue of historic lettering, looking after our international guests or even with the classic work of the association.&lt;lb/&gt; Just write to us!&lt;lb/&gt; bindabei@buchstabenmuseum.de&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.buchstabenmuseum.de/en/"/><published>2025-10-04T11:58:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45472765</id><title>Thunderscan: A clever device transforms a printer into a scanner (2004)</title><updated>2025-10-04T20:33:43.419775+00:00</updated><content>&lt;doc fingerprint="8549597119742b9e"&gt;
  &lt;main&gt;
    &lt;p&gt;The first project that I worked on for Apple after starting in August 1979 was writing low level software for the Silentype printer (see What Hath Woz Wrought), a cute, inexpensive thermal printer for the Apple II, that was based on technology licensed from a local company named Trendcom. In typical Apple fashion, we improved on Trendcom's design by replacing their relatively expensive controller board with a much simpler one that relied on the microprocessor in the Apple II to do most of the dirty work.&lt;/p&gt;
    &lt;p&gt;The only other engineer working on the project was Victor Bull, who was the hardware designer and also the project leader. Vic was smart, taciturn and easy to work with, and I learned a lot from him about how thermal printers worked, as well as how things worked at Apple. We finished the project quickly, and the Silentype shipped in November 1979, less than four months after I began working on it.&lt;/p&gt;
    &lt;p&gt;In May 1984, during my leave of absence from Apple (see Leave Of Absence), I received a phone call from Victor Bull, who I hadn't heard from in a couple of years. He had left Apple more than a year ago to work with his friend Tom Petrie at a tiny company based in Orinda named Thunderware, that sold a single product called Thunderclock, an inexpensive calendar/clock card for the Apple II. Victor said that he thought that I might be interested in writing software for an exciting, clever new product that Thunderware was developing for the Macintosh, which he refused to describe over the phone. He invited me to come visit them to check it out.&lt;/p&gt;
    &lt;p&gt;In early June, I drove up to Thunderware's office in Orinda, which was about an hour's drive from my house in Palo Alto. After I arrived at their modest headquarters, Vic introduced me to his partner, Tom Petrie, and I signed a non-disclosure agreement before they ushered me into a back room to see their demo.&lt;/p&gt;
    &lt;p&gt;The most popular printer for both the Apple II and the Macintosh was the ImageWriter, a $500 dot-matrix printer capable of rendering bitmapped graphics, that was designed and manufactured by Japanese company named C.Itoh Electronics and marketed by Apple. Virtually every Macintosh owner purchased an ImageWriter, since it was the only printer that was supported by Apple. Tom's demo consisted of an ImageWriter printer hooked up to an Apple II, that at first glance appeared to be busily printing away. But when I looked closer, I noticed that instead of blank paper, there was a glossy photograph of a cat threaded through the printer's platen, and the printer's black plastic ribbon cartridge was missing, replaced by a makeshift contraption containing an optical sensing device that trailed an umbilical cord back to the Apple II.&lt;/p&gt;
    &lt;p&gt;Their potential new product, Thunderscan, was a low cost way to temporarily turn an ImageWriter printer into a high resolution scanner, by replacing the ribbon cartridge with an optical sensor and writing some clever software. Since the resolution was determined by the precision of the printer's stepper motors, which had to be very accurate in order to print detailed graphics, Thunderscan, priced at under $200, had better resolution than flat bed scanners costing more than ten times as much. I loved the cleverness of the ingenious concept, and the Woz-like elegance of saving money and adding flexibility by doing everything in software, but there were also a few problems.&lt;/p&gt;
    &lt;p&gt;The biggest problem was that Thunderscan could only capture one scan line's worth of data on each pass of the print head, which made it nine times slower than regular printing, since the print head could deposit nine dots at a time. This made for frustratingly slow scanning, often taking over an hour to scan a full page at the highest resolution. Thunderscan was never going to win any races.&lt;/p&gt;
    &lt;p&gt;Another apparent problem was the disappointingly low quality of the image being captured and displayed by Tom Petrie's Apple II application. Tom and Vic said their scanner was capable of capturing up to 32 different levels of light intensity, but both the Apple II (in hi-res mode) and the Macintosh only had one bit per pixel to display, so the software had to simulate gray scales using patterns of black and white dots. It looked like Tom was using a simple threshold algorithm to do the rendering, which threw away most of the gray scale information and made the resulting image look unacceptably blotchy. It was hard to tell if the quality promised by Tom and Vic was there or not.&lt;/p&gt;
    &lt;p&gt;Tom and Vic proposed to hire me to write Macintosh software for Thunderscan. I knew that a low cost scanner would be a great product for the image hungry Macintosh, but only if it had sufficient quality, and I wasn't sure about that. I told them that I'd think it over during the next few days, and, as I did, I became more excited about the potential of Thunderscan for the Macintosh, realizing that the slow speed wouldn't be that much of an impediment if the quality and resolution was good enough. The low image quality in Tom's prototype was probably caused more by the Apple II software than by anything inherent in the scanner. The Macintosh was almost ten times faster than the Apple II, so it should be able to sample the incoming data better to obtain more horizontal resolution. Plus, I knew a much better algorithm for gray scale rendering that would be fun to try out in practice.&lt;/p&gt;
    &lt;p&gt;My friend and colleague Bill Atkinson was a talented photographer, and one of his hobbies was playing around with digitized pictures, periodically experimenting to find the best algorithms for rendering them. Bill loved to explain his current work to whoever would listen to him, so I learned a lot about rendering gray scale images over the years simply by being around him. Bill had progressed over the years from using an "ordered dither" algorithm, where varying threshold values are specified in a sliding matrix, to his current favorite, which was a modified version of what was known as the "Floyd-Steinberg" algorithm, where an error term is maintained and distributed proportionally to neighboring pixels.&lt;/p&gt;
    &lt;p&gt;I called Thunderware and told them I was interested in working on Macintosh software for Thunderscan, in exchange for a per-unit royalty. I drove back up to Orinda, where Tom and Vic gave me lots of documentation about the scanner, and the sample code that Tom had written for the Apple II. For the next couple of months, I drove up to Orinda once a week, usually on Thursday, to meet with Tom and Vic show them my progress, prioritize development issues and discussion various complications as they arose. We would also discuss business terms, but we didn't sign a formal contract until the software was almost finished, when we settled on a royalty of $7.50 per unit.&lt;/p&gt;
    &lt;p&gt;Tom and Vic had already encountered and surmounted a number of tough problems just to get scanning going at all. For example, the ImageWriter printer was not really designed to be stepped one scanline at a time, and if you tried that the paper would bunch up against the platen, causing distortion. Tom and Vic solved the problem by commanding the printer to move three steps up and then two steps back, instead of a single step up, which held the paper snugly against the platen as required. There were also various techniques for sensing the beginning and end of the scan line, and some timings that were determined by tedious experimentation for how long it took the printer to respond to a command.&lt;/p&gt;
    &lt;p&gt;It took a week or so to get basic scanning working on the Macintosh, and then a few more days to render the gray scale data with Bill's modified Floyd-Steinberg dithering. After shaking out a variety of problems, mostly involving synchronization between the printer and the software, I was surprised and impressed by the consistent high quality of the results. I went through a brief, elated phase of scanning every image in sight that would fit through the printer, just to see how it would turn out.&lt;/p&gt;
    &lt;p&gt;One important design decision that I made early on was to keep the gray scale data around, to allow more flexible image processing. Thunderscan documents were five bits per pixel, before the Macintosh generally supported gray scale, and the user could manipulate the contrast and brightness of selected areas of the image, dodging and burning to reveal detail in the captured image. This also paid off in later versions when we implemented gray scale printing for Postscript printers.&lt;/p&gt;
    &lt;p&gt;My favorite feature that I came up with for Thunderscan had to do with two dimensional scrolling. Thunderscan documents could be quite large, so you could only show a portion of them in the image area of the window. You could scroll the image by dragging with a MacPaint-style "hand" scrolling tool, but you had to drag an awful lot to get to the extremes of a large image. I decided to add what I called "inertial" scrolling, where you gave the image a push and it kept scrolling at a variable speed in the direction of the push, after the mouse button was released. I had to add some hysteresis to keep the image from moving accidentally, and make various other tweaks, but soon I had it working and it felt great to be able to zip around large images by pushing them.&lt;/p&gt;
    &lt;p&gt;The hardest feature to perfect was bidirectional scanning. At first, Thunderscan only scanned from left to right, but it wasted time to return the scannner to the left after every scan line. We could almost double the speed if we scanned in both directions, but it was hard to get the adjacent scan lines that were scanned in opposite directions to line up properly. Ultimately, we made bidirectional scanning an optional feature, if you wanted to trade a little quality for greater speed.&lt;/p&gt;
    &lt;p&gt;I finished the software in November 1984, after taking a short break to work on something else (see Switcher). Thunderscan shipped in December 1984, and did well from the very beginning, with sales gradually rising from around 1,000 units/month to over 7,500 units/month at its peak in 1987. For a while, it was both the least expensive and highest quality scanning alternative for the Macintosh, although I'm sure it frustrated a lot of users by being too slow. I did three major revisions of the software over the next few years, improving the scan quality and adding features like gray scale printing and eventually gray scale display for the Macintosh II.&lt;/p&gt;
    &lt;p&gt;Eventually, the flat bed scanners caught up to Thunderscan, and then surpassed it, in both cost, quality and convenience. Over its lifetime, Thunderscan sold approximately 100,000 units and improved countless documents by providing users with an inexpensive way to capture high resolution graphics with their Macintoshes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.folklore.org/Thunderscan.html"/><published>2025-10-04T12:16:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45473019</id><title>How functional programming shaped and twisted front end development</title><updated>2025-10-04T20:33:42.628198+00:00</updated><content>&lt;doc fingerprint="bf66d30b0925bfc8"&gt;
  &lt;main&gt;
    &lt;p&gt;A friend called me last week. Someone whoâd built web applications back for a long time before moving exclusively to backend and infra work. Heâd just opened a modern React codebase for the first time in over a decade.&lt;/p&gt;
    &lt;p&gt;âWhat the hell is this?â he asked. âWhat are all these generated class names? Did we justâ¦ cancel the cascade? Who made the web work this way?â&lt;/p&gt;
    &lt;p&gt;I laughed, but his confusion cut deeper than he realized. He remembered a web where CSS cascaded naturally, where the DOM was something you worked with, where the browser handled routing, forms, and events without twenty abstractions in between. To him, our modern frontend stack looked like weâd declared war on the platform itself.&lt;/p&gt;
    &lt;p&gt;He asked me to explain how we got here. That conversation became this essay.&lt;/p&gt;
    &lt;p&gt;A disclaimer before we begin: This is one perspective, shaped by having lived through the first browser war. I applied &lt;code&gt;pngfix.js&lt;/code&gt; to make 24-bit PNGs work in IE6. I debugged hasLayout bugs at 2 AM. I wrote JavaScript when you couldnât trust &lt;code&gt;addEventListener&lt;/code&gt; to work the same way across browsers. I watched jQuery become necessary, then indispensable, then legacy. I might be wrong about some of this. My perspective is biased for sure, but it also comes with the memory that the web didnât need constant reinvention to be useful.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Thereâs a strange irony at the heart of modern web development. The web was born from documents, hyperlinks, and a cascading stylesheet language. It was always messy, mutable, and gloriously side-effectful. Yet over the past decade, our most influential frontend tools have been shaped by engineers chasing functional programming purity: immutability, determinism, and the elimination of side effects.&lt;/p&gt;
    &lt;p&gt;This pursuit gave us powerful abstractions. React taught us to think in components. Redux made state changes traceable. TypeScript brought compile-time safety to a dynamic language. But it also led us down a strange path. A one where we fought against the platform instead of embracing it. We rebuilt the browserâs native capabilities in JavaScript, added layers of indirection to âprotectâ ourselves from the DOM, and convinced ourselves that the webâs inherent messiness was a problem to solve rather than a feature to understand.&lt;/p&gt;
    &lt;p&gt;The question isnât whether functional programming principles have value. They do. The question is whether applying them dogmatically to the web (a platform designed around mutability, global scope, and user-driven chaos) made our work better, or just more complex.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Nature of the Web&lt;/head&gt;
    &lt;p&gt;To understand why functional programming ideals clash with web development, we need to acknowledge what the web actually is.&lt;/p&gt;
    &lt;p&gt;The web is fundamentally side-effectful. CSS cascades globally by design. Styles defined in one place affect elements everywhere, creating emergent patterns through specificity and inheritance. The DOM is a giant mutable tree that browsers optimize obsessively; changing it directly is fast and predictable. User interactions arrive asynchronously and unpredictably: clicks, scrolls, form submissions, network requests, resize events. Thereâs no pure function that captures âuser intent.â&lt;/p&gt;
    &lt;p&gt;This messiness is not accidental. Itâs how the web scales across billions of devices, remains backwards-compatible across decades, and allows disparate systems to interoperate. The browser is an open platform with escape hatches everywhere. You can style anything, hook into any event, manipulate any node. That flexibility and that refusal to enforce rigid abstractions is the webâs superpower.&lt;/p&gt;
    &lt;p&gt;When we approach the web with functional programming instincts, we see this flexibility as chaos. We see globals as dangerous. We see mutation as unpredictable. We see side effects as bugs waiting to happen. And so we build walls.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enter Functional Programming Ideals&lt;/head&gt;
    &lt;p&gt;Functional programming revolves around a few core principles: functions should be pure (same inputs â same outputs, no side effects), data should be immutable, and state changes should be explicit and traceable. These ideas produce code thatâs easier to reason about, test, and parallelize, in the right context of course.&lt;/p&gt;
    &lt;p&gt;These principles had been creeping into JavaScript long before React. Underscore.js (2009) brought map, reduce, and filter to the masses. Lodash and Ramda followed with deeper FP toolkits including currying, composition and immutability helpers. The ideas were in the air: avoid mutation, compose small functions, treat data transformations as pipelines.&lt;/p&gt;
    &lt;p&gt;React itself started with class components and &lt;code&gt;setState&lt;/code&gt;, hardly pure FP. But the conceptual foundation was there: treat UI as a function of state, make rendering deterministic, isolate side effects. Then came Elm, a purely functional language created by Evan Czaplicki that codified the âModel-View-Updateâ architecture. When Dan Abramov created Redux, he explicitly cited Elm as inspiration. Reduxâs reducers are directly modeled on Elmâs update functions: &lt;code&gt;(state, action) =&amp;gt; newState&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Redux formalized what had been emerging patterns. Combined with React Hooks (which replaced stateful classes with functional composition), the ecosystem shifted decisively toward FP. Immutability became non-negotiable. Pure components became the ideal. Side effects were corralled into &lt;code&gt;useEffect&lt;/code&gt;. Through this convergence (library patterns, Elmâs rigor, and Reactâs evolution) Haskell-derived ideas about purity became mainstream JavaScript practice.&lt;/p&gt;
    &lt;p&gt;In the early 2010s, as JavaScript applications grew more complex, developers looked to FP for salvation. jQuery spaghetti had become unmaintainable. Backboneâs two-way binding caused cascading updates (ironically, Backboneâs documentation explicitly advised against two-way binding saying âit doesnât tend to be terribly useful in your real-world appâ yet many developers implemented it through plugins). The community wanted discipline, and FP offered it: treat your UI as a pure function of state. Make data flow in one direction. Eliminate shared mutable state.&lt;/p&gt;
    &lt;p&gt;Reactâs arrival in 2013 crystallized these ideals. It promised a world where &lt;code&gt;UI = f(state)&lt;/code&gt;: give it data, get back a component tree, re-render when data changes. No manual DOM manipulation. No implicit side effects. Just pure, predictable transformations.&lt;/p&gt;
    &lt;p&gt;This was seductive. And in many ways, it worked. But it also set us on a path toward rebuilding the web in JavaScriptâs image, rather than JavaScript in the webâs image.&lt;/p&gt;
    &lt;head rend="h2"&gt;How FP Purism Shaped Modern Frontend&lt;/head&gt;
    &lt;head rend="h3"&gt;CSS-in-JS: The War on Global Scope&lt;/head&gt;
    &lt;p&gt;CSS was designed to be global. Styles cascade, inherit, and compose across boundaries. This enables tiny stylesheets to control huge documents, and lets teams share design systems across applications. But to functional programmers, global scope is dangerous. It creates implicit dependencies and unpredictable outcomes.&lt;/p&gt;
    &lt;p&gt;Enter CSS-in-JS: styled-components, Emotion, JSS. The promise was component isolation. Styles scoped to components, no cascading surprises, no naming collisions. Styles become data, passed through JavaScript, predictably bound to elements.&lt;/p&gt;
    &lt;p&gt;But this came at a cost. CSS-in-JS libraries generate styles at runtime, injecting them into &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags as components mount. This adds JavaScript execution to the critical rendering path. Server-side rendering becomes complicated. You need to extract styles during the render, serialize them, and rehydrate them on the client. Debugging involves runtime-generated class names like &lt;code&gt;.css-1xbq8d9&lt;/code&gt;. And you lose the cascade; the very feature that made CSS powerful in the first place.&lt;/p&gt;
    &lt;p&gt;Worse, youâve moved a browser-optimized declarative language into JavaScript, a single-threaded runtime. The browser can parse and apply CSS in parallel, off the main thread. Your styled-components bundle? Thatâs main-thread work, blocking interactivity.&lt;/p&gt;
    &lt;p&gt;The web had a solution. Itâs called a stylesheet. But it wasnât pure enough.&lt;/p&gt;
    &lt;p&gt;The industry eventually recognized these problems and pivoted to Tailwind CSS. Instead of runtime CSS generation, use utility classes. Instead of styled-components, compose classes in JSX. This was better, at least itâs compile-time, not runtime. No more blocking the main thread to inject styles. No more hydration complexity.&lt;/p&gt;
    &lt;p&gt;But Tailwind still fights the cascade. Instead of writing &lt;code&gt;.button { padding: 1rem; }&lt;/code&gt; once and letting it cascade to all buttons, you write &lt;code&gt;class="px-4 py-2 bg-blue-500"&lt;/code&gt; on every single button element. Youâve traded runtime overhead for a different set of problems: class soup in your markup, massive HTML payloads, and losing the cascadeâs ability to make sweeping design changes in one place.&lt;/p&gt;
    &lt;p&gt;And hereâs where it gets truly revealing: when Tailwind added support for nested selectors using &lt;code&gt;&amp;amp;&lt;/code&gt; (a feature that would let developers write more cascade-like styles), parts of the community revolted. David Khourshid (creator of XState) shared examples of using nested selectors in Tailwind, and the backlash was immediate. Developers argued this defeated the purpose of Tailwind, that it brought back the âproblemsâ of traditional CSS, that it violated the utility-first philosophy.&lt;/p&gt;
    &lt;p&gt;Think about what this means. The platform has cascade. CSS-in-JS tried to eliminate it and failed. Tailwind tried to work around it with utilities. And when Tailwind cautiously reintroduced a cascade-like feature, developers who were trained by years of anti-cascade ideology rejected it. Weâve spent so long teaching people that the cascade is dangerous that even when their own tools try to reintroduce platform capabilities, they donât want them.&lt;/p&gt;
    &lt;p&gt;Weâre not just ignorant of the platform anymore. Weâre ideologically opposed to it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Synthetic Events: Abstracting Away the Platform&lt;/head&gt;
    &lt;p&gt;React introduced synthetic events to normalize browser inconsistencies and integrate events into its rendering lifecycle. Instead of attaching listeners directly to DOM nodes, React uses event delegation. It listens at the root, then routes events to handlers through its own system.&lt;/p&gt;
    &lt;p&gt;This feels elegant from a functional perspective. Events become data flowing through your component tree. You donât touch the DOM directly. Everything stays inside Reactâs controlled universe.&lt;/p&gt;
    &lt;p&gt;But native browser events already work. They bubble, they capture, theyâre well-specified. The browser has spent decades optimizing event dispatch. By wrapping them in a synthetic layer, React adds indirection: memory overhead for event objects, translation logic for every interaction, and debugging friction when something behaves differently than the native API.&lt;/p&gt;
    &lt;p&gt;Worse, it trains developers to avoid the platform. Developers learn Reactâs event system, not the webâs. When they need to work with third-party libraries or custom elements, they hit impedance mismatches. &lt;code&gt;addEventListener&lt;/code&gt; becomes a foreign API in their own codebase.&lt;/p&gt;
    &lt;p&gt;Again: the web had this. The browserâs event system is fast, flexible, and well-understood. But it wasnât controlled enough for the FP ideal of a closed system.&lt;/p&gt;
    &lt;head rend="h3"&gt;Client-Side Rendering and Hydration: Reinventing the Browser&lt;/head&gt;
    &lt;p&gt;The logical extreme of âUI as a pure function of stateâ is client-side rendering: the server sends an empty HTML shell, JavaScript boots up, and the app renders entirely in the browser. From a functional perspective, this is clean. Your app is a deterministic function that takes initial state and produces a DOM tree.&lt;/p&gt;
    &lt;p&gt;From a web perspective, itâs a disaster. The browser sits idle while JavaScript parses, executes, and manually constructs the DOM. Users see blank screens. Screen readers get empty documents. Search engines see nothing. Progressive rendering which is one of the browserâs most powerful features, goes unused.&lt;/p&gt;
    &lt;p&gt;The industry noticed. Server-side rendering came back. But because the mental model was still âJavaScript owns the DOM,â we got hydration: the server renders HTML, the client renders the same tree in JavaScript, then React walks both and attaches event handlers. During hydration, the page is visible but inert. Clicks do nothing, forms donât submit.&lt;/p&gt;
    &lt;p&gt;This is architecturally absurd. The browser already rendered the page. It already knows how to handle clicks. But because the framework wants to own all interactions through its synthetic event system, it must re-create the entire component tree in JavaScript before anything works.&lt;/p&gt;
    &lt;p&gt;The absurdity extends beyond the client. Infrastructure teams watch in confusion as every user makes double the number of requests: the server renders the page and fetches data, then the client boots up and fetches the exact same data again to reconstruct the component tree for hydration. Why? Because the framework canât trust the HTML it just generated. It needs to rebuild its internal representation of the UI in JavaScript to attach event handlers and manage state.&lt;/p&gt;
    &lt;p&gt;This isnât just wasteful, itâs expensive. Database queries run twice. API calls run twice. Cache layers get hit twice. CDN costs double. And for what? So the framework can maintain its pure functional model where all state lives in JavaScript. The browser had the data. The HTML had the data. But that data wasnât in the right shape. It wasnât a JavaScript object tree, so we throw it away and fetch it again.&lt;/p&gt;
    &lt;p&gt;Hydration is what happens when you treat the web like a blank canvas instead of a platform with capabilities. The web gave us streaming HTML, progressive enhancement, and instant interactivity. We replaced it with JSON, JavaScript bundles, duplicate network requests, and âplease wait while we reconstruct reality.â&lt;/p&gt;
    &lt;head rend="h3"&gt;The Modal Problem: Teaching Malpractice as Best Practice&lt;/head&gt;
    &lt;p&gt;Consider the humble modal dialog. The web has &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt;, a native element with built-in functionality: it manages focus trapping, handles Escape key dismissal, provides a backdrop, controls scroll-locking on the body, and integrates with the accessibility tree. It exists in the DOM but remains hidden until opened. No JavaScript mounting required. Itâs fast, accessible, and battle-tested by browser vendors.&lt;/p&gt;
    &lt;p&gt;Now observe what gets taught in tutorials, bootcamps, and popular React courses: build a modal with &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; elements. Conditionally render it when &lt;code&gt;isOpen&lt;/code&gt; is true. Manually attach a click-outside handler. Write an effect to listen for the Escape key. Add another effect for focus trapping. Implement your own scroll-lock logic. Remember to add ARIA attributes. Oh, and make sure to clean up those event listeners, or youâll have memory leaks.&lt;/p&gt;
    &lt;p&gt;Youâve just written 100+ lines of JavaScript to poorly recreate what the browser gives you for free. Worse, youâve trained developers to not even look for native solutions. The platform becomes invisible. When someone asks âhow do I build a modal?â, the answer is âinstall a libraryâ or âhereâs my custom hook,â never âuse &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt;.â&lt;/p&gt;
    &lt;p&gt;The teaching is the problem. When influential tutorial authors and bootcamp curricula skip native APIs in favor of React patterns, theyâre not just showing an alternative approach. Theyâre actively teaching malpractice. A generation of developers learns to build inaccessible &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; soup because thatâs what fits the frameworkâs reactivity model, never knowing the platform already solved these problems.&lt;/p&gt;
    &lt;p&gt;And itâs not just bootcamps. Even the most popular component libraries make the same choice: shadcn/ui builds its Dialog component on Radix UI primitives, which use &lt;code&gt;&amp;lt;div role="dialog"&amp;gt;&lt;/code&gt; instead of the native &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt; element. There are open GitHub issues requesting native &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt; support, but the implicit message is clear: itâs easier to reimplement the browser than to work with it.&lt;/p&gt;
    &lt;head rend="h3"&gt;When Frameworks Canât Keep Up with the Platform&lt;/head&gt;
    &lt;p&gt;The problem runs deeper than ignorance or inertia. The frameworks themselves increasingly struggle to work with the platformâs evolution. Not because the platform features are bad, but because the frameworkâs architectural assumptions canât accommodate them.&lt;/p&gt;
    &lt;p&gt;Consider why component libraries like Radix UI choose &lt;code&gt;&amp;lt;div role="dialog"&amp;gt;&lt;/code&gt; over &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt;. The native &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt; element manages its own state: it knows when itâs open, it handles its own visibility, it controls focus internally. But Reactâs reactivity model expects all state to live in JavaScript, flowing unidirectionally into the DOM. When a native element manages its own state, Reactâs mental model breaks down. Keeping &lt;code&gt;isOpen&lt;/code&gt; in your React state synchronized with the &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt; elementâs actual open/closed state becomes a nightmare of refs, effects, and imperative calls. Precisely what React was supposed to eliminate.&lt;/p&gt;
    &lt;p&gt;Rather than adapt their patterns to work with stateful native elements, library authors reimplement the entire behavior in a way that fits the framework. Itâs architecturally easier to build a fake dialog in JavaScript than to integrate with the platformâs real one.&lt;/p&gt;
    &lt;p&gt;But the conflict extends beyond architectural preferences. Even when the platform adds features that developers desperately want, frameworks canât always use them.&lt;/p&gt;
    &lt;p&gt;Accordions? The web has &lt;code&gt;&amp;lt;details&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;summary&amp;gt;&lt;/code&gt;. Tooltips? Thereâs &lt;code&gt;title&lt;/code&gt; attribute and the emerging &lt;code&gt;popover&lt;/code&gt; API. Date pickers? &lt;code&gt;&amp;lt;input type="date"&amp;gt;&lt;/code&gt;. Custom dropdowns? The web now supports styling &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; elements with &lt;code&gt;appearance: base-select&lt;/code&gt; and &lt;code&gt;::picker(select)&lt;/code&gt; pseudo-elements. You can even put &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; elements with images inside &lt;code&gt;&amp;lt;option&amp;gt;&lt;/code&gt; elements now. It eliminates the need for the countless JavaScript select libraries that exist solely because designers wanted custom styling.&lt;/p&gt;
    &lt;p&gt;Frameworks encourage conditional rendering and component state, so these elements donât get rendered until JavaScript decides they should exist. The mental model is âUI appears when state changes,â not âUI exists, state controls visibility.â Even when the platform adds the exact features developers have been rebuilding in JavaScript for years, the ecosystem momentum means most developers never learn these features exist.&lt;/p&gt;
    &lt;p&gt;And hereâs the truly absurd part: even when developers do know about these new platform features, the frameworks themselves canât handle them. MDNâs documentation for customizable &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; elements includes this warning: âSome JavaScript frameworks block these features; in others, they cause hydration failures when Server-Side Rendering (SSR) is enabled.â The platform evolved. The HTML parser now allows richer content inside &lt;code&gt;&amp;lt;option&amp;gt;&lt;/code&gt; elements. But Reactâs JSX parser and hydration system werenât designed for this. They expect &lt;code&gt;&amp;lt;option&amp;gt;&lt;/code&gt; to only contain text. Updating the framework to accommodate the platformâs evolution takes time, coordination, and breaking changes that teams are reluctant to make.&lt;/p&gt;
    &lt;p&gt;The web platform added features that eliminate entire categories of JavaScript libraries, but the dominant frameworks canât use those features without causing hydration errors. The stack that was supposed to make development easier now lags behind the platform itâs built on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Routing and Forms: JavaScript All the Way Down&lt;/head&gt;
    &lt;p&gt;The browser has native routing: &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tags, the History API, forward/back buttons. It has native forms: &lt;code&gt;&amp;lt;form&amp;gt;&lt;/code&gt; elements, validation attributes, submit events. These work without JavaScript. Theyâre accessible by default. Theyâre fast.&lt;/p&gt;
    &lt;p&gt;Modern frameworks threw them out. React Router, Next.jsâs router, Vue Router; they intercept link clicks, prevent browser navigation, and handle routing in JavaScript. Why? Because client-side routing feels like a pure state transition: URL changes, state updates, component re-renders. No page reload. No âlostâ JavaScript state.&lt;/p&gt;
    &lt;p&gt;But youâve now made navigation depend on JavaScript. Ctrl+click to open in a new tab? Broken, unless you carefully re-implement it. Right-click to copy link? The URL might not match whatâs rendered. Accessibility tools that rely on standard navigation patterns? Confused.&lt;/p&gt;
    &lt;p&gt;Forms got the same treatment. Instead of letting the browser handle submission, validation, and accessibility, frameworks encourage JavaScript-controlled forms. Formik, React Hook Form, uncontrolled vs. controlled inputs; entire libraries exist to manage what &lt;code&gt;&amp;lt;form&amp;gt;&lt;/code&gt; already does. The browser can validate &lt;code&gt;&amp;lt;input type="email"&amp;gt;&lt;/code&gt; instantly, with no JavaScript. But thatâs not reactive enough, so we rebuild validation in JavaScript, ship it to the client, and hope we got the logic right.&lt;/p&gt;
    &lt;p&gt;The web had these primitives. We rejected them because they didnât fit our FP-inspired mental model of âstate flows through JavaScript.â&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Lost in the Process&lt;/head&gt;
    &lt;p&gt;Progressive enhancement used to be a best practice: start with working HTML, layer on CSS for style, add JavaScript for interactivity. The page works at every level. Now, we start with JavaScript and work backwards, trying to squeeze HTML out of our component trees and hoping hydration doesnât break.&lt;/p&gt;
    &lt;p&gt;We lost built-in accessibility. Native HTML elements have roles, labels, and keyboard support by default. Custom JavaScript widgets require &lt;code&gt;aria-*&lt;/code&gt; attributes, focus management, and keyboard handlers. All easy to forget or misconfigure.&lt;/p&gt;
    &lt;p&gt;We lost performance. The browserâs streaming parser can render HTML as it arrives. Modern frameworks send JavaScript, parse JavaScript, execute JavaScript, then finally render. Thatâs slower. The browser can cache CSS and HTML aggressively. JavaScript bundles invalidate on every deploy.&lt;/p&gt;
    &lt;p&gt;We lost simplicity. &lt;code&gt;&amp;lt;a href="/about"&amp;gt;&lt;/code&gt; is eight characters. A client-side router is a dependency, a config file, and a mental model. &lt;code&gt;&amp;lt;form action="/submit" method="POST"&amp;gt;&lt;/code&gt; is self-documenting. A controlled form with validation is dozens of lines of state management.&lt;/p&gt;
    &lt;p&gt;And we lost alignment with the platform. The browser vendors spend millions optimizing HTML parsing, CSS rendering, and event dispatch. We spend thousands of developer-hours rebuilding those features in JavaScript, slower.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Happened&lt;/head&gt;
    &lt;p&gt;This isnât a story of incompetence. Smart people built these tools for real reasons.&lt;/p&gt;
    &lt;p&gt;By the early 2010s, JavaScript applications had become unmaintainable. jQuery spaghetti sprawled across codebases. Two-way data binding caused cascading updates that were impossible to debug. Teams needed discipline, and functional programming offered it: pure components, immutable state, unidirectional data flow. For complex, stateful applications (like dashboards with hundreds of interactive components, real-time collaboration tools, data visualization platforms) Reactâs model was genuinely better than manually wiring up event handlers and tracking mutations.&lt;/p&gt;
    &lt;p&gt;The FP purists werenât wrong that unpredictable mutation causes bugs. They were wrong that the solution was avoiding the platformâs mutation-friendly APIs instead of learning to use them well. But in the chaos of 2013, that distinction didnât matter. React worked. It scaled. And Facebook was using it in production.&lt;/p&gt;
    &lt;p&gt;Then came the hype cycle. React dominated the conversation. Every conference had React talks. Every tutorial assumed React as the starting point. CSS-in-JS became âmodern.â Client-side rendering became the default. When big companies like Facebook, Airbnb, Netflix and others adopted these patterns, they became industry standards. Bootcamps taught React exclusively. Job postings required React experience. The narrative solidified: this is how you build for the web now.&lt;/p&gt;
    &lt;p&gt;The ecosystem became self-reinforcing through its own momentum. Once React dominated hiring pipelines and Stack Overflow answers, alternatives faced an uphill battle. Teams that had already invested in React by training developers, building component libraries, establishing patterns are now facing enormous switching costs. New developers learned React because thatâs what jobs required. Jobs required React because thatâs what developers knew. The cycle fed itself, independent of whether React was the best tool for any particular job.&lt;/p&gt;
    &lt;p&gt;This is where we lost the plot. Somewhere in the transition from âReact solves complex application problemsâ to âReact is how you build websites,â we stopped asking whether the problems we were solving actually needed these solutions. Iâve watched developers build personal blogs with Next.js. Sites that are 95% static content with maybe a contact form, because thatâs what they learned in bootcamp. Iâve seen companies choose React for marketing sites with zero interactivity, not because itâs appropriate, but because they canât hire developers who know anything else.&lt;/p&gt;
    &lt;p&gt;The tool designed for complex, stateful applications became the default for everything, including problems the web solved in 1995 with HTML and CSS. A generation of developers never learned that most websites donât need a framework at all. The question stopped being âdoes this problem need React?â and became âwhich React pattern should I use?â The platformâs native capabilities like progressive rendering, semantic HTML, the cascade, instant navigation are now considered âold-fashioned.â Reinventing them in JavaScript became âbest practices.â&lt;/p&gt;
    &lt;p&gt;We chased functional purity on a platform that was never designed for it. And we built complexity to paper over the mismatch.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Way Forward&lt;/head&gt;
    &lt;p&gt;The good news: weâre learning. The industry is rediscovering the platform.&lt;/p&gt;
    &lt;p&gt;HTMX embraces HTML as the medium of exchange. Server sends HTML, browser renders it, no hydration needed. Qwik resumable architecture avoids hydration entirely, serializing only whatâs needed. Astro defaults to server-rendered HTML with minimal JavaScript. Remix and SvelteKit lean into web standards: forms that work without JS, progressive enhancement, leveraging the browserâs cache.&lt;/p&gt;
    &lt;p&gt;These tools acknowledge what the web is: a document-based platform with powerful native capabilities. Instead of fighting it, they work with it.&lt;/p&gt;
    &lt;p&gt;This doesnât mean abandoning components or reactivity. It means recognizing that &lt;code&gt;UI = f(state)&lt;/code&gt; is a useful model inside your framework, not a justification to rebuild the entire browser stack. It means using CSS for styling, native events for interactions, and HTML for structure and then reaching for JavaScript when you need interactivity beyond what the platform provides.&lt;/p&gt;
    &lt;p&gt;The best frameworks of the next decade will be the ones that feel like the web, not in spite of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In chasing functional purity, we built a frontend stack that is more complex, more fragile, and less aligned with the platform it runs on. We recreated CSS in JavaScript, events in synthetic wrappers, rendering in hydration layers, and routing in client-side state machines. We did this because we wanted predictability, control, and clean abstractions.&lt;/p&gt;
    &lt;p&gt;But the web was never meant to be pure. Itâs a sprawling, messy, miraculous platform built on decades of emergent behavior, pragmatic compromises, and radical openness. Its mutability isnât a bug. Itâs the reason a document written in 1995 still renders in 2025. Its global scope isnât dangerous. Itâs what lets billions of pages share a design language.&lt;/p&gt;
    &lt;p&gt;Maybe the web didnât need to be purified. Maybe it just needed to be understood.&lt;/p&gt;
    &lt;p&gt;I want to thank my friend Ihab Khattab for reviewing this piece and providing invaluable feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alfy.blog/2025/10/04/how-functional-programming-shaped-modern-frontend.html"/><published>2025-10-04T13:04:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45473698</id><title>Flock's gunshot detection microphones will start listening for human voices</title><updated>2025-10-04T20:33:42.560641+00:00</updated><content>&lt;doc fingerprint="8e719bc83b37fb49"&gt;
  &lt;main&gt;
    &lt;p&gt;Flock Safety, the police technology company most notable for their extensive network of automated license plate readers spread throughout the United States, is rolling out a new and troubling product that may create headaches for the cities that adopt it: detection of “human distress” via audio. As part of their suite of technologies, Flock has been pushing Raven, their version of acoustic gunshot detection. These devices capture sounds in public places and use machine learning to try to identify gunshots and then alert police—but EFF has long warned that they are also high powered microphones parked above densely-populated city streets. Cities now have one more reason to follow the lead of many other municipalities and cancel their Flock contracts, before this new feature causes civil liberties harms to residents and headaches for cities.&lt;/p&gt;
    &lt;p&gt;In marketing materials, Flock has been touting new features to their Raven product—including the ability of the device to alert police based on sounds, including “distress.” The online ad for the product, which allows cities to apply for early access to the technology, shows the image of police getting an alert for “screaming.”&lt;/p&gt;
    &lt;p&gt;It’s unclear how this technology works. For acoustic gunshot detection, generally the microphones are looking for sounds that would signify gunshots (though in practice they often mistake car backfires or fireworks for gunshots). Flock needs to come forward now with an explanation of exactly how their new technology functions. It is unclear how these devices will interact with state “eavesdropping” laws that limit listening to or recording the private conversations that often take place in public.&lt;/p&gt;
    &lt;p&gt;Flock is no stranger to causing legal challenges for the cities and states that adopt their products. In Illinois, Flock was accused of violating state law by allowing Immigration and Customs Enforcement (ICE), a federal agency, access to license plate reader data taken within the state. That’s not all. In 2023, a North Carolina judge halted the installation of Flock cameras statewide for operating in the state without a license. When the city of Evanston, Illinois recently canceled its contract with Flock, it ordered the company to take down their license plate readers–only for Flock to mysteriously reinstall them a few days later. This city has now sent Flock a cease and desist order and in the meantime, has put black tape over the cameras. For some, the technology isn’t worth its mounting downsides. As one Illinois village trustee wrote while explaining his vote to cancel the city’s contract with Flock, “According to our own Civilian Police Oversight Commission, over 99% of Flock alerts do not result in any police action.”&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Gunshot detection technology is dangerous enough as it is—police showing up to alerts they think are gunfire only to find children playing with fireworks is a recipe for innocent people to get hurt. This isn’t hypothetical: in Chicago a child really was shot at by police who thought they were responding to a shooting thanks to a ShotSpotter alert. Introducing a new feature that allows these pre-installed Raven microphones all over cities to begin listening for human voices in distress is likely to open up a whole new can of unforeseen legal, civil liberties, and even bodily safety consequences. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/deeplinks/2025/10/flocks-gunshot-detection-microphones-will-start-listening-human-voices"/><published>2025-10-04T14:49:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45473730</id><title>Self-hosting email like it's 1984</title><updated>2025-10-04T20:33:41.983271+00:00</updated><content>&lt;doc fingerprint="ec9d2d5b3e50b695"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Self-hosting an email server is useful for automating tasks like mailing lists, newsletters, or email verification APIs.&lt;/p&gt;
    &lt;p&gt;The elephant in the room is real-world deliverability. With self-hosting you risk not receiving mail or someone missing your mail. I accept this for my personal projects, but you may not. Keep this in mind.&lt;/p&gt;
    &lt;p&gt;For me the selling point of self-hosting is that itâs practically free. If youâre already self-hosting a website, installing some extra packages on your server and just a bit of your time is all thatâs required. Mail takes very little storage and the software is light, so youâre unlikely to significantly change energy consumption or disk usage.&lt;/p&gt;
    &lt;p&gt;For the longest time, I perceived self-hosting email as too difficult, but after doing it for one of my projects, I can say itâs not much harder or more time-consuming than configuring some email SaaS.&lt;/p&gt;
    &lt;p&gt;I changed my goals a bit to make the setup easier though. Self-hosting a multi-user webmail looks heavy and is more involved than I was willing to get into, so I just skipped it. That way, I didnât have to bother with user accounts, databases, or the web at all, and the task became easy.&lt;/p&gt;
    &lt;p&gt;With my config, manually sending and receiving email is possible if you SSH to your mail server and use the minimal sendmail or mailx commands, or Mutt if you like TUI. The setup is enough for me now, but I could expand it in the future, and multi-user webmail isnât off the table anyway. Maybe Iâll even write a simple webmail package myself!&lt;/p&gt;
    &lt;head rend="h2"&gt;Setting up Postfix&lt;/head&gt;
    &lt;p&gt;You just need to open port 25, and install and configure Postfix and OpenDKIM on your machine. Postfix is a complete SMTP server, and is enough for basic mail alone, but in practice you also need OpenDKIM to get your mail delivered to popular services like Gmail.&lt;/p&gt;
    &lt;p&gt;Here's my Postfix config to show how easy it is. I left the master.cf file as it was, because Iâm always submitting email locally. Notice there's no mention of POP3 or IMAP. I didn't set them up, because I didn't need them.&lt;/p&gt;
    &lt;p&gt;The default alias and header check config files are practically self-explanatory (just open them and read the comments!).&lt;/p&gt;
    &lt;head&gt;/etc/postfix/main.cf&lt;/head&gt;
    &lt;quote&gt;compatibility_level = 3.8 mail_owner = postfix myhostname = mx.idx.cy myorigin = idx.cy mydestination = localhost, idx.cy, maxadamski.com, localchat.cc inet_interfaces = all inet_protocols = ipv4 # Addresses alias_maps = hash:/etc/postfix/aliases alias_database = $alias_maps recipient_delimiter = + # I'm the only user on my machine, so I send from whichever address I want. #smtpd_sender_login_maps = hash:/etc/postfix/sender_login_maps #smtpd_sender_restrictions = reject_authenticated_sender_login_mismatch # spam #in_flow_delay = 1s header_checks = regexp:/etc/postfix/header_checks setgid_group = postdrop # TLS (strict) smtpd_tls_cert_file = /etc/ssl/tls/mx.idx.cy.crt smtpd_tls_key_file = /etc/ssl/tls/mx.idx.cy.key smtpd_tls_security_level = encrypt smtpd_tls_mandatory_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtpd_tls_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtp_tls_security_level = encrypt smtp_tls_mandatory_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtp_tls_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 # DKIM smtpd_milters = inet:localhost:8891 non_smtpd_milters = inet:localhost:8891 milter_default_action = accept&lt;/quote&gt;
    &lt;head rend="h2"&gt;TLS&lt;/head&gt;
    &lt;p&gt;You will also need an SSL certificate for encryption in transit. I hate getting and renewing SSL certificates, because the tools are bulky and automation is yet another moving part in your system (I used the lego package, with the manual DNS challenge for simplicity, but Iâm not too happy about it). I wonât give you a tutorial on getting SSL certificates, but note that you donât have to get and renew a certificate for each of your custom domains!&lt;/p&gt;
    &lt;p&gt;You just need one SSL certificate for your machine to encrypt data in transit to other SMTP servers. If you create an A record mx.example.com pointing to your email machineâs IP address, then grab a free certificate for mx.example.com from Letâs Encrypt. Then point to it in the Postfix configuration, and youâve got transport encryption! In short, only the MX hostname needs a cert for STARTTLS to be used for encryption.&lt;/p&gt;
    &lt;p&gt;Why no certificates for your actual email domains like example.com? Because the email domain has little to do with transport encryption. TLS only secures the connection between servers. You can still set whatever you want in the From header.&lt;/p&gt;
    &lt;head rend="h2"&gt;DKIM, SPF, and DMARC&lt;/head&gt;
    &lt;p&gt;You should prove that your emails actually come from your domain to make your mail trustworthy and deliver to Gmail and co. Thatâs what DKIM is for, and fortunately itâs a one-time deal per email domain. First you generate a key pair for each domain with OpenDKIM, and then you publish the public key in a TXT record in DNS. The keys donât expire automatically, but itâs best practice to rotate them periodically. My config uses a naming scheme that allows smooth rotation, but it doesnât complicate things if you skip it.&lt;/p&gt;
    &lt;p&gt;There are two more TXT records that you need to publish in DNS: the SPF and DMARC records. You say which hosts are allowed to send mail from your email domain, and give instructions to other email servers about what to do with mail that fails DKIM checks. In my case I told others to reject mail that canât be verified as coming from my domains, and send reports to my postmaster address.&lt;/p&gt;
    &lt;p&gt;Take a look at my OpenDKIM config to understand how things come together.&lt;/p&gt;
    &lt;head&gt;/etc/opendkim.conf&lt;/head&gt;
    &lt;quote&gt;UserID opendkim:opendkim Socket inet:8891@localhost KeyTable refile:/etc/opendkim/KeyTable SigningTable refile:/etc/opendkim/SigningTable ExternalIgnoreList refile:/etc/opendkim/TrustedHosts InternalHosts refile:/etc/opendkim/TrustedHosts Canonicalization relaxed/relaxed ReportAddress postmaster@idx.cy SendReports no LogWhy yes Syslog yes SyslogSuccess no&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/KeyTable&lt;/head&gt;
    &lt;quote&gt;key1._domainkey.idx.cy idx.cy:key1:/etc/opendkim/keys/idx.cy/key1.private key1._domainkey.maxadamski.com maxadamski.com:key1:/etc/opendkim/keys/maxadamski.com/key1.private key1._domainkey.localchat.cc localchat.cc:key1:/etc/opendkim/keys/localchat.cc/key1.private&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/SigningTable&lt;/head&gt;
    &lt;quote&gt;*@idx.cy key1._domainkey.idx.cy *@maxadamski.com key1._domainkey.maxadamski.com *@localchat.cc key1._domainkey.localchat.cc&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/TrustedHosts&lt;/head&gt;
    &lt;quote&gt;127.0.0.1 localhost&lt;/quote&gt;
    &lt;p&gt;I generate DKIM keys with the following command:&lt;/p&gt;
    &lt;quote&gt;opendkim-genkey -D /etc/opendkim/keys/example.com -d example.com -s key1&lt;/quote&gt;
    &lt;p&gt;And for each email domain I have the following records in DNS:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MX&lt;/cell&gt;
        &lt;cell&gt;example.com&lt;/cell&gt;
        &lt;cell&gt;mx.idx.cy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;example.com&lt;/cell&gt;
        &lt;cell&gt;v=spf1 mx a -all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;key1._domainkey&lt;/cell&gt;
        &lt;cell&gt;v=DKIM1; k=rsa; s=email; p=&amp;lt;public-key&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;_dmarc&lt;/cell&gt;
        &lt;cell&gt;v=DMARC1; p=reject; rua=mailto:postmaster@idx.cy&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Reverse DNS&lt;/head&gt;
    &lt;p&gt;One more thing about email trust. I've read that reverse DNS (PTR record) will boost the reputation of your email server. The thing is that your ISP has to set it up, and I suspect my ISP to reply with a polite "no", so I didn't do it yet. As you'll see in the next section, my email gets delivered to Gmail just fine. I also tested with GMX, but I don't know about Microsoft/Yahoo/Proton.&lt;/p&gt;
    &lt;p&gt;However, if you want wide deliverability, PTR isn't optional.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing with Gmail&lt;/head&gt;
    &lt;p&gt;To try it out, let's send a test mail to Gmail with the sendmail command:&lt;/p&gt;
    &lt;quote&gt;sendmail -vt &amp;lt; test.mail&lt;/quote&gt;
    &lt;head&gt;test.mail&lt;/head&gt;
    &lt;quote&gt;Content-Type: text/html From: max@idx.cy To: myaddress@gmail.com Subject: DKIM test Test message from idx.cy!&lt;/quote&gt;
    &lt;p&gt;I got the mail instantly and Gmail confirmed TLS encryption.&lt;/p&gt;
    &lt;p&gt;Click "Show original" in Gmail to see the raw mail. There's lots of text in the headers, so let's just focus on passing SPF, DKIM, and DMARC :)&lt;/p&gt;
    &lt;p&gt;You'll also get a mail with a report because of the -v option. I receive mail with Heirloom Mail like this:&lt;/p&gt;
    &lt;quote&gt;You have new mail in /var/mail/max fool ~ | mailx Heirloom Mail version 12.5 7/5/10. Type ? for help. "/var/mail/max": 1 message 1 new &amp;gt;N 1 Mail Delivery System Sat Oct 4 15:40 74/2437 "Mail Delivery Status Report"&lt;/quote&gt;
    &lt;p&gt;I use the p command to print the mail.&lt;/p&gt;
    &lt;head&gt;&amp;amp; p&lt;/head&gt;
    &lt;quote&gt;Message 1: From MAILER-DAEMON Sat Oct 4 15:40:50 2025 X-Original-To: max@idx.cy Delivered-To: max@idx.cy Date: Sat, 4 Oct 2025 15:40:50 +0200 (CEST) From: Mail Delivery System &amp;lt;MAILER-DAEMON@idx.cy&amp;gt; Subject: Mail Delivery Status Report To: max@idx.cy Auto-Submitted: auto-replied Content-Type: multipart/report; report-type=delivery-status; boundary="3C311BFF8D.1759585250/mx.idx.cy" Status: R Part 1: Content-Description: Notification Content-Type: text/plain; charset=utf-8 This is the mail system at host mx.idx.cy. Enclosed is the mail delivery report that you requested. The mail system &amp;lt;myaddress@gmail.com&amp;gt;: delivery via gmail-smtp-in.l.google.com[X.X.X.X]:25: 250 2.0.0 OK 1759585250 4fb4d7f45d1cf-6393b6ba951si3187039a12.40 - gsmtp&lt;/quote&gt;
    &lt;p&gt;Great, everything is working! And I'm feeling the spirit of the 80s thanks to mailx :)&lt;/p&gt;
    &lt;p&gt;If something isn't working for you, please double-check your DNS records, and triple-check that TLS certificates are readable by the Postfix user, and that DKIM keys are readable by the OpenDKIM user. Postfix and OpenDKIM logs will also be useful. The OpenDKIM config file is especially unforgiving of typos, so watch out for small mistakes!&lt;/p&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;In my next post on email, I'll show you how to use Python to build useful email applications. Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Btw, if you notice anything about my config (or just want to share some thoughts) just email me at max@idx.cy :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maxadamski.com/blog/2025/10/email.html"/><published>2025-10-04T14:53:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45473852</id><title>How I influence tech company politics as a staff software engineer</title><updated>2025-10-04T20:33:41.892072+00:00</updated><content>&lt;doc fingerprint="be6db154ba4555d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I influence tech company politics as a staff software engineer&lt;/head&gt;
    &lt;p&gt;Many software engineers are fatalistic about company politics. They believe that it’s pointless to get involved, because1:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Technical decisions are often made for completely selfish reasons that cannot be influenced by a well-meaning engineer&lt;/item&gt;
      &lt;item&gt;Powerful stakeholders are typically so stupid and dysfunctional that it’s effectively impossible for you to identify their needs and deliver solutions to them&lt;/item&gt;
      &lt;item&gt;The political game being played depends on private information that software engineers do not have, so any attempt to get involved will result in just blundering around&lt;/item&gt;
      &lt;item&gt;Managers and executives spend most of their time playing politics, while engineers spend most of their time doing engineering, so engineers are at a serious political disadvantage before they even start&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The general idea here is that software engineers are simply not equipped to play the game at the same level as real political operators. This is true! It would be a terrible mistake for a software engineer to think that you ought to start scheming and plotting like you’re in Game of Thrones. Your schemes will be immediately uncovered and repurposed to your disadvantage and other people’s gain. Scheming takes practice and power, and neither of those things are available to software engineers.&lt;/p&gt;
    &lt;p&gt;It is simply a fact that software engineers are tools in the political game being played at large companies, not players in their own right. However, there are many ways to get involved in politics without scheming.&lt;/p&gt;
    &lt;p&gt;The easiest way is to actively work to make a high-profile project successful. This is more or less what you ought to be doing anyway, just as part of your ordinary job. If your company is heavily investing in some new project - these days, likely an AI project - using your engineering skill to make it successful2 is a politically advantageous move for whatever VP or executive is spearheading that project. In return, you’ll get the rewards that executives can give at tech companies: bonuses, help with promotions, and positions on future high-profile projects. I wrote about this almost a year ago in Ratchet effects determine engineer reputation at large companies.&lt;/p&gt;
    &lt;p&gt;A slightly harder way (but one that gives you more control) is to make your pet idea available for an existing political campaign. Suppose you’ve wanted for a while to pull out some existing functionality into its own service. There are two ways to make that happen.&lt;/p&gt;
    &lt;p&gt;The hard way is to expend your own political capital: drum up support, let your manager know how important it is to you, and slowly wear doubters down until you can get the project formally approved. The easy way is to allow some executive to spend their (much greater) political capital on your project. You wait until there’s a company-wide mandate for some goal that aligns with your project (say, a push for reliability, which often happens in the wake of a high-profile incident). Then you suggest to your manager that your project might be a good fit for this. If you’ve gauged it correctly, your org will get behind your project. Not only that, but it’ll increase your political capital instead of you having to spend it.&lt;/p&gt;
    &lt;p&gt;Organizational interest comes in waves. When it’s reliability time, VPs are desperate to be doing something. They want to come up with plausible-sounding reliability projects that they can fund, because they need to go to their bosses and point at what they’re doing for reliability, but they don’t have the skillset to do it on their own. They’re typically happy to fund anything that the engineering team suggests. On the other hand, when the organization’s attention is focused somewhere else - say, on a big new product ship - the last thing they want is for engineers to spend their time on an internal reliability-focused refactor that’s invisible to customers.&lt;/p&gt;
    &lt;p&gt;So if you want to get something technical done in a tech company, you ought to wait for the appropriate wave. It’s a good idea to prepare multiple technical programs of work, all along different lines. Strong engineers will do some of this kind of thing as an automatic process, simply by noticing things in the normal line of work. For instance, you might have rough plans:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;to migrate the billing code to stored-data-updated-by-webhooks instead of cached API calls&lt;/item&gt;
      &lt;item&gt;to rip out the ancient hand-rolled build pipeline and replace it with Vite&lt;/item&gt;
      &lt;item&gt;to rewrite a crufty high-volume Python service in Golang&lt;/item&gt;
      &lt;item&gt;to replace the slow CMS frontend that backs your public documentation with a fast static site&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When executives are concerned about billing, you can offer the billing refactor as a reliability improvement. When they’re concerned about developer experience, you can suggest replacing the build pipeline. When customers are complaining about performance, you can point to the Golang rewrite as a good option. When the CEO checks the state of the public documentation and is embarrassed, you can make the case for rebuilding it as a static site. The important thing is to have a detailed, effective program of work ready to go for whatever the flavor of the month is.&lt;/p&gt;
    &lt;p&gt;Some program of work will be funded whether you do this or not. However, if you don’t do this, you have no control over what that program is. In my experience, this is where companies make their worst technical decisions: when the political need to do something collides with a lack of any good ideas. When there are no good ideas, a bad idea will do, in a pinch. But nobody prefers this outcome. It’s bad for the executives, who then have to sell a disappointing technical outcome as if it were a success4, and it’s bad for the engineers, who have to spend their time and effort building the wrong idea.&lt;/p&gt;
    &lt;p&gt;If you’re a very senior engineer, the VPs (or whoever) will quietly blame you for this. They’ll be right to! Having the right idea handy at the right time is your responsibility.&lt;/p&gt;
    &lt;p&gt;You can view all this in two different ways. Cynically, you can read this as a suggestion to make yourself a convenient tool for the sociopaths who run your company to use in their endless internecine power struggles. Optimistically, you can read this as a suggestion to let executives set the overall priorities for the company - that’s their job, after all - and to tailor your own technical plans to fit3. Either way, you’ll achieve more of your technical goals if you push the right plan at the right time.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;I was prompted to write this after reading Terrible Software’s article Don’t avoid workplace politics and its comments on Hacker News. Disclaimer: I am talking here about broadly functional tech companies (i.e. ones that are making money). If you’re working somewhere that’s completely dysfunctional, I have no idea whether this advice would apply at all.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;What it takes to make a project successful is itself a complex political question that every senior+ engineer is eventually forced to grapple with (or to deliberately avoid, with consequences for their career). For more on that, see How I ship projects at large tech companies.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For more along these lines, see Is it cynical to do what your manager wants?&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Just because they can do this doesn’t mean they want to.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;October 4, 2025 │ Tags: good engineers, tech companies&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/how-to-influence-politics/"/><published>2025-10-04T15:09:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45473861</id><title>A comparison of Ada and Rust, using solutions to the Advent of Code</title><updated>2025-10-04T20:33:41.358669+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/johnperry-math/AoC2023/blob/master/More_Detailed_Comparison.md"/><published>2025-10-04T15:10:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45474441</id><title>Privacy Harm Is Harm</title><updated>2025-10-04T20:33:41.288666+00:00</updated><content>&lt;doc fingerprint="ac0ebb5f6eb0f1d0"&gt;
  &lt;main&gt;
    &lt;p&gt;Every day, corporations track our movements through license plate scanners, building detailed profiles of where we go, when we go there, and who we visit. When they do this to us in violation of data privacy laws, we’ve suffered a real harm—period. We shouldn’t need to prove we’ve suffered additional damage, such as physical injury or monetary loss, to have our day in court.&lt;/p&gt;
    &lt;p&gt;That's why EFF is proud to join an amicus brief in Mata v. Digital Recognition Network, a lawsuit by drivers against a corporation that allegedly violated a California statute that regulates Automatic License Plate Readers (ALPRs). The state trial court erroneously dismissed the case, by misinterpreting this data privacy law to require proof of extra harm beyond privacy harm. The brief was written by the ACLU of Northern California, Stanford’s Juelsgaard Clinic, and UC Law SF’s Center for Constitutional Democracy.&lt;/p&gt;
    &lt;p&gt;The amicus brief explains:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This case implicates critical questions about whether a California privacy law, enacted to protect people from harmful surveillance, is not just words on paper, but can be an effective tool for people to protect their rights and safety.&lt;/p&gt;
      &lt;p&gt;California’s Constitution and laws empower people to challenge harmful surveillance at its inception without waiting for its repercussions to manifest through additional harms. A foundation for these protections is article I, section 1, which grants Californians an inalienable right to privacy.&lt;/p&gt;
      &lt;p&gt;People in the state have long used this constitutional right to challenge the privacy-invading collection of information by private and governmental parties, not only harms that are financial, mental, or physical. Indeed, widely understood notions of privacy harm, as well as references to harm in the California Code, also demonstrate that term’s expansive meaning.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h4"&gt;What’s At Stake&lt;/head&gt;
    &lt;p&gt;The defendant, Digital Recognition Network, also known as DRN Data, is a subsidiary of Motorola Solutions that provides access to a massive searchable database of ALPR data collected by private contractors. Its customers include law enforcement agencies and private companies, such as insurers, lenders, and repossession firms. DRN is the sister company to the infamous surveillance vendor Vigilant Solutions (now Motorola Solutions), and together they have provided data to ICE through a contract with Thomson Reuters.&lt;/p&gt;
    &lt;p&gt;The consequences of weak privacy protections are already playing out across the country. This year alone, authorities in multiple states have used license plate readers to hunt for people seeking reproductive healthcare. Police officers have used these systems to stalk romantic partners and monitor political activists. ICE has tapped into these networks to track down immigrants and their families for deportation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Strong Privacy Laws&lt;/head&gt;
    &lt;p&gt;This case could determine whether privacy laws have real teeth or are just words on paper. If corporations can collect your personal information with impunity—knowing that unless you can prove bodily injury or economic loss, you can’t fight back—then privacy laws lose value.&lt;/p&gt;
    &lt;p&gt;We need strong data privacy laws. We need a private right of action so when a company violates our data privacy rights, we can sue them. We need a broad definition of “harm,” so we can sue over our lost privacy rights, without having to prove collateral injury. EFF wages this battle when writing privacy laws, when interpreting those laws, and when asserting “standing” in federal and state courts.&lt;/p&gt;
    &lt;p&gt;The fight for privacy isn’t just about legal technicalities. It’s about preserving your right to move through the world without being constantly tracked, catalogued, and profiled by corporations looking to profit from your personal information.&lt;/p&gt;
    &lt;p&gt;You can read the amicus brief here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/deeplinks/2025/10/privacy-harm-harm"/><published>2025-10-04T16:19:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45474900</id><title>Knowledge Infusion Scaling Law for Pre-Training Large Language Models</title><updated>2025-10-04T20:33:41.060976+00:00</updated><content>&lt;doc fingerprint="ad1db54858d58a0a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 19 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.19371"/><published>2025-10-04T17:18:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45475528</id><title>Show HN: Run – a CLI universal code runner I built while learning Rust</title><updated>2025-10-04T20:33:40.555982+00:00</updated><content>&lt;doc fingerprint="24c05ddd9ba0d2c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Polyglot command runner &amp;amp; smart REPL that lets you script, compile, and iterate in 25+ languages without touching another CLI.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Built in Rust for developers who live in multiple runtimes.&lt;/p&gt;&lt;code&gt;run&lt;/code&gt;gives you a consistent CLI, persistent REPLs, and batteries-included examples for your favorite languages.&lt;/quote&gt;
    &lt;head&gt;Table of contents&lt;/head&gt;
    &lt;code&gt;# Show build metadata for the current binary
run --version

# Execute a snippet explicitly
run --lang python --code "print('hello, polyglot world!')"

# Let run detect language from the file extension
run examples/go/hello/main.go

# Drop into the interactive REPL (type :help inside)
run

# Pipe stdin (here: JSON) into Node.js
echo '{"name":"Ada"}' | run js --code "const data = JSON.parse(require('fs').readFileSync(0, 'utf8')); console.log(`hi ${data.name}`)"&lt;/code&gt;
    &lt;p&gt;All release assets are published on the GitHub Releases page, including macOS builds for both Apple Silicon (arm64) and Intel (x86_64). Pick the method that fits your platform:&lt;/p&gt;
    &lt;head&gt;Cargo (Rust)&lt;/head&gt;
    &lt;code&gt;cargo install run-kit&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Installs the&lt;/p&gt;&lt;code&gt;run&lt;/code&gt;binary from the&lt;code&gt;run-kit&lt;/code&gt;crate. Updating? Run&lt;code&gt;cargo install run-kit --force&lt;/code&gt;.&lt;/quote&gt;
    &lt;head&gt;Homebrew (macOS)&lt;/head&gt;
    &lt;code&gt;brew install --formula https://github.com/Esubaalew/run/releases/latest/download/homebrew-run.rb&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;This formula is published as a standalone file on each release; it isn’t part of the default Homebrew taps. Installing by name (&lt;/p&gt;&lt;code&gt;brew install homebrew-run&lt;/code&gt;) will fail—always point Homebrew to the release URL above (or download the file and run&lt;code&gt;brew install ./homebrew-run.rb&lt;/code&gt;).&lt;/quote&gt;
    &lt;p&gt;Once the latest release artifacts are published, Homebrew automatically selects the correct macOS binary for your CPU (Intel or Apple Silicon) based on this formula.&lt;/p&gt;
    &lt;head&gt;Debian / Ubuntu&lt;/head&gt;
    &lt;code&gt;curl -LO https://github.com/Esubaalew/run/releases/latest/download/run-deb.sha256
DEB_FILE=$(awk '{print $2}' run-deb.sha256)
curl -LO "https://github.com/Esubaalew/run/releases/latest/download/${DEB_FILE}"
sha256sum --check run-deb.sha256
sudo apt install "./${DEB_FILE}"&lt;/code&gt;
    &lt;head&gt;Windows (Scoop)&lt;/head&gt;
    &lt;code&gt;scoop install https://github.com/Esubaalew/run/releases/latest/download/run-scoop.json&lt;/code&gt;
    &lt;head&gt;Install script (macOS / Linux)&lt;/head&gt;
    &lt;code&gt;curl -fsSLO https://raw.githubusercontent.com/Esubaalew/run/master/scripts/install.sh
chmod +x install.sh
./install.sh --add-path           # optional: append ~/.local/bin to PATH&lt;/code&gt;
    &lt;p&gt;Pass &lt;code&gt;--version v0.2.0&lt;/code&gt;, &lt;code&gt;--prefix /usr/local/bin&lt;/code&gt;, or &lt;code&gt;--repo yourname/run&lt;/code&gt; to customize the install.&lt;/p&gt;
    &lt;head&gt;Download the archive directly&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Grab the &lt;code&gt;tar.gz&lt;/code&gt;(macOS/Linux) or&lt;code&gt;zip&lt;/code&gt;(Windows) from the latest release.&lt;/item&gt;
      &lt;item&gt;Extract it and copy &lt;code&gt;run&lt;/code&gt;/&lt;code&gt;run.exe&lt;/code&gt;onto your&lt;code&gt;PATH&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Optionally execute the bundled &lt;code&gt;install.sh&lt;/code&gt;to handle the copy for you.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Build from source&lt;/head&gt;
    &lt;code&gt;cargo install run-kit&lt;/code&gt;
    &lt;p&gt;The project targets Rust 1.70+. Installing from crates.io gives you the same &lt;code&gt;run&lt;/code&gt; binary that CI publishes; use &lt;code&gt;--force&lt;/code&gt; when upgrading to a newer release.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;run&lt;/code&gt; shells out to real toolchains under the hood. Each &lt;code&gt;LanguageEngine&lt;/code&gt; implements a small trait that knows how to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detect whether the toolchain is available (e.g. &lt;code&gt;python3&lt;/code&gt;,&lt;code&gt;go&lt;/code&gt;,&lt;code&gt;rustc&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Prepare a temporary workspace (compilation for compiled languages, transient scripts for interpreters).&lt;/item&gt;
      &lt;item&gt;Execute snippets, files, or stdin streams and surface stdout/stderr consistently.&lt;/item&gt;
      &lt;item&gt;Manage session state for the interactive REPL (persistent modules, stateful scripts, or regenerated translation units).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This architecture keeps the core lightweight while making it easy to add new runtimes or swap implementations.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;run&lt;/code&gt; supports 25+ languages:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Languages &amp;amp; aliases&lt;/cell&gt;
        &lt;cell role="head"&gt;Toolchain expectations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scripting &amp;amp; shells&lt;/cell&gt;
        &lt;cell&gt;Bash (&lt;code&gt;bash&lt;/code&gt;), Python (&lt;code&gt;py&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt;), Ruby (&lt;code&gt;rb&lt;/code&gt;, &lt;code&gt;ruby&lt;/code&gt;), PHP (&lt;code&gt;php&lt;/code&gt;), Perl (&lt;code&gt;perl&lt;/code&gt;), Lua (&lt;code&gt;lua&lt;/code&gt;), R (&lt;code&gt;r&lt;/code&gt;), Elixir (&lt;code&gt;ex&lt;/code&gt;, &lt;code&gt;elixir&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Matching interpreter on &lt;code&gt;PATH&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Web &amp;amp; typed scripting&lt;/cell&gt;
        &lt;cell&gt;JavaScript (&lt;code&gt;js&lt;/code&gt;, &lt;code&gt;node&lt;/code&gt;), TypeScript (&lt;code&gt;ts&lt;/code&gt;, &lt;code&gt;deno&lt;/code&gt;), Dart (&lt;code&gt;dart&lt;/code&gt;), Swift (&lt;code&gt;swift&lt;/code&gt;), Kotlin (&lt;code&gt;kt&lt;/code&gt;, &lt;code&gt;kotlin&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;node&lt;/code&gt;, &lt;code&gt;deno&lt;/code&gt;, &lt;code&gt;dart&lt;/code&gt;, &lt;code&gt;swift&lt;/code&gt;, &lt;code&gt;kotlinc&lt;/code&gt; + JRE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Systems &amp;amp; compiled&lt;/cell&gt;
        &lt;cell&gt;C (&lt;code&gt;c&lt;/code&gt;), C++ (&lt;code&gt;cpp&lt;/code&gt;, &lt;code&gt;cxx&lt;/code&gt;), Rust (&lt;code&gt;rs&lt;/code&gt;, &lt;code&gt;rust&lt;/code&gt;), Go (&lt;code&gt;go&lt;/code&gt;), Zig (&lt;code&gt;zig&lt;/code&gt;), Nim (&lt;code&gt;nim&lt;/code&gt;), Haskell (&lt;code&gt;hs&lt;/code&gt;, &lt;code&gt;haskell&lt;/code&gt;), Crystal (&lt;code&gt;cr&lt;/code&gt;, &lt;code&gt;crystal&lt;/code&gt;), C# (&lt;code&gt;cs&lt;/code&gt;, &lt;code&gt;csharp&lt;/code&gt;), Java (&lt;code&gt;java&lt;/code&gt;), Julia (&lt;code&gt;jl&lt;/code&gt;, &lt;code&gt;julia&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Respective compiler / toolchain&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Real programs live under the &lt;code&gt;examples/&lt;/code&gt; tree—each language has a &lt;code&gt;hello&lt;/code&gt; and a &lt;code&gt;progress&lt;/code&gt; scenario. The headers document expected output so you can diff your toolchain.&lt;/p&gt;
    &lt;code&gt;run examples/rust/hello.rs
run examples/typescript/progress.ts
run examples/python/counter.py&lt;/code&gt;
    &lt;p&gt;Being inside REPL we can use the ff commands&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:help&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List available meta commands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:languages&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show detected engines and status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;:lang &amp;lt;id&amp;gt;&lt;/code&gt; or &lt;code&gt;:&amp;lt;alias&amp;gt;&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Switch the active language (&lt;code&gt;:py&lt;/code&gt;, &lt;code&gt;:go&lt;/code&gt;, …)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:detect on/off/toggle&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Control snippet language auto-detection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:load path/to/file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execute a file inside the current session&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:reset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Clear the accumulated session state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;:exit&lt;/code&gt; / &lt;code&gt;:quit&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Leave the REPL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Apache 2.0. See LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Built with ❤️ in Rust. If &lt;code&gt;run&lt;/code&gt; unblocks your workflow, star the repo and share it with other polyglot hackers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Esubaalew/run"/><published>2025-10-04T18:34:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45475529</id><title>ProofOfThought: LLM-based reasoning using Z3 theorem proving</title><updated>2025-10-04T20:33:40.058937+00:00</updated><content>&lt;doc fingerprint="880be56c94434dea"&gt;
  &lt;main&gt;
    &lt;p&gt;LLM-based reasoning using Z3 theorem proving.&lt;/p&gt;
    &lt;code&gt;from openai import OpenAI
from z3dsl.reasoning import ProofOfThought

client = OpenAI(api_key="...")
pot = ProofOfThought(llm_client=client)

result = pot.query("Would Nancy Pelosi publicly denounce abortion?")
print(result.answer)  # False&lt;/code&gt;
    &lt;code&gt;from z3dsl.reasoning import EvaluationPipeline

evaluator = EvaluationPipeline(pot, output_dir="results/")
result = evaluator.evaluate(
    dataset="strategyqa_train.json",
    max_samples=10
)
print(f"Accuracy: {result.metrics.accuracy:.2%}")&lt;/code&gt;
    &lt;code&gt;pip install z3-solver openai scikit-learn numpy&lt;/code&gt;
    &lt;p&gt;The system has two layers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;High-level API (&lt;code&gt;z3dsl.reasoning&lt;/code&gt;) - Simple Python interface for reasoning tasks&lt;/item&gt;
      &lt;item&gt;Low-level DSL (&lt;code&gt;z3dsl&lt;/code&gt;) - JSON-based Z3 theorem prover interface&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most users should use the high-level API.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;examples/&lt;/code&gt; directory for complete examples including Azure OpenAI support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DebarghaG/proofofthought"/><published>2025-10-04T18:34:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45475808</id><title>Blog Feeds</title><updated>2025-10-04T20:33:39.196061+00:00</updated><content>&lt;doc fingerprint="769f1fb99e6596b5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tired of social media?&lt;/p&gt;
    &lt;p&gt;Keep doom scrolling through addicting feeds?&lt;/p&gt;
    &lt;p&gt;Miss the days when the web was just about connecting with people and their thoughts or ideas?&lt;/p&gt;
    &lt;p&gt;We believe there's an answer to that problem, and it's called&lt;/p&gt;
    &lt;p&gt;Starting a blog is actually a lot simpler than what you're probably thinking. This doesn't have to be some well polished highly viewed monetization machine, or even something professional or formal. It's just a simple website where you can casually talk about whatever you want to talk about! It can be long, short, a list of small things, or just a quote. It should be how you talk with other people in your own life, or how you communicate with the outside world. It should be you on a page. Here's a few places you can make a blog that are RSS enabled:&lt;/p&gt;
    &lt;p&gt;These are great if you are not quite a technical person and need everything to be simple and easy to use.&lt;/p&gt;
    &lt;p&gt;RSS is actually already familiar to you if you have ever subscribed to a newsletter. You put your email into someoneâs website, and when they have updates, they send you emails to your inbox so you can stay in the loop. In the case of RSS, you have a dedicated app, called an RSS reader usually, and you can put in someoneâs website into the app to subscribe. When they make a new post, just open your news reader app, and their posts will be retrieved and ready to read. Some reader apps even let you make folders and tags to organize blogs you are subscribed to, similar to how an email app lets you make folders to sort mail. Would highly recommend trying a few of the apps or services and seeing which works best!&lt;/p&gt;
    &lt;p&gt;This takes us to our final point: Feeds. You can probably get away with just the first two items and then sharing it with people you already know, but what about meeting or talking to people you don't know? That's where Feeds come in. The idea is to create another page on your blog that has all the RSS feeds you're subscribed to. By keeping this public and always up to date, someone can visit your page, find someone new and follow them. Perhaps that person also has a feeds page, and the cycle continues until there is a natural and organic network of people all sharing with each other. So if you have a blog, consider making a feeds page and sharing it! If your RSS reader supports OPML file exports and imports, perhaps you can share that file as well to make it easier to share your feeds.&lt;/p&gt;
    &lt;p&gt;Here's an example Feeds Page which should help get the idea across!&lt;/p&gt;
    &lt;p&gt;The best part about blog feeds? It's just an idea. There's no central authority. There's no platform. No massive tech giant trying to take your data. It's just you, basic web standards, and the people you care about.&lt;/p&gt;
    &lt;p&gt;Made by Steve&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blogfeeds.net"/><published>2025-10-04T19:08:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45476071</id><title>Why NetNewsWire Is Not a Web App</title><updated>2025-10-04T20:33:38.934014+00:00</updated><content>&lt;doc fingerprint="f3c8d9182a245dd9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why NetNewsWire Is Not a Web App&lt;/head&gt;
    &lt;p&gt;Tim Bray writes, on Mastodon, I think correctly:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The canceling of ICEBlock is more evidence, were any needed, that the Web is the platform of the future, the only platform without a controlling vendor. Anything controversial should be available through a pure browser interface.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is not the first time I’ve had reason to think about this — I think about issues of tech freedom every day, and I still bristle, after all these years (now more than ever), at having to publish NetNewsWire for iOS through the App Store. (The Mac version has no such requirement — it’s available via the website, and I have no plans to ever offer it via the Mac App Store.)&lt;/p&gt;
    &lt;p&gt;But what if I wanted to do a web app, in addition to or instead of a native app?&lt;/p&gt;
    &lt;p&gt;I can picture a future, as I bet you can, where RSS readers aren’t allowed on any app store, and we’re essentially required to use billionaire-owned social media and platform-owned news apps.&lt;/p&gt;
    &lt;p&gt;But there are issues with making NetNewsWire a web app.&lt;/p&gt;
    &lt;head rend="h3"&gt;Money&lt;/head&gt;
    &lt;p&gt;I explain in this post that NetNewsWire has almost no expenses at all. The biggest expense is my Apple developer membership, and I pay just a little bit to host some websites. It adds up to a couple hundred bucks a year.&lt;/p&gt;
    &lt;p&gt;If it were a web app instead, I could drop the developer membership, but I’d have to pay way more money for web and database hosting. Probably need a CDN too, and who knows what else. (I don’t have recent web app experience, so I don’t even know what my requirements would be, but I’m sure they’d cost substantially more than a couple hundred bucks a year.)&lt;/p&gt;
    &lt;p&gt;I could charge for NetNewsWire, but that would go against my political goal of making sure there’s a good and free RSS reader available to everyone.&lt;/p&gt;
    &lt;p&gt;I could take donations instead, but that’s never going to add up to enough to cover the costs.&lt;/p&gt;
    &lt;p&gt;And in either case I’d have to create a way to take money and start up some kind of entity and then do bookkeeping and report money things to the right places — all stuff I don’t have to waste time on right now. I can just work on the app.&lt;/p&gt;
    &lt;p&gt;Alternately I could create a web app that people would self-host — but there’s no way I could handle the constant support requests for installation issues. There are free self-hosted RSS readers already anyway, and NetNewsWire would be just another one. This also wouldn’t further my goal of making a free RSS reader available to everyone, since only people with the skills and willingness to self-host would do it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Protecting Users&lt;/head&gt;
    &lt;p&gt;Second issue. Right now, if law enforcement comes to me and demands I turn over a given user’s subscriptions list, I can’t. Literally can’t. I don’t have an encrypted version, even — I have nothing at all. The list lives on their machine (iOS or macOS). If they use a syncing system, it lives there too — but I don’t run a syncing system. I don’t have that info and can’t get it.&lt;/p&gt;
    &lt;p&gt;If that happened, I’d have to pay a lawyer to see if the demand is legit and possibly help me fight it. That’s yet more money and time.&lt;/p&gt;
    &lt;p&gt;(Could I encrypt the subscription lists on the server? Yes, but the server would have to be able to decrypt it, or else the app couldn’t possibly work. Which means I could decrypt the lists and turn them over.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Another type of freedom&lt;/head&gt;
    &lt;p&gt;Not an issue, exactly, but a thing.&lt;/p&gt;
    &lt;p&gt;I was 12 years old when I got my first computer, an Apple II Plus, and I’ve never stopped loving the freedom of having my own computer and being able to run whatever the hell I want to.&lt;/p&gt;
    &lt;p&gt;My computer is not a terminal. It’s a world I get to control, and I can use — and, especially, make — whatever I want. I’m not stuck using just what’s provided to me on some other machines elsewhere: I’m not dialing into a mainframe or doing the modern equivalent of using only websites that other people control.&lt;/p&gt;
    &lt;p&gt;A world where everything is on the web and nothing is on the machines that we own is a sad world where we’ve lost a core freedom.&lt;/p&gt;
    &lt;p&gt;I want to preserve that freedom. I like making apps that show the value of that freedom.&lt;/p&gt;
    &lt;p&gt;What I want to see happen is for Apple to allow iPhone and iPad users to load — not sideload, a term I detest, because it assumes Apple’s side of things — whatever apps they want to. Because those devices are computers.&lt;/p&gt;
    &lt;p&gt;I get it. It’s not looking good. And even with the much greater freedom for Mac apps there is still always the possibility of being shut down by Apple (by revoking developer memberships, refusing to notarize, or other technical means).&lt;/p&gt;
    &lt;p&gt;Still, though, I keep at it, because this freedom matters.&lt;/p&gt;
    &lt;p&gt;But, again…&lt;/p&gt;
    &lt;p&gt;Apple keeps doing things that make us all feel sick. Removing ICEBlock is just the latest and it won’t be the last. So I am sympathetic to the idea of making web apps, and my brain goes there more often. And if I could solve the problems of money and of protecting users, I’d be way more inclined.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://inessential.com/2025/10/04/why-netnewswire-is-not-web-app.html"/><published>2025-10-04T19:40:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45476273</id><title>The UK Is Still Trying to Backdoor Encryption for Apple Users</title><updated>2025-10-04T20:33:38.746786+00:00</updated><content>&lt;doc fingerprint="8f988c715aa8d158"&gt;
  &lt;main&gt;
    &lt;p&gt;The Financial Times reports that the U.K. is once again demanding that Apple create a backdoor into its encrypted backup services. The only change since the last time they demanded this is that the order is allegedly limited to only apply to British users. That doesn’t make it any better.&lt;/p&gt;
    &lt;p&gt;The demand uses a power called a “Technical Capability Notice” (TCN) in the U.K.’s Investigatory Powers Act. At the time of its signing we noted this law would likely be used to demand Apple spy on its users. &lt;/p&gt;
    &lt;p&gt;After the U.K. government first issued the TCN in January, Apple was forced to either create a backdoor or block its Advanced Data Protection feature—which turns on end-to-end encryption for iCloud—for all U.K. users. The company decided to remove the feature in the U.K. instead of creating the backdoor.&lt;/p&gt;
    &lt;p&gt;The initial order from January targeted the data of all Apple users. In August, the US claimed the U.K. withdrew the demand, but Apple did not re-enable Advanced Data Protection. The new order provides insight into why: the U.K. was just rewriting it to only apply to British users. &lt;/p&gt;
    &lt;p&gt;This is still an unsettling overreach that makes U.K. users less safe and less free. As we’ve said time and time again, any backdoor built for the government puts everyone at greater risk of hacking, identity theft, and fraud. It sets a dangerous precedent to demand similar data from other companies, and provides a runway for other authoritarian governments to issue comparable orders. The news of continued server-side access to users' data comes just days after the UK government announced an intrusive mandatory digital ID scheme, framed as a measure against illegal migration.&lt;/p&gt;
    &lt;p&gt;A tribunal hearing was initially set to take place in January 2026, though it’s currently unclear if that will proceed or if the new order changes the legal process. Apple must continue to refuse these types of backdoors. Breaking end-to-end encryption for one country breaks it for everyone. These repeated attempts to weaken encryption violates fundamental human rights and destroys our right to private spaces.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/deeplinks/2025/10/uk-still-trying-backdoor-encryption-apple-users"/><published>2025-10-04T20:07:11+00:00</published></entry></feed>