<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-03T15:09:32.616081+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45451577</id><title>Playball – Watch MLB games from a terminal</title><updated>2025-10-03T15:09:45.185478+00:00</updated><content>&lt;doc fingerprint="607031164bc1d92f"&gt;
  &lt;main&gt;
    &lt;p&gt;Watch MLB games from the comfort of your own terminal&lt;/p&gt;
    &lt;p&gt;MLB Gameday and MLB.tv are great, but sometimes you want to keep an eye on a game a bit more discreetly. &lt;code&gt;playball&lt;/code&gt; puts the game in a terminal window.&lt;/p&gt;
    &lt;p&gt;Just want to try it out?&lt;/p&gt;
    &lt;code&gt;$ npx playball
&lt;/code&gt;
    &lt;p&gt;Ready for the big leagues? Install the package globally&lt;/p&gt;
    &lt;code&gt;$ npm install -g playball
&lt;/code&gt;
    &lt;p&gt;Then run it&lt;/p&gt;
    &lt;code&gt;$ playball
&lt;/code&gt;
    &lt;code&gt;$ docker build -t playball .
$ docker run -it --rm --name playball playball:latest
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;q&lt;/cell&gt;
        &lt;cell&gt;quit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;c&lt;/cell&gt;
        &lt;cell&gt;go to schedule view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;s&lt;/cell&gt;
        &lt;cell&gt;go to standings view&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;↓/j, ↑/k, ←/h, →/l&lt;/cell&gt;
        &lt;cell&gt;change highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;enter&lt;/cell&gt;
        &lt;cell&gt;view highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;show previous day's schedule/results&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;n&lt;/cell&gt;
        &lt;cell&gt;show next day's schedule&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;return to today's schedule&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;↓/j, ↑/k&lt;/cell&gt;
        &lt;cell&gt;scroll list of all plays&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Playball can be configured using the &lt;code&gt;config&lt;/code&gt; subcommand. To list the current configuration values run the subcommand with no additional arguments:&lt;/p&gt;
    &lt;code&gt;playball config&lt;/code&gt;
    &lt;p&gt;You should see output similar to:&lt;/p&gt;
    &lt;code&gt;color.ball = green
color.favorite-star = yellow
color.in-play-no-out = blue
color.in-play-out = white
color.in-play-runs-bg = white
color.in-play-runs-fg = black
color.on-base = yellow
color.other-event = white
color.out = red
color.strike = red
color.strike-out = red
color.walk = green
favorites = 
&lt;/code&gt;
    &lt;p&gt;To get the value of a single setting pass the key as an additional argument:&lt;/p&gt;
    &lt;code&gt;playball config color.strike&lt;/code&gt;
    &lt;p&gt;To change a setting pass the key and value as arguments:&lt;/p&gt;
    &lt;code&gt;playball config color.strike blue&lt;/code&gt;
    &lt;p&gt;To revert a setting to its default value provide the key and the &lt;code&gt;--unset&lt;/code&gt; flag:&lt;/p&gt;
    &lt;code&gt;playball config color.strike --unset&lt;/code&gt;
    &lt;p&gt;This table summarizes the available settings:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
        &lt;cell role="head"&gt;default&lt;/cell&gt;
        &lt;cell role="head"&gt;allowed values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.ball&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing balls in top row of game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;One of the following: &lt;code&gt;black&lt;/code&gt;, &lt;code&gt;red&lt;/code&gt;, &lt;code&gt;green&lt;/code&gt;, &lt;code&gt;yellow&lt;/code&gt;, &lt;code&gt;blue&lt;/code&gt;, &lt;code&gt;magenta&lt;/code&gt;, &lt;code&gt;cyan&lt;/code&gt;, &lt;code&gt;white&lt;/code&gt;, &lt;code&gt;grey&lt;/code&gt;. Any of those colors may be prefixed by &lt;code&gt;bright-&lt;/code&gt; or &lt;code&gt;light-&lt;/code&gt; (for example &lt;code&gt;bright-green&lt;/code&gt;). The exact color used will depend on your terminal settings. The value &lt;code&gt;default&lt;/code&gt; may be used to specify the default text color for your terminal. Finally hex colors (e.g &lt;code&gt;#FFA500&lt;/code&gt;) can be specified. If your terminal does not support true color, the closest supported color may be used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.favorite-star&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of star indiciating favorite team in schedule and standing views&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-no-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and no out was made (single, double, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;blue&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and an out was made (flyout, fielder's choice, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-bg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Background color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-fg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Foreground color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;black&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.on-base&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of diamonds representing runners on base in top row of game view&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.other-event&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of other events (mound visit, injury delay, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing outs in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing strikes in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a strike (strike out) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.walk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a ball (walk, hit by pitch) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;favorites&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Teams to highlight in schedule and standings views&lt;/cell&gt;
        &lt;cell&gt;Any one of the following: &lt;code&gt;ATL&lt;/code&gt;, &lt;code&gt;AZ&lt;/code&gt;, &lt;code&gt;BAL&lt;/code&gt;, &lt;code&gt;BOS&lt;/code&gt;, &lt;code&gt;CHC&lt;/code&gt;, &lt;code&gt;CIN&lt;/code&gt;, &lt;code&gt;CLE&lt;/code&gt;, &lt;code&gt;COL&lt;/code&gt;, &lt;code&gt;CWS&lt;/code&gt;, &lt;code&gt;DET&lt;/code&gt;, &lt;code&gt;HOU&lt;/code&gt;, &lt;code&gt;KC&lt;/code&gt;, &lt;code&gt;LAA&lt;/code&gt;, &lt;code&gt;LAD&lt;/code&gt;, &lt;code&gt;MIA&lt;/code&gt;, &lt;code&gt;MIL&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;NYM&lt;/code&gt;, &lt;code&gt;NYY&lt;/code&gt;, &lt;code&gt;OAK&lt;/code&gt;, &lt;code&gt;PHI&lt;/code&gt;, &lt;code&gt;PIT&lt;/code&gt;, &lt;code&gt;SD&lt;/code&gt;, &lt;code&gt;SEA&lt;/code&gt;, &lt;code&gt;SF&lt;/code&gt;, &lt;code&gt;STL&lt;/code&gt;, &lt;code&gt;TB&lt;/code&gt;, &lt;code&gt;TEX&lt;/code&gt;, &lt;code&gt;TOR&lt;/code&gt;, &lt;code&gt;WSH&lt;/code&gt;. Or a comma-separated list of multiple (e.g. &lt;code&gt;SEA,MIL&lt;/code&gt;).&lt;p&gt;Note: in some terminals the list must be quoted:&lt;/p&gt;&lt;code&gt;playball config favorites "SEA,MIL"&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;git clone https://github.com/paaatrick/playball.git
cd playball
npm install
npm start
&lt;/code&gt;
    &lt;p&gt;Contributions are welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/paaatrick/playball"/><published>2025-10-02T16:09:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45452261</id><title>Why I chose Lua for this blog</title><updated>2025-10-03T15:09:44.567818+00:00</updated><content>&lt;doc fingerprint="7f443511072312dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I chose Lua for this blog&lt;/head&gt;
    &lt;p&gt;This blog used to run using with a stack based on Racket using Pollen and lots of hacks on top of it. At some point I realised that my setup was working against me. The moving parts and workflow I created added too much friction to keep my blog active. That happened mostly because it was a static generator trying to behave as if it was dynamic website with an editing interface. That can be done really well â cue Grav CMS â but that was not the case for me.&lt;/p&gt;
    &lt;p&gt;Once I decided to rewrite this blog as a simpler system, I faced the dilema of what stack to choose. The obvious choice for me would be Javascript, it is the language I use more often and one that I am quite confortable with. Still, I don't think it is a wise choice for the kind of blog I want to maintain.&lt;/p&gt;
    &lt;p&gt;Talking to some friends recently, I noticed that many people I know that have implemented their own blogging systems face many challenges keeping them running over many years. Not because it is hard to keep software running, but because their stack of choice is moving faster than their codebase.&lt;/p&gt;
    &lt;p&gt;This problem is specially prevalent in the Javascript world. It is almost a crime that JS as understood by the browser is this beautiful language with extreme retrocompatibility, while JS as understood and used by the current tooling and workflows is this mess moving at lightspeed. Let me unpack that for a bit.&lt;/p&gt;
    &lt;p&gt;You can open a web page from 1995 on your browser of choice and it will just work because browser vendors try really hard to make sure they don't break the web.&lt;/p&gt;
    &lt;p&gt;Developers who built the whole ecosystem of NodeJS, NPM, and all those libraries and frameworks don't share the same ethos. They all make a big case of semantic versioning and thus being able to handle breaking changes, but they have breaking changes all the time. You'd be hardpressed to actually run some JS code from ten years ago based on NodeJS and NPM. There is a big chance that dependencies might be gone, broken, or it might be incompatible with the current NodeJS.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I know this sounds like FUD, and that for many many projects, maybe even most projects, that will not be the case. But I heard from many people that keeping their blogging systems up to date requires a lot more work than they would like to do and if they don't, then they're screwed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That is also true about other languages even though many of them move at a slower speed. A friend recently complained about a blogging system he implemented that requires Ruby 2.0 and that keeping that running sucks.&lt;/p&gt;
    &lt;p&gt;I want a simpler blogging system; one that requires minimal changes over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Now we talk about Lua&lt;/head&gt;
    &lt;p&gt;Lua is a wonderful and nimble language that is often misunderstood.&lt;/p&gt;
    &lt;p&gt;One characteristic that I love about it, is that is evolves very slowly. Lua 5.1 was introduced in 2006, Lua 5.4 which is the current version initial release was in 2020. Yes, there are point released in between, but you can see how much slower it moves when compared to JS.&lt;/p&gt;
    &lt;p&gt;The differences between Lua 5.1 and Lua 5.4 are minimal when compared with how much other languages changed in the same time period.&lt;/p&gt;
    &lt;p&gt;Lua only requires a C89 compiler to bootstrap itself. It is very easy to make Lua work and even easier to make it interface with something.&lt;/p&gt;
    &lt;p&gt;JS is a lot larger than Lua, there is more to understand and more to remember. My blog needs are very simple and Lua can handle them with ease.&lt;/p&gt;
    &lt;head rend="h2"&gt;How this blog works&lt;/head&gt;
    &lt;p&gt;This is an old-school blog. I uses cgi-bin â aka Comon Gateway Interface â scripts to run it. It is a dynamic website with a SQLite database holding its data. When you open a page, it fetches the data from a database and assembles a HTML to send to the browser using Mustache templates.&lt;/p&gt;
    &lt;p&gt;One process per request. Like the old days.&lt;/p&gt;
    &lt;p&gt;You might argue that if I went with NodeJS, I'd be able to serve more requests using fewer resources. That is true. I don't need to serve that many requests though. My peak access was a couple years ago with 50k visitors on a week, even my old Racket blog could handle that fine. The Lua one should handle it too; and if it breaks it breaks. I'm a flawed human being, my code can be flawed too, we're in this together, holding hands.&lt;/p&gt;
    &lt;p&gt;Your blog is your place to experiment and program how you want it. You can drop the JS fatigue, you can drop your fancy Haskell types, you can just do whatever you find fun and keep going (and that includes JS and Haskell if that's your thing. You do you).&lt;/p&gt;
    &lt;p&gt;Cause I'm using Lua, I don't have as many libraries and frameworks available to me as JS people have, but I still have quite a large collection via Luarocks. I try not to add many dependencies to my blog. At the moment there are about ten and that is mostly because Lua is a batteries-not-included language so you start from a minimal core and build things up to suit your needs.&lt;/p&gt;
    &lt;p&gt;For a lot of things I went with the questionable choice of implementing things myself. I got my own little CGI library. It is 200 lines long and does the bare minimum to make this blog work. I got my own little libraries for many things. Micropub and IndieAuth were all implemented by hand.&lt;/p&gt;
    &lt;p&gt;At the moment I'm &lt;del&gt;despairing&lt;/del&gt;&lt;del&gt;frustrated&lt;/del&gt; having a lot of fun implementing WebMentions. Doing the Microformats2 &lt;del&gt;exorcism&lt;/del&gt; extraction on my own is teaching me a lot of things.&lt;/p&gt;
    &lt;p&gt;What I want to say is that by choosing a small language that moves very slowly and very few dependencies, I can keep all of my blogging system in my head. I can make sure it will run without too much change for the next ten or twenty years.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lua is a lego set, a toolkit, it adapts to you and your needs. I don't need to keep chasing the new shiny or the latest framework du jour. I can focus on making the features I want and actually understanding how they work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Instead of installing a single dependency in another language and it pulling a hundred of other small dependencies all of which were transpiled into something the engine understands to the point that understanding how all the pieces work and fit together takes more time than to learn a new language, I decided to keep things simple.&lt;/p&gt;
    &lt;p&gt;I got 29 Luarocks installed here and that is for all my Lua projects in this machine. That is my blog, my game development, my own work scripts for my day job. Not even half of those are for my blog.&lt;/p&gt;
    &lt;p&gt;I often see wisdom in websites such as Hacker News and Lobsters around the idea of "choosing boring" because it is proven, safe, easier to maintain. I think that boring is not necessarily applicable to my case. I don't find Lua boring at all, but all that those blog posts talk about that kind of mindset are all applicable to my own choices here.&lt;/p&gt;
    &lt;p&gt;Next time you're building your own blogging software, consider for a bit for how long do you want to maintain it. I first started blogging on macOS 8 in 2001. I choose badly many times and in the end couldn't keep my content moving forward in time with me as softwares I used or created became impossible to run. The last two changes: from JS to Racket and from Racket to Lua have been a lot safer and I managed to carry all my content forward into increasingly simpler setups and workflows.&lt;/p&gt;
    &lt;p&gt;My blogging system is not becoming more complex over the years, it is becoming smaller, because with each change I select a stack that is more nimble and smaller than the one I had before. I don't think I can go smaller than Lua though.&lt;/p&gt;
    &lt;p&gt;By small I mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A language I can fully understand and keep on my head.&lt;/item&gt;
      &lt;item&gt;A language that I know how to build the engine and can do it if needed.&lt;/item&gt;
      &lt;item&gt;An engine that requires very few resources and is easy to interface with third-party libraries in native code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I chose Lua because of all that, and I'm happy with it and hope this engine will see me through the next ten or so years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andregarzia.com/2025/03/why-i-choose-lua-for-this-blog.html"/><published>2025-10-02T16:58:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45453222</id><title>Babel is why I keep blogging with Emacs</title><updated>2025-10-03T15:09:44.099939+00:00</updated><content>&lt;doc fingerprint="26d8bd04cf2550b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Keep Blogging With Emacs&lt;/head&gt;
    &lt;p&gt;Every time I look at someone’s simple static site generation setup for their blog, I feel a pang of envy. I’m sure I could make a decent blogging engine in 2,000 lines of code, and it would be something I’d understand, be proud over, able to extend, and willing to share with others.&lt;/p&gt;
    &lt;p&gt; Instead, I write these articles in Org mode, and use mostly the standard Org publishing functions to export them to html. This is sometimes brittle, but most annoyingly, I don’t understand it. I have been asked for details on how my publishing flow works, but the truth is I have no idea what happens when I run the &lt;code&gt;org-publish-current-file&lt;/code&gt; command.
&lt;/p&gt;
    &lt;p&gt; I could find out by tracing the evaluation of the Lisp code that runs on export, but I won’t, because just the html exporting code (&lt;code&gt;ox-html.el&lt;/code&gt;) is 5,000
lines of complexity. The general exporting framework (&lt;code&gt;ox-publish.el&lt;/code&gt; and
&lt;code&gt;ox.el&lt;/code&gt;) is 8,000 lines. The framework depends on Org parsing code
(&lt;code&gt;org-element.el&lt;/code&gt;) which is at least another 9,000 lines. This is over 20,000
lines of complexity I’d need to contend with.
&lt;/p&gt;
    &lt;p&gt;It might seem like a no-brainer to just write that 2,000 line custom static generator and use that instead.&lt;/p&gt;
    &lt;p&gt;Except one thing: Babel.&lt;/p&gt;
    &lt;p&gt;Any lightweight markup format (like Markdown or ReStructuredText or whatever) allows for embedding code blocks, but Org, through Babel, can run that code on export, and then display the output in the published document, even when the output is a table or an image. It supports sessions that lets code reuse definitions from earlier code blocks. It allows for injecting variables from the markup into the code, and vice versa. As a bonus, Org doesn’t require a JavaScript syntax highlighter, because it generates inline styles in the source code.&lt;/p&gt;
    &lt;p&gt;It does this for a large number of languages, although I mainly use it with R for drawing plots. Being able to do this is incredibly convenient, because it makes it trivial to draft data, illustrations, and text at the same time, adjusting both until the article coheres. Having tried it, I cannot see myself living without it.&lt;/p&gt;
    &lt;p&gt;A simple 2,000 line blogging engine would be a fun weekend project. Mirroring the features of Babel I use would turn it into a multi-month endeavour for someone with limited time such as myself. Not going to happen, and I will continue to beat myself up for overcomplicating my publishing workflow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/why-stick-to-emacs-blog"/><published>2025-10-02T18:06:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45453936</id><title>Litestream v0.5.0</title><updated>2025-10-03T15:09:43.879434+00:00</updated><content>&lt;doc fingerprint="e75e0cc710643f1b"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâm Ben Johnson, and I work on Litestream at Fly.io. Litestream makes it easy to build SQLite-backed full-stack applications with resilience to server failure. Itâs open source, runs anywhere, and itâs easy to get started.&lt;/p&gt;
    &lt;p&gt;Litestream is the missing backup/restore system for SQLite. It runs as a sidecar process in the background, alongside unmodified SQLite applications, intercepting WAL checkpoints and streaming them to object storage in real time. Your application doesn’t even know it’s there. But if your server crashes, Litestream lets you quickly restore the database to your new hardware.&lt;/p&gt;
    &lt;p&gt;The result: you can safely build whole full-stack applications on top of SQLite.&lt;/p&gt;
    &lt;p&gt;A few months back, we announced plans for a major update to Litestream. I’m psyched to announce that the first batch of those changes are now “shipping”. Litestream is faster and now supports efficient point-in-time recovery (PITR).&lt;/p&gt;
    &lt;p&gt;I’m going to take a beat to recap Litestream and how we got here, then talk about how these changes work and what you can expect to see with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Litestream to LiteFS to Litestream&lt;/head&gt;
    &lt;p&gt;Litestream is one of two big SQLite things I’ve built. The other one, originally intended as a sort of sequel to Litestream, is LiteFS.&lt;/p&gt;
    &lt;p&gt;Boiled down to a sentence: LiteFS uses a FUSE filesystem to crawl further up into SQLite’s innards, using that access to perform live replication, for unmodified SQLite-backed apps.&lt;/p&gt;
    &lt;p&gt;The big deal about LiteFS for us is that it lets you do the multiregion primary/read-replica deployment people love Postgres for: reads are fast everywhere, and writes are sane and predictable. We were excited to make this possible for SQLite, too.&lt;/p&gt;
    &lt;p&gt;But the market has spoken! Users prefer Litestream. And honestly, we get it: Litestream is easier to run and to reason about. So we’ve shifted our focus back to it. First order of business: take what we learned building LiteFS and stick as much of it as we can back into Litestream.&lt;/p&gt;
    &lt;head rend="h2"&gt;The LTX File Format&lt;/head&gt;
    &lt;p&gt;Consider this basic SQL table:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE sandwiches (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    description TEXT NOT NULL,
    star_rating INTEGER, 
    reviewer_id INTEGER NOT NULL
);
&lt;/code&gt;
    &lt;p&gt;In our hypothetical, this table backs a wildly popular sandwich-reviewing app that we keep trying to get someone to write. People eat a lot of sandwiches and this table gets a lot of writes. Because it makes my point even better and it’s funny, assume people dither a lot about their sandwich review for the first couple minutes after they leave it. This Quiznos subâ¦ is it â or ââ?&lt;/p&gt;
    &lt;p&gt;Underneath SQLite is a B-tree. Like databases everywhere, SQLite divides storage up into disk-aligned pages, working hard to read as few pages as possible for any task while treating work done within a page as more or less free. SQLite always reads and writes in page-sized chunks.&lt;/p&gt;
    &lt;p&gt;Our &lt;code&gt;sandwiches&lt;/code&gt; table includes a feature that’s really painful for a tool like Litestream that thinks in pages: an automatically updating primary key. That key dictates that every insert into the table hits the rightmost leaf page in the underlying table B-tree. For SQLite itself, that’s no problem. But Litestream has less information to go on: it sees only a feed of whole pages it needs to archive.&lt;/p&gt;
    &lt;p&gt;Worse still, when it comes time to restore the database â something you tend to want to happen quickly â you have to individually apply those small changes, as whole pages. Your app is down, PagerDuty is freaking out, and you’re sitting there watching Litestream reconstruct your Quiznos uncertainty a page (and an S3 fetch) at a time.&lt;/p&gt;
    &lt;p&gt;So, LTX. Let me explain. We needed LiteFS to be transaction-aware. It relies on finer-grained information than just raw dirty pages (that’s why it needs the FUSE filesystem). To ship transactions, rather than pages, we invented a file format we call LTX.&lt;/p&gt;
    &lt;p&gt;LTX was designed as an interchange format for transactions, but for our purposes in Litestream, all we care about is that LTX files represent ordered ranges of pages, and that it supports compaction.&lt;/p&gt;
    &lt;p&gt;Compaction is straightforward. You’ve stored a bunch of LTX files that collect numbered pages. Now you want to to restore a coherent picture of the database. Just replay them newest to oldest, skipping duplicate pages (newer wins), until all changed pages are accounted for.&lt;/p&gt;
    &lt;p&gt;Importantly, LTX isn’t limited to whole database backups. We can use LTX compaction to compress a bunch of LTX files into a single file with no duplicated pages. And Litestream now uses this capability to create a hierarchy of compactions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;at Level 1, we compact all the changes in a 30-second time window&lt;/item&gt;
      &lt;item&gt;at Level 2, all the Level 1 files in a 5-minute window&lt;/item&gt;
      &lt;item&gt;at Level 3, all the Level 2’s over an hour.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Net result: we can restore a SQLite database to any point in time, using only a dozen or so files on average.&lt;/p&gt;
    &lt;p&gt;Litestream performs this compaction itself. It doesn’t rely on SQLite to process the WAL file. Performance is limited only by I/O throughput.&lt;/p&gt;
    &lt;head rend="h2"&gt;No More Generations&lt;/head&gt;
    &lt;p&gt;What people like about Litestream is that it’s just an ordinary Unix program. But like any Unix program, Litestream can crash. It’s not supernatural, so when it’s not running, it’s not seeing database pages change. When it misses changes, it falls out of sync with the database.&lt;/p&gt;
    &lt;p&gt;Lucky for us, that’s easy to detect. When it notices a gap between the database and our running “shadow-WAL” backup, Litestream resynchronizes from scratch.&lt;/p&gt;
    &lt;p&gt;The only time this gets complicated is if you have multiple Litestreams backing up to the same destination. To keep multiple Litestreams from stepping on each other, Litestream divides backups into “generations”, creating a new one any time it resyncs. You can think of generations as Marvel Cinematic Universe parallel dimensions in which your database might be simultaneously living in.&lt;/p&gt;
    &lt;p&gt;Yeah, we didn’t like those movies much either.&lt;/p&gt;
    &lt;p&gt;LTX-backed Litestream does away with the concept entirely. Instead, when we detect a break in WAL file continuity, we re-snapshot with the next LTX file. Now we have a monotonically incrementing transaction ID. We can use it look up database state at any point in time, without searching across generations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Upgrading to Litestream v0.5.0&lt;/head&gt;
    &lt;p&gt;Due to the file format changes, the new version of Litestream can’t restore from old v0.3.x WAL segment files.&lt;/p&gt;
    &lt;p&gt;That’s OK though! The upgrade process is simple: just start using the new version. It’ll leave your old WAL files intact, in case you ever need to revert to the older version.The new LTX files are stored cleanly in an &lt;code&gt;ltx&lt;/code&gt; directory on your replica.&lt;/p&gt;
    &lt;p&gt;The configuration file is fully backwards compatible.&lt;/p&gt;
    &lt;p&gt;There’s one small catch. We added a new constraint. You only get a single replica destination per database. This probably won’t affect you, since it’s how most people use Litestream already. We’ve made it official.&lt;/p&gt;
    &lt;p&gt;The rationale: having a single source of truth simplifies development for us, and makes the tool easier to reason about. Multiple replicas can diverge and are sensitive to network availability. Conflict resolution is brain surgery.&lt;/p&gt;
    &lt;p&gt;Litestream commands still work the same. But you’ll see references to “transaction IDs” (TXID) for LTX files, rather than the &lt;code&gt;generation/index/offset&lt;/code&gt; we used previously with WAL segments.&lt;/p&gt;
    &lt;p&gt;We’ve also changed &lt;code&gt;litestream wal&lt;/code&gt; to &lt;code&gt;litestream ltx&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other Stuff v0.5.0 Does Better&lt;/head&gt;
    &lt;p&gt;We’ve beefed up the underlying LTX file format library. It used to be an LTX file was just a sorted list of pages, all compressed together. Now we compress per-page, and keep an index at the end of the LTX file to pluck individual pages out.&lt;/p&gt;
    &lt;p&gt;You’re not seeing it yet, but we’re excited about this change: we can operate page-granularly even dealing with large LTX files. This allows for more features. A good example: we can build features that query from any point in time, without downloading the whole database.&lt;/p&gt;
    &lt;p&gt;We’ve also gone back through old issues &amp;amp; PRs to improve quality-of-life. CGO is now gone. We’ve settled the age-old contest between &lt;code&gt;mattn/go-sqlite3&lt;/code&gt; and &lt;code&gt;modernc.org/sqlite&lt;/code&gt; in favor of &lt;code&gt;modernc.org&lt;/code&gt;. This is super handy for people with automated build systems that want to run from a MacBook but deploy on an x64 server, since it lets the cross-compiler work.&lt;/p&gt;
    &lt;p&gt;We’ve also added a replica type for NATS JetStream. Users that already have JetStream running can get Litestream going without adding an object storage dependency.&lt;/p&gt;
    &lt;p&gt;And finally, we’ve upgraded all our clients (S3, Google Storage, &amp;amp; Azure Blob Storage) to their latest versions. We’ve also moved our code to support newer S3 APIs.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next?&lt;/head&gt;
    &lt;p&gt;The next major feature we’re building out is a Litestream VFS for read replicas. This will let you instantly spin up a copy of the database and immediately read pages from S3 while the rest of the database is hydrating in the background.&lt;/p&gt;
    &lt;p&gt;We already have a proof of concept working and we’re excited to show it off when it’s ready!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fly.io/blog/litestream-v050-is-here/"/><published>2025-10-02T19:02:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45455882</id><title>The strangest letter of the alphabet: The rise and fall of yogh</title><updated>2025-10-03T15:09:43.622599+00:00</updated><content>&lt;doc fingerprint="b6728524aea13b82"&gt;
  &lt;main&gt;
    &lt;p&gt;English spelling has a reputation. And it’s not a good one.&lt;/p&gt;
    &lt;p&gt;It’s full of silent letters, as in numb, knee, and honour. A given sound can be spelled in multiple ways (farm, laugh, photo), and many letters make multiple sounds (get, gist, mirage).&lt;/p&gt;
    &lt;p&gt;English spelling is so complex that we’ve made mastering it into a competitive sport: what would be the point of a spelling bee in a language with a predictable spelling system? Where’s the fun unless you have to sweat a little as you struggle to recall whether this particular word is one where “‘i’ before ‘e’ except after ‘c’” doesn’t apply?&lt;/p&gt;
    &lt;p&gt;In short, English has a complicated writing system.&lt;/p&gt;
    &lt;p&gt;I’ve written about the origin of some of this complexity before, blaming everyone from the French to stingy printers and late medieval yuppies. But I’ve not yet plumbed the depths of this complexity. To do so, I will need to tell you the story of yogh,1 an obscure medieval letter whose rise and fall allows us to peer into this abyss.&lt;/p&gt;
    &lt;p&gt;But like an Icelandic family saga, we begin not with the story of yogh, but with the story of its parent. So allow me to introduce you to the letter ‘g,’ which, as you’ll soon see, is a complicated letter in its own right, dating back to Old English.&lt;/p&gt;
    &lt;p&gt;It starts with the shape of the letter. When modern editors print Old English today, they print nice, modern-looking ‘g’s — that is, the ones we use today, with an open or closed loop on the bottom, depending on the typeface.&lt;/p&gt;
    &lt;p&gt;This modern form of ‘g’ is called the Carolingian ‘g.’ It had its origin in the Carolingian minuscule, the script used by the scribes of the Carolingian Renaissance, the great revival of learning which flourished in the vast realm of Charlemagne (reigned 768–814).2&lt;/p&gt;
    &lt;p&gt;But Old English scribes didn’t write their g-sounds with a Carolingian ‘g.’ The Old English letter ‘g’ was written in a form called the insular ‘g.’ Here’s what it looked like: ‘ᵹ.’ It’s like a mix between a ‘z’ and a ‘3.’&lt;/p&gt;
    &lt;p&gt;Let’s see it in action. When the first lines of Beowulf are written in a modern edition, they look like this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Hwæt, we Gardena in geardagum&lt;/p&gt;&lt;lb/&gt;þeodcyninga þrym gefrunon,&lt;p&gt;‘How we have heard of the glory of the kings of the spear-Danes in days of old’&lt;/p&gt;&lt;p&gt;(Beowulf 1–2)&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;But in the manuscript, they’re written like this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;HǷÆT ǷE GARDE&lt;/p&gt;&lt;lb/&gt;na inᵹear daᵹum þeod cyninᵹa&lt;lb/&gt;þrym ᵹefrunon&lt;p&gt;London, British Library, Cotton MS Vitellius A XV, f. 132r.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Now, there’s clearly lots of other weird stuff going on in the manuscript, but focus on how the ‘g’ is represented. While the majuscule (capital) ‘G’ in ‘gardena’ is spelled more like a modern ‘g,’ all the others are insular ‘ᵹ.’&lt;/p&gt;
    &lt;p&gt;Interestingly, the Anglo-Saxons did use the Carolingian ‘g’ — just not for Old English. They used it when writing Latin, at least after the late 10th century. This was when the Church in England underwent a set of reforms, which caused a flowering of literature both in Latin and Old English.3 As part of these reforms, the Carolingian minuscule script was adopted for Latin texts.&lt;/p&gt;
    &lt;p&gt;So for a period, both ‘g’ and ‘ᵹ’ were used in England, but generally speaking, each was used for writing the ‘g’ sound in a different language. Simple enough, but the stage was set for things to get a lot more complicated.&lt;/p&gt;
    &lt;p&gt;But for that, we need the help of the Normans.&lt;/p&gt;
    &lt;p&gt;You're reading The Dead Language Society. I'm Colin Gorrie, linguist, ancient language teacher, and your guide through the history of the English language and its relatives.&lt;/p&gt;
    &lt;p&gt;Subscribe for a free issue every Wednesday, or upgrade to support my mission of bringing historical linguistics out of the ivory tower and receive two extra Saturday deep-dives per month.&lt;/p&gt;
    &lt;p&gt;If you upgrade, you’ll also be able to join our ongoing Beowulf Book Club and watch our discussion of the first 915 lines (part 1, part 2) right away.&lt;/p&gt;
    &lt;head rend="h1"&gt;Of course, they would have spelled it ȝoȝ&lt;/head&gt;
    &lt;p&gt;For the history of the English language, no single year was more momentous than 1066. In this year, William, Duke of Normandy, invaded and took the English throne, bringing with him Norman knights, and more importantly for our purposes, Norman scribes.&lt;/p&gt;
    &lt;p&gt;These Norman scribes inherited the writing traditions that the Carolingian renaissance had given birth to. This meant the latest, greatest, 11th-century French versions of the Carolingian minuscule.&lt;/p&gt;
    &lt;p&gt;These weren’t so different from the way Anglo-Saxon scribes had written Latin. But they were very different from the way they had written Old English, especially in the ‘g’ department.&lt;/p&gt;
    &lt;p&gt;But that wasn’t so much of an issue, since these Norman-trained scribes, and those of the generations that came after them, didn’t write much English at all. In fact, writing in English of any kind was very scarce up until the end of the 12th century.&lt;/p&gt;
    &lt;p&gt;Over the course of that tumultuous — and, for English, silent — century, the language had changed a great deal. All the scribes trained in the old, Anglo-Saxon traditions were long dead, so when a new generation of scribes turned their attention once again to English, they had to devise some new strategies for writing it.&lt;/p&gt;
    &lt;p&gt;And this, after a surprisingly long delay, is where we first meet the star of today’s issue: ‘ȝ,’ also known as yogh.&lt;/p&gt;
    &lt;p&gt;Yogh is descended from a variant form of the old insular ‘ᵹ.’ But, while the insular ‘ᵹ’ was thought of as the same letter as the Carolingian ‘g’ in Anglo-Saxon times, the yogh ‘ȝ’ of the 12th century was an entirely different letter from the Carolingian-derived ‘g.’&lt;/p&gt;
    &lt;p&gt;And, stranger still, ‘ȝ’ was used to write two completely different sounds in Middle English, the form of English spoken from around 1100–1450: the y-sound as in young or yesterday, and another sound that English has lost altogether.&lt;/p&gt;
    &lt;p&gt;The other sound that ‘ȝ’ once spelled is the “harsh” or “guttural” sound made in the back of the mouth, which you hear in Scots loch or German Bach.4 This sound is actually the reason for the most famous bit of English spelling chaos: the sometimes-silent, sometimes-not sequence ‘gh’ that you see in laugh, cough, night, and daughter. Maybe one day I’ll tell you that story too.&lt;/p&gt;
    &lt;p&gt;For today, however, just know that the spelling ‘gh,’ which causes spellers so much trouble, was originally a replacement for ‘ȝ’ in these words. But more on that later. Let’s dwell for a moment on the bizarre situation we had in Middle English, where the same letter ‘ȝ’ could represent either a y-sound or that now-vanished gh-sound.&lt;/p&gt;
    &lt;head rend="h1"&gt;But not as bizarre as this painting&lt;/head&gt;
    &lt;p&gt;Or is it actually so bizarre?&lt;/p&gt;
    &lt;p&gt;Modern English spelling is, of course, chaotic. So perhaps it shouldn’t surprise us that we too have a very yogh-like situation with two of our letters: ‘c’ and — wait for it — ‘g.’&lt;/p&gt;
    &lt;p&gt;Each of these regularly represents two not particularly similar sounds. The letter ‘c’ sometimes represents a k-sound, like in cat, and sometimes an s-sound, like in city. The letter ‘g’ is similar: sometimes it writes a true g-sound, like in good, but other times, what it represents is a j-sound, like in gem.&lt;/p&gt;
    &lt;p&gt;If these sounds seem similar to you, pay attention to your tongue as you make each one: the k-sound and the true g-sound are made with the back of your tongue hitting against your soft palate. The s-sound and the ‘j’ sound are made in slightly different places, but in both cases, they use the tip of your tongue coming up against (or close to) just behind your upper teeth.&lt;/p&gt;
    &lt;p&gt;Each of our two double-sounding letters, ‘c’ and ‘g,’ has two variants, which are made at completely different ends of the mouth. This is how yogh worked too: it had one variant made at the back of the mouth (the gh-sound) and the other made towards the front (that’s the y-sound).&lt;/p&gt;
    &lt;p&gt;There’s actually a good linguistic reason why this pattern keeps happening. It’s a sound change called palatalization: this happens when a sound made towards the back of the mouth, like a k- or g-sound, gets pulled forward because it’s next to another sound made at the front of the mouth. Often, these front-of-the-mouth sounds are vowels.&lt;/p&gt;
    &lt;p&gt;In Old English, these front-of-the-mouth vowels (front vowels for short) included the ones spelled ‘i’ and ‘e,’ which sounded like the vowels in bee and bay.5 Sometime in the deep prehistory of the English language, the ‘g’ sound got pulled forward in the mouth to sound like ‘y’ whenever it was next to these front vowels.&lt;/p&gt;
    &lt;p&gt;But when it came time to spell these ‘y’ sounds in the Latin alphabet, they still seemed to the Anglo-Saxon scribes to be versions of ‘g’ sounds. So they spelled them ‘ᵹ’ just like they spelled other ‘g’ sounds. This is why, when you read Old English, you can often replace ‘ᵹ’ (or ‘g’ in modern editions) with ‘y’ and get recognizable Modern English words: ᵹear is year, dæᵹ is day, weᵹ is way.6&lt;/p&gt;
    &lt;p&gt;A process like this happened in the ancestor of French too, just slightly differently. Instead of the ‘g’ being pulled forward into a ‘y’ sound before ‘i’ and ‘e,’ it got pulled forward into a ‘j’ sound.&lt;/p&gt;
    &lt;p&gt;This is why the Modern English ‘g’ represents two sounds, one before the letters ‘i’ and ‘e’, and the other before the other letters. It’s because we took our spelling conventions from how the Norman scribes wrote their language, which was an old form of French.7&lt;/p&gt;
    &lt;p&gt;While the Norman scribes, and the later English scribes they trained, were used to the letter ‘g’ writing two different sounds, neither was a y-sound. They needed another letter for that, and they found one: the old insular ‘ᵹ,’ or rather, its descendant, the yogh ‘ȝ.’&lt;/p&gt;
    &lt;p&gt;So that’s why ‘ȝ’ spelled a ‘y’ sound. To understand why ‘ȝ’ also spelled that vanished ‘gh’ sound, however, we need to go back into the distant history of English, long before it was ever written down. Actually, long before there even was an English.&lt;/p&gt;
    &lt;p&gt;Back then, there was just one single language, which would later split into English, Dutch, German, Swedish, and all the other Germanic languages.&lt;/p&gt;
    &lt;p&gt;In this Proto-Germanic language (as it’s called today), the sound that would become the English g-sound — which the Anglo-Saxons would later spell ‘ᵹ’ — was not the g-sound we have today in words like good or bag. That came later.&lt;/p&gt;
    &lt;p&gt;Instead, the Proto-Germanic ancestor of words like good had a sound very similar to the later Middle English gh-sound at the start.8 Only later would English harden that gh-sound into the g-sound we know today. But this only happened in certain places in the word, especially at the start. At the end of the word, the gh-sound remained. In Old English, both versions were spelled the same: ‘ᵹ.’&lt;/p&gt;
    &lt;p&gt;This means that ‘ᵹ’ actually had three pronunciations in Old English, not two: the y-sound (next to front vowels), the g-sound (at the start of words), and the gh-sound (in the middle of words). So when those Norman-trained scribes turned their attention to writing English in the 12th century, they had no problem writing the g-sound at the start of words with ‘g.’&lt;/p&gt;
    &lt;p&gt;But when they wanted to write the gh-sound, they ran into the same problem they had in writing the y-sound. They didn’t have a good way to write the gh-sound, which didn’t exist in French at the time, so they pressed yogh into service again.&lt;/p&gt;
    &lt;p&gt;And that’s why yogh has two sounds, each of which corresponds to a pronunciation of the Old English letter ‘ᵹ’ that the French scribal tradition couldn’t accept writing with ‘g.’&lt;/p&gt;
    &lt;head rend="h1"&gt;Wait, we’ve been saying ‘Mackenzie’ wrong?&lt;/head&gt;
    &lt;p&gt;When you’re reading Middle English, it can get a bit confusing: Which kind of yogh is which?&lt;/p&gt;
    &lt;p&gt;Look at this line from Sir Gawain and the Green Knight:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Þaȝ ȝe ȝourself be talenttyf, to take hit to yourseluen,&lt;/p&gt;&lt;lb/&gt;‘Though you yourself are eager to accept it [a challenge] personally,’ (Sir Gawain and the Green Knight 350)&lt;/quote&gt;
    &lt;p&gt;In this line, the first yogh makes the gh-sound, while the second and third represent y-sounds. But you just have to know the words in order to figure that out.&lt;/p&gt;
    &lt;p&gt;As inconvenient as this confusion is for us modern readers of Middle English, that isn’t the reason yogh disappeared from the English language. The fate of yogh was sealed by a conspiracy of factors.&lt;/p&gt;
    &lt;p&gt;One is that yogh was never the only alternative for writing the sounds it wrote. The y-sound could also be written in the way the French wrote it, that is, ‘i’ or ‘y.’ The latter is the spelling that English ended up using for this sound, hence yourself instead of ȝourself. The gh-sound could also be written ‘h,’ ‘ȝh,’ or ‘gh.’ The last of these, of course, is what English ended up adopting.&lt;/p&gt;
    &lt;p&gt;But the death blow dealt to yogh was the printing press. The earliest printing press in England was a Flemish import, as were the typefaces. But the yogh letter was unique to English, and like the other letters unique to English, it would be expensive to print. And, as we just saw, there were ready alternatives, so yogh disappeared without a trace… from English.&lt;/p&gt;
    &lt;p&gt;In Scotland, on the other hand, it stuck around for longer. Scots used the combination ‘lȝ’ to represent an ‘ly’ sequence like we have in million, and ‘nȝ’ to represent either the ‘ny’ sequence like in canyon, or an ‘ng’ sound like in singer.&lt;/p&gt;
    &lt;p&gt;And Scottish printers were more eager to keep it than English printers were. So they took advantage of the visual similarity between ‘ȝ’ and ‘z’ — most forms of cursive writing in English still write ‘z’ like ‘ȝ’ — to write their yoghs with ‘z’s.&lt;/p&gt;
    &lt;p&gt;You still see the results of this substitution, ‘lz’ and ‘nz,’ in certain Scottish names. But the ‘z’ has led them to be pronounced in ways that have nothing to do with their traditional forms. So Menzies and Mackenzie were meant to spell things that sounded more like Mingus and Mackenyie.&lt;/p&gt;
    &lt;p&gt;And that’s how one single letter of the Middle English alphabet ended up being pronounced like ‘y,’ ‘gh,’ or even eventually like ‘z.’ I warned you it would be complicated.&lt;/p&gt;
    &lt;p&gt;But the journey through the history of yogh has allowed us to peer down some interesting side alleys of the history of writing, from Carolingian scribal practices to the compromises of Scottish printers.&lt;/p&gt;
    &lt;p&gt;I don’t lament the loss of yogh myself, not nearly as much as I lament the fate of other lost letters. But if the cause of yogh is one ȝou fancy taking up ȝourself, there’s nothing standing in ȝour waȝ (it’s included in many modern fonts), althouȝ I can imagine hiȝer causes to aspire to.&lt;/p&gt;
    &lt;p&gt;The name of the letter yogh is pronounced today in many ways: you can say it to rhyme with log, loch, or brogue.&lt;/p&gt;
    &lt;p&gt;Technically, it was the double-storey, closed-loop ‘g’ that was associated with Carolingian minuscule. The open-loop, single-storey ‘g’ was a later development. Today, they’re seen as more or less interchangeable: in fact, I don’t even know what version you’re seeing when you read this, since it’ll look different on the web and in your email client.&lt;/p&gt;
    &lt;p&gt;If you know phonetic terminology, I’ll be more specific: this sound was the voiceless velar fricative, or [x] in the International Phonetic Alphabet. In some situations, it was likely the voiceless palatal fricative (IPA [ç]).&lt;/p&gt;
    &lt;p&gt;If you want to know why the names of these letters sound completely different in Modern English (‘i’ and ‘e’ sound more like the vowels in buy and bee), let me tell you all about it.&lt;/p&gt;
    &lt;p&gt;Conscientious modern editors (myself included, he said humbly) will spell the ‘g’ that you’re supposed to pronounce like ‘y’ with a little dot on top: ‘ġ.’&lt;/p&gt;
    &lt;p&gt;Ditto for the two pronunciations of ‘c.’&lt;/p&gt;
    &lt;p&gt;Note for nerds: this was the voiced velar fricative [ɣ]. Yogh would later be used for the voiceless velar fricative [x], which actually had a different origin in the ancestor of Old English. But the two sounds ended up sounding identical at the end of a word, so yogh ended up being used for both.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.deadlanguagesociety.com/p/history-of-letter-yogh"/><published>2025-10-02T21:34:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45457460</id><title>Microcomputers – The Second Wave: Toward a Mass Market</title><updated>2025-10-03T15:09:43.332593+00:00</updated><content>&lt;doc fingerprint="a568d34923a725fc"&gt;
  &lt;main&gt;
    &lt;p&gt;In 1977, three new microcomputers appeared on the scene that broke free from the industry’s hobbyist roots: the Apple II, the Commodore PET, and the Tandy/Radio Shack TRS-80. Much later, in the 1990s, journalists and historians began reverently referring to this group as “the Trinity.” Though all three machines had different origins and different trajectories (Apple, for example, appeared in 1978 to be an also-ran before rising to eclipse all of its rivals), the distinctiveness of the 1977 generation of computers is not merely a retrospective imputation by later writers. The hobby journalists of the time recognized that with the Trinity, something like an “appliance” computer had arrived on the scene, “a clean break from commercial and hobbyist computer systems requiring technical skill and dedication from their operators into a consumer market where no qualifications are required of the customer.”[1]&lt;/p&gt;
    &lt;p&gt;Three factors were required to join this holy ensemble: the technical expertise to design a capable and reliable microcomputer, a nose for the larger business opportunity latent in the hobby computer market, and the capital resources to produce, market, and sell thousands (or even tens of thousands) of computers per month. Most of all it required a certain measure of daring, a willingness to a take a leap in the dark.&lt;/p&gt;
    &lt;p&gt;After all, the transformation of the microcomputer hobby into a large-scale commercial enterprise came as a surprise to most outsiders. In 1977, the established mainframe and minicomputer makers remained cooly aloof from the microcomputer business. Clearly, computer enthusiasts had found in the Altair and its successors a fascinating gadget to occupy their spare hours. It did not necessarily follow that these toys had anything to do with the “real” computer business, any more than model rocketry had to do with putting a man on the moon. Two of the leading minicomputer makers, Hewlett-Packard and Digital, were offered ready-made micro designs by computing-loving engineers within their ranks (Steve Wozniak and Dave Ahl, respectively), but both rejected the idea, unwilling to pursue a fringe market that seemed to have nothing to do with their business.[2]&lt;/p&gt;
    &lt;p&gt;Even many of the hobbyists themselves didn’t believe that the market for a home computer would extend much beyond the existing circle of electronic hobbyists and computer enthusiasts. But a few people with access to deep pockets (not necessarily their own) smelled an opportunity in the microcomputer, and decided to pursue it, and those people formed the breaking edge of the second wave of personal computers.&lt;/p&gt;
    &lt;p&gt;The obvious place to start to tell the story of the second wave is Apple Computer: not because it is, retrospectively, the most well-known of the three, but because it had the deepest roots in the first-wave hobby community. Commodore and Tandy were well-established companies, dragged into the computer business almost against their will by internal agitators who believed fervently in the idea of the personal computer. Apple was founded by and for hobbyists. If not for the fickle whims of fate and the chutzpah of Steve Jobs, it would have met a quiet demise in total obscurity, like so many other hobby computer companies of the day.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apple Computer&lt;/head&gt;
    &lt;p&gt;The story of Apple Computer (later, simply Apple) continues to fascinate because of the company’s massive economic success and cultural impact (first in the early 1980s, then again in the twenty-first century), because of its meteoric rise from humble beginnings, and because of the vivid and contrasting personalities of its two primary co-founders: Steve Jobs and Steve Wozniak. No other topic I will cover in this series has a comparably extensive literature: all of the known details of the early years of the two Steves and their company could fill more than one book (and have). Here we can note only the most important highlights.&lt;/p&gt;
    &lt;p&gt;What became the first Apple computer began as an anonymous circuit board, the product of an intense burst of creative energy by Steven “Woz” Wozniak. Wozniak’s engineer father moved his family to Sunnyvale, California, at the southern end of San Francisco Bay, in the late 1950s, to take a job at Lockheed. The younger Wozniak developed an early fascination with electronics, and he came of age in the perfect environment to feed and reinforce that fascination: a suburban neighborhood teeming with engineer dads on every block, who had bins full of parts and minds full of expertise to lend to the eager young gadget enthusiasts who roamed the sidewalks.[3]&lt;/p&gt;
    &lt;p&gt;By the time Woz graduated from high school in 1968, he had grown into a true electronics genius, with a level of insight and skill far beyond the typical hobbyist. He could envision a design that would produce the desired effect in the most efficient way possible, with the elegant finality of a mathematical proof. Socially isolated, he lived an inward life of imagination, spending every spare moment at home and in school sketching designs for electronic systems. His social awkwardness dwelled side-by-side with a love of pranks and juvenile humor: in his early twenties he ran a Dial-a-Joke service out of his home that played pre-recorded Polack jokes from an answering machine.[4]&lt;/p&gt;
    &lt;p&gt;His high school electronics teacher gave Wozniak the opportunity to make weekly visits to a nearby corporate computer center, and he learned about minicomputers from the trade literature at the Stanford Linear Accelerator Center (SLAC). Like many other young men of his generation, these brushes with computing got him pining for his own computer; unlike most of them, he decided to do something about it. In 1971, while a student at University of California, Berkeley, he built a home computer with the help of a younger friend, a high school student named Bill Fernandez. This “Cream Soda Computer,” named for the beverage that fueled its creation, was roughly similar in character and capabilities to the Kenbak-1 sold by John Blankenbaker that same year: a very basic processor, a tiny memory, and a handful of lights for output. Meanwhile, Woz continued to pore over the brochures and manuals for dream machines like the Data General Nova minicomputer, and to work out schematics on paper for minicomputers of his own design.[5]&lt;/p&gt;
    &lt;p&gt;For years, Wozniak’s dream of a computer to call his own slumbered. Then, in the spring of 1975, a sudden shock jolted it awake. His friend Allen Baum took Wozniak to the first Homebrew Computer Club meeting under false pretenses—he told Woz that the people there were working on computer video terminals like Don Lancaster’s TV Typewriter. Wozniak knew all about that: he had already designed his own terminal to use with a local time-sharing service, Call Computer. He was unprepared, then, when he arrived at Don French’s Menlo Park garage and found everyone chattering about microprocessors. Accustomed to mastery of all things electronic, Wozniak initially recoiled at being thrown into an environment that exposed him as ignorant.[6]&lt;/p&gt;
    &lt;p&gt;After the meeting, though, he began looking into how these newfangled microprocessors worked, and found that they replicated the structure of his favorite minicomputers in miniature (indeed, the first Intel microprocessor was consciously modeled on the DEC PDP-8). Moreover, he saw that with a microprocessor, he could put a whole computer right in his TV terminal, and eliminate the need to actually “call” some outside computer.[7]&lt;/p&gt;
    &lt;p&gt;Wozniak designed and built his second computer over the following months. For the processor, he opted at first for the Motorola 6800, but then found a much cheaper device with a similar design: the MOS Technology 6502, a new product that he acquired for $25 at the Western Electric Show and Convention (WESCON) at San Francisco’s Cow Palace, in June 1975. He assembled a circuit board around the processor with controller chips for a keyboard, TV screen, 4K bytes of static RAM, and a tiny program in ROM to bring up the keyboard interface at power on. Later in the year he upgraded to cheaper (but more finicky) dynamic RAM, with the help of his friend Steve Jobs, a classmate of Bill Fernandez.[8]&lt;/p&gt;
    &lt;p&gt;Had it been only up to Wozniak, the story of his computer would end there, as a minor anecdote in the history of hobby microcomputers. Proud of his accomplishment but shy of self-promotion, he would set up his computer at Homebrew meetings and wait patiently to lure in fellow hobbyists, to whom he would offer free copies of the plans for the machine. He found few takers. Wozniak’s design was clean and self-contained, but others were cobbling together the same capabilities by combining an Altair, a Processor Technology memory board, and a TV Typewriter. Moreover, using the 6502 processor put him out of step with the mainline hobby culture of Intel 8080 machines based on Altair.&lt;/p&gt;
    &lt;p&gt;Wozniak didn’t mind that at all. He had not designed his computer to fill a market need, but because he wanted it. Comfortably settled into his dream job—designing calculators for Hewlett-Packard (HP)—and into his first real romantic relationship (with Alice Robertson, whom he met through his Dial-a-Joke line), he was content with the direction of his life and felt no pull to parlay his talent for computer design into fame, recognition, or money. His young friend Steve Jobs, on the other hand, bristled with restless energy. Though he found electronics interesting, he lacked Wozniak’s all-consuming passion for the field, and did not know where to turn instead for fulfillment. Dissatisfied with the well-worn patterns around which others organized their lives, he spent his late teens and early twenties in search of some kind of vital spiritual awakening, whether through bizarre diets inspired by German-born writer Arnold Ehret, primal scream therapy, the ancient wisdom of India, or the All One Farm, a hippie commune in Oregon.[9]&lt;/p&gt;
    &lt;p&gt;Jobs saw in Woz’s computer the potential for a business: instead of giving away the plans, they could sell the pre-assembled boards (with no case, keyboard, monitor, or other accessories). In early 1976, he cajoled his friend into a partnership; Woz agreed, but still considered this a sideline to his real work at HP. Inspired by the orchard at All One Farm where he had been spending much of his time, Jobs proposed the name Apple Computer. The partnership had a clear division of labor: Wozniak would design the hardware and software, Jobs would make the business decisions.&lt;/p&gt;
    &lt;p&gt;Financed by the sale of their most valuable property (Jobs’ Volkswagen bus and Wozniak’s HP calculator), and thirty-day credit with a parts supplier, they made a fifty-computer deal with Paul Terrell, owner of the Byte Shop computer store. But this promising beginning also turned out to also be the end of their success; very few other Apples were sold. By mid-1976, the barebones board offered by Apple didn’t cut it anymore. A hobbyist could order a Processor Technology Sol-20 that came in a handsome case with integrated power supply, keyboard and cassette interface. Even when outfitted with all of that (which Terrell had to do himself in order to make a saleable computer), the Apple lacked compatibility with the growing array of software and hardware designed for the 8080 processor and S-100 bus.[10]&lt;/p&gt;
    &lt;p&gt;Jobs knew that they would need a better computer and more money to build it. In late 1976, Woz lost himself in a second design. While he had built the first Apple so he could finally have a minicomputer of his own, he built the Apple II so he could create and play arcade games with full-color graphics. Woz knew the field well, because Jobs had an on-again, off-again job at Atari in Sunnyvale and Woz had taken a commission from his friend to design a Breakout game for the company—a single-player Pong variant, with the goal to break a whole array of blocks before losing the ball off the bottom of the screen. Unlike the Atari games of the time, though, Apple II games would be written in mutable BASIC code, not fossilized in hardware. As usual for Woz, the Apple II’s design was spare and efficient. The pièce de resistance was circuitry to time-share the memory bus so that the video card could read from memory while drawing each line of the screen, then hand control to the processor while waiting for the cathode ray beam to reset to the start of the next line.&lt;/p&gt;
    &lt;p&gt;Jobs, meanwhile, went to work on finding an investor. He tried selling Apple Computer to Atari and Commodore (a calculator firm about which we’ll have much more to say very shortly), but the former had no interest in the product and the latter balked at the price. Jobs asked Atari founder Bushnell (his former employer) for an investment, and was rebuffed again. But Bushnell did ask venture capitalist Don Valentine to come take a look at Apple. Valentine was turned off by Jobs’ hippie vibes, but put Jobs in touch with a former employee of his from his Fairchild Semiconductor days, Mike Markkula.[11]&lt;/p&gt;
    &lt;p&gt;Through this chain of happenstance, Apple Computer found exactly the right person to make it a success. Jobs exhibited taste and enterprise, but untempered by experience and marred by streaks of cruelty and pettiness. Wozniak possessed exceptional engineering talent, but entirely lacked business sense, and avoided any work that he didn’t find personal interesting.&lt;/p&gt;
    &lt;p&gt;Markkula was an even-keeled veteran of Fairchild and intel, living in semi-retirement in his thirties after making several million dollars off of his stock. He brought to Apple $250,000 of his own money, and the connections to bring in far more. He also brought knowledge about how to build and develop a company, and a deep belief in the market for something like the Apple. Markkula was already personally invested in the idea of computing at home: at Intel he had used a home teletype terminal to connect to an Intel time-sharing computer. He used it for company business but also to balance his personal checkbook, and had always wondered why Intel didn’t package the 8008 or 8080 into a computer and sell it.[12]&lt;/p&gt;
    &lt;p&gt;By the spring of 1977, the Apple II was ready to sell. Wozniak had finished the circuit design, Markkula had laid out a business plan, and Jobs had selected, a sleek, molded-plastic case that would make Apple II the most elegant-looking computer on the market. At a starting price of $1,298, it was far from the cheapest, however, and its novel processor and bus still put it outside the mainstream of hobby hardware and software. Sales were initially slow.&lt;/p&gt;
    &lt;p&gt;But Markkula kept the investment money flowing, and coaxed one last engineering miracle from Woz. Having ported his checkbook program to the Apple II, Markkula was annoyed at how long it took to load from cassette tape, and gave Wozniak the mandate to produce a magnetic disk drive for the Apple II, using the new 5.25 inch drives available from Shugart Associates. A floppy disk of the time could hold less data than a tape, but the computer could read and write it far faster and could instantly access any part of the rapidly-spinning disk (whereas tape could only be read or written at whatever position it was currently wound to).&lt;/p&gt;
    &lt;p&gt;The $595 Disk II immediately set Apple apart from its competitors, and set the stage for the company’s future growth. With the disk drive, color graphics, eight expansion slots, and the capacity for up to forty-eight kilobytes of memory, the Apple II was by far the most fully-featured and extensible of the Trinity. In 1977 and 1978, however, Apple Computer remained an also-ran. Other companies, with pre-existing manufacturing and distribution networks, raced ahead of with mass-market computers at half the price and many times the reach. They put up sales and production numbers far exceeding anything that the hobby computer market had ever seen. The first of these was Commodore.&lt;/p&gt;
    &lt;head rend="h3"&gt;Commodore&lt;/head&gt;
    &lt;p&gt;Commodore, co-founded by Jack Tramiel and Manfred Kapp, began life as a typewriter importer in Toronto, but pivoted into the calculator business in the late 1960s, selling re-branded Japanese imports from Casio and then later building its own factories, with the backing of Canadian financier Irving Gould.[13]&lt;/p&gt;
    &lt;p&gt;Tramiel, born Idek Trzmiel in Poland, survived Auschwitz and came to the U.S. in his early twenties, where he worked his way up from the bottom. Not much of an idea man or product innovator, he got ahead on hard work, a hard nose, and ruthlessness (this adjective is attributed to him seven times by various persons in Brian Bagnall’s book, Commodore: A Company on the Edge). Behind him lurked the shadowy Canadian financier Gould, who brought large quantities of cash, international contacts, and clever tax evasion schemes.&lt;/p&gt;
    &lt;p&gt;Like most North American calculator makers, Commodore found itself on the brink of disaster in 1974, after semiconductor firms like Texas Instruments began to flood the market with dirt-cheap calculators. Tramiel (now CEO of Commodore) decided to beat them by joining them: he vertically integrated by buying one of his suppliers, MOS Technology, the very firm that had built the 6502 processor used in the Apple I and II. Once again, we find a thread woven into the story of the personal computer out of the unraveling fabric of the North American calculator business.&lt;/p&gt;
    &lt;p&gt;Tramiel acquired MOS in September 1976 in exchange for a modest dollop of cash, a more substantial stake in Commodore, and a generous serving of pressure on his (also financially beleaguered) supplier.[14] In the bargain he had also acquired Charles “Chuck” Peddle, whose vision for the future of computing would pull Commodore in an entirely new direction.&lt;/p&gt;
    &lt;p&gt;A cocky striver from a poor family in Maine, Peddle studied electrical engineering at the state university, then began bouncing around the country in search of bigger and better opportunities: to California to take a job at General Electric (GE), then to GE’s time-sharing systems division offices in Arizona and Ohio, then back to Arizona to take a swing at starting a smart terminal business, miss, and join the semiconductor design team at Motorola, then to Pennsylvania to join MOS Technology as part of a breakaway group of engineers who had worked on Motorola’s 6800 microprocessor. In contrast to the introverted Woz and the otherworldly Jobs, Peddle was brash and frankly carnal—the type of man who would compare the pleasures of computer use to sex and boast that his wife had the figure of Zsa Zsa Gabor.[15]&lt;/p&gt;
    &lt;p&gt;Peddle helped to design the 6502 processor, but it was never targeted at microcomputers. It was designed as a controller for some larger application, such as an industrial machine, a traffic signal, or an automobile. At a price point of $25, it aimed to compete with the Intel 4040, the recently released successor to the 4004. MOS also sold the KIM-1, a single-board computer with a calculator screen and keypad built into it. It was intended as a marketing showpiece for the 6502, not a consumer product. Nonetheless, it sold in surprisingly large numbers to hobbyists who appreciated having a very inexpensive all-in-one computer.&lt;/p&gt;
    &lt;p&gt;In late 1975, Peddle was visiting the Miami suburbs to help Allied Leisure, a maker of electro-mechanical arcade games, design a microprocessor-powered pinball machine using the 6502. Peddle discovered that one of the Allied engineers, Bill Seiler, was a hobbyist who had bought a computer from the Digital Group in Denver, but then struggled to figure out what to do with it.[16]&lt;/p&gt;
    &lt;p&gt;Peddle decided he wanted to launch a full-fledged computer product: not because he had always dreamed of a computer of his own, but because he believed in the market opportunity, having witnessed both the not-quite-satisfied demand of hobbyists like Seiler for a computer that was both easy-to-use and able to do something useful, and the eagerness with which those same hobbyists had taken up the cheap, simple KIM-1. Up to this time, the microcomputer business was almost fully autochthonous, built by native hobbyists; Peddle was the first immigrant. With MOS scaling back under financial pressure, Peddle intended to jump ship for Allied Leisure, which had agreed to launch his planned computer. But then, in the fall of 1976, came the Commodore acquisition.[17]&lt;/p&gt;
    &lt;p&gt;Commodore’s vice president of engineering, Andre Sousan, later a major player at Apple, didn’t want to lose Peddle, and agreed to help him pitch his computer to Tramiel. Tramiel, in turn, agreed to give Peddle a computer division in Palo Alto. Sousan and Peddle succeeded with Tramiel because they framed the project in terms of the familiar calculator market: as historian Brian Bagnall writes, “[they] pitched the product as an evolution of the calculator which would surpass the HP65,” adding new features like a TV monitor and cassette deck. Tramiel especially appreciated the idea of having electronics retailer Radio Shack market the computer under its own brand, in exchange for more Commodore calculator distribution in their stores.[18]&lt;/p&gt;
    &lt;p&gt;Tramiel wanted something to show at the January 1977 Consumer Electronics Show (CES), just a few months away, so Peddle went to Apple, the only existing company with a working 6502-based computer, to try to acquire their design. As we have seen, those negotiations went nowhere, so Peddle and his team slapped the guts of a 6502-based sprinkler-system controller into a case with a rubberized keyboard sourced from Commodore’s existing calculator designs. Unimpressed by a demo of this barely-working machine at CES, Radio Shack decided they could do at least as well themselves, and broke off negotiations, too.[19]&lt;/p&gt;
    &lt;p&gt;Commodore went ahead alone, and announced the imminent arrival of the “Commodore PET 2001” – its name inspired by the pet rock fad, and its numerical designation borrowed from the space odyssey. Peddle built hype for the machine with an early prototype at the West Coast Computer Faire in San Francisco in April 1977, and the mob of over ten thousand attendees convinced Tramiel that this computer idea was something more than just a means of selling more calculators. As company revenues from calculators continued to dwindle, Tramiel kept the company afloat by transforming the excitement for the PET into cash flow with an advanced payment plan that secured eager buyers a place in line—once the computers were actually built.[20]&lt;/p&gt;
    &lt;p&gt;In the fall of 1977, Commodore finally began to ship PETs to customers. The resulting computer had serious shortcomings. Tramiel had refused to pay the up-front cost for a modern-looking plastic-molded exterior, instead funneling some revenue to his struggling file cabinet subsidiary by ordering sheet metal cases. He also insisted on a calculator-style keyboard with small rubberized keys, which proved unpleasantly cramped and flimsy. With black-and-white character graphics and no expansion slots or speaker, the PET was neither as handsome nor (in most ways) as capable as the Apple II. But it held two decided advantages: Commodore’s existing retail distribution network and a lower price. An 8KB model with integrated tape deck and monitor included cost just $795—two-thirds the price of an Apple II without those accessories.[21]&lt;/p&gt;
    &lt;p&gt;PET continued to attract attention from the press that fall, with an article in BYTE touting it as an “appliance computer,” extensive coverage in Personal Computing, and a cover feature in Popular Science. At first production failed to keep up with interest: “By the end of 1977,” Bagnall writes, “Commodore had only managed to assemble a meager 500 machines.” But Commodore straightened out its production lines, and by one estimate they sold about 25,000 PETs in 1978.[22]&lt;/p&gt;
    &lt;p&gt;Nonetheless, Commodore remained in a slightly uncomfortable position. Peddle had tried and failed to a launch a disk drive for the PET, in large part because his lack of understanding of the hobby market led him to overshoot, aiming for a complex dual-disk drive that would appeal to the kind of corporate mini-computer user that he used to be (His failure incurred Tramiel’s wrath, leading Peddle to rashly jump ship to Apple –a move that wouldn’t last.) So, on the one hand, with no color graphics, limited memory, and no disk drive, the PET could not compete for the high-end customers who wanted those capabilities and could find them in an Apple II. On the other, Commodore had failed to dominate the market for low-cost, mass-produced machines either: the PET’s 1978 sales would have made it the fastest-selling computer of all time, if not for the final entry in the 1977 Trinity, the TRS-80.[23]&lt;/p&gt;
    &lt;head rend="h3"&gt;Radio Shack&lt;/head&gt;
    &lt;p&gt;Tandy Radio Shack was the creation of Charles Tandy, a driven World War II veteran from Brownsville, Texas, who expanded his father’s Forth Worth shoe leather business into a nationally-known leather and leathercrafts empire. By the 1960s, Tandy felt that leather had taken him as far as it could; he wanted to expand into new markets with more growth potential. So, in 1963, he acquired an ailing electronics chain headquartered in Boston called Radio Shack. Pouring money from the leather business into his new venture, he expanded Radio Shack from just nine stores to several thousand over the following decade. Customers could walk into a Radio Shack in nearly every city in the United States, from Eugene, Oregon to Fort Lauderdale, Florida: even sparsely populated states like Montana and Wyoming boasted several stores each. This ubiquity provided the springboard for the TRS-80’s success.[24]&lt;/p&gt;
    &lt;p&gt;The impetus for Tandy’s entry into the computer business came from a hobbyist on the inside. Don French started his career as a teenage salesman in a Radio Shack store outside San Diego and rose rapidly through the ranks to become a project manager at the Fort Worth headquarters in 1973, while still in his mid-twenties. His fascination with computers was sparked by a course on the topic he took at Grossmont Junior College, and kindled anew by the arrival of the first hobby computers. He ordered the Mark-8 kit from Jonathan Titus in 1974, and then the MITS Altair when it became available the following year. French, enamored with his new toys, became convinced that Radio Shack should bring out its own kit computer, and tried earnestly to convince his bosses of the same.[25]&lt;/p&gt;
    &lt;p&gt;Leadership gradually came around to the idea that an expensive, innovative computer product offered an opportunity to revise Radio Shack’s image as a purveyor of low-cost, low-quality, imitative merchandise. According to French, the key turning point came in the fall of 1975, when audio equipment maker Advent wowed the market with a new speaker design. An infuriated Charles Tandy railed at the fact that his company couldn’t innovate like that, and latched onto French’s computer proposal as a way to make similar headlines. His passion for the idea could not have run too deep, however, because French’s plans for a Radio Shack computer went nowhere until the spring of 1976.[26]&lt;/p&gt;
    &lt;p&gt;At that time, a group of Tandy Radio Shack buyers including John Roach (Radio Shack’s Vice President) visited National Semiconductor in Santa Clara, California, to check out the latest chip offerings, including the company’s new SC/MP microprocessor. While there, they met a young electrical engineer named Steve Leininger. Then, when they visited the local Byte Shop to scope out the hobby computer scene, there was Leininger again, working a part-time gig behind the counter. Leininger, as it turned out, in addition to his engineering credentials, regularly attended Homebrew Computer Club meetings and was spending his spare hours building a hobby computer and writing his own BASIC. Tandy had found the perfect man to kick-off French’s computer project, combining a passion for hobby computers with the engineering chops to understand the nuances of integrated circuits and microprocessor design. The opportunity to design a computer as a commercial product and access to a better job market for his out-of-work geologist wife were sufficient enticements to bring Leininger to Texas.[27]&lt;/p&gt;
    &lt;p&gt;Leininger grew up near South Bend, Indiana, and earned a bachelor and master’s degree in electric engineering in just four years at Purdue University before accepting a job at National Semiconductor. He did not have the bold personality of Woz, Jobs, or even Peddle, but he certainly had the skills that Tandy needed. French’s notional computer project only began in earnest after Leininger’s move to Fort Worth in July 1976. However, most of the company’s leaders did not really believe that a computer would or could be a successful Radio Shack product, and they left Leininger toiling away in almost total isolation, first in a speaker plant at the Fort Worth stockyards, then in a saddle factory. [28]&lt;/p&gt;
    &lt;p&gt;It took several months before Leininger even knew what he should build. French had initially planned on a kit, as kits fell within his preserve at Tandy, making it relatively easy to sneak by leadership. But in October he and Leininger agreed to develop a pre-assembled computer instead, at the insistence of Radio Shack president Lew Kornfeld. Kornfeld felt burnt by a recent digital clock kit, which precipitated a high rate of returns from customers unable to put it together, and didn’t want to send and even more complicated kit project out the door. It is likely at this moment of uncertainty that French turned to Tramiel and Peddle to explore having Commodore build the computer instead, a notion which fell through at CES in January 1977, likely because the Tandy people saw that Peddle had nothing better to offer than what Leininger had prototyped to that point.[29]&lt;/p&gt;
    &lt;p&gt;Other than general skepticism about the viability of a Radio Shack computer in the first place, the clearest message French and Leininger got from Tandy leadership was to keep the price as low as possible: the original target (under the kit plan) was a $195 retail price. Cost control guided the rest of the design. For example, Leininger’s choice of the Zilog Z80 microprocessor, an 8080-compatible architected by the same Federico Faggin who had led the design of that seminal Intel chip. It offered more built-in circuitry than its Intel predecessor, including circuitry for refreshing dynamic RAM. Because dynamic RAM was cheaper than static, this lowered the cost of building the TRS-80. To save more money on hardware, Leininger wrote software to generate the tones for saving data to tape rather than using a physical tone-generator circuit. Meanwhile, because RCA offered to provide a cheap black-and-white TV already finished in a plastic “Mercedes silver” case, Radio Shack let the tail wag the dog, and adopted the same color for the case of the computer and keyboard, housed as a single unit.[30]&lt;/p&gt;
    &lt;p&gt;In February 1977 French, Leininger, and Roach presented the prototype to Charles Tandy and got the go-ahead to enter production. French thought they could sell 50,000 units; Lew Kornfeld found that laughable and proposed building 1,000; Roach finalized the number at 3,500 units, enough to get better economies of scale from the factory and to use a computer to manage inventory in each store if they found no buyers.[31]&lt;/p&gt;
    &lt;p&gt;Despite this continuing internal skepticism, the TRS-80 attracted tremendous interest when it was announced to the public at the Waldorf Hotel New York City on August 3rd, and even more so when French showed up with a working TRS-80s at the ComputerMania hobby computer convention in Boston, later that month. At just $600 for a monitor, keyboard, cassette recorder, four kilobytes of memory, and a simple built-in BASIC, it was substantially cheaper even than the recently-debuted PET. Just like Commodore, it took Radio Shack several more months to get their factory production lines humming, but once they did, Tandy sold TRS-80s at an astonishing rate: 100,000 in 1978, exceeding the number of minicomputers sold by all manufacturers combined that year (though certainly not coming close to their total dollar value). Rather than the flop that company leaders feared, it proved to be a vital new revenue source, accounting for 10% of Tandy’s revenue in 1978. The TRS-80 arrived just in time to take up the slack from the flagging citizens-band (CB) radio craze that had filled Radio Shack’s sails in the mid-1970s.[32]&lt;/p&gt;
    &lt;p&gt;These astonishing sales numbers are not explained by the TRS-80’s low price alone. It was also more physically accessible and visible to a broad audience than any computer before it. Radio Shack store managers across the United States set up a TRS-80, powered it on to the BASIC programming language prompt, and left it sitting out for anyone to play with. The experience that students, scientists, and engineers had been discovering and falling in love with in computer centers, labs, and offices for the previous decade became available to anyone who walked into a Radio Shack—and available to take home for just a few hundred dollars. Every electronic hobbyist who had not yet caught the computer bug was exposed every time they came into the store to pick up a few parts for a project, but so were millions of people who dropped in just to get some flashlight batteries or blank cassette tapes. One TRS-80 owner reported: “I discovered the magic of computers in a Radio Shack. My brother and I typed in a small sample program [10 INPUT “WHAT IS YOUR NAME”; A$:PRINT “HELLO,“A$ ] and I was absolutely astounded at what it did.” Others became Radio Shack bums, hanging out in the store all day for a chance to play with the computer.[33]&lt;/p&gt;
    &lt;p&gt;The TRS-80, however, did little to shake Radio Shack’s reputation for cut-rate products: the nickname “TRASH-80” appeared in print in late 1978, and would dog the product line for the rest of its existence, fairly or not. All of the personal computers of this era were clumsy and feeble compared to their minicomputer counterparts. The TRS-80 at least had a proper keyboard, unlike the early PETs. But much about it did exude a particular cheapness, including a mediocre BASIC written by Leininger (Radio Shack, like the rest of the Trinity, ended having to go to MicroSoft for a better version) and a finicky tape controller. Worst of all was the $300 Expansion Interface released in 1978, with ports for connecting a printer and a disk drive, and space for up to 32 kilobytes of additional memory and an expansion card. You could not do anything serious with the TRS-80 without an Expansion Interface, but it was notoriously unreliable, causing reboots, freezes, and monitor glitches. Even the TRS-80 advertisements were cheap knock-offs of Apple’s, with the attractive models in high-end homes replaced by unprepossessing Tandy employees posing awkwardly in Tandy offices or their suburban Fort Worth kitchens.[34]&lt;/p&gt;
    &lt;p&gt;The culture of cheapness extended to French and Leininger, as well. As mere cogs in the Tandy corporate machine, they would never see the kind of monetary rewards that awaited the founders of Apple. French, at least, expected a promotion to vice presidency and a bonus from his work on the TRS-80, but was blocked by Radio Shack president Lew Kornfeld after Charles Tandy’s death in late 1978. Disgruntled, he left to start his own company, implementing the CP/M disk operating system for the TRS-80. Leininger stayed on for several more years, designing Radio Shack computers.[35]&lt;/p&gt;
    &lt;head rend="h3"&gt;The Winnowing&lt;/head&gt;
    &lt;p&gt;In 1975 and 1976, a slew of hobby-entrepreneurs had founded companies to try to turn their love of computers into a living. As in so many new, innovation-driven markets, this early burst of entrepreneurial energy was followed by a brutal winnowing. The majority of the hobby computer companies collapsed by 1979, unable to survive in a suddenly much larger and more competitive market. [36]&lt;/p&gt;
    &lt;p&gt;MITS, though not founded as a hobby computer company, was the first to enter the market and also the first to leave. The company struggled to develop a focused and reliable line of products, and never really rose to the challenge of IMSAI, much less further waves of more powerful and easy-to-use computers. In May 1977, Roberts, who had tired of the business and wanted to cash in, sold MITS for six million dollars to Pertec, a maker of disk and tape drive systems. Most MITS employees disliked the new management. The best of them had already fled to MicroSoft or other ventures, or soon would (Dave Bunnell, for example, started the magazine Personal Computing). Pertec continued to make higher end microprocessor-based minicomputers into the 1980s, but retired the Altair in 1978, and never made another computer targeted at individuals.[37]&lt;/p&gt;
    &lt;p&gt;Bill Millard’s IMSAI, as we have already seen, launched a dud of a second product and was sucked dry of cash to help fund Millard’s chain of ComputerLand retail stores. It went bankrupt in 1979. Millard continued his financial shenanigans for decades, and in the 1990s became a wanted man for over $100 million in unpaid taxes. Processor Technology simply froze, failing to update its Sol product line in response to the threats posed by the rise of the Trinity, and also met its demise in 1979.[38]&lt;/p&gt;
    &lt;p&gt;The Digital Group of Denver had bet their company on a CPU-independent bus, the key product decision that made their computers attractive to hardcore electronic hobbyists who wanted to experiment with different processors. But this expensive and complicated setup also contributed to the company’s abysmal quality control, and new buyers began demanding to pay cash-on-delivery, not up front. This put Digital Group in a cash flow death spiral, and they, too declared bankruptcy in 1979 after a total of 3,000 computers sold.[39]&lt;/p&gt;
    &lt;p&gt;One hobby enterprise, as we know, survived by metamorphosing into a venture-capital-fueled bet on a large-scale personal computer market: Apple Computer. The other survivors found specialized niches unserved by the mass-market Trinity: Cromemco, for example, focused on delivering reliable, powerful, rigorously engineered hardware, and became the darling of the hardcore scientific computerist; Vector Graphics built turn-key business systems.&lt;/p&gt;
    &lt;p&gt;In 1975 and 1976, almost everyone entering the microcomputer business was attracted to it by their passion for the machines themselves; by 1978, with computers backed by deep-pocketed companies flying off of retail shelves, there was a new lure: the scent of money. Prior to that year, for example, almost every franchisee of the retailer ComputerLand was a computer-loving hobbyist. But after a Fortune magazine profile of the company ran in April, the franchise office was flooded with inquiries from businessmen and salesman with no prior interest in computers. For better and for worse, the microcomputer market was leaving its childhood, and its innocence was lost.[40]&lt;/p&gt;
    &lt;p&gt;Even more money was in the offing for those who could figure out how best to market these dazzling devices: make them more accessible, make them more fun, or even convince buyers that they were actually useful. Our next few installments will focus on who was buying these computers in such large quantities, and what they wanted them for.&lt;/p&gt;
    &lt;p&gt;[1] “Most Important Companies,” BYTE (September 1995), 100; Carl Helmers, “Reflections on Entry into Our Third Year,” BYTE (September 1977), 6; “Chuck Peddle on the PET Computer,” Personal Computing (September/October 1977), 31.&lt;/p&gt;
    &lt;p&gt;[2] Freiberger and Swaine, Fire in the Valley, 25-28.&lt;/p&gt;
    &lt;p&gt;[3] Michael Moritz, Return to the Little Kingdom: Steve Jobs, the Creation of Apple, and How it Changed the World (New York: Overlook Press, 2009), 31-32.&lt;/p&gt;
    &lt;p&gt;[4] Moritz, Return to the Little Kingdom, 49-52, 129.&lt;/p&gt;
    &lt;p&gt;[5] One important difference was the use of punched cards for input, though how exactly this worked is unclear. Punch card reader accessories for the computers of the time were typically large and expensive pieces of equipment, designed to read in hundreds of cards per minute. Steve Wozniak, iWoz: Computer Geek to Cult Icon (New York: W.W. Norton, 2006), 86-88. Moritz, Return to the Little Kingdom, 54-63.&lt;/p&gt;
    &lt;p&gt;[6] Wozniak, iWoz, 152-154. Wozniak also, at some point, began work on a video terminal design called the “Computer Conversor,” intended to be commercialized by Call Computer’s owner, Alex Kamradt. Whether this project began before or after the Homebrew meeting is unclear in the sources. Moritz, Return to the Little Kingdom, 124-126, 146-147. Wozniak briefly mentions the Call Computer project in his memoir (p. 170), but never mentions Kamradt, perhaps out of embarrassment that he ditched the never-finished Computer Conversor in favor of Apple Computer. Kamradt, a closeted homosexual, was murdered by a group of young men he picked up in 1991. Will Johnson, “Alex Kamradt,” 2010 (http://www.countyhistorian.com/knol/4hmquk6fx4gu-414-alex-kamradt.html).&lt;/p&gt;
    &lt;p&gt;[7] Wozniak, iWoz, 155-158.&lt;/p&gt;
    &lt;p&gt;[8] Wozniak, iWoz, 162-170; Mos Technology, “Mos 6502 Saves More Money,” September 1975 (https://upload.wikimedia.org/wikipedia/commons/1/14/MOS_6501_6502_Ad_Sept_1975.jpg).&lt;/p&gt;
    &lt;p&gt;[9] Moritz, Return to the Little Kingdom, 96-109.&lt;/p&gt;
    &lt;p&gt;[10] Moritz, Return to the Little Kingdom, 144-161.&lt;/p&gt;
    &lt;p&gt;[11] Moritz, Return to the Little Kingdom, 169-170, 182-185; Wozniak, iWoz, 194-196; Walter Isaacson, Steve Jobs (New York: Simon &amp;amp; Schuster, 2011), 72-76.&lt;/p&gt;
    &lt;p&gt;[12] Moritz, Return to the Little Kingdom, 183-185, 227-229; John Hollar, “Oral History of Armas Clifford (Mike) Markkula, Jr.,” Computer History Museum (May 1, 2012), 22-23.&lt;/p&gt;
    &lt;p&gt;[13] Gareth Edwards, “How Commodore Invented the Mass Market Computer,” Every (March 10, 2025), https://every.to/the-crazy-ones/the-first-king-of-home-computing.&lt;/p&gt;
    &lt;p&gt;[14] Brian Bagnall, Commodore: A Company on the Edge (Variant Press, 2011) 13, 55-58; “Calculator Maker Integrates Downward,” New Scientist (September 9, 1976), 541.&lt;/p&gt;
    &lt;p&gt;[15] Bagnall, Commodore, 4-13, 99, 115.&lt;/p&gt;
    &lt;p&gt;[16] Bagnall, Commodore, 43-44.&lt;/p&gt;
    &lt;p&gt;[17] Bagnall, Commodore, 51-54.&lt;/p&gt;
    &lt;p&gt;[18] Bagnall, Commodore, 59-61.&lt;/p&gt;
    &lt;p&gt;[19] Bagnall, Commodore, 70, 73-77, 84.&lt;/p&gt;
    &lt;p&gt;[20] Bganall, Commodore, 78, 97-101, 111, 117.&lt;/p&gt;
    &lt;p&gt;[21] Early promotional materials put the planned MSRP at $495, and some sources still report that price, but it was never offered that cheaply. A $495 4KB model was quickly discontinued when it was found to be a money loser. Bagnall, Commodore, 99, 101, 111, 114-115.&lt;/p&gt;
    &lt;p&gt;[22] “Commodore’s New PET Computer,” BYTE (October 1977), 50; “The PET Discussion,” Personal Computing (September/October 1977), 30-42; William J. Hawkins, “New Home Computers Can Change Your Lifestyle,” Popular Science (October 1977), 30-36; Bagnall, Commodore, 132; “BYTE News,” BYTE (May 1979), 117.&lt;/p&gt;
    &lt;p&gt;[23] Bagnall, Commodore, 161, 171-127.&lt;/p&gt;
    &lt;p&gt;[24] Irvin Farman, Tandy’s Money Machine: How Charles Tandy Built Radio Shack Into the World’s Largest Electronics Chain (Chicago: Mobium Press, 1992), 28-51, 154-163; Radio Shack, “1975 Electronics Catalog,” 84-85.&lt;/p&gt;
    &lt;p&gt;[25] Ira Goldklang, “TRS-80 Computers: Don French – The Father of the TRS-80,” June 13, 2021 (https://www.trs-80.com/wordpress/trs-80-computers-don-french/); “Interview with Don French, Co-designer of the TRS-80 Model I,” Floppy Days (February 21, 2016), https://floppydays.libsyn.com/floppy-days-53-interview-with-don-french-co-designer-of-the-trs-80-model-i.&lt;/p&gt;
    &lt;p&gt;[26] “Interview with Don French, Co-designer of the TRS-80 Model I,” Floppy Days (February 21, 2016).&lt;/p&gt;
    &lt;p&gt;[27] Farman, Tandy’s Money Machine, 402; David Welsh and Theresa Welsh, Priming the Pump: How TRS-80 Enthusiasts Helped Spark the PC Revolution (Ferndale, Michigan: The Seeker Books, 2013), 2-4.&lt;/p&gt;
    &lt;p&gt;[28] Welsh and Welsh, Priming the Pump, 4-5.&lt;/p&gt;
    &lt;p&gt;[29] The timeline of the TRS-80’s development has proved hard to pin down. My reconstruction of the timeline is anchored the very clear and precise dates given by Leininger in a 2024 interview: he started working for Tandy July 5th 1976, and demoed the computer to Tandy in February 1977. “Floppy Days 144 – Interview with Don French and Steve Leininger, Co-Designers of the TRS-80 Model I”, Floppy Days (Oct 27, 2024), https://floppydays.libsyn.com/floppy-days-144-interview-with-don-french-and-steve-leininger. In the same interview (and in other accounts), Don French places himself and John Roach at the West Coast Computer Faire before hiring Leininger to start working on the TRS-80, but with the first West Coast Computer Faire took place in April 1977, months after the prototype TRS-80 had already been demonstrated and approved for production by Charles Tandy. He also puts his conversations with Peddle at Commodore about possibly designing a computer for Radio Shack at a point before hiring, Leininger, but Peddle didn’t work for Commodore at that point. A 1981 article on Leininger gives an even more nonsensical timeline, in which Leininger leaves California to work for Tandy in the fall of 1975 (before the first Byte Shop, where he worked evenings, had even opened), chooses the Z80 in February 1976 (months before its release), and presents the prototype for Tandy’s approval in April 1976 for delivery in August of that same year (a year early). Jonathan Erickson, “The Men Behind the TRS-80,” Popular Computing (December 1981), 26-27.&lt;/p&gt;
    &lt;p&gt;[30] Welsh and Welsh, Priming the Pump, 4-5, 8.&lt;/p&gt;
    &lt;p&gt;[31] Welsh and Welsh, Priming the Pump, 6-7.&lt;/p&gt;
    &lt;p&gt;[32] Welsh and Welsh, priming the Pump, 25; “BYTE News,” BYTE (May 1979), 117; James L. Pelkey, “Chapter 7 – Data Communications: Market Order 1973-1979,” The History of Computer Communications (https://historyofcomputercommunications.info/section/7.1/Minicomputers,-Distributed-Data-Processing-and-Microprocessors).&lt;/p&gt;
    &lt;p&gt;[33] Welsh and Welsh, Priming the Pump, 30.&lt;/p&gt;
    &lt;p&gt;[34] Welsh and Welsh, Priming the Pump, 28, 36, 38; “Letters,” Kilobaud (December 1978), 18; Computer History Museum, “Radio Shack TRS-80 advertisement” (1977), https://www.computerhistory.org/revolution/personal-computers/17/298/1163; “Interview with Don French, Co-designer of the TRS-80 Model I”, Floppy Days (February 21, 2016), https://floppydays.libsyn.com/floppy-days-53-interview-with-don-french-co-designer-of-the-trs-80-model-i.&lt;/p&gt;
    &lt;p&gt;[35] Welsh and Welsh, Priming the Pump, 37.&lt;/p&gt;
    &lt;p&gt;[36] Apple and Cromemco are the only pre-1977 computer makers that survived long enough to be covered in Robert Levering, et al.’s The Computer Entrepreneurs, a 1984 collection of sixty-five industry founder profiles.&lt;/p&gt;
    &lt;p&gt;[37] Frieberger and Swaine, Fire in the Valley, 70-73, 153-155.&lt;/p&gt;
    &lt;p&gt;[38] Robert Frank, “After 20 Years, Missing CEO Reappears,” The Wall Street Journal (September 12, 2011).&lt;/p&gt;
    &lt;p&gt;[39] Robert Suding, “Digital Group Computers – The Real Story,” (ca. 2004), https://web.archive.org/web/20060820083602/www.ultimatecharger.com/dg.html.&lt;/p&gt;
    &lt;p&gt;[40] Jonathan Littman, Once Upon a Time in ComputerLand (New York: Touchstone, 1990), 133.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://technicshistory.com/2025/10/03/microcomputers-the-second-wave-towards-a-mass-market/"/><published>2025-10-03T00:51:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45457670</id><title>I spent the day teaching seniors how to use an iPhone</title><updated>2025-10-03T15:09:43.032330+00:00</updated><content>&lt;doc fingerprint="ae3e8916e6c093d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Honestly, I think Apple really needs to simplify the iPhone for the elderly. I know there are accessibility modes, but you don’t want to have to go through all that and spend hours trying to customize the phone. Also, the whole phone setup process needs to be delayed; having to go through it for an hour puts them off from even wanting to bother. I first set the phones up to make accounts, but it turns out none of them could understand how to unlock the phone. Entering a passcode was a nightmare because they kept forgetting it, even though it was a birthday they knew, lol. &lt;lb/&gt;So, I tried Touch ID and Face ID, and that was even more complicated and kept erroring out. Then, the Siri thing kept popping up on the phones with Touch ID, despite turning it off, and the whole swiping from the button kept making the screen go down to the bottom half. :/ There were too many apps; all they wanted was the phone app, but it doesn’t default to the keypad, which was too much for them to find.&lt;lb/&gt;The phones are too fiddly now, and pressing random things as they try to hold the phone meant the phone got lost in a sea of opening stuff up. So, I tried the assistive access, but why isn’t this an option from the get-go? It asks you the age of setup; why not have a 65+ or something for a senior mode?&lt;lb/&gt;They don’t need passcodes, accounts, and a sea of information. It’s insane, and it’s insane how fiddly these phones are. I never noticed because I’m used to it, but for these people with hands that barely move, the fake Touch ID button and the swiping from the bottom on Face ID phones seem to be the worst! I think having a proper physical button, like iPhones used to have, would have been superior. The one complaint about the fake button was that it didn’t feel like a real button, so they couldn’t gauge it.&lt;lb/&gt;I left there achieving nothing because they couldn’t figure out their old Nokia phones. The unlock thing on the keypad was too difficult, and if I turned that off, they kept dialing 999 in their pockets for some reason. That’s why I was there: they were calling emergency services 100 times a day, lol.&lt;lb/&gt;I think what I’ve realized is that I need to go back with flip phones that answer and hang up when you open and close them. However, the two I tried before didn’t act like that, and they had too many features. I really thought I could make the iPhone simple, but NOPE!&lt;lb/&gt;Apple should work on their phones to make them more accessible and less fiddly, without having to go through a sea of menus.&lt;/p&gt;
    &lt;p&gt;So, I tried Touch ID and Face ID, and that was even more complicated and kept erroring out. Then, the Siri thing kept popping up on the phones with Touch ID, despite turning it off, and the whole swiping from the button kept making the screen go down to the bottom half. :/ There were too many apps; all they wanted was the phone app, but it doesn’t default to the keypad, which was too much for them to find.&lt;/p&gt;
    &lt;p&gt;The phones are too fiddly now, and pressing random things as they try to hold the phone meant the phone got lost in a sea of opening stuff up. So, I tried the assistive access, but why isn’t this an option from the get-go? It asks you the age of setup; why not have a 65+ or something for a senior mode?&lt;/p&gt;
    &lt;p&gt;They don’t need passcodes, accounts, and a sea of information. It’s insane, and it’s insane how fiddly these phones are. I never noticed because I’m used to it, but for these people with hands that barely move, the fake Touch ID button and the swiping from the bottom on Face ID phones seem to be the worst! I think having a proper physical button, like iPhones used to have, would have been superior. The one complaint about the fake button was that it didn’t feel like a real button, so they couldn’t gauge it.&lt;/p&gt;
    &lt;p&gt;I left there achieving nothing because they couldn’t figure out their old Nokia phones. The unlock thing on the keypad was too difficult, and if I turned that off, they kept dialing 999 in their pockets for some reason. That’s why I was there: they were calling emergency services 100 times a day, lol.&lt;/p&gt;
    &lt;p&gt;I think what I’ve realized is that I need to go back with flip phones that answer and hang up when you open and close them. However, the two I tried before didn’t act like that, and they had too many features. I really thought I could make the iPhone simple, but NOPE!&lt;/p&gt;
    &lt;p&gt;Apple should work on their phones to make them more accessible and less fiddly, without having to go through a sea of menus.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forums.macrumors.com/threads/i-spent-the-day-trying-to-teach-seniors-how-to-use-an-iphone-and-it-was-a-nightmare.2468117/"/><published>2025-10-03T01:20:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45458122</id><title>FyneDesk: A full desktop environment for Linux written in Go</title><updated>2025-10-03T15:09:42.495957+00:00</updated><content>&lt;doc fingerprint="5141e05b0107b1ec"&gt;
  &lt;main&gt;
    &lt;p&gt;FyneDesk is an easy to use Linux/Unix desktop environment following material design. It is built using the Fyne toolkit and is designed to be easy to use as well as easy to develop. We use the Go language and welcome any contributions or feedback for the project.&lt;/p&gt;
    &lt;p&gt;Compiling requires the same dependencies as Fyne. See the Getting Started documentation for installation steps.&lt;/p&gt;
    &lt;p&gt;For a full desktop experience you will also need the following external tools installed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;arandr&lt;/code&gt;for modifying display settings&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;xbacklight&lt;/code&gt;or&lt;code&gt;brightnessctl&lt;/code&gt;for laptop brightness&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;connman-gtk&lt;/code&gt;is currently used for configuring Wi-Fi network settings&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;compton&lt;/code&gt;for compositor support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The desktop does work without the runtime dependencies but the experience will be degraded.&lt;/p&gt;
    &lt;p&gt;Using standard Go tools you can install FyneDesk using:&lt;/p&gt;
    &lt;code&gt;go get fyshos.com/fynedesk/cmd/fynedesk
&lt;/code&gt;
    &lt;p&gt;This will add &lt;code&gt;fynedesk&lt;/code&gt; to your $GOPATH (usually ~/go/bin).
You can now run the app in "preview" mode like any other Fyne app.
Doing so is not running a window manager, to do so requires another few steps:&lt;/p&gt;
    &lt;p&gt;To use this as your main desktop you can run the following commands to set up fynedesk as a selectable desktop option in your login manager (such as LightDM for example):&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/fyshos/fynedesk
cd fynedesk
make
sudo make install
&lt;/code&gt;
    &lt;p&gt;You can now log out and see that it is in your desktop selection list at login.&lt;/p&gt;
    &lt;p&gt;You can also run the window manager components in an embedded X window for testing. You will need the &lt;code&gt;Xephyr&lt;/code&gt; tool installed for your platform (often installed as part of Xorg).
Once it is present you can use the following command from the same directory as above:&lt;/p&gt;
    &lt;code&gt;make embed
&lt;/code&gt;
    &lt;p&gt;It should look like this:&lt;/p&gt;
    &lt;p&gt;If you run the command when there is a window manager running, or on an operating system that does not support window managers (Windows or macOS) then the app will start in UI test mode. When loaded in this way you can run all of the features except the controlling of windows - they will load on your main desktop.&lt;/p&gt;
    &lt;p&gt;A desktop needs to be rock solid, and whilst we are working hard to get there, any alpha or beta software can run into unexpected issues. For that reason, we have included a &lt;code&gt;fynedesk_runner&lt;/code&gt; utility that can help
manage unexpected events. If you start the desktop using the runner, then
if a crash occurs, it will normally recover where it left off with no loss
of data in your applications.&lt;/p&gt;
    &lt;p&gt;Using standard Go tools you can install the runner using:&lt;/p&gt;
    &lt;code&gt;go get fyshos.com/fynedesk/cmd/fynedesk_runner
&lt;/code&gt;
    &lt;p&gt;From then on execute that instead of the &lt;code&gt;fynedesk&lt;/code&gt; command for a more
resilient desktop when testing out pre-release builds.&lt;/p&gt;
    &lt;p&gt;Design concepts, and the abstract wallpapers have been contributed by Jost Grant.&lt;/p&gt;
    &lt;p&gt;If you are installing FyneDesk by default on a distribution, or making it available as a standard option, you should consider the following points. You do not need to ship the library or any dependencies, but it is recommended to add the following apps as well:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;app&lt;/cell&gt;
        &lt;cell role="head"&gt;go get&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;fin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;github.com/fyshos/fin&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;A display manager app that matches the look and feel of FyneDesk&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please do let us know if you package FyneDesk for your system, so we can include a link from here :).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/FyshOS/fynedesk"/><published>2025-10-03T02:13:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45458249</id><title>Stdlib: A library of frameworks, templates, and guides for technical leadership</title><updated>2025-10-03T15:09:41.232297+00:00</updated><content>&lt;doc fingerprint="a1c3470a6cbb65b3"&gt;
  &lt;main&gt;
    &lt;p&gt;Your library of frameworks, templates, and guides for technical leadership&lt;/p&gt;
    &lt;p&gt;A blog post discussing how clearer delegation practices can lead to smoother incident response.&lt;/p&gt;
    &lt;p&gt;The article explores how agile teams can effectively manage unplanned work that arises during a sprint, offering practical strategies to maintain flow and delivery commitments.&lt;/p&gt;
    &lt;p&gt;The article argues that engineering processes are often overemphasized and highlights people, capabilities, and team dynamics as the true drivers of successful engineering teams.&lt;/p&gt;
    &lt;p&gt;A concise guide that helps technology leaders adopt a business mindset to drive strategic outcomes and improve cross-functional collaboration.&lt;/p&gt;
    &lt;p&gt;LeadDev's AI Impact Report 2025 explores the challenges and opportunities for early-career engineers as AI transforms coding, mentorship, and skill development.&lt;/p&gt;
    &lt;p&gt;Join us to learn why 500 engineering leaders still find measuring team performance a major challenge.&lt;/p&gt;
    &lt;p&gt;Working smarter, not harder, to achieve high velocity.&lt;/p&gt;
    &lt;p&gt;Practical rituals to normalize the awkwardness.&lt;/p&gt;
    &lt;p&gt;An essay explaining why treating engineers as interchangeable heads leads to flawed roadmaps, hiring decisions, and attrition, and why recognizing individual contributions is essential for effective technical leadership.&lt;/p&gt;
    &lt;p&gt;A short article urging technical leaders to engage with organizational politics rather than avoid it.&lt;/p&gt;
    &lt;p&gt;An exploration of the reasons why software teams tend to over-engineer solutions and how to avoid it.&lt;/p&gt;
    &lt;p&gt;Software needs maintenance just like your car does. With proper love and attention, it can serve you well for years to come.&lt;/p&gt;
    &lt;p&gt;Packed with evidence-based strategies, Atomic Habits will teach you how to make small changes that will transform your habits and deliver amazing results.&lt;/p&gt;
    &lt;p&gt;A novel that illustrates DevOps principles through the story of an IT manager tasked with rescuing a failing project and delivering business value.&lt;/p&gt;
    &lt;p&gt;The Mind the Gap Model identifies three categories of challenges managers face when trying to close the gap between the current state and the desired future state.&lt;/p&gt;
    &lt;p&gt;Hippos, babbles and gish-gallop.&lt;/p&gt;
    &lt;p&gt;Three Toptal engineers share how they use generative AI for software development and offer actionable advice for others.&lt;/p&gt;
    &lt;p&gt;A practical guide that teaches software developers how to write clean, maintainable code and adopt craftsmanship principles.&lt;/p&gt;
    &lt;p&gt;A novel that illustrates DevOps principles and the importance of leadership, collaboration, and process improvement in modern IT organizations.&lt;/p&gt;
    &lt;p&gt;Accelerate explains how technology organizations can improve software delivery performance and drive business outcomes through proven practices and metrics.&lt;/p&gt;
    &lt;p&gt;Continuous Delivery provides a comprehensive guide to automating software build, test, and deployment processes to achieve reliable releases.&lt;/p&gt;
    &lt;p&gt;A valuable resource discovered from amazon.co.uk. This content provides insights and best practices for technical leadership and engineering management.&lt;/p&gt;
    &lt;p&gt;A practical guide that introduces the Value Flywheel concept to help organizations continuously improve and accelerate performance through systematic value creation and delivery.&lt;/p&gt;
    &lt;p&gt;Team Topologies provides a practical guide to structuring software teams and their interactions to enable fast flow of change and improve delivery outcomes.&lt;/p&gt;
    &lt;p&gt;A practical guide that teaches how to embed continuous improvement into everyday work, helping technical leaders drive sustainable performance gains.&lt;/p&gt;
    &lt;p&gt;A practical guide that introduces domain-driven design principles to help engineers manage complex software projects.&lt;/p&gt;
    &lt;p&gt;A concise guide to the Extreme Programming methodology, teaching how to improve software quality and respond quickly to changing requirements.&lt;/p&gt;
    &lt;p&gt;In this InfoQ podcast episode, Pat Kua discusses how to build strong engineering culture, effective leadership practices, and strategies for growth.&lt;/p&gt;
    &lt;p&gt;Privacy principles can be integrated into lean, fast-moving startups without sacrificing speed, by treating privacy as a core design practice rather than a compliance afterthought.&lt;/p&gt;
    &lt;p&gt;Leading through a layoff is one of the most challenging things you can do as a manager or HR professional. The article provides practical tips to help your team recover and stay productive.&lt;/p&gt;
    &lt;p&gt;When and how to tell them&lt;/p&gt;
    &lt;p&gt;A brief exploration of whether engineering productivity can be accurately measured, questioning common metrics and advice.&lt;/p&gt;
    &lt;p&gt;A concise guide offering techniques for leaders to clarify their thinking and overcome common leadership obstacles.&lt;/p&gt;
    &lt;p&gt;This ebook demystifies product strategy, providing a solid structure, examples, resources, and guidance on making product strategy stick.&lt;/p&gt;
    &lt;p&gt;We know how hard it can be to have a challenging person in your groups - whether it's a workshop, webinar or group coaching session. So we created this&lt;/p&gt;
    &lt;p&gt;Avoiding conflict is the death knell of organizations that leads to a lack of progress and careers that implode.&lt;/p&gt;
    &lt;p&gt;An article describing how the Financial Times built an engineering enablement function to improve developer productivity and delivery speed.&lt;/p&gt;
    &lt;p&gt;An article that examines the concept of failureship and offers practical guidance on how technical leaders can improve escalation processes and accountability during incidents.&lt;/p&gt;
    &lt;p&gt;Scientific insights into the causes of stress and how motivation can buffer it, with five practical ways for technical leaders to reduce stress and sustain team performance.&lt;/p&gt;
    &lt;p&gt;A short article that proposes five practical questions leaders can ask to quickly evaluate and improve their strategic thinking.&lt;/p&gt;
    &lt;p&gt;A comprehensive framework outlining career progression, roles, and competencies for engineers at Dropbox.&lt;/p&gt;
    &lt;p&gt;Peopleware explores how to create productive software teams by focusing on the human aspects of work, offering practical guidance for managers and leaders.&lt;/p&gt;
    &lt;p&gt;A practical guide that provides a clear framework for engineering leaders to build high-performing teams and make effective technical decisions.&lt;/p&gt;
    &lt;p&gt;Drive explores the science of motivation, revealing that autonomy, mastery, and purpose are the key drivers of performance and satisfaction.&lt;/p&gt;
    &lt;p&gt;A practical guide that teaches you how to navigate tough conversations at work and in life, offering proven techniques to communicate effectively and resolve conflict.&lt;/p&gt;
    &lt;p&gt;The author uses a bowling alley analogy to explain how leaders can create environments where every team member gets the chance to excel.&lt;/p&gt;
    &lt;p&gt;A clear introduction to systems thinking that helps leaders understand complex problems and improve decision-making.&lt;/p&gt;
    &lt;p&gt;A practical guide that shows how to uncover hidden work, visualise it, and improve flow for knowledge-work teams.&lt;/p&gt;
    &lt;p&gt;The stdlib collection is a community-curated library of practical, immediately useful, battle-tested resources for technical leadership. Each resource is designed to be immediately applicable to your role. New resources are added based on community feedback and emerging best practices.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://debuggingleadership.com/stdlib"/><published>2025-10-03T02:33:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45458550</id><title>You Want Technology with Warts</title><updated>2025-10-03T15:09:40.660438+00:00</updated><content>&lt;doc fingerprint="72499015d9efb9ee"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You Want Technology With Warts&lt;/head&gt;
    &lt;p&gt;I normally skip presentations because I prefer reading, but Building the Hundred-Year Web Service (YouTube) was worth the time.1 Note that despite “htmx” featuring in the title, very little of the presentation is actually about htmx. It is about choosing and using technology in such a way that it won’t require maintenance suddenly due to external factors changing. That’s a drum I’ve been banging for the last few years too, although less visibly.&lt;/p&gt;
    &lt;p&gt;Petros observes that we know how to build bridges that last hundreds of years: stone, concrete, and steel can all do this with the right engineering. We also know how to build hypertext that is likely to last at least a few decades: use plain html and css. But, Petros asks, how do we create database-y web services that lasts for decades?&lt;/p&gt;
    &lt;p&gt;Where do we store the data? Where do we perform business logic? He answers thusly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;sqlite for data storage,&lt;/item&gt;
      &lt;item&gt;sql queries for most of the application logic,&lt;/item&gt;
      &lt;item&gt;Express-on-Node.js for routing and presentation logic,&lt;/item&gt;
      &lt;item&gt;Jinja2 templates for additional presentation logic, and&lt;/item&gt;
      &lt;item&gt;html and vanilla js for triggering http requests.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I won’t debate the specifics here. 2 I’d be tempted to jam Perl into the backend instead of Node.js if I wanted truly low maintenance. I have a feeling a Perl script is more likely to run unmodified 20 years from now than some Node.js thing. But maybe I’m wrong on this. But there were other nuggets in the presentation. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I’ve frequently wondered why I turn to the web browser when I want to make cross-platform software. There’s a chart in the presentation that shows how environmental churn and api deprecation leads desktop applications to have an expected lifetime of maybe a decade, and phone apps closer to a couple of years. On the other hand, simple web pages have worked unmodified for over 40 years! That’s a good reason to default to the web as a technology.&lt;/item&gt;
      &lt;item&gt;When a page load is fast enough, the browser does not do the whole flicker-a-blank-page-before-doing-a-full-repaint, it just shows the new content right away as a sort of partial update. This is apparently a recent browser innovation, but it is what allows e.g. Decision Drill to do a full page reload when a user interacts with it, and it still feels like one of them smooth xmlHttpRequest things. Rest assured, it’s a full page reload.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But then the thing that triggered this article: sqlite. One of the more powerful arguments I’ve read against sqlite is that it has a few warts in its defaults, such tables being flexibly typed, foreign keys not being enforced, primary keys being nullable, etc.&lt;/p&gt;
    &lt;p&gt; I’ve usually thought of these warts as a bad thing. Haskell has them too, like how the built in &lt;code&gt;String&lt;/code&gt; type is bad data structure for storing text, and how
we’re stuck with a bunch of misnamed functions (mapM, ap, msum, etc.) because we
didn’t know better. Oh and the list of Perl’s warts is probably longer than its
implementation.
&lt;/p&gt;
    &lt;p&gt;Petros reframes this problem. Every single wart that annoys us today, used to be a reasonable feature that someone relied on in their production code. Every wart we see today is a testament to the care the maintainers put into backward compatibility. If we choose a technology today, we want one that saves us from future maintenance by keeping our wartful code running – even if we don’t yet know it is wartful. The best indicator of this is whether the technology has warts today.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I would much rather, the first time I install an application, “enable foreign keys” – it’s just one line of config – I’d rather do that once, build the thing correctly, and then be confident that if there’s any other built-in behaviour that I didn’t account for, that behaviour isn’t going to change on me and break my application at some point in the future.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Right on.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/you-want-technology-with-warts"/><published>2025-10-03T03:13:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45458791</id><title>Blender 4.5 brings big changes</title><updated>2025-10-03T15:09:40.132320+00:00</updated><content>&lt;doc fingerprint="ebc971191932e59c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Blender 4.5 brings big changes&lt;/head&gt;
    &lt;quote&gt;Did you know...?&lt;p&gt;LWN.net is a subscriber-supported publication; we rely on subscribers to keep the entire operation going. Please help out by buying a subscription and keeping LWN on the net.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Blender 4.5 LTS was released on July 15, 2025, and will be supported through 2027. This is the last feature release of the 3D graphics-creation suite's 4.x series; it includes quality-of-life improvements, including work to bring the Vulkan backend up to par with the default OpenGL backend. With 4.5 released, Blender developers are turning their attention toward Blender 5.0, planned for release later this year. It will introduce substantial changes, particularly in the Geometry Nodes system, a central feature of Blender's procedural workflows.&lt;/p&gt;
    &lt;head rend="h4"&gt;Brief introduction&lt;/head&gt;
    &lt;p&gt;Blender is an open-source creative application released under the GPLv3. The Blender Foundation stewards development, with significant funding from the Blender Development Fund as well as backing from individual contributors and industry sponsors. Its code is primarily written in C and C++, with Python used extensively for scripting and add-ons.&lt;/p&gt;
    &lt;p&gt;While Blender is often known as a 3D modeling and animation tool, it has grown into a comprehensive open-source suite for digital content creation. Alongside powerful 3D tools, it features compositing, nonlinear video editing, and 2D animation in 3D space. This integrated suite of tools enables designers, animators, and other creators to work with a single application across their digital pipeline. Blender also provides access to its core functions through Nodes, a visual programming system that enables procedural workflows for complex operations. The Grease Pencil tool, also accessible through the Geometry Nodes system, is used for 2D animation, cut-out animation, motion graphics, and more. Blender's procedural systems rely heavily on these node-based graphical interfaces, and the 4.5 LTS focuses on their continued evolution. These systems enable fully non-destructive workflows, preserving all original data at every stage of the editing process.&lt;/p&gt;
    &lt;p&gt;Blender strives to be compatible with visual-effects (VFX) industry standards through alignment with the VFX Reference Platform, which is updated annually. This allows Blender to be run on the same systems as other VFX software, as well as share files with them. 4.5 brings a slew of library updates to maintain alignment with the reference platform.&lt;/p&gt;
    &lt;head rend="h4"&gt;A solid foundation&lt;/head&gt;
    &lt;p&gt;Historically, Blender has relied on OpenGL for drawing its user interface and powering its 3D-display capabilities. However, efforts are underway to modernize this aspect of its core functionality by abstracting away the rendering backend, bringing support for running on additional graphics APIs, including Vulkan and Apple's Metal API. The Vulkan API is a low-overhead, cross-platform standard that allows applications like Blender to communicate more directly with GPU hardware than OpenGL. Being the final feature release of the 4.x series, this LTS brings a critical step in the maturity of the Vulkan backend. Though still not enabled by default due to multiple outstanding issues, it now rivals the OpenGL backend in both features and performance.&lt;/p&gt;
    &lt;p&gt;Vulkan is built on a parallel-execution model, allowing applications to send multiple commands to the GPU simultaneously, while OpenGL relies on a sequential model. Vulkan's execution model makes better use of the increased number of cores found in modern GPUs. This is a crucial step toward smoother viewport performance and more responsive interaction with complex scenes.&lt;/p&gt;
    &lt;p&gt;There are known limitations still blocking the new backend from being adopted as the default. Notably, large meshes with 100-million vertices or more are not yet supported, resulting in poor performance on the Vulkan backend for virtual reality and other high-mesh-count applications. Future driver updates may address some of these issues.&lt;/p&gt;
    &lt;p&gt;The viewport in Blender is an interactive view space where 3D scenes are displayed and constructed. Rendering converts 3D scenes into 2D images or video, producing the final output with Blender's built-in engines or third-party renderers. Rendering can also be performed without the graphical interface by running Blender in headless mode, both on individual systems and at scale on render farms. The viewport and rendering upgrades in Blender 4.5 extend beyond the improvements to its Vulkan backend. Specifically, work continues on EEVEE, the realtime rendering engine built for rapid, interactive rendering on modern GPUs. EEVEE 2.0, also known as EEVEE Next, receives several critical improvements focused on stability and visual accuracy. Shadows now render more smoothly thanks to the addition of shadow terminator normal bias. This is an area where EEVEE has struggled to match other renderers, including Blender's own Cycles rendering engine.&lt;/p&gt;
    &lt;p&gt;Two settings control shadow termination bias: "Geometry Offset" and "Shading Offset", found in the "Shading" tab of the "Object Properties" panel. This gives artists greater control over the position and angle of shadows. However, due to the difficulties of creating shadows that work equally well for all projects, the default for these settings is "no bias". These visual improvements coincide with fixes for rendering problems such as light leaking from large light sources. Light leaking is a phenomenon where light incorrectly passes through or around solid objects, creating unrealistic bright spots in the rendered scene. Overall, these changes aim to bring EEVEE Next closer to parity with other renderers.&lt;/p&gt;
    &lt;p&gt;Beyond rendering quality, this LTS release delivers improvements to workflow fluidity. Texture loading, shader compilation, and startup times all contribute to overall performance and user experience, and all three have been improved. Textures are now loaded using a deferred, multithreaded process, resulting in more than double the speed of the previous method. This change introduces a small CPU overhead due to loading textures before redrawing the viewport, but the cost is not significant enough to severely impact performance.&lt;/p&gt;
    &lt;p&gt;Shaders are also now compiled in parallel. Crucially, this optimization is independent of the viewport backend in use, whether Vulkan or OpenGL, translating to immediate benefits from these core improvements. That said, a new preference allows users to revert to sub-process shader calculation, if desired, which is faster but consumes more RAM. Additionally, by skipping unnecessary shading steps during viewport initialization, startup times have been improved significantly.&lt;/p&gt;
    &lt;p&gt;With ongoing efforts to improve the workspace, users can now control which tabs are visible in the Properties Editor through the right-click pop-up menu. The Asset Browser, used for importing and organizing assets (including scenes, 3D objects, textures, and more), has been continually refined throughout the 4.x series. In 4.5 LTS, it receives some key usability enhancements, particularly in how assets are displayed, such as wrapping long lines used for asset labels, and making it easier to create thumbnails for assets.&lt;/p&gt;
    &lt;head rend="h4"&gt;Nodes, Grease Pencil, and modeling polish&lt;/head&gt;
    &lt;p&gt;Rather than focusing solely on fixes and performance gains, this cycle emphasized tighter integration between the various node systems in Blender. The result is that Shader Nodes, Compositor Nodes, and Geometry Nodes (including Grease Pencil Nodes) now share more capabilities and have a more consistent workflow.&lt;/p&gt;
    &lt;p&gt;The common nodes (including mathematical operations) and procedural textures available with Shader Nodes and Geometry Nodes are now available for use in the Compositor. This change enables effects such as procedurally generated visual noise or cloudiness applied to an image or video during post-processing. Common nodes can be copied across Shader and Geometry Node setups, further aligning node logic and capability design across the toolset. In Geometry Nodes, the new "Set Mesh Normal" node grants artists direct control over custom normals, which are perpendicular vectors that are used to represent the orientation of a surface. By allowing users to define normals via Fields, Blender provides fine-grained procedural controls for surface shading. For instance, an animator could drive this node with a value to simulate a material seamlessly transitioning from a soft, smooth surface to a rough, hard-edged one, all without the need for manually editing the mesh.&lt;/p&gt;
    &lt;p&gt;In 4.5, Point Clouds debut as a new object type, accessible from the "Add" menu in the viewport or through various Geometry Nodes. Point Clouds represent objects as a group of points in 3D space and have long been used in scientific and industrial 3D scanning. According to the Blender developer documentation on Point Clouds, this new object type supports motion graphics, physics simulations (including particle systems), granular materials (such as sand and gravel), and 3D scanning. To this end, Blender includes comprehensive tools and editable object attributes, including standard transformations like rotation and scale. It also maintains high rendering performance through EEVEE and Cycles, putting point clouds on par with meshes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Looking ahead&lt;/head&gt;
    &lt;p&gt;With 4.5 LTS out the door, the Blender developers have shifted focus to 5.0, the next major release, which is now under active development. As the beginning of a new, feature-breaking series, 5.0 introduces significant refinements and modernized workflows without abandoning the user interface paradigm established by Blender 2.80 in 2019. The release notes outline several key features planned for the release. Among these improvements is the ability to mark scenes as assets, allowing entire scenes with their contents and setup to be pulled directly into the visual scene editor using the asset browser.&lt;/p&gt;
    &lt;p&gt;The Grease Pencil tool in 5.0 supports the motion blur effect, controlled by the new "Motion Blur Steps" setting in the "Grease Pencil" render panel. Linux users now benefit from HDR support in the viewport on Wayland when using the Vulkan backend. Additionally, a change to the .blend file format to handle larger content allows Blender to store meshes with more than a few hundred million vertices. This feature required a change to the file structure of the .blend file format, meaning that files created in version 5.0 are incompatible with Blender 4.4 and prior releases, but can be loaded in 4.5 LTS. While the new format supports meshes with hundreds of millions of vertices, working with such files still demands powerful hardware, specifically large amounts of system RAM and GPU memory.&lt;/p&gt;
    &lt;p&gt;Blender 5.0 improves symmetry in Edit Mode by ensuring mirrored operations no longer fail or produce inconsistent results. UV mapping, the process of unfolding the surface of a 3D model onto a 2D image for applying textures, sees improvements in Blender 5.0 thanks to improved synchronization. This change ensures selections remain aligned between the viewport and the UV Editor. Blender 5.0 finally resolves this longstanding limitation.&lt;/p&gt;
    &lt;p&gt;Those interested in downloading Blender 4.5 can get official builds from the project web site, install the Flatpak via Flathub, or install the snap package from the Snap Store.&lt;/p&gt;
    &lt;p&gt;DOGWALK, a game by the Blender Foundation, which was built using Blender and the Godot engine, was released at the same time as 4.5. The game is freely available for download from Blender Studio, Steam, and Itch.io.&lt;/p&gt;
    &lt;p&gt;According to the release schedule, Blender 5.0 will enter beta on October 1, 2025. Interested users can access official daily builds from Blender's experimental downloads page. Blender's development is open to contributors of all backgrounds; instructions on contributing code, documentation, and more are available in the developer portal.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;GuestArticles&lt;/cell&gt;
        &lt;cell&gt;Taylor, Roland&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/Articles/1036262/"/><published>2025-10-03T03:52:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45458948</id><title>Fp8 runs ~100 tflops faster when the kernel name has "cutlass" in it</title><updated>2025-10-03T15:09:38.371546+00:00</updated><content>&lt;doc fingerprint="cef8dd4f2e620e8a"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 2.3k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;[Gluon][Tutorial] Persistent attention #7298&lt;/head&gt;
    &lt;head id="button-f3bb3ca79144a209" class="btn btn-sm btn-primary m-0 ml-0 ml-md-2"&gt;New issue&lt;/head&gt;
    &lt;p&gt;Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.&lt;/p&gt;
    &lt;p&gt;By clicking “Sign up for GitHub”, you agree to our terms of service and privacy statement. We’ll occasionally send you account related emails.&lt;/p&gt;
    &lt;p&gt;Already on GitHub? Sign in to your account&lt;/p&gt;
    &lt;head rend="h2"&gt;Conversation&lt;/head&gt;
    &lt;p&gt;Rewrite the attention kernel to be persistent. This gives better performance at low-contexts. However, fp16 at large context has suffered a bit due to a ptxas instruction scheduling issue in the softmax partition. fp8 is ~100 tflops faster when the kernel name has "cutlass" in it.&lt;/p&gt;
    &lt;code&gt;Attention Z=4 H=32 D=64 causal=False:
     N_CTX  triton-fp16  triton-fp8
0   1024.0   359.574448  370.119987
1   2048.0   612.103928  641.204555
2   4096.0   653.868402  682.337948
3   8192.0   692.102228  721.555690
4  16384.0   696.972041  726.190035
5  32768.0   698.723685  727.983456
6  65536.0   699.865817  728.558321
Attention Z=4 H=32 D=64 causal=True:
     N_CTX  triton-fp16  triton-fp8
0   1024.0   181.879039  177.982453
1   2048.0   441.315463  454.310072
2   4096.0   532.170527  539.995252
3   8192.0   633.620646  638.544937
4  16384.0   667.687180  670.681255
5  32768.0   684.276329  688.571907
6  65536.0   692.953202  694.648353
Attention Z=4 H=32 D=128 causal=False:
     N_CTX  triton-fp16   triton-fp8
0   1024.0   718.580015   709.863720
1   2048.0  1133.490258  1222.548477
2   4096.0  1247.605551  1369.800195
3   8192.0  1243.482713  1406.799697
4  16384.0  1125.744367  1514.857403
5  32768.0  1124.116305  1521.267973
6  65536.0  1064.588719  1518.738037
Attention Z=4 H=32 D=128 causal=True:
     N_CTX  triton-fp16   triton-fp8
0   1024.0   355.642522   351.161232
1   2048.0   846.404095   854.547917
2   4096.0  1013.840017  1021.676435
3   8192.0  1176.258395  1152.844234
4  16384.0  1190.290681  1325.786204
5  32768.0  1063.658200  1394.413325
6  65536.0   970.531569  1413.282610
&lt;/code&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;wow!&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;For posterity, these are the best results prior to converting the kernel to persistent&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I don't see a "cutlass" in the kernel names?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Before:&lt;/p&gt;
          &lt;p&gt;After&lt;/p&gt;
          &lt;p&gt;I'm not sure if I interpreted it incorrectly, but seems like perf is dropped based on the numbers?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;Great stuff. Couple small NITs though.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;_, corr_bar, corr_producer = corr_producer.acquire()&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"/&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;p = gl.join(p0, p1).permute(0, 2, 1).reshape([config.SPLIT_M, config.BLOCK_N])&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;p = gl.convert_layout(p, config.qk_layout)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;This shouldn't be needed any more after I introduced the slice layout for split, right?&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;The convert layout coming out of the &lt;code&gt;split&lt;/code&gt; is no longer needed, but&lt;/p&gt;
    &lt;code&gt;ValueError('Layout mismatch in broadcast: 

SliceLayout(dim=1, parent=BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])) 
vs 
SliceLayout(dim=1, parent=DistributedLinearLayout(reg_bases=[[0, 64], [0, 1], [0, 2], [0, 4], [0, 8], [0, 16], [0, 32]], lane_bases=[[1, 0], [2, 0], [4, 0], [8, 0], [16, 0]], warp_bases=[[32, 0], [64, 0]], block_bases=[], shape=[128, 128]))')
&lt;/code&gt;
    &lt;p&gt;It seems that &lt;code&gt;p&lt;/code&gt; ends up with a linear layout instead of a blocked layout. I am not sure why though -- I believe the layout inference should try a blocked layout first before falling back to linear layout.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;name = "gluon_attention"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;# Up to 150 TFLOPS faster for fp8!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;if specialization.constants["dtype"] == gl.float8e5:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;name = "cutlass_" + name&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;very cool... did you check if other names change the scheduling (e.g. because of non-determinism or code alignment) or if it's literally just special cased for cutlass.&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;it's literally just special cased for cutlass.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yup&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;wow! You literally beat the nvcc team!&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;@AlexMaclean Just a FYI, in case you can prod the right folks on your side. There must be a better way to enable this optimization. A PTX directive, perhaps, if ptxas can't figure out the right thing by itself?&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;@Mogball have you checked the accuracy, is it the same? The Deepseek technical report mentioned that fp8 tensor cores use reduced mantissa for the accumulator, maybe this is what indirectly enabled/disabled by the name of the kernel.&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Deepseek technical report mentioned that fp8 tensor cores use reduced mantissa for the accumulator, maybe this is what indirectly enabled/disabled by the name of the kernel.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That's only on Hopper&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;By disassembly of&lt;/p&gt;&lt;code&gt;ptxas&lt;/code&gt;, it is indeed hard-coded that they have logic like&lt;code&gt;strstr(kernel_name, "cutlass")&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;That's Interesting! I'm curious is it feasible to modifty asm code for &lt;code&gt;ptxas&lt;/code&gt; that make the &lt;code&gt;al&lt;/code&gt; return register always be true (maybe we could modify code in the address between &lt;code&gt;2165-216c&lt;/code&gt;), did you have a try?&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;Admittedly it is feasible. But it is more likely that, this is an unstable, experimental, aggressive optimization by NVIDIA, and blindly always enabling it may produce some elusive bugs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;For D64 it did drop quite a bit during the transition to persistent. This is due to a scheduling issue in ptxas that I couldn't find a workaround for.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/triton-lang/triton/pull/7298"/><published>2025-10-03T04:21:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45459233</id><title>In Praise of RSS and Controlled Feeds of Information</title><updated>2025-10-03T15:09:36.624055+00:00</updated><content>&lt;doc fingerprint="e7f45b221ea4acbd"&gt;
  &lt;main&gt;
    &lt;p&gt;The way we consume content on the internet is increasingly driven by walled-garden platforms and black-box feed algorithms. This shift is making our media diets miserable. Ironically, a solution to the problem predates algorithmic feeds, social media and other forms of informational junk food. It is called RSS (Really Simple Syndication) and it is beautiful.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the hell is RSS?&lt;/head&gt;
    &lt;p&gt;RSS is just a format that defines how websites can publish updates (articles, posts, episodes, and so on) in a standard feed that you can subscribe to using an RSS reader (or aggregator). Don’t worry if this sounds extremely uninteresting to you; there aren’t many people that get excited about format specifications; the beauty of RSS is in its simplicity. Any content management system or blog platform supports RSS out of the box, and often enables it by default. As a result, a large portion of the content on the internet is available to you in feeds that you can tap into. But this time, you’re in full control of what you’re receiving, and the feeds are purely reverse chronological bliss. Coincidentally, you might already be using RSS without even knowing, because the whole podcasting world runs on RSS.&lt;/p&gt;
    &lt;p&gt;There are many amazing articles about the utility and elegance of RSS, and I do not think the world needs another, so I will spare you and instead focus on my personal experience and tips. If you are interested in a deeper dive, I highly recommend Molly White’s article Curate your own newspaper with RSS. It is a convincing, well-written article that you can also listen to in Molly’s own voice if you wish to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Broken distribution models&lt;/head&gt;
    &lt;p&gt;Here’s a little story about the promise of social media. In 2011, my band was getting a little more serious and preparing to record our first album. Facebook was rapidly growing all over the world, so I created an account - mostly to manage my band’s Facebook page. Back then, social media (and Facebook in particular) felt very different: vibrant and full of promise for the brave new future of web 2.0. I looked up all my favorite bands so that every time they put out an album or tour near me, I wouldn’t miss it. Many bands either lacked proper websites or rarely updated them in a useful way, so this felt like the perfect use case for Facebook.&lt;/p&gt;
    &lt;p&gt;It didn’t take long for me to start seeing the cracks. As Facebook would push for more engagement, some bands would flood their pages with multiple posts per day, especially if they were touring or had a new release coming up. Others would be more restrained, but then their posts would often be lost in the feed. There was no way to opt in only for a certain type of updates from my followed pages, and the increasingly algorithmic feed would simply prioritize posts by engagement. I realized that I wouldn’t be able to get just the important updates; instead, I’d get a wild mish-mash of engagement-bait that I wasn’t willing to work my way through. And don’t get me started about how over time, page owners had to pay to promote their posts to get any reach on the platform - that is simply extortion.&lt;/p&gt;
    &lt;p&gt;I no longer use Facebook (or any similar social media for that matter) for many reasons, though algorithmic feeds are at the top of the list. Algorithms on social media are very unlikely to be written with your best interest in mind: The goal of social media is to keep you glued to the feed for as long as possible. It optimizes for the most time spent, for engagement, for serving the most ads. It will not necessarily optimize for keeping you well informed, showing you balanced opinions, giving you control or even showing you all the information you’d like. The misalignment of incentives has become very apparent in the last few years, but the problem goes deeper. Any type of curation (because algorithmic feeds are simply curation machines) will never be flexible enough to account for every person’s needs. The story we are sold with algorithmic curation is that it adapts to everyone’s taste and interests, but that’s only true until the interests of the advertisers enter the picture.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I use RSS&lt;/head&gt;
    &lt;p&gt;My RSS journey starting many moons ago with Opera and Thunderbird, continued with Google Reader (RIP) and The Old Reader, and finally led me to running my own instance of FreshRSS. However, in the last year, I have read most of the content from my RSS feeds on my phone via the FeedMe app. I find that it scratches the itch of unlocking your phone and wanting to see something novel (probably gravitating towards social media). On the upside, it feeds me only articles and media that a) I have picked upfront and nothing more, b) is typically longer-form and more thoughtful than your typical social media posts.&lt;/p&gt;
    &lt;p&gt;Also, unlike algorithmic feeds, it allows me to pick what category of my interests I am in the mood for. If I’m in the mood for something lighter, I can just look into my “Fun” folder to check out new stuff from The Oatmeal or xkcd. If I feel like reading something more thoughtful, I’d dive into my “Reads” folder for The Marginalian or Sentiers. Feeling like catching up on the newest AI research? I can browse the latest research papers from arXiv that have specific keywords in the abstracts (such as prompt injection). Or I could just browse everything at once to see what piques my interest. I am the master of what information I consume, how and in what order, and no one can take that away from me by rearranging my feed or tweaking the algorithm.&lt;/p&gt;
    &lt;p&gt;One of the many small advantages is the consistency of the interface and the lack of distractions when reading. Modern browsers support reader modes, but you need to enter the mode manually and some pages might not be displayed correctly. I don’t have any attention problems (that I know of), but reading articles on certain newspaper sites feels like a cruel joke: the text of the article is often drowned by ads, suggested articles, polls, and other visual smog. Not a pleasant reading experience. Your RSS reader always uses the same font, font size, screen real estate and never shows anything but the article itself.&lt;/p&gt;
    &lt;p&gt;The focused, reductive nature of RSS readers means you don’t get the full website experience, but that is arguably for the better in a lot of cases. We already mentioned the lack of suggested articles with engagement bait that could easily draw you in, but another notable omission is the comments section. It is very easy to slip into the comments section at the bottom of an article and spend far too much time reading those. You can still do that in an RSS reader by opening the article in your browser, scrolling down to the comments and diving in. At least in my case, that is a safe amount of friction to prevent me from doing it most of the time. Less is more!&lt;/p&gt;
    &lt;head rend="h2"&gt;Tips to get you going&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many of the websites you open regularly, follow on social media or get a newsletter from, likely have an RSS feed. Look out for the RSS icon or the words RSS or feed. There are also tools like Lighthouse that can sniff out the feed for you. That said, my experience is that simply adding the homepage URL of the website into an aggregator usually works.&lt;/item&gt;
      &lt;item&gt;Remember my frustration with Facebook as a source of news for new music releases? Turns out there is a much better free solution called Muspy, where you enter all your favorite artists and it will notify you of their new releases. And guess what? You either get notified via email, or you use your personal RSS feed. Highly recommended!&lt;/item&gt;
      &lt;item&gt;Start easy with something like The Old Reader or Feedly - both offer relatively generous free tiers. And if you outgrow them or want to try something else, you simply export an OPML file with all your feeds and import them into your new RSS solution. This is the upside of open standards: freedom, ownership, and portability.&lt;/item&gt;
      &lt;item&gt;Once you have more than 5-10 feeds, start putting them into folders/categories. No need to overthink it, but doing this will help you be more selective about the content you read if you’re in a specific mood.&lt;/item&gt;
      &lt;item&gt;RSS readers can be great when traveling or whenever your internet connection might be down or spotty. You can set up your RSS client in a way that automatically fetches new content, so when you board the plane and go dark, you can still read through the already downloaded articles. (Beware, though: not all RSS feeds include full content - sometimes they’re more like teasers.)&lt;/item&gt;
      &lt;item&gt;Some websites that limit how many articles you can browse for free are actually less strict about content accessed through RSS feeds. There are obvious ethical concerns with abusing this, but it is still an upside, and you are only consuming what they provide.&lt;/item&gt;
      &lt;item&gt;If you want to tinker, you can set up an RSS aggregator like FreshRSS, tiny tiny RSS or selfoss on a shared web hosting service. If you want to go full self-hosted, there are many more options available.&lt;/item&gt;
      &lt;item&gt;Get a good mobile app. Try a few before you settle! This is a highly personal choice because even small UI quirks and differences may bother you. If you’re anything like me, you’ll do most of the reading on your phone, so make sure it feels good.&lt;/item&gt;
      &lt;item&gt;RSS readers/clients often have bookmarking/starring system which works much like dedicated bookmarking apps.&lt;/item&gt;
      &lt;item&gt;Bigger publications often have separate feeds for individual categories or tags - check those to avoid getting your main feed flooded.&lt;/item&gt;
      &lt;item&gt;Some websites have very elaborate RSS APIs which allow you to query for specific types of content. For example, arXiv has a really elaborate one, allowing you to only follow specific topics. The documentation is quite complex, so here is a quick example to kick start you:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;https://export.arxiv.org/api/query?search_query=abs:LLM+AND+multilingual&amp;amp;sortBy=submittedDate&amp;amp;sortOrder=descending&lt;/code&gt;&lt;/item&gt;&lt;item&gt;The query searches through the most recently submitted papers with the words LLM and multilingual in the abstract.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Do a little cleanup from time to time: unsubscribe from feeds that no longer seem to interest you. It’s fine, no one will take offense, and your attention is too precious to be wasted on stuff that is not for you.&lt;/item&gt;
      &lt;item&gt;Don’t know where to start? Check out this list of 100 most popular RSS feeds, Feedspot’s 70 most popular feeds or Hostinger’s list of 55 popular blogs. Apart from that, Google is your friend (especially if you start searching for specific topics or niches), and good blogs often link to other blogs - all you need to do is to follow the breadcrumbs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Happy RSS-ing!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.burkert.me/posts/in_praise_of_syndication/"/><published>2025-10-03T05:13:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45461500</id><title>Niri – A scrollable-tiling Wayland compositor</title><updated>2025-10-03T15:09:36.100653+00:00</updated><content>&lt;doc fingerprint="f76fe4761cb74b90"&gt;
  &lt;main&gt;
    &lt;p&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt;
    &lt;p&gt;Getting Started | Configuration | Setup Showcase&lt;/p&gt;
    &lt;p&gt;Windows are arranged in columns on an infinite strip going to the right. Opening a new window never causes existing windows to resize.&lt;/p&gt;
    &lt;p&gt;Every monitor has its own separate window strip. Windows can never "overflow" onto an adjacent monitor.&lt;/p&gt;
    &lt;p&gt;Workspaces are dynamic and arranged vertically. Every monitor has an independent set of workspaces, and there's always one empty workspace present all the way down.&lt;/p&gt;
    &lt;p&gt;The workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense. When a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built from the ground up for scrollable tiling&lt;/item&gt;
      &lt;item&gt;Dynamic workspaces like in GNOME&lt;/item&gt;
      &lt;item&gt;An Overview that zooms out workspaces and windows&lt;/item&gt;
      &lt;item&gt;Built-in screenshot UI&lt;/item&gt;
      &lt;item&gt;Monitor and window screencasting through xdg-desktop-portal-gnome &lt;list rend="ul"&gt;&lt;item&gt;You can block out sensitive windows from screencasts&lt;/item&gt;&lt;item&gt;Dynamic cast target that can change what it shows on the go&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Touchpad and mouse gestures&lt;/item&gt;
      &lt;item&gt;Group windows into tabs&lt;/item&gt;
      &lt;item&gt;Configurable layout: gaps, borders, struts, window sizes&lt;/item&gt;
      &lt;item&gt;Gradient borders with Oklab and Oklch support&lt;/item&gt;
      &lt;item&gt;Animations with support for custom shaders&lt;/item&gt;
      &lt;item&gt;Live-reloading config&lt;/item&gt;
      &lt;item&gt;Works with screen readers&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;demo.mp4&lt;/head&gt;
    &lt;p&gt;Also check out this video from Brodie Robertson that showcases a lot of the niri functionality: Niri Is My New Favorite Wayland Compositor&lt;/p&gt;
    &lt;p&gt;Niri is stable for day-to-day use and does most things expected of a Wayland compositor. Many people are daily-driving niri, and are happy to help in our Matrix channel.&lt;/p&gt;
    &lt;p&gt;Give it a try! Follow the instructions on the Getting Started page. Have your waybars and fuzzels ready: niri is not a complete desktop environment. Also check out awesome-niri, a list of niri-related links and projects.&lt;/p&gt;
    &lt;p&gt;Here are some points you may have questions about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-monitor: yes, a core part of the design from the very start. Mixed DPI works.&lt;/item&gt;
      &lt;item&gt;Fractional scaling: yes, plus all niri UI stays pixel-perfect.&lt;/item&gt;
      &lt;item&gt;NVIDIA: seems to work fine.&lt;/item&gt;
      &lt;item&gt;Floating windows: yes, starting from niri 25.01.&lt;/item&gt;
      &lt;item&gt;Input devices: niri supports tablets, touchpads, and touchscreens. You can map the tablet to a specific monitor, or use OpenTabletDriver. We have touchpad gestures, but no touchscreen gestures yet.&lt;/item&gt;
      &lt;item&gt;Wlr protocols: yes, we have most of the important ones like layer-shell, gamma-control, screencopy. You can check on wayland.app at the bottom of each protocol's page.&lt;/item&gt;
      &lt;item&gt;Performance: while I run niri on beefy machines, I try to stay conscious of performance. I've seen someone use it fine on an Eee PC 900 from 2008, of all things.&lt;/item&gt;
      &lt;item&gt;Xwayland: integrated via xwayland-satellite starting from niri 25.08.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;niri: Making a Wayland compositor in Rust · December 2024&lt;/p&gt;
    &lt;p&gt;My talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency. The talk is in Russian, but I prepared full English subtitles that you can find in YouTube's subtitle language selector.&lt;/p&gt;
    &lt;p&gt;An interview with Ivan, the developer behind Niri · June 2025&lt;/p&gt;
    &lt;p&gt;An interview by a German tech podcast Das Triumvirat (in English). We talk about niri development and history, and my experience building and maintaining niri.&lt;/p&gt;
    &lt;p&gt;A tour of the niri scrolling-tiling Wayland compositor · July 2025&lt;/p&gt;
    &lt;p&gt;An LWN article with a nice overview and introduction to niri.&lt;/p&gt;
    &lt;p&gt;If you'd like to help with niri, there are plenty of both coding- and non-coding-related ways to do so. See CONTRIBUTING.md for an overview.&lt;/p&gt;
    &lt;p&gt;Niri is heavily inspired by PaperWM which implements scrollable tiling on top of GNOME Shell.&lt;/p&gt;
    &lt;p&gt;One of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors. Being a GNOME Shell extension, PaperWM has to work against Shell's global window coordinate space to prevent windows from overflowing.&lt;/p&gt;
    &lt;p&gt;Here are some other projects which implement a similar workflow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PaperWM: scrollable tiling on top of GNOME Shell.&lt;/item&gt;
      &lt;item&gt;karousel: scrollable tiling on top of KDE.&lt;/item&gt;
      &lt;item&gt;scroll and papersway: scrollable tiling on top of sway/i3.&lt;/item&gt;
      &lt;item&gt;hyprscrolling and hyprslidr: scrollable tiling on top of Hyprland.&lt;/item&gt;
      &lt;item&gt;PaperWM.spoon: scrollable tiling on top of macOS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our main communication channel is a Matrix chat, feel free to join and ask a question: https://matrix.to/#/#niri:matrix.org&lt;/p&gt;
    &lt;p&gt;We also have a community Discord server: https://discord.gg/vT8Sfjy7sx&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/YaLTeR/niri"/><published>2025-10-03T11:08:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45461930</id><title>QuestDB (YC S20) Is Hiring a Core Database Engineer – C++ and Rust</title><updated>2025-10-03T15:09:35.793512+00:00</updated><content>&lt;doc fingerprint="a1558018b6966268"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Core Database Engineer&lt;/head&gt;&lt;head rend="h2"&gt;About QuestDB&lt;/head&gt;&lt;p&gt;As a specialized database, QuestDB stores, processes and analyzes time series data in real-time, with a focus on reliability, extreme performance and simplicity. It provides best-in-class hardware efficiency and robust features, saving costs and accelerating time-to-value.&lt;/p&gt;&lt;p&gt;Our open source repository has gathered 16k stars and QuestDB is the fastest growing database in the time-series category, according to DB-Engines. We are a product-first company with a large community of developers. As a team, we are globally distributed, remote-first and backed by leading venture capital firms and Y Combinator.&lt;/p&gt;&lt;p&gt;Teams have had success with QuestDB across a wide range of industries, such as Financial Services, Energy and Space Exploration. Category leading companies such as OKX, Mizuho, and Airbus rely on QuestDB for large-scale, data-intensive production systems. Emerging and disruptive startups also leverage QuestDB to gain a significant edge within traditional industries.&lt;/p&gt;&lt;head rend="h2"&gt;The role&lt;/head&gt;&lt;p&gt;As a Core Database Engineer, you will bring your experience in design, development, and testing to improve our open source time-series SQL database. You will continuously improve the system's performance, ensuring that QuestDB remains scalable and easy to use as we roll out new features built with C++ and Java (zero-GC). You will have the opportunity to interact with and gather feedback from QuestDB's growing community of users and contributors. You'll have the chance to work in an open and collaborative environment to improve user experience and the system's consistency along the way.&lt;/p&gt;&lt;head rend="h2"&gt;Requirements&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;3+ years of experience building data-centric software, including writing and tuning data-processing algorithms&lt;/item&gt;&lt;item&gt;BS in Computer Science or equivalent experience&lt;/item&gt;&lt;item&gt;Proficiency in C++ and Rust&lt;/item&gt;&lt;item&gt;Intermediate knowledge of Java&lt;/item&gt;&lt;item&gt;Strong understanding of data structures, algorithms, OS internals, and hardware&lt;/item&gt;&lt;item&gt;Can-do attitude, good communication skills, empathy&lt;/item&gt;&lt;item&gt;Experience working in teams, with Git workflows, CI, and code reviews&lt;/item&gt;&lt;item&gt;Cloud experience is a plus&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;What we offer&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Generous equity options package&lt;/item&gt;&lt;item&gt;Flexible working hours&lt;/item&gt;&lt;item&gt;100% remote work&lt;/item&gt;&lt;item&gt;Freedom to choose your technical equipment&lt;/item&gt;&lt;item&gt;Wonderful, highly qualified colleagues&lt;/item&gt;&lt;item&gt;Truly international team (10+ nationalities)&lt;/item&gt;&lt;item&gt;Transparent, collaborative, and inclusive culture&lt;/item&gt;&lt;item&gt;Exciting opportunities for career progression as we grow&lt;/item&gt;&lt;item&gt;High autonomy, minimal controls, and a collaborative environment&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Working at QuestDB&lt;/head&gt;&lt;p&gt;We hire talented and passionate people who share our mission to empower developers to solve their problems with data. We are building breakthrough technology to power the infrastructure of tomorrow.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Work with world-class engineers - including two teammates who placed in the top 10 of the One Billion Row Challenge (1BRC)&lt;/item&gt;&lt;item&gt;We are a company with thousands of users; our mission is to empower them&lt;/item&gt;&lt;item&gt;We invest in a culture that promotes ownership, autonomy and independent thinking&lt;/item&gt;&lt;item&gt;We have transparent leadership and value employees' strategic input&lt;/item&gt;&lt;item&gt;Our team is ambitious and tackles the most difficult problems at the deepest data infrastructure layer&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Get in touch!&lt;/head&gt;&lt;p&gt;Click below to apply for the Core Database Engineer position.&lt;/p&gt;Apply here&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://questdb.com/careers/core-database-engineer/"/><published>2025-10-03T12:00:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45462143</id><title>A Thermometer for Measuring Quantumness</title><updated>2025-10-03T15:09:35.600831+00:00</updated><content>&lt;doc fingerprint="f5dbf1ebe585c5e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Thermometer for Measuring Quantumness&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;If there’s one law of physics that seems easy to grasp, it’s the second law of thermodynamics: Heat flows spontaneously from hotter bodies to colder ones. But now, gently and almost casually, Alexssandre de Oliveira Jr. has just shown me I didn’t truly understand it at all.&lt;/p&gt;
    &lt;p&gt;Take this hot cup of coffee and this cold jug of milk, the Brazilian physicist said as we sat in a café in Copenhagen. Bring them into contact and, sure enough, heat will flow from the hot object to the cold one, just as the German scientist Rudolf Clausius first stated formally in 1850. However, in some cases, de Oliveira explained, physicists have learned that the laws of quantum mechanics can drive heat flow the opposite way: from cold to hot.&lt;/p&gt;
    &lt;p&gt;This doesn’t really mean that the second law fails, he added as his coffee reassuringly cooled. It’s just that Clausius’ expression is the “classical limit” of a more complete formulation demanded by quantum physics.&lt;/p&gt;
    &lt;p&gt;Physicists began to appreciate the subtlety of this situation more than two decades ago and have been exploring the quantum mechanical version of the second law ever since. Now, de Oliveira, a postdoctoral researcher at the Technical University of Denmark, and colleagues have shown that the kind of “anomalous heat flow” that’s enabled at the quantum scale could have a convenient and ingenious use.&lt;/p&gt;
    &lt;p&gt;It can serve, they say, as an easy method for detecting “quantumness” — sensing, for instance, that an object is in a quantum “superposition” of multiple possible observable states, or that two such objects are entangled, with states that are interdependent — without destroying those delicate quantum phenomena. Such a diagnostic tool could be used to ensure that a quantum computer is truly using quantum resources to perform calculations. It might even help to sense quantum aspects of the force of gravity, one of the stretch goals of modern physics. All that’s needed, the researchers say, is to connect a quantum system to a second system that can store information about it, and to a heat sink: a body that’s able to absorb a lot of energy. With this setup, you can boost the transfer of heat to the heat sink, exceeding what would be permitted classically. Simply by measuring how hot the sink is, you could then detect the presence of superposition or entanglement in the quantum system.&lt;/p&gt;
    &lt;p&gt;Practical benefits aside, the research demonstrates a new aspect of a deep truth about thermodynamics: How heat and energy can be transformed and moved in physical systems is intimately bound up with information — what is or can be known about those systems. In this case, we “pay for” the anomalous heat flow by sacrificing stored information about the quantum system.&lt;/p&gt;
    &lt;p&gt;“I love the idea that thermodynamic quantities can signal quantum phenomena,” said the physicist Nicole Yunger Halpern of the University of Maryland. “The topic is fundamental and deep.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Knowledge Is Power&lt;/head&gt;
    &lt;p&gt;The connection between the second law of thermodynamics and information was first explored in the 19th century by the Scottish physicist James Clerk Maxwell. To Maxwell’s distress, Clausius’ second law seemed to imply that pockets of heat will dissipate throughout the universe until all temperature differences disappear. In the process, the total entropy of the universe — crudely, a measure of how disordered and featureless it is — will inexorably increase. Maxwell realized that this trend would eventually remove all possibility of harnessing heat flows to do useful work, and the universe would settle into a sterile equilibrium pervaded by a uniform buzz of thermal motion: a “heat death.” That forecast would be troubling enough to anyone. It was anathema to the devoutly Christian Maxwell. But in a letter to his friend Peter Guthrie Tait in 1867, Maxwell claimed to have found a way to “pick a hole” in the second law.&lt;/p&gt;
    &lt;p&gt;He imagined a tiny being (later dubbed a demon) who could see the motions of individual molecules in a gas. The gas would fill a box that was divided in two by a wall with a trapdoor. By opening and closing the trapdoor selectively, the demon could sequester the faster-moving molecules in one compartment and the slower-moving ones in the other, making a hot gas and a cold one, respectively. By acting on the information it gathered about molecules’ motions, the demon thus reduced the entropy of the gas, creating a temperature gradient that could be used to do mechanical work, such as pushing a piston.&lt;/p&gt;
    &lt;p&gt;Scientists felt sure that Maxwell’s demon couldn’t really violate the second law, but it took nearly 100 years to figure out why not. The answer is that the information the demon collects and stores about the molecular motions will eventually fill up its finite memory. Its memory must then be erased and reset for it to keep working. The physicist Rolf Landauer showed in 1961 that this erasure burns energy and produces entropy — more entropy than is reduced by the demon’s sorting actions. Landauer’s analysis established an equivalence between information and entropy, implying that information itself can act as a thermodynamic resource: It can be transformed into work. Physicists experimentally demonstrated this information-to-energy conversion in 2010.&lt;/p&gt;
    &lt;p&gt;But quantum phenomena allow information to be processed in ways that classical physics does not permit — that’s the entire basis of technologies such as quantum computing and quantum cryptography. And that’s why quantum theory messes with the conventional second law.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploiting Correlations&lt;/head&gt;
    &lt;p&gt;Entangled quantum objects have mutual information: They are correlated, so we can discover properties of one by looking at the other. That in itself is not so strange; if you look at one of a pair of gloves and find it’s left-handed, you know the other is right-handed. But a pair of entangled quantum particles differs from gloves in a particular way: Whereas the handedness of gloves is already fixed before you look, this isn’t the case for the particles, according to quantum mechanics. Before we measure them, it’s undecided which value of the observable property each particle in the entangled pair has. At that stage the only things we can know are the probabilities of the possible combinations of values, such as 50% left-right and 50% right-left. Only when we measure the state of one of the particles do these possibilities resolve themselves into a definite outcome. In that measurement process, the entanglement is destroyed.&lt;/p&gt;
    &lt;p&gt;If gas molecules are entangled in this way, then a Maxwell’s demon can manipulate them more efficiently than if all the molecules are moving independently. If, say, the demon knows that any fast-moving molecule it sees coming is correlated in such a way that it will be trailed by another fast one just a moment later, the demon doesn’t have to bother observing the second particle before opening the trapdoor to admit it. The thermodynamic cost of (temporarily) foiling the second law is lowered.&lt;/p&gt;
    &lt;p&gt;In 2004, the quantum theorists Časlav Brukner of the University of Vienna and Vlatko Vedral, then at Imperial College London, pointed out that this means macroscopic thermodynamic measurements can be used as a “witness” to reveal the presence of quantum entanglement between particles. Under certain conditions, they showed, a system’s heat capacity or its response to an applied magnetic field should carry an imprint of entanglement, if it is present.&lt;/p&gt;
    &lt;p&gt;In a similar vein, other physicists calculated that you can extract more work from a warm body when there is quantum entanglement in the system than when it is purely classical.&lt;/p&gt;
    &lt;p&gt;And in 2008, the physicist Hossein Partovi of California State University identified a particularly dramatic implication of the way quantum entanglement can undermine preconceptions derived from classical thermodynamics. He realized that the presence of entanglement can actually reverse the spontaneous flow of heat from a hot object to a cold one, seemingly upending the second law itself.&lt;/p&gt;
    &lt;p&gt;That reversal is a special kind of refrigeration, Yunger Halpern said. And as usual with refrigeration, it doesn’t come for free (and so doesn’t truly subvert the second law). Classically, refrigerating an object takes work: We have to pump the heat the “wrong” way by consuming fuel, thereby repaying the entropy that’s lost by making the cold object colder and the hot object hotter. But in the quantum case, Yunger Halpern said, instead of burning fuel to achieve refrigeration, “you burn the correlations.” In other words, as the anomalous heat flow proceeds, the entanglement gets destroyed: Particles that initially had correlated properties become independent. “We can use the correlations as a resource to push heat in the opposite direction,” Yunger Halpern said.&lt;/p&gt;
    &lt;p&gt;In effect, the fuel here is information itself: specifically the mutual information of the entangled hot and cold bodies.&lt;/p&gt;
    &lt;p&gt;Two years later, David Jennings and Terry Rudolph of Imperial College London clarified what’s going on. They showed how the second law of thermodynamics can be reformulated to include the case where mutual information is present, and they calculated the limits on how much the classical heat flow can be altered and even reversed by the consumption of quantum correlations.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Demon Knows&lt;/head&gt;
    &lt;p&gt;When quantum effects are in play, then, the second law isn’t so simple. But can we do anything useful with the way quantum physics loosens the bounds of thermodynamic laws? That’s one of the goals of the discipline called quantum thermodynamics, in which some researchers seek to make quantum engines that run more efficiently than classical ones, or quantum batteries that charge more quickly.&lt;/p&gt;
    &lt;p&gt;Patryk Lipka-Bartosik of the Center for Theoretical Physics at the Polish Academy of Sciences has sought practical applications in the other direction: using thermodynamics as a tool for probing quantum physics. Last year, he and his co-workers saw how to realize Brukner and Vedral’s 2004 idea to use thermodynamic properties as a witness of quantum entanglement. Their scheme involves hot and cold quantum systems that are correlated with each other, and a third system to mediate the heat flow between the two. We can think of this third system as a Maxwell’s demon, except now it has a “quantum memory” that can itself be entangled with the systems it is manipulating. Being entangled with the demon’s memory effectively links the hot and cold systems so that the demon can infer something about one from the properties of the other.&lt;/p&gt;
    &lt;p&gt;Such a quantum demon can act as a kind of catalyst, helping heat transfer happen by accessing correlations that are inaccessible otherwise. That is, because it is entangled with the hot and cold objects, the demon can divine and exploit all their correlations systematically. And, again like a catalyst, this third system returns to its original state once the heat exchange between the objects is completed. In this way, the process can boost the anomalous heat flow beyond what can be achieved without such a catalyst.&lt;/p&gt;
    &lt;p&gt;The paper this year by de Oliveira, co-authored by Lipka-Bartosik and Jonatan Bohr Brask of the Technical University of Denmark, uses some of these same ideas but with a crucial difference that turns the setup into a kind of thermometer for measuring quantumness. In the earlier work, the demonlike quantum memory interacted with a correlated pair of quantum systems, one hot and one cold. But in the latest work, it sits between a quantum system (say, an array of entangled quantum bits, or qubits, in a quantum computer) and a simple heat sink with which the quantum system is not directly entangled.&lt;/p&gt;
    &lt;p&gt;Because the memory is entangled with both the quantum system and the sink, it can again catalyze heat flow between them beyond what is possible classically. In that process, entanglement within the quantum system converts into extra heat that enters the sink. So measuring the energy stored in the heat sink (akin to reading its “temperature”) reveals the presence of entanglement in the quantum system. But since the system and sink aren’t themselves entangled, the measurement doesn’t affect the state of the quantum system. This gambit circumvents the notorious way that measurements destroy quantumness. “If you simply tried to make a measurement on the [quantum] system directly, you’d destroy its entanglement before the process could even unfold,” de Oliveira said.&lt;/p&gt;
    &lt;p&gt;The new scheme has the advantage of being simple and general, said Vedral, who is now at the University of Oxford. “These verification protocols are very important,” he said: Whenever some quantum computer company makes a new announcement about the performance of its latest device, he said the question always arises of how (or if) they really know that entanglement among the qubits is helping with the computation. A heat sink could serve as a detector of such quantum phenomena purely via its energy change. To implement the idea, you might designate one quantum bit as the memory whose state reveals that of other qubits, and then couple this memory qubit to a set of particles that will serve as the sink, whose energy you can measure. (One proviso, Vedral added, is that you need to have very good control over your system to be sure there aren’t other sources of heat flow contaminating the measurements. Another is that the method will not detect all entangled states.)&lt;/p&gt;
    &lt;p&gt;De Oliveira thinks that a system already exists for testing their idea experimentally. He and his colleagues are discussing that goal with Roberto Serra’s research group at the Federal University of ABC in São Paulo, Brazil. In 2016, Serra and colleagues used the magnetic orientations, or spins, of carbon and hydrogen atoms in molecules of chloroform as quantum bits between which they could transfer heat.&lt;/p&gt;
    &lt;p&gt;Using this setup, de Oliveira says it should be possible to exploit a quantum behavior — in this case coherence, meaning that the properties of two or more spins are evolving in phase with one another — to change the heat flow between the atoms. Coherence of qubits is essential for quantum computing, so being able to verify it by detecting anomalous heat exchange could be helpful.&lt;/p&gt;
    &lt;p&gt;The stakes could be even higher. Several research groups are trying to design experiments to determine whether gravity is a quantum force like the other three fundamental forces. Some of these efforts involve looking for quantum entanglement between two objects generated purely by their mutual gravitational attraction. Perhaps researchers could probe such gravity-induced entanglement by making simple thermodynamic measurements on them — thereby verifying (or not) that gravity really is quantized.&lt;/p&gt;
    &lt;p&gt;To study one of the deepest questions in physics, Vedral said, “wouldn’t it be lovely if you could do something as easy and macroscopic as this?”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/a-thermometer-for-measuring-quantumness-20251001/"/><published>2025-10-03T12:24:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45462297</id><title>Faroes</title><updated>2025-10-03T15:09:34.196225+00:00</updated><content>&lt;doc fingerprint="b6601c6e8caa7e9e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Faroes (2025)&lt;/head&gt;
    &lt;p&gt;The Faroe Islands are like the child that Denmark and Iceland had, but forgot to tell the world about. This group of eighteen small islands receives the least amount of sunshine in the world per year. Constant rain and heavy winds have always battered these lands.&lt;lb/&gt;Politically part of Denmark (for now) but fiercely independent in spirit, the Faroes exist in their own bubble of Nordic culture. Here, sheep outnumber humans two to one, villages of colorful houses cling to clifftops like they're holding on for dear life, and the weather can shift from apocalyptic storms to sunny calm in the space of an hour.&lt;/p&gt;
    &lt;p&gt;Situated between Iceland, Norway and Scotland, the Faroes face the brunt of the North Atlantic weather system. Constant storms and crashing waves have sculpted the volcanic rock over millions of years into some of the most jaw-dropping (and vertigo-inducing) coastlines on Earth. These towering basalt cliffs can reach heights of over 400 meters, dropping straight into churning seas below.&lt;lb/&gt;What's most striking is how abruptly the land stops. There are no sandy beaches or gentle slopes here—the islands simply plunge headfirst into the Atlantic. One step you're on grass-covered clifftops, the next you're staring down hundreds of meters of sheer volcanic rock to where waves explode against the base far below.&lt;/p&gt;
    &lt;p&gt;The weather here is unpredictable, and changes faster than you can put your raincoat on—one minute you're in thick fog, the next you're hit with winds and piercing rain that'll knock you sideways, then suddenly the clouds part to reveal views that'll make your camera work overtime.&lt;/p&gt;
    &lt;p&gt;Meet the true locals of the Faroes. These wooly sheep have been roaming the islands for over a thousand years, and they outnumber people on the islands. They couldn't care less about your hiking plans and will casually block paths or graze on the edge of 200-meter cliffs like it's the most natural thing in the world.&lt;lb/&gt;Faroe's name comes from a combination of fær (sheep) and eyjar (islands). &lt;/p&gt;
    &lt;p&gt;Unlike their farm-bound cousins elsewhere, Faroese sheep roam completely free across the islands, somehow always managing to find the most photogenic spots for an impromptu rest. This fellow right here is the only one that gave me any sort of attention. Otherwise, they are all busy grazing on all the grass they could ever ask for.&lt;/p&gt;
    &lt;p&gt;Why fight the landscape? For over a millennium, islanders have been topping their huts with birch bark and soil and let the grass grow wild. They act as insulation, and the thick roots are an excellent waterproof seal against the weather.&lt;lb/&gt;The grass grows quickly and does need tending every once in a while. In typical Faroese fashion, the solution is simple: put a sheep on top for an afternoon.&lt;/p&gt;
    &lt;p&gt;On the northern tip of Kalsoy lies the Kallur lighthouse. Like most regions on the islands, the land is privately owned. Hiking usually incurs a modest fee paid at the trailhead to the land owners, and the rest is up to you. Trails are just sheep paths, worn smooth by countless hooves over years rather than any official trail maintenance.&lt;/p&gt;
    &lt;p&gt;There are no guardrails, no warning signs, and definitely no liability waivers - just you, the weather, and whatever route the sheep decided made sense. The approach to Kallur is particularly gnarly, following a knife-edge ridge with steep drops on both sides before reaching the lighthouse perched dramatically on sea cliffs.&lt;/p&gt;
    &lt;p&gt;In No Time To Die (2021), Daniel Craig's James Bond meets his end at the villain's lair, which happened to be here on Kalsoy. The Faroese then followed through with the obvious next step.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://photoblog.nk412.com/Faroe2025/Faroes/n-cPCNFr"/><published>2025-10-03T12:41:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45462687</id><title>Amber Room</title><updated>2025-10-03T15:09:33.942788+00:00</updated><content>&lt;doc fingerprint="e1445fb01ac236a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Amber Room&lt;/head&gt;&lt;p&gt;The Amber Room (Russian: Янтарная комната, romanized: Yantarnaya Komnata, German: Bernsteinzimmer) was a chamber decorated in amber panels backed with gold leaf and mirrors, located in the Catherine Palace of Tsarskoye Selo near Saint Petersburg.&lt;/p&gt;&lt;p&gt;Constructed in the 18th century in Prussia, the room was dismantled and eventually disappeared during World War II. Before its loss, it was considered an "Eighth Wonder of the World". A reconstruction was made, starting in 1979 and completed and installed in the Catherine Palace in 2003.&lt;/p&gt;&lt;p&gt;The Amber Room was intended in 1701 for the Charlottenburg Palace, in Berlin, Prussia, but was eventually installed at the Berlin City Palace. It was designed by German baroque sculptor Andreas Schlüter and Danish amber craftsman Gottfried Wolfram. Schlüter and Wolfram worked on the room until 1707, when work was continued by amber masters Gottfried Turau and Ernst Schacht from Danzig (Gdańsk).&lt;/p&gt;&lt;p&gt;It remained in Berlin until 1716, when it was given by the Prussian King Frederick William I to his ally Tsar Peter the Great of the Russian Empire. In Russia, the room was installed in the Catherine Palace. After expansion and several renovations, it covered more than 55 square metres (590 sq ft) and contained over 6 tonnes (13,000 lb) of amber.&lt;/p&gt;&lt;p&gt;The Amber Room was looted during World War II by the Army Group North of Nazi Germany, and taken to Königsberg for reconstruction and display. Some time in early 1944, with Allied forces closing in on Germany, the room was disassembled and crated for storage in the Castle basement.[1] Königsberg was destroyed by Allied bombers in August 1944 and documentation of the room location ends there. Its eventual fate and current whereabouts, if it survives, remain a mystery. In 1979, the decision was taken to create a reconstructed Amber Room at the Catherine Palace in Pushkin. After decades of work by Russian craftsmen and donations from Germany, it was completed and inaugurated in 2003.&lt;/p&gt;&lt;head rend="h2"&gt;Architecture&lt;/head&gt;[edit]&lt;p&gt;The Amber Room is a priceless piece of art, with extraordinary architectural features, such as gilding, carvings, 450 kg (990 lb) of amber panels, gold leaf, gemstones, and mirrors, all highlighted with candle light.[2][3][4] Additional architectural and design features include statues of angels and children.[3][4]&lt;/p&gt;&lt;p&gt;Because of its unique features and singular beauty, the original Amber Room was sometimes dubbed the "Eighth Wonder of the World".[3] Modern estimates of the room's value range from $142 million (2007)[5] to over $500 million (2016).[6]&lt;/p&gt;&lt;head rend="h2"&gt;History&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Creation&lt;/head&gt;[edit]&lt;p&gt;The Amber Room had begun in 1701 with the purpose of being installed at Charlottenburg Palace, the residence of Frederick, the first King in Prussia, at the urging of his second wife, Sophia Charlotte.[7][2] The concept and design of the room was drafted by Andreas Schlüter.[7][2] It was fabricated by Gottfried Wolfram, master craftsman to the Danish court of King Frederick IV of Denmark, with help from the amber masters Ernst Schacht and Gottfried Turau from Danzig, now Gdańsk in Poland.[7][2][3]&lt;/p&gt;&lt;p&gt;Although originally intended for installation at Charlottenburg Palace, the complete panels were eventually installed at Berlin City Palace.[8] The Amber Room did not, however, remain at Berlin City Palace for long. Peter the Great of Russia admired it during a visit, and in 1716, King Frederick I's son Frederick William I presented the room to Peter as a gift, which forged a Russo-Prussian alliance against Sweden.[2][3]&lt;/p&gt;&lt;p&gt;The original Berlin design of the Amber Room was reworked in Russia in a joint effort by German and Russian craftsmen.[7][2] It was Peter's daughter Empress Elizabeth who decided the amber treasure should be installed at Catherine Palace, where the Russian Imperial family typically spent their summers.[3][4] After several other 18th-century renovations, the room covered more than 55 square metres (590 sq ft) and contained over 6 tonnes (13,000 lb) of amber.[2] The room took over ten years to construct.[7][2]&lt;/p&gt;&lt;head rend="h3"&gt;Theft during World War II&lt;/head&gt;[edit]&lt;p&gt;Shortly after the beginning of the German invasion of the Soviet Union in World War II, the curators responsible for removing the art treasures in Leningrad tried to disassemble and remove the Amber Room.[7] However, over the years the amber had dried out and become brittle, making it impossible to move the room without the amber crumbling.[7] The Amber Room was therefore hidden behind mundane wallpaper, in an attempt to keep German forces from seizing it, but the attempt to hide such a well-known piece of art failed.[3]&lt;/p&gt;&lt;p&gt;German soldiers of Army Group North disassembled the Amber Room within 36 hours under the supervision of two experts.[2][3][4][9] On 14 October 1941, the priceless room reached Königsberg in East Prussia, for storage and display in the town's castle.[2][3][9] On 13 November 1941, a Königsberg newspaper announced an exhibition of the Amber Room at Königsberg Castle.[9]&lt;/p&gt;&lt;head rend="h3"&gt;Last days in Königsberg&lt;/head&gt;[edit]&lt;p&gt;Orders given by Hitler on 21 and 24 January 1945 ordered the movement of looted possessions from Königsberg.[10] This allowed Albert Speer, Reichsminister of Armaments, and his administration team to transport cultural goods of priority.[10] However, before the Amber Room could be moved, Erich Koch, who was in charge of civil administration in Königsberg during the final months of the war, abandoned his post and fled from the city, leaving General Otto Lasch in command.[11]&lt;/p&gt;&lt;p&gt;In August 1944, Königsberg was heavily fire-bombed by the Royal Air Force. It suffered further extensive damage from the artillery of the advancing Red Army before the final occupation on 9 April 1945.[12]&lt;/p&gt;&lt;head rend="h2"&gt;Disappearance and mysteries&lt;/head&gt;[edit]&lt;p&gt;After the war, the Amber Room was never seen in public again, though reports have occasionally surfaced stating that pieces of the Amber Room survived the war.[13] Several eyewitnesses claimed to have spotted the famous room being loaded on board the Wilhelm Gustloff, which left Gdynia (at the time Gotenhafen) on 30 January 1945, and was then promptly torpedoed and sunk by a Soviet submarine.[13]&lt;/p&gt;&lt;p&gt;In 1997, an Italian stone mosaic "Feel and Touch" that was part of a set of four stones which had decorated the Amber Room was found in Germany, in the possession of the family of a soldier who claimed to have helped pack up the amber chamber.[2][14] The mosaic came into the hands of the Russian authorities and was used in the reconstruction.[2][14]&lt;/p&gt;&lt;p&gt;In 1998, two separate teams, one German and one Lithuanian, announced they had located the Amber Room.[15] The German team pointed to a silver mine while the Lithuanian team believed the amber treasure was buried in a lagoon; neither of the two locations turned out to hold the Amber Room.[15]&lt;/p&gt;&lt;p&gt;In 2004, a lengthy investigation by British investigative journalists Catherine Scott-Clark and Adrian Levy concluded that the Amber Room was most likely destroyed when Königsberg Castle was damaged,[16] first during the bombing of Königsberg by the Royal Air Force in 1944 and then by the Soviets' burning of the castle followed by shelling of the remaining walls.[16][17] Official assessments, set out in documents from the Russian National Archives written by Alexander Brusov, head of the Soviet team charged with locating the Amber Room following the war, agreed with this theory. The official report stated: "Summarizing all the facts, we can say that the Amber Room was destroyed between 9 and 11 April 1945."[18]&lt;/p&gt;&lt;p&gt;These dates correspond with the end of the Battle of Königsberg, which on 9 April ended with the surrender of the German garrison. A few years later, Brusov publicly voiced a contrary opinion;[19] this is believed to have been done due to pressure from Soviet authorities, who did not want to be seen as responsible for the loss of the Amber Room.[19]&lt;/p&gt;&lt;p&gt;Among other information retrieved from the archives was the revelation that the remaining Italian stone mosaics were found in the burned debris of the castle.[20] Scott-Clark and Levy concluded in their report that the reason the Soviets conducted extensive searches for the Amber Room, even though their own experts had concluded that it was destroyed, was because they wanted to know if any of their own soldiers had been responsible for the destruction.[21] Scott-Clark and Levy also assessed that others in the Soviet government found the theft of the Amber Room a useful Cold War propaganda tool.[22] Russian government officials have since denied these conclusions. Adelaida Yolkina, senior researcher at the Pavlovsk Palace, reportedly stated: "It is impossible to see the Red Army being so careless that they let the Amber Room be destroyed".[23]&lt;/p&gt;&lt;p&gt;After the report was made public, Leonid Arinshtein, who was a lieutenant in the Red Army in charge of a rifle platoon during the Battle of Königsberg, said: "I probably was one of the last people who saw the Amber Room".[24] At the same time, he explained that the whole city was burning due to artillery bombardments, but also denied allegations that the Red Army burned the city on purpose, saying: "What soldiers would burn the city where they will have to stay?"[24]&lt;/p&gt;&lt;p&gt;A variation of this theory by some present-day residents of Kaliningrad (formerly Königsberg), is that at least parts of the room were found in the Königsberg Castle cellars after World War II by the Red Army. The Amber Room was allegedly still in good condition; this was not admitted at the time so the blame could fall upon the Nazis.[24] To preserve this story, access to the ruins of the castle, which was allowed after World War II, was suddenly restricted to all, including historical and archaeological surveys, but the room is said to be in a storehouse near Königsberg Castle.[24]&lt;/p&gt;&lt;p&gt;Then in 1968, despite academic protests worldwide, Soviet general secretary Leonid Brezhnev ordered the destruction of Königsberg Castle, thus making any onsite research of the last known resting place of the Amber Room all but impossible.[24] Later the search for the Amber Room continued in different locations, including near Wuppertal, Germany.[25]&lt;/p&gt;&lt;p&gt;Another hypothesis involves a bunker in Mamerki in northeastern Poland, or that Stalin ordered the Amber Room replaced with a replica prior to its looting, hiding the original. The main problem with finding the Amber Room is that the Nazi regime hid many items in many difficult-to-reach places, usually without documentation, leaving a wide search area. The Germans also moved items to destinations far from Europe in some cases. The search for the Amber Room has also been halted by authorities. In the case of Frýdlant castle it was halted because of the historic value of the castle.[26][27]&lt;/p&gt;&lt;p&gt;In October 2020 Polish divers from the Baltictech group found the wreck of the SS Karlsruhe, a ship which took part in Operation Hannibal, a sea evacuation which allowed more than a million German troops and civilians from East Prussia to escape advancing Soviet forces. The ship was attacked off the coast of Poland by Soviet aircraft after it sailed from Königsberg in 1945. The wreck holds many crates with unknown contents.[28] An online news website, Live Science, reports that this German steamship may hold crates that contain parts of the Amber Room,[29] but divers subsequently discovered that the crates on the ship contained military equipment and personal belongings.&lt;/p&gt;&lt;head rend="h2"&gt;Reconstruction&lt;/head&gt;[edit]&lt;p&gt;In 1979, the Soviet government decided to construct a replica of the Amber Room at Tsarskoye Selo, a process that was to take 24 years and require 40 Russian and German experts in amber craftsmanship.[2][3] Using original drawings and old black-and-white photographs, every attempt was made to duplicate the original Amber Room. This included the 350 shades of amber in the original panels and fixtures that adorned the room.[30] A major problem was the lack of skilled workers, since amber carving was a nearly lost art form.[30]&lt;/p&gt;&lt;p&gt;The financial difficulties that plagued the reconstruction project from the start were solved with the donation of US$3.5 million from the German company E.ON.[31] By 2003, the work of the Russian craftsmen was mostly completed.[30] The new room was dedicated by Russian President Vladimir Putin and German Chancellor Gerhard Schröder at the 300th anniversary of the city of Saint Petersburg.[32] In Kleinmachnow, near Berlin, there is a miniature Amber Room, fabricated after the original.[33] The Berlin miniature collector Ulla Klingbeil had this copy made of original East Prussian amber.[33]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Amber case presented by Friedrich Wilhelm I to Peter the Great during his stay in Berlin in 1716.[34]&lt;/item&gt;&lt;item&gt; Mirror presented by Friedrich Wilhelm I to Peter the Great in 1716&lt;/item&gt;&lt;item&gt; Amber room on a 2004 postage stamp&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ "A Brief History of the Amber Room". Smithsonian Magazine. Retrieved 2022-01-15.&lt;/item&gt;&lt;item&gt;^ a b c d e f g h i j k l m RIA Novosti 2010.&lt;/item&gt;&lt;item&gt;^ a b c d e f g h i j About 2014.&lt;/item&gt;&lt;item&gt;^ a b c d St. Petersburg 2001.&lt;/item&gt;&lt;item&gt;^ Jess Blumberg (July 21, 2007). "A Brief History of the Amber Room". Smithsonian Magazine. Retrieved April 24, 2016.&lt;/item&gt;&lt;item&gt;^ Marilyn Malara (April 23, 2016). "Historian claims to have struck gold at Nazi bunker". United Press International. Retrieved April 24, 2016.&lt;/item&gt;&lt;item&gt;^ a b c d e f g Smithsonian Institution 2014.&lt;/item&gt;&lt;item&gt;^ Wermusch 1991, p. 15.&lt;/item&gt;&lt;item&gt;^ a b c Torney 2009, pp. 185–186.&lt;/item&gt;&lt;item&gt;^ a b Toptenz 2014.&lt;/item&gt;&lt;item&gt;^ Los Angeles Times 1986.&lt;/item&gt;&lt;item&gt;^ Spiegel 2014.&lt;/item&gt;&lt;item&gt;^ a b Lucas 2000, pp. 25–28.&lt;/item&gt;&lt;item&gt;^ a b Seattle Times 1997.&lt;/item&gt;&lt;item&gt;^ a b Scotland on Sunday 2006.&lt;/item&gt;&lt;item&gt;^ a b Guardian 2004.&lt;/item&gt;&lt;item&gt;^ Denny 2007, p. 163.&lt;/item&gt;&lt;item&gt;^ Khatri 2012, pp. 90–95.&lt;/item&gt;&lt;item&gt;^ a b Scott-Clark &amp;amp; Levy 2004, pp. 309–330.&lt;/item&gt;&lt;item&gt;^ Scott-Clark &amp;amp; Levy 2004, pp. 322–323, 328.&lt;/item&gt;&lt;item&gt;^ Scott-Clark &amp;amp; Levy 2004, pp. 108–109, 325.&lt;/item&gt;&lt;item&gt;^ Scott-Clark &amp;amp; Levy 2004, p. 108-109, 325.&lt;/item&gt;&lt;item&gt;^ Scott-Clark &amp;amp; Levy 2004, pp. 301–313.&lt;/item&gt;&lt;item&gt;^ a b c d e ABC News 2004.&lt;/item&gt;&lt;item&gt;^ "Go Inside Search for Nazi-Looted '8th Wonder of World'". NBC News. Retrieved Feb 3, 2020.&lt;/item&gt;&lt;item&gt;^ "10 Rumored Locations Of The Lost Amber Room". listverse.com. Jul 28, 2018. Retrieved Feb 3, 2020.&lt;/item&gt;&lt;item&gt;^ "Treasure Hunters Claim They Have Found the Long Lost Nazi Amber Room". historycollection.co. Jun 23, 2017. Retrieved Feb 3, 2020.&lt;/item&gt;&lt;item&gt;^ "Nazi shipwreck found off Poland may solve Amber Room mystery". The Guardian. 1 October 2020. ISSN 0261-3077. Retrieved 28 February 2021.&lt;/item&gt;&lt;item&gt;^ Metcalfe, Tom (14 October 2020). "Nazi wreck may hold looted treasures bbfrom Russian palace's 'Amber Room'". www.livescience.com. Retrieved 16 February 2021.&lt;/item&gt;&lt;item&gt;^ a b c Russia Beyond the Headlines 2013.&lt;/item&gt;&lt;item&gt;^ Pravda 2003.&lt;/item&gt;&lt;item&gt;^ Telegraph 2004.&lt;/item&gt;&lt;item&gt;^ a b AskMen 2004.&lt;/item&gt;&lt;item&gt;^ "Ларец". hermitagemuseum.org (in Russian). Retrieved 18 April 2023.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Sources&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Printed&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Denny, Isabel (2007). The Fall of Hitler's Fortress City: the Battle for Königsberg, 1945. MBI Publishing. ISBN 978-1935149200.&lt;/item&gt;&lt;item&gt;Khatri, Vikas (2012). World Famous Treasures Lost and Found. Pustak Mahal Publishing. ISBN 978-8122312744.&lt;/item&gt;&lt;item&gt;Lucas, James (2000). Last Days of the Reich: The Collapse of Nazi Germany, May 1945. Cassell Publishing. ISBN 978-0304354481.&lt;/item&gt;&lt;item&gt;Scott-Clark, Catherine; Levy, Adrian (2004). The Amber Room: The Untold Story of the Greatest Hoax of the Twentieth Century. Atlantic Books. ISBN 1-84354-340-0.&lt;/item&gt;&lt;item&gt;Torney, Austin (2009). The Guide to the All-Embracing Realm of the Ultimate. Torney Publishing. ISBN 978-1448617272.&lt;/item&gt;&lt;item&gt;Wermusch, Günter (1991). Die Bernsteinzimmer Saga: Spuren, Irrwege, Rätsel (in German). Yale University. ISBN 978-3861530190.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Online&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;"A Brief History of the Amber Room". Smithsonian Institution. 2014. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"Amber Room Hunt Makes Lake the Tsar Attraction". Scotland on Sunday. 2006. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"Amber Room Remnants Found? — Discoveries Delight Russian Art Experts". Seattle Times. 1997. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"Catherine Palace". St. Petersburg. 2001. Retrieved 23 February 2015.&lt;/item&gt;&lt;item&gt;"Erich Koch, Regarded as One of Cruelest of Hitler's SS Men, Dies in Prison at 90". Los Angeles Times. 1986. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"Greed, Glory and a Tsar's Lost Treasure". The Guardian. 2004. Retrieved 22 February 2015.&lt;/item&gt;&lt;item&gt;"Mystery of the Amber Room Resurfaces". ABC News. 2004. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"Red Army, Not the Nazis, Destroyed Tsar's Amber Room". Telegraph. 2004. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"Resurrecting Königsberg: Russian City Looks to German Roots". Der Spiegel. 2014. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"Restoration of the Amber Chamber is Coming to an End". Pravda. 2007. Retrieved 26 June 2007.&lt;/item&gt;&lt;item&gt;"Russian Jeweller Recreates the Amber Room In His Workshop". Russia Beyond the Headlines. 2013. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"Top 10 Famous Pieces of Art Stolen by the Nazis". Toptenz. 2014. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"The Amber Room: History, Figures, Facts and Mysteries" (in Russian). RIA Novosti. 2010. Retrieved 25 February 2015.&lt;/item&gt;&lt;item&gt;"The Amber Room: Long Lost Treasure". AskMen. 2004. Retrieved 19 February 2015.&lt;/item&gt;&lt;item&gt;"The Amber Room". About. 2014. Archived from the original on 13 January 2017. Retrieved 19 February 2015.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;External links&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Ekaterininsky park&lt;/item&gt;&lt;item&gt;1701 establishments in the Holy Roman Empire&lt;/item&gt;&lt;item&gt;Buildings and structures completed in 1701&lt;/item&gt;&lt;item&gt;Amber&lt;/item&gt;&lt;item&gt;Art museums and galleries in Russia&lt;/item&gt;&lt;item&gt;Baroque architecture in Berlin&lt;/item&gt;&lt;item&gt;Baroque architecture in Russia&lt;/item&gt;&lt;item&gt;Frederick I of Prussia&lt;/item&gt;&lt;item&gt;Frederick William I of Prussia&lt;/item&gt;&lt;item&gt;Individual rooms&lt;/item&gt;&lt;item&gt;Lost works of art&lt;/item&gt;&lt;item&gt;Nazi-looted art&lt;/item&gt;&lt;item&gt;Peter the Great&lt;/item&gt;&lt;item&gt;Tourist attractions in Saint Petersburg&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Amber_Room"/><published>2025-10-03T13:19:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45463251</id><title>Webbol: A minimal static web server written in COBOL</title><updated>2025-10-03T15:09:33.375585+00:00</updated><content>&lt;doc fingerprint="11acbc59fd060f87"&gt;
  &lt;main&gt;
    &lt;p&gt;A minimal static web server written in COBOL using GnuCOBOL.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Serves static files from the current directory&lt;/item&gt;
      &lt;item&gt;Automatic MIME type detection for common file types&lt;/item&gt;
      &lt;item&gt;HTTP status codes: 200 (OK), 403 (Forbidden), 404 (Not Found)&lt;/item&gt;
      &lt;item&gt;Path traversal attack prevention&lt;/item&gt;
      &lt;item&gt;Clean request logging with full HTTP headers&lt;/item&gt;
      &lt;item&gt;Defaults to &lt;code&gt;index.html&lt;/code&gt;for root path requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GnuCOBOL (cobc) compiler&lt;/item&gt;
      &lt;item&gt;POSIX-compatible operating system (Linux, macOS, BSD)&lt;/item&gt;
      &lt;item&gt;make&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;code&gt;brew install gnucobol&lt;/code&gt;
    &lt;p&gt;Ubuntu/Debian:&lt;/p&gt;
    &lt;code&gt;sudo apt-get install gnucobol&lt;/code&gt;
    &lt;p&gt;Fedora/RHEL:&lt;/p&gt;
    &lt;code&gt;sudo dnf install gnucobol&lt;/code&gt;
    &lt;p&gt;Clone or download the repository, then compile:&lt;/p&gt;
    &lt;code&gt;make&lt;/code&gt;
    &lt;p&gt;This will compile all modules and create the &lt;code&gt;webserver&lt;/code&gt; executable.&lt;/p&gt;
    &lt;p&gt;To clean build artifacts:&lt;/p&gt;
    &lt;code&gt;make clean&lt;/code&gt;
    &lt;p&gt;Start the server from the directory you want to serve:&lt;/p&gt;
    &lt;code&gt;./webserver&lt;/code&gt;
    &lt;p&gt;The server will start on port 8080 and serve files from the current directory.&lt;/p&gt;
    &lt;code&gt;# Create a test HTML file
echo "&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello from COBOL!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;" &amp;gt; index.html

# Start the server
./webserver

# In another terminal, test it
curl http://localhost:8080/&lt;/code&gt;
    &lt;p&gt;Once running, you can access files via:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/&lt;/code&gt;- serves&lt;code&gt;index.html&lt;/code&gt;from the current directory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/filename.html&lt;/code&gt;- serves the specified file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/path/to/file.txt&lt;/code&gt;- serves files from subdirectories&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Press &lt;code&gt;Ctrl+C&lt;/code&gt; to stop the server.&lt;/p&gt;
    &lt;p&gt;To change the server port, edit &lt;code&gt;config.cpy&lt;/code&gt; and modify the &lt;code&gt;SERVER-PORT&lt;/code&gt; value:&lt;/p&gt;
    &lt;code&gt;01 SERVER-PORT          PIC 9(5) VALUE 8080.&lt;/code&gt;
    &lt;p&gt;Then recompile with &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;webbol/
├── Makefile              # Build configuration
├── README.md            # This file
├── config.cpy           # Server configuration
├── socket-defs.cpy      # Socket structure definitions
├── http-structs.cpy     # HTTP data structures
├── file-structs.cpy     # File handling structures
├── path-utils.cbl       # Path validation and sanitization
├── mime-types.cbl       # MIME type detection
├── file-ops.cbl         # File reading operations
├── http-handler.cbl     # HTTP request/response handling
└── webserver.cbl        # Main server program
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HTML: &lt;code&gt;text/html&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;CSS: &lt;code&gt;text/css&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JavaScript: &lt;code&gt;application/javascript&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JSON: &lt;code&gt;application/json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;XML: &lt;code&gt;application/xml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Plain text: &lt;code&gt;text/plain&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PNG: &lt;code&gt;image/png&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JPEG: &lt;code&gt;image/jpeg&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GIF: &lt;code&gt;image/gif&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;SVG: &lt;code&gt;image/svg+xml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ICO: &lt;code&gt;image/x-icon&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PDF: &lt;code&gt;application/pdf&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additional MIME types can be added by editing &lt;code&gt;mime-types.cbl&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Path traversal prevention: Blocks requests containing &lt;code&gt;..&lt;/code&gt;sequences&lt;/item&gt;
      &lt;item&gt;Directory access restriction: Only serves files from the current directory and subdirectories&lt;/item&gt;
      &lt;item&gt;Safe file handling: Validates all paths before file system access&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single-threaded: Handles one request at a time&lt;/item&gt;
      &lt;item&gt;No SSL/TLS support&lt;/item&gt;
      &lt;item&gt;Maximum file size: 64KB&lt;/item&gt;
      &lt;item&gt;Line sequential file organization only (text files)&lt;/item&gt;
      &lt;item&gt;No caching or compression&lt;/item&gt;
      &lt;item&gt;No range requests or partial content support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Port already in use:&lt;/p&gt;
    &lt;code&gt;Bind failed - check if port is in use
&lt;/code&gt;
    &lt;p&gt;Another process is using port 8080. Either stop that process or change the port in &lt;code&gt;config.cpy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Permission denied: Ensure the files you're trying to serve have read permissions and the current user can access them.&lt;/p&gt;
    &lt;p&gt;File not found (404): Verify the file exists in the current directory where the server is running. File paths are case-sensitive.&lt;/p&gt;
    &lt;p&gt;This project is released into the public domain. Use it however you'd like.&lt;/p&gt;
    &lt;p&gt;Built with GnuCOBOL, demonstrating that COBOL can still be used for modern systems programming tasks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/jmsdnns/webbol"/><published>2025-10-03T14:13:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45463642</id><title>Microsoft CTO says he wants to swap most AMD and Nvidia GPUs for homemade chips</title><updated>2025-10-03T15:09:32.999483+00:00</updated><content>&lt;doc fingerprint="37f0403644aec8d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft CTO says he wants to swap most AMD and Nvidia GPUs for homemade chips&lt;/head&gt;
    &lt;head rend="h2"&gt;Pivot will hinge on success of next-gen Maia accelerator&lt;/head&gt;
    &lt;p&gt;Microsoft buys a lot of GPUs from both Nvidia and AMD. But moving forward, Redmond's leaders want to shift the majority of its AI workloads from GPUs to its own homegrown accelerators.&lt;/p&gt;
    &lt;p&gt;The software titan is rather late to the custom silicon party. While Amazon and Google have been building custom CPUs and AI accelerators for years, Microsoft only revealed its Maia AI accelerators in late 2023.&lt;/p&gt;
    &lt;p&gt;Driving the transition is a focus on performance per dollar, which for a hyperscale cloud provider is arguably the only metric that really matters. Speaking during a fireside chat moderated by CNBC on Wednesday, Microsoft CTO Kevin Scott said that up to this point, Nvidia has offered the best price-performance, but he's willing to entertain anything in order to meet demand.&lt;/p&gt;
    &lt;p&gt;Going forward, Scott suggested Microsoft hopes to use its homegrown chips for the majority of its datacenter workloads.&lt;/p&gt;
    &lt;p&gt;When asked, "Is the longer term idea to have mainly Microsoft silicon in the data center?" Scott responded, "Yeah, absolutely."&lt;/p&gt;
    &lt;p&gt;Later, he told CNBC, "It's about the entire system design. It's the networks and cooling, and you want to be able to have the freedom to make decisions that you need to make in order to really optimize your compute for the workload."&lt;/p&gt;
    &lt;p&gt;With its first in-house AI accelerator, the Maia 100, Microsoft was able to free up GPU capacity by shifting OpenAI's GPT-3.5 to its own silicon back in 2023. However, with just 800 teraFLOPS of BF16 performance, 64GB of HBM2e, and 1.8TB/s of memory bandwidth, the chip fell well short of competing GPUs from Nvidia and AMD.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alibaba unveils $53B global AI plan – but it will need GPUs to back it up&lt;/item&gt;
      &lt;item&gt;Arm wrestles away 25% share of server market thanks to Nvidia's home-grown CPUs&lt;/item&gt;
      &lt;item&gt;SiPearl ships reference node design for Rhea1 high-spec Arm chip&lt;/item&gt;
      &lt;item&gt;Arm reckons it'll own 50% of the datacenter by year's end&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Microsoft is reportedly in the process of bringing a second-generation Maia accelerator to market next year that will no doubt offer more competitive compute, memory, and interconnect performance.&lt;/p&gt;
    &lt;p&gt;But while we may see a change in the mix of GPUs to AI ASICs in Microsoft data centers moving forward, they're unlikely to replace Nvidia and AMD's chips entirely.&lt;/p&gt;
    &lt;p&gt;Over the past few years, Google and Amazon have deployed tens of thousands of their TPUs and Trainium accelerators. While these chips have helped them secure some high-profile customer wins, Anthropic for example, these chips are more often used to accelerate the company's own in-house workloads.&lt;/p&gt;
    &lt;p&gt;As such, we continue to see large-scale Nvidia and AMD GPU deployments on these cloud platforms, in part because customers still want them.&lt;/p&gt;
    &lt;p&gt;It should be noted that AI accelerators aren't the only custom chips Microsoft has been working on. Redmond also has its own CPU called Cobalt and a whole host of platform security silicon designed to accelerate cryptography and safeguard key exchanges across its vast datacenter domains. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/02/microsoft_maia_dc/"/><published>2025-10-03T14:48:36+00:00</published></entry></feed>