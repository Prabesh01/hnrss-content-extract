<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-07T19:08:02.061935+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45502541</id><title>Qualcomm to Acquire Arduino</title><updated>2025-10-07T19:08:17.091299+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qualcomm.com/news/releases/2025/10/qualcomm-to-acquire-arduino-accelerating-developers--access-to-i"/><published>2025-10-07T13:00:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502543</id><title>Erlang ARM32 JIT is born</title><updated>2025-10-07T19:08:16.172566+00:00</updated><content>&lt;doc fingerprint="ea279a502acd05f8"&gt;
  &lt;main&gt;
    &lt;p&gt;A blog series recounting our adventures in the quest to port the BEAM JIT to the ARM32-bit architecture.&lt;/p&gt;
    &lt;p&gt;This work is made possible thanks to funding from the Erlang Ecosystem Foundation and the ongoing support of its Embedded Working Group.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Erlang ARM32 JIT is born!&lt;/head&gt;
    &lt;p&gt;This week we finally achieved our first milestone in developing the ARM32 JIT. We executed our first Erlang function through JITted ARM32 machine code!&lt;/p&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/arm32-jit/otp/RELEASE -progname erl -home /home
    ~/arm32-jit$ echo $?
    42&lt;/code&gt;
    &lt;p&gt;The BEAM successfully runs and terminates with error code 42! That 42 comes from an Erlang function, just-in-time compiled by our ARM32 JIT!&lt;/p&gt;
    &lt;p&gt;Announcement is done! All code is available at https://github.com/stritzinger/otp/tree/arm32-jit&lt;/p&gt;
    &lt;p&gt;Keep reading for a lot of interesting details!&lt;/p&gt;
    &lt;head rend="h2"&gt;The first piece of Erlang code&lt;/head&gt;
    &lt;code&gt;-module(hello).
-export([start/2]).

start(_BootMod, _BootArgs) -&amp;gt;
    halt(42, [{flush, false}]).&lt;/code&gt;
    &lt;p&gt;This is &lt;code&gt;hello.erl&lt;/code&gt; that contains a &lt;code&gt;start/2&lt;/code&gt; function. The function head mimics the &lt;code&gt;erl_init:start/2&lt;/code&gt; function, which is the entry point of the first Erlang process. We replaced &lt;code&gt;erl_init:start/2&lt;/code&gt; with &lt;code&gt;hello:start/2&lt;/code&gt; in the &lt;code&gt;erl_init.c&lt;/code&gt; module of the BEAM VM. This way, we forced the runtime to execute this Erlang function.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;hello:start/2&lt;/code&gt; is very simple as it just calls the &lt;code&gt;erlang:halt/2&lt;/code&gt;. This function is a BIF (Built-in Function) that executes C code, part of the BEAM VM. This code executes an ordered shutdown of the BEAM and allows us to customize the error code, in this case: &lt;code&gt;42&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;(Why &lt;code&gt;{flush, false}&lt;/code&gt;? At the time I am writing this, letting it be true causes a segmentation fault EHEH)&lt;/p&gt;
    &lt;p&gt;Obviously, we need to compile this Erlang module, but I will also generate the BEAM assembly so we can have a look at what we will have to deal with.&lt;/p&gt;
    &lt;code&gt;{module, hello}.  %% version = 0
{exports, [{module_info,0},{module_info,1},{start,2}]}.
{attributes, []}.
{labels, 7}.

{function, start, 2, 2}.
  {label,1}.
    {line,[{location,"erts/preloaded/src/hello.erl",74}]}.
    {func_info,{atom,hello},{atom,start},2}.
  {label,2}.
    {move,{literal,[{flush,false}]},{x,1}}.
    {move,{integer,42},{x,0}}.
    {line,[{location,"erts/preloaded/src/hello.erl",76}]}.
    {call_ext_only,2,{extfunc,erlang,halt,2}}.

{function, module_info, 0, 4}.
  {label,3}.
    {line,[]}.
    {func_info,{atom,hello},{atom,module_info},0}.
  {label,4}.
    {move,{atom,hello},{x,0}}.
    {call_ext_only,1,{extfunc,erlang,get_module_info,1}}.

{function, module_info, 1, 6}.
  {label,5}.
    {line,[]}.
    {func_info,{atom,hello},{atom,module_info},1}.
  {label,6}.
    {move,{x,0},{x,1}}.
    {move,{atom,hello},{x,0}}.
    {call_ext_only,2,{extfunc,erlang,get_module_info,2}}.&lt;/code&gt;
    &lt;p&gt;You can spot the start function and the two standard module_info functions that all Erlang modules have. We do not care much about those right now as we discovered that they are not executed and are not required to work, for now.&lt;/p&gt;
    &lt;p&gt;We can see that the core of the start function is just two &lt;code&gt;move&lt;/code&gt; operations and one &lt;code&gt;call_ext_only&lt;/code&gt;. But bear in mind that the BEAM loader will transmute these Generic BEAM Operations into Specific operations. More complexity will pop up!&lt;/p&gt;
    &lt;head rend="h2"&gt;Execution&lt;/head&gt;
    &lt;p&gt;We are using &lt;code&gt;qemu-arm&lt;/code&gt; to emulate &lt;code&gt;Arm32&lt;/code&gt; and we are directly using &lt;code&gt;beam.smp&lt;/code&gt; to run the BEAM.&lt;/p&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/vagrant/arm32-jit/otp/RELEASE -progname erl -home /home/vagrant&lt;/code&gt;
    &lt;head rend="h3"&gt;JIT initialization&lt;/head&gt;
    &lt;p&gt;At boot, the BEAM initializes the JIT if enabled. The JIT leverages the AsmJit library to emit all machine code instructions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Emission of all global shared fragments&lt;/head&gt;
    &lt;p&gt;There are 90+ code snippets that are shared among all modules. The JIT loads them one single time and sets up jumps to them in every other module. It is like a global library for all modules.&lt;/p&gt;
    &lt;p&gt;We skipped most of these because just the shared fragments involved in the &lt;code&gt;hello:start/2&lt;/code&gt; execution were needed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Emission of the erts_beamasm module&lt;/head&gt;
    &lt;p&gt;As part of the JIT initialization, &lt;code&gt;erts_beamasm&lt;/code&gt; is emitted. This module is an internal hardcoded module that exists only when BEAM is using the JIT. It holds 7 fundamental instructions used to manage the Erlang process executions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;run_process - The main process execution entry point&lt;/item&gt;
      &lt;item&gt;normal_exit - Normal process termination&lt;/item&gt;
      &lt;item&gt;continue_exit - Continue after exit handling&lt;/item&gt;
      &lt;item&gt;exception_trace - Exception tracing functionality&lt;/item&gt;
      &lt;item&gt;return_trace - Return value tracing&lt;/item&gt;
      &lt;item&gt;return_to_trace - Return to tracing state&lt;/item&gt;
      &lt;item&gt;call_trace_return - Call tracing return handling&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Preloaded modules&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;hello.erl&lt;/code&gt; module has been compiled and put as first and single Erlang module in the list of preloaded modules. Preloaded modules are Erlang fundamental modules that are always loaded by the BEAM before the first Erlang process can start. They implement, in Erlang, the core features of the Erlang Runtime System (ERTS). The OTP build scripts group all &lt;code&gt;ebin&lt;/code&gt; files into a single C header that is then linked into the executable. This makes the Erlang binaries available as a static C array in the BEAM source code. These are then loaded one by one after the BEAM VM is initialized.&lt;/p&gt;
    &lt;p&gt;Cool, let's nuke all these modules and leave just our &lt;code&gt;hello.erl&lt;/code&gt;. It does not need many BEAM instructions and we can easily verify that it executes. To do the substitution we just need to change this build variable in otp/erts/emulator/Makefile.in&lt;/p&gt;
    &lt;p&gt;We are running BEAMASM with &lt;code&gt;-JDdump true&lt;/code&gt; so &lt;code&gt;asmjit&lt;/code&gt; will dump all ARM32 assembly for each module! This is incredibly useful if monitored while executing with a debugger, as we can see the assembler being printed line by line by our code.&lt;/p&gt;
    &lt;code&gt;~/arm32-jit$ cat hello.asm 
L6:
.byte 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_1:
# i_func_info_IaaI
# hello:start/2
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x0B, 0xA4, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00
# aligned_label_Lt
start/2:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L9
    bl L11
L9:
# i_test_yield
    adr r2, start/2
    subs r9, r9, 1
    b.le L13
# i_move_sd
    ldr r12, [L14]
    str r12, [r4, 68]
# i_move_sd
    movw r12, 687
    str r12, [r4, 64]
# line_I
# allocate_tt
# call_light_bif_be
L15:
    ldr r3, [L16]
    movw r1, 10188
    movt r1, 16432
    adr r2, L15
# BIF: erlang:halt/2
    sub r12, r7, 4
    cmp r10, r12
    b.ls L17
    udf 48879
L17:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L18
    udf 57005
L18:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_3:
# i_func_info_IaaI
# hello:module_info/0
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x4B, 0x6B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
# aligned_label_Lt
module_info/0:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L23
    bl L11
L23:
# i_test_yield
    adr r2, module_info/0
    subs r9, r9, 1
    b.le L13
# i_move_sd
    movw r12, 20235
    str r12, [r4, 64]
# allocate_tt
# call_light_bif_be
L24:
    ldr r3, [L25]
    movw r1, 4772
    movt r1, 16425
    adr r2, L24
# BIF: erlang:get_module_info/1
    sub r12, r7, 4
    cmp r10, r12
    b.ls L26
    udf 48879
L26:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L27
    udf 57005
L27:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_5:
# i_func_info_IaaI
# hello:module_info/1
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x4B, 0x6B, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00
# aligned_label_Lt
module_info/1:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L28
    bl L11
L28:
# i_test_yield
    adr r2, module_info/1
    subs r9, r9, 1
    b.le L13
# i_move_sd
    ldr r12, [r4, 64]
    str r12, [r4, 68]
# i_move_sd
    movw r12, 20235
    str r12, [r4, 64]
# allocate_tt
# call_light_bif_be
L29:
    ldr r3, [L30]
    movw r1, 4868
    movt r1, 16425
    adr r2, L29
# BIF: erlang:get_module_info/2
    sub r12, r7, 4
    cmp r10, r12
    b.ls L31
    udf 48879
L31:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L32
    udf 57005
L32:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# int_code_end
L33:
    movw r0, 18576
    movt r0, 16480
    blx L22
L13:
L12:
    movw r12, 1968
    movt r12, 14656
    blx r12
L22:
L21:
    movw r12, 29192
    movt r12, 16399
    blx r12
L11:
L10:
    movw r12, 1752
    movt r12, 14656
    blx r12
L20:
L19:
    movw r12, 680
    movt r12, 14656
    blx r12
L8:
L7:
    movw r12, 1824
    movt r12, 14656
    blx r12
# Begin stub section
L14:
.xword 0x000000007FFFFFFF
L16:
.xword 0x000000007FFFFFFF
L25:
.xword 0x000000007FFFFFFF
L30:
.xword 0x000000007FFFFFFF
# End stub section
L34:
.section .rodata {#1}
md5:
.byte 0x6D, 0xC4, 0x1E, 0xF1, 0x13, 0x1E, 0xBF, 0xF2, 0x4B, 0xF5, 0xC0, 0x41, 0x57, 0x86, 0xDF, 0xD5
.section .text {#0}
; CODE_SIZE: 632&lt;/code&gt;
    &lt;p&gt;Bear in mind, this assembler is not what hello should look like. We are missing a lot of things.&lt;/p&gt;
    &lt;p&gt;You can spot many sequences like:&lt;/p&gt;
    &lt;code&gt;    movw r0, 64676
    movt r0, 16480
    blx L22 # &amp;lt;---- branch to NYI&lt;/code&gt;
    &lt;p&gt;This is a call to &lt;code&gt;nyi&lt;/code&gt; (Not Yet Implemented) function and the argument loaded to R0 is the pointer to a string that contains the name of the BEAM instruction that should have been emitted instead. You can spot many of these since we are only emitting the code to reach halt. Everything after that is not important now as halt will never return!&lt;/p&gt;
    &lt;p&gt;There are many more comments we could make around all the details in this assembler dump, but let's move on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Jumping into Jitted code!&lt;/head&gt;
    &lt;p&gt;Later in the BEAM initialization the first Erlang process will be allocated and started.&lt;/p&gt;
    &lt;p&gt;We swap the module and function with hello in erts/emulator/beam/erl_init.c&lt;/p&gt;
    &lt;code&gt;    erl_spawn_system_process(&amp;amp;parent, am_hello, am_start, args, &amp;amp;so);&lt;/code&gt;
    &lt;p&gt;One BEAM scheduler thread will jump to the &lt;code&gt;process_main&lt;/code&gt; function. You can find it here in the source code. This is emitted by our JIT and is the first emitted code that will run.&lt;/p&gt;
    &lt;p&gt;Here we need to handle the Erlang processes scheduling by calling BEAM routines that implement the algorithms of Erlang concurrency, like &lt;code&gt;erts_schedule&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;erts_schedule&lt;/code&gt; will return the pointer to the &lt;code&gt;Process&lt;/code&gt; C structure that holds all information about the process that is going to execute. We then load all necessary data inside registers and then we branch to the exact point where the program execution stopped.&lt;/p&gt;
    &lt;head rend="h3"&gt;The first Erlang function call&lt;/head&gt;
    &lt;p&gt;In this case we are calling &lt;code&gt;hello:start/2&lt;/code&gt; so the first instruction to execute is &lt;code&gt;apply_only&lt;/code&gt; that does a few things but ends up calling the C &lt;code&gt;apply&lt;/code&gt; routine.&lt;/p&gt;
    &lt;p&gt;The routine processes the Module-Function-Arity information to get the address where the function code resides in memory.&lt;/p&gt;
    &lt;p&gt;What follows is the Erlang function prologue. You can see it in the assembler code section above. For example, all functions have these instructions in their prologue:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;i_breakpoint_trampoline: handle breakpoints for the &lt;code&gt;debugger&lt;/code&gt;app&lt;/item&gt;
      &lt;item&gt;i_test_yield: checks if the function should yield and go back to the scheduler&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We have minimal or partial implementations of these since we do not really need them. We have to emit them though, as the C++ generated loader functions from the BEAM are expanding the Erlang function call Operation into a more specific and complex function prologue sequence.&lt;/p&gt;
    &lt;p&gt;After that, we added support for the &lt;code&gt;call_light_bif&lt;/code&gt; operation that precedes the call to the halt_2 BIF routine. This implementation is also minimal.&lt;/p&gt;
    &lt;p&gt;Question for later: did you notice that we put a &lt;code&gt;42&lt;/code&gt; as a number in the code? Numeric constants are printed as decimals in the dump, but we cannot spot any 42!?&lt;/p&gt;
    &lt;p&gt;After the call, we see two other operations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;dealloc&lt;/item&gt;
      &lt;item&gt;return&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are just calls to NYI as we will never reach this code! So for now, we can skip them...&lt;/p&gt;
    &lt;head rend="h3"&gt;Let's roll the JIT!&lt;/head&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/arm32-jit/otp/RELEASE -progname erl -home /home
    ~/arm32-jit$&lt;/code&gt;
    &lt;p&gt;Impressive, the program returns immediately without even saying "Hi" ... and without Segmentation Fault!!&lt;/p&gt;
    &lt;p&gt;But let's check the program return code!&lt;/p&gt;
    &lt;code&gt;~/arm32-jit$ echo $?
42
&lt;/code&gt;
    &lt;p&gt;We can safely say that number is not there by accident! This is a great achievement as from now on we will be able to incrementally add Erlang instructions.&lt;/p&gt;
    &lt;p&gt;Every Erlang line we add will trigger new Opcodes. By emitting them and running the code we will have immediate feedback on everything.&lt;/p&gt;
    &lt;p&gt;The next goal now is to complete the &lt;code&gt;hello&lt;/code&gt; module to host all possible beam instructions!&lt;/p&gt;
    &lt;head rend="h4"&gt;Hey where is 42???&lt;/head&gt;
    &lt;p&gt;One interesting thing I spotted looking at the assembly: You cannot find the number &lt;code&gt;42&lt;/code&gt; in there. Or actually, you can, it is just hidden in plain sight. To understand you need to know how we are using ARM32 registers.&lt;/p&gt;
    &lt;p&gt;In particular the register &lt;code&gt;r4&lt;/code&gt;, a callee-saved register. We are using it to store the pointer to the &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt; struct. The &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt; contains the X register array. When a function is called, X registers are used to store the arguments of the call.&lt;/p&gt;
    &lt;p&gt;This becomes more obvious if we compare the Erlang assembly to the Arm32 assembly.&lt;/p&gt;
    &lt;code&gt;# i_move_sd                       &amp;lt;---- {move,{literal,[{flush,false}]},{x,1}}. % List at X[1]
    ldr r12, [L14]
    str r12, [r4, 68]
# i_move_sd                       &amp;lt;---- {move,{integer,42},{x,0}}. % 42 at X[0]
    movw r12, 687 
    str r12, [r4, 64]
# line_I
# allocate_tt
# call_light_bif_be
L15:
    ldr r3, [L16]
    movw r1, 10188
    movt r1, 16432
    adr r2, L15
# BIF: erlang:halt/2
# ...&lt;/code&gt;
    &lt;p&gt;42 is stored at &lt;code&gt;r4&lt;/code&gt;+64.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;r4: pointer to the &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt;struct&lt;/item&gt;
      &lt;item&gt;64: base offset from the beginning of the struct to the beginning of the &lt;code&gt;x_reg_array&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The list is stored at &lt;code&gt;r4&lt;/code&gt;+68.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;68: is the base offset + the size of one &lt;code&gt;Eterm&lt;/code&gt;(4 bytes on ARM32)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But why in assembly do we see 687 and not 42?&lt;/p&gt;
    &lt;p&gt;Converting both numbers to hex we get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;42 -&amp;gt; 2A&lt;/item&gt;
      &lt;item&gt;687 -&amp;gt; 2AF !!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yep, this is an example of a Tagged Value. If we consult the BEAM book we can learn about the Tagging Scheme:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;00 11 Pid&lt;/item&gt;
      &lt;item&gt;01 11 Port&lt;/item&gt;
      &lt;item&gt;10 11 Immediate 2&lt;/item&gt;
      &lt;item&gt;11 11 Small integer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;42 is tagged with &lt;code&gt;1111&lt;/code&gt; at the low end. So the BEAM can quickly recognize during a pattern match that this Erlang Term is a Small Integer!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.grisp.org/blog/posts/2025-10-07-jit-arm32.3"/><published>2025-10-07T13:00:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502748</id><title>3M May Escape Toxic Chemical, PFAS Manufacturing Legacy</title><updated>2025-10-07T19:08:15.844019+00:00</updated><content>&lt;doc fingerprint="2e10915d0a385ff3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;3M Might Just Escape Its Toxic Chemical Legacy&lt;/head&gt;
    &lt;p&gt;Decades of selling PFAS left the iconic American manufacturer mired in legal liabilities. A new CEO is hoping to spark a turnaround.&lt;/p&gt;
    &lt;p&gt;The Command strip is one of those quintessential 3M products. Released in 1996, it was simple yet revolutionary, strong enough to hold items without stripping paint when removed. Soon it was fastening framed photos, bathroom towels and outdoor decorations around the world.&lt;/p&gt;
    &lt;p&gt;The adhesive that made the strip possible was invented by a 3M scientist in the late 1980s. The exact science is a closely guarded secret, but it can resist gravity while somehow pulling away clean when tugged as instructed. In hindsight the market for such an invention is obvious, and yet it was almost lost to corporate bureaucracy. 3M shelved the project at one point in the ’90s and revived it only after impassioned pleas from a determined product development executive. His persistence paid off: Within three years of its debut, the Command strip was turning a $10 million profit. 3M Co. now sells more than 200 varieties, bringing in $500 million a year.&lt;/p&gt;
    &lt;p&gt;As Command strips turned into a product empire, though, they also became captive to 3M’s unique form of industrial sprawl. Historically the company has organized its factories by material science and manufacturing processes, rather than by product—think chemicals in one facility, adhesives in another and packaging somewhere else, for example, regardless of their ultimate end use. This way, the thinking went, with each innovation, 3M could wring more value from existing machinery and underlying technologies. But every new product and geographic market also brought with it new costs and complexities, resulting in a labyrinthine factory network. This approach meant that Command strip production took place at multiple sites, sometimes hundreds of miles apart. Add in more steps for distribution, and the journey to Walmart shelves of what is, at base, just a particularly good sticky plastic hook looks pretty convoluted.&lt;/p&gt;
    &lt;p&gt;3M is one of the most sparkling brand names in US business history, known for advancing material science to the point that the first astronauts who walked on the moon were equipped with boots made from its synthetic rubber. If America wanted it, 3M could invent and produce it, building its fortunes on products as innocuous as Post-it notes and as dangerous as chemicals lining nonstick pans. From its headquarters in St. Paul, the company also became known in the business world for its Minnesota-nice corporate culture, a contrast to the ruthlessness championed by the likes of longtime General Electric Co. Chief Executive Officer Jack Welch. 3M’s executives were unfailingly polite, its senior scientists were allowed to devote 15% of their time to research projects of their choosing, and many employees worked at 3M for their entire careers, long after staying in one job forever ceased to be fashionable.&lt;/p&gt;
    &lt;p&gt;But problems were bubbling under the surface, ones that went well beyond struggles with successful lines like the Command strip. First and foremost, those pan-lining chemicals—specifically certain types of perfluoroalkyl and polyfluoroalkyl substances, or PFAS, which were also used in products such as Scotchgard fabric protector and firefighting foam—have been found to increase the risk of cancer, decrease fertility and suppress the immune system, including weakening response to vaccines. Those findings have fueled a vast array of lawsuits that analysts estimate could end up costing 3M in the neighborhood of $20 billion, including ones it has already settled. Adding to those legal woes, the company spent years facing down major multidistrict litigation claiming it knowingly sold defective earplugs that left US military service members with hearing loss and tinnitus. (In agreements to resolve both of those lawsuits, 3M hasn’t admitted to any liability or wrongdoing.)&lt;/p&gt;
    &lt;p&gt;As 3M dealt with these issues, its traditional strengths were atrophying. New product introductions slowed to a trickle, and sales growth was lackluster. If America wanted it, 3M grew less and less likely to be making it. By late 2021, as most businesses were regaining their footing after the pandemic, it appeared to many observers almost as if 3M had forgotten how to be a manufacturing company. Investors stopped treating it like one, viewing it more through the lens of liability and risk than of factory output and projected revenue.&lt;/p&gt;
    &lt;p&gt;Even 3M’s Minnesota-nice reputation has taken some hits. Senior executives privately complain that in recent years, beneath all the Midwestern cheer, the working environment has been marked by tone deafness, overbearing bureaucracy and resistance to change. Decisions were slow, fixes pushed by management felt blunt and grating, and accountability was hard to come by. Worker-safety protocols also lapsed, with the incident rate at its factories becoming significantly higher than at North American peers such as Corteva, Dow and GE.&lt;/p&gt;
    &lt;p&gt;Effectively, 3M became “a case study of everything not to do over a very long period of time,” says Scott Davis, an analyst at Melius Research LLC. It became a “broken company.” And when that happens, Davis says, “you want change, you crave change.”&lt;/p&gt;
    &lt;p&gt;With almost $100 billion of market value wiped out from the peak in 2018 to the start of 2024, 3M last year tapped Bill Brown, an outsider who’d spent much of his career in the defense industry, to turn things around. He hosted his first earnings call about three months into the job. Analysts and investors expected him to offer the usual platitudes about hosting listening sessions and considering all options. Instead he talked about the Command strip—how it demonstrated perfectly that the company had gotten too big, too slow, too convoluted. Brown said he had a plan to tackle all that. 3M’s stock price shot up more than 20% by close, its best one-day rally in more than four decades on the public markets.&lt;/p&gt;
    &lt;p&gt;The company declined to comment on the reporting in this story, instead saying in a statement that it’s “focused on creating shareholder value and positioning 3M for success by driving profitable growth, embedding a culture of operational excellence across the enterprise.”&lt;/p&gt;
    &lt;p&gt;It will help Brown’s turnaround plan that 3M’s liabilities have become less open-ended: A major settlement with water utilities over PFAS pollution and a deal to resolve the earplugs litigation, both agreed to in 2023, have reset investor expectations for the overall bill. The Trump administration also appears set to take a lighter touch to environmental regulation, providing relief to companies like 3M.&lt;/p&gt;
    &lt;p&gt;But even the optimists concede that this is no easy fix. In the months since Brown took the reins, he’s been relying on the same playbook that Dave Cote used to restore Honeywell International Inc. in the early 2000s, when it faced a raft of asbestos lawsuits and the aftereffects of several dubious megamergers, and that Larry Culp deployed this decade to resuscitate GE when it was overwhelmed by debt and a $15 billion mess in a legacy insurance business. Brown’s plan starts with running 3M better. Get that right, and the really big problems—the amorphous liabilities, the cultural corrosion, the public-image crisis—won’t seem quite as insurmountable.&lt;/p&gt;
    &lt;p&gt;Minnesota Mining &amp;amp; Manufacturing started in 1902 as a venture to mine corundum, which is harder than all other minerals save for diamonds and an ideal ingredient in sandpaper. But when its founders discovered they were in fact extracting a different low-grade material, they decided to get out of mining and into making the sandpaper instead, using raw materials sourced elsewhere.&lt;/p&gt;
    &lt;p&gt;Similarly, 3M didn’t discover PFAS. These synthetic chemicals trace back to the Manhattan Project, which required scientists to invent a way to extract uranium to make an atomic bomb. A chemical engineering professor at Pennsylvania State College named Joseph H. Simons developed a process to create almost unbreakable carbon fluorine bonds by passing raw fluorine, a highly volatile gas, through a carbon arc. He later developed a method for practical production of fluorocarbons, a process 3M employed to pioneer the use of PFAS in consumer and industrial products starting in the 1950s. The company eventually became a major producer both for its own products and for customers such as DuPont de Nemours Inc., which used it in Teflon.&lt;/p&gt;
    &lt;p&gt;Near-indestructibility is useful for everything from developing nuclear weapons to repelling grease and water. But these chemicals take years to break down, making them highly problematic in the environment and the human body. For a long time, 3M billed manufactured PFAS and its PFAS-infused products as perfectly safe, even as internal documents and studies unearthed through lawsuits showed the company was aware of the chemicals’ toxicity at least as far back as the 1970s.&lt;/p&gt;
    &lt;p&gt;In 1998, Richard Purdy, an ecological toxicologist at 3M, conducted a study that found PFAS even in the blood of bald eagle nestlings, which primarily eat fish and inhabit remote areas. He quit the next year and sent a copy of his resignation letter to the US Environmental Protection Agency, saying the company had failed to properly communicate to customers and regulators the results of research showing how prevalent the chemicals had become in people and the environment. In 2000, 3M announced it would voluntarily stop making the forms of the chemicals that had the most research documenting their hazards, even as it maintained the chemicals were safe and didn’t pose a long-term health issue. In 2006, the company reached a $1.5 million settlement with the EPA over reporting violations related to PFAS.&lt;/p&gt;
    &lt;p&gt;More research about the health consequences of PFAS followed, including an influential 2012 study of children born in the remote Faroe Islands that found the chemicals in bloodstreams at levels comparable to the US population and indicated a resulting weakened immune response to vaccines. As the production of certain kinds of PFAS declined, the concentration of the chemicals in people’s bloodstreams fell sharply, according to research from the US Centers for Disease Control and Prevention and others. But after decades of use, the legal challenges, and the public fallout, were just beginning.&lt;/p&gt;
    &lt;p&gt;The tipping point came in 2018. That January, The Devil We Know—an investigative documentary examining a West Virginia community’s fight against a plant, then owned by DuPont, that made Teflon—premiered at the Sundance Film Festival. A few weeks after that, 3M agreed, without admitting wrongdoing, to pay $850 million to settle a lawsuit brought by Minnesota over claims the company had poisoned drinking water in its own backyard. And in June the CDC, drawing on scientific advances that allowed PFAS detection at much lower levels, published a draft report warning that the chemicals can cause health problems at significantly lower exposure thresholds than what the EPA had claimed at the time was safe.&lt;/p&gt;
    &lt;p&gt;The following month, Michael Roman took over as 3M’s CEO. A 30-year company veteran, Roman had overseen businesses in the US, Europe and Asia and had served as 3M’s chief operating officer. He was the embodiment of its culture of politesse, but this ultimately wouldn’t help him much with the storm that was brewing. PFAS claims came to span challenges from state attorneys general, water utilities, people with personal injury and property complaints, and foreign governments such as those of Belgium and the Netherlands.&lt;/p&gt;
    &lt;p&gt;Adding to the pile, 3M was also being sued over an earplug business it had acquired through the 2008 purchase of Aearo Technologies. In July 2018, the same month Roman took over, 3M had agreed, without admitting wrongdoing, to a $9.1 million settlement with the US Department of Justice to resolve allegations that it had sold earplugs to the US military without disclosing defects. Cases brought by veterans claiming hearing damage mushroomed after the settlement and dragged on for years.&lt;/p&gt;
    &lt;p&gt;With potential liabilities stewing outside its factories, 3M was simultaneously dealing with issues inside them. Having long prided itself on being a desirable and safe place to work, the company was seeing an uptick in the number of safety violations at its factories. From 2019 to 2024, 3M received at least 27 “serious” initial citations, which are based on worksite investigations, from the Occupational Safety and Health Administration—more than any other company among a group of its North American industrial and health-care technology peers, according to a Bloomberg Businessweek analysis.&lt;/p&gt;
    &lt;p&gt;3M also got one repeat admonishment and notice of three willful violations, meaning it knowingly failed to comply with a legal requirement or acted with “plain indifference to employee safety.” (3M and other companies in the dataset have contested many of the citations, and have in some cases succeeded in getting them downgraded or dismissed as part of settlements. In response to requests for comment from Businessweek, Carlisle said that its injury rate is “very low” compared with the industry average and that it seeks to “continuously improve” its safety standards, and GE said it took immediate action to report the incident that led to its citations and to prevent a reoccurrence.)&lt;/p&gt;
    &lt;p&gt;Two of 3M’s willful violations were linked to inadequate safeguards for a plastic extrusion machine that required employees to thread material through by hand. Trisha Jones, who operated the machine at the company’s plant in Prairie du Chien, Wisconsin, died in May 2023 after getting caught in the large rollers and suffering traumatic head injuries. “They’re not managing these facilities well,” says David Michaels, who ran OSHA from 2009 to 2017 and is now a professor of public health at George Washington University. “That’s a sign of safety management not being implemented.”&lt;/p&gt;
    &lt;p&gt;The safety incidents jarred with the benevolent culture workers had come to expect. Peter Gibbons, formerly in charge of overseeing supply chains across the company, would regularly contend that the biggest cause of injuries at factories was employees falling down stairs while using their phones, so much so that “Use the Handrail” became a running joke among staff, according to people with the company who, like others interviewed for this piece, asked not to be named discussing private interactions or information. (Gibbons didn’t respond to requests for comment.)&lt;/p&gt;
    &lt;p&gt;People close to 3M say the rise in safety incidents reflected years of underinvestment in factories to save costs, as well as a decision by Roman in 2020 to make oversight of manufacturing operations the responsibility of managers in charge of the business lines, rather than ones who were on-site at plants. The shift was part of a 2020 reorganization, dubbed “Advanced 3M,” that was intended to make the company more responsive to customer needs.&lt;/p&gt;
    &lt;p&gt;It was one of many sweeping gestures Roman undertook during his six years at the helm. Another was Polaris, a suite of digital tools introduced in 2021 to give customers a more Amazon-like experience when placing and tracking orders; the initiative ended up costing more than planned and hasn’t been fully implemented since going online, the people close to the company say. Roman also pursued several cost-cutting efforts that cost hundreds of workers their jobs while barely denting 3M’s financial results. In fact its profit margins went sideways, in contrast to peers that were improving manufacturing processes, raising prices and adding market share.&lt;/p&gt;
    &lt;p&gt;During his tenure, Roman developed a reputation internally for being thin-skinned. In 2020, at the height of the pandemic, he hosted virtual town halls as part of a planned series. But, according to the people close to the company, the series was abruptly canceled after he learned that anonymous commenters had criticized his presentations for reasons that included a lack of detail and transparency about Covid-19 strategy and protocols. (Roman didn’t respond to requests for comment.)&lt;/p&gt;
    &lt;p&gt;Roman’s biggest challenge may have been that an outsize amount of his time, rather than being spent running an industrial company, was spent managing the PFAS problem, even though manufactured PFAS represented a fraction of 3M’s business. In 2022 the Biden administration proposed to designate certain PFAS as hazardous materials under the federal Superfund law, and the EPA declared that virtually no amount of the chemicals is safe in drinking water. Not long after, Roman announced 3M would cease all remaining production of PFAS and work to discontinue their use in its products by the end of 2025. In internal deliberations, people familiar with the conversations say, executives had pointed out to Roman that PFAS are integral to the production of semiconductors, electric-vehicle batteries and weapons systems—the very types of goods the US government under President Joe Biden was seeking to make more of at home. At the very least, some argued, 3M should sell its PFAS manufacturing facilities rather than simply shutting them down. Roman didn’t want to hear it: 3M was done with PFAS, and that was that.&lt;/p&gt;
    &lt;p&gt;Getting all the PFAS out of its products has proved complicated, though. In a testament to how prevalent the chemicals have become, 3M has cautioned investors that they may continue to be present beyond 2025 in certain products that contain components manufactured by third parties, including lithium-ion batteries, printed circuit boards and some seals and gaskets. 3M said this is because approved substitutes that meet regulatory and industry standards may not be available.&lt;/p&gt;
    &lt;p&gt;In the year after Roman’s announcement, 3M finally started putting some of its legal challenges behind it. The company announced a deal in June 2023 to pay as much as $12.5 billion over 13 years to test and treat city water supplies for PFAS, resolving current and future claims by municipal utilities over pollution. A few months later it announced it had reached a separate $6 billion agreement to resolve claims in the earplug litigation.&lt;/p&gt;
    &lt;p&gt;3M continued to chip away at its PFAS lawsuits after Brown replaced Roman in 2024. This May it agreed to pay New Jersey as much as $450 million to resolve claims related to PFAS pollution at the 3M-supplied Chambers Works plant, as well as general complaints about natural resource damage from the chemicals.&lt;/p&gt;
    &lt;p&gt;As settlements have accrued, estimates for how much the company might ultimately have to pay have gone down. Andrew Obin, an analyst at Bank of America Corp., says 3M might be on the hook for $11.5 billion in settlements related to US suits over damages to natural resources alone. But he says the settlement math in New Jersey—which has particularly acute levels of PFAS pollution—implies his estimate for this category of claims should be materially lower, closer to $6 billion.&lt;/p&gt;
    &lt;p&gt;The regulatory landscape is also changing in ways that might help 3M. The shift began with the US Supreme Court’s decision, handed down last June 28, to overturn the Chevron doctrine, the legal principle that empowered executive branch agencies to interpret and enforce regulations. The ruling gave momentum to lawsuits filed by chemical industry and manufacturing groups as well as water utility associations, challenging the Biden administration’s limits on PFAS in drinking water as arbitrary and financially impossible. This May, President Donald Trump’s EPA announced it was delaying its compliance deadline for removal of the two best-known types of PFAS and was rescinding and reconsidering the limits on four other categories. And earlier this month, in a major shift, the EPA asked the court to vacate the drinking water rule for the four other categories of PFAS, concurring with the water utility plaintiffs that aspects of the process by which the standards had been established were unlawful. The agency has delayed a rule, too, that would have required PFAS manufacturers to file reports about environmental and health effects from 2011 to 2022. (The EPA said in a statement that it’s “committed to protecting public health by addressing PFAS in drinking water while following the law and ensuring that regulatory compliance is achievable for drinking water systems.”)&lt;/p&gt;
    &lt;p&gt;The US Chamber of Commerce is also leading a lawsuit against the EPA over the designation of certain types of PFAS as hazardous materials under the Superfund law—which it contends could force companies to collectively pay as much as $17.4 billion in cleanup costs just for nonfederal sites that have been identified as a priority. After repeatedly getting the proceedings postponed, the Trump administration announced this month that it would retain the Superfund designation for the two best-known kinds of PFAS, while calling upon Congress to address concerns about entities getting stuck with cleanup bills for chemicals they didn’t themselves manufacture. The litigation itself continues.&lt;/p&gt;
    &lt;p&gt;Obin, the Bank of America analyst, pegs 3M’s liability for cleaning up Superfund sites at as much as $9 billion but says that bill could also end up being lower as legal challenges drag out the policy’s implementation. And he points out that the company could recover as much as 25% of its total PFAS liabilities through insurance payouts. “In conversations with investors, the narrative has changed,” Obin says. “Maybe three years ago, people would say, ‘Why are your estimates so low?’”&lt;/p&gt;
    &lt;p&gt;3M’s other big remaining undefined PFAS liability is personal injury lawsuits. These, too, may turn out to be less financially crippling than analysts previously feared. In major multidistrict litigation, plaintiffs are arguing that they got cancer by being exposed to water contaminated with PFAS from firefighting foam and are seeking damages from 3M and other companies. The defendants’ lawyers have sought to exclude expert testimony for the plaintiffs on the link between PFAS and cancer, contending that they relied on research findings that aren’t statistically significant at the relevant exposure levels for the claimants. A bellwether trial had been set to come before a federal district court in South Carolina in October. It’s since been postponed “until such a time as the court deems appropriate,” to allow both sides to sift through a large number of unfiled claims and determine which of these should be included in the proceeding. The judge overseeing the case has urged the two sides to settle, and this latest development may make such an agreement more likely, according to Barclays Plc analyst Julian Mitchell. As it stands, Mitchell estimates that all of 3M’s outstanding PFAS liabilities, including personal injury and natural resources claims, could be as much as $11 billion, down from his $16 billion calculation immediately after an agreement to settle the water lawsuits was announced in 2023.&lt;/p&gt;
    &lt;p&gt;Companies have recovered from liability disasters before. Davis, the Melius Research analyst, recalls a peer predicting in the early 2000s that Honeywell would go bankrupt because of asbestos claims, as well as a short seller who infamously claimed GE was on a path to financial collapse because of festering liabilities in a legacy insurance business. Neither forecast came true. The critical thing for 3M, Davis says, is improving cash flow so its liabilities seem less like an existential wrecking ball than an expense. It needs to improve to the point that “cash-flow growth is so strong that people say, ‘You know what? They’ve got the liabilities, and they can do buybacks and M&amp;amp;A, and operate as a real entity.’” The key for Honeywell and GE, he points out, was finding the right CEO to clean up their operations.&lt;/p&gt;
    &lt;p&gt;3M’s choice for that was Brown. Before signing on, he had a successful career overseeing defense contractor Harris Corp., now known as L3Harris Technologies Inc. after he shepherded a merger with L3 Technologies in 2019. Former colleagues say they thought that, after Brown stepped down in 2022, he might land a cushy private equity gig, not join a struggling conglomerate facing giant legal liabilities. Asked earlier this year at a JPMorgan Chase &amp;amp; Co. conference why he took the 3M job given the PFAS baggage, Brown joked that it wasn’t the -15F winter weather at headquarters in Minnesota. Rather, he said, PFAS had been so all-consuming for 3M that it had an opportunity to simply pay more attention to everything else—as long as the liabilities didn’t end up being worse than investors’ downtrodden expectations, then 3M’s stock price could climb much higher. It was, he added, “a great opportunity to engage with this great iconic company called 3M and try to help make it great again.”&lt;/p&gt;
    &lt;p&gt;Filings show that Brown is either 62 or 63 (the company declined to specify which). Known as an exercise fanatic, he’s trying to bring a comparable rigor of routine to 3M. The plan he’s revealed for fixing it isn’t all that complex. For starters, he cleared out much of the upper management under his predecessor, including replacing the chief financial officer, the head of investor relations and the top supply chain manager. Brown also lured in Amazon.com Inc. executive Wendy Bauer to take over 3M’s transportation and electronics division.&lt;/p&gt;
    &lt;p&gt;He’s vowed, too, to reboot 3M’s vaunted “innovation machine,” to cut costs and to drag the company’s manufacturing operations into the modern era, including making factories safer. “Our performance in safety has not been where we should be,” he said at the conference. “Our focus is around zero injuries, zero spills, zero incidents.”&lt;/p&gt;
    &lt;p&gt;Brown has also been pushing to improve how the company satisfies orders—when he took over, more than 10% of 3M deliveries were showing up late or incomplete—and how it makes each product in its sprawling portfolio. He’s suggested the company might reduce its 25,000-strong network of suppliers and eventually its empire of 110 factories, with its manufacturing equipment currently being used at only about 60% of capacity. Every week he and his top lieutenants review 3M’s business lines and factories, hunting for logjams and inefficiencies that cause deliveries to fall short of expectations.&lt;/p&gt;
    &lt;p&gt;Those who worked with Brown at L3Harris praise his leadership style. An engineer by training, he’s obsessed with pursuing excellence in all facets of a company’s operations, according to Jay Malave, a former L3Harris executive who was recently named Boeing Co.’s CFO. Rahul Ghai, CFO of GE Aerospace and a former Brown deputy himself, says, “What makes him successful is his tremendous attention to detail.” He adds: “This is the right guy for the job.” Both say that even though Brown can be demanding, he delivers marching orders with positivity.&lt;/p&gt;
    &lt;p&gt;At a Bank of America conference in May, Brown said 3M’s culture needs to encourage more individual accountability if his other improvements are to stick. When he joined the company, only about 10 people had performance-based stock compensation agreements. Now 1,500 people are paid this way. And everyone is being encouraged to act with urgency. The mantra, he said, is “Get it done tonight, not tomorrow. If it can be done in the next minute, do it in the next minute.”&lt;/p&gt;
    &lt;p&gt;Despite recent market volatility related to the global trade war started by Trump’s tariffs, 3M shares are still worth about 60% more than they were when Brown started last May. That’s more than double the gains for a broader group of industrial companies on the S&amp;amp;P 500 over that time period. But the stock is still down about 30% from its peak in early 2018, before the PFAS challenges came to a head.&lt;/p&gt;
    &lt;p&gt;Brown has been at pains to emphasize to investors that this turnaround isn’t going to be quick. Success would be resolving 3M’s legal cases without any jarring surprises, revving its innovation apparatus back up to the point that investors are convinced it can increase revenue, and improving the efficiency of its supply chain and manufacturing network.&lt;/p&gt;
    &lt;p&gt;In a testament to how hard that last challenge will be, Brown isn’t the first 3M CEO to cite the Command strip as an example of what needs to be fixed: George Buckley, who ran the company from 2005 to 2012, used to describe 3M’s disjointed assembly processes for the strip and other products as “hairballs.” At the time he started trying to untangle these knots, one part of the production process for Command hooks began with adhesives made at a 3M plant in Missouri, which were then shipped to Indiana to be applied to polyethylene foam. The foam then went to Minnesota, where the 3M logo was added and the strips were cut to size. Finally the tabs went to Wisconsin to be packaged along with a plastic hook. According to company executives interviewed by the Wall Street Journal in 2012, under Buckley’s oversight, 3M consolidated these particular steps in the Command strip production at one plant.&lt;/p&gt;
    &lt;p&gt;It was an improvement, sure, though it didn’t resolve the problem. Today, the end-to-end process still spans a handful of factories.&lt;/p&gt;
    &lt;p&gt;“You will never fix it, because that was how the company was built,” Obin says of 3M’s manufacturing labyrinth. But perhaps that’s OK. He points out that it wasn’t an issue during periods in the company’s history when revenue and profits were growing. “It just needs to be run properly.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bloomberg.com/features/2025-3m-pfas-toxic-legacy-turnaround/"/><published>2025-10-07T13:22:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502784</id><title>Tcl-Lang Showcase</title><updated>2025-10-07T19:08:15.594935+00:00</updated><content>&lt;doc fingerprint="598b77957e6a418d"&gt;
  &lt;main&gt;&lt;p&gt;Canvas3d&lt;/p&gt;Canvas3d wiki page&lt;p&gt;HP-15 Simulation&lt;/p&gt;HP-15 Simulation wiki page&lt;p&gt;By clicking on the image, an interactive demonstration of the Tcl/Tk application is launched using CloudTk. Over 100 Tcl/Tk applications listed from this wiki are demonstrated here . To view the Tcl/Tk Widget Demonstration, go to the "Playground" from the menu above and then select "Demos" in the "Tcl-Playground" - Console menu.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wiki.tcl-lang.org/page/Showcase"/><published>2025-10-07T13:25:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45503726</id><title>No account? No Windows 11, Microsoft says as another loophole snaps shut</title><updated>2025-10-07T19:08:15.307795+00:00</updated><content>&lt;doc fingerprint="3f986a36e76660d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;No account? No Windows 11, Microsoft says as another loophole snaps shut&lt;/head&gt;
    &lt;head rend="h2"&gt;Workaround sent to the big OOBE in the sky with latest Insider builds&lt;/head&gt;
    &lt;p&gt;Microsoft is closing a popular loophole that allowed users to install Windows 11 without a Microsoft account.&lt;/p&gt;
    &lt;p&gt;The change has appeared in recent Insider builds of Windows 11, indicating it is likely to be included in the production version soon.&lt;/p&gt;
    &lt;p&gt;Microsoft refers to these loopholes as "known mechanisms" and is talking about local commands in this instance. You can learn all about these in our piece for getting Windows 11 installed with a local account, but suffice to say &lt;code&gt;start ms-cxh:localonly&lt;/code&gt; is no more.&lt;/p&gt;
    &lt;p&gt;"While these mechanisms were often used to bypass Microsoft account setup, they also inadvertently skip critical setup screens, potentially causing users to exit OOBE with a device that is not fully configured for use," Microsoft said.&lt;/p&gt;
    &lt;p&gt;"Users will need to complete OOBE with internet and a Microsoft account, to ensure [the] device is set up correctly."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 10 refuses to go gentle into that good night&lt;/item&gt;
      &lt;item&gt;Hundreds of orgs urge Microsoft: don't kill off free Windows 10 updates&lt;/item&gt;
      &lt;item&gt;Windows 11 25H2 is mostly 24H2 with bits bolted on or ripped out&lt;/item&gt;
      &lt;item&gt;Healthcare lags in Windows 11 upgrades – and lives may depend on it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As far as Redmond is concerned, this is all for the user's own good. It is also important to note that managed devices are not directly affected, just hardware that users want to get running with Windows 11 without having to deal with a Microsoft Account during setup.&lt;/p&gt;
    &lt;p&gt;The change is part of Microsoft's ongoing game of Whac-A-Mole with users trying to find ways of avoiding its online services. In March, it removed the &lt;code&gt;bypassnro.cmd&lt;/code&gt; script that allowed users to get through the Windows 11 setup without needing an internet connection. That time, Microsoft said the change was to "enhance security and user experience of Windows 11."&lt;/p&gt;
    &lt;p&gt;There remain a number of ways to avoid the Microsoft account requirement during setup, including setting up an unattended installation, but these are more complicated. It is also clear that Microsoft is determined to continue closing loopholes where it can.&lt;/p&gt;
    &lt;p&gt;It is getting increasingly difficult to use Windows 11 on an unmanaged device without a Microsoft account. Users who don't want to sign up should perhaps consider whether it's time to look at an alternative operating system instead. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/07/windows_11_local_account_loophole/"/><published>2025-10-07T14:45:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45503882</id><title>Swiss glaciers have shrunk by a quarter since 2015, study says</title><updated>2025-10-07T19:08:15.021441+00:00</updated><content>&lt;doc fingerprint="3e43618bca203f15"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Swiss glaciers have shrunk by a quarter since 2015, study says&lt;/head&gt;
    &lt;p&gt;Switzerland’s glaciers have lost 24 percent of their volume over the past decade, researchers said Wednesday, warning that accelerated melting in 2025 brought ice loss close to record levels. Scientists warn that Switzerland’s glaciers could nearly vanish by the end of this century without stronger actions to counter global warming.&lt;/p&gt;
    &lt;p&gt;Switzerland's glaciers, which are disproportionately impacted by climate change, have lost a quarter of their volume in the past decade alone, a study warned Wednesday, heightening concerns over accelerating melting.&lt;/p&gt;
    &lt;p&gt;In 2025, glacial melting in the Alpine nation was once again "enormous", the Glacier Monitoring in Switzerland (GLAMOS) network said, adding that it was close to the record set in 2022.&lt;/p&gt;
    &lt;p&gt;A winter with little snow combined with summer heatwaves in June and August saw Switzerland's glaciers lose three percent of their volume.&lt;/p&gt;
    &lt;p&gt;That marks the fourth-largest level of shrinkage since measurements began, trailing only 2022, 2023 and 2003, according to GLAMOS's annual report.&lt;/p&gt;
    &lt;p&gt;Glaciers across the Alps have been retreating for more than a century.&lt;/p&gt;
    &lt;p&gt;But in recent decades, the process has sped up as the climate warms, driven by humanity's burning of fossil fuels.&lt;/p&gt;
    &lt;p&gt;"Since about 20 years, all glaciers in Switzerland are losing ice, and the rate of this loss is accelerating," GLAMOS chief Matthias Huss told AFP.&lt;/p&gt;
    &lt;p&gt;Between 2015 and 2025 alone, the glaciers shed 24 percent of their volume, Wednesday's report said, compared to 10 percent between 1990 and 2000.&lt;/p&gt;
    &lt;head rend="h2"&gt;Melting away&lt;/head&gt;
    &lt;p&gt;GLAMOS researchers did extensive measurements at around 20 reference glaciers in September, and extrapolated the findings to Switzerland's 1,400 glaciers.&lt;/p&gt;
    &lt;p&gt;Europe's Alpine region has been hard-hit by climate change, with warming in Switzerland progressing at twice the pace of the global average, according to the Swiss Federal Office of Meteorology and Climatology.&lt;/p&gt;
    &lt;p&gt;Other Alpine countries are also seeing glaciers retreat, and researchers highlight that those in Switzerland -- whose mountain peaks are higher than in neighbouring Austria -- may have a better chance of surviving the increasingly hot summers.&lt;/p&gt;
    &lt;p&gt;Even so, scientists warn that Switzerland's glaciers could all but disappear by the end of this century without more action to rein in global warming.&lt;/p&gt;
    &lt;p&gt;"We can't avoid the glacier melting overall," GLAMOS head Huss said, but "we can slow it down... with globally coordinated climate action".&lt;/p&gt;
    &lt;p&gt;If carbon dioxide emissions "are brought to zero within 30 years... we could still save about one-third of the Swiss glaciers", Huss added.&lt;/p&gt;
    &lt;p&gt;Since the early 1970s, more than 1,100 Swiss glaciers have disappeared completely, according to GLAMOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Destabilising' mountains&lt;/head&gt;
    &lt;p&gt;Overlooking the Rhone Glacier, near Gletsch village, Huss said the giant ice mass had lost more than 100 metres (330 feet) in height in the last 20 years.&lt;/p&gt;
    &lt;p&gt;"It's really a devastation of the ice," he said.&lt;/p&gt;
    &lt;p&gt;Argentine tourist Wincho Ponte, 29, agreed.&lt;/p&gt;
    &lt;p&gt;It was "really sad that it's melting so quickly", Pointe said.&lt;/p&gt;
    &lt;p&gt;Water reserves have meanwhile been dwindling as the glaciers retreat, causing increasing problems in the summer months.&lt;/p&gt;
    &lt;p&gt;Huss cautioned that this could hit "water availability not only up here in the mountains but also all the way down to the Mediterranean Sea".&lt;/p&gt;
    &lt;p&gt;"The continuous diminishing of glaciers also contributes to the destabilising of mountains", he warned, pointing to the Swiss village of Blatten, which was wiped out by a dramatic glacier collapse in May.&lt;/p&gt;
    &lt;p&gt;GLAMOS determined that Swiss glacier volume will total 45.1 cubic kilometres (10.8 cubic miles) at the end of this year -- or 30 km3 less than in 2000.&lt;/p&gt;
    &lt;p&gt;At present, the surface area of Swiss glaciers covers 755 square kilometres -- a decline of 30 percent over the past 25 years.&lt;/p&gt;
    &lt;p&gt;This year, Switzerland's second-hottest June on record contributed to snow melting rapidly, even at the highest altitudes.&lt;/p&gt;
    &lt;p&gt;August brought a fresh heatwave, pushing the freezing line as high as 5,000 metres above sea level -- well above the peak of western Europe's highest mountain, Mont Blanc.&lt;/p&gt;
    &lt;p&gt;Only a rather cool and damp July "provided some relief and prevented an even worse outcome", GLAMOS said, with a few cold fronts resulting in individual days with fresh snow at higher altitudes.&lt;/p&gt;
    &lt;p&gt;The overall summer melt this year was therefore only 15 percent above the 2010-2020 average -- its lowest level in the past four years.&lt;/p&gt;
    &lt;p&gt;(FRANCE 24 with AFP)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.france24.com/en/live-news/20251001-swiss-glaciers-shrank-by-a-quarter-in-past-decade-study"/><published>2025-10-07T14:56:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45504127</id><title>Show HN: MARS – Personal AI robot for builders (&lt; $2k)</title><updated>2025-10-07T19:08:14.897954+00:00</updated><content>&lt;doc fingerprint="f1df9a7b68a781cd"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey, we’re Axel and Vignesh, cofounders of Innate (&lt;/p&gt;https://www.innate.bot/&lt;p&gt;). We just launched MARS, a general-purpose robot with an open onboard agentic OS built on top of ROS2.&lt;/p&gt;&lt;p&gt;Overview: https://youtu.be/GEOMYDXv6pE&lt;/p&gt;&lt;p&gt;Control demo: https://youtu.be/_Cw5fGa8i3s&lt;/p&gt;&lt;p&gt;Videos of autonomous use-cases: https://docs.innate.bot/welcome/mars-example-use-cases&lt;/p&gt;&lt;p&gt;Quickstart: https://docs.innate.bot/welcome/mars-quick-start.&lt;/p&gt;&lt;p&gt;Our last thread: https://news.ycombinator.com/item?id=42451707&lt;/p&gt;&lt;p&gt;When we started we felt there is currently no good affordable general-purpose that anyone can build on. There’s no lack of demand: hugging face’s SO-100 and LeKiwi are pretty clear successes already; but the hardware is unreliable, the software experience is barebone and keeps changing, and you often need to buy hidden extras to make them work (starting with a computer with a good gpu). The Turtlebots were good, but are getting outdated.&lt;/p&gt;&lt;p&gt;The open-source hobbyist movement lacks really good platforms to build on, and we wanted something robust and accessible. MARS is our attempt at making a first intuitive AI robot for everyone.&lt;/p&gt;&lt;p&gt;What it is:&lt;/p&gt;&lt;p&gt;- It comes assembled and calibrated&lt;/p&gt;&lt;p&gt;- Has onboard compute with a jetson orin nano 8gb&lt;/p&gt;&lt;p&gt;- a 5DoF arm with a wrist camera&lt;/p&gt;&lt;p&gt;- Sensors: RGBD wide-angle cam, 2D LiDAR, speakers&lt;/p&gt;&lt;p&gt;- Control via a dedicated app and a leader arm that plugs in iPhone and Android&lt;/p&gt;&lt;p&gt;- 2 additional USB ports + GPIO pins for extra sensors or effectors.&lt;/p&gt;&lt;p&gt;- And our novel SDK called BASIC that allows to run it like an AI agent with VLAs.&lt;/p&gt;&lt;p&gt;It boots in a minute, can be controlled via phone, programmable in depth with a PC, and the onboard agent lets it see, talk, plan, and act in real-time.&lt;/p&gt;&lt;p&gt;Our SDK BASIC allows to create “behaviors” (our name for programs) ranging from a simple hello world to a very complex long-horizon task involving reasoning, planning, navigation and manipulation. You can create skills that behaviors can run autonomously by training the arm or writing code tools, like for an AI agent.&lt;/p&gt;&lt;p&gt;You can also call the ROS2 topics to control the robot at a low-level. And anything created on top of this SDK can be easily shared with anyone else by just sharing the files.&lt;/p&gt;&lt;p&gt;This is intended for hobbyist builders and education, and we would love to have your feedback!&lt;/p&gt;&lt;p&gt;p.s. If you want to try it, there’s a temporary code HACKERNEWS-INNATE-MARS that lowers the price to $1,799.&lt;/p&gt;&lt;p&gt;p.p.s The hardware and software will be open-sourced too, if some of you want to contribute or help us prepare it properly feel free to join our discord at https://discord.gg/YvqQbGKH&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45504127"/><published>2025-10-07T15:11:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45504388</id><title>Launch HN: LlamaFarm (YC W22) – Open-source framework for distributed AI</title><updated>2025-10-07T19:08:14.234708+00:00</updated><content>&lt;doc fingerprint="ed1fa4511bbd11f2"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Build powerful AI locally, extend anywhere.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;LlamaFarm is an open-source framework for building retrieval-augmented and agentic AI applications. It ships with opinionated defaults (Ollama for local models, Chroma for vector storage) while staying 100% extendable—swap in vLLM, remote OpenAI-compatible hosts, new parsers, or custom stores without rewriting your app.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local-first developer experience with a single CLI (&lt;code&gt;lf&lt;/code&gt;) that manages projects, datasets, and chat sessions.&lt;/item&gt;
      &lt;item&gt;Production-ready architecture that mirrors server endpoints and enforces schema-based configuration.&lt;/item&gt;
      &lt;item&gt;Composable RAG pipelines you can tailor through YAML, not bespoke code.&lt;/item&gt;
      &lt;item&gt;Extendable everything: runtimes, embedders, databases, extractors, and CLI tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;📺 Video demo (90 seconds): https://youtu.be/W7MHGyN0MdQ&lt;/p&gt;
    &lt;p&gt;Prerequisites:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install the CLI&lt;/p&gt;
        &lt;p&gt;macOS / Linux&lt;/p&gt;
        &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh | bash&lt;/code&gt;
        &lt;p&gt;Windows (via winget)&lt;/p&gt;
        &lt;code&gt;winget install LlamaFarm.CLI&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Adjust Ollama context window&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Open the Ollama app, go to Settings → Advanced, and set the context window to match production (e.g., 100K tokens).&lt;/item&gt;
          &lt;item&gt;Larger context windows improve RAG answers when long documents are ingested.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create and run a project&lt;/p&gt;
        &lt;quote&gt;lf init my-project # Generates llamafarm.yaml using the server template lf start # Spins up Docker services &amp;amp; opens the dev chat UI&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Start an interactive project chat or send a one-off message&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Interactive project chat (auto-detects namespace/project from llamafarm.yaml)
lf chat

# One-off message
lf chat "Hello, LlamaFarm!"&lt;/code&gt;
    &lt;p&gt;Need the full walkthrough with dataset ingestion and troubleshooting tips? Jump to the Quickstart guide.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Prefer building from source? Clone the repo and follow the steps in Development &amp;amp; Testing.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Run services manually (without Docker auto-start):&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/llama-farm/llamafarm.git
cd llamafarm

# Install Nx globally and bootstrap the workspace
npm install -g nx
nx init --useDotNxInstallation --interactive=false

# Option 1: start both server and RAG worker with one command
nx dev

# Option 2: start services in separate terminals
# Terminal 1
nx start rag
# Terminal 2
nx start server&lt;/code&gt;
    &lt;p&gt;Open another terminal to run &lt;code&gt;lf&lt;/code&gt; commands (installed or built from source). This is equivalent to what &lt;code&gt;lf start&lt;/code&gt; orchestrates automatically.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Own your stack – Run small local models today and swap to hosted vLLM, Together, or custom APIs tomorrow by changing &lt;code&gt;llamafarm.yaml&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Battle-tested RAG – Configure parsers, extractors, embedding strategies, and databases without touching orchestration code.&lt;/item&gt;
      &lt;item&gt;Config over code – Every project is defined by YAML schemas that are validated at runtime and easy to version control.&lt;/item&gt;
      &lt;item&gt;Friendly CLI – &lt;code&gt;lf&lt;/code&gt;handles project bootstrapping, dataset lifecycle, RAG queries, and non-interactive chats.&lt;/item&gt;
      &lt;item&gt;Built to extend – Add a new provider or vector store by registering a backend and regenerating schema types.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Task&lt;/cell&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Initialize a project&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf init my-project&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Creates &lt;code&gt;llamafarm.yaml&lt;/code&gt; from server template.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Start dev stack + chat TUI&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf start&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Spins up server, rag worker, monitors Ollama/vLLM.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Interactive project chat&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Opens TUI using project from &lt;code&gt;llamafarm.yaml&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Send single prompt&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat "Explain retrieval augmented generation"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Uses RAG by default; add &lt;code&gt;--no-rag&lt;/code&gt; for pure LLM.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Preview REST call&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat --curl "What models are configured?"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Prints sanitized &lt;code&gt;curl&lt;/code&gt; command.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Create dataset&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets create -s pdf_ingest -b main_db research-notes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Validates strategy/database against project config.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Upload files&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets upload research-notes ./docs/*.pdf&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Supports globs and directories.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Process dataset&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets process research-notes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Streams heartbeat dots during long processing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Semantic query&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf rag query --database main_db "What did the 2024 FDA letters require?"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Use &lt;code&gt;--filter&lt;/code&gt;, &lt;code&gt;--include-metadata&lt;/code&gt;, etc.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;See the CLI reference for full command details and troubleshooting advice.&lt;/p&gt;
    &lt;p&gt;LlamaFarm provides a comprehensive REST API (compatible with OpenAI's format) for integrating with your applications. The API runs at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Chat Completions (OpenAI-compatible)&lt;/p&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "What are the FDA requirements?"}
    ],
    "stream": false,
    "rag_enabled": true,
    "database": "main_db"
  }'&lt;/code&gt;
    &lt;p&gt;RAG Query&lt;/p&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "clinical trial requirements",
    "database": "main_db",
    "top_k": 5
  }'&lt;/code&gt;
    &lt;p&gt;Dataset Management&lt;/p&gt;
    &lt;code&gt;# Upload file
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/data \
  -F "file=@document.pdf"

# Process dataset
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/process&lt;/code&gt;
    &lt;p&gt;Check your &lt;code&gt;llamafarm.yaml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;name: my-project        # Your project name
namespace: my-org       # Your namespace&lt;/code&gt;
    &lt;p&gt;Or inspect the file system: &lt;code&gt;~/.llamafarm/projects/{namespace}/{project}/&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;See the complete API Reference for all endpoints, request/response formats, Python/TypeScript clients, and examples.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;llamafarm.yaml&lt;/code&gt; is the source of truth for each project. The schema enforces required fields and documents every extension point.&lt;/p&gt;
    &lt;code&gt;version: v1
name: fda-assistant
namespace: default

runtime:
  provider: openai                   # "openai" for any OpenAI-compatible host, "ollama" for local Ollama
  model: qwen2.5:7b
  base_url: http://localhost:8000/v1 # Point to vLLM, Together, etc.
  api_key: sk-local-placeholder
  instructor_mode: tools             # Optional: json, md_json, tools, etc.

prompts:
  - role: system
    content: &amp;gt;-
      You are an FDA specialist. Answer using short paragraphs and cite document titles when available.

rag:
  databases:
    - name: main_db
      type: ChromaStore
      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: filtered_search
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text:latest
      retrieval_strategies:
        - name: filtered_search
          type: MetadataFilteredStrategy
          config:
            top_k: 5
  data_processing_strategies:
    - name: pdf_ingest
      parsers:
        - type: PDFParser_LlamaIndex
          config:
            chunk_size: 1500
            chunk_overlap: 200
      extractors:
        - type: HeadingExtractor
        - type: ContentStatisticsExtractor

datasets:
  - name: research-notes
    data_processing_strategy: pdf_ingest
    database: main_db&lt;/code&gt;
    &lt;p&gt;Configuration reference: Configuration Guide • Extending LlamaFarm&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swap runtimes by pointing to any OpenAI-compatible endpoint (vLLM, Mistral, Anyscale). Update &lt;code&gt;runtime.provider&lt;/code&gt;,&lt;code&gt;base_url&lt;/code&gt;, and&lt;code&gt;api_key&lt;/code&gt;; regenerate schema types if you add a new provider enum.&lt;/item&gt;
      &lt;item&gt;Bring your own vector store by implementing a store backend, adding it to &lt;code&gt;rag/schema.yaml&lt;/code&gt;, and updating the server service registry.&lt;/item&gt;
      &lt;item&gt;Add parsers/extractors to support new file formats or metadata pipelines. Register implementations and extend the schema definitions.&lt;/item&gt;
      &lt;item&gt;Extend the CLI with new Cobra commands under &lt;code&gt;cli/cmd&lt;/code&gt;; the docs include guidance on adding dataset utilities or project tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check the Extending guide for step-by-step instructions.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
        &lt;cell role="head"&gt;What it Shows&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FDA Letters Assistant&lt;/cell&gt;
        &lt;cell&gt;Multi-document PDF ingestion, RAG queries, reference-style prompts&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;examples/fda_rag/&lt;/code&gt; &amp;amp; Docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Raleigh UDO Planning Helper&lt;/cell&gt;
        &lt;cell&gt;Large ordinance ingestion, long-running processing tips, geospatial queries&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;examples/gov_rag/&lt;/code&gt; &amp;amp; Docs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;lf datasets&lt;/code&gt; and &lt;code&gt;lf rag query&lt;/code&gt; commands from each example folder to reproduce the flows demonstrated in the docs.&lt;/p&gt;
    &lt;code&gt;# Python server + RAG tests
cd server
uv sync
uv run --group test python -m pytest

# CLI tests
cd ../cli
go test ./...

# RAG tooling smoke tests
cd ../rag
uv sync
uv run python cli.py test

# Docs build (ensures navigation/link integrity)
cd ..
nx build docs&lt;/code&gt;
    &lt;p&gt;Linting: &lt;code&gt;uv run ruff check --fix .&lt;/code&gt; (Python), &lt;code&gt;go fmt ./...&lt;/code&gt; and &lt;code&gt;go vet ./...&lt;/code&gt; (Go).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord – chat with the team, share feedback, find collaborators.&lt;/item&gt;
      &lt;item&gt;GitHub Issues – bug reports and feature requests.&lt;/item&gt;
      &lt;item&gt;Discussions – ideas, RFCs, roadmap proposals.&lt;/item&gt;
      &lt;item&gt;Contributing Guide – code style, testing expectations, doc updates, schema regeneration steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Want to add a new provider, parser, or example? Start a discussion or open a draft PR—we love extensions!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Licensed under the Apache 2.0 License.&lt;/item&gt;
      &lt;item&gt;Built by the LlamaFarm community and inspired by the broader open-source AI ecosystem. See CREDITS for detailed acknowledgments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build locally. Deploy anywhere. Own your AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/llama-farm/llamafarm"/><published>2025-10-07T15:30:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45504470</id><title>IKEA Catalogs 1951-2021</title><updated>2025-10-07T19:08:12.898611+00:00</updated><content>&lt;doc fingerprint="71c3125f48ed4455"&gt;
  &lt;main&gt;
    &lt;p&gt;Good question! We know that a lot of people are curious about what the IKEA catalogue has looked like through the ages. The catalogue has always reflected the age and its views on interior design and everyday living, especially in Sweden, but in recent decades also internationally. The catalogue was in print for 70 years, and by digitising all the catalogues we could make them available to everybody. Making the story of IKEA available to as many people as possible is our main task at IKEA Museum. So we hope that the catalogues will bring some joy and nostalgia, and maybe even a few surprises.&lt;/p&gt;
    &lt;head rend="h1"&gt;IKEA catalogue&lt;/head&gt;
    &lt;p&gt;For over 70 years, the IKEA catalogue was produced in Ãlmhult, constantly growing in number, scope and distribution. From the 1950s when Ingvar Kamprad wrote most of the texts himself, via the poppy, somewhat radical 1970s and all the way into the scaled-down 2000s â the IKEA catalogue always captured the spirit of the time. The 2021 IKEA catalogue was the very last one printed on paper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1950s&lt;/item&gt;
      &lt;item&gt;1960s&lt;/item&gt;
      &lt;item&gt;1970s&lt;/item&gt;
      &lt;item&gt;1980s&lt;/item&gt;
      &lt;item&gt;1990s&lt;/item&gt;
      &lt;item&gt;2000s&lt;/item&gt;
      &lt;item&gt;2010s&lt;/item&gt;
      &lt;item&gt;2020s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just like the perception of the home, the catalogue has changed dramatically since 1951, when it was first published. Look in the older catalogues and youâll be amazed at what you find. In fact, youâll probably even have a giggle or two. In the 1950s and 1960s, there are rarely any people in the pictures, and never any children. But in the 1970s there are children playing all over the home, you can see adults smoking and even the occasional political poster on the wall. Browse on to the 1980s IKEA catalogues and the trends have changed again, with shiny fabrics and other fancy materials. In the 1990s homes become more scaled-down and clearly inspired by a Scandinavian tradition. In this way, the IKEA catalogues are a kind of time capsule for you to travel in. And who knows? When we look back at the most recent catalogues in 10 or 20 yearsâ time, weâll probably shake our heads and give a sigh.&lt;/p&gt;
    &lt;p&gt;IKEA Museum decided to start with the Swedish catalogue as it has been around the longest. In the future, we hope to be able to digitise catalogues from more countries in more languages.&lt;/p&gt;
    &lt;p&gt;No. The IKEA catalogue has always only shown a selection of whatâs available in the stores. The catalogues from the 1970s and onwards show around 30â50 per cent of the entire range. The products that are not featured are generally smaller ones in textiles, decorations and lighting. Temporary collections are rarely included either. But the farther back you go, the higher a percentage of the range can be found in the catalogue.&lt;/p&gt;
    &lt;p&gt;Yes, but the older a product is, the harder it may be to find information about it. If you have a specific question about a product, weâre happy to help you out if we can. But 70 years is a long time, so we canât promise anything. While youâre waiting for our response you can always browse through the catalogues â the product texts that are there are quite detailed. You can search in the catalogues by product name and product type. There are also various stories about different products on our site, and more are constantly being added.&lt;lb/&gt; Browse through stories about IKEA products from 7 decades. &lt;/p&gt;
    &lt;p&gt;IKEA was founded in the 1940s, so why are you showing no catalogues from before 1951?&lt;lb/&gt; The first catalogue did not come out until 1951. Before that, IKEA was a mail order company that didnât sell furniture, but pens, clocks, electric razors, wallets and bags. At that time, the range was only presented in a small mail order brochure called ikÃ©a-nytt (literally ikÃ©a news). Sometimes it was distributed as a supplement in farming paper Jordbrukarnas FÃ¶reningsblad, which reached hundreds of thousands of people in the Swedish countryside. From autumn 1948 Ingvar Kamprad started including furniture in the range, and things quickly grew from there. In the 1950 ikÃ©a-nytt, as many as six of the 18 pages featured furniture. And when you look at the 1951 catalogue, youâll see that there are no more pens and wallets. Ingvar Kamprad was now truly focusing on home furnishing, and shelving the rest.&lt;lb/&gt; Browse through all issues of ikÃ©a-nytt.&lt;/p&gt;
    &lt;p&gt;Not really. We do have a few copies of each yearâs IKEA catalogue in our archives, which weâre saving for posterity. They should be handled as little as possible to keep them in good condition, so weâve made the catalogues available digitally, both online and on monitors at IKEA Museum. You can browse through those as much as you like!&lt;/p&gt;
    &lt;p&gt;Yes you can. The easiest way to share the catalogues is to click on the arrow at the bottom left corner for each catalogue, or in the left-hand menu once youâve started browsing through. This will copy a link which you can share on a website or social media. If you would like to download and publish on your own digital platform, you can share a maximum of three complete digital catalogues. Donât forget to state the copyright details, “Â© Inter IKEA Systems B.V.”, the catalogue year, and the link /en/explore/ikea-catalogue/ so that anyone interested can find out more. You may not publish the digital catalogues for commercial purposes.&lt;/p&gt;
    &lt;p&gt;Absolutely! You can share up to 30 images from the catalogues on your own digital platform, such as a blog, on Instagram or similar (as long as itâs not for commercial purposes). Donât forget to state the copyright details, “Â© Inter IKEA Systems B.V.”, the catalogue year, and the link /en/explore/ikea-catalogue/ so that anyone interested can find out more.&lt;/p&gt;
    &lt;p&gt;Yes! You can find all press material, including images, information about current exhibitions and much more, in the IKEA Museum press room.&lt;/p&gt;
    &lt;p&gt;At the moment we have a good amount of catalogues in all languages at the museum, and do not need any more. Having said that, please contact us anyway if youâve been collecting catalogues for several decades, or if you have any other material you think might be of interest to IKEA Museum.&lt;/p&gt;
    &lt;p&gt;Unfortunately not. We sometimes wish we did, as we handle quite a lot of old products that may need putting together and taking apart.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ikeamuseum.com/en/explore/ikea-catalogue/"/><published>2025-10-07T15:35:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45504973</id><title>Show HN: Timelinize – Privately organize your own data from everywhere, locally</title><updated>2025-10-07T19:08:12.583640+00:00</updated><content>&lt;doc fingerprint="e64842e6eda1dcc1"&gt;
  &lt;main&gt;
    &lt;p&gt;Timelinize ("time-lynn-eyes") is an open source personal archival suite, designed for modern family history. It organizes all your data onto a single, unified timeline on your own computer.&lt;/p&gt;
    &lt;p&gt;Photos, videos, text messages, locations, chats, social media, and more. Timelinize unifies it all.&lt;/p&gt;
    &lt;p&gt;By adding all your data, Timelinize documents your family's life with more detail and privacy, and gives you a more complete view of your story, than standard photo library and journaling apps.&lt;/p&gt;
    &lt;p&gt;Most apps store your data "in the cloud" and out of your control. What if you lost access to your Google/Apple/Facebook accounts, or your phone? By bringing that data home to your own computer, Timelinize preserves a richer story than any one app or service can do alone.&lt;/p&gt;
    &lt;p&gt;Timelinize isn't a replacement for the apps and services you already use, so you don't need to disrupt your way of life. Instead, it "sits behind" what you already use to become the permanent private archive of your working copy from:&lt;/p&gt;
    &lt;p&gt;With several projections for your data, it's easy to keep moments alive that would otherwise be forgotten, rotting on a hard drive in your closet... or in a bigcorp's cloud.&lt;/p&gt;
    &lt;p&gt;The timeline view semantically groups all your data into a single linear layout. Easily see what occurred on a specific day in the order it happened.&lt;/p&gt;
    &lt;p&gt;Visualize your data on a huge, beautiful map of the world that plots points when and where they happened, even for data that doesn't have coordinates (like text messages and emails).&lt;/p&gt;
    &lt;p&gt;Follow connections with people across all kinds of chats and messages. Combine conversations with people across platforms into one view.&lt;/p&gt;
    &lt;p&gt;Browse through a rich display of photos and videos from photo libraries, messages sent and received, and other sources.&lt;/p&gt;
    &lt;p&gt;Add millions of data points to your timeline in a matter of minutes. You get full control over background jobs like imports, thumbnails, and embeddings.&lt;/p&gt;
    &lt;p&gt;Timelinize supports playing "live photos" (or "motion photos") for photos taken on Apple, Google, and Samsung devices.&lt;/p&gt;
    &lt;p&gt;Timelinize specializes in combining data from multiple sets and sources. It can identify people and other entities across data sources by their attributes. If a person or contact appears in multiple data sets, it will automatically merge them if possible. If not, you can easily merge entities with the click of a button.&lt;/p&gt;
    &lt;p&gt;Because Timelinize is entity-aware, it can project data points onto a map even without coordinate data. If a geolocated point is known for an entity around the same time of others of that entity's data points, it will appear on the map.&lt;/p&gt;
    &lt;p&gt;Customize the map to change its theme, layers, and even make it 3D.&lt;/p&gt;
    &lt;p&gt;The heatmap shows where your data is concentrated. It smoothly blends as you zoom in and out.&lt;/p&gt;
    &lt;p&gt;Customize what defines a duplicate item, and how to handle that, with a fine degree of control—perfect for merging separate, disparate data sets.&lt;/p&gt;
    &lt;p&gt;An implicit conversation is discovered when a data source links items and entities with a "sent to" relation. You can easily view conversations between entities across modalities in a single scroll: chats, emails, messages, texts, and more.&lt;/p&gt;
    &lt;p&gt;Since I need this to function well for my own family, I have tried to give special attention to less-visible aspects of this application, such as:&lt;/p&gt;
    &lt;p&gt;Timelinize deduplicates, denoises, clusters, and simplifies location data for optimal preservation, with an algorithm that subjectively performs better than Google Maps Timeline.&lt;/p&gt;
    &lt;p&gt;For nerds like me: you can use Timelinize through its CLI, which mirrors all the functions of the HTTP API used by the frontend.&lt;/p&gt;
    &lt;p&gt;Search for pictures and messages by describing them, or find similar items to what you're viewing.&lt;/p&gt;
    &lt;p&gt;All items are stored verbatim, then thumbnails are generated for all images and video media, which are stored separately. Your original data is not modified.&lt;/p&gt;
    &lt;p&gt;The database schema has been meticulously designed and refined to be as adaptable as possible.&lt;/p&gt;
    &lt;p&gt;Timelinize will continue to develop and evolve. In the future, I anticipate the following capabilities:&lt;/p&gt;
    &lt;p&gt;Annotate your timeline, write rich stories with live embeddings from your timeline data, or make physical media like photo books (but with more than just photos!).&lt;/p&gt;
    &lt;p&gt;Add context to your timeline with additional public timelines which have weather, local/regional news, and global events.&lt;/p&gt;
    &lt;p&gt;Securely and privately share parts of your timeline with trusted friends and family members, directly from your computer to theirs.&lt;/p&gt;
    &lt;p&gt;Right now, Timelinize sits "behind" the apps and platforms you already use. But in the future, you could sync data directly to your timeline as it is originated.&lt;/p&gt;
    &lt;p&gt;Collect your data from various sources. Import it with a few clicks. Within minutes, explore millions of your data points in several intuitive ways.&lt;/p&gt;
    &lt;p&gt;Imported data is copied into your timeline folder, ensuring long-term stability and integrity. Timelines are portable—you can copy them or move them to other devices and computers.&lt;/p&gt;
    &lt;p&gt;Your timeline is simply a folder on disk containing a SQLite database alongside your data files. You can freely explore it with other tooling, so you're not locked into Timelinize.&lt;/p&gt;
    &lt;p&gt;Unlike writing a journal, you don't have to take extra time to create content. You're already making the data your timeline can display! And it doesn't replace your current workflow or apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://timelinize.com"/><published>2025-10-07T16:10:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505103</id><title>Police Said They Surveilled Woman Who Had an Abortion for Her 'Safety.'</title><updated>2025-10-07T19:08:12.520654+00:00</updated><content>&lt;doc fingerprint="3de23113b1ea22e1"&gt;
  &lt;main&gt;&lt;p&gt;In May, 404 Media reported that the Johnson County Sheriff’s Office in Texas searched a nationwide network of Flock cameras, a powerful AI-enabled license plate surveillance tool, to look for a woman who self-administered an abortion. At the time, the sheriff told us that the search had nothing to do with criminality and that they were concerned solely about the woman’s safety, specifically the idea that she could be bleeding to death from the abortion. Flock itself said “she was never under criminal investigation by Johnson County. She was being searched for as a missing person, not as a suspect of a crime.”&lt;/p&gt;&lt;head rend="h2"&gt;This post is for paid members only&lt;/head&gt;&lt;p&gt;Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.&lt;/p&gt; Subscribe &lt;head rend="h2"&gt;Sign up for free access to this post&lt;/head&gt;&lt;p&gt;Free members get access to posts like this one along with an email round-up of our week's stories.&lt;/p&gt; Subscribe &lt;p&gt;Already have an account? Sign in&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.404media.co/police-said-they-surveilled-woman-who-had-an-abortion-for-her-safety-court-records-show-they-considered-charging-her-with-a-crime/"/><published>2025-10-07T16:18:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505398</id><title>Cache-Friendly B+Tree Nodes with Dynamic Fanout</title><updated>2025-10-07T19:08:12.295099+00:00</updated><content>&lt;doc fingerprint="eafbc81bf4b08ea8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Cache-Friendly B+Tree Nodes With Dynamic Fanout&lt;/head&gt;&lt;p&gt;For a high-performance B+Tree, the memory layout of each node must be a single contiguous block. This improves locality of reference, increasing the likelihood that the node's contents reside in the CPU cache.&lt;/p&gt;&lt;p&gt;In C++, achieving this means forgoing the use of &lt;code&gt;std::vector&lt;/code&gt;, as it introduces a layer of indirection through a separate memory allocation. The solution to this problem though inevitably increases the implementation complexity and is mired with hidden drawbacks. Nevertheless, this is still a necessary trade-off for unlocking high performance.&lt;/p&gt;&lt;code&gt;  +----------------------+&lt;/code&gt;&lt;head rend="h2"&gt;Challenges&lt;/head&gt;&lt;p&gt;Using &lt;code&gt;std::vector&lt;/code&gt; for a B+Tree node's entries is a non-starter. A &lt;code&gt;std::vector&lt;/code&gt; object holds a pointer to its entries which are stored in a separate block of memory on the heap. This indirection fragments the memory layout, forcing us to fall back on C-style arrays for a contiguous layout when storing variable-length node entries.&lt;/p&gt;&lt;p&gt;This leads to a dilemma. The size of the array must be known at compilation time, yet we need to allow users to configure the fanout (the array's size) at runtime. Furthermore, the implementation should allow inner nodes and leaf nodes to have different fanouts.&lt;/p&gt;&lt;p&gt;This isn't just a B+Tree problem. It is a common challenge in systems programming whenever an object needs to contain a variable-length payload whose size is only known at runtime. How can you define a class that occupies a single block of memory when a part of the block has a dynamic size?&lt;/p&gt;&lt;p&gt;The solution isn't obvious, but it's a well-known trick that systems programmers have used for decades, a technique so common it has eventually been standardized in C99.&lt;/p&gt;&lt;head rend="h2"&gt;The Struct Hack&lt;/head&gt;&lt;p&gt;The solution to this problem is a technique originating in C programming known as the struct hack. The variable-length member (array) is placed at the last position in the struct. To satisfy the compiler an array size of one is hard-coded, ensuring the array size is known at compilation time.&lt;/p&gt;&lt;code&gt;struct Payload {&lt;/code&gt;&lt;p&gt;At runtime, when the required size &lt;code&gt;N&lt;/code&gt; is known, you allocate a single block of memory for the struct and the &lt;code&gt;N&lt;/code&gt; elements combined. The compiler treats this as an opaque block, and provides no safety guarantees. However, accessing the extra allocated space is safe because the variable-length member is the final field in the struct.&lt;/p&gt;&lt;code&gt;// The (N - 1) adjusts for the 1-element array in Payload struct&lt;/code&gt;&lt;p&gt;This pattern was officially standardized in C99, where it is known as a flexible array member.&lt;/p&gt;&lt;p&gt;The C++11 standard formally incorporates the flexible array member, referring to it as an array of unknown bound when it is the last member of a struct.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Arrays of unknown bound&lt;/p&gt;&lt;p&gt;If&lt;/p&gt;&lt;code&gt;expr&lt;/code&gt;is omitted in the declaration of an array, the type declared is "array of unknown bound of T", which is a kind of incomplete type, ...&lt;code&gt;extern int x[]; // the type of x is "array of unknown bound of int"&lt;/code&gt;&lt;lb/&gt;int a[] = {1, 2, 3}; // the type of a is "array of 3 int"&lt;/quote&gt;&lt;p&gt;This means that in C++, the size can be omitted from the final array declaration (e.g. &lt;code&gt;entries_[]&lt;/code&gt;), and the code will compile, enabling the same pattern.&lt;/p&gt;&lt;head rend="h2"&gt;B+Tree Node Declaration&lt;/head&gt;&lt;p&gt;Using the flexible array member syntax, we can now declare a B+Tree node with a memory layout which is a contiguous single block in the heap.&lt;/p&gt;&lt;code&gt;template &amp;lt;typename KeyType, typename ValueType&amp;gt;&lt;/code&gt;&lt;p&gt;Using a &lt;code&gt;std::vector&amp;lt;KeyValuePair&amp;gt;&lt;/code&gt; for the node's entries would result in an indirection. This immediately fragments the memory layout. Accessing an entry within a node is slower, and has higher latency because of the pointer indirection. Chasing the pointer increases the probability of a cache miss, which will force the CPU to stall while it waits for the cache line to be fetched from a different region in main memory.&lt;/p&gt;&lt;p&gt;A cache miss will cost hundreds of CPU cycles compared to just a few cycles for a cache hit. This cumulative latency is unacceptable for any high-performance data structure.&lt;/p&gt;&lt;p&gt;This technique avoids the pointer indirection and provides fine-grained control over memory layout. The node header and data are co-located in one continuous memory block. This layout is cache-friendly and will result in fewer cache misses.&lt;/p&gt;&lt;head rend="h2"&gt;Raw Memory Buffer&lt;/head&gt;&lt;p&gt;This is the key step. The construction of the object has to be separate from its memory allocation. We cannot therefore use the standard &lt;code&gt;new&lt;/code&gt; syntax which will attempt to allocate storage, and then initialize the object in the same storage.&lt;/p&gt;&lt;p&gt;Instead, we use the placement new syntax which only constructs an object in a preallocated memory buffer provided by us. We know exactly how much space to allocate, which is information the standard &lt;code&gt;new&lt;/code&gt; operator does not have in this scenario because of the flexible array member.&lt;/p&gt;&lt;code&gt;// A static helper to allocate storage for a B+Tree node.&lt;/code&gt;&lt;p&gt;The result is a cache-friendly B+Tree node with a fanout that can be configured at runtime.&lt;/p&gt;&lt;head rend="h2"&gt;The Price Of Fine-Grained Control&lt;/head&gt;&lt;p&gt;To create an instance of a B+Tree node with a fanout of &lt;code&gt;256&lt;/code&gt;, it is not possible to write simple idiomatic code like this: &lt;code&gt;new BPlusTreeNode(256)&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Instead we use the custom &lt;code&gt;BPlusTreeNode::Get&lt;/code&gt; helper which knows how much raw memory to allocate for the object including the data section.&lt;/p&gt;&lt;code&gt;BPlusTreeNode *root = BPlusTreeNode&amp;lt;KeyValuePair&amp;gt;::Get(256);&lt;/code&gt;&lt;head rend="h3"&gt;Manual Handling Of Deallocation&lt;/head&gt;&lt;p&gt;The destructor code is also not idiomatic anymore. When the lifetime of the B+Tree node ends, the deallocation code has to be carefully crafted to avoid resource or memory leaks.&lt;/p&gt;&lt;code&gt;class BPlusTreeNode {&lt;/code&gt;&lt;p&gt;This carefully ordered cleanup is necessary because we took manual control of memory. The process is the mirror opposite of our &lt;code&gt;Get&lt;/code&gt; function. We constructed the object outside in: raw memory buffer -&amp;gt; node object -&amp;gt; individual elements. So we teardown in the opposite direction, from the inside out: individual elements -&amp;gt; node object -&amp;gt; raw memory buffer.&lt;/p&gt;&lt;head rend="h3"&gt;Adding New Members In A Derived Class&lt;/head&gt;&lt;p&gt;Adding a new member to a derived class will result in data corruption. It is not possible to add new fields to a specialized &lt;code&gt;InnerNode&lt;/code&gt; or &lt;code&gt;LeafNode&lt;/code&gt; class.&lt;/p&gt;&lt;code&gt;+----------------------+&lt;/code&gt;&lt;code&gt;entries_&lt;/code&gt; array in memory.&lt;p&gt;The raw memory we manually allocated is opaque to the compiler and it cannot safely reason about where the newly added members to the derived class are physically located. The end result is it will overwrite the data buffer and cause data corruption.&lt;/p&gt;&lt;p&gt;The workaround is to break encapsulation and add derived members to the base class so that the flexible array member is always in the last position. This is a significant drawback when we begin using flexible array members.&lt;/p&gt;&lt;code&gt;+----------------------+&lt;/code&gt;
&lt;code&gt;InnerNode&lt;/code&gt; and &lt;code&gt;LeafNode&lt;/code&gt; implementations.&lt;head rend="h3"&gt;Reinventing The Wheel&lt;/head&gt;&lt;p&gt;By using a raw C-style array, we effectively reinvent parts of &lt;code&gt;std::vector&lt;/code&gt;, implementing our own utilities for insertion, deletion and iteration. This not only raises the complexity and maintenance burden but also means we are responsible for ensuring our custom implementation is as performant as the highly-optimized standard library version.&lt;/p&gt;&lt;p&gt;The engineering cost to make this implementation production-grade is significant.&lt;/p&gt;&lt;head rend="h3"&gt;Hidden Data Type Assumptions&lt;/head&gt;&lt;p&gt;The &lt;code&gt;BPlusTreeNode&lt;/code&gt;'s generic signature implies it will work for any &lt;code&gt;KeyType&lt;/code&gt; or &lt;code&gt;ValueType&lt;/code&gt;, but this is dangerously misleading. Using a non-trivial type like &lt;code&gt;std::string&lt;/code&gt; will cause undefined behavior.&lt;/p&gt;&lt;code&gt;template &amp;lt;typename KeyType, typename ValueType&amp;gt;&lt;/code&gt;
&lt;p&gt;To understand why, let's look at how entries are inserted. To make space for a new element, existing entries must be shifted to the right. With our low-level memory layout, this is done using bitwise copy, as the following implementation shows.&lt;/p&gt;&lt;code&gt;bool Insert(const KeyValuePair &amp;amp;element, KeyValuePair *pos) {&lt;/code&gt;
&lt;p&gt;The use of &lt;code&gt;std::memmove&lt;/code&gt; introduces a hidden constraint: &lt;code&gt;KeyValuePair&lt;/code&gt; must be trivially copyable. This means the implementation only works correctly for simple, C-style data structures despite its generic-looking interface.&lt;/p&gt;&lt;p&gt;Using &lt;code&gt;std::memmove&lt;/code&gt; on a &lt;code&gt;std::string&lt;/code&gt; object creates a shallow copy. We now have two &lt;code&gt;std::string&lt;/code&gt; objects whose internal pointers both point to the same character buffer on the heap. When the destructor of the original string is eventually called, it deallocates that buffer. The copied string is now left with a dangling pointer to freed memory, leading to use-after-free errors or a double-free crash when its own destructor runs.&lt;/p&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;The initial hurdle when implementing a B+Tree implementation is solving the contiguous memory layout puzzle avoiding heap indirection. The solution is flexible array members, which makes it possible to compile the program when the number of entries in the B+Tree node is dynamic, and a runtime value.&lt;/p&gt;&lt;p&gt;However, the implementation complexity goes up because of manual memory management, lack of inheritance, and hidden data type constraints. This is unavoidable for high performance.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/"/><published>2025-10-07T16:39:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505407</id><title>Show HN: Arc – high-throughput time-series warehouse with DuckDB analytics</title><updated>2025-10-07T19:08:11.600836+00:00</updated><content>&lt;doc fingerprint="6f1b68d98fb25cce"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance time-series data warehouse built on DuckDB, Parquet, and MinIO.&lt;/p&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Alpha Release - Technical Preview Arc Core is currently in active development and evolving rapidly. While the system is stable and functional, it is not recommended for production workloads at this time. We are continuously improving performance, adding features, and refining the API. Use in development and testing environments only.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High-Performance Ingestion: MessagePack binary protocol (recommended), InfluxDB Line Protocol (drop-in replacement), JSON&lt;/item&gt;
      &lt;item&gt;DuckDB Query Engine: Fast analytical queries with SQL&lt;/item&gt;
      &lt;item&gt;Distributed Storage with MinIO: S3-compatible object storage for unlimited scale and cost-effective data management (recommended). Also supports local disk, AWS S3, and GCS&lt;/item&gt;
      &lt;item&gt;Data Import: Import data from InfluxDB, TimescaleDB, HTTP endpoints&lt;/item&gt;
      &lt;item&gt;Query Caching: Configurable result caching for improved performance&lt;/item&gt;
      &lt;item&gt;Production Ready: Docker deployment with health checks and monitoring&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arc achieves 1.89M records/sec with MessagePack binary protocol!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;1.89M records/sec&lt;/cell&gt;
        &lt;cell&gt;MessagePack binary protocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;p50 Latency&lt;/cell&gt;
        &lt;cell&gt;21ms&lt;/cell&gt;
        &lt;cell&gt;Median response time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;p95 Latency&lt;/cell&gt;
        &lt;cell&gt;204ms&lt;/cell&gt;
        &lt;cell&gt;95th percentile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Success Rate&lt;/cell&gt;
        &lt;cell&gt;99.9998%&lt;/cell&gt;
        &lt;cell&gt;Production-grade reliability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;vs Line Protocol&lt;/cell&gt;
        &lt;cell&gt;7.9x faster&lt;/cell&gt;
        &lt;cell&gt;240K → 1.89M RPS&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tested on Apple M3 Max (14 cores), native deployment with MinIO&lt;/p&gt;
    &lt;p&gt;🎯 Optimal Configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workers: 3x CPU cores (e.g., 14 cores = 42 workers)&lt;/item&gt;
      &lt;item&gt;Deployment: Native mode (2.4x faster than Docker)&lt;/item&gt;
      &lt;item&gt;Storage: MinIO native (not containerized)&lt;/item&gt;
      &lt;item&gt;Protocol: MessagePack binary (&lt;code&gt;/write/v2/msgpack&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Native deployment delivers 1.89M RPS vs 570K RPS in Docker (2.4x faster).&lt;/p&gt;
    &lt;code&gt;# One-command start (auto-installs MinIO, auto-detects CPU cores)
./start.sh native

# Alternative: Manual setup
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env

# Start MinIO natively (auto-configured by start.sh)
brew install minio/stable/minio minio/stable/mc  # macOS
# OR download from https://min.io/download for Linux

# Start Arc (auto-detects optimal worker count: 3x CPU cores)
./start.sh native&lt;/code&gt;
    &lt;p&gt;Arc API will be available at &lt;code&gt;http://localhost:8000&lt;/code&gt;
MinIO Console at &lt;code&gt;http://localhost:9001&lt;/code&gt; (minioadmin/minioadmin)&lt;/p&gt;
    &lt;code&gt;# Start Arc Core with MinIO
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f arc-api

# Stop
docker-compose down&lt;/code&gt;
    &lt;p&gt;Note: Docker mode achieves ~570K RPS. For maximum performance (1.89M RPS), use native deployment.&lt;/p&gt;
    &lt;p&gt;Deploy Arc Core to a remote server:&lt;/p&gt;
    &lt;code&gt;# Docker deployment
./deploy.sh -h your-server.com -u ubuntu -m docker

# Native deployment
./deploy.sh -h your-server.com -u ubuntu -m native&lt;/code&gt;
    &lt;p&gt;Arc Core uses a centralized &lt;code&gt;arc.conf&lt;/code&gt; configuration file (TOML format). This provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clean, organized configuration structure&lt;/item&gt;
      &lt;item&gt;Environment variable overrides for Docker/production&lt;/item&gt;
      &lt;item&gt;Production-ready defaults&lt;/item&gt;
      &lt;item&gt;Comments and documentation inline&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Edit the &lt;code&gt;arc.conf&lt;/code&gt; file for all settings:&lt;/p&gt;
    &lt;code&gt;# Server Configuration
[server]
host = "0.0.0.0"
port = 8000
workers = 8  # Adjust based on load: 4=light, 8=medium, 16=high

# Authentication
[auth]
enabled = true
default_token = ""  # Leave empty to auto-generate

# Query Cache
[query_cache]
enabled = true
ttl_seconds = 60

# Storage Backend (MinIO recommended)
[storage]
backend = "minio"

[storage.minio]
endpoint = "http://minio:9000"
access_key = "minioadmin"
secret_key = "minioadmin123"
bucket = "arc"
use_ssl = false

# For AWS S3
# [storage]
# backend = "s3"
# [storage.s3]
# bucket = "arc-data"
# region = "us-east-1"

# For Google Cloud Storage
# [storage]
# backend = "gcs"
# [storage.gcs]
# bucket = "arc-data"
# project_id = "my-project"&lt;/code&gt;
    &lt;p&gt;Configuration Priority (highest to lowest):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Environment variables (e.g., &lt;code&gt;ARC_WORKERS=16&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;arc.conf&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;Built-in defaults&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can override any setting via environment variables:&lt;/p&gt;
    &lt;code&gt;# Server
ARC_HOST=0.0.0.0
ARC_PORT=8000
ARC_WORKERS=8

# Storage
STORAGE_BACKEND=minio
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_BUCKET=arc

# Cache
QUERY_CACHE_ENABLED=true
QUERY_CACHE_TTL=60

# Logging
LOG_LEVEL=INFO&lt;/code&gt;
    &lt;p&gt;Legacy Support: &lt;code&gt;.env&lt;/code&gt; files are still supported for backward compatibility, but &lt;code&gt;arc.conf&lt;/code&gt; is recommended.&lt;/p&gt;
    &lt;p&gt;After starting Arc Core, create an admin token for API access:&lt;/p&gt;
    &lt;code&gt;# Docker deployment
docker exec -it arc-api python3 -c "
from api.auth import AuthManager
auth = AuthManager(db_path='/data/historian.db')
token = auth.create_token('my-admin', description='Admin token')
print(f'Admin Token: {token}')
"

# Native deployment
cd /path/to/arc-core
source venv/bin/activate
python3 -c "
from api.auth import AuthManager
auth = AuthManager()
token = auth.create_token('my-admin', description='Admin token')
print(f'Admin Token: {token}')
"&lt;/code&gt;
    &lt;p&gt;Save this token - you'll need it for all API requests.&lt;/p&gt;
    &lt;p&gt;All endpoints require authentication via Bearer token:&lt;/p&gt;
    &lt;code&gt;# Set your token
export ARC_TOKEN="your-token-here"&lt;/code&gt;
    &lt;code&gt;curl http://localhost:8000/health&lt;/code&gt;
    &lt;p&gt;MessagePack binary protocol offers 3x faster ingestion with zero-copy PyArrow processing:&lt;/p&gt;
    &lt;code&gt;import msgpack
import requests
from datetime import datetime

# Prepare data in MessagePack format
data = {
    "database": "metrics",
    "table": "cpu_usage",
    "records": [
        {
            "timestamp": int(datetime.now().timestamp() * 1e9),  # nanoseconds
            "host": "server01",
            "cpu": 0.64,
            "memory": 0.82
        },
        {
            "timestamp": int(datetime.now().timestamp() * 1e9),
            "host": "server02",
            "cpu": 0.45,
            "memory": 0.71
        }
    ]
}

# Send via MessagePack
response = requests.post(
    "http://localhost:8000/write/v2/msgpack",
    headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/msgpack"
    },
    data=msgpack.packb(data)
)
print(response.json())&lt;/code&gt;
    &lt;p&gt;Batch ingestion (for high throughput):&lt;/p&gt;
    &lt;code&gt;# Send 10,000 records at once
records = [
    {
        "timestamp": int(datetime.now().timestamp() * 1e9),
        "sensor_id": f"sensor_{i}",
        "temperature": 20 + (i % 10),
        "humidity": 60 + (i % 20)
    }
    for i in range(10000)
]

data = {
    "database": "iot",
    "table": "sensors",
    "records": records
}

response = requests.post(
    "http://localhost:8000/write/v2/msgpack",
    headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/msgpack"
    },
    data=msgpack.packb(data)
)&lt;/code&gt;
    &lt;p&gt;For drop-in replacement of InfluxDB - compatible with Telegraf and InfluxDB clients:&lt;/p&gt;
    &lt;code&gt;# InfluxDB 1.x compatible endpoint
curl -X POST "http://localhost:8000/write/line?db=mydb" \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: text/plain" \
  --data-binary "cpu,host=server01 value=0.64 1633024800000000000"

# Multiple measurements
curl -X POST "http://localhost:8000/write/line?db=metrics" \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: text/plain" \
  --data-binary "cpu,host=server01,region=us-west value=0.64 1633024800000000000
memory,host=server01,region=us-west used=8.2,total=16.0 1633024800000000000
disk,host=server01,region=us-west used=120.5,total=500.0 1633024800000000000"&lt;/code&gt;
    &lt;p&gt;Telegraf configuration (drop-in InfluxDB replacement):&lt;/p&gt;
    &lt;code&gt;[[outputs.influxdb]]
  urls = ["http://localhost:8000"]
  database = "telegraf"
  skip_database_creation = true

  # Authentication
  username = ""  # Leave empty
  password = "$ARC_TOKEN"  # Use your Arc token as password

  # Or use HTTP headers
  [outputs.influxdb.headers]
    Authorization = "Bearer $ARC_TOKEN"&lt;/code&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/query \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "database": "mydb",
    "query": "SELECT * FROM cpu_usage WHERE host = '\''server01'\'' ORDER BY timestamp DESC LIMIT 100"
  }'&lt;/code&gt;
    &lt;p&gt;Advanced queries with DuckDB SQL:&lt;/p&gt;
    &lt;code&gt;# Aggregations
curl -X POST http://localhost:8000/query \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "database": "metrics",
    "query": "SELECT host, AVG(cpu) as avg_cpu, MAX(memory) as max_memory FROM cpu_usage WHERE timestamp &amp;gt; now() - INTERVAL 1 HOUR GROUP BY host"
  }'

# Time-series analysis
curl -X POST http://localhost:8000/query \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "database": "iot",
    "query": "SELECT time_bucket(INTERVAL '\''5 minutes'\'', timestamp) as bucket, AVG(temperature) as avg_temp FROM sensors GROUP BY bucket ORDER BY bucket"
  }'&lt;/code&gt;
    &lt;code&gt;┌─────────────────────────────────────────────────────────────┐
│                     Client Applications                      │
│  (Telegraf, Python, Go, JavaScript, curl, etc.)             │
└──────────────────┬──────────────────────────────────────────┘
                   │
                   │ HTTP/HTTPS
                   ▼
┌─────────────────────────────────────────────────────────────┐
│                   Arc API Layer (FastAPI)                    │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────────┐  │
│  │ Line Protocol│  │  MessagePack │  │  Query Engine    │  │
│  │   Endpoint   │  │   Binary API │  │   (DuckDB)       │  │
│  └──────────────┘  └──────────────┘  └──────────────────┘  │
└──────────────────┬──────────────────────────────────────────┘
                   │
                   │ Write Pipeline
                   ▼
┌─────────────────────────────────────────────────────────────┐
│              Buffering &amp;amp; Processing Layer                    │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  ParquetBuffer (Line Protocol)                       │  │
│  │  - Batches records by measurement                    │  │
│  │  - Polars DataFrame → Parquet                        │  │
│  │  - Snappy compression                                │  │
│  └──────────────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  ArrowParquetBuffer (MessagePack Binary)             │  │
│  │  - Zero-copy PyArrow RecordBatch                     │  │
│  │  - Direct Parquet writes (3x faster)                 │  │
│  │  - Columnar from start                               │  │
│  └──────────────────────────────────────────────────────┘  │
└──────────────────┬──────────────────────────────────────────┘
                   │
                   │ Parquet Files
                   ▼
┌─────────────────────────────────────────────────────────────┐
│              Storage Backend (Pluggable)                     │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  MinIO (Recommended - S3-compatible)                   │ │
│  │  ✓ Unlimited scale          ✓ Distributed             │ │
│  │  ✓ Cost-effective           ✓ Self-hosted             │ │
│  │  ✓ High availability        ✓ Erasure coding          │ │
│  │  ✓ Multi-tenant             ✓ Object versioning       │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  Alternative backends: Local Disk, AWS S3, Google Cloud     │
└─────────────────────────────────────────────────────────────┘
                   │
                   │ Query Path (Direct Parquet reads)
                   ▼
┌─────────────────────────────────────────────────────────────┐
│              Query Engine (DuckDB)                           │
│  - Direct Parquet reads from object storage                 │
│  - Columnar execution engine                                │
│  - Query cache for common queries                           │
│  - Full SQL interface (Postgres-compatible)                 │
└─────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;Arc Core is designed with MinIO as the primary storage backend for several key reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Unlimited Scale: Store petabytes of time-series data without hitting storage limits&lt;/item&gt;
      &lt;item&gt;Cost-Effective: Commodity hardware or cloud storage at fraction of traditional database costs&lt;/item&gt;
      &lt;item&gt;Distributed Architecture: Built-in replication and erasure coding for data durability&lt;/item&gt;
      &lt;item&gt;S3 Compatibility: Works with any S3-compatible storage (AWS S3, GCS, Wasabi, etc.)&lt;/item&gt;
      &lt;item&gt;Performance: Direct Parquet reads from object storage with DuckDB's efficient execution&lt;/item&gt;
      &lt;item&gt;Separation of Compute &amp;amp; Storage: Scale storage and compute independently&lt;/item&gt;
      &lt;item&gt;Self-Hosted Option: Run on your own infrastructure without cloud vendor lock-in&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MinIO + Parquet + DuckDB combination provides the perfect balance of cost, performance, and scalability for analytical time-series workloads.&lt;/p&gt;
    &lt;p&gt;Arc Core has been benchmarked using ClickBench - the industry-standard analytical database benchmark with 100M row dataset (14GB) and 43 analytical queries.&lt;/p&gt;
    &lt;p&gt;Hardware: AWS c6a.4xlarge (16 vCPU AMD EPYC 7R13, 32GB RAM, 500GB gp2)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cold Run Total: 35.18s (sum of 43 queries, first execution)&lt;/item&gt;
      &lt;item&gt;Hot Run Average: 0.81s (average per query after caching)&lt;/item&gt;
      &lt;item&gt;Aggregate Performance: ~2.8M rows/sec cold, ~123M rows/sec hot (across all queries)&lt;/item&gt;
      &lt;item&gt;Storage: MinIO (S3-compatible)&lt;/item&gt;
      &lt;item&gt;Success Rate: 43/43 queries (100%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hardware: Apple M3 Max (14 cores ARM, 36GB RAM)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cold Run Total: 23.86s (sum of 43 queries, first execution)&lt;/item&gt;
      &lt;item&gt;Hot Run Average: 0.52s (average per query after caching)&lt;/item&gt;
      &lt;item&gt;Aggregate Performance: ~4.2M rows/sec cold, ~192M rows/sec hot (across all queries)&lt;/item&gt;
      &lt;item&gt;Storage: Local NVMe SSD&lt;/item&gt;
      &lt;item&gt;Success Rate: 43/43 queries (100%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Columnar Storage: Parquet format with Snappy compression&lt;/item&gt;
      &lt;item&gt;Query Engine: DuckDB with default settings (ClickBench compliant)&lt;/item&gt;
      &lt;item&gt;Result Caching: 60s TTL for repeated queries (production mode)&lt;/item&gt;
      &lt;item&gt;End-to-End: All timings include HTTP/JSON API overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Time (avg)&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q1&lt;/cell&gt;
        &lt;cell&gt;0.021s&lt;/cell&gt;
        &lt;cell&gt;Simple aggregation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q8&lt;/cell&gt;
        &lt;cell&gt;0.034s&lt;/cell&gt;
        &lt;cell&gt;String parsing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q27&lt;/cell&gt;
        &lt;cell&gt;0.086s&lt;/cell&gt;
        &lt;cell&gt;Complex grouping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q41&lt;/cell&gt;
        &lt;cell&gt;0.048s&lt;/cell&gt;
        &lt;cell&gt;URL parsing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q42&lt;/cell&gt;
        &lt;cell&gt;0.044s&lt;/cell&gt;
        &lt;cell&gt;Multi-column filter&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Time (avg)&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q29&lt;/cell&gt;
        &lt;cell&gt;7.97s&lt;/cell&gt;
        &lt;cell&gt;Heavy string operations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q19&lt;/cell&gt;
        &lt;cell&gt;1.69s&lt;/cell&gt;
        &lt;cell&gt;Multiple joins&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q33&lt;/cell&gt;
        &lt;cell&gt;1.86s&lt;/cell&gt;
        &lt;cell&gt;Complex aggregations&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Benchmark Configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dataset: 100M rows, 14GB Parquet (ClickBench hits.parquet)&lt;/item&gt;
      &lt;item&gt;Protocol: HTTP REST API with JSON responses&lt;/item&gt;
      &lt;item&gt;Caching: Disabled for benchmark compliance&lt;/item&gt;
      &lt;item&gt;Tuning: None (default DuckDB settings)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See full results and methodology at ClickBench Results (Arc submission pending).&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;docker-compose.yml&lt;/code&gt; includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;arc-api: Main API server (port 8000)&lt;/item&gt;
      &lt;item&gt;minio: S3-compatible storage (port 9000, console 9001)&lt;/item&gt;
      &lt;item&gt;minio-init: Initializes MinIO buckets on startup&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run with auto-reload
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# Run tests (if available in parent repo)
pytest tests/&lt;/code&gt;
    &lt;p&gt;Health check endpoint:&lt;/p&gt;
    &lt;code&gt;curl http://localhost:8000/health&lt;/code&gt;
    &lt;p&gt;Logs:&lt;/p&gt;
    &lt;code&gt;# Docker
docker-compose logs -f arc-api

# Native (systemd)
sudo journalctl -u arc-api -f&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /&lt;/code&gt;- API information&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /health&lt;/code&gt;- Service health check&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /ready&lt;/code&gt;- Readiness probe&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /docs&lt;/code&gt;- Swagger UI documentation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /redoc&lt;/code&gt;- ReDoc documentation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /openapi.json&lt;/code&gt;- OpenAPI specification&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: All other endpoints require Bearer token authentication.&lt;/p&gt;
    &lt;p&gt;MessagePack Binary Protocol (Recommended - 3x faster):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /write/v2/msgpack&lt;/code&gt;- Write data via MessagePack&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/v2/msgpack&lt;/code&gt;- Alternative endpoint&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /write/v2/msgpack/stats&lt;/code&gt;- Get ingestion statistics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /write/v2/msgpack/spec&lt;/code&gt;- Get protocol specification&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Line Protocol (InfluxDB compatibility):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /write&lt;/code&gt;- InfluxDB 1.x compatible write&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/v1/write&lt;/code&gt;- InfluxDB 1.x API format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/v2/write&lt;/code&gt;- InfluxDB 2.x API format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/v1/query&lt;/code&gt;- InfluxDB 1.x query format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /write/health&lt;/code&gt;- Write endpoint health check&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /write/stats&lt;/code&gt;- Write statistics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /write/flush&lt;/code&gt;- Force flush write buffer&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /query&lt;/code&gt;- Execute DuckDB SQL query&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /query/estimate&lt;/code&gt;- Estimate query cost&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /query/stream&lt;/code&gt;- Stream large query results&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /query/{measurement}&lt;/code&gt;- Get measurement data&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /query/{measurement}/csv&lt;/code&gt;- Export measurement as CSV&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /measurements&lt;/code&gt;- List all measurements/tables&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /auth/verify&lt;/code&gt;- Verify token validity&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /auth/tokens&lt;/code&gt;- List all tokens&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /auth/tokens&lt;/code&gt;- Create new token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /auth/tokens/{id}&lt;/code&gt;- Get token details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PATCH /auth/tokens/{id}&lt;/code&gt;- Update token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DELETE /auth/tokens/{id}&lt;/code&gt;- Delete token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /auth/tokens/{id}/rotate&lt;/code&gt;- Rotate token (generate new)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /health&lt;/code&gt;- Service health check&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /ready&lt;/code&gt;- Readiness probe&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics&lt;/code&gt;- Prometheus metrics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics/timeseries/{type}&lt;/code&gt;- Time-series metrics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics/endpoints&lt;/code&gt;- Endpoint statistics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics/query-pool&lt;/code&gt;- Query pool status&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics/memory&lt;/code&gt;- Memory profile&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /logs&lt;/code&gt;- Application logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;InfluxDB Connections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /connections/influx&lt;/code&gt;- List InfluxDB connections&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /connections/influx&lt;/code&gt;- Create InfluxDB connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PUT /connections/influx/{id}&lt;/code&gt;- Update connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DELETE /connections/{type}/{id}&lt;/code&gt;- Delete connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /connections/{type}/{id}/activate&lt;/code&gt;- Activate connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /connections/{type}/test&lt;/code&gt;- Test connection&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Storage Connections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /connections/storage&lt;/code&gt;- List storage backends&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /connections/storage&lt;/code&gt;- Create storage connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PUT /connections/storage/{id}&lt;/code&gt;- Update storage connection&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /jobs&lt;/code&gt;- List all export jobs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /jobs&lt;/code&gt;- Create new export job&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PUT /jobs/{id}&lt;/code&gt;- Update job configuration&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DELETE /jobs/{id}&lt;/code&gt;- Delete job&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /jobs/{id}/executions&lt;/code&gt;- Get job execution history&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /jobs/{id}/run&lt;/code&gt;- Run job immediately&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /jobs/{id}/cancel&lt;/code&gt;- Cancel running job&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /monitoring/jobs&lt;/code&gt;- Monitor job status&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /api/http-json/connections&lt;/code&gt;- Create HTTP/JSON connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /api/http-json/connections&lt;/code&gt;- List connections&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /api/http-json/connections/{id}&lt;/code&gt;- Get connection details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PUT /api/http-json/connections/{id}&lt;/code&gt;- Update connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DELETE /api/http-json/connections/{id}&lt;/code&gt;- Delete connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/http-json/connections/{id}/test&lt;/code&gt;- Test connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/http-json/connections/{id}/discover-schema&lt;/code&gt;- Discover schema&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/http-json/export&lt;/code&gt;- Export data via HTTP&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /cache/stats&lt;/code&gt;- Cache statistics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /cache/health&lt;/code&gt;- Cache health status&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /cache/clear&lt;/code&gt;- Clear query cache&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arc Core includes auto-generated API documentation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger UI: &lt;code&gt;http://localhost:8000/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ReDoc: &lt;code&gt;http://localhost:8000/redoc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;OpenAPI JSON: &lt;code&gt;http://localhost:8000/openapi.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arc Core is under active development. Current focus areas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Performance Optimization: Further improvements to ingestion and query performance&lt;/item&gt;
      &lt;item&gt;API Stability: Finalizing core API contracts&lt;/item&gt;
      &lt;item&gt;Enhanced Monitoring: Additional metrics and observability features&lt;/item&gt;
      &lt;item&gt;Documentation: Expanded guides and tutorials&lt;/item&gt;
      &lt;item&gt;Production Hardening: Testing and validation for production use cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome feedback and feature requests as we work toward a stable 1.0 release.&lt;/p&gt;
    &lt;p&gt;Arc Core is licensed under the GNU Affero General Public License v3.0 (AGPL-3.0).&lt;/p&gt;
    &lt;p&gt;This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Free to use - Use Arc Core for any purpose&lt;/item&gt;
      &lt;item&gt;✅ Free to modify - Modify the source code as needed&lt;/item&gt;
      &lt;item&gt;✅ Free to distribute - Share your modifications with others&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Share modifications - If you modify Arc and run it as a service, you must share your changes under AGPL-3.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AGPL-3.0 ensures that improvements to Arc benefit the entire community, even when run as a cloud service. This prevents the "SaaS loophole" where companies could take the code, improve it, and keep changes proprietary.&lt;/p&gt;
    &lt;p&gt;For organizations that require:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Proprietary modifications without disclosure&lt;/item&gt;
      &lt;item&gt;Commercial support and SLAs&lt;/item&gt;
      &lt;item&gt;Enterprise features and managed services&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please contact us at: enterprise[at]basekick[dot]net&lt;/p&gt;
    &lt;p&gt;We offer dual licensing and commercial support options.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Community Support: GitHub Issues&lt;/item&gt;
      &lt;item&gt;Enterprise Support: enterprise[at]basekick[dot]net&lt;/item&gt;
      &lt;item&gt;General Inquiries: support[at]basekick[dot]net&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arc Core is provided "as-is" in alpha state. While we use it extensively for development and testing, it is not yet production-ready. Features and APIs may change without notice. Always back up your data and test thoroughly in non-production environments before considering any production deployment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Basekick-Labs/arc"/><published>2025-10-07T16:40:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505626</id><title>Robin Williams' daughter pleads for people to stop sending AI videos of her dad</title><updated>2025-10-07T19:08:11.448883+00:00</updated><content>&lt;doc fingerprint="97d23aa93261cd6d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Robin Williams' daughter pleads for people to stop sending AI videos of her dad&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Published&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Zelda Williams, the daughter of Robin Williams, has asked people to stop sending her AI-generated videos of her father, the celebrated US actor and comic who died in 2014.&lt;/p&gt;
    &lt;p&gt;"Please, just stop sending me AI videos of Dad," Zelda Williams posted on her Instagram stories.&lt;/p&gt;
    &lt;p&gt;"Stop believing I wanna see it or that I'll understand, I don't and I won't. If you're just trying to troll me, I've seen way worse, I'll restrict and move on.&lt;/p&gt;
    &lt;p&gt;"But please, if you've got any decency, just stop doing this to him and to me, to everyone even, full stop. It's dumb, it's a waste of time and energy, and believe me, it's NOT what he'd want."&lt;/p&gt;
    &lt;p&gt;This is not the first time Zelda Williams, a film director, has criticised AI versions of her father, who took his own life in 2014 at his Californian home at the age of 63.&lt;/p&gt;
    &lt;p&gt;Williams, who was famous for films such as Good Morning Vietnam, Dead Poets Society and Mrs Doubtfire, was understood to have been battling depression at the time of his death.&lt;/p&gt;
    &lt;p&gt;In 2023, in an Instagram post supporting a campaign against AI by US media union SAG-Aftra, she described attempts at recreating his voice as "personally disturbing", while also pointing to the wider implications.&lt;/p&gt;
    &lt;p&gt;Her post on Tuesday reflects a trend on social media, where images of people who have died are animated, featuring captions like "bring your loved ones back to life".&lt;/p&gt;
    &lt;p&gt;Williams continued: "To watch the legacies of real people be condensed down to 'this vaguely looks and sounds like them so that's enough', just so other people can churn out horrible TikTok slop puppeteering them is maddening," she continued.&lt;/p&gt;
    &lt;p&gt;"You're not making art, you're making disgusting, over-processed hotdogs out of the lives of human beings, out of the history of art and music, and then shoving them down someone else's throat hoping they'll give you a little thumbs up and like it. Gross."&lt;/p&gt;
    &lt;p&gt;She concluded: "And for the love of EVERY THING, stop calling it 'the future,' AI is just badly recycling and regurgitating the past to be re-consumed. You are taking in the Human Centipede of content, and from the very very end of the line, all while the folks at the front laugh and laugh, consume and consume."&lt;/p&gt;
    &lt;p&gt;The Human Centipede is a reference to the 2009 body horror film.&lt;/p&gt;
    &lt;head rend="h2"&gt;'She sparks conversation'&lt;/head&gt;
    &lt;p&gt;Her latest comments come in the wake of unease following the unveiling of "AI actor", Tilly Norwood.&lt;/p&gt;
    &lt;p&gt;Norwood was created by Dutch actor and comedian Eline Van der Velden, who reportedly said she wanted Norwood to become the "next Scarlett Johansson".&lt;/p&gt;
    &lt;p&gt;In a statement, SAG-Aftra said Norwood "is not an actor, it's a character generated by a computer program that was trained on the work of countless professional performers.&lt;/p&gt;
    &lt;p&gt;"It has no life experience to draw from, no emotion and, from what we've seen, audiences aren't interested in watching computer-generated content untethered from the human experience," the union added.&lt;/p&gt;
    &lt;p&gt;Actress Emily Blunt also recently said she found the idea of Norwood terrifying.&lt;/p&gt;
    &lt;p&gt;"That is really, really scary, Come on, agencies, don't do that. Please stop. Please stop taking away our human connection," she said on a podcast with Variety.&lt;/p&gt;
    &lt;p&gt;Van der Velden later said in a statement, external: "To those who have expressed anger over the creation of my AI character, Tilly Norwood, she is not a replacement for a human being, but a creative work â a piece of art.&lt;/p&gt;
    &lt;p&gt;"Like many forms of art before her, she sparks conversation, and that in itself shows the power of creativity."&lt;/p&gt;
    &lt;head rend="h2"&gt;Related topics&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published6 days ago&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published25 September 2023&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published27 February 2015&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published13 August 2014&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.co.uk/news/articles/c0r0erqk18jo"/><published>2025-10-07T16:56:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505666</id><title>Pigeon (YC W23) is hiring a lead full stack engineer</title><updated>2025-10-07T19:08:10.950165+00:00</updated><content>&lt;doc fingerprint="aa9be2aaf687b428"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;head rend="h1"&gt;Lead Full Stack Software Engineer at Pigeon (YC W23)&lt;/head&gt;
        &lt;p&gt;Pigeon (YC W23) is looking for a motivated Lead Full Stack Software Engineer to join our engineering team. You can work from our NYC office or remotely if you’re not local.&lt;/p&gt;
        &lt;p&gt;As a Lead Full Stack Software Engineer at Pigeon, you will help lead a small and fast-paced engineering team and spearhead the development of new features and systems from the ground up. You will be given a unique opportunity to shape our stack, processes, and culture while making a tangible impact on our technology and our customers.&lt;/p&gt;
        &lt;head rend="h3"&gt;Why Join Pigeon&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Impact: You will own high-value features for Pigeon’s entire customer base that will help shape their everyday business processes.&lt;/item&gt;
          &lt;item&gt;Culture: You will help shape how we work on a day-to-day basis and inform core values as we grow.&lt;/item&gt;
          &lt;item&gt;Leadership: You will be placed in a key leadership position with the opportunity to contribute to our direction, goals, and vision.&lt;/item&gt;
          &lt;item&gt;Learning: You will be encouraged to experiment with new methods and technologies to enable innovative experiences for customers.&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;What You’ll Do&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Own core services, APIs, and integrations with third-party systems&lt;/item&gt;
          &lt;item&gt;Build and scale our AI-powered document processing system&lt;/item&gt;
          &lt;item&gt;Ship new features end-to-end - from conception to implementation to deployment&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;What We’re Looking For&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;5+ years experience as a full-stack software engineer&lt;/item&gt;
          &lt;item&gt;Comfortable with fast-paced development environment and early-stage ambiguity&lt;/item&gt;
          &lt;item&gt;Ability to take full ownership of projects (scoping, system design, implementation, QA, deployment, and maintenance)&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;Our Stack&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;AWS, Kubernetes, Vercel&lt;/item&gt;
          &lt;item&gt;Python, Flask, FastAPI, SqlAlchemy&lt;/item&gt;
          &lt;item&gt;NextJS, Javascript/Typescript, React, CSS, Tailwind&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;Benefits:&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Healthcare (Medical, Vision, and Dental)&lt;/item&gt;
          &lt;item&gt;OneMedical&lt;/item&gt;
          &lt;item&gt;401(k)&lt;/item&gt;
          &lt;item&gt;Unlimited PTO&lt;/item&gt;
          &lt;item&gt;16” Macbook Pro (M2 Chip)&lt;/item&gt;
          &lt;item&gt;Free Pigeon Merchandise (shop.pigeondocuments.com)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;p&gt;Pigeon automates the entire document lifecycle: collecting documents from clients, reviewing and extracting data with AI, and syncing with CRMs or storage systems. Pigeon eliminates the manual back-and-forth of document handling and eliminates thousands of hours of manual tasks.&lt;/p&gt;
      &lt;p&gt;We're growing fast, and want someone who can help join our team as we prepare for the next stage of growth.&lt;/p&gt;
      &lt;head rend="h3"&gt;Team&lt;/head&gt;
      &lt;p&gt;We are a team of 4 who previously worked at Google, Squarespace, Deloitte, and HonorLock.&lt;/p&gt;
      &lt;head rend="h3"&gt;Funding Status&lt;/head&gt;
      &lt;p&gt;We closed a $3.5M Seed round post-YC.&lt;/p&gt;
      &lt;head rend="h3"&gt;About our Technology:&lt;/head&gt;
      &lt;p&gt;Our Stack:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;AWS, Kubernetes, Vercel&lt;/item&gt;
        &lt;item&gt;Python, Flask, SqlAlchemy&lt;/item&gt;
        &lt;item&gt;NextJS, Javascript/Typescript, React, CSS, Tailwind&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/pigeon/jobs/sjuJOg3-lead-full-stack-software-engineer-remote-us"/><published>2025-10-07T17:00:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505692</id><title>You're Doing Rails Wrong</title><updated>2025-10-07T19:08:10.580250+00:00</updated><content>&lt;doc fingerprint="2cd7f1377fc53f68"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt; You're doing Rails wrong. copy link &lt;/head&gt;
    &lt;head rend="h6"&gt;Tuesday, 07 October 2025&lt;/head&gt;
    &lt;p&gt; Kevin: Hey, have you tried Vite for Rails 8? It’s insanely fast.&lt;/p&gt;
    &lt;p&gt; John: I’ve heard of it. Isn’t that a build tool? Didn’t Rails already come with one?&lt;/p&gt;
    &lt;p&gt; K: Well, it did, but Vite is like… modern. You’ll need to install Node, npm, and configure a few scripts, but it’s totally worth it.&lt;/p&gt;
    &lt;p&gt; J: Wait, Rails needs Node now?&lt;/p&gt;
    &lt;p&gt; K: Well, yeah — if you want to use React. Everyone’s using React.&lt;/p&gt;
    &lt;p&gt; J: Didn’t Rails have something for that?&lt;/p&gt;
    &lt;p&gt; K: It did, but now you’ll want to use Vite with React Refresh so you get instant component reloads. And if you want TypeScript support, you’ll have to configure that too.&lt;/p&gt;
    &lt;p&gt; J: Sounds… like a lot.&lt;/p&gt;
    &lt;p&gt; K: Oh, not really. Just install Babel, configure your .babelrc, add vite-plugin-ruby, then you’ll want PostCSS for your styles.&lt;/p&gt;
    &lt;p&gt; J: PostCSS?&lt;/p&gt;
    &lt;p&gt; K: Yeah, and then Tailwind, obviously — you don’t want to write CSS like a peasant.&lt;/p&gt;
    &lt;p&gt; J: Of course not.&lt;/p&gt;
    &lt;p&gt; K: Then you’ll probably want to add ESLint and Prettier to make sure your code looks clean, and maybe Husky for pre-commit hooks.&lt;/p&gt;
    &lt;p&gt; J: So... Vite, React, Babel, PostCSS, Tailwind, ESLint, Prettier, Husky. That’s it?&lt;/p&gt;
    &lt;p&gt; K: Pretty much. Oh, unless you want server-side rendering — then you’ll need Next.js or Remix.&lt;/p&gt;
    &lt;p&gt; J: Wait, we’re still talking about a Rails app, right?&lt;/p&gt;
    &lt;p&gt; K: Yeah, but hybrid stacks are the way to go! You could also use StimulusReflex or Hotwire if you want reactive components without JS frameworks.&lt;/p&gt;
    &lt;p&gt; J: StimulusReflex sounds like a Marvel character.&lt;/p&gt;
    &lt;p&gt; K: Ha! No, it’s for real-time updates. But you’ll need ActionCable configured, Redis running, and—&lt;/p&gt;
    &lt;p&gt; J: Redis?&lt;/p&gt;
    &lt;p&gt; K: Yeah, you need a pub/sub layer. Don’t worry, it’s just another Docker container.&lt;/p&gt;
    &lt;p&gt; J: Docker too?&lt;/p&gt;
    &lt;p&gt; K: Yeah, to isolate your dependencies. And if you want everything reproducible, you’ll need Docker Compose, maybe Fly.io for deployment, and a build pipeline with GitHub Actions.&lt;/p&gt;
    &lt;p&gt; J: That’s... quite a setup.&lt;/p&gt;
    &lt;p&gt; K: It’s just modern web development, man. Keeps things simple. What are you doing?&lt;/p&gt;
    &lt;p&gt; J: Just tinkering.&lt;/p&gt;
    &lt;p&gt;(John runs a single command. The app boots instantly, working forms, instant loading times, blazing fast navigation.)&lt;/p&gt;
    &lt;p&gt; K: Wow, that looks like a pretty complex setup. What stack’s that?&lt;/p&gt;
    &lt;p&gt; J: Vanilla Rails.&lt;/p&gt;
    &lt;p&gt;Just F#$%^&amp;amp; use Rails.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bananacurvingmachine.com/articles/you-re-doing-rails-wrong"/><published>2025-10-07T17:01:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505713</id><title>Building a Browser for Reverse Engineers</title><updated>2025-10-07T19:08:10.083113+00:00</updated><content>&lt;doc fingerprint="aa009620a90d26e8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Preamble&lt;/head&gt;
    &lt;p&gt;In the expanding world of AI my heart still lies in AST transforms, browser fingerprinting, and anti-bot circumvention. In fact, that's the majority of this blog's content. But my workflow always felt... primitive. I was still manually sifting through page scripts, pasting suspicious snippets into an editor, and writing bespoke deobfuscators by hand. Tools like Webcrack and deobfuscate.io help, but the end-to-end loop still felt slow and manual. I wanted to build a tool that would be my web reverse-engineering Swiss Army knife&lt;/p&gt;
    &lt;p&gt;If you're just curious about what it looks like and don't care about how it works then here's a quick showcase:&lt;/p&gt;
    &lt;head rend="h2"&gt;Humble Beginnings&lt;/head&gt;
    &lt;p&gt;My first idea was simple: make a browser extension. For an MVP I wanted to hook an arbitrary function like &lt;code&gt;Array.prototype.push&lt;/code&gt; as early as possible and log every call to it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hooking functions in JavaScript&lt;/head&gt;
    &lt;p&gt;In JavaScript, it's trivial to hook into and override existing functions because you can reassign references at runtime. A common pattern is to stash the original function, replace it with a wrapper that does whatever instrumentation you want, and then call the original so the page keeps behaving normally:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
&lt;/code&gt;
    &lt;p&gt;Here's what that looks like in Chrome's devtools:&lt;/p&gt;
    &lt;p&gt;This technique should make it pretty straightforward to build a Chrome extension that hooks arbitrary global functions on page load and surfaces calls in a small UI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Content Scripts&lt;/head&gt;
    &lt;p&gt;Chrome's content scripts are files that run in the context of web pages, which we can use to install our hooks early.&lt;/p&gt;
    &lt;p&gt;The idea is simple, we create a content script that runs at document_start that injects a tiny bit of code that replaces &lt;code&gt;Array.prototype.push&lt;/code&gt; with a wrapper that logs and then calls the original.&lt;/p&gt;
    &lt;code&gt;{
 "name": "My extension",
 "content_scripts": [
   {
     "run_at": "document_start", // Script is injected after any files from css, but before any other DOM is constructed or any other script is run.
     "matches": ["&amp;lt;all_urls&amp;gt;"],
     "js": ["content-script.js"]
   }
 ]
}
&lt;/code&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
&lt;/code&gt;
    &lt;p&gt;Running this on a page that clearly used Array.push gave me... absolutely nothing. At first, I thought it had to be an execution order issue. Maybe my hook was loading too late? But after another read through the docs, I found this painfully obvious note staring me right in the face:&lt;/p&gt;
    &lt;p&gt;âContent scripts live in an isolated world, allowing a content script to make changes to its JavaScript environment without conflicting with the page or other extensionsâ content scripts.â&lt;/p&gt;
    &lt;p&gt;In hindsight, of course that makes sense. Still, it sucked. I wasnât ready to give up yet, though. I had a potentially clever workaround: injecting a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag directly into the page with my hook inside. But, naturally, it could never be that easy.&lt;/p&gt;
    &lt;p&gt;I knew if I wanted to get this done, I would have to go down a layer.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Editor's Note: Some readers have kindly pointed out to me that this is actually possible to accomplish from an extension. However, the custom browser approach still has benefits explained later in the post :-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Chrome Devtools Protocol&lt;/head&gt;
    &lt;p&gt;The Chrome DevTools Protocol (CDP) is the low-level bridge for instrumenting, inspecting, and debugging Chromium-based browsers. Itâs what automation tools like Selenium and Playwright use under the hood. CDP exposes a large set of methods and events split across domains. The docs publish a convenient, comprehensive list of them.&lt;/p&gt;
    &lt;p&gt;While reading the domains, one method jumped out: &lt;code&gt;Page.addScriptToEvaluateOnNewDocument&lt;/code&gt;. Its description "Evaluates given script in every frame upon creation (before loading frame's scripts)" sounded like exactly the hook we needed: run code before the pageâs own scripts so we can win the prototype race.&lt;/p&gt;
    &lt;p&gt;To prove the idea I built a tiny test: a page with a script that pushes a secret value into an array, and a CDP-injected hook that tries to observe that push. If the hook sees the secret, the technique works. I chose to prototype this using Electron. I could have spoken directly to the browser over raw CDP, but Electron made wiring up a UI, IPC, and a quick demo app way faster for a weekend PoC.&lt;/p&gt;
    &lt;code&gt;const { app, BrowserWindow } = require("electron/main");

function createWindow() {
  const win = new BrowserWindow({
    width: 800,
    height: 600,
  });

  const dbg = win.webContents.debugger;
  dbg.attach("1.3");
  // Enables the Page domain so we can run the script on new document command after
  dbg.sendCommand("Page.enable");
  dbg.sendCommand("Page.addScriptToEvaluateOnNewDocument", {
    source: `(() =&amp;gt; {
        const _origPush = Array.prototype.push;
        Array.prototype.push = function (...args) {
            console.log('Array.push called on', this, 'with', args);
            return _origPush.apply(this, args);
        };
})();`,
  });
  win.webContents.openDevTools();
  win.loadURL("file:///Users/veritas/demo/index.html");
}

app.whenReady().then(() =&amp;gt; {
  createWindow();
});
&lt;/code&gt;
    &lt;p&gt;The result?:&lt;/p&gt;
    &lt;p&gt;It worked! I knew this PoC could take me far. I could hook any arbitrary global function or property and log (or spoof!) arguments and return values. The next step was building a user interface around it.&lt;/p&gt;
    &lt;p&gt;Since this started as a fun weekend project, I wanted the fastest path to a working demo. In true open-source fashion I searched for âelectron web browserâ and stumbled across electron-browser-shell by Samuel Maddock.&lt;/p&gt;
    &lt;p&gt;That project gave me an address bar, tabs, and a basic IPC-ready shell bridging the webview environment and my browser UI.&lt;/p&gt;
    &lt;p&gt;From there I added a sidebar that would display hooked function events as they fired.&lt;/p&gt;
    &lt;p&gt;To make things more interesting I needed to hook more than Array.push. A favorite target of fingerprinting scripts is the Canvas API. Sites can draw a static image to a &lt;code&gt;&amp;lt;canvas&amp;gt;&lt;/code&gt;, call &lt;code&gt;toDataURL()&lt;/code&gt; (or read pixel data), and use the resulting hash to fingerprint your GPU using subtle rendering differences. By correlating canvas hashes with other signals (user agent, installed fonts, etc.), trackers can build a surprisingly robust fingerprint. Watching and optionally spoofing these kind of calls is extremely useful for this kind of RE work.&lt;/p&gt;
    &lt;p&gt;The result looked as follows:&lt;/p&gt;
    &lt;p&gt;I was pretty happy with the direction the project was taking. The PoC actually felt useful. Remembering my previous work reverse-engineering TikTokâs web collector and how aggressively those collectors scrape client-side signals, I couldnât resist testing the hook there. I fired up the demo, pointed it at TikTok, and watched the UI for activity.&lt;/p&gt;
    &lt;p&gt;The site was pulling a decent amount of telemetry. Canvas calls (like &lt;code&gt;toDataURL&lt;/code&gt;), WebGL stats, font and plugin probes, and other subtle signals that, when combined, paint a detailed fingerprint. Seeing those calls appear in my sidebar made this project feel immediately worthwhile.&lt;/p&gt;
    &lt;p&gt;I even made sure to include all canvas operations in a secrion of the detail pane to be able to recreate a canvas if necessary.&lt;/p&gt;
    &lt;p&gt;I wanted to run this against more anti-bots. Out of curiosity I pointed the demo at a site using Cloudflareâs Turnstile. I knew Turnstile was collecting various browser signals, but to my surprise, my sidebar showed nothing. Why was I seeing zero logs?&lt;/p&gt;
    &lt;head rend="h2"&gt;OOPif(S) I did it again&lt;/head&gt;
    &lt;p&gt;Cloudflare renders the Turnstile widget inside a sandboxed iframe tucked into a closed shadow root. This iframe is an OOPIF (out-of-process iframe). It lives in a different renderer process so page-level scripts (and our injected hooks) simply wonât run there, thus, no logs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hopping the Turnstile&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2024, the M.T.A. reports to have lost a combined $568 million in unpaid bus fares and $350 million in unpaid subway fares, wait, wrong turnstile.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We needed a way to run our hooks inside those out-of-process frames. While scanning CDP I noticed the &lt;code&gt;Target.attachedToTarget&lt;/code&gt; event. It fires when the debugger auto-attaches to a new target or when you explicitly call &lt;code&gt;attachToTarget&lt;/code&gt;. This was the key: if we tell CDP to auto-attach to targets, it will notify us (and give us a &lt;code&gt;sessionId&lt;/code&gt;) for every new frame/process as it appears. With that target/session info we can evaluate code in the correct context so our hook actually runs inside OOPIFs as they spawn.&lt;/p&gt;
    &lt;code&gt;const dbg = view.webContents.debugger
dbg.on('message', async (_, method, params) =&amp;gt; {
    if (method === 'Target.attachedToTarget') {
        const { sessionId, targetInfo } = params
        // Prepare child session
        dbg.sendCommand('Runtime.enable', {}, sessionId).catch(() =&amp;gt; {})
        dbg.sendCommand('Page.enable', {}, sessionId).catch(() =&amp;gt; {})
        // Inject hook script into child frames (iframes)
        dbg.sendCommand('Page.addScriptToEvaluateOnNewDocument', { source: hook }, sessionId)
    }
})
&lt;/code&gt;
    &lt;p&gt;Tada, we have events!&lt;/p&gt;
    &lt;p&gt;Iâm not the first to hook common globals and dynamically analyze page scripts. Anti-bots are well aware of this trick and will use a variety of techniques to detect runtime JS patches, so you canât assume your wrappers will stay hidden.&lt;/p&gt;
    &lt;p&gt;How is this possible?&lt;/p&gt;
    &lt;head rend="h2"&gt;toString theory&lt;/head&gt;
    &lt;p&gt;In JavaScript, functions contain a &lt;code&gt;toString&lt;/code&gt; instance method. Let's try calling this on a native function:&lt;/p&gt;
    &lt;code&gt;const mapToString = Array.prototype.map.toString()
// returns 'function map() { [native code] }'
&lt;/code&gt;
    &lt;p&gt;This means that the implementation of this function is provided by the browser's native code. How does this look like with our hook applied?:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
const pushToString = Array.prototype.push.toString(); // Returns "function (...args) {\n  console.log('Array.push called on', this, 'with', args);\n  return _origPush.apply(this, args);\n}"
&lt;/code&gt;
    &lt;p&gt;Oh no, our hook has been discovered! Luckily for us, this is easily patched&lt;/p&gt;
    &lt;code&gt;Array.prototype.push.toString = () =&amp;gt; "function push() { [native code] }";
&lt;/code&gt;
    &lt;p&gt;Phew, that was close.&lt;/p&gt;
    &lt;code&gt;const haha = Array.prototype.push.toString.toString(); // '() =&amp;gt; "function push() { [native code] }"'
&lt;/code&gt;
    &lt;p&gt;Oh no, another leak!&lt;/p&gt;
    &lt;code&gt;Array.prototype.push.toString.toString = () =&amp;gt; "function toString() { [native code] }"; // Yay it's fixed
&lt;/code&gt;
    &lt;p&gt;Ahhh! Another one!&lt;/p&gt;
    &lt;code&gt;const haha = Array.prototype.push.toString.toString.toString.toString.toString.toString();
&lt;/code&gt;
    &lt;p&gt;Wait, you can do what now!?&lt;/p&gt;
    &lt;code&gt;const youCantEscape = Function.toString.call(Array.prototype.push); // Returns "function (...args) {\n  console.log('Array.push called on', this, 'with', args);\n  return _origPush.apply(this, args);\n}"
&lt;/code&gt;
    &lt;p&gt;These JS runtime patches turned out to be frustratingly leaky. Patch one hole and another opens. Fixes were possible, but every patch felt like a bandaid that introduced new detection vectors. see:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};

// Yay, we patched this
Array.prototype.push.toString = () =&amp;gt; "function push() { [native code] }";
Array.prototype.push.toString.toString = () =&amp;gt; "function toString() { [native code] }";

// *facepalm*
const anotherLeak = Array.prototype.push.name; // returns "" instead of "push"
&lt;/code&gt;
    &lt;p&gt;Those runtime patches were very brittle. Targets we were analyzing could detect the instrumentation and change behavior or even self destruct if they noticed they were being watched. Fixing each leak felt like an endless game of whack-a-mole, so I decided to go a layer deeper.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forking Chromium&lt;/head&gt;
    &lt;p&gt;At this point I knew I wanted to fork Chromium. I still planned to use Electron, at least for now, since Iâd already built a decent UI and didnât feel like rewriting it all in native C++. The idea was simple: fork Electron (and by extension Chromium), patch into the Blink layer where these API calls happen, and expose them somehow.&lt;/p&gt;
    &lt;p&gt;I didnât spend too long figuring out how Iâd surface those events. I was already using CDP, so why not create my own custom CDP domain and emit events from there? That way my existing Electron app could just subscribe to them like any other CDP event.&lt;/p&gt;
    &lt;p&gt;Luckily, Electron has a well-documented guide for building from source. Unluckily, building it took more than three hours on my M2 Pro Mac Mini. To make things worse, macOS 26 had broken parts of the build chain. The Metal toolchain wasnât being detected no matter what I had tried. Eventually, I hardcoded the path into the build script just to move forward. After several hours of C++ compilation errors and boredom, I finally had a locally built Electron binary running from source.&lt;/p&gt;
    &lt;p&gt;Now came the hard part: creating a custom CDP domain. The &lt;code&gt;devtools-frontend&lt;/code&gt; repository actually provides documentation on defining new protocol domains.&lt;/p&gt;
    &lt;p&gt;The gist is that protocols are defined in a &lt;code&gt;.pdl&lt;/code&gt; (Protocol Definition Language) file, specifically &lt;code&gt;browser_protocol.pdl&lt;/code&gt;. To add your own domain, you simply declare it there alongside the existing ones.&lt;/p&gt;
    &lt;p&gt;I decided to name my new domain &lt;code&gt;Snitch&lt;/code&gt;, and defined it like this:&lt;/p&gt;
    &lt;code&gt;experimental domain Snitch
  command disable
  command enable
  event toDataURLCalled
    parameters
      string dataURL
      optional string frameId
      optional string contextId
&lt;/code&gt;
    &lt;p&gt;Next, you include your new protocol files in Blinkâs build configuration, found in &lt;code&gt;third_party/blink/renderer/core/inspector/BUILD.gn&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;From there, you define an agent, the bridge that connects Blinkâs internals to the DevTools Protocol so your new CDP domain can send and receive events.&lt;/p&gt;
    &lt;p&gt;Iâll be honest, the documentation for this part was pretty lacking. The only promising link in the documentation pointed to a locked Google Doc presumably restricted to Chrome team members. They say there's no better documentation than the source code itself! By dissecting existing domains like &lt;code&gt;DOMStorage&lt;/code&gt;, &lt;code&gt;Network&lt;/code&gt;, and others, I reverse-engineered how they registered and dispatched events, then adapted that pattern for my own &lt;code&gt;Snitch&lt;/code&gt; domain.&lt;/p&gt;
    &lt;p&gt;I eventually landed on this:&lt;/p&gt;
    &lt;p&gt;snitch_agent.h&lt;/p&gt;
    &lt;code&gt;#ifndef THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_
#define THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_
#include &amp;lt;optional&amp;gt;

#include "third_party/blink/renderer/core/core_export.h"
#include "third_party/blink/renderer/core/inspector/inspector_base_agent.h"
#include "third_party/blink/renderer/core/inspector/protocol/snitch.h"

namespace blink {

class InspectedFrames;

class CORE_EXPORT SnitchAgent final
    : public InspectorBaseAgent&amp;lt;protocol::Snitch::Metainfo&amp;gt; {
 public:
  SnitchAgent(InspectedFrames*);
  SnitchAgent(const SnitchAgent&amp;amp;) = delete;
  SnitchAgent&amp;amp; operator=(const SnitchAgent&amp;amp;) = delete;
  ~SnitchAgent() override;

  void Trace(Visitor*) const override;
  protocol::Response enable() override;
  protocol::Response disable() override;

  void DidCanvasToDataURL(ExecutionContext*, const String&amp;amp; data_url,
                          const String&amp;amp; frame_id,
                          const String&amp;amp; context_id);

 private:
  Member&amp;lt;InspectedFrames&amp;gt; inspected_frames_;
  InspectorAgentState::Boolean enabled_;
};

}  // namespace blink

#endif  // THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_

&lt;/code&gt;
    &lt;p&gt;snitch_agent.cpp&lt;/p&gt;
    &lt;code&gt;#include "third_party/blink/renderer/core/inspector/snitch_agent.h"
#include "third_party/blink/renderer/core/inspector/inspected_frames.h"

namespace blink {

SnitchAgent::SnitchAgent(
  InspectedFrames* inspected_frames)
  : inspected_frames_(inspected_frames),
    enabled_(&amp;amp;agent_state_, /*default_value=*/false) {}


SnitchAgent::~SnitchAgent() = default;

void SnitchAgent::Trace(Visitor* visitor) const {
  visitor-&amp;gt;Trace(inspected_frames_);
  InspectorBaseAgent::Trace(visitor);
}

protocol::Response SnitchAgent::enable() {
  enabled_.Set(true);
  instrumenting_agents_-&amp;gt;AddSnitchAgent(this);
  return protocol::Response::Success();
}

protocol::Response SnitchAgent::disable() {
  enabled_.Clear();
  instrumenting_agents_-&amp;gt;RemoveSnitchAgent(this);
  return protocol::Response::Success();
}

void SnitchAgent::DidCanvasToDataURL(ExecutionContext* context, const String&amp;amp; data_url,
                                     const String&amp;amp; frame_id,
                                     const String&amp;amp; context_id) {

  if (!enabled_.Get()) {
    return;
  }

  std::optional&amp;lt;String&amp;gt; maybe_frame;
  if (!frame_id.empty()) {
    maybe_frame = frame_id;
  }

  std::optional&amp;lt;String&amp;gt; maybe_ctx;
  if (!context_id.empty()) {
    maybe_ctx = context_id;
  }

  GetFrontend()-&amp;gt;toDataURLCalled(data_url, maybe_frame, maybe_ctx);
}

}  // namespace blink

&lt;/code&gt;
    &lt;p&gt;Now I needed a way to trigger my new event from the native C++ implementation of &lt;code&gt;toDataURL&lt;/code&gt;. The implementation for that function lives in &lt;code&gt;src/third_party/blink/renderer/core/html/canvas/html_canvas_element.cc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;While digging through how other events were dispatched, I noticed something interesting. The agents werenât called directly. Instead, events were emitted through probes. These probes act as intermediary hooks that Blink uses to fire instrumentation events into the DevTools pipeline.&lt;/p&gt;
    &lt;p&gt;Hereâs a comment from that same class showing how a probe fires when a canvas element is created:&lt;/p&gt;
    &lt;code&gt;CanvasRenderingContext* HTMLCanvasElement::GetCanvasRenderingContextInternal(
    ExecutionContext* execution_context,
    const String&amp;amp; type,
    const CanvasContextCreationAttributesCore&amp;amp; attributes) {
  CanvasRenderingContext::CanvasRenderingAPI rendering_api =
      CanvasRenderingContext::RenderingAPIFromId(type);

  // ...

  CanvasRenderingContextFactory* factory =
      GetRenderingContextFactory(static_cast&amp;lt;int&amp;gt;(rendering_api));

  // Tell the debugger about the attempt to create a canvas context
  // even if it will fail, to ease debugging.
  probe::DidCreateCanvasContext(&amp;amp;GetDocument());
  // ...
}  
&lt;/code&gt;
    &lt;p&gt;These probes are defined in a file called &lt;code&gt;core_probes.pidl&lt;/code&gt;. The comment at the top of this file states:&lt;/p&gt;
    &lt;code&gt;/*
 * make_instrumenting_probes.py uses this file as a source to generate
 * core_probes_inl.h, core_probes_impl.cc and core_probe_sink.h.
 *
 * The code below is not a correct IDL but a mix of IDL and C++.
 *
 * The syntax for an instrumentation method is as follows:
 *
 *    returnValue methodName([paramAttr1] param1, [paramAttr2] param2, ...)
&lt;/code&gt;
    &lt;p&gt;Following this syntax, I added my custom probe:&lt;/p&gt;
    &lt;code&gt;void DidCanvasToDataURL([Keep] ExecutionContext*, String&amp;amp; data_url, String&amp;amp; frame_id, String&amp;amp; context_id);
&lt;/code&gt;
    &lt;p&gt;A similarly named &lt;code&gt;core_probes.json5&lt;/code&gt; holds the mappings of which agents are responsible for which probes. We can add our entry as such:&lt;/p&gt;
    &lt;code&gt;{
    observers: {
        // ...,
        SnitchAgent: {
            probes: ["DidCanvasToDataURL"]
        }
        // ...
    }
}
&lt;/code&gt;
    &lt;p&gt;The final step in adding our custom domain is to register the agent in &lt;code&gt;WebDevToolsAgentImpl::AttachSession&lt;/code&gt; like so:&lt;/p&gt;
    &lt;code&gt;session-&amp;gt;CreateAndAppend&amp;lt;SnitchAgent&amp;gt;(inspected_frames);
&lt;/code&gt;
    &lt;p&gt;and actually calling it in the implementation of &lt;code&gt;toDataURL&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;String HTMLCanvasElement::toDataURL(const String&amp;amp; mime_type,
                                    const ScriptValue&amp;amp; quality_argument,
                                    ExceptionState&amp;amp; exception_state) const {
  // ...
  String data_url = ToDataURLInternal(mime_type, quality, kBackBuffer,
                                      ReadbackType::kWebExposed);

  // // Hook up call to our new CDP event (Snitch.toDataURLCalled)
  probe::DidCanvasToDataURL(GetExecutionContext(), data_url, frame_id, context_id);

  return data_url;
}
&lt;/code&gt;
    &lt;p&gt;After accidentally nuking my local repository and having to restart the entire process, including sitting through another 3 hour compilation ð¥², it was ready to test!&lt;/p&gt;
    &lt;p&gt;We can create a simple Electron test application:&lt;/p&gt;
    &lt;code&gt;const { app, BrowserWindow } = require("electron/main");

function createWindow() {
  const win = new BrowserWindow({
    width: 800,
    height: 600,
  });

  const dbg = win.webContents.debugger;
  dbg.attach("1.3");
  dbg.sendCommand("Snitch.enable");
  dbg.on("message", (_, method, { dataURL }) =&amp;gt; {
    if (method === "Snitch.toDataURLCalled") {
      console.log("toDataURL called", dataURL);
    }
  });
  win.loadURL("https://demo.fingerprint.com/playground");
}

app.whenReady().then(() =&amp;gt; {
  createWindow();
});
&lt;/code&gt;
    &lt;p&gt;and run it pointing to our custom Electron build:&lt;/p&gt;
    &lt;code&gt;$ /Users/veritas/electron/src/out/Testing/Electron.app/Contents/MacOS/Electron demo.js
&lt;/code&gt;
    &lt;p&gt;Drumroll, please!&lt;/p&gt;
    &lt;p&gt;It worked! We can see our custom CDP event firing and returning to us the result of a toDataURL call on FingerprintJS' playground. We can now use these stealthy CDP events and not leak the fact that we're instrumenting these functions.&lt;/p&gt;
    &lt;p&gt;Note: Depending on what we do in these hooks, it may still be possible to detect us through any side-effects we introduce or potentially through timing checks (Is the function slower than it would usually be?).&lt;/p&gt;
    &lt;head rend="h2"&gt;Extras&lt;/head&gt;
    &lt;p&gt;This was powerful, but I wanted more. I needed a few extra tools to make this thing a real web reverse-engineering Swiss Army knife.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deobfuscation&lt;/head&gt;
    &lt;p&gt;One of the biggest time sinks in this kind of work is dealing with obfuscated scripts. I wanted a built-in tool that could automatically detect and attempt to deobfuscate scripts as they load. Using CDPâs &lt;code&gt;Network&lt;/code&gt; domain, I intercept incoming JavaScript files and run a few lightweight heuristics to score their likelihood of being obfuscated. Suspicious ones are displayed in a separate tab, where I integrate tools like bensbâs deobfuscate.io to automatically try recovering a more readable version. The plan is to add more tools such as Webcrack and even custom deobfuscators of my own.&lt;/p&gt;
    &lt;p&gt;I also added a section that extracts and displays recovered string literals from the processed script for added speed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overwriting properties and functions&lt;/head&gt;
    &lt;p&gt;Hooking and reading is fun, but sometimes you want to change behavior. You now know that doing so in a browser environment, across OOPIFs and with anti-tamper checks is non-trivial. I built an Overrides tab where you can define custom JavaScript snippets that overwrite functions or properties across all frames. These execute without triggering common integrity checks, giving a clean way to spoof or alter these values.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fingerprint payload decryption&lt;/head&gt;
    &lt;p&gt;My bread and butter is dissecting anti-bot and fingerprinting scripts. These scripts often encrypt or encode their payloads before sending them to backend validators, which makes analysis painful. To make life easier, I added a feature that detects known collectors and automatically intercepts their outbound requests. It decrypts (or decodes) the payloads and displays both the plaintext and structured data in a neat table view.&lt;/p&gt;
    &lt;p&gt;Of course, each collector still needs to be reverse-engineered beforehand. Maybe this is where AI-assisted payload analysis could step in someday? Maybe, but for now I will continue to hand-roll my own parsers :-)&lt;/p&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;Iâm really happy with how this project has evolved. Itâs gone from my quick weekend curiosity to a genuinely useful research tool. Still, I have a few major goals remain before I can say I'm truly proud:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Abandon Electron&lt;/p&gt;&lt;lb/&gt;Electron was great for rapid prototyping, but it is heavy and adds its own leaks. Theyâre fixable, sure, but the cleaner path is to embed the UI directly inside Chromium. Iâm looking into how other Chromium forks (like Brave) integrate their native UIs and exploring whether I can do the same.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Hook all the things&lt;/p&gt;&lt;lb/&gt;Iâve already implemented a broad set of hooks. Canvas, WebGL, audio fingerprinting,&lt;code&gt;navigator&lt;/code&gt;accessors, document and window properties, and more. But can we hook everything?&lt;lb/&gt;Iâve experimented with injecting hooks deeper in V8 where function calls are dispatched, however, V8âs optimizations quickly complicate things. Disabling those optimizations would work but at the cost of performance (and thus introducing timing leaks). Another idea is to modify the IDL code generator to automatically insert hooks during buildtime. This is likely the approach I will take.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Release?&lt;/p&gt;&lt;lb/&gt;I havenât decided what to do once itâs ready. Maybe open source it? Would others find it useful? Was this all built into Chromium this entire time under some obscure setting that I missed? Who knows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And with that, I present to you a gallery of canvas fingerprint images that I've collected during this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fingerprint Gallery&lt;/head&gt;
    &lt;head rend="h3"&gt;Tiktok&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
const gradient0 = context.createLinearGradient(10, 0, 180, 1)
gradient0.addColorStop(0, "red")
gradient0.addColorStop(0.1, "white")
gradient0.addColorStop(0.2, "blue")
gradient0.addColorStop(0.3, "yellow")
gradient0.addColorStop(0.4, "purple")
gradient0.addColorStop(0.7, "orange")
gradient0.addColorStop(1, "magenta")
context.fillStyle = gradient0
context.fillRect(0, 10, 100, 6)
const gradient1 = context.createLinearGradient(0, 0, 100, 100)
gradient1.addColorStop(0, "green")
gradient1.addColorStop(0.5, "yellow")
gradient1.addColorStop(0.7, "orange")
gradient1.addColorStop(1, "magenta")
context.beginPath()
context.fillStyle = gradient1
context.arc(50, 10, 25, 0, 6.283185307179586)
context.stroke()
context.fillStyle = "rgba(150, 32, 170, .97)"
context.font = "12px Sans"
context.textBaseline = "top"
context.fillText("*+(}#?ð¼ ð", 18, 18)
context.shadowBlur = 1
context.fillStyle = "rgba(47, 211, 69, .99)"
context.font = "14px Sans"
context.textBaseline = "top"
context.fillText("ð¼OynG@%tp$", 3, 3)
context.beginPath()
context.arc(30, 10, 20, 0, 6.283185307179586)
context.strokeStyle = "rgba(255, 12, 220, 1)"
context.stroke()
&lt;/code&gt;
    &lt;head rend="h3"&gt;FingerprintJS&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
context.rect(0, 0, 10, 10)
context.rect(2, 2, 6, 6)
context.isPointInPath(5, 5, "evenodd")
context.textBaseline = "alphabetic"
context.fillStyle = "#f60"
context.fillRect(100, 1, 62, 20)
context.fillStyle = "#069"
context.font = "11pt \"Times New Roman\""
context.fillText("Cwm fjordbank gly ð", 2, 15)
context.fillStyle = "rgba(102, 204, 0, 0.2)"
context.font = "18pt Arial"
context.fillText("Cwm fjordbank gly ð", 4, 45)
&lt;/code&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
context.globalCompositeOperation = "multiply"
context.fillStyle = "#f2f"
context.beginPath()
context.arc(40, 40, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#2ff"
context.beginPath()
context.arc(80, 40, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#ff2"
context.beginPath()
context.arc(60, 80, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#f9c"
context.arc(60, 60, 60, 0, 6.283185307179586, true)
context.arc(60, 60, 20, 0, 6.283185307179586, true)
context.fill("evenodd")
&lt;/code&gt;
    &lt;head rend="h3"&gt;Cloudflare&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
const gradient0 = context.createRadialGradient(33, 18, 8, 42, 10, 226)
gradient0.addColorStop(0, "#809900")
gradient0.addColorStop(1, "#404041")
context.fillStyle = gradient0
context.shadowBlur = 11
context.shadowColor = "#F38020"
context.beginPath()
context.moveTo(9, 14)
context.quadraticCurveTo(93, 48, 116, 111)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient1 = context.createRadialGradient(77, 98, 2, 27, 30, 206)
gradient1.addColorStop(0, "#809900")
gradient1.addColorStop(1, "#404041")
context.fillStyle = gradient1
context.beginPath()
context.ellipse(58, 55, 31, 28, 1.4441705959829747, 0.5401125108618993, 4.052233984744969)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient2 = context.createRadialGradient(108, 12, 10, 65, 118, 169)
gradient2.addColorStop(0, "#1AB399")
gradient2.addColorStop(1, "#E666B3")
context.fillStyle = gradient2
context.shadowBlur = 16
context.shadowColor = "#809980"
context.font = "27.77777777777778px aanotafontaa"
context.fillText("Ry", 13, 67)
context.shadowBlur = 0
const gradient3 = context.createRadialGradient(46, 47, 0, 101, 108, 207)
gradient3.addColorStop(0, "#4DB380")
gradient3.addColorStop(1, "#FF4D4D")
context.fillStyle = gradient3
context.shadowBlur = 3
context.shadowColor = "#FF6633"
context.beginPath()
context.moveTo(54, 5)
context.bezierCurveTo(54, 90, 32, 74, 71, 120)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient4 = context.createRadialGradient(119, 123, 3, 109, 90, 137)
gradient4.addColorStop(0, "#E6B333")
gradient4.addColorStop(1, "#3366E6")
context.fillStyle = gradient4
context.shadowBlur = 4
context.shadowColor = "#B3B31A"
context.beginPath()
context.moveTo(76, 0)
context.bezierCurveTo(1, 49, 103, 67, 49, 125)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient5 = context.createRadialGradient(34, 47, 1, 37, 59, 245)
gradient5.addColorStop(0, "#809900")
gradient5.addColorStop(1, "#404041")
context.fillStyle = gradient5
context.beginPath()
context.ellipse(56, 57, 14, 8, 1.2273132071162383, 4.1926143018618225, 2.8853539230051624)
context.stroke()
context.fill()
context.shadowBlur = 0
context.shadowBlur = 14
context.shadowColor = "#809900"
context.font = "11.904761904761905px aanotafontaa"
context.strokeText("@H1", 30, 73)
context.shadowBlur = 0;
&lt;/code&gt;
    &lt;head rend="h2"&gt;Until next time&lt;/head&gt;
    &lt;p&gt;I'd love to know if you found this even remotely interesting or think it's just a giant waste of time :-) I certainly had fun building it and had even more fun using it&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits&lt;/head&gt;
    &lt;p&gt;pimothyxd: Helped with the design of the UI! Always someone I can depend on.&lt;/p&gt;
    &lt;p&gt;bensb: Used his deobfuscator for the scripts tab. Also very knowledgable and a great person to chat ideas with.&lt;/p&gt;
    &lt;p&gt;samuelmaddock: Your electron-browser-shell project made it very easy to get this spun up.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nullpt.rs/reverse-engineering-browser"/><published>2025-10-07T17:03:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45506143</id><title>German government comes out against Chat Control</title><updated>2025-10-07T19:08:09.433758+00:00</updated><content>&lt;doc fingerprint="77d803d92c0426bd"&gt;
  &lt;main&gt;
    &lt;p&gt;Great news and big win for privacy in the EU! 🇪🇺🇩🇪 Germany’s ruling CDU/CSU party made it clear today: there will be no chat control - as pushed for by other EU countries - with this German government.&lt;/p&gt;
    &lt;p&gt;40 Sekunden kurz und präzise: Mit der CDU/CSU wird es keine anlasslose Chatkontrolle geben, wie sie von einigen Staaten in der EU gefordert wird.&lt;/p&gt;
    &lt;p&gt;Oct 7, 2025 · 4:13 PM UTC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xcancel.com/paddi_hansen/status/1975595307800142205"/><published>2025-10-07T17:31:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45506268</id><title>Less Is More: Recursive Reasoning with Tiny Networks</title><updated>2025-10-07T19:08:09.295816+00:00</updated><content>&lt;doc fingerprint="3cbf352bcdf5c28f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 6 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Less is More: Recursive Reasoning with Tiny Networks&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Alexia Jolicoeur-Martineau [view email]&lt;p&gt;[v1] Mon, 6 Oct 2025 14:58:08 UTC (259 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2510.04871"/><published>2025-10-07T17:42:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45506365</id><title>Solar energy is now the cheapest source of power, study</title><updated>2025-10-07T19:08:02.674149+00:00</updated><content>&lt;doc fingerprint="a545355d0abc55b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Solar energy is now the world’s cheapest source of power, a Surrey study finds&lt;/head&gt;
    &lt;p&gt;Solar energy is now so cost-effective that, in the sunniest countries, it costs as little as £0.02 to produce one unit of power, making it cheaper than electricity generated from coal, gas or wind, according to a new study from the University of Surrey.&lt;/p&gt;
    &lt;p&gt;In a study published in Energy and Environment Materials, researchers from Surrey’s Advanced Technology Institute (ATI) argue that solar photovoltaic (PV) technology is now the key driver of the world’s transition to clean, renewable power.&lt;/p&gt;
    &lt;p&gt;The research team also found that the price of lithium-ion batteries has fallen by 89% since 2010, making solar-plus-storage systems as cost-effective as gas power plants. These hybrid setups, which combine solar panels with batteries, are now standard in many regions and allow solar energy to be stored and released when needed, turning it into a more reliable, dispatchable source of power that helps balance grid demand.&lt;/p&gt;
    &lt;p&gt;Despite many reasons to be optimistic, the ATI research team points to several challenges – particularly connecting large amounts of solar power to existing electricity networks. In some regions, such as California and China, high solar generation has led to grid congestion and wasted energy when supply exceeds demand.&lt;/p&gt;
    &lt;p&gt;###&lt;/p&gt;
    &lt;p&gt;Notes to editors&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Professor Ravi Silva is available for interview; please contact mediarelations@surrey.ac.uk to arrange.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The full paper can be found here.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Related sustainable development goals&lt;/head&gt;
    &lt;head rend="h2"&gt;Featured Academics&lt;/head&gt;
    &lt;head rend="h2"&gt;Media Contacts&lt;/head&gt;
    &lt;p&gt;External Communications and PR team&lt;lb/&gt; Phone: +44 (0)1483 684380 / 688914 / 684378&lt;lb/&gt; Email: mediarelations@surrey.ac.uk&lt;lb/&gt; Out of hours: +44 (0)7773 479911&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.surrey.ac.uk/news/solar-energy-now-worlds-cheapest-source-power-surrey-study-finds"/><published>2025-10-07T17:50:17+00:00</published></entry></feed>