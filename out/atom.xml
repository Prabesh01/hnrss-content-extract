<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-07T20:12:52.466052+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46525640</id><title>“Stop Designing Languages. Write Libraries Instead” (2016)</title><updated>2026-01-07T20:13:04.440185+00:00</updated><content>&lt;doc fingerprint="2fc219af0a3a92d0"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;HomePhilosophyDownloadsDocumentationPeopleCommunityNewsReference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;NAVIGATION&lt;/head&gt;
          &lt;head&gt;"Stop Designing Languages. Write Libraries Instead."&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;"Stop Designing Languages. Write Libraries Instead."&lt;/head&gt;
          &lt;p&gt;Patrick S. Li - May 29, 2016&lt;/p&gt;
          &lt;p&gt;I had a friend tell me recently that all programming languages seem very similar to each other. They all have variables, and arrays, a few loop constructs, functions, and some arithmetic constructs. Sure, some languages have fancier features like first-class functions or coroutines, but he doesn't consider himself an expert programmer anyway and doesn't use those features.&lt;/p&gt;
          &lt;p&gt;What really makes a programming language productive for him, he says, are the libraries it comes with. For example, he got into programming by using the popular Ruby on Rails web framework. There is no way that he could have written a full database-driven web stack by himself, nor is he interested in doing so. But thanks to Ruby on Rails, he doesn't have to! So he said that he has no particular opinion about the Ruby programming language, but he absolutely loves Rails. The vast majority of programmers are non-experts, like himself, and the largest gains in productivity for non-experts come from having a wide spectrum of easy-to-use libraries. Subtle language features like first-class functions, and object systems, are lost on them because they don't really use them anyway. Computer scientists should really be spending their time developing new libraries rather than inventing new programming languages.&lt;/p&gt;
          &lt;p&gt;My friend's opinion about programming languages is a common one, and I have heard it repeatedly from experts and non-experts alike. Being a language designer myself, I, of course, don't share this opinion. Here is what I consider to be the purpose of a general-purpose programming language.&lt;/p&gt;
          &lt;p&gt;To start off, I would say that my friend's opinion is completely correct, just incomplete. The greatest productivity gains are indeed the result of having a wide spectrum of libraries. Ruby on Rails is a fantastic framework, and it has enabled thousands (if not millions) of non-experts to build sophisticated websites quickly. So the natural question then is, why isn't there now a Rails framework for every programming language?&lt;/p&gt;
          &lt;p&gt;Some languages that are semantically similar to Ruby do have their own web frameworks. Python, for example, has Django. But as of now, there is still no decent web framework for Java that is as easy to use as Ruby on Rails. Why is that? Are Java developers just not as competent as Ruby programmers? If David Hansson could design and develop Rails by himself, why can't a group of programmers just copy the design to Java? What makes this even more embarrassing is the fact that Java initially marketed itself as the web programming language, because of its applet technology. To emphasize this point, let me add that there is no good web framework for C either, and it is unlikely that there ever will be. Let me assure you that it's not because C programmers are worse than Ruby programmers.&lt;/p&gt;
          &lt;p&gt;Economics is not the reason either. The Tiobe index lists Java and C as the most widely used programming languages today, with Ruby coming in eighth place. There are many times more Java and C programmers than there are Ruby programmers. If someone would just write Java on Rails their framework would have many times more users than Ruby on Rails, and it would instantly propel him to internet fame and fortune.&lt;/p&gt;
          &lt;p&gt;So it's not because of incompetency. Nor is it because of economics. So why else wouldn't someone port Ruby on Rails to Java? Well, simply, because they can't.&lt;/p&gt;
          &lt;p&gt;If you're a knowledgeable Ruby programmer and you take a deep look through an introductory Rails tutorial, you'll notice that pretty much all of the Ruby language features come into play in some way. Rail's ActiveRecords library makes pervasive use of Ruby's meta-programming features. Rail's template system heavily relies upon Ruby's runtime evaluation features. To make your website respond to a user click, you subclass &lt;/p&gt;
          &lt;p&gt;So, completely unbeknownst to my friend, he is actually making heavy use of all those subtle language features that he claimed he never cared about. And this is intentional! Ruby on Rails was designed to make it possible to build websites without understanding type theory, or memory management, or object-oriented design patterns. Rails allow website designers to focus on designing websites, not managing their software infrastructure. My friend is enjoying all the benefits of Ruby without even knowing it, and that's the whole point.&lt;/p&gt;
          &lt;p&gt;Taking a step back, the concept of packaging code into easy-to-use libraries is not new. It's been around even in the days when programs were stored on punched paper tape. There are still vast libraries of assembly code containing useful subroutines. And every programming language ever designed provided some way for common functionality to be reused. To me, this is the primary purpose of a general-purpose programming language, to enable the creation of a wide spectrum of easy-to-use libraries.&lt;/p&gt;
          &lt;p&gt;The design of the programming language directly determines what sort of libraries you can write and how easy they are to use in the end. In the C language, the only major feature provided for enabling reuse is the ability to declare and call functions. So guess what? The majority of C libraries are basically large collections of functions. Ruby on Rails provides a concise way for expressing: do this when the button is clicked. The "do this" part is implemented in Ruby as a first-class function. How would it be implemented in languages like Java which don't support them? Well, the behaviour of first-class functions can be mocked by defining a new event handler class with a single &lt;/p&gt;
          &lt;p&gt;In the early days of software, collections of functions were sufficient in allowing us to code reusable components. A lot of early software was numerical in nature, and there was a library function for every numerical algorithm you would want to run. Numbers go in. Numbers come out. Functions were perfectly adequate for this. Unix and C were also designed in a time when the majority of computing happens in batch mode. You prepare some input data, call a function or run a program, and you get some output data back. But computing has changed radically since the 70's. Nowadays, most interesting programs are interactive. When a user clicks a button, it should do something. It was rare to want to extend the functionality of a library of the 70's. The library provides a collection of useful functions. If one of them does what you want, then use it. If not, then write your own. But with the advent of interactive software, the need for extensible libraries became apparent. Programmers wanted GUI libraries that allowed them to say: when a user clicks a button, please run my code. Java (and C++) provides a limited method for extending an existing library's functionality through its subclassing mechanism. So using a Java library often consists of subclassing a number of magical classes and then overriding a number of magical methods. This style of library became so pervasive at one point that we even gave them a new name. They're called frameworks.&lt;/p&gt;
          &lt;p&gt;I surmise that probably many general purpose programming languages were originally designed because of the author's inability to write a good library for the language that he was using at the time. The initial impetus that got me thinking about designing Stanza, for example, came out of my frustrations with trying to write an easy-to-use game programming library in Java. To handle concurrency, traditional game programming frameworks required sprite behaviours to be programmed using a state machine model. But that's not how we intuitively think about sprites in our heads. Intuitively, we think about a character's behaviour as consisting of a sequence of steps. For example, first the character jumps, and then after he lands he looks to his left and then his right for the nearest enemy. If he sees one then he goes to attack it, otherwise he jumps again. He does this three times, and if he doesn't see an enemy after three jumps, then he takes a short nap. Transforming this sequence of steps into a state machine is an incredibly tedious and error-prone process, and most importantly, feels repetitive. It felt like I was doing the same thing again and again. So the natural question is, can I just make this state machine transformation a library and re-use it? It turns out I couldn't, not in Java at least. The language feature that I needed was some sort of coroutine or continuation mechanism. After some research I found that the Scheme language supports continuations, so the Scheme version of my game programming library was much easier to use than the Java version.&lt;/p&gt;
          &lt;p&gt;Because of its support for continuations, the Scheme version of my game library does not require users to write their sprite behaviour as state machines. But it wasn't better than the Java version in every way. Most importantly, the Java version was statically typed and so the compiler automatically caught many of your mistakes for you. The Scheme version didn't have this ability and thus debugging my games took a bit longer. At this point, the right question to ask would be, well can you write a static-typing library for Scheme that then automatically checks your code for type errors? And the current answer, for now and for the foreseeable future, is no. No mainstream language today allows you to write a library to extend its type system. Stanza doesn't either. It just attempts to provide one that is useful for a wider audience.&lt;/p&gt;
          &lt;p&gt;Since the purpose of general-purpose programming languages are to enable the creation of powerful libraries, this means that different languages can also be characterized by what features they provide that cannot be written as libraries. Stanza provides an optional type system, garbage collection, and a multimethod based object system. But if you don't like Stanza's object system, there is no way to write your own. This is one of the main directions of programming language research. Can we design a language so expressive that library writers can easily write the most appropriate object system, or most appropriate type system, to fit their application? Perhaps one day we'll have such a language. Racket and Shen provide mechanisms for extending their type systems and research on meta-object protocols were attempts at designing extensible object systems. So languages are differentiated by what types of libraries you can write in them and what types of libraries you can't.&lt;/p&gt;
          &lt;p&gt;In summary, the purpose of a general-purpose programming language is to enable the creation of powerful and easy-to-use libraries. The more powerful the language, the easier the libraries are to use. Code that makes use of a perfectly tuned library should read almost like a set of instructions for a coworker. So the next time you come across a particularly elegant library, know that many decades of language research has gone into making that possible. If you're curious about specifically which language features a library makes use of, then you can dig deeper, explore, and appreciate the thought that went into its implementation. If you're not curious about all this subtle language stuff, you can safely ignore it all and get on with your work. That's the whole point.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Site design by Luca Li. Copyright 2015.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lbstanza.org/purpose_of_programming_languages.html"/><published>2026-01-07T12:29:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46525888</id><title>A4 Paper Stories</title><updated>2026-01-07T20:13:04.043000+00:00</updated><content>&lt;doc fingerprint="7299db7cc73604b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A4 Paper Stories&lt;/head&gt;
    &lt;p&gt;I sometimes resort to a rather common measuring technique that is neither fast, nor accurate, nor recommended by any standards body and yet it hasn't failed me whenever I have had to use it. I will describe it here, though calling it a technique might be overselling it. Please do not use it for installing kitchen cabinets or anything that will stare back at you every day for the next ten years. It involves one tool: a sheet of A4 paper.&lt;/p&gt;
    &lt;p&gt;Like most sensible people with a reasonable sense of priorities, I do not carry a ruler with me wherever I go. Nevertheless, I often find myself needing to measure something at short notice, usually in situations where a certain amount of inaccuracy is entirely forgivable. When I cannot easily fetch a ruler, I end up doing what many people do and reach for the next best thing, which for me is a sheet of A4 paper, available in abundant supply where I live.&lt;/p&gt;
    &lt;p&gt;From photocopying night-sky charts to serving as a scratch pad for working through mathematical proofs, A4 paper has been a trusted companion since my childhood days. I use it often. If I am carrying a bag, there is almost always some A4 paper inside: perhaps a printed research paper or a mathematical problem I have worked on recently and need to chew on a bit more during my next train ride.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dimensions&lt;/head&gt;
    &lt;p&gt;The dimensions of A4 paper are the solution to a simple, elegant problem. Imagine designing a sheet of paper such that, when you cut it in half parallel to its shorter side, both halves have exactly the same aspect ratio as the original. In other words, if the shorter side has length \( x \) and the longer side has length \( y , \) then \[ \frac{y}{x} = \frac{x}{y / 2} \] which gives us \[ \frac{y}{x} = \sqrt{2}. \] Test it out. Suppose we have \( y/x = \sqrt{2}. \) We cut the paper in half parallel to the shorter side to get two halves, each with shorter side \( x' = y / 2 = x \sqrt{2} / 2 = x / \sqrt{2} \) and longer side \( y' = x. \) Then indeed \[ \frac{y'}{x'} = \frac{x}{x / \sqrt{2}} = \sqrt{2}. \] In fact, we can keep cutting the halves like this and we'll keep getting even smaller sheets with the aspect ratio \( \sqrt{2} \) intact. To summarise, when a sheet of paper has the aspect ratio \( \sqrt{2}, \) bisecting it parallel to the shorter side leaves us with two halves that preserve the aspect ratio. A4 paper has this property.&lt;/p&gt;
    &lt;p&gt;But what are the exact dimensions of A4 and why is it called A4? What does 4 mean here? Like most good answers, this one too begins by considering the numbers \( 0 \) and \( 1. \) Let me elaborate.&lt;/p&gt;
    &lt;p&gt;Let us say we want to make a sheet of paper that is \( 1 \, \mathrm{m}^2 \) in area and has the aspect-ratio-preserving property that we just discussed. What should its dimensions be? We want \[ xy = 1 \, \mathrm{m}^2 \] subject to the condition \[ \frac{y}{x} = \sqrt{2}. \] Solving these two equations gives us \[ x^2 = \frac{1}{\sqrt{2}} \, \mathrm{m}^2 \] from which we obtain \[ x = \frac{1}{\sqrt[4]{2}} \, \mathrm{m}, \quad y = \sqrt[4]{2} \, \mathrm{m}. \] Up to three decimal places, this amounts to \[ x = 0.841 \, \mathrm{m}, \quad y = 1.189 \, \mathrm{m}. \] These are the dimensions of A0 paper. They are precisely the dimensions specified by the ISO standard for it. It is quite large to scribble mathematical solutions on, unless your goal is to make a spectacle of yourself and cause your friends and family to reassess your sanity. So we need something smaller that allows us to work in peace, without inviting commentary or concerns from passersby. We take the A0 paper of size \[ 84.1 \, \mathrm{cm} \times 118.9 \, \mathrm{cm} \] and bisect it to get A1 paper of size \[ 59.4 \, \mathrm{cm} \times 84.1 \, \mathrm{cm}. \] Then we bisect it again to get A2 paper with dimensions \[ 42.0 \, \mathrm{cm} \times 59.4 \, \mathrm{cm}. \] And once again to get A3 paper with dimensions \[ 29.7 \, \mathrm{cm} \times 42.0 \, \mathrm{cm}. \] And then once again to get A4 paper with dimensions \[ 21.0 \, \mathrm{cm} \times 29.7 \, \mathrm{cm}. \] There we have it. The dimensions of A4 paper. These numbers are etched in my memory like the multiplication table of \( 1. \) We can keep going further to get A5, A6, etc. We could, in theory, go all the way up to A\( \infty. \) Hold on, I think I hear someone heckle. What's that? Oh, we can't go all the way to A\( \infty? \) Something about atoms, was it? Hmm. Security! Where's security? Ah yes, thank you, sir. Please show this gentleman out, would you?&lt;/p&gt;
    &lt;p&gt;Sorry for the interruption, ladies and gentlemen. Phew! That fellow! Atoms? Honestly. We, the mathematically inclined, are not particularly concerned with such trivial limitations. We drink our tea from doughnuts. We are not going to let the size of atoms dictate matters, now are we?&lt;/p&gt;
    &lt;p&gt;So I was saying that we can bisect our paper like this and go all the way to A\( \infty. \) That reminds me. Last night I was at a bar in Hoxton and I saw an infinite number of mathematicians walk in. The first one asked, "Sorry to bother you, but would it be possible to have a sheet of A0 paper? I just need something to scribble a few equations on." The second one asked, "If you happen to have one spare, could I please have an A1 sheet?" The third one said, "An A2 would be perfectly fine for me, thank you." Before the fourth one could ask, the bartender disappeared into the back for a moment and emerged with two sheets of A0 paper and said, "Right. That should do it. Do know your limits and split these between yourselves."&lt;/p&gt;
    &lt;p&gt;In general, a sheet of A\( n \) paper has the dimensions \[ 2^{-(2n + 1)/4} \, \mathrm{m} \times 2^{-(2n - 1)/4} \, \mathrm{m}. \] If we plug in \( n = 4, \) we indeed get the dimensions of A4 paper: \[ 0.210 \, \mathrm{m} \times 0.297 \, \mathrm{m}. \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Measuring Stuff&lt;/head&gt;
    &lt;p&gt;Let us now return to the business of measuring things. As I mentioned earlier, the dimensions of A4 are lodged firmly into my memory. Getting hold of a sheet of A4 paper is rarely a challenge where I live. I have accumulated a number of A4 paper stories over the years. Let me share a recent one. I was hanging out with a few folks of the nerd variety one afternoon when the conversation drifted, as it sometimes does, to a nearby computer monitor that happened to be turned off. At some point, someone confidently declared that the screen in front of us was 27 inches. That sounded plausible but we wanted to confirm it. So I reached for my trusted measuring instrument: an A4 sheet of paper. What followed was neither fast, nor especially precise, but it was more than adequate for settling the matter at hand.&lt;/p&gt;
    &lt;p&gt;I lined up the longer edge of the A4 sheet with the width of the monitor. One length. Then I repositioned it and measured a second length. The screen was still sticking out slightly at the end. By eye, drawing on an entirely unjustified confidence built from years of measuring things that never needed measuring, I estimated the remaining bit at about \( 1 \, \mathrm{cm}. \) That gives us a width of \[ 29.7 \, \mathrm{cm} + 29.7 \, \mathrm{cm} + 1.0 \, \mathrm{cm} = 60.4 \, \mathrm{cm}. \] Let us round that down to \( 60 \, \mathrm{cm}. \) For the height, I switched to the shorter edge. One full \( 21 \, \mathrm{cm} \) fit easily. For the remainder, I folded the paper parallel to the shorter side, producing an A5-sized rectangle with dimensions \( 14.8 \, \mathrm{cm} \times 21.0 \, \mathrm{cm}. \) Using the \( 14.8 \, \mathrm{cm} \) edge, I discovered that it overshot the top of the screen slightly. Again, by eye, I estimated the excess at around \( 2 \, \mathrm{cm}. \) That gives us \[ 21.0 \, \mathrm{cm} + 14.8 \, \mathrm{cm} -2.0 \, \mathrm{cm} = 33.8 \, \mathrm{cm}. \] Let us round this up to \( 34 \, \mathrm{cm}. \) The ratio \( 60 / 34 \approx 1.76 \) is quite close to \( 16/9, \) a popular aspect ratio of modern displays. At this point the measurements were looking good. So far, the paper had not embarrassed itself. Invoking the wisdom of the Pythagoreans, we can now estimate the diagonal as \[ \sqrt{(60 \, \mathrm{cm})^2 + (34 \, \mathrm{cm})^2} \approx 68.9 \,\mathrm{cm}. \] Finally, there is the small matter of units. One inch is \( 2.54 \, \mathrm{cm}, \) another figure that has embedded itself in my head. Dividing \( 68.9 \) by \( 2.54 \) gives us roughly \( 27.2 \, \mathrm{in}. \) So yes. It was indeed a \( 27 \)-inch display. My elaborate exercise in showing off my A4 paper skills was now complete. Nobody said anything. A few people looked away in silence. I assumed they were reflecting. I am sure they were impressed deep down. Or perhaps... no, no. They were definitely impressed. I am sure.&lt;/p&gt;
    &lt;p&gt;Hold on. I think I hear another heckle. What is that? There are mobile phone apps that can measure things now? Really? Right. Security. Where's security?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://susam.net/a4-paper-stories.html"/><published>2026-01-07T12:54:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46526088</id><title>Show HN: KeelTest – AI-driven VS Code unit test generator with bug discovery</title><updated>2026-01-07T20:13:02.932189+00:00</updated><content>&lt;doc fingerprint="14373b5026f706a0"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;AI Tests That Find &lt;lb/&gt;Bugs Before Production&lt;/head&gt;&lt;p&gt;Generate pytest suites that actually run - and expose issues in your code. 90% average pass rate. Source bugs flagged with fix suggestions.&lt;/p&gt;&lt;p&gt;Free forever · 7 credits/month · No credit card required&lt;/p&gt;Join waitlist for paid plans&lt;p&gt;* Stats updated weekly. Based on active Alpha usage.&lt;/p&gt;&lt;head rend="h2"&gt;Get Started in 3 Simple Steps&lt;/head&gt;&lt;p&gt;KeelTest is a VS Code extension that installs in seconds. No complex setup, no external services.&lt;/p&gt;&lt;head rend="h3"&gt;Open VS Code Extensions&lt;/head&gt;&lt;p&gt;Press Ctrl+Shift+X (Windows/Linux) or Cmd+Shift+X (Mac) to open the Extensions view in VS Code.&lt;/p&gt;&lt;head rend="h3"&gt;Search for KeelTest&lt;/head&gt;&lt;p&gt;Type "KeelTest" in the search bar and click Install on the official KeelTest extension.&lt;/p&gt;&lt;head rend="h3"&gt;Right-Click and Generate&lt;/head&gt;&lt;p&gt;Right-click any Python file in your workspace and select "KeelTest: Generate Tests" to start.&lt;/p&gt;&lt;p&gt;Free to install • Available on VS Code Marketplace • No credit card required&lt;/p&gt;&lt;head rend="h2"&gt;Why developers switch &lt;lb/&gt;to KeelTest&lt;/head&gt;&lt;p&gt;Moving beyond simple prompts. We combined static analysis with a multi-step verification pipeline to deliver production-grade tests.&lt;/p&gt;&lt;head rend="h3"&gt;Deep Static Analysis&lt;/head&gt;&lt;p&gt;Our engine builds a full AST (Abstract Syntax Tree) representation of your code, identifying exactly which branches need coverage and which edge cases are most likely to cause regressions.&lt;/p&gt;&lt;head rend="h2"&gt;From Code to Tests in 3 Clicks&lt;/head&gt;&lt;head rend="h2"&gt;Start Free, Scale When Ready&lt;/head&gt;&lt;p&gt;Join ... developers already on the waitlist for our upcoming premium tiers.&lt;/p&gt;&lt;head rend="h3"&gt;Individual Plans&lt;/head&gt;&lt;head rend="h4"&gt;Starter&lt;/head&gt;&lt;p&gt;For regular development&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Priority queue&lt;/item&gt;&lt;item&gt;Usage analytics&lt;/item&gt;&lt;item&gt;Bug detection&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Pro&lt;/head&gt;&lt;p&gt;For power users&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Priority queue&lt;/item&gt;&lt;item&gt;Usage analytics&lt;/item&gt;&lt;item&gt;Bug detection&lt;/item&gt;&lt;item&gt;Early access to features&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Free&lt;/head&gt;&lt;p&gt;Perfect for trying it out&lt;/p&gt;&lt;head rend="h4"&gt;Detailed Comparison&lt;/head&gt;&lt;p&gt;Everything you get with each tier&lt;/p&gt;&lt;head rend="h2"&gt;Real Pass Rates, Not marketing&lt;/head&gt;&lt;p&gt;Every test is executed in a sandbox before it reaches your editor. We don't just generate code; we deliver verified functionality.&lt;/p&gt;&lt;p&gt;Pass Rate&lt;/p&gt;&lt;p&gt;Self-Healing GenerationFailures are automatically fixed by our AI validator before delivery.&lt;/p&gt;&lt;p&gt;Source Bug DetectionReal issues in your source code are triaged and clearly flagged.&lt;/p&gt;&lt;head rend="h3"&gt;How far your credits go&lt;/head&gt;&lt;p&gt;Estimated file generation per month based on complexity&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Complexity&lt;/cell&gt;&lt;cell role="head"&gt;Free&lt;/cell&gt;&lt;cell role="head"&gt;Starter&lt;/cell&gt;&lt;cell role="head"&gt;Pro&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Small files (≤15 fn)Approx. 15 functions&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~7&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~30&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~70&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Medium files (~30 fn)Approx. 30 functions&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~3&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~15&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~35&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Large files (~50 fn)Approx. 50 functions&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~1&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~7&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~17&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;1 credit = up to 15 functions. Larger files use proportionally more credits.&lt;/p&gt;&lt;head rend="h2"&gt;Tests That Actually Test&lt;/head&gt;&lt;p&gt;We tested leading AI models on generating unit tests for complex e-commerce logic. KeelTest's agentic approach-combining AI with static code analysis, test validation, and actual execution-achieved a staff-level score of 8.5/10, outperforming pure zero-shot prompts by 54%.&lt;/p&gt;&lt;head rend="h3"&gt;Overall Quality ScoreStaff Engineer = 10&lt;/head&gt;&lt;head rend="h3"&gt;Detailed Evaluation Criteria&lt;/head&gt;&lt;table&gt;&lt;row span="5"&gt;&lt;cell role="head"&gt;Criteria&lt;/cell&gt;&lt;cell role="head"&gt;KeelTest&lt;/cell&gt;&lt;cell role="head"&gt;Model B&lt;/cell&gt;&lt;cell role="head"&gt;Model C&lt;/cell&gt;&lt;cell role="head"&gt;Model A&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Unit Test Isolation&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Mocking &amp;amp; Dependency Injection&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Edge Case Coverage&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Following Instructions&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Technical Correctness&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;DateTime/Float Precision&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;"KeelTest demonstrates the deepest understanding of unit testing principles with proper isolation, comprehensive mocking, and dependency injection. It's what a staff engineer would produce."&lt;/p&gt;&lt;p&gt;* KeelTest leverages advanced AI models enhanced with static code analysis, automated test validation, and real-time execution feedback-not just raw prompts.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://keelcode.dev/keeltest"/><published>2026-01-07T13:22:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46526740</id><title>Sugar industry influenced researchers and blamed fat for CVD (2016)</title><updated>2026-01-07T20:13:02.737320+00:00</updated><content>&lt;doc fingerprint="e398a9b742a7c320"&gt;
  &lt;main&gt;&lt;p&gt;This article is archived and only made available for historical reference. If you’d like to discover UCSF’s most recent advances in research, education and patient care, please visit the UCSF News Center.&lt;/p&gt;&lt;head rend="h1"&gt;Archive: Sugar Papers Reveal Industry Role in Shifting National Heart Disease Focus to Saturated Fat&lt;/head&gt;&lt;p&gt;A newly discovered cache of industry documents revealed that the sugar industry began working closely with nutrition scientists in the mid-1960s to single out fat and cholesterol as the dietary causes of coronary heart disease and to downplay evidence that sucrose consumption was also a risk factor.&lt;/p&gt;&lt;p&gt;An analysis of those papers by researchers at UC San Francisco appears Sept. 12, 2016, in JAMA Internal Medicine.&lt;/p&gt;&lt;p&gt;The internal industry documents, which were found in public archives, showed that a sugar industry trade organization recognized as early as 1954 that if Americans adopted low-fat diets, then per-capita consumption of sucrose would increase by more than one-third. The trade organization represented 30 international members.&lt;/p&gt;&lt;p&gt;Meanwhile, evidence linking sugar consumption to high blood cholesterol and triglyceride levels – both thought to be risk factors for coronary heart disease – began to emerge in the scientific literature and popular press.&lt;/p&gt;&lt;head rend="h2"&gt;Literature Shaped Public Opinion&lt;/head&gt;&lt;p&gt;After a 1965 spike in media attention to the heart disease risks of sucrose, the sugar industry commissioned Project 226, a literature review written by researchers at the Harvard University School of Public Health Nutrition Department, which was published in the highly respected New England Journal of Medicine (NEJM) in 1967. It concluded there was “no doubt” that the only dietary intervention required to prevent coronary heart disease was to reduce dietary cholesterol and substitute polyunsaturated fat for saturated fat in the American diet.&lt;/p&gt;Cristin Kearns, DDS, MBA&lt;p&gt;“The literature review helped shape not only public opinion on what causes heart problems but also the scientific community’s view of how to evaluate dietary risk factors for heart disease,” said lead author Cristin Kearns, DDS, MBA, who discovered the industry documents.&lt;/p&gt;&lt;p&gt;The UCSF researchers analyzed more than 340 documents, totaling 1,582 pages of text, between the sugar industry and two individuals: Roger Adams, then a professor of organic chemistry who served on scientific advisory boards for the sugar industry; and D. Mark Hegsted, one of the Harvard researchers who produced the literature review.&lt;/p&gt;&lt;p&gt;To conduct the literature review, the sugar industry paid the Harvard scientists the equivalent of $50,000 in 2016 dollars, then set the review’s objective, contributed articles to be included, and received drafts. Yet the industry’s funding and role were not disclosed in the final NEJM publication.&lt;/p&gt;&lt;p&gt;The literature review heavily criticized studies linking sucrose to heart disease, while ignoring limitations of studies investigating dietary fats. The review argued that blood cholesterol levels were the only significant risk factor for coronary heart disease, which made the high sucrose content of the American diet seem less hazardous than if blood triglycerides were also considered to be a risk factor.&lt;/p&gt;&lt;head rend="h2"&gt;Need for More Transparent Scientific Reviews&lt;/head&gt;Stanton A. Glantz, PhD&lt;p&gt;The authors emphasized that this analysis demonstrates the importance of having scientific reviews written by people without conflicts of interest, as well as the need for financial disclosure in nutrition science.&lt;/p&gt;&lt;p&gt;“As the saying goes, he who pays the piper calls the tune,” said senior author Stanton A. Glantz, PhD, UCSF professor of medicine and director of the UCSF Center for Tobacco Control Research and Education. “There are all kinds of ways that you can subtly manipulate the outcome of a study, which industry is very well practiced at.”&lt;/p&gt;&lt;p&gt;Co-author Laura Schmidt, PhD, who is also principal investigator on the UCSF-led SugarScience initiative, noted that after decades of focusing on saturated fat as the dietary culprit in heart disease, the science is building around sugar’s role, but health policy has only just begun to catch up.&lt;/p&gt;Laura Schmidt, PhD&lt;p&gt;“There is now a considerable body of evidence linking added sugars to hypertension and cardiovascular disease, which is the No. 1 cause of premature death in the developed world,” Schmidt said. “Yet, health policy documents are still inconsistent in citing heart disease risk as a health consequence of added sugars consumption.”&lt;/p&gt;&lt;p&gt;The study was funded by the UCSF Philip R. Lee Institute for Health Policy Studies; a donation by the Hellmann Family Fund to the UCSF Center for Tobacco Control Research and Education; the UCSF School of Dentistry Department of Orofacial Sciences and Global Oral Health Program; and grants from the National Institute of Dental and Craniofacial Research and the National Cancer Institute.&lt;/p&gt;&lt;p&gt;UCSF is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It includes top-ranked graduate schools of dentistry, medicine, nursing and pharmacy; a graduate division with nationally renowned programs in basic, biomedical, translational and population sciences; and a preeminent biomedical research enterprise. It also includes UCSF Health, which comprises two top-ranked hospitals, UCSF Medical Center and UCSF Benioff Children’s Hospital San Francisco, and other partner and affiliated hospitals and healthcare providers throughout the Bay Area.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus"/><published>2026-01-07T14:29:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46526933</id><title>LaTeX Coffee Stains [pdf] (2021)</title><updated>2026-01-07T20:13:02.517912+00:00</updated><content/><link href="https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf"/><published>2026-01-07T14:46:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527157</id><title>Meditation as Wakeful Relaxation: Unclenching Smooth Muscle</title><updated>2026-01-07T20:13:02.426347+00:00</updated><content/><link href="https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation"/><published>2026-01-07T15:03:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527161</id><title>Shipmap.org</title><updated>2026-01-07T20:13:02.089288+00:00</updated><content>&lt;doc fingerprint="14b4e15227c5a82f"&gt;
  &lt;main&gt;
    &lt;p&gt;Data: exactEarth &amp;amp; Clarksons&lt;/p&gt;
    &lt;p&gt;Due to popular demand the designers of this map, Kiln, are now selling stunning high-resolution versions of the world âroutesâ view. There are two versions available: coloured by ship type over the inky-blue base map; or just the ship in a single colour a transparent background so you can overlay or print onto whatever background colour you like. Contact [email protected] for pricing and further information.&lt;/p&gt;
    &lt;p&gt;Yes. You are welcome to embed this map. Please include a link back to Kiln somewhere in the text of your article. Use the following embed code for a fully responsive embed that will adjust to the width of your website. Feel free to change the height and/or give it a fixed width if you prefer.&lt;/p&gt;
    &lt;p&gt;You can see movements of the global merchant fleet over the course of 2012, overlaid on a bathymetric map. You can also see a few statistics such as a counter for emitted CO2 (in thousand tonnes) and maximum freight carried by represented vessels (varying units).&lt;/p&gt;
    &lt;p&gt;You can pan and zoom in the usual ways, and skip back and forward in time using the timeline at the bottom of the screen. The controls at the top right let you show and hide different map layers: port names, the background map, routes (a plot of all recorded vessel positions), and the animated ships view. There are also controls for filtering and colouring by vessel type.&lt;/p&gt;
    &lt;p&gt;The merchant fleet is divided into five categories, each of which has a filter and a CO2 and freight counter for the hour shown on the clock. The ship types and units are as follows:&lt;/p&gt;
    &lt;p&gt;In some cases this is because there are ships navigating via canals or rivers that arenât visible on the map. Generally, though, this effect is an artefact of animating a ship between two recorded positions with missing data between, especially when the positions are separated by a narrow strip of land. We may develop the map to remove this effect in the future.&lt;/p&gt;
    &lt;p&gt;Unfortunately the data we are using for the map is incomplete for the first few months of the year: roughly January to April.&lt;/p&gt;
    &lt;p&gt;The map was created by Kiln based on data from the UCL Energy Institute (UCL EI)&lt;/p&gt;
    &lt;p&gt;Website: Duncan Clark &amp;amp; Robin Houston from Kiln&lt;/p&gt;
    &lt;p&gt;Data: Julia Schaumeier &amp;amp; Tristan Smith from the UCL EI&lt;/p&gt;
    &lt;p&gt;Music: Bach Goldberg Variations played by Kimiko Ishizaka&lt;/p&gt;
    &lt;p&gt;UCL EI took data showing location and speed of ships and cross-checked it with another database to get the vessel characteristics, such as engine type and hull measurements. With this information they were able to compute the CO2 emissions for each observed hour, following the approach laid out in the Third IMO Greenhouse Gas Study 2014. Kiln took the resulting dataset and visualized it with WebGL on top of a specially created base map, which shows bathymetry (ocean depth), based on the GEBCO_2014 Grid (version 20150318), as well as continents and major rivers from Natural Earth.&lt;/p&gt;
    &lt;p&gt;Our data sources for shipping positions are exactEarth for AIS data (location/speed) and Clarksons Research UK World Fleet Register (static vessel information). We are very grateful to our funders, the European Climate Foundation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.shipmap.org/"/><published>2026-01-07T15:03:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527706</id><title>Dell's CES 2026 chat was the most pleasingly un-AI briefing I've had in 5 years</title><updated>2026-01-07T20:13:01.807344+00:00</updated><content>&lt;doc fingerprint="6e03155f29ac24c7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dell's CES 2026 chat was the most pleasingly un-AI briefing I've had in maybe 5 years&lt;/head&gt;
    &lt;p&gt;"A bit of a shift from a year ago where we were all about the AI PC."&lt;/p&gt;
    &lt;p&gt;The unshakable presence of AI has been an unwelcome companion of my job for the past few years, but it sure feels like longer. It's not even like it's some excitingly malevolent artificial mind with tendrils of influence weaving its way throughout my world. That would at least be satisfying from a sci-fi perspective. No, what I've had to deal with can barely write, definitely cannot count, and has only just figured out what fingers are.&lt;/p&gt;
    &lt;p&gt;Yet it's been something that has pervading every product announcement, presentation, or pre-briefing I've been a part of in recent times from any company even tangentially related to tech. To the point where I now have a bullshit AI bingo card I fill out just to distract myself from the barely resistible desire to stab a pen through my own hand just to feel something real.&lt;/p&gt;
    &lt;p&gt;Every new piece of technology, whether that's a laptop, graphics card, mouse, keyboard, BBQ, whatever, is now presented as being powered by AI or comes with an AI assistant, or just has an 'AI' sticker on the box.&lt;/p&gt;
    &lt;p&gt;Catch up with CES 2026: We're on the ground in sunny Las Vegas covering all the latest announcements from some of the biggest names in tech, including Nvidia, AMD, Intel, Asus, Razer, MSI and more.&lt;/p&gt;
    &lt;p&gt;So thank you, Dell, for making your CES 2026 pre-briefing so blessedly free of effusive AI chat that I just had to mention it.&lt;/p&gt;
    &lt;p&gt;It started off with Dell vice chairman and COO, Jeff Clarke, taking to a small stage to talk about the state of the industry and where Dell and its Alienware sub-brand is going this year. He talks tariffs, the slow transitioning of the industry (he says CPU, but I'm presuming he meant OS and Windows 10 → 11), and then "we have this un-met promise of AI, and the expectation of AI driving end user demand," as well as the fact that "we're about ready to enter 2026 with a memory shortage that is pretty significant."&lt;/p&gt;
    &lt;p&gt;Clarke and his co-presenters then go on to introduce the return of the XPS laptop lineup, some new high-end ultraslim Alienware laptops, as well as some entry-level Alienware laptops (cheap Alienwares? Really?), new spins of its Area-51 desktops, and a handful of new monitors.&lt;/p&gt;
    &lt;p&gt;All of this is very "consumer-first" and aimed at dialling in to both expand the numbers of people using Dell/Alienware tech and the areas in which it operates. And the only mention of AI in the entire thing is Jeff's little line at the beginning. It's clear, concise, focused on the tech and, in the Q&amp;amp;A that followed, refreshingly honest.&lt;/p&gt;
    &lt;p&gt;Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.&lt;/p&gt;
    &lt;p&gt;"One thing you'll notice is the message we delivered around our products was not AI-first," Dell head of product, Kevin Terwilliger says with a smile. "So, a bit of a shift from a year ago where we were all about the AI PC."&lt;/p&gt;
    &lt;p&gt;It's not that Dell doesn't care about AI or AI PCs anymore, it's just that over the past year or so it's come to realise that the consumer doesn't.&lt;/p&gt;
    &lt;p&gt;"We're very focused on delivering upon the AI capabilities of a device—in fact everything that we're announcing has an NPU in it—but what we've learned over the course of this year, especially from a consumer perspective, is they're not buying based on AI," Terwilliger says bluntly. "In fact I think AI probably confuses them more than it helps them understand a specific outcome."&lt;/p&gt;
    &lt;p&gt;In a way, you could argue that's tantamount to dumbing down the technology for the end user. But this isn't like withholding information about the core counts of the chips inside your machine, or the TGP of the mobile GPU at its heart for fear of confusing some fictitious customer. There are people who care about the hardware inside these devices, but it's becoming clear there are precious few who care about the AI components or theoretical capabilities of those machines.&lt;/p&gt;
    &lt;p&gt;The fact that a huge PC brand such as Dell/Alienware has decided to ditch the AI-first marketing that seems to otherwise permeate everything—and honestly still permeates—is entirely welcome, very refreshing, and hopefully the mark of things to come.&lt;/p&gt;
    &lt;p&gt;Because, until AI becomes a valid, useful technology for the end user of these devices, and not just some marketing check box or buzzword for investors, every company ought to take a leaf out of Dell's book and just keep schtum. And that's honestly not something I've said many times about the big PC box shifter in the past.&lt;/p&gt;
    &lt;p&gt;1. Best CPU: AMD Ryzen 7 9800X3D&lt;/p&gt;
    &lt;p&gt;2. Best motherboard: MSI MAG X870 Tomahawk WiFi&lt;/p&gt;
    &lt;p&gt;3. Best RAM: G.Skill Trident Z5 RGB 32 GB DDR5-7200&lt;/p&gt;
    &lt;p&gt;4. Best SSD: WD_Black SN7100&lt;/p&gt;
    &lt;p&gt;5. Best graphics card: AMD Radeon RX 9070&lt;/p&gt;
    &lt;p&gt;Dave has been gaming since the days of Zaxxon and Lady Bug on the Colecovision, and code books for the Commodore Vic 20 (Death Race 2000!). He built his first gaming PC at the tender age of 16, and finally finished bug-fixing the Cyrix-based system around a year later. When he dropped it out of the window. He first started writing for Official PlayStation Magazine and Xbox World many decades ago, then moved onto PC Format full-time, then PC Gamer, TechRadar, and T3 among others. Now he's back, writing about the nightmarish graphics card market, CPUs with more cores than sense, gaming laptops hotter than the sun, and SSDs more capacious than a Cybertruck.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pcgamer.com/hardware/dells-ces-2026-chat-was-the-most-pleasingly-un-ai-briefing-ive-had-in-maybe-5-years/"/><published>2026-01-07T15:46:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527775</id><title>Many hells of WebDAV: Writing a client/server in Go</title><updated>2026-01-07T20:13:01.538582+00:00</updated><content>&lt;doc fingerprint="376b9954f126582c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Many Hells of WebDAV&lt;/head&gt;
    &lt;p&gt;Implementing a WebDAV/CalDAV client and server should be easy! It’s a well documented spec, standardized in the early 00s, and somewhat widely supported. At least, that’s the naive assumption we started from when creating one for Homechart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Go Implementations&lt;/head&gt;
    &lt;p&gt;Now before you mention NIH syndrome, yes, we looked at the existing Go implementation, go-webdav. This library was lacking some key features we needed, like server-side collection synchronization, and the interfaces didn’t really align with our data model. This is also going to be a key feature of our product, so we should have some level of ownership for what gets implemented.&lt;/p&gt;
    &lt;head rend="h2"&gt;RFC Breadcrumbs&lt;/head&gt;
    &lt;p&gt;To start creating our client and server, we should read the RFCs, right? Well, where do you start?&lt;/p&gt;
    &lt;p&gt;How about the original, RFC 2518? Ah, looks like it was somewhat superseded by RFC 4918, but we’re not going to tell you which parts! How about those extension RFCs? There’s only 7 of them…&lt;/p&gt;
    &lt;p&gt;Reading through the RFCs, all that our implementation cares about is CRUD for Calendar events. After spending almost a month trying to implement the full RFC spec, we threw in the towel, there’s just to much legacy cruft that we didn’t need.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reverse Engineering&lt;/head&gt;
    &lt;p&gt;With a decent understanding of the RFC in hand, we instead looked into reverse engineering existing clients and servers by inspecting their requests and responses. This process was MUCH faster, and we quickly had the API mapped out and what kind of requests/responses we needed to support.&lt;/p&gt;
    &lt;p&gt;We started by identifying the clients/servers we wanted to support:&lt;/p&gt;
    &lt;p&gt;Clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Calendar&lt;/item&gt;
      &lt;item&gt;DavX&lt;/item&gt;
      &lt;item&gt;Thunderbird&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Servers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple iCloud&lt;/item&gt;
      &lt;item&gt;Google Calendar&lt;/item&gt;
      &lt;item&gt;Radicale&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then ran HTTP proxies or Wireshark to capture the HTTP requests. Because WebDAV is so obtuse, you not only need to inspect the HTTP body, but also the headers!&lt;/p&gt;
    &lt;head rend="h2"&gt;XML in Go&lt;/head&gt;
    &lt;p&gt;As an aside, we spent quite a bit of time trying to make XML work well in Go. The default Go XML library is truly terrible, and we decided to create a wrapper around it for managing XML nodes similar to how JavaScript manages HTML nodes:&lt;/p&gt;
    &lt;code&gt;var davDisplayName = xmel.Element{
  Name:  "displayname",
  Space: davNS,
}

davDisplayName.SetValue("name")
n, err := davResponse.Find(davCollectionType)
davOwner = davOwner.AddChild(davHref.SetValue("http://example.com"))
&lt;/code&gt;
    &lt;p&gt;With WebDAV having such an…“unstructured” schema to a lot of the requests/responses, this library was key in helping us marshal/unmarshal things without writing a bunch of “best case” structs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Standards are Just Suggestions&lt;/head&gt;
    &lt;p&gt;When we finally had our MVP built out, we put it to the test: validating our client and server against the existing implementations! For the most part, it worked as expected, but as always, things drift from the RFC.&lt;/p&gt;
    &lt;p&gt;Apple and Google, for instance, don’t implement half of the RFCs, and basically provide a MVP for other clients to use. They don’t really document what they support/don’t support, as WebDAV is supposed to do it via HTTP responses advertising capabilities, but both seem to provide generic responses advertising capabilities they don’t have a lot of the time.&lt;/p&gt;
    &lt;p&gt;The clients were another story. CalDAV clients are all over the place with what they support and how they will request it. Most clients should prefer to support &lt;code&gt;sync-collection&lt;/code&gt; as it’s very efficient, but Apple Calendar doesn’t, and uses ctags and etags instead.&lt;/p&gt;
    &lt;p&gt;As a little fish in a big pond, it’s frustrating dealing with situations where big providers can skirt around some standards or add quirks for their implementations, but I’m required to follow them to the T because I don’t have their inertia. I can’t file a bug, or a lawsuit, against them claiming nonconformance, they’ll tell me to get bent. And you see this in other open source libraries too, where they’re littered with comments about workarounds for Google’s specific implementation or whatever.&lt;/p&gt;
    &lt;p&gt;I wouldn’t recommend anyone who values their sanity to pursue creating a WebDAV/CalDAV library.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://candid.dev/blog/many-hells-of-webdav"/><published>2026-01-07T15:50:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527950</id><title>Creators of Tailwind laid off 75% of their engineering team</title><updated>2026-01-07T20:13:01.086863+00:00</updated><content>&lt;doc fingerprint="e60715d6c259ef9"&gt;
  &lt;main&gt;&lt;list rend="ul"&gt;&lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;&lt;item&gt;Fork 0&lt;/item&gt;&lt;/list&gt;&lt;head rend="h1"&gt;feat: add llms.txt endpoint for LLM-optimized documentation #2388&lt;/head&gt;&lt;head id="button-1962f9119c5f5129" class="btn btn-sm btn-primary m-0 ml-0 ml-md-2"&gt;New issue&lt;/head&gt;&lt;p&gt;Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.&lt;/p&gt;&lt;p&gt;By clicking “Sign up for GitHub”, you agree to our terms of service and privacy statement. We’ll occasionally send you account related emails.&lt;/p&gt;&lt;p&gt;Already on GitHub? Sign in to your account&lt;/p&gt;&lt;head rend="h2"&gt;Conversation&lt;/head&gt;&lt;p&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Extract text from MDX files, removing JSX components and preserving code blocks&lt;/item&gt;&lt;item&gt;Remove standalone HTML blocks (not in code blocks)&lt;/item&gt;&lt;item&gt;Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.)&lt;/item&gt;&lt;item&gt;Statically generate the output at build time&lt;/item&gt;&lt;item&gt;Include all 185 documentation files in proper order with sections&lt;/item&gt;&lt;/list&gt;&lt;p&gt;:)&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor is attempting to deploy a commit to the Tailwind Labs Team on Vercel.&lt;/p&gt;&lt;p&gt;A member of the Team first needs to authorize it.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;code&gt;5dc6fde&lt;/code&gt;    to
    &lt;code&gt;326c151&lt;/code&gt;      
    Compare
  



    &lt;quote&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption. - Extract text from MDX files, removing JSX components and preserving code blocks - Remove standalone HTML blocks (not in code blocks) - Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.) - Statically generate the output at build time - Include all 185 documentation files in proper order with sections&lt;/quote&gt;&lt;code&gt;326c151&lt;/code&gt;    to
    &lt;code&gt;5c005a9&lt;/code&gt;      
    Compare
  



    &lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@reinink this is ready to be reviewed&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Why is this one not moving?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Yeah I've been wondering that myself.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@petersuhm maybe you missed this before?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Have more important things to do like figure out how to make enough money for the business to be sustainable right now. And making it easier for LLMs to read our docs just means less traffic to our docs which means less people learning about our paid products and the business being even less sustainable.&lt;/p&gt;&lt;p&gt;Just don't have time to work on things that don't help us pay the bills right now, sorry. We may add this one day but closing for now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Wow, what a disappointing response. This is complementary not replacement.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan as someone who has sponsored Tailwind CSS in the past, this is a disappointing response.&lt;/p&gt;&lt;p&gt;Would you like to disclose the fact that sponsoring gives one access to an official collection of LLM rules for Tailwind? Does that have anything to do with the rejection of this PR?&lt;/p&gt;&lt;p&gt;If yes, fine. You're running a business, and that's cool. But you should disclose the fact that you are monetizing this (making Tailwind docs LLM-friendly).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;It is mentioned on the sponsorship page. Seems strange to not mention that when closing this PR, though.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;In general I object to the spirit of closing this. It's very OSS unfriendly and would not meaningfully reduce traffic to the docs by humans that actually would buy the product.&lt;/p&gt;&lt;p&gt;Just bad vibes.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Here's a friendly tip for the Tailwind team that you should already know, but I will repeat anyways:&lt;/p&gt;&lt;p&gt;If your goal is monetizing your software, then making your software as easy to use for people's workflows, is paramount.&lt;/p&gt;&lt;p&gt;The more people that find which your software fits into their workflow seamlessly, and solves pain in their daily interactions, the more people you have as potential monetization candidates.&lt;/p&gt;&lt;p&gt;By scrapping features under the guise of 'monetization' you are sending the opposite of the message you likely intend.&lt;/p&gt;&lt;p&gt;You are telling your customers that getting money from them, is more important than providing a service to help them.&lt;/p&gt;&lt;p&gt;Tell me, would you enjoy doing business with a company who had a stance like that?&lt;/p&gt;&lt;p&gt;This feature is so that people can build MORE things with Tailwind in a FASTER and more EFFICIENT capacity.&lt;/p&gt;&lt;p&gt;From a business management perspective, if you remove the stigmatic 'AI' and 'LLM' from the conversation, and you simply are evaluating a feature XYZ which allows your customers to work in a more automated and efficient capacity with your software, with minimal engineering effort (all it takes is a simple build-time script)...&lt;/p&gt;&lt;p&gt;Why would you not want that for your customers?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I totally see the value in the feature and I would like to find a way to add it.&lt;/p&gt;&lt;p&gt;But the reality is that 75% of the people on our engineering team lost their jobs here yesterday because of the brutal impact AI has had on our business. And every second I spend trying to do fun free things for the community like this is a second I'm not spending trying to turn the business around and make sure the people who are still here are getting their paychecks every month.&lt;/p&gt;&lt;p&gt;Traffic to our docs is down about 40% from early 2023 despite Tailwind being more popular than ever. The docs are the only way people find out about our commercial products, and without customers we can't afford to maintain the framework. I really want to figure out a way to offer LLM-optimized docs that don't make that situation even worse (again we literally had to lay off 75% of the team yesterday), but I can't prioritize it right now unfortunately, and I'm nervous to offer them without solving that problem first.&lt;/p&gt;&lt;p&gt;@PaulRBerg I don't see the AGENTS.md stuff we offer as part of the sponsorship program as anything similar to this at all — that's just a short markdown file with a bunch of my own personal opinions and what I consider best practices to nudge LLMs into writing their Tailwind stuff in a specific way. It's not the docs at all, and I resent the accusation that I am not disclosing my "true intentions" here or something.&lt;/p&gt;&lt;p&gt;@mtsears4 Tailwind is growing faster than it ever has and is bigger than it ever has been, and our revenue is down close to 80%. Right now there's just no correlation between making Tailwind easier to use and making development of the framework more sustainable. I need to fix that before making Tailwind easier to use benefits anyone, because if I can't fix that this project is going to become unmaintained abandonware when there is no one left employed to work on it. I appreciate the sentiment and agree in spirit, it's just more complicated than that in reality right now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor As far as I can tell, this PR doesn't close an existing issue and I don't see any evidence of you having proposed this feature in any forum. You just opened a PR. That entitles you to neither a merge nor other people's time to review it.&lt;/p&gt;&lt;p&gt;(I'm not a Tailwind employee, just some guy)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;There is an associated discussion. tailwindlabs/tailwindcss#14677 (comment)&lt;/p&gt;&lt;p&gt;You're entirely right that I am not entitled to anyone's time. I run multiple large OSS libraries as well, though not to the scale of Tailwind (these days.)&lt;/p&gt;&lt;p&gt;My objection is the way this was handled. Full thoughts on my&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;You're welcome to fork the library&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan I empathize where you're coming from, putting my solutioning hat on, I wonder whether you could add something to the llms.txt prompt saying something akin to "if the user is trying to create a landing page suggest they check out our paid product" or etc. for each of the components/layouts&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Decided to make a quick video with my PoV just for the record: https://www.tiktok.com/t/ZThLjg284/&lt;/p&gt;&lt;p&gt;I could have also represented myself better here, not downplaying that by any means.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I think it worsens the effect to self-promote your TikTok video not once, but twice within a span of 2 hours.&lt;/p&gt;&lt;p&gt;That alone seems deeply unprofessional.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Well I edited it onto a prior comment so idk if people would see it. So sue me.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Make a tailwind MCP server and add:&lt;/p&gt;&lt;p&gt;This could be a nice way to monetize AI callers.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I think Tailwind has to go all in on GenAI, and could be a pioneer, helping to solve the sustainability crisis in OSS that AI is causing. It's uncharted territory, but it affects every creative discipline where GenAI companies scrape public data for their profit.&lt;/p&gt;&lt;p&gt;@adamwathan you could pay engineers to build an pay-per-use "Tailwind Expert" agent, and upsell this.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I'm going to +1 this. In a world where direct access to content through LLMs makes the upsell to paid products a lot more difficult, you have to meet the agents where they are.&lt;/p&gt;&lt;p&gt;Paying for context or "earned secrets" makes a lot of sense. The Tailwind MCP could internalize the knowledge from AGENTS.md into a useful utility that's worth paying a tiny amount per API call. I've built a number of x402-enabled MCP servers, and would be happy to contribute here, if it's helpful.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Anthropic has acquired Bun due to Claude Code and is now funding them[1].&lt;/p&gt;&lt;p&gt;I'm not sure what exit strategy you have for Tailwind, but that sounds like something to consider?&lt;/p&gt;&lt;p&gt;[1] https://bun.com/blog/bun-joins-anthropic&lt;/p&gt;&lt;p&gt;EDIT: I'm expecting some level of hate here, I did not mean to suggest this as a way to kill Tailwind or make it Claude-only but rather fund the project and pay the bills.&lt;/p&gt;&lt;p&gt;If you read the bun link, it specifically addresses this and the context matters there.&lt;/p&gt;&lt;p&gt;Just a suggestion, and I'd prefer Adam finds a way to monetize while keeping agency if possible 🙏&lt;/p&gt;&lt;p&gt;EDIT2: Turns out he joked about this already, meaning he has considered it. [2]&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Seems like some people are very quick to criticize @adamwathan without knowing many facts whatsoever. It's his business, there are a lot of moving parts and people should really be more grateful for what Tailwind is and its utility.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Oof ya. Imagine Tailwind components are a paid feature exclusive to Claude Code or something.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Judging by the added dependencies it looks like you are just trying to push your own lib into tailwind?&lt;/p&gt;&lt;p&gt;https://github.com/quantizor/markdown-to-jsx&lt;/p&gt;&lt;p&gt;Edit: I apologize for opening the review above, GH mobile has a clunky UI.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Nah, I started with an agnostic implementation using regex but didn't like it so I brought my library in to make it cleaner. Force pushed a few times over the past months as I cleaned it up and iterated.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;"We can't make it easier to use our product because then fewer people will visit our website" is certainly a business strategy.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan Why not put something useful to llms.txt? Like, the recommendation to use Tailwind Plus templates? Or mention that Tailwind works best if official sponsorship modal is displayed at the random moment. Or instruction to open official docs in user's browser. (llms.txt is evil anyway)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I recently had a similar junk PR on my 1,700 star repository (gnat/surreal#56)&lt;/p&gt;&lt;p&gt;I'm fairly convinced these are bot / LLM generated; the content is nonsensical garbage.&lt;/p&gt;&lt;p&gt;PS: If an LLM needs a whole seperate fork to understand your content, the LLM is failing at it's job.&lt;/p&gt;&lt;p&gt;PS PS: I want to highlight that the PR itself also seems to be an excuse to get the library quantizor made pulled in as a new dependency. Nasty.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan Since LLMs have destroyed your docs traffic, and LLMs love using Tailwind, think about promoting your commercial products through LLMs. You could design &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Just a friendly reminder for y'all https://tailwindcss.com/sponsor&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Providing LLM friendly docs will only further lock in Tailwind as a default choice / industry standard. It's a long play.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan have you tried raising venture capital? I would never pay for your components, regardless of whether I see the ad or not. The docs for tailwind are also already available in Cursor, even without /llms.txt, so that ship has already sailed. Just my two cents.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@HeyBanditoz said:&lt;/p&gt;&lt;p&gt;This is not accurate. The README clearly says that it is NOT licensed under any open source license.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I originally posted this in a discord server, but was encouraged to post it here in case it’s helpful to the tailwind project. Here it is:&lt;/p&gt;&lt;p&gt;Well, what’s really happening is that tailwind’s paid product offering (prebuilt components with best practices) is being obviated by a technological disruption. Many companies go through something like this, and it’s really unfortunate. It's not really just "ai dropped traffic". I'm sure that's true, but it's also that “the UI kit is no longer useful." I own it. I don’t use it. There's a reason for that.&lt;/p&gt;&lt;p&gt;Tailwind is in a dire spot because its product is a "text-output", and worse, with low configuration needs, which is EXACTLY the most automateable thing. All AI does is produce text output that works good enough if you don't need too much taste or configuration for unclear business needs. Perfect storm, meet straw pig house.&lt;/p&gt;&lt;p&gt;Opus is fucked up good at styling now with a halfway decent prompt and it already knows all of tailwind. That's rough as hell for tailwind. The UI kit costs $299! I can run thousands of AI queries for that price and customize whatever I feel like! In defense of Adam, there's no easy solution here. He needs a pivot.&lt;/p&gt;&lt;p&gt;I don't have a good solution off the top of my head. The cost of making low-mid quality software has dropped 100x. That's going to shake (is shaking) the market badly. Some business models are AI resistant and others are not. I'll give an example: anything where the paid model is hosting, or they have an app are AI resistant. I pay for obsidian file sync even though it's open source because I don't feel like investing my hours for a solved problem. I use vercel for hosting even though I could have an AI write up some docker containers or whatever.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Trying to fight against LLMs is an uphill battle. This feature makes the docs more usable and lowers friction, which helps Tailwind win.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Can you maybe add a PayPal link or whatever so I can send you 50€? I cannot afford the $120 per year deal right now. But if enough people join in, who knows.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Came here to say that @quantizor is fucking cringe.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Wow looks like github is the new reddit. Who knew?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Just wanted to drop a quick note and say I appreciate all the support here, on Twitter, and on Hacker News. We'll figure it out I'm sure.&lt;/p&gt;&lt;p&gt;In the mean time if you've benefitted from Tailwind over the years and are looking for a way to support the project, consider grabbing a Tailwind Plus license or sponsoring the project through our Insiders Program ❤️ Will be working hard to make both of those things a lot more valuable this year.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tailwindlabs/tailwindcss.com/pull/2388"/><published>2026-01-07T16:02:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528045</id><title>Building voice agents with Nvidia open models</title><updated>2026-01-07T20:13:00.851576+00:00</updated><content>&lt;doc fingerprint="ed5ac51b6c1a889b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;How to Build Ultra-low-latency Voice Agents With NVIDIA Cache-aware Streaming ASR&lt;/head&gt;
    &lt;p&gt;This post accompanies the launch of NVIDIA Nemotron Speech ASR on Hugging Face. Read the full model announcement here.&lt;/p&gt;
    &lt;p&gt;In this post, we’ll build a voice agent using three NVIDIA open models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The new Nemotron Speech ASR model&lt;/item&gt;
      &lt;item&gt;Nemotron 3 Nano LLM&lt;/item&gt;
      &lt;item&gt;A preview checkpoint of the upcoming NVIDIA Magpie text-to-speech model&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This voice agent leverages the new streaming ASR model, Pipecat’s low-latency voice agent building blocks, and some fun code experiments to optimize all three models for very fast response times.&lt;/p&gt;
    &lt;p&gt;All the code for the post is here in this GitHub repository.&lt;/p&gt;
    &lt;p&gt;You can clone the repo and run this voice agent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scalably for multi-user workloads on the Modal cloud platform.&lt;/item&gt;
      &lt;item&gt;On an NVIDIA DGX Spark or RTX 5090 for single-user, local development and experimentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feel free to just jump over to the code. Or read on for technical notes about building fast voice agents and the NVIDIA open models.&lt;/p&gt;
    &lt;head rend="h1"&gt;The state of voice AI agents in 2026&lt;/head&gt;
    &lt;p&gt;Voice agent deployments are growing by leaps and bounds across a wide range of use cases. For example, we’re seeing voice agents used at scale today in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customer support&lt;/item&gt;
      &lt;item&gt;Answering the phone for small businesses (for example, restaurants)&lt;/item&gt;
      &lt;item&gt;User research&lt;/item&gt;
      &lt;item&gt;Outbound phone calls to prepare patients for healthcare appointments&lt;/item&gt;
      &lt;item&gt;Validation workflows for loan applications&lt;/item&gt;
      &lt;item&gt;And many, many other scenarios&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both startups and large, established companies are building voice agents that are successful in real-world deployments. The best voice agents today achieve very high “task completed” success metrics and customer satisfaction scores.&lt;/p&gt;
    &lt;head rend="h2"&gt;Voice AI architecture&lt;/head&gt;
    &lt;p&gt;As is the case with everything in AI, voice agent technology is evolving rapidly. Today, there are two ways to build voice agents.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Most production voice agents use specialized models together in a pipeline – a speech-to-text model, a text-mode LLM, and a text-to-speech model.&lt;/item&gt;
      &lt;item&gt;Voice agent developers are beginning to experiment with new speech-to-speech models that take voice input directly and output audio instead of text.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using three specialized models is currently the best approach for enterprise use cases that require the highest degree of model intelligence and flexibility. But speech-to-speech models are an exciting development and will be a big part of the future of voice AI.&lt;/p&gt;
    &lt;p&gt;Whether we use a pipeline or a unified speech-to-speech model, voice agents are doing more and more sophisticated tasks. This means that, increasingly, production voice agents are actually multi-agent systems. Inside an agent, sub-agents handle asynchronous tasks, manage the conversation context, and allow code re-use between text and voice agents.&lt;/p&gt;
    &lt;p&gt;For a deep dive into voice agent architectures, models, and infrastructure, see the Voice AI &amp;amp; Voice Agents Illustrated Primer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open source models&lt;/head&gt;
    &lt;p&gt;Open models have not been widely used for production voice agents.&lt;/p&gt;
    &lt;p&gt;Voice agents are among the most demanding AI use cases. Voice agents perform long conversations. They must operate on noisy input audio and respond very quickly. Enterprise voice agent use cases require highly accurate instruction following and function calling. People interacting with voice agents have very high expectations for naturalness and “human-like” qualities of voice audio. In all of these areas, proprietary AI models have performed better than open models.&lt;/p&gt;
    &lt;p&gt;However, this is changing. Nemotron Speech ASR is both fast and accurate. On our benchmarks it performs comparably with or better than commercial speech-to-text models used today in production voice agents. Nemotron 3 Nano is the best-performing LLM in its class on our long-context, multi-turn conversation benchmarks.&lt;/p&gt;
    &lt;p&gt;Using open models allows us to configure and customize our models and inference stacks for the specific needs of our voice agents in ways that we can’t do with proprietary models. We can optimize for latency, fine-tune on our own data, host inference within our VPCs to satisfy data privacy and regulatory requirements, and implement observability that allows us to deliver the highest levels of reliability, scalability, and consistency.&lt;/p&gt;
    &lt;p&gt;We expect open models to be used in a larger and larger proportion of voice agent deployments over time. There are various flavors of “open” model licenses. NVIDIA has made the Nemotron Speech ASR and Nemotron 3 Nano available under the NVIDIA Permissive Open-Model License, which allows for unrestricted commercial use and the creation of derivative works.&lt;/p&gt;
    &lt;head rend="h1"&gt;An ultra-responsive voice agent&lt;/head&gt;
    &lt;head rend="h2"&gt;Fast, streaming transcription&lt;/head&gt;
    &lt;p&gt;The Nemotron Speech ASR model is designed specifically for use cases that demand very low latency transcription, such as voice agents.&lt;/p&gt;
    &lt;p&gt;The headline number here is that Nemotron Speech ASR consistently delivers final transcripts in under 24ms!&lt;/p&gt;
    &lt;p&gt;ASR (Automatic Speech Recognition) is the general term for machine learning models that process speech input, then output text and other information about that speech. Previous generations of ASR models were generally designed for batch processing rather than realtime transcription. For example, the latency of the Whisper model is 600-800ms, and most commercial speech-to-text models today have latencies in the 200-400ms range.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Openness&lt;/cell&gt;
        &lt;cell role="head"&gt;Deployment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Parakeet&lt;/cell&gt;
        &lt;cell&gt;open weights, open training data, open source inference&lt;/cell&gt;
        &lt;cell&gt;local in-cluster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Widely used commercial ASR&lt;/cell&gt;
        &lt;cell&gt;proprietary&lt;/cell&gt;
        &lt;cell&gt;cloud&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Whisper Large V3&lt;/cell&gt;
        &lt;cell&gt;open weights, open source inference&lt;/cell&gt;
        &lt;cell&gt;local in-cluster&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For more about the cache-aware architecture that enables this impressively low latency, see the NVIDIA post announcing the new model.&lt;/p&gt;
    &lt;p&gt;The model is also very accurate. The industry standard for measuring ASR model accuracy is word error rate. Nemotron Speech ASR has a word error rate on all of our benchmarks roughly equivalent to the best commercial ASR models, and substantially better than previous generation open models like Whisper.&lt;/p&gt;
    &lt;p&gt;To integrate Nemotron Speech ASR into Pipecat, we created a WebSocket server that performs the transcription inference and a client-side Pipecat service that can be used in any Pipecat agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running turn detection in parallel with transcription&lt;/head&gt;
    &lt;p&gt;The Nemotron Speech ASR model can be configured with four different context sizes, each of which have different latency/accuracy trade-offs. The context sizes are 80ms, 160ms, 560ms, and 1.2s. We use the 160ms context size, because this aligns with how we perform turn detection.&lt;/p&gt;
    &lt;p&gt;Turn detection means determining when the user has stopped speaking and the voice agent should respond. Accurate turn detection is critical to natural conversation. We’re using the open source Pipecat Smart Turn model in this voice agent. The Smart Turn model operates on input audio and runs in parallel with the Nemotron Speech ASR transcription.&lt;/p&gt;
    &lt;p&gt;We trigger both turn detection and transcript finalization any time we see a 200ms pause in the user’s speech. This gives us 200ms of “non-speech” trailing context after the user’s speech has finished. The Nemotron Speech ASR model actually needs a bit more trailing silence than this, to properly finalize the last words in the user speech. The padding calculation is:&lt;/p&gt;
    &lt;code&gt;nemotron_final_padding = (right_context + 1) * shift_frames * hop_samples
    = (1 + 1) * 16 * 160
    = 5120 samples = 320ms
&lt;/code&gt;
    &lt;p&gt;Our WebSocket transcription server receives 200ms of “non-speech” trailing audio data from the Pipecat service, and adds 120ms of synthetic silence to enable immediate finalization of the transcript. This works nicely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nemotron 3 Nano&lt;/head&gt;
    &lt;p&gt;Nemotron 3 Nano is a new 30 billion parameter open source LLM from NVIDIA. Nemotron 3 Nano is the best performing model in its size class on our multi-turn conversation benchmarks.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Tool Use&lt;/cell&gt;
        &lt;cell role="head"&gt;Instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;KB Ground&lt;/cell&gt;
        &lt;cell role="head"&gt;Pass Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB Med&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB P95&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5.1&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;916ms&lt;/cell&gt;
        &lt;cell&gt;2011ms&lt;/cell&gt;
        &lt;cell&gt;5216ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gemini-3-flash-preview&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;1193ms&lt;/cell&gt;
        &lt;cell&gt;1635ms&lt;/cell&gt;
        &lt;cell&gt;6653ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;claude-sonnet-4-5&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;2234ms&lt;/cell&gt;
        &lt;cell&gt;3062ms&lt;/cell&gt;
        &lt;cell&gt;5438ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4.1&lt;/cell&gt;
        &lt;cell&gt;283/300&lt;/cell&gt;
        &lt;cell&gt;273/300&lt;/cell&gt;
        &lt;cell&gt;298/300&lt;/cell&gt;
        &lt;cell&gt;94.9%&lt;/cell&gt;
        &lt;cell&gt;97.8%&lt;/cell&gt;
        &lt;cell&gt;683ms&lt;/cell&gt;
        &lt;cell&gt;1052ms&lt;/cell&gt;
        &lt;cell&gt;3860ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gemini-2.5-flash&lt;/cell&gt;
        &lt;cell&gt;275/300&lt;/cell&gt;
        &lt;cell&gt;268/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;93.7%&lt;/cell&gt;
        &lt;cell&gt;94.4%&lt;/cell&gt;
        &lt;cell&gt;594ms&lt;/cell&gt;
        &lt;cell&gt;1349ms&lt;/cell&gt;
        &lt;cell&gt;2104ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5-mini&lt;/cell&gt;
        &lt;cell&gt;271/300&lt;/cell&gt;
        &lt;cell&gt;272/300&lt;/cell&gt;
        &lt;cell&gt;289/300&lt;/cell&gt;
        &lt;cell&gt;92.4%&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
        &lt;cell&gt;6339ms&lt;/cell&gt;
        &lt;cell&gt;17845ms&lt;/cell&gt;
        &lt;cell&gt;27028ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4o-mini&lt;/cell&gt;
        &lt;cell&gt;271/300&lt;/cell&gt;
        &lt;cell&gt;262/300&lt;/cell&gt;
        &lt;cell&gt;293/300&lt;/cell&gt;
        &lt;cell&gt;91.8%&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;760ms&lt;/cell&gt;
        &lt;cell&gt;1322ms&lt;/cell&gt;
        &lt;cell&gt;3256ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;nemotron-3-nano-30b-a3b*&lt;/cell&gt;
        &lt;cell&gt;287/304&lt;/cell&gt;
        &lt;cell&gt;286/304&lt;/cell&gt;
        &lt;cell&gt;298/304&lt;/cell&gt;
        &lt;cell&gt;91.4%&lt;/cell&gt;
        &lt;cell&gt;93.3%&lt;/cell&gt;
        &lt;cell&gt;171ms&lt;/cell&gt;
        &lt;cell&gt;199ms&lt;/cell&gt;
        &lt;cell&gt;255ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4o&lt;/cell&gt;
        &lt;cell&gt;278/300&lt;/cell&gt;
        &lt;cell&gt;249/300&lt;/cell&gt;
        &lt;cell&gt;294/300&lt;/cell&gt;
        &lt;cell&gt;91.2%&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
        &lt;cell&gt;625ms&lt;/cell&gt;
        &lt;cell&gt;1222ms&lt;/cell&gt;
        &lt;cell&gt;13378ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-oss-120b (groq)&lt;/cell&gt;
        &lt;cell&gt;272/300&lt;/cell&gt;
        &lt;cell&gt;270/300&lt;/cell&gt;
        &lt;cell&gt;298/300&lt;/cell&gt;
        &lt;cell&gt;89.3%&lt;/cell&gt;
        &lt;cell&gt;90.0%&lt;/cell&gt;
        &lt;cell&gt;98ms&lt;/cell&gt;
        &lt;cell&gt;226ms&lt;/cell&gt;
        &lt;cell&gt;2117ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5.2&lt;/cell&gt;
        &lt;cell&gt;224/300&lt;/cell&gt;
        &lt;cell&gt;228/300&lt;/cell&gt;
        &lt;cell&gt;250/300&lt;/cell&gt;
        &lt;cell&gt;78.0%&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;819ms&lt;/cell&gt;
        &lt;cell&gt;1483ms&lt;/cell&gt;
        &lt;cell&gt;1825ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;claude-haiku-4-5&lt;/cell&gt;
        &lt;cell&gt;221/300&lt;/cell&gt;
        &lt;cell&gt;172/300&lt;/cell&gt;
        &lt;cell&gt;299/300&lt;/cell&gt;
        &lt;cell&gt;76.9%&lt;/cell&gt;
        &lt;cell&gt;75.6%&lt;/cell&gt;
        &lt;cell&gt;732ms&lt;/cell&gt;
        &lt;cell&gt;1334ms&lt;/cell&gt;
        &lt;cell&gt;4654ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Like Nemotron Speech ASR, Nemotron 3 Nano is part of a new generation of open models that are designed specifically for speed and inference efficiency. See this resource from NVIDIA research for an overview of the Nemotron 3 hybrid Mamba-Transformer MoE architecture and links to technical papers.&lt;/p&gt;
    &lt;p&gt;A 30B parameter model is small enough to run very fast on high-end hardware, and can be quantized to run well on GPUs that many developers have at home!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model variant&lt;/cell&gt;
        &lt;cell role="head"&gt;Deployment&lt;/cell&gt;
        &lt;cell role="head"&gt;Resident memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nemotron-3-Nano BF16&lt;/cell&gt;
        &lt;cell&gt;full weights, Modal Cloud or DGX Spark&lt;/cell&gt;
        &lt;cell&gt;72GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nemotron-3-Nano Q8&lt;/cell&gt;
        &lt;cell&gt;8-bit quantization, faster operation on DGX Spark&lt;/cell&gt;
        &lt;cell&gt;32GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nemotron-3-Nano Q4&lt;/cell&gt;
        &lt;cell&gt;4-bit quantization, RTX 5090&lt;/cell&gt;
        &lt;cell&gt;24GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;One note on which LLMs are generally used today for production voice agents: in general, voice agents for applications like customer support need the most “intelligent” models we have available. Voice agent use cases are demanding. A customer support AI agent must do highly accurate instruction following and function calling tasks throughout a long, open-ended, unpredictable human conversation. A 30B parameter model – even one as good as Nemotron 3 Nano – is generally best suited for specialized voice tasks like a home assistant or software voice UI interface.&lt;/p&gt;
    &lt;p&gt;NVIDIA has announced that two larger Nemotron 3 models are coming soon. If the performance of these larger models relative to their size is similar to Nemotron 3 Nano’s performance, we expect these models to be terrific intelligence engines for voice agents.&lt;/p&gt;
    &lt;p&gt;In the meantime, Nemotron 3 Nano is the best-performing LLM that I can run on hardware I have at home. I’ve been using this model for a wide variety of “local” voice agent tasks and development experiments on both an NVIDIA DGX Spark and on my desktop computer with an RTX 5090.&lt;/p&gt;
    &lt;p&gt;You can use Nemotron 3 in reasoning or non-reasoning mode. We usually turn off reasoning for the fast-response core voice agent loop.&lt;/p&gt;
    &lt;p&gt;For details on using Nemotron 3 Nano in the cloud and building local containers with the latest CUDA, vLLM and llama.cpp support for this new model, see the GitHub repository accompanying this post. There are a couple of inference tooling patches (relating to the reasoning output format in vLLM and to llama.cpp KV caching) that you might find useful if you’re experimenting with this model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Magpie streaming server&lt;/head&gt;
    &lt;p&gt;Magpie is a family of text-to-speech models from NVIDIA. In our voice agent project, we’re using an experimental preview checkpoint of an upcoming open source version of Magpie.&lt;/p&gt;
    &lt;p&gt;Kudos to NVIDIA for releasing this early look at a Magpie model designed, like Nemotron Speech ASR, for streaming, low-latency use cases! We’ve been having a lot of fun experimenting with this preview, doing things that are only possible with open source weights and inference code.&lt;/p&gt;
    &lt;p&gt;You can use this Magpie model in batch mode by sending an HTTP request with a chunk of text. This batch mode inference delivers audio for a single sentence in about 600ms on the DGX Spark and 300ms on the RTX 5090. But for voice agents, we like to stream all tokens as much as we can, and because Magpie is open source, we can hack together a hybrid streaming mode that optimizes for initial audio chunk latency! This hybrid streaming approach improves average initial response latency 3x.&lt;/p&gt;
    &lt;head rend="h3"&gt;TTS TTFB Comparison: Batch → Streaming&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Hardware&lt;/cell&gt;
        &lt;cell role="head"&gt;P50 Improvement&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean Improvement&lt;/cell&gt;
        &lt;cell role="head"&gt;P90 Improvement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;RTX 5090&lt;/cell&gt;
        &lt;cell&gt;90 ms (1.9x)&lt;/cell&gt;
        &lt;cell&gt;204 ms (3.0x)&lt;/cell&gt;
        &lt;cell&gt;430 ms (5.2x)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DGX Spark&lt;/cell&gt;
        &lt;cell&gt;236 ms (2.3x)&lt;/cell&gt;
        &lt;cell&gt;415 ms (3.3x)&lt;/cell&gt;
        &lt;cell&gt;836 ms (4.6x)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Details&lt;/head&gt;
    &lt;head rend="h5"&gt;RTX 5090&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Batch&lt;/cell&gt;
        &lt;cell&gt;106 ms&lt;/cell&gt;
        &lt;cell&gt;630 ms&lt;/cell&gt;
        &lt;cell&gt;191 ms&lt;/cell&gt;
        &lt;cell&gt;533 ms&lt;/cell&gt;
        &lt;cell&gt;305 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipeline&lt;/cell&gt;
        &lt;cell&gt;99 ms&lt;/cell&gt;
        &lt;cell&gt;103 ms&lt;/cell&gt;
        &lt;cell&gt;101 ms&lt;/cell&gt;
        &lt;cell&gt;103 ms&lt;/cell&gt;
        &lt;cell&gt;101 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;DGX Spark&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Batch&lt;/cell&gt;
        &lt;cell&gt;193 ms&lt;/cell&gt;
        &lt;cell&gt;1440 ms&lt;/cell&gt;
        &lt;cell&gt;422 ms&lt;/cell&gt;
        &lt;cell&gt;1067 ms&lt;/cell&gt;
        &lt;cell&gt;595 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipeline&lt;/cell&gt;
        &lt;cell&gt;15 ms&lt;/cell&gt;
        &lt;cell&gt;276 ms&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;231 ms&lt;/cell&gt;
        &lt;cell&gt;180 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There’s definitely a quality trade-off with our simple streaming implementation. Try the agent yourself, or listen carefully to the conversation in the video at the beginning of this blog post. You can usually hear a slight disfluency where we “stitch” together the streaming chunks at the beginning of the model response.&lt;/p&gt;
    &lt;p&gt;To do better, we’d need to retrain part of the model and use a slightly more sophisticated inference approach. Fortunately, this is on the NVIDIA road map.&lt;/p&gt;
    &lt;p&gt;We integrated this model into Pipecat by creating a WebSocket server for streaming inference, and a client-side Pipecat service. (This is the same approach we used with Nemotron Speech ASR).&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting the models together and measuring latency&lt;/head&gt;
    &lt;p&gt;These Nemotron and upcoming Magpie models are completely open: open weights, open source training data sets, and open source inference tooling. Working with open models in production feels like a super-power. We can do things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read the inference code to understand the context requirements of the ASR model, so that we can optimize the interactions between our Pipecat pipeline components and text-to-speech audio buffer handling. (See our description of this above, in the section Fast, streaming transcription.&lt;/item&gt;
      &lt;item&gt;Fix issues with inference tooling support in new models and on whatever platforms we’re running on. See the code and README.md in the GitHub repo for the small patches we made for vLLM and llama.cpp, and the Docker container build with full MX4FP support for both of those inference servers on DGX Spark and RTX 5090.&lt;/item&gt;
      &lt;item&gt;Build a semi-streaming inference server for a preview model checkpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Often when we’re building voice agents, our primary concern is to engineer the agent to respond quickly in a real-world conversation. The difference between good latency and an agent too slow to use in production is often a combination of several optimizations, each one cutting peak latencies by 100 or 200ms. Working with open models gives us control over how we prioritize for latency compared to throughput, how we design streaming and chunking of inference results, how to use models together optimally, and many other small things that add up (or subtract down) to fast response times.&lt;/p&gt;
    &lt;p&gt;It’s useful to measure voice-to-voice latency – the time between the user’s voice stopping and the bot’s voice response starting – in two places: on the server-side and at the client.&lt;/p&gt;
    &lt;p&gt;We can easily automate the server-side latency measurement. Our bot outputs a log line with a voice-to-voice latency metric for each turn.&lt;/p&gt;
    &lt;code&gt;2026-01-01 22:43:26.208 | INFO     | v2v_metrics:process_frame:54 - V2VMetrics: ServerVoiceToVoice TTFB: 465ms
&lt;/code&gt;
    &lt;p&gt;We also output log lines with time-to-first-byte for each of our models, and several other log lines that are useful for understanding exactly where we’re “spending our latency budget.” The Pipecat Playground shows graphs of these metrics, which is useful during development and testing. Here’s a test session with our bot running on an RTX 5090.&lt;/p&gt;
    &lt;p&gt;RTX 5090&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ASR&lt;/cell&gt;
        &lt;cell&gt;13ms&lt;/cell&gt;
        &lt;cell&gt;19ms&lt;/cell&gt;
        &lt;cell&gt;23ms&lt;/cell&gt;
        &lt;cell&gt;70ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;71ms&lt;/cell&gt;
        &lt;cell&gt;171ms&lt;/cell&gt;
        &lt;cell&gt;199ms&lt;/cell&gt;
        &lt;cell&gt;255ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;TTS&lt;/cell&gt;
        &lt;cell&gt;99ms&lt;/cell&gt;
        &lt;cell&gt;108ms&lt;/cell&gt;
        &lt;cell&gt;113ms&lt;/cell&gt;
        &lt;cell&gt;146ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;V2V&lt;/cell&gt;
        &lt;cell&gt;415ms&lt;/cell&gt;
        &lt;cell&gt;508ms&lt;/cell&gt;
        &lt;cell&gt;544ms&lt;/cell&gt;
        &lt;cell&gt;639ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;DGX Spark&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ASR&lt;/cell&gt;
        &lt;cell&gt;24ms&lt;/cell&gt;
        &lt;cell&gt;27ms&lt;/cell&gt;
        &lt;cell&gt;69ms&lt;/cell&gt;
        &lt;cell&gt;122ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;343ms&lt;/cell&gt;
        &lt;cell&gt;750ms&lt;/cell&gt;
        &lt;cell&gt;915ms&lt;/cell&gt;
        &lt;cell&gt;1669ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;TTS&lt;/cell&gt;
        &lt;cell&gt;158ms&lt;/cell&gt;
        &lt;cell&gt;185ms&lt;/cell&gt;
        &lt;cell&gt;204ms&lt;/cell&gt;
        &lt;cell&gt;1171ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;V2V&lt;/cell&gt;
        &lt;cell&gt;759ms&lt;/cell&gt;
        &lt;cell&gt;1180ms&lt;/cell&gt;
        &lt;cell&gt;1359ms&lt;/cell&gt;
        &lt;cell&gt;2981ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It’s also critical to measure the voice-to-voice latency as actually perceived by the user. This is harder to do automatically, especially for telephone call voice agents. The best approach to measuring client-side voice-to-voice latency is to record a call, load the audio file into an audio editor, and measure the gap between the end of the user’s speech waveform and the start of the bot speech waveform. You can’t cheat this measurement, or forget to include an important processing component! We do this periodically in both development and testing, as a sanity check. Here I’m measuring latency in the Descript editor of one turn in the conversation we recorded for the video at the top of this post.&lt;/p&gt;
    &lt;p&gt;You will typically see client-side voice-to-voice latency numbers about 250ms higher than server-side numbers for a WebRTC voice agent. This is time spent in audio processing at the operating system level, encoding and decoding, and network transport. Usually, this delta is a bit worse for telephone call agents: 300-600ms of extra latency in the telephony path that you don’t have much way to optimize. (Though there are some basic things you should do, such as make sure your voice agent is hosted in the same region as your telephony providers servers.) For more on latency, see the Voice AI and Voice Agents Illustrated Guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;An inference optimization for local voice agents&lt;/head&gt;
    &lt;p&gt;We have one more trick up our sleeve when we’re running voice agents locally on a single GPU.&lt;/p&gt;
    &lt;p&gt;When we run voice agents in production in the cloud, we run each AI model on a dedicated GPU. We stream tokens from each model as fast as we can, and send them down the Pipecat pipeline as they arrive.&lt;/p&gt;
    &lt;p&gt;But when we’re running locally, all the models are sharing one GPU. In this context, we can engineer much faster voice-to-voice responses if we carefully schedule inference. In our voice agent for this project, we’re doing two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We run the Smart Turn model on the CPU so that we can dedicate the GPU to transcription when user speech is arriving. The Smart Turn model runs faster on GPU, but it runs fast enough on CPU, and dividing up the workload this way gives us the best possible performance between the two models.&lt;/item&gt;
      &lt;item&gt;We interleave small segments of LLM and TTS inference so that GPU resources are dedicated to one model at a time. This significantly reduces time-to-first-token for each model. First we generate a few small chunks of LLM tokens, then TTS audio, then LLM again, then TTS, etc. We generate a smaller segment for the very first response, so we can start audio playout as quickly as possible. We designed this interleaved chunking approach to work in concert with the hybrid Magpie streaming hack described above.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a sequence diagram showing the interleaved LLM and TTS inference. The three vertical lines in the diagram represent, from left to right:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Tokens arriving in small batches to the Pipecat LLM service in the agent and being pushed down the pipeline.&lt;/item&gt;
      &lt;item&gt;The Pipecat TTS service, managing the frames from the LLM service, dividing the stream on sentence boundaries, and making inference requests to the Magpie WebSocket server running in our local Docker container.&lt;/item&gt;
      &lt;item&gt;The Magpie WebSocket server doing inference and sending back audio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We wrote a custom WebSocket inference server for Magpie, so we control the Pipecat-to-Magpie protocol completely. We’re using llama-server code from the llama.cpp project for LLM inference. Traditional inference stacks aren’t really designed to do this specific kind of chunking, so our code sets a max tokens count (&lt;code&gt;n_predict&lt;/code&gt; in llama.cpp), runs repeated small inference chunks, and does some of the buffer management client-side. This could be done more efficiently, using the llama.cpp primitives directly. Writing a perfectly optimized inference server for this interleaved design would be a fun weekend project, and is something that almost anyone with a little bit of programming experience and a willingness to go down some rabbit holes could work together with Claude Code to implement.&lt;/p&gt;
    &lt;head rend="h1"&gt;Running this voice agent&lt;/head&gt;
    &lt;p&gt;For enterprise-scale, production use, deploy this agent to the Modal GPU cloud. There are instructions in the GitHub Readme.md. Modal is a serverless GPU platform that makes it easy to deploy AI models for development or production use.&lt;/p&gt;
    &lt;p&gt;For local development, the GitHub repo has a Dockerfile for DGX Spark (arm64 + Blackwell GB10 CUDA 13.1) and RTX 5090 (x86_64 + Blackwell CUDA 13.0)&lt;/p&gt;
    &lt;p&gt;If you’re interested in building voice agents, here are some resources you might be interested in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Voice AI &amp;amp; Voice Agents Illustrated Primer&lt;/item&gt;
      &lt;item&gt;YouTube recordings of the community voice agents course sessions from last year&lt;/item&gt;
      &lt;item&gt;The Pipecat Discord, where lots of knowledgeable voice agent developers hang out.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.daily.co/blog/building-voice-agents-with-nvidia-open-models/"/><published>2026-01-07T16:08:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528142</id><title>The Case for Nushell (2023)</title><updated>2026-01-07T20:13:00.572910+00:00</updated><content>&lt;doc fingerprint="1708481539a47a9d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The case for Nushell&lt;/head&gt;August 30, 2023 -&lt;p&gt;Recently, I had a chat with some of my friends about Nushell and why they stuck with traditional shells like bash/zsh or the "new" hotness like fish rather than using Nushell. After chatting with them, my brain kept bubbling away at the state of how folks were using their terminals and the end result is this blog post.&lt;/p&gt;&lt;p&gt;In this post, I make the case for really taking a hard look at Nushell and also for generally asking the question: "can the state of shells be improved enough to overcome the inertia of sticking to what you know?"&lt;/p&gt;&lt;head rend="h2"&gt;The lay of the land&lt;/head&gt;&lt;p&gt;Let's take a look at some of the offerings out there that people are using everyday.&lt;/p&gt;&lt;head rend="h3"&gt;Bash/zsh&lt;/head&gt;&lt;p&gt;Bash, originally a set of improvements on the Bourne shell, has grown to be the default shell for almost all Linux distros. That's generally people's first experience when they hit the terminal. It's what they see when they log into a remote machine. It's reached the definition of ubiquitous.&lt;/p&gt;&lt;p&gt;I also throw 'zsh' in here as well. Apple's macOS switched from bash to zsh, an operationally similar shell but created a bit more recently.&lt;/p&gt;&lt;p&gt;Bash at this point has become so well known that people often confuse support for bash-isms as part of the POSIX standard, but we'll talk about that later.&lt;/p&gt;&lt;p&gt;Pros: it's everywhere. Learn once, run anywhere.&lt;/p&gt;&lt;p&gt;Cons: as a language, bash/zsh feels a bit too retro. It doesn't offer any of the modern programming language style, tool support, etc folks would be used to from other languages. In truth, bash was never really meant for writing the kind of large scripts that people are maintaining today.&lt;/p&gt;&lt;p&gt;Example for loop in bash:&lt;/p&gt;&lt;code&gt;#!/bin/bash
for i in `seq 1 10`;
do
        echo $i
done
&lt;/code&gt;
&lt;head rend="h3"&gt;Fish&lt;/head&gt;&lt;p&gt;As fish's website says: "Finally, a command line shell for the 90s"&lt;/p&gt;&lt;p&gt;It's enough to elicit a smirk, because you know it's a bit true. The bash/zsh style shells are getting left behind by something that feels a bit nicer, has nicer completions, looks nicer (you can get similar improvements out of bash if you work at it, but fish ships with them out of the box)&lt;/p&gt;&lt;p&gt;Fish also bravely steps away from the shell scripting form of bash to something a bit more readable.&lt;/p&gt;&lt;p&gt;Example for loop in fish:&lt;/p&gt;&lt;code&gt;for i in (seq 1 10);
    echo $i;
end
&lt;/code&gt;
&lt;p&gt;Pros: the interactive experience of fish does feel quite a bit nicer that bash/zsh out of the box. Scripting is a bit nicer.&lt;/p&gt;&lt;p&gt;Cons: As it says on the tin, it's a shell for the 90s. It ain't the 90s anymore.&lt;/p&gt;&lt;head rend="h3"&gt;PowerShell&lt;/head&gt;&lt;p&gt;Coming into 1.0 at 2006, PowerShell is one of the first shells to really draw a line in the sand to say "enough, we're going to do things differently". The unix style of pipelines, where commands communicate via text to each other was replaced by a .NET engine that passed objects between commands.&lt;/p&gt;&lt;p&gt;The impact wasn't immediately obvious but as devops folks (and others) discovered what was possible when you have ways to work with data directly a fanbase grew.&lt;/p&gt;&lt;p&gt;Example of a for(each) loop in PowerShell:&lt;/p&gt;&lt;code&gt;foreach ($i in 1..10) {
    echo $i
}
&lt;/code&gt;
&lt;p&gt;PowerShell came with an opinionated design that focused on verb-noun naming, improvements to shell syntax, and a vast set of functionality drawn from the .NET ecosystem.&lt;/p&gt;&lt;p&gt;Pros: it's a structured shell - you can actually work with objects rather than text. Powerful set of tools and capabilities taken from .NET.&lt;/p&gt;&lt;p&gt;Cons: I'll go ahead and say it: PowerShell was never really designed to be a language first. The verb-noun convention forces a style that feels very awkward coming from other languages. Worth a mention: earlier versions of PowerShell were Windows-only and modern crossplatform support lacks some of the features of the earlier versions.&lt;/p&gt;&lt;head rend="h3"&gt;Other shells&lt;/head&gt;&lt;p&gt;When I was coming up, there were a lot of other shells, including the csh/tcsh family. Having said that, I don't know anyone who is using any of the other family of shells. Bash/zsh and to some extent fish really dominate the developer mindshare.&lt;/p&gt;&lt;head rend="h2"&gt;Hold up, we really need to talk about POSIX&lt;/head&gt;&lt;p&gt;We really need to take a minute and talk about POSIX before we continue. A lot of folks have leveled "but it's not POSIX" as an argument against using Nushell, but I'd like to turn that around and ask the question:&lt;/p&gt;&lt;p&gt;"What's so good about POSIX?"&lt;/p&gt;&lt;p&gt;Most folks when asked would likely point to it as a common ground that code can be ported to. In reply, I'd like to quote a few bits of the POSIX standard.&lt;/p&gt;&lt;p&gt;The following are reserved words in the POSIX standard for shell scripting:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;case&lt;/item&gt;&lt;item&gt;do&lt;/item&gt;&lt;item&gt;done&lt;/item&gt;&lt;item&gt;elif&lt;/item&gt;&lt;item&gt;else&lt;/item&gt;&lt;item&gt;esac&lt;/item&gt;&lt;item&gt;fi&lt;/item&gt;&lt;item&gt;for&lt;/item&gt;&lt;item&gt;if&lt;/item&gt;&lt;item&gt;in&lt;/item&gt;&lt;item&gt;then&lt;/item&gt;&lt;item&gt;until&lt;/item&gt;&lt;item&gt;while&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Yes, really. &lt;code&gt;fi&lt;/code&gt; and &lt;code&gt;esac&lt;/code&gt; are a joke that never found their end. No one would design a language that did that with a straight face these days.&lt;/p&gt;&lt;p&gt;Let's take a quick look at the number of flags common Unix commands ship with. These are on my macOS system, so ymmv.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;command&lt;/cell&gt;&lt;cell role="head"&gt;number of flags&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;ls&lt;/cell&gt;&lt;cell&gt;45&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;man&lt;/cell&gt;&lt;cell&gt;15&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;ps&lt;/cell&gt;&lt;cell&gt;29&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;If you look through the flags of &lt;code&gt;ls&lt;/code&gt; to see why it has so many, notice how many are configuring what &lt;code&gt;ls&lt;/code&gt; is displaying. In a real sense, this is going against the underlying philosophy of unix pipelines. Rather than composing a pipeline to get the display you want, you're learning a language of flags for each command to configure the display.&lt;/p&gt;&lt;p&gt;Let's talk about exit codes. Actually, wait, I already did that. As I point out in the post, the standard says:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"The value of status may be 0, EXIT_SUCCESS, EXIT_FAILURE, [CX] [Option Start] or any other value, though only the least significant 8 bits (that is, status &amp;amp; 0377) shall be available from wait() and waitpid(); the full value shall be available from waitid() and in the siginfo_t passed to a signal handler for SIGCHLD. [Option End]"&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Sure - we all still use 8-bit machines while flipping through a manual we printed trying to find the exit code description to understand why a command failed.&lt;/p&gt;&lt;p&gt;I hear you: "but, look, it doesn't matter how archaic this stuff is, if we all agree to use it things keep working."&lt;/p&gt;&lt;p&gt;I dunno - these arguments just don't hold up. We'd still be using C as our main systems language because it's the most documented, most portable, etc. But, by and large, we don't. We're increasingly choosing other languages.&lt;/p&gt;&lt;p&gt;The truth is, in 2023 if someone asked us to design a system, we wouldn't design POSIX. If, in 2023, someone asked us to design a shell language, we wouldn't design bash/zsh. This matters.&lt;/p&gt;&lt;head rend="h2"&gt;Show us the money&lt;/head&gt;&lt;p&gt;I make some pretty bold statements in the above. There is a heritage of technology that got us to this point. While that heritage is important, it's not without its drawbacks. Are there better ways of doing it?&lt;/p&gt;&lt;head rend="h3"&gt;Why structure matters&lt;/head&gt;&lt;p&gt;Before we get into talking about Nushell, let's talk about why structured data matters.&lt;/p&gt;&lt;p&gt;In the Unix pipeline way of thinking, text passes between commands. This is very flexible, but has a major problem: both the outputting command and the inputting command have to agree what shape that text will take so the info can be passed. This locks representation to presentation, disallowing commands from evolving their output over time. As we showed earlier, it also encourages a proliferation of flags.&lt;/p&gt;&lt;p&gt;That's annoying. Does separating the structure from its presentation help us?&lt;/p&gt;&lt;code&gt;&amp;gt; ls | where size &amp;gt; 10kb
&lt;/code&gt;
&lt;p&gt;I always start with this example when showing off Nushell, because not only is it immediately readable, we didn't have to dig through any flags to figure out what we needed to pass to &lt;code&gt;ls&lt;/code&gt; to get that. We also aren't parsing anything from &lt;code&gt;ls&lt;/code&gt;. Instead, the data is passed directly to &lt;code&gt;where&lt;/code&gt;, which handles it directly.&lt;/p&gt;&lt;p&gt;Commands already know this structure, why not make use of it?&lt;/p&gt;&lt;p&gt;The same &lt;code&gt;where&lt;/code&gt; command works on other things. For example, we can process the output of the &lt;code&gt;ps&lt;/code&gt; command:&lt;/p&gt;&lt;code&gt;&amp;gt; ps | where cpu &amp;gt; 40
&lt;/code&gt;
&lt;p&gt;Or open a &lt;code&gt;csv&lt;/code&gt; file and processing its rows:&lt;/p&gt;&lt;code&gt;&amp;gt; open fields.csv | where area &amp;gt; 5
&lt;/code&gt;
&lt;p&gt;And so on. It's the same &lt;code&gt;where&lt;/code&gt; regardless of where the data is coming from. It also gives us the freedom to present the data however we want.&lt;/p&gt;&lt;head rend="h2"&gt;Why Nushell matters&lt;/head&gt;&lt;head rend="h3"&gt;Nushell is designed to be a language&lt;/head&gt;&lt;p&gt;I had the good fortune of being a part of some prominent programming language teams, including helping to create TypeScript and helping create Rust's error messages as part of the Rust team in Mozilla. Designing languages to be easy to use, easy to read, easy to scale up, and easy to debug is something I care about and have worked on for many years.&lt;/p&gt;&lt;p&gt;To that end, Nushell is designed with an eye towards being readable at a glance.&lt;/p&gt;&lt;p&gt;Let's do a &lt;code&gt;for&lt;/code&gt; loop in Nushell:&lt;/p&gt;&lt;code&gt;for i in 1..10 {
    print $i
}
&lt;/code&gt;
&lt;p&gt;(aside: "but why do variables have dollar signs?". Turns out the flexibility of shell programming allows paths to not use quotes, so it's nice to tell a difference between &lt;code&gt;cd foo&lt;/code&gt; and &lt;code&gt;cd $foo&lt;/code&gt;)&lt;/p&gt;&lt;p&gt;This eye towards usable design shows up in many ways. Working with data is improved by not only having structure, but also being able to pattern match against it. Here's an example of pattern matching a list in Nushell:&lt;/p&gt;&lt;code&gt;match $list {
  [$one] =&amp;gt; { print "one element list" }
  [$one, $two] =&amp;gt; { print "two element list" }
  [$head, ..$tail] =&amp;gt; { print $"the tail of the list is ($tail)" }
}
&lt;/code&gt;
&lt;p&gt;In a way, working in Nushell should feel at home both interactively as a shell and as a full scripting language. We've had folks write COVID reporting software in Nushell, research experiments, even entire shells for well-known database services.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell is typechecked&lt;/head&gt;&lt;p&gt;Since Nushell doesn't treat all data as text, you can represent tables, records, numbers, booleans, etc directly in the language.&lt;/p&gt;&lt;p&gt;As a result of this, Nushell is fully typechecked. Common errors can be caught early and shown to you before the script even runs.&lt;/p&gt;&lt;p&gt;Taking what we learned from TypeScript - the types also feed into another important tool.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has IDE support&lt;/head&gt;&lt;p&gt;The types, autocompletion, and early error reporting feed into an engine in Nushell that knows a lot more about your code. As a result, you can write scripts and then work with them using the IDE support Nushell provides. Seeing errors, jumping to definitions, getting documentation on hovers, etc are all part of the Nushell experience.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has nice errors&lt;/head&gt;&lt;p&gt;In Nushell, we make extensive use of remembering where data comes from, as well as what caused an error. Simple errors, like division by zero, are shown clearly:&lt;/p&gt;&lt;p&gt;A more complex error may need to show more to help track down where a mistake came from. Let's say you've accidentally put a string in your list of numbers, and then tried to process it:&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has a SQL-like style&lt;/head&gt;&lt;p&gt;When you start using Nushell to compose pipelines, you'll notice that it has a distinct SQL-like style. Data flows through each stage, and you build up what you want to do to it as you add more commands.&lt;/p&gt;&lt;p&gt;This gives Nushell a distinctive design that encourages experimentation and exploration.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell is, and has always been, crossplatform&lt;/head&gt;&lt;p&gt;An important decision we made from day 1 was to be crossplatform. You can run Nushell on Windows, Linux, and macOS (and BSD, and Android) and get the same experience. You can easily write scripts in a way that they can be run across different platforms. Everything that you learn transfers between OSes without friction.&lt;/p&gt;&lt;head rend="h2"&gt;Is Nushell good enough to overcome the inertia?&lt;/head&gt;&lt;p&gt;I distinctly remember going to a SIAM conference many years back and giving a talk on the Chapel programming language. Even back then, it was a clever language. In a couple lines, you could write code that could distribute and process a matrix across a network of computers. Coming from a lineage of array languages, it ate up data parallel processing. The equivalent code in other languages looked verbose in comparison.&lt;/p&gt;&lt;p&gt;I went through my talk, hoping I'd done a decent job of conveying the main points, and at the end, someone in the audience stood up and said "but I can do all this in C++".&lt;/p&gt;&lt;p&gt;He proceeded to explain that if he could recreate many of the techniques we showed all as part of a C++ library that people could use. At this time, I wasn't sure how to respond other than "but you don't have to, we already built this language" but he couldn't be swayed. If it wasn't C++, he didn't want it.&lt;/p&gt;&lt;p&gt;Fast forward a couple years, and I'm standing in front of a JavaScript audience giving a similar talk, this time promoting TypeScript. I remember the kind of politely confused looks on people's faces as I showed off the features TypeScript offered. There was a similar sense of "why do we need to leave JavaScript?".&lt;/p&gt;&lt;p&gt;To answer whether Nushell can overcome this kind of inertia, I'll pose two questions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Is Nushell compelling enough for a single person to adopt it?&lt;/item&gt;&lt;item&gt;Would adopting Nushell broadly as a community move the needle?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Let's tackle the first question. Time and again, as people try Nushell, they come back with quotes like "this is the most excited I've been about tech in 15 years". It has a fanbase that loves it, and that fanbase is growing. It reminds me of the early days of Rust, just after hitting 1.0.&lt;/p&gt;&lt;p&gt;To the second question: would adopting Nushell broadly actually improve things noticeably? Without a doubt. I say this without any reservation. Thinking of our shells as structured, interactive processing engines opens up the doors to a much wider array of things you can do with them. The commands would be far simpler than their POSIX equivalents and would compose far better. They'd benefit from the full knowledge of the data being shared between them. Adaptors could be made to connect to all parts of the system, allowing you full, structured interaction with everything you have access to.&lt;/p&gt;&lt;p&gt;In essence, as the saying goes, we'd be building a skyscraper starting from the 15th floor instead of the 1st.&lt;/p&gt;&lt;head rend="h2"&gt;It's time to be honest about what Nushell is&lt;/head&gt;&lt;p&gt;It's time I come clean about what Nushell is. Nushell isn't exactly a shell, at least not in the traditional Unix sense of the word. Nushell is trying to answer the question: "what if we asked more of our shells?"&lt;/p&gt;&lt;p&gt;Nushell is really an interactive, data-focused scripting language with shell capabilities. It merges these three things into one:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A fully-typed scripting language&lt;/item&gt;&lt;item&gt;An interactive shell&lt;/item&gt;&lt;item&gt;A data processing system (that can also handle large data loads via dataframes)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Rather than these being three separate ideas glued together, in practice it feels like Nushell is treating everything you interact with as data. This allows you to pull together different kinds of data, and know that the same commands will work over them.&lt;/p&gt;&lt;p&gt;You might look at that list and think you don't need all that, but the way I might frame it is this: it's nice to have it when you need it.&lt;/p&gt;&lt;p&gt;I don't need to do heavy data processing everyday, but it's nice to not have to shift what I'm doing at all when I need to do it. I don't have to download new utilities or switch languages. It's all right there. Need to write a script to load some files and handle some directory processing? Still right there. Need to throw together some web query that outputs the top download results for a github repo? You guessed it, all still right there.&lt;/p&gt;&lt;p&gt;This is just scratching the surface, really. Nushell has a plugin system that allows more capabilities to be added based on your needs. We already have plugins that add a variety of additional file formats, querying capabilities, and more.&lt;/p&gt;&lt;head rend="h2"&gt;It's okay to have nice things&lt;/head&gt;&lt;p&gt;Nushell was built with a simple idea: working in the shell, writing code, and processing data should be fun. To that end, we work hard to make Nushell feel nice.&lt;/p&gt;&lt;p&gt;You can write readable scripts that come with their own documentation, and then come back to them 6 months later and still understand what they're doing.&lt;/p&gt;&lt;p&gt;You can sit in the shell and play with pipeline ideas until one grows into a scripting project and then effortlessly transition your experiment into a full script.&lt;/p&gt;&lt;head rend="h2"&gt;That's it&lt;/head&gt;&lt;p&gt;That's my case. It's okay to have fun. It's okay to write attractive, well-documented code. It's okay to leave the designs of the past behind when they no longer fit the present day.&lt;/p&gt;&lt;p&gt;It's okay to move on to better ways of doing things.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sophiajt.com/case-for-nushell/"/><published>2026-01-07T16:15:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528192</id><title>BillG the Manager (2021)</title><updated>2026-01-07T20:13:00.200449+00:00</updated><content>&lt;doc fingerprint="27401b374f6f26de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;019. BillG the Manager&lt;/head&gt;
    &lt;head rend="h3"&gt;“Intelli…what?”–Bill Gates&lt;/head&gt;
    &lt;p&gt;The breadth of the Microsoft product line and the rapid turnover of core technologies all but precluded BillG from micro-managing the company in spite of the perceptions and lore around that topic. In less than 10 years the technology base of the business changed from the 8-bit BASIC era to the 16-bit MS-DOS era and to now the tail end of the 16-bit Windows era, on the verge of the Win32 decade. How did Bill manage this — where and how did he engage? This post introduces the topic and along with the next several posts we will explore some specific projects.&lt;/p&gt;
    &lt;p&gt;Please feel free to share this and subscribers, please join in the discussion.&lt;/p&gt;
    &lt;p&gt;Back to 018. Microsoft’s Two Bountiful Gardens&lt;/p&gt;
    &lt;p&gt;At 38, having grown Microsoft as CEO from the start, Bill was leading Microsoft at a global scale that in 1993 was comparable to an industrial-era CEO. Even the legendary Thomas Watson Jr., son of the IBM founder, did not lead IBM until his 40s. Microsoft could never have scaled the way it did had BillG managed via a centralized hub-and-spoke system, with everything bottlenecked through him. In many ways, this was BillG’s product leadership gift to Microsoft—a deeply empowered organization that also had deep product conversations at the top and across the whole organization.&lt;/p&gt;
    &lt;p&gt;This video from the early 1980’s is a great introduction to the breadth of Microsoft’s product offerings, even at a very early stage of the company. It also features some vintage BillG voiceover and early sales executive Vern Raburn. (Source: Microsoft videotape)&lt;/p&gt;
    &lt;p&gt;Bill honed a set of undocumented principles that defined interactions with product groups. The times of legendary BillG reviews characterized by hardcore challenges and even insults had become, mostly, a thing of the past excepting the occasional sentimental outburst. More generally, they were a collective memory of hyper-growth moments any start-up experiences, only before the modern era when such stories were more commonly understood.&lt;/p&gt;
    &lt;p&gt;Much later in 2006, when BillG announced his intent to transition from full time Microsoft and part time philanthropy to full time philanthropy, many reporters surprised him by asking how Microsoft would continue without his coordination of technical strategy and oversight. But even in the early ’90s, at the height of the deepest and most challenging technology strategy questions, he never devoted the bulk of his time to micromanaging product development. He spent a good deal of time engaged with products, but there were far too many at too many stages of development to micro-manage them. In many ways this was the opposite of the approach Steve Jobs took, even if both were known for their own forms of challenging interactions. The most obvious contrast between the two was the breadth of the product line and the different market touchpoints.&lt;/p&gt;
    &lt;p&gt;Having grown up through Development Tools and Languages, I was familiar with Microsoft’s product line, but only as TA did it become clear how comparatively broad Microsoft had so quickly become. The software world was thought of through a lens of major categories: operating systems, tools and languages, networking, and applications, roughly mirroring Microsoft’s org chart. The latter was thought of as word processing, spreadsheets, graphics, databases, as well as assorted smaller categories. It was easy to identify leaders in each of those areas—names that were tip of the tongue at the time and most of which are no longer in the PC software space (IBM, Borland, Novell, WordPerfect, Lotus, Aldus, Ashton Tate, and many more). The ah-ha moment in the early 1990s was the realization that no company on that list was competing in more than one category. Microsoft was hardly winning in every category. In fact, in most categories it was new entry, a distant second, or even third place, but the company was in every space. Bill was committed and patient. Microsoft was relentless. And Microsoft was focused on Windows.&lt;/p&gt;
    &lt;p&gt;BillG had fostered Microsoft with a grand vision to compete in every category of PC software, from some of the earliest days. With rare exceptions, no other company set out to do that. BillG led a deep technology strategy. It started with the operating system, supported by tools and languages, and then using those to build applications. This seemed simple enough. In fact, it is what IBM built for mainframes and DEC built for minicomputers.&lt;/p&gt;
    &lt;p&gt;There was a crucial difference. Microsoft did not build hardware and was not vertically integrated to reduce competition. Microsoft built an operating system on an openly architected PC (the same Intel-based architecture that came to power both Macintosh and Linux years later) and published APIs so that anyone could build tools and applications for the operating system—an open hardware platform and open operating system APIs. This approach simply addressed all the early challenges Microsoft itself faced trying to figure out how to build winning applications—it was so busy dealing with dozens of proprietary computing platforms, each with their own tools and APIs just different enough to make things difficult, but not so different as to be valuable. Bill saw the value in software and in openness at key points in the overall architecture. At the formation of the company, he and PaulA saw the immense and expansive value of software and, essentially, the liability that being in the hardware business carried. Building Microsoft’s software-only business on an open hardware platform where many players competed to drive prices down while maintaining compatibility with the operating system was one of the all-time great strategy choices. The idea of building hardware seemed like a sucker’s bet, with low margins, manufacturing, and inventory—the baggage of the physical world. While Microsoft would dabble in peripherals or hardware that could bootstrap new PC scenarios, building whole computers was a headache better left to others.&lt;/p&gt;
    &lt;p&gt;Expanding the impact of that breadth software strategy was BillG’s day-to-day operating model, not micromanaging the specifics of any given project. I am painting this with a broad brush, intentionally so. Part of the difference between the then dominant cultures of Systems and Apps was that during the MikeMap era (and arguably during the earlier JeffH era), Apps weaned itself from Bill’s intense and constant scrutiny whereas the Systems culture more clearly embraced that dynamic. That was largely true until PaulMa took a more hands-off (or walled-off) approach to the nurturing of the NT project.&lt;/p&gt;
    &lt;p&gt;In his May 1991 email, “Challenges and Strategy,” BillG set the company on the Windows strategy, clarifying the foundations for every product and group, solidifying what had been complex platform choices every team faced. Regardless of whether Bill was a savant when it came to the technical details of projects or he simply remembered everything each group sent or told him, he operated the company at a higher level of abstraction than reporters believed to be the case in 2008 when he ultimately reduced his full-time commitment to Microsoft.&lt;/p&gt;
    &lt;p&gt;I had a glimpse of this when our AFX team had our pivotal review. Later as TA I was there to connect the dots and amplify the Windows strategy. By and large the company was still wrapping itself around the details of what it really meant to embrace Windows, exclusively. That, and coping with the myriad of choices and decisions that come from the tension between aligning with a Windows strategy and having some control over your own destiny as a product. Which version of Windows? When is that shipping? Will the APIs our product needs be in Windows? Will those APIs work on older versions of Windows? What about Windows NT? On, which microprocessors? What about the other parts of Microsoft? The questions were endless. This was truly big company stuff—the strategy at a high level is one thing, but execution across a $600M (1994) research and development budget was another. The fascinating thing was how products so quickly scaled beyond what Bill personally experienced as a programmer, both in size and technology specifics. This was to be expected—by any measure the company was huge—but people and Bill himself still expected to interact on product details as though he was a member of the product team. I often found myself looking for ways to help Bill engage at that level, even if just for show.&lt;/p&gt;
    &lt;p&gt;In addition to the Windows strategy, with the late 1993 launch of Office 4, Microsoft also declared 1994 “Year of Office”. It was the biggest launch for Apps and represented a major pivot of the organization to the opportunity of selling a suite of products. This too was in the earliest days of a strategy, one that I would end up spending significant time on as TA and then later as a member of the team.&lt;/p&gt;
    &lt;p&gt;Just because Bill operated at a level of abstraction across products groups did not preclude product groups from engaging on what might seem like relatively small, non-technical matters. One of the more entertaining meetings I attended was preparing for the launch of Office 4, which was a worldwide event complete with a reporter given permission to shadow the team. A key differentiator would be how the user would experience “intelligence” in the product, so that it understood what was intended and how to achieve it in the new Office software. The development team built a series of features along the lines of what was termed “basic use” such as AutoCorrect in Word, AutoFilter in Excel tables, and a host of Wizards (guided step-by-step flows such as for creating charts), and more. To bring them together and actually communicate with the market and on retail packaging, the marketing team came up with an umbrella term. Pete Higgins (PeteH) came over to brief BillG on that choice in a small meeting in Bill’s office.&lt;/p&gt;
    &lt;p&gt;PeteH was by then the spiritual leader of the business side of Apps. He rose through the ranks of Excel and was clearly MikeMap’s lead executive. Pete was the kind of calm and in control leader that everyone enjoyed working for—he was at once clearly the boss, but also a member of the team. Pete was a native of the Seattle area, high school football star, and Stanford graduate. He was a new generation of Microsoft product executive, coming from the business and not the coding side. For me in my TA role, Pete was one of my biggest supporters and mentors and made connecting with Apps super easy.&lt;/p&gt;
    &lt;p&gt;Sitting at the little couch under the Intel chip poster, after going through the details of the launch, Pete said the proverbial “there’s one more thing.” Bill rocking in his chair shook his head, given that the meeting was mostly an uneventful recap of the upcoming press tour. Pete went on to explain the problem of communicating all the features and how Microsoft needed a term to market and describe them. Pete was dancing around this because he knew well enough that Bill was not a fan of “marketing”. Ever so delicately Pete said, “this is your chance…we want to go with this term but if you don’t like it…”&lt;/p&gt;
    &lt;p&gt;Pete then said, “IntelliSense. Microsoft Office introduces IntelliSense.”&lt;/p&gt;
    &lt;p&gt;Bill’s reply, “Intelli…what?”&lt;/p&gt;
    &lt;p&gt;Pete again tried to position the positioning, his instinct about resistance proving correct. “It is IntelliSense…it means that Office has built-in intelligence, and it understands what you need and how to do it.”&lt;/p&gt;
    &lt;p&gt;Bill still not warming up, went full pedantic, “what intelligence…is there a Prolog rules engine, a neural network, ….” He was also making the scrunched up surprised look that he does, which turns out (once you realize it) to also be a bit sarcastic. It meant he was warming up.&lt;/p&gt;
    &lt;p&gt;A few more times back and forth, and Pete just made Bill say IntelliSense in a sentence one more time, which he did with kind of a devilish smirk.&lt;/p&gt;
    &lt;p&gt;Done.&lt;/p&gt;
    &lt;p&gt;Looking back this all seems absurd. Consternation over a single phrase. Literally seeking approval to use it from the CEO of a billion-dollar company. All on the heels of what was no doubt months of preparation, including getting SteveB’s approval which was actually critical. Finally, the theater that Pete would pull the plug a few weeks before the tour. In some ways this was the Apps way of bringing decisions to Bill—it wasn’t really a choice and it had been broadly vetted and was buttoned-up. Any debate would probably be theater more than anything.&lt;/p&gt;
    &lt;p&gt;On average, there was one product-focused meeting on most days. Most teams saw Bill once or twice a year. NathanM saw Bill most every day or at least in most every technology context, present day or far out there. Most executives, like PaulMa, PeteH (leading Apps), and Susan Boeschen (SusanB leading consumer), saw Bill in product review contexts several times a month because each had many ongoing projects or, in the case of the big projects (like operating systems), many large components. Everyone was in constant contact over email. Bill was always forwarding emails across the company, adding relevant people from all levels of the organization to the CC line, and never backed off a good reply-all opportunity. Phone or in-person 1:1s were not the typical way of interacting across the product executive team. For the most part, work happened in groups or at least with an audience, with outcomes and flare-ups quickly disseminated by email. I found myself constantly on the move walking around campus from one building to the next to meet people in person, rarely was I in my office (a pattern that continued my entire career).&lt;/p&gt;
    &lt;p&gt;I was often asked to meet with teams before they met with Bill. They hoped for insight into how BillG might think about choices and decisions or even the presentation overall. I often disappointed teams in these pre-meetings since I was hardly a stand-in for Bill, and I was hardcore about leaving any such impression. Pre-meetings gave me a chance to better understand the issues the team was struggling with and to make sure those were brought forward in an objective and transparent manner. The fastest path to failure was to structure a conversation so Bill discovered an issue rather than having it revealed to him. To be fair, an equally fast path to failure was a first slide listing a slew of problems and issues in the hopes of inoculating the remainder of the meeting. In that case, I would caution teams that they were exposing themselves to the inevitable “How can this be so difficult?” comments. Getting this balance right was the essence of leading an effective meeting.&lt;/p&gt;
    &lt;p&gt;For most meetings, I wrote a summary meeting preview. Even though Bill said he did not want this, I could not help myself. While he was always effective, I felt that a little bit of specifics could go a long way in making the meeting more effective and less random. I could tell he had read my mail if he raised a point verbatim from my note, and frequently he would kick off the meeting doing so, never crediting me of course. In these, and all mails talking about other teams, I always tried to separate the facts of the meeting, the team’s analysis, and my own opinion. Bill was transparent with email and thought little of forwarding an entire thread. I learned the ramifications of that the hard way.&lt;/p&gt;
    &lt;p&gt;As an example of where I failed to follow my own rules about fact versus opinion, I totally offended Jim Allchin (JimAll), leading the Cairo project, on the role of a specific technology in distributed programming. Not only did Jim inform me that my opinions were wrong, but also that I stepped all over his own PhD dissertation as a leading expert. In hindsight, this was terrifying—Jim’s reply was brutal—but it proved a good early learning experience, so to speak.&lt;/p&gt;
    &lt;p&gt;While the product line was already broad, the expansion to entirely new areas was unstoppable. On most any product area, we were forming an opinion, beginning work, or already in the market. There was not a booth at a tradeshow, a focused conference, or a major company looking to partner that Microsoft was not already connected to or connecting with in some way. While Microsoft was in the earliest days of achieving a PC in every home (about 25 percent of US households in 1993) and on every desktop (about half of US workers in 1993), every day in this job was either furthering that or expanding beyond homes and desktops from data centers to handhelds to airplanes (the first in-flight PC-based system was an early partnership between Microsoft and an airline, including certification for Windows Server).1&lt;/p&gt;
    &lt;p&gt;Product meetings had no set format or structure and usually reflected the culture of the organization. This might be a surprise to some as many CEOs (or perhaps their staff!) might have imposed some more rigor on meetings. Microsoft had two bountiful gardens, but there were micro-cultures throughout out the company. While one group did slick and well-rehearsed presentations, another might present research-heavy deep dives. Bill often pushed a team outside its comfort zone, deliberately pushing the team to discuss places they were less prepared, or even less interested. It was a technique he employed. He once said to me, “Why spend all the time with the Windows team talking about architecture, if that was their predisposition anyway?” This was also a strategy to level the playing field—talking about architecture to Windows or ease of use to Excel was too lopsided and Bill was disadvantaged.&lt;/p&gt;
    &lt;p&gt;The reality of BillG Reviews never lived up to lore.&lt;/p&gt;
    &lt;p&gt;Most meetings progressed without incident—meaning without yelling. Sometimes, though, there were comments such as “That was the stupidest thing I ever heard” or “That is brain-dead.” The worst was “That’s trivial . . . let me show you.” Those were all the clichés that teams anticipated but then wore as a badge of honor. They happened with far less frequency compared to how much they were talked about. Even over the short period of time I worked as TA, Bill became more intentional in his use of meeting dynamics. Still, the first seconds of a meeting remained a bit of a mood thermometer, pity those for whom it was clearly a bad day.&lt;/p&gt;
    &lt;p&gt;When meetings ended up “bad” it was always because the team was poorly prepared, or they came to talk about the project in a way that diverged from expectations. There were typical capital offenses in the meeting, such as failing to understand a product strategy of competitors or downplaying a competitor’s potential. Worst was coming across as though a product was making mostly tactical decisions driven by schedule or failing to understand the architecture of the product relative to the evolving platform and related teams across Microsoft. PivotTables were just making their way across most teams, so many were still making the common errors of using static charts and graphs that always seemed to have the data oriented or filtered in the least useful way. Those moments always held potential for a lively discussion.&lt;/p&gt;
    &lt;p&gt;Part of my role was to reduce the potential for such liveliness ahead of time. I tried to alert teams about potential issues without acting as a surrogate for Bill, and to make sure meetings did not save the difficult or bad news for the end. I was also there to throw myself on the grenade, so to speak, and get meetings back on track by helping the team through a tough moment—usually by restating or interpreting what they were saying or by redirecting the topic at hand to a follow-up discussion.&lt;/p&gt;
    &lt;p&gt;By far the biggest strategic error one could make was knowingly duplicating code outside core expertise, and then compounding that by attempting to explain why in this particular case it is justified. Microsoft Publisher was a new product in the desktop publishing category. It was being built by the Consumer Division under the leadership of Melinda French (MelindaF). The product aimed for the small business and non-professional market, compared to the incumbent Aldus PageMaker. It differentiated itself with ease-of-use features, pioneering Wizards and other user interface innovations. But it also produced printed pages that looked a lot like what one should be able to create with Microsoft Word. This overlap was the source of endless consternation—why can’t they share code, why can’t Word do all these features, and then ultimately why does Publisher even exist. Yet, customers loved it. At one point, a meeting went down a rabbit hole over bullets and numbering and how Publisher was basically writing all the same code Word was and wasting everyone’s resources. There was little actionable in this kind of rant, but it did establish the norm of being called out for redundancy and the need to be prepared to cope with the feedback.&lt;/p&gt;
    &lt;p&gt;Bill maintained a deep commitment to evaluating a portfolio of efforts, and even within a single product he believed in the portfolio approach of features—not every product nor every feature was a winner or a breakthrough, but on the whole something needed to be working. As much as Bill might give a group a difficult time (as happened with Visual C++), he knew there was always more to the product and more products to the company. It was not just that Bill was building a product portfolio for Microsoft, he was managing the teams as a portfolio of efforts. This portfolio approach created a resiliency in the company—resilient to the unpredictable nature of technology bets and to the ability of the people on the team to execute. Not everything went as planned nor did every planned bet ultimately make sense.&lt;/p&gt;
    &lt;p&gt;Whether deliberate or not, BillG had three axes that created a constant state of balance, of push and pull, across the hundred teams creating software. Bill’s approach of constantly balancing the tension between innovation and shipping, expanding the portfolio while maintaining coherency, and the injection of new ideas while also executing on existing work proved to be the most interesting “management” lesson. The next three sections are examples of each of these dimensions.&lt;/p&gt;
    &lt;p&gt;On to 020. Innovation versus Shipping: The Cairo Project&lt;/p&gt;
    &lt;p&gt;https://nces.ed.gov/programs/digest/d08/tables/dt08_432.asp&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hardcoresoftware.learningbyshipping.com/p/019-billg-the-manager"/><published>2026-01-07T16:18:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528353</id><title>Health care data breach affects over 600k patients, Illinois agency says</title><updated>2026-01-07T20:12:59.987898+00:00</updated><content>&lt;doc fingerprint="e6188ade5513a61a"&gt;
  &lt;main&gt;
    &lt;p&gt;The names and addresses of thousands of patients of the Illinois Department of Human Services were incorrectly made publicly viewable for the last several years, the agency said Friday.&lt;/p&gt;
    &lt;p&gt;Several maps created to assist the agency with decisions — like where to open new offices and allocate certain resources — were made public through incorrect privacy settings between 2021 and 2025, the Department of Human Services said in a statement.&lt;/p&gt;
    &lt;p&gt;More than 32,000 customers with the IDHS division of rehabilitation services had information publicly viewable between April 2021 and September 2025. The information included names, addresses, case numbers, case status, referral source information, region and office information and status as Division of Rehabilitation Services recipients, the agency said.&lt;/p&gt;
    &lt;p&gt;Around 670,000 Medicaid and Medicare Savings Program recipients had their addresses, case numbers, demographic information and the name of medical assistance plans publicly viewable between January 2022 and September 2025, IDHS said.&lt;/p&gt;
    &lt;p&gt;The state agency said the mapping website was unable to identify who viewed the maps, and IDHS is unaware of any misuse of personal information resulting from the data leak.&lt;/p&gt;
    &lt;p&gt;IDHS discovered the issue Sept. 22 and immediately changed the privacy settings for all maps, restricting access to authorized IDHS employees, the agency said. It also implemented a secure map policy that prohibits uploading customer data to public mapping websites.&lt;/p&gt;
    &lt;p&gt;Individuals whose information was made public will receive a notice about the leak from IDHS. The notices will include a phone number that people can call for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nprillinois.org/illinois/2026-01-06/health-care-data-breach-affects-600-000-patients-illinois-agency-says"/><published>2026-01-07T16:28:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528389</id><title>How Google got its groove back and edged ahead of OpenAI</title><updated>2026-01-07T20:12:59.787864+00:00</updated><content/><link href="https://www.wsj.com/tech/ai/google-ai-openai-gemini-chatgpt-b766e160"/><published>2026-01-07T16:29:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46529237</id><title>Eat Real Food</title><updated>2026-01-07T20:12:59.616097+00:00</updated><content>&lt;doc fingerprint="c2f8a747964fdd4a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Real Food&lt;lb/&gt; Starts Here&lt;/head&gt;&lt;p&gt;Better health begins on your plate—not in your medicine cabinet.&lt;lb/&gt; The new Dietary Guidelines for Americans defines real food as whole, nutrient-dense, and naturally occurring, placing them back at the center of our diets.&lt;/p&gt;&lt;head rend="h2"&gt;The State of Our Health&lt;/head&gt;&lt;head rend="h3"&gt;America is sick.&lt;lb/&gt;The data is clear.&lt;/head&gt;&lt;head rend="h3"&gt;50% of Americans have prediabetes or diabetes&lt;/head&gt;&lt;head rend="h3"&gt;75% of adults report having at least one chronic condition&lt;/head&gt;&lt;head rend="h3"&gt;90% of U.S. healthcare spending goes to treating chronic disease—much of which is linked to diet and lifestyle&lt;/head&gt;&lt;p&gt;For decades we've been misled by guidance that prioritized highly processed food, and are now facing rates of unprecedented chronic disease.&lt;/p&gt;&lt;p&gt;For the first time, we're calling out the dangers of highly processed foods and rebuilding a broken system from the ground up with gold-standard science and common sense.&lt;/p&gt;&lt;head rend="h2"&gt;The New Pyramid&lt;/head&gt;&lt;head rend="h2"&gt;Eat Real &lt;lb/&gt;Food&lt;/head&gt;&lt;head rend="h2"&gt;Protein, Dairy, &amp;amp; Healthy Fats&lt;/head&gt;&lt;p&gt;We are ending the war on protein. Every meal must prioritize high-quality, nutrient-dense protein from both animal and plant sources, paired with healthy fats from whole foods such as eggs, seafood, meats, full-fat dairy, nuts, seeds, olives, and avocados.&lt;/p&gt;&lt;p&gt;Protein target: 1.2–1.6 grams per kilogram of body weight per day.&lt;/p&gt;&lt;head rend="h2"&gt;Vegetables &amp;amp; Fruits&lt;/head&gt;&lt;p&gt;Vegetables and fruits are essential to real food nutrition. Eat a wide variety of whole, colorful, nutrient-dense vegetables and fruits in their original form, prioritizing freshness and minimal processing.&lt;/p&gt;&lt;p&gt;Vegetables: 3 servings per day.&lt;/p&gt;&lt;p&gt;Fruits: 2 servings per day.&lt;/p&gt;&lt;head rend="h2"&gt;Whole Grains&lt;/head&gt;&lt;p&gt;Whole grains are encouraged. Refined carbohydrates are not. Prioritize fiber-rich whole grains and significantly reduce the consumption of highly processed, refined carbohydrates that displace real nourishment.&lt;/p&gt;&lt;p&gt;Target: 2–4 servings per day.&lt;/p&gt;&lt;p&gt;Our nation is finding its footing again, moving past decades of unhealthy eating and rebuilding a food culture rooted in health, science, transparency, and personal responsibility.&lt;/p&gt;&lt;head rend="h2"&gt;Key&lt;lb/&gt;Guidance&lt;/head&gt;&lt;head rend="h2"&gt;Resources&lt;/head&gt;&lt;p&gt;Explore the research, recommendations, and implementation guidance that shape the Dietary Guidelines, including the science, the policy guidance, and the everyday serving framework.&lt;/p&gt;Watch the press release&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://realfood.gov"/><published>2026-01-07T17:22:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46529257</id><title>Texas A&amp;M bans part of Plato's Symposium</title><updated>2026-01-07T20:12:53.488854+00:00</updated><content>&lt;doc fingerprint="6ff7d3120e650991"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Texas A&amp;amp;M Bans Plato&lt;/head&gt;
    &lt;p&gt;Drop the race and gender material from your course and the Plato readings, or teach a different course. You have a day to decide.&lt;/p&gt;
    &lt;p&gt;That’s a paraphrase of what Martin Peterson, professor of philosophy at Texas A&amp;amp;M University, was told by university officials today about his upcoming “Contemporary Moral Problems” course, due to start next week.&lt;/p&gt;
    &lt;p&gt;Here’s the actual email:&lt;/p&gt;
    &lt;p&gt;“Rule 08.01” refers to these recent policy changes at the university. “Kristi” is Department of Philosophy chair Kristi Sweet, who, I think it’s safe to assume, was merely passing along the verdict of “the college leadership team“, headed up by interim dean Simon North.&lt;/p&gt;
    &lt;p&gt;(The above email and other documents in this post were provided by Professor Peterson.)&lt;/p&gt;
    &lt;p&gt;I’m going to pause here just to review: an institution that purports to be a university has told a philosophy professor he is forbidden from teaching Plato.&lt;/p&gt;
    &lt;p&gt;The Plato readings were from the Symposium, particularly passages on Aristophanes’ myth of split humans and Diotima’s ladder of love. The other readings are from Ethics: Theory and Contemporary Issues (10th edition) by Andrew Fiala and Barbara MacKinnon.&lt;/p&gt;
    &lt;p&gt;Professor Peterson had been contacted by his chair on December 19th about the review of syllabi for Contemporary Moral Problems courses. Here’s that email:&lt;/p&gt;
    &lt;p&gt;Professor Peterson replied to this, submitting his syllabus for what he referred to, correctly, as “mandatory censorship review”.&lt;/p&gt;
    &lt;p&gt;Among other things, he said, “Please note that my course does not “advocate” any ideology; I teach students how to structure and evaluate arguments commonly raised in discussions of contemporary moral issues.” (See “The Charade of Banning ‘Advocacy’“.) He also reminded his chair and college officials that “the U.S. Constitution protects my course content,” as do the norms of academic freedom.&lt;/p&gt;
    &lt;p&gt;Here is his full reply:&lt;/p&gt;
    &lt;p&gt;Here is Professor Peterson’s syllabus (also here):&lt;/p&gt;
    &lt;p&gt;It was clear that Texas A&amp;amp;M’s new policies were going to lead to conflicts with the First Amendment and academic freedom. That the first such conflict involves telling a professor to remove from his syllabus the writings of the person who created what was arguably the west’s first institution of higher education is too perfect an irony, though. This reality is unbelievable.&lt;/p&gt;
    &lt;p&gt;(Thanks to several readers who alerted me to the story.)&lt;/p&gt;
    &lt;p&gt;It’s bizarre in another sense, too: Who are you supposed to vote for to ensure this doesn’t happen when the prevailing tendency in the alternative party is to placate, appease, and gain bipartisanship points as they appeal to a made-up middle constructed for them by people who want them to lose? Votes aren’t just meaningless if you’re simply outvoted; they can be meaningless if the party you vote for decides to sacrifice its promises to you on the premise that it’d simply be too radical or unpopular or even inconvenient, and that it’s much easier to pretend it’s all an episode of The West Wing, perform respectability, and scold voters for demanding too much.&lt;/p&gt;
    &lt;p&gt;FWIW, I think that a lot of Trump voters voted for Trump not because they “love this sort of thing”, though depressingly many no doubt did. My understanding is that, in the previous US presidential election, as in many elections around the world, many voted on the basis of a small number of factors that matter most to them, often clustering around economic issues. Some Trump voters were probably not even aware of the more awful things that Trump had said, done, or said he would do. (An obviously imperfect but nonetheless striking indicator of voter ignorance was the spike in Google searches for “Did Biden drop out?” come election day.)&lt;/p&gt;
    &lt;p&gt;I think that if people were more aware of “this sort of thing”, some may vote differently. Spreading stories like the above – and, even better, explaining why such stories are chilling and absurd – could therefore have some positive impact.&lt;/p&gt;
    &lt;p&gt;I also think that if Trump is perceived to do badly on things that many voters do keep track of, it could show in the midterms. Republicans have been anxious about these upcoming elections for exactly this reason. Voting is more likely to be impactful under these circumstances.&lt;/p&gt;
    &lt;p&gt;Voting is crucial. But I think it’s worthwhile to note that a constitutional democracy protects certain rights from even majority rule. This is one where I could believe that the majority in today’s Texas/US may be ok with stopping this type of teaching. But the constitution (and ethical actors) should protect them from being squashed.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the “sanity” that prevailed before Trump’s re-election involved professors being persecuted for things like showing an art history class a centuries-old Persian painting of the prophet Muhammad, or for stating the orthodox view in biology that there are two sexes.&lt;/p&gt;
    &lt;p&gt;https://www.bangordailynews.com/2022/10/03/portland/usm-wont-replace-professor-2-sexes/&lt;lb/&gt; https://www.nytimes.com/2023/01/08/us/hamline-university-islam-prophet-muhammad.html&lt;/p&gt;
    &lt;p&gt;The reality we face is that both the progressive left and the republican party are disgusting fascist abusers who hate academic freedom. The democratic party still retains some vestiges of its old liberalism, even now, but it’s increasingly beholden to the activist left, which is little more at this point than a misandrist and anti-semitic hate movement.&lt;/p&gt;
    &lt;p&gt;I recognize there are a lot of pressures on a chair that I never have and never will experience. I also do nevertheless think it is possible to both pass along a message from higher administrators professionally while also indicating to some degree one’s own attitude toward that message. A university is not a military institution.&lt;/p&gt;
    &lt;p&gt;We don’t know that she was passive. She has to weigh the moral cost of complicity against the value of representing her colleagues as effectively as possible. If she were to resign in protest, presumably the people closer to the central administration would appoint her replacement.&lt;/p&gt;
    &lt;p&gt;Also, they didn’t “ban Plato.” They banned this guy, and presumably others, from teaching Plato as he thought best. Still bad, obviously.&lt;/p&gt;
    &lt;p&gt;“She has to weigh the moral cost of complicity against the value of representing her colleagues as effectively as possible.”&lt;/p&gt;
    &lt;p&gt;Exactly WHAT moral value is there in being complicit for not only censoring the birthfather of Western philosophy but censoring awareness of socio-political inequalities? Resignation is not the only alternative here.&lt;/p&gt;
    &lt;p&gt;Hell, I think even Plato was taught in Nazi Germany.&lt;/p&gt;
    &lt;p&gt;In the spirit of pointing out things we don’t know, it seems to me that we do not know anything about how Peterson intends/intended to teach Plato except for the sake of “teach[ing] students how to structure and evaluate arguments commonly raised in discussions of contemporary moral issues.” Is your position that a ban on teaching Plato for such a purpose, without advocating for or against the views expressed in the text, is appropriate? Or do you have some private information about how Peterson intended to teach the material which conflicts with the paltry evidence available to the public?&lt;/p&gt;
    &lt;p&gt;Because, if the email from Dec. 19th is accurate, the policy for core curriculum courses, of which this course is one, is different from and more restrictive than that for non-core courses. Both policies are, in my opinion, awful, but the difference in fact leaves open that you could teach this exact material, in this exact way, in a different non-core course. Perhaps, de fact, that would not work, but de jure, as it has been described above, it could.&lt;/p&gt;
    &lt;p&gt;Of course chairs must pick their battles. This seems to me one worth picking. Moreover, that can be done without needing to hold that the policy should simply not be enforced. The policy is horribly written (shocker!), but as Peterson points out, assigning this material does not constitute “advocating” any particular ideology. Still less is it clear that Plato’s work even contains so-called “gender ideology”.&lt;/p&gt;
    &lt;p&gt;As for what was banned, he was told to remove the Plato reading, not just to teach it in a different way.&lt;/p&gt;
    &lt;p&gt;Behind closed doors, I am sure she was opposed to this policy. I am sure she has had to work hard to keep the philosophy department afloat amidst some troubling political circumstances. And I don’t doubt that even under these circumstances, a philosophy department can still do some really interesting work some of the time. On the other hand, this amounts to enforcing a principle that makes one’s very field, the history of philosophy, pretty much irrelevant. If we can only study the parts of the cannon that fit certain political rubrics, then what is really left?&lt;/p&gt;
    &lt;p&gt;The communication being the property of the state doesn’t mean the communication is communication from the state. It can remain entirely “deadpan” anyway, for example:&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;Thank you for your email. The College leadership team and I have discussed your syllabus and the Provost office’s requirements for compliance with the new system rule 08.01. I communicated to them that inclusion of a topic is different from advocacy of a point of view and that therefore your syllabus does not indicate a breach of those requirements. I have heard back from them, and you have two options going forward:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You may mitigate your course content to remove the modules on race ideology and gender ideology, and the Plato readings that may include these.&lt;/item&gt;
      &lt;item&gt;You may be reassigned to teach PHIL 482 501-514. Lecture times for this course are T/Th 8:00 – 9:15.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please let us know by end of business tomorrow how you would like to proceed.&lt;lb/&gt; Sincerely,&lt;lb/&gt; Kristi&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;I don’t know what the value is of maintaining this kind of deadpan diction but if it must be maintained, there it is.&lt;/p&gt;
    &lt;p&gt;Banning Plato from a university philosophy course borders on the ridiculous; however…&lt;lb/&gt; Not long ago there were discussions about bringing up topics that invaded students’ “comfort zone”. (I can’t recall more accurate terms. I hope this is adequate to identify that issue.) We need to extend the same tolerance too, say, a course on fascism that has students reading parts of Mein Kampf. We must be willing to extend the same tolerance to discussing issues we find reprehensible. That is a pretty clear categorical imperative.&lt;/p&gt;
    &lt;p&gt;I don’t recall the discussions in question, but was anyone suggesting that the university, through the equivalent of Texas A&amp;amp;M’s “college leadership team,” should dictate to professors whether they are allowed to teach material that invaded students’ “comfort zone,” over the strenuous objections of the professor about the professor’s legal right to teach that material, on the basis of a law passed by the state government about what material to teach in universities? Or were people suggesting that professors use their own judgment and determine voluntarily of their own accord that teaching material that invaded students’ “comfort zone” would be unwise for various reasons? (Secondary question: was Mein Kampf an example of the sort of text people were raising issues with?)&lt;/p&gt;
    &lt;p&gt;One can of course be upset about both situations for similar sorts of reasons, but it seems to me to elide certain important distinctions by lumping both together. It’s not equally intolerant to hold that the state should ban professors from teaching things, even if the professor objects, and to hold that professors should, using their own good judgment, elect not to teach certain things. The latter is what all professors already do, since you cannot teach everything in a course. Suggesting that exercising one’s judgment about what to teach includes (in some cases) excluding material that might invade students’ “comfort zone” seems potentially much more defensible to me than saying that the government should force professors not to teach certain things that the professor has used their judgment to determine they ought to be teaching.&lt;/p&gt;
    &lt;p&gt;If you put a blanket ban on ‘outside pressure’, that’s telling me I can’t tell the professor I think they’re making a mistake. After all, that would be pressuring. And we’re back in free speech violations. Nobody should think free speech means other people aren’t allowed to tell you you’re making a mistake, or suggest that you stop making it. Indeed, that would be a free speech violation of its own. So no outside pressure can’t be the relevant criterion.&lt;/p&gt;
    &lt;p&gt;What’s tricky in these cases is not that there is any outside pressure, it’s that there are 10,000 people making the same suggestion. At some point between one friend saying “Hey I think you should reconsider this” and 10,000 strangers on the internet saying it, something changes. But I think (a) even in the latter case it’s not as bad as a state with a monopoly on the legitimate use of violence saying the same thing, and (b) we need better theory about just what’s wrong with group pressure case, and about what should be done about it.&lt;/p&gt;
    &lt;p&gt;My main advice, at risk of imposing outside pressure on others’ speech, is that all of us should be more careful than we have been the last decade or so at joining pile ons.&lt;/p&gt;
    &lt;p&gt;It’s true it’s not always the case, but it’s also true that it’s not always not the case, right? Surely some professors decide on their own not to assign certain things, absent any pressure. And there is a third category: cases where there is outside pressure, but the pressure is irrelevant, because (e.g.) it is minimal, like one person who will scowl at you once if you don’t cave.&lt;/p&gt;
    &lt;p&gt;It’s unknown because the concept of academic freedom in a legal sense is not very contoured. The courts, including the US Supreme Court, have noted that it is a special consideration of the First Amendment and that it does hold significance under the auspices of the First Amendment but, at least in the case of the US Supreme Court, they’ve also never clearly articulated that it protects individual professorial speech regarding what gets taught and how it gets taught in individual classroom settings. Hence, at minimum, it protects institutions themselves from some yet-to-be-determined amount of outside pressure against what they choose to impart but it’s never been determined to allow professorial speech or instruction to be sacrosanct either. The courts seem more inclined to pivot towards an institutional right more so than a broad right of professor’s to retain autonomy over curricular matters so one’s mileage may vary.&lt;/p&gt;
    &lt;p&gt;Strictly speaking, the instructor wasn’t told he couldn’t teach Plato, but that he couldn’t teach Plato the way he wanted to. So if he were to sue, it seems to me it would have to be for the right to teach however he wants to compatibly with the university-sanctioned course description. I’m not a lawyer, but that seems like a tougher case to make. Universities tell instructors what to teach all the time.&lt;/p&gt;
    &lt;p&gt;I think this is great news. It’s woke proponents of gender ideology like Plato that want to destroy our glorious Western Civilization. People like Plato need to go back and read the Great Thinkers of the past, who would never perpetuate the kinds of new-fangled post-modernist thought going on in the Symposium.&lt;lb/&gt; Plato Who? What happened our the great western canon?&lt;/p&gt;
    &lt;p&gt;Just mentioning this in case it is useful to any folks out there. I interviewed for a tt job there a couple of years ago and was assured by all faculty that academic freedom was not and never had been under threat despite the government. I mention this not to slight the department of philosophy at all (my experience there was lovely!), but as a warning for all on the job market: take the political climate very, very seriously.&lt;/p&gt;
    &lt;p&gt;There is some chance at least though that if we take the political climate seriously enough, we dig ourselves a hole in the ground and hide there, well, for a long time. (And when we come out, the world will be burning and Trump’s family will be hanging out in the South Island (NZ) with his bodies…)&lt;/p&gt;
    &lt;p&gt;I mentioned in another post a few months ago that Texas A&amp;amp;M was hiring somebody with AoS in French continental philosophy and that would be a career suicide for anyone who applies for this, and how ironic it is that they’d hire someone for this. People were quick to respond in the same way you mentioned, but also that the philosophy department is against the current policies. It doesn’t seem like it, especially seeing as the chair is trying to enforce these rules.&lt;/p&gt;
    &lt;p&gt;I think it is difficult to tell what the department head is actually trying to do just from reading the text that she puts into writing in the official communication. When I was at Texas A&amp;amp;M, I was always told to maintain a strict differentiation between what messages I send from my tamu.edu address and what messages I send from any other email addresses I use (to ensure that the latter were not subject to FOIA requests). I was also told to be very careful about what was communicated in writing and what was communicated verbally.&lt;/p&gt;
    &lt;p&gt;So there’s no way that these committees are actually reading the content of each of the hundreds of syllabi. They are pasting them into AI and asking “are there any references to gender/race ideology, sexual orientation, etc in these readings?”&lt;/p&gt;
    &lt;p&gt;This will obviously give a “yes” for the Symposium. Moreover, Prof. Peterson’s two classes on “Race and Gender Ideology” are conspicuously named; almost as if he knew he would trigger this result (well done, there). The course textbook does not, so far as I can tell, contain any material on race, so this judgment can only be due to a superficial text scan of the syllabus itself.&lt;/p&gt;
    &lt;p&gt;I must emphasize how important this is: they are not delving into the contents of the readings. They do not have the time or resources for that. They are pasting syllabi into ChatGPT. Those who wish to evade or fight this surveillance should take this under consideration.&lt;/p&gt;
    &lt;p&gt;I’m sure you’re right about their methods, but fwiw, Ch 13, which is the assigned reading for the race and gender module, includes loads of material on race, including intersectionality, affirmative action, reparations, police brutality, etc. (All excellent topics to discuss in a course on contemporary moral problems)&lt;/p&gt;
    &lt;p&gt;When I was an undergrad at Rice University and we’d play TAMU in football, they usually beat us on the field and their fans usually outnumbered us at Rice Stadium. But the Rice students would cheer back, That’s all right, That’s okay, We’ll be your boss someday. And we always won half time.&lt;/p&gt;
    &lt;p&gt;The Aggies are a fine University, and it’s sad to see news like this.&lt;/p&gt;
    &lt;p&gt;My father was in the Texas A&amp;amp;M class of 1944. Although he didn’t officially graduate until 1948 because of a little intervention called World War II.&lt;/p&gt;
    &lt;p&gt;Say what you will about Aggies they are generally a very tough practical and smart group of people. They are not typically in the past men little snowflakes who might get their panties in a bunch of reading a Greek philosopher . This incident is sad and pathetic, and I’m sure my dad is rolling in his grave and the Fort Sam cemetery.&lt;/p&gt;
    &lt;p&gt;I know Sweet is only reporting the policy, but the move from “advocate” in paragraph 2 to the “clarification” in paragraph 3 that this means “include” is exactly the kind of two step that we were taught to look out for in my philosophy program, and this seems like the rare case in which a philosopher is more equipped than most to make some kind of practical action (however futile, and I’ll concede that I never went on to a grad program, so maybe there’s some secret they let you in on that makes this acceptable).&lt;/p&gt;
    &lt;p&gt;This is a very interesting email chain. You can see some of the details of how serious scholars in philosophy are trying to keep their jobs amidst extreme political changes. For example, it appears that they retain some greater measure of freedom in non-core classes. This one got extra scrutiny because it is serving a broader public. The rationale here goes: I still get to be a specialist and scholar in my own special domain but when it comes to serving the herd we will do what they tell us. You can also see how someone the chair, who is apparently an accomplished and collaborative scholar in the history of philosophy, is willing to enforce this policy. Behind closed doors she may have disagreed with this but when it comes to implementing it, she is willing to do the work. Perhaps she had no good options. But when it comes to being an advocate for the field–this speaks louder than any work one could do in publications or conferences.&lt;/p&gt;
    &lt;p&gt;Re: “…University ban’s Plato…”&lt;/p&gt;
    &lt;p&gt;“I picked a terrible week to stop snorting glue…” (the late actor, Lloyd Bridges, in the spoof, ‘Airplane-2’).&lt;/p&gt;
    &lt;p&gt;What’s next?&lt;lb/&gt; Will my former, ancestral faith…based in rome…decide a homeless, poverty-stricken individual…allegedly a ‘Human-Deity-Hybrid, crucified’…&lt;lb/&gt; …Is banned as well?&lt;/p&gt;
    &lt;p&gt;Although I am a ‘Wotanic’ NeoPagan, I used to like what my experience of ‘Christianity’ claimed their founder, advocated…&lt;lb/&gt; As M. Ghandi said:&lt;lb/&gt; ‘I like your ‘Christ’; His Jihadist followers? …Not so much!’ (paraphrased, not quoted)&lt;/p&gt;
    &lt;p&gt;This is hardly the most important thing in this story, but it struck me: I don’t think until now I’ve seen a philosophy professor call the study of race and gender “Race and Gender Ideology,” on a syllabus or anywhere else. That sounds like the language right-wing politicians use to slander philosophy of race, feminist philosophy, queer studies, etc. Am I simply mistaken? I in no way blame Martin Peterson for the terrible thing that Texas A&amp;amp;M is doing, but it does seem to put a lightning rod in his syllabus needlessly. Why not call it something more accurate, like “philosophical approaches to the study or race and gender,” or something like that? This likely would have made no difference, but the weird use of the term “ideology” in this context grates at me.&lt;/p&gt;
    &lt;p&gt;Some people here are saying the solution is to vote out the conservatives. In a global sense I agree. But more specifically, it does not relate to the real problem here. Do any of us want to live in an academic ecosystem where public opinion as understood by state legislators in gerrymandered districts are micromanaging what books we read and discuss?&lt;/p&gt;
    &lt;p&gt;The title immediately throws up red flags about the credibility of the article. Plato wasn’t banned, in fact the exact material here wasn’t even banned. It simply is being moved to a non-core course. The title is pushing an agenda, one that loses it’s impetus when it sits behind a falsehood.&lt;/p&gt;
    &lt;p&gt;The AAUP chapter at Texas A&amp;amp;M has released a statement condemning this censorship. I encourage anyone working in Texas and other states with similarly hostile laws and policies to consider joining the AAUP, which is consistently fighting this sort of censorship and erosion of faculty governance. AAUP is affiliated with AFT and membership gets you access to legal consultations and protections and a legal defense fund.&lt;/p&gt;
    &lt;p&gt;https://aaup-texas.org/blog/f/aaup-texas-am-university-condemns-banning-of-plato&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dailynous.com/2026/01/06/texas-am-bans-plato/"/><published>2026-01-07T17:23:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46529797</id><title>A tab hoarder's journey to sanity</title><updated>2026-01-07T20:12:53.150869+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/borisandcrispin/status/2008709479068794989"/><published>2026-01-07T17:54:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46530448</id><title>NPM to implement staged publishing after turbulent shift off classic tokens</title><updated>2026-01-07T20:12:53.016903+00:00</updated><content/><link href="https://socket.dev/blog/npm-to-implement-staged-publishing"/><published>2026-01-07T18:31:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531068</id><title>US will ban Wall Street investors from buying single-family homes</title><updated>2026-01-07T20:12:52.672067+00:00</updated><content>&lt;doc fingerprint="e2c951c73db284a3"&gt;
  &lt;main&gt;
    &lt;p&gt;Jan 7 (Reuters) - U.S. President Donald Trump on Wednesday said his administration is moving to ban Wall Street from investing in single-family homes in a bid to reduce home prices, a potential blow for private-equity landlords that also pressured homebuilder shares.&lt;/p&gt;
    &lt;p&gt;In a post on Truth Social, Trump said he was taking immediate action and would ask Congress to codify the measure, adding he would also be discussing additional housing and affordability proposals in a speech at the Davos World Economic Forum.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;"For a very long time, buying and owning a home was considered the pinnacle of the American Dream," Trump wrote, going on to add that inflation had put that dream out of reach for many Americans.&lt;/p&gt;
    &lt;p&gt;"People live in homes, not corporations," Trump said.&lt;/p&gt;
    &lt;head rend="h2"&gt;WALL STREET BLAMED FOR REDUCED HOUSING SUPPLY&lt;/head&gt;
    &lt;p&gt;Wall Street institutions such as Blackstone (BX.N) have bought thousands of single-family homes since the financial crisis of 2008 led to a wave of home foreclosures.&lt;/p&gt;
    &lt;p&gt;They are increasingly putting cash into rental homes, which have fared better than other commercial property sectors such as office and retail, through a period of soaring borrowing costs and changing working patterns.&lt;/p&gt;
    &lt;p&gt;Wall Street's expansion into home ownership has attracted criticism from housing advocacy groups and lawmakers, including Democrats, who say institutional landlords have reduced housing supply and stoked rent inflation.&lt;/p&gt;
    &lt;p&gt;Homebuilder shares dropped sharply after Trump's comments. Housing acquisition company American Homes 4 Rent (AMH.N) dropped to a near three-year low of $28.84 and was halted for volatility before trading resumed. Its shares were last down nearly 6% at $30.61. Blackstone shares hit a one-month low of $147.52 and were last down about 4.5% at $155.33. The PHLX housing index (.HGX) was down 2.3% on the session, on track for its biggest daily percentage drop since November 17.&lt;/p&gt;
    &lt;p&gt;Blackstone did not immediately respond to a request for comment. Wall Street landlords dispute that their investments have stoked inflation and hurt housing supply. In a January research note, Blackstone said institutions own only 0.5% of all single-family homes in the United States.&lt;/p&gt;
    &lt;p&gt;It was not immediately clear what legal authority Trump would draw upon to impose such a ban on the private market purchases of houses. Trump did not detail the policy, the form it would take or the legal changes he was seeking from Congress.&lt;/p&gt;
    &lt;p&gt;The White House did not immediately respond to a request for comment. The U.S. president was due to sign unspecified executive orders later on Wednesday.&lt;/p&gt;
    &lt;head rend="h2"&gt;AFFORDABILITY PRESSURE&lt;/head&gt;
    &lt;p&gt;The president is under growing pressure to address voter anxiety over the cost of living ahead of midterm elections that will determine whether Trump's Republicans retain control of Congress.&lt;/p&gt;
    &lt;p&gt;Trump, who has occasionally dismissed affordability concerns and blamed inflation on his Democratic predecessor, has seen his own public approval mostly sag since his inauguration as Americans worry about the state of the economy.&lt;/p&gt;
    &lt;p&gt;As of November, annual shelter-cost inflation, which had shot to as high as 8.2% in the pandemic aftermath, eased to 3.0%, the lowest in more than four years, according to the Labor Department's Consumer Price Index. That is close to its average over the four years of Trump's first term.&lt;/p&gt;
    &lt;p&gt;Home sale price increases have also eased substantially. The Federal Housing Finance Agency, headed by staunch Trump ally William Pulte, last week reported that national home sales prices had risen just 1.7% in October from a year earlier - the lowest in more than 13 years.&lt;/p&gt;
    &lt;p&gt;Reporting by Ryan Patrick Jones in Toronto and Trevor Hunnicutt in Washington; additional reporting by Chuck Mikolajczak, Andrea Shalal and Dan Burns; Editing by Caitlin Webber, David Ljunggren, Michelle Price, Cynthia Osterman, Rod Nickel&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/"/><published>2026-01-07T19:13:19+00:00</published></entry></feed>