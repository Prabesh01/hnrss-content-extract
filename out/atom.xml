<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-29T09:36:28.084270+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46080289</id><title>Airloom – 3D Flight Tracker</title><updated>2025-11-29T09:36:35.197365+00:00</updated><content>&lt;doc fingerprint="77ed4fa771de6491"&gt;
  &lt;main&gt;
    &lt;p&gt;W/A/S/D - Move forward/left/back/right&lt;/p&gt;
    &lt;p&gt;Space - Move up&lt;/p&gt;
    &lt;p&gt;Shift - Move down&lt;/p&gt;
    &lt;p&gt;Mouse - Look around&lt;/p&gt;
    &lt;p&gt;Click anywhere to start flying&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://objectiveunclear.com/airloom.html"/><published>2025-11-28T16:49:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46080364</id><title>So you wanna build a local RAG?</title><updated>2025-11-29T09:36:34.936998+00:00</updated><content>&lt;doc fingerprint="fb5fccd27d06c5d6"&gt;
  &lt;main&gt;
    &lt;p&gt;When we launched Skald, we wanted it to not only be self-hostable, but also for one to be able to run it without sending any data to third-parties.&lt;/p&gt;
    &lt;p&gt;With LLMs getting better and better, privacy-sensitive organizations shouldn't have to choose between being left behind by not accessing frontier models and doing away with their committment to (or legal requirement for) data privacy.&lt;/p&gt;
    &lt;p&gt;So here's what we did to support this use case and also some benchmarks comparing performance when using proprietary APIs vs self-hosted open-source tech.&lt;/p&gt;
    &lt;head rend="h2"&gt;RAG components and their OSS alternatives&lt;/head&gt;
    &lt;p&gt;A basic RAG usually has the following core components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A vector database&lt;/item&gt;
      &lt;item&gt;A vector embeddings model&lt;/item&gt;
      &lt;item&gt;An LLM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And most times it also has these as well:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A reranker&lt;/item&gt;
      &lt;item&gt;Document parsing (for PDFs, PowerPoints, etc)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What that means is that when you're looking to build a fully local RAG setup, you'll need to substitute whatever SaaS providers you're using for a local option for each of those components.&lt;/p&gt;
    &lt;p&gt;Here's a table with some examples of what we might use in a scenario where we can use third-party Cloud services and one where we can't:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Proprietary Options&lt;/cell&gt;
        &lt;cell role="head"&gt;Open-Source Options&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vector Database&lt;/cell&gt;
        &lt;cell&gt;Pinecone, Turbopuffer, Weaviate Cloud, Qdrant Cloud&lt;/cell&gt;
        &lt;cell&gt;Qdrant, Weaviate, Postgres with pgvector&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vector Embeddings Provider&lt;/cell&gt;
        &lt;cell&gt;OpenAI, Cohere, Voyage&lt;/cell&gt;
        &lt;cell&gt;Sentence Transformers, BGE, E5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;GPT, Claude, Gemini&lt;/cell&gt;
        &lt;cell&gt;Llama, Mistral, GPT-OSS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reranker&lt;/cell&gt;
        &lt;cell&gt;Cohere, Voyage&lt;/cell&gt;
        &lt;cell&gt;BGE Reranker, Sentence Transformers Cross-Encoder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Document Parsing&lt;/cell&gt;
        &lt;cell&gt;Reducto, Datalab&lt;/cell&gt;
        &lt;cell&gt;Docling&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Do note that running something locally does not mean it needs to be open-source, as one could pay for a license to self-host proprietary software. But at Skald our goal was to use fully open-source tech, which is what I'll be convering here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The table above is far from covering all available options on both columns, but basically it gives you an indication of what to research into in order to pick a tool that works for you.&lt;/p&gt;
    &lt;p&gt;As with anything, what works for you will greatly depend on your use case. And you need to be prepared to run a few more services than you're used to if you've just been calling APIs.&lt;/p&gt;
    &lt;p&gt;For our local stack, we went with the easiest setup for now to get it working (and it does! see writeup on this lower down) but will be running benchmarks on all other options to determine the best possible setup.&lt;/p&gt;
    &lt;p&gt;This is what we have today:&lt;/p&gt;
    &lt;p&gt;Vector DB: Postgres + pgvector. We already use Postgres and didn't want to bundle another service into our stack, but this is controversial and we will be running benchmarks to make a better informed decision here. Note that pgvector will serve a lot of use cases well all the way up to hundreds of thousands of documents, though.&lt;/p&gt;
    &lt;p&gt;Vector embeddings: Users can configure this in Skald and we use Sentence Transformers (all-MiniLM-L6-v2) as our default (solid all-around performer for speed and retrieval, English-only). I also ran Skald with bge-m3 (larger, multi-language) and share the results later in this post.&lt;/p&gt;
    &lt;p&gt;LLM: We don't even bundle a default with Skald and it's up to the users to run and manage this. I tested our setup with GPT-OSS 20B on EC2 (results shown below).&lt;/p&gt;
    &lt;p&gt;Reranker: Users can also configure this in Skald, and the default is the Sentence Transformers cross encoder (solid, English-only). I've also used bge-reranker-v2-m3 and mmarco-mMiniLMv2-L12-H384-v1 which offer multi-lingual support.&lt;/p&gt;
    &lt;p&gt;Document parsing: There isn't much of a question on this one. We're using Docling. It's great. We run it via docling-serve.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does it perform though?&lt;/head&gt;
    &lt;p&gt;So the main goal here was first to get something working then ensure it worked well with our platform and could be easily deployed. From here we'll be running extensive benchmarks and working with our clients to provide a solid setup that both performs well but is also not a nightmare to deploy and manage.&lt;/p&gt;
    &lt;p&gt;From that perspective, this was a great success.&lt;/p&gt;
    &lt;p&gt;Deploying a production instance of Skald with this whole stack took me 8 minutes, and that comes bundled with the vector database (well, Postgres), a reranking and embedding service, and Docling.&lt;/p&gt;
    &lt;p&gt;The only thing I needed to run separately was the LLM, which I did via llama.cpp.&lt;/p&gt;
    &lt;p&gt;Having gotten this sorted, I imported all the content from the PostHog website [1] and set up a tiny dataset [2] of questions and expected answers inside of Skald, then used our Experiments feature to run the RAG over this dataset.&lt;/p&gt;
    &lt;p&gt;I explicitly kept the topK values really high (100 for the vector search and 50 for post-reranking), as I was mostly testing for accuracy and wanted to see the performance when questions required e.g. aggregating context over 15+ documents.&lt;/p&gt;
    &lt;head&gt;Full config&lt;/head&gt;
    &lt;p&gt;Here are the params configured in the Skald UI for the the experiment.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Config option&lt;/cell&gt;
        &lt;cell role="head"&gt;Selection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extra system prompt&lt;/cell&gt;
        &lt;cell&gt;Be really concise in your answers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Query rewriting&lt;/cell&gt;
        &lt;cell&gt;Off&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vector search topK&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vector search distance threshold&lt;/cell&gt;
        &lt;cell&gt;0.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reranking&lt;/cell&gt;
        &lt;cell&gt;On&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reranking topK&lt;/cell&gt;
        &lt;cell&gt;50&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;References&lt;/cell&gt;
        &lt;cell&gt;Off&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So without any more delay, here are the results of my not-very-scientific at all benchmark using the experimentation platform inside of Skald.&lt;/p&gt;
    &lt;head rend="h3"&gt;Voyage + Claude&lt;/head&gt;
    &lt;p&gt;This is our default Cloud setup. We use voyage-3-large and rerank-2.5 from Voyage AI as our embedding and reranking models respectively, and we default to Claude Sonnet 3.7 for responses (users can configure the model though).&lt;/p&gt;
    &lt;p&gt;It passed with flying colors.&lt;/p&gt;
    &lt;p&gt;Our LLM-as-a-Judge gave an average score of 9.45 to the responses, and I basically agree with the assessment. All answers were correct, with one missing a few extra bits of context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Voyage + GPT-OSS 20B&lt;/head&gt;
    &lt;p&gt;With the control experiment done, I then moved on to a setup where I kept Voyage as the embeddings provider and reranker, and then used GPT-OSS 20B running on a llama.cpp server on a g5.2xlarge EC2 instance as the LLM.&lt;/p&gt;
    &lt;p&gt;The goal here was to see how well the open-source LLM model itself stacked up against a frontier model accessed via API.&lt;/p&gt;
    &lt;p&gt;And it did great!&lt;/p&gt;
    &lt;p&gt;We don't yet support LLM-as-a-Judge on fully local deployments, so the only score we have here is mine. I scored the answers an average of 9.18 and they were all correct, with two of them just missing a few bits of information or highlighting less relevant information from the context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fully local + GPT-OSS 20B&lt;/head&gt;
    &lt;p&gt;Lastly, it was time for the moment of truth: running a fully local setup.&lt;/p&gt;
    &lt;p&gt;For this I ran two tests:&lt;/p&gt;
    &lt;p&gt;1. Default sentence transformers embedding and reranking models&lt;/p&gt;
    &lt;p&gt;The most popular open-source models are all-MiniLM-L6-v2 for embeddings and ms-marco-MiniLM-L6-v2 as the reranker, so I used those for my first benchmark.&lt;/p&gt;
    &lt;p&gt;Here the average score was 7.10. Not bad, but definitely not great. However, when we dig into the results, we can get a better understanding of how this setup fails.&lt;/p&gt;
    &lt;p&gt;Basically, it got all point queries right, which are questions where the answer is somewhere in the mess of documents, but can be found from one specific place.&lt;/p&gt;
    &lt;p&gt;Where it failed was:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-english query: The embeddings model and the reranker are English-based, so my question in Portuguese obviously got no answer&lt;/item&gt;
      &lt;item&gt;An ambiguous question with very little context ("what's ch")&lt;/item&gt;
      &lt;item&gt;Aggregating information from multiple documents/chunks e.g. it only found 5 out of PostHog's 7 funding rounds, and only a subset of the PostHog competitors that offer session replay (as mentioned in the source data)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In my view, this is good news. That means that the default options will go a long way and should give you very good performance if your use case is only doing point queries in English. The other great thing is that these models are also fast.&lt;/p&gt;
    &lt;p&gt;Now, if you need to handle ambiguity better, or handle questions in other languages, then this setup is simply not for you.&lt;/p&gt;
    &lt;p&gt;2. Multi-lingual models&lt;/p&gt;
    &lt;p&gt;The next test I did used bge-m3 as the embeddings model and mmarco-mMiniLMv2-L12-H384-v1 as the reranker. The embeddings model is supposedly much better than the one used in the previous test and is also multi-lingual. The reranker on the other hand uses the same cross-encoder from the previous test as the base model but also adds multi-lingual support. The more standard option here would have been the much more popular bge-reranker-v2-m3 model, but I found it to be much slower. I intend to tweak my setup and test it again, however.&lt;/p&gt;
    &lt;p&gt;Anyway, onto the results! I scored it 8.63 on average, which is very good. There were no complete failures, and it handled the question in Portuguese well.&lt;/p&gt;
    &lt;p&gt;The mistakes it made were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This new setup also did not do the best job at aggregating information, missing 2 of PostHog's funding rounds, and a couple of its session replay competitors&lt;/item&gt;
      &lt;item&gt;It also answered a question correctly, but added incorrect additional context after it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So overall it performed quite well. Again what we what saw was the main problem is when the context needed for the response is scattered across multiple documents. There are various techniques to help with this and we'll be trialing some soon! They haven't been needed on the Cloud version because better models save you from having to add complexity for minimal performance gains, but as we're focused on building a really solid setup for local deploys, we'll be looking into this more and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Now what?&lt;/head&gt;
    &lt;p&gt;I hope this writeup has provided you with at least some insight and context into building a local RAG, and also the fact that it does work, it can serve a lot of use cases, and that the tendency is for this setup to get better and better as a) models improve b) we get more open-source models across the board, with both being things that we seem to be trending towards.&lt;/p&gt;
    &lt;p&gt;As for us at Skald, we intend to polish this setup further in order to serve even more use cases really well, as well as intend to soon be publishing more legitimate benchmarks for models in the open-source space, from LLMs to rerankers.&lt;/p&gt;
    &lt;p&gt;If you're a company that needs to run AI tooling in air-gapped infrastructure, let's chat -- feel free to email me at yakko [at] useskald [dot] com.&lt;/p&gt;
    &lt;p&gt;Lastly, if you want to get involved, feel free to chat to us over on our GitHub repo (MIT-licensed) or catch us on Slack.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;[1] I used the PostHog website here because the website content is MIT-licensed (yes, wild) and readily-available as markdown on GitHub and having worked there I know a lot of answers off the top of my head making it a great dataset of ~2000 documents that I know well.&lt;/p&gt;
    &lt;p&gt;[2] The questions and answers dataset I used for the experiments was the following:&lt;/p&gt;
    &lt;head&gt;Dataset&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Question&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected answer&lt;/cell&gt;
        &lt;cell role="head"&gt;Comments&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;How many raises did PostHog do?&lt;/cell&gt;
        &lt;cell&gt;PostHog has raised money 7 times: it raised $150k from YCombinator, then did a seed round ($3.025M), a series A ($12M), a series B ($15M), a series C ($10M), a series D ($70M), and a series E ($75M).&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating context from at least 7 documents&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;When did group analytics launch?&lt;/cell&gt;
        &lt;cell&gt;December 16, 2021.&lt;/cell&gt;
        &lt;cell&gt;Point query, multiple mentions to "group analytics" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Why was the sessions page removed?&lt;/cell&gt;
        &lt;cell&gt;The sessions page was removed because it was confusing and limited in functionality. It was replaced by the 'Recordings' tab.&lt;/cell&gt;
        &lt;cell&gt;Point query, multiple mentions to "sessions" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;What's the difference between a product engineer and other roles?&lt;/cell&gt;
        &lt;cell&gt;Compared to product managers, product engineers focus more on building rather than deep research and planning. When it comes to software engineers, both product and software engineers write code, but software engineers focus on building great software, whereas product engineers focus on building great products.&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating context from multiple docs + there are a ton of mentions of "product engineer" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;What were the main benefits of YC?&lt;/cell&gt;
        &lt;cell&gt;The main benefits of YC were: Network Access, Investor Reviews, Office Hours, Funding Opportunities, Hiring Resources, Angel Investing Opportunities, Accelerated Growth and Experience, Shift in Self-Perception, Customer Acquisition, Product Market Fit, Ambitious Goal Setting, Access to Thought Leaders, Community Support&lt;/cell&gt;
        &lt;cell&gt;Point query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;quem foi o primeiro investidor da posthogg?&lt;/cell&gt;
        &lt;cell&gt;O primeiro investidor da PostHog foi o YCombinator.&lt;/cell&gt;
        &lt;cell&gt;Question in Portuguese, with PostHog misspelled&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what posthog competitors also offer session replays&lt;/cell&gt;
        &lt;cell&gt;LogRocket, Smartlook, FullStory, Microsoft Clarity, Contentsquare, Mouseflow, Heap, Pendo, Hotjar, Glassbox, and Amplitude.&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating content from at least 11 docs (more because I actually missed some in my expected answer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;top tips find client&lt;/cell&gt;
        &lt;cell&gt;1. Leverage your inner circle 2. Join relevant communities 3. Be laser-focused 4. Set achievable goals 5. Frame conversations properly&lt;/cell&gt;
        &lt;cell&gt;Point query, worded weirdly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what's ch&lt;/cell&gt;
        &lt;cell&gt;CH most likely refers to ClickHouse, a column-oriented OLAP database.&lt;/cell&gt;
        &lt;cell&gt;Really ambiguous. I meant ClickHouse with my question.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what is mixedpanel&lt;/cell&gt;
        &lt;cell&gt;Mixpanel is a popular product analytics tool that was founded in 2009&lt;/cell&gt;
        &lt;cell&gt;Mixpanel misspelled as Mixedpanel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;how was prpoerty filtering made faster?&lt;/cell&gt;
        &lt;cell&gt;Using materialized columns allowed ClickHouse to skip JSON parsing during queries and made queries with property filtering 25x faster.&lt;/cell&gt;
        &lt;cell&gt;Point query with a typo&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.yakkomajuri.com/blog/local-rag"/><published>2025-11-28T16:54:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46080916</id><title>Molly: An Improved Signal App</title><updated>2025-11-29T09:36:34.643198+00:00</updated><content>&lt;doc fingerprint="fa4c68be1bd036dc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;An improved Signal app&lt;/head&gt;
    &lt;p&gt;Molly is an independent Signal fork for Android with improved features:&lt;/p&gt;
    &lt;p&gt; Fully FOSS Contains no proprietary blobs, unlike Signal &lt;/p&gt;
    &lt;p&gt; Encrypted Protects database with Passphrase Encryption &lt;/p&gt;
    &lt;p&gt; Multi-Device Pair multiple devices to a single account &lt;/p&gt;
    &lt;p&gt; Material You Extra theme that follows your device palette &lt;/p&gt;
    &lt;p&gt; UnifiedPush Ungoogled notification system &lt;/p&gt;
    &lt;p&gt; Automatic Locking When you are gone for a set period of time &lt;/p&gt;
    &lt;p&gt; RAM Shredding Securely shreds sensitive data &lt;/p&gt;
    &lt;p&gt; And more New and better features to come &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://molly.im/"/><published>2025-11-28T17:48:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46081053</id><title>28M Hacker News comments as vector embedding search dataset</title><updated>2025-11-29T09:36:34.418055+00:00</updated><content>&lt;doc fingerprint="11938b322ad7669e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Generate embeddings for search query&lt;/head&gt;
      &lt;p&gt;Sentence Transformers provide local, easy to use embedding models for capturing the semantic meaning of sentences and paragraphs.&lt;/p&gt;
      &lt;p&gt;The dataset in this HackerNews dataset contains vector emebeddings generated from the all-MiniLM-L6-v2 model.&lt;/p&gt;
      &lt;p&gt;An example Python script is provided below to demonstrate how to programmatically generate embedding vectors using &lt;code&gt;sentence_transformers1 Python package. The search embedding vector is then passed as an argument to the [&lt;/code&gt;cosineDistance()&lt;code&gt;](/sql-reference/functions/distance-functions#cosineDistance) function in the &lt;/code&gt;SELECT` query.&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;from sentence_transformers import SentenceTransformer
import sys

import clickhouse_connect

print("Initializing...")

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

chclient = clickhouse_connect.get_client() # ClickHouse credentials here

while True:
    # Take the search query from user
    print("Enter a search query :")
    input_query = sys.stdin.readline();
    texts = [input_query]

    # Run the model and obtain search vector
    print("Generating the embedding for ", input_query);
    embeddings = model.encode(texts)

    print("Querying ClickHouse...")
    params = {'v1':list(embeddings[0]), 'v2':20}
    result = chclient.query("SELECT id, title, text FROM hackernews ORDER BY cosineDistance(vector, %(v1)s) LIMIT %(v2)s", parameters=params)
    print("Results :")
    for row in result.result_rows:
        print(row[0], row[2][:100])
        print("---------")

&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;p&gt;An example of running the above Python script and similarity search results are shown below (only 100 characters from each of the top 20 posts are printed):&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;Initializing...

Enter a search query :
Are OLAP cubes useful

Generating the embedding for  "Are OLAP cubes useful"

Querying ClickHouse...

Results :

27742647 smartmic:
slt2021: OLAP Cube is not dead, as long as you use some form of:&amp;lt;p&amp;gt;1. GROUP BY multiple fi
---------
27744260 georgewfraser:A data mart is a logical organization of data to help humans understand the schema. Wh
---------
27761434 mwexler:&amp;amp;quot;We model data according to rigorous frameworks like Kimball or Inmon because we must r
---------
28401230 chotmat:
erosenbe0: OLAP database is just a copy, replica, or archive of data with a schema designe
---------
22198879 Merick:+1 for Apache Kylin, it&amp;amp;#x27;s a great project and awesome open source community. If anyone i
---------
27741776 crazydoggers:I always felt the value of an OLAP cube was uncovering questions you may not know to as
---------
22189480 shadowsun7:
_Codemonkeyism: After maintaining an OLAP cube system for some years, I&amp;amp;#x27;m not that
---------
27742029 smartmic:
gengstrand: My first exposure to OLAP was on a team developing a front end to Essbase that
---------
22364133 irfansharif:
simo7: I&amp;amp;#x27;m wondering how this technology could work for OLAP cubes.&amp;lt;p&amp;gt;An OLAP cube
---------
23292746 scoresmoke:When I was developing my pet project for Web analytics (&amp;lt;a href="https:&amp;amp;#x2F;&amp;amp;#x2F;github
---------
22198891 js8:It seems that the article makes a categorical error, arguing that OLAP cubes were replaced by co
---------
28421602 chotmat:
7thaccount: Is there any advantage to OLAP cube over plain SQL (large historical database r
---------
22195444 shadowsun7:
lkcubing: Thanks for sharing. Interesting write up.&amp;lt;p&amp;gt;While this article accurately capt
---------
22198040 lkcubing:Thanks for sharing. Interesting write up.&amp;lt;p&amp;gt;While this article accurately captures the issu
---------
3973185 stefanu:
sgt: Interesting idea. Ofcourse, OLAP isn't just about the underlying cubes and dimensions,
---------
22190903 shadowsun7:
js8: It seems that the article makes a categorical error, arguing that OLAP cubes were r
---------
28422241 sradman:OLAP Cubes have been disrupted by Column Stores. Unless you are interested in the history of
---------
28421480 chotmat:
sradman: OLAP Cubes have been disrupted by Column Stores. Unless you are interested in the
---------
27742515 BadInformatics:
quantified: OP posts with inverted condition: “OLAP != OLAP Cube” is the actual titl
---------
28422935 chotmat:
rstuart4133: I remember hearing about OLAP cubes donkey&amp;amp;#x27;s years ago (probably not far
---------
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Summarization demo application&lt;/head&gt;
      &lt;p&gt;The example above demonstrated semantic search and document retrieval using ClickHouse.&lt;/p&gt;
      &lt;p&gt;A very simple but high potential generative AI example application is presented next.&lt;/p&gt;
      &lt;p&gt;The application performs the following steps:&lt;/p&gt;
      &lt;list class="custom-ol" rend="ol"&gt;
        &lt;item class="custom-li"&gt;Accepts a topic as input from the user&lt;/item&gt;
        &lt;item class="custom-li"&gt;Generates an embedding vector for the topic by using the &lt;code&gt;SentenceTransformers&lt;/code&gt; with model &lt;code&gt;all-MiniLM-L6-v2&lt;/code&gt;&lt;/item&gt;
        &lt;item class="custom-li"&gt;Retrieves highly relevant posts/comments using vector similarity search on the &lt;code&gt;hackernews&lt;/code&gt; table&lt;/item&gt;
        &lt;item class="custom-li"&gt;Uses &lt;code&gt;LangChain&lt;/code&gt; and OpenAI &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; Chat API to summarize the content retrieved in step #3.
The posts/comments retrieved in step #3 are passed as context to the Chat API and are the key link in Generative AI.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;An example from running the summarization application is first listed below, followed by the code for the summarization application. Running the application requires an OpenAI API key to be set in the environment variable &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. The OpenAI API key can be obtained after registering at https://platform.openai.com.&lt;/p&gt;
      &lt;p&gt;This application demonstrates a Generative AI use-case that is applicable to multiple enterprise domains like : customer sentiment analysis, technical support automation, mining user conversations, legal documents, medical records, meeting transcripts, financial statements, etc&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;$ python3 summarize.py

Enter a search topic :
ClickHouse performance experiences

Generating the embedding for ----&amp;gt;  ClickHouse performance experiences

Querying ClickHouse to retrieve relevant articles...

Initializing chatgpt-3.5-turbo model...

Summarizing search results retrieved from ClickHouse...

Summary from chatgpt-3.5:
The discussion focuses on comparing ClickHouse with various databases like TimescaleDB, Apache Spark,
AWS Redshift, and QuestDB, highlighting ClickHouse's cost-efficient high performance and suitability
for analytical applications. Users praise ClickHouse for its simplicity, speed, and resource efficiency
in handling large-scale analytics workloads, although some challenges like DMLs and difficulty in backups
are mentioned. ClickHouse is recognized for its real-time aggregate computation capabilities and solid
engineering, with comparisons made to other databases like Druid and MemSQL. Overall, ClickHouse is seen
as a powerful tool for real-time data processing, analytics, and handling large volumes of data
efficiently, gaining popularity for its impressive performance and cost-effectiveness.
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;p&gt;Code for the above application :&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;print("Initializing...")

import sys
import json
import time
from sentence_transformers import SentenceTransformer

import clickhouse_connect

from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
import textwrap
import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -&amp;gt; int:
    encoding = tiktoken.encoding_for_model(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

chclient = clickhouse_connect.get_client(compress=False) # ClickHouse credentials here

while True:
    # Take the search query from user
    print("Enter a search topic :")
    input_query = sys.stdin.readline();
    texts = [input_query]

    # Run the model and obtain search or reference vector
    print("Generating the embedding for ----&amp;gt; ", input_query);
    embeddings = model.encode(texts)

    print("Querying ClickHouse...")
    params = {'v1':list(embeddings[0]), 'v2':100}
    result = chclient.query("SELECT id,title,text FROM hackernews ORDER BY cosineDistance(vector, %(v1)s) LIMIT %(v2)s", parameters=params)

    # Just join all the search results
    doc_results = ""
    for row in result.result_rows:
        doc_results = doc_results + "\n" + row[2]

    print("Initializing chatgpt-3.5-turbo model")
    model_name = "gpt-3.5-turbo"

    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
        model_name=model_name
    )

    texts = text_splitter.split_text(doc_results)

    docs = [Document(page_content=t) for t in texts]

    llm = ChatOpenAI(temperature=0, model_name=model_name)

    prompt_template = """
Write a concise summary of the following in not more than 10 sentences:


{text}


CONSCISE SUMMARY :
"""

    prompt = PromptTemplate(template=prompt_template, input_variables=["text"])

    num_tokens = num_tokens_from_string(doc_results, model_name)

    gpt_35_turbo_max_tokens = 4096
    verbose = False

    print("Summarizing search results retrieved from ClickHouse...")

    if num_tokens &amp;lt;= gpt_35_turbo_max_tokens:
        chain = load_summarize_chain(llm, chain_type="stuff", prompt=prompt, verbose=verbose)
    else:
        chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=prompt, combine_prompt=prompt, verbose=verbose)

    summary = chain.run(docs)

    print(f"Summary from chatgpt-3.5: {summary}")
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://clickhouse.com/docs/getting-started/example-datasets/hackernews-vector-search-dataset"/><published>2025-11-28T18:02:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46081188</id><title>Imgur geo-blocked the UK, so I geo-unblocked my network</title><updated>2025-11-29T09:36:34.318035+00:00</updated><content>&lt;doc fingerprint="b582e1f56725aa92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Imgur Geo-Blocked the UK, So I Geo-Unblocked My Entire Network&lt;/head&gt;
    &lt;p&gt;Imgur decided to block UK users. Honestly? I don’t really care that much. I haven’t actively browsed the site in years. But it used to be everywhere. Back when Reddit embedded everything on Imgur, maybe fifteen years ago, it was genuinely useful. Then Reddit built their own image hosting, Discord did the same, and Imgur slowly faded into the background.&lt;/p&gt;
    &lt;p&gt;Except it never fully disappeared. And since the block, I keep stumbling across Imgur links that just show “unavailable.” It’s mildly infuriating.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Minecraft shader problem#&lt;/head&gt;
    &lt;p&gt;Here’s a concrete example. I was playing Minecraft with some work colleagues and wanted to try different shaders. Most shader pages embed preview images hosted on Imgur. So I’d click through shader after shader, and every single preview was just gone. I couldn’t see what any of them looked like without the images.&lt;/p&gt;
    &lt;p&gt;This kind of thing happens constantly now. Old forum posts, Reddit threads, documentation pages, random project READMEs. Imgur links are still scattered across the internet, and in the UK, they’re all broken.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I didn’t just install a VPN#&lt;/head&gt;
    &lt;p&gt;The obvious solution is to use a VPN. Change your location, problem solved. But I have a few issues with that approach.&lt;/p&gt;
    &lt;p&gt;First, I just upgraded to 2.5 Gbps internet and I don’t want to route all my traffic through a VPN and take the speed hit. I have this bandwidth for a reason.&lt;/p&gt;
    &lt;p&gt;Second, even if I installed a VPN on my main machine, what about my phone? My laptop? My desktop? Every device would need the VPN running, and I’d have to remember to connect it before browsing. It’s messy.&lt;/p&gt;
    &lt;p&gt;I wanted something cleaner: a solution that works for every device on my network, automatically, without any client-side configuration.&lt;/p&gt;
    &lt;head rend="h2"&gt;The network-level approach#&lt;/head&gt;
    &lt;p&gt;I already run a homelab with Traefik as my reverse proxy, Pi-hole for DNS, and everything declaratively configured with NixOS. If you’ve read my previous post on Docker containers with secrets, you’ll recognise the pattern.&lt;/p&gt;
    &lt;p&gt;The idea was simple: intercept all requests to &lt;code&gt;i.imgur.com&lt;/code&gt; at the DNS level, route them through a VPN-connected container, and serve the images back. Every device on my network automatically uses Pi-hole for DNS via DHCP, so this would be completely transparent.&lt;/p&gt;
    &lt;p&gt;Here’s the flow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Device requests &lt;code&gt;i.imgur.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Pi-hole returns my Traefik instance’s IP instead&lt;/item&gt;
      &lt;item&gt;Traefik sees the SNI hostname and routes to Gluetun&lt;/item&gt;
      &lt;item&gt;Gluetun tunnels the request through a VPN&lt;/item&gt;
      &lt;item&gt;Nginx (attached to Gluetun’s network) proxies to the real Imgur&lt;/item&gt;
      &lt;item&gt;Image comes back through the tunnel to the device&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why Nginx when I already have Traefik?#&lt;/head&gt;
    &lt;p&gt;Good question. Gluetun isn’t a reverse proxy. It’s a container that provides VPN connectivity to other containers attached to its network namespace. So I needed something inside Gluetun’s network to actually handle the proxying. Nginx was the simplest choice.&lt;/p&gt;
    &lt;p&gt;The Nginx config is minimal. It just does TCP passthrough with SNI:&lt;/p&gt;
    &lt;code&gt;user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
}

stream {
    resolver 127.0.0.1 valid=30s;
    resolver_timeout 5s;

    server {
        listen 443;
        ssl_preread on;
        proxy_pass i.imgur.com:443;
        proxy_connect_timeout 10s;
        proxy_timeout 60s;
    }
}
&lt;/code&gt;
    &lt;p&gt;This listens on port 443, reads the SNI header to confirm the destination, and passes the connection through to the real &lt;code&gt;i.imgur.com&lt;/code&gt;. The TLS handshake happens end-to-end; Nginx never sees the decrypted traffic.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Docker setup#&lt;/head&gt;
    &lt;p&gt;The compose file runs two containers. Gluetun handles the VPN connection, and Nginx attaches to Gluetun’s network:&lt;/p&gt;
    &lt;code&gt;version: '3.8'

services:
  gluetun:
    image: qmcgaw/gluetun:latest
    container_name: gluetun
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      - VPN_SERVICE_PROVIDER=${VPN_SERVICE_PROVIDER}
      - VPN_TYPE=wireguard
      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY}
      - SERVER_COUNTRIES=${SERVER_COUNTRIES}
      - FIREWALL=on
      - FIREWALL_INPUT_PORTS=443
      - FIREWALL_OUTBOUND_SUBNETS=10.0.0.0/8
      - DOT=on
      - DOT_PROVIDERS=cloudflare
      - HEALTH_VPN_DURATION_INITIAL=30s
    volumes:
      - ./gluetun:/gluetun
    restart: unless-stopped
    networks:
      - proxy
    healthcheck:
      test: ["CMD", "/gluetun-entrypoint", "healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  imgur-proxy:
    image: nginx:alpine
    container_name: imgur-proxy
    depends_on:
      gluetun:
        condition: service_healthy
    network_mode: "service:gluetun"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped

networks:
  proxy:
    external: true
&lt;/code&gt;
    &lt;p&gt;The key detail is &lt;code&gt;network_mode: "service:gluetun"&lt;/code&gt;. This makes Nginx share Gluetun’s network stack, so all its traffic automatically goes through the VPN tunnel.&lt;/p&gt;
    &lt;p&gt;I’m not going to mention which VPN provider I use. It’s one of the major ones with WireGuard support, but honestly I’m not thrilled with it. Use whatever you have.&lt;/p&gt;
    &lt;head rend="h2"&gt;Traefik routing#&lt;/head&gt;
    &lt;p&gt;The final piece is telling Traefik to route &lt;code&gt;i.imgur.com&lt;/code&gt; traffic to the Gluetun container. This uses TCP routing with TLS passthrough:&lt;/p&gt;
    &lt;code&gt;tcp:
  routers:
    imgur-router:
      rule: "HostSNI(`i.imgur.com`)"
      entryPoints:
        - https
      service: imgur-service
      tls:
        passthrough: true
  services:
    imgur-service:
      loadBalancer:
        servers:
          - address: "gluetun:443"
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;passthrough: true&lt;/code&gt; is important. It means Traefik doesn’t terminate TLS; it just inspects the SNI header and forwards the connection.&lt;/p&gt;
    &lt;head rend="h2"&gt;NixOS integration#&lt;/head&gt;
    &lt;p&gt;Following the same pattern from my Docker with secrets post, I created a systemd service that runs the compose stack with Agenix-managed secrets:&lt;/p&gt;
    &lt;code&gt;{ pkgs, config, ... }:
let
  docker-env = config.age.secrets.docker-imgur-proxy.path;
in
{
  systemd.services.imgur-proxy = {
    description = "Imgur Proxy with VPN";
    after = [
      "network.target"
      "docker.service"
      "docker-create-proxy-network.service"
    ];
    wants = [
      "docker.service"
      "docker-create-proxy-network.service"
    ];
    serviceConfig = {
      ExecStart = "${pkgs.docker}/bin/docker compose --env-file ${docker-env} -f docker-compose.yml up --force-recreate";
      ExecStop = "${pkgs.docker}/bin/docker compose -f docker-compose.yml down";
      WorkingDirectory = "/home/tymscar/dotfiles/apps/nixos/docker/imgur-proxy";
      Restart = "always";
    };
    wantedBy = [ "multi-user.target" ];
  };
}
&lt;/code&gt;
    &lt;p&gt;The VPN credentials are stored encrypted with Agenix, so my entire dotfiles repo stays public while keeping secrets safe.&lt;/p&gt;
    &lt;head rend="h2"&gt;The result#&lt;/head&gt;
    &lt;p&gt;Now when any device on my network requests an Imgur image, it works. My phone, my laptop, guest devices, everything. No VPN apps to install, no browser extensions, no manual configuration. Pi-hole intercepts the DNS, Traefik routes the connection, and Gluetun tunnels it through a non-UK exit point.&lt;/p&gt;
    &lt;p&gt;The latency increase is negligible for loading images, and it only affects Imgur traffic. Everything else still goes direct at full speed.&lt;/p&gt;
    &lt;p&gt;Is this overkill for viewing the occasional Imgur image? Probably. But it’s a clean solution that requires minimal ongoing maintenance, and it scratches the homelab itch. Plus I can finally see what those Minecraft shaders look like.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.tymscar.com/posts/imgurukproxy/"/><published>2025-11-28T18:15:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46081704</id><title>Effective harnesses for long-running agents</title><updated>2025-11-29T09:36:34.124472+00:00</updated><content>&lt;doc fingerprint="3d429ef368998aee"&gt;
  &lt;main&gt;
    &lt;p&gt;As AI agents become more capable, developers are increasingly asking them to take on complex tasks requiring work that spans hours, or even days. However, getting agents to make consistent progress across multiple context windows remains an open problem.&lt;/p&gt;
    &lt;p&gt;The core challenge of long-running agents is that they must work in discrete sessions, and each new session begins with no memory of what came before. Imagine a software project staffed by engineers working in shifts, where each new engineer arrives with no memory of what happened on the previous shift. Because context windows are limited, and because most complex projects cannot be completed within a single window, agents need a way to bridge the gap between coding sessions.&lt;/p&gt;
    &lt;p&gt;We developed a two-fold solution to enable the Claude Agent SDK to work effectively across many context windows: an initializer agent that sets up the environment on the first run, and a coding agent that is tasked with making incremental progress in every session, while leaving clear artifacts for the next session. You can find code examples in the accompanying quickstart.&lt;/p&gt;
    &lt;head rend="h2"&gt;The long-running agent problem&lt;/head&gt;
    &lt;p&gt;The Claude Agent SDK is a powerful, general-purpose agent harness adept at coding, as well as other tasks that require the model to use tools to gather context, plan, and execute. It has context management capabilities such as compaction, which enables an agent to work on a task without exhausting the context window. Theoretically, given this setup, it should be possible for an agent to continue to do useful work for an arbitrarily long time.&lt;/p&gt;
    &lt;p&gt;However, compaction isn’t sufficient. Out of the box, even a frontier coding model like Opus 4.5 running on the Claude Agent SDK in a loop across multiple context windows will fall short of building a production-quality web app if it’s only given a high-level prompt, such as “build a clone of claude.ai.”&lt;/p&gt;
    &lt;p&gt;Claude’s failures manifested in two patterns. First, the agent tended to try to do too much at once—essentially to attempt to one-shot the app. Often, this led to the model running out of context in the middle of its implementation, leaving the next session to start with a feature half-implemented and undocumented. The agent would then have to guess at what had happened, and spend substantial time trying to get the basic app working again. This happens even with compaction, which doesn’t always pass perfectly clear instructions to the next agent.&lt;/p&gt;
    &lt;p&gt;A second failure mode would often occur later in a project. After some features had already been built, a later agent instance would look around, see that progress had been made, and declare the job done.&lt;/p&gt;
    &lt;p&gt;This decomposes the problem into two parts. First, we need to set up an initial environment that lays the foundation for all the features that a given prompt requires, which sets up the agent to work step-by-step and feature-by-feature. Second, we should prompt each agent to make incremental progress towards its goal while also leaving the environment in a clean state at the end of a session. By “clean state” we mean the kind of code that would be appropriate for merging to a main branch: there are no major bugs, the code is orderly and well-documented, and in general, a developer could easily begin work on a new feature without first having to clean up an unrelated mess.&lt;/p&gt;
    &lt;p&gt;When experimenting internally, we addressed these problems using a two-part solution:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Initializer agent: The very first agent session uses a specialized prompt that asks the model to set up the initial environment: an &lt;code&gt;init.sh&lt;/code&gt;script, a claude-progress.txt file that keeps a log of what agents have done, and an initial git commit that shows what files were added.&lt;/item&gt;
      &lt;item&gt;Coding agent: Every subsequent session asks the model to make incremental progress, then leave structured updates.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key insight here was finding a way for agents to quickly understand the state of work when starting with a fresh context window, which is accomplished with the claude-progress.txt file alongside the git history. Inspiration for these practices came from knowing what effective software engineers do every day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Environment management&lt;/head&gt;
    &lt;p&gt;In the updated Claude 4 prompting guide, we shared some best practices for multi-context window workflows, including a harness structure that uses “a different prompt for the very first context window.” This “different prompt” requests that the initializer agent set up the environment with all the necessary context that future coding agents will need to work effectively. Here, we provide a deeper dive on some of the key components of such an environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feature list&lt;/head&gt;
    &lt;p&gt;To address the problem of the agent one-shotting an app or prematurely considering the project complete, we prompted the initializer agent to write a comprehensive file of feature requirements expanding on the user’s initial prompt. In the claude.ai clone example, this meant over 200 features, such as “a user can open a new chat, type in a query, press enter, and see an AI response.” These features were all initially marked as “failing” so that later coding agents would have a clear outline of what full functionality looked like.&lt;/p&gt;
    &lt;code&gt;{
    "category": "functional",
    "description": "New chat button creates a fresh conversation",
    "steps": [
      "Navigate to main interface",
      "Click the 'New Chat' button",
      "Verify a new conversation is created",
      "Check that chat area shows welcome state",
      "Verify conversation appears in sidebar"
    ],
    "passes": false
  }
&lt;/code&gt;
    &lt;p&gt;We prompt coding agents to edit this file only by changing the status of a passes field, and we use strongly-worded instructions like “It is unacceptable to remove or edit tests because this could lead to missing or buggy functionality.” After some experimentation, we landed on using JSON for this, as the model is less likely to inappropriately change or overwrite JSON files compared to Markdown files.&lt;/p&gt;
    &lt;head rend="h3"&gt;Incremental progress&lt;/head&gt;
    &lt;p&gt;Given this initial environment scaffolding, the next iteration of the coding agent was then asked to work on only one feature at a time. This incremental approach turned out to be critical to addressing the agent’s tendency to do too much at once.&lt;/p&gt;
    &lt;p&gt;Once working incrementally, it’s still essential that the model leaves the environment in a clean state after making a code change. In our experiments, we found that the best way to elicit this behavior was to ask the model to commit its progress to git with descriptive commit messages and to write summaries of its progress in a progress file. This allowed the model to use git to revert bad code changes and recover working states of the code base.&lt;/p&gt;
    &lt;p&gt;These approaches also increased efficiency, as they eliminated the need for an agent to have to guess at what had happened and spend its time trying to get the basic app working again.&lt;/p&gt;
    &lt;head rend="h3"&gt;Testing&lt;/head&gt;
    &lt;p&gt;One final major failure mode that we observed was Claude’s tendency to mark a feature as complete without proper testing. Absent explicit prompting, Claude tended to make code changes, and even do testing with unit tests or &lt;code&gt;curl&lt;/code&gt; commands against a development server, but would fail recognize that the feature didn’t work end-to-end.&lt;/p&gt;
    &lt;p&gt;In the case of building a web app, Claude mostly did well at verifying features end-to-end once explicitly prompted to use browser automation tools and do all testing as a human user would.&lt;/p&gt;
    &lt;p&gt;Providing Claude with these kinds of testing tools dramatically improved performance, as the agent was able to identify and fix bugs that weren’t obvious from the code alone.&lt;/p&gt;
    &lt;p&gt;Some issues remain, like limitations to Claude’s vision and to browser automation tools making it difficult to identify every kind of bug. For example, Claude can’t see browser-native alert modals through the Puppeteer MCP, and features relying on these modals tended to be buggier as a result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting up to speed&lt;/head&gt;
    &lt;p&gt;With all of the above in place, every coding agent is prompted to run through a series of steps to get its bearings, some quite basic but still helpful:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;pwd&lt;/code&gt;to see the directory you’re working in. You’ll only be able to edit files in this directory.&lt;/item&gt;
      &lt;item&gt;Read the git logs and progress files to get up to speed on what was recently worked on.&lt;/item&gt;
      &lt;item&gt;Read the features list file and choose the highest-priority feature that’s not yet done to work on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach saves Claude some tokens in every session since it doesn’t have to figure out how to test the code. It also helps to ask the initializer agent to write an init.sh script that can run the development server, and then run through a basic end-to-end test before implementing a new feature.&lt;/p&gt;
    &lt;p&gt;In the case of the claude.ai clone, this meant that the agent always started the local development server and used the Puppeteer MCP to start a new chat, send a message, and receive a response. This ensured that Claude could quickly identify if the app had been left in a broken state, and immediately fix any existing bugs. If the agent had instead started implementing a new feature, it would likely make the problem worse.&lt;/p&gt;
    &lt;p&gt;Given all this, a typical session starts off with the following assistant messages:&lt;/p&gt;
    &lt;code&gt;[Assistant] I'll start by getting my bearings and understanding the current state of the project.
[Tool Use] &amp;lt;bash - pwd&amp;gt;
[Tool Use] &amp;lt;read - claude-progress.txt&amp;gt;
[Tool Use] &amp;lt;read - feature_list.json&amp;gt;
[Assistant] Let me check the git log to see recent work.
[Tool Use] &amp;lt;bash - git log --oneline -20&amp;gt;
[Assistant] Now let me check if there's an init.sh script to restart the servers.
&amp;lt;Starts the development server&amp;gt;
[Assistant] Excellent! Now let me navigate to the application and verify that some fundamental features are still working.
&amp;lt;Tests basic functionality&amp;gt;
[Assistant] Based on my verification testing, I can see that the fundamental functionality is working well. The core chat features, theme switching, conversation loading, and error handling are all functioning correctly. Now let me review the tests.json file more comprehensively to understand what needs to be implemented next.
&amp;lt;Starts work on a new feature&amp;gt;&lt;/code&gt;
    &lt;p&gt;Agent failure modes and solutions&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Problem&lt;/cell&gt;
        &lt;cell role="head"&gt;Initializer Agent Behavior&lt;/cell&gt;
        &lt;cell role="head"&gt;Coding Agent Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude declares victory on the entire project too early.&lt;/cell&gt;
        &lt;cell&gt;Set up a feature list file: based on the input spec, set up a structured JSON file with a list of end-to-end feature descriptions.&lt;/cell&gt;
        &lt;cell&gt;Read the feature list file at the beginning of a session. Choose a single feature to start working on.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude leaves the environment in a state with bugs or undocumented progress.&lt;/cell&gt;
        &lt;cell&gt;An initial git repo and progress notes file is written.&lt;/cell&gt;
        &lt;cell&gt;Start the session by reading the progress notes file and git commit logs, and run a basic test on the development server to catch any undocumented bugs. End the session by writing a git commit and progress update.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude marks features as done prematurely.&lt;/cell&gt;
        &lt;cell&gt;Set up a feature list file.&lt;/cell&gt;
        &lt;cell&gt;Self-verify all features. Only mark features as “passing” after careful testing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Claude has to spend time figuring out how to run the app.&lt;/cell&gt;
        &lt;cell&gt;Write an &lt;code&gt;init.sh&lt;/code&gt; script that can run the development server.&lt;/cell&gt;
        &lt;cell&gt;Start the session by reading &lt;code&gt;init.sh&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;This research demonstrates one possible set of solutions in a long-running agent harness to enable the model to make incremental progress across many context windows. However, there remain open questions.&lt;/p&gt;
    &lt;p&gt;Most notably, it’s still unclear whether a single, general-purpose coding agent performs best across contexts, or if better performance can be achieved through a multi-agent architecture. It seems reasonable that specialized agents like a testing agent, a quality assurance agent, or a code cleanup agent, could do an even better job at sub-tasks across the software development lifecycle.&lt;/p&gt;
    &lt;p&gt;Additionally, this demo is optimized for full-stack web app development. A future direction is to generalize these findings to other fields. It’s likely that some or all of these lessons can be applied to the types of long-running agentic tasks required in, for example, scientific research or financial modeling.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Written by Justin Young. Special thanks to David Hershey, Prithvi Rajasakeran, Jeremy Hadfield, Naia Bouscal, Michael Tingley, Jesse Mu, Jake Eaton, Marius Buleandara, Maggie Vo, Pedram Navid, Nadine Yasser, and Alex Notov for their contributions.&lt;/p&gt;
    &lt;p&gt;This work reflects the collective efforts of several teams across Anthropic who made it possible for Claude to safely do long-horizon autonomous software engineering, especially the code RL &amp;amp; Claude Code teams. Interested candidates who would like to contribute are welcome to apply at anthropic.com/careers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;1. We refer to these as separate agents in this context only because they have different initial user prompts. The system prompt, set of tools, and overall agent harness was otherwise identical.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"/><published>2025-11-28T19:05:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46082204</id><title>The original ABC language, Python's predecessor (1991)</title><updated>2025-11-29T09:36:33.545068+00:00</updated><content>&lt;doc fingerprint="2fe8520fe9cb456a"&gt;
  &lt;main&gt;
    &lt;p&gt;ABC was Python's most direct predecessor; I worked on it from around 1983-1986.&lt;/p&gt;
    &lt;p&gt;I downloaded these sources from cwi.nl, specifically the abc-unix tarball.&lt;/p&gt;
    &lt;p&gt;Another copy of the ABC sources lives in Luciano Ramalho's GitHub. I hope one day to compare the two trees and unify them.&lt;/p&gt;
    &lt;p&gt;Most files have 1991 as their latest modification time in the tar ball; a few have 1996 or 2021.&lt;/p&gt;
    &lt;p&gt;The old README file has build instructions.&lt;/p&gt;
    &lt;p&gt;The current sources assume a 32-bit system where int and pointers have the same size. I hope to eventually upgrade the source code to work on 64-bit systems too (where int is 32 bits and pointers are 64 bits).&lt;/p&gt;
    &lt;p&gt;CWI never put a license on ABC, but it says:&lt;/p&gt;
    &lt;p&gt;Copyright (c) Stichting Mathematisch Centrum, Amsterdam, 1988-2011.&lt;/p&gt;
    &lt;p&gt;I'll try to negotiate with Steven Pemberton eventually (hopefully MIT).&lt;/p&gt;
    &lt;p&gt;Eddy Boeve, Frank van Dijk, Leo Geurts, Timo Krijnen, Lambert Meertens, Steven Pemberton, Guido van Rossum.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leo Geurts, Lambert Meertens and Steven Pemberton, The ABC Programmer's Handbook, Prentice-Hall, Englewood Cliffs, New Jersey, 1990, ISBN 0-13- 000027-2.&lt;/item&gt;
      &lt;item&gt;Steven Pemberton, An Alternative Simple Language and Environment for PCs, IEEE Software, Vol. 4, No. 1, January 1987, pp. 56-64. http://www.cwi.nl/~steven/abc.html&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Steven Pemberton's home page. https://homepages.cwi.nl/~steven/abc/&lt;/item&gt;
      &lt;item&gt;Lambert Meertens, The Origins of Python. https://inference-review.com/article/the-origins-of-python&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/gvanrossum/abc-unix"/><published>2025-11-28T19:58:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46082223</id><title>How good engineers write bad code at big companies</title><updated>2025-11-29T09:36:33.460575+00:00</updated><content>&lt;doc fingerprint="bf4dd1049e34635e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How good engineers write bad code at big companies&lt;/head&gt;
    &lt;p&gt;Every couple of years somebody notices that large tech companies sometimes produce surprisingly sloppy code. If you haven’t worked at a big company, it might be hard to understand how this happens. Big tech companies pay well enough to attract many competent engineers. They move slowly enough that it looks like they’re able to take their time and do solid work. How does bad code happen?&lt;/p&gt;
    &lt;head rend="h3"&gt;Most code changes are made by relative beginners&lt;/head&gt;
    &lt;p&gt;I think the main reason is that big companies are full of engineers working outside their area of expertise. The average big tech employee stays for only a year or two1. In fact, big tech compensation packages are typically designed to put a four-year cap on engineer tenure: after four years, the initial share grant is fully vested, causing engineers to take what can be a 50% pay cut. Companies do extend temporary yearly refreshes, but it obviously incentivizes engineers to go find another job where they don’t have to wonder if they’re going to get the other half of their compensation each year.&lt;/p&gt;
    &lt;p&gt;If you count internal mobility, it’s even worse. The longest I have ever stayed on a single team or codebase was three years, near the start of my career. I expect to be re-orged at least every year, and often much more frequently.&lt;/p&gt;
    &lt;p&gt;However, the average tenure of a codebase in a big tech company is a lot longer than that. Many of the services I work on are a decade old or more, and have had many, many different owners over the years. That means many big tech engineers are constantly “figuring it out”. A pretty high percentage of code changes are made by “beginners”: people who have onboarded to the company, the codebase, or even the programming language in the past six months.&lt;/p&gt;
    &lt;head rend="h3"&gt;Old hands&lt;/head&gt;
    &lt;p&gt;To some extent, this problem is mitigated by “old hands”: engineers who happen to have been in the orbit of a particular system for long enough to develop real expertise. These engineers can give deep code reviews and reliably catch obvious problems. But relying on “old hands” has two problems.&lt;/p&gt;
    &lt;p&gt;First, this process is entirely informal. Big tech companies make surprisingly little effort to develop long-term expertise in individual systems, and once they’ve got it they seem to barely care at all about retaining it. Often the engineers in question are moved to different services, and have to either keep up their “old hand” duties on an effectively volunteer basis, or abandon them and become a relative beginner on a brand new system.&lt;/p&gt;
    &lt;p&gt;Second, experienced engineers are always overloaded. It is a busy job being one of the few engineers who has deep expertise on a particular service. You don’t have enough time to personally review every software change, or to be actively involved in every decision-making process. Remember that you also have your own work to do: if you spend all your time reviewing changes and being involved in discussions, you’ll likely be punished by the company for not having enough individual output.&lt;/p&gt;
    &lt;head rend="h3"&gt;The median productive engineer&lt;/head&gt;
    &lt;p&gt;Putting all this together, what does the median productive2 engineer at a big tech company look like? They are usually:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;competent enough to pass the hiring bar and be able to do the work, but either&lt;/item&gt;
      &lt;item&gt;working on a codebase or language that is largely new to them, or&lt;/item&gt;
      &lt;item&gt;trying to stay on top of a flood of code changes while also juggling their own work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They are almost certainly working to a deadline, or to a series of overlapping deadlines for different projects. In other words, they are trying to do their best in an environment that is not set up to produce quality code.&lt;/p&gt;
    &lt;p&gt;That’s how “obviously” bad code happens. For instance, a junior engineer picks up a ticket for an annoying bug in a codebase they’re barely familiar with. They spend a few days figuring it out and come up with a hacky solution. One of the more senior “old hands” (if they’re lucky) glances over it in a spare half-hour, vetoes it, and suggests something slightly better that would at least work. The junior engineer implements that as best they can, tests that it works, it gets briefly reviewed and shipped, and everyone involved immediately moves on to higher-priority work. Five years later somebody notices this3 and thinks “wow, that’s hacky - how did such bad code get written at such a big software company”?&lt;/p&gt;
    &lt;head rend="h3"&gt;Big tech companies are fine with this&lt;/head&gt;
    &lt;p&gt;I have written a lot about the internal tech company dynamics that contribute to this. Most directly, in Seeing like a software company I argue that big tech companies consistently prioritize internal legibility - the ability to see at a glance who’s working on what and to change it at will - over productivity. Big companies know that treating engineers as fungible and moving them around destroys their ability to develop long-term expertise in a single codebase. That’s a deliberate tradeoff. They’re giving up some amount of expertise and software quality in order to gain the ability to rapidly deploy skilled engineers onto whatever the problem-of-the-month is.&lt;/p&gt;
    &lt;p&gt;I don’t know if this is a good idea or a bad idea. It certainly seems to be working for the big tech companies, particularly now that “how fast can you pivot to something AI-related” is so important. But if you’re doing this, then of course you’re going to produce some genuinely bad code. That’s what happens when you ask engineers to rush out work on systems they’re unfamiliar with.&lt;/p&gt;
    &lt;p&gt;Individual engineers are entirely powerless to alter this dynamic. This is particularly true in 2025, when the balance of power has tilted away from engineers and towards tech company leadership. The most you can do as an individual engineer is to try and become an “old hand”: to develop expertise in at least one area, and to use it to block the worst changes and steer people towards at least minimally-sensible technical decisions. But even that is often swimming against the current of the organization, and if inexpertly done can cause you to get PIP-ed or worse.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pure and impure engineering&lt;/head&gt;
    &lt;p&gt;I think a lot of this comes down to the distinction between pure and impure software engineering. To pure engineers - engineers working on self-contained technical projects, like a programming language - the only explanation for bad code is incompetence. But impure engineers operate more like plumbers or electricians. They’re working to deadlines on projects that are relatively new to them, and even if their technical fundamentals are impeccable, there’s always something about the particular setup of this situation that’s awkward or surprising. To impure engineers, bad code is inevitable. As long as the overall system works well enough, the project is a success.&lt;/p&gt;
    &lt;p&gt;At big tech companies, engineers don’t get to decide if they’re working on pure or impure engineering work. It’s not their codebase! If the company wants to move you from working on database infrastructure to building the new payments system, they’re fully entitled to do that. The fact that you might make some mistakes in an unfamiliar system - or that your old colleagues on the database infra team might suffer without your expertise - is a deliberate tradeoff being made by the company, not the engineer.&lt;/p&gt;
    &lt;p&gt;It’s fine to point out examples of bad code at big companies. If nothing else, it can be an effective way to get those specific examples fixed, since execs usually jump at the chance to turn bad PR into good PR. But I think it’s a mistake4 to attribute primary responsibility to the engineers at those companies. If you could wave a magic wand and make every engineer twice as strong, you would still have bad code, because almost nobody can come into a brand new codebase and quickly make changes with zero mistakes. The root cause is that most big company engineers are forced to do most of their work in unfamiliar codebases.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;I struggled to find a good original source on this. There’s a 2013 PayScale report citing a 1.1 year median turnover at Google, which seems low.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Many engineers at big tech companies are not productive, but that’s a post all to itself. I don’t want to get into it here for two reasons. First, I think competent engineers produce enough bad code that it’s fine to be a bit generous and just scope the discussion to them. Second, even if an incompetent engineer wrote the code, there’s almost always competent engineers who could have reviewed it, and the question of why that didn’t happen is still interesting.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The example I’m thinking of here is not the recent GitHub Actions one, which I have no first-hand experience of. I can think of at least ten separate instances of this happening to me.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In my view, mainly a failure of imagination: thinking that your own work environment must be pretty similar to everyone else’s.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News. Here's a preview of a related post that shares tags with this one.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;How I influence tech company politics as a staff software engineer&lt;/p&gt;&lt;p&gt;Many software engineers are fatalistic about company politics. They believe that it’s pointless to get involved, because:&lt;/p&gt;&lt;p&gt;The general idea here is that software engineers are simply not equipped to play the game at the same level as real political operators. This is true! It would be a terrible mistake for a software engineer to think that you ought to start scheming and plotting like you’re in Game of Thrones. Your schemes will be immediately uncovered and repurposed to your disadvantage and other people’s gain. Scheming takes practice and power, and neither of those things are available to software engineers.&lt;/p&gt;&lt;lb/&gt;Continue reading...&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/bad-code-at-big-companies/"/><published>2025-11-28T19:59:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46082291</id><title>Electron vs. Tauri</title><updated>2025-11-29T09:36:33.295572+00:00</updated><content>&lt;doc fingerprint="a50a94511423a794"&gt;
  &lt;main&gt;
    &lt;p&gt;The Dolt Workbench is an open-source SQL workbench supporting MySQL, Postgres, Dolt, and Doltgres databases. We built the workbench using Electron, which is a popular framework that allows you to convert web apps built with traditional web technologies like HTML, CSS, and Javascript to desktop applications. Since the workbench shares much in common with DoltHub and Hosted Dolt, the architecture is very similar to those products. That is, the workbench uses Next.js for the frontend with an additional GraphQL layer that handles database interactions. For this reason, it made a lot of sense to use Electron to get the desktop version of our application up and running.&lt;/p&gt;
    &lt;p&gt;That said, Electron comes with a few rather significant drawbacks, and those drawbacks have started to become more apparent as the workbench has matured. Because of this, I spent some time exploring Tauri, a newer framework that supports the same web-to-desktop use case as Electron. In this article, weâll discuss how well Electron and Tauri integrate with the workbench, and weigh some pros and cons between the two frameworks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next.js Support#&lt;/head&gt;
    &lt;p&gt;Next.js doesnât translate very cleanly to a desktop application context. This is primarily due to the frameworkâs architecture around server-side rendering and API routing features. In a desktop app, thereâs no application server interacting with a client; we just need to render HTML, CSS, and JavaScript in a window. For these reasons, Electron only loosely supports Next.js applications. Thatâs not to say you canât build an Electron app with Next.js, but it requires some workarounds to make it function properly. One of the more popular workarounds is a project called Nextron, which aims to wire Next.js applications to the Electron framework and streamline the build process. This is the project we use for the workbench. The issue is that, at the time of writing, it appears that Nextron is no longer being maintained, and we started hitting a few bugs with it.&lt;/p&gt;
    &lt;p&gt;Tauri is largely frontend-framework agnostic. For Next, specifically, you still canât use the server-side features, but Tauri makes the integration process much simpler by relying on Nextâs static-site generation capabilities. To make a Next app work with Tauri, you just need to set &lt;code&gt;output: 'export'&lt;/code&gt; in your Next configuration file, and Tauri handles the rest.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Webview#&lt;/head&gt;
    &lt;p&gt;The biggest difference between Electron and Tauri comes from how they render the UI. The Electron framework comes with a full Chromium browser engine bundled in your application, which is the same engine that backs Google Chrome. This is useful because it means you donât have to worry about browser compatibility issues. Regardless of the end userâs machine or architecture, the same Chromium instance renders your application UI. This results in a very standardized experience that ensures your app will look the same regardless of where itâs running. However, this also results in a fair amount of bloat. For the vast majority of desktop apps, a full Chromium browser engine is overkill. Even the simplest âHello Worldâ applications using Electron can run you up to 150 megabytes of disk space.&lt;/p&gt;
    &lt;p&gt;Tauri solves this problem by leveraging the systemâs native webview. Instead of bundling a full browser engine, Tauri uses a library called WRY, which provides a cross-platform interface to the appropriate webview for the operating system. As youâd expect, this makes Tauri apps far more lightweight. The downside here is that you no longer have a hard guarantee on compatibility. From what I can tell, however, this mostly seems to be a non-issue. Compatibility issues across system webviews are exceedingly rare, especially for the major operating systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Node.js vs. Rust#&lt;/head&gt;
    &lt;p&gt;Another major difference between the two frameworks is how they handle the âmainâ process. This refers to the backend process that orchestrates the application windows, menus, and other components of a desktop app that require interaction with system APIs. In Electron, the main process runs in a Node.js environment. This means you get access to all the typical Node APIs, you can import things like normal, and, perhaps most importantly, you can write your Electron-specific code in pure JavaScript. This is a huge bonus for Electronâs target audience: web developers.&lt;/p&gt;
    &lt;p&gt;Tauri, on the other hand, uses Rust. All the framework code and the main process entrypoint are written in Rust. Obviously, this makes it a bit less accessible to the average web developer. That said, Tauri provides a fairly robust set of JavaScript APIs to interact with the Rust layer. For most applications, these APIs will be sufficient to do what you need to do. In the case of the workbench, I was able to fully replicate the functionality of the Electron version using the JavaScript APIs and some minimal Rust code.&lt;/p&gt;
    &lt;p&gt;In my experience, I found the Tauri APIs to fit more naturally in our application code. With Electron, if you need the main process to do something, you must always use inter-process communication, even for the simplest of tasks. If you want to write to a file on the host machine, for instance, your frontend needs to send a signal to the Electron main process, which will then spawn a new process and run the function you wrote that performs the write. With Tauri, you can just use Tauriâs filesystem API directly in your application code. Under the hood, the same sort of IPC pattern is happening, but I think the Tauri abstraction is a bit nicer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sidecars#&lt;/head&gt;
    &lt;p&gt;Since Electron runs on Node.js, it also bundles a full Node.js runtime with your application. This comes with some pros and cons. For the workbench, specifically, this is beneficial because the GraphQL layer is itself a separate Node.js application that needs to run alongside the frontend. Since Electron ships with Node.js, this means we can directly spin up the GraphQL server from the Electron main process using the Node runtime. This eliminates a lot of the headache associated with bundling and running a typical sidecar process. For instance, our app also ships with a copy of Dolt, which allows users to start up local Dolt servers directly from the workbench. To make this work, we have to bundle the appropriate Dolt binary with each workbench release that corresponds to the correct architecture. Without the Node runtime, weâd have to do something similar for the GraphQL layer.&lt;/p&gt;
    &lt;p&gt;With Tauri, this is exactly the problem we run into. To get around it, we need to compile the GraphQL server into a binary using a tool like &lt;code&gt;pkg&lt;/code&gt;, then run it as a sidecar the same way we run Dolt. Thankfully, this seems to be a fairly common use case for Tauri applications, and they have a useful guide on how to run Node.js apps as a sidecar.&lt;/p&gt;
    &lt;p&gt;Itâs also worth mentioning that the full Node.js runtime is quite heavy, which also contributes to bloated Electron app sizes. After building the workbench using both Electron and Tauri, the difference in size was substantial. The left is the Electron version and the right is Tauri:&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations#&lt;/head&gt;
    &lt;p&gt;After replicating the workbenchâs functionality in Tauri, weâre holding off on making the full transition for a couple reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lack of support for .appx and .msix bundles on Windows - Currently, Tauri only support .exe and .msi bundles on Windows. This means your Microsoft Store entry will only link to the unpacked application. The workbench is currently bundled and published using the .appx format. To address this, we would need to take down the workbench entirely from the Microsoft store and create a new application that uses the .exe format.&lt;/item&gt;
      &lt;item&gt;Issues with MacOS universal binaries - This is more an annoyance than a bug, but I ran into a few issues related to codesigning universal binaries for MacOS. Namely, Tauri doesnât seem to be able to create Mac universal binaries from their arm64 and x64 subcomponents. It also seems to be codesigning the Mac builds twice.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Neither of these are hard blockers, but theyâre annoying enough that Iâm holding off on migrating until theyâre resolved or our issues with Nextron become more problematic. For now, Iâm leaving my branch with the migration open and hope to revisit soon. If youâre on the Tauri team, let us know if you have solutions!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;Overall, Iâm impressed with Tauri. It eliminates much of the classic Electron bloat and integrates naturally with our existing codebase. If youâre curious about Tauri or the Dolt Workbench, let us know on Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dolthub.com/blog/2025-11-13-electron-vs-tauri/"/><published>2025-11-28T20:06:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46082825</id><title>Flight disruption warning as Airbus requests modifications to 6k planes</title><updated>2025-11-29T09:36:33.104742+00:00</updated><content>&lt;doc fingerprint="7d1bddd78b37b35b"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;What we know about Airbus disruption so farpublished at 02:34 GMT&lt;/head&gt;
    &lt;p&gt;The discovery that computers installed on one of the most widely operated passenger planes could be vulnerable to interference from the sun has led to mass cancellations. Here is what we know.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;European aerospace giant Airbus has warned flights will be disrupted after it said it must carry out immediate software updates to thousands of its planes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Over 6,000 planes, mainly the A320 model, are thought to be affected&lt;/item&gt;
      &lt;item&gt;The disruption has fallen on a major holiday weekend in the US, which is home to four of the biggest A320 model operators: American Airlines, Delta Air Lines, JetBlue and United Airlines&lt;/item&gt;
      &lt;item&gt;The issue was discovered after a JetBlue aircraft en-route from Mexico to the United States experienced a "sudden drop in altitude" in October&lt;/item&gt;
      &lt;item&gt;Itâs thought the incident was caused by interference from intense solar radiation, which corrupted data in a computer which controls the aircraft's elevation&lt;/item&gt;
      &lt;item&gt;Disruption at UK airports has been fairly limited so far, though several airlines around the world have reported cancellations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are pausing our live coverage for now, but you can read our latest updates on the story here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/live/cvg4y6g74ert"/><published>2025-11-28T21:16:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46083004</id><title>Airbus A320 – intense solar radiation may corrupt data critical for flight</title><updated>2025-11-29T09:36:33.012760+00:00</updated><content>&lt;doc fingerprint="359e27eeea79718f"&gt;
  &lt;main&gt;
    &lt;p&gt;Toulouse, France, 28 November 2025 – Analysis of a recent event involving an A320 Family aircraft has revealed that intense solar radiation may corrupt data critical to the functioning of flight controls.&lt;/p&gt;
    &lt;p&gt;Airbus has consequently identified a significant number of A320 Family aircraft currently in-service which may be impacted.&lt;/p&gt;
    &lt;p&gt;Airbus has worked proactively with the aviation authorities to request immediate precautionary action from operators via an Alert Operators Transmission (AOT) in order to implement the available software and/or hardware protection, and ensure the fleet is safe to fly. This AOT will be reflected in an Emergency Airworthiness Directive from the European Union Aviation Safety Agency (EASA).&lt;/p&gt;
    &lt;p&gt;Airbus acknowledges these recommendations will lead to operational disruptions to passengers and customers. We apologise for the inconvenience caused and will work closely with operators, while keeping safety as our number one and overriding priority.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contacts&lt;/head&gt;
    &lt;head rend="h3"&gt;Guillaume Steuer&lt;/head&gt;
    &lt;p&gt;AIRBUS&lt;/p&gt;
    &lt;head rend="h3"&gt;Sara Ricci&lt;/head&gt;
    &lt;p&gt;AIRBUS | Commercial Aircraft&lt;/p&gt;
    &lt;head rend="h3"&gt;Justin Dubon&lt;/head&gt;
    &lt;p&gt;AIRBUS | Commercial Aircraft&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.airbus.com/en/newsroom/press-releases/2025-11-airbus-update-on-a320-family-precautionary-fleet-action"/><published>2025-11-28T21:40:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46083051</id><title>A first look at Django's new background tasks</title><updated>2025-11-29T09:36:31.948499+00:00</updated><content>&lt;doc fingerprint="7f08801f19f010ee"&gt;
  &lt;main&gt;
    &lt;p&gt;Django 6.0 introduces a built-in background tasks framework in &lt;code&gt;django.tasks&lt;/code&gt;. But don't expect to phase out Celery, Huey or other preferred solutions just yet. &lt;/p&gt;
    &lt;p&gt;The release notes are quite clear on this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Django handles task creation and queuing, but does not provide a worker mechanism to run tasks. Execution must be managed by external infrastructure, such as a separate process or service.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The main purpose of the new &lt;code&gt;django.tasks&lt;/code&gt; module is to provide a common API for task queues implementations. Jake Howard is the driving force behind this enhancement. Check out the introduction on the Django forum.&lt;/p&gt;
    &lt;p&gt;His reference implementation, and simultaneously a backport for earlier versions of Django, is available as &lt;code&gt;django-tasks&lt;/code&gt; on GitHub.&lt;/p&gt;
    &lt;p&gt;But let's ignore that and play with the more minimal version included in Django 6.0 instead. By creating our very own backend and worker.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our project: notifications&lt;/head&gt;
    &lt;p&gt;We're going to create an app to send notifications to phones and other devices using ntfy.sh. (I'm a fan!)&lt;/p&gt;
    &lt;p&gt;If you prefer to dive into the code yourself, check out the final version of the project on GitHub.&lt;/p&gt;
    &lt;p&gt;All that's required to send a notification to your phone using nfty is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Register for an account&lt;/item&gt;
      &lt;item&gt;Create a topic.&lt;/item&gt;
      &lt;item&gt;Install the app for your phone and log in.&lt;/item&gt;
      &lt;item&gt;Send HTTP requests to &lt;code&gt;https://ntfy.sh/&amp;lt;yourtopic&amp;gt;&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The free version only provides public topics and messages. Meaning anyone can see the stuff you're sending if they subscribe to the topic. For our purpose we can simply create a topic with a randomized name, like a UUID.&lt;/p&gt;
    &lt;p&gt;The project's settings expect the URL from step 4 to be supplied as an environment variable. For example:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;NTFY_URL=https://ntfy.sh/062519693d9c4913826f0a39aeea8a4c&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Here's our function that does the heavy lifting:&lt;/p&gt;
    &lt;code&gt;import httpx
from django.conf import settings

def send_notification(message: str, title: str | None):
    # Pass the title if specified.
    headers = {"title": title} if title else {}
    httpx.post(
        settings.NTFY_URL,
        content=message,
        headers=headers,
    )
&lt;/code&gt;
    &lt;p&gt;Really. That's all there is to it to start sending and receiving notifications.&lt;/p&gt;
    &lt;head rend="h2"&gt;A quick primer&lt;/head&gt;
    &lt;p&gt;You really should have a look at the Django documentation on the Task framework for details, but we'll save you a bit of time and give a quick primer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Defining a task&lt;/head&gt;
    &lt;p&gt;This is the main goal of the new framework: defining tasks using Django's standard API, rather than using task queue specific decorators, or other methods.&lt;/p&gt;
    &lt;p&gt;So here it goes:&lt;/p&gt;
    &lt;code&gt;# ...
from django.tasks import task

@task
def send_notification(message: str, title: str | None):
    # ...as before
&lt;/code&gt;
    &lt;p&gt;Our function is now a task. In fact it's a &lt;code&gt;django.tasks.Task&lt;/code&gt;. &lt;/p&gt;
    &lt;p&gt;You cannot call &lt;code&gt;send_notification&lt;/code&gt; directly anymore. Tasks can only be run by using the &lt;code&gt;enqueue&lt;/code&gt; method. It might not be the behavior you'd expect or want, but this seems to be the best option. This design eliminates the possibility of accidentally invoking a task in-process, rather than in the background.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;task&lt;/code&gt; decorator allows you to specify the task's priority, queue name and backend name. You can override these settings with the &lt;code&gt;using&lt;/code&gt; method, which returns a new &lt;code&gt;django.tasks.Task&lt;/code&gt; instance.&lt;/p&gt;
    &lt;p&gt;If you need more control over task behavior, you can set &lt;code&gt;takes_context&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; in the decorator and add &lt;code&gt;context&lt;/code&gt; as the first argument. This context currently provides you with access to the task result and thereby useful information like the number of attempts. &lt;/p&gt;
    &lt;p&gt;There's no way of defining retries and backoffs, or other fancy things you might expect from a full-blown task queue implementation. But that's not what this is. You can easily add your own retry logic by inspecting the task context if needed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enqueuing a task&lt;/head&gt;
    &lt;p&gt;Adding a task to the queue is easy:&lt;/p&gt;
    &lt;code&gt;task_result = send_notification.enqueue(
    message="Season's greeting!", 
    title="Santa has something to tell you"
)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Executing a task&lt;/head&gt;
    &lt;p&gt;This is where things start to fall short. At least right now. Django 6.0 will ship with the &lt;code&gt;ImmediateBackend&lt;/code&gt; and the &lt;code&gt;DummyBackend&lt;/code&gt;. The first will execute the task immediately, while the latter will not execute the task at all. &lt;/p&gt;
    &lt;p&gt;Which is why our project includes a (demo) backend backed by the database and a worker process!&lt;/p&gt;
    &lt;head rend="h3"&gt;Fetching the result&lt;/head&gt;
    &lt;p&gt;If you're not going to wait around for the result, you can get a hold of it later on using its id. Simply call &lt;code&gt;get_result(result_id)&lt;/code&gt; on your task. &lt;/p&gt;
    &lt;p&gt;Our project includes a view that's polled periodically for outstanding results using htmx.&lt;/p&gt;
    &lt;p&gt;The list underneath the form shows the results for each execution of our task. When the form's submitted, a new result is added to the top of the list. Htmx is instructed to keep polling for changes as long as the result's status isn't &lt;code&gt;FAILED&lt;/code&gt; or &lt;code&gt;SUCCESSFUL&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;def task_result(request, result_id, status):
    result = send_notification.get_result(result_id)
    if result.status == status:
        # No need to swap the result.
        return HttpResponse(status=204)
    return TemplateResponse(request, "index.html#result", {"result": result})
&lt;/code&gt;
    &lt;p&gt;Wondering what &lt;code&gt;index.html#results&lt;/code&gt; is doing? Django 6.0 also introduces template partials. In this case our view effectively sends a response containing only the template partial named &lt;code&gt;result&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Behind the scenes&lt;/head&gt;
    &lt;p&gt;When you decorate a callable with &lt;code&gt;task&lt;/code&gt;, the configured backend's &lt;code&gt;task_class&lt;/code&gt; is used to wrap the callable. The default's &lt;code&gt;django.task.Task&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;That class's &lt;code&gt;enqueue&lt;/code&gt; method will in turn invoke the configured backend's &lt;code&gt;enqueue&lt;/code&gt; method.&lt;/p&gt;
    &lt;p&gt;Calling its &lt;code&gt;get_result&lt;/code&gt; method is similar: call the configured backend's &lt;code&gt;get_result&lt;/code&gt; method and pass on the result.&lt;/p&gt;
    &lt;p&gt;Since there's no workers, that's basically all a task backend needs to provide. Cool. Let's add one, shall we?&lt;/p&gt;
    &lt;head rend="h2"&gt;A task database backend&lt;/head&gt;
    &lt;p&gt;Our goals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A basic task backend, backed by our database.&lt;/item&gt;
      &lt;item&gt;We want to support "automagic" retries&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our &lt;code&gt;enqueue&lt;/code&gt; and &lt;code&gt;get_result&lt;/code&gt; methods will return an instance of the default &lt;code&gt;django.tasks.TaskResult&lt;/code&gt;. This determines the minimum amount of data we need to store, and we're going to do so in a model called &lt;code&gt;Task&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Models&lt;/head&gt;
    &lt;p&gt;Let's create a first draft of our &lt;code&gt;Task&lt;/code&gt; model, based on the properties of &lt;code&gt;TaskResult&lt;/code&gt; and &lt;code&gt;Task&lt;/code&gt; in &lt;code&gt;django.tasks&lt;/code&gt; (the "dataclasses"):&lt;/p&gt;
    &lt;code&gt;class Task(models.Model):
    priority = models.IntegerField(default=0)
    callable_path = models.CharField(max_length=255)
    backend = models.CharField(max_length=200)
    queue_name = models.CharField(max_length=100)
    run_after = models.DateTimeField(null=True, blank=True)
    takes_context = models.BooleanField(default=False)
    # Stores args and kwargs
    arguments = models.JSONField(null=True, blank=True)
    status = models.CharField(
        choices=TaskResultStatus.choices, max_length=10, default=TaskResultStatus.READY
    )
    enqueued_at = models.DateTimeField()
    started_at = models.DateTimeField(blank=True, null=True)
    finished_at = models.DateTimeField(blank=True, null=True)
    last_attempted_at = models.DateTimeField(blank=True, null=True)
    return_value = models.JSONField(null=True, blank=True)
&lt;/code&gt;
    &lt;p&gt;What's missing? For one, the &lt;code&gt;TaskResult&lt;/code&gt; also includes a list of encountered errors, and ids of the workers that processed the task. Something that we could perhaps ignore. &lt;/p&gt;
    &lt;p&gt;Except the &lt;code&gt;TaskResult.attempts&lt;/code&gt; property is based on the number of worker ids. And if you're using the task context within a task, you're bound to be relying on that type of information.&lt;/p&gt;
    &lt;p&gt;We could add these details to the &lt;code&gt;Task&lt;/code&gt; model by adding a &lt;code&gt;JSONField&lt;/code&gt; for each. This is the current approach in the reference implementation. &lt;/p&gt;
    &lt;p&gt;But let's be more explicit in our approach and define models for these as well. We'll record each attempt to execute a task and its potential error, linking them to the task with a foreign key:&lt;/p&gt;
    &lt;code&gt;class Error(models.Model):
    exception_class_path = models.TextField()
    traceback = models.TextField()

class AttemptResultStatus(TextChoices):
    # Subset of TaskResultStatus.
    FAILED = TaskResultStatus.FAILED
    SUCCESSFUL = TaskResultStatus.SUCCESSFUL

class Attempt(models.Model):
    task = models.ForeignKey(Task, related_name="attempts", on_delete=models.CASCADE)
    error = models.OneToOneField(
        Error, related_name="attempt", on_delete=models.CASCADE, null=True, blank=True
    )
    worker_id = models.CharField(max_length=MAX_LENGTH_WORKER_ID)
    started_at = models.DateTimeField()
    stopped_at = models.DateTimeField(blank=True, null=True)
    status = models.CharField(
        choices=AttemptResultStatus.choices, max_length=10, blank=True
    )
&lt;/code&gt;
    &lt;p&gt;This setup ensures we have all necessary information to execute a task, plus we can provide every single bit of detail when a &lt;code&gt;TaskResult&lt;/code&gt; is requested.&lt;/p&gt;
    &lt;p&gt;All fine and dandy, but we need to think about the worker's requirements as well. It needs to be able to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Quickly check for outstanding tasks&lt;/item&gt;
      &lt;item&gt;Claim one of those tasks&lt;/item&gt;
      &lt;item&gt;Process that task and either mark it as failed, successful or ready (to retry later)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We could do all of that with how it's set up right now, but I'd like to refine things a bit.&lt;/p&gt;
    &lt;code&gt;class Task(models.Model):
    # ...
    # This field is used to keep track of when to run a task (again).
    # run_after remains unchanged after enqueueing.
    available_after = models.DateTimeField()
    # Denormalized count of attempts.
    attempt_count = models.IntegerField(default=0)
    # Set when a worker starts processing this task.
    worker_id = models.CharField(max_length=MAX_LENGTH_WORKER_ID, blank=True)
    # ...
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;available_after&lt;/code&gt; field will contain the earliest time at which the task can be executed. If the task's &lt;code&gt;run_after&lt;/code&gt; is specified (which can be done by using a task's... &lt;code&gt;using()&lt;/code&gt; method), &lt;code&gt;available_after&lt;/code&gt; is set to that value. Otherwise we're using the current datetime; all in UTC.&lt;/p&gt;
    &lt;p&gt;Once a task needs to be retried, &lt;code&gt;available_after&lt;/code&gt; will be set to the next possible point in time the task can be executed. In other words: we can back off.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;attempt_count&lt;/code&gt; field makes querying for available tasks a bit easier. Any tasks with an &lt;code&gt;attempt_count&lt;/code&gt; greater than the maximum allowed value can be ignored. Yes, their status should have been set to &lt;code&gt;FAILED&lt;/code&gt; which means they should be excluded by default, but we could change the configuration and tweak the maximum number of attempts.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;worker_id&lt;/code&gt; field is filled when a worker claims a task. This, among other things, prevents any other workers from picking up the task. Assuming the worker id is unique.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enqueueing and fetching a result&lt;/head&gt;
    &lt;p&gt;Enqueueing a task could not be easier: create a &lt;code&gt;Task&lt;/code&gt; model instance from the &lt;code&gt;Task&lt;/code&gt; dataclass instance, save it, done! Well, at least after turning the end result into a &lt;code&gt;TaskResult&lt;/code&gt;. &lt;/p&gt;
    &lt;p&gt;We use the string version of the model's database id as the id of the result.&lt;/p&gt;
    &lt;p&gt;Retrieving a result is likewise only a matter of loading the task and its attempts, and turning that into a &lt;code&gt;TaskResult&lt;/code&gt;. &lt;/p&gt;
    &lt;p&gt;Here's a simplified version of our task backend as it stands:&lt;/p&gt;
    &lt;code&gt;class DatabaseBackend(BaseTaskBackend):
    supports_defer = True
    supports_async_task = False
    supports_get_result = True
    supports_priority = True

    def enqueue(self, task: Task, args, kwargs):
        self.validate_task(task)
        model = self.queue_store.enqueue(task, args, kwargs)
        task_result = TaskResult(
            task=task,
            id=str(model.pk),
            # ...
            # More properties being set
            # ...
        )
        return task_result

    def get_result(self, result_id):
        return self.model_to_result(
            self.queue_store.get(result_id)
        )

    def model_to_result(self, model: models.Task) -&amp;gt; TaskResult:
        ...
&lt;/code&gt;
    &lt;p&gt;At lot of functionality is deferred to this &lt;code&gt;queue_store&lt;/code&gt; property. Before we dive into that, we'll explain the configuration options for this backend.&lt;/p&gt;
    &lt;head rend="h3"&gt;Configuration&lt;/head&gt;
    &lt;p&gt;We want to be able to specify defaults for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the maximum number of attempts (retries)&lt;/item&gt;
      &lt;item&gt;the backoff factor; i.e. we'll back off using &lt;code&gt;math.pow(factor, attempts)&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These can be customized for each individual queue. So we end up with something like this in our &lt;code&gt;OPTIONS&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;TASKS = {
    "default": {
        "BACKEND": "messagecenter.dbtasks.backend.DatabaseBackend",
        "OPTIONS": {
            "queues": {
                "low_priority": {
                    "max_attempts": 5,
                }
            },
            "max_attempts": 10,
            "backoff_factor": 3,
            "purge": {"finished": "10 days", "unfinished": "20 days"},
        },
    }
}
&lt;/code&gt;
    &lt;p&gt;A task added to the &lt;code&gt;low_priority&lt;/code&gt; queue will be attempted up to five times, with a backoff factor of &lt;code&gt;3&lt;/code&gt;. Other tasks will be attempted up to ten times with the same backoff factor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Queue store&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;QueueStore&lt;/code&gt; class is a companion of our backend. It's focus is on retrieving and enqueueing tasks, checking for tasks to execute and claiming tasks. &lt;/p&gt;
    &lt;p&gt;However the main reason it's included is to simplify the worker. As we'll see, the worker gets it's own copy of the queue store, limited to the queues it needs to process.&lt;/p&gt;
    &lt;head rend="h3"&gt;The worker&lt;/head&gt;
    &lt;p&gt;The worker's job, at least in this project, is to provide information on outstanding tasks to the runner and to drive the processing of those tasks by the backend. Which means it looks like this:&lt;/p&gt;
    &lt;code&gt;class Worker:
    def __init__(
        self,
        id_: str | None,
        backend_name: str,
        only: set[str] | None,
        excluding: set[str] | None,
    ):
        # Grab the backend and its queue_store.
        self.backend = task_backends[backend_name]
        queue_store: QueueStore = self.backend.queue_store
        # Limit the queue_store to the select queues.
        if only or excluding:
            queue_store = queue_store.subset(only=only, excluding=excluding)
        self.queue_store = queue_store
        # Use or create and id. "Must" be unique.
        self.id = (
            id_ if id_ else create_id(backend_name, queues=queue_store.queue_names)
        )

    def has_more(self) -&amp;gt; bool:
        return self.queue_store.has_more()

    def process(self):
        with transaction.atomic():
            tm = self.queue_store.claim_first_available(worker_id=self.id)
        if tm is not None:
            self.backend.process_task(tm)
&lt;/code&gt;
    &lt;p&gt;All we need to do to have a functioning worker runner:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create an instance of the worker.&lt;/item&gt;
      &lt;item&gt;Ask it if there's tasks to execute using &lt;code&gt;has_more&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If so: tell it to &lt;code&gt;process&lt;/code&gt;the first available task. If not: go to 4.&lt;/item&gt;
      &lt;item&gt;Wait, then return to 2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's what our &lt;code&gt;dbtasks_worker&lt;/code&gt; command does.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claiming a task&lt;/head&gt;
    &lt;p&gt;Our queue store provides a &lt;code&gt;peek&lt;/code&gt; method which returns the id of the task in our queues with the most urgency; a combination of &lt;code&gt;available_after&lt;/code&gt;, &lt;code&gt;priority&lt;/code&gt; and &lt;code&gt;attempt_count&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This lets the runner know whether there's more tasks to process. The next step is to claim one of those tasks. So we call &lt;code&gt;peek&lt;/code&gt; again and if it returns a task id, we'll try to claim that particular task.&lt;/p&gt;
    &lt;p&gt;Here's a more basic, clearer version than the one included in our project's &lt;code&gt;QueueStore&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;def claim_first_available(
    self, worker_id: str, attempts: int = 3
) -&amp;gt; models.Task | None:
    qs = models.Task.filter(
        worker_id="", 
        status=TaskResultStatus.READY,
    )
    for _ in range(attempts):
        task_id = self.peek()
        if not task_id:
            return None
        count = qs.filter(pk=task_id).update(
            worker_id=self.id_,
            status=TaskResultStatus.RUNNING,
        )
        if count:
            return models.Task.objects.get(pk=task_id)
    return None
&lt;/code&gt;
    &lt;p&gt;If the &lt;code&gt;count&lt;/code&gt; is zero, we failed to claim the task. Otherwise we retrieve it from the database and can start processing. &lt;/p&gt;
    &lt;p&gt;The loop is included because we ended up here after trying to claim the task identified by &lt;code&gt;peek&lt;/code&gt;. Which apparently has already been picked up by another worker. We might as well make the most of it and try to grab another task from the queue.&lt;/p&gt;
    &lt;head rend="h3"&gt;Processing the task&lt;/head&gt;
    &lt;p&gt;And finally the thing that actually does something!&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;process_task&lt;/code&gt; method of our backend:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Creates an &lt;code&gt;Attempt&lt;/code&gt;and constructs the current&lt;code&gt;TaskResult&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Executes the task, capturing anything extending &lt;code&gt;BaseException&lt;/code&gt;, or returning the&lt;code&gt;return_value&lt;/code&gt;of the task when everything went according to plan.&lt;/item&gt;
      &lt;item&gt;Either updates the &lt;code&gt;Task&lt;/code&gt;model, the&lt;code&gt;Attempt&lt;/code&gt;and the&lt;code&gt;TaskResult&lt;/code&gt;with the final details of the successful execution, or with details about the failure to do so.&lt;/item&gt;
      &lt;item&gt;And in the latter case: check if the task can be retried.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Again: if you want to dive into the details, have a look at the repository.&lt;/p&gt;
    &lt;head rend="h2"&gt;That's it&lt;/head&gt;
    &lt;p&gt;Of course this demo project leaves out all the things you really need to think hard about. Like signals for the worker. Or database transaction logic. That's not to say it's impossible. Far from it. It just wasn't the goal of this article.&lt;/p&gt;
    &lt;p&gt;The inclusion of this functionality in Django will certainly allow new libraries or adapters for existing task queues to pop up. And we'll probably soon see some complaints that &lt;code&gt;django.tasks&lt;/code&gt; isn't extensive enough.&lt;/p&gt;
    &lt;p&gt;Because, if you're currently using the more advanced functionality of your task queue, there's probably a few things you're missing in &lt;code&gt;django.tasks&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complex orchestration&lt;/head&gt;
    &lt;p&gt;Some task queue libraries, like Celery, provide ways of combining tasks. You can feed the result of one task into another, enqueue tasks for each item in a list, and so on.&lt;/p&gt;
    &lt;p&gt;It should be clear by now that supporting this kind of orchestration isn't the goal of &lt;code&gt;django.tasks&lt;/code&gt;. And I don't mind at all. There's no feasible way of creating a unified API to support this. I've had my share of problems with libraries that do claim to support it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Retry&lt;/head&gt;
    &lt;p&gt;As mentioned before, there's currently no way to automatically retry a failed task, unless your backend does the heavy lifting. Like ours does.&lt;/p&gt;
    &lt;p&gt;Depending on your backend this might be easy enough to handle yourself. For example using a decorator:&lt;/p&gt;
    &lt;code&gt;def retry(func):
    @functools.wraps(func)
    def wrapper(context: TaskContext, *args, **kwargs):
        try:
            return func(context, *args, **kwargs)
        except BaseException as e:
            result = context.task_result
            backoff = math.pow(2, result.attempts)
            run_after = datetime.now(tz=UTC) + timedelta(seconds=backoff)
            result.task.using(run_after=run_after).enqueue(*args, **kwargs)
            raise e
    return wrapper


@task(takes_context=True)
@retry
def send_email(context: TaskContext, to: str, subject: str, body: str):
    # Do your thing 
    ...
&lt;/code&gt;
    &lt;head rend="h3"&gt;An actual worker mechanism&lt;/head&gt;
    &lt;p&gt;True. But the reference implementation does provide actual workers. Be patient, or even better: start helping out!&lt;/p&gt;
    &lt;head rend="h2"&gt;There is no perfect solution&lt;/head&gt;
    &lt;p&gt;I reckon &lt;code&gt;django.tasks&lt;/code&gt; will soon result in covering at least the most common 80% of use cases. Yes, its API is simple and limited, but to me that's more a benefit rather than a fault. I think this is as close as you can get to a standardized approach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://roam.be/notes/2025/a-first-look-at-djangos-new-background-tasks/"/><published>2025-11-28T21:46:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46083303</id><title>Confessions of a Software Developer: No More Self-Censorship</title><updated>2025-11-29T09:36:31.100438+00:00</updated><content>&lt;doc fingerprint="2f4551170fb65fb1"&gt;
  &lt;main&gt;
    &lt;p&gt;I haven’t published since April because I’ve been afraid. I also avoided social media, news aggregators, and discussion forums for months. I’m done letting fear stop me. What was I afraid of? In this post I detail every single thing I’ve avoided admitting on this blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Knowledge Gap Confessions&lt;/head&gt;
    &lt;p&gt;First, why am I admitting these things now? I realized I am not the only working software developer missing crucial skills. My learning path through my career looked a lot like a slime mold seeking morsels of food: strengthening what has utility, but letting the rest wither. But lately, I’ve been building a better base of knowledge. Writing or speaking about what I learn–which helps me learn better–requires me to admit I didn’t know. Plus, I’d like to show others in my situation that it’s never too late to learn what you don’t know. I can fill in those fundamentals, and so can you.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It’s from that very ignorance that sprouts the drive for knowledge.&lt;/p&gt;
      &lt;p&gt;— Leticia Portella, A Friendly Guide to Software Development&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;I Didn’t Understand Polymorphism For a Decade&lt;/head&gt;
    &lt;p&gt;Learning about polymorphism over the past twelve months was the first time I was embarrassed to admit I didn’t already know something. I’ve been writing ostensibly object-oriented software since 2012. And yet, my lack of awareness of polymorphism showed me I’ve been writing little more than structured programs. That I could replace conditionals and case staments with specialized classes had never crossed my mind.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;As a hiring manager I interviewed software engineers and tried to filter for object-oriented knowledge. Retroactively, it’s clear I was hypocritical. This gap reveals that I spent the early part of my career learning tools, not principles. Plus, it highlights my lack of formal education. Polymorphism is covered in every college OO course.&lt;/p&gt;
    &lt;head rend="h3"&gt;I Forgot SQL&lt;/head&gt;
    &lt;p&gt;I took a college database course as a student. As a working professional, I read and worked through the exercises in Learning SQL, 3rd Edition. For a while, I could write SQL. But I specialized in front-end web development, and had no professional use for SQL. Like any unused skill, it atrophied. I remember how to write basic queries, but not much more. For example, I cannot tell you the difference between a left inner join and an outer join without looking it up.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;I’m not accustomed to forgetting. Growing up I had a remarkable ability to remember almost everything. It didn’t matter whether I did it, read it, or heard it. It could be a fact, a skill, or an event. Four years later I could access that knowledge, with the slightest reminder unlocking a flood of memories. Now that I’m in my mid-thirties, that isn’t always true. SQL is the first time I’ve lost an entire skill to atrophy. It’s tough to come to terms with the start of aging. It’s tougher to admit it publicly.&lt;/p&gt;
    &lt;head rend="h3"&gt;I Don’t Write Automated Tests&lt;/head&gt;
    &lt;p&gt;An estimated 95% of the code I’ve shipped to production had no automated tests. Early in my career, I had no exposure to the concept. Later, I was writing front-ends in Ember, whose testing story was looked good but felt pretty bad at the time. More recently, I’ve been working legacy code, and I haven’t put in the work to make it testable. The only time I tend to write new tests is when I’m writing a new subsystem, which can be designed testable from the start. I’m convinced that writing automated tests needs to be part of my daily practice, but I haven’t gotten there yet.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;This may be my most professionally-damaging confession. If you believe Uncle Bob, shipping production code without tests is not more than risky, it’s unethical. I stopped myself from posting about my learning journey for fear that a future hiring manager would decide that I was unfit to work with them on this basis.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How much of the code should be tested with these automated unit tests? Do I really need to answer that question? All of it! All. Of. It.&lt;/p&gt;
      &lt;p&gt;Am I suggesting 100% test coverage? No, I’m not suggesting it. I’m demanding it. Every single line of code that you write should be tested. Period.&lt;/p&gt;
      &lt;p&gt;Isn’t that unrealistic? Of course not. You only write code because you expect it to get executed. If you expect it to get executed, you ought to know that it works. The only way to know this is to test it.&lt;/p&gt;
      &lt;p&gt;— Robert C. Martin, The Clean Coder, Chapter 1: Professionalism&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Personal Confessions&lt;/head&gt;
    &lt;head rend="h3"&gt;I Didn’t End Up Learning Blazor&lt;/head&gt;
    &lt;p&gt;People have been waiting for a follow-up from me about my journey learning C#, .NET, and Blazor. This isn’t that post. I don’t know if that post will ever come.&lt;/p&gt;
    &lt;p&gt;C# was never the language I wanted to learn for side projects. .NET was never the platform I wanted to work with professionally. I was learning them for one reason: my job. My engineering department decided to switch our tech stack from Angular to Blazor. I was the only person on the team with no C# skills. I started fixing that immediately.&lt;/p&gt;
    &lt;p&gt;A couple months later, almost as suddenly, the decision was undone. Our tech stack would not change after all. With no intrinsic motivation to push me along, I abandoned the C# / .NET book I was reading without finishing. I’ve got more important things to learn.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;No matter what software thoughts crossed my mind, I intended to post about them. Writing helps me solidify ephemeral thoughts. Publishing offers an opportunity for feedback. But I made two errors that locked me into a pattern of fear. First, I promised a follow up article at the end of my last post about the Blazor stack. I then felt worse every time I published an article other than the promised follow-up. Second, I began to see value in the amount of traffic a blog post got. The posts about my first steps in that learning journey were the winners. Admitting I changed tack when the company did felt like admitting defeat.&lt;/p&gt;
    &lt;head rend="h3"&gt;I Want to Write More Ruby&lt;/head&gt;
    &lt;p&gt;I love Ruby. I use it in my code examples. It’s my default language for open source projects. I write Ruby for code katas, etudes, and hackathons. But I haven’t been paid to write Ruby since 2013.&lt;/p&gt;
    &lt;p&gt;The best possible option–and yet the most improbable–would be for my current employer to start a Ruby project. I’ve worked with a few of my teammates for 12 years across two companies. I’ve always chosen to keep working with fantastic people at the cost of working with a less-than-fantastic language.&lt;/p&gt;
    &lt;p&gt;Sadly that means I’m limited to being a Rubyist after work and on weekends. I spend fewer of those hours than I’d like writing Ruby, instead favoring other obligations, hobbies, and professional development goals. The only way I foresee getting to work with Ruby as much as I’d like would be to paid for it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;My manager and his manager, the CTO, read this blog. I found it difficult to write freely about my distaste for the tools I use every day. I found it ever harder to admit that I actively want my daily job duties to be different. I feared they might take it as a hint that I’m quitting (I’m not), or that I’d push to use a tool at work nobody else is familiar with at work (I won’t).&lt;lb/&gt;A bonus, even more personal confession…&lt;/p&gt;
    &lt;head&gt;A bonus, even more personal confession…&lt;/head&gt;
    &lt;head rend="h3"&gt;Cyber Bullying Hurts, Even as an Adult&lt;/head&gt;
    &lt;p&gt;I spent a lot of my young adulthood online. My early days on the internet were spent in intellectual spaces, where the interactions felt like The Nets of Ender’s Game: a marketplace of ideas. Strong criticism came swiftly, but it was about the ideas, not the person. Even Reddit and Hacker News–forums which have a reputation for harsh comments sections–don’t bother me, because the vitriol is aimed at taking down bad ideas, not insulting people.&lt;/p&gt;
    &lt;p&gt;Other websites, though, are different. I learned this when I got bullied on a different online forum. I was called incapable, sneaky, disgusting, incompetent, uncaring, and a representative of a threat to human expression.&lt;/p&gt;
    &lt;p&gt;What triggered this vitriol? I requested a small feature in an open source project, and the maintainer said they would accept a PR. The project was written in a language I haven’t used. I used an LLM to generate a small commit (a few dozen lines), reviewed and tested the patch, and submitted a pull request. This was months ago–there were few social norms around AI-assisted patches, and AI policies were rare. Since the project didn’t have a policy, I did not disclose my use of AI.&lt;/p&gt;
    &lt;p&gt;When I told the story of that pull request on the forum and defended my ethical position, the bullying started. I was followed across websites, contacted via email and SMS, and even called on the phone. I no longer felt safe having a presence on that website. I deleted my comments, removed PII from my profile, and asked the administrators of the forum to scrub my real name to prevent further harassment. Instead, they attached more PII to my profile, locked me out of editing it, and permanently vandalized it with the false claim that I lied about being contacted about the discussion outside the forum.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;This incident was one of the most toxic things I’ve ever experienced, and it lasted for days. Writing about it leaves me feeling its echoes even now. I was afraid that one of these people will use the comments section, or email, or even my phone number, to re-litigate the issue. Even now, I am afraid that the administrator’s (possibly defamatory) statement on my profile will make me less employable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Workplace Confessions&lt;/head&gt;
    &lt;head rend="h3"&gt;Your SaaS Team Doesn’t Need a Special Process&lt;/head&gt;
    &lt;p&gt;Hundreds of companies, thousands of researchers, tens of thousands of workers, and millions of dollars have gone into shaping our industry’s best practices. The agile manifesto is old enough to drink. Software as a Service has dominated the market for over a decade. Your company has a limited innovation budget. Do you want to spend it on coming up with a custom software development lifecycle, or making a product that wins in the marketplace? Follow Scrum, Lean / Kanban, or eXtreme Programming to the letter, and let your team focus on the product.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;Like any author, I write what I know. Here, I was motivated to write because a co-worker pushed to create a custom software development process. I don’t know that I have the tact to avoid it seeming to be a takedown of that person or their ideas. I admire the ability of authors like Kent Beck and Martin Fowler to write about how to work better without calling out coworkers who made mistakes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Remote Work Sucks&lt;/head&gt;
    &lt;p&gt;Remote work eliminates a lot of problems with office work: commutes, inefficient use of real estate, and land value distortion. But software development is better when you breathe the same air as the folks you work with. Even with a camera-on policy, video calls are a low-bandwidth medium. You lose ambient awareness of coworkers’ problems, and asking for help is a bigger burden. Pair programming is less fruitful. Attempts to represent ideas spatially get mutilated by online whiteboard and sticky note software. Even conflict gets worse: it’s easy to form an enemy image of somebody at the end of video call, but difficult to keep that image when you share a room with them and sense their pain.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;When COVID-19 hit, the company I worked for went remote “for a couple of weeks.” After a few months of productive work without an office and with no vaccine in sight, it became permanent. I took the opportunity to move to a rural area. Geographic arbitrage meant I could afford 27 acres, and I even bought a family milking cow. My family has since put down roots: close friendships, community involvement, and a lifestyle built around the lack of a commute.&lt;/p&gt;
    &lt;p&gt;I feared that writing negatively about remote work might jeopardize my current remote job–and every future remote job I might look for. I thought, “who would hire a remote worker who prefers in-office work?” Because even though I prefer working side-by-side with others, I won’t likely move for a job. I have a 30-year mortgage with a low interest rate. My house was purchased before the post-pandemic price spike. I have an acre of lawn &amp;amp; garden, not to mention the farm acreage. I’d need to double my current income to maintain my current lifestyle in a city, which is unlikely.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Now?&lt;/head&gt;
    &lt;p&gt;Now that the dam has burst, nothing is holding me back from publishing. I’m going to continue to work on skill building, but now I feel free to write about it. If this article resonated with you–whether you also have knowledge gaps you’d like to fill, you’d like to help me fill mine, or you just want to see what happens–please let me know in the comments. Subscribe via Mastodon to see everything I post, use RSS to customize your subscription, or subscribe to my mailing list to get notified when I post a larger article.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mastodon &amp;amp; Fediverse&lt;/head&gt;
    &lt;p&gt;To help you follow along, I’ve enabled ActivityPub on this blog, meaning it’s a fully-functioning Mastodon account: &lt;code&gt;@[email protected]&lt;/code&gt;. &lt;/p&gt;
    &lt;head rend="h3"&gt;RSS &amp;amp; Feed Readers&lt;/head&gt;
    &lt;p&gt;If you prefer to kick it old school, grab an XML feed reader like NetNewsWire for MacOS, Feedmill for Windows, or Newsflash for Linux and choose one or more feeds:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The “Everything” feed &lt;list rend="ul"&gt;&lt;item&gt;Just the “Articles” feed &lt;list rend="ul"&gt;&lt;item&gt;Specifically the “Advice” feed&lt;/item&gt;&lt;item&gt;Specifically the “Blog Posts” feed&lt;/item&gt;&lt;item&gt;Specifically the “Book Reviews” feed&lt;/item&gt;&lt;item&gt;Specifically the “News” feed&lt;/item&gt;&lt;item&gt;Specifically the “Tutorials” feed&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Just the “Micro-Posts” feed &lt;list rend="ul"&gt;&lt;item&gt;Specifically the “Responses” feed&lt;/item&gt;&lt;item&gt;Specifically the “Quick Tips” feed&lt;/item&gt;&lt;item&gt;Specifically the “Shared Links” feed&lt;/item&gt;&lt;item&gt;Specifically the “Tutorials” feed&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Just the “Articles” feed &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Email Newsletter&lt;/head&gt;
    &lt;p&gt;Of course, the classic (and my favorite) way is to subscribe to email notifications. I’ll only send emails when I publish a new article, not for every micro-post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kerrick.blog/articles/2025/confessions-of-a-software-developer-no-more-self-censorship/"/><published>2025-11-28T22:21:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46083375</id><title>Fabric Project</title><updated>2025-11-29T09:36:30.576993+00:00</updated><content>&lt;doc fingerprint="ab5bd85145309005"&gt;
  &lt;main&gt;
    &lt;p&gt;Fabric is a creative code and rapid prototyping environment focusing on interactive visuals, image and video processing and analysis and 3D content authoring.&lt;/p&gt;
    &lt;p&gt;Fabric&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Provides an intuitive Visual Node based content authoring environment&lt;/item&gt;
      &lt;item&gt;Provides an SDK to load an common interchange file format&lt;/item&gt;
      &lt;item&gt;Provides an SDK to extend Fabric by creating custom nodes via a plugin architecture&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fabric is inspired by Apple's deprecated Quartz Composer ecosystem, and its design philosophy.&lt;/p&gt;
    &lt;p&gt;Fabric is intended to be used as&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Creative coding tool requires little to no programming experience.&lt;/item&gt;
      &lt;item&gt;Pro User tool to create reusable documents (similar to Quartz Composer Compositions) that can be loaded in the Fabric runtime and embedded into 3rd party applications.&lt;/item&gt;
      &lt;item&gt;Developer environment built on Satin that can render high fidelity visual output in a procedural way, using modern rendering techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An early alpha of Satin rendering a instances of a sphere geometry, along with an HDRI environment and a PBR Shader at 120Hz:&lt;/p&gt;
    &lt;p&gt;Think of Fabric as a playground of visual capabilies you can combine together.&lt;/p&gt;
    &lt;p&gt;Author&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Interacive 3D graphics&lt;/item&gt;
      &lt;item&gt;Image processing and effects&lt;/item&gt;
      &lt;item&gt;Audio reactive scenes&lt;/item&gt;
      &lt;item&gt;images and video analysis pipelines&lt;/item&gt;
      &lt;item&gt;embed your scenes into your own apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check out the Samples&lt;/p&gt;
    &lt;p&gt;Fabric supports, thanks to Satin and Lygia, high fidelity modern rendering techniques including&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physically based rendering&lt;/item&gt;
      &lt;item&gt;Scene graph&lt;/item&gt;
      &lt;item&gt;Lighting and Shadow casting&lt;/item&gt;
      &lt;item&gt;Realtime shader editing (live coding, hot reloading)&lt;/item&gt;
      &lt;item&gt;GPU Compute&lt;/item&gt;
      &lt;item&gt;Image Based Lighting&lt;/item&gt;
      &lt;item&gt;3D Model Loading&lt;/item&gt;
      &lt;item&gt;Material System&lt;/item&gt;
      &lt;item&gt;ML based realtime segmentation and keypoint detection&lt;/item&gt;
      &lt;item&gt;Shader based Image Processing and Mixing&lt;/item&gt;
      &lt;item&gt;Local LLM calling&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fabric is authored by by Anton Marini.&lt;/p&gt;
    &lt;p&gt;Fabric uses Satin 3D engine Satin rendering engine written by @Reza Ali.&lt;/p&gt;
    &lt;p&gt;Fabric includes a licensed Metal port of Lygia shader library, powering Image effects and more, written by @Patricio Gonzalez Vivo and contributors.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;Please note Fabric is heavily under construction.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 14 +&lt;/item&gt;
      &lt;item&gt;XCode 15 +&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please See Releases for code signed App downloads.&lt;/p&gt;
    &lt;p&gt;For Developers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checkout Fabric and ensure you check out with submodules enabled, as Satin is a dependency.&lt;/item&gt;
      &lt;item&gt;Open the XCode project&lt;/item&gt;
      &lt;item&gt;Ensure that &lt;code&gt;Fabric Editor&lt;/code&gt;is the active target.&lt;/item&gt;
      &lt;item&gt;Build and run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Checkout our Architecture Document to understand the underlying paradigms of working with Fabric's Nodes and execution model, learn what a &lt;code&gt;Node&lt;/code&gt; and a &lt;code&gt;Port&lt;/code&gt; is, the types of data Fabric can generate and process, and how Fabric executes your compositions.&lt;/p&gt;
    &lt;p&gt;We also provide a set of evolving tutorial / getting started and sample Fabric compositions along with a readme walk through. You can use the Sample Compositions to learn and build off of.&lt;/p&gt;
    &lt;p&gt;You can view a comprehensive list of available and planned Nodes here to explore and learn how to compose more advanced and custom setups with Fabric.&lt;/p&gt;
    &lt;p&gt;Don't hesitate to file a feature request if a Node is missing!&lt;/p&gt;
    &lt;p&gt;Checkout our Roadmap Document&lt;/p&gt;
    &lt;p&gt;I ( Anton Marini ) are looking to build a community of developers who long for the ease of use and interoperability of Quartz Composer, its ecosystem and plugin community.&lt;/p&gt;
    &lt;p&gt;If you are interested in contributing, please do not hesitate to reach out / comment in the git repository, or join our discord via invite&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Will Fabric ever be cross platform?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;No. Fabric is purpose built on top of Satin and aims to provide a best in class Apple platform experience using Metal.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;What languages are used?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Fabric Editor is written in Swift and SwiftUI. Satin is written in Swift and C++&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Why not just use Vuo or Touch Designer or some other node based tool?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;I do not like them.&lt;/item&gt;
          &lt;item&gt;Don't get me wrong, they are incredible tools, but they are not for me.&lt;/item&gt;
          &lt;item&gt;They do not think the way I think.&lt;/item&gt;
          &lt;item&gt;They do not expose the layers of abstraction I want to work with.&lt;/item&gt;
          &lt;item&gt;They do not provide the user experience I want.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Fabric-Project/Fabric"/><published>2025-11-28T22:32:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46084122</id><title>A triangle whose interior angles sum to zero</title><updated>2025-11-29T09:36:30.472909+00:00</updated><content>&lt;doc fingerprint="eb9b90f084dff3a0"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Spherical geometry&lt;/head&gt;
    &lt;p&gt;In spherical geometry, the interior angles of a triangle add up to more than π. And in fact you can determine the area of a spherical triangle by how much the angle sum exceeds π. On a sphere of radius 1, the area equals the triangle excess&lt;/p&gt;
    &lt;p&gt;Area = E = interior angle sum − π.&lt;/p&gt;
    &lt;p&gt;Small triangles have interior angle sum near π. But you could, for example, have a triangle with three right angles: put a vertex on the north pole and two vertices on the equator 90° longitude apart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hyperbolic geometry&lt;/head&gt;
    &lt;p&gt;In hyperbolic geometry, the sum of the interior angles of a triangle is always less than π. In a space with curvature −1, the area equals the triangle defect, the difference between π and the angle sum.&lt;/p&gt;
    &lt;p&gt;Area = D = π − interior angle sum.&lt;/p&gt;
    &lt;p&gt;Again small triangles have an interior angle sum near π. Both spherical and hyperbolic geometry are locally Euclidean.&lt;/p&gt;
    &lt;p&gt;The interior angle sum can be any value less than π, and so as the angle sum goes to 0, the triangle defect, and hence the area, goes to π. Since the minimum angle sum is 0, the maximum area of a triangle is π.&lt;/p&gt;
    &lt;p&gt;The figure below has interior angle sum 0 and area π in hyperbolic geometry.&lt;/p&gt;
    &lt;p&gt;Strictly speaking this is an improper triangle because the three hyperbolic lines (i.e. half circles) don’t intersect within the hyperbolic plane per se but at ideal points on the real axis. But you could come as close to this triangle as you like, staying within the hyperbolic plane.&lt;/p&gt;
    &lt;p&gt;Note that the radii of the (Euclidean) half circles doesn’t change the area. Any three semicircles that intersect on the real line as above make a triangle with the same area. Note also that the triangle has infinite perimeter but finite area.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.johndcook.com/blog/2025/11/28/tricusp-triangle/"/><published>2025-11-29T00:26:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46084535</id><title>Every mathematician has only a few tricks (2020)</title><updated>2025-11-29T09:36:30.098749+00:00</updated><content>&lt;doc fingerprint="3758f87a661d0767"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;From a physicist point of view I want to mention this trick and its generalization for operators:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;      "Two commuting matrices are simultaneously diagonalizable"
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;(for physicists all matrices are diagonalizable). Of course the idea is that if you know the eigenvectors of one matrix/operator then diagonalizing the other one is much easier. Here are some applications.&lt;/p&gt;
      &lt;p&gt;1)The system is translation invariant : Because the eigenvectors of the translation operator are $e^{ik.x}$, then one should use the Fourier transform. It solves all the wave equations for light, acoustics, of free quantum electrons or the heat equation in homogeneous media.&lt;/p&gt;
      &lt;p&gt;2)The system has a discrete translation symmetry: The typical system is the atoms in a solid state that form a crystal. We have a discrete translation operator $T_a\phi(x)=\phi(x+a)$ with $a$ the size of the lattice and then we should try $\phi_k(x+a)=e^{ik.a}\phi_k(x)$ as it is an eigenvector of $T_a$. This gives the Bloch-Floquet theory where the spectrum is divided into band structure. It is one of the most famous model of condensed matter as it explains the different between conductors or insulators.&lt;/p&gt;
      &lt;p&gt;3)The system is rotational invariant: One should then use and diagonalize the rotation operator first. This will allow us to find the eigenvalue/eigenvectors of the Hydrogen atom. By the way we notice the eigenspace of the Hydrogen are stable by rotation and are therefore finite dimension representations of $SO(3)$. The irreducible representations of $SO(3)$ have dimension 1,3,5,... and they appears, considering also the spin of the electron, as the columns of the periodic table of the elements (2,6,10,14,...).&lt;/p&gt;
      &lt;p&gt;4)$SU(3)$ symmetry: Particle physics is extremely complicated. However physicists have discovered that there is an underlying $SU(3)$ symmetry. Then considering the representations of $SU(3)$ the zoology of particles seems much more organized (A, B).&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mathoverflow.net/questions/363119/every-mathematician-has-only-a-few-tricks"/><published>2025-11-29T01:37:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46084956</id><title>System 7 natively boots on the Mac mini G4</title><updated>2025-11-29T09:36:29.329240+00:00</updated><content>&lt;doc fingerprint="b63c99572fc08fad"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;(And Mac OS 8!)&lt;/p&gt;&lt;p&gt;Hey, guys!&lt;/p&gt;&lt;p&gt;Surely y'all know and have enjoyed Mac OS 9.2.2 booting and beautifully-running on all four Mac mini G4 models for close to 8 years now. (Wow!)&lt;/p&gt;&lt;p&gt;Well, that was one massive revolution...&lt;/p&gt;&lt;p&gt;... But most of us did not think we would live to see the day New World ROM machines, even more so the likes of the Mac mini G4, to NATIVELY boot System 7:&lt;/p&gt;&lt;p&gt;(Gotta love it trying to display 1 GB RAM capacity.)&lt;/p&gt;&lt;p&gt;Before your eyeballs leave your eyesockets completely, I ought to warn that there's still much to be sorted out in this, especially sound, video and networking (the usual suspects). In other words, your mileage may vary, so keep expectations in check!&lt;/p&gt;========================================================&lt;lb/&gt;OK, so HOW in the WORLD is any of this possible?&lt;lb/&gt;========================================================&lt;p&gt;It turned out "New World ROM" Macs had a cousin born out of the clone program (until the usual villain, Steve Jobs, came and killed it), which was an architecture called "&lt;/p&gt;CHRP&lt;p&gt;" (pronounced "chirp"). It was the successor to &lt;/p&gt;PReP&lt;p&gt;, but, unlike PReP, Mac OS was also going to be officially-bootable on it. Close to no CHRP machines ever saw the light of the day, thanks to Jobs' return. Nonetheless, Apple internally developed Mac OS 7.6 ~ 8.0 for CHRP systems before it got axed. It's just that they never released it, but the development was done regardless. On October 2025, it turned out someone preserved some of these Mac versions, which were then acquired and preserved and shared with the world. (&lt;/p&gt;Macintosh Garden link&lt;p&gt;, &lt;/p&gt;archive.org link&lt;p&gt;.)&lt;/p&gt;&lt;p&gt;Although CHRP was left to die, the so-called "New World ROM" Macs inherited much of its architecture and design. As you probably know, these Macs rely on an extra system file called "Mac OS ROM", whereas "Old World ROM" Macs do not need it, and can use their own actual ROM to get Mac OS going. This meant any Mac OS version unaware of the concept of a Mac OS ROM file could not just simply boot in a New World ROM Mac normally. People were able to boot Mac OS versions as low as 8.1, but not any lower, and that too only for the very first few New World ROM Macs, but none of the later ones, which increasingly had a higher and higher minimum OS version.&lt;/p&gt;&lt;p&gt;But not anymore, as the following major events happened:&lt;/p&gt;&lt;p&gt;- The recent Mac OS 8.0 CHRP leaks provided an earlier ROM file that, it turns out, allows regular Mac OS 8.0 to boot, as well. Or, alternatively, the Mac OS ROM file that always worked with Mac OS 8.1 also worked on these Mac OS 8.0 CHRP releases. (Exact details are fuzzy in my memory by now, so someone else might want to correct me if I got something wrong.)&lt;/p&gt;&lt;p&gt;- The recent Mac OS 7.6 CHRP leak provided an additional System Enabler file, which could be exploited for loading Mac OS ROM files. I forget if that's how it worked out-of-the-box, or if a bit of hacking to the System Enabler was required for that, however what I do remember clearly is that, while the System Enabler was hardcoded so that artifically no OS earlier than 7.6 could use it, the OS version check could be patched out of it, so that System 7.5.x (and potentially earlier) can also use it.&lt;/p&gt;&lt;p&gt;In other words, &lt;/p&gt;this file is the reason that earlier Mac OS versions can make use of the Mac OS ROM file&lt;p&gt;, thus bringing Mac OS 7.6.1 and earlier potentially to ALL New World ROM Macs!&lt;/p&gt;&lt;p&gt;(Trivia tidbit: Apparently this enabler was also present in certain archives of the Mac OS 8.0 betas from when it was still known as "Mac OS 7.7". Oops! This thing was right under our nose all this while!)&lt;/p&gt;&lt;p&gt;- Of course, as hinted at previously, a System Enabler _alone_ is NOT enough to boot System 7 and the like when even much newer systems that were already aware of the Mac OS ROM file could not boot. The newer the model of the New World ROM Mac, the less you could actually "go back". The reason is simple: Mac OS ROM files, over time through its various versions, would get new features added, BUT also would remove older ones which were required by older OS versions. The solution? Using &lt;/p&gt;ELN's great Mac OS ROM patching tools&lt;p&gt; (plus other tools of his own), "Rairii" AKA "Wack0", known for his amazing PPC Windows NT 3.51 / NT 4.0 project on &lt;/p&gt;PowerMacs&lt;p&gt; and the &lt;/p&gt;Nintendo GC / Wii / Wii U&lt;p&gt;, analyzed many of these Mac OS ROM files, and fixed + patched + stitched together new Mac OS ROM files that attempt to keep ALL the old features that were removed AND all the new features that were added. In other words, the ultimate Mac OS ROM file that boots everything and runs everything (roughly-speaking). He also is the one who figured out and hacked the System Enabler to also accept OSes earlier than Mac OS 7.6.&lt;/p&gt;&lt;p&gt;Keep in mind, however, that this effort essentially allows Macs that are already able to boot SOME version of Mac OS to ALSO boot older versions. But if a given machine cannot boot ANY Mac OS version, such as the two DLSD PowerBook G4s (&lt;/p&gt;15"&lt;p&gt;, &lt;/p&gt;17"&lt;p&gt;), these patches cannot do anything about that: Their incompatibilities need to be addressed first and separately.&lt;/p&gt;&lt;p&gt;One more interesting thing to note about the similarity between CHRP systems and New World ROM Macs: If you check ANY "Mac OS ROM" file to see its TYPE and CREATOR codes, you will see they are "tbxi" and, you guessed it, "&lt;/p&gt;chrp&lt;p&gt;", respectively. I couldn't believe "chrp" was in ALL the Mac OS ROM files all these years!&lt;/p&gt;========================================================&lt;lb/&gt;Where can I get ahold of this EPIC stuff ? ? ? ? ?&lt;lb/&gt;========================================================&lt;p&gt;Rairii's "super" ROMs are available on &lt;/p&gt;this GitHub repository&lt;p&gt;, under &lt;/p&gt;releases&lt;p&gt;. You may also fetch the patched System Enabler for Mac OS 7.6.1 and earlier from there, and place it in the System Folder. Make sure to download the files from the latest release there.&lt;/p&gt;&lt;p&gt;Note that he applied his patches to 3 different versions of the (US) ROMs:&lt;/p&gt;&lt;p&gt;- 10.2.1 with CPU Software 5.9: The "latest and greatest" Mac OS ROM file of all Mac OS. For reference, this is also the ROM version that the &lt;/p&gt;1.628 GB max RAM Mac OS ROM we have was based on (thus going beyond the 1.5 GB limit)&lt;p&gt;, although do note that the RAM limit break patches are NOT included in this, at least not yet as of the time of writing.&lt;/p&gt;&lt;p&gt;- 2.5.1: A much earlier version of the ROM, but still new enough to support USB. See the GitHub page for details.&lt;/p&gt;&lt;p&gt;- 1.7.1: A very early ROM, which can be well-leveraged by very early New World ROM Macs. See the GitHub page for details.&lt;/p&gt;&lt;p&gt;Note you need ROM version 9.1 or higher to use ATA-6 AKA Ultra ATA/100 AKA Kauai drivers, which are essential on the likes of the Mac mini G4 and the MDD. Special notes for the Mac mini G4 are further down.&lt;/p&gt;========================================================&lt;lb/&gt;What is the COMPLETE list of Mac OS versions that now boot?&lt;lb/&gt;========================================================&lt;p&gt;To be exact, this is the complete list of OSes I have attempted, all on the Mac mini G4 1.5GHz model, with the following results:&lt;/p&gt;&lt;p&gt;- System 6.0.8: &lt;/p&gt;No boot&lt;p&gt;. You get a Happy Mac, followed by a blinking question mark in a floppy icon. (Note: Although this very attempt is UTTERLY insane for multiple technical reasons, it might be not AS seemingly-impossible as one may think, as the 68k emulator resides within the Mac OS ROM file.)&lt;/p&gt;&lt;p&gt;- System 7.0: &lt;/p&gt;No boot&lt;p&gt;. You get a Happy Mac, but then a warning window pops up saying System 7.0 cannot boot on this computer.&lt;/p&gt;&lt;p&gt;- System 7.1.2: &lt;/p&gt;No boot&lt;p&gt;. You get a Happy Mac, but then a warning window pops up saying System 7.1 cannot boot on this computer.&lt;/p&gt;&lt;p&gt;- System 7.5: &lt;/p&gt;BOOTS AND IS STABLE&lt;p&gt;. It requires you to hold shift to turn Extensions (and Control Panels / INITs) off, though, or to get rid of the "Mouse" Control Panel (and possibly more). The system is surprisingly stable! I tested the British version of this one, as Apple's Mac OS Anthology discs did not include the US installers, for some very slacker-y reason.&lt;/p&gt;&lt;p&gt;- System 7.5.2: &lt;/p&gt;Boots, but very broken, close to nothing works&lt;p&gt;. It could be because System 7.5.2 was always VERY machine-specific, and is apparently one of the most broken versions of Mac OS of ALL time, regardless. The machine-specific enablers, and other things, might be what is making it so unstable.&lt;/p&gt;&lt;p&gt;- System 7.5.3: &lt;/p&gt;BOOTS AND IS STABLE&lt;p&gt;. It requires you to hold shift to turn Extensions (and Control Panels / INITs) off, though, or to get rid of the "Mouse" Control Panel (and possibly more). The system is surprisingly stable!&lt;/p&gt;&lt;p&gt;- Mac OS 7.6: &lt;/p&gt;BOOTS AND IS STABLE&lt;p&gt;. Holding shift is not required here. What else can I say? It "works".&lt;/p&gt;&lt;p&gt;- Mac OS 8.1: &lt;/p&gt;BOOTS AND IS STABLE&lt;p&gt;. Holding shift is not required here, either. Behaves much the same as the others, except we now have HFS+ by default. Still, it did NOT like me having a 940 GB HFS+ partition, and prompted me to either eject it or format it. (To be fair, older OSes tried to do that, too, but Mac OS 8.1 was THE OS to _officially_ be able to handle HFS+ properly, so there are no excuses for it to fail here. Mac OS 9.2 ~ 9.2.2 all work perfectly with it.)&lt;/p&gt;&lt;p&gt;- Mac OS 8.5: No boot. Rather, it seems like it WOULD boot, but starting with Mac OS 8.5, Mac OS now always checks to see if the machine you are booting from is within a list of Apple-endorsed machine IDs for the given Mac OS version. In other words, Mac OS 8.5 does not know what the Mac mini G4 is, nor what a G4 Cube is (our Mac mini G4 ROM file makes the mini pretend to be the latter). It seems it should be possible to patch out the machine check. According to Rairii, this should be able to be patched out by disabling such a check on the "boot" resource in the Resource Fork of the System file, in ID 3 (also known as "boot3"). For Mac OS 8.6, it seems like this check happens at the end of boot3, wherever a check for machine ID 406 is located, in which after it's detected, the code checks to see if the exact Mac model is whitelisted or not.&lt;/p&gt;&lt;p&gt;- Mac OS 8.5.1: &lt;/p&gt;No boot&lt;p&gt;. All that applies to Mac OS 8.5 also applies to Mac OS 8.5.1.&lt;/p&gt;&lt;p&gt;- Mac OS 8.6: &lt;/p&gt;No boot&lt;p&gt;. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. See the top-left corner of the picture for a glitchy visual artifact. Same happens if you try to boot with Extensions off.&lt;/p&gt;&lt;p&gt;- Mac OS 9.0.4: No boot. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. Same happens if you try to boot with Extensions off. Exact same symptoms as when trying to boot Mac OS 8.6 at least on this mini model, including the visual artifact on the top-left corner.&lt;/p&gt;&lt;p&gt;- Mac OS 9.1: &lt;/p&gt;No boot&lt;p&gt;. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. Same happens if you try to boot with Extensions off. Exact same symptoms as when trying to boot Mac OS 8.6 and Mac OS 9.0.4 at least on this mini model, including the visual artifact on the top-left corner.&lt;/p&gt;&lt;p&gt;- Mac OS 9.2 ~ 9.2.2: BEST OS EVER, BOOTS AND RUNS BEAUTIFULLY. 'Nuff said.&lt;/p&gt;&lt;p&gt;Note that, although I describe many of these as "stable", I mean you can use much of it normally (sound/video/networking aside) without it crashing or misbehaving, at least not too hard, but that is not to say everything works, because that is just not the case. For example, when present, avoid opening the Apple System Profiler, unless you want a massive crash as it struggles trying to profile and gather all the information about your system. Some other apps or Control Panels might either not work, or work up to a certain point, after which they might freeze, requiring you to Force Quit the Finder to keep on going. And so on.&lt;/p&gt;&lt;p&gt;As you can see, I did not yet try System 7.5.5, Mac OS 7.6.1 and Mac OS 8.0. That's because they all are most likely working exactly as their neighbouring versions. But feel free to confirm.&lt;/p&gt;&lt;p&gt;Most non-mini systems should be able to boot Mac OS 8.6 ~ Mac OS 9.1 just fine. A "Mac OS 8.6 Enabler", so to speak, by LightBulbFun, can be renamed as e.g. "Sawteeth" and put inside the System Folder for some machines that cannot boot Mac OS 8.6 normally, so that they can, then, boot it. It is actually a Mac OS ROM file, but can function as a complementary, helper file to aid the actual Mac OS ROM file in this case. If you'd like, check &lt;/p&gt;here&lt;p&gt; for more info. I have attached "Sawteeth.bin" to this post for convenience. LightBulbFun first shared it on &lt;/p&gt;this post&lt;p&gt;, specifically through this &lt;/p&gt;MEGA link&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Most non-mini systems should also be able to boot Mac OS 8.5 and 8.5.1, especially on G3s and earlier. Some G4 Macs might need to spoof the Mac model in Open Firmware (or some other Forth script added to ROM) to boot, though, or patch the check out like I mentioned for the mini earlier. The reason the mini doesn't have the spoofing as an option is that any spoofing in OF would be overwritten by its own specialized Mac OS ROM, which spoofs a G4 Cube, which is clearly not in the whitelist of supported machines for Mac OS 8.5 and 8.5.1.&lt;/p&gt;&lt;p&gt;Also note that the mini behaves as reported above with Mac OS 8.6 with or without this "8.6 enabler" file (and with or without the System Enabler for Mac OS 7.6.1 and earlier, both of which don't seem to get in the way of later, nor earlier, OSes).&lt;/p&gt;&lt;p&gt;Most importantly, I did &lt;/p&gt;not&lt;p&gt; yet attempt to identify which are the latest versions of each Control Panel and Extension for each of these OSes. If I did, I'm sure it would help a lot, and perhaps address quite a number of these problems. The more people chime in on this effort, the better! Imagine if we had a proper "Mac mini G4 System 7.5.5" CD, then an "MDD Mac OS 8.5.1" CD, then an "iBook G3 Mac OS 7.6.1" CD, and so on. Everyone with a G3 or G4 Mac can help by trying things out!&lt;/p&gt;&lt;p&gt;Namely, something akin to MacTron's efforts highlighting the latest Extensions for Mac OS 9.2.2 and Mac OS 8.6 like this, but also for every other Mac OS version:&lt;/p&gt;========================================================&lt;lb/&gt;But how did you get the mini to boot? It requires its own special ROM!&lt;lb/&gt;========================================================&lt;p&gt;Indeed it does! All credit goes to ELN and all of those who helped him on Mac OS 9 Lives!: you can simply use his tooling (which was also very useful for Rairii) to re-apply the Mac-mini-G4-specific ROM patches to Rairii's latest 10.2.1 ROM, and voila! It works as well as you would hope it to! &lt;/p&gt;&lt;p&gt;You can even use the resulting ROM for Mac OS 9.2.2, as well, even though you don't have to: Originally, the Mac mini G4 ROM as we see them in RossDarker's Mac mini G4 CDs version 8 and 9 (AKA v8 and v9), as well as in all the previous versions, were based on the US ROM v9.6.1. I could not find an explanation as to why ROM v10.2.1 wasn't used in the end, even when digging the old Mac mini G4 thread again that started it all. Perhaps because we already had a working ROM with v9.6.1 and did not want to risk breaking anything, or who knows. However, I have thoroughly tested Mac OS 9.2.2 with this new ROM combination (latest Rairii 10.2.1 + latest Mac mini G4 patches AKA v9 patches), and from what I could tell, everything behaves &lt;/p&gt;exactly&lt;p&gt; the same as with the previous ROM we always used. Except now we have the ability to use the same ROM to also boot System 7.5 (I still can't believe this, even though it is true).&lt;/p&gt;&lt;p&gt;(For the record, while the 9.6.1 ROM was also modified to spoof the Mac mini G4 model identifier as a G4 Cube, we also tried to spoof it as a QuickSilver 2002 at one point, but someone reported sound issues with that, and so it was quickly changed back to a G4 Cube and such a change never made it into one of RossDarker's CDs. So just about everyone using Mac OS on the mini for all these years has had a ROM reporting to the OS as a G4 Cube, exclusively.)&lt;/p&gt;&lt;p&gt;To apply the Mac mini G4 patches, I used ELN's &lt;/p&gt;tbxi&lt;p&gt; and &lt;/p&gt;tbxi-patches&lt;p&gt; to apply his "macmini.py" script. You can follow the instructions as per the tbxi-patches page, which you should not let intimidate you even if you are not used to this kind of thing. It's quick and easy, and the scripts are also fully-commentated very nicely by ELN if you are curious about what it is doing and why.&lt;/p&gt;&lt;p&gt;In my case, first I tried using the latest Python 3.13.9 both from Windows 7 (bad idea due to resource fork loss) and macOS 10.14.6 Mojave, but neither worked: it seems like that version of Python was just too new. I then retried with &lt;/p&gt;Python 3.8.10&lt;p&gt; instead (which I chose thinking it might be more period-appropriate for the script's age) on Mojave, which worked &lt;/p&gt;flawlessly&lt;p&gt;. I didn't try it, but perhaps an older Python version might work on PowerPC OS X, as well.&lt;/p&gt;&lt;p&gt;I used the Python installer &lt;/p&gt;from the official website&lt;p&gt;, and I also used an "official" Git installer from &lt;/p&gt;here&lt;p&gt; (thus avoiding any package manager headache... man, how I hate non-Mac-OS systems, including OS X, and package managers in general...)&lt;/p&gt;&lt;p&gt;If somehow someone with plenty of Python knowledge and the willingness to put enough time into it wished to, both tbxi and tbxi-tools could, perhaps, be ported to &lt;/p&gt;MacPython 2.3.5&lt;p&gt;, so that we could do all this patching from Mac OS 9.2.2 directly and natively without leaving our main OS. That would also be awesome! (Of course, it helps that this is also available on more recent systems nonetheless, because then everyone gets to join in on the fun with all kinds of different backgrounds and setups.)&lt;/p&gt;&lt;p&gt;For convenience, I attached the final patched ROM to this post, so that anyone can go wild on their minis right away!&lt;/p&gt;========================================================&lt;lb/&gt;Why should I care when Mac OS 9.2.2 already boots, and runs better?&lt;lb/&gt;========================================================&lt;p&gt;It is also my opinion Mac OS 9.2.2 is the greatest OS, and Mac OS, ever, but not everything that is possible in earlier Mac OS versions is possible in Mac OS 9.2.2. For example, some software requires Mac OS 9.0.4 or earlier to work. A lot of software is System-7-exclusive.&lt;/p&gt;&lt;p&gt;Some people also just prefer the likes of System 7 for its even-lighter memory footprint, lack of mandated Appearance Manager and the like. Mac OS 9.2.2 is already overkill-fast on the mini, and on most New World ROM Macs, but the likes of System 7.5 are just RIDICULOUSLY fast. Even more ridiculously. I still am trying to come into terms with how indescribably fast using it on the mini was. It got even faster when I thought there was no way to get "faster than instantaneous", as Mac OS 9.2.2 always felt instantaneous like no other system already!&lt;/p&gt;&lt;p&gt;People might also have some other kind of reason and/or special attachment to an earlier OS version. Or maybe people want to explore older OS APIs and behaviors, perhaps even make a new application they want to know how it will behave on bare-metal not just on Mac OS 9, but also System 7 etc..&lt;/p&gt;&lt;p&gt;The value is in opening up the doors that give us, the users, more options that help us all out. &lt;/p&gt;========================================================&lt;lb/&gt;Final remarks&lt;lb/&gt;========================================================&lt;p&gt;Above all, thank you to everyone that made this possible. But I wanted to emphasize and give special thanks to Rairii for engineering all these ROMs, Mac84 for archiving and sharing all the CHRP discs, ELN for engineering all the Mac mini G4 ROM compatibility scripts and creating all the ROM and other Mac OS tooling, and to the Mac community at large everywhere that assisted in all of this into becoming reality. There's honestly many, many people to thank we owe over this one way or another, both in small and big ways.&lt;/p&gt;&lt;p&gt;I can't wait to see what people will do with all these new Mac OS versions on their New World ROM systems over the course of time!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://macos9lives.com/smforum/index.php?topic=7711.0"/><published>2025-11-29T03:26:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46085276</id><title>The Great Downzoning</title><updated>2025-11-29T09:36:29.132181+00:00</updated><content>&lt;doc fingerprint="b8e8909d3eb12efb"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;It was once legal to build almost anything, everywhere. Then, in the space of a few decades, nearly every city in the Western world banned densification. What happened?&lt;/head&gt;
    &lt;p&gt;In 1890, most continental European cities allowed between five and ten storeys to be built anywhere. In the British Empire and the United States, the authorities generally imposed no height limits at all. Detailed fire safety rules had existed for centuries, but development control systems were otherwise highly permissive.&lt;/p&gt;
    &lt;p&gt;Over the following half century, these liberties disappeared in nearly all Western countries. I call this process ‘the Great Downzoning’. The Great Downzoning is the main cause of the housing shortages that afflict the great cities of the West today, with baleful consequences for health, family formation, the environment, and economic growth. One study found that loosening these restrictions in just five major American cities would increase the country’s GDP by 25 percent. The Downzoning is one of the most profound and important events in modern economic history.&lt;/p&gt;
    &lt;p&gt;Subscribe for $100 to receive six beautiful issues per year.&lt;/p&gt;
    &lt;p&gt;The Great Downzoning happened during a period in which anti-density views were widespread among planners, officials, and educated people generally. Most people thought that urban density was unhealthy and dysfunctional, and supported government efforts to reduce it. It is natural to assume that this was why the Downzoning happened. Although there is surprisingly little literature on the Great Downzoning, historians who do discuss it often implicitly take this view, seeing it as of a piece with other anti-density measures taken by municipal governments across the West.&lt;/p&gt;
    &lt;p&gt;While there is undoubtedly some truth in this explanation, the evidence for it is surprisingly ambiguous. The Downzoning was extremely pervasive in existing suburbs, where it tended to raise property values by prohibiting kinds of development that were seen as undesirable. But in other contexts, it proved much harder to apply anti-density rules. In some European countries, ferocious battles were fought over whether municipal authorities should restrict the density of greenfield development. Doing so tended to reduce land values, prompting fervent resistance by rural landowners, who were generally successful in thwarting the proposed reforms. In the late twentieth century, planners and governments reversed their views on density, and became notionally committed to densification as a public policy objective. But they have had very limited success in reforming rules on suburban densification.&lt;/p&gt;
    &lt;p&gt;The general pattern is that the Great Downzoning was driven by interests more than by ideology. The Downzoning happened where it served the perceived interests of property owners, and failed to happen where it did not. Ideas-driven explanations of social changes are sometimes absolutely correct. But in this case, the correct explanation seems more materialist.&lt;/p&gt;
    &lt;p&gt;This has implications for the politics of housing reform today. In some places, anti-densification rules continue to raise property values, and in these places we should expect the Downzoning to be as politically robust as it has been for the last century: it really does give property owners something they want.&lt;/p&gt;
    &lt;p&gt;But in the great cities of the West, the housing shortage that the Downzoning has created may prove to be its undoing. Anti-density rules now reduce property values in these places rather than increasing them, and there is growing evidence that property owners opt out of such rules when they have the opportunity to do so. Winning the principled argument for density will not be enough to undo the Great Downzoning, because it never rested chiefly on principled arguments in the first place. But where the Downzoning is doing the most damage, it may now be possible to build new coalitions of interests in favor of reform.&lt;/p&gt;
    &lt;head rend="h3"&gt;The story of the Great Downzoning&lt;/head&gt;
    &lt;p&gt;In most European cities before the nineteenth century, elites were concentrated in city centers. Suburbs were unplanned, mixed use, and generally impoverished, a home to those people and businesses that were excluded from the urban core. Their inhabitants were powerless to resist densification, and often had little reason to do so anyway. The situation for suburbs was especially bad in continental Europe, where many cities retained massive fortifications until the nineteenth century that physically cut them off from their outskirts. Paris, Rome, Vienna, Milan, Madrid, and Barcelona are all examples of this.&lt;/p&gt;
    &lt;p&gt;These walls were often customs barriers as well as defensive ones – most Continental municipalities charged customs duties on goods entering the city until the late nineteenth or early twentieth century, and Paris’s excise duty lasted until 1943. This meant that municipal governments had an incentive to discourage economic activity in suburbs, through, for example, excluding their residents from membership of economically vital guilds.&lt;/p&gt;
    &lt;p&gt;More affluent suburbs did occasionally emerge, such as those that grew up around royal palaces and hunting lodges like Hampton Court and Saint-Germain-en-Laye, or along arterial roads like the Strand in London or the Bockenheimer Landstraße in Frankfurt. But even these were often developed haphazardly and generally remained quite mixed in social and economic terms.&lt;/p&gt;
    &lt;p&gt;A key step in the emergence of modern low-density suburbia was taken when developers began developing entire suburban neighborhoods, rather than just individual houses. This first became common in eighteenth-century Britain, probably because of Britain’s relatively deep capital markets and high rate of urban growth: developing a whole neighborhood involves huge outlays on laying out streets and amenities before any revenue starts to come in, so it is facilitated by low borrowing costs and confidence in future healthy sales.&lt;/p&gt;
    &lt;p&gt;Examples of early planned neighbourhoods include the West End of London, the Georgian extensions of Bath, the Bristol suburb of Clifton, and the Edinburgh New Town. These neighborhoods were still built with relatively high densities, so they are not exactly what we think of as suburbs today. But they were lower density than the urban core, as well as being exclusively or nearly exclusively residential, and exclusively upper-middle or upper class.&lt;/p&gt;
    &lt;p&gt;Suburbia really took off internationally in the nineteenth century, when planned suburbs spread across the British Empire, the United States, Germany, Austria-Hungary, and, to a lesser extent, France. The most universal and decisive factor behind this was probably again deepening capital markets and higher rates of urban growth. Other factors – none of which applied everywhere, but all of which were important in some places – included better roads, the development of suburban railways, buses and trams, improved policing, the abolition of customs boundaries around towns, the reform of feudal land tenures, and the demolition of city walls.&lt;/p&gt;
    &lt;p&gt;At the upper end of the market, densities quickly fell to levels similar to those of modern affluent suburbs. Many of the elite suburbs of this period are still famous neighborhoods today. American examples include Llewellyn Park in New Jersey, Forest Hills on Long Island, and Riverside outside Chicago. Examples from the British Isles include Rathmines in Dublin, Bedford Park in London, and Edgbaston in Birmingham. Some continental examples are Le Vésinet outside Paris, Pasing in Munich, or Westend in Berlin, named after London’s West End as a marketing gambit. Even small towns often had a tiny ‘villa district’, maybe just a couple of streets, like the Kingsland neighborhood of Shrewsbury. Only in the poorer countries of Mediterranean Europe did planned suburbs fail to catch on.&lt;/p&gt;
    &lt;p&gt;Social decline was common, even normal, for nineteenth-century neighborhoods, and homeowners lived in constant fear of it. Right from the start, suburb developers tried to safeguard neighborhood character through imposing covenants. This episode forms a fascinating prequel to the Great Downzoning, so much so that we might think of it as a ‘First Downzoning’ or ‘Proto-Downzoning’.&lt;/p&gt;
    &lt;p&gt;A covenant is a kind of legal agreement in which the homebuyer agrees to various restrictions on what they can do to their new property. Covenants generally forbade nearly all non-residential uses, as well as forbidding subdivision into bedsits or flats. They frequently imposed minimum sales prices, and in the United States, they often excluded sale or letting to non-white people. In all countries, they often included explicit restrictions on built density. Most covenants were intended to ‘run with the land’, binding not only the initial buyer but all subsequent ones too.&lt;/p&gt;
    &lt;p&gt;Here are the rules binding homeowners in Grunewald, Berlin’s premier suburb, reading like thousands of other similar documents before and since:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a) buildings may not be constructed higher than three storeys inclusive of the ground floor; b) all buildings must be provided with [ornamented] facades on all sides; c) at most two houses may be built conjoined to each other; otherwise there must always be a gap of at least eight meters between buildings, for which exception may be made only in the case of covered walkways; d) a fenced front garden of at least four metres in depth must be retained between any building and the street.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Covenants were extremely widespread. Although rigorous quantitative studies do not exist, my impression from wide reading is that all elite planned suburbs were covenanted, along with many middle-class ones. They were used in all English-speaking countries, and similar mechanisms existed in France (servitudes in cahiers des charges), Germany (Grunddienstbarkeiten), the Low Countries (erfdienstbaarheden), and elsewhere (e.g. Italy, Spain, Scandinavia). Under Japanese law at the time, covenants were legally unenforceable, but the idea was so appealing that Meiji-era suburban developers sometimes imposed them anyway, apparently as a purely moral inducement to conformity.&lt;/p&gt;
    &lt;p&gt;Covenants became more elaborate over time, and by the early twentieth century they sometimes included provisions on such matters as where laundry could be hung and what colours joinery could be painted in.&lt;/p&gt;
    &lt;p&gt;Developers would not have imposed covenants if they had not expected them to increase the value of neighborhoods, so their pervasiveness reveals a widespread demand for development control among nineteenth-century people. But they were not very effective. One problem concerned whether courts would enforce them. To secure the development in perpetuity, covenants had to apply not only to the initial homebuyer, but to all future ones – people with whom the developer would never have any direct dealings, and who might indeed live long after the developer’s death. It was legally impossible for normal contracts to do this, so alternative mechanisms had to be employed.&lt;/p&gt;
    &lt;p&gt;Continental countries generally used the ancient concept of servitudes from Roman law, but common-law jurisdictions had to rely on the system of equity developed in the English Court of Chancery. The problem with this was that courts varied greatly in which restrictions they considered equitable, creating a system fraught with risk and unpredictability for developers and homeowners. Although there was a general tendency for covenants to become better established in law as the nineteenth century went on, there remained much uncertainty about exactly what development rights could be restricted, who had standing to enforce against infringements, and when restrictions could be discharged (voided). In fact, the law on this is still hazy today.&lt;/p&gt;
    &lt;p&gt;Another problem with covenants was that they could not be modified retroactively, meaning that any flaws or loopholes were unfixable. This could prove disastrous for covenanters. For example, as already mentioned, many covenants included minimum price thresholds. These were normally given in nominal figures, which worked fine in the nineteenth century because there was no inflation. After 1914, however, inflation took off, swiftly making the thresholds trivially easy to meet. There was no way to insert inflation-adjustment clauses retroactively, so one of the pillars of nineteenth-century covenanting effectively vanished. For example, one Edwardian covenant stipulated that no dwelling worth less than £375 be built on the plot. To achieve the same exclusionary effect in 1920, the corresponding figure would have been £1,030. Since the number could not be increased retroactively, the covenant became effectively useless.&lt;/p&gt;
    &lt;p&gt;A third problem concerned the costs of enforcement. Covenants fall under private law: breaking one is not a crime, and the state will not prosecute it. Enforcement thus requires a private lawsuit, which was and is expensive. Today, a simple case will cost at least £25,000 in Britain, while a complex one can cost £60,000. Costs in the USA are similar or higher. Historical costs of litigation varied but were notoriously high. Developers were often willing to bear these expenses as long as they still had plots elsewhere in the development to sell, but once the entire neighborhood had been sold off, they usually lost any interest in policing its built form.&lt;/p&gt;
    &lt;p&gt;In theory, the covenants would subsequently be enforced by affected neighbors under a system known as ‘reciprocal enforcement’. In practice, however, this was beleaguered by free-rider problems, with no one resident willing to bear the costs of enforcement alone. Mechanisms eventually developed for pooling enforcement costs in some places, like the famous homeowner associations in the United States. But the overall bill remained high, meaning that covenant enforcement tended to be haphazard in any but the most affluent neighborhoods.&lt;/p&gt;
    &lt;p&gt;The upshot of all this was that covenants were usually a weak kind of development control, which disintegrated upon contact with serious demand for densification. An example of this is the Berlin suburb of Friedenau, originally developed in the late nineteenth century as what the Germans called a ‘villa colony’, an elite suburb of large detached houses.&lt;/p&gt;
    &lt;p&gt;Friedenau was originally built some way from the edge of Berlin, but the urban core expanded rapidly and reached Friedenau during the 1880s. Friedenau’s restrictions proved completely ineffective and the entire neighborhood was redeveloped with large blocks of flats. Only a handful of villas endured long enough to be protected by later conservation laws, surviving today as a curious reminder of Friedenau’s original form.&lt;/p&gt;
    &lt;p&gt;The stage was set for the Great Downzoning proper, when suburban density restrictions were introduced by public authorities. This began in the final years of the nineteenth century in Germany and Austria-Hungary. The key innovation was ‘differential area zoning’, whereby different areas within a given jurisdiction were subjected to different building restrictions. This allowed for development controls to be applied to suburban areas that would keep them at suburban densities without having the absurd side effect of applying suburban density restrictions to dense city centers.&lt;/p&gt;
    &lt;p&gt;After a couple of decades of experimentation, the 1891 Frankfurt zoning code caught the imagination of municipal governments across Central Europe. It was swiftly emulated. By 1914 every German city had a zoning code, and many had gone through multiple iterations, usually with progressively lower densities. In existing elite suburbs, these zoning codes tended to effectively duplicate the content of developers’ covenants, but because they had a stronger legal basis and were enforced by the state, they were far more effective.&lt;/p&gt;
    &lt;p&gt;For example, Grunewald’s zoning designation in the first decade of the twentieth century permitted two storeys plus a roof storey and basement, banned everything except detached and semi-detached buildings, and required a four-meter setback from the street – much the same as the covenant quoted above. The zoning code remains similar today, and has successfully preserved Grunewald as Berlin’s premier villa colony.&lt;/p&gt;
    &lt;p&gt;The example of Germany and Austria was quickly followed abroad. The Netherlands introduced a kind of zoning system in 1901. Italian cities began to follow suit before the First World War. Japan began to introduce a zoning system nationally in 1919, albeit one that continued to permit fairly high densities. Poland introduced a national system in 1928. American and Canadian cities started introducing zoning systems in the 1910s, which became widespread in the course of the interwar period. Zoning provisions began to be introduced in interwar Australia and were consolidated in the 1940s.&lt;/p&gt;
    &lt;p&gt;Britain and France followed relatively late: although they introduced planning systems of a sort in 1909 and 1919, respectively, these had limited effectiveness, and robust national systems were not introduced in either country until the 1940s. In broad terms, the Great Downzoning was in place by the 1950s, though density restrictions continued to be tightened in the following decades in many countries.&lt;/p&gt;
    &lt;p&gt;There are some limitations on the spread of the Great Downzoning, which we will explore in the next section. In many ways, though,the Downzoning was remarkably thorough. Virtually every wealthy suburb that existed in 1914 retains its suburban character today. Long ago, too, the Downzoning spread beyond the homes of the elites. When Central European cities began introducing zoning in the 1890s, suburbia was still largely the preserve of their upper-middle and upper classes. Today, a great part of the working and middle classes of all Northwest European countries and of North America live in suburbs, and they too enjoy the ambiguous blessing of the Downzoning’s protection. Wherever planned residential suburbs of owner-occupiers develop, it seems, the Downzoning has ultimately followed.&lt;/p&gt;
    &lt;head rend="h3"&gt;An idealist explanation for the Great Downzoning&lt;/head&gt;
    &lt;p&gt;At the time of the Great Downzoning, a negative view of the cities of the nineteenth century was extremely common. Frank Lloyd Wright described the cities of his day as a ‘conspiracy against manlike freedom’, a ‘disease of the spirit’, and a ‘senseless reiteration of insignificance’. Werner Hegemann, a prominent German urbanist who later wrote the United States’s first suburban zoning code in Berkeley, described Berlin’s urban fabric as comprised of ‘Dwellings so bad that neither the stupidest devil nor the most diligent speculator could have devised anything worse’. Le Corbusier claimed that ‘The nineteenth century has made the house into a ridiculous, revolting, and dangerous thing’ and observed that ‘We are living in a dustbin … in a kind of scum choked by its own excretions.’&lt;/p&gt;
    &lt;p&gt;These views spread far beyond architectural and planning elites, to the point that a negative view of nineteenth-century urbanism became one of the standard background opinions of educated people. To give one striking example, Lord Rosebery, chairman of the London County Council and thus the closest thing that existed to a mayor of London, said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There is no thought of pride associated in my mind with the idea of London. I am always haunted by the awfulness of London […] Sixty years ago a great Englishman, Cobbett, called it a wen. If it was a wen then, what is it now? A tumour, an elephantiasis sucking into its gorged system half the life and the blood and the bone of the rural districts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Both professionals and laypeople tended to see lowering residential densities as part of the solution. Planners often converged on twelve dwellings per acre (30 per hectare) as a good upper limit. In Britain, the famous urbanist Raymond Unwin promoted the slogan ‘twelve houses to the acre’ as the norm for residential areas. Ebenezer Howard also advocated twelve dwellings per acre in Garden Cities of Tomorrow, perhaps the most influential planning text of modern times. The influential American planner John Nolen adopted the same figure, arguing that ‘there must be a limitation of houses to not more than twelve per gross acre’. Josef Stübben, whose textbook Der Städtebau was the standard authority on urban planning in German-speaking Europe, advocated twelve dwellings per acre in most contexts, though he allowed for somewhat higher densities in central areas.&lt;/p&gt;
    &lt;p&gt;Public policy reflected this view in a range of ways. In all countries, public transport was subsidized and price controlled with the explicit aim of fostering urban diffusion. In Britain, the 1918 Tudor Walters Committee set a standard of twelve dwellings per hectare for social housing. This benchmark remained influential for many decades, and local councils often succeeded in meeting it.&lt;/p&gt;
    &lt;p&gt;In the United States, the Federal Government began to conditionalize mortgage support on densities of 4-8 dwellings per acre, while conceding that this might rise to 12-16 dwellings per acre in central areas. The French and Belgian governments sponsored an extensive program of garden cities in the interwar period, aspiring to similarly low densities, though not always reaching them. In Germany, the Weimar government extended subsidies for single-family houses with private gardens under the 1920 Reichsheimstättengesetz (Reich Homestead Act). The Nazis continued these in their own Reichsheimstättengesetz in 1937, illustrating how the aim of urban diffusion was shared between otherwise radically different political movements.&lt;/p&gt;
    &lt;p&gt;The other instrument that planners used to achieve low densities was, of course, zoning. All contemporary written justifications for suburban low-density zoning appealed to these background anti-density views, and virtually any of the planning officials who worked on early zoning plans would have seen their work as justified by such considerations. So part of the explanation for the Great Downzoning is very simple: it happened because it was seen as an obvious way of achieving an uncontroversial public policy objective.&lt;/p&gt;
    &lt;p&gt;But this cannot be the whole explanation. When existing suburbs were downzoned, the new rules merely confirmed the densities that market forces had already chosen for the neighborhood. Indeed, as we shall discuss in the next section, brownfield downzoning almost certainly tended to increase land values by protecting neighborhoods from blight. In such cases, then, planners were simply going with the grain of property owners’ interests. In places where planners’ priorities and property owners’ interests were not so aligned, planners’ success was far more doubtful.&lt;/p&gt;
    &lt;p&gt;A vivid example of this is the attempt of planners to lower the density of greenfield development (new neighborhoods on previously undeveloped land). In Anglophone countries, the density of greenfield development was already fairly low by the early twentieth century, and there was not much for planners to do in lowering it further. In continental countries, however, much greenfield development still took the form of densely massed apartment blocks, which were seen by planners and officials as a shameful humanitarian disaster. Lowering these densities was widely seen as just as much of a priority as protecting existing suburbs, and in many countries it dominated public debate about zoning.&lt;/p&gt;
    &lt;p&gt;The problem was that, unlike in existing suburbs, downzoning greenfield sites generally reduced their value. Developers built dense apartment blocks because, given prevailing market conditions, that was the most valuable use of the land. Requiring them to build terraced houses or cottages instead crashed land values and annihilated the asset wealth of landowners. Planners and municipal officials thus faced a powerful special interest group, against which they had great difficulty in prevailing.&lt;/p&gt;
    &lt;p&gt;The classic illustration of this is Rome. Like most Mediterranean cities, Rome had not really developed planned low-density suburbs in the nineteenth century, but Italian planners shared the contemporary belief that public policy should promote lower densities. In 1907, a coalition of liberals and socialists won the municipal elections under the leadership of Ernesto Nathan, breaking the longstanding hold of the landowning interests over the city’s government. The coalition prepared a zoning plan that aimed at making Rome’s urban extensions into international models of good practice, by the standards of the day.&lt;/p&gt;
    &lt;p&gt;The 1909 zoning plan for Rome was radical. The red-shaded areas still allowed traditional courtyard blocks, but at lower densities than before. The green-shaded areas allowed only ‘villini’, small detached apartment buildings of no more than three storeys, covering a maximum of half of their block area. The huge areas with green outline and white infill are marked for ‘giardini’, literally ‘gardens’. Only 1/20th of the plot area in giardini areas could be built over, a density that would count as low even by modern American standards.&lt;/p&gt;
    &lt;p&gt;The affected land was mostly owned by the ‘black nobility’, the traditional Roman ruling class (black was a symbol of mourning for the Papal government that had ruled Rome before its annexation by the Kingdom of Italy). The black nobility was appalled at the loss of land value that Nathan’s downzoning had wrought, and they embarked on a long campaign to reverse it. In 1913, an alliance of Catholic and nationalist parties won the municipal elections and loosened the zoning restrictions. Then in 1923 Mussolini seized power, dissolved Rome’s municipal government, and appointed a black noble as governor. For many decades thereafter, higher densities were permitted on many Roman greenfield sites than in the urban core.&lt;/p&gt;
    &lt;p&gt;Rome’s story is particularly dramatic, but the basic pattern is typical of Southern Europe. Planners in Spain, Portugal, Italy, and Greece generally shared the standard European aspiration towards lower densities, but they had few existing planned suburbs to defend: the only possible downzoning would be on greenfield land, suppressing density in new urban areas. This would run contrary to the interests of the landowners.&lt;/p&gt;
    &lt;p&gt;In all four countries, this failed to happen, and urban densities remained stubbornly high, only falling gradually in the late twentieth century under the influence of market forces. Today, these cities may seem like rather remarkable survivals of semi-traditional urban forms, but they were generally not so understood by contemporaries: Southern European writers in the twentieth century generally saw them as obvious urbanistic failures, the product of avaricious landowners and weak, corruptible states.&lt;/p&gt;
    &lt;p&gt;The story was initially similar in Germany and France. German planners made strenuous efforts to downzone greenfield sites before the First World War, but met with fierce resistance from landowners. In general, the landowners were successful in preserving their right to build apartment blocks, although they sometimes had to include larger courtyards and front gardens. In France, the planning movement was much weaker, and made little progress against landowner and developer interests. The maximum densities permitted in Paris actually increased in 1884 and 1902.&lt;/p&gt;
    &lt;p&gt;A kind of greenfield downzoning did later happen in France and Germany, but its story is a strange one, and the anti-density views of planners played no role in it. In 1914, both countries introduced tight rent controls to protect the families of soldiers from eviction (Britain followed a year later). As so often in the history of rent control, these rules proved difficult to lift when the crisis that had occasioned them was over. Rent controls persisted in both France and Germany throughout the interwar period and long into the second half of the twentieth century. Coupled with high inflation, this meant that the real value of rents rapidly fell.&lt;/p&gt;
    &lt;p&gt;This undermined the build-to-rent sector, because the rental value of apartments was generally no longer great enough to cover their build cost. Neither country had a well-developed system for selling buildings into multiple ownership: the modern French and German equivalents of condominium ownership, copropriété and Wohnungseigentum, only developed later in the twentieth century. The remarkable effect of this was that there was generally no way to build flats profitably, resulting in the collapse of the private flat-building sector. The surviving private builders switched over to building small houses for owner occupation, beginning the vast low-density suburbs with which German and especially French cities are surrounded today.&lt;/p&gt;
    &lt;p&gt;We are confronted, then, with a striking contrast: nearly total success in downzoning existing suburbs, and nearly total failure in downzoning greenfield development. This contrast casts doubt on the idea that the downzoning was driven by the will of planning elites.&lt;/p&gt;
    &lt;p&gt;Another context in which planners struggled to lower or even cap densities was in city centers. Many American city centers declined in the decades after the Second World War due to rising crime and traffic congestion, while densification was prevented in some European centers by architectural conservation laws. But in places where neither of these factors applied, densification of city centers continued apace, reaching some of the highest floorspace densities ever attained. Many Australian and Canadian cities are particularly clear examples of this, though there are also cases elsewhere. Again, this is puzzling for the ideas-driven theory of the Great Downzoning: in places which lacked an owner- occupier lobby for restrictions on densification, planning ideology seems to have been ineffective.&lt;/p&gt;
    &lt;p&gt;What happened at the end of the twentieth century is no less problematic for the planner-driven explanation of the Great Downzoning. From the 1960s onwards, the intellectual tide began to turn in favor of density, and by the 1990s, density was wildly fashionable again.&lt;/p&gt;
    &lt;p&gt;I once worked as the research assistant to a British government commission on the built environment, in the course of which I had the unenviable task of reading every major official document on British urban policy since the 1990s. From Richard Rogers’s Towards an Urban Renaissance (1999), through the Urban Design Compendium of English Partnerships and the Housing Corporation (2000) and the Commission for Architecture and the Built Environment’s By Design: Urban Design in the Planning System (2000), to the Greater London Authority’s Defining, Measuring and Implementing Density Standards in London (2006) and the Farrell Review of Architecture and the Built Environment (2014), they were united in praising urban density. Government documents like Planning Practice Guidance Note 3 (2000), Planning Policy Statement 3 (2006), and the National Planning Policy Framework (2012) endorsed and besought it. Every planning school in Britain teaches its students the importance of density, walkability, and mixed use.&lt;/p&gt;
    &lt;p&gt;This was not just empty talk. There have been huge increases in the population of virtually every British city centre since the 1990s, enabled and fostered by a range of public programmes. In 1990, fewer than 1,000 people lived in Central Manchester. Today, around 100,000 people do. But virtually none of this increase has taken place in private suburbs.&lt;/p&gt;
    &lt;p&gt;Instead, it has been concentrated in former industrial or logistics sites, in city-centre commercial areas, or in social housing, which the authorities regularly demolish and rebuild at greater densities. Towns without much of this, like Oxford and Cambridge, have stable or even declining populations in their city centres. An effort to enable more suburban densification nationally in the 2000s aroused much controversy and was soon abandoned. A more recent attempt to allow more densification in an area of South London, widely praised by planners, led to a local political revolution and the policy’s revocation.&lt;/p&gt;
    &lt;p&gt;This is not a British peculiarity. All over the West, urban density is valued by planners and officials. Governments pursue it, and have had some success in enabling it in industrial and commercial areas and through the redevelopment of public housing. In the United States, densification is the central theme of a vast YIMBY movement. But progress on densifying owner-occupier suburbs has been extremely limited, and the vast suburbs of the nineteenth and twentieth centuries remain almost untouched. The unified opinion of the planning and policy elites has proved ineffective in the face of homeowner opposition. If the idealist theory were the whole truth, and the Downzoning was purely the creation of planners, this would be extremely strange.&lt;/p&gt;
    &lt;head rend="h3"&gt;A materialist explanation of the Great Downzoning&lt;/head&gt;
    &lt;p&gt;The alternative theory is that the Downzoning was driven less by elite ideas than by the way that the Downzoning served the perceived interests of homeowners. This theory fits better with the evidence.&lt;/p&gt;
    &lt;p&gt;When people buy a home, they care not only about the home itself, but about the neighborhood in which it stands. This was why nineteenth-century developers started building whole villa colonies and streetcar suburbs rather than just individual houses: by developing entire neighborhoods, they could satisfy a wider range of buyers’ preferences, giving people the neighborhood of their dreams rather than just the house.&lt;/p&gt;
    &lt;p&gt;All else being equal, many people prefer neighborhoods built at low densities. Some of the perceived advantages of low density will apply virtually anywhere, like quieter nights, greener streets, more and larger private gardens, and greater scope for social exclusivity. Other attractions are more specific to certain contexts. Where urban pollution is bad, people seek suburbs for cleaner air. Where crime is high, suburbs are often seen as a way of securing greater safety. In eras with high levels of racism and increasing racial diversity, people moved to suburbs to secure racial homogeneity.&lt;/p&gt;
    &lt;p&gt;Restrictions on densification were a way of preserving these ‘neighborhood goods’ in perpetuity. The prevalence of covenanting constitutes extremely strong evidence that suburban people wanted this. Covenants were imposed by developers, whose only interest was in maximizing sales value. They judged that the average homebuyer valued the neighborhood goods that covenants safeguarded more than they valued the development rights that covenants removed. The ubiquity of covenants shows that, under nineteenth-century market conditions, density restrictions were generally desired by suburban residents. As we have seen, however, covenants were not very effective. The fact that public zoning followed under these conditions is not greatly surprising: it gave suburban people something they demonstrably wanted, and were not able to secure without the help of public authorities.&lt;/p&gt;
    &lt;p&gt;One question we might ask about this theory is: why did the Great Downzoning happen when it did, as opposed to at some earlier point in history? The answer is simple: it happened because of the emergence of planned suburbia in the preceding century. The whole point of planned suburbs was that they provided neighborhood goods like exclusivity and amenity: this was what made the large upfront costs of developing a neighborhood worthwhile. The impoverished peripheries of medieval and early modern cities may have had some of these goods by accident – presumably they were greener than medieval city centers, for example – but they would not have had many of them, because they had no way of excluding noxious land uses and ‘undesirable’ people. Many were regarded as dangerous and blighted places, where nobody would live if they had any alternative. Until the nineteenth century, suburban people frequently did not stand to lose much from densification.&lt;/p&gt;
    &lt;p&gt;The inhabitants of planned suburbia had some obvious political advantages over their predecessors, too. They were relatively affluent, which was an advantage for lobbying purposes in 1890-1950 as in all times and places. They were also extremely homogeneous, in the sense that most neighborhoods were planned for exclusively residential use by people of a given social class. This meant that their interests were likely to be aligned, and they could form a united front to campaign for their community’s interests.&lt;/p&gt;
    &lt;p&gt;One other feature of planned suburbs deserves mention, which is home ownership. In nearly all countries, planned residential suburbs were predominantly sold into owner occupation. If you are renting a property and its neighborhood declines in amenity, your economic loss is low: neither your income nor your asset value is affected, and if you move to another neighborhood with better amenity, your only economic loss is the cost of relocation. If you are a homeowner, the loss of neighborhood values destroys your wealth, as embodied in your property value. So you are much more invested in these neighborhood values, and probably more likely to fight for them.&lt;/p&gt;
    &lt;p&gt;A second question is why the Downzoning began in Central Europe, rather than in, say, England, where suburbia had existed for much longer. I offer three possibilities.&lt;/p&gt;
    &lt;p&gt;One possible explanation is varying state capacity, together with varying tolerance of state intervention. The princely states in Germany had been relatively activist in city planning throughout the early modern period: a number of German cities had planned street networks, and the authorities sometimes even micromanaged details of facades, as in Potsdam. In Berlin, the authorities enforced a minimum height limit because they felt that low-rise buildings gave the royal capital a ridiculously countrified appearance.&lt;/p&gt;
    &lt;p&gt;In the nineteenth century, the German state became much more activist than the French or Anglophone ones in a whole range of areas, pioneering mandatory health insurance, old age pensions, universal compulsory education, and a range of labor regulations. It is plausible that this attitude to government made zoning a more natural intervention for Germans than it was elsewhere.&lt;/p&gt;
    &lt;p&gt;Another factor was that pressure for densification was greater. As we have seen, Central European cities at the end of the nineteenth century had a strikingly clear distinction between a dense urban core and low-density villa colonies. When the expanding dense core reached a villa colony, the villa colony was faced with total transformation.&lt;/p&gt;
    &lt;p&gt;No other country had quite such a neat dichotomy. France did not have elite suburbs to the same extent, while Italy and Spain hardly had them at all. Anglophone cities had few purpose-built flats until the late nineteenth century, even in the urban core, and the ‘mansion blocks’ and ‘co-ops’ that then started to emerge tended to be targeted at the middle classes rather than the poor, alleviating one motive for exclusion. Perhaps, then, the German villa colonies were exposed to a form of densification that was particularly alarming to their residents. The late introduction of density controls in Britain and France may also be explained in this way: as we have seen, rent controls stymied flat building in the interwar period, thus relieving the political demand for restrictions on densification.&lt;/p&gt;
    &lt;p&gt;A third factor is that private sector density controls were probably varyingly effective in different countries. In Britain, most suburbs were developed under the unique ‘Great Estates’ system. Instead of selling homes outright, they sold extremely long leases of eighty or a hundred years, after which the properties reverted to the original landowner, called a ‘Great Estate’. The Great Estates thus retained an interest in safeguarding neighborhood goods in order to preserve the reversionary value of the properties. They thus acted as a form of quasi-government, enforcing against covenant breaches much more effectively than neighbors usually would. Some even provided local public services like parks and sanitation. As noted above, some other countries eventually developed systems somewhat analogous to this, like American homeowner associations. But in Britain they existed right from the start, and were ubiquitous. It is plausible that this contributed to the relatively lower demand for public density controls in Britain, in spite of Britain’s precocious suburbanization.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Great Upzoning?&lt;/head&gt;
    &lt;p&gt;One element of the preceding argument may have puzzled some readers. I have argued that density controls were originally imposed because they increased property values, suggesting that allowing densification is net value destroying. But many housing reformers, including me, have argued that granting additional development rights to streets or neighborhoods increases their value, for the obvious reason that the additional floorspace is worth a lot. This has been confirmed by recent examples. For example, residents of the London neighborhood South Tottenham recently persuaded their local councils to let them double the height of their houses. All properties in the neighborhood enjoyed an immediate boost in value once the council agreed.&lt;/p&gt;
    &lt;p&gt;In South Korea, some neighborhoods are allowed to vote for much larger increases in development rights. This generates abundant value uplift, as a result of which residents of such neighborhoods nearly always vote in favor. In Israel, apartment dwellers can vote to upzone their building: this has proved so popular that half of the country’s new housing supply is now generated this way. How can such cases be reconciled with the argument I have given here?&lt;/p&gt;
    &lt;p&gt;The answer lies in how the housing market has changed since the nineteenth century. Over the last century, in large part because of the Downzoning, housing shortages have emerged in many major cities, in the sense that floorspace there sells for much more than it costs to build. This means that the development rights lost through density controls have become steadily more valuable. At a certain point, their summed value became greater than that of the neighborhood values for which they had been sacrificed. It was at this point that they became value-destroying.&lt;/p&gt;
    &lt;p&gt;It is also at this point that opportunities for innovative reforms like the one in South Tottenham start to emerge, because existing residents would now be net economic beneficiaries of allowing greater densities. The Downzoning was originally extremely desirable to residents, because the neighborhood goods it secured were more valuable than the floorspace it precluded. In places where that is no longer true, we should be cautiously optimistic about the prospects of reform. This is the reasoning behind proposals like street votes, which would allow individual streets or blocks to vote by qualified majority to upzone themselves to higher densities.&lt;/p&gt;
    &lt;p&gt;It is important to stress, though, that this is not true everywhere. Although housing shortages exist in nearly all Western countries, they do not exist in all parts of those countries. In fact, they are heavily concentrated in a small number of major cities. In most of the United States, sales prices are generally not far above the physical costs of construction: only in a handful of the major cities, like New York and San Francisco, are they consistently substantially higher.&lt;/p&gt;
    &lt;p&gt;In Britain, the housing shortage is heavily concentrated in the South East, with prices fairly near build costs in much of the rest of the country. In France, a large divergence has emerged only in Paris and certain areas popular with wealthy holidaymakers. Similar results are evident elsewhere in continental Europe, Australia, and Canada. In much of the West – probably the majority of its urban area – market conditions are not fundamentally altered since the nineteenth century, and density controls probably still maximize property value.&lt;/p&gt;
    &lt;p&gt;The fact that something maximizes property value need not make it morally good. Property value is determined by preparedness to pay. It may be maximized by preserving the view from a billionaire’s spare bedroom rather than by providing housing for a thousand destitute people, or by fulfilling the exclusionary preferences of snobs and racists. This means that there may be strong arguments for zoning reform even in places where its net effect on the value of individual neighborhoods would be negative.&lt;/p&gt;
    &lt;p&gt;Politically, however, reforms that reduce particular people’s property values are likely to be much more difficult. For more than a century, there has been an overwhelming tendency for residential suburbia to secure protections for itself against densification. We saw in the last section that this is hardly surprising: it is almost as though this neighborhood type had been designed to generate exactly this political outcome. The history of the Downzoning suggests it is very hard to triumph against the united interests of suburban homeowners.&lt;/p&gt;
    &lt;p&gt;The political upshot of this history, then, is that reform efforts should be focused. Making the principled case for density is useful, but unlikely to be sufficient: principled argument did not make the Downzoning, and it probably won’t unmake it either. Instead, campaigners should consider ways in which the changing structure of homeowners’ interests can be mobilized in the cause of reform. The examples of South Tottenham, Seoul, and Tel Aviv suggest that homeowners may be vigorous in pursuit of upzoning when they realize how much they stand to benefit from it. There is no reason why this could not be replicated elsewhere. Across the major cities of the West, homeowners are sitting unwittingly on one of the greatest mines of potential wealth in the history of the world. Once they notice, their actions may amaze us all.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://worksinprogress.co/issue/the-great-downzoning/"/><published>2025-11-29T05:00:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46085344</id><title>The 'S&amp;P 493' reveals a different U.S. economy</title><updated>2025-11-29T09:36:28.929155+00:00</updated><link href="https://www.msn.com/en-us/money/markets/the-s-p-493-reveals-a-very-different-us-economy/ar-AA1R1VUJ"/><published>2025-11-29T05:22:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46085585</id><title>Garfield's Proof of the Pythagorean Theorem</title><updated>2025-11-29T09:36:28.786772+00:00</updated><content>&lt;doc fingerprint="cf79536f86bd73e1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Garfield's proof of the Pythagorean theorem&lt;/head&gt;&lt;p&gt;Garfield's proof of the Pythagorean theorem is an original proof of the Pythagorean theorem discovered by James A. Garfield (November 19, 1831 – September 19, 1881), the 20th president of the United States. The proof appeared in print in the New-England Journal of Education (Vol. 3, No.14, April 1, 1876).[1][2] At the time of the publication of the proof Garfield was a congressman from Ohio. He assumed the office of President on March 4, 1881, and served in that position until his death on September 19, 1881, having succumbed to injuries sustained when he was shot in an assassination in July.[3] Garfield is thus far the only President of the United States to have contributed anything original to mathematics. The proof is nontrivial and, according to the historian of mathematics William Dunham, "Garfield's is really a very clever proof."[4] The proof appears as the 231st proof in The Pythagorean Proposition, a compendium of 370 different proofs of the Pythagorean theorem.[5]&lt;/p&gt;&lt;head rend="h2"&gt;The proof&lt;/head&gt;[edit]&lt;p&gt;In the figure, is a right-angled triangle with right angle at . The side-lengths of the triangle are . Pythagorean theorem asserts that .&lt;/p&gt;&lt;p&gt;To prove the theorem, Garfield drew a line through perpendicular to and on this line chose a point such that . Then, from he dropped a perpendicular upon the extended line . From the figure, one can easily see that the triangles and are congruent. Since and are both perpendicular to , they are parallel and so the quadrilateral is a trapezoid. The theorem is proved by computing the area of this trapezoid in two different ways.&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dd-1"&gt;.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;From these one gets&lt;/p&gt;&lt;p&gt;which on simplification yields&lt;/p&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ G., J. A. (1876). "PONS ASINORUM". New England Journal of Education. 3 (14): 161. ISSN 2578-4145. JSTOR 44764657.&lt;/item&gt;&lt;item&gt;^ Kolpas, Sid J. "Mathematical Treasure: James A. Garfield's Proof of the Pythagorean Theorem". maa.org. Mathematical Association of America. Archived from the original on 8 December 2023. Retrieved 29 November 2023. (The article appeared in the peer-reviewed online journal Convergence published by the Mathematical Association of America.)&lt;/item&gt;&lt;item&gt;^ Del Arte, Alonso (February 2019). "A future president once published a mathematical proof". medium.com. Retrieved 29 November 2023.&lt;/item&gt;&lt;item&gt;^ Dunham, William (1994). The Mathematical Universe: An Alphabetical Journey Through the Great Proofs, Problems, and Personalities. New York: John Wiley &amp;amp; Sons. p. 99. ISBN 0-471-53656-3.&lt;/item&gt;&lt;item&gt;^ Loomis, Elisha Scott (1940). The Pythagorean Proposition (2 ed.). Washington DC: National Council of Teachers of Mathematics. p. 109. ISBN 978-0-87353-036-1. Retrieved 28 November 2023. &lt;code&gt;{{cite book}}&lt;/code&gt;: ISBN / Date incompatibility (help) (A collection of 370 different proofs of the Pythagorean theorem.)&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Garfield%27s_proof_of_the_Pythagorean_theorem"/><published>2025-11-29T06:37:05+00:00</published></entry></feed>