<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-02T16:12:36.643214+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45095475</id><title>The future of 32-bit support in the kernel</title><updated>2025-09-02T16:12:46.453285+00:00</updated><content>&lt;doc fingerprint="fc43991fd8863dfd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The future of 32-bit support in the kernel&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Arnd Bergmann started his Open Source Summit Europe 2025 talk with a clear statement of position: 32-bit systems are obsolete when it comes to use in any sort of new products. The only reason to work with them at this point is when there is existing hardware and software to support. Since Bergmann is the overall maintainer for architecture support in the kernel, he is frequently asked whether 32-bit support can be removed. So, he concluded, the time has come to talk more about that possibility.&lt;/p&gt;
    &lt;p&gt;People naturally think about desktop machines first, he continued. If you were running Linux in the 1990s, you had a 32-bit, desktop system. Unix systems, though, moved to 64-bit platforms around 30 years ago, and the Linux desktop made that move about 20 years ago. Even phones and related devices have been 64-bit for the last decade. If those systems were all that Linux had to support, 32-bit support would have long since been removed from the kernel. He summarized the situation with this slide, showing how the non-embedded architectures have transitioned to either 64-bit or nonexistence over time:&lt;/p&gt;
    &lt;p&gt;The world is not all desktops ‚Äî or servers ‚Äî though; embedded Linux exists as well. About 90% of those systems are running on Arm processors. The kernel has accumulated a lot of devicetree files describing those systems over the years; only in this last year has the number of devicetrees for armv8 (64-bit) systems exceeded the number for armv7 (32-bit) systems.&lt;/p&gt;
    &lt;p&gt;For Arm processors with pre-armv7 architectures, there are only three for which it is still possible to buy hardware, but a number are still supported by the kernel community:&lt;/p&gt;
    &lt;p&gt;Many other pre-armv7 CPUs are out of production, but the kernel still has support for them. Of those, he said, there are about ten that could be removed now. It would be nice to be able to say that support for the others will be removed after a fixed period, ten years perhaps, but hardware support does not work that way. Instead, one has to think in terms of half lives; every so often, it becomes possible to remove support for half of the platforms. It all depends on whether there are users for the processors in question.&lt;/p&gt;
    &lt;p&gt;The kernel is still adding support for some 32-bit boards, he said, but at least ten new 64-bit boards gain support for each 32-bit one.&lt;/p&gt;
    &lt;p&gt;There are a number of non-Arm 32-bit architectures that still have support in the kernel; these include arc, microblaze, nios2, openrisc, rv32, sparc/leon, and xtensa. All of them are being replaced by RISC-V processors in new products. RISC-V is what you use if you don't care about Arm compatibility, he said.&lt;/p&gt;
    &lt;p&gt; Then, there is the dusty corner where nommu (processors without a memory-management unit) live; these include armv7-m, m68k, superh, and xtensa. Nobody is building anything with this kind of hardware now, and the only people who are working on them in any way are those who have to support existing systems. "&lt;quote&gt;Or to prove that it can be done&lt;/quote&gt;." &lt;/p&gt;
    &lt;p&gt;There are still some people who need to run 32-bit applications that cannot be updated; the solution he has been pushing people toward is to run a 32-bit user space on a 64-bit kernel. This is a good solution for memory-constrained systems; switching to 32-bit halves the memory usage of the system. Since, on most systems, almost all memory is used by user space, running a 64-bit kernel has a relatively small cost. Please, he asked, do not run 32-bit kernels on 64-bit processors.&lt;/p&gt;
    &lt;p&gt;There are some definite pain points that come with maintaining 32-bit support; most of the complaints, he said, come from developers in the memory-management subsystem. The biggest problem there is the need to support high memory; it is complex, and requires support throughout the kernel. High memory is needed when the kernel lacks the address space to map all of the installed physical memory; that tends to be at about 800MB on 32-bit systems. (See this article for more information about high memory).&lt;/p&gt;
    &lt;p&gt; Currently the kernel is able to support 32-bit systems with up to 16GB of installed memory. Such systems are exceedingly rare, though, and support for them will be going away soon. There are a few 4GB systems out there, including some Chromebooks. Systems with 2GB are a bit more common. Even these systems, he said, are "&lt;quote&gt;a bit silly&lt;/quote&gt;" since the memory costs more than the CPU does. There are some use cases for such systems, though. Most 32-bit systems now have less than 1GB of installed memory. The kernel, soon, will not try to support systems with more than 4GB. &lt;/p&gt;
    &lt;p&gt;There are some ideas out there for how to support the larger-memory 32-bit systems without needing the high-memory abstraction. Linus Walleij is working on entirely separating the kernel and user-space address spaces, giving each 4GB to work with; this is a variant on the "4G/4G" approach that has occasionally been tried for many years. It is difficult to make such a system work efficiently, so this effort may never succeed, Bergmann said.&lt;/p&gt;
    &lt;p&gt;Another approach is the proposed "densemem" memory model, which does some fancy remapping to close holes in the physical address space. Densemem can support up to 2GB and is needed to replace the SPARSEMEM memory model, the removal of which which will eventually be necessary in any case. This work has to be completed before high memory can be removed; Bergmann said that he would be interested in hearing from potential users of the densemem approach.&lt;/p&gt;
    &lt;p&gt;One other possibility is to drop high memory, but allow the extra physical memory to be used as a zram swap device. That would not be as efficient as accessing the memory directly, but it is relatively simple and would make it possible to drop the complexity of high memory.&lt;/p&gt;
    &lt;p&gt;Then, there is the year-2038 problem, which he spent several years working on. The kernel-side work was finished in 2020; the musl C library was updated that same year, and the GNU C Library followed the year after. Some distributors have been faster than others to incorporate this work; Debian and Ubuntu have only become year-2038-safe this year.&lt;/p&gt;
    &lt;p&gt;The year-2038 problem is not yet completely solved, though; there are a lot of packages that have unfixed bugs in this area. Anything using futex(), he said, has about a 50% chance of getting time handling right. The legacy 32-bit system calls, which are not year-2038 safe, are still enabled in the kernel, but they will go away at some point, exposing more bugs. There are languages, including Python and Rust, that have a lot of broken language bindings. Overall, he said, he does not expect any 32-bit desktop system to survive the year-2038 apocalypse.&lt;/p&gt;
    &lt;p&gt;A related problem is big-endian support, which is also 32-bit only, and also obsolete. Its removal is blocked because IBM is still supporting big-endian mainframe and PowerPC systems; as long as that support continues, big-endian support will stay in the kernel.&lt;/p&gt;
    &lt;p&gt; A number of other types of support are under discussion. There were once 32-bit systems with more than eight CPUs, but nobody is using those machines anymore, so support could be removed. Support for armv4 processors, such as the DEC StrongARM CPU, should be removed. Support for early armv6 CPUs, including the omap2 and i.mx31, "&lt;quote&gt;complicates everything&lt;/quote&gt;"; he would like to remove it, even though there are still some Nokia 770 systems in the wild. The time is coming for the removal of all non-devicetree board files. Removal of support for Cortex M CPUs, which are nommu systems, is coming in a couple of years. Developers are eyeing i486 CPU support, but that will not come out yet. Bergmann has sent patches to remove support for KVM on 32-bit CPUs, but there is still "&lt;quote&gt;one PowerPC user&lt;/quote&gt;", so that support will be kept for now. &lt;/p&gt;
    &lt;p&gt;To summarize, he said, the kernel will have to retain support for armv7 systems for at least another ten years. Boards are still being produced with these CPUs, so even ten years may be optimistic for removal. Everything else, he said, will probably fade away sooner than that. The removal of high-memory support has been penciled in for sometime around 2027, and nommu support around 2028. There will, naturally, need to be more discussion before these removals can happen.&lt;/p&gt;
    &lt;p&gt;An audience member asked how developers know whether a processor is still in use or not; Bergmann acknowledged that it can be a hard question. For x86 support, he looked at a lot of old web pages to make a list of which systems existed, then showed that each of those systems was already broken in current kernels for other reasons; the lack of complaints showed that there were no users. For others, it is necessary to dig through the Git history, see what kinds of changes are being made, and ask the developers who have worked on the code; they are the ones who will know who is using that support.&lt;/p&gt;
    &lt;p&gt; Another person asked about whether the kernel would support big-endian RISC-V systems. Bergmann answered that those systems are not supported now, and he hoped that it would stay that way. "&lt;quote&gt;With RISC-V, anybody can do anything, so they do, but it is not always a good idea&lt;/quote&gt;". The final question was about support for nommu esp32 CPUs; he answered that patches for those CPUs exist, but have not been sent upstream. Those processors are "&lt;quote&gt;a cool toy&lt;/quote&gt;", but he does not see any practical application for them. &lt;/p&gt;
    &lt;p&gt;The slides for this talk are available. The curious may also want to look at Bergmann's 2020 take on this topic.&lt;/p&gt;
    &lt;p&gt; [Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my travel to this event.]&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Architectures&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Conference&lt;/cell&gt;
        &lt;cell&gt;Open Source Summit Europe/2025&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Sep 1, 2025 19:55 UTC (Mon) by jrtc27 (subscriber, #107748) [Link] (5 responses) I don't understand this point. 64-bit big-endian systems exist in the form of sparc64 and s390x (and powerpc64, though that's less of a thing given powerpc64le). Posted Sep 1, 2025 21:47 UTC (Mon) by mjw (subscriber, #16740) [Link] (4 responses) Posted Sep 1, 2025 22:27 UTC (Mon) by sam_c (subscriber, #139836) [Link] (1 responses) (Of course, BE isn't the future, I just don't think the platforms are all rotting near-universally as 32-bit ones are on average.) Posted Sep 2, 2025 8:07 UTC (Tue) by arnd (subscriber, #8866) [Link] For the moment, the problems are mainly in newly written code, where portability to both big-endian and 32-bit targets relies on someone actively testing those and submitting fixes. This is in contrast to nommu and highmem systems that no longer have the same level of commercial backing and are much further into the inevitable bitrot. [I believe the article citing "big-endian support, which is also 32-bit only" was a transcription error in an otherwise excellent report -- what I actually said is that the majority of big-endian (embedded) systems are 32-bit, while pointing to s390x/powerpc64 servers as the ones that keep them viable.] Posted Sep 2, 2025 7:05 UTC (Tue) by glaubitz (subscriber, #96452) [Link] (1 responses) Does IBM already know that s390x is supposed to be obsolete now? Posted Sep 2, 2025 12:53 UTC (Tue) by maxfragg (subscriber, #122266) [Link] Posted Sep 1, 2025 19:58 UTC (Mon) by jrtc27 (subscriber, #107748) [Link] (2 responses) That's not really true. Halving is the limit of what you could hope to achieve, if every data type in your system was a machine word or pointer. But char, short, int and long long don't change, (u)intNN_t don't change, and float/double don't change, so depending on what data you're actually operating on you can see anywhere between almost no change (e.g. some purely computational workload on large amounts of raw data) and halving the memory usage. Posted Sep 1, 2025 20:24 UTC (Mon) by arnd (subscriber, #8866) [Link] (1 responses) I had expected to see a much smaller difference between the two environments like you describe, the numbers I saw for anything I tried were always roughly 100% overhead for the 64-bit code compared to armhf. I think this is a combination of multiple effects I measured in addition to the obvious sizeof(long) difference: arm64 has longer instruction words than armv7/thumb2, small malloc() calls are aligned to larger cache lines (128 vs 64 bytes), ELF sections have a larger default alignment (64K vs 4K), etc. For many real workloads, a larger portion of RAM if of course going to be filled with the same user data (text, video, sensor input, network packages, ...) so the difference becomes smaller; for the most memory constrained systems, 100% overhead is surprisingly good estimate that holds true for both kernel data and userland. Posted Sep 2, 2025 5:59 UTC (Tue) by jrtc27 (subscriber, #107748) [Link] Posted Sep 1, 2025 20:03 UTC (Mon) by jrtc27 (subscriber, #107748) [Link] (9 responses) Unfortunately people are building those systems and others are proposing supporting them[1]. It is a real shame this is the approach that's being taken rather than having dedicated byte-swapping load/store instructions to accelerate processing big-endian data structures like network packets, which is the main use case that I'm aware of, outside of being compatible with old software written for big-endian mainframes. [1] https://lore.kernel.org/all/20250822165248.289802-1-ben.d... Posted Sep 2, 2025 7:06 UTC (Tue) by glaubitz (subscriber, #96452) [Link] (8 responses) Why is this unfortunate? I don't understand. Is Linux supposed to run only on architectures that a small group of people likes? Posted Sep 2, 2025 7:55 UTC (Tue) by jrtc27 (subscriber, #107748) [Link] (7 responses) What I have a problem with is needless fragmentation and needless complexity within one specific architecture, in this case RISC-V. Sometimes the right design choice is to say no to options. Nobody was building big-endian RISC-V hardware, nobody was seriously writing big-endian RISC-V software and everything was just fine, but now everyone is expected to support yet another variant of RISC-V that doesn't really achieve anything except create a whole lot of work. It's not some new and interesting architecture that approaches things differently that happens to be big-endian, it is just RISC-V but with big-endian because nobody was willing to say no. Posted Sep 2, 2025 8:06 UTC (Tue) by glaubitz (subscriber, #96452) [Link] (6 responses) I wasn't actually looking at the username and only realized now it was you ;-). &amp;gt; What I have a problem with is needless fragmentation and needless complexity within one specific architecture, in this case RISC-V. I think this is a developer-centric perspective rather than a user-centric perspective. If there are legitimate use cases such as building open-source networking hardware based on RISC-V, then it should be legitimate to add big-endian support if someone is willing to maintain it. Posted Sep 2, 2025 8:43 UTC (Tue) by arnd (subscriber, #8866) [Link] (5 responses) Adding big-endian Armv8 support to Linux made sense in 2013 as users were still porting software from big-endian MIPS/Octeon or PowerPC/QorIQ systems, but it ended up being a mistake for the same reason that Jessica explained about RISC-V: it's an extra ABI that requires testing resources in order to keep running, for very little practical use. Now that all the Linux networking applications moved to arm64le, the only remaining use case for arm64be is to have an easily available platform find and fix endianness bugs, typically in a VM guest running on a LE host. This means we can't easily remove from the arm64 kernel, but it would still be a mistake to add it to riscv64 Linux. Posted Sep 2, 2025 9:01 UTC (Tue) by glaubitz (subscriber, #96452) [Link] (3 responses) How do you know that? Posted Sep 2, 2025 9:14 UTC (Tue) by arnd (subscriber, #8866) [Link] (2 responses) Additionally, all newer networking SoCs come with SystemReady certification these days, but SystemReady requires booting in little-endian UEFI mode rather than the traditional Linux boot interface that supports both big-endian and little-endian kernels. Posted Sep 2, 2025 14:02 UTC (Tue) by hailfinger (subscriber, #76962) [Link] (1 responses) Posted Sep 2, 2025 16:10 UTC (Tue) by arnd (subscriber, #8866) [Link] Realtek's other product lines like the rtl819x wifi line or the STB/NAS rtd1xxx chips already migrated from big-endian mips32 lexra to little-endian mips32 r24k/r34k/r74k, then little-endian arm32 and now little-endian arm64, so I would expect them to do a similar progression here, possibly skipping one or two steps. Posted Sep 2, 2025 10:19 UTC (Tue) by pabs (subscriber, #43278) [Link] Posted Sep 1, 2025 20:53 UTC (Mon) by ajb (subscriber, #9694) [Link] (7 responses) There are also going to be many products where it doesn't make much difference whether an ancient processor or an up to date riscv is used, and manufacturers are very likely to continue to choose ones where the licence fee is already paid off. Posted Sep 1, 2025 22:11 UTC (Mon) by smoogen (subscriber, #97) [Link] (5 responses) Posted Sep 2, 2025 1:44 UTC (Tue) by willy (subscriber, #9762) [Link] (3 responses) Posted Sep 2, 2025 4:56 UTC (Tue) by ajb (subscriber, #9694) [Link] (2 responses) Posted Sep 2, 2025 9:56 UTC (Tue) by hailfinger (subscriber, #76962) [Link] (1 responses) The three options for such vendors are: Posted Sep 2, 2025 13:16 UTC (Tue) by foom (subscriber, #14868) [Link] Sounds like that'd make maintenance generally easier, not harder, if your goal is only to check a box for compliance. If there are no security vulnerabilities reported which apply to your particular kernel, obviously that means it has no security issues and therefore requires no updates, so there's no work to do. Yay! Posted Sep 2, 2025 7:30 UTC (Tue) by taladar (subscriber, #68407) [Link] Posted Sep 1, 2025 22:54 UTC (Mon) by iabervon (subscriber, #722) [Link] Posted Sep 2, 2025 0:42 UTC (Tue) by wileypob (subscriber, #139361) [Link] Posted Sep 2, 2025 2:44 UTC (Tue) by wtarreau (subscriber, #51152) [Link] (4 responses) Posted Sep 2, 2025 6:06 UTC (Tue) by jrtc27 (subscriber, #107748) [Link] (1 responses) Posted Sep 2, 2025 16:04 UTC (Tue) by wtarreau (subscriber, #51152) [Link] Yes definitely, that's the principle of endianness. Similarly when you read an in as a short you can get the wrong one, and so on. It's particularly common when dealing with port numbers for example. For the 64-BE I'm using an old UltraSparc, it does the job pretty well and cannot stand unaligned accesses either. It's just used much less often as being more annoying to boot than a good old GL-iNet router that fits in your hand! Posted Sep 2, 2025 14:09 UTC (Tue) by arnd (subscriber, #8866) [Link] (1 responses) The chart is a little hard to read, but what it shows is that the oldest mainline-supported ath79 machine is from 2007, the replacement ipq40xx platform (armv7) was added in 2016, and at least one mips32 chip (i.e. qca9531) is still commercially available and continues to be sold alongside the armv7/v8 successors. As far as I can tell, qca9531 and rtlx3xx are now the only big-endian-only 32-bit chip that is still actively marketed, as coldfire mcf5441x and ppc32 qoriq p4 are at the end of their official 15 year support life this year. The 64-bit qoriq txxxx on the other hand have extended support until 2035 according to NXP. Posted Sep 2, 2025 16:08 UTC (Tue) by wtarreau (subscriber, #51152) [Link] Ah indeed, sorry Arnd! I didn't notice them because the page was talking about Arm and most of the first ones were apparently Arm. I'm even seeing the ralink one. Thanks for all the details BTW, these complete the table fairly well ;-) Posted Sep 2, 2025 6:30 UTC (Tue) by andy_shev (subscriber, #75870) [Link] (4 responses) Posted Sep 2, 2025 7:33 UTC (Tue) by taladar (subscriber, #68407) [Link] (2 responses) Posted Sep 2, 2025 12:38 UTC (Tue) by andy_shev (subscriber, #75870) [Link] (1 responses) Posted Sep 2, 2025 13:18 UTC (Tue) by pizza (subscriber, #46) [Link] Uh, modern 32-bit userspace has had the ability to deal with post-2038 dates for a long time. It's just that there's a lot of old binaries out there that won't ever be recompiled. That said, even if you do recompile the old sources with modern libc and -D_TIME_BITS=64, you can still be bitten by binary representations (network protocols, file formats, etc) that directly embed a 32-bit time_t. Posted Sep 2, 2025 15:51 UTC (Tue) by arnd (subscriber, #8866) [Link] The specific processors that I mentioned that may be removed (Intel strongarm/armv4, omap2/armv6, imx31/armv6 are no longer used in any industrial machines as far as I can tell, only in early handhelds, and they do cause problems for maintainability. They are easy to confuse with similar industrial products though: moxart (armv4), at91rm9200 (armv4t), ep93xx (armv4t), ixp4xx/pxa (armv5), imx35 (armv6k), omap3/am3 (armv7). These all have relatively sane CPUs and DT support, so they are easy enough to maintain for many more years. Importantly, I don't expect we'll even consider dropping armv4 (fa926) and armv4t (arm720t/arm920t) CPU support as long as we support armv5 (arm926e/pj1/xscale) based SoCs like the recently added Microchip SAM9X7. Posted Sep 2, 2025 7:04 UTC (Tue) by glaubitz (subscriber, #96452) [Link] (2 responses) Neither M68k nor SuperH are nommu only. That statement is simply incorrect. And people are actually building new m68k-based hardware with the help of FPGAs. The retro community around the Amiga, Atari, Macintosh 68k as well as SuperH-based video game consoles such as the Sega Saturn and especially the Sega Dreamcast. Not sure why this article is so much focused on commercial applications. And I don't think big-endian targets are going to be obsolete any time soon unless someone convinces IBM to drop s390x. Posted Sep 2, 2025 10:52 UTC (Tue) by DrMcCoy (subscriber, #86699) [Link] (1 responses) They're not running Linux on these systems, though, are they? Posted Sep 2, 2025 12:40 UTC (Tue) by andy_shev (subscriber, #75870) [Link] Posted Sep 2, 2025 14:36 UTC (Tue) by milek7 (subscriber, #141321) [Link] This reminds me of Linus complaints about PAE (from 2007!): https://www.realworldtech.com/forum/?threadid=76912&amp;amp;c... Posted Sep 2, 2025 14:45 UTC (Tue) by urjaman (subscriber, #130506) [Link] (1 responses) (I'm not expecting actual answers, just expressing frustrating at the whole premise...) Posted Sep 2, 2025 14:53 UTC (Tue) by corbet (editor, #1) [Link] Sorry, but sometimes I get frustrated too... &lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;lb/&gt; The slides don't say that big-endian is 32-bit only, but "Equally obsolete as 32-bit".&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;Not all types are words / pointers&lt;/head&gt;&lt;head&gt;Not all types are words / pointers&lt;/head&gt;&lt;head&gt;Not all types are words / pointers&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;lb/&gt; 1. maintain an older kernel indefinitely (good luck with that, especially now that older upstream-unsupported kernels won't get CVEs allocated for security issues)&lt;lb/&gt; 2. work hard on keeping the affected chipses supported by the mainline kernel&lt;lb/&gt; 3. scrap their designs and start anew from another incopmatible design (not worth the effort)&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;Embedded systems&lt;/head&gt;&lt;head&gt;MIPS ?&lt;/head&gt;&lt;head&gt;MIPS ?&lt;/head&gt;&lt;head&gt;MIPS ?&lt;/head&gt;&lt;head/&gt; The ath79 family is on the second chart linked in the article, along with 7 other mips32 targets that are still supported (I left off a couple of mips and armv5 targets that only support a single reference board but no actual products in mainline kernels, though I did miss the rtl83xx/93xx platform). &lt;head&gt;MIPS ?&lt;/head&gt;&lt;head&gt;MIPS ?&lt;/head&gt;&lt;head&gt;Industrial computers&lt;/head&gt;&lt;head&gt;Industrial computers&lt;/head&gt;&lt;head&gt;Industrial computers&lt;/head&gt;&lt;head&gt;Industrial computers&lt;/head&gt;&lt;head&gt;Industrial computers&lt;/head&gt;&lt;head&gt;M68k and SuperH are MMU ports!&lt;/head&gt;&lt;head&gt;M68k and SuperH are MMU ports!&lt;/head&gt;&lt;head&gt;M68k and SuperH are MMU ports!&lt;/head&gt;&lt;head&gt;PAE&lt;/head&gt;&lt;head&gt;Why "how soon can we remove X" instead of "how long can we have X"?&lt;/head&gt;&lt;head/&gt; What is the source of your frustration? You have an old system that is nicely supported, and will continue to be supported ‚Äî a gift you are getting for free from the people who do that work. Rather than condemn them for trying to figure out when they can stop doing work for platforms that are no longer in use, maybe you could try thanking them? &lt;head&gt;Why "how soon can we remove X" instead of "how long can we have X"?&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1035727/4837b0d3dccf1cbb/"/></entry><entry><id>https://news.ycombinator.com/item?id=45095603</id><title>Amazon has mostly sat out the AI talent war</title><updated>2025-09-02T16:12:46.333440+00:00</updated><content>&lt;doc fingerprint="cf8db5f163666cc"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amazon struggles to attract AI talent due to its pay model and perception of falling behind others.&lt;/item&gt;
      &lt;item&gt;Amazon's compensation model has long caused complaints from employees.&lt;/item&gt;
      &lt;item&gt;Competitors like Meta and OpenAI offer more attractive packages for AI engineers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As the AI talent war sweeps across Silicon Valley, Amazon has largely sat on the sidelines. A confidential internal document, and accounts from people familiar with the matter, reveal why.&lt;/p&gt;
    &lt;p&gt;The company has flagged its unique pay structure, lagging AI reputation, and rigid return-to-office rules as major hurdles. Now, the tech giant is being pushed to rethink its recruiting strategy as it scrambles to compete for top talent.&lt;/p&gt;
    &lt;p&gt;The document, from late last year, was written by the HR team covering Amazon's non-retail businesses, including Amazon Web Services, advertising, devices, entertainment, and the newly formed artificial general intelligence team.&lt;/p&gt;
    &lt;p&gt;"GenAI hiring faces challenges like location, compensation, and Amazon's perceived lag in the space," the document noted. "Competitors often provide more comprehensive and aggressive packages." Business Insider obtained a copy of the document.&lt;/p&gt;
    &lt;p&gt;Amazon's absence from recent splashy AI hires underscores those concerns. Meta has pulled in high-profile talent from ScaleAI, Apple, and OpenAI. Google and OpenAI continue to be top destinations for AI experts, while Microsoft has even drafted a wish list of Meta AI employees it hopes to recruit.&lt;/p&gt;
    &lt;p&gt;Amazon's spokesperson initially told BI that the company continues to "adapt our approach to remain highly competitive, maintaining flexibility in both our compensation packages and work arrangements to attract and retain the best AI talent in this dynamic market."&lt;/p&gt;
    &lt;p&gt;Hours later, the spokesperson updated the statement, saying the premise of the story was "wrong," without providing any specifics.&lt;/p&gt;
    &lt;p&gt;"We continue to attract and retain some of the best people in the world and they're building and deploying GenAI applications at a rapid clip. Our compensation is competitive, but we also want missionaries who are passionate about inventing things that will make a meaningful difference for customers ‚Äî for those kinds of people, there's no better place in the world to build."&lt;/p&gt;
    &lt;head rend="h2"&gt;Door desks and 'egalitarian' pay&lt;/head&gt;
    &lt;p&gt;Amazon is famously frugal. One of its origin stories recounts how the company bought cheap doors from Home Depot and hacked them together as office desks. This became guiding symbol of Amazon's cautious spending, with founder Jeff Bezos still using door desks today.&lt;/p&gt;
    &lt;p&gt;This penny-pinching culture has smashed straight into an AI hiring battle that's being fueled by unprecedented spending, putting Amazon in a tricky situation.&lt;/p&gt;
    &lt;p&gt;The internal document described compensation as one of the "hotly debated topics" among Amazon recruiters, citing the company's strict use of fixed salary bands for each role. Amazon's "egalitarian philosophy" on pay leaves its offers "below par" compared with top rivals, it added.&lt;/p&gt;
    &lt;p&gt;"The lack of salary range increases for several key job families over the past few years does not position Amazon as an employer of choice for top tech talent," the document warned.&lt;/p&gt;
    &lt;p&gt;For Amazon, missing out on top AI talent is a potential risk. The pool of top-tier AI researchers and engineers is limited, and without experts with deep knowhow, it's hard to compete at the frontier of the field. Indeed, Amazon has yet to find a blockbuster AI product like OpenAI's ChatGPT or Anthropic's Claude, although its Bedrock AI cloud service has made progress.&lt;/p&gt;
    &lt;p&gt;Amazon's pay structure has been a long-standing source of tension.&lt;/p&gt;
    &lt;p&gt;Several people who spoke to Business Insider cited the 2020 departure of Amazon robotics VP Brad Porter as evidence of the company's frugal approach hampering talent recruitment and retention. Porter left in part after Amazon refused to raise his pay band.&lt;/p&gt;
    &lt;p&gt;Amazon's stock vesting schedule is also heavily backloaded, a structure that can be less attractive to new hires. The policy extends even to top executives, who generally receive no cash bonuses.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Voting with their feet'&lt;/head&gt;
    &lt;p&gt;In addition to highlighting Amazon's "perceived lag in the AI space," the internal document said generative AI has further intensified the competition for specialized talent, particularly individuals with expertise in large language models.&lt;/p&gt;
    &lt;p&gt;An August report from venture capital firm SignalFire shows Amazon is on the lower end of engineering retention, far below Meta, OpenAI, and Anthropic. Jarod Reyes, SignalFire's head of developer community, told Business Insider that Amazon rivals are making bigger strides in AI, across open models, foundational research, and developer tooling.&lt;/p&gt;
    &lt;p&gt;"Amazon hasn't clearly positioned itself as a leader in the generative AI wave," Reyes said. "Engineers are paying attention and they're voting with their feet."&lt;/p&gt;
    &lt;p&gt;Some investors share that view. On Amazon's earnings call last month, Morgan Stanley analyst Brian Nowak pressed CEO Andy Jassy on Wall Street's "narrative right now that AWS is falling behind" in AI and fears of losing market share to rivals. Jassy's response fell flat, sending Amazon's stock lower during the call.&lt;/p&gt;
    &lt;p&gt;Amazon intends to tackle these concerns. According to the document, the company will refine its "compensation and location strategy" and host more events designed to highlight its generative AI capabilities. It also intends to set up dedicated recruiting teams for generative AI within business units like AWS to boost efficiency.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Hubs' constrain talent&lt;/head&gt;
    &lt;p&gt;Another point of contention is Amazon's aggressive return-to-office mandate, which has already caused logistical issues.&lt;/p&gt;
    &lt;p&gt;The company's new "hub" policy ‚Äî which requires employees to relocate to a central office or risk termination ‚Äî has further limited its access to "high-demand talent like those with GenAI skills," according to the internal document.&lt;/p&gt;
    &lt;p&gt;"Hubs constrain market availability," it stated.&lt;/p&gt;
    &lt;p&gt;Amazon is exploring ways to allow for more "location-flexible" roles, the document added.&lt;/p&gt;
    &lt;p&gt;Amazon's spokesperson told BI that the company is "always looking for ways to optimize our recruiting strategies and looking at alternate talent rich locations."&lt;/p&gt;
    &lt;p&gt;Amazon hasn't been entirely on the sidelines. Last year, it brought on Adept CEO David Luan as part of a licensing deal with the AI startup. Luan now heads Amazon's AI agents lab. But the company has also seen departures, including senior AI leaders like chip designer Rami Sinno and VP Vasi Philomin, who worked on Bedrock.&lt;/p&gt;
    &lt;p&gt;One Amazon recruiter told Business Insider that a growing number of job candidates started declining offers last year because of the company's RTO policy. Even if a competitor pays less, people are open to taking the job if they can stay remote, this person said.&lt;/p&gt;
    &lt;p&gt;"We are losing out on talent," this person added.&lt;/p&gt;
    &lt;p&gt;Indeed, Bloomberg reported recently that Oracle has hired away more than 600 Amazon employees in the past two years because Amazon's strict RTO policy has made poaching easier.&lt;/p&gt;
    &lt;head rend="h2"&gt;Staying the course&lt;/head&gt;
    &lt;p&gt;The internal Amazon document dates to late last year, leaving open the possibility that the company has since adjusted its compensation approach to make exceptions for top AI talent.&lt;/p&gt;
    &lt;p&gt;Still, multiple people familiar with the situation told Business Insider there haven't been any formal updates to internal pay guidelines. One current Amazon manager said it remains almost impossible for the company to enact sweeping changes, given its long track record of sticking to the existing system. The people who spoke with Business Insider asked not to be identified discussing sensitive matters.&lt;/p&gt;
    &lt;p&gt;"Based on how we run our business and what we have achieved, there are more risks than potential benefits from changing an approach that has been so successful for our shareholders over the past several decades," Amazon wrote this year about executive compensation in its annual proxy statement.&lt;/p&gt;
    &lt;p&gt;Of course, the AI talent war may end up being an expensive and misguided strategy, stoked by hype and investor over-exuberance.&lt;/p&gt;
    &lt;p&gt;Some of the high-profile recruits Meta recently lured have already departed.&lt;/p&gt;
    &lt;p&gt;Have a tip? Contact this reporter via email at ekim@businessinsider.com or Signal, Telegram, or WhatsApp at 650-942-3061. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8"/></entry><entry><id>https://news.ycombinator.com/item?id=45095849</id><title>Patrick Winston: How to Speak (2018) [video]</title><updated>2025-09-02T16:12:45.253348+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=Unzc731iCUY"/></entry><entry><id>https://news.ycombinator.com/item?id=45096585</id><title>Raspberry Pi 5 support (OpenBSD)</title><updated>2025-09-02T16:12:44.720895+00:00</updated><content>&lt;doc fingerprint="e73f12d23de611fc"&gt;
  &lt;main&gt;&lt;quote&gt;[prev in list] [next in list] [prev in thread] [next in thread] List: openbsd-cvs Subject: CVS: cvs.openbsd.org: src From: Marcus Glocker &amp;lt;mglocker () cvs ! openbsd ! org&amp;gt; Date: 2025-09-01 18:56:04 Message-ID: dd1203a530237b22 () cvs ! openbsd ! org [Download RAW message or body] CVSROOT: /cvs Module name: src Changes by: mglocker@cvs.openbsd.org 2025/09/01 12:56:04 Modified files: distrib/arm64/iso: Makefile distrib/arm64/ramdisk: Makefile install.md list Log message: Add Raspberry Pi 5 Model B support for RAMDISK. Known issues: * Booting from PCIe storage HATs doesn't work because of missing U-Boot support. * WiFi on the Raspberry Pi 5 Model B "d0" boards doesn't work. * The active cooler (fan) doesn't work because of missing pwm/clock drivers (some work is in-progress). ok kettenis@, deraadt@ [prev in list] [next in list] [prev in thread] [next in thread] &lt;/quote&gt;&lt;lb/&gt;Configure |
About |
News |
Add a list |
Sponsored by KoreLogic&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://marc.info/?l=openbsd-cvs&amp;m=175675287220070&amp;w=2"/></entry><entry><id>https://news.ycombinator.com/item?id=45098269</id><title>Kazeta: An operating system that brings the console gaming experience of 90s</title><updated>2025-09-02T16:12:44.117068+00:00</updated><content>&lt;doc fingerprint="859b1d2912326fbb"&gt;
  &lt;main&gt;&lt;p&gt;An operating system that brings the console gaming experience of the '90s to modern PC hardware and games: insert cart, power on, play.&lt;/p&gt;Explore Kazeta&lt;p&gt;An operating system that brings the console gaming experience of the '90s to modern PC hardware and games: insert cart, power on, play.&lt;/p&gt;Explore Kazeta&lt;p&gt;Insert a game cart, press power, and you're gaming instantly. Relive that nostalgic golden age where nothing stood between you and the games you love.&lt;/p&gt;&lt;p&gt;Transform your digital library into something tangible and permanent. Create physical game carts from your DRM-free titles and build a collection that you can play forever.&lt;/p&gt;&lt;p&gt;Say goodbye to the complexities of modern gaming and just play.&lt;/p&gt;&lt;p&gt;Save data is captured automatically, so you never lose progress. When no cart is inserted, boot into a retro console inspired BIOS menu to manage your saves.&lt;/p&gt;&lt;p&gt;Play almost any DRM-free game from platforms past or present.&lt;/p&gt;&lt;p&gt;Bring back the family-friendly simplicity of gaming's distant past. Perfect for kids, parents, and grandparents who just want to play.&lt;/p&gt;&lt;p&gt;Download Kazeta today and rediscover the joy of pure gaming.&lt;/p&gt;Download Now&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kazeta.org/"/></entry><entry><id>https://news.ycombinator.com/item?id=45098722</id><title>FreeDroidWarn</title><updated>2025-09-02T16:12:43.396691+00:00</updated><content>&lt;doc fingerprint="27a090d3f3147310"&gt;
  &lt;main&gt;
    &lt;p&gt;This library shows an alert dialog with a deprecation warning informing that Google will require developer verification for Android apps outside the Play Store from 2026/2027 which the developer is not going to provide.&lt;/p&gt;
    &lt;code&gt;Google has announced that, starting in 2026/2027, all apps on certified Android devices
will require the developer to submit personal identity details directly to Google.
Since the developers of this app do not agree to this requirement, this app will no longer 
work on certified Android devices after that time.
&lt;/code&gt;
    &lt;p&gt;https://www.androidauthority.com/android-developer-verification-requirements-3590911/&lt;/p&gt;
    &lt;p&gt;https://developer.android.com/developer-verification&lt;/p&gt;
    &lt;p&gt;Add the JitPack repository to your root build.gradle at the end of repositories:&lt;/p&gt;
    &lt;code&gt;allprojects {
  repositories {
    ...
    maven { url 'https://jitpack.io' }
  }
}&lt;/code&gt;
    &lt;p&gt;Add the library dependency to your build.gradle file.&lt;/p&gt;
    &lt;code&gt;dependencies {
    implementation 'com.github.woheller69:FreeDroidWarn:V1.3'
}&lt;/code&gt;
    &lt;p&gt;In onCreate of your app just add:&lt;/p&gt;
    &lt;code&gt;     FreeDroidWarn.showWarningOnUpgrade(this, BuildConfig.VERSION_CODE);

&lt;/code&gt;
    &lt;p&gt;This library is licensed under the Apache V2.0 license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/woheller69/FreeDroidWarn"/></entry><entry><id>https://news.ycombinator.com/item?id=45099124</id><title>WinBoat: Run Windows apps on Linux with seamless integration</title><updated>2025-09-02T16:12:42.802568+00:00</updated><content>&lt;doc fingerprint="39bff56e5f27b5dc"&gt;
  &lt;main&gt;
    &lt;p&gt;WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üé® Elegant Interface: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience&lt;/item&gt;
      &lt;item&gt;üì¶ Automated Installs: Simple installation process through our interface - pick your preferences &amp;amp; specs and let us handle the rest&lt;/item&gt;
      &lt;item&gt;üöÄ Run Any App: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment&lt;/item&gt;
      &lt;item&gt;üñ•Ô∏è Full Windows Desktop: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow&lt;/item&gt;
      &lt;item&gt;üìÅ Filesystem Integration: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle&lt;/item&gt;
      &lt;item&gt;‚ú® And many more: Smartcard passthrough, resource monitoring, and more features being added regularly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before running WinBoat, ensure your system meets the following requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RAM: At least 4 GB of RAM&lt;/item&gt;
      &lt;item&gt;CPU: At least 2 CPU threads&lt;/item&gt;
      &lt;item&gt;Storage: At least 32 GB free space in &lt;code&gt;/var&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Virtualization: KVM enabled in BIOS/UEFI&lt;/item&gt;
      &lt;item&gt;Docker: Required for containerization&lt;/item&gt;
      &lt;item&gt;Docker Compose v2: Required for compatibility with docker-compose.yml files&lt;/item&gt;
      &lt;item&gt;Docker User Group: Add your user to the &lt;code&gt;docker&lt;/code&gt;group&lt;/item&gt;
      &lt;item&gt;FreeRDP: Required for remote desktop connection (Please make sure you have Version 3.x.x with sound support included)&lt;/item&gt;
      &lt;item&gt;Kernel Modules: &lt;code&gt;iptables&lt;/code&gt;and&lt;code&gt;iptable_nat&lt;/code&gt;modules must be loaded&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can download the latest Linux builds under the Releases tab. We currently offer two variants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AppImage: A popular &amp;amp; portable app format which should run fine on most distributions&lt;/item&gt;
      &lt;item&gt;Unpacked: The raw unpacked files, simply run the executable (&lt;code&gt;linux-unpacked/winboat&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Podman is unsupported for now&lt;/item&gt;
      &lt;item&gt;Docker Desktop is unsupported for now&lt;/item&gt;
      &lt;item&gt;Distros that emulate Docker through a Podman socket are unsupported&lt;/item&gt;
      &lt;item&gt;Any rootless containerization solution is currently unsupported&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For building you need to have NodeJS and Go installed on your system&lt;/item&gt;
      &lt;item&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Build the app and the guest server using &lt;code&gt;npm run build:linux-gs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;You can now find the built app under &lt;code&gt;dist&lt;/code&gt;with an AppImage and an Unpacked variant&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure you meet the prerequisites&lt;/item&gt;
      &lt;item&gt;Additionally, for development you need to have NodeJS and Go installed on your system&lt;/item&gt;
      &lt;item&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Build the guest server (&lt;code&gt;npm run build-guest-server&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Run the app (&lt;code&gt;npm run dev&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.&lt;/p&gt;
    &lt;p&gt;Please note: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! üöÄ&lt;/p&gt;
    &lt;p&gt;Feel free to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Report bugs and issues&lt;/item&gt;
      &lt;item&gt;Submit feature requests&lt;/item&gt;
      &lt;item&gt;Contribute code improvements&lt;/item&gt;
      &lt;item&gt;Help with documentation&lt;/item&gt;
      &lt;item&gt;Share feedback and suggestions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.&lt;/p&gt;
    &lt;p&gt;WinBoat is licensed under the MIT license&lt;/p&gt;
    &lt;p&gt;These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.&lt;lb/&gt; They're awesome and you should check them out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WinApps&lt;/item&gt;
      &lt;item&gt;Cassowary&lt;/item&gt;
      &lt;item&gt;dockur/windows (üåü Also used in WinBoat)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üåê Website: winboat.app&lt;/item&gt;
      &lt;item&gt;üê¶ Twitter/X: @winboat_app&lt;/item&gt;
      &lt;item&gt;ü¶ã Bluesky: winboat.app&lt;/item&gt;
      &lt;item&gt;üó®Ô∏è Discord: Join our community&lt;/item&gt;
      &lt;item&gt;üìß Email: staff@winboat.app&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/TibixDev/winboat"/></entry><entry><id>https://news.ycombinator.com/item?id=45099192</id><title>Keyboards from my collection (2023)</title><updated>2025-09-02T16:12:42.245493+00:00</updated><content>&lt;doc fingerprint="7f544c11e59d1554"&gt;
  &lt;main&gt;
    &lt;p&gt;Marcin Wichary&lt;/p&gt;
    &lt;p&gt;12 February 2023 / 50 posts / 60 photos&lt;/p&gt;
    &lt;head rend="h1"&gt;50 keyboards from my collection&lt;/head&gt;
    &lt;p&gt;This is an archive of a Mastodon thread from 2023. You can still read the thread (and all the replies) at its original location, however the photo quality is much better on this page.&lt;/p&gt;
    &lt;p&gt;To celebrate the Kickstarter for Shift Happens going well, I thought I would show you 50 keyboards from my collection of really strange/esoteric/meaningful keyboards that I gathered over the years. (It might be the world√¢s strangest keyboard collection!)&lt;/p&gt;
    &lt;p&gt;This is technically a bit of a spoiler for the book, but a) a lot of them are not in the book, and b) the book comes out in half a year, and we√¢ll all forget by then! &lt;lb/&gt;Let√¢s start!&lt;lb/&gt;Shift Happens on Kickstarter&lt;/p&gt;
    &lt;p&gt;Let√¢s start!&lt;/p&gt;
    &lt;p&gt;Shift Happens on Kickstarter&lt;/p&gt;
    &lt;p&gt;1.&lt;lb/&gt;I have a SafeType, thanks to a friend who noticed one about to be thrown away. This is among the most notable and interesting √¢ergonomic√¢ keyboards, complete with mirrors that help you orient yourself when you√¢re starting out.&lt;/p&gt;
    &lt;p&gt;I have a SafeType, thanks to a friend who noticed one about to be thrown away. This is among the most notable and interesting √¢ergonomic√¢ keyboards, complete with mirrors that help you orient yourself when you√¢re starting out.&lt;/p&gt;
    &lt;p&gt;2.&lt;lb/&gt;The Comfort System keyboard is another √¢ergonomic√¢ device that is honestly pretty frightening to look at (explaining the challenge of making keyboards like these). You can reposition and reorient each of the three parts independently.&lt;/p&gt;
    &lt;p&gt;The Comfort System keyboard is another √¢ergonomic√¢ device that is honestly pretty frightening to look at (explaining the challenge of making keyboards like these). You can reposition and reorient each of the three parts independently.&lt;/p&gt;
    &lt;p&gt;3.&lt;lb/&gt;I love these DataDesk Little Fingers keyboards with smaller keys because you can see exactly when iMac was introduced and how the company tried to √¢redesign√¢√Ç the keyboard to fit the new style.&lt;/p&gt;
    &lt;p&gt;I love these DataDesk Little Fingers keyboards with smaller keys because you can see exactly when iMac was introduced and how the company tried to √¢redesign√¢√Ç the keyboard to fit the new style.&lt;/p&gt;
    &lt;p&gt;4.&lt;lb/&gt;This is another Mac √¢alternate universe√¢ keyboard - an Adesso ergonomic keyboard that feels like √¢what if Apple Adjustable still existed when iMac came around√¢?&lt;/p&gt;
    &lt;p&gt;This is another Mac √¢alternate universe√¢ keyboard - an Adesso ergonomic keyboard that feels like √¢what if Apple Adjustable still existed when iMac came around√¢?&lt;/p&gt;
    &lt;p&gt;5.&lt;lb/&gt;This strange √¢medical√¢ keyboard is more mechanical than you√¢d expect! I wrote more about it here: A tale of three skeuomorphs. Cleaning required when flashing!&lt;/p&gt;
    &lt;p&gt;This strange √¢medical√¢ keyboard is more mechanical than you√¢d expect! I wrote more about it here: A tale of three skeuomorphs. Cleaning required when flashing!&lt;/p&gt;
    &lt;p&gt;6.&lt;lb/&gt;Once you√¢re done with your shift (no pun intended) at the hospital, how about some Pizza? This is i-Opener, one of the many shortlived internet appliances, this one with a gimmick that keeps on gimmicking.&lt;/p&gt;
    &lt;p&gt;Once you√¢re done with your shift (no pun intended) at the hospital, how about some Pizza? This is i-Opener, one of the many shortlived internet appliances, this one with a gimmick that keeps on gimmicking.&lt;/p&gt;
    &lt;p&gt;7.&lt;lb/&gt;Speaking of spacebar-adjacent gimmicks, I am mildly obsessed with how beautiful is this first NeXT keyboard from 1987, with a bunch of cool subtle things including a Command *bar* underneath the spacebar. As a matter of fact, I just finished writing an essay on it today!&lt;/p&gt;
    &lt;p&gt;Speaking of spacebar-adjacent gimmicks, I am mildly obsessed with how beautiful is this first NeXT keyboard from 1987, with a bunch of cool subtle things including a Command *bar* underneath the spacebar. As a matter of fact, I just finished writing an essay on it today!&lt;/p&gt;
    &lt;p&gt;8.&lt;lb/&gt;This is Olivetti Praxis 48: perhaps one of the most beautiful among the most beautiful typewriters, and strangely similar in palette to the above NeXT keyboard. You could turn on this (electric) typewriter just by pressing any key. That√¢s pretty wild.&lt;/p&gt;
    &lt;p&gt;This is Olivetti Praxis 48: perhaps one of the most beautiful among the most beautiful typewriters, and strangely similar in palette to the above NeXT keyboard. You could turn on this (electric) typewriter just by pressing any key. That√¢s pretty wild.&lt;/p&gt;
    &lt;p&gt;9.&lt;lb/&gt;This Olympia Reporter typewriter is not beautiful, but it has a lot of POWER THIS and POWER THAT keys that celebrate its marriage with electricity? Why is X and some other keys red? Those are the ones that auto repeat!√¢¬®&lt;/p&gt;
    &lt;p&gt;This Olympia Reporter typewriter is not beautiful, but it has a lot of POWER THIS and POWER THAT keys that celebrate its marriage with electricity? Why is X and some other keys red? Those are the ones that auto repeat!√¢¬®&lt;/p&gt;
    &lt;p&gt;10.&lt;lb/&gt;This is another typewriter, so proud of a functioning (erasing!) Backspace that it gives this a treatment I have never seen before or after.&lt;/p&gt;
    &lt;p&gt;This is another typewriter, so proud of a functioning (erasing!) Backspace that it gives this a treatment I have never seen before or after.&lt;/p&gt;
    &lt;p&gt;11.&lt;lb/&gt;This Turkish typewriter (another Olympia!) means so much to me √¢ the small success of this article from 2015 was probably what was needed for me to start thinking about the book: What I learned about languages just by looking at a Turkish typewriter&lt;/p&gt;
    &lt;p&gt;This Turkish typewriter (another Olympia!) means so much to me √¢ the small success of this article from 2015 was probably what was needed for me to start thinking about the book: What I learned about languages just by looking at a Turkish typewriter&lt;/p&gt;
    &lt;p&gt;12.&lt;lb/&gt;This keypad√¢¬¶ is so bad.&lt;/p&gt;
    &lt;p&gt;This keypad√¢¬¶ is so bad.&lt;/p&gt;
    &lt;p&gt;13.&lt;lb/&gt;This was meant to be mounted atop Commodore 64 (which I don√¢t have), an interesting reversal from the early typewriters being nothing more than repurposed music keyboards.&lt;/p&gt;
    &lt;p&gt;This was meant to be mounted atop Commodore 64 (which I don√¢t have), an interesting reversal from the early typewriters being nothing more than repurposed music keyboards.&lt;/p&gt;
    &lt;p&gt;14.&lt;lb/&gt;These two are taking this idea even further √¢ mount these overlays on regular keyboards to turn them into new kinds of interfaces.&lt;/p&gt;
    &lt;p&gt;These two are taking this idea even further √¢ mount these overlays on regular keyboards to turn them into new kinds of interfaces.&lt;/p&gt;
    &lt;p&gt;15.&lt;lb/&gt;There√¢s also professional gaming. It was cheaper for me to buy QSENN keyboards and replicate what professional StarCraft gamers were doing in the 1990s, than to find a good existing photo of one of these keyboards.&lt;/p&gt;
    &lt;p&gt;There√¢s also professional gaming. It was cheaper for me to buy QSENN keyboards and replicate what professional StarCraft gamers were doing in the 1990s, than to find a good existing photo of one of these keyboards.&lt;/p&gt;
    &lt;p&gt;16.&lt;lb/&gt;And speaking of gaming √¢ we√¢re all used to the thumb style of typing from the first photo that it was fun to discover the short moment where the gaming keyboards looked like the one in the second photo.&lt;/p&gt;
    &lt;p&gt;And speaking of gaming √¢ we√¢re all used to the thumb style of typing from the first photo that it was fun to discover the short moment where the gaming keyboards looked like the one in the second photo.&lt;/p&gt;
    &lt;p&gt;17.&lt;lb/&gt;And a bit earlier, some game consoles tried to reinvent themselves as home computers with keyboard accessories. This is among the strangest of them: a √¢keyboard√¢ to add BASIC to the Atari 2600.&lt;/p&gt;
    &lt;p&gt;And a bit earlier, some game consoles tried to reinvent themselves as home computers with keyboard accessories. This is among the strangest of them: a √¢keyboard√¢ to add BASIC to the Atari 2600.&lt;/p&gt;
    &lt;p&gt;18.&lt;lb/&gt;I commissioned this √¢joystick√¢ from @benjedwards and I am so happy with how it turned out. It√¢s technically a joystick without a stick, but software turned it into a one-key keyboard. It√¢s F11, currently mapped to muting/unmuting in Zoom. It√¢s *incredibly* rewarding to press.&lt;/p&gt;
    &lt;p&gt;I commissioned this √¢joystick√¢ from @benjedwards and I am so happy with how it turned out. It√¢s technically a joystick without a stick, but software turned it into a one-key keyboard. It√¢s F11, currently mapped to muting/unmuting in Zoom. It√¢s *incredibly* rewarding to press.&lt;/p&gt;
    &lt;p&gt;19.&lt;lb/&gt;Speaking of strange keyboards, this is my √¢space cadet√¢ keyboard √¢ a mini keyboard that outputs only spaces, and instead of legends, each key *feels* different. Wrote about it more here: Stop me if you√¢ve seen this one before&lt;/p&gt;
    &lt;p&gt;Speaking of strange keyboards, this is my √¢space cadet√¢ keyboard √¢ a mini keyboard that outputs only spaces, and instead of legends, each key *feels* different. Wrote about it more here: Stop me if you√¢ve seen this one before&lt;/p&gt;
    &lt;p&gt;20.&lt;lb/&gt;And here is a keyboard I built and hid in my shoes, made for one very specific reason. Are you interested what it is? Check out the whole story here: To walk among keyboard magicians&lt;/p&gt;
    &lt;p&gt;And here is a keyboard I built and hid in my shoes, made for one very specific reason. Are you interested what it is? Check out the whole story here: To walk among keyboard magicians&lt;/p&gt;
    &lt;p&gt;21.&lt;lb/&gt;This is one of the most rare keyboards I have √¢√Ç the strange abKey Evolution imported through a friend from Singapore √¢ a keyboard that tried to reinvent perhaps one thing too many. Wrote more about it here: The worst keyboard ever made&lt;/p&gt;
    &lt;p&gt;This is one of the most rare keyboards I have √¢√Ç the strange abKey Evolution imported through a friend from Singapore √¢ a keyboard that tried to reinvent perhaps one thing too many. Wrote more about it here: The worst keyboard ever made&lt;/p&gt;
    &lt;p&gt;22.√¢¬®&lt;lb/&gt;And this one from Commodore is not really that unique, except it has this fun property √¢√Ç it reverses the usual beige colour scheme making the keys inside darker. It√¢s kinda neat!&lt;/p&gt;
    &lt;p&gt;And this one from Commodore is not really that unique, except it has this fun property √¢√Ç it reverses the usual beige colour scheme making the keys inside darker. It√¢s kinda neat!&lt;/p&gt;
    &lt;p&gt;23.&lt;lb/&gt;This is a really cheap Bulgarian keyboard with such a poor build quality it cannot be unseen! I wrote more about it here: The worst keyboard ever made&lt;/p&gt;
    &lt;p&gt;This is a really cheap Bulgarian keyboard with such a poor build quality it cannot be unseen! I wrote more about it here: The worst keyboard ever made&lt;/p&gt;
    &lt;p&gt;24.&lt;lb/&gt;Oh, it gets worse. This calculator keyboard is so cheap it√¢s not a keyboard at all √¢ just an exposed PCB with a pen to complete the circuit. More about it here: The worst keyboard ever made&lt;/p&gt;
    &lt;p&gt;Oh, it gets worse. This calculator keyboard is so cheap it√¢s not a keyboard at all √¢ just an exposed PCB with a pen to complete the circuit. More about it here: The worst keyboard ever made&lt;/p&gt;
    &lt;p&gt;25.&lt;lb/&gt;And this is the opposite, an incredibly well-built IBM Model F banking typewriter with an enclosure made out of zinc. Hefty enough to stop a bank robbery? Perhaps. More here: To save a keyboard, pt. 2&lt;/p&gt;
    &lt;p&gt;And this is the opposite, an incredibly well-built IBM Model F banking typewriter with an enclosure made out of zinc. Hefty enough to stop a bank robbery? Perhaps. More here: To save a keyboard, pt. 2&lt;/p&gt;
    &lt;p&gt;Halfway through! I need a bit of a break. Is this interesting? Should I keep going!?&lt;/p&gt;
    &lt;p&gt;26.&lt;lb/&gt;If your bank robbery goes poorly, you probably end up typing on this Swintec, transparent so that no contraband could be hidden inside. More about transparent tech for prisons in this Techmoan video: YouTube&lt;/p&gt;
    &lt;p&gt;If your bank robbery goes poorly, you probably end up typing on this Swintec, transparent so that no contraband could be hidden inside. More about transparent tech for prisons in this Techmoan video: YouTube&lt;/p&gt;
    &lt;p&gt;27.&lt;lb/&gt;This simple braille keyboard √¢ Tellatouch √¢ was gorgeous and important. Type a key on one side, and the right braille letter assembles itself on the other.&lt;/p&gt;
    &lt;p&gt;This simple braille keyboard √¢ Tellatouch √¢ was gorgeous and important. Type a key on one side, and the right braille letter assembles itself on the other.&lt;/p&gt;
    &lt;p&gt;28.&lt;lb/&gt;This is a more modern version of an adjacent idea. Connect this device to a phone line, and you can speak even if you cannot talk. (Also, I just love any time a keyboard lands itself next to a segmented display.)&lt;/p&gt;
    &lt;p&gt;This is a more modern version of an adjacent idea. Connect this device to a phone line, and you can speak even if you cannot talk. (Also, I just love any time a keyboard lands itself next to a segmented display.)&lt;/p&gt;
    &lt;p&gt;29.&lt;lb/&gt;The creators of this Seiko keyboard recognized a watch with a keyboard wouldn√¢t make sense √¢ so you could dock your watch and type this way. (I don√¢t have the watch itself. Too expensive!)&lt;/p&gt;
    &lt;p&gt;The creators of this Seiko keyboard recognized a watch with a keyboard wouldn√¢t make sense √¢ so you could dock your watch and type this way. (I don√¢t have the watch itself. Too expensive!)&lt;/p&gt;
    &lt;p&gt;30.&lt;lb/&gt;Just kidding! Here√¢s a keyboard on another Seiko watch. It√¢s an index keyboard √¢ you don√¢t touch the keys directly, just move the cursor left and right like on Apple TV √¢√Ç since the keys are smaller than 1mm.&lt;/p&gt;
    &lt;p&gt;Just kidding! Here√¢s a keyboard on another Seiko watch. It√¢s an index keyboard √¢ you don√¢t touch the keys directly, just move the cursor left and right like on Apple TV √¢√Ç since the keys are smaller than 1mm.&lt;/p&gt;
    &lt;p&gt;31.&lt;lb/&gt;This TI calculator for school use has tiny keys√¢¬¶ in between other keys. What a strange thing.&lt;/p&gt;
    &lt;p&gt;This TI calculator for school use has tiny keys√¢¬¶ in between other keys. What a strange thing.&lt;/p&gt;
    &lt;p&gt;32.&lt;lb/&gt;This calculator went√¢¬¶ a different way.&lt;/p&gt;
    &lt;p&gt;This calculator went√¢¬¶ a different way.&lt;/p&gt;
    &lt;p&gt;33.&lt;lb/&gt;I love hybrid things and in-betweeners. This tiny Panasonic Toughbook asks a question: what if a BlackBerry keyboard, but twice the width?&lt;/p&gt;
    &lt;p&gt;I love hybrid things and in-betweeners. This tiny Panasonic Toughbook asks a question: what if a BlackBerry keyboard, but twice the width?&lt;/p&gt;
    &lt;p&gt;34.&lt;lb/&gt;This one, for TermiFlex, is a one-hand operation, inspired by phone keypads. There are three shifts under your long fingers!&lt;/p&gt;
    &lt;p&gt;This one, for TermiFlex, is a one-hand operation, inspired by phone keypads. There are three shifts under your long fingers!&lt;/p&gt;
    &lt;p&gt;35.&lt;lb/&gt;Speaking of complex shortcuts, look at this Apple keyboard with Avid software keycaps. The icon on Z is my favourite. I don√¢t even wanna know what this function does.&lt;/p&gt;
    &lt;p&gt;Speaking of complex shortcuts, look at this Apple keyboard with Avid software keycaps. The icon on Z is my favourite. I don√¢t even wanna know what this function does.&lt;/p&gt;
    &lt;p&gt;36.&lt;lb/&gt;One among many foldable keyboards √¢ this one for Palm devices (RIP).&lt;/p&gt;
    &lt;p&gt;One among many foldable keyboards √¢ this one for Palm devices (RIP).&lt;/p&gt;
    &lt;p&gt;37.&lt;lb/&gt;This Sony remote had a built-in keyboard for typing in MiniDisc titles.&lt;/p&gt;
    &lt;p&gt;This Sony remote had a built-in keyboard for typing in MiniDisc titles.&lt;/p&gt;
    &lt;p&gt;38.&lt;lb/&gt;And *this* Sony keyboard had two numeric keypads going in two different directions! One for typical calculator use, and one inspired by mobile phones to allow to chat as easily for people who got used to chatting that way.&lt;/p&gt;
    &lt;p&gt;And *this* Sony keyboard had two numeric keypads going in two different directions! One for typical calculator use, and one inspired by mobile phones to allow to chat as easily for people who got used to chatting that way.&lt;/p&gt;
    &lt;p&gt;39.&lt;lb/&gt;Very happy (and also maybe also a little concerned) to report I am in possession of the entire ProHance lineup of the strange pointing device/keyboard hybrids!&lt;/p&gt;
    &lt;p&gt;Very happy (and also maybe also a little concerned) to report I am in possession of the entire ProHance lineup of the strange pointing device/keyboard hybrids!&lt;/p&gt;
    &lt;p&gt;40.&lt;lb/&gt;But it√¢s amazing how rarely the graphical user interfaces and keyboards intersect. This here √¢ an old AT&amp;amp;T terminal keyboard √¢ is an exception, providing dedicated keys for window management.&lt;/p&gt;
    &lt;p&gt;But it√¢s amazing how rarely the graphical user interfaces and keyboards intersect. This here √¢ an old AT&amp;amp;T terminal keyboard √¢ is an exception, providing dedicated keys for window management.&lt;/p&gt;
    &lt;p&gt;41.&lt;lb/&gt;I had to get this keyboard for a now-obscure Harris word processor, just because LOOK AT THE SHAPE OF THIS ENTER KEY.&lt;/p&gt;
    &lt;p&gt;I had to get this keyboard for a now-obscure Harris word processor, just because LOOK AT THE SHAPE OF THIS ENTER KEY.&lt;/p&gt;
    &lt;p&gt;42.&lt;lb/&gt;I have seen so many keyboards, but only this one √¢ from a strange titling device meant to be connected to your TV √¢√Ç treats uppercase and lowercase exactly like all the other shifted and unshifted symbols. (With the exception of keyboards for kids, I assume!)&lt;/p&gt;
    &lt;p&gt;I have seen so many keyboards, but only this one √¢ from a strange titling device meant to be connected to your TV √¢√Ç treats uppercase and lowercase exactly like all the other shifted and unshifted symbols. (With the exception of keyboards for kids, I assume!)&lt;/p&gt;
    &lt;p&gt;43.&lt;lb/&gt;Back in the day, keyboards were so expensive that you often started on a √¢training√¢ keyboard that came without the machine connected to it. Here√¢s a training keyboard for a Linotype, which is itself a fascinating machine.&lt;/p&gt;
    &lt;p&gt;Back in the day, keyboards were so expensive that you often started on a √¢training√¢ keyboard that came without the machine connected to it. Here√¢s a training keyboard for a Linotype, which is itself a fascinating machine.&lt;/p&gt;
    &lt;p&gt;44.&lt;lb/&gt;Here√¢s another one for the first popular line of desk calculators that predates a 10-key keypad.&lt;/p&gt;
    &lt;p&gt;Here√¢s another one for the first popular line of desk calculators that predates a 10-key keypad.&lt;/p&gt;
    &lt;p&gt;45.&lt;lb/&gt;(I also have the actual calculator, called a Comptometer. It√¢s beautiful, really fun to use, and honestly a work of art. A truly impressive machine from the bygone era. I bought it because I was so impressed reading what it can do.)&lt;/p&gt;
    &lt;p&gt;(I also have the actual calculator, called a Comptometer. It√¢s beautiful, really fun to use, and honestly a work of art. A truly impressive machine from the bygone era. I bought it because I was so impressed reading what it can do.)&lt;/p&gt;
    &lt;p&gt;46.&lt;lb/&gt;Here√¢s another practice keyboard, with a record to play to teach you how to type!&lt;/p&gt;
    &lt;p&gt;Here√¢s another practice keyboard, with a record to play to teach you how to type!&lt;/p&gt;
    &lt;p&gt;47.&lt;lb/&gt;And here√¢s the most modern version of a practice keyboard I know of √¢ itself a small computer. After that, the likes of Mavis Beacon took over teaching typing in software.&lt;/p&gt;
    &lt;p&gt;And here√¢s the most modern version of a practice keyboard I know of √¢ itself a small computer. After that, the likes of Mavis Beacon took over teaching typing in software.&lt;/p&gt;
    &lt;p&gt;48.&lt;lb/&gt;Speaking of the 1980s, keyboards from failed computers often found a second life as Radio Shack components you could reuse in your DIY projects. Here√¢s one from a home computer called Coleco Adam.&lt;/p&gt;
    &lt;p&gt;Speaking of the 1980s, keyboards from failed computers often found a second life as Radio Shack components you could reuse in your DIY projects. Here√¢s one from a home computer called Coleco Adam.&lt;/p&gt;
    &lt;p&gt;49.&lt;lb/&gt;While we√¢re speaking about failed computers, this is One Laptop Per Child√¢s interesting-looking keyboard. (I think OLPC is considered a failure? I√¢m not 100% sure. This computer is not in the book, so I haven√¢t researched that carefully.)&lt;/p&gt;
    &lt;p&gt;While we√¢re speaking about failed computers, this is One Laptop Per Child√¢s interesting-looking keyboard. (I think OLPC is considered a failure? I√¢m not 100% sure. This computer is not in the book, so I haven√¢t researched that carefully.)&lt;/p&gt;
    &lt;p&gt;50. And here is Canon Cat, maybe my favourite failed machine of all time. Look at these Leap keys! I√¢m somewhat in love with this machine.&lt;lb/&gt;Adult-onset felinophilia&lt;/p&gt;
    &lt;p&gt;Adult-onset felinophilia&lt;/p&gt;
    &lt;p&gt;That√¢s it! I hope you liked this sneak peek of my collection√¢ if you did, consider backing the book since this is the level of quality I√¢ve been aiming at for the visual side√¢¬¶ there are a lot more photos like these, and of course a lot more great stories attached to them. &lt;lb/&gt;Shift Happens on Kickstarter&lt;/p&gt;
    &lt;p&gt;Shift Happens on Kickstarter&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aresluna.org/50-keyboards-from-my-collection/"/></entry><entry><id>https://news.ycombinator.com/item?id=45099418</id><title>Collecting All Causal Knowledge</title><updated>2025-09-02T16:12:42.147828+00:00</updated><content>&lt;doc fingerprint="ef076201388bcbc9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Collecting All Causal Knowledge&lt;/head&gt;
    &lt;p&gt;CauseNet aims at creating a causal knowledge base that comprises all human causal knowledge and to separate it from mere causal beliefs, with the goal of enabling large-scale research into causal inference.&lt;/p&gt;
    &lt;head rend="h1"&gt;CauseNet: Towards a Causality Graph Extracted from the Web&lt;/head&gt;
    &lt;p&gt;Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83\% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download&lt;/head&gt;
    &lt;p&gt;We provide three versions of our causality graph CauseNet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CauseNet-Full: The complete dataset&lt;/item&gt;
      &lt;item&gt;CauseNet-Precision: A subset of CauseNet-Full with higher precision&lt;/item&gt;
      &lt;item&gt;CauseNet-Sample: A small sample dataset for first explorations and experiments without provenance data&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Statistics&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Relations&lt;/cell&gt;
        &lt;cell role="head"&gt;Concepts&lt;/cell&gt;
        &lt;cell role="head"&gt;File Size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CauseNet-Full&lt;/cell&gt;
        &lt;cell&gt;11,609,890&lt;/cell&gt;
        &lt;cell&gt;12,186,195&lt;/cell&gt;
        &lt;cell&gt;1.8GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CauseNet-Precision&lt;/cell&gt;
        &lt;cell&gt;199,806&lt;/cell&gt;
        &lt;cell&gt;80,223&lt;/cell&gt;
        &lt;cell&gt;135MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;CauseNet-Sample&lt;/cell&gt;
        &lt;cell&gt;264&lt;/cell&gt;
        &lt;cell&gt;524&lt;/cell&gt;
        &lt;cell&gt;54KB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Data Model&lt;/head&gt;
    &lt;p&gt;The core of CauseNet consists of causal concepts which are connected by causal relations. Each causal relation has comprehensive provenance data on where and how it was extracted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Examples of Causal Relations&lt;/head&gt;
    &lt;p&gt;Causal relations are represented as shown in the following example. Provenance data is omitted.&lt;/p&gt;
    &lt;code&gt;{
    "causal_relation": {
        "cause": {
            "concept": "disease"
        },
        "effect": {
            "concept": "death"
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;For CauseNet-Full and CauseNet-Precision, we include comprehensive provenance data. In the following, we give one example per source.&lt;/p&gt;
    &lt;p&gt;For relations extracted from natural language sentences we provide:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;surface&lt;/code&gt;: the surface form of the sentence, i.e., the original string&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;path_pattern&lt;/code&gt;: the linguistic path pattern used for extraction&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ClueWeb12 Sentences&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;clueweb12_page_id&lt;/code&gt;: page id as provided in the ClueWeb12 corpus&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clueweb12_page_reference&lt;/code&gt;: page reference as provided in the ClueWeb12 corpus&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clueweb12_page_timestamp&lt;/code&gt;: page access data as stated in the ClueWeb12 corpus&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
    "causal_relation":{
        "cause":{
            "concept":"smoking"
        },
        "effect":{
            "concept":"disability"
        }
    },
    "sources":[
        {
            "type":"clueweb12_sentence",
            "payload":{
                "clueweb12_page_id":"urn:uuid:4cbae00e-8c7f-44b1-9f02-d797f53d448a",
                "clueweb12_page_reference":"http://atlas.nrcan.gc.ca/site/english/maps/health/healthbehaviors/smoking",
                "clueweb12_page_timestamp":"2012-02-23T21:10:45Z",
                "sentence": "In Canada, smoking is the most important cause of preventable illness, disability and premature death.",
                "path_pattern":"[[cause]]/N\t-nsubj\tcause/NN\t+nmod:of\t[[effect]]/N"
            }
        }
    ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Wikipedia Sentences&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;wikipedia_page_id&lt;/code&gt;: the Wikipedia page id&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;wikipedia_page_title&lt;/code&gt;: the Wikipedia page title&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;wikipedia_revision_id&lt;/code&gt;: the Wikipedia revision id of the last edit&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;wikipedia_revision_timestamp&lt;/code&gt;: the timestamp of the Wikipedia revision id of the last edit&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sentence_section_heading&lt;/code&gt;: the section heading where the sentence comes from&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sentence_section_level&lt;/code&gt;: the level where the section heading comes from&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
    "causal_relation":{
        "cause":{
            "concept":"human_activity"
        },
        "effect":{
            "concept":"climate_change"
        }
    },
    "sources":[
        {
            "type":"wikipedia_sentence",
            "payload":{
                "wikipedia_page_id":"13109",
                "wikipedia_page_title":"Global warming controversy",
                "wikipedia_revision_id":"860220175",
                "wikipedia_revision_timestamp":"2018-09-19T04:52:18Z",
                "sentence_section_heading":"Global warming controversy",
                "sentence_section_level":"1",
                "sentence": "The controversy is, by now, political rather than scientific: there is a scientific consensus that climate change is happening and is caused by human activity.",
                "path_pattern":"[[cause]]/N\t-nmod:agent\tcaused/VBN\t+nsubjpass\t[[effect]]/N"
            }
        }
    ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Wikipedia Lists&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;list_toc_parent_title&lt;/code&gt;: The heading of the parent section the list appears in&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;list_toc_section_heading&lt;/code&gt;: The heading of the section the list appears in&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;list_toc_section_level&lt;/code&gt;: The nesting level of the section within the table of content (toc)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
    "causal_relation":{
        "cause":{
            "concept":"separation_from_parents"
        },
        "effect":{
            "concept":"stress_in_early_childhood"
        }
    },
    "sources":[
        {
            "type":"wikipedia_list",
            "payload":{
                "wikipedia_page_id":"33096801",
                "wikipedia_page_title":"Stress in early childhood",
                "wikipedia_revision_id":"859225864",
                "wikipedia_revision_timestamp":"2018-09-12T16:22:05Z",
                "list_toc_parent_title":"Stress in early childhood",
                "list_toc_section_heading":"Causes",
                "list_toc_section_level":"2"
            }
        }
    ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Wikipedia Infoboxes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;infobox_template&lt;/code&gt;: The Wikipedia template of the infobox&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;infobox_title&lt;/code&gt;: The title of the Wikipedia infobox&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;infobox_argument&lt;/code&gt;: The argument of the infobox (the key of the key-value pair)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
    "causal_relation":{
        "cause":{
            "concept":"alcohol"
        },
        "effect":{
            "concept":"cirrhosis"
        }
    },
    "sources":[
        {
            "type":"wikipedia_infobox",
            "payload":{
                "wikipedia_page_id":"21365918",
                "wikipedia_page_title":"Cirrhosis",
                "wikipedia_revision_id":"861860835",
                "wikipedia_revision_timestamp":"2018-09-30T15:40:21Z",
                "infobox_template":"Infobox medical condition (new)",
                "infobox_title":"Cirrhosis",
                "infobox_argument":"causes"
            }
        }
    ]
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Loading CauseNet into Neo4j&lt;/head&gt;
    &lt;p&gt;We provide sample code to load CauseNet into the graph database Neo4j.&lt;/p&gt;
    &lt;p&gt;The following figure shows an excerpt of CauseNet within Neo4j (showing a coronavirus causing the disease SARS):&lt;/p&gt;
    &lt;head rend="h2"&gt;Concept Spotting Datasets&lt;/head&gt;
    &lt;p&gt;For the construction of CauseNet, we employ a causal concept spotter as a causal concept can be composed of multiple words (e.g., ‚Äúglobal warming‚Äù, ‚Äúhuman activity‚Äù, or ‚Äúlack of exercise‚Äù). We determine the exact start and end of a causal concept in a sentence with a sequence tagger. Our training and evaluation data is available as part of our concept spotting datasets: one for Wikipedia infoboxes, Wikipedia lists, and ClueWeb sentences. We split each dataset into 80% training, 10% development and 10% test set&lt;/p&gt;
    &lt;head rend="h2"&gt;Paper&lt;/head&gt;
    &lt;p&gt;CauseNet forms the basis for our CIKM 2020 paper CauseNet: Towards a Causality Graph Extracted from the Web. Please make sure to refer to it as follows:&lt;/p&gt;
    &lt;code&gt;@inproceedings{heindorf2020causenet,
  author    = {Stefan Heindorf and
               Yan Scholten and
               Henning Wachsmuth and
               Axel-Cyrille Ngonga Ngomo and
               Martin Potthast},
  title     = {CauseNet: Towards a Causality Graph Extracted from the Web},
  booktitle = {{CIKM}},
  publisher = {{ACM}},
  year      = {2020}
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Contact&lt;/head&gt;
    &lt;p&gt;For questions and feedback please contact:&lt;/p&gt;
    &lt;p&gt;Stefan Heindorf, Paderborn University&lt;lb/&gt; Yan Scholten, Technical University of Munich&lt;lb/&gt; Henning Wachsmuth, Paderborn University&lt;lb/&gt; Axel-Cyrille Ngonga Ngomo, Paderborn University&lt;lb/&gt; Martin Potthast, Leipzig University&lt;/p&gt;
    &lt;head rend="h2"&gt;Licenses&lt;/head&gt;
    &lt;p&gt;The code is licensed under a MIT license. The data is licensed under a Creative Commons Attribution 4.0 International license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://causenet.org/"/></entry><entry><id>https://news.ycombinator.com/item?id=45099922</id><title>Next.js is infuriating</title><updated>2025-09-02T16:12:41.509345+00:00</updated><content>&lt;doc fingerprint="d05093007b4164b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Hey, it's finally happened. I've decided to write a blog post. And if you're reading this, I've also finished one. I have wanted to do this for a long time, but could never find the motivation to start. But you know what they say: anger is the best motivator. They do say that, right?&lt;/p&gt;
    &lt;head rend="h3"&gt;Some context that's in the background&lt;/head&gt;
    &lt;p&gt;We're going on a journey, you and I. But first, we need to set the scene. Imagine we're working for $COMPANY and one of our Next.js services did an oopsie. This being Next.js, we of course have no idea what actually happened since the default logging is only enabled during development.&lt;/p&gt;
    &lt;p&gt;Our quest is to go in and setup some production ready logging. It's not going to be easy, but then again, nothing ever is.&lt;/p&gt;
    &lt;head rend="h3"&gt;Middleware? Middle of nowhere!&lt;/head&gt;
    &lt;p&gt;The first step of our journey is the middleware. The documentation even states this:&lt;/p&gt;
    &lt;quote&gt;Middleware executes before routes are rendered. It's particularly useful for implementing custom server-side logic like authentication, logging, or handling redirects.&lt;/quote&gt;
    &lt;p&gt;Alright, looks simple enough. Time to pick a logging library. I went with pino since we have used it before. Anything is an upgrade over &lt;code&gt;console.log&lt;/code&gt; anyways. We'll get this done before lunch.&lt;/p&gt;
    &lt;p&gt;Let's set up a basic middleware:&lt;/p&gt;
    &lt;code&gt;// middleware.ts
import { NextResponse, NextRequest } from "next/server";

export async function middleware(request: NextRequest) {
  return new NextResponse.next({
    request: request,
    headers: request.headers,
    // status: 200,
    // statusText: 'OK'
  });
}

export const config = {
  matcher: "/:path*",
};&lt;/code&gt;
    &lt;p&gt;I think we already have a problem here. You can pass a grand total of 4 parameters from your middleware. The only thing that actually affects the invoked route is the &lt;code&gt;headers&lt;/code&gt;. Let's not skip over the fact that you can't have multiple middlewares or chain them either. How do you fuck this up so bad? We've had middlewares since at least the early 2010s when Express came out.&lt;/p&gt;
    &lt;p&gt;Anyways, we're smart and modern Node.js has some pretty nifty tools. Let's reach for &lt;code&gt;AsyncLocalStorage&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;// app/logger.ts
import { AsyncLocalStorage } from "async_hooks";
import { Logger, pino } from "pino";

const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "trace",
});

export const LoggerStorage = new AsyncLocalStorage&amp;lt;Logger&amp;gt;();

export function logger(): Logger | null {
  return LoggerStorage.getStore() ?? null;
}

export function requestLogger(): Logger {
  return loggerInstance.child({ requestId: crypto.randomUUID() });
}

// middleware.ts
export async function middleware(request: NextRequest) {
  LoggerStorage.enterWith(requestLogger());
  logger()?.debug({ url: request.url }, "Started processing request!");

  return NextResponse.next();
}&lt;/code&gt;
    &lt;p&gt;Whew, hard work done. Let's test it out. Visit localhost:3000 and we see this:&lt;/p&gt;
    &lt;code&gt;{ requestId: 'ec7718fa-b1a2-473e-b2e2-8f51188efa8f' } { url: 'http://localhost:3000/' } 'Started processing request!'
 GET / 200 in 71ms
{ requestId: '09b526b1-68f4-4e90-971f-b0bc52ad167c' } { url: 'http://localhost:3000/next.svg' } 'Started processing request!'
{ requestId: '481dd2ff-e900-4985-ae15-0b0a1eb5923f' } { url: 'http://localhost:3000/vercel.svg' } 'Started processing request!'
{ requestId: 'e7b29301-171c-4c91-af25-771471502ee4' } { url: 'http://localhost:3000/file.svg' } 'Started processing request!'
{ requestId: '13766de3-dd00-42ce-808a-ac072dcfd4c6' } { url: 'http://localhost:3000/window.svg' } 'Started processing request!'
{ requestId: '317e054c-1a9a-4dd8-ba21-4c0201fbeada' } { url: 'http://localhost:3000/globe.svg' } 'Started processing request!'&lt;/code&gt;
    &lt;p&gt;I don't know if you've ever used pino before, but this is wrong. Can you figure out why?&lt;/p&gt;
    &lt;p&gt;Unlike Next.js I won't keep you waiting in limbo. This is the browser output. Why? Well, it's because the default Next.js middleware runtime is &lt;code&gt;edge&lt;/code&gt;. We can of course switch to the &lt;code&gt;nodejs&lt;/code&gt; runtime which should work. Except, of course, it might not.&lt;/p&gt;
    &lt;p&gt;I've tried it on a fresh Next.js project and it does work. But it didn't when I was trying it out on our actual project. I swear I'm not crazy. Anyways, this isn't the main issue. We're slowly getting there.&lt;/p&gt;
    &lt;head rend="h3"&gt;Paging the local mental asylum&lt;/head&gt;
    &lt;p&gt;Logging in the middleware is cool and all, but it's not where the bulk of the magic happens. For that, we need to log in pages and layouts. Let's try it out.&lt;/p&gt;
    &lt;code&gt;// app/page.tsx
export default function Home() {
  logger()?.info("Logging from the page!");

  return &amp;lt;div&amp;gt;Real simple website!&amp;lt;/div&amp;gt;
}&lt;/code&gt;
    &lt;p&gt;Refresh the page and we get this:&lt;/p&gt;
    &lt;code&gt;‚úì Compiled / in 16ms
 GET / 200 in 142ms&lt;/code&gt;
    &lt;p&gt;That's it? That's it. Nothing. Nada. Zilch.&lt;/p&gt;
    &lt;p&gt;For posterity's sake, this is what it's supposed to look like:&lt;/p&gt;
    &lt;code&gt;‚úì Compiled / in 2.2s
[11:38:59.259] INFO (12599): Logging from the page!
    requestId: "2ddef9cf-6fee-4d1d-8b1e-6bb16a3e636b"
 GET / 200 in 2520ms&lt;/code&gt;
    &lt;p&gt;Ok,this is getting a bit long, so I'll get to the point. The &lt;code&gt;logger&lt;/code&gt; function returns &lt;code&gt;null&lt;/code&gt;. Why? I'm not entirely sure, but it seems like rendering is not executed in the same async context as the middleware.&lt;/p&gt;
    &lt;p&gt;What's the solution then? You're not going to believe this. Remember how the only value you can pass from the middleware is &lt;code&gt;headers&lt;/code&gt;? Yeah. That's what we need to use.&lt;/p&gt;
    &lt;p&gt;The following is for people with strong stomachs:&lt;/p&gt;
    &lt;code&gt;// app/log/serverLogger.ts
import { pino } from "pino";

export const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "info",
});

// app/log/middleware.ts
// Yes, we need to split up the loggers ...
// Mostly the same as before
import { loggerInstance } from "./serverLogger";

export function requestLogger(requestId: string): Logger {
  return loggerInstance.child({ requestId });
}

// app/log/server.ts
import { headers } from "next/headers";
import { loggerInstance } from "./serverLogger";
import { Logger } from "pino";
import { NextRequest } from "next/server";

const REQUEST_ID_HEADER = "dominik-request-id";

export function requestHeaders(
  request: NextRequest,
  requestId: string,
): Headers {
  const head = new Headers(request.headers);
  head.set(REQUEST_ID_HEADER, requestId);
  return head;
}

// Yeah, this has to be async ...
export async function logger(): Promise&amp;lt;Logger&amp;gt; {
  const hdrs = await headers();
  const requestId = hdrs.get(REQUEST_ID_HEADER);

  return loggerInstance.child({ requestId });
}

// middleware.ts
import { logger, LoggerStorage, requestLogger } from "./app/log/middleware";
import { requestHeaders } from "./app/log/server";

export async function middleware(request: NextRequest) {
  const requestId = crypto.randomUUID();
  LoggerStorage.enterWith(requestLogger(requestId));

  logger()?.debug({ url: request.url }, "Started processing request!");

  return NextResponse.next({ headers: requestHeaders(request, requestId) });
}

// app/page.tsx
export default async function Home() {
  (await logger())?.info("Logging from the page!");

  // ...
}&lt;/code&gt;
    &lt;p&gt;Isn't it beautiful? I especially appreciate how it's now possible to import the middleware logging code from the server. Which of course won't work. Or import the server logging code from the middleware. Which also won't work. Better not mess up. Also, we haven't even touched upon logging in client components, which despite the name also run on the server. Yeah, that's a third split.&lt;/p&gt;
    &lt;head rend="h3"&gt;Congratulations, you're being coddled. Please do not resist.&lt;/head&gt;
    &lt;p&gt;Listen. I wanted to apologize, because I've led you into this trap. You see, I have already fallen into it several times before. A middleware system can be pretty useful when designed correctly and I wanted you to see what it looks like when it's not. The reason for writing this blog post actually started here.&lt;/p&gt;
    &lt;p&gt;I think every one of us has reached a point in their lives where they've had enough. For me, it was right here. Fuck it, let's use a custom server.&lt;/p&gt;
    &lt;quote&gt;A custom Next.js server allows you to programmatically start a server for custom patterns. The majority of the time, you will not need this approach. However, it's available if you need to eject.&lt;/quote&gt;
    &lt;p&gt;Let's take a look at the example from the documentation:&lt;/p&gt;
    &lt;code&gt;import { createServer } from 'http'
import { parse } from 'url'
import next from 'next'
 
const port = parseInt(process.env.PORT || '3000', 10)
const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handle = app.getRequestHandler()
 
app.prepare().then(() =&amp;gt; {
  createServer((req, res) =&amp;gt; {
    const parsedUrl = parse(req.url!, true)
    handle(req, res, parsedUrl)
  }).listen(port)
 
  console.log(
    `&amp;gt; Server listening at http://localhost:${port} as ${
      dev ? 'development' : process.env.NODE_ENV
    }`
  )
})&lt;/code&gt;
    &lt;p&gt;Note that once again, &lt;code&gt;handle&lt;/code&gt; doesn't really take any parameters. Only the request URL and the raw request and response.&lt;/p&gt;
    &lt;p&gt;Anyways, we still have &lt;code&gt;AsyncLocalStorage&lt;/code&gt; so this doesn't concern us too much. Let's modify the example a bit.&lt;/p&gt;
    &lt;code&gt;// app/logger.ts
// Reverted back to our AsyncLocalStorage variaton
import { pino, Logger } from "pino";
import { AsyncLocalStorage } from "async_hooks";

const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "info",
});

export const LoggerStorage = new AsyncLocalStorage&amp;lt;Logger&amp;gt;();

export function logger(): Logger | null {
  return LoggerStorage.getStore() ?? null;
}

export function requestLogger(): Logger {
  return loggerInstance.child({ requestId: crypto.randomUUID() });
}

// server.ts
import { logger, LoggerStorage, requestLogger } from "./app/logger";

app.prepare().then(() =&amp;gt; {
  createServer(async (req, res) =&amp;gt; {
    // This is new
    LoggerStorage.enterWith(requestLogger());
    logger()?.info({}, "Logging from server!");

    const parsedUrl = parse(req.url!, true);
    await handle(req, res, parsedUrl);
  }).listen(port);
});

// middleware.ts
import { logger } from "./app/logger";

export async function middleware(request: NextRequest) {
  logger()?.info({}, "Logging from middleware!");
  return NextResponse.next();
}

// app/page.tsx
import { logger } from "./logger";

export default async function Home() {
  logger()?.info("Logging from the page!");
  
  // ...
}&lt;/code&gt;
    &lt;p&gt;Ok, let's test it out. Refresh the browser and ...&lt;/p&gt;
    &lt;code&gt;&amp;gt; Server listening at http://localhost:3000 as development
[12:29:52.183] INFO (19938): Logging from server!
    requestId: "2ffab9a2-7e15-4188-8959-a7822592108f"
 ‚úì Compiled /middleware in 388ms (151 modules)
 ‚óã Compiling / ...
 ‚úì Compiled / in 676ms (769 modules)&lt;/code&gt;
    &lt;p&gt;That's it. Are you fucking kidding me right now? What the fuck?&lt;/p&gt;
    &lt;p&gt;Now, you might be thinking that this is just not how &lt;code&gt;AsyncLocalStorage&lt;/code&gt; works. And you might be right. But I would like to point out that &lt;code&gt;headers()&lt;/code&gt; and &lt;code&gt;cookies()&lt;/code&gt; use &lt;code&gt;AsyncLocalStorage&lt;/code&gt;. This is a power that the Next.js devs have that we don't.&lt;/p&gt;
    &lt;p&gt;As far as I can tell there are only two ways to pass information from a middleware to a page.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Headers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NextResponse.redirect&lt;/code&gt;/&lt;code&gt;NextResponse.rewrite&lt;/code&gt;to a route with extra params (eg. /[requestId]/page.tsx)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As you might have noticed, neither of these are very pleasant to use in this case.&lt;/p&gt;
    &lt;p&gt;You are being coddled. The Next.js devs have a vision and it's either their way or the highway. Note that if it was just the middleware, I wouldn't be sitting here, wasting away my weekend, ranting about a React framework. Believe it or not, I've got better things to do. It's constant pain you encounter daily when working with Next.js.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vercel can do better&lt;/head&gt;
    &lt;p&gt;What's infuriating about this example is that Vercel can very much do better. I don't want to sing too many praises at Svelte(Kit) because I have some misgivings about its recent direction, but it's so much better than Next.js. Let's look at their middleware docs:&lt;/p&gt;
    &lt;quote&gt;handle - This function runs every time the SvelteKit server receives a request [...] This allows you to modify response headers or bodies, or bypass SvelteKit entirely (for implementing routes programmatically, for example).&lt;/quote&gt;
    &lt;p&gt;Looking good so far.&lt;/p&gt;
    &lt;quote&gt;locals - To add custom data to the request, which is passed to handlers in&lt;code&gt;+server.js&lt;/code&gt;and server&lt;code&gt;load&lt;/code&gt;functions, populate the&lt;code&gt;event.locals&lt;/code&gt;object, as shown below.&lt;/quote&gt;
    &lt;p&gt;I'm crying tears of joy right now. You can also stuff real objects/classes in there. Like a logger for instance.&lt;/p&gt;
    &lt;quote&gt;You can define multiple handle functions and execute them with sequence.&lt;/quote&gt;
    &lt;p&gt;This is what real engineering looks like. SvelteKit is a Vercel product. How is the flagship offering worse than what is essentially a side project. What the hell?&lt;/p&gt;
    &lt;head rend="h3"&gt;Scientists discover a new super massive black hole at https://github.com/vercel/next.js/issues&lt;/head&gt;
    &lt;p&gt;I don't have anything else to add, but while I'm here I feel like I have to talk about the GitHub issue tracker. This is perhaps the crown jewel of the dumpster fire that is Next.js. It's a place where hopes and issues come to die. The mean response time for a bug report is never. I've made it a sport to search the issue tracker/discussion for problems I'm currently facing and bet on how many years it takes to even get a response from a Next.js dev.&lt;/p&gt;
    &lt;p&gt;You think I'm joking? There are hundreds of issues with as many üëç emojis with no official response for years. And when you finally get a response, it's to tell you that what you're doing is wrong and a solution to your real problems is on the way. Then they proceed to keep the "solution" in canary for years on end.&lt;/p&gt;
    &lt;p&gt;I personally reported two issues a year ago. Keep in mind that to have a valid bug report, you need a reproduction.&lt;/p&gt;
    &lt;p&gt;So, what do you get for taking the time to make a minimal reproduction? That's right. Complete silence.&lt;/p&gt;
    &lt;p&gt;I would have reported about a dozen other issues I have encountered, but after this experience I gave up.&lt;/p&gt;
    &lt;p&gt;Honestly, I don't even know if the issues are still valid.&lt;/p&gt;
    &lt;head rend="h3"&gt;Have we learned anything?&lt;/head&gt;
    &lt;p&gt;I don't know. For me, personally, I don't want to use Next.js anymore. You might think that this is just a singular issue and I'm overreacting. But there's bugs and edge cases around every corner. How did they manage to make TypeScript compile slower than Rust? Why make a distinction between code running on client and server and then not give me any tools to take advantage of that? Why? Why? Why?&lt;/p&gt;
    &lt;p&gt;I don't think I quite have enough pull to move us out of Next.js land. But, I think I will voice my opinion if we end up writing another app. We'll see if the grass is any greener on the other side.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.meca.sh/3lxoty3shjc2z"/></entry><entry><id>https://news.ycombinator.com/item?id=45099939</id><title>Kapa.ai (YC S23) is hiring research and software engineers</title><updated>2025-09-02T16:12:40.672026+00:00</updated><content>&lt;doc fingerprint="bdc48a3b7f7b6dd6"&gt;
  &lt;main&gt;
    &lt;p&gt;The fastest way to build AI assistants on technical content&lt;/p&gt;
    &lt;p&gt;We make it easy for technical companies to build AI assistants. Companies like Docker, Grafana and Mixpanel deploy kapa in the following ways:&lt;/p&gt;
    &lt;p&gt;We leverage companies existing technical knowledge sources including documentation, tutorials, forum posts, Slack channels, GitHub issues and many more to generate AI assistants that can handle complicated technical questions. More than 200 companies use kapa and we have answered more than 10 million questions to date.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/kapa-ai/jobs"/></entry><entry><id>https://news.ycombinator.com/item?id=45100499</id><title>Run Erlang/Elixir on Microcontrollers and Embedded Linux</title><updated>2025-09-02T16:12:39.906111+00:00</updated><content>&lt;doc fingerprint="3ddf79035115e1e6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GRiSP Software Stacks&lt;/head&gt;
    &lt;head rend="h2"&gt;Bring Erlang/Elixir all the way to the edge. Deterministic, fault-tolerant, and production-ready&lt;/head&gt;
    &lt;p&gt;Open-source on GitHub&lt;/p&gt;
    &lt;head rend="h1"&gt;GRiSP Metal&lt;/head&gt;
    &lt;head rend="h2"&gt;Erlang/Elixir on RTEMS. &lt;lb/&gt;Tiny BEAM for devices.&lt;/head&gt;
    &lt;p&gt;GRiSP Metal, formerly just GRiSP, boots straight into the Erlang/Elixir VM on RTEMS for deterministic, real‚Äëtime behavior with a minimal footprint. It runs on microcontrollers, and we've made the full stack fit in 16 MB of RAM, ideal when every byte and millisecond matter.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Boots directly to the BEAM (Erlang/Elixir) on RTEMS&lt;/item&gt;
      &lt;item&gt;MCU-class footprint (fits in 16 MB RAM)&lt;/item&gt;
      &lt;item&gt;Real-time scheduling with predictable I/O&lt;/item&gt;
      &lt;item&gt;Direct, low-overhead access to hardware interfaces&lt;/item&gt;
      &lt;item&gt;Supervision trees bring BEAM reliability to the edge&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;GRiSP Alloy&lt;/head&gt;
    &lt;head rend="h2"&gt;Erlang/Elixir on Linux RT. &lt;lb/&gt;Buildroot edition.&lt;/head&gt;
    &lt;p&gt;GRiSP Alloy boots directly into the Erlang/Elixir VM atop a lean, Buildroot-based real-time Linux. Run multiple Erlang/Elixir VMs with distinct priorities and/or pinned to different cores, connected via efficient, secure distributed Erlang links.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimal Linux RT image (Buildroot) with BEAM-first boot path&lt;/item&gt;
      &lt;item&gt;Multiple BEAM instances with priority separation and core affinity&lt;/item&gt;
      &lt;item&gt;Local, secure node-to-node links via distributed Erlang&lt;/item&gt;
      &lt;item&gt;Full access to Linux drivers, filesystems, and networking&lt;/item&gt;
      &lt;item&gt;Fast boot, small attack surface, production-ready&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;GRiSP Forge&lt;/head&gt;
    &lt;head rend="h2"&gt;Erlang/Elixir on Linux RT.&lt;lb/&gt;Yocto edition.&lt;/head&gt;
    &lt;p&gt;GRiSP Forge applies the same architecture as GRiSP Alloy to Yocto, for teams requiring long-term, customizable Linux stacks and BSP integration. Boots directly into the Erlang/Elixir VM with the same multi-VM model and secure local links via distributed Erlang.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yocto-based builds with reproducible, customizable images&lt;/item&gt;
      &lt;item&gt;Multiple Erlang/Elixir VMs by design (priorities and/or core pinning)&lt;/item&gt;
      &lt;item&gt;Efficient, secure local links via distributed Erlang&lt;/item&gt;
      &lt;item&gt;Industrial Linux ecosystem compatibility and tooling&lt;/item&gt;
      &lt;item&gt;Built for long lifecycles and enterprise requirements&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;GRiSP-io: Manage Embedded Systems at Scale&lt;/head&gt;
    &lt;p&gt;GRiSP-io is our cloud and edge platform for deploying, monitoring, and managing distributed embedded systems built with GRiSP stacks. From remote updates to real-time system insights, it helps you stay in control‚Äîwhether you're testing a prototype or scaling a fleet.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deploy and manage GRiSP-based devices remotely&lt;/item&gt;
      &lt;item&gt;Monitor system performance and health in real-time&lt;/item&gt;
      &lt;item&gt;Perform over-the-air updates with confidence&lt;/item&gt;
      &lt;item&gt;Integrate cloud and edge control into your workflows&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Why Use GRiSP Software?&lt;/head&gt;
    &lt;head rend="h2"&gt;GRiSP brings the power of Erlang/Elixir to embedded systems, making development efficient, scalable, and fault-tolerant&lt;/head&gt;
    &lt;head rend="h2"&gt;For Developers&lt;/head&gt;
    &lt;p&gt;GRiSP enables developers to run Erlang and Elixir on bare metal or embedded Linux, reducing complexity with minimal overhead and real-time capabilities. With GRiSP stacks and GRiSP-io, they can build and deploy robust, distributed applications optimized for embedded environments.&lt;/p&gt;
    &lt;head rend="h2"&gt;For IoT &amp;amp; Industrial Systems&lt;/head&gt;
    &lt;p&gt;From prototyping to production, GRiSP provides open-source tools that scale with project needs. Its real-time execution supports automation, robotics, and connected devices, while GRiSP-io enables remote management and monitoring of deployments.&lt;/p&gt;
    &lt;head rend="h1"&gt;Manage Embedded Systems with GRiSP-io&lt;/head&gt;
    &lt;p&gt;Deploy, monitor, and control your connected devices from anywhere. GRiSP-io brings cloud and edge integration to embedded applications, simplifying system management at scale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.grisp.org/software"/></entry><entry><id>https://news.ycombinator.com/item?id=45101300</id><title>What's New with Firefox 142</title><updated>2025-09-02T16:12:39.658095+00:00</updated><content>&lt;doc fingerprint="df0ca1d73ef8abed"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Less clutter and more focus with vertical tabs&lt;/head&gt;&lt;p&gt;Firefox‚Äôs new sidebar lets you move tabs to the side, pin key sites and keep your AI assistant handy.&lt;/p&gt;Learn more&lt;p&gt;What‚Äôs New | Firefox 142&lt;/p&gt;&lt;p&gt;Firefox Relay generates secure email masks when you sign up for new online accounts, so you can stay anonymous and get less spam in your inbox.&lt;/p&gt;&lt;p&gt;Firefox helps you preview links and keep tabs tidy so nothing slows you down.&lt;/p&gt;&lt;p&gt;Link Previews&lt;/p&gt;&lt;p&gt;Link Previews show a snapshot of a page before you open it, helping you decide what‚Äôs worth your time. Just long press any link to preview and reduce distractions.&lt;/p&gt;&lt;p&gt;AI-Enhanced Tab Groups&lt;/p&gt;&lt;p&gt;A local AI model identifies similar tabs, automatically organizes them into groups, and even suggests group names, helping you stay organized. Everything happens on your device to respect your privacy.&lt;/p&gt;&lt;p&gt;Stay organized, find what you need, and browse your way.&lt;/p&gt;&lt;p&gt;Firefox‚Äôs new sidebar lets you move tabs to the side, pin key sites and keep your AI assistant handy.&lt;/p&gt;Learn more&lt;p&gt;Create a space that‚Äôs uniquely yours. Set the mood with Firefox wallpapers, custom colors, or your own photo. Open a new tab and click the icon to start customizing.&lt;/p&gt;Learn more&lt;p&gt;No more tab overload. Stay organized and in control of your time and attention. Drag one tab onto another to create your first group.&lt;/p&gt;Learn more&lt;p&gt;Get monthly how-tos, advice and news to make your Firefox experience work best for you.&lt;/p&gt;&lt;p&gt;New tools for focus, privacy, and smoother mobile browsing.&lt;/p&gt;&lt;p&gt;Your private tabs lock automatically when you step away ‚Äî and only unlock with your face, fingerprint, or PIN.&lt;/p&gt;&lt;p&gt;Now translate web pages into Japanese, Chinese, Korean and more, so you can browse in your preferred language.&lt;/p&gt;&lt;p&gt;Firefox suggests strong passwords when you‚Äôre creating a new account on any site, and keeps them secure, ready on any device when you sync.&lt;/p&gt;&lt;p&gt;A streamlined UI and upgraded dark mode on Firefox for iOS bring clarity and calm to everything you browse.&lt;/p&gt;&lt;p&gt;Scan the QR code to get Firefox Mobile and browse with calm, focus, and control ‚Äî wherever you go.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.mozilla.org/en-US/firefox/142.0.1/whatsnew/?oldversion=139.0.4&amp;utm_medium=firefox-desktop&amp;utm_source=update&amp;utm_campaign=142"/></entry><entry><id>https://news.ycombinator.com/item?id=45101794</id><title>The Sudden Surges That Forge Evolutionary Trees</title><updated>2025-09-02T16:12:39.379083+00:00</updated><content>&lt;doc fingerprint="b7d4d1f59591835c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Sudden Surges That Forge Evolutionary Trees&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Over the last half-billion years, squid, octopuses and their kin have evolved much like a fireworks display, with long, anticipatory pauses interspersed with intense, explosive changes. The many-armed diversity of cephalopods is the result of the evolutionary rubber hitting the road right after lineages split into new species, and precious little of their evolution has been the slow accumulation of gradual change.&lt;/p&gt;
    &lt;p&gt;They aren‚Äôt alone. Sudden accelerations spring from the crooks of branches in evolutionary trees, across many scales of life ‚Äî seemingly wherever there‚Äôs a branching system of inherited modifications ‚Äî in a dynamic not examined in traditional evolutionary models.&lt;/p&gt;
    &lt;p&gt;That‚Äôs the perspective emerging from a new mathematical framework published in Proceedings of the Royal Society B that describes the pace of evolutionary change. The new model, part of a roughly 50-year-long reimagining of evolution‚Äôs tempo, is rooted in the concept of punctuated equilibrium, which was introduced by the paleontologists Niles Eldredge and Stephen Jay Gould in 1972.&lt;/p&gt;
    &lt;p&gt;‚ÄúSpecies would just sit still in the fossil record for millions of years, and then all of a sudden ‚Äî bang! ‚Äî they would turn into something else,‚Äù explained Mark Pagel, an evolutionary biologist at the University of Reading in the United Kingdom.&lt;/p&gt;
    &lt;p&gt;Punctuated equilibrium was initially a controversial proposal. The theory diverged from the dominant, century-long view that evolution adhered to a slow, steady pace of Darwinian gradualism, in which species incrementally and almost imperceptibly developed into new ones. It opened the confounding possibility that there was a discontinuity between the selection processes behind the microevolutionary changes that occur within a population and those driving the long-term, broad-scale changes that take place higher than the species level, known as macroevolution.&lt;/p&gt;
    &lt;p&gt;In the decades since, researchers have continued to debate these views as they‚Äôve gathered more data: Paleontologists have accumulated fossil datasets tracing macroevolutionary changes in ancient lineages, while molecular biologists have reconstructed microevolution on a more compressed timescale ‚Äî in DNA and the proteins they encode.&lt;/p&gt;
    &lt;p&gt;Now there are enough datasets to more fully test the theories of evolutionary change. Recently, a team of scientists blended insights from several evolutionary models with new methods to build a mathematical framework that better captures real evolutionary processes. When the team applied their tools to a selection of evolutionary datasets (including their own data from research into an ancient protein family), they found that evolutionary spikes weren‚Äôt just common, but somewhat predictably clustered at the forks in the evolutionary tree.&lt;/p&gt;
    &lt;p&gt;Their model showed that proteins contort themselves into new iterations more rapidly around the time they diverge from each other. Human languages twist and recast themselves at the bifurcations in their own family tree. Cephalopods‚Äô soft bodies sprout arms and bloom with suckers at these same splits.&lt;/p&gt;
    &lt;p&gt;The new study adds to previous support for the punctuated equilibrium phenomenon, said Pagel, who wasn‚Äôt involved in the project. However, the rapid evolutionary behavior isn‚Äôt a unique process separate from natural selection, as Eldredge and Gould suggested, but rather the result of periods of extremely rapid adaptation propelling evolutionary change.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis is really a rather beautiful story in the philosophy of science,‚Äù Pagel said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Phantom Bursts&lt;/head&gt;
    &lt;p&gt;Jordan Douglas, an evolutionary biologist at the Australian National University in Canberra, is fascinated by the origins of the genetic code. To understand those first stages of life‚Äôs evolution, he studies aminoacyl-tRNA synthetases (aaRSs), a family of enzymes essential to building proteins. The aaRS enzymes appear to predate the last universal common ancestor for all life on the planet.&lt;/p&gt;
    &lt;p&gt;‚ÄúThese enzymes are responsible for creating that kind of reflexive logic that nature uses to build itself, by helping to translate RNA into proteins which copy RNA, which build more proteins, which copy more RNA,‚Äù Douglas said.&lt;/p&gt;
    &lt;p&gt;Marcelo Cortes&lt;/p&gt;
    &lt;p&gt;Douglas administers a growing database of aaRS structures and sequences from across the tree of life, which he and his colleagues use to reconstruct the protein family‚Äôs roughly 4-billion-year evolutionary history. Studying the sequences, he observed that these enzymes must have evolved in very fast bursts. Although the molecules are not species per se, they changed through time in a bifurcating treelike pattern, much like populations of organisms, as their new forms created branches with quasi-species. Their evolutionary tree pattern reminded Douglas, who did this research while working at the University of Auckland, and his colleagues of the debate about punctuated equilibrium.&lt;/p&gt;
    &lt;p&gt;The team went down a ‚Äúrabbit hole,‚Äù Douglas said, to surface and assess any evidence in support of Eldredge and Gould‚Äôs theory ‚Äî datasets on everything from the evolution of mammalian body size to Australasian parrots‚Äô specialized digestive system for slurping nectar, to the early global spread of the virus behind the Covid-19 pandemic. They wanted to build a cohesive model of how punctuated equilibria take shape across many forms and scales of life, including and beyond enzymes. They were especially curious about those elusive moments where one species becomes two.&lt;/p&gt;
    &lt;p&gt;A key part of their approach introduced ‚Äúspikes,‚Äù a model parameter that measures the amount of change that occurs every time a branch appears. ‚Äú[The spike is] a novel contribution of this algorithm that is not usually done in phylogenetics,‚Äù Douglas said.&lt;/p&gt;
    &lt;p&gt;The paradigm in phylogenetics (the study of evolutionary relationships), he said, is that changes happen not only slowly and gradually, but often independently once a new species forms its own branch. The assumption was that when one species splits from another, the two new forms passively evolve away from each other, drifting solo on their evolutionary paths like two untethered buoys at sea. But Douglas doesn‚Äôt think this is how evolution always plays out in reality ‚Äî he thinks there can be a split-and-hit-the-gas dynamic.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhen one group or population splits into two, it‚Äôs like there‚Äôs often this magnetic propulsion that immediately drives them apart,‚Äù Douglas said. ‚ÄúThen afterwards they go through a kind of slow, independent evolution.‚Äù&lt;/p&gt;
    &lt;p&gt;The new model also incorporates past branching events that we can‚Äôt see today. If a lineage branches, but that branch is cut short ‚Äî sliced off millions of years ago when the lineage went extinct ‚Äî then it may not appear at all in modern data. Douglas and his team accounted for what he called ‚Äúphantom bursts‚Äù of evolution, termed ‚Äústubs‚Äù in their model. ‚ÄúEven though the branch is gone, it‚Äôs left behind a footprint,‚Äù Douglas said.&lt;/p&gt;
    &lt;p&gt;The approach builds on the insights of other evolutionary biologists, including Pagel, who in 2010 co-developed a method to account for lost branches of extinct species. Douglas‚Äôs team‚Äôs model is more general than previous approaches, Pagel said, allowing researchers to develop trees where the rate of evolution varies throughout. ‚ÄúThere‚Äôs a lot of little bits of this story that come together in a really nice way,‚Äù he said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Explanatory Power&lt;/head&gt;
    &lt;p&gt;Once the team developed their new model, they tested it on over a dozen evolutionary datasets across multiple fields of study.&lt;/p&gt;
    &lt;p&gt;When the researchers applied the model to their own research on aaRS enzymes they saw rapid changes accumulate around the branches of the evolutionary tree. When they compared their aaRS tree to others that assumed more gradual changes, they saw that the relationships between lineages were similar. However, the new model‚Äôs evolutionary spikes made the new trees 30% shorter with respect to gradual change, which suggested that less time had passed between the earliest ancestors and the tips of the branches, and that the enzymes had evolved more quickly.&lt;/p&gt;
    &lt;p&gt;Douglas and his colleagues also reanalyzed a dataset on cephalopod traits, such as the emergence of tentacles and evolution of body shapes, from 27 living species and 52 fossils. The results showed what the researchers describe as being a trivial contribution of gradual evolution toward the physical shape of cephalopods over some 500 million years, with 99% of the evolution occurring in spectacular bursts near the forking of branches.&lt;/p&gt;
    &lt;p&gt;Jaap Bleijenberg/Alamy&lt;/p&gt;
    &lt;p&gt;This sudden acceleration when lineages split ‚Äî termed ‚Äúsaltative branching‚Äù by Douglas and his colleagues ‚Äî isn‚Äôt limited to the evolution of living things. They found that it also applies to systems that living things create. The team turned their model loose on the series of treelike modifications and convolutions in the Indo-European family of languages. By accounting for early bursts in language evolution, the team developed an time estimate for the family‚Äôs origins in Eurasia.&lt;/p&gt;
    &lt;p&gt;The lesson from these results, Douglas said, is that punctuated equilibria are ‚Äúquite pervasive, quite general‚Äù across many different faces of evolution. ‚ÄúIt‚Äôs difficult to build up a solid understanding of evolution without accounting for this process,‚Äù he said. Saltative branching may be fundamental across biological and cultural evolution.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Test of Time&lt;/head&gt;
    &lt;p&gt;The study merges the long-standing, and often conflicting, perspectives of paleontologists and molecular biologists when it comes to the pacing of evolution. Paleontologists ‚Äî who primarily work with long-term morphological data from fossils ‚Äî more often encounter punctuated equilibria. ‚ÄúWhat paleontologists have been much less able to get at is the whole narrowing down that actually happens at speciation,‚Äù said Gene Hunt, an evolutionary paleontologist at the Smithsonian National Museum of Natural History who was not involved in the new research. The details surrounding species emergence are difficult to capture in the fossil record simply because the process happens so quickly, he said.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the phenomenon is less defined in genetic and molecular data, which tends to reveal more subtle, incremental differences as species diverge. ‚Äú[Molecular biologists] definitely expressed the perspective that Gould was wrong or he didn‚Äôt understand molecular data,‚Äù said April Wright, a statistical phylogeneticist at Southeastern Louisiana University who was not involved in the new research. ‚ÄúSo it‚Äôs definitely interesting to see this pattern [of abrupt evolution] getting measured in molecular data.‚Äù&lt;/p&gt;
    &lt;p&gt;What conditions could change the evolutionary tempo where trees fork? After spending time in new surroundings or experiencing new evolutionary pressures, two groups of organisms may split apart physically and quickly accumulate differences. The same may occur when humans and their cultures or languages become isolated from an initial, larger group.&lt;/p&gt;
    &lt;p&gt;‚ÄúMaybe they‚Äôre in a new environment. Maybe they‚Äôre just adapting different sets of cultural norms as they grow as a group,‚Äù Wright said. ‚ÄúThat would make a lot of sense, that you would see that same signature of change cluster at the nodes.‚Äù&lt;/p&gt;
    &lt;p&gt;These evolutionary bursts could also be equated with splitting or speciation events. Pagel describes species mostly being held in a kind of temporary stasis. Every so often, that stability is perturbed by environmental changes, and populations quickly evolve new ways to survive, occupying a different niche in their ecosystem.&lt;/p&gt;
    &lt;p&gt;The framework needs to be tested further. Just over a dozen studies were used in evaluating the new model, Hunt said. But there are probably hundreds of paleontological datasets alone that could be analyzed by these new tools, plus more on molecular evolution. ‚ÄúThere‚Äôs a huge amount of data sitting around that could be used for this,‚Äù he said. There‚Äôs no telling which of those tests could perturb this new saltative branching model into its next evolutionary spike.&lt;/p&gt;
    &lt;p&gt;Update: September 2, 2025&lt;lb/&gt; The story was updated to acknowledge that while, at the time of story publication, Jordan Douglas is employed by ANU Australia, the research was performed while he was based at the University of Auckland.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/the-sudden-surges-that-forge-evolutionary-trees-20250828/"/></entry><entry><id>https://news.ycombinator.com/item?id=45102186</id><title>RubyMine is now free for non-commercial use</title><updated>2025-09-02T16:12:39.277705+00:00</updated><content>&lt;doc fingerprint="c4b5fff47a7a3f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RubyMine Is Now Free for Non-Commercial Use&lt;/head&gt;
    &lt;p&gt;Hold on to your helper methods ‚Äì RubyMine is now FREE for non-commercial use! Whether you‚Äôre learning Ruby and Rails, pushing open-source forward, creating dev content, or building your passion project, we want to make sure you have the tools to enjoy what you do even more‚Ä¶ for free.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another chapter in the story&lt;/head&gt;
    &lt;p&gt;We recently introduced a new licensing model for WebStorm, RustRover, Rider, and CLion ‚Äì making them free for non-commercial use. RubyMine is now joining the party! For commercial use, our existing licensing model still applies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are we doing this?&lt;/head&gt;
    &lt;p&gt;We believe developers do their best work when the right tools are accessible. We‚Äôve been listening closely to the Ruby and Rails community ‚Äì their feedback, success, challenges, and passion for building with joy. Now, we‚Äôre making a change that reflects what we‚Äôve heard.&lt;/p&gt;
    &lt;p&gt;By making RubyMine free for non-commercial use, we hope to lower the barrier to starting and help more people write clean, confident Ruby code from day one. It‚Äôs our way of supporting the unique Ruby community ‚Äì from those who choose Ruby for their projects to maintainers of gems and frameworks who contribute to the Ruby ecosystem. Whether you‚Äôre debugging at midnight, crafting clever DSLs, or launching your first Rails app, RubyMine is here to help you build smarter (and crash less).&lt;/p&gt;
    &lt;head rend="h2"&gt;Commercial vs. non-commercial use&lt;/head&gt;
    &lt;p&gt;As defined in the Toolbox Subscription Agreement for Non-Commercial Use, commercial use means developing products and earning commercial benefits from your activities. However, certain categories are explicitly excluded from this definition. Common examples of non-commercial uses include learning and self-education, open-source contributions without earning commercial benefits, any form of content creation, and hobby development.&lt;/p&gt;
    &lt;p&gt;It‚Äôs important to note that, if you‚Äôre using a non-commercial license, you cannot opt out of the collection of anonymous usage statistics. We use this information to improve our products. The data we collect is exclusively that of anonymous feature usages of our IDEs. It is focused on what actions are performed and what types of functionality of the IDE are used. We do not collect any other data. This is similar to our Early Access Program (EAP) and is in compliance with our Privacy Policy.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Below are answers to the most common questions. Check out the full FAQ for more information.&lt;/p&gt;
    &lt;head rend="h3"&gt;Licensing&lt;/head&gt;
    &lt;head rend="h4"&gt;What features are included under the free license?&lt;/head&gt;
    &lt;p&gt;With the new non-commercial license type, you can enjoy a full-featured IDE that is identical to its paid version. The only difference is in the Code With Me feature ‚Äì you get Code With Me Community with your free license.&lt;/p&gt;
    &lt;head rend="h4"&gt;Which license should I choose if I want to use RubyMine for both non-commercial and commercial projects?&lt;/head&gt;
    &lt;p&gt;If you intend to use RubyMine for commercial development for which you will receive direct or indirect commercial advantage or monetary compensation within the meaning of the definitions provided in the Toolbox Subscription Agreement for Non-Commercial Use, you will need to purchase a commercial subscription (either individual or organizational). This license can then also be used for non-commercial development.&lt;/p&gt;
    &lt;head rend="h4"&gt;How do renewals and upgrades work now?&lt;/head&gt;
    &lt;p&gt;Non-commercial subscriptions are issued for one year and will automatically renew after that. However, for the renewal to happen, you must have used the assigned license at least once during the last 6 months of the subscription period. If it has been more than 6 months since you last used an IDE activated with this type of license and the renewal did not occur automatically, you can request a new non-commercial subscription again at any time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Am I eligible for a refund if I‚Äôve already bought a paid subscription but do non-commercial development?&lt;/head&gt;
    &lt;p&gt;If you‚Äôre unsure whether you qualify for a refund, you‚Äôll find full details of our policy here. Please note that if you also work on projects that qualify as commercial usage, you can‚Äôt use the free license for them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anonymous data collection&lt;/head&gt;
    &lt;head rend="h4"&gt;Does my IDE send any data to JetBrains?&lt;/head&gt;
    &lt;p&gt;The terms of the non-commercial agreement assume that the product may also electronically send JetBrains anonymized statistics (IDE telemetry) related to your usage of the product‚Äôs features. This information may include but is not limited to frameworks, file templates used in the product, actions invoked, and other interactions with the product‚Äôs features. This information does not contain personal data.&lt;/p&gt;
    &lt;head rend="h4"&gt;Is there a way to opt out of sending anonymized statistics?&lt;/head&gt;
    &lt;p&gt;We appreciate that this might not be convenient for everyone, but there is unfortunately no way to opt out of sending anonymized statistics to JetBrains under the terms of the Toolbox agreement for non-commercial use. The only way to opt out is by switching to either a paid subscription or one of the complimentary options mentioned here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Getting a non-commercial subscription&lt;/head&gt;
    &lt;head rend="h4"&gt;What should I do to apply for this subscription?&lt;/head&gt;
    &lt;p&gt;It can be easily done right inside your IDE:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install RubyMine and run it.&lt;/item&gt;
      &lt;item&gt;Upon startup, there will be a license dialog box where you can choose the Non-commercial use option.&lt;/item&gt;
      &lt;item&gt;Log in to your JetBrains account or create a new one.&lt;/item&gt;
      &lt;item&gt;Accept the Toolbox Subscription Agreement for Non-Commercial Use.&lt;/item&gt;
      &lt;item&gt;Enjoy development in your IDE.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you‚Äôve already started a trial period or have activated your IDE using a paid license, you still can switch to a non-commercial subscription by following these steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to Help | Register.&lt;/item&gt;
      &lt;item&gt;In the window that opens, click on the Deactivate License button.&lt;/item&gt;
      &lt;item&gt;Choose Non-commercial use.&lt;/item&gt;
      &lt;item&gt;Log in to your JetBrains account or create a new one.&lt;/item&gt;
      &lt;item&gt;Accept the Toolbox Subscription Agreement for Non-Commercial Use.&lt;/item&gt;
      &lt;item&gt;Enjoy development in your IDE.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;I don‚Äôt see the Non-commercial use option in my IDE. What should I do?&lt;/head&gt;
    &lt;p&gt;The most likely explanation for this is that you‚Äôre using an older version of RubyMine. Unfortunately, we don‚Äôt support obtaining the non-commercial license for any releases prior to RubyMine 2025.2.1.&lt;/p&gt;
    &lt;p&gt;That‚Äôs it for today! If you don‚Äôt find an answer to your question, feel free to leave a comment or contact us at sales@jetbrains.com.&lt;/p&gt;
    &lt;p&gt;The RubyMine team&lt;/p&gt;
    &lt;p&gt;JetBrains&lt;/p&gt;
    &lt;p&gt;Make it happen. With code.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jetbrains.com/ruby/2025/09/rubymine-is-now-free-for-non-commercial-use/"/></entry><entry><id>https://news.ycombinator.com/item?id=45102664</id><title>Show HN: Moribito ‚Äì A TUI for LDAP Viewing/Queries</title><updated>2025-09-02T16:12:38.687756+00:00</updated><content>&lt;doc fingerprint="3019ca5b5f87b9dc"&gt;
  &lt;main&gt;
    &lt;p&gt;A terminal-based LDAP server explorer built with Go and BubbleTea, providing an interactive interface for browsing LDAP directory trees, viewing records, and executing custom queries.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üå≤ Interactive Tree Navigation: Browse LDAP directory structure with keyboard/mouse&lt;/item&gt;
      &lt;item&gt;üìÑ Record Viewer: View detailed LDAP entry attributes&lt;/item&gt;
      &lt;item&gt;üìã Clipboard Integration: Copy attribute values to system clipboard&lt;/item&gt;
      &lt;item&gt;üîç Custom Query Interface: Execute custom LDAP queries with real-time results&lt;/item&gt;
      &lt;item&gt;üìñ Paginated Results: Efficient pagination for large result sets with automatic loading&lt;/item&gt;
      &lt;item&gt;‚öôÔ∏è Flexible Configuration: Support for config files and command-line options&lt;/item&gt;
      &lt;item&gt;üîê Secure Authentication: Support for SSL/TLS and various authentication methods&lt;/item&gt;
      &lt;item&gt;üîÑ Auto-Update Notifications: Optional checking for newer releases from GitHub&lt;/item&gt;
      &lt;item&gt;üé® Modern TUI: Clean, intuitive interface built with BubbleTea&lt;/item&gt;
      &lt;item&gt;üîÄ Multiple Connections: Save and switch between multiple LDAP server configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Initial startup screen with connection options&lt;/p&gt;
    &lt;p&gt;_Interface for adding new LDAP Connections&lt;/p&gt;
    &lt;p&gt;Browse LDAP directory structure with keyboard/mouse navigation&lt;/p&gt;
    &lt;p&gt;View detailed LDAP entry attributes with clipboard integration&lt;/p&gt;
    &lt;p&gt;Execute custom LDAP queries with real-time results and formatting&lt;/p&gt;
    &lt;code&gt;brew install ericschmar/tap/moribito&lt;/code&gt;
    &lt;code&gt;brew install https://raw.githubusercontent.com/ericschmar/moribito/main/homebrew/moribito.rb&lt;/code&gt;
    &lt;p&gt;Download the latest pre-built binary from GitHub Releases:&lt;/p&gt;
    &lt;p&gt;Linux/Unix:&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install.sh | bash&lt;/code&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install-macos.sh | bash&lt;/code&gt;
    &lt;p&gt;Windows (PowerShell):&lt;/p&gt;
    &lt;code&gt;irm https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install.ps1 | iex&lt;/code&gt;
    &lt;p&gt;The install scripts will:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download the appropriate binary for your platform&lt;/item&gt;
      &lt;item&gt;Install it to the system PATH&lt;/item&gt;
      &lt;item&gt;Create OS-specific configuration directories&lt;/item&gt;
      &lt;item&gt;Generate sample configuration files&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Linux x86_64
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-linux-amd64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# Linux ARM64
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-linux-arm64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# macOS Intel
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-darwin-amd64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# macOS Apple Silicon
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-darwin-arm64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/&lt;/code&gt;
    &lt;p&gt;For Windows, download &lt;code&gt;moribito-windows-amd64.exe&lt;/code&gt; from the releases page.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Homebrew is also available for Windows via WSL (Windows Subsystem for Linux). If you have WSL installed, you can use the Homebrew installation method above.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;git clone https://github.com/ericschmar/moribito
cd moribito
go build -o moribito cmd/moribito/main.go&lt;/code&gt;
    &lt;code&gt;# Connect with command line options
moribito -host ldap.example.com -base-dn "dc=example,dc=com" -user "cn=admin,dc=example,dc=com"

# Enable automatic update checking
moribito -check-updates -host ldap.example.com -base-dn "dc=example,dc=com"

# Use a configuration file
moribito -config /path/to/config.yaml

# Get help
moribito -help&lt;/code&gt;
    &lt;p&gt;Moribito will automatically look for configuration files in OS-specific locations:&lt;/p&gt;
    &lt;p&gt;Linux/Unix:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;~/.config/moribito/config.yaml&lt;/code&gt;(XDG config directory)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.moribito/config.yaml&lt;/code&gt;(user directory)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.moribito.yaml&lt;/code&gt;(user home file)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;~/.moribito/config.yaml&lt;/code&gt;(user directory)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/Library/Application Support/moribito/config.yaml&lt;/code&gt;(macOS standard)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.moribito.yaml&lt;/code&gt;(user home file)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Windows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;%APPDATA%\moribito\config.yaml&lt;/code&gt;(Windows standard)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;%USERPROFILE%\.moribito.yaml&lt;/code&gt;(user home file)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All platforms also check:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;./config.yaml&lt;/code&gt;(current directory)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use the built-in command to create a configuration file:&lt;/p&gt;
    &lt;code&gt;moribito --create-config&lt;/code&gt;
    &lt;p&gt;Or manually create a configuration file:&lt;/p&gt;
    &lt;code&gt;ldap:
    host: "ldap.example.com"
    port: 389
    base_dn: "dc=example,dc=com"
    use_ssl: false
    use_tls: false
    bind_user: "cn=admin,dc=example,dc=com"
    bind_pass: "your-password"
pagination:
    page_size: 50 # Number of entries per page
retry:
    enabled: true # Connection retries (default: true)
    max_attempts: 3 # Retry attempts (default: 3)
    initial_delay_ms: 500 # Initial delay (default: 500)
    max_delay_ms: 5000 # Max delay cap (default: 5000)&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tab - Switch between views (Tree ‚Üí Record ‚Üí Query ‚Üí Tree)&lt;/item&gt;
      &lt;item&gt;1/2/3 - Jump directly to Tree/Record/Query view&lt;/item&gt;
      &lt;item&gt;q - Quit application&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚Üë/‚Üì or k/j - Navigate up/down&lt;/item&gt;
      &lt;item&gt;Page Up/Down - Navigate by page&lt;/item&gt;
      &lt;item&gt;Home/End - Jump to top/bottom&lt;/item&gt;
      &lt;item&gt;‚Üí or l - Expand node (load children)&lt;/item&gt;
      &lt;item&gt;‚Üê or h - Collapse node&lt;/item&gt;
      &lt;item&gt;Enter - View record details&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚Üë/‚Üì or k/j - Scroll up/down&lt;/item&gt;
      &lt;item&gt;Page Up/Down - Scroll by page&lt;/item&gt;
      &lt;item&gt;Home/End - Jump to top/bottom&lt;/item&gt;
      &lt;item&gt;c - Copy current attribute value to clipboard&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;/ or Escape - Focus query input&lt;/item&gt;
      &lt;item&gt;Ctrl+Enter or Ctrl+J - Execute query&lt;/item&gt;
      &lt;item&gt;Ctrl+F - Format query with proper indentation&lt;/item&gt;
      &lt;item&gt;Escape - Clear query&lt;/item&gt;
      &lt;item&gt;Ctrl+V - Paste from clipboard&lt;/item&gt;
      &lt;item&gt;‚Üë/‚Üì - Navigate results (when not in input mode)&lt;/item&gt;
      &lt;item&gt;Page Up/Down - Navigate by page (automatically loads more results)&lt;/item&gt;
      &lt;item&gt;Enter - View selected record&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: The Query View uses automatic pagination to efficiently handle large result sets. When you scroll near the end of loaded results, the next page is automatically fetched from the LDAP server.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Ctrl+F key combination formats complex LDAP queries with proper indentation for better readability:&lt;/p&gt;
    &lt;code&gt;# Before formatting:
(&amp;amp;(objectClass=person)(|(cn=john*)(sn=smith*))(department=engineering))

# After formatting (Ctrl+F):
(&amp;amp;
  (objectClass=person)
  (|
    (cn=john*)
    (sn=smith*)
  )
  (department=engineering)
)
&lt;/code&gt;
    &lt;p&gt;The tool supports various LDAP authentication methods:&lt;/p&gt;
    &lt;code&gt;bind_user: "cn=admin,dc=example,dc=com"
bind_pass: "password"&lt;/code&gt;
    &lt;code&gt;bind_user: "uid=john,ou=users,dc=example,dc=com"
bind_pass: "password"&lt;/code&gt;
    &lt;code&gt;bind_user: "john@example.com"
bind_pass: "password"&lt;/code&gt;
    &lt;code&gt;# Leave bind_user and bind_pass empty or omit them&lt;/code&gt;
    &lt;code&gt;ldap:
    host: "ldaps.example.com"
    port: 636
    use_ssl: true&lt;/code&gt;
    &lt;code&gt;ldap:
    host: "ldap.example.com"
    port: 389
    use_tls: true&lt;/code&gt;
    &lt;p&gt;In the Query view, you can execute custom LDAP filters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;(objectClass=*)&lt;/code&gt;- All objects&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(objectClass=person)&lt;/code&gt;- All person objects&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(cn=john*)&lt;/code&gt;- Objects with cn starting with "john"&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(&amp;amp;(objectClass=person)(mail=*@example.com))&lt;/code&gt;- People with example.com emails&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(|(cn=admin)(uid=admin))&lt;/code&gt;- Objects with cn=admin OR uid=admin&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For complex nested queries, use Ctrl+F to automatically format them for better readability:&lt;/p&gt;
    &lt;p&gt;Simple queries remain unchanged:&lt;/p&gt;
    &lt;code&gt;(objectClass=person)
&lt;/code&gt;
    &lt;p&gt;Complex queries are formatted with proper indentation:&lt;/p&gt;
    &lt;code&gt;# Original
(&amp;amp;(objectClass=person)(|(cn=john*)(sn=smith*))(department=engineering))

# After Ctrl+F
(&amp;amp;
  (objectClass=person)
  (|
    (cn=john*)
    (sn=smith*)
  )
  (department=engineering)
)
&lt;/code&gt;
    &lt;p&gt;LDAP CLI uses intelligent pagination to provide optimal performance when working with large directories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default Page Size: 50 entries per page&lt;/item&gt;
      &lt;item&gt;Configurable: Adjust via config file or &lt;code&gt;--page-size&lt;/code&gt;flag&lt;/item&gt;
      &lt;item&gt;On-Demand Loading: Next pages load automatically as you scroll&lt;/item&gt;
      &lt;item&gt;Memory Efficient: Only loaded entries are kept in memory&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Command line override
moribito --page-size 100 --host ldap.example.com

# Configuration file
pagination:
  page_size: 25  # Smaller pages for slower networks&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smaller page sizes (10-25) for slower networks or limited LDAP servers&lt;/item&gt;
      &lt;item&gt;Larger page sizes (100-200) for fast networks and powerful LDAP servers&lt;/item&gt;
      &lt;item&gt;Use specific queries to reduce result sets instead of browsing all entries&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;LDAP CLI includes automatic retry functionality to handle connection failures gracefully:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default: Enabled with 3 retry attempts&lt;/item&gt;
      &lt;item&gt;Exponential Backoff: Delay doubles between attempts (500ms ‚Üí 1s ‚Üí 2s ‚Üí ...)&lt;/item&gt;
      &lt;item&gt;Connection Recovery: Automatically re-establishes broken connections&lt;/item&gt;
      &lt;item&gt;Smart Detection: Only retries connection-related errors, not authentication failures&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Default retry settings (automatically applied)
# No configuration needed - retries work out of the box&lt;/code&gt;
    &lt;code&gt;# Custom retry configuration
retry:
    enabled: true
    max_attempts: 5 # Maximum retry attempts (default: 3)
    initial_delay_ms: 1000 # Initial delay in milliseconds (default: 500)
    max_delay_ms: 10000 # Maximum delay cap (default: 5000)&lt;/code&gt;
    &lt;code&gt;# Disable retries if needed
retry:
    enabled: false&lt;/code&gt;
    &lt;p&gt;The system automatically retries for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Network timeouts and connection drops&lt;/item&gt;
      &lt;item&gt;Connection refused errors&lt;/item&gt;
      &lt;item&gt;Server unavailable responses&lt;/item&gt;
      &lt;item&gt;Connection reset by peer&lt;/item&gt;
      &lt;item&gt;LDAP server down errors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Authentication errors, invalid queries, and permission issues are not retried.&lt;/p&gt;
    &lt;code&gt;# Build for current platform
make build

# Build for all platforms
make build-all

# Clean build artifacts
make clean&lt;/code&gt;
    &lt;code&gt;# Format code
make fmt

# Run linter
make lint

# Run tests
make test

# Run all CI checks (format, lint, test, build)
make ci&lt;/code&gt;
    &lt;code&gt;go test ./...&lt;/code&gt;
    &lt;p&gt;This project uses GitHub Actions for CI/CD:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;CI Workflow: Runs on every push and pull request to&lt;/p&gt;&lt;code&gt;main&lt;/code&gt;and&lt;code&gt;develop&lt;/code&gt;branches&lt;list rend="ul"&gt;&lt;item&gt;Code formatting verification&lt;/item&gt;&lt;item&gt;Linting (with warnings)&lt;/item&gt;&lt;item&gt;Testing&lt;/item&gt;&lt;item&gt;Building for current platform&lt;/item&gt;&lt;item&gt;Multi-platform build artifacts (on main branch pushes)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Release Workflow: Triggered by version tags (e.g.,&lt;/p&gt;&lt;code&gt;v1.0.0&lt;/code&gt;)&lt;list rend="ul"&gt;&lt;item&gt;Runs full CI checks&lt;/item&gt;&lt;item&gt;Builds for all platforms (Linux amd64/arm64, macOS amd64/arm64, Windows amd64)&lt;/item&gt;&lt;item&gt;Creates GitHub releases with binaries and checksums&lt;/item&gt;&lt;item&gt;Generates installation instructions&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BubbleTea - TUI framework&lt;/item&gt;
      &lt;item&gt;Lipgloss - Styling&lt;/item&gt;
      &lt;item&gt;go-ldap - LDAP client&lt;/item&gt;
      &lt;item&gt;golang.org/x/term - Terminal utilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project includes full Homebrew support for easy installation on macOS and Linux. See the homebrew/ directory for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ready-to-use Homebrew formula&lt;/item&gt;
      &lt;item&gt;Formula generation and maintenance scripts&lt;/item&gt;
      &lt;item&gt;Documentation for creating custom taps&lt;/item&gt;
      &lt;item&gt;Instructions for submitting to homebrew-core&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project follows Semantic Versioning. See docs/versioning.md for details on the release process.&lt;/p&gt;
    &lt;p&gt;Comprehensive documentation is available using DocPress. To build and view the documentation:&lt;/p&gt;
    &lt;code&gt;# Build static documentation website
make docs

# Serve documentation locally with live reload
make docs-serve&lt;/code&gt;
    &lt;p&gt;The documentation covers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Installation and setup&lt;/item&gt;
      &lt;item&gt;Usage guide with examples&lt;/item&gt;
      &lt;item&gt;Interface navigation&lt;/item&gt;
      &lt;item&gt;Development setup&lt;/item&gt;
      &lt;item&gt;Contributing guidelines&lt;/item&gt;
      &lt;item&gt;API reference and advanced features&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Visit the generated documentation site for the complete guide.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create your feature branch (&lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some amazing feature'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Push to the branch (&lt;code&gt;git push origin feature/amazing-feature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ericschmar/moribito"/></entry><entry><id>https://news.ycombinator.com/item?id=45103436</id><title>The Little Book of Linear Algebra</title><updated>2025-09-02T16:12:37.681958+00:00</updated><content>&lt;doc fingerprint="b70d9b7e132f9799"&gt;
  &lt;main&gt;
    &lt;p&gt;A concise, beginner-friendly introduction to the core ideas of linear algebra.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download PDF ‚Äì print-ready version&lt;/item&gt;
      &lt;item&gt;Download EPUB ‚Äì e-reader friendly&lt;/item&gt;
      &lt;item&gt;View LaTeX ‚Äì Latex source&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A scalar is a single numerical quantity, most often taken from the real numbers, denoted by &lt;/p&gt;
    &lt;p&gt;An element of &lt;/p&gt;
    &lt;p&gt;Example 1.1.1.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A 2-dimensional vector: &lt;math-renderer&gt;$(3, -1) \in \mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;A 3-dimensional vector: &lt;math-renderer&gt;$(2, 0, 5) \in \mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;A 1-dimensional vector: &lt;math-renderer&gt;$(7) \in \mathbb{R}^1$&lt;/math-renderer&gt;, which corresponds to the scalar&lt;math-renderer&gt;$7$&lt;/math-renderer&gt;itself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vectors are often written vertically in column form, which emphasizes their role in matrix multiplication:&lt;/p&gt;
    &lt;p&gt;The vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.&lt;/p&gt;
    &lt;p&gt;In &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As a point in space, described by its coordinates.&lt;/item&gt;
      &lt;item&gt;As a displacement or arrow, described by a direction and a length.&lt;/item&gt;
      &lt;item&gt;As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vectors are written in boldface lowercase letters: &lt;math-renderer&gt;$\mathbf{v}, \mathbf{w}, \mathbf{x}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The i-th entry of a vector &lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;is written&lt;math-renderer&gt;$v_i$&lt;/math-renderer&gt;, where indices begin at 1.&lt;/item&gt;
      &lt;item&gt;The set of all n-dimensional vectors over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;is denoted&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Column vectors will be the default form unless otherwise stated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Scalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear transformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once vectors are understood, we can define operations such as addition and scalar multiplication, then generalize to subspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with powerful applications to geometry, computation, and data.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write three different vectors in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;and sketch them as arrows from the origin. Identify their coordinates explicitly.&lt;/item&gt;
      &lt;item&gt;Give an example of a vector in &lt;math-renderer&gt;$\mathbb{R}^4$&lt;/math-renderer&gt;. Can you visualize it directly? Explain why high-dimensional visualization is challenging.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$\mathbf{v} = (4, -3, 2)$&lt;/math-renderer&gt;. Write&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;in column form and state&lt;math-renderer&gt;$v_1, v_2, v_3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In what sense is the set &lt;math-renderer&gt;$\mathbb{R}^1$&lt;/math-renderer&gt;both a line and a vector space? Illustrate with examples.&lt;/item&gt;
      &lt;item&gt;Consider the vector &lt;math-renderer&gt;$\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$&lt;/math-renderer&gt;. What is special about this vector when&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;is large? What might it represent in applications?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two fundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations satisfy simple but far-reaching rules that underpin the entire subject.&lt;/p&gt;
    &lt;p&gt;Given two vectors of the same dimension, their sum is obtained by adding corresponding entries. Formally, if&lt;/p&gt;
    &lt;p&gt;then their sum is&lt;/p&gt;
    &lt;p&gt;Example 1.2.1. Let &lt;/p&gt;
    &lt;p&gt;Geometrically, vector addition corresponds to the parallelogram rule. If we draw both vectors as arrows from the origin, then placing the tail of one vector at the head of the other produces the sum. The diagonal of the parallelogram they form represents the resulting vector.&lt;/p&gt;
    &lt;p&gt;Multiplying a vector by a scalar stretches or shrinks the vector while preserving its direction, unless the scalar is negative, in which case the vector is also reversed. If &lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Example 1.2.2. Let &lt;/p&gt;
    &lt;p&gt;This corresponds to flipping the vector through the origin and doubling its length.&lt;/p&gt;
    &lt;p&gt;The interaction of addition and scalar multiplication allows us to form linear combinations. A linear combination of vectors &lt;/p&gt;
    &lt;p&gt;Linear combinations are the mechanism by which we generate new vectors from existing ones. The span of a set of vectors-the collection of all their linear combinations-will later lead us to the idea of a subspace.&lt;/p&gt;
    &lt;p&gt;Example 1.2.3. Let &lt;/p&gt;
    &lt;p&gt;Thus &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Addition: &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v}$&lt;/math-renderer&gt;means component-wise addition.&lt;/item&gt;
      &lt;item&gt;Scalar multiplication: &lt;math-renderer&gt;$c\mathbf{v}$&lt;/math-renderer&gt;scales each entry of&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;by&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Linear combination: a sum of the form &lt;math-renderer&gt;$c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vector addition and scalar multiplication are the defining operations of linear algebra. They give structure to vector spaces, allow us to describe geometric phenomena like translation and scaling, and provide the foundation for solving systems of equations. Everything that follows-basis, dimension, transformations-builds on these simple but profound rules.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v}$&lt;/math-renderer&gt;where&lt;math-renderer&gt;$\mathbf{u} = (1,2,3)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v} = (4, -1, 0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find &lt;math-renderer&gt;$3\mathbf{v}$&lt;/math-renderer&gt;where&lt;math-renderer&gt;$\mathbf{v} = (-2,5)$&lt;/math-renderer&gt;. Sketch both vectors to illustrate the scaling.&lt;/item&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$(5,7)$&lt;/math-renderer&gt;can be written as a linear combination of&lt;math-renderer&gt;$(1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Write &lt;math-renderer&gt;$(4,4)$&lt;/math-renderer&gt;as a linear combination of&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$&lt;/math-renderer&gt;, then&lt;math-renderer&gt;$(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$&lt;/math-renderer&gt;for scalars&lt;math-renderer&gt;$c,d \in \mathbb{R}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The dot product is the fundamental operation that links algebra and geometry in vector spaces. It allows us to measure lengths, compute angles, and determine orthogonality. From this single definition flow the notions of norm and angle, which give geometry to abstract vector spaces.&lt;/p&gt;
    &lt;p&gt;For two vectors in &lt;/p&gt;
    &lt;p&gt;Equivalently, in matrix notation:&lt;/p&gt;
    &lt;p&gt;Example 1.3.1. Let &lt;/p&gt;
    &lt;p&gt;The dot product outputs a single scalar, not another vector.&lt;/p&gt;
    &lt;p&gt;The Euclidean norm of a vector is the square root of its dot product with itself:&lt;/p&gt;
    &lt;p&gt;This generalizes the Pythagorean theorem to arbitrary dimensions.&lt;/p&gt;
    &lt;p&gt;Example 1.3.2. For &lt;/p&gt;
    &lt;p&gt;This is exactly the length of the vector as an arrow in the plane.&lt;/p&gt;
    &lt;p&gt;The dot product also encodes the angle between two vectors. For nonzero vectors &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Example 1.3.3. Let &lt;/p&gt;
    &lt;p&gt;Hence&lt;/p&gt;
    &lt;p&gt;The vectors are perpendicular.&lt;/p&gt;
    &lt;p&gt;Two vectors are said to be orthogonal if their dot product is zero:&lt;/p&gt;
    &lt;p&gt;Orthogonality generalizes the idea of perpendicularity from geometry to higher dimensions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dot product: &lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Norm (length): &lt;math-renderer&gt;$|\mathbf{v}|$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Orthogonality: &lt;math-renderer&gt;$\mathbf{u} \perp \mathbf{v}$&lt;/math-renderer&gt;if&lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v} = 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The dot product turns vector spaces into geometric objects: vectors gain lengths, angles, and notions of perpendicularity. This foundation will later support the study of orthogonal projections, Gram‚ÄìSchmidt orthogonalization, eigenvectors, and least squares problems.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v}$&lt;/math-renderer&gt;for&lt;math-renderer&gt;$\mathbf{u} = (1,2,3)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$\mathbf{v} = (4,5,6)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the norm of &lt;math-renderer&gt;$\mathbf{v} = (2, -2, 1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine whether &lt;math-renderer&gt;$\mathbf{u} = (1,1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v} = (1,-1,2)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$\mathbf{u} = (3,4)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$\mathbf{v} = (4,3)$&lt;/math-renderer&gt;. Compute the angle between them.&lt;/item&gt;
      &lt;item&gt;Prove that &lt;math-renderer&gt;$|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$&lt;/math-renderer&gt;. This identity is the algebraic version of the Law of Cosines.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonality captures the notion of perpendicularity in vector spaces. It is one of the most important geometric ideas in linear algebra, allowing us to decompose vectors, define projections, and construct special bases with elegant properties.&lt;/p&gt;
    &lt;p&gt;Two vectors &lt;/p&gt;
    &lt;p&gt;This condition ensures that the angle between them is &lt;/p&gt;
    &lt;p&gt;Example 1.4.1. In &lt;/p&gt;
    &lt;p&gt;A collection of vectors is called orthogonal if every distinct pair of vectors in the set is orthogonal. If, in addition, each vector has norm 1, the set is called orthonormal.&lt;/p&gt;
    &lt;p&gt;Example 1.4.2. In &lt;/p&gt;
    &lt;p&gt;form an orthonormal set: each has length 1, and their dot products vanish when the indices differ.&lt;/p&gt;
    &lt;p&gt;Orthogonality makes possible the decomposition of a vector into two components: one parallel to another vector, and one orthogonal to it. Given a nonzero vector &lt;/p&gt;
    &lt;p&gt;The difference&lt;/p&gt;
    &lt;p&gt;is orthogonal to &lt;/p&gt;
    &lt;p&gt;Example 1.4.3. Let &lt;/p&gt;
    &lt;p&gt;Thus&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;In general, if &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{v} = \text{proj}{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} - \text{proj}{\mathbf{u}}(\mathbf{v})\big), $$&lt;/p&gt;
    &lt;p&gt;where the first term is parallel to &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{u} \perp \mathbf{v}$&lt;/math-renderer&gt;: vectors&lt;math-renderer&gt;$\mathbf{u}$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;An orthogonal set: vectors pairwise orthogonal.&lt;/item&gt;
      &lt;item&gt;An orthonormal set: pairwise orthogonal, each of norm 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonality gives structure to vector spaces. It provides a way to separate independent directions cleanly, simplify computations, and minimize errors in approximations. Many powerful algorithms in numerical linear algebra and data science (QR decomposition, least squares regression, PCA) rely on orthogonality.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that the vectors &lt;math-renderer&gt;$(1,2,2)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(2,0,-1)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Find the projection of &lt;math-renderer&gt;$(3,4)$&lt;/math-renderer&gt;onto&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that any two distinct standard basis vectors in &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Decompose &lt;math-renderer&gt;$(5,2)$&lt;/math-renderer&gt;into components parallel and orthogonal to&lt;math-renderer&gt;$(2,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v}$&lt;/math-renderer&gt;are orthogonal and nonzero, then&lt;math-renderer&gt;$(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v}) = 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrices are the central objects of linear algebra, providing a compact way to represent and manipulate linear transformations, systems of equations, and structured data. A matrix is a rectangular array of numbers arranged in rows and columns.&lt;/p&gt;
    &lt;p&gt;An &lt;/p&gt;
    &lt;p&gt;Each entry &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$m = n$&lt;/math-renderer&gt;, the matrix is square.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$m = 1$&lt;/math-renderer&gt;, the matrix is a row vector.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$n = 1$&lt;/math-renderer&gt;, the matrix is a column vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, vectors are simply special cases of matrices.&lt;/p&gt;
    &lt;p&gt;Example 2.1.1. A &lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Example 2.1.2. A &lt;/p&gt;
    &lt;p&gt;This will later serve as the representation of a linear transformation on &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrices are denoted by uppercase bold letters: &lt;math-renderer&gt;$A, B, C$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Entries are written as &lt;math-renderer&gt;$a_{ij}$&lt;/math-renderer&gt;, with the row index first, column index second.&lt;/item&gt;
      &lt;item&gt;The set of all real &lt;math-renderer&gt;$m \times n$&lt;/math-renderer&gt;matrices is denoted&lt;math-renderer&gt;$\mathbb{R}^{m \times n}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, a matrix is a function &lt;/p&gt;
    &lt;p&gt;Matrices generalize vectors and give us a language for describing linear operations systematically. They encode systems of equations, rotations, projections, and transformations of data. With matrices, algebra and geometry come together: a single compact object can represent both numerical data and functional rules.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write a &lt;math-renderer&gt;$3 \times 2$&lt;/math-renderer&gt;matrix of your choice and identify its entries&lt;math-renderer&gt;$a_{ij}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Is every vector a matrix? Is every matrix a vector? Explain.&lt;/item&gt;
      &lt;item&gt;Which of the following are square matrices: &lt;math-renderer&gt;$A \in \mathbb{R}^{4\times4}$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$B \in \mathbb{R}^{3\times5}$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$C \in \mathbb{R}^{1\times1}$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Let $D = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 1 \end{bmatrix}$. What kind of matrix is this?&lt;/item&gt;
      &lt;item&gt;Consider the matrix $E = \begin{bmatrix} a &amp;amp; b \ c &amp;amp; d \end{bmatrix}$. Express &lt;math-renderer&gt;$e_{11}, e_{12}, e_{21}, e_{22}$&lt;/math-renderer&gt;explicitly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once matrices are defined, the next step is to understand how they combine. Just as vectors gain meaning through addition and scalar multiplication, matrices become powerful through two operations: addition and multiplication.&lt;/p&gt;
    &lt;p&gt;Two matrices of the same size are added by adding corresponding entries. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Example 2.2.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} 0 &amp;amp; 2 \ 8 &amp;amp; 6 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Matrix addition is commutative (&lt;/p&gt;
    &lt;p&gt;For a scalar &lt;/p&gt;
    &lt;p&gt;This stretches or shrinks all entries of the matrix uniformly.&lt;/p&gt;
    &lt;p&gt;Example 2.2.2. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;The defining operation of matrices is multiplication. If&lt;/p&gt;
    &lt;p&gt;then their product is the &lt;/p&gt;
    &lt;p&gt;Thus, the entry in the &lt;/p&gt;
    &lt;p&gt;Example 2.2.3. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;head rend="h1"&gt;$$ AB = \begin{bmatrix} 1\cdot4 + 2\cdot2 &amp;amp; 1\cdot(-1) + 2\cdot5 \ 0\cdot4 + 3\cdot2 &amp;amp; 0\cdot(-1) + 3\cdot5 \end{bmatrix}&lt;/head&gt;
    &lt;p&gt;\begin{bmatrix} 8 &amp;amp; 9 \ 6 &amp;amp; 15 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Notice that matrix multiplication is not commutative in general: &lt;/p&gt;
    &lt;p&gt;Matrix multiplication corresponds to the composition of linear transformations. If &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrix sum: &lt;math-renderer&gt;$A+B$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Scalar multiple: &lt;math-renderer&gt;$cA$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Product: &lt;math-renderer&gt;$AB$&lt;/math-renderer&gt;, defined only when the number of columns of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;equals the number of rows of&lt;math-renderer&gt;$B$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrix multiplication is the core mechanism of linear algebra: it encodes how transformations combine, how systems of equations are solved, and how data flows in modern algorithms. Addition and scalar multiplication make matrices into a vector space, while multiplication gives them an algebraic structure rich enough to model geometry, computation, and networks.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$A+B$&lt;/math-renderer&gt;for&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find &lt;math-renderer&gt;$3A$&lt;/math-renderer&gt;where&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Multiply&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify with an explicit example that &lt;math-renderer&gt;$AB \neq BA$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that matrix multiplication is distributive: &lt;math-renderer&gt;$A(B+C) = AB + AC$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Two special operations on matrices-the transpose and the inverse-give rise to deep algebraic and geometric properties. The transpose rearranges a matrix by flipping it across its main diagonal, while the inverse, when it exists, acts as the undo operation for matrix multiplication.&lt;/p&gt;
    &lt;p&gt;The transpose of an &lt;/p&gt;
    &lt;p&gt;Formally,&lt;/p&gt;
    &lt;p&gt;$$ (A^T){ij} = a{ji}. $$&lt;/p&gt;
    &lt;p&gt;Example 2.3.1. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Properties of the Transpose.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ (A^T)^T = A$.&lt;/item&gt;
      &lt;item&gt;$ (A+B)^T = A^T + B^T$.&lt;/item&gt;
      &lt;item&gt;$ (cA)^T = cA^T$, for scalar &lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;$ (AB)^T = B^T A^T$.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The last rule is crucial: the order reverses.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Not every matrix is invertible. A necessary condition is that &lt;/p&gt;
    &lt;p&gt;Example 2.3.2. Let&lt;/p&gt;
    &lt;p&gt;Its determinant is &lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} -2 &amp;amp; 1 \ 1.5 &amp;amp; -0.5 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Verification:&lt;/p&gt;
    &lt;head rend="h1"&gt;$$ AA^{-1} = \begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; 4 \end{bmatrix} \begin{bmatrix} -2 &amp;amp; 1 \ 1.5 &amp;amp; -0.5 \end{bmatrix}&lt;/head&gt;
    &lt;p&gt;\begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 1 \end{bmatrix}. $$&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The transpose corresponds to reflecting a linear transformation across the diagonal. For vectors, it switches between row and column forms.&lt;/item&gt;
      &lt;item&gt;The inverse, when it exists, corresponds to reversing a linear transformation. For example, if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;scales and rotates vectors,&lt;math-renderer&gt;$A^{-1}$&lt;/math-renderer&gt;rescales and rotates them back.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transpose: &lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Inverse: &lt;math-renderer&gt;$A^{-1}$&lt;/math-renderer&gt;, defined only for invertible square matrices.&lt;/item&gt;
      &lt;item&gt;Identity: &lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;, acts as the multiplicative identity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The transpose allows us to define symmetric and orthogonal matrices, central to geometry and numerical methods. The inverse underlies the solution of linear systems, encoding the idea of undoing a transformation. Together, these operations set the stage for determinants, eigenvalues, and orthogonalization.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the transpose of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$(AB)^T = B^T A^T$&lt;/math-renderer&gt;for&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine whether&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is invertible. If so, find &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the inverse of&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and explain its geometric action on vectors in the plane.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible, then so is&lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;, and&lt;math-renderer&gt;$(A^T)^{-1} = (A^{-1})^T$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Certain matrices occur so frequently in theory and applications that they are given special names. Recognizing their properties allows us to simplify computations and understand the structure of linear transformations more clearly.&lt;/p&gt;
    &lt;p&gt;The identity matrix &lt;/p&gt;
    &lt;p&gt;It acts as the multiplicative identity:&lt;/p&gt;
    &lt;p&gt;Geometrically, &lt;/p&gt;
    &lt;p&gt;A diagonal matrix has all off-diagonal entries zero:&lt;/p&gt;
    &lt;p&gt;Multiplication by a diagonal matrix scales each coordinate independently:&lt;/p&gt;
    &lt;p&gt;Example 2.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;A permutation matrix is obtained by permuting the rows of the identity matrix. Multiplying a vector by a permutation matrix reorders its coordinates.&lt;/p&gt;
    &lt;p&gt;Example 2.4.2. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;p&gt;Permutation matrices are always invertible; their inverses are simply their transposes.&lt;/p&gt;
    &lt;p&gt;A matrix is symmetric if&lt;/p&gt;
    &lt;p&gt;and skew-symmetric if&lt;/p&gt;
    &lt;p&gt;Symmetric matrices appear in quadratic forms and optimization, while skew-symmetric matrices describe rotations and cross products in geometry.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;Equivalently, the rows (and columns) of &lt;/p&gt;
    &lt;p&gt;Example 2.4.3. The rotation matrix in the plane:&lt;/p&gt;
    &lt;p&gt;is orthogonal, since&lt;/p&gt;
    &lt;p&gt;Special matrices serve as the building blocks of linear algebra. Identity matrices define the neutral element, diagonal matrices simplify computations, permutation matrices reorder data, symmetric and orthogonal matrices describe fundamental geometric structures. Much of modern applied mathematics reduces complex problems to operations involving these simple forms.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that the product of two diagonal matrices is diagonal, and compute an example.&lt;/item&gt;
      &lt;item&gt;Find the permutation matrix that cycles &lt;math-renderer&gt;$(a,b,c)$&lt;/math-renderer&gt;into&lt;math-renderer&gt;$(b,c,a)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that every permutation matrix is invertible and its inverse is its transpose.&lt;/item&gt;
      &lt;item&gt;Verify that&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is orthogonal. What geometric transformation does it represent? 5. Determine whether&lt;/p&gt;
    &lt;p&gt;are symmetric, skew-symmetric, or neither.&lt;/p&gt;
    &lt;p&gt;One of the central motivations for linear algebra is solving systems of linear equations. These systems arise naturally in science, engineering, and data analysis whenever multiple constraints interact. Matrices provide a compact language for expressing and solving them.&lt;/p&gt;
    &lt;p&gt;A linear system consists of equations where each unknown appears only to the first power and with no products between variables. A general system of &lt;/p&gt;
    &lt;p&gt;Here the coefficients &lt;/p&gt;
    &lt;p&gt;The system can be expressed compactly as:&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$A \in \mathbb{R}^{m \times n}$&lt;/math-renderer&gt;is the coefficient matrix&lt;math-renderer&gt;$[a_{ij}]$&lt;/math-renderer&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{x} \in \mathbb{R}^n$&lt;/math-renderer&gt;is the column vector of unknowns,&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{b} \in \mathbb{R}^m$&lt;/math-renderer&gt;is the column vector of constants.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This formulation turns the problem of solving equations into analyzing the action of a matrix.&lt;/p&gt;
    &lt;p&gt;Example 3.1.1. The system&lt;/p&gt;
    &lt;p&gt;can be written as&lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} 5 \ 4 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;A linear system may have:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;No solution (inconsistent): The equations conflict. Example: $ \begin{cases} x + y = 1 \ x + y = 2 \end{cases} $ has no solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Exactly one solution (unique): The system‚Äôs equations intersect at a single point. Example: The above system with coefficient matrix $ \begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; -1 \end{bmatrix} $ has a unique solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Infinitely many solutions: The equations describe overlapping constraints (e.g., multiple equations representing the same line or plane).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The nature of the solution depends on the rank of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;, each linear equation represents a line. Solving a system means finding intersection points of lines.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, each equation represents a plane. A system may have no solution (parallel planes), one solution (a unique intersection point), or infinitely many (a line of intersection).&lt;/item&gt;
      &lt;item&gt;In higher dimensions, the picture generalizes: solutions form intersections of hyperplanes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear systems are the practical foundation of linear algebra. They appear in balancing chemical reactions, circuit analysis, least-squares regression, optimization, and computer graphics. Understanding how to represent and classify their solutions is the first step toward systematic solution methods like Gaussian elimination.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Write the following system in matrix form: $ \begin{cases} 2x + 3y - z = 7, \ x - y + 4z = 1, \ 3x + 2y + z = 5 \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether the system $ \begin{cases} x + y = 1, \ 2x + 2y = 2 \end{cases} $ has no solution, one solution, or infinitely many solutions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Geometrically interpret the system $ \begin{cases} x + y = 3, \ x - y = 1 \end{cases} $ in the plane.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solve the system $ \begin{cases} 2x + y = 1, \ x - y = 4 \end{cases} $ and check your solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, describe the solution set of $ \begin{cases} x + y + z = 0, \ 2x + 2y + 2z = 0 \end{cases} $. What geometric object does it represent?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To solve linear systems efficiently, we use Gaussian elimination: a systematic method of transforming a system into a simpler equivalent one whose solutions are easier to see. The method relies on elementary row operations that preserve the solution set.&lt;/p&gt;
    &lt;p&gt;On an augmented matrix &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row swapping: interchange two rows.&lt;/item&gt;
      &lt;item&gt;Row scaling: multiply a row by a nonzero scalar.&lt;/item&gt;
      &lt;item&gt;Row replacement: replace one row by itself plus a multiple of another row.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations correspond to re-expressing equations in different but equivalent forms.&lt;/p&gt;
    &lt;p&gt;A matrix is in row echelon form (REF) if:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;All nonzero rows are above any zero rows.&lt;/item&gt;
      &lt;item&gt;Each leading entry (the first nonzero number from the left in a row) is to the right of the leading entry in the row above.&lt;/item&gt;
      &lt;item&gt;All entries below a leading entry are zero.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Further, if each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced row echelon form (RREF).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the augmented matrix for the system.&lt;/item&gt;
      &lt;item&gt;Use row operations to create zeros below each pivot (the leading entry in a row).&lt;/item&gt;
      &lt;item&gt;Continue column by column until the matrix is in echelon form.&lt;/item&gt;
      &lt;item&gt;Solve by back substitution: starting from the last pivot equation and working upward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we continue to RREF, the solution can be read off directly.&lt;/p&gt;
    &lt;p&gt;Example 3.2.1. Solve&lt;/p&gt;
    &lt;p&gt;Step 1. Augmented matrix&lt;/p&gt;
    &lt;p&gt;Step 2. Eliminate below the first pivot&lt;/p&gt;
    &lt;p&gt;Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3:&lt;/p&gt;
    &lt;p&gt;Step 3. Pivot in column 2&lt;/p&gt;
    &lt;p&gt;Divide row 2 by -3:&lt;/p&gt;
    &lt;p&gt;Add 7 times row 2 to row 3:&lt;/p&gt;
    &lt;p&gt;Step 4. Pivot in column 3&lt;/p&gt;
    &lt;p&gt;Divide row 3 by -2:&lt;/p&gt;
    &lt;p&gt;Step 5. Back substitution&lt;/p&gt;
    &lt;p&gt;From the last row: $ z = \tfrac{11}{3}. $&lt;/p&gt;
    &lt;p&gt;Second row: $ y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} = \tfrac{10}{3}. $&lt;/p&gt;
    &lt;p&gt;First row: $ x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3. $&lt;/p&gt;
    &lt;p&gt;So $ x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0. $&lt;/p&gt;
    &lt;p&gt;Solution: $ (x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big). $&lt;/p&gt;
    &lt;p&gt;Gaussian elimination is the foundation of computational linear algebra. It reduces complex systems to a form where solutions are visible, and it forms the basis for algorithms used in numerical analysis, scientific computing, and machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve by Gaussian elimination: $ \begin{cases} x + y = 2, \ 2x - y = 0. \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reduce the following augmented matrix to REF: $ \left[\begin{array}{ccc|c} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 6 \ 2 &amp;amp; -1 &amp;amp; 3 &amp;amp; 14 \ 1 &amp;amp; 4 &amp;amp; -2 &amp;amp; -2 \end{array}\right]. $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that Gaussian elimination always produces either:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;a unique solution,&lt;/item&gt;
          &lt;item&gt;infinitely many solutions, or&lt;/item&gt;
          &lt;item&gt;a contradiction (no solution).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use Gaussian elimination to find all solutions of $ \begin{cases} x + y + z = 0, \ 2x + y + z = 1. \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain why pivoting (choosing the largest available pivot element) is useful in numerical computation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Gaussian elimination not only provides solutions but also reveals the structure of a linear system. Two key ideas are the rank of a matrix and the consistency of a system. Rank measures the amount of independent information in the equations, while consistency determines whether the system has at least one solution.&lt;/p&gt;
    &lt;p&gt;The rank of a matrix is the number of leading pivots in its row echelon form. Equivalently, it is the maximum number of linearly independent rows or columns.&lt;/p&gt;
    &lt;p&gt;Formally,&lt;/p&gt;
    &lt;p&gt;The rank tells us the effective dimension of the space spanned by the rows (or columns).&lt;/p&gt;
    &lt;p&gt;Example 3.3.1. For&lt;/p&gt;
    &lt;p&gt;row reduction gives&lt;/p&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;p&gt;Consider the system &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = \text{rank}(A|\mathbf{b}) = n$&lt;/math-renderer&gt;(number of unknowns), the system has a unique solution.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = \text{rank}(A|\mathbf{b}) &amp;amp;lt; n$&lt;/math-renderer&gt;, the system has infinitely many solutions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 3.3.2. Consider&lt;/p&gt;
    &lt;p&gt;The augmented matrix is&lt;/p&gt;
    &lt;p&gt;Row reduction gives&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Example 3.3.3. For&lt;/p&gt;
    &lt;p&gt;the augmented matrix reduces to&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Rank is a measure of independence: it tells us how many truly distinct equations or directions are present. Consistency explains when equations align versus when they contradict. These concepts connect linear systems to vector spaces and prepare for the ideas of dimension, basis, and the Rank‚ÄìNullity Theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the rank of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine whether the system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is consistent.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Show that the rank of the identity matrix&lt;/p&gt;&lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;is&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Give an example of a system in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;with infinitely many solutions, and explain why it satisfies the rank condition.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that for any matrix&lt;/p&gt;&lt;math-renderer&gt;$A \in \mathbb{R}^{m \times n}$&lt;/math-renderer&gt;, $ \text{rank}(A) \leq \min(m,n). $&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A homogeneous system is a linear system in which all constant terms are zero:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Every homogeneous system has at least one solution:&lt;/p&gt;
    &lt;p&gt;This is called the trivial solution. The interesting question is whether nontrivial solutions (nonzero vectors) exist.&lt;/p&gt;
    &lt;p&gt;Nontrivial solutions exist precisely when the number of unknowns exceeds the rank of the coefficient matrix:&lt;/p&gt;
    &lt;p&gt;In this case, there are infinitely many solutions, forming a subspace of &lt;/p&gt;
    &lt;p&gt;where null(A) is the set of all solutions to &lt;/p&gt;
    &lt;p&gt;Example 3.4.1. Consider&lt;/p&gt;
    &lt;p&gt;The augmented matrix is&lt;/p&gt;
    &lt;p&gt;Row reduction:&lt;/p&gt;
    &lt;p&gt;So the system is equivalent to:&lt;/p&gt;
    &lt;p&gt;From the second equation, &lt;/p&gt;
    &lt;p&gt;Thus solutions are:&lt;/p&gt;
    &lt;p&gt;The null space is the line spanned by the vector &lt;/p&gt;
    &lt;p&gt;The solution set of a homogeneous system is always a subspace of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n$&lt;/math-renderer&gt;, the only solution is the zero vector.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n-1$&lt;/math-renderer&gt;, the solution set is a line through the origin.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n-2$&lt;/math-renderer&gt;, the solution set is a plane through the origin.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More generally, the null space has dimension &lt;/p&gt;
    &lt;p&gt;Homogeneous systems are central to understanding vector spaces, subspaces, and dimension. They lead directly to the concepts of kernel, null space, and linear dependence. In applications, homogeneous systems appear in equilibrium problems, eigenvalue equations, and computer graphics transformations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve the homogeneous system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What is the dimension of its solution space?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find all solutions of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Show that the solution set of any homogeneous system is a subspace of&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Suppose&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is a&lt;math-renderer&gt;$3 \times 3$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$\text{rank}(A) = 2$&lt;/math-renderer&gt;. What is the dimension of the null space of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;compute a basis for the null space of &lt;/p&gt;
    &lt;p&gt;Up to now we have studied vectors and matrices concretely in &lt;/p&gt;
    &lt;p&gt;A vector space over the real numbers &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Vector addition: For any &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in V$&lt;/math-renderer&gt;, there is a vector&lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} \in V$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Scalar multiplication: For any scalar &lt;math-renderer&gt;$c \in \mathbb{R}$&lt;/math-renderer&gt;and any&lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;, there is a vector&lt;math-renderer&gt;$c\mathbf{v} \in V$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations must satisfy the following axioms (for all &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Commutativity of addition: &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Associativity of addition: &lt;math-renderer&gt;$(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Additive identity: There exists a zero vector &lt;math-renderer&gt;$\mathbf{0} \in V$&lt;/math-renderer&gt;such that&lt;math-renderer&gt;$\mathbf{v} + \mathbf{0} = \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Additive inverses: For each &lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;, there exists&lt;math-renderer&gt;$(-\mathbf{v} \in V$&lt;/math-renderer&gt;such that&lt;math-renderer&gt;$\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Compatibility of scalar multiplication: &lt;math-renderer&gt;$a(b\mathbf{v}) = (ab)\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Identity element of scalars: &lt;math-renderer&gt;$1 \cdot \mathbf{v} = \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Distributivity over vector addition: &lt;math-renderer&gt;$a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Distributivity over scalar addition: &lt;math-renderer&gt;$(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If a set &lt;/p&gt;
    &lt;p&gt;Example 4.1.1. Standard Euclidean space &lt;/p&gt;
    &lt;p&gt;Example 4.1.2. Polynomials The set of all polynomials with real coefficients, denoted &lt;/p&gt;
    &lt;p&gt;Example 4.1.3. Functions The set of all real-valued functions on an interval, e.g. &lt;/p&gt;
    &lt;p&gt;Not every set with operations qualifies. For instance, the set of positive real numbers under usual addition is not a vector space, because additive inverses (negative numbers) are missing. The axioms must all hold.&lt;/p&gt;
    &lt;p&gt;In familiar cases like &lt;/p&gt;
    &lt;p&gt;The concept of vector space unifies seemingly different mathematical objects under a single framework. Whether dealing with forces in physics, signals in engineering, or data in machine learning, the common language of vector spaces allows us to use the same techniques everywhere.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;with standard addition and scalar multiplication satisfies all eight vector space axioms.&lt;/item&gt;
      &lt;item&gt;Show that the set of integers &lt;math-renderer&gt;$\mathbb{Z}$&lt;/math-renderer&gt;with ordinary operations is not a vector space over&lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;. Which axiom fails?&lt;/item&gt;
      &lt;item&gt;Consider the set of all polynomials of degree at most 3. Show it forms a vector space over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;. What is its dimension?&lt;/item&gt;
      &lt;item&gt;Give an example of a vector space where the vectors are not geometric objects.&lt;/item&gt;
      &lt;item&gt;Prove that in any vector space, the zero vector is unique.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A subspace is a smaller vector space living inside a larger one. Just as lines and planes naturally sit inside three-dimensional space, subspaces generalize these ideas to higher dimensions and more abstract settings.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{0} \in W$&lt;/math-renderer&gt;(contains the zero vector),&lt;/item&gt;
      &lt;item&gt;For all &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in W$&lt;/math-renderer&gt;, the sum&lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} \in W$&lt;/math-renderer&gt;(closed under addition),&lt;/item&gt;
      &lt;item&gt;For all scalars &lt;math-renderer&gt;$c \in \mathbb{R}$&lt;/math-renderer&gt;and vectors&lt;math-renderer&gt;$\mathbf{v} \in W$&lt;/math-renderer&gt;, the product&lt;math-renderer&gt;$c\mathbf{v} \in W$&lt;/math-renderer&gt;(closed under scalar multiplication).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If these hold, then &lt;/p&gt;
    &lt;p&gt;Example 4.2.1. Line through the origin in &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.2.2. The x‚Äìy plane in &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.2.3. Null space of a matrix For a matrix &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Not every subset is a subspace.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The set &lt;math-renderer&gt;${ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }$&lt;/math-renderer&gt;is not a subspace: it is not closed under scalar multiplication (a negative scalar breaks the condition).&lt;/item&gt;
      &lt;item&gt;Any line in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;that does not pass through the origin is not a subspace, because it does not contain&lt;math-renderer&gt;$\mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subspaces are the linear structures inside vector spaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;, the subspaces are: the zero vector, any line through the origin, or the entire plane.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, the subspaces are: the zero vector, any line through the origin, any plane through the origin, or the entire space.&lt;/item&gt;
      &lt;item&gt;In higher dimensions, the same principle applies: subspaces are the flat linear pieces through the origin.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subspaces capture the essential structure of linear problems. Column spaces, row spaces, and null spaces are all subspaces. Much of linear algebra consists of understanding how these subspaces intersect, span, and complement each other.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prove that the set &lt;math-renderer&gt;$W = { (x,0) \mid x \in \mathbb{R} } \subseteq \mathbb{R}^2$&lt;/math-renderer&gt;is a subspace.&lt;/item&gt;
      &lt;item&gt;Show that the line &lt;math-renderer&gt;${ (1+t, 2t) \mid t \in \mathbb{R} }$&lt;/math-renderer&gt;is not a subspace of&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;. Which condition fails?&lt;/item&gt;
      &lt;item&gt;Determine whether the set of all vectors &lt;math-renderer&gt;$(x,y,z) \in \mathbb{R}^3$&lt;/math-renderer&gt;satisfying&lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;is a subspace.&lt;/item&gt;
      &lt;item&gt;For the matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;describe the null space of &lt;/p&gt;
    &lt;p&gt;The ideas of span, basis, and dimension provide the language for describing the size and structure of subspaces. Together, they tell us how a vector space is generated, how many building blocks it requires, and how those blocks can be chosen.&lt;/p&gt;
    &lt;p&gt;Given a set of vectors &lt;/p&gt;
    &lt;p&gt;The span is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.3.1. In &lt;/p&gt;
    &lt;p&gt;A basis of a vector space &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Span &lt;math-renderer&gt;$V$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Are linearly independent (no vector in the set is a linear combination of the others).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If either condition fails, the set is not a basis.&lt;/p&gt;
    &lt;p&gt;Example 4.3.2. In &lt;/p&gt;
    &lt;p&gt;form a basis. Every vector &lt;/p&gt;
    &lt;p&gt;The dimension of a vector space &lt;/p&gt;
    &lt;p&gt;Examples 4.3.3.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\dim(\mathbb{R}^2) = 2$&lt;/math-renderer&gt;, with basis&lt;math-renderer&gt;$(1,0), (0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\dim(\mathbb{R}^3) = 3$&lt;/math-renderer&gt;, with basis&lt;math-renderer&gt;$(1,0,0), (0,1,0), (0,0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The set of polynomials of degree at most 3 has dimension 4, with basis &lt;math-renderer&gt;$(1, x, x^2, x^3)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The span is like the reach of a set of vectors.&lt;/item&gt;
      &lt;item&gt;A basis is the minimal set of directions needed to reach everything in the space.&lt;/item&gt;
      &lt;item&gt;The dimension is the count of those independent directions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lines, planes, and higher-dimensional flats can all be described in terms of span, basis, and dimension.&lt;/p&gt;
    &lt;p&gt;These concepts classify vector spaces and subspaces in terms of size and structure. Many theorems in linear algebra-such as the Rank‚ÄìNullity Theorem-are consequences of understanding span, basis, and dimension. In practical terms, bases are how we encode data in coordinates, and dimension tells us how much freedom a system truly has.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$(1,0,0)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$(0,1,0)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;span the&lt;math-renderer&gt;$xy$&lt;/math-renderer&gt;-plane in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;. Are they a basis?&lt;/item&gt;
      &lt;item&gt;Find a basis for the line &lt;math-renderer&gt;${(2t,-3t,t) : t \in \mathbb{R}}$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine the dimension of the subspace of &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;defined by&lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that any two different bases of &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;must contain exactly&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;vectors.&lt;/item&gt;
      &lt;item&gt;Give a basis for the set of polynomials of degree &lt;math-renderer&gt;$\leq 2$&lt;/math-renderer&gt;. What is its dimension?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once a basis for a vector space is chosen, every vector can be expressed uniquely as a linear combination of the basis vectors. The coefficients in this combination are called the coordinates of the vector relative to that basis. Coordinates allow us to move between the abstract world of vector spaces and the concrete world of numbers.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;be an ordered basis for &lt;/p&gt;
    &lt;p&gt;The scalars &lt;/p&gt;
    &lt;p&gt;Example 4.4.1. Let the basis be&lt;/p&gt;
    &lt;p&gt;To find the coordinates of &lt;/p&gt;
    &lt;p&gt;This gives the system&lt;/p&gt;
    &lt;p&gt;Adding: &lt;/p&gt;
    &lt;p&gt;So,&lt;/p&gt;
    &lt;p&gt;In &lt;/p&gt;
    &lt;p&gt;Relative to this basis, the coordinates of a vector are simply its entries. Thus, column vectors are coordinate representations by default.&lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;with basis vectors as columns. For any vector &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{u} = P[\mathbf{u}]{\mathcal{B}}, \qquad [\mathbf{u}]{\mathcal{B}} = P^{-1}\mathbf{u}. $$&lt;/p&gt;
    &lt;p&gt;Thus, switching between bases reduces to matrix multiplication.&lt;/p&gt;
    &lt;p&gt;Coordinates are the address of a vector relative to a chosen set of directions. Different bases are like different coordinate systems: Cartesian, rotated, skewed, or scaled. The same vector may look very different numerically depending on the basis, but its geometric identity is unchanged.&lt;/p&gt;
    &lt;p&gt;Coordinates turn abstract vectors into concrete numerical data. Changing basis is the algebraic language for rotations of axes, diagonalization of matrices, and principal component analysis in data science. Mastery of coordinates is essential for moving fluidly between geometry, algebra, and computation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Express &lt;math-renderer&gt;$(4,2)$&lt;/math-renderer&gt;in terms of the basis&lt;math-renderer&gt;$(1,1), (1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the coordinates of &lt;math-renderer&gt;$(1,2,3)$&lt;/math-renderer&gt;relative to the standard basis of&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\mathcal{B} = {(2,0), (0,3)}$&lt;/math-renderer&gt;, compute&lt;math-renderer&gt;$[ (4,6) ]_{\mathcal{B}}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Construct the change of basis matrix from the standard basis of &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;to&lt;math-renderer&gt;$\mathcal{B} = {(1,1), (1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that coordinate representation with respect to a basis is unique.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A central theme of linear algebra is understanding linear transformations: functions between vector spaces that preserve their algebraic structure. These transformations generalize the idea of matrix multiplication and capture the essence of linear behavior.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;is called a linear transformation (or linear map) if for all vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Additivity:&lt;/p&gt;
        &lt;p&gt;$$ T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}), $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Homogeneity:&lt;/p&gt;
        &lt;p&gt;$$ T(c\mathbf{u}) = cT(\mathbf{u}). $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If both conditions hold, then &lt;/p&gt;
    &lt;p&gt;Example 5.1.1. Scaling in &lt;/p&gt;
    &lt;p&gt;This doubles the length of every vector, preserving direction. It is linear.&lt;/p&gt;
    &lt;p&gt;Example 5.1.2. Rotation. Let &lt;/p&gt;
    &lt;p&gt;This rotates vectors by angle &lt;/p&gt;
    &lt;p&gt;Example 5.1.3. Differentiation. Let &lt;/p&gt;
    &lt;p&gt;The map &lt;/p&gt;
    &lt;p&gt;is not linear, because &lt;/p&gt;
    &lt;p&gt;Linear transformations are exactly those that preserve the origin, lines through the origin, and proportions along those lines. They include familiar operations: scaling, rotations, reflections, shears, and projections. Nonlinear transformations bend or curve space, breaking these properties.&lt;/p&gt;
    &lt;p&gt;Linear transformations unify geometry, algebra, and computation. They explain how matrices act on vectors, how data can be rotated or projected, and how systems evolve under linear rules. Much of linear algebra is devoted to understanding these transformations, their representations, and their invariants.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Verify that&lt;/p&gt;&lt;math-renderer&gt;$T(x,y) = (3x-y, 2y)$&lt;/math-renderer&gt;is a linear transformation on&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that&lt;/p&gt;&lt;math-renderer&gt;$T(x,y) = (x+1, y)$&lt;/math-renderer&gt;is not linear. Which axiom fails?&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that if&lt;/p&gt;&lt;math-renderer&gt;$T$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$S$&lt;/math-renderer&gt;are linear transformations, then so is&lt;math-renderer&gt;$T+S$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Give an example of a linear transformation from&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;to&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Let&lt;/p&gt;&lt;math-renderer&gt;$T:\mathbb{R}[x] \to \mathbb{R}[x]$&lt;/math-renderer&gt;be integration:&lt;p&gt;$$ T(p(x)) = \int_0^x p(t),dt. $$&lt;/p&gt;&lt;p&gt;Prove that&lt;/p&gt;&lt;math-renderer&gt;$T$&lt;/math-renderer&gt;is a linear transformation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. This correspondence is one of the central insights of linear algebra: it lets us use the tools of matrix arithmetic to study abstract transformations.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;The action of &lt;/p&gt;
    &lt;p&gt;$$ T(\mathbf{e}j) = \begin{bmatrix} a{1j} \ a_{2j} \ \vdots \ a_{mj} \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Placing these outputs as columns gives the matrix of &lt;/p&gt;
    &lt;p&gt;Then for any vector &lt;/p&gt;
    &lt;p&gt;Example 5.2.1. Scaling in &lt;/p&gt;
    &lt;p&gt;So the matrix is&lt;/p&gt;
    &lt;p&gt;Example 5.2.2. Rotation in the plane. The rotation transformation &lt;/p&gt;
    &lt;p&gt;Example 5.2.3. Projection onto the x-axis. The map &lt;/p&gt;
    &lt;p&gt;Matrix representations depend on the chosen basis. If &lt;/p&gt;
    &lt;p&gt;Matrices are not just convenient notation-they are linear maps once a basis is fixed. Every rotation, reflection, projection, shear, or scaling corresponds to multiplying by a specific matrix. Thus, studying linear transformations reduces to studying their matrices.&lt;/p&gt;
    &lt;p&gt;Matrix representations make linear transformations computable. They connect abstract definitions to explicit calculations, enabling algorithms for solving systems, finding eigenvalues, and performing decompositions. Applications from graphics to machine learning depend on this translation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the matrix representation of &lt;math-renderer&gt;$T:\mathbb{R}^2 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y) = (x+y, x-y)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine the matrix of the linear transformation &lt;math-renderer&gt;$T:\mathbb{R}^3 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y,z) = (x+z, y-2z)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;What matrix represents reflection across the line &lt;math-renderer&gt;$y=x$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Show that the matrix of the identity transformation on &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;is&lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For the differentiation map &lt;math-renderer&gt;$D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$&lt;/math-renderer&gt;, where&lt;math-renderer&gt;$\mathbb{R}_k[x]$&lt;/math-renderer&gt;is the space of polynomials of degree at most&lt;math-renderer&gt;$k$&lt;/math-renderer&gt;, find the matrix of&lt;math-renderer&gt;$D$&lt;/math-renderer&gt;relative to the bases&lt;math-renderer&gt;${1,x,x^2}$&lt;/math-renderer&gt;and&lt;math-renderer&gt;${1,x}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To understand a linear transformation deeply, we must examine what it kills and what it produces. These ideas are captured by the kernel and the image, two fundamental subspaces associated with any linear map.&lt;/p&gt;
    &lt;p&gt;The kernel (or null space) of a linear transformation &lt;/p&gt;
    &lt;p&gt;The kernel is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 5.3.1. Let &lt;/p&gt;
    &lt;p&gt;In matrix form,&lt;/p&gt;
    &lt;p&gt;To find the kernel, solve&lt;/p&gt;
    &lt;p&gt;This gives the equations &lt;/p&gt;
    &lt;p&gt;a line in &lt;/p&gt;
    &lt;p&gt;The image (or range) of a linear transformation &lt;/p&gt;
    &lt;p&gt;Equivalently, it is the span of the columns of the representing matrix. The image is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 5.3.2. For the same transformation as above,&lt;/p&gt;
    &lt;p&gt;the columns are &lt;/p&gt;
    &lt;p&gt;For a linear transformation &lt;/p&gt;
    &lt;p&gt;This fundamental result connects the lost directions (kernel) with the achieved directions (image).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The kernel describes how the transformation flattens space (e.g., projecting a 3D object onto a plane).&lt;/item&gt;
      &lt;item&gt;The image describes the target subspace reached by the transformation.&lt;/item&gt;
      &lt;item&gt;The rank‚Äìnullity theorem quantifies the tradeoff: the more dimensions collapse, the fewer remain in the image.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Kernel and image capture the essence of a linear map. They classify transformations, explain when systems have unique or infinite solutions, and form the backbone of important results like the Rank‚ÄìNullity Theorem, diagonalization, and spectral theory.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the kernel and image of &lt;math-renderer&gt;$T:\mathbb{R}^2 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y) = (x-y, x+y)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Let $A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \ 0 &amp;amp; 1 &amp;amp; 4 \end{bmatrix}$. Find bases for &lt;math-renderer&gt;$\ker(A)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\text{im}(A)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For the projection map &lt;math-renderer&gt;$P(x,y,z) = (x,y,0)$&lt;/math-renderer&gt;, describe the kernel and image.&lt;/item&gt;
      &lt;item&gt;Prove that &lt;math-renderer&gt;$\ker(T)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\text{im}(T)$&lt;/math-renderer&gt;are always subspaces.&lt;/item&gt;
      &lt;item&gt;Verify the Rank‚ÄìNullity Theorem for the transformation in Example 5.3.1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear transformations can look very different depending on the coordinate system we use. The process of rewriting vectors and transformations relative to a new basis is called a change of basis. This concept lies at the heart of diagonalization, orthogonalization, and many computational techniques.&lt;/p&gt;
    &lt;p&gt;Suppose &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;Equivalently,&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Thus, changing basis corresponds to a similarity transformation of the matrix.&lt;/p&gt;
    &lt;p&gt;Example 5.4.1. Let &lt;/p&gt;
    &lt;p&gt;In the standard basis, its matrix is&lt;/p&gt;
    &lt;p&gt;Now consider the basis &lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Computing gives&lt;/p&gt;
    &lt;p&gt;In this new basis, the transformation is diagonal: one direction is scaled by 4, the other collapsed to 0.&lt;/p&gt;
    &lt;p&gt;Change of basis is like rotating or skewing your coordinate grid. The underlying transformation does not change, but its description in numbers becomes simpler or more complicated depending on the basis. Finding a basis that simplifies a transformation (often a diagonal basis) is a key theme in linear algebra.&lt;/p&gt;
    &lt;p&gt;Change of basis connects the abstract notion of similarity to practical computation. It is the tool that allows us to diagonalize matrices, compute eigenvalues, and simplify complex transformations. In applications, it corresponds to choosing a more natural coordinate system-whether in geometry, physics, or machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Let $A = \begin{bmatrix} 2 &amp;amp; 1 \ 0 &amp;amp; 2 \end{bmatrix}$. Compute its representation in the basis &lt;math-renderer&gt;${(1,0),(1,1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the change-of-basis matrix from the standard basis of &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;to&lt;math-renderer&gt;${(2,1),(1,1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that similar matrices (related by &lt;math-renderer&gt;$P^{-1}AP$&lt;/math-renderer&gt;) represent the same linear transformation under different bases.&lt;/item&gt;
      &lt;item&gt;Diagonalize the matrix $A = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; -1 \end{bmatrix}$ in the basis &lt;math-renderer&gt;${(1,1),(1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, let&lt;math-renderer&gt;$\mathcal{B} = {(1,0,0),(1,1,0),(1,1,1)}$&lt;/math-renderer&gt;. Construct the change-of-basis matrix&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;and compute&lt;math-renderer&gt;$P^{-1}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Determinants are numerical values associated with square matrices. At first they may appear as a complicated formula, but their importance comes from what they measure: determinants encode scaling, orientation, and invertibility of linear transformations. They bridge algebra and geometry.&lt;/p&gt;
    &lt;p&gt;For a &lt;/p&gt;
    &lt;p&gt;the determinant is defined as&lt;/p&gt;
    &lt;p&gt;Geometric meaning: If &lt;/p&gt;
    &lt;p&gt;For&lt;/p&gt;
    &lt;p&gt;the determinant can be computed as&lt;/p&gt;
    &lt;p&gt;Geometric meaning: In &lt;/p&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;: the transformation squashes space into a lower dimension, so&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is not invertible.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) &amp;amp;gt; 0$&lt;/math-renderer&gt;: volume is scaled by&lt;math-renderer&gt;$\det(A)$&lt;/math-renderer&gt;, orientation preserved.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) &amp;amp;lt; 0$&lt;/math-renderer&gt;: volume is scaled by&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;, orientation reversed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Shear in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $A = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(A) = 1$&lt;/math-renderer&gt;. The transformation slants the unit square into a parallelogram but preserves area.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Projection in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $A = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 0 \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;. The unit square collapses into a line segment: area vanishes.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Rotation in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $R_\theta = \begin{bmatrix} \cos\theta &amp;amp; -\sin\theta \ \sin\theta &amp;amp; \cos\theta \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(R_\theta) = 1$&lt;/math-renderer&gt;. Rotations preserve area and orientation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The determinant is not just a formula-it is a measure of transformation. It tells us whether a matrix is invertible, how it distorts space, and whether it flips orientation. This geometric insight makes the determinant indispensable in analysis, geometry, and applied mathematics.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the determinant of $\begin{bmatrix} 2 &amp;amp; 3 \ 1 &amp;amp; 4 \end{bmatrix}$. What area scaling factor does it represent?&lt;/item&gt;
      &lt;item&gt;Find the determinant of the shear matrix $\begin{bmatrix} 1 &amp;amp; 2 \ 0 &amp;amp; 1 \end{bmatrix}$. What happens to the area of the unit square?&lt;/item&gt;
      &lt;item&gt;For the &lt;math-renderer&gt;$3 \times 3$&lt;/math-renderer&gt;matrix $\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 2 &amp;amp; 0 \ 0 &amp;amp; 0 &amp;amp; 3 \end{bmatrix}$, compute the determinant. How does it scale volume in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Show that any rotation matrix in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;has determinant&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Give an example of a &lt;math-renderer&gt;$2 \times 2$&lt;/math-renderer&gt;matrix with determinant&lt;math-renderer&gt;$-1$&lt;/math-renderer&gt;. What geometric action does it represent?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Beyond their geometric meaning, determinants satisfy a collection of algebraic rules that make them powerful tools in linear algebra. These properties allow us to compute efficiently, test invertibility, and understand how determinants behave under matrix operations.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Identity:&lt;/p&gt;
        &lt;p&gt;$$ \det(I_n) = 1. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Triangular matrices: If&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is upper or lower triangular, then&lt;p&gt;$$ \det(A) = a_{11} a_{22} \cdots a_{nn}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Row/column swap: Interchanging two rows (or columns) multiplies the determinant by&lt;/p&gt;&lt;math-renderer&gt;$-1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Row/column scaling: Multiplying a row (or column) by a scalar&lt;/p&gt;&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;multiplies the determinant by&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Row/column addition: Adding a multiple of one row to another does not change the determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Transpose:&lt;/p&gt;
        &lt;p&gt;$$ \det(A^T) = \det(A). $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiplicativity:&lt;/p&gt;
        &lt;p&gt;$$ \det(AB) = \det(A)\det(B). $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Invertibility:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible if and only if&lt;math-renderer&gt;$\det(A) \neq 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.2.1. For&lt;/p&gt;
    &lt;p&gt;Example 6.2.2. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Since &lt;/p&gt;
    &lt;p&gt;This matches the multiplicativity rule: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Row swaps: flipping orientation of space.&lt;/item&gt;
      &lt;item&gt;Scaling a row: stretching space in one direction.&lt;/item&gt;
      &lt;item&gt;Row replacement: sliding hyperplanes without altering volume.&lt;/item&gt;
      &lt;item&gt;Multiplicativity: performing two transformations multiplies their scaling factors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These properties make determinants both computationally manageable and geometrically interpretable.&lt;/p&gt;
    &lt;p&gt;Determinant properties connect computation with geometry and theory. They explain why Gaussian elimination works, why invertibility is equivalent to nonzero determinant, and why determinants naturally arise in areas like volume computation, eigenvalue theory, and differential equations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the determinant of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \ 0 &amp;amp; 1 &amp;amp; 4 \ 0 &amp;amp; 0 &amp;amp; 2 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that if two rows of a square matrix are identical, then its determinant is zero.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Verify&lt;/p&gt;&lt;math-renderer&gt;$\det(A^T) = \det(A)$&lt;/math-renderer&gt;for&lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; -1 \ 3 &amp;amp; 4 \end{bmatrix}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible, prove that&lt;p&gt;$$ \det(A^{-1}) = \frac{1}{\det(A)}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Suppose&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is a&lt;math-renderer&gt;$3\times 3$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$\det(A) = 5$&lt;/math-renderer&gt;. What is&lt;math-renderer&gt;$\det(2A)$&lt;/math-renderer&gt;?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While determinants of small matrices can be computed directly from formulas, larger matrices require a systematic method. The cofactor expansion (also known as Laplace expansion) provides a recursive way to compute determinants by breaking them into smaller ones.&lt;/p&gt;
    &lt;p&gt;For an &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The minor &lt;math-renderer&gt;$M_{ij}$&lt;/math-renderer&gt;is the determinant of the&lt;math-renderer&gt;$(n-1) \times (n-1)$&lt;/math-renderer&gt;matrix obtained by deleting the&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;-th row and&lt;math-renderer&gt;$j$&lt;/math-renderer&gt;-th column of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The cofactor &lt;math-renderer&gt;$C_{ij}$&lt;/math-renderer&gt;is defined by&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The sign factor &lt;/p&gt;
    &lt;p&gt;$$ \begin{bmatrix}&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; - &amp;amp; + &amp;amp; - &amp;amp; \cdots \&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; + &amp;amp; - &amp;amp; + &amp;amp; \cdots \&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; - &amp;amp; + &amp;amp; - &amp;amp; \cdots \ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots \end{bmatrix}. $$&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The determinant of &lt;/p&gt;
    &lt;p&gt;Example 6.3.1. Compute&lt;/p&gt;
    &lt;p&gt;Expand along the first row:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{11}$&lt;/math-renderer&gt;: $M_{11} = \det \begin{bmatrix} 4 &amp;amp; 5 \ 0 &amp;amp; 6 \end{bmatrix} = 24$, so&lt;math-renderer&gt;$C_{11} = (+1)(24) = 24$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{12}$&lt;/math-renderer&gt;: $M_{12} = \det \begin{bmatrix} 0 &amp;amp; 5 \ 1 &amp;amp; 6 \end{bmatrix} = 0 - 5 = -5$, so&lt;math-renderer&gt;$C_{12} = (-1)(-5) = 5$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{13}$&lt;/math-renderer&gt;: $M_{13} = \det \begin{bmatrix} 0 &amp;amp; 4 \ 1 &amp;amp; 0 \end{bmatrix} = 0 - 4 = -4$, so&lt;math-renderer&gt;$C_{13} = (+1)(-4) = -4$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus,&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Expansion along any row or column yields the same result.&lt;/item&gt;
      &lt;item&gt;The cofactor expansion provides a recursive definition of determinant: a determinant of size &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;is expressed in terms of determinants of size&lt;math-renderer&gt;$n-1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Cofactors are fundamental in constructing the adjugate matrix, which gives a formula for inverses:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cofactor expansion breaks down the determinant into contributions from sub-volumes defined by fixing one row or column at a time. Each cofactor measures how that row/column influences the overall volume scaling.&lt;/p&gt;
    &lt;p&gt;Cofactor expansion generalizes the small-matrix formulas and provides a conceptual definition of determinants. While not the most efficient way to compute determinants for large matrices, it is essential for theory, proofs, and connections to adjugates, Cramer‚Äôs rule, and classical geometry.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the determinant of&lt;/p&gt;
        &lt;p&gt;$$ \begin{bmatrix} 2 &amp;amp; 0 &amp;amp; 1 \ 3 &amp;amp; -1 &amp;amp; 4 \ 1 &amp;amp; 2 &amp;amp; 0 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;by cofactor expansion along the first column.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify that expanding along the second row of Example 6.3.1 gives the same determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that expansion along any row gives the same value.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that if a row of a matrix is zero, then its determinant is zero.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use cofactor expansion to prove that&lt;/p&gt;&lt;math-renderer&gt;$\det(A) = \det(A^T)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Determinants are not merely algebraic curiosities; they have concrete geometric and computational uses. Two of the most important applications are measuring volumes and testing invertibility of matrices.&lt;/p&gt;
    &lt;p&gt;Given vectors &lt;/p&gt;
    &lt;p&gt;Then &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;gives the area of the parallelogram spanned by&lt;math-renderer&gt;$\mathbf{v}_1, \mathbf{v}_2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;gives the volume of the parallelepiped spanned by&lt;math-renderer&gt;$\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In higher dimensions, it generalizes to &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;-dimensional volume (hypervolume).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;So the parallelepiped has volume &lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;: the transformation collapses space into a lower dimension (area/volume is zero). No inverse exists.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) \neq 0$&lt;/math-renderer&gt;: the transformation scales volume by&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;, and is reversible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.4.2. The matrix&lt;/p&gt;
    &lt;p&gt;has determinant &lt;/p&gt;
    &lt;p&gt;Determinants also provide an explicit formula for solving systems of linear equations when the matrix is invertible. For &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;The sign of &lt;/p&gt;
    &lt;p&gt;Determinants condense key information: they measure scaling, test invertibility, and track orientation. These insights are indispensable in geometry (areas and volumes), analysis (Jacobian determinants in calculus), and computation ( solving systems and checking singularity).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Compute the area of the parallelogram spanned by&lt;/p&gt;&lt;math-renderer&gt;$(2,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,3)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Find the volume of the parallelepiped spanned by&lt;/p&gt;&lt;math-renderer&gt;$(1,0,0), (1,1,0), (1,1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether the matrix $\begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; 6 \end{bmatrix}$ is invertible. Justify using determinants.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use Cramer‚Äôs rule to solve&lt;/p&gt;
        &lt;p&gt;$$ \begin{cases} x + y = 3, \ 2x - y = 0. \end{cases} $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain geometrically why a determinant of zero implies no inverse exists.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To extend the geometric ideas of length, distance, and angle beyond &lt;/p&gt;
    &lt;p&gt;An inner product on a real vector space &lt;/p&gt;
    &lt;p&gt;that assigns to each pair of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Symmetry:&lt;/p&gt;
        &lt;math-renderer&gt;$\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$&lt;/math-renderer&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Linearity in the first argument:&lt;/p&gt;
        &lt;math-renderer&gt;$\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$&lt;/math-renderer&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Positive-definiteness:&lt;/p&gt;&lt;math-renderer&gt;$\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$&lt;/math-renderer&gt;, and equality holds if and only if&lt;math-renderer&gt;$\mathbf{v} = \mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The standard inner product on &lt;/p&gt;
    &lt;p&gt;The norm of a vector is its length, defined in terms of the inner product:&lt;/p&gt;
    &lt;p&gt;For the dot product in &lt;/p&gt;
    &lt;p&gt;The inner product allows us to define the angle &lt;/p&gt;
    &lt;p&gt;Thus, two vectors are orthogonal if &lt;/p&gt;
    &lt;p&gt;Example 7.1.1. In &lt;/p&gt;
    &lt;p&gt;So,&lt;/p&gt;
    &lt;p&gt;Example 7.1.2. In the function space &lt;/p&gt;
    &lt;p&gt;defines a length&lt;/p&gt;
    &lt;p&gt;This generalizes geometry to infinite-dimensional spaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inner product: measures similarity between vectors.&lt;/item&gt;
      &lt;item&gt;Norm: length of a vector.&lt;/item&gt;
      &lt;item&gt;Angle: measure of alignment between two directions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These concepts unify algebraic operations with geometric intuition.&lt;/p&gt;
    &lt;p&gt;Inner products and norms allow us to extend geometry into abstract vector spaces. They form the basis of orthogonality, projections, Fourier series, least squares approximation, and many applications in physics and machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Compute&lt;/p&gt;&lt;math-renderer&gt;$\langle (2,-1,3), (1,4,0) \rangle$&lt;/math-renderer&gt;. Then find the angle between them.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that&lt;/p&gt;&lt;math-renderer&gt;$|(x,y)| = \sqrt{x^2+y^2}$&lt;/math-renderer&gt;satisfies the properties of a norm.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, verify that&lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,-1,0)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$C[0,1]$&lt;/math-renderer&gt;, compute&lt;math-renderer&gt;$\langle f,g \rangle$&lt;/math-renderer&gt;for&lt;math-renderer&gt;$f(x)=x$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$g(x)=1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove the Cauchy‚ÄìSchwarz inequality:&lt;/p&gt;
        &lt;p&gt;$$ |\langle \mathbf{u}, \mathbf{v} \rangle| \leq |\mathbf{u}| , |\mathbf{v}|. $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One of the most useful applications of inner products is the notion of orthogonal projection. Projection allows us to approximate a vector by another lying in a subspace, minimizing error in the sense of distance. This idea underpins geometry, statistics, and numerical analysis.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Given a vector &lt;/p&gt;
    &lt;p&gt;The formula is&lt;/p&gt;
    &lt;p&gt;The error vector &lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;The error vector is &lt;/p&gt;
    &lt;p&gt;Suppose &lt;/p&gt;
    &lt;p&gt;This is the unique vector in &lt;/p&gt;
    &lt;p&gt;Orthogonal projection explains the method of least squares. To solve an overdetermined system &lt;/p&gt;
    &lt;p&gt;Thus, least squares is just projection in disguise.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Projection finds the closest point in a subspace to a given vector.&lt;/item&gt;
      &lt;item&gt;It minimizes distance (error) in the sense of Euclidean norm.&lt;/item&gt;
      &lt;item&gt;Orthogonality ensures the error vector points directly away from the subspace.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonal projection is central in both pure and applied mathematics. It underlies the geometry of subspaces, the theory of Fourier series, regression in statistics, and approximation methods in numerical linear algebra. Whenever we fit data with a simpler model, projection is at work.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the projection of &lt;math-renderer&gt;$(2,3)$&lt;/math-renderer&gt;onto the vector&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$&lt;/math-renderer&gt;is orthogonal to&lt;math-renderer&gt;$\mathbf{u}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$W = \text{span}{(1,0,0), (0,1,0)} \subseteq \mathbb{R}^3$&lt;/math-renderer&gt;. Find the projection of&lt;math-renderer&gt;$(1,2,3)$&lt;/math-renderer&gt;onto&lt;math-renderer&gt;$W$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Explain why least squares fitting corresponds to projection onto the column space of &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that projection onto a subspace &lt;math-renderer&gt;$W$&lt;/math-renderer&gt;is unique: there is exactly one closest vector in&lt;math-renderer&gt;$W$&lt;/math-renderer&gt;to a given&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Gram‚ÄìSchmidt process is a systematic way to turn any linearly independent set of vectors into an orthonormal basis. This is especially useful because orthonormal bases simplify computations: inner products become simple coordinate comparisons, and projections take clean forms.&lt;/p&gt;
    &lt;p&gt;Given a linearly independent set of vectors &lt;/p&gt;
    &lt;p&gt;We proceed step by step:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with &lt;math-renderer&gt;$\mathbf{v}_1$&lt;/math-renderer&gt;, normalize it to get&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Subtract from &lt;math-renderer&gt;$\mathbf{v}_2$&lt;/math-renderer&gt;its projection onto&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;, leaving a vector orthogonal to&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;. Normalize to get&lt;math-renderer&gt;$\mathbf{u}_2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For each &lt;math-renderer&gt;$\mathbf{v}_k$&lt;/math-renderer&gt;, subtract projections onto all previously constructed $\mathbf{u}1, \dots, \mathbf{u}{k-1}$, then normalize.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{w}_k = \mathbf{v}k - \sum{j=1}^{k-1} \langle \mathbf{v}_k, \mathbf{u}_j \rangle \mathbf{u}_j, $$&lt;/p&gt;
    &lt;p&gt;The result &lt;/p&gt;
    &lt;p&gt;Take &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize &lt;math-renderer&gt;$\mathbf{v}_1$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Subtract projection of &lt;math-renderer&gt;$\mathbf{v}_2$&lt;/math-renderer&gt;on&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;Normalize:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Subtract projections from &lt;math-renderer&gt;$\mathbf{v}_3$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After computing, normalize to obtain &lt;/p&gt;
    &lt;p&gt;The result is an orthonormal basis of the span of &lt;/p&gt;
    &lt;p&gt;Gram‚ÄìSchmidt is like straightening out a set of vectors: you start with the original directions and adjust each new vector to be perpendicular to all previous ones. Then you scale to unit length. The process ensures orthogonality while preserving the span.&lt;/p&gt;
    &lt;p&gt;Orthonormal bases simplify inner products, projections, and computations in general. They make coordinate systems easier to work with and are crucial in numerical methods, QR decomposition, Fourier analysis, and statistics (orthogonal polynomials, principal component analysis).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apply Gram‚ÄìSchmidt to &lt;math-renderer&gt;$(1,0), (1,1)$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Orthogonalize &lt;math-renderer&gt;$(1,1,1), (1,0,1)$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that each step of Gram‚ÄìSchmidt yields a vector orthogonal to all previous ones.&lt;/item&gt;
      &lt;item&gt;Show that Gram‚ÄìSchmidt preserves the span of the original vectors.&lt;/item&gt;
      &lt;item&gt;Explain how Gram‚ÄìSchmidt leads to the QR decomposition of a matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An orthonormal basis is a basis of a vector space in which all vectors are both orthogonal to each other and have unit length. Such bases are the most convenient possible coordinate systems: computations involving inner products, projections, and norms become exceptionally simple.&lt;/p&gt;
    &lt;p&gt;A set of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$&lt;/math-renderer&gt;whenever&lt;math-renderer&gt;$i \neq j$&lt;/math-renderer&gt;(orthogonality),&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$|\mathbf{u}_i| = 1$&lt;/math-renderer&gt;for all&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;(normalization),&lt;/item&gt;
      &lt;item&gt;The set spans &lt;math-renderer&gt;$V$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 7.4.1. In &lt;/p&gt;
    &lt;p&gt;is orthonormal under the dot product.&lt;/p&gt;
    &lt;p&gt;Example 7.4.2. In &lt;/p&gt;
    &lt;p&gt;is orthonormal.&lt;/p&gt;
    &lt;p&gt;Example 7.4.3. Fourier basis on functions:&lt;/p&gt;
    &lt;p&gt;is an orthogonal set in the space of square-integrable functions on &lt;/p&gt;
    &lt;p&gt;After normalization, it becomes an orthonormal basis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Coordinate simplicity: If&lt;/p&gt;&lt;math-renderer&gt;${\mathbf{u}_1,\dots,\mathbf{u}_n}$&lt;/math-renderer&gt;is an orthonormal basis of&lt;math-renderer&gt;$V$&lt;/math-renderer&gt;, then any vector&lt;math-renderer&gt;$\mathbf{v}\in V$&lt;/math-renderer&gt;has coordinates&lt;p&gt;$$ [\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle \ \vdots \ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}. $$&lt;/p&gt;&lt;p&gt;That is, coordinates are just inner products.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Parseval‚Äôs identity: For any&lt;/p&gt;&lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;,&lt;p&gt;$$ |\mathbf{v}|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i \rangle|^2. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Projections: The orthogonal projection onto the span of&lt;/p&gt;&lt;math-renderer&gt;${\mathbf{u}_1,\dots,\mathbf{u}_k}$&lt;/math-renderer&gt;is&lt;p&gt;$$ \text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i. $$&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start with any linearly independent set, then apply the Gram‚ÄìSchmidt process to obtain an orthonormal set spanning the same subspace.&lt;/item&gt;
      &lt;item&gt;In practice, orthonormal bases are often chosen for numerical stability and simplicity of computation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An orthonormal basis is like a perfectly aligned and equally scaled coordinate system. Distances and angles are computed directly using coordinates without correction factors. They are the ideal rulers of linear algebra.&lt;/p&gt;
    &lt;p&gt;Orthonormal bases simplify every aspect of linear algebra: solving systems, computing projections, expanding functions, diagonalizing symmetric matrices, and working with Fourier series. In data science, principal component analysis produces orthonormal directions capturing maximum variance.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$(1/\sqrt{2})(1,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1/\sqrt{2})(1,-1)$&lt;/math-renderer&gt;form an orthonormal basis of&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Express &lt;math-renderer&gt;$(3,4)$&lt;/math-renderer&gt;in terms of the orthonormal basis&lt;math-renderer&gt;${(1/\sqrt{2})(1,1), (1/\sqrt{2})(1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove Parseval‚Äôs identity for &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;with the dot product.&lt;/item&gt;
      &lt;item&gt;Find an orthonormal basis for the plane &lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Explain why orthonormal bases are numerically more stable than arbitrary bases in computations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The concepts of eigenvalues and eigenvectors reveal the most fundamental behavior of linear transformations. They identify the special directions in which a transformation acts by simple stretching or compressing, without rotation or distortion.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;for some scalar &lt;/p&gt;
    &lt;p&gt;Equivalently, if &lt;/p&gt;
    &lt;p&gt;Example 8.1.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;So &lt;/p&gt;
    &lt;p&gt;Example 8.1.2. Rotation matrix in &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;Eigenvalues arise from solving the characteristic equation:&lt;/p&gt;
    &lt;p&gt;This polynomial in &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Eigenvectors are directions that remain unchanged in orientation under a transformation; only their length is scaled.&lt;/item&gt;
      &lt;item&gt;Eigenvalues tell us the scaling factor along those directions.&lt;/item&gt;
      &lt;item&gt;If a matrix has many independent eigenvectors, it can often be simplified (diagonalized) by changing basis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stretching along principal axes of an ellipse (quadratic forms).&lt;/item&gt;
      &lt;item&gt;Stable directions of dynamical systems.&lt;/item&gt;
      &lt;item&gt;Principal components in statistics and machine learning.&lt;/item&gt;
      &lt;item&gt;Quantum mechanics, where observables correspond to operators with eigenvalues.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues and eigenvectors are a bridge between algebra and geometry. They provide a lens for understanding linear transformations in their simplest form. Nearly every application of linear algebra-differential equations, statistics, physics, computer science-relies on eigen-analysis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the eigenvalues and eigenvectors of $\begin{bmatrix} 4 &amp;amp; 0 \ 0 &amp;amp; -1 \end{bmatrix}$.&lt;/item&gt;
      &lt;item&gt;Show that every scalar multiple of an eigenvector is again an eigenvector for the same eigenvalue.&lt;/item&gt;
      &lt;item&gt;Verify that the rotation matrix &lt;math-renderer&gt;$R_\theta$&lt;/math-renderer&gt;has no real eigenvalues unless&lt;math-renderer&gt;$\theta = 0$&lt;/math-renderer&gt;or&lt;math-renderer&gt;$\pi$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Compute the characteristic polynomial of $\begin{bmatrix} 1 &amp;amp; 2 \ 2 &amp;amp; 1 \end{bmatrix}$.&lt;/item&gt;
      &lt;item&gt;Explain geometrically what eigenvectors and eigenvalues represent for the shear matrix $\begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix}$.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A central goal in linear algebra is to simplify the action of a matrix by choosing a good basis. Diagonalization is the process of rewriting a matrix so that it acts by simple scaling along independent directions. This makes computations such as powers, exponentials, and solving differential equations far easier.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;The diagonal entries of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A matrix is diagonalizable if it has &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;linearly independent eigenvectors.&lt;/item&gt;
      &lt;item&gt;Equivalently, the sum of the dimensions of its eigenspaces equals &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Symmetric matrices (over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;) are always diagonalizable, with an orthonormal basis of eigenvectors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Characteristic polynomial:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So eigenvalues are &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda = 4$&lt;/math-renderer&gt;, solve&lt;math-renderer&gt;$(A-4I)\mathbf{v}=0$&lt;/math-renderer&gt;: $\begin{bmatrix} 0 &amp;amp; 1 \ 0 &amp;amp; -2 \end{bmatrix}\mathbf{v} = 0$, giving&lt;math-renderer&gt;$\mathbf{v}_1 = (1,0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda = 2$&lt;/math-renderer&gt;:&lt;math-renderer&gt;$(A-2I)\mathbf{v}=0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$\mathbf{v}_2 = (1,-2)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Construct $P = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; -2 \end{bmatrix}$. Then&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Computing powers: If&lt;/p&gt;&lt;math-renderer&gt;$A = P D P^{-1}$&lt;/math-renderer&gt;, then&lt;p&gt;$$ A^k = P D^k P^{-1}. $$&lt;/p&gt;&lt;p&gt;Since&lt;/p&gt;&lt;math-renderer&gt;$D$&lt;/math-renderer&gt;is diagonal,&lt;math-renderer&gt;$D^k$&lt;/math-renderer&gt;is easy to compute.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Matrix exponentials:&lt;/p&gt;&lt;math-renderer&gt;$e^A = P e^D P^{-1}$&lt;/math-renderer&gt;, useful in solving differential equations.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Understanding geometry: Diagonalization reveals the directions along which a transformation stretches or compresses space independently.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not all matrices can be diagonalized.&lt;/p&gt;
    &lt;p&gt;has only one eigenvalue &lt;/p&gt;
    &lt;p&gt;Diagonalization means we have found a basis of eigenvectors. In this basis, the matrix acts by simple scaling along each coordinate axis. It transforms complicated motion into independent 1D motions.&lt;/p&gt;
    &lt;p&gt;Diagonalization is a cornerstone of linear algebra. It simplifies computation, reveals structure, and is the starting point for the spectral theorem, Jordan form, and many applications in physics, engineering, and data science.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Diagonalize&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; 0 \ 0 &amp;amp; 3 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is diagonalizable. Why or why not?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Find&lt;/p&gt;&lt;math-renderer&gt;$A^5$&lt;/math-renderer&gt;for&lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 1 \ 0 &amp;amp; 2 \end{bmatrix} $$&lt;/p&gt;&lt;p&gt;using diagonalization.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that any&lt;/p&gt;&lt;math-renderer&gt;$n \times n$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;distinct eigenvalues is diagonalizable.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain why real symmetric matrices are always diagonalizable.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key to finding eigenvalues is the characteristic polynomial of a matrix. This polynomial encodes the values of &lt;/p&gt;
    &lt;p&gt;For an &lt;/p&gt;
    &lt;p&gt;The roots of &lt;/p&gt;
    &lt;p&gt;Example 8.3.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Thus eigenvalues are &lt;/p&gt;
    &lt;p&gt;Example 8.3.2. For&lt;/p&gt;
    &lt;p&gt;(rotation by 90¬∞),&lt;/p&gt;
    &lt;p&gt;Eigenvalues are &lt;/p&gt;
    &lt;p&gt;Example 8.3.3. For a triangular matrix&lt;/p&gt;
    &lt;p&gt;the determinant is simply the product of diagonal entries minus &lt;/p&gt;
    &lt;p&gt;So eigenvalues are &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The characteristic polynomial of an&lt;/p&gt;&lt;math-renderer&gt;$n \times n$&lt;/math-renderer&gt;matrix has degree&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The sum of the eigenvalues (counted with multiplicity) equals the trace of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;:&lt;p&gt;$$ \text{tr}(A) = \lambda_1 + \cdots + \lambda_n. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The product of the eigenvalues equals the determinant of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;:&lt;p&gt;$$ \det(A) = \lambda_1 \cdots \lambda_n. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Similar matrices have the same characteristic polynomial, hence the same eigenvalues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The characteristic polynomial captures when &lt;/p&gt;
    &lt;p&gt;Characteristic polynomials provide the computational tool to extract eigenvalues. They connect matrix invariants (trace and determinant) with geometry, and form the foundation for diagonalization, spectral theorems, and stability analysis in dynamical systems.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the characteristic polynomial of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 1 &amp;amp; 3 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify that the sum of the eigenvalues of $\begin{bmatrix} 5 &amp;amp; 0 \ 0 &amp;amp; -2 \end{bmatrix}$ equals its trace, and their product equals its determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that for any triangular matrix, the eigenvalues are just the diagonal entries.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$B$&lt;/math-renderer&gt;are similar matrices, then&lt;math-renderer&gt;$p_A(\lambda) = p_B(\lambda)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the characteristic polynomial of $\begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 0 \ 0 &amp;amp; 1 &amp;amp; 1 \ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}$.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues and eigenvectors are not only central to the theory of linear algebra-they are indispensable tools across mathematics and applied science. Two classic applications are solving systems of differential equations and analyzing Markov chains.&lt;/p&gt;
    &lt;p&gt;Consider the system&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;is a solution.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Eigenvalues determine the growth or decay rate:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda &amp;amp;lt; 0$&lt;/math-renderer&gt;, solutions decay (stable).&lt;/item&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda &amp;amp;gt; 0$&lt;/math-renderer&gt;, solutions grow (unstable).&lt;/item&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda$&lt;/math-renderer&gt;is complex, oscillations occur.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;If &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By combining eigenvector solutions, we can solve general initial conditions.&lt;/p&gt;
    &lt;p&gt;Example 8.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then eigenvalues are &lt;/p&gt;
    &lt;p&gt;Thus one component grows exponentially, the other decays.&lt;/p&gt;
    &lt;p&gt;A Markov chain is described by a stochastic matrix &lt;/p&gt;
    &lt;p&gt;Iterating gives&lt;/p&gt;
    &lt;p&gt;Understanding long-term behavior reduces to analyzing powers of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The eigenvalue &lt;math-renderer&gt;$\lambda = 1$&lt;/math-renderer&gt;always exists. Its eigenvector gives the steady-state distribution.&lt;/item&gt;
      &lt;item&gt;All other eigenvalues satisfy &lt;math-renderer&gt;$|\lambda| \leq 1$&lt;/math-renderer&gt;. Their influence decays as&lt;math-renderer&gt;$k \to \infty$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 8.4.2. Consider&lt;/p&gt;
    &lt;p&gt;Eigenvalues are &lt;/p&gt;
    &lt;p&gt;Thus, regardless of the starting distribution, the chain converges to &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In differential equations, eigenvalues determine the time evolution: exponential growth, decay, or oscillation.&lt;/item&gt;
      &lt;item&gt;In Markov chains, eigenvalues determine the long-term equilibrium of stochastic processes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalue methods turn complex iterative or dynamical systems into tractable problems. In physics, engineering, and finance, they describe stability and resonance. In computer science and statistics, they power algorithms from Google‚Äôs PageRank to modern machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 &amp;amp; 0 \ 0 &amp;amp; -2 \end{bmatrix}\mathbf{x}$.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;has a complex eigenvalue&lt;math-renderer&gt;$\alpha \pm i\beta$&lt;/math-renderer&gt;, then solutions of&lt;math-renderer&gt;$\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$&lt;/math-renderer&gt;involve oscillations of frequency&lt;math-renderer&gt;$\beta$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the steady-state distribution of&lt;/p&gt;
        &lt;p&gt;$$ P = \begin{bmatrix} 0.7 &amp;amp; 0.2 \ 0.3 &amp;amp; 0.8 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that for any stochastic matrix&lt;/p&gt;&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;is always an eigenvalue.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Explain why all eigenvalues of a stochastic matrix satisfy&lt;/p&gt;&lt;math-renderer&gt;$|\lambda| \leq 1$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A quadratic form is a polynomial of degree two in several variables, expressed neatly using matrices. Quadratic forms appear throughout mathematics: in optimization, geometry of conic sections, statistics (variance), and physics (energy functions).&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Expanded,&lt;/p&gt;
    &lt;p&gt;Because &lt;/p&gt;
    &lt;p&gt;Example 9.1.1. For&lt;/p&gt;
    &lt;p&gt;Example 9.1.2. The quadratic form&lt;/p&gt;
    &lt;p&gt;corresponds to the matrix &lt;/p&gt;
    &lt;p&gt;Example 9.1.3. The conic section equation&lt;/p&gt;
    &lt;p&gt;is described by the quadratic form &lt;/p&gt;
    &lt;p&gt;By choosing a new basis consisting of eigenvectors of &lt;/p&gt;
    &lt;p&gt;Thus quadratic forms can always be expressed as a sum of weighted squares:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Quadratic forms describe geometric shapes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In 2D: ellipses, parabolas, hyperbolas.&lt;/item&gt;
      &lt;item&gt;In 3D: ellipsoids, paraboloids, hyperboloids.&lt;/item&gt;
      &lt;item&gt;In higher dimensions: generalizations of ellipsoids.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diagonalization aligns the coordinate axes with the principal axes of the shape.&lt;/p&gt;
    &lt;p&gt;Quadratic forms unify geometry and algebra. They are central in optimization (minimizing energy functions), statistics ( covariance matrices and variance), mechanics (kinetic energy), and numerical analysis. Understanding quadratic forms leads directly to the spectral theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the quadratic form &lt;math-renderer&gt;$Q(x,y) = 3x^2 + 4xy + y^2$&lt;/math-renderer&gt;as&lt;math-renderer&gt;$\mathbf{x}^T A \mathbf{x}$&lt;/math-renderer&gt;for some symmetric matrix&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For $A = \begin{bmatrix} 1 &amp;amp; 2 \ 2 &amp;amp; 1 \end{bmatrix}$, compute &lt;math-renderer&gt;$Q(x,y)$&lt;/math-renderer&gt;explicitly.&lt;/item&gt;
      &lt;item&gt;Diagonalize the quadratic form &lt;math-renderer&gt;$Q(x,y) = 2x^2 + 2xy + 3y^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Identify the conic section given by &lt;math-renderer&gt;$Q(x,y) = x^2 - y^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is symmetric, quadratic forms defined by&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;are identical.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Quadratic forms are especially important when their associated matrices are positive definite, since these guarantee positivity of energy, distance, or variance. Positive definiteness is a cornerstone in optimization, numerical analysis, and statistics.&lt;/p&gt;
    &lt;p&gt;A symmetric matrix &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Positive definite if&lt;/p&gt;
        &lt;p&gt;$$ \mathbf{x}^T A \mathbf{x} &amp;gt; 0 \quad \text{for all nonzero } \mathbf{x} \in \mathbb{R}^n. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Positive semidefinite if&lt;/p&gt;
        &lt;p&gt;$$ \mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}. $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Similarly, negative definite (always &amp;lt; 0) and indefinite (can be both &amp;lt; 0 and &amp;gt; 0) matrices are defined.&lt;/p&gt;
    &lt;p&gt;Example 9.2.1.&lt;/p&gt;
    &lt;p&gt;is positive definite, since&lt;/p&gt;
    &lt;p&gt;for all &lt;/p&gt;
    &lt;p&gt;Example 9.2.2.&lt;/p&gt;
    &lt;p&gt;has quadratic form&lt;/p&gt;
    &lt;p&gt;This matrix is not positive definite, since &lt;/p&gt;
    &lt;p&gt;For a symmetric matrix &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Eigenvalue test:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if all eigenvalues of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;are positive.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Principal minors test (Sylvester‚Äôs criterion):&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if all leading principal minors ( determinants of top-left&lt;math-renderer&gt;$k \times k$&lt;/math-renderer&gt;submatrices) are positive.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Cholesky factorization:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if it can be written as&lt;p&gt;$$ A = R^T R, $$&lt;/p&gt;&lt;p&gt;where&lt;/p&gt;&lt;math-renderer&gt;$R$&lt;/math-renderer&gt;is an upper triangular matrix with positive diagonal entries.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Positive definite matrices correspond to quadratic forms that define ellipsoids centered at the origin.&lt;/item&gt;
      &lt;item&gt;Positive semidefinite matrices define flattened ellipsoids (possibly degenerate).&lt;/item&gt;
      &lt;item&gt;Indefinite matrices define hyperbolas or saddle-shaped surfaces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimization: Hessians of convex functions are positive semidefinite; strict convexity corresponds to positive definite Hessians.&lt;/item&gt;
      &lt;item&gt;Statistics: Covariance matrices are positive semidefinite.&lt;/item&gt;
      &lt;item&gt;Numerical methods: Cholesky decomposition is widely used to solve systems with positive definite matrices efficiently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Positive definiteness provides stability and guarantees in mathematics and computation. It ensures energy functions are bounded below, optimization problems have unique solutions, and statistical models are meaningful.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Use Sylvester‚Äôs criterion to check whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; -1 \ -1 &amp;amp; 2 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is positive definite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 0 &amp;amp; 1 \ 1 &amp;amp; 0 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is positive definite, semidefinite, or indefinite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the eigenvalues of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 2 &amp;amp; 3 \end{bmatrix}, $$&lt;/p&gt;
        &lt;p&gt;and use them to classify definiteness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that all diagonal matrices with positive entries are positive definite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite, then so is&lt;math-renderer&gt;$P^T A P$&lt;/math-renderer&gt;for any invertible matrix&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The spectral theorem is one of the most powerful results in linear algebra. It states that symmetric matrices can always be diagonalized by an orthogonal basis of eigenvectors. This links algebra (eigenvalues), geometry (orthogonal directions), and applications (stability, optimization, statistics).&lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;All eigenvalues of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;are real.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There exists an orthonormal basis of&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;consisting of eigenvectors of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Thus,&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;can be written as&lt;p&gt;$$ A = Q \Lambda Q^T, $$&lt;/p&gt;&lt;p&gt;where&lt;/p&gt;&lt;math-renderer&gt;$Q$&lt;/math-renderer&gt;is an orthogonal matrix (&lt;math-renderer&gt;$Q^T Q = I$&lt;/math-renderer&gt;) and&lt;math-renderer&gt;$\Lambda$&lt;/math-renderer&gt;is diagonal with eigenvalues of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;on the diagonal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Symmetric matrices are always diagonalizable, and the diagonalization is numerically stable.&lt;/item&gt;
      &lt;item&gt;Quadratic forms &lt;math-renderer&gt;$\mathbf{x}^T A \mathbf{x}$&lt;/math-renderer&gt;can be expressed in terms of eigenvalues and eigenvectors, showing ellipsoids aligned with eigen-directions.&lt;/item&gt;
      &lt;item&gt;Positive definiteness can be checked by confirming that all eigenvalues are positive.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Characteristic polynomial:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues: &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda=1$&lt;/math-renderer&gt;: solve&lt;math-renderer&gt;$(A-I)\mathbf{v} = 0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$(1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda=3$&lt;/math-renderer&gt;: solve&lt;math-renderer&gt;$(A-3I)\mathbf{v} = 0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Then&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;The spectral theorem says every symmetric matrix acts like independent scaling along orthogonal directions. In geometry, this corresponds to stretching space along perpendicular axes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ellipses, ellipsoids, and quadratic surfaces can be fully understood via eigenvalues and eigenvectors.&lt;/item&gt;
      &lt;item&gt;Orthogonality ensures directions remain perpendicular after transformation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimization: The spectral theorem underlies classification of critical points via eigenvalues of the Hessian.&lt;/item&gt;
      &lt;item&gt;PCA (Principal Component Analysis): Data covariance matrices are symmetric, and PCA finds orthogonal directions of maximum variance.&lt;/item&gt;
      &lt;item&gt;Differential equations &amp;amp; physics: Symmetric operators correspond to measurable quantities with real eigenvalues ( stability, energy).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The spectral theorem guarantees that symmetric matrices are as simple as possible: they can always be analyzed in terms of real, orthogonal eigenvectors. This provides both deep theoretical insight and powerful computational tools.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Diagonalize&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 2 &amp;amp; 3 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;using the spectral theorem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that all eigenvalues of a real symmetric matrix are real.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain geometrically how the spectral theorem describes ellipsoids defined by quadratic forms.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apply the spectral theorem to the covariance matrix&lt;/p&gt;
        &lt;p&gt;$$ \Sigma = \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 2 \end{bmatrix}, $$&lt;/p&gt;
        &lt;p&gt;and interpret the eigenvectors as principal directions of variance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Principal Component Analysis (PCA) is a widely used technique in data science, machine learning, and statistics. At its core, PCA is an application of the spectral theorem to covariance matrices: it finds orthogonal directions (principal components) that capture the maximum variance in data.&lt;/p&gt;
    &lt;p&gt;Given a dataset of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Center the data by subtracting the mean vector&lt;/p&gt;&lt;math-renderer&gt;$\bar{\mathbf{x}}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Form the covariance matrix&lt;/p&gt;
        &lt;p&gt;$$ \Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Apply the spectral theorem:&lt;/p&gt;&lt;math-renderer&gt;$\Sigma = Q \Lambda Q^T$&lt;/math-renderer&gt;.&lt;list rend="ul"&gt;&lt;item&gt;Columns of &lt;math-renderer&gt;$Q$&lt;/math-renderer&gt;are orthonormal eigenvectors (principal directions).&lt;/item&gt;&lt;item&gt;Eigenvalues in &lt;math-renderer&gt;$\Lambda$&lt;/math-renderer&gt;measure variance explained by each direction.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Columns of &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first principal component is the eigenvector corresponding to the largest eigenvalue; it is the direction of maximum variance.&lt;/p&gt;
    &lt;p&gt;Suppose we have two-dimensional data points roughly aligned along the line &lt;/p&gt;
    &lt;p&gt;Eigenvalues are about &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First principal component: the line &lt;math-renderer&gt;$y = x$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Most variance lies along this direction.&lt;/item&gt;
      &lt;item&gt;Second component is nearly orthogonal (&lt;math-renderer&gt;$y = -x$&lt;/math-renderer&gt;), but variance there is tiny.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus PCA reduces the data to essentially one dimension.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dimensionality reduction: Represent data with fewer features while retaining most variance.&lt;/item&gt;
      &lt;item&gt;Noise reduction: Small eigenvalues correspond to noise; discarding them filters data.&lt;/item&gt;
      &lt;item&gt;Visualization: Projecting high-dimensional data onto top 2 or 3 principal components reveals structure.&lt;/item&gt;
      &lt;item&gt;Compression: PCA is used in image and signal compression.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The covariance matrix &lt;/p&gt;
    &lt;p&gt;PCA demonstrates how abstract linear algebra directly powers modern applications. Eigenvalues and eigenvectors give a practical method for simplifying data, revealing patterns, and reducing complexity. It is one of the most important algorithms derived from the spectral theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that the covariance matrix is symmetric and positive semidefinite.&lt;/item&gt;
      &lt;item&gt;Compute the covariance matrix of the dataset &lt;math-renderer&gt;$(1,2), (2,3), (3,4)$&lt;/math-renderer&gt;, and find its eigenvalues and eigenvectors.&lt;/item&gt;
      &lt;item&gt;Explain why the first principal component captures the maximum variance.&lt;/item&gt;
      &lt;item&gt;In image compression, explain how PCA can reduce storage by keeping only the top &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;principal components.&lt;/item&gt;
      &lt;item&gt;Prove that the sum of the eigenvalues of the covariance matrix equals the total variance of the dataset.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra is the language of modern computer graphics. Every image rendered on a screen, every 3D model rotated or projected, is ultimately the result of applying matrices to vectors. Rotations, reflections, scalings, and projections are all linear transformations, making matrices the natural tool for manipulating geometry.&lt;/p&gt;
    &lt;p&gt;A counterclockwise rotation by an angle &lt;/p&gt;
    &lt;p&gt;For any vector &lt;/p&gt;
    &lt;p&gt;This preserves lengths and angles, since &lt;/p&gt;
    &lt;p&gt;In three dimensions, rotations are represented by &lt;/p&gt;
    &lt;p&gt;Similar formulas exist for rotations about the &lt;/p&gt;
    &lt;p&gt;More general 3D rotations can be described by axis‚Äìangle representation or quaternions, but the underlying idea is still linear transformations represented by matrices.&lt;/p&gt;
    &lt;p&gt;To display 3D objects on a 2D screen, we use projections:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Orthogonal projection: drops the&lt;/p&gt;&lt;math-renderer&gt;$z$&lt;/math-renderer&gt;-coordinate, mapping&lt;math-renderer&gt;$(x,y,z) \mapsto (x,y)$&lt;/math-renderer&gt;.&lt;p&gt;$$ P = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Perspective projection: mimics the effect of a camera. A point&lt;/p&gt;&lt;math-renderer&gt;$(x,y,z)$&lt;/math-renderer&gt;projects to&lt;p&gt;$$ \left(\frac{x}{z}, \frac{y}{z}\right), $$&lt;/p&gt;&lt;p&gt;capturing how distant objects appear smaller.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations are linear (orthogonal projection) or nearly linear (perspective projection becomes linear in homogeneous coordinates).&lt;/p&gt;
    &lt;p&gt;To unify translations and projections with linear transformations, computer graphics uses homogeneous coordinates. A 3D point &lt;/p&gt;
    &lt;p&gt;Example: Translation by &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotations preserve shape and size, only changing orientation.&lt;/item&gt;
      &lt;item&gt;Projections reduce dimension: from 3D world space to 2D screen space.&lt;/item&gt;
      &lt;item&gt;Homogeneous coordinates allow us to combine multiple transformations (rotation + translation + projection) into a single matrix multiplication.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra enables all real-time graphics: video games, simulations, CAD software, and movie effects. By chaining simple matrix operations, complex transformations are applied efficiently to millions of points per second.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the rotation matrix for a 90¬∞ counterclockwise rotation in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;. Apply it to&lt;math-renderer&gt;$(1,0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Rotate the point &lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;about the&lt;math-renderer&gt;$z$&lt;/math-renderer&gt;-axis by 180¬∞.&lt;/item&gt;
      &lt;item&gt;Show that the determinant of any 2D or 3D rotation matrix is 1.&lt;/item&gt;
      &lt;item&gt;Derive the orthogonal projection matrix from &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;to the&lt;math-renderer&gt;$xy$&lt;/math-renderer&gt;-plane.&lt;/item&gt;
      &lt;item&gt;Explain how homogeneous coordinates allow translations to be represented as matrix multiplications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra provides the foundation for many data science techniques. Two of the most important are dimensionality reduction, where high-dimensional datasets are compressed while preserving essential information, and the least squares method, which underlies regression and model fitting.&lt;/p&gt;
    &lt;p&gt;High-dimensional data often contains redundancy: many features are correlated, meaning the data essentially lies near a lower-dimensional subspace. Dimensionality reduction identifies these subspaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;PCA (Principal Component Analysis): As introduced earlier, PCA diagonalizes the covariance matrix of the data.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Eigenvectors (principal components) define orthogonal directions of maximum variance.&lt;/item&gt;
          &lt;item&gt;Eigenvalues measure how much variance lies along each direction.&lt;/item&gt;
          &lt;item&gt;Keeping only the top &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;components reduces data from&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;-dimensional space to&lt;math-renderer&gt;$k$&lt;/math-renderer&gt;-dimensional space while retaining most variability.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may have most variance captured by just 50 eigenvectors of the covariance matrix. Projecting onto these components compresses the data while preserving essential features.&lt;/p&gt;
    &lt;p&gt;Often, we have more equations than unknowns-an overdetermined system:&lt;/p&gt;
    &lt;p&gt;An exact solution may not exist. Instead, we seek &lt;/p&gt;
    &lt;p&gt;This leads to the normal equations:&lt;/p&gt;
    &lt;p&gt;The solution is the orthogonal projection of &lt;/p&gt;
    &lt;p&gt;Fit a line &lt;/p&gt;
    &lt;p&gt;Matrix form:&lt;/p&gt;
    &lt;p&gt;Solve &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dimensionality reduction: Find the best subspace capturing most variance.&lt;/item&gt;
      &lt;item&gt;Least squares: Project the target vector onto the subspace spanned by predictors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both are projection problems, solved using inner products and orthogonality.&lt;/p&gt;
    &lt;p&gt;Dimensionality reduction makes large datasets tractable, filters noise, and reveals structure. Least squares fitting powers regression, statistics, and machine learning. Both rely directly on eigenvalues, eigenvectors, and projections-core tools of linear algebra.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Explain why PCA reduces noise in datasets by discarding small eigenvalue components.&lt;/item&gt;
      &lt;item&gt;Compute the least squares solution to fitting a line through &lt;math-renderer&gt;$(0,0), (1,1), (2,2)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that the least squares solution is unique if and only if &lt;math-renderer&gt;$A^T A$&lt;/math-renderer&gt;is invertible.&lt;/item&gt;
      &lt;item&gt;Prove that the least squares solution minimizes the squared error by projection arguments.&lt;/item&gt;
      &lt;item&gt;Apply PCA to the data points &lt;math-renderer&gt;$(1,0), (2,1), (3,2)$&lt;/math-renderer&gt;and find the first principal component.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Graphs and networks provide a natural setting where linear algebra comes to life. From modeling flows and connectivity to predicting long-term behavior, matrices translate network structure into algebraic form. Markov chains, already introduced in Section 8.4, are a central example of networks evolving over time.&lt;/p&gt;
    &lt;p&gt;A network (graph) with &lt;/p&gt;
    &lt;p&gt;For weighted graphs, entries may be positive weights instead of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The number of walks of length &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;from node&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;to node&lt;math-renderer&gt;$j$&lt;/math-renderer&gt;is given by the entry&lt;math-renderer&gt;$(A^k)_{ij}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Powers of adjacency matrices thus encode connectivity over time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Another important matrix is the graph Laplacian:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$L$&lt;/math-renderer&gt;is symmetric and positive semidefinite.&lt;/item&gt;
      &lt;item&gt;The smallest eigenvalue is always &lt;math-renderer&gt;$0$&lt;/math-renderer&gt;, with eigenvector&lt;math-renderer&gt;$(1,1,\dots,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The multiplicity of eigenvalue &lt;math-renderer&gt;$0$&lt;/math-renderer&gt;equals the number of connected components in the graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This connection between eigenvalues and connectivity forms the basis of spectral graph theory.&lt;/p&gt;
    &lt;p&gt;A Markov chain can be viewed as a random walk on a graph. If &lt;/p&gt;
    &lt;p&gt;describes the distribution of positions after &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The steady-state distribution is given by the eigenvector of &lt;math-renderer&gt;$P$&lt;/math-renderer&gt;with eigenvalue&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The speed of convergence depends on the gap between the largest eigenvalue (which is always &lt;math-renderer&gt;$1$&lt;/math-renderer&gt;) and the second largest eigenvalue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider a simple 3-node cycle graph:&lt;/p&gt;
    &lt;p&gt;This Markov chain cycles deterministically among the nodes. Eigenvalues are the cube roots of unity: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search engines: Google‚Äôs PageRank algorithm models the web as a Markov chain, where steady-state probabilities rank pages.&lt;/item&gt;
      &lt;item&gt;Network analysis: Eigenvalues of adjacency or Laplacian matrices reveal communities, bottlenecks, and robustness.&lt;/item&gt;
      &lt;item&gt;Epidemiology and information flow: Random walks model how diseases or ideas spread through networks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra transforms network problems into matrix problems. Eigenvalues and eigenvectors reveal connectivity, flow, stability, and long-term dynamics. Networks are everywhere-social media, biology, finance, and the internet-so these tools are indispensable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Write the adjacency matrix of a square graph with 4 nodes. Compute&lt;/p&gt;&lt;math-renderer&gt;$A^2$&lt;/math-renderer&gt;and interpret the entries.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that the Laplacian of a connected graph has exactly one zero eigenvalue.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the steady-state distribution of the Markov chain with&lt;/p&gt;
        &lt;p&gt;$$ P = \begin{bmatrix} 0.5 &amp;amp; 0.5 \ 0.4 &amp;amp; 0.6 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain how eigenvalues of the Laplacian can detect disconnected components of a graph.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Describe how PageRank modifies the transition matrix of the web graph to ensure a unique steady-state distribution.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Modern machine learning is built on linear algebra. From the representation of data as matrices to the optimization of large-scale models, nearly every step relies on concepts such as vector spaces, projections, eigenvalues, and matrix decompositions.&lt;/p&gt;
    &lt;p&gt;A dataset with &lt;/p&gt;
    &lt;p&gt;$$ X = \begin{bmatrix}&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_1^T &amp;amp; - \&lt;/item&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_2^T &amp;amp; - \ &amp;amp; \vdots &amp;amp; \&lt;/item&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_m^T &amp;amp; - \end{bmatrix}, $$&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;where each row &lt;/p&gt;
    &lt;p&gt;At the heart of machine learning are linear predictors:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;This is solved efficiently using matrix factorizations.&lt;/p&gt;
    &lt;p&gt;The SVD of a matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Singular values measure the importance of directions in feature space.&lt;/item&gt;
      &lt;item&gt;SVD is used for dimensionality reduction (low-rank approximations), topic modeling, and recommender systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCA (Principal Component Analysis): diagonalization of the covariance matrix identifies directions of maximal variance.&lt;/item&gt;
      &lt;item&gt;Spectral clustering: uses eigenvectors of the Laplacian to group data points into clusters.&lt;/item&gt;
      &lt;item&gt;Stability analysis: eigenvalues of Hessian matrices determine whether optimization converges to a minimum.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even deep learning, though nonlinear, uses linear algebra at its core:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each layer is a matrix multiplication followed by a nonlinear activation.&lt;/item&gt;
      &lt;item&gt;Training requires computing gradients, which are expressed in terms of matrix calculus.&lt;/item&gt;
      &lt;item&gt;Backpropagation is essentially repeated applications of the chain rule with linear algebra.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Machine learning models often involve datasets with millions of features and parameters. Linear algebra provides the algorithms and abstractions that make training and inference possible. Without it, large-scale computation in AI would be intractable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Show that ridge regression leads to the normal equations&lt;/p&gt;
        &lt;p&gt;$$ (X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain how SVD can be used to compress an image represented as a matrix of pixel intensities.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For a covariance matrix&lt;/p&gt;&lt;math-renderer&gt;$\Sigma$&lt;/math-renderer&gt;, show why its eigenvalues represent variances along principal components.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Give an example of how eigenvectors of the Laplacian matrix can be used for clustering a small graph.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In a neural network with one hidden layer, write the forward pass in matrix form.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/the-litte-book-of/linear-algebra"/></entry><entry><id>https://news.ycombinator.com/item?id=45103646</id><title>You don't want to hire "the best engineers"</title><updated>2025-09-02T16:12:37.364813+00:00</updated><content>&lt;doc fingerprint="ae39800658847ffc"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;√¢We only want to hire the best engineers√¢&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I hear this from almost every client I speak to. So does every other recruiter.&lt;/p&gt;
    &lt;p&gt;Seriously - just say those eight words to any room full of recruiting people, and everyone will give a wry chuckle and roll their eyes. We've all heard it a million times.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;√¢We only want to hire the best engineers.√¢&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;No. No, you do not.&lt;/p&gt;
    &lt;p&gt;The best engineers make more than your entire payroll. They have opinions on tech debt and timelines. They have remote jobs, if they want them. They don√¢t go √¢oh, well, this is your third company, so I guess I√¢ll defer to you on all product decisions√¢. They care about comp, a trait you consider disqualifying. They can care about work-life balance, because they√¢re not desperate enough to feel the need not to. And however successful your company has been so far, they have other options they like better.&lt;/p&gt;
    &lt;p&gt;You√¢re not stupid. If I asked you, point blank, √¢do you actually think the best engineers in the world would give your company a second thought,√¢ I bet you could say √¢well, no, obviously not√¢.&lt;/p&gt;
    &lt;p&gt;But you don√¢t act like it.&lt;/p&gt;
    &lt;p&gt;You lock in the same set of criteria as every other startup. Experience at early stage. Highly independent. In-office in the Bay Area. Not too √¢salary motivated√¢. Don√¢t even apply if you want a 40h/week job - we work hard and play hard.&lt;/p&gt;
    &lt;p&gt;Four months later, you haven√¢t found a good founding engineer. Do you know how long four months is in the life of a young startup? That√¢s an eternity, and you√¢ve spent it in stasis.&lt;/p&gt;
    &lt;p&gt;Hiring is a negotiation, and you√¢re acting like you√¢re holding all the cards when you aren't. You√¢re looking for a highly competitive candidate pool, and you√¢re not being competitive: you√¢re just checking the same baselines as everybody else. You're acting like a replacement-level employer and expecting more than replacement-level candidates.&lt;/p&gt;
    &lt;p&gt;Would you rather spend four months in stasis waiting for a senior candidate who hits the ground running on day one, or hire a skilled midlevel hacker who will be at full capacity in two weeks immediately?&lt;/p&gt;
    &lt;p&gt;Would you rather spend four months in stasis waiting for a 50h/week candidate, or have a 40h/week candidate now?&lt;/p&gt;
    &lt;p&gt;Would you rather be a green bar in this chart, or a red one?&lt;/p&gt;
    &lt;p&gt;You don√¢t ask yourself these questions. You say √¢I want a candidate with these traits,√¢ and sit on your hands until one materializes, until you run out of money, or - more likely - until someone manages to worm through your unrealistic expectations and convince you to compromise for them.&lt;/p&gt;
    &lt;p&gt;If you had accepted compromise, you could have opened the floodgates on day one and had your pick of ten great-but-not-perfect candidates. Instead, you waited months and settled for one.&lt;/p&gt;
    &lt;p&gt;When you accept that you need a great engineer, and not the best engineer, you can deal with the trade-offs consciously. What traits are actually important? How much are you willing to give up to get them? What√¢s the dollar value of a hire this month versus next month? √¢What actually matters today?√¢ is the most important question a startup can ask, and you haven't applied it to one of the most important aspects of running a company!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Well, we're a little different from other companies, because we have really high standards."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Does it sound like you're different?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"We just raised a very exciting Series A!"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So did literally a thousand other companies. There was $26B in early stage venture investment last quarter, and you can do the math as to how much of that your $10M raise occupies. The hires you need aren't looking at your company as the slam-dunk success that a founder necessarily needs to believe that it is. Maybe they will once they talk to you, but that's later - at the top of your funnel, you're just another face in the crowd, and you need to act like it.&lt;/p&gt;
    &lt;p&gt;I√¢m not telling you to hire people who aren√¢t good. I√¢m not even telling you that the traits you want aren√¢t good things to look for. I√¢m not telling you to actually compromise on quality. I√¢m telling you that trying to hire the best engineers is the enemy of actually hiring great ones. You√¢re going to have to give up something (possibly time, possibly comp, possibly workplace policy) to make the hire you want.&lt;/p&gt;
    &lt;p&gt;The longer you aren√¢t thinking about what to give up, the more you√¢re implicitly choosing to give up time, the thing startups treasure more than anything else. And you√¢re giving up time to - what, play it safe?&lt;/p&gt;
    &lt;p&gt;The default outcome for a startup is always failure. You took a risk by even starting one. You ship things that might be broken all the time, because you know that speed is more important than perfection. You take moonshots, because you know that big wins matter more than small losses. And then you give up months of time because you refuse to apply the same philosophy to hiring!&lt;/p&gt;
    &lt;p&gt;I run a recruiting company. It√¢s no skin off my back if you want to be irrational about hiring. Please, by all means, continue. You√¢re leaving a thousand great engineers to sit in my database instead of your ATS, and I would much rather you pay me 40 grand to find them than find them yourself.&lt;/p&gt;
    &lt;p&gt;Or you can act like the scrappy realist you probably like to think you are, stop insisting on perfection, and move fast.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.otherbranch.com/shared/blog/no-you-dont-want-to-hire-the-best-engineers"/></entry><entry><id>https://news.ycombinator.com/item?id=45104082</id><title>Americans Lose Faith That Hard Work Leads to Economic Gains, WSJ-NORC Poll Finds</title><updated>2025-09-02T16:12:37.296052+00:00</updated><content/><link href="https://www.wsj.com/economy/wsj-norc-economic-poll-73bce003"/></entry><entry><id>https://news.ycombinator.com/item?id=45104330</id><title>US stocks fall as bond sell-off spills into equities</title><updated>2025-09-02T16:12:37.189054+00:00</updated><content/><link href="https://subs.ft.com/products"/></entry></feed>