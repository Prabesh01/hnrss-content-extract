<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-07T19:31:55.005162+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45502543</id><title>Erlang ARM32 JIT is born</title><updated>2025-10-07T19:32:08.898650+00:00</updated><content>&lt;doc fingerprint="ea279a502acd05f8"&gt;
  &lt;main&gt;
    &lt;p&gt;A blog series recounting our adventures in the quest to port the BEAM JIT to the ARM32-bit architecture.&lt;/p&gt;
    &lt;p&gt;This work is made possible thanks to funding from the Erlang Ecosystem Foundation and the ongoing support of its Embedded Working Group.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Erlang ARM32 JIT is born!&lt;/head&gt;
    &lt;p&gt;This week we finally achieved our first milestone in developing the ARM32 JIT. We executed our first Erlang function through JITted ARM32 machine code!&lt;/p&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/arm32-jit/otp/RELEASE -progname erl -home /home
    ~/arm32-jit$ echo $?
    42&lt;/code&gt;
    &lt;p&gt;The BEAM successfully runs and terminates with error code 42! That 42 comes from an Erlang function, just-in-time compiled by our ARM32 JIT!&lt;/p&gt;
    &lt;p&gt;Announcement is done! All code is available at https://github.com/stritzinger/otp/tree/arm32-jit&lt;/p&gt;
    &lt;p&gt;Keep reading for a lot of interesting details!&lt;/p&gt;
    &lt;head rend="h2"&gt;The first piece of Erlang code&lt;/head&gt;
    &lt;code&gt;-module(hello).
-export([start/2]).

start(_BootMod, _BootArgs) -&amp;gt;
    halt(42, [{flush, false}]).&lt;/code&gt;
    &lt;p&gt;This is &lt;code&gt;hello.erl&lt;/code&gt; that contains a &lt;code&gt;start/2&lt;/code&gt; function. The function head mimics the &lt;code&gt;erl_init:start/2&lt;/code&gt; function, which is the entry point of the first Erlang process. We replaced &lt;code&gt;erl_init:start/2&lt;/code&gt; with &lt;code&gt;hello:start/2&lt;/code&gt; in the &lt;code&gt;erl_init.c&lt;/code&gt; module of the BEAM VM. This way, we forced the runtime to execute this Erlang function.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;hello:start/2&lt;/code&gt; is very simple as it just calls the &lt;code&gt;erlang:halt/2&lt;/code&gt;. This function is a BIF (Built-in Function) that executes C code, part of the BEAM VM. This code executes an ordered shutdown of the BEAM and allows us to customize the error code, in this case: &lt;code&gt;42&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;(Why &lt;code&gt;{flush, false}&lt;/code&gt;? At the time I am writing this, letting it be true causes a segmentation fault EHEH)&lt;/p&gt;
    &lt;p&gt;Obviously, we need to compile this Erlang module, but I will also generate the BEAM assembly so we can have a look at what we will have to deal with.&lt;/p&gt;
    &lt;code&gt;{module, hello}.  %% version = 0
{exports, [{module_info,0},{module_info,1},{start,2}]}.
{attributes, []}.
{labels, 7}.

{function, start, 2, 2}.
  {label,1}.
    {line,[{location,"erts/preloaded/src/hello.erl",74}]}.
    {func_info,{atom,hello},{atom,start},2}.
  {label,2}.
    {move,{literal,[{flush,false}]},{x,1}}.
    {move,{integer,42},{x,0}}.
    {line,[{location,"erts/preloaded/src/hello.erl",76}]}.
    {call_ext_only,2,{extfunc,erlang,halt,2}}.

{function, module_info, 0, 4}.
  {label,3}.
    {line,[]}.
    {func_info,{atom,hello},{atom,module_info},0}.
  {label,4}.
    {move,{atom,hello},{x,0}}.
    {call_ext_only,1,{extfunc,erlang,get_module_info,1}}.

{function, module_info, 1, 6}.
  {label,5}.
    {line,[]}.
    {func_info,{atom,hello},{atom,module_info},1}.
  {label,6}.
    {move,{x,0},{x,1}}.
    {move,{atom,hello},{x,0}}.
    {call_ext_only,2,{extfunc,erlang,get_module_info,2}}.&lt;/code&gt;
    &lt;p&gt;You can spot the start function and the two standard module_info functions that all Erlang modules have. We do not care much about those right now as we discovered that they are not executed and are not required to work, for now.&lt;/p&gt;
    &lt;p&gt;We can see that the core of the start function is just two &lt;code&gt;move&lt;/code&gt; operations and one &lt;code&gt;call_ext_only&lt;/code&gt;. But bear in mind that the BEAM loader will transmute these Generic BEAM Operations into Specific operations. More complexity will pop up!&lt;/p&gt;
    &lt;head rend="h2"&gt;Execution&lt;/head&gt;
    &lt;p&gt;We are using &lt;code&gt;qemu-arm&lt;/code&gt; to emulate &lt;code&gt;Arm32&lt;/code&gt; and we are directly using &lt;code&gt;beam.smp&lt;/code&gt; to run the BEAM.&lt;/p&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/vagrant/arm32-jit/otp/RELEASE -progname erl -home /home/vagrant&lt;/code&gt;
    &lt;head rend="h3"&gt;JIT initialization&lt;/head&gt;
    &lt;p&gt;At boot, the BEAM initializes the JIT if enabled. The JIT leverages the AsmJit library to emit all machine code instructions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Emission of all global shared fragments&lt;/head&gt;
    &lt;p&gt;There are 90+ code snippets that are shared among all modules. The JIT loads them one single time and sets up jumps to them in every other module. It is like a global library for all modules.&lt;/p&gt;
    &lt;p&gt;We skipped most of these because just the shared fragments involved in the &lt;code&gt;hello:start/2&lt;/code&gt; execution were needed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Emission of the erts_beamasm module&lt;/head&gt;
    &lt;p&gt;As part of the JIT initialization, &lt;code&gt;erts_beamasm&lt;/code&gt; is emitted. This module is an internal hardcoded module that exists only when BEAM is using the JIT. It holds 7 fundamental instructions used to manage the Erlang process executions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;run_process - The main process execution entry point&lt;/item&gt;
      &lt;item&gt;normal_exit - Normal process termination&lt;/item&gt;
      &lt;item&gt;continue_exit - Continue after exit handling&lt;/item&gt;
      &lt;item&gt;exception_trace - Exception tracing functionality&lt;/item&gt;
      &lt;item&gt;return_trace - Return value tracing&lt;/item&gt;
      &lt;item&gt;return_to_trace - Return to tracing state&lt;/item&gt;
      &lt;item&gt;call_trace_return - Call tracing return handling&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Preloaded modules&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;hello.erl&lt;/code&gt; module has been compiled and put as first and single Erlang module in the list of preloaded modules. Preloaded modules are Erlang fundamental modules that are always loaded by the BEAM before the first Erlang process can start. They implement, in Erlang, the core features of the Erlang Runtime System (ERTS). The OTP build scripts group all &lt;code&gt;ebin&lt;/code&gt; files into a single C header that is then linked into the executable. This makes the Erlang binaries available as a static C array in the BEAM source code. These are then loaded one by one after the BEAM VM is initialized.&lt;/p&gt;
    &lt;p&gt;Cool, let's nuke all these modules and leave just our &lt;code&gt;hello.erl&lt;/code&gt;. It does not need many BEAM instructions and we can easily verify that it executes. To do the substitution we just need to change this build variable in otp/erts/emulator/Makefile.in&lt;/p&gt;
    &lt;p&gt;We are running BEAMASM with &lt;code&gt;-JDdump true&lt;/code&gt; so &lt;code&gt;asmjit&lt;/code&gt; will dump all ARM32 assembly for each module! This is incredibly useful if monitored while executing with a debugger, as we can see the assembler being printed line by line by our code.&lt;/p&gt;
    &lt;code&gt;~/arm32-jit$ cat hello.asm 
L6:
.byte 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_1:
# i_func_info_IaaI
# hello:start/2
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x0B, 0xA4, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00
# aligned_label_Lt
start/2:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L9
    bl L11
L9:
# i_test_yield
    adr r2, start/2
    subs r9, r9, 1
    b.le L13
# i_move_sd
    ldr r12, [L14]
    str r12, [r4, 68]
# i_move_sd
    movw r12, 687
    str r12, [r4, 64]
# line_I
# allocate_tt
# call_light_bif_be
L15:
    ldr r3, [L16]
    movw r1, 10188
    movt r1, 16432
    adr r2, L15
# BIF: erlang:halt/2
    sub r12, r7, 4
    cmp r10, r12
    b.ls L17
    udf 48879
L17:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L18
    udf 57005
L18:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_3:
# i_func_info_IaaI
# hello:module_info/0
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x4B, 0x6B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
# aligned_label_Lt
module_info/0:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L23
    bl L11
L23:
# i_test_yield
    adr r2, module_info/0
    subs r9, r9, 1
    b.le L13
# i_move_sd
    movw r12, 20235
    str r12, [r4, 64]
# allocate_tt
# call_light_bif_be
L24:
    ldr r3, [L25]
    movw r1, 4772
    movt r1, 16425
    adr r2, L24
# BIF: erlang:get_module_info/1
    sub r12, r7, 4
    cmp r10, r12
    b.ls L26
    udf 48879
L26:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L27
    udf 57005
L27:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_5:
# i_func_info_IaaI
# hello:module_info/1
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x4B, 0x6B, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00
# aligned_label_Lt
module_info/1:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L28
    bl L11
L28:
# i_test_yield
    adr r2, module_info/1
    subs r9, r9, 1
    b.le L13
# i_move_sd
    ldr r12, [r4, 64]
    str r12, [r4, 68]
# i_move_sd
    movw r12, 20235
    str r12, [r4, 64]
# allocate_tt
# call_light_bif_be
L29:
    ldr r3, [L30]
    movw r1, 4868
    movt r1, 16425
    adr r2, L29
# BIF: erlang:get_module_info/2
    sub r12, r7, 4
    cmp r10, r12
    b.ls L31
    udf 48879
L31:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L32
    udf 57005
L32:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# int_code_end
L33:
    movw r0, 18576
    movt r0, 16480
    blx L22
L13:
L12:
    movw r12, 1968
    movt r12, 14656
    blx r12
L22:
L21:
    movw r12, 29192
    movt r12, 16399
    blx r12
L11:
L10:
    movw r12, 1752
    movt r12, 14656
    blx r12
L20:
L19:
    movw r12, 680
    movt r12, 14656
    blx r12
L8:
L7:
    movw r12, 1824
    movt r12, 14656
    blx r12
# Begin stub section
L14:
.xword 0x000000007FFFFFFF
L16:
.xword 0x000000007FFFFFFF
L25:
.xword 0x000000007FFFFFFF
L30:
.xword 0x000000007FFFFFFF
# End stub section
L34:
.section .rodata {#1}
md5:
.byte 0x6D, 0xC4, 0x1E, 0xF1, 0x13, 0x1E, 0xBF, 0xF2, 0x4B, 0xF5, 0xC0, 0x41, 0x57, 0x86, 0xDF, 0xD5
.section .text {#0}
; CODE_SIZE: 632&lt;/code&gt;
    &lt;p&gt;Bear in mind, this assembler is not what hello should look like. We are missing a lot of things.&lt;/p&gt;
    &lt;p&gt;You can spot many sequences like:&lt;/p&gt;
    &lt;code&gt;    movw r0, 64676
    movt r0, 16480
    blx L22 # &amp;lt;---- branch to NYI&lt;/code&gt;
    &lt;p&gt;This is a call to &lt;code&gt;nyi&lt;/code&gt; (Not Yet Implemented) function and the argument loaded to R0 is the pointer to a string that contains the name of the BEAM instruction that should have been emitted instead. You can spot many of these since we are only emitting the code to reach halt. Everything after that is not important now as halt will never return!&lt;/p&gt;
    &lt;p&gt;There are many more comments we could make around all the details in this assembler dump, but let's move on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Jumping into Jitted code!&lt;/head&gt;
    &lt;p&gt;Later in the BEAM initialization the first Erlang process will be allocated and started.&lt;/p&gt;
    &lt;p&gt;We swap the module and function with hello in erts/emulator/beam/erl_init.c&lt;/p&gt;
    &lt;code&gt;    erl_spawn_system_process(&amp;amp;parent, am_hello, am_start, args, &amp;amp;so);&lt;/code&gt;
    &lt;p&gt;One BEAM scheduler thread will jump to the &lt;code&gt;process_main&lt;/code&gt; function. You can find it here in the source code. This is emitted by our JIT and is the first emitted code that will run.&lt;/p&gt;
    &lt;p&gt;Here we need to handle the Erlang processes scheduling by calling BEAM routines that implement the algorithms of Erlang concurrency, like &lt;code&gt;erts_schedule&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;erts_schedule&lt;/code&gt; will return the pointer to the &lt;code&gt;Process&lt;/code&gt; C structure that holds all information about the process that is going to execute. We then load all necessary data inside registers and then we branch to the exact point where the program execution stopped.&lt;/p&gt;
    &lt;head rend="h3"&gt;The first Erlang function call&lt;/head&gt;
    &lt;p&gt;In this case we are calling &lt;code&gt;hello:start/2&lt;/code&gt; so the first instruction to execute is &lt;code&gt;apply_only&lt;/code&gt; that does a few things but ends up calling the C &lt;code&gt;apply&lt;/code&gt; routine.&lt;/p&gt;
    &lt;p&gt;The routine processes the Module-Function-Arity information to get the address where the function code resides in memory.&lt;/p&gt;
    &lt;p&gt;What follows is the Erlang function prologue. You can see it in the assembler code section above. For example, all functions have these instructions in their prologue:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;i_breakpoint_trampoline: handle breakpoints for the &lt;code&gt;debugger&lt;/code&gt;app&lt;/item&gt;
      &lt;item&gt;i_test_yield: checks if the function should yield and go back to the scheduler&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We have minimal or partial implementations of these since we do not really need them. We have to emit them though, as the C++ generated loader functions from the BEAM are expanding the Erlang function call Operation into a more specific and complex function prologue sequence.&lt;/p&gt;
    &lt;p&gt;After that, we added support for the &lt;code&gt;call_light_bif&lt;/code&gt; operation that precedes the call to the halt_2 BIF routine. This implementation is also minimal.&lt;/p&gt;
    &lt;p&gt;Question for later: did you notice that we put a &lt;code&gt;42&lt;/code&gt; as a number in the code? Numeric constants are printed as decimals in the dump, but we cannot spot any 42!?&lt;/p&gt;
    &lt;p&gt;After the call, we see two other operations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;dealloc&lt;/item&gt;
      &lt;item&gt;return&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are just calls to NYI as we will never reach this code! So for now, we can skip them...&lt;/p&gt;
    &lt;head rend="h3"&gt;Let's roll the JIT!&lt;/head&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/arm32-jit/otp/RELEASE -progname erl -home /home
    ~/arm32-jit$&lt;/code&gt;
    &lt;p&gt;Impressive, the program returns immediately without even saying "Hi" ... and without Segmentation Fault!!&lt;/p&gt;
    &lt;p&gt;But let's check the program return code!&lt;/p&gt;
    &lt;code&gt;~/arm32-jit$ echo $?
42
&lt;/code&gt;
    &lt;p&gt;We can safely say that number is not there by accident! This is a great achievement as from now on we will be able to incrementally add Erlang instructions.&lt;/p&gt;
    &lt;p&gt;Every Erlang line we add will trigger new Opcodes. By emitting them and running the code we will have immediate feedback on everything.&lt;/p&gt;
    &lt;p&gt;The next goal now is to complete the &lt;code&gt;hello&lt;/code&gt; module to host all possible beam instructions!&lt;/p&gt;
    &lt;head rend="h4"&gt;Hey where is 42???&lt;/head&gt;
    &lt;p&gt;One interesting thing I spotted looking at the assembly: You cannot find the number &lt;code&gt;42&lt;/code&gt; in there. Or actually, you can, it is just hidden in plain sight. To understand you need to know how we are using ARM32 registers.&lt;/p&gt;
    &lt;p&gt;In particular the register &lt;code&gt;r4&lt;/code&gt;, a callee-saved register. We are using it to store the pointer to the &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt; struct. The &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt; contains the X register array. When a function is called, X registers are used to store the arguments of the call.&lt;/p&gt;
    &lt;p&gt;This becomes more obvious if we compare the Erlang assembly to the Arm32 assembly.&lt;/p&gt;
    &lt;code&gt;# i_move_sd                       &amp;lt;---- {move,{literal,[{flush,false}]},{x,1}}. % List at X[1]
    ldr r12, [L14]
    str r12, [r4, 68]
# i_move_sd                       &amp;lt;---- {move,{integer,42},{x,0}}. % 42 at X[0]
    movw r12, 687 
    str r12, [r4, 64]
# line_I
# allocate_tt
# call_light_bif_be
L15:
    ldr r3, [L16]
    movw r1, 10188
    movt r1, 16432
    adr r2, L15
# BIF: erlang:halt/2
# ...&lt;/code&gt;
    &lt;p&gt;42 is stored at &lt;code&gt;r4&lt;/code&gt;+64.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;r4: pointer to the &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt;struct&lt;/item&gt;
      &lt;item&gt;64: base offset from the beginning of the struct to the beginning of the &lt;code&gt;x_reg_array&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The list is stored at &lt;code&gt;r4&lt;/code&gt;+68.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;68: is the base offset + the size of one &lt;code&gt;Eterm&lt;/code&gt;(4 bytes on ARM32)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But why in assembly do we see 687 and not 42?&lt;/p&gt;
    &lt;p&gt;Converting both numbers to hex we get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;42 -&amp;gt; 2A&lt;/item&gt;
      &lt;item&gt;687 -&amp;gt; 2AF !!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yep, this is an example of a Tagged Value. If we consult the BEAM book we can learn about the Tagging Scheme:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;00 11 Pid&lt;/item&gt;
      &lt;item&gt;01 11 Port&lt;/item&gt;
      &lt;item&gt;10 11 Immediate 2&lt;/item&gt;
      &lt;item&gt;11 11 Small integer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;42 is tagged with &lt;code&gt;1111&lt;/code&gt; at the low end. So the BEAM can quickly recognize during a pattern match that this Erlang Term is a Small Integer!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.grisp.org/blog/posts/2025-10-07-jit-arm32.3"/><published>2025-10-07T13:00:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502784</id><title>Tcl-Lang Showcase</title><updated>2025-10-07T19:32:08.434626+00:00</updated><content>&lt;doc fingerprint="598b77957e6a418d"&gt;
  &lt;main&gt;&lt;p&gt;Canvas3d&lt;/p&gt;Canvas3d wiki page&lt;p&gt;HP-15 Simulation&lt;/p&gt;HP-15 Simulation wiki page&lt;p&gt;By clicking on the image, an interactive demonstration of the Tcl/Tk application is launched using CloudTk. Over 100 Tcl/Tk applications listed from this wiki are demonstrated here . To view the Tcl/Tk Widget Demonstration, go to the "Playground" from the menu above and then select "Demos" in the "Tcl-Playground" - Console menu.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wiki.tcl-lang.org/page/Showcase"/><published>2025-10-07T13:25:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45503726</id><title>No account? No Windows 11, Microsoft says as another loophole snaps shut</title><updated>2025-10-07T19:32:08.251536+00:00</updated><content>&lt;doc fingerprint="3f986a36e76660d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;No account? No Windows 11, Microsoft says as another loophole snaps shut&lt;/head&gt;
    &lt;head rend="h2"&gt;Workaround sent to the big OOBE in the sky with latest Insider builds&lt;/head&gt;
    &lt;p&gt;Microsoft is closing a popular loophole that allowed users to install Windows 11 without a Microsoft account.&lt;/p&gt;
    &lt;p&gt;The change has appeared in recent Insider builds of Windows 11, indicating it is likely to be included in the production version soon.&lt;/p&gt;
    &lt;p&gt;Microsoft refers to these loopholes as "known mechanisms" and is talking about local commands in this instance. You can learn all about these in our piece for getting Windows 11 installed with a local account, but suffice to say &lt;code&gt;start ms-cxh:localonly&lt;/code&gt; is no more.&lt;/p&gt;
    &lt;p&gt;"While these mechanisms were often used to bypass Microsoft account setup, they also inadvertently skip critical setup screens, potentially causing users to exit OOBE with a device that is not fully configured for use," Microsoft said.&lt;/p&gt;
    &lt;p&gt;"Users will need to complete OOBE with internet and a Microsoft account, to ensure [the] device is set up correctly."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 10 refuses to go gentle into that good night&lt;/item&gt;
      &lt;item&gt;Hundreds of orgs urge Microsoft: don't kill off free Windows 10 updates&lt;/item&gt;
      &lt;item&gt;Windows 11 25H2 is mostly 24H2 with bits bolted on or ripped out&lt;/item&gt;
      &lt;item&gt;Healthcare lags in Windows 11 upgrades – and lives may depend on it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As far as Redmond is concerned, this is all for the user's own good. It is also important to note that managed devices are not directly affected, just hardware that users want to get running with Windows 11 without having to deal with a Microsoft Account during setup.&lt;/p&gt;
    &lt;p&gt;The change is part of Microsoft's ongoing game of Whac-A-Mole with users trying to find ways of avoiding its online services. In March, it removed the &lt;code&gt;bypassnro.cmd&lt;/code&gt; script that allowed users to get through the Windows 11 setup without needing an internet connection. That time, Microsoft said the change was to "enhance security and user experience of Windows 11."&lt;/p&gt;
    &lt;p&gt;There remain a number of ways to avoid the Microsoft account requirement during setup, including setting up an unattended installation, but these are more complicated. It is also clear that Microsoft is determined to continue closing loopholes where it can.&lt;/p&gt;
    &lt;p&gt;It is getting increasingly difficult to use Windows 11 on an unmanaged device without a Microsoft account. Users who don't want to sign up should perhaps consider whether it's time to look at an alternative operating system instead. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/07/windows_11_local_account_loophole/"/><published>2025-10-07T14:45:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45504127</id><title>Show HN: MARS – Personal AI robot for builders (&lt; $2k)</title><updated>2025-10-07T19:32:07.831415+00:00</updated><content>&lt;doc fingerprint="f1df9a7b68a781cd"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey, we’re Axel and Vignesh, cofounders of Innate (&lt;/p&gt;https://www.innate.bot/&lt;p&gt;). We just launched MARS, a general-purpose robot with an open onboard agentic OS built on top of ROS2.&lt;/p&gt;&lt;p&gt;Overview: https://youtu.be/GEOMYDXv6pE&lt;/p&gt;&lt;p&gt;Control demo: https://youtu.be/_Cw5fGa8i3s&lt;/p&gt;&lt;p&gt;Videos of autonomous use-cases: https://docs.innate.bot/welcome/mars-example-use-cases&lt;/p&gt;&lt;p&gt;Quickstart: https://docs.innate.bot/welcome/mars-quick-start.&lt;/p&gt;&lt;p&gt;Our last thread: https://news.ycombinator.com/item?id=42451707&lt;/p&gt;&lt;p&gt;When we started we felt there is currently no good affordable general-purpose that anyone can build on. There’s no lack of demand: hugging face’s SO-100 and LeKiwi are pretty clear successes already; but the hardware is unreliable, the software experience is barebone and keeps changing, and you often need to buy hidden extras to make them work (starting with a computer with a good gpu). The Turtlebots were good, but are getting outdated.&lt;/p&gt;&lt;p&gt;The open-source hobbyist movement lacks really good platforms to build on, and we wanted something robust and accessible. MARS is our attempt at making a first intuitive AI robot for everyone.&lt;/p&gt;&lt;p&gt;What it is:&lt;/p&gt;&lt;p&gt;- It comes assembled and calibrated&lt;/p&gt;&lt;p&gt;- Has onboard compute with a jetson orin nano 8gb&lt;/p&gt;&lt;p&gt;- a 5DoF arm with a wrist camera&lt;/p&gt;&lt;p&gt;- Sensors: RGBD wide-angle cam, 2D LiDAR, speakers&lt;/p&gt;&lt;p&gt;- Control via a dedicated app and a leader arm that plugs in iPhone and Android&lt;/p&gt;&lt;p&gt;- 2 additional USB ports + GPIO pins for extra sensors or effectors.&lt;/p&gt;&lt;p&gt;- And our novel SDK called BASIC that allows to run it like an AI agent with VLAs.&lt;/p&gt;&lt;p&gt;It boots in a minute, can be controlled via phone, programmable in depth with a PC, and the onboard agent lets it see, talk, plan, and act in real-time.&lt;/p&gt;&lt;p&gt;Our SDK BASIC allows to create “behaviors” (our name for programs) ranging from a simple hello world to a very complex long-horizon task involving reasoning, planning, navigation and manipulation. You can create skills that behaviors can run autonomously by training the arm or writing code tools, like for an AI agent.&lt;/p&gt;&lt;p&gt;You can also call the ROS2 topics to control the robot at a low-level. And anything created on top of this SDK can be easily shared with anyone else by just sharing the files.&lt;/p&gt;&lt;p&gt;This is intended for hobbyist builders and education, and we would love to have your feedback!&lt;/p&gt;&lt;p&gt;p.s. If you want to try it, there’s a temporary code HACKERNEWS-INNATE-MARS that lowers the price to $1,799.&lt;/p&gt;&lt;p&gt;p.p.s The hardware and software will be open-sourced too, if some of you want to contribute or help us prepare it properly feel free to join our discord at https://discord.gg/YvqQbGKH&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45504127"/><published>2025-10-07T15:11:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45504388</id><title>Launch HN: LlamaFarm (YC W22) – Open-source framework for distributed AI</title><updated>2025-10-07T19:32:07.236593+00:00</updated><content>&lt;doc fingerprint="ed1fa4511bbd11f2"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Build powerful AI locally, extend anywhere.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;LlamaFarm is an open-source framework for building retrieval-augmented and agentic AI applications. It ships with opinionated defaults (Ollama for local models, Chroma for vector storage) while staying 100% extendable—swap in vLLM, remote OpenAI-compatible hosts, new parsers, or custom stores without rewriting your app.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local-first developer experience with a single CLI (&lt;code&gt;lf&lt;/code&gt;) that manages projects, datasets, and chat sessions.&lt;/item&gt;
      &lt;item&gt;Production-ready architecture that mirrors server endpoints and enforces schema-based configuration.&lt;/item&gt;
      &lt;item&gt;Composable RAG pipelines you can tailor through YAML, not bespoke code.&lt;/item&gt;
      &lt;item&gt;Extendable everything: runtimes, embedders, databases, extractors, and CLI tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;📺 Video demo (90 seconds): https://youtu.be/W7MHGyN0MdQ&lt;/p&gt;
    &lt;p&gt;Prerequisites:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install the CLI&lt;/p&gt;
        &lt;p&gt;macOS / Linux&lt;/p&gt;
        &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh | bash&lt;/code&gt;
        &lt;p&gt;Windows (via winget)&lt;/p&gt;
        &lt;code&gt;winget install LlamaFarm.CLI&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Adjust Ollama context window&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Open the Ollama app, go to Settings → Advanced, and set the context window to match production (e.g., 100K tokens).&lt;/item&gt;
          &lt;item&gt;Larger context windows improve RAG answers when long documents are ingested.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create and run a project&lt;/p&gt;
        &lt;quote&gt;lf init my-project # Generates llamafarm.yaml using the server template lf start # Spins up Docker services &amp;amp; opens the dev chat UI&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Start an interactive project chat or send a one-off message&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Interactive project chat (auto-detects namespace/project from llamafarm.yaml)
lf chat

# One-off message
lf chat "Hello, LlamaFarm!"&lt;/code&gt;
    &lt;p&gt;Need the full walkthrough with dataset ingestion and troubleshooting tips? Jump to the Quickstart guide.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Prefer building from source? Clone the repo and follow the steps in Development &amp;amp; Testing.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Run services manually (without Docker auto-start):&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/llama-farm/llamafarm.git
cd llamafarm

# Install Nx globally and bootstrap the workspace
npm install -g nx
nx init --useDotNxInstallation --interactive=false

# Option 1: start both server and RAG worker with one command
nx dev

# Option 2: start services in separate terminals
# Terminal 1
nx start rag
# Terminal 2
nx start server&lt;/code&gt;
    &lt;p&gt;Open another terminal to run &lt;code&gt;lf&lt;/code&gt; commands (installed or built from source). This is equivalent to what &lt;code&gt;lf start&lt;/code&gt; orchestrates automatically.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Own your stack – Run small local models today and swap to hosted vLLM, Together, or custom APIs tomorrow by changing &lt;code&gt;llamafarm.yaml&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Battle-tested RAG – Configure parsers, extractors, embedding strategies, and databases without touching orchestration code.&lt;/item&gt;
      &lt;item&gt;Config over code – Every project is defined by YAML schemas that are validated at runtime and easy to version control.&lt;/item&gt;
      &lt;item&gt;Friendly CLI – &lt;code&gt;lf&lt;/code&gt;handles project bootstrapping, dataset lifecycle, RAG queries, and non-interactive chats.&lt;/item&gt;
      &lt;item&gt;Built to extend – Add a new provider or vector store by registering a backend and regenerating schema types.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Task&lt;/cell&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Initialize a project&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf init my-project&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Creates &lt;code&gt;llamafarm.yaml&lt;/code&gt; from server template.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Start dev stack + chat TUI&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf start&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Spins up server, rag worker, monitors Ollama/vLLM.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Interactive project chat&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Opens TUI using project from &lt;code&gt;llamafarm.yaml&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Send single prompt&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat "Explain retrieval augmented generation"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Uses RAG by default; add &lt;code&gt;--no-rag&lt;/code&gt; for pure LLM.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Preview REST call&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat --curl "What models are configured?"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Prints sanitized &lt;code&gt;curl&lt;/code&gt; command.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Create dataset&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets create -s pdf_ingest -b main_db research-notes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Validates strategy/database against project config.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Upload files&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets upload research-notes ./docs/*.pdf&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Supports globs and directories.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Process dataset&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets process research-notes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Streams heartbeat dots during long processing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Semantic query&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf rag query --database main_db "What did the 2024 FDA letters require?"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Use &lt;code&gt;--filter&lt;/code&gt;, &lt;code&gt;--include-metadata&lt;/code&gt;, etc.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;See the CLI reference for full command details and troubleshooting advice.&lt;/p&gt;
    &lt;p&gt;LlamaFarm provides a comprehensive REST API (compatible with OpenAI's format) for integrating with your applications. The API runs at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Chat Completions (OpenAI-compatible)&lt;/p&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "What are the FDA requirements?"}
    ],
    "stream": false,
    "rag_enabled": true,
    "database": "main_db"
  }'&lt;/code&gt;
    &lt;p&gt;RAG Query&lt;/p&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "clinical trial requirements",
    "database": "main_db",
    "top_k": 5
  }'&lt;/code&gt;
    &lt;p&gt;Dataset Management&lt;/p&gt;
    &lt;code&gt;# Upload file
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/data \
  -F "file=@document.pdf"

# Process dataset
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/process&lt;/code&gt;
    &lt;p&gt;Check your &lt;code&gt;llamafarm.yaml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;name: my-project        # Your project name
namespace: my-org       # Your namespace&lt;/code&gt;
    &lt;p&gt;Or inspect the file system: &lt;code&gt;~/.llamafarm/projects/{namespace}/{project}/&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;See the complete API Reference for all endpoints, request/response formats, Python/TypeScript clients, and examples.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;llamafarm.yaml&lt;/code&gt; is the source of truth for each project. The schema enforces required fields and documents every extension point.&lt;/p&gt;
    &lt;code&gt;version: v1
name: fda-assistant
namespace: default

runtime:
  provider: openai                   # "openai" for any OpenAI-compatible host, "ollama" for local Ollama
  model: qwen2.5:7b
  base_url: http://localhost:8000/v1 # Point to vLLM, Together, etc.
  api_key: sk-local-placeholder
  instructor_mode: tools             # Optional: json, md_json, tools, etc.

prompts:
  - role: system
    content: &amp;gt;-
      You are an FDA specialist. Answer using short paragraphs and cite document titles when available.

rag:
  databases:
    - name: main_db
      type: ChromaStore
      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: filtered_search
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text:latest
      retrieval_strategies:
        - name: filtered_search
          type: MetadataFilteredStrategy
          config:
            top_k: 5
  data_processing_strategies:
    - name: pdf_ingest
      parsers:
        - type: PDFParser_LlamaIndex
          config:
            chunk_size: 1500
            chunk_overlap: 200
      extractors:
        - type: HeadingExtractor
        - type: ContentStatisticsExtractor

datasets:
  - name: research-notes
    data_processing_strategy: pdf_ingest
    database: main_db&lt;/code&gt;
    &lt;p&gt;Configuration reference: Configuration Guide • Extending LlamaFarm&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swap runtimes by pointing to any OpenAI-compatible endpoint (vLLM, Mistral, Anyscale). Update &lt;code&gt;runtime.provider&lt;/code&gt;,&lt;code&gt;base_url&lt;/code&gt;, and&lt;code&gt;api_key&lt;/code&gt;; regenerate schema types if you add a new provider enum.&lt;/item&gt;
      &lt;item&gt;Bring your own vector store by implementing a store backend, adding it to &lt;code&gt;rag/schema.yaml&lt;/code&gt;, and updating the server service registry.&lt;/item&gt;
      &lt;item&gt;Add parsers/extractors to support new file formats or metadata pipelines. Register implementations and extend the schema definitions.&lt;/item&gt;
      &lt;item&gt;Extend the CLI with new Cobra commands under &lt;code&gt;cli/cmd&lt;/code&gt;; the docs include guidance on adding dataset utilities or project tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check the Extending guide for step-by-step instructions.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
        &lt;cell role="head"&gt;What it Shows&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FDA Letters Assistant&lt;/cell&gt;
        &lt;cell&gt;Multi-document PDF ingestion, RAG queries, reference-style prompts&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;examples/fda_rag/&lt;/code&gt; &amp;amp; Docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Raleigh UDO Planning Helper&lt;/cell&gt;
        &lt;cell&gt;Large ordinance ingestion, long-running processing tips, geospatial queries&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;examples/gov_rag/&lt;/code&gt; &amp;amp; Docs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;lf datasets&lt;/code&gt; and &lt;code&gt;lf rag query&lt;/code&gt; commands from each example folder to reproduce the flows demonstrated in the docs.&lt;/p&gt;
    &lt;code&gt;# Python server + RAG tests
cd server
uv sync
uv run --group test python -m pytest

# CLI tests
cd ../cli
go test ./...

# RAG tooling smoke tests
cd ../rag
uv sync
uv run python cli.py test

# Docs build (ensures navigation/link integrity)
cd ..
nx build docs&lt;/code&gt;
    &lt;p&gt;Linting: &lt;code&gt;uv run ruff check --fix .&lt;/code&gt; (Python), &lt;code&gt;go fmt ./...&lt;/code&gt; and &lt;code&gt;go vet ./...&lt;/code&gt; (Go).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord – chat with the team, share feedback, find collaborators.&lt;/item&gt;
      &lt;item&gt;GitHub Issues – bug reports and feature requests.&lt;/item&gt;
      &lt;item&gt;Discussions – ideas, RFCs, roadmap proposals.&lt;/item&gt;
      &lt;item&gt;Contributing Guide – code style, testing expectations, doc updates, schema regeneration steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Want to add a new provider, parser, or example? Start a discussion or open a draft PR—we love extensions!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Licensed under the Apache 2.0 License.&lt;/item&gt;
      &lt;item&gt;Built by the LlamaFarm community and inspired by the broader open-source AI ecosystem. See CREDITS for detailed acknowledgments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build locally. Deploy anywhere. Own your AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/llama-farm/llamafarm"/><published>2025-10-07T15:30:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45504470</id><title>IKEA Catalogs 1951-2021</title><updated>2025-10-07T19:32:05.937105+00:00</updated><content>&lt;doc fingerprint="71c3125f48ed4455"&gt;
  &lt;main&gt;
    &lt;p&gt;Good question! We know that a lot of people are curious about what the IKEA catalogue has looked like through the ages. The catalogue has always reflected the age and its views on interior design and everyday living, especially in Sweden, but in recent decades also internationally. The catalogue was in print for 70 years, and by digitising all the catalogues we could make them available to everybody. Making the story of IKEA available to as many people as possible is our main task at IKEA Museum. So we hope that the catalogues will bring some joy and nostalgia, and maybe even a few surprises.&lt;/p&gt;
    &lt;head rend="h1"&gt;IKEA catalogue&lt;/head&gt;
    &lt;p&gt;For over 70 years, the IKEA catalogue was produced in Ãlmhult, constantly growing in number, scope and distribution. From the 1950s when Ingvar Kamprad wrote most of the texts himself, via the poppy, somewhat radical 1970s and all the way into the scaled-down 2000s â the IKEA catalogue always captured the spirit of the time. The 2021 IKEA catalogue was the very last one printed on paper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1950s&lt;/item&gt;
      &lt;item&gt;1960s&lt;/item&gt;
      &lt;item&gt;1970s&lt;/item&gt;
      &lt;item&gt;1980s&lt;/item&gt;
      &lt;item&gt;1990s&lt;/item&gt;
      &lt;item&gt;2000s&lt;/item&gt;
      &lt;item&gt;2010s&lt;/item&gt;
      &lt;item&gt;2020s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just like the perception of the home, the catalogue has changed dramatically since 1951, when it was first published. Look in the older catalogues and youâll be amazed at what you find. In fact, youâll probably even have a giggle or two. In the 1950s and 1960s, there are rarely any people in the pictures, and never any children. But in the 1970s there are children playing all over the home, you can see adults smoking and even the occasional political poster on the wall. Browse on to the 1980s IKEA catalogues and the trends have changed again, with shiny fabrics and other fancy materials. In the 1990s homes become more scaled-down and clearly inspired by a Scandinavian tradition. In this way, the IKEA catalogues are a kind of time capsule for you to travel in. And who knows? When we look back at the most recent catalogues in 10 or 20 yearsâ time, weâll probably shake our heads and give a sigh.&lt;/p&gt;
    &lt;p&gt;IKEA Museum decided to start with the Swedish catalogue as it has been around the longest. In the future, we hope to be able to digitise catalogues from more countries in more languages.&lt;/p&gt;
    &lt;p&gt;No. The IKEA catalogue has always only shown a selection of whatâs available in the stores. The catalogues from the 1970s and onwards show around 30â50 per cent of the entire range. The products that are not featured are generally smaller ones in textiles, decorations and lighting. Temporary collections are rarely included either. But the farther back you go, the higher a percentage of the range can be found in the catalogue.&lt;/p&gt;
    &lt;p&gt;Yes, but the older a product is, the harder it may be to find information about it. If you have a specific question about a product, weâre happy to help you out if we can. But 70 years is a long time, so we canât promise anything. While youâre waiting for our response you can always browse through the catalogues â the product texts that are there are quite detailed. You can search in the catalogues by product name and product type. There are also various stories about different products on our site, and more are constantly being added.&lt;lb/&gt; Browse through stories about IKEA products from 7 decades. &lt;/p&gt;
    &lt;p&gt;IKEA was founded in the 1940s, so why are you showing no catalogues from before 1951?&lt;lb/&gt; The first catalogue did not come out until 1951. Before that, IKEA was a mail order company that didnât sell furniture, but pens, clocks, electric razors, wallets and bags. At that time, the range was only presented in a small mail order brochure called ikÃ©a-nytt (literally ikÃ©a news). Sometimes it was distributed as a supplement in farming paper Jordbrukarnas FÃ¶reningsblad, which reached hundreds of thousands of people in the Swedish countryside. From autumn 1948 Ingvar Kamprad started including furniture in the range, and things quickly grew from there. In the 1950 ikÃ©a-nytt, as many as six of the 18 pages featured furniture. And when you look at the 1951 catalogue, youâll see that there are no more pens and wallets. Ingvar Kamprad was now truly focusing on home furnishing, and shelving the rest.&lt;lb/&gt; Browse through all issues of ikÃ©a-nytt.&lt;/p&gt;
    &lt;p&gt;Not really. We do have a few copies of each yearâs IKEA catalogue in our archives, which weâre saving for posterity. They should be handled as little as possible to keep them in good condition, so weâve made the catalogues available digitally, both online and on monitors at IKEA Museum. You can browse through those as much as you like!&lt;/p&gt;
    &lt;p&gt;Yes you can. The easiest way to share the catalogues is to click on the arrow at the bottom left corner for each catalogue, or in the left-hand menu once youâve started browsing through. This will copy a link which you can share on a website or social media. If you would like to download and publish on your own digital platform, you can share a maximum of three complete digital catalogues. Donât forget to state the copyright details, “Â© Inter IKEA Systems B.V.”, the catalogue year, and the link /en/explore/ikea-catalogue/ so that anyone interested can find out more. You may not publish the digital catalogues for commercial purposes.&lt;/p&gt;
    &lt;p&gt;Absolutely! You can share up to 30 images from the catalogues on your own digital platform, such as a blog, on Instagram or similar (as long as itâs not for commercial purposes). Donât forget to state the copyright details, “Â© Inter IKEA Systems B.V.”, the catalogue year, and the link /en/explore/ikea-catalogue/ so that anyone interested can find out more.&lt;/p&gt;
    &lt;p&gt;Yes! You can find all press material, including images, information about current exhibitions and much more, in the IKEA Museum press room.&lt;/p&gt;
    &lt;p&gt;At the moment we have a good amount of catalogues in all languages at the museum, and do not need any more. Having said that, please contact us anyway if youâve been collecting catalogues for several decades, or if you have any other material you think might be of interest to IKEA Museum.&lt;/p&gt;
    &lt;p&gt;Unfortunately not. We sometimes wish we did, as we handle quite a lot of old products that may need putting together and taking apart.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ikeamuseum.com/en/explore/ikea-catalogue/"/><published>2025-10-07T15:35:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45504973</id><title>Show HN: Timelinize – Privately organize your own data from everywhere, locally</title><updated>2025-10-07T19:32:05.333253+00:00</updated><content>&lt;doc fingerprint="e64842e6eda1dcc1"&gt;
  &lt;main&gt;
    &lt;p&gt;Timelinize ("time-lynn-eyes") is an open source personal archival suite, designed for modern family history. It organizes all your data onto a single, unified timeline on your own computer.&lt;/p&gt;
    &lt;p&gt;Photos, videos, text messages, locations, chats, social media, and more. Timelinize unifies it all.&lt;/p&gt;
    &lt;p&gt;By adding all your data, Timelinize documents your family's life with more detail and privacy, and gives you a more complete view of your story, than standard photo library and journaling apps.&lt;/p&gt;
    &lt;p&gt;Most apps store your data "in the cloud" and out of your control. What if you lost access to your Google/Apple/Facebook accounts, or your phone? By bringing that data home to your own computer, Timelinize preserves a richer story than any one app or service can do alone.&lt;/p&gt;
    &lt;p&gt;Timelinize isn't a replacement for the apps and services you already use, so you don't need to disrupt your way of life. Instead, it "sits behind" what you already use to become the permanent private archive of your working copy from:&lt;/p&gt;
    &lt;p&gt;With several projections for your data, it's easy to keep moments alive that would otherwise be forgotten, rotting on a hard drive in your closet... or in a bigcorp's cloud.&lt;/p&gt;
    &lt;p&gt;The timeline view semantically groups all your data into a single linear layout. Easily see what occurred on a specific day in the order it happened.&lt;/p&gt;
    &lt;p&gt;Visualize your data on a huge, beautiful map of the world that plots points when and where they happened, even for data that doesn't have coordinates (like text messages and emails).&lt;/p&gt;
    &lt;p&gt;Follow connections with people across all kinds of chats and messages. Combine conversations with people across platforms into one view.&lt;/p&gt;
    &lt;p&gt;Browse through a rich display of photos and videos from photo libraries, messages sent and received, and other sources.&lt;/p&gt;
    &lt;p&gt;Add millions of data points to your timeline in a matter of minutes. You get full control over background jobs like imports, thumbnails, and embeddings.&lt;/p&gt;
    &lt;p&gt;Timelinize supports playing "live photos" (or "motion photos") for photos taken on Apple, Google, and Samsung devices.&lt;/p&gt;
    &lt;p&gt;Timelinize specializes in combining data from multiple sets and sources. It can identify people and other entities across data sources by their attributes. If a person or contact appears in multiple data sets, it will automatically merge them if possible. If not, you can easily merge entities with the click of a button.&lt;/p&gt;
    &lt;p&gt;Because Timelinize is entity-aware, it can project data points onto a map even without coordinate data. If a geolocated point is known for an entity around the same time of others of that entity's data points, it will appear on the map.&lt;/p&gt;
    &lt;p&gt;Customize the map to change its theme, layers, and even make it 3D.&lt;/p&gt;
    &lt;p&gt;The heatmap shows where your data is concentrated. It smoothly blends as you zoom in and out.&lt;/p&gt;
    &lt;p&gt;Customize what defines a duplicate item, and how to handle that, with a fine degree of control—perfect for merging separate, disparate data sets.&lt;/p&gt;
    &lt;p&gt;An implicit conversation is discovered when a data source links items and entities with a "sent to" relation. You can easily view conversations between entities across modalities in a single scroll: chats, emails, messages, texts, and more.&lt;/p&gt;
    &lt;p&gt;Since I need this to function well for my own family, I have tried to give special attention to less-visible aspects of this application, such as:&lt;/p&gt;
    &lt;p&gt;Timelinize deduplicates, denoises, clusters, and simplifies location data for optimal preservation, with an algorithm that subjectively performs better than Google Maps Timeline.&lt;/p&gt;
    &lt;p&gt;For nerds like me: you can use Timelinize through its CLI, which mirrors all the functions of the HTTP API used by the frontend.&lt;/p&gt;
    &lt;p&gt;Search for pictures and messages by describing them, or find similar items to what you're viewing.&lt;/p&gt;
    &lt;p&gt;All items are stored verbatim, then thumbnails are generated for all images and video media, which are stored separately. Your original data is not modified.&lt;/p&gt;
    &lt;p&gt;The database schema has been meticulously designed and refined to be as adaptable as possible.&lt;/p&gt;
    &lt;p&gt;Timelinize will continue to develop and evolve. In the future, I anticipate the following capabilities:&lt;/p&gt;
    &lt;p&gt;Annotate your timeline, write rich stories with live embeddings from your timeline data, or make physical media like photo books (but with more than just photos!).&lt;/p&gt;
    &lt;p&gt;Add context to your timeline with additional public timelines which have weather, local/regional news, and global events.&lt;/p&gt;
    &lt;p&gt;Securely and privately share parts of your timeline with trusted friends and family members, directly from your computer to theirs.&lt;/p&gt;
    &lt;p&gt;Right now, Timelinize sits "behind" the apps and platforms you already use. But in the future, you could sync data directly to your timeline as it is originated.&lt;/p&gt;
    &lt;p&gt;Collect your data from various sources. Import it with a few clicks. Within minutes, explore millions of your data points in several intuitive ways.&lt;/p&gt;
    &lt;p&gt;Imported data is copied into your timeline folder, ensuring long-term stability and integrity. Timelines are portable—you can copy them or move them to other devices and computers.&lt;/p&gt;
    &lt;p&gt;Your timeline is simply a folder on disk containing a SQLite database alongside your data files. You can freely explore it with other tooling, so you're not locked into Timelinize.&lt;/p&gt;
    &lt;p&gt;Unlike writing a journal, you don't have to take extra time to create content. You're already making the data your timeline can display! And it doesn't replace your current workflow or apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://timelinize.com"/><published>2025-10-07T16:10:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505398</id><title>Cache-Friendly B+Tree Nodes with Dynamic Fanout</title><updated>2025-10-07T19:32:05.151610+00:00</updated><content>&lt;doc fingerprint="eafbc81bf4b08ea8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Cache-Friendly B+Tree Nodes With Dynamic Fanout&lt;/head&gt;&lt;p&gt;For a high-performance B+Tree, the memory layout of each node must be a single contiguous block. This improves locality of reference, increasing the likelihood that the node's contents reside in the CPU cache.&lt;/p&gt;&lt;p&gt;In C++, achieving this means forgoing the use of &lt;code&gt;std::vector&lt;/code&gt;, as it introduces a layer of indirection through a separate memory allocation. The solution to this problem though inevitably increases the implementation complexity and is mired with hidden drawbacks. Nevertheless, this is still a necessary trade-off for unlocking high performance.&lt;/p&gt;&lt;code&gt;  +----------------------+&lt;/code&gt;&lt;head rend="h2"&gt;Challenges&lt;/head&gt;&lt;p&gt;Using &lt;code&gt;std::vector&lt;/code&gt; for a B+Tree node's entries is a non-starter. A &lt;code&gt;std::vector&lt;/code&gt; object holds a pointer to its entries which are stored in a separate block of memory on the heap. This indirection fragments the memory layout, forcing us to fall back on C-style arrays for a contiguous layout when storing variable-length node entries.&lt;/p&gt;&lt;p&gt;This leads to a dilemma. The size of the array must be known at compilation time, yet we need to allow users to configure the fanout (the array's size) at runtime. Furthermore, the implementation should allow inner nodes and leaf nodes to have different fanouts.&lt;/p&gt;&lt;p&gt;This isn't just a B+Tree problem. It is a common challenge in systems programming whenever an object needs to contain a variable-length payload whose size is only known at runtime. How can you define a class that occupies a single block of memory when a part of the block has a dynamic size?&lt;/p&gt;&lt;p&gt;The solution isn't obvious, but it's a well-known trick that systems programmers have used for decades, a technique so common it has eventually been standardized in C99.&lt;/p&gt;&lt;head rend="h2"&gt;The Struct Hack&lt;/head&gt;&lt;p&gt;The solution to this problem is a technique originating in C programming known as the struct hack. The variable-length member (array) is placed at the last position in the struct. To satisfy the compiler an array size of one is hard-coded, ensuring the array size is known at compilation time.&lt;/p&gt;&lt;code&gt;struct Payload {&lt;/code&gt;&lt;p&gt;At runtime, when the required size &lt;code&gt;N&lt;/code&gt; is known, you allocate a single block of memory for the struct and the &lt;code&gt;N&lt;/code&gt; elements combined. The compiler treats this as an opaque block, and provides no safety guarantees. However, accessing the extra allocated space is safe because the variable-length member is the final field in the struct.&lt;/p&gt;&lt;code&gt;// The (N - 1) adjusts for the 1-element array in Payload struct&lt;/code&gt;&lt;p&gt;This pattern was officially standardized in C99, where it is known as a flexible array member.&lt;/p&gt;&lt;p&gt;The C++11 standard formally incorporates the flexible array member, referring to it as an array of unknown bound when it is the last member of a struct.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Arrays of unknown bound&lt;/p&gt;&lt;p&gt;If&lt;/p&gt;&lt;code&gt;expr&lt;/code&gt;is omitted in the declaration of an array, the type declared is "array of unknown bound of T", which is a kind of incomplete type, ...&lt;code&gt;extern int x[]; // the type of x is "array of unknown bound of int"&lt;/code&gt;&lt;lb/&gt;int a[] = {1, 2, 3}; // the type of a is "array of 3 int"&lt;/quote&gt;&lt;p&gt;This means that in C++, the size can be omitted from the final array declaration (e.g. &lt;code&gt;entries_[]&lt;/code&gt;), and the code will compile, enabling the same pattern.&lt;/p&gt;&lt;head rend="h2"&gt;B+Tree Node Declaration&lt;/head&gt;&lt;p&gt;Using the flexible array member syntax, we can now declare a B+Tree node with a memory layout which is a contiguous single block in the heap.&lt;/p&gt;&lt;code&gt;template &amp;lt;typename KeyType, typename ValueType&amp;gt;&lt;/code&gt;&lt;p&gt;Using a &lt;code&gt;std::vector&amp;lt;KeyValuePair&amp;gt;&lt;/code&gt; for the node's entries would result in an indirection. This immediately fragments the memory layout. Accessing an entry within a node is slower, and has higher latency because of the pointer indirection. Chasing the pointer increases the probability of a cache miss, which will force the CPU to stall while it waits for the cache line to be fetched from a different region in main memory.&lt;/p&gt;&lt;p&gt;A cache miss will cost hundreds of CPU cycles compared to just a few cycles for a cache hit. This cumulative latency is unacceptable for any high-performance data structure.&lt;/p&gt;&lt;p&gt;This technique avoids the pointer indirection and provides fine-grained control over memory layout. The node header and data are co-located in one continuous memory block. This layout is cache-friendly and will result in fewer cache misses.&lt;/p&gt;&lt;head rend="h2"&gt;Raw Memory Buffer&lt;/head&gt;&lt;p&gt;This is the key step. The construction of the object has to be separate from its memory allocation. We cannot therefore use the standard &lt;code&gt;new&lt;/code&gt; syntax which will attempt to allocate storage, and then initialize the object in the same storage.&lt;/p&gt;&lt;p&gt;Instead, we use the placement new syntax which only constructs an object in a preallocated memory buffer provided by us. We know exactly how much space to allocate, which is information the standard &lt;code&gt;new&lt;/code&gt; operator does not have in this scenario because of the flexible array member.&lt;/p&gt;&lt;code&gt;// A static helper to allocate storage for a B+Tree node.&lt;/code&gt;&lt;p&gt;The result is a cache-friendly B+Tree node with a fanout that can be configured at runtime.&lt;/p&gt;&lt;head rend="h2"&gt;The Price Of Fine-Grained Control&lt;/head&gt;&lt;p&gt;To create an instance of a B+Tree node with a fanout of &lt;code&gt;256&lt;/code&gt;, it is not possible to write simple idiomatic code like this: &lt;code&gt;new BPlusTreeNode(256)&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Instead we use the custom &lt;code&gt;BPlusTreeNode::Get&lt;/code&gt; helper which knows how much raw memory to allocate for the object including the data section.&lt;/p&gt;&lt;code&gt;BPlusTreeNode *root = BPlusTreeNode&amp;lt;KeyValuePair&amp;gt;::Get(256);&lt;/code&gt;&lt;head rend="h3"&gt;Manual Handling Of Deallocation&lt;/head&gt;&lt;p&gt;The destructor code is also not idiomatic anymore. When the lifetime of the B+Tree node ends, the deallocation code has to be carefully crafted to avoid resource or memory leaks.&lt;/p&gt;&lt;code&gt;class BPlusTreeNode {&lt;/code&gt;&lt;p&gt;This carefully ordered cleanup is necessary because we took manual control of memory. The process is the mirror opposite of our &lt;code&gt;Get&lt;/code&gt; function. We constructed the object outside in: raw memory buffer -&amp;gt; node object -&amp;gt; individual elements. So we teardown in the opposite direction, from the inside out: individual elements -&amp;gt; node object -&amp;gt; raw memory buffer.&lt;/p&gt;&lt;head rend="h3"&gt;Adding New Members In A Derived Class&lt;/head&gt;&lt;p&gt;Adding a new member to a derived class will result in data corruption. It is not possible to add new fields to a specialized &lt;code&gt;InnerNode&lt;/code&gt; or &lt;code&gt;LeafNode&lt;/code&gt; class.&lt;/p&gt;&lt;code&gt;+----------------------+&lt;/code&gt;&lt;code&gt;entries_&lt;/code&gt; array in memory.&lt;p&gt;The raw memory we manually allocated is opaque to the compiler and it cannot safely reason about where the newly added members to the derived class are physically located. The end result is it will overwrite the data buffer and cause data corruption.&lt;/p&gt;&lt;p&gt;The workaround is to break encapsulation and add derived members to the base class so that the flexible array member is always in the last position. This is a significant drawback when we begin using flexible array members.&lt;/p&gt;&lt;code&gt;+----------------------+&lt;/code&gt;
&lt;code&gt;InnerNode&lt;/code&gt; and &lt;code&gt;LeafNode&lt;/code&gt; implementations.&lt;head rend="h3"&gt;Reinventing The Wheel&lt;/head&gt;&lt;p&gt;By using a raw C-style array, we effectively reinvent parts of &lt;code&gt;std::vector&lt;/code&gt;, implementing our own utilities for insertion, deletion and iteration. This not only raises the complexity and maintenance burden but also means we are responsible for ensuring our custom implementation is as performant as the highly-optimized standard library version.&lt;/p&gt;&lt;p&gt;The engineering cost to make this implementation production-grade is significant.&lt;/p&gt;&lt;head rend="h3"&gt;Hidden Data Type Assumptions&lt;/head&gt;&lt;p&gt;The &lt;code&gt;BPlusTreeNode&lt;/code&gt;'s generic signature implies it will work for any &lt;code&gt;KeyType&lt;/code&gt; or &lt;code&gt;ValueType&lt;/code&gt;, but this is dangerously misleading. Using a non-trivial type like &lt;code&gt;std::string&lt;/code&gt; will cause undefined behavior.&lt;/p&gt;&lt;code&gt;template &amp;lt;typename KeyType, typename ValueType&amp;gt;&lt;/code&gt;
&lt;p&gt;To understand why, let's look at how entries are inserted. To make space for a new element, existing entries must be shifted to the right. With our low-level memory layout, this is done using bitwise copy, as the following implementation shows.&lt;/p&gt;&lt;code&gt;bool Insert(const KeyValuePair &amp;amp;element, KeyValuePair *pos) {&lt;/code&gt;
&lt;p&gt;The use of &lt;code&gt;std::memmove&lt;/code&gt; introduces a hidden constraint: &lt;code&gt;KeyValuePair&lt;/code&gt; must be trivially copyable. This means the implementation only works correctly for simple, C-style data structures despite its generic-looking interface.&lt;/p&gt;&lt;p&gt;Using &lt;code&gt;std::memmove&lt;/code&gt; on a &lt;code&gt;std::string&lt;/code&gt; object creates a shallow copy. We now have two &lt;code&gt;std::string&lt;/code&gt; objects whose internal pointers both point to the same character buffer on the heap. When the destructor of the original string is eventually called, it deallocates that buffer. The copied string is now left with a dangling pointer to freed memory, leading to use-after-free errors or a double-free crash when its own destructor runs.&lt;/p&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;The initial hurdle when implementing a B+Tree implementation is solving the contiguous memory layout puzzle avoiding heap indirection. The solution is flexible array members, which makes it possible to compile the program when the number of entries in the B+Tree node is dynamic, and a runtime value.&lt;/p&gt;&lt;p&gt;However, the implementation complexity goes up because of manual memory management, lack of inheritance, and hidden data type constraints. This is unavoidable for high performance.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/"/><published>2025-10-07T16:39:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505407</id><title>Show HN: Arc – high-throughput time-series warehouse with DuckDB analytics</title><updated>2025-10-07T19:32:04.560788+00:00</updated><content>&lt;doc fingerprint="6f1b68d98fb25cce"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance time-series data warehouse built on DuckDB, Parquet, and MinIO.&lt;/p&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Alpha Release - Technical Preview Arc Core is currently in active development and evolving rapidly. While the system is stable and functional, it is not recommended for production workloads at this time. We are continuously improving performance, adding features, and refining the API. Use in development and testing environments only.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High-Performance Ingestion: MessagePack binary protocol (recommended), InfluxDB Line Protocol (drop-in replacement), JSON&lt;/item&gt;
      &lt;item&gt;DuckDB Query Engine: Fast analytical queries with SQL&lt;/item&gt;
      &lt;item&gt;Distributed Storage with MinIO: S3-compatible object storage for unlimited scale and cost-effective data management (recommended). Also supports local disk, AWS S3, and GCS&lt;/item&gt;
      &lt;item&gt;Data Import: Import data from InfluxDB, TimescaleDB, HTTP endpoints&lt;/item&gt;
      &lt;item&gt;Query Caching: Configurable result caching for improved performance&lt;/item&gt;
      &lt;item&gt;Production Ready: Docker deployment with health checks and monitoring&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arc achieves 1.89M records/sec with MessagePack binary protocol!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;1.89M records/sec&lt;/cell&gt;
        &lt;cell&gt;MessagePack binary protocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;p50 Latency&lt;/cell&gt;
        &lt;cell&gt;21ms&lt;/cell&gt;
        &lt;cell&gt;Median response time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;p95 Latency&lt;/cell&gt;
        &lt;cell&gt;204ms&lt;/cell&gt;
        &lt;cell&gt;95th percentile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Success Rate&lt;/cell&gt;
        &lt;cell&gt;99.9998%&lt;/cell&gt;
        &lt;cell&gt;Production-grade reliability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;vs Line Protocol&lt;/cell&gt;
        &lt;cell&gt;7.9x faster&lt;/cell&gt;
        &lt;cell&gt;240K → 1.89M RPS&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tested on Apple M3 Max (14 cores), native deployment with MinIO&lt;/p&gt;
    &lt;p&gt;🎯 Optimal Configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workers: 3x CPU cores (e.g., 14 cores = 42 workers)&lt;/item&gt;
      &lt;item&gt;Deployment: Native mode (2.4x faster than Docker)&lt;/item&gt;
      &lt;item&gt;Storage: MinIO native (not containerized)&lt;/item&gt;
      &lt;item&gt;Protocol: MessagePack binary (&lt;code&gt;/write/v2/msgpack&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Native deployment delivers 1.89M RPS vs 570K RPS in Docker (2.4x faster).&lt;/p&gt;
    &lt;code&gt;# One-command start (auto-installs MinIO, auto-detects CPU cores)
./start.sh native

# Alternative: Manual setup
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env

# Start MinIO natively (auto-configured by start.sh)
brew install minio/stable/minio minio/stable/mc  # macOS
# OR download from https://min.io/download for Linux

# Start Arc (auto-detects optimal worker count: 3x CPU cores)
./start.sh native&lt;/code&gt;
    &lt;p&gt;Arc API will be available at &lt;code&gt;http://localhost:8000&lt;/code&gt;
MinIO Console at &lt;code&gt;http://localhost:9001&lt;/code&gt; (minioadmin/minioadmin)&lt;/p&gt;
    &lt;code&gt;# Start Arc Core with MinIO
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f arc-api

# Stop
docker-compose down&lt;/code&gt;
    &lt;p&gt;Note: Docker mode achieves ~570K RPS. For maximum performance (1.89M RPS), use native deployment.&lt;/p&gt;
    &lt;p&gt;Deploy Arc Core to a remote server:&lt;/p&gt;
    &lt;code&gt;# Docker deployment
./deploy.sh -h your-server.com -u ubuntu -m docker

# Native deployment
./deploy.sh -h your-server.com -u ubuntu -m native&lt;/code&gt;
    &lt;p&gt;Arc Core uses a centralized &lt;code&gt;arc.conf&lt;/code&gt; configuration file (TOML format). This provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clean, organized configuration structure&lt;/item&gt;
      &lt;item&gt;Environment variable overrides for Docker/production&lt;/item&gt;
      &lt;item&gt;Production-ready defaults&lt;/item&gt;
      &lt;item&gt;Comments and documentation inline&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Edit the &lt;code&gt;arc.conf&lt;/code&gt; file for all settings:&lt;/p&gt;
    &lt;code&gt;# Server Configuration
[server]
host = "0.0.0.0"
port = 8000
workers = 8  # Adjust based on load: 4=light, 8=medium, 16=high

# Authentication
[auth]
enabled = true
default_token = ""  # Leave empty to auto-generate

# Query Cache
[query_cache]
enabled = true
ttl_seconds = 60

# Storage Backend (MinIO recommended)
[storage]
backend = "minio"

[storage.minio]
endpoint = "http://minio:9000"
access_key = "minioadmin"
secret_key = "minioadmin123"
bucket = "arc"
use_ssl = false

# For AWS S3
# [storage]
# backend = "s3"
# [storage.s3]
# bucket = "arc-data"
# region = "us-east-1"

# For Google Cloud Storage
# [storage]
# backend = "gcs"
# [storage.gcs]
# bucket = "arc-data"
# project_id = "my-project"&lt;/code&gt;
    &lt;p&gt;Configuration Priority (highest to lowest):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Environment variables (e.g., &lt;code&gt;ARC_WORKERS=16&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;arc.conf&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;Built-in defaults&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can override any setting via environment variables:&lt;/p&gt;
    &lt;code&gt;# Server
ARC_HOST=0.0.0.0
ARC_PORT=8000
ARC_WORKERS=8

# Storage
STORAGE_BACKEND=minio
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_BUCKET=arc

# Cache
QUERY_CACHE_ENABLED=true
QUERY_CACHE_TTL=60

# Logging
LOG_LEVEL=INFO&lt;/code&gt;
    &lt;p&gt;Legacy Support: &lt;code&gt;.env&lt;/code&gt; files are still supported for backward compatibility, but &lt;code&gt;arc.conf&lt;/code&gt; is recommended.&lt;/p&gt;
    &lt;p&gt;After starting Arc Core, create an admin token for API access:&lt;/p&gt;
    &lt;code&gt;# Docker deployment
docker exec -it arc-api python3 -c "
from api.auth import AuthManager
auth = AuthManager(db_path='/data/historian.db')
token = auth.create_token('my-admin', description='Admin token')
print(f'Admin Token: {token}')
"

# Native deployment
cd /path/to/arc-core
source venv/bin/activate
python3 -c "
from api.auth import AuthManager
auth = AuthManager()
token = auth.create_token('my-admin', description='Admin token')
print(f'Admin Token: {token}')
"&lt;/code&gt;
    &lt;p&gt;Save this token - you'll need it for all API requests.&lt;/p&gt;
    &lt;p&gt;All endpoints require authentication via Bearer token:&lt;/p&gt;
    &lt;code&gt;# Set your token
export ARC_TOKEN="your-token-here"&lt;/code&gt;
    &lt;code&gt;curl http://localhost:8000/health&lt;/code&gt;
    &lt;p&gt;MessagePack binary protocol offers 3x faster ingestion with zero-copy PyArrow processing:&lt;/p&gt;
    &lt;code&gt;import msgpack
import requests
from datetime import datetime

# Prepare data in MessagePack format
data = {
    "database": "metrics",
    "table": "cpu_usage",
    "records": [
        {
            "timestamp": int(datetime.now().timestamp() * 1e9),  # nanoseconds
            "host": "server01",
            "cpu": 0.64,
            "memory": 0.82
        },
        {
            "timestamp": int(datetime.now().timestamp() * 1e9),
            "host": "server02",
            "cpu": 0.45,
            "memory": 0.71
        }
    ]
}

# Send via MessagePack
response = requests.post(
    "http://localhost:8000/write/v2/msgpack",
    headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/msgpack"
    },
    data=msgpack.packb(data)
)
print(response.json())&lt;/code&gt;
    &lt;p&gt;Batch ingestion (for high throughput):&lt;/p&gt;
    &lt;code&gt;# Send 10,000 records at once
records = [
    {
        "timestamp": int(datetime.now().timestamp() * 1e9),
        "sensor_id": f"sensor_{i}",
        "temperature": 20 + (i % 10),
        "humidity": 60 + (i % 20)
    }
    for i in range(10000)
]

data = {
    "database": "iot",
    "table": "sensors",
    "records": records
}

response = requests.post(
    "http://localhost:8000/write/v2/msgpack",
    headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/msgpack"
    },
    data=msgpack.packb(data)
)&lt;/code&gt;
    &lt;p&gt;For drop-in replacement of InfluxDB - compatible with Telegraf and InfluxDB clients:&lt;/p&gt;
    &lt;code&gt;# InfluxDB 1.x compatible endpoint
curl -X POST "http://localhost:8000/write/line?db=mydb" \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: text/plain" \
  --data-binary "cpu,host=server01 value=0.64 1633024800000000000"

# Multiple measurements
curl -X POST "http://localhost:8000/write/line?db=metrics" \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: text/plain" \
  --data-binary "cpu,host=server01,region=us-west value=0.64 1633024800000000000
memory,host=server01,region=us-west used=8.2,total=16.0 1633024800000000000
disk,host=server01,region=us-west used=120.5,total=500.0 1633024800000000000"&lt;/code&gt;
    &lt;p&gt;Telegraf configuration (drop-in InfluxDB replacement):&lt;/p&gt;
    &lt;code&gt;[[outputs.influxdb]]
  urls = ["http://localhost:8000"]
  database = "telegraf"
  skip_database_creation = true

  # Authentication
  username = ""  # Leave empty
  password = "$ARC_TOKEN"  # Use your Arc token as password

  # Or use HTTP headers
  [outputs.influxdb.headers]
    Authorization = "Bearer $ARC_TOKEN"&lt;/code&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/query \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "database": "mydb",
    "query": "SELECT * FROM cpu_usage WHERE host = '\''server01'\'' ORDER BY timestamp DESC LIMIT 100"
  }'&lt;/code&gt;
    &lt;p&gt;Advanced queries with DuckDB SQL:&lt;/p&gt;
    &lt;code&gt;# Aggregations
curl -X POST http://localhost:8000/query \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "database": "metrics",
    "query": "SELECT host, AVG(cpu) as avg_cpu, MAX(memory) as max_memory FROM cpu_usage WHERE timestamp &amp;gt; now() - INTERVAL 1 HOUR GROUP BY host"
  }'

# Time-series analysis
curl -X POST http://localhost:8000/query \
  -H "Authorization: Bearer $ARC_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "database": "iot",
    "query": "SELECT time_bucket(INTERVAL '\''5 minutes'\'', timestamp) as bucket, AVG(temperature) as avg_temp FROM sensors GROUP BY bucket ORDER BY bucket"
  }'&lt;/code&gt;
    &lt;code&gt;┌─────────────────────────────────────────────────────────────┐
│                     Client Applications                      │
│  (Telegraf, Python, Go, JavaScript, curl, etc.)             │
└──────────────────┬──────────────────────────────────────────┘
                   │
                   │ HTTP/HTTPS
                   ▼
┌─────────────────────────────────────────────────────────────┐
│                   Arc API Layer (FastAPI)                    │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────────┐  │
│  │ Line Protocol│  │  MessagePack │  │  Query Engine    │  │
│  │   Endpoint   │  │   Binary API │  │   (DuckDB)       │  │
│  └──────────────┘  └──────────────┘  └──────────────────┘  │
└──────────────────┬──────────────────────────────────────────┘
                   │
                   │ Write Pipeline
                   ▼
┌─────────────────────────────────────────────────────────────┐
│              Buffering &amp;amp; Processing Layer                    │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  ParquetBuffer (Line Protocol)                       │  │
│  │  - Batches records by measurement                    │  │
│  │  - Polars DataFrame → Parquet                        │  │
│  │  - Snappy compression                                │  │
│  └──────────────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  ArrowParquetBuffer (MessagePack Binary)             │  │
│  │  - Zero-copy PyArrow RecordBatch                     │  │
│  │  - Direct Parquet writes (3x faster)                 │  │
│  │  - Columnar from start                               │  │
│  └──────────────────────────────────────────────────────┘  │
└──────────────────┬──────────────────────────────────────────┘
                   │
                   │ Parquet Files
                   ▼
┌─────────────────────────────────────────────────────────────┐
│              Storage Backend (Pluggable)                     │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  MinIO (Recommended - S3-compatible)                   │ │
│  │  ✓ Unlimited scale          ✓ Distributed             │ │
│  │  ✓ Cost-effective           ✓ Self-hosted             │ │
│  │  ✓ High availability        ✓ Erasure coding          │ │
│  │  ✓ Multi-tenant             ✓ Object versioning       │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  Alternative backends: Local Disk, AWS S3, Google Cloud     │
└─────────────────────────────────────────────────────────────┘
                   │
                   │ Query Path (Direct Parquet reads)
                   ▼
┌─────────────────────────────────────────────────────────────┐
│              Query Engine (DuckDB)                           │
│  - Direct Parquet reads from object storage                 │
│  - Columnar execution engine                                │
│  - Query cache for common queries                           │
│  - Full SQL interface (Postgres-compatible)                 │
└─────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;Arc Core is designed with MinIO as the primary storage backend for several key reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Unlimited Scale: Store petabytes of time-series data without hitting storage limits&lt;/item&gt;
      &lt;item&gt;Cost-Effective: Commodity hardware or cloud storage at fraction of traditional database costs&lt;/item&gt;
      &lt;item&gt;Distributed Architecture: Built-in replication and erasure coding for data durability&lt;/item&gt;
      &lt;item&gt;S3 Compatibility: Works with any S3-compatible storage (AWS S3, GCS, Wasabi, etc.)&lt;/item&gt;
      &lt;item&gt;Performance: Direct Parquet reads from object storage with DuckDB's efficient execution&lt;/item&gt;
      &lt;item&gt;Separation of Compute &amp;amp; Storage: Scale storage and compute independently&lt;/item&gt;
      &lt;item&gt;Self-Hosted Option: Run on your own infrastructure without cloud vendor lock-in&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MinIO + Parquet + DuckDB combination provides the perfect balance of cost, performance, and scalability for analytical time-series workloads.&lt;/p&gt;
    &lt;p&gt;Arc Core has been benchmarked using ClickBench - the industry-standard analytical database benchmark with 100M row dataset (14GB) and 43 analytical queries.&lt;/p&gt;
    &lt;p&gt;Hardware: AWS c6a.4xlarge (16 vCPU AMD EPYC 7R13, 32GB RAM, 500GB gp2)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cold Run Total: 35.18s (sum of 43 queries, first execution)&lt;/item&gt;
      &lt;item&gt;Hot Run Average: 0.81s (average per query after caching)&lt;/item&gt;
      &lt;item&gt;Aggregate Performance: ~2.8M rows/sec cold, ~123M rows/sec hot (across all queries)&lt;/item&gt;
      &lt;item&gt;Storage: MinIO (S3-compatible)&lt;/item&gt;
      &lt;item&gt;Success Rate: 43/43 queries (100%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hardware: Apple M3 Max (14 cores ARM, 36GB RAM)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cold Run Total: 23.86s (sum of 43 queries, first execution)&lt;/item&gt;
      &lt;item&gt;Hot Run Average: 0.52s (average per query after caching)&lt;/item&gt;
      &lt;item&gt;Aggregate Performance: ~4.2M rows/sec cold, ~192M rows/sec hot (across all queries)&lt;/item&gt;
      &lt;item&gt;Storage: Local NVMe SSD&lt;/item&gt;
      &lt;item&gt;Success Rate: 43/43 queries (100%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Columnar Storage: Parquet format with Snappy compression&lt;/item&gt;
      &lt;item&gt;Query Engine: DuckDB with default settings (ClickBench compliant)&lt;/item&gt;
      &lt;item&gt;Result Caching: 60s TTL for repeated queries (production mode)&lt;/item&gt;
      &lt;item&gt;End-to-End: All timings include HTTP/JSON API overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Time (avg)&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q1&lt;/cell&gt;
        &lt;cell&gt;0.021s&lt;/cell&gt;
        &lt;cell&gt;Simple aggregation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q8&lt;/cell&gt;
        &lt;cell&gt;0.034s&lt;/cell&gt;
        &lt;cell&gt;String parsing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q27&lt;/cell&gt;
        &lt;cell&gt;0.086s&lt;/cell&gt;
        &lt;cell&gt;Complex grouping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q41&lt;/cell&gt;
        &lt;cell&gt;0.048s&lt;/cell&gt;
        &lt;cell&gt;URL parsing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q42&lt;/cell&gt;
        &lt;cell&gt;0.044s&lt;/cell&gt;
        &lt;cell&gt;Multi-column filter&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Time (avg)&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q29&lt;/cell&gt;
        &lt;cell&gt;7.97s&lt;/cell&gt;
        &lt;cell&gt;Heavy string operations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Q19&lt;/cell&gt;
        &lt;cell&gt;1.69s&lt;/cell&gt;
        &lt;cell&gt;Multiple joins&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q33&lt;/cell&gt;
        &lt;cell&gt;1.86s&lt;/cell&gt;
        &lt;cell&gt;Complex aggregations&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Benchmark Configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dataset: 100M rows, 14GB Parquet (ClickBench hits.parquet)&lt;/item&gt;
      &lt;item&gt;Protocol: HTTP REST API with JSON responses&lt;/item&gt;
      &lt;item&gt;Caching: Disabled for benchmark compliance&lt;/item&gt;
      &lt;item&gt;Tuning: None (default DuckDB settings)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See full results and methodology at ClickBench Results (Arc submission pending).&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;docker-compose.yml&lt;/code&gt; includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;arc-api: Main API server (port 8000)&lt;/item&gt;
      &lt;item&gt;minio: S3-compatible storage (port 9000, console 9001)&lt;/item&gt;
      &lt;item&gt;minio-init: Initializes MinIO buckets on startup&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run with auto-reload
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# Run tests (if available in parent repo)
pytest tests/&lt;/code&gt;
    &lt;p&gt;Health check endpoint:&lt;/p&gt;
    &lt;code&gt;curl http://localhost:8000/health&lt;/code&gt;
    &lt;p&gt;Logs:&lt;/p&gt;
    &lt;code&gt;# Docker
docker-compose logs -f arc-api

# Native (systemd)
sudo journalctl -u arc-api -f&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /&lt;/code&gt;- API information&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /health&lt;/code&gt;- Service health check&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /ready&lt;/code&gt;- Readiness probe&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /docs&lt;/code&gt;- Swagger UI documentation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /redoc&lt;/code&gt;- ReDoc documentation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /openapi.json&lt;/code&gt;- OpenAPI specification&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: All other endpoints require Bearer token authentication.&lt;/p&gt;
    &lt;p&gt;MessagePack Binary Protocol (Recommended - 3x faster):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /write/v2/msgpack&lt;/code&gt;- Write data via MessagePack&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/v2/msgpack&lt;/code&gt;- Alternative endpoint&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /write/v2/msgpack/stats&lt;/code&gt;- Get ingestion statistics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /write/v2/msgpack/spec&lt;/code&gt;- Get protocol specification&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Line Protocol (InfluxDB compatibility):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /write&lt;/code&gt;- InfluxDB 1.x compatible write&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/v1/write&lt;/code&gt;- InfluxDB 1.x API format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/v2/write&lt;/code&gt;- InfluxDB 2.x API format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/v1/query&lt;/code&gt;- InfluxDB 1.x query format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /write/health&lt;/code&gt;- Write endpoint health check&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /write/stats&lt;/code&gt;- Write statistics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /write/flush&lt;/code&gt;- Force flush write buffer&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /query&lt;/code&gt;- Execute DuckDB SQL query&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /query/estimate&lt;/code&gt;- Estimate query cost&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /query/stream&lt;/code&gt;- Stream large query results&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /query/{measurement}&lt;/code&gt;- Get measurement data&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /query/{measurement}/csv&lt;/code&gt;- Export measurement as CSV&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /measurements&lt;/code&gt;- List all measurements/tables&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /auth/verify&lt;/code&gt;- Verify token validity&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /auth/tokens&lt;/code&gt;- List all tokens&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /auth/tokens&lt;/code&gt;- Create new token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /auth/tokens/{id}&lt;/code&gt;- Get token details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PATCH /auth/tokens/{id}&lt;/code&gt;- Update token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DELETE /auth/tokens/{id}&lt;/code&gt;- Delete token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /auth/tokens/{id}/rotate&lt;/code&gt;- Rotate token (generate new)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /health&lt;/code&gt;- Service health check&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /ready&lt;/code&gt;- Readiness probe&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics&lt;/code&gt;- Prometheus metrics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics/timeseries/{type}&lt;/code&gt;- Time-series metrics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics/endpoints&lt;/code&gt;- Endpoint statistics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics/query-pool&lt;/code&gt;- Query pool status&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /metrics/memory&lt;/code&gt;- Memory profile&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /logs&lt;/code&gt;- Application logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;InfluxDB Connections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /connections/influx&lt;/code&gt;- List InfluxDB connections&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /connections/influx&lt;/code&gt;- Create InfluxDB connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PUT /connections/influx/{id}&lt;/code&gt;- Update connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DELETE /connections/{type}/{id}&lt;/code&gt;- Delete connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /connections/{type}/{id}/activate&lt;/code&gt;- Activate connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /connections/{type}/test&lt;/code&gt;- Test connection&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Storage Connections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /connections/storage&lt;/code&gt;- List storage backends&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /connections/storage&lt;/code&gt;- Create storage connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PUT /connections/storage/{id}&lt;/code&gt;- Update storage connection&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /jobs&lt;/code&gt;- List all export jobs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /jobs&lt;/code&gt;- Create new export job&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PUT /jobs/{id}&lt;/code&gt;- Update job configuration&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DELETE /jobs/{id}&lt;/code&gt;- Delete job&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /jobs/{id}/executions&lt;/code&gt;- Get job execution history&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /jobs/{id}/run&lt;/code&gt;- Run job immediately&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /jobs/{id}/cancel&lt;/code&gt;- Cancel running job&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /monitoring/jobs&lt;/code&gt;- Monitor job status&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /api/http-json/connections&lt;/code&gt;- Create HTTP/JSON connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /api/http-json/connections&lt;/code&gt;- List connections&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /api/http-json/connections/{id}&lt;/code&gt;- Get connection details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PUT /api/http-json/connections/{id}&lt;/code&gt;- Update connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DELETE /api/http-json/connections/{id}&lt;/code&gt;- Delete connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/http-json/connections/{id}/test&lt;/code&gt;- Test connection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/http-json/connections/{id}/discover-schema&lt;/code&gt;- Discover schema&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /api/http-json/export&lt;/code&gt;- Export data via HTTP&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /cache/stats&lt;/code&gt;- Cache statistics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /cache/health&lt;/code&gt;- Cache health status&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /cache/clear&lt;/code&gt;- Clear query cache&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arc Core includes auto-generated API documentation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger UI: &lt;code&gt;http://localhost:8000/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ReDoc: &lt;code&gt;http://localhost:8000/redoc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;OpenAPI JSON: &lt;code&gt;http://localhost:8000/openapi.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arc Core is under active development. Current focus areas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Performance Optimization: Further improvements to ingestion and query performance&lt;/item&gt;
      &lt;item&gt;API Stability: Finalizing core API contracts&lt;/item&gt;
      &lt;item&gt;Enhanced Monitoring: Additional metrics and observability features&lt;/item&gt;
      &lt;item&gt;Documentation: Expanded guides and tutorials&lt;/item&gt;
      &lt;item&gt;Production Hardening: Testing and validation for production use cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome feedback and feature requests as we work toward a stable 1.0 release.&lt;/p&gt;
    &lt;p&gt;Arc Core is licensed under the GNU Affero General Public License v3.0 (AGPL-3.0).&lt;/p&gt;
    &lt;p&gt;This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Free to use - Use Arc Core for any purpose&lt;/item&gt;
      &lt;item&gt;✅ Free to modify - Modify the source code as needed&lt;/item&gt;
      &lt;item&gt;✅ Free to distribute - Share your modifications with others&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Share modifications - If you modify Arc and run it as a service, you must share your changes under AGPL-3.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AGPL-3.0 ensures that improvements to Arc benefit the entire community, even when run as a cloud service. This prevents the "SaaS loophole" where companies could take the code, improve it, and keep changes proprietary.&lt;/p&gt;
    &lt;p&gt;For organizations that require:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Proprietary modifications without disclosure&lt;/item&gt;
      &lt;item&gt;Commercial support and SLAs&lt;/item&gt;
      &lt;item&gt;Enterprise features and managed services&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please contact us at: enterprise[at]basekick[dot]net&lt;/p&gt;
    &lt;p&gt;We offer dual licensing and commercial support options.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Community Support: GitHub Issues&lt;/item&gt;
      &lt;item&gt;Enterprise Support: enterprise[at]basekick[dot]net&lt;/item&gt;
      &lt;item&gt;General Inquiries: support[at]basekick[dot]net&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arc Core is provided "as-is" in alpha state. While we use it extensively for development and testing, it is not yet production-ready. Features and APIs may change without notice. Always back up your data and test thoroughly in non-production environments before considering any production deployment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Basekick-Labs/arc"/><published>2025-10-07T16:40:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505626</id><title>Robin Williams' daughter pleads for people to stop sending AI videos of her dad</title><updated>2025-10-07T19:32:04.425637+00:00</updated><content>&lt;doc fingerprint="97d23aa93261cd6d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Robin Williams' daughter pleads for people to stop sending AI videos of her dad&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Published&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Zelda Williams, the daughter of Robin Williams, has asked people to stop sending her AI-generated videos of her father, the celebrated US actor and comic who died in 2014.&lt;/p&gt;
    &lt;p&gt;"Please, just stop sending me AI videos of Dad," Zelda Williams posted on her Instagram stories.&lt;/p&gt;
    &lt;p&gt;"Stop believing I wanna see it or that I'll understand, I don't and I won't. If you're just trying to troll me, I've seen way worse, I'll restrict and move on.&lt;/p&gt;
    &lt;p&gt;"But please, if you've got any decency, just stop doing this to him and to me, to everyone even, full stop. It's dumb, it's a waste of time and energy, and believe me, it's NOT what he'd want."&lt;/p&gt;
    &lt;p&gt;This is not the first time Zelda Williams, a film director, has criticised AI versions of her father, who took his own life in 2014 at his Californian home at the age of 63.&lt;/p&gt;
    &lt;p&gt;Williams, who was famous for films such as Good Morning Vietnam, Dead Poets Society and Mrs Doubtfire, was understood to have been battling depression at the time of his death.&lt;/p&gt;
    &lt;p&gt;In 2023, in an Instagram post supporting a campaign against AI by US media union SAG-Aftra, she described attempts at recreating his voice as "personally disturbing", while also pointing to the wider implications.&lt;/p&gt;
    &lt;p&gt;Her post on Tuesday reflects a trend on social media, where images of people who have died are animated, featuring captions like "bring your loved ones back to life".&lt;/p&gt;
    &lt;p&gt;Williams continued: "To watch the legacies of real people be condensed down to 'this vaguely looks and sounds like them so that's enough', just so other people can churn out horrible TikTok slop puppeteering them is maddening," she continued.&lt;/p&gt;
    &lt;p&gt;"You're not making art, you're making disgusting, over-processed hotdogs out of the lives of human beings, out of the history of art and music, and then shoving them down someone else's throat hoping they'll give you a little thumbs up and like it. Gross."&lt;/p&gt;
    &lt;p&gt;She concluded: "And for the love of EVERY THING, stop calling it 'the future,' AI is just badly recycling and regurgitating the past to be re-consumed. You are taking in the Human Centipede of content, and from the very very end of the line, all while the folks at the front laugh and laugh, consume and consume."&lt;/p&gt;
    &lt;p&gt;The Human Centipede is a reference to the 2009 body horror film.&lt;/p&gt;
    &lt;head rend="h2"&gt;'She sparks conversation'&lt;/head&gt;
    &lt;p&gt;Her latest comments come in the wake of unease following the unveiling of "AI actor", Tilly Norwood.&lt;/p&gt;
    &lt;p&gt;Norwood was created by Dutch actor and comedian Eline Van der Velden, who reportedly said she wanted Norwood to become the "next Scarlett Johansson".&lt;/p&gt;
    &lt;p&gt;In a statement, SAG-Aftra said Norwood "is not an actor, it's a character generated by a computer program that was trained on the work of countless professional performers.&lt;/p&gt;
    &lt;p&gt;"It has no life experience to draw from, no emotion and, from what we've seen, audiences aren't interested in watching computer-generated content untethered from the human experience," the union added.&lt;/p&gt;
    &lt;p&gt;Actress Emily Blunt also recently said she found the idea of Norwood terrifying.&lt;/p&gt;
    &lt;p&gt;"That is really, really scary, Come on, agencies, don't do that. Please stop. Please stop taking away our human connection," she said on a podcast with Variety.&lt;/p&gt;
    &lt;p&gt;Van der Velden later said in a statement, external: "To those who have expressed anger over the creation of my AI character, Tilly Norwood, she is not a replacement for a human being, but a creative work â a piece of art.&lt;/p&gt;
    &lt;p&gt;"Like many forms of art before her, she sparks conversation, and that in itself shows the power of creativity."&lt;/p&gt;
    &lt;head rend="h2"&gt;Related topics&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published6 days ago&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published25 September 2023&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published27 February 2015&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published13 August 2014&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.co.uk/news/articles/c0r0erqk18jo"/><published>2025-10-07T16:56:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505666</id><title>Pigeon (YC W23) is hiring a lead full stack engineer</title><updated>2025-10-07T19:32:03.811520+00:00</updated><content>&lt;doc fingerprint="aa9be2aaf687b428"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;head rend="h1"&gt;Lead Full Stack Software Engineer at Pigeon (YC W23)&lt;/head&gt;
        &lt;p&gt;Pigeon (YC W23) is looking for a motivated Lead Full Stack Software Engineer to join our engineering team. You can work from our NYC office or remotely if you’re not local.&lt;/p&gt;
        &lt;p&gt;As a Lead Full Stack Software Engineer at Pigeon, you will help lead a small and fast-paced engineering team and spearhead the development of new features and systems from the ground up. You will be given a unique opportunity to shape our stack, processes, and culture while making a tangible impact on our technology and our customers.&lt;/p&gt;
        &lt;head rend="h3"&gt;Why Join Pigeon&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Impact: You will own high-value features for Pigeon’s entire customer base that will help shape their everyday business processes.&lt;/item&gt;
          &lt;item&gt;Culture: You will help shape how we work on a day-to-day basis and inform core values as we grow.&lt;/item&gt;
          &lt;item&gt;Leadership: You will be placed in a key leadership position with the opportunity to contribute to our direction, goals, and vision.&lt;/item&gt;
          &lt;item&gt;Learning: You will be encouraged to experiment with new methods and technologies to enable innovative experiences for customers.&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;What You’ll Do&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Own core services, APIs, and integrations with third-party systems&lt;/item&gt;
          &lt;item&gt;Build and scale our AI-powered document processing system&lt;/item&gt;
          &lt;item&gt;Ship new features end-to-end - from conception to implementation to deployment&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;What We’re Looking For&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;5+ years experience as a full-stack software engineer&lt;/item&gt;
          &lt;item&gt;Comfortable with fast-paced development environment and early-stage ambiguity&lt;/item&gt;
          &lt;item&gt;Ability to take full ownership of projects (scoping, system design, implementation, QA, deployment, and maintenance)&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;Our Stack&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;AWS, Kubernetes, Vercel&lt;/item&gt;
          &lt;item&gt;Python, Flask, FastAPI, SqlAlchemy&lt;/item&gt;
          &lt;item&gt;NextJS, Javascript/Typescript, React, CSS, Tailwind&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;Benefits:&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Healthcare (Medical, Vision, and Dental)&lt;/item&gt;
          &lt;item&gt;OneMedical&lt;/item&gt;
          &lt;item&gt;401(k)&lt;/item&gt;
          &lt;item&gt;Unlimited PTO&lt;/item&gt;
          &lt;item&gt;16” Macbook Pro (M2 Chip)&lt;/item&gt;
          &lt;item&gt;Free Pigeon Merchandise (shop.pigeondocuments.com)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;p&gt;Pigeon automates the entire document lifecycle: collecting documents from clients, reviewing and extracting data with AI, and syncing with CRMs or storage systems. Pigeon eliminates the manual back-and-forth of document handling and eliminates thousands of hours of manual tasks.&lt;/p&gt;
      &lt;p&gt;We're growing fast, and want someone who can help join our team as we prepare for the next stage of growth.&lt;/p&gt;
      &lt;head rend="h3"&gt;Team&lt;/head&gt;
      &lt;p&gt;We are a team of 4 who previously worked at Google, Squarespace, Deloitte, and HonorLock.&lt;/p&gt;
      &lt;head rend="h3"&gt;Funding Status&lt;/head&gt;
      &lt;p&gt;We closed a $3.5M Seed round post-YC.&lt;/p&gt;
      &lt;head rend="h3"&gt;About our Technology:&lt;/head&gt;
      &lt;p&gt;Our Stack:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;AWS, Kubernetes, Vercel&lt;/item&gt;
        &lt;item&gt;Python, Flask, SqlAlchemy&lt;/item&gt;
        &lt;item&gt;NextJS, Javascript/Typescript, React, CSS, Tailwind&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/pigeon/jobs/sjuJOg3-lead-full-stack-software-engineer-remote-us"/><published>2025-10-07T17:00:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505692</id><title>Doing Rails Wrong</title><updated>2025-10-07T19:32:03.615216+00:00</updated><content>&lt;doc fingerprint="2cd7f1377fc53f68"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt; You're doing Rails wrong. copy link &lt;/head&gt;
    &lt;head rend="h6"&gt;Tuesday, 07 October 2025&lt;/head&gt;
    &lt;p&gt; Kevin: Hey, have you tried Vite for Rails 8? It’s insanely fast.&lt;/p&gt;
    &lt;p&gt; John: I’ve heard of it. Isn’t that a build tool? Didn’t Rails already come with one?&lt;/p&gt;
    &lt;p&gt; K: Well, it did, but Vite is like… modern. You’ll need to install Node, npm, and configure a few scripts, but it’s totally worth it.&lt;/p&gt;
    &lt;p&gt; J: Wait, Rails needs Node now?&lt;/p&gt;
    &lt;p&gt; K: Well, yeah — if you want to use React. Everyone’s using React.&lt;/p&gt;
    &lt;p&gt; J: Didn’t Rails have something for that?&lt;/p&gt;
    &lt;p&gt; K: It did, but now you’ll want to use Vite with React Refresh so you get instant component reloads. And if you want TypeScript support, you’ll have to configure that too.&lt;/p&gt;
    &lt;p&gt; J: Sounds… like a lot.&lt;/p&gt;
    &lt;p&gt; K: Oh, not really. Just install Babel, configure your .babelrc, add vite-plugin-ruby, then you’ll want PostCSS for your styles.&lt;/p&gt;
    &lt;p&gt; J: PostCSS?&lt;/p&gt;
    &lt;p&gt; K: Yeah, and then Tailwind, obviously — you don’t want to write CSS like a peasant.&lt;/p&gt;
    &lt;p&gt; J: Of course not.&lt;/p&gt;
    &lt;p&gt; K: Then you’ll probably want to add ESLint and Prettier to make sure your code looks clean, and maybe Husky for pre-commit hooks.&lt;/p&gt;
    &lt;p&gt; J: So... Vite, React, Babel, PostCSS, Tailwind, ESLint, Prettier, Husky. That’s it?&lt;/p&gt;
    &lt;p&gt; K: Pretty much. Oh, unless you want server-side rendering — then you’ll need Next.js or Remix.&lt;/p&gt;
    &lt;p&gt; J: Wait, we’re still talking about a Rails app, right?&lt;/p&gt;
    &lt;p&gt; K: Yeah, but hybrid stacks are the way to go! You could also use StimulusReflex or Hotwire if you want reactive components without JS frameworks.&lt;/p&gt;
    &lt;p&gt; J: StimulusReflex sounds like a Marvel character.&lt;/p&gt;
    &lt;p&gt; K: Ha! No, it’s for real-time updates. But you’ll need ActionCable configured, Redis running, and—&lt;/p&gt;
    &lt;p&gt; J: Redis?&lt;/p&gt;
    &lt;p&gt; K: Yeah, you need a pub/sub layer. Don’t worry, it’s just another Docker container.&lt;/p&gt;
    &lt;p&gt; J: Docker too?&lt;/p&gt;
    &lt;p&gt; K: Yeah, to isolate your dependencies. And if you want everything reproducible, you’ll need Docker Compose, maybe Fly.io for deployment, and a build pipeline with GitHub Actions.&lt;/p&gt;
    &lt;p&gt; J: That’s... quite a setup.&lt;/p&gt;
    &lt;p&gt; K: It’s just modern web development, man. Keeps things simple. What are you doing?&lt;/p&gt;
    &lt;p&gt; J: Just tinkering.&lt;/p&gt;
    &lt;p&gt;(John runs a single command. The app boots instantly, working forms, instant loading times, blazing fast navigation.)&lt;/p&gt;
    &lt;p&gt; K: Wow, that looks like a pretty complex setup. What stack’s that?&lt;/p&gt;
    &lt;p&gt; J: Vanilla Rails.&lt;/p&gt;
    &lt;p&gt;Just F#$%^&amp;amp; use Rails.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bananacurvingmachine.com/articles/you-re-doing-rails-wrong"/><published>2025-10-07T17:01:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505713</id><title>Building a Browser for Reverse Engineers</title><updated>2025-10-07T19:32:03.205762+00:00</updated><content>&lt;doc fingerprint="aa009620a90d26e8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Preamble&lt;/head&gt;
    &lt;p&gt;In the expanding world of AI my heart still lies in AST transforms, browser fingerprinting, and anti-bot circumvention. In fact, that's the majority of this blog's content. But my workflow always felt... primitive. I was still manually sifting through page scripts, pasting suspicious snippets into an editor, and writing bespoke deobfuscators by hand. Tools like Webcrack and deobfuscate.io help, but the end-to-end loop still felt slow and manual. I wanted to build a tool that would be my web reverse-engineering Swiss Army knife&lt;/p&gt;
    &lt;p&gt;If you're just curious about what it looks like and don't care about how it works then here's a quick showcase:&lt;/p&gt;
    &lt;head rend="h2"&gt;Humble Beginnings&lt;/head&gt;
    &lt;p&gt;My first idea was simple: make a browser extension. For an MVP I wanted to hook an arbitrary function like &lt;code&gt;Array.prototype.push&lt;/code&gt; as early as possible and log every call to it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hooking functions in JavaScript&lt;/head&gt;
    &lt;p&gt;In JavaScript, it's trivial to hook into and override existing functions because you can reassign references at runtime. A common pattern is to stash the original function, replace it with a wrapper that does whatever instrumentation you want, and then call the original so the page keeps behaving normally:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
&lt;/code&gt;
    &lt;p&gt;Here's what that looks like in Chrome's devtools:&lt;/p&gt;
    &lt;p&gt;This technique should make it pretty straightforward to build a Chrome extension that hooks arbitrary global functions on page load and surfaces calls in a small UI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Content Scripts&lt;/head&gt;
    &lt;p&gt;Chrome's content scripts are files that run in the context of web pages, which we can use to install our hooks early.&lt;/p&gt;
    &lt;p&gt;The idea is simple, we create a content script that runs at document_start that injects a tiny bit of code that replaces &lt;code&gt;Array.prototype.push&lt;/code&gt; with a wrapper that logs and then calls the original.&lt;/p&gt;
    &lt;code&gt;{
 "name": "My extension",
 "content_scripts": [
   {
     "run_at": "document_start", // Script is injected after any files from css, but before any other DOM is constructed or any other script is run.
     "matches": ["&amp;lt;all_urls&amp;gt;"],
     "js": ["content-script.js"]
   }
 ]
}
&lt;/code&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
&lt;/code&gt;
    &lt;p&gt;Running this on a page that clearly used Array.push gave me... absolutely nothing. At first, I thought it had to be an execution order issue. Maybe my hook was loading too late? But after another read through the docs, I found this painfully obvious note staring me right in the face:&lt;/p&gt;
    &lt;p&gt;âContent scripts live in an isolated world, allowing a content script to make changes to its JavaScript environment without conflicting with the page or other extensionsâ content scripts.â&lt;/p&gt;
    &lt;p&gt;In hindsight, of course that makes sense. Still, it sucked. I wasnât ready to give up yet, though. I had a potentially clever workaround: injecting a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag directly into the page with my hook inside. But, naturally, it could never be that easy.&lt;/p&gt;
    &lt;p&gt;I knew if I wanted to get this done, I would have to go down a layer.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Editor's Note: Some readers have kindly pointed out to me that this is actually possible to accomplish from an extension. However, the custom browser approach still has benefits explained later in the post :-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Chrome Devtools Protocol&lt;/head&gt;
    &lt;p&gt;The Chrome DevTools Protocol (CDP) is the low-level bridge for instrumenting, inspecting, and debugging Chromium-based browsers. Itâs what automation tools like Selenium and Playwright use under the hood. CDP exposes a large set of methods and events split across domains. The docs publish a convenient, comprehensive list of them.&lt;/p&gt;
    &lt;p&gt;While reading the domains, one method jumped out: &lt;code&gt;Page.addScriptToEvaluateOnNewDocument&lt;/code&gt;. Its description "Evaluates given script in every frame upon creation (before loading frame's scripts)" sounded like exactly the hook we needed: run code before the pageâs own scripts so we can win the prototype race.&lt;/p&gt;
    &lt;p&gt;To prove the idea I built a tiny test: a page with a script that pushes a secret value into an array, and a CDP-injected hook that tries to observe that push. If the hook sees the secret, the technique works. I chose to prototype this using Electron. I could have spoken directly to the browser over raw CDP, but Electron made wiring up a UI, IPC, and a quick demo app way faster for a weekend PoC.&lt;/p&gt;
    &lt;code&gt;const { app, BrowserWindow } = require("electron/main");

function createWindow() {
  const win = new BrowserWindow({
    width: 800,
    height: 600,
  });

  const dbg = win.webContents.debugger;
  dbg.attach("1.3");
  // Enables the Page domain so we can run the script on new document command after
  dbg.sendCommand("Page.enable");
  dbg.sendCommand("Page.addScriptToEvaluateOnNewDocument", {
    source: `(() =&amp;gt; {
        const _origPush = Array.prototype.push;
        Array.prototype.push = function (...args) {
            console.log('Array.push called on', this, 'with', args);
            return _origPush.apply(this, args);
        };
})();`,
  });
  win.webContents.openDevTools();
  win.loadURL("file:///Users/veritas/demo/index.html");
}

app.whenReady().then(() =&amp;gt; {
  createWindow();
});
&lt;/code&gt;
    &lt;p&gt;The result?:&lt;/p&gt;
    &lt;p&gt;It worked! I knew this PoC could take me far. I could hook any arbitrary global function or property and log (or spoof!) arguments and return values. The next step was building a user interface around it.&lt;/p&gt;
    &lt;p&gt;Since this started as a fun weekend project, I wanted the fastest path to a working demo. In true open-source fashion I searched for âelectron web browserâ and stumbled across electron-browser-shell by Samuel Maddock.&lt;/p&gt;
    &lt;p&gt;That project gave me an address bar, tabs, and a basic IPC-ready shell bridging the webview environment and my browser UI.&lt;/p&gt;
    &lt;p&gt;From there I added a sidebar that would display hooked function events as they fired.&lt;/p&gt;
    &lt;p&gt;To make things more interesting I needed to hook more than Array.push. A favorite target of fingerprinting scripts is the Canvas API. Sites can draw a static image to a &lt;code&gt;&amp;lt;canvas&amp;gt;&lt;/code&gt;, call &lt;code&gt;toDataURL()&lt;/code&gt; (or read pixel data), and use the resulting hash to fingerprint your GPU using subtle rendering differences. By correlating canvas hashes with other signals (user agent, installed fonts, etc.), trackers can build a surprisingly robust fingerprint. Watching and optionally spoofing these kind of calls is extremely useful for this kind of RE work.&lt;/p&gt;
    &lt;p&gt;The result looked as follows:&lt;/p&gt;
    &lt;p&gt;I was pretty happy with the direction the project was taking. The PoC actually felt useful. Remembering my previous work reverse-engineering TikTokâs web collector and how aggressively those collectors scrape client-side signals, I couldnât resist testing the hook there. I fired up the demo, pointed it at TikTok, and watched the UI for activity.&lt;/p&gt;
    &lt;p&gt;The site was pulling a decent amount of telemetry. Canvas calls (like &lt;code&gt;toDataURL&lt;/code&gt;), WebGL stats, font and plugin probes, and other subtle signals that, when combined, paint a detailed fingerprint. Seeing those calls appear in my sidebar made this project feel immediately worthwhile.&lt;/p&gt;
    &lt;p&gt;I even made sure to include all canvas operations in a secrion of the detail pane to be able to recreate a canvas if necessary.&lt;/p&gt;
    &lt;p&gt;I wanted to run this against more anti-bots. Out of curiosity I pointed the demo at a site using Cloudflareâs Turnstile. I knew Turnstile was collecting various browser signals, but to my surprise, my sidebar showed nothing. Why was I seeing zero logs?&lt;/p&gt;
    &lt;head rend="h2"&gt;OOPif(S) I did it again&lt;/head&gt;
    &lt;p&gt;Cloudflare renders the Turnstile widget inside a sandboxed iframe tucked into a closed shadow root. This iframe is an OOPIF (out-of-process iframe). It lives in a different renderer process so page-level scripts (and our injected hooks) simply wonât run there, thus, no logs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hopping the Turnstile&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2024, the M.T.A. reports to have lost a combined $568 million in unpaid bus fares and $350 million in unpaid subway fares, wait, wrong turnstile.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We needed a way to run our hooks inside those out-of-process frames. While scanning CDP I noticed the &lt;code&gt;Target.attachedToTarget&lt;/code&gt; event. It fires when the debugger auto-attaches to a new target or when you explicitly call &lt;code&gt;attachToTarget&lt;/code&gt;. This was the key: if we tell CDP to auto-attach to targets, it will notify us (and give us a &lt;code&gt;sessionId&lt;/code&gt;) for every new frame/process as it appears. With that target/session info we can evaluate code in the correct context so our hook actually runs inside OOPIFs as they spawn.&lt;/p&gt;
    &lt;code&gt;const dbg = view.webContents.debugger
dbg.on('message', async (_, method, params) =&amp;gt; {
    if (method === 'Target.attachedToTarget') {
        const { sessionId, targetInfo } = params
        // Prepare child session
        dbg.sendCommand('Runtime.enable', {}, sessionId).catch(() =&amp;gt; {})
        dbg.sendCommand('Page.enable', {}, sessionId).catch(() =&amp;gt; {})
        // Inject hook script into child frames (iframes)
        dbg.sendCommand('Page.addScriptToEvaluateOnNewDocument', { source: hook }, sessionId)
    }
})
&lt;/code&gt;
    &lt;p&gt;Tada, we have events!&lt;/p&gt;
    &lt;p&gt;Iâm not the first to hook common globals and dynamically analyze page scripts. Anti-bots are well aware of this trick and will use a variety of techniques to detect runtime JS patches, so you canât assume your wrappers will stay hidden.&lt;/p&gt;
    &lt;p&gt;How is this possible?&lt;/p&gt;
    &lt;head rend="h2"&gt;toString theory&lt;/head&gt;
    &lt;p&gt;In JavaScript, functions contain a &lt;code&gt;toString&lt;/code&gt; instance method. Let's try calling this on a native function:&lt;/p&gt;
    &lt;code&gt;const mapToString = Array.prototype.map.toString()
// returns 'function map() { [native code] }'
&lt;/code&gt;
    &lt;p&gt;This means that the implementation of this function is provided by the browser's native code. How does this look like with our hook applied?:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
const pushToString = Array.prototype.push.toString(); // Returns "function (...args) {\n  console.log('Array.push called on', this, 'with', args);\n  return _origPush.apply(this, args);\n}"
&lt;/code&gt;
    &lt;p&gt;Oh no, our hook has been discovered! Luckily for us, this is easily patched&lt;/p&gt;
    &lt;code&gt;Array.prototype.push.toString = () =&amp;gt; "function push() { [native code] }";
&lt;/code&gt;
    &lt;p&gt;Phew, that was close.&lt;/p&gt;
    &lt;code&gt;const haha = Array.prototype.push.toString.toString(); // '() =&amp;gt; "function push() { [native code] }"'
&lt;/code&gt;
    &lt;p&gt;Oh no, another leak!&lt;/p&gt;
    &lt;code&gt;Array.prototype.push.toString.toString = () =&amp;gt; "function toString() { [native code] }"; // Yay it's fixed
&lt;/code&gt;
    &lt;p&gt;Ahhh! Another one!&lt;/p&gt;
    &lt;code&gt;const haha = Array.prototype.push.toString.toString.toString.toString.toString.toString();
&lt;/code&gt;
    &lt;p&gt;Wait, you can do what now!?&lt;/p&gt;
    &lt;code&gt;const youCantEscape = Function.toString.call(Array.prototype.push); // Returns "function (...args) {\n  console.log('Array.push called on', this, 'with', args);\n  return _origPush.apply(this, args);\n}"
&lt;/code&gt;
    &lt;p&gt;These JS runtime patches turned out to be frustratingly leaky. Patch one hole and another opens. Fixes were possible, but every patch felt like a bandaid that introduced new detection vectors. see:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};

// Yay, we patched this
Array.prototype.push.toString = () =&amp;gt; "function push() { [native code] }";
Array.prototype.push.toString.toString = () =&amp;gt; "function toString() { [native code] }";

// *facepalm*
const anotherLeak = Array.prototype.push.name; // returns "" instead of "push"
&lt;/code&gt;
    &lt;p&gt;Those runtime patches were very brittle. Targets we were analyzing could detect the instrumentation and change behavior or even self destruct if they noticed they were being watched. Fixing each leak felt like an endless game of whack-a-mole, so I decided to go a layer deeper.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forking Chromium&lt;/head&gt;
    &lt;p&gt;At this point I knew I wanted to fork Chromium. I still planned to use Electron, at least for now, since Iâd already built a decent UI and didnât feel like rewriting it all in native C++. The idea was simple: fork Electron (and by extension Chromium), patch into the Blink layer where these API calls happen, and expose them somehow.&lt;/p&gt;
    &lt;p&gt;I didnât spend too long figuring out how Iâd surface those events. I was already using CDP, so why not create my own custom CDP domain and emit events from there? That way my existing Electron app could just subscribe to them like any other CDP event.&lt;/p&gt;
    &lt;p&gt;Luckily, Electron has a well-documented guide for building from source. Unluckily, building it took more than three hours on my M2 Pro Mac Mini. To make things worse, macOS 26 had broken parts of the build chain. The Metal toolchain wasnât being detected no matter what I had tried. Eventually, I hardcoded the path into the build script just to move forward. After several hours of C++ compilation errors and boredom, I finally had a locally built Electron binary running from source.&lt;/p&gt;
    &lt;p&gt;Now came the hard part: creating a custom CDP domain. The &lt;code&gt;devtools-frontend&lt;/code&gt; repository actually provides documentation on defining new protocol domains.&lt;/p&gt;
    &lt;p&gt;The gist is that protocols are defined in a &lt;code&gt;.pdl&lt;/code&gt; (Protocol Definition Language) file, specifically &lt;code&gt;browser_protocol.pdl&lt;/code&gt;. To add your own domain, you simply declare it there alongside the existing ones.&lt;/p&gt;
    &lt;p&gt;I decided to name my new domain &lt;code&gt;Snitch&lt;/code&gt;, and defined it like this:&lt;/p&gt;
    &lt;code&gt;experimental domain Snitch
  command disable
  command enable
  event toDataURLCalled
    parameters
      string dataURL
      optional string frameId
      optional string contextId
&lt;/code&gt;
    &lt;p&gt;Next, you include your new protocol files in Blinkâs build configuration, found in &lt;code&gt;third_party/blink/renderer/core/inspector/BUILD.gn&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;From there, you define an agent, the bridge that connects Blinkâs internals to the DevTools Protocol so your new CDP domain can send and receive events.&lt;/p&gt;
    &lt;p&gt;Iâll be honest, the documentation for this part was pretty lacking. The only promising link in the documentation pointed to a locked Google Doc presumably restricted to Chrome team members. They say there's no better documentation than the source code itself! By dissecting existing domains like &lt;code&gt;DOMStorage&lt;/code&gt;, &lt;code&gt;Network&lt;/code&gt;, and others, I reverse-engineered how they registered and dispatched events, then adapted that pattern for my own &lt;code&gt;Snitch&lt;/code&gt; domain.&lt;/p&gt;
    &lt;p&gt;I eventually landed on this:&lt;/p&gt;
    &lt;p&gt;snitch_agent.h&lt;/p&gt;
    &lt;code&gt;#ifndef THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_
#define THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_
#include &amp;lt;optional&amp;gt;

#include "third_party/blink/renderer/core/core_export.h"
#include "third_party/blink/renderer/core/inspector/inspector_base_agent.h"
#include "third_party/blink/renderer/core/inspector/protocol/snitch.h"

namespace blink {

class InspectedFrames;

class CORE_EXPORT SnitchAgent final
    : public InspectorBaseAgent&amp;lt;protocol::Snitch::Metainfo&amp;gt; {
 public:
  SnitchAgent(InspectedFrames*);
  SnitchAgent(const SnitchAgent&amp;amp;) = delete;
  SnitchAgent&amp;amp; operator=(const SnitchAgent&amp;amp;) = delete;
  ~SnitchAgent() override;

  void Trace(Visitor*) const override;
  protocol::Response enable() override;
  protocol::Response disable() override;

  void DidCanvasToDataURL(ExecutionContext*, const String&amp;amp; data_url,
                          const String&amp;amp; frame_id,
                          const String&amp;amp; context_id);

 private:
  Member&amp;lt;InspectedFrames&amp;gt; inspected_frames_;
  InspectorAgentState::Boolean enabled_;
};

}  // namespace blink

#endif  // THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_

&lt;/code&gt;
    &lt;p&gt;snitch_agent.cpp&lt;/p&gt;
    &lt;code&gt;#include "third_party/blink/renderer/core/inspector/snitch_agent.h"
#include "third_party/blink/renderer/core/inspector/inspected_frames.h"

namespace blink {

SnitchAgent::SnitchAgent(
  InspectedFrames* inspected_frames)
  : inspected_frames_(inspected_frames),
    enabled_(&amp;amp;agent_state_, /*default_value=*/false) {}


SnitchAgent::~SnitchAgent() = default;

void SnitchAgent::Trace(Visitor* visitor) const {
  visitor-&amp;gt;Trace(inspected_frames_);
  InspectorBaseAgent::Trace(visitor);
}

protocol::Response SnitchAgent::enable() {
  enabled_.Set(true);
  instrumenting_agents_-&amp;gt;AddSnitchAgent(this);
  return protocol::Response::Success();
}

protocol::Response SnitchAgent::disable() {
  enabled_.Clear();
  instrumenting_agents_-&amp;gt;RemoveSnitchAgent(this);
  return protocol::Response::Success();
}

void SnitchAgent::DidCanvasToDataURL(ExecutionContext* context, const String&amp;amp; data_url,
                                     const String&amp;amp; frame_id,
                                     const String&amp;amp; context_id) {

  if (!enabled_.Get()) {
    return;
  }

  std::optional&amp;lt;String&amp;gt; maybe_frame;
  if (!frame_id.empty()) {
    maybe_frame = frame_id;
  }

  std::optional&amp;lt;String&amp;gt; maybe_ctx;
  if (!context_id.empty()) {
    maybe_ctx = context_id;
  }

  GetFrontend()-&amp;gt;toDataURLCalled(data_url, maybe_frame, maybe_ctx);
}

}  // namespace blink

&lt;/code&gt;
    &lt;p&gt;Now I needed a way to trigger my new event from the native C++ implementation of &lt;code&gt;toDataURL&lt;/code&gt;. The implementation for that function lives in &lt;code&gt;src/third_party/blink/renderer/core/html/canvas/html_canvas_element.cc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;While digging through how other events were dispatched, I noticed something interesting. The agents werenât called directly. Instead, events were emitted through probes. These probes act as intermediary hooks that Blink uses to fire instrumentation events into the DevTools pipeline.&lt;/p&gt;
    &lt;p&gt;Hereâs a comment from that same class showing how a probe fires when a canvas element is created:&lt;/p&gt;
    &lt;code&gt;CanvasRenderingContext* HTMLCanvasElement::GetCanvasRenderingContextInternal(
    ExecutionContext* execution_context,
    const String&amp;amp; type,
    const CanvasContextCreationAttributesCore&amp;amp; attributes) {
  CanvasRenderingContext::CanvasRenderingAPI rendering_api =
      CanvasRenderingContext::RenderingAPIFromId(type);

  // ...

  CanvasRenderingContextFactory* factory =
      GetRenderingContextFactory(static_cast&amp;lt;int&amp;gt;(rendering_api));

  // Tell the debugger about the attempt to create a canvas context
  // even if it will fail, to ease debugging.
  probe::DidCreateCanvasContext(&amp;amp;GetDocument());
  // ...
}  
&lt;/code&gt;
    &lt;p&gt;These probes are defined in a file called &lt;code&gt;core_probes.pidl&lt;/code&gt;. The comment at the top of this file states:&lt;/p&gt;
    &lt;code&gt;/*
 * make_instrumenting_probes.py uses this file as a source to generate
 * core_probes_inl.h, core_probes_impl.cc and core_probe_sink.h.
 *
 * The code below is not a correct IDL but a mix of IDL and C++.
 *
 * The syntax for an instrumentation method is as follows:
 *
 *    returnValue methodName([paramAttr1] param1, [paramAttr2] param2, ...)
&lt;/code&gt;
    &lt;p&gt;Following this syntax, I added my custom probe:&lt;/p&gt;
    &lt;code&gt;void DidCanvasToDataURL([Keep] ExecutionContext*, String&amp;amp; data_url, String&amp;amp; frame_id, String&amp;amp; context_id);
&lt;/code&gt;
    &lt;p&gt;A similarly named &lt;code&gt;core_probes.json5&lt;/code&gt; holds the mappings of which agents are responsible for which probes. We can add our entry as such:&lt;/p&gt;
    &lt;code&gt;{
    observers: {
        // ...,
        SnitchAgent: {
            probes: ["DidCanvasToDataURL"]
        }
        // ...
    }
}
&lt;/code&gt;
    &lt;p&gt;The final step in adding our custom domain is to register the agent in &lt;code&gt;WebDevToolsAgentImpl::AttachSession&lt;/code&gt; like so:&lt;/p&gt;
    &lt;code&gt;session-&amp;gt;CreateAndAppend&amp;lt;SnitchAgent&amp;gt;(inspected_frames);
&lt;/code&gt;
    &lt;p&gt;and actually calling it in the implementation of &lt;code&gt;toDataURL&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;String HTMLCanvasElement::toDataURL(const String&amp;amp; mime_type,
                                    const ScriptValue&amp;amp; quality_argument,
                                    ExceptionState&amp;amp; exception_state) const {
  // ...
  String data_url = ToDataURLInternal(mime_type, quality, kBackBuffer,
                                      ReadbackType::kWebExposed);

  // // Hook up call to our new CDP event (Snitch.toDataURLCalled)
  probe::DidCanvasToDataURL(GetExecutionContext(), data_url, frame_id, context_id);

  return data_url;
}
&lt;/code&gt;
    &lt;p&gt;After accidentally nuking my local repository and having to restart the entire process, including sitting through another 3 hour compilation ð¥², it was ready to test!&lt;/p&gt;
    &lt;p&gt;We can create a simple Electron test application:&lt;/p&gt;
    &lt;code&gt;const { app, BrowserWindow } = require("electron/main");

function createWindow() {
  const win = new BrowserWindow({
    width: 800,
    height: 600,
  });

  const dbg = win.webContents.debugger;
  dbg.attach("1.3");
  dbg.sendCommand("Snitch.enable");
  dbg.on("message", (_, method, { dataURL }) =&amp;gt; {
    if (method === "Snitch.toDataURLCalled") {
      console.log("toDataURL called", dataURL);
    }
  });
  win.loadURL("https://demo.fingerprint.com/playground");
}

app.whenReady().then(() =&amp;gt; {
  createWindow();
});
&lt;/code&gt;
    &lt;p&gt;and run it pointing to our custom Electron build:&lt;/p&gt;
    &lt;code&gt;$ /Users/veritas/electron/src/out/Testing/Electron.app/Contents/MacOS/Electron demo.js
&lt;/code&gt;
    &lt;p&gt;Drumroll, please!&lt;/p&gt;
    &lt;p&gt;It worked! We can see our custom CDP event firing and returning to us the result of a toDataURL call on FingerprintJS' playground. We can now use these stealthy CDP events and not leak the fact that we're instrumenting these functions.&lt;/p&gt;
    &lt;p&gt;Note: Depending on what we do in these hooks, it may still be possible to detect us through any side-effects we introduce or potentially through timing checks (Is the function slower than it would usually be?).&lt;/p&gt;
    &lt;head rend="h2"&gt;Extras&lt;/head&gt;
    &lt;p&gt;This was powerful, but I wanted more. I needed a few extra tools to make this thing a real web reverse-engineering Swiss Army knife.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deobfuscation&lt;/head&gt;
    &lt;p&gt;One of the biggest time sinks in this kind of work is dealing with obfuscated scripts. I wanted a built-in tool that could automatically detect and attempt to deobfuscate scripts as they load. Using CDPâs &lt;code&gt;Network&lt;/code&gt; domain, I intercept incoming JavaScript files and run a few lightweight heuristics to score their likelihood of being obfuscated. Suspicious ones are displayed in a separate tab, where I integrate tools like bensbâs deobfuscate.io to automatically try recovering a more readable version. The plan is to add more tools such as Webcrack and even custom deobfuscators of my own.&lt;/p&gt;
    &lt;p&gt;I also added a section that extracts and displays recovered string literals from the processed script for added speed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overwriting properties and functions&lt;/head&gt;
    &lt;p&gt;Hooking and reading is fun, but sometimes you want to change behavior. You now know that doing so in a browser environment, across OOPIFs and with anti-tamper checks is non-trivial. I built an Overrides tab where you can define custom JavaScript snippets that overwrite functions or properties across all frames. These execute without triggering common integrity checks, giving a clean way to spoof or alter these values.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fingerprint payload decryption&lt;/head&gt;
    &lt;p&gt;My bread and butter is dissecting anti-bot and fingerprinting scripts. These scripts often encrypt or encode their payloads before sending them to backend validators, which makes analysis painful. To make life easier, I added a feature that detects known collectors and automatically intercepts their outbound requests. It decrypts (or decodes) the payloads and displays both the plaintext and structured data in a neat table view.&lt;/p&gt;
    &lt;p&gt;Of course, each collector still needs to be reverse-engineered beforehand. Maybe this is where AI-assisted payload analysis could step in someday? Maybe, but for now I will continue to hand-roll my own parsers :-)&lt;/p&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;Iâm really happy with how this project has evolved. Itâs gone from my quick weekend curiosity to a genuinely useful research tool. Still, I have a few major goals remain before I can say I'm truly proud:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Abandon Electron&lt;/p&gt;&lt;lb/&gt;Electron was great for rapid prototyping, but it is heavy and adds its own leaks. Theyâre fixable, sure, but the cleaner path is to embed the UI directly inside Chromium. Iâm looking into how other Chromium forks (like Brave) integrate their native UIs and exploring whether I can do the same.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Hook all the things&lt;/p&gt;&lt;lb/&gt;Iâve already implemented a broad set of hooks. Canvas, WebGL, audio fingerprinting,&lt;code&gt;navigator&lt;/code&gt;accessors, document and window properties, and more. But can we hook everything?&lt;lb/&gt;Iâve experimented with injecting hooks deeper in V8 where function calls are dispatched, however, V8âs optimizations quickly complicate things. Disabling those optimizations would work but at the cost of performance (and thus introducing timing leaks). Another idea is to modify the IDL code generator to automatically insert hooks during buildtime. This is likely the approach I will take.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Release?&lt;/p&gt;&lt;lb/&gt;I havenât decided what to do once itâs ready. Maybe open source it? Would others find it useful? Was this all built into Chromium this entire time under some obscure setting that I missed? Who knows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And with that, I present to you a gallery of canvas fingerprint images that I've collected during this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fingerprint Gallery&lt;/head&gt;
    &lt;head rend="h3"&gt;Tiktok&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
const gradient0 = context.createLinearGradient(10, 0, 180, 1)
gradient0.addColorStop(0, "red")
gradient0.addColorStop(0.1, "white")
gradient0.addColorStop(0.2, "blue")
gradient0.addColorStop(0.3, "yellow")
gradient0.addColorStop(0.4, "purple")
gradient0.addColorStop(0.7, "orange")
gradient0.addColorStop(1, "magenta")
context.fillStyle = gradient0
context.fillRect(0, 10, 100, 6)
const gradient1 = context.createLinearGradient(0, 0, 100, 100)
gradient1.addColorStop(0, "green")
gradient1.addColorStop(0.5, "yellow")
gradient1.addColorStop(0.7, "orange")
gradient1.addColorStop(1, "magenta")
context.beginPath()
context.fillStyle = gradient1
context.arc(50, 10, 25, 0, 6.283185307179586)
context.stroke()
context.fillStyle = "rgba(150, 32, 170, .97)"
context.font = "12px Sans"
context.textBaseline = "top"
context.fillText("*+(}#?ð¼ ð", 18, 18)
context.shadowBlur = 1
context.fillStyle = "rgba(47, 211, 69, .99)"
context.font = "14px Sans"
context.textBaseline = "top"
context.fillText("ð¼OynG@%tp$", 3, 3)
context.beginPath()
context.arc(30, 10, 20, 0, 6.283185307179586)
context.strokeStyle = "rgba(255, 12, 220, 1)"
context.stroke()
&lt;/code&gt;
    &lt;head rend="h3"&gt;FingerprintJS&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
context.rect(0, 0, 10, 10)
context.rect(2, 2, 6, 6)
context.isPointInPath(5, 5, "evenodd")
context.textBaseline = "alphabetic"
context.fillStyle = "#f60"
context.fillRect(100, 1, 62, 20)
context.fillStyle = "#069"
context.font = "11pt \"Times New Roman\""
context.fillText("Cwm fjordbank gly ð", 2, 15)
context.fillStyle = "rgba(102, 204, 0, 0.2)"
context.font = "18pt Arial"
context.fillText("Cwm fjordbank gly ð", 4, 45)
&lt;/code&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
context.globalCompositeOperation = "multiply"
context.fillStyle = "#f2f"
context.beginPath()
context.arc(40, 40, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#2ff"
context.beginPath()
context.arc(80, 40, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#ff2"
context.beginPath()
context.arc(60, 80, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#f9c"
context.arc(60, 60, 60, 0, 6.283185307179586, true)
context.arc(60, 60, 20, 0, 6.283185307179586, true)
context.fill("evenodd")
&lt;/code&gt;
    &lt;head rend="h3"&gt;Cloudflare&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
const gradient0 = context.createRadialGradient(33, 18, 8, 42, 10, 226)
gradient0.addColorStop(0, "#809900")
gradient0.addColorStop(1, "#404041")
context.fillStyle = gradient0
context.shadowBlur = 11
context.shadowColor = "#F38020"
context.beginPath()
context.moveTo(9, 14)
context.quadraticCurveTo(93, 48, 116, 111)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient1 = context.createRadialGradient(77, 98, 2, 27, 30, 206)
gradient1.addColorStop(0, "#809900")
gradient1.addColorStop(1, "#404041")
context.fillStyle = gradient1
context.beginPath()
context.ellipse(58, 55, 31, 28, 1.4441705959829747, 0.5401125108618993, 4.052233984744969)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient2 = context.createRadialGradient(108, 12, 10, 65, 118, 169)
gradient2.addColorStop(0, "#1AB399")
gradient2.addColorStop(1, "#E666B3")
context.fillStyle = gradient2
context.shadowBlur = 16
context.shadowColor = "#809980"
context.font = "27.77777777777778px aanotafontaa"
context.fillText("Ry", 13, 67)
context.shadowBlur = 0
const gradient3 = context.createRadialGradient(46, 47, 0, 101, 108, 207)
gradient3.addColorStop(0, "#4DB380")
gradient3.addColorStop(1, "#FF4D4D")
context.fillStyle = gradient3
context.shadowBlur = 3
context.shadowColor = "#FF6633"
context.beginPath()
context.moveTo(54, 5)
context.bezierCurveTo(54, 90, 32, 74, 71, 120)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient4 = context.createRadialGradient(119, 123, 3, 109, 90, 137)
gradient4.addColorStop(0, "#E6B333")
gradient4.addColorStop(1, "#3366E6")
context.fillStyle = gradient4
context.shadowBlur = 4
context.shadowColor = "#B3B31A"
context.beginPath()
context.moveTo(76, 0)
context.bezierCurveTo(1, 49, 103, 67, 49, 125)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient5 = context.createRadialGradient(34, 47, 1, 37, 59, 245)
gradient5.addColorStop(0, "#809900")
gradient5.addColorStop(1, "#404041")
context.fillStyle = gradient5
context.beginPath()
context.ellipse(56, 57, 14, 8, 1.2273132071162383, 4.1926143018618225, 2.8853539230051624)
context.stroke()
context.fill()
context.shadowBlur = 0
context.shadowBlur = 14
context.shadowColor = "#809900"
context.font = "11.904761904761905px aanotafontaa"
context.strokeText("@H1", 30, 73)
context.shadowBlur = 0;
&lt;/code&gt;
    &lt;head rend="h2"&gt;Until next time&lt;/head&gt;
    &lt;p&gt;I'd love to know if you found this even remotely interesting or think it's just a giant waste of time :-) I certainly had fun building it and had even more fun using it&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits&lt;/head&gt;
    &lt;p&gt;pimothyxd: Helped with the design of the UI! Always someone I can depend on.&lt;/p&gt;
    &lt;p&gt;bensb: Used his deobfuscator for the scripts tab. Also very knowledgable and a great person to chat ideas with.&lt;/p&gt;
    &lt;p&gt;samuelmaddock: Your electron-browser-shell project made it very easy to get this spun up.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nullpt.rs/reverse-engineering-browser"/><published>2025-10-07T17:03:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45505854</id><title>ICE bought vehicles equipped with fake cell towers to spy on phones</title><updated>2025-10-07T19:32:03.043175+00:00</updated><content>&lt;doc fingerprint="be3c0913216837ed"&gt;
  &lt;main&gt;
    &lt;p&gt;U.S. Immigration and Customs Enforcement (ICE) paid $825,000 earlier this year to a company that manufactures vehicles equipped with various technologies for law enforcement, including fake cellphone towers known as “cell-site simulators,” which can be used to spy on nearby phones.&lt;/p&gt;
    &lt;p&gt;According to public records, the award dated May 8 “provides Cell Site Simulator (CSS) Vehicles to support the Homeland Security Technical Operations program” and is a modification for “additional CSS Vehicles.”&lt;/p&gt;
    &lt;p&gt;The contract was signed with TechOps Specialty Vehicles (TOSV), a Maryland-based company. TOSV also signed a similar contract with ICE in September 2024 for $818,000, showing that the relationship between the agency and the company predates the Trump administration.&lt;/p&gt;
    &lt;p&gt;TOSV president Jon Brianas told TechCrunch in an email that he could not provide details about the ICE contracts and the vehicles, citing “trade secrets.” But Brianas did confirm that the company does provide cell-site simulators, although it does not make them.&lt;/p&gt;
    &lt;p&gt;“We don’t manufacture electrical, comms, and technology components, we integrate that product into our overall design of the vehicle,” said Brianas, who declined to say from where TOSV sources its cell-site simulators.&lt;/p&gt;
    &lt;p&gt;This is the latest federal contract that reveals some of the technologies powering the Trump administration’s deportation crackdown.&lt;/p&gt;
    &lt;p&gt;In early September, Forbes found a recently unsealed search warrant that showed that ICE used a cell-site simulator to track down a person who allegedly was part of a criminal gang in the United States, and who had been ordered to leave the country in 2023. In the article, Forbes reported that it also found a contract for “cell site simulator vehicles,” but the article did not name the company that provides the vans to the agency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Join 10k+ tech and VC leaders for growth and connections at Disrupt 2025&lt;/head&gt;
    &lt;head rend="h4"&gt;Netflix, Box, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla — just some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch, and a chance to learn from the top voices in tech. Grab your ticket before doors open to save up to $444.&lt;/head&gt;
    &lt;head rend="h3"&gt;Join 10k+ tech and VC leaders for growth and connections at Disrupt 2025&lt;/head&gt;
    &lt;head rend="h4"&gt;Netflix, Box, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla — just some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss a chance to learn from the top voices in tech. Grab your ticket before doors open to save up to $444.&lt;/head&gt;
    &lt;p&gt;Cell-site simulators also go by the name “stingrays” because some of the earlier types of these devices, made by defense contractor Harris (now L3Harris), were named that way. Since then, stingrays have become a catch-all name for this type of technology, also known as IMSI catchers. (IMSI stands for International Mobile Subscriber Identity, a unique number that identifies every cellphone user in the world.)&lt;/p&gt;
    &lt;p&gt;As the name suggests, cell-site simulator tools can mimic a cellphone tower, tricking every phone in its nearby range to connect to the device and thus giving law enforcement the ability to better identify the real-world location of those phones and their owners.&lt;/p&gt;
    &lt;p&gt;Some cell-site simulators can also intercept regular calls, text messages, and internet traffic.&lt;/p&gt;
    &lt;p&gt;Authorities can get data from traditional cellphone towers to find the current or past location of a suspect, but the location is usually not very precise.&lt;/p&gt;
    &lt;p&gt;Stingray-like devices have been in use by law enforcement for more than a decade and have long been controversial because authorities do not always get a warrant for their use, and critics say these devices ensnare innocent people by default. These devices are also shrouded in secrecy, because the law enforcement agencies that use them are under strict non-disclosure agreements not to reveal how the devices work.&lt;/p&gt;
    &lt;p&gt;ICE has a long history of using cell-site simulators. In 2020, documents obtained by the American Civil Liberties Union showed that ICE deployed them at least 466 times between 2017 and 2019. The agency used these tools more than 1,885 times between 2013 and 2017, according to documents obtained by BuzzFeed News at the time.&lt;/p&gt;
    &lt;p&gt;ICE acknowledged TechCrunch’s request for comment, but did not respond to a series of questions, which included: what ICE uses these vehicles for, whether and where they have recently been deployed, and whether the agency always gets a warrant when using cell-site simulators.&lt;/p&gt;
    &lt;head rend="h2"&gt;From surveillance vans to bookmobiles&lt;/head&gt;
    &lt;p&gt;Headquartered just outside of Washington, DC, TOSV sells a wide range of customizable vehicles to law enforcement, such as vans for SWAT armed response teams, bomb squads, and so-called “mobile lab” and “cover surveillance” vehicles.&lt;/p&gt;
    &lt;p&gt;Among these vehicles for police forces, TOSV lists several “projects,” including one described as DHS Mobile Forensic Labs, referring to the Department of Homeland Security.&lt;/p&gt;
    &lt;p&gt;According to the website, these mobile forensic vans are “equipped for on-site forensic analysis and documentation,” have “secure compartments for evidence preservation and investigative tools,” and enable “seamless case file updates and evidence logging.”&lt;/p&gt;
    &lt;p&gt;Another project is the DHS Mobile Command Van,” which TOSV says is “configurable for advanced surveillance and mission coordination.”&lt;/p&gt;
    &lt;p&gt;It’s unclear if these vans are the same vehicles that include cell-site simulators, as there’s no mention of the phone surveillance tool anywhere on TOSV’s website.&lt;/p&gt;
    &lt;p&gt;ICE has other contracts with TOSV for mobile forensic labs, which don’t specify which technologies are located in the vans.&lt;/p&gt;
    &lt;p&gt;According to its website, TOSV also sells so-called “bookmobiles,” which appear to be libraries on wheels, as well as medical and fire department vehicles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2025/10/07/ice-bought-vehicles-equipped-with-fake-cell-towers-to-spy-on-phones/"/><published>2025-10-07T17:12:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45506143</id><title>German government comes out against Chat Control</title><updated>2025-10-07T19:32:02.548228+00:00</updated><content>&lt;doc fingerprint="77d803d92c0426bd"&gt;
  &lt;main&gt;
    &lt;p&gt;Great news and big win for privacy in the EU! 🇪🇺🇩🇪 Germany’s ruling CDU/CSU party made it clear today: there will be no chat control - as pushed for by other EU countries - with this German government.&lt;/p&gt;
    &lt;p&gt;40 Sekunden kurz und präzise: Mit der CDU/CSU wird es keine anlasslose Chatkontrolle geben, wie sie von einigen Staaten in der EU gefordert wird.&lt;/p&gt;
    &lt;p&gt;Oct 7, 2025 · 4:13 PM UTC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xcancel.com/paddi_hansen/status/1975595307800142205"/><published>2025-10-07T17:31:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45506268</id><title>Less Is More: Recursive Reasoning with Tiny Networks</title><updated>2025-10-07T19:32:02.119512+00:00</updated><content>&lt;doc fingerprint="3cbf352bcdf5c28f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 6 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Less is More: Recursive Reasoning with Tiny Networks&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Alexia Jolicoeur-Martineau [view email]&lt;p&gt;[v1] Mon, 6 Oct 2025 14:58:08 UTC (259 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2510.04871"/><published>2025-10-07T17:42:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45506365</id><title>Solar energy is now the cheapest source of power, study</title><updated>2025-10-07T19:31:55.949735+00:00</updated><content>&lt;doc fingerprint="a545355d0abc55b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Solar energy is now the world’s cheapest source of power, a Surrey study finds&lt;/head&gt;
    &lt;p&gt;Solar energy is now so cost-effective that, in the sunniest countries, it costs as little as £0.02 to produce one unit of power, making it cheaper than electricity generated from coal, gas or wind, according to a new study from the University of Surrey.&lt;/p&gt;
    &lt;p&gt;In a study published in Energy and Environment Materials, researchers from Surrey’s Advanced Technology Institute (ATI) argue that solar photovoltaic (PV) technology is now the key driver of the world’s transition to clean, renewable power.&lt;/p&gt;
    &lt;p&gt;The research team also found that the price of lithium-ion batteries has fallen by 89% since 2010, making solar-plus-storage systems as cost-effective as gas power plants. These hybrid setups, which combine solar panels with batteries, are now standard in many regions and allow solar energy to be stored and released when needed, turning it into a more reliable, dispatchable source of power that helps balance grid demand.&lt;/p&gt;
    &lt;p&gt;Despite many reasons to be optimistic, the ATI research team points to several challenges – particularly connecting large amounts of solar power to existing electricity networks. In some regions, such as California and China, high solar generation has led to grid congestion and wasted energy when supply exceeds demand.&lt;/p&gt;
    &lt;p&gt;###&lt;/p&gt;
    &lt;p&gt;Notes to editors&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Professor Ravi Silva is available for interview; please contact mediarelations@surrey.ac.uk to arrange.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The full paper can be found here.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Related sustainable development goals&lt;/head&gt;
    &lt;head rend="h2"&gt;Featured Academics&lt;/head&gt;
    &lt;head rend="h2"&gt;Media Contacts&lt;/head&gt;
    &lt;p&gt;External Communications and PR team&lt;lb/&gt; Phone: +44 (0)1483 684380 / 688914 / 684378&lt;lb/&gt; Email: mediarelations@surrey.ac.uk&lt;lb/&gt; Out of hours: +44 (0)7773 479911&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.surrey.ac.uk/news/solar-energy-now-worlds-cheapest-source-power-surrey-study-finds"/><published>2025-10-07T17:50:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45507173</id><title>Google's Requirement for Developers to Be Verified Threatens App Store F-Droid</title><updated>2025-10-07T19:31:55.799079+00:00</updated><content>&lt;doc fingerprint="be60b11190975461"&gt;
  &lt;main&gt;
    &lt;p&gt;Google back then: “Don’t be evil.”&lt;/p&gt;
    &lt;p&gt;Google now: “Don’t be stupid by being good.”&lt;/p&gt;
    &lt;p&gt;It would be something of an understatement to say that Alphabet, Google’s holding company, is big and successful. Some Wall Street analysts are even predicting it could become the world’s most valuable corporation. Of course, even for business giants, enough is never enough. They always want more: more money, more power. As part of that tendency, Google seems to have decided that F-Droid, the free and open source app store for the Android platform, is a threat to the official Google Play Store that needs to be neutralized. At least that is likely to be the effect of Google’s announcement that it will require all Android developers to register and be verified before their apps can be allowed to run on certified Android devices. A post on the F-Droid blog explains what the problem is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In addition to demanding payment of a registration fee and agreement to their (non-negotiable and ever-changing) terms and conditions, Google will also require the uploading of personally identifying documents, including government ID, by the authors of the software, as well as enumerating all the unique “application identifiers” for every app that is to be distributed by the registered developer.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;According to the blog post, the impact on the F-Droid project would be severe:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;the developer registration decree will end the F-Droid project and other free/open-source app distribution sources as we know them today, and the world will be deprived of the safety and security of the catalog of thousands of apps that can be trusted and verified by any and all. F-Droid’s myriad users will be left adrift, with no means to install — or even update their existing installed — applications.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Google says registration is needed to “better protect users from repeat bad actors spreading malware and scams”. Registration “creates crucial accountability, making it much harder for malicious actors to quickly distribute another harmful app after we take the first one down.” Slightly less convenient, perhaps, but not much harder. The F-Droid blog post points out that its open source app store already has a far better approach to security than Google’s proposed registration and verification:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;every [F-Droid] app is free and open source, the code can be audited by anyone, the build process and logs are public, and reproducible builds ensure that what is published matches the source code exactly. This transparency and accountability provides a stronger basis for trust than closed platforms, while still giving users freedom to choose. Restricting direct app installation not only undermines that choice, it also erodes the diversity and resilience of the open-source ecosystem by consolidating control in the hands of a few corporate players.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Google is at pains to emphasize “Verified developers will have the same freedom to distribute their apps directly to users through sideloading or through any app store they prefer.” But that’s not true: their “freedom” will be soon be conditional, subject to Google’s whim and veto (as the company’s recent removal of the ICE-spotting app ‘Red Dot’ demonstrates). As a special concession, the company says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;we are also introducing a free developer account type that will allow teachers, students, and hobbyists to distribute apps to a limited number of devices without needing to provide a government ID.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But again that is subject to Google’s approval, and only allows distribution to a “limited number of devices” – a circumscribed “freedom”, in other words. And for F-Droid it’s not even an option, because of the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How many F-Droid users are there, exactly? We don’t know, because we don’t track users or have any registration: “No user accounts, by design”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As the F-Droid post comments, Google’s move is not credibly about “security”, but actually about “consolidating power and tightening control over a formerly open ecosystem”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you own a computer, you should have the right to run whatever programs you want on it. This is just as true with the apps on your Android/iPhone mobile device as it is with the applications on your Linux/Mac/Windows desktop or server. Forcing software creators into a centralized registration scheme in order to publish and distribute their works is as egregious as forcing writers and artists to register with a central authority in order to be able to distribute their creative works. It is an offense to the core principles of free speech and thought that are central to the workings of democratic societies around the world.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Google’s attack on F-Droid is ironic. At the heart of Android, and the key element that allowed it to become so successful so quickly, is the GPL-licensed Linux kernel. Over the years, Google has increased its control over Android by adding more non-free elements. If, as seems likely, its latest move leads to the shutdown of the 15-year-old F-Droid platform, it would represent a further betrayal of the open source world it once supported.&lt;/p&gt;
    &lt;p&gt; Filed Under: android, f-droid, freedom, id, linux, macintosh, malware, open source, registration, reproducibility, scams, security, verification, wall street, windows &lt;lb/&gt; Companies: alphabet, google &lt;/p&gt;
    &lt;p&gt;Google back then: “Don’t be evil.”&lt;/p&gt;
    &lt;p&gt;Google now: “Don’t be stupid by being good.”&lt;/p&gt;
    &lt;p&gt;They blessed our anti-trust and we’ve bent the knee to trump, now is the time to shine at being bastards!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.techdirt.com/2025/10/07/googles-requirement-for-all-android-developers-to-register-and-be-verified-threatens-to-close-down-open-source-app-store-f-droid/"/><published>2025-10-07T18:51:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45507195</id><title>The murky economics of the data-centre investment boom</title><updated>2025-10-07T19:31:55.680855+00:00</updated><content/><link href="https://www.economist.com/business/2025/09/30/the-murky-economics-of-the-data-centre-investment-boom"/><published>2025-10-07T18:52:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45507236</id><title>The Publishing Industry Has a Gambling Problem</title><updated>2025-10-07T19:31:55.386658+00:00</updated><content>&lt;doc fingerprint="342080157fa50dc9"&gt;
  &lt;main&gt;
    &lt;p&gt;In 1970, a New York publishing company put out a debut novel by an editor and former teacher from Ohio. The press, then known as Holt, Rinehart and Winston, had taken a chance on the book, which had been rejected by numerous other houses. The initial print run was somewhere between 1,200 and 1,500 units—modest expectations that looked justified when, in the first year, sales barely cleared 2,000. This despite getting positive reviews in the New York Times and The New Yorker and being assigned to freshman classes at the City College of New York. The attention wasn’t enough. Four years later, the novel was out of print.&lt;/p&gt;
    &lt;p&gt;The author stayed in the game, albeit precariously. While working on her second book, she was a single parent commuting to Manhattan for a job in publishing. At the time, she was “so strapped for money that the condition moved from debilitating stress to hilarity.” Despite her first book’s lacklustre sales, she found a publisher for her second. The debut had attracted the admiration of a high-profile editor, one who happened to work in the same building she did. He acquired her next title, and the next, keeping her in house as she steadily built acclaim and an audience.&lt;/p&gt;
    &lt;p&gt;Eventually, the writer scored an opportunity still regarded as a grail of book marketing: her debut was chosen for Oprah’s Book Club. Sales reportedly soared to 800,000 copies. Today, publishers hope that their titles will nab the book club stamp—and the ensuing bump in sales—straight out of the gate. But, in this case, the Oprah endorsement came only at the turn of the millennium, thirty years after the novel was first released. By then, the author had published some half dozen other books and cleared the stable of major literary accolades. She had won the National Book Award, the Pulitzer, the Nobel. The author was Toni Morrison. The novel was The Bluest Eye.&lt;/p&gt;
    &lt;p&gt;The careers of many literary titans of the late-twentieth and early twenty-first centuries bear similar hallmarks: The disappointing debut. The stalwart editorial advocate. The understanding that, in order for a writer to truly break out, time is a meaningful factor. For every author whose first try strikes gold—like Philip Roth, whose debut won him the National Book Award at twenty-seven—there’s one like Morrison—or Cormac McCarthy, or Jack Kerouac, or 2025’s Pulitzer Prize–winning fiction writer Percival Everett—on whom a publisher had to take a second, or third, or fourth, or fifth chance. In 1993, reflecting on The Bluest Eye’s reception, Morrison noted its initial life had echoed that of its young Black protagonist, Pecola Breedlove: “dismissed, trivialized, [and] misread.” Nevertheless, the novel is now an essential part of a legacy that reshaped literature.&lt;/p&gt;
    &lt;p&gt;Nowadays, it might not get that opportunity. It’s true most debuts are not, aesthetically, The Bluest Eye. But nor are they as easily granted second chances after commercial disappointment. Instead, there is tremendous pressure to succeed from the beginning. If they fail, all bets are off, sometimes literally. Countless factors contribute to how well a book sells, and there are many points in that chain at which things can break down. If they do, much of the responsibility converges on the writer. That a publisher bet on them and lost means it will be harder to secure the next deal. No matter the reasons for the flop—a tiny marketing budget, staff turnover at the press, cutbacks in culture coverage, backlash toward a hot literary trend—the writer carries the failure on their record.&lt;/p&gt;
    &lt;p&gt;Sales track—or simply track, in industry parlance—is an invisible force shaping contemporary literature. Much depends on that number. On the basis of track, published authors struggle to keep going; those just starting out fear their careers will be severed at the root. Track shapes how an agent pitches a book and how editors assess whether to buy it. Track restricts reader choice by dictating which books are served up as the next big thing (and the next, and the next) and by kneecapping writers deemed insufficiently commercial. The primacy of track, in other words, is a barometer for the health of literary culture. Right now, when the industry is especially skittish, the obsession with finding the next blockbuster hit privileges the survival of the few at the expense of the many.&lt;/p&gt;
    &lt;p&gt;Track is like credit: it might be better to have none at all. When a writer has a book that’s ready to sell, their agent takes the manuscript or proposal out on submission by pitching it to editors. If an editor is interested, they will in turn pitch the project to their company for approval. One of the things that publishing teams look at, when evaluating a book for potential acquisition, is a writer’s past sales. Using tools like BookScan, the industry’s pricey software for tracking units sold, publishers gauge how a previous title fared and whether the author warrants further investment. (BookNet, the equivalent in Canada, is a nonprofit. The data provided by both is incomplete.)&lt;/p&gt;
    &lt;p&gt;Being trailed by one’s sales data gives first-time writers a certain advantage. Debuts are deeply attractive to publishers because, as writer and researcher Laura McGrath puts it, “there is nothing but potential. If your track is zero, there’s only one place for it to go.” The book’s advance is therefore set by anticipation—the publisher’s bid is roughly commensurate with how big they think they can break it out. They reach this number by assigning a value to what McGrath, who studies publishing analytics, calls “soft data”—a bouquet of assumptions about readership, authorship, markets, and genre. Those assumptions are then “turned into something that seems like it should have been arrived upon in a rigorous fashion,” she says, “but it’s not.” If enough bidders get ensorcelled by a project—or by the bloodlust of an auction—the price can be driven up into six or seven figures. The book business may be centred in New York, but the logic is pure Las Vegas.&lt;/p&gt;
    &lt;p&gt;“These books that have huge price tags are given impossible expectations to meet.”&lt;/p&gt;
    &lt;p&gt;If buying the debut is a rollicking night at the craps table, then the sophomore project is the sober morning after. Gone is the clean slate. What publishers really want to see, McGrath says, is growth. “More than any particular number, they’re looking to see a track that is always on the rise.” This is impossible to prove after only one book, especially a book that loses the publisher money. Which is to say: almost all of them. “Most books don’t sell well,” says Alia Hanna Habib, literary agent and author of the forthcoming Take It from Me, a career guide for non-fiction writers. (She counts McGrath among her clients.)&lt;/p&gt;
    &lt;p&gt;Because the majority of books don’t earn out, most people in publishing have the disappointing experience of working on a book they love that, for whatever reason, didn’t hit: “If you’re a fair person, you know it’s not the author’s fault. It’s just the realities of a very difficult market.” Habib won’t suddenly drop a client whose first book didn’t sell. At the same time, that track creates challenges for her. She must come up with a narrative to explain the failure and a case for how the next book might do better. Sometimes, the original publisher wants to move on, so she also has to find someone else willing to take a chance.&lt;/p&gt;
    &lt;p&gt;“When I get sent a project, one of the first things I’ll do is look at the track,” says an editor from an imprint at one of the Big Five presses—the largest, corporate-owned trade publishers: Penguin Random House, Simon &amp;amp; Schuster, HarperCollins, Hachette, and Macmillan—who asked to remain anonymous. Though he evaluates every submission on its merits, he must balance his enthusiasm with practical considerations. Track becomes either an asset to his case for acquiring a book or a hurdle he must overcome by crafting a compelling strategy to convince his team it’s still worth buying.&lt;/p&gt;
    &lt;p&gt;Bad track won’t stop him from considering a project, especially one he is passionate about. Instead, he weighs the factors that may have led to it—maybe the book’s editor was laid off and the author did not receive as much attention. He’ll even reach out to the agent to learn more about what happened. Those conversations can reveal subtler things about how a book was not well supported. Perhaps the publisher positioned it in a way the author disagreed with—a risk, he says, with projects that feature diverse protagonists or are written from a very specific perspective. “Publishers are very fallible,” he says. “Sometimes an author needs a fresh start.”&lt;/p&gt;
    &lt;p&gt;This is another quirk of track. Publishing’s habit of jumping on a trend, especially if that trend is identity based, can come down hard on writers who have been underrepresented in mainstream culture. It can even set them up for failure down the line. Habib cites the moment in 2020 when presses eagerly began acquiring books by Black authors. Many of those presses had never published Black authors in a meaningful way and lacked the infrastructure to properly support those books or help them find readers. “It becomes very easy for a publisher to say now, five years later, ‘Oh, we tried that, and it didn’t work,’” simply because their particular iteration of it didn’t, she says.&lt;/p&gt;
    &lt;p&gt;The other thing the Big Five editor considers when assessing track is the investment the prior book received. If it was put out by a small press and still sold 5,000 copies, that looks like the growth potential McGrath described—imagine what might happen with even more marketing muscle. Conversely, “if there’s someone who we all know was in a huge, million-dollar auction and the book sells 20,000 copies even though it was supposed to sell 100,000, then that’s a different consideration,” he says.&lt;/p&gt;
    &lt;p&gt;Such knowledge is highly piecemeal, even more so than the spotty sales data. Who we all know is more rumour than fact. Publishers don’t know exactly how much a book sold for. Neither the writer nor their agent has to disclose; in fact, it’s in their best interests not to, in case it backfires later.&lt;/p&gt;
    &lt;p&gt;There are the euphemisms used in deal announcements on the industry website Publishers Marketplace—a “very nice” deal connotes an advance in the mid to high five figures, a “good” deal signifies low six figures—or the trades might report on a high-profile auction. An author may also just be forthcoming, in the interest of equity, about how much they were paid, as when the 2020 hashtag #PublishingPaidMe revealed stark disparities in pay between white authors and authors of colour. But transparency at that scale is unusual. “Information about advances is so unreliable,” says McGrath. “When an advance gets published in Publishers Marketplace or Publishers Weekly, I don’t believe that for a second, because that’s all a way of generating excitement.”&lt;/p&gt;
    &lt;p&gt;But if publishers can’t verify a book’s purchase price, on what are they basing the decision that the track is bad? Bad relative to what, other than a general vibes-based sense of hype? There is no solid number that constitutes “good track,” Habib says, and what counts as good varies depending on the genre. In addition to evaluating track based on incomplete BookScan data, publishers are making decisions based on advance sizes they don’t have access to, maybe heard a rumour about, and in fairness to the writer probably shouldn’t be told at all.&lt;/p&gt;
    &lt;p&gt;What’s undeniable is that the market has become harder to break into for writers whose work does not scream commercial.&lt;/p&gt;
    &lt;p&gt;Still, the stigma of overpaying persists. If a writer is the beneficiary of such conditional faith, and then the book’s performance fails to justify it, it’s the writer who bears the stain. “These books that have huge price tags are given impossible expectations to meet,” the editor says. “The fact that a book received a $1 million cheque versus a $50,000 cheque means it’s going to be very hard for their work to continue moving forward.”&lt;/p&gt;
    &lt;p&gt;Habib disagrees with the idea that a high advance automatically sets a writer up for failure. When she is able to get her clients a competitive debut advance, she prepares them for the possibility that it might be the most money they ever receive. “Don’t think of your debut advance as your rate,” she tells them. “Think about it as funding for this stage of your career.” It comes with perks they might get offered only once, like a big publicity budget. It’s a chance to launch a huge career. And for writers, especially those who don’t come from wealth, it is a life-changing amount of money.&lt;/p&gt;
    &lt;p&gt;Despite the careful narrative that an agent and an editor may weave, both separately and in tandem, whether a book gets bought or for how much is ultimately not their call. Even if the Big Five editor loves a project, he still needs to share it with his colleagues and pitch it at an editorial board meeting. If it passes that hurdle, he writes up a formal proposal. Past that, it’s out of his hands: “At the end of the day, the people I am beholden to can say no to just about anything.”&lt;/p&gt;
    &lt;p&gt;Like Habib, he acknowledges that this can be unfair. “The thing that is toughest about track is that it really has nothing to do at all with the author and the author’s work.” This is a vexed Catch-22—that track has nothing to do with the author and yet the author is the one over whose head it hangs. Many writers seem to feel the opposite: that track has everything to do with them.&lt;/p&gt;
    &lt;p&gt;“Due to the author’s previous book sales, this is a pass. . . . I’m afraid the low units will present challenges as our sales team presents to retailers and our marketing and publicity teams pitch [this writer] to media.”&lt;/p&gt;
    &lt;p&gt;This was an email sent to Jeanna Kadlec’s agent when her second book went out on submission to publishers. Her first book, Heretic—a memoir about leaving evangelical Christianity—was acquired by Houghton Mifflin Harcourt at auction for $150,000 (US), the highest offer she received. Almost a year later, HMH was bought by HarperCollins. While Kadlec’s editor stayed on, she suspects she was allocated fewer resources at HarperCollins than she would have been at HMH.&lt;/p&gt;
    &lt;p&gt;The difficulties mounted from there. First came a months-long HarperCollins strike. With it came reviewer boycotts. Readers, too, may have been boycotting the company’s books, even though this wasn’t something the striking employees called for. Once the workers got their deal, Kadlec asked for renewed promotional attention. But HarperCollins declined, seemingly writing off anything that came out during the strike as a loss. When she took her next project to the press, who had a right of first refusal, they passed.&lt;/p&gt;
    &lt;p&gt;It’s on readers to look beyond the season’s biggest titles that they’re being spoon-fed by major publishers.&lt;/p&gt;
    &lt;p&gt;This sounds a bit like trying to buy new insurance for your car after it was totalled by your friend, the professional driver. The circumstances were obviously beyond Kadlec’s control. Still, when she tried to sell her sophomore project, a few editors, especially those at other HarperCollins imprints, explicitly cited her track as part of their rejection. Heretic has sold a few thousand copies—respectable for memoir but, when compared to her six-figure advance, which hints at higher commercial hopes, she admits, “not good math.”&lt;/p&gt;
    &lt;p&gt;These figures seem impossible to separate from the fact that, among other things, Kadlec’s publicist was marching on a picket line rather than continuing to pitch her book to media. (Kadlec, who speaks glowingly of the team assigned to her book, supported their strike demands and even marched alongside them.) But the feedback on submission didn’t seem to consider this. “We couldn’t really see a way to break out the new book,” said another reply, from a HarperCollins editor. “She might be better served with a fresh start in a new home.” Despite the track, Kadlec has managed to sell her next project, albeit at the much lower advance of $30,000 (US).&lt;/p&gt;
    &lt;p&gt;“I don’t know how people are supposed to develop in their careers,” says a novelist who also spoke on the condition of anonymity. She has published multiple books but, owing to editorial shuffles at her publishing houses, has had to take her books out on submission multiple times. “Every single time, the sales track becomes heavier.”&lt;/p&gt;
    &lt;p&gt;It’s frustrating that she alone is saddled with the track, given that a publisher plays just as big a role in a book’s fate, if not bigger. It’s as if they take a book over when they buy it, and then, if it misfires, renounce all responsibility. Like many authors, she feels abandoned by this logic. That writers shoulder the most risk when they have so much less power strikes her as unsustainable. We think of careers as things that progress linearly—the more skills and experience you have, the greater the salary, stability, and respect you can command. “But if you’ve got a mediocre sales track, that’s not the case. You’re lucky if you get a lower offer. You’re lucky if you get an offer at all.”&lt;/p&gt;
    &lt;p&gt;Online, certain tactics are suggested for how to “get over” the ailment of bad track—home remedies meant to replace the old curatives of editorial advocacy and time. A surprising number of sources suggest writing under a pseudonym. They can’t pin a bad track on you, the logic goes, if you take a different name. (Gotcha!) Habib seems unimpressed by this gambit. “It’s very hard to publish under a pseudonym,” she says. “The books that get the most promotion have an author to promote them. You can’t keep making up personae.” (“Or faking your own death,” I say, a tactic neither copped to nor suggested by anyone I spoke to.)&lt;/p&gt;
    &lt;p&gt;The suggestion to write in a new genre also comes up fairly often. This pre-empts the concerns about reaching a different, hopefully bigger audience—the genre will start to do that on its own. Switching genres is one way to mitigate a publisher’s concerns about track, the Big Five editor tells me, especially if the new project is markedly more commercial. This can also get hairy, in that it incentivizes bending one’s career to chase the market. It’s hard enough to keep financial pressures out of one’s creative process, especially under the gun of bad track; this advice doesn’t just let commerce in but puts it smack in the centre of one’s art.&lt;/p&gt;
    &lt;p&gt;Certainly, plenty of people write in multiple genres. But Habib cautions writers against pursuing forms they’re not interested in for the purpose of trying to sell books. As she correctly put it to me, a memoirist and essayist, “You’re in no position to write a great romantasy novel.” Kadlec and her agent tried a subtler genre shift. They hoped that switching from narrative to prescriptive nonfiction would count as enough of a fresh start; that hope wasn’t borne out. “A lot of the advice I hear for folks who do switch genres,” Kadlec says, “is they do a memoir and then they do a novel, and regardless of how the memoir did, the novel is considered a totally clean slate.”&lt;/p&gt;
    &lt;p&gt;This was the sequence Joseph Osmundson was hoping for—to sell a novel after his nonfiction book. His first release with a major press, Virology, is an essay collection that fuses science, queer writing, literary analysis, and memoir. Though his editor was eager to take a chance on the project, the publisher had low expectations. Norton bought it for a modest $15,000 (US), and in trade paperback rather than hardback, a cheaper format that also means the author gets a lower percentage of each sale in royalties counted against the advance.&lt;/p&gt;
    &lt;p&gt;Upon its release, Virology sold so well that Osmundson earned out his advance in three months—something most books never manage, let alone so quickly. According to industry rules, he was golden. A publisher had bet small on him and won big. He had a strong sales track, growth potential, and a proven audience. Fiction gave him the additional advantage of a clean slate even if he didn’t need one. But when his agent took his novel out on submission, the response he kept getting was the same: “We don’t see Joe having a platform or a pattern of fiction publications.”&lt;/p&gt;
    &lt;p&gt;Osmundson was frustrated. “I had been told: work my ass off on my first book, set up a solid track, and you’ll get a bigger advance next time,” only to discover it did not translate into anything for fiction. The novel was either rejected or offered an advance that was on par with his first. In the end, he sold the novel alongside a memoir, but the novel never made it to print. “Do I get tired of proving people wrong?” he asks. “Yes. It is exhausting to constantly feel like my work is undervalued.”&lt;/p&gt;
    &lt;p&gt;Despite widespread conversations in 2020 around equity in publishing, he believes we’re witnessing a general retrenchment in the industry. Decision makers are adopting an even more conservative stance, which steers them toward acquiring books from people who already have built-in audiences—like celebrities or influencers. Such retrenchment is also a labour issue. Publishing is an industry with stagnant salaries, considerable instability, and high turnover. “It’s hard to invest in authors when the people who are working on the books are not being invested in,” says the Big Five editor. He may not have the luxury of nurturing a writer across books, as much as he may want to. The more pressing issue can be, as he puts it, “I need to make a profit so I don’t get fired from this job.”&lt;/p&gt;
    &lt;p&gt;Many factors have likely contributed to this heightened risk aversion: corporate consolidation, a rapidly slimming media market, a volatile political climate. These are not favourable conditions for creative experimentation. While many people I spoke to agree that things feel particularly chilly at the moment, McGrath also takes the long view: “Publishing is always more conservative today than it was ten years ago,” she says. The industry has a habit of glancing back toward the rosy past. But one moment in particular, around 2001, marked a shift, when Nielsen BookScan (now Circana BookScan) first came on the market and began tracking sales. The current, pervasive sense of conservatism, McGrath says, has been exacerbated by the increased reliance on data to justify decisions. If you can put a number on the risk, maybe you think twice before taking it.&lt;/p&gt;
    &lt;p&gt;No matter the reason, what’s undeniable is that the market has become harder to break into for writers whose work does not scream commercial. “I worry a lot about writers who are a decade behind me in their career,” Osmundson says. It was his hope that the success of Virology, despite the book not being obviously mainstream, would create space for more ambitious queer books—his own and others’. Instead, he says, “it feels like that space is actually getting smaller.”&lt;/p&gt;
    &lt;p&gt;When I ask Norm Nehmetallah, publisher of Ontario-based small press Invisible Publishing, about the effect of track’s primacy on literary culture, he sighs. Working at a small press, Nehmetallah and his team can adopt a mandate less beholden to the bottom line than those of the bigger conglomerates. Invisible, in particular, has an explicit focus on finding and nurturing emerging writers. To Nehmetallah, a successful book is less a number than a feeling that it has travelled beyond the expected networks, like the writers’ friends or a particular literary scene.&lt;/p&gt;
    &lt;p&gt;But Nehmetallah, who worked various jobs in the industry before becoming a publisher, has seen the hunger for good track touch his work in various ways—including, oddly, his current role. More and more, he says, his press and others of similar size have been getting submissions from writers who have been dropped from bigger houses, like the imprints of Penguin Random House Canada (one of which is my current publisher). There may be less loyalty there, he says. “I think, in a lot of ways, they are more willing to take on debut authors, and I think that may be coming at the expense of what we would have called their ‘mid-list authors.’”&lt;/p&gt;
    &lt;p&gt;This mid-list cohort is exerting a downward pressure on the publishing landscape. By seeking support at smaller presses, they risk filling the spaces meant for more experimental or early-career authors. This isn’t just bad for writers—it’s bad for literature. If people aren’t given chances to grow and explore in ways the market doesn’t recognize, Nehmetallah says, then readers lose out too. Toronto-based writer Jean Marc Ah-Sen, who has published several books with small and independent presses, feels that he and his peers are being crowded out of their own game. “I used to think that the frontier of literary culture was the indie presses,” he says. “But when a person who has done three books with Penguin gets pushed down, it makes less room for the people who were doing the independent stuff to begin with.” Publishers, as McGrath says, have always been risk averse. But with higher pressure to find a sure thing, more writers who may have been able to sell a book five or ten years ago, whether to a corporate or an independent press, are being left out in the cold.&lt;/p&gt;
    &lt;p&gt;Such retrenchment has come alongside a shrunken and fragmented media industry, in which the shuttering of culture outlets and the decentralization of social media has created a different kind of missing middle: an arid landscape of coverage that’s no longer bustling enough to put a wide range of books on readers’ radars.&lt;lb/&gt; Consumers, too, have a role to play here. As Habib points out, track is also built by the people who buy books, or are supposed to. Nehmetallah makes a similar argument. It’s on readers to look beyond the season’s biggest titles that they’re being spoon-fed by major publishers, he says. This would help literary culture across the board. But it’s a two-way street. By shutting down writers’ chances to build audiences and careers, and restricting the range of what makes it to bookshelves—by spoon-feeding with so much aggression that it can take a lot of effort to close your mouth and turn away to find alternatives—publishers are jeopardizing that ecosystem too.&lt;/p&gt;
    &lt;p&gt;“The literary culture that supports a community of reading and that supports the word-of-mouth spreading is really diminishing,” Kadlec says. “Publishers could be doing so much more than they are actually doing to uplift it. You know,” she says to me. She then names one of my former employers, a company that shuttered its in-house magazine—which sometimes functioned as a built-in arm for promoting the books that it published, as well as a proven space to incubate reputations and platforms.&lt;/p&gt;
    &lt;p&gt;I’ve come to expect this moment of citation, even dread it—to be borne back ceaselessly into this past. But Kadlec is right, I do know. For the past three years since that magazine was shut down, I have seen the writers of my generation lament the loss of that particular launching pad for their essays, their book promotion, their careers. I have seen their debut books get only a handful of reviews in trade magazines. I have seen their publishers decide not to reissue their books in paperback because they didn’t sell “enough” copies. I have also seen my new email address added onto my former employer’s publicity list, asking me to cover their books.&lt;/p&gt;
    &lt;p&gt;Three years after its closure, the magazine’s absence is still felt across the spectrum of publishing. Without spaces like it—in which writers have the opportunity to build an audience prior to selling a book, and publishing workers have a place to secure promotional coverage in a way that actually gets that book out to readers—the infrastructure required to build a decent track erodes into nothing. During those same three years, I’ve watched as the story of that magazine’s shutdown has been reported as if it were purely an outcome of the funding model rather than the choices of its parent company, a publishing house that decided to prioritize profit no differently from how book publishers do every day.&lt;/p&gt;
    &lt;p&gt;The publication’s closure is, in the end, a parable about the whims of capital, albeit not in the way people seem to think. It’s true that publishing professionals hate this state of affairs just as much as writers do. I believe that even more so now than I did before I started working on this piece. But it’s also true that the assault on literary culture that has shattered the avenues for book coverage and ratcheted up the impossible standards by which authors must be rapidly, conclusively adjudged a success or doomed to career failure is not simply a frame being forced on the industry from without. It’s also being perpetuated from within by those who claim to love literature. Trace the call and you’ll find it’s coming from—where else—inside the house.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thewalrus.ca/the-publishing-industry-has-a-gambling-problem/"/><published>2025-10-07T18:55:36+00:00</published></entry></feed>