<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-01T19:07:59.839094+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45437323</id><title>Uxntal: A programming language for the Uxn virtual machine</title><updated>2025-10-01T19:08:08.360171+00:00</updated><content>&lt;doc fingerprint="7aa1d890ac86bad4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;A programming language for the Uxn virtual machine.&lt;/head&gt;
    &lt;p&gt;Uxn programs are written in a concatenative flavor of assembly designed especially to map to the idiosyncrasies of this strange little computer. &lt;lb/&gt;See the Quick Setup to get started.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cheatsheet, by Weeble.&lt;/item&gt;
      &lt;item&gt;Zine, by Clemens Scott.&lt;/item&gt;
      &lt;item&gt;Tutorial, by Compudanzas&lt;/item&gt;
      &lt;item&gt;Manual(7), by Eiríkr Åsheim.&lt;/item&gt;
      &lt;item&gt;BNF Notation, by Jack Leightcap.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Firstly, there are no precedence rules, the calculations are merely performed in the sequence in which they are presented. The order with which elements come off the stack is known as Last In, First Out. In the stack a b c, the c item was the last to be added, and will be the first to be removed.&lt;/p&gt;
    &lt;quote&gt;#01 DUP ADD #03 MUL program&lt;/quote&gt;
    &lt;quote&gt;01 01 02 03 06 stack 01 02&lt;/quote&gt;
    &lt;p&gt;Uxntal numbers are expressed in hexadecimal. Which means that counting goes like: one, two, three, four, five, six, seven, eight, nine, ha, be, ce, de, he, fe, ten! It takes some getting used to, but don't worry, you'll get the hang of it. Now, without further ado..&lt;/p&gt;
    &lt;head rend="h3"&gt;Let's dive into it!&lt;/head&gt;
    &lt;p&gt;The following example program prints the phrase "Hello World!" by pushing the address to a label on the stack, and iterating through each letter found at that address with a loop that increments the pointer until it reaches end of the phrase, at which point, the stack is emptied and the evaluation halts.&lt;/p&gt;
    &lt;p&gt;A word starting with @ defines a label, and one starting with ; pushes the absolute address of a label to the stack. With that in mind, ;text pushes the two bytes equal to the address of the @text label to the stack. In the interpreter above, press "step" to walk through each step of the evaluation.&lt;/p&gt;
    &lt;p&gt;Next, we define a new label named @while, to mark the start of the loop that will print each character stored at the text label.&lt;/p&gt;
    &lt;p&gt;The LDAk opcode loads a byte at the address currently at the top of the stack, in this case, the ascii letter H(48). The k-mode indicates that the operator will not consume the address.&lt;/p&gt;
    &lt;p&gt;The DUP opcode makes a copy of the letter. The ?{ pops that copy from the stack, and if it is not zero, we jump to the corresponding }, which is an anonymous label(λ00).&lt;/p&gt;
    &lt;quote&gt;( Disassembly of the example above: |addr bytecode Uxntal ----- -------- ------- ) |0100 a0 01 12 ( ;text ) @while |0103 94 ( LDAk ) |0104 06 ( DUP ) |0105 20 00 03 ( ?λ00 ) |0108 02 ( POP ) |0109 22 ( POP2 ) |010a 00 ( BRK )&lt;/quote&gt;
    &lt;p&gt;The #18 word pushes a number to the stack, which maps to the Console/write port(#18), followed by the DEO opcode that pops both bytes(the letter and the port) and sends the letter to that device port, telling the Console to print it, leaving only the address on top of the stack.&lt;/p&gt;
    &lt;p&gt;The INC2 opcode increments the address, moving the text pointer to the next letter. The 2-mode is used because address addresses are always made of two bytes.&lt;/p&gt;
    &lt;quote&gt;@λ00 |010b 80 18 ( #18 ) |010d 17 ( DEO ) |010e 21 ( INC2 ) |010f 40 ff f1 ( !while )&lt;/quote&gt;
    &lt;p&gt;Finally, with !while we jump back to the @while label, and repeat the loop until there are no more letters to load. When that happens, we POP to remove the duplicated letter, and POP2 to remove the address on the stack to keep the stack clean at the end of the evaluation.&lt;/p&gt;
    &lt;quote&gt;@text |0112 48 65 6c ( H e l ) |0115 6c 6f 20 ( l o ) |0117 57 6f 72 ( W o r ) |011a 6c 64 21 ( l d ! )&lt;/quote&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Comments are within parentheses, numbers are lowercase hexadecimal shorts or bytes, and opcodes are uppercase reserved words with lowercase modes. Addressing is done by one of six runes. Valid label and macro names are unique non-numeric, non-opcode and non-runic.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;Padding Runes&lt;/cell&gt;
        &lt;cell role="head"&gt;Number Rune&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;|&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;absolute&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;relative&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;literal number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Label Runes&lt;/cell&gt;
        &lt;cell&gt;Ascii Runes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;@&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;parent&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;&amp;amp;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;child&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;raw ascii&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Addressing Runes&lt;/cell&gt;
        &lt;cell&gt;Wrapping Runes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;,&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;literal relative&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;_&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;raw relative&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;( )&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;comment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;.&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;literal zero-page&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;raw zero-page&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{ }&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;anonymous&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;literal absolute&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;=&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;raw absolute&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[ ]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ignored&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Immediate Runes&lt;/cell&gt;
        &lt;cell&gt;Pre-processor Runes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;!&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;jmi&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;jci&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;% { }&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;macro&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Uxntal Stacks&lt;/head&gt;
    &lt;p&gt;All programming in Uxntal is done by manipulating the working stack, and return stack, each stack contains 256 bytes. Here are some stack primitives assuming the initial state of the stack is &lt;code&gt;a b c&lt;/code&gt; where
&lt;code&gt;c&lt;/code&gt; is the top of the stack: &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;POP&lt;/cell&gt;
        &lt;cell&gt;a b&lt;/cell&gt;
        &lt;cell&gt;Discard top item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NIP&lt;/cell&gt;
        &lt;cell&gt;a c&lt;/cell&gt;
        &lt;cell&gt;Discard second item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SWP&lt;/cell&gt;
        &lt;cell&gt;a c b&lt;/cell&gt;
        &lt;cell&gt;Move second item to top.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ROT&lt;/cell&gt;
        &lt;cell&gt;b c a&lt;/cell&gt;
        &lt;cell&gt;Move third item to top.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DUP&lt;/cell&gt;
        &lt;cell&gt;a b c c&lt;/cell&gt;
        &lt;cell&gt;Copy top item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;OVR&lt;/cell&gt;
        &lt;cell&gt;a b c b&lt;/cell&gt;
        &lt;cell&gt;Copy second item to top.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A byte is a number between 0-255(256 values), a short is a number between 0-65535(65536 values) made of two bytes, each byte in a short can be manipulated individually:&lt;/p&gt;
    &lt;quote&gt;#0a #0b POP 0a #12 #3456 NIP 12 56 #1234 DUP 12 34 34&lt;/quote&gt;
    &lt;p&gt;The two stacks are circular, to pop an empty stack does not trigger an error, but merely means to set the stack pointer to 255. There are no invalid programs, any sequence of bytes is a potential Uxn program. Values are moved between stacks with the STH opcode.&lt;/p&gt;
    &lt;quote&gt;WST 00 00 00 00 00 00|12 34 &amp;lt;02 RST 00 00 00 00 00 00 00|56 &amp;lt;01&lt;/quote&gt;
    &lt;p&gt;The program above contains 12 and 34 on the working stack, and 56 on the return stack. The stack content can always be printed by sending a non-null byte to the System/debug port.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uxntal Notation&lt;/head&gt;
    &lt;p&gt;The stack-effect notation follows that of the Forth programming language, where each item on the left of the -- spacer is the state of the stack before, and to the right, the state of the stack after. On each side, the right-most item is the last to the pushed and the first to be removed:&lt;/p&gt;
    &lt;quote&gt;@routine ( a b -- a b res ) ADDk JMP2r&lt;/quote&gt;
    &lt;p&gt;Single items are a byte long, and shorts are indicated with a * suffix, the order in which they appear is the order of the stack with the top item to the right:&lt;/p&gt;
    &lt;quote&gt;@routine ( a b* -- b* a ) ROT JMP2r&lt;/quote&gt;
    &lt;p&gt;The dot notation is used to indicate that stack-effects to the right of the dot are happening on the return stack:&lt;/p&gt;
    &lt;quote&gt;@routine ( a . b -- c ) STHr ADD JMP2r&lt;/quote&gt;
    &lt;p&gt;If a routine is a vector, it uses the arrow notation.&lt;/p&gt;
    &lt;quote&gt;@on-event ( -&amp;gt; ) BRK&lt;/quote&gt;
    &lt;p&gt;This notation also holds for macros as well, the notation goes before the macro's body:&lt;/p&gt;
    &lt;quote&gt;%macro ( a b -- res ) { DIVk MUL SUB }&lt;/quote&gt;
    &lt;p&gt;The stack notation is merely present to help readability but can be altogether disregarded without impacting the program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments&lt;/head&gt;
    &lt;p&gt;A comment starts with any token beginning with opened parenthesis, and ends at its corresponding closed parenthesis. Comments may be nested, the enclosed comments parentheses must be whitespace separated on both sides.&lt;/p&gt;
    &lt;quote&gt;( ( nested ) ) ( 1+2*(4/3) )&lt;/quote&gt;
    &lt;p&gt;Outermost comments may be named, which means that sometimes the open parenthesis is immediately followed by a word holding some meaning to external tools.&lt;/p&gt;
    &lt;quote&gt;(doc This is a docstring. )&lt;/quote&gt;
    &lt;p&gt;Special comments are sometimes used to group routines together, they are similar to the &lt;code&gt;pragma mark&lt;/code&gt; notation:&lt;/p&gt;
    &lt;quote&gt;( @|Group )&lt;/quote&gt;
    &lt;head rend="h3"&gt;Brackets&lt;/head&gt;
    &lt;p&gt;The square brackets do nothing, they are used as an aid for readability and formatting, they are useful for making explicit certain things like grouping behaviors, joining literals or indicating lookup tables.&lt;/p&gt;
    &lt;quote&gt;@routine ( -- ) [ LIT2 20 -Console/write ] DEO JMP2r %min ( a b -- r ) { GTHk [ JMP SWP ] POP } @sprite [ 00 66 ff ff ff 7e 3c 18 ]&lt;/quote&gt;
    &lt;head rend="h2"&gt;Uxntal Numbers&lt;/head&gt;
    &lt;p&gt;Uxntal uses only lowercase unsigned hexadecimal numbers of either 2 or 4 characters in length. There are two types of numbers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Literal Hex, like #ab, denotes a number that will be pushed on the stack when evaluated, it is made of a LIT opcode that matches its length, followed by a Raw Hex number.&lt;/item&gt;
      &lt;item&gt;A Raw Hex, like aa, is the standard textual encoding of data in a program, generally speaking these are more often loaded than evaluated. It can be anything, an opcode, an ascii byte, an address, part of a sprite.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
#12 #34 LIT2 5678 ADD2 68 ac
&lt;/code&gt;
    &lt;head rend="h2"&gt;Uxntal Opcodes&lt;/head&gt;
    &lt;p&gt;Uxn has 32 standard opcodes and 4 immediate opcodes. In the table below, the pipe(|) character indicates an effect on the return stack, the &lt;code&gt;pc is the program counter, a &lt;/code&gt;&lt;code&gt;value8&lt;/code&gt; indicates a byte
length, a &lt;code&gt;value*&lt;/code&gt; indicates a short length, an unspecified length follows the short mode and a &lt;code&gt;[value]&lt;/code&gt; is read from memory.&lt;/p&gt;
    &lt;quote&gt;Stack I Logic Memory I Arithmetic BRK -- EQU a b -- a=b LDZ abs8 -- [abs8] ADD a b -- a+b INC a -- a+1 NEQ a b -- a≠b STZ val abs8 -- SUB a b -- a-b POP a -- GTH a b -- a&amp;gt;b LDR rel8 -- [rel8] MUL a b -- a×b NIP a b -- b LTH a b -- a&amp;lt;b STR val rel8 -- DIV a b -- a÷b Stack II Stash Memory II Bitwise SWP a b -- b a JMP addr -- LDA abs* -- [abs*] AND a b -- a&amp;amp;b ROT a b c -- b c a JCN cond8 addr -- STA val abs* -- ORA a b -- a|b DUP a -- a a JSR addr -- | pc* DEI dev -- [dev] EOR a b -- a^b OVR a b -- a b a STH a -- | a DEO val dev -- SFT a sft8 -- res LIT -- [pc*] JCI cond8 -- JMI -- JSI -- | pc*&lt;/quote&gt;
    &lt;head rend="h3"&gt;Modes&lt;/head&gt;
    &lt;p&gt;An opcode is any name in which the 3 first characters are found in the opcode table, followed by any combination of 2, k and r. Each opcode has 3 possible modes, which can combined:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The short mode 2 operates on shorts, instead of bytes.&lt;/item&gt;
      &lt;item&gt;The keep mode k operates without consuming items.&lt;/item&gt;
      &lt;item&gt;The return mode r operates on the return stack.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;INC2r&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;k&lt;/cell&gt;
        &lt;cell&gt;r&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;opcode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;By default, operators consume bytes from the working stack, notice how in the following example only the last two bytes &lt;code&gt;#45&lt;/code&gt; and &lt;code&gt;#67&lt;/code&gt;
are added, even if there are two shorts on the stack.&lt;/p&gt;
    &lt;code&gt;#1234 #4567 ADD12 34 ac&lt;/code&gt;
    &lt;p&gt;The short mode consumes two bytes from the stack. In the case of jump opcodes, the short-mode operation jumps to an absolute address in memory. For the memory accessing opcodes, the short mode operation indicates the size of the data to read and write.&lt;/p&gt;
    &lt;code&gt;#1234 #4567 ADD2 57 9b&lt;/code&gt;
    &lt;p&gt;The keep mode does not consume items from the stack, and pushes the result on top. Every opcode begins by popping values from the stack before operating on them. This mode keeps a copy of the stack pointer to recover after the popping stage.&lt;/p&gt;
    &lt;code&gt;#1234 #4567 ADD2k 12 34 45 67 57 9b&lt;/code&gt;
    &lt;p&gt;The return mode swaps the stacks on which an opcode operates. Under this mode, a return address will be pushed to the working stack, and stashing will take from the return stack. For that reason, there is no return opcode. For example, the &lt;code&gt;JSR&lt;/code&gt; opcode pushes the
return address onto the return stack, and &lt;code&gt;JMP2r&lt;/code&gt; jumps to that
address.&lt;/p&gt;
    &lt;code&gt;LITr 12 #34 STH ADDr STHr 46&lt;/code&gt;
    &lt;head rend="h3"&gt;Immediate opcodes&lt;/head&gt;
    &lt;p&gt;Immediate opcodes are operators that do not take items from the stack, but read values stored immediately after the opcode in the program's memory. Uxntal has 4 immediate opcodes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The literal LIT opcode, also written as #.&lt;/item&gt;
      &lt;item&gt;The jump !routine.&lt;/item&gt;
      &lt;item&gt;The conditional jump ?routine.&lt;/item&gt;
      &lt;item&gt;The subroutine routine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The immediate jump opcodes are slightly faster than their standard opcode counterparts, but do not have modes and cannot be used to do pointer arithmetic. The address value of the immediate opcodes are stored in memory as relative shorts, enabling routines making use of these opcodes to be moved around in the program's memory.&lt;/p&gt;
    &lt;quote&gt;@on-reset ( -&amp;gt; ) #0007 fact BRK @fact ( n* -- res* ) ORAk ?{ POP2 #0001 JMP2r } DUP2 #0001 SUB2 fact MUL2 JMP2r&lt;/quote&gt;
    &lt;p&gt;To learn more about each opcode, see the Opcode Reference.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uxntal Labels&lt;/head&gt;
    &lt;p&gt;A label is a non-numeric, non-opcode, and non-runic symbol that correspond to a number between 0 and 65536. A label name is made of two parts, a scope and a sublabel. Sublabels can be added to a scope with the &amp;amp;name rune, or by writing the full name, like @scope/name. Note that a labels like bed, add and cafe are considered numeric.&lt;/p&gt;
    &lt;p&gt;Functions are simply labels that will be jumped to, and returned from.&lt;/p&gt;
    &lt;quote&gt;@func ( a b -- c ) &amp;amp;loop INC GTHk ?&amp;amp;loop ADD JMP2r&lt;/quote&gt;
    &lt;p&gt;Constants are labels that hold a specific value through the entire execution of the program. They allow to assign a name to a number, making the code more readable.&lt;/p&gt;
    &lt;quote&gt;|1400 @limit&lt;/quote&gt;
    &lt;p&gt;Enums are labels with padded members of equal sizes that can be used as constants in a program, they typically begin by rolling back the program address with |00:&lt;/p&gt;
    &lt;quote&gt;|00 @Suit &amp;amp;clubs $1 &amp;amp;diamonds $1 &amp;amp;hearts $1 &amp;amp;spades&lt;/quote&gt;
    &lt;p&gt;Structs are labels with padded members of different sizes, that maps on a data-structure, they typically space the different members with $1:&lt;/p&gt;
    &lt;quote&gt;|00 @Person &amp;amp;name $2 &amp;amp;age $1 &amp;amp;height $2&lt;/quote&gt;
    &lt;p&gt;Labels can also be used with the padding runes to define a global length. For example, if one needs to specify a length of 0x30 for multiple members of a struct, a value can be specified as follow:&lt;/p&gt;
    &lt;quote&gt;|30 @length |00 @Struct &amp;amp;field $length&lt;/quote&gt;
    &lt;head rend="h3"&gt;Scope&lt;/head&gt;
    &lt;p&gt;Uxntal objects are defined statically and allow for the enclosed methods to access encapsulated local &amp;amp;members. The example below contains an object with the method set-color, accessible from outside the scope as pen/set-color.&lt;/p&gt;
    &lt;quote&gt;@pen &amp;amp;position &amp;amp;x $2 &amp;amp;y $2 &amp;amp;color $1 &amp;amp;set-color ( color -- ) ,/color STR JMP2r&lt;/quote&gt;
    &lt;p&gt;New methods and members can extend an existing scope from anywhere by creating a label with the scope name followed by a slash and the name of the extension. The &amp;amp;labels declared within the extension have the same access to local labels as the rest of the object.&lt;/p&gt;
    &lt;quote&gt;@pen/get-position ( -- x* y* ) ,/x LDR2 ,/y LDR2 JMP2r&lt;/quote&gt;
    &lt;p&gt;When calling local methods the scope's name can be omitted, starting at the slash, like /method:&lt;/p&gt;
    &lt;quote&gt;@pen/paint ( -- ) /get-position canvas/draw-line-to JMP2r&lt;/quote&gt;
    &lt;head rend="h3"&gt;Addressing&lt;/head&gt;
    &lt;p&gt;A labels is a way of assigning a name to a number. There are six ways to get the number corresponding to that label. Literal addressing prefixes the label with a &lt;code&gt;LIT&lt;/code&gt; for Relative and Zero-Page addressing, and
&lt;code&gt;LIT2&lt;/code&gt; for absolute addressing.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Literal Relative, like ,label, pushes a relative distance byte to the label.&lt;/item&gt;
      &lt;item&gt;Literal Zero-Page, like .label, pushes an absolute address byte to the label.&lt;/item&gt;
      &lt;item&gt;Literal Absolute, like ;label, pushes an absolute address short to the label.&lt;/item&gt;
      &lt;item&gt;Raw Relative, like _label, writes a relative distance byte to the label.&lt;/item&gt;
      &lt;item&gt;Raw Zero-Page, like -label, writes an absolute address byte to the label.&lt;/item&gt;
      &lt;item&gt;Raw Absolute, like =label, writes an absolute address short to the label.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Raw addressing is used for building data-structures and more advanced programs. A relatively common usage of raw runes is to create literals directly into the return stack:&lt;/p&gt;
    &lt;code&gt;
[ LIT2r 08 -label ] LDZr ADDr | [.label]+8
&lt;/code&gt;
    &lt;head rend="h3"&gt;Anonymous Labels&lt;/head&gt;
    &lt;p&gt;Anonymous labels are designated with a curly bracket that points to its associated closing bracket, and can be nested. Under the hood, the opening bracket assembles to the address of the closing bracket which allows the destination address to be used like any other label such as a JCI ?{, a JMI, !{ or a plain literal ;{. Here are some example data-structures:&lt;/p&gt;
    &lt;quote&gt;@counted-string _{ "foo 20 "bar } @linked-list ={ ={ "A } ={ "B ={ "C } } }&lt;/quote&gt;
    &lt;head rend="h4"&gt;Unless Blocks&lt;/head&gt;
    &lt;p&gt;It is important to notice that in the case of a conditional jump, the lambda's content is jumped over when the flag byte is true.&lt;/p&gt;
    &lt;quote&gt;[ LIT2 &amp;amp;last $1 -Mouse/state ] DEI DUP ,&amp;amp;last STR DUP2 #0001 NEQ2 ?{ ( on down ) } DUP2 #0101 NEQ2 ?{ ( on drag ) } DUP2 #0100 NEQ2 ?{ ( on release ) } POP2&lt;/quote&gt;
    &lt;p&gt;The opening curly bracket assembles to a unique label reference, and the closing bracket to a corresponding matching label definition. They do not affect the scope.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uxntal Macros&lt;/head&gt;
    &lt;p&gt;A macro is a way of defining inline routines, it allows to create new words that will be replaced by the body of the macro, as opposed to a jump where the program counter will move to a routine and back, therefore it needs to be defined before its usage, as follow:&lt;/p&gt;
    &lt;quote&gt;%modulo ( num denum -- res ) { DIVk MUL SUB } @routine ( -- c* ) #18 #03 modulo JMP2r&lt;/quote&gt;
    &lt;p&gt;In the previous example, the token modulo will get replaced by the body of the macro during assembly:&lt;/p&gt;
    &lt;quote&gt;@routine ( -- c* ) #18 #03 DIVk MUL SUB JMP2r&lt;/quote&gt;
    &lt;head rend="h2"&gt;Uxntal Memory&lt;/head&gt;
    &lt;p&gt;There are 64kb of addressable memory. Roms are always loaded at 0x0100, which is the address of the Reset Vector and where evaluation begins. During boot, the stacks, device and addressable memories are zeroed. During a soft-reboot, the content of the zero-page is preserved.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Shared&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;cell&gt;RAM&lt;/cell&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;64kb pages&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Private&lt;/cell&gt;
        &lt;cell&gt;Stacks&lt;/cell&gt;
        &lt;cell&gt;Working Stack&lt;/cell&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;256 bytes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Pointer&lt;/cell&gt;
        &lt;cell&gt;1 byte&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Return Stack&lt;/cell&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;256 bytes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Pointer&lt;/cell&gt;
        &lt;cell&gt;1 byte&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;IO&lt;/cell&gt;
        &lt;cell&gt;Devices&lt;/cell&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;256 bytes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The device page and stacks are located outside of addressable memory.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An Absolute Padding, like |100 moves the program generation to an address specified by a number or label.&lt;/item&gt;
      &lt;item&gt;A Relative Padding, like $18 moves the program generation by a distance specified by a number or label.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
|18 @width

|100 @on-reset ( -&amp;gt; )
	;buffer/end BRK 02 18

|200 @buffer $width &amp;amp;end
&lt;/code&gt;
    &lt;p&gt;Memory is big-endian, when writing or reading a short from memory, the position is that of the high-byte. The low-byte of a short written at 0xffff wraps to 0x0000.&lt;/p&gt;
    &lt;quote&gt;#12 #0200 STA 0x0200=12 #3456 #0400 STA2 0x0400=34, 0x0401=56 #0400 LDA 34&lt;/quote&gt;
    &lt;p&gt;The zero-page is the memory located below 0x0100, its purpose is to store variables that will be accessed often, or needs to be preserved across a soft-reboot. It is sligthly faster to read and write from the zero-page using the LDZ and STZ opcodes as they use only a single byte instead of a short. This memory space cannot be pre-filled in the rom prior to initialization. The low-byte of a short written at 0xff wraps to 0x00.&lt;/p&gt;
    &lt;quote&gt;#1234 #80 STZ2 0x0080=12, 0x0081=34 #81 LDZ 34&lt;/quote&gt;
    &lt;head rend="h2"&gt;Uxntal Devices&lt;/head&gt;
    &lt;p&gt;Uxn is non-interruptible, vectors are locations in programs that are evaluated when certain events occur. A vector is evaluated until a BRK opcode is encountered. Uxn can communicate with a maximum of 16 devices, each device has 16 ports, each port handles a specific I/O message. Ports are mapped to the devices memory page, which is located outside of the main addressable memory.&lt;/p&gt;
    &lt;p&gt;All programs begin by executing the reset vector located at &lt;code&gt;0x100&lt;/code&gt;. The content of the stacks are preserved between vectors,
but it is discouraged to use the stacks to pass data between vectors.&lt;/p&gt;
    &lt;quote&gt;@on-reset ( -&amp;gt; ) ( set vector ) ;on-mouse .Mouse/vector DEO2 BRK @on-mouse ( -&amp;gt; ) ( read state ) .Mouse/state DEI ?&amp;amp;on-touch BRK &amp;amp;on-touch ( -&amp;gt; ) ( A mouse button is pressed ) BRK&lt;/quote&gt;
    &lt;p&gt;For example, the address stored in the Mouse/vector ports points to a part of the program to be evaluated when the cursor is moved, or a button state has changed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uxntal Utilities&lt;/head&gt;
    &lt;p&gt;Here's a list of small self-hosted development tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Drifblim is an assembler that also emits a symbols file.&lt;/item&gt;
      &lt;item&gt;Uxnfor is a formatter that standardize the source code, this is the formatting style used across the Uxntal documentation.&lt;/item&gt;
      &lt;item&gt;Uxnlin is a peephole optimizer that reveals potential optimizations in opcode sequences.&lt;/item&gt;
      &lt;item&gt;Uxnbal is a program validator that warns when routines do not match their definitions.&lt;/item&gt;
      &lt;item&gt;Uxndis is a disassembler that prints the opcodes in a rom file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;incoming: left noodle drifblim theme catclock oquonie bifurcan yufo programming languages concatenative gly format ufx format ulz format proquints brainfuck uxn uxntal reference uxntal alphabet bicycle beetbug arvelie about oscean computer 2025 2021&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wiki.xxiivv.com/site/uxntal.html"/><published>2025-10-01T13:14:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45437594</id><title>Show HN: ChartDB Agent – Cursor for DB schema design</title><updated>2025-10-01T19:08:08.089933+00:00</updated><link href="https://app.chartdb.io/ai"/><published>2025-10-01T13:38:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45437735</id><title>Cursor 1.7</title><updated>2025-10-01T19:08:05.960059+00:00</updated><content>&lt;doc fingerprint="ae26a31815e9b8b4"&gt;
  &lt;main&gt;
    &lt;p&gt;1.7 · Changelog&lt;/p&gt;
    &lt;head rend="h1"&gt;Agent Autocomplete, Hooks, and Team Rules&lt;/head&gt;
    &lt;head rend="h3"&gt;Autocomplete for Agent&lt;/head&gt;
    &lt;p&gt;When writing prompts, autocomplete suggestions will appear based on recent changes. Tab to accept suggestions and attach files to context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hooks (beta)&lt;/head&gt;
    &lt;p&gt;You can now observe, control, and extend the Agent loop using custom scripts. Hooks give you a way to customize and influence Agent behavior at runtime.&lt;/p&gt;
    &lt;p&gt;Use Hooks to audit Agent usage, block commands, or redact secrets from context. It's still in beta and we'd love to hear your feedback.&lt;/p&gt;
    &lt;head rend="h3"&gt;Team rules&lt;/head&gt;
    &lt;p&gt;Teams can now define and share global rules from the dashboard that will be applied to all projects. We’ve also shipped team rules for Bugbot, so behavior is consistent across repos.&lt;/p&gt;
    &lt;head rend="h3"&gt;Share prompts with deeplinks (beta)&lt;/head&gt;
    &lt;p&gt;Generate shareable deeplinks for reusable prompts. Useful for setup instructions in documentation, team resources, and sharing workflows. See our documentation for how to create them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sandboxed terminals&lt;/head&gt;
    &lt;p&gt;Commands now execute in a secure, sandboxed environment. If you’re on allowlist mode, non-allowlisted commands will automatically run in a sandbox with read/write access to your workspace and no internet access.&lt;/p&gt;
    &lt;p&gt;If a command fails and we detect the sandbox was the cause, you’ll be prompted to retry outside of the sandbox.&lt;/p&gt;
    &lt;head rend="h3"&gt;Monitor Agents from menubar&lt;/head&gt;
    &lt;p&gt;Quickly check the status of Cursor Agents right from your menubar.&lt;/p&gt;
    &lt;head rend="h3"&gt;Image file support for Agent&lt;/head&gt;
    &lt;p&gt;Agent can now read image files directly from your workspace and include them in context. Previously, only pasted images were supported.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/changelog/1-7"/><published>2025-10-01T13:51:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45437893</id><title>Unix philosophy and filesystem access makes Claude Code amazing</title><updated>2025-10-01T19:08:05.715277+00:00</updated><content>&lt;doc fingerprint="370009d837378e9b"&gt;
  &lt;main&gt;
    &lt;p&gt;Noah Brier, September 30, 2025&lt;/p&gt;
    &lt;p&gt;If you've talked to me lately about AI, you've almost certainly been subject to a long soliloquy about the wonders of Claude Code. What started as a tool I ran in parallel with other tools to aid coding has turned into my full-fledged agentic operating system, supporting all kinds of workflows.&lt;/p&gt;
    &lt;p&gt;Most notably, Obsidian, the tool I use for note-taking. The difference between Obsidian and Notion or Evernote is that all the files are just plain old Markdown files stored on your computer. You can sync, style, and save them, but ultimately, it's still a text file on your hard drive. A few months ago, I realized that this fact made my Obsidian notes and research a particularly interesting target for AI coding tools. What first started with trying to open my vault in Cursor quickly moved to a sort of note-taking operating system that I grew so reliant on, I ended up standing up a server in my house so I could connect via SSH from my phone into my Claude Code + Obsidian setup and take notes, read notes, and think through things on the go.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, I went on Dan Shipper's AI &amp;amp; I Podcast to wax poetic about my love for this setup. I did a pretty deep dive into the system I use, how it works, why it works, etc. I won't retread all those details—you can read the transcript or listen to the podcast—but I want to talk about a few other things related to Claude Code that I've come to realize since the conversation.&lt;/p&gt;
    &lt;p&gt;I've really struggled to answer this question. I'm also not sure it's better than Cursor for all things, but I do think there are a set of fairly exceptional pieces that work together in concert to make me turn to Claude Code whenever I need to build anything these days. Increasingly, that's not even about applying it to existing codebases as much as it's building entirely new things on top of its functionality (more on that in a bit).&lt;/p&gt;
    &lt;p&gt;So what's the secret? Part of it lies in how Claude Code approaches tools. As a terminal-based application, it trades accessibility for something powerful: native Unix command integration. While I typically avoid long blockquotes, the Unix Philosophy deserves an exception—Doug McIlroy's original formulation captures it perfectly:&lt;/p&gt;
    &lt;p&gt;The Unix philosophy is documented by Doug McIlroy in the Bell System Technical Journal from 1978:&lt;/p&gt;
    &lt;p&gt;It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):&lt;/p&gt;
    &lt;p&gt;These fifty-year-old principles are exactly how LLMs want to use tools. If you look at how these models actually use the tools they're given, they are constantly "piping" output to input (albeit using their own fuzziness in between). (As an aside, the Unix | command allows you to string the output from one command into the input of another.) When models fail to weld their tools effectively, it is almost always because the tools are overly complex.&lt;/p&gt;
    &lt;p&gt;So part one of why Claude Code can be so mind-blowing is that the commands that power Unix happen to be perfectly suited for use by LLMs. This is both because they're simple and also incredibly well-documented, meaning the models had ample source material to teach them the literal ins and outs.&lt;/p&gt;
    &lt;p&gt;But that still wasn't the whole thing. The other piece was obviously Claude Code's ability to write code initially and, more recently, prose (for me, at least). But while other applications like ChatGPT and Claude can write output, there was something different going on here. Last week, while reading The Pragmatic Engineer's deep dive into how Claude Code is built. The answer was staring me in the face: filesystem access.&lt;/p&gt;
    &lt;p&gt;The filesystem changes everything. ChatGPT and Claude in the browser have two fatal flaws: no memory between conversations and a cramped context window. A filesystem solves both. Claude Code writes notes to itself, accumulates knowledge, and keeps running tallies. It has state and memory. It can think beyond a single conversation.&lt;/p&gt;
    &lt;p&gt;Back in 2022, when I first played with the GPT-3 API, I said that even if models never got better than they were in that moment, we would still have a decade to discover the use cases. They did get better—reasoning models made tool calling reliable—but the filesystem discovery proves my point.&lt;/p&gt;
    &lt;p&gt;I bring this up because in the Pragmatic Engineer interview, Boris Cherney, who built the initial version of Claude Code, uses it to describe the aha:&lt;/p&gt;
    &lt;p&gt;In AI, we talk about “product overhang”, and this is what we discovered with the prototype. Product overhang means that a model is able to do a specific thing, but the product that the AI runs in isn’t built in a way that captures this capability. What I discovered about Claude exploring the filesystem was pure product overhang. The model could already do this, but there wasn’t a product built around this capability!&lt;/p&gt;
    &lt;p&gt;Again, I'd argue it's filesystem + Unix commands, but the point is that the capability was there in the model just waiting to be woken up, and once it was, we were off to the races. Claude Code works as a blueprint for building reliable agentic systems because it captures model capabilities instead of limiting them through over-engineered interfaces.&lt;/p&gt;
    &lt;p&gt;I talked about my Claude Code + Obsidian setup, and I've actually taken it a step further by open-sourcing "Claudesidian," which pulls in a bunch of the tools and commands I use in my own Claude Code + Obsidian setup. It also goes beyond that and was a fun experimental ground for me. Most notably, I built an initial upgrade tool so that if changes are made centrally, you can pull them into your own Claudesidian, and the AI will help you check to see if you've made changes to the files being updated and, if so, attempt to smartly merge your changes with the new updates. Both projects follow the same Unix philosophy principles—simple, composable tools that do one thing well and work together. This is the kind of stuff that Claude Code makes possible, and why it's so exciting for me as a new way of building applications.&lt;/p&gt;
    &lt;p&gt;Speaking of which, one I'm not quite ready to release, but hopefully will be soon, is something I've been calling "Inbox Magic," though I'll surely come up with a better name. It's a Claude Code repo with access to a set of Gmail tools and a whole bunch of prompts and commands to effectively start operating like your own email EA. Right now, the functionality is fairly simple: it can obviously run searches or send emails on your behalf, but it can also do things like triage and actually run a whole training run on how you sound over email so it can more effectively draft emails for you. While Claude Code and ChatGPT both have access to my emails, they mostly grab one or two at a time. This system, because it can write things out to files and do lots of other fancy tricks, can perform a task like “find every single travel-related email in my inbox and use that to build a profile of my travel habits that I can use as a prompt to help ChatGPT/Claude do travel research that's actually aligned with my preferences.” Anyway, more on this soon, and if it's something you want to try out, ping me with your GitHub username, and as soon as I feel like I have something ready to test, I'll happily share it.&lt;/p&gt;
    &lt;p&gt;While I generally shy away from conclusions, I think there are a few here worth reiterating.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.alephic.com/writing/the-magic-of-claude-code"/><published>2025-10-01T14:05:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438346</id><title>Show HN: Autism Simulator</title><updated>2025-10-01T19:08:05.457933+00:00</updated><link href="https://autism-simulator.vercel.app/"/><published>2025-10-01T14:48:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438496</id><title>Building the heap: racking 30 petabytes of hard drives for pretraining</title><updated>2025-10-01T19:08:04.987550+00:00</updated><content>&lt;doc fingerprint="2dd91d7735ac7f8d"&gt;
  &lt;main&gt;
    &lt;p&gt;We built a storage cluster in downtown SF to store 90 million hours worth of video data. Why? We’re pretraining models to solve computer use. Compared to text LLMs like LLaMa-405B, which require ~60 TB of text data to train, videos are sufficiently large that we need 500 times more storage. Instead of paying the $12 million / yr it would cost to store all of this on AWS, we rented space from a colocation center in San Francisco to bring that cost down ~40x to $354k per year, including depreciation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why&lt;/head&gt;
    &lt;p&gt;Our use case for data is unique. Most cloud providers care highly about redundancy, availability, and data integrity, which tends to be unnecessary for ML training data. Since pretraining data is a commodity—we can lose any individual 5% with minimal impact—we can handle relatively large amounts of data corruption compared to enterprises who need guarantees that their user data isn’t going anywhere. In other words, we don’t need AWS’s 13 nines of reliability; 2 is more than enough.&lt;/p&gt;
    &lt;p&gt;Additionally, storage tends to be priced substantially above cost. Most companies use relatively small amounts of storage (even ones like Discord still use under a petabyte for messages), and the companies that use petabytes are so large that storage remains a tiny fraction of their total compute spend.&lt;/p&gt;
    &lt;p&gt;Data is one of our biggest contraints, and would be prohibitively expensive otherwise. As long as the cost predictions work out in favor of a local datacenter, and it would not consume too much of the core team’s time, it would make sense to stack hard drives ourselves. [1] 1. We talked to some engineers at the Internet Archive, which had basically the same problem as us; even after massive friends &amp;amp; family discounts on AWS, it was still 10 times more cost-effective to buy racks and store the data themselves!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost Breakdown: Cloud Alternatives vs In-House&lt;/head&gt;
    &lt;p&gt;Internet and electricity total $17.5k as our only recurring expenses (the price of colocation space, cooling, etc were bundled into electricity costs). One-time costs were dominated by hard drive capex. [2] 2. When deciding the datacenter location we had multiple options across the Bay Area, including options in Fremont through Hurricane Electric for around $10k in setup fees and $12.8k per month, saving us $38.5k initially and $4.7k per month, but ended up opting for a datacenter that was only a couple blocks from our office in SF. Though this came at a premium, it was extremely helpful to get the initial nodes setup and for ongoing maintenance. Our team is just 5 people, so any friction in going to the datacenter would come at a noticeable cost to team productivity.&lt;/p&gt;
    &lt;p&gt;Table 1: Cost comparison of cloud alternatives vs in-house. AWS is $1,130,000/month including estimated egress, Cloudflare is $270,000/month (with bulk-discounted pricing), and our datacenter is $29,500/month (including recurring costs and depreciation).&lt;/p&gt;
    &lt;head rend="h3"&gt;Monthly Recurring Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Internet&lt;/cell&gt;
        &lt;cell&gt;$7,500/month&lt;/cell&gt;
        &lt;cell&gt;100Gbps DIA from Zayo, 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Electricity&lt;/cell&gt;
        &lt;cell&gt;$10,000/month&lt;/cell&gt;
        &lt;cell&gt;1 kW/PB, $330/kW. Includes cabinet space &amp;amp; cooling. 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Monthly&lt;/cell&gt;
        &lt;cell&gt;$17,500/month&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;One-Time Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;Hard drives (HDDs)&lt;/cell&gt;
        &lt;cell&gt;$300,000&lt;/cell&gt;
        &lt;cell&gt;2,400 drives. Mostly 12TB used enterprise drives (3/4 SATA, 1/4 SAS). The JBOD DS4246s work for either.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage Infrastructure&lt;/cell&gt;
        &lt;cell&gt;NetApp DS4246 chassis&lt;/cell&gt;
        &lt;cell&gt;$35,000&lt;/cell&gt;
        &lt;cell&gt;100 dual SATA/SAS chassis, 4U each&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compute&lt;/cell&gt;
        &lt;cell&gt;CPU head nodes&lt;/cell&gt;
        &lt;cell&gt;$6,000&lt;/cell&gt;
        &lt;cell&gt;10 Intel RR2000s from eBay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Datacenter Setup&lt;/cell&gt;
        &lt;cell&gt;Install fee&lt;/cell&gt;
        &lt;cell&gt;$38,500&lt;/cell&gt;
        &lt;cell&gt;One-off datacenter install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Labor&lt;/cell&gt;
        &lt;cell&gt;Contractors&lt;/cell&gt;
        &lt;cell&gt;$27,000&lt;/cell&gt;
        &lt;cell&gt;Contractors to help physically screw in / install racks and wire cables&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Networking &amp;amp; Misc&lt;/cell&gt;
        &lt;cell&gt;Install expenses&lt;/cell&gt;
        &lt;cell&gt;$20,000&lt;/cell&gt;
        &lt;cell&gt;Power cables, 100GbE QSFP CX4 NICs, Arista router, copper jumpers, one-time internet install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total One-Time&lt;/cell&gt;
        &lt;cell&gt;$426,500&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Our price assuming three-year depreciation (including for the one-off install fees) is $17.5k/month in fixed monthly costs (internet, power, etc.) and $12k/month in depreciation, for $29.5k/month overall.&lt;/p&gt;
    &lt;p&gt;We compare our costs to two main providers: AWS’s public pricing numbers as a baseline, and Cloudflare’s discounted pricing for 30PB of storage. It’s important to note that AWS egress would be substantially lower if we utilized AWS GPUs. This is not reflected on our graph because AWS GPUs are priced at substantially above market prices and large clusters are difficult to attain, untenable at our compute scales.&lt;/p&gt;
    &lt;p&gt;Here are the pricing breakdowns:&lt;/p&gt;
    &lt;head rend="h3"&gt;AWS Pricing Breakdown&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Cost Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;$0.021/GB/month&lt;/cell&gt;
        &lt;cell&gt;$630,000&lt;/cell&gt;
        &lt;cell&gt;For data over 500TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Egress&lt;/cell&gt;
        &lt;cell&gt;$0.05/GB&lt;/cell&gt;
        &lt;cell&gt;$500,000&lt;/cell&gt;
        &lt;cell&gt;Entire dataset egressed quarterly (10 PB/month)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total AWS Monthly&lt;/cell&gt;
        &lt;cell&gt;$1,130,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare R2 Pricing&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Pricing Tier&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Published Rate&lt;/cell&gt;
        &lt;cell&gt;$0.015/GB/month&lt;/cell&gt;
        &lt;cell&gt;$450,000&lt;/cell&gt;
        &lt;cell&gt;No egress fees&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Estimated Private Pricing [3] 3. Cloudflare has a more reasonable estimate for the 30 PB, placing it at an overall monthly cost of $270k without egress fees. We also have bulk-discounted pricing estimates after getting pricing quotes—this was our main point of comparison for the datacenter.&lt;/cell&gt;
        &lt;cell&gt;$0.009/GB/month&lt;/cell&gt;
        &lt;cell&gt;$270,000&lt;/cell&gt;
        &lt;cell&gt;Estimated rate for &amp;gt;20 PB scale&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That brings monthly costs to $38/TB/month for AWS, $10/TB/month for Cloudflare, and $1/TB/month for our datacenter—about 38x lower and 10x lower respectively. (At the very cheapest end of the spectrum, Backblaze has a $6/TB product that is unsuitable for model training due to egress speed limitations; their $15/TB Overdrive AI-specific storage product is closer to Cloudflare’s in price &amp;amp; performance)&lt;/p&gt;
    &lt;p&gt;While we use Cloudflare as a comparison point, we’ve sometimes done too much load for their R2 servers. In particular, in the past we’ve done enough load during large model training runs that they rate-limited us, later confirming we were saturating their metadata layer and the rate limit wasn’t synthetic. Because our metadata on the heap is so simple, and we have a 100Gbps DIA connection, we haven’t ran into any issues there. [4] 4. We love Cloudflare and use many of their products often; we include this anecdote as a fact about our scale being difficult to handle, not as a dig!&lt;/p&gt;
    &lt;p&gt;This setup was and is necessary for our video data pipelines, and we’re extremely happy that we made this investment. By gathering large scale data at low costs, we can be competitive with frontier labs with billions of dollars in capital.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup/The Process&lt;/head&gt;
    &lt;p&gt;We cared a lot about getting this built fast, because this kind of project can easily stretch on for months if not careful. Hence Storage Stacking Saturday, or S3. We threw a hard drive stacking party in downtown SF and got our friends to come, offering food and custom-engraved hard drives to all who helped. The hard drive stacking started at 6am and continued for 36 hours (with a break to sleep), and by the end of that time we had 30 PB of functioning hardware racked and wired up. We brought in contractors for additional help and professional installation later on in the event.&lt;/p&gt;
    &lt;p&gt;People at the hard drive stacking party! Cool shots of the servers&lt;/p&gt;
    &lt;p&gt;Our software is 200 lines of Rust code for writing (to determine the drive to write data onto) and a nginx webserver for reading data, with a simple SQLite db for tracking metadata like which heap node each file is on and what data split it belongs to. We kept this obsessively simple instead of using MinIO or Ceph because we didn’t need any of the features they provided; it’s much, much simpler to debug a 200-line program than to debug Ceph, and we weren’t worried about redundancy or sharding. All our drives were formatted with XFS.&lt;/p&gt;
    &lt;p&gt;The storage software landscape offers many options, but every option available comes with drawbacks. People experienced with Ceph strongly warned us to avoid it unless we were willing to hire dedicated Ceph specialists—our research confirmed this advice. Ceph appears far more complex than justified for most use cases, only worthwhile for companies that absolutely need maximum performance and customizability and are prepared to invest heavily in tuning. Minio presents an interesting option if S3 compatibility is essential, but otherwise remains a bit too fancy for us and similar use-cases. Weka and Vast are absurdly expensive at 2k / TB / year or so and are primarily designed for NVMEs, not spinning disks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Post-Mortem&lt;/head&gt;
    &lt;p&gt;Building the datacenter was a large endeavor and we definitely learned lessons, both good and bad.&lt;/p&gt;
    &lt;head rend="h3"&gt;Things That We Got Correct&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We think the redundancy &amp;amp; capability tradeoffs we made are very reasonable at our disk speeds. We’re able to approximately saturate our 100G network for both read &amp;amp; write.&lt;/item&gt;
      &lt;item&gt;Doing this locally a couple blocks away was well worth it because of the amount of debugging and manual work needed.&lt;/item&gt;
      &lt;item&gt;Ebay is good to find vendors but bad to actually buy things with. After finding vendors, they can often individually supply all the parts we need and provide warranties, which are extremely valuable.&lt;/item&gt;
      &lt;item&gt;100G dedicated internet is pretty important, and much much easier to debug issues with than using cloud products.&lt;/item&gt;
      &lt;item&gt;Having high-quality cable management during the racking process saved us a ton of time debugging in the long run; making it easy to switch up the networking saved us a lot of headache.&lt;/item&gt;
      &lt;item&gt;We had a very strong simplicity prior, and this saved an immense amount of effort. We are quite happy that we didn’t use ceph or minio. Unlike e.g. nginx, they do not work out of the box. We were willing to write a simple Rust script and roughly saturated our network read &amp;amp; write at 100 Gbps without any fancy code.&lt;/item&gt;
      &lt;item&gt;We were basically right about the price and advantages this offered, and did not substantially overestimate the amount of time / effort it would take. While the improvements list is longer than this, most of those are minor; fundamentally we built a cluster rivaling massive clouds for 40x cheaper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Difficult Bits&lt;/head&gt;
    &lt;p&gt;A map of reality only gets you so far—while setting up the datacenter we ran into a couple problems and unexpected challenges. We’ll include a list:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We used frontloaders instead of toploaders for our server rack. This meant we had to screw every single individual drive in—tedious for 2.4k HDDs&lt;/item&gt;
      &lt;item&gt;Our storage was not dense—we could have saved 5x the work on physical placement and screwing by having a denser array of hard drives&lt;/item&gt;
      &lt;item&gt;Shortcuts like daisy-chaining are usually a bad idea. We could have gotten substantially higher read/write speeds without daisy chaining networked nodes, giving each chassis its own HBA (Host Bus Adapter, not a significant cost).&lt;/item&gt;
      &lt;item&gt;Compatibility is key—specifically in networking functionally everything is locked to a specific brand. We had many pain points here. Fiber transceivers will ~never work unless used with the right brand, but copper cables are much more forgiving. FS.com is pretty good and well priced (though their speed estimates were pretty inconsistent); Amazon will also often have the parts you need rapidly.&lt;/item&gt;
      &lt;item&gt;Networking was a substantial cost and required experimentation. We did not use DHCP as most enterprise switches don’t support it and we wanted public IPs for the nodes for convenient and performant access from our servers. While this is an area where we would have saved time with a cloud solution, we had our networking up within days and kinks ironed out within ~3 weeks.&lt;/item&gt;
      &lt;item&gt;We were often bottlenecked by easy access to servers via monitor/keyboard; idle crash carts during setup are helpful.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ideas Worth Trying&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Working KVMs are extremely useful, and you shouldn’t go without them or good IPMI. Physically going to a datacenter is really inconvenient, even if it’s a block away. IPMI is good, but only if you have pretty consistent machines.&lt;/item&gt;
      &lt;item&gt;Think through your management Ethernet network as much as your real network - it’s really nice to be able to SSH into servers while configuring the network, and IPMI is great!&lt;/item&gt;
      &lt;item&gt;Overprovision your network—e.g. if doable it’s worth having 400 Gigabit internally (you can use 100G cards etc for this!)&lt;/item&gt;
      &lt;item&gt;We could have substantially increased density at additional upfront cost by buying 90-drive SuperMicro SuperServers and putting 20TB drives into them. This would allow us to use 2 racks instead of 10, given ours had about the equivalent of 20 AMD 9654s in total CPU capacity, and used less total power.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How You Can Build This Yourself&lt;/head&gt;
    &lt;p&gt;Here’s what you need to replicate our setup.&lt;/p&gt;
    &lt;head rend="h3"&gt;Storage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;10 CPU head nodes.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Intel Rr2000 with Dual Intel Gold 6148 and 128GB of DDR4 ECC RAM per server (which are incredibly cheap and roughly worked for our use cases) but you have a lot of flexibility in what you use.&lt;/item&gt;
          &lt;item&gt;If you use the above configuration you likely won’t be able to do anything at all CPU-intensive on the servers (like on-device data processing or ZFS data compression / deduplication / etc, which is valuable if you’re storing non-video data).&lt;/item&gt;
          &lt;item&gt;Our CPU nodes cost $600 each—it seems quite reasonable to us to spend up to $3k each if you want ZFS / compression or the abiliy to do data processing on-CPU.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;100 DS4246 chassis—each can hold 24 hard drives.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2,400 3.5 inch HDDs—need to be all SATA or all SAS in each chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We would recommend SAS hard drives if possible [5] 5. if you use SAS drives you’ll need to deal with or disable mulipathing, which is reasonably simple as they roughly double speed over similar SATA drives.&lt;/item&gt;
          &lt;item&gt;We used a mix of 12TB and 14TB drives—basically any size should work, roughly the larger the better holding price constant (density makes stacking easier + in general increases resale value).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Physical parts to mount the chassis—you’ll need rails or l-brackets. We used l-brackets which worked well, as we haven’t needed to take the chassis out to slot hard drives. If you buy toploaders, you’ll need rails.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple “crash carts” with monitors and keyboards that allow you to physically connect to your CPU head nodes and configure them—this is invaluable when you’re debugging network issues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A 100 GbE switch&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;A used Arista is fine, should be QSFP28, should cost about $1-2k.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HBAs (Host Bus Adapters), which connect your head nodes to your DS4246 chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The best configuration we tried was with Broadcom 9305-16E HBAs, with 3x HBAs per server (make sure your server has physical space for them!) with SFF-8644 to QSFP mini SAS cables.&lt;/item&gt;
          &lt;item&gt;There are 4 slots per HBA, so you can cable each DS4246 chassis directly to the HBA. [6] 6. The option we ended up going with for convenience was putting LSI SAS9207-8e HBAs, which have 2 ports each, into the CPU head nodes- then daisy-chaining the DS4246s together with QSFP+ to QSFP+ DACs.. We deployed this on Storage Stacking Saturday, then while debugging speeds tried the above method on one of the servers and got to ~4 Gbps per chassis-but didn’t find it worth it to swap everything out in pure labor because of the way we had set up some of our head nodes such that they were difficult to take out. Insofar as it is reasonably cheap to just do the above thing to start and we’ve tested it to work, you should probably do as we say, not as we did in this case!&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network cards (NICs).&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Mellanox ConnectX-4 100GbE. Make sure they come in Ethernet mode and not Infiniband mode for ease of config.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DAC (Direct Attach Copper) or AOC (Active Optical) cables, to connect the NICs in your head nodes to your switch and therefore the internet. You almost certainly want DACs if your racks are close together, as they are far more compatible with arbitrary networking equipment than AOCs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We would recommend that you find a supplier to sell you the CPU head nodes with the HBAs and NICs installed—there are a number of used datacenter / enterprise parts suppliers who are willing to do this. This is a substantial positive because it means that you don’t have to spend hours installing the HBAs/NICs yourself and can have a substantially higher degree of confidence in your operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Serial cables—you’ll need these to connect to your switch!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Optional but recommended: an Ethernet management network of some kind. If you can’t easily get ethernet, we’d recommend getting a wifi adapter like this and then a ethernet switch like this —it’s substantially easier to set up than the 100GbE, is a great backup for when that’s not working, and will allow you to do ~everything over SSH from the comfort of the office instead of in the datacenter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Datacenter Requirements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3.5 kW of usable power per cabinet, with 10 4U chassis + 1 2U (cabinets are 42U tall)&lt;/item&gt;
      &lt;item&gt;1 spare cabinet for the 1U or 2U 100GbE switch (you can obviously also just swap out one of the 4U chassis in another cabinet for the switch).&lt;/item&gt;
      &lt;item&gt;1 42U cabinet per 3 PB of storage&lt;/item&gt;
      &lt;item&gt;A dedicated 100G connection (will come in as a fiber pair probably via QSFP28 LR4, but confirm with your datacenter provider before buying parts here!)&lt;/item&gt;
      &lt;item&gt;Ideally physically near your office—there is a lot of value in being able to walk over and debug issues instead of e.g. dealing with remote hands services to get internet to the nodes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some setup tips:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure to first properly configure your switch. Depending on your switch model this should be relatively straightforward—you’ll need to physically connect to the switch and then configure the specific port that your 100GbE is connected to (you’ll get a fiber cross-connect from your datacenter that you should plug into a QSFP28 transceiver. Make sure that you get a transceiver that is compatible in form with the ISP, probably LR4, and specifically branded with your switch brand, otherwise it is very unlikely to work). Depending on your ISP you might have to talk to them to make sure that you can get “light” through the fiber cables from both ends, which might involve rolling the fiber and otherwise making sure it’s working properly. &lt;list rend="ul"&gt;&lt;item&gt;If your switch isn’t working / you haven’t configured one before, I’d suggest trying to directly plug the fiber cable from the ISP into one of your 10 heap servers, making sure to buy a transceiver that is compatible with your NIC brand (e.g. Mellanox). Once you get it working from there, move over to your switch and get it working.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Once you can connect to the internet from your switch (simply ping 1.1.1.1 to check) you are ready to set up the netplans for the individual nodes. this is most easily done during the Ubuntu setup process, which will walk you through setting up internet for your CPU head nodes, but is also doable outside of that&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you have internet access to your nodes and have properly connected 1 cable to each DS4246, you should format &amp;amp; mount the drives on each node, test that all of them are properly working, and then you are ready to deploy any software you want.&lt;/p&gt;
    &lt;p&gt;If you end up building a similar storage cluster based on this writeup we’d love to hear from you—we’re very curious what can be improved, both in our guidance and in the object-level process. You can reach us at [email protected]&lt;/p&gt;
    &lt;p&gt;If you came away from this post excited about our work, we’d love to chat. We’re a research lab currently focused on pretraining models to use computers, with the long-term goal of building general models that can learn in-context and do arbitrary tasks while aligned with human values; we’re hiring top researchers and engineers to help us train these. If you’re interested in chatting, shoot us an email at [email protected].&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://si.inc/posts/the-heap/"/><published>2025-10-01T15:00:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438501</id><title>Ask HN: Who wants to be hired? (October 2025)</title><updated>2025-10-01T19:08:04.432330+00:00</updated><content>&lt;doc fingerprint="301182753412e0e7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Share your information if you are looking for work. Please use this format:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  Location:
  Remote:
  Willing to relocate:
  Technologies:
  Résumé/CV:
  Email:
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Please only post if you are personally looking for work. Agencies, recruiters, job boards, and so on, are off topic here.&lt;/p&gt;
      &lt;p&gt;Readers: please only email these addresses to discuss work opportunities.&lt;/p&gt;
      &lt;p&gt;There's a site for searching these posts at https://www.wantstobehired.com.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45438501"/><published>2025-10-01T15:01:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438503</id><title>Ask HN: Who is hiring? (October 2025)</title><updated>2025-10-01T19:08:03.786181+00:00</updated><content>&lt;doc fingerprint="3651195a341ae364"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, https://amber-williams.github.io/hackernews-whos-hiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss these other fine threads:&lt;/p&gt;&lt;p&gt;Who wants to be hired? https://news.ycombinator.com/item?id=45438501&lt;/p&gt;&lt;p&gt;Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45438502&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45438503"/><published>2025-10-01T15:01:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438704</id><title>MIT technology can see microbes from 90 meters away</title><updated>2025-10-01T19:08:03.304890+00:00</updated><content>&lt;doc fingerprint="bec452110dbf0459"&gt;
  &lt;main&gt;
    &lt;p&gt;Nature has evolved a stunning array of biosensors for detecting the physical world.&lt;/p&gt;
    &lt;p&gt;A single E. coli cell, for example, can precisely sense chemical gradients and “swim” toward or away from them. Some bird species, including robins and warblers, can see magnetic fields using cryptochrome proteins embedded in their eyes to guide them during their annual migration. Bogong moths use photons from distant stars as a compass while soaring 1,000 kilometers across southeast Australia. In other words, organisms can sense not only tastes and smells, but also individual molecules, magnetic fields, and infrared or ultraviolet light.&lt;/p&gt;
    &lt;p&gt;Humans have long used other creatures’ senses to aid and extend our own, too. As far back as 1,000 BCE, humans employed pigeons to carry messages across cities and kingdoms, taking advantage of their remarkable homing instinct. Dogs’ superior sense of smell is often used to sniff out disease, truffles, contraband, and explosives. And today, the city of Poznań, in Poland, uses just eight mussels to monitor their water quality.1&lt;/p&gt;
    &lt;p&gt;But increasingly, over the last quarter century, scientists have not only used entire organisms to sense the natural world, but have also taken particular genes from those organisms and adapted them into molecular biosensors. Just as a smoke detector has a sensor that detects particles in the air and a buzzer that then alerts us, all human-made biosensors have two basic components.&lt;/p&gt;
    &lt;p&gt;The first is the sensor itself — an enzyme, antibody, or engineered cell — that physically recognizes a target, whether a pollutant, virus, or rise in temperature. The second is the transducer, which converts that recognition event into a signal we can perceive, such as a glowing light.&lt;/p&gt;
    &lt;p&gt;Although bioengineers have adapted hundreds of biosensors from nature, they have been less successful in making better transducers.2 Nearly every biosensor today still relies on a narrow set of outputs (aka “reporters”), such as green fluorescent protein (GFP), luciferase, or colorful pigments. Most transducers can only be seen from close up with a direct line of sight, usually using a microscope. And almost all man-made reporters fail to work inside the body or at a distance. This is because visible light does not penetrate solid materials, such as human skin, and easily “blends in” with other photons in the environment.3&lt;/p&gt;
    &lt;p&gt;Recently, however, bioengineers have developed transducers that transcend such limitations. To make biosensors that work inside the body, scientists have discovered genetically encoded transducers that can be measured using ultrasound or even MRI machines. And for a recent paper in Nature Biotechnology, scientists have reported — for the first time — a new type of transducer that can even be seen from up to 90 meters away using “hyperspectral” cameras mounted to drones. This new technology makes it feasible to monitor individual molecules, as sensed by engineered bacteria, across entire ecosystems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hyperspectral Photos&lt;/head&gt;
    &lt;p&gt;The first hyperspectral cameras were developed in the early 1980s by NASA scientists, who wanted to capture information about Earth, including mineral deposits and ocean algal blooms, from the air. Unlike conventional cameras, which record just three bands of light (red, green, blue), hyperspectral cameras split incoming light into hundreds of narrow spectral bands, including ultraviolet and near-infrared wavelengths.&lt;/p&gt;
    &lt;p&gt;Because each type of molecule absorbs and reflects light in a distinct way, the camera can be mounted onto satellites and used to record a full spectrum for every pixel on the ground. In plants, for example, these cameras can quantify shifts in chlorophyll levels because those molecules strongly absorb light in the blue and red regions. For soils, the spectra contain characteristic dips and peaks that correspond to moisture levels.&lt;/p&gt;
    &lt;p&gt;But the idea that these same cameras could be used to detect bacteria required a leap of imagination. It first came to Chris Voigt, professor of biological engineering at MIT, while touring a military facility, where soldiers explained how hyperspectral drones were being used to spot plastic objects from the sky. Foreign militaries sometimes hide explosives or sensors inside plastic casings and disguise them as rocks, but because real rocks reflect light differently than plastic dupes, hyperspectral cameras can distinguish between them.&lt;/p&gt;
    &lt;p&gt;If the military can distinguish plastic from rock, Voigt wondered, why not microbes from soil?&lt;/p&gt;
    &lt;p&gt;The work to answer this question fell to Yonatan Chemla and Itai Levin, a postdoctoral fellow and graduate student in Voigt’s laboratory. Their first challenge was to find molecules that cells make that could produce a distinctive hyperspectral fingerprint visible from a distance. So the duo began by searching through hundreds of thousands of metabolites listed in scientific databases, finding that only about 100 have any recorded absorption spectra.&lt;/p&gt;
    &lt;p&gt;Upon realizing that we don’t understand how the overwhelming majority of biomolecules reflect light, Chemla and Levin decided to investigate themselves. They bought a hyperspectral camera and a large number of purified molecules from online chemical suppliers — such as indigo and porphyrins — and started testing them in the laboratory. They sprayed these molecules onto soils or rocks, took pictures, and then tried to work out which ones produced a clear signal against background noise.&lt;/p&gt;
    &lt;p&gt;The duo also used computational tools to identify candidate molecules that might act as hyperspectral reporters. Together with collaborators at MIT, they ran quantum chemistry simulations on a selection of 20,000 metabolites to predict how each one would respond to light. These simulations calculated which wavelengths of light each chemical would absorb, and how strong those peaks would be.4 After running these computational tests, Chemla and Levin filtered this list down to a few hundred with unusual peaks or that absorbed light in parts of the spectrum where biology is usually quiet, especially near-infrared wavelengths.&lt;/p&gt;
    &lt;p&gt;Finally, they considered which of these molecules would be easiest and most efficient for a microbe to make, favoring ones that could be made by slightly altering natural pathways or requiring the addition of only a few recombinant genes. Since microbes can have very different metabolisms, they also weighed which hosts would be the best for each possible molecule. After this winnowing process, they ended up with just two: biliverdin IXα made by Pseudomonas putida, and bacteriochlorophyll a made by Rhodocyclus gelatinosus.&lt;/p&gt;
    &lt;p&gt;Biliverdin IXα is a green pigment that naturally forms when heme, the molecule carrying oxygen in red blood cells, is broken down and recycled. To make it in P. putida, the team only needed to add two enzymes. Bacteriochlorophyll a, on the other hand, is a photosynthetic pigment found in purple bacteria.5 R. gelatinosus is itself a purple bacterium, meaning that all the team needed to do was amend its existing genome to produce much larger quantities of bacteriochlorophyll a.&lt;/p&gt;
    &lt;p&gt;With these two engineered microbial strains in hand, the researchers traveled to Fort Devens in Massachusetts — alongside two undergraduate students, Anna Johnson and Yueyang Fan — and sprayed the cells onto little patches of soil. They flew a hyperspectral drone overhead and took pictures of one acre, or about 4,000 square meters, across the entire military facility. Using a hyperspectral detection algorithm that separated the molecular signal from background “noise” of soil and dirt, Chemla and Levin could clearly identify the engineered microbes from up to 90 meters away.6&lt;/p&gt;
    &lt;p&gt;Alas, the cells were layered on top of sand, in direct line of sight to the camera. But in many cases, the things we want to sense — like explosives or pathogens invading plant roots — are hidden underground. Chemla is now searching for volatile molecules that diffuse upward through the soil and into the air, creating a spectral signature that a camera can detect from high above (possibly even from outer space.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Environmental Release&lt;/head&gt;
    &lt;p&gt;Despite this scientific breakthrough, it will be difficult to move these biosensors into the real world. Researchers have been testing engineered microbes in field trials for the last four decades, but few have been commercialized.&lt;/p&gt;
    &lt;p&gt;Field trials for genetically-engineered microbes peaked in the early 1990s but have fallen off since then, mainly due to increased regulations and mixed field trial results. In the late 1980s, engineered Agrobacterium radiobacter K1026 was approved in both Australia and the U.S. to fight crown gall disease in trees. (The microbe outcompetes disease-causing bacteria, killing them.)&lt;/p&gt;
    &lt;p&gt;But getting approval to release a microbe into the wild, without containment, can be incredibly arduous. The regulatory pathway is divided across the EPA, USDA, and FDA. Each agency has jurisdiction depending on the intended use; pesticides fall to the EPA, other agricultural products go to the USDA, and ingestible microbes fall under the province of the FDA. Anything that does not easily fit into these categories, including environmental biosensors, is lumped under the EPA’s Toxic Substances Control Act, or TSCA.&lt;/p&gt;
    &lt;p&gt;The TSCA regulates genetically engineered microbes based on their method of engineering, rather than the product itself. This practice is outdated and should be revised, Chemla says. Any microbe containing DNA from another genus — say, moving a gene from Escherichia coli into Pseudomonas putida — is flagged by the TSCA and unlikely to get approval, even if researchers can prove that the product is safe. More than 200 TSCA submissions were filed between 1987 and 2018, but none of those submissions have led to a commercialized product.&lt;/p&gt;
    &lt;p&gt;There are ways to skirt these regulations, though. Pivot Bio sells genetically-engineered microbes that colonize plant roots and convert atmospheric nitrogen (N₂) into ammonia (NH₃), a chemical form that plants can use. This reduces the amount of fertilizer needed for a field, thus decreasing the leaching of fertilizer byproducts into water.7&lt;/p&gt;
    &lt;p&gt;Pivot Bio sidestepped some regulatory hurdles by avoiding the transfer of genes from one species to another; they simply remodeled their organism’s existing genome. The company still must get USDA approval to ship its product across state lines, but that is a simpler and less insurmountable regulatory hurdle.&lt;/p&gt;
    &lt;p&gt;In the case of hyperspectral reporters, there may be similar ways to circumvent the most onerous regulations. Even in this study, the R. gelatinosus strain engineered to make bacteriochlorophyll a did not have any DNA from foreign microbes. It could, in principle, sidestep the TSCA regulations. A startup called Fieldstone Bio has spun out from the Voigt laboratory with the goal of commercializing this hyperspectral technology.&lt;/p&gt;
    &lt;p&gt;Regardless, the barrier to commercializing these biosensors is not scientific feasibility but rather a patchwork of rules written long before anyone imagined microbes capable of broadcasting messages into space.&lt;/p&gt;
    &lt;p&gt;Still, it’s promising to see that synthetic biology is moving past its reliance on visible light toward a broader range of transducers that let us measure biology in places once thought inaccessible, from the molecules inside a tumor to antibiotic resistance genes hidden in soil. The challenge ahead is not discovering what cells can sense, but engineering more reliable ways for them to communicate those impressions back to us.&lt;/p&gt;
    &lt;p&gt;Niko McCarty is a founding editor of Asimov Press.&lt;/p&gt;
    &lt;p&gt;Thanks to Xander Balwit and Ella Watkins-Dulaney for reading drafts of this.&lt;/p&gt;
    &lt;p&gt;Cite: McCarty, Niko. “Seeing Microbes from the Sky.” Asimov Press (2025). https://doi.org/10.62211/23jr-64kt&lt;/p&gt;
    &lt;p&gt;These mussels are used as natural biosensors because they filter large amounts of water and quickly react to pollutants. When they sense harmful chemicals, heavy metals, or sudden changes in water quality, they clamp their shells shut to protect themselves. When they close, a piece of metal hot glued to their shell completes a circuit which alerts the city to check their water system.&lt;/p&gt;
    &lt;p&gt;The synthetic biology community borrows many terms from computer science and electrical engineering. In electrical engineering, a transducer is a part that converts what a sensor has detected into electrical signals. A biological transducer is any sort of read out that signals what a biosensor has detected.&lt;/p&gt;
    &lt;p&gt;Visible wavelengths of light only penetrate about one millimeter into the body, for example. There is a tissue transparency window between 800 and 950 nanometers, though, in which light penetrates about a centimeter.&lt;/p&gt;
    &lt;p&gt;They used three databases, called BKMS, MetaCyc, and Rhea.&lt;/p&gt;
    &lt;p&gt;Bacteriochlorophyll a absorbs infrared light with a wavelength of 860 nanometers.&lt;/p&gt;
    &lt;p&gt;The camera could see the microbes provided there were at least 4 million cells per square centimeter of sand. This is quite a large number of cells, though, as a square centimeter of human skin has between 100 thousand and one million cells. A single gram of soil usually contains hundreds of millions of microbes.&lt;/p&gt;
    &lt;p&gt;The Haber-Bosch process, used to make ammonia, also accounts for between 1-2 percent of all global CO2 emissions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.asimov.press/p/hyperspectral"/><published>2025-10-01T15:15:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45439670</id><title>No more "check mail from other accounts" in Gmail web</title><updated>2025-10-01T19:08:03.035459+00:00</updated><content>&lt;doc fingerprint="fda841133f20facd"&gt;
  &lt;main&gt;&lt;p&gt;Starting January 2026, Gmail will no longer provide support for the following:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Gmailify: This feature allows you to get special features like spam protection or inbox organization applied to your third-party email account. Learn more about Gmailify.&lt;/item&gt;&lt;item&gt;POP: This feature allows you to read your messages from a third-party account in Gmail. Unlike IMAP connections, POP only works with a single device and doesn’t sync your email in real time. Instead, emails are downloaded, and you decide how often you want to download new emails. As an alternative, you can still link your third-party accounts in the Gmail app.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;These changes help provide the most secure and current options to access your messages in Gmail.&lt;/p&gt;&lt;head rend="h2"&gt;Learn about changes to Gmailify&lt;/head&gt;&lt;p&gt;You won’t be able to get specific features in Gmail applied to your third-party account, like:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Spam protection&lt;/item&gt;&lt;item&gt;Better email notifications on mobile&lt;/item&gt;&lt;item&gt;Inbox categories&lt;/item&gt;&lt;item&gt;Faster search with advanced search operators&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;What you need to do&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;You can still read and send emails from your other account within the Gmail app. This uses a standard IMAP connection, which is supported in the Gmail mobile app.&lt;/item&gt;&lt;item&gt;Learn how to add another email account to the Gmail app.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Learn about changes to POP connections&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Gmail will no longer support checking emails from third-party accounts through POP.&lt;/item&gt;&lt;item&gt;The option to "Check mail from other accounts" will no longer be available in Gmail on your computer.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;What you need to do&lt;/head&gt;&lt;p&gt;Important: If you have a work or school account, your administrator can help migrate your email data into Google Workspace. Learn more about the data migration service.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;To continue to receive messages from your other account in Gmail, you need to set up IMAP access. &lt;list rend="ul"&gt;&lt;item&gt;Check your email provider’s documentation for instructions on how to enable IMAP for your account.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;To read your messages from your other account, use the Gmail app. Learn how to add another email account to the Gmail app.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;Will I lose the emails I already imported?&lt;p&gt;No. All messages synced before the deprecation stay in Gmail.&lt;/p&gt;&lt;p&gt;Yes. For third-party accounts like Yahoo! and Outlook, you can add them to the Gmail mobile app on Android and iPhone and iPad.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://support.google.com/mail/answer/16604719?hl=en"/><published>2025-10-01T16:25:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45439721</id><title>Fossabot: AI code review for Dependabot/Renovate on breaking changes and impacts</title><updated>2025-10-01T19:08:02.859107+00:00</updated><content>&lt;doc fingerprint="af4799a7096b4451"&gt;
  &lt;main&gt;
    &lt;p&gt;Today we're announcing fossabot, a new AI Agent for making strategic dependency updates, backed by a comprehensive accuracy, consistency, and correctness framework.&lt;/p&gt;
    &lt;p&gt;fossabot is able to deliver completed work just like an engineer, including researching new versions, finding app impact and adapating code if needed. This product fulfills our philosophy for automating dependency updates and EdgeBit acquisition.&lt;/p&gt;
    &lt;p&gt;fossabot is currently available as a public preview, with a focus on the JavaScript and TypeScript ecosystems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Your dependencies are simultaneously moving too fast and too slow&lt;/head&gt;
    &lt;p&gt;For a decade, FOSSA has protected businesses from open source risk in two large categories: compliance and security. We’ve identified a new, third category of risk that is emerging: dependency churn and update stagnation.&lt;/p&gt;
    &lt;p&gt;AI coding agents churning out new repos and dependencies trees faster than we can follow.&lt;/p&gt;
    &lt;p&gt;At the same time, crown jewel apps can’t keep up with the fast pace of upstream development and fall more behind.&lt;/p&gt;
    &lt;p&gt;Neither are good, but fossabot is here to help...as if your best engineer managed updates 24/7.&lt;/p&gt;
    &lt;head rend="h3"&gt;Every dependency update program is broken&lt;/head&gt;
    &lt;p&gt;The root of the problem is that every enterprise dependency update program is broken. Why? Our tools can’t make strategic updates like our engineers are capable of.&lt;/p&gt;
    &lt;p&gt;Instead, enterprises focus is making the smallest update possible to fix an alert, only to do it again next month. No time is devoted to figuring out how to upgrade to the latest version of a package and the benefits it may bring to the app.&lt;/p&gt;
    &lt;p&gt;fossabot, our dependency updating AI agent, is capable of large complexity upgrades – the ones that require a senior engineer because they’re always an unexpected multi-hour research and coding task.&lt;/p&gt;
    &lt;p&gt;Bump lodash from &lt;code&gt;4.17.20&lt;/code&gt; to &lt;code&gt;4.17.21&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary by fossabot&lt;/head&gt;
    &lt;p&gt;I recommend merging this lodash update from 4.17.20 to 4.17.21. This is a patch release that fixes several security vulnerabilities and includes performance improvements. Your application's usage patterns are compatible with this update.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;•Analyzed 47 files using lodash utilities across components/, utils/, and services/&lt;/item&gt;
      &lt;item&gt;•Verified no deprecated methods or breaking changes affect your codebase&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Change Details&lt;/head&gt;
    &lt;p&gt;1. Fixed prototype pollution vulnerability in merge function&lt;/p&gt;
    &lt;p&gt;2. Improved input validation for template method&lt;/p&gt;
    &lt;p&gt;3. Enhanced sanitization in defaultsDeep&lt;/p&gt;
    &lt;p&gt;fossabot started out as an internal tool and became invaluable to our engineers and trusted testers, so we’re releasing it as a public preview for all to use.&lt;/p&gt;
    &lt;p&gt;fossabot is available as a GitHub app and all users get $15 in free usage credit each month.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does fossabot work so well?&lt;/head&gt;
    &lt;p&gt;fossabot proposes strategic updates because it can balance risk vs. reward, understand breaking changes in the context of your app, and even adapt code to handle newer paradigms.&lt;/p&gt;
    &lt;p&gt;Existing updaters like Dependabot or Renovate can’t do this reasoning, so they end up being configured to be “dumb,” like patch releases only.&lt;/p&gt;
    &lt;p&gt;Plus, mechanically making the update is not the hard and slow part. It’s the research and understanding of risk to your app that takes forever and ultimately relegates most updates into the backlog forever.&lt;/p&gt;
    &lt;head rend="h3"&gt;Codebase Reasoning&lt;/head&gt;
    &lt;p&gt;fossabot analysis determines the impact of an update to your specific codebase and usage of dependencies instead of making guesses about compatibility, which allows for smart reasoning. Examples of this reasoning include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a rewritten React library and update your component to use the more modern syntax&lt;/item&gt;
      &lt;item&gt;Upgrade a major version of a library safely because you use APIs in forward-compatible ways&lt;/item&gt;
      &lt;item&gt;Adapt your code to an undeclared behavior change in a patch update&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a partial excerpt of this reasoning in action:&lt;/p&gt;
    &lt;p&gt;fossabot uses a perfect balance of hard facts from static analysis paired with a scalable and detail-oriented AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scale Through AI&lt;/head&gt;
    &lt;p&gt;fossabot outperforms human engineers because it can scale beyond what a reasonable person would do. It researches harder, deeper, and longer, with perfect memory about your first-party code, the dependency code and the library’s release notes, migration guides, and docs.&lt;/p&gt;
    &lt;p&gt;While a human would become fatigued after an hour (or even minutes), fossabot will keep going until every modified function is triaged and every impact is understood throughout your entire codebase.&lt;/p&gt;
    &lt;p&gt;No engineer can hold a full picture of dependency usage, especially when multiple teams are involved. fossabot is able to take in more analysis and relationships that can be mapped out in your brain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Delivers Completed Tasks&lt;/head&gt;
    &lt;p&gt;Customers tell us that understanding the level of effort for a change can be just as hard as the update itself. When fossabot is in charge of your updates, you can skip all of this toil and receive completed tasks, delivered right to a pull request.&lt;/p&gt;
    &lt;p&gt;fossabot understands its limitations and can request assistance to “last-mile” an update across the finish line. Backed by our evaluation framework and ability to classify different types of updates, we’re confident in fossabot’s ability to handle large complexity updates in the JavaScript/TypeScript ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Internal Tool to Public Preview&lt;/head&gt;
    &lt;p&gt;Earlier this year, FOSSA engineers hypothesized that with the right context, we could eliminate the toil from dependency updates. We started providing a custom AI framework with details from FOSSA’s dependency metadata scanning, upgrade path guidance, and open source health signals. This grew into a robust breaking change detection engine that continues to surprise us with its detail and accuracy.&lt;/p&gt;
    &lt;p&gt;With breaking changes found, the next challenge was impact detection for each customer’s codebase. Static analysis is the ideal tool for this, which led to a partnership and eventual acquisition of EdgeBit, which pioneered a new type analysis that is designed for dependency update use-cases.&lt;/p&gt;
    &lt;p&gt;Static analysis prevents the AI agent from making silly mistakes, and in our experience, perfectly balances the desired fuzziness you gain from using AI agents and sub agents. fossabot resembles a “focused agent” that resembles a pipeline for determinism but includes agentic steps as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accuracy, Consistency, Correctness&lt;/head&gt;
    &lt;p&gt;While iterating on fossabot, we quickly realized that the evaluation framework and ground truth dataset was just as important as the tool itself, and in many ways, just as challenging as writing the code.&lt;/p&gt;
    &lt;p&gt;fossabot continually scores itself on Accuracy, Consistency, Correctness (ACC) against a set of validated dependency updates with varying degrees of breaking changes, changed lines of code and usage of those libraries in real-world apps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Accuracy, Consistency, Correctness by Group &amp;amp; Complexity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Group&lt;/cell&gt;
        &lt;cell&gt;Complexity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;routine_minor_updates&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;multi_dependency_updates&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;major_version_upgrades&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;dev_dependencies&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This process quickly highlighted a key learning: the importance of weighted scoring in our evaluation. A false positive (where the tool incorrectly deems a breaking change safe) carries a much higher cost in terms of potential disruption and lost trust than a false negative (where a safe update is flagged for extra scrutiny). This phase also helped us debunk early, overly simplistic assumptions, such as the fallacy that all major version upgrades are inherently breaking.&lt;/p&gt;
    &lt;p&gt;Our public preview is targeted at the JavaScript/TypeScript ecosystem because our ACC dataset is robustly populated — other ecosystems will follow shortly as we build out more ground truth.&lt;/p&gt;
    &lt;p&gt;We believe that several design decisions set at the genesis of fossabot make it a trusted foundation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Striving for determinism at key steps&lt;/item&gt;
      &lt;item&gt;Smartly using static analysis&lt;/item&gt;
      &lt;item&gt;Use AI to be doggedly persistent and detail oriented&lt;/item&gt;
      &lt;item&gt;Measuring ourselves against the ACC ground truth&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We hope fossabot earns your trust, and we’d love your feedback on analysis that looks great or needs refinement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Out fossabot&lt;/head&gt;
    &lt;p&gt;fossabot’s public preview is available as a GitHub app. Every user gets $15 of analysis credit, replenished every month. Let loose the updates!&lt;/p&gt;
    &lt;p&gt;Today, fossabot will auto-analyze Pull Requests opened from Dependabot, Renovate or Snyk. Soon, fossabot will open its own PRs with pre-planning and pre-analysis taken into account.&lt;/p&gt;
    &lt;p&gt;Reach out to get a demo of fossabot and let's figure out how to get your teams caught up on updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fossa.com/blog/fossabot-dependency-upgrade-ai-agent/"/><published>2025-10-01T16:30:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45439883</id><title>DuckDuckGo Donates $25,000 to The Perl and Raku Foundation v2025</title><updated>2025-10-01T19:08:02.486129+00:00</updated><content>&lt;doc fingerprint="7f060808cbcd80d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;DuckDuckGo Donates $25,000 to The Perl and Raku Foundation v2025&lt;/head&gt;
    &lt;p&gt;For the second consecutive year, The Perl and Raku Foundation (TPRF) is overjoyed to announce a donation of USD 25,000 from DuckDuckGo.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;DuckDuckGo has demonstrated how Perl and its ecosystem can deliver power and scale to drive the DuckDuckGo core systems, plug-in framework and Instant Answers. The Foundation is grateful that DuckDuckGo recognises the importance of Perl, and for their generous funding support for a second year through their charitable donations programme.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;– Stuart J Mackintosh, President of The Perl and Raku Foundation&lt;/p&gt;
    &lt;p&gt;Last year’s donation of USD 25,000 from DuckDuckGo was instrumental in helping to fund the foundation’s Core Perl Maintenance Fund and this year’s donation will help to fund more of the same crucial work that keeps the Perl language moving forward.&lt;/p&gt;
    &lt;p&gt;Paul “LeoNerd” Evans is one of the developers who gets regular funding from the Perl Core Maintenance Fund. Here is a short list of just some of the many contributions which Paul has made to core Perl as part of the maintenance fund work:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The builtin module (5.36), making available many new useful language-level utilities that were previously loaded from modules like Scalar::Util&lt;/item&gt;
      &lt;item&gt;The complete feature ‘class’ system (5.38), adding proper object-orientation syntax and abilities&lt;/item&gt;
      &lt;item&gt;Lexical method support (5.42), adding &lt;code&gt;my method&lt;/code&gt;and the&lt;code&gt;$obj-&amp;gt;&amp;amp;method&lt;/code&gt;invocation syntax for better object encapsulation&lt;/item&gt;
      &lt;item&gt;Stabilising some of the recent experiments - signatures (5.36), try/catch (5.40), foreach on multiple vars (5.40)&lt;/item&gt;
      &lt;item&gt;Ability to use the //= and ||= operators in signatures (5.38), performance improvements and named parameters (upcoming in next release)&lt;/item&gt;
      &lt;item&gt;The new any and all keywords (5.42)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We look forward to many more innovative contributions from Paul over the coming year.&lt;/p&gt;
    &lt;p&gt;While TPRF never takes continued support for granted, when it does arrive, it allows the foundation to plan for the future with much greater confidence. Multi-year partnerships with our sponsors allow us to continue to prioritize important work, knowing that we will have the runway that we need to fund the work which helps to sustain the Perl Language and its associated communities.&lt;/p&gt;
    &lt;p&gt;For more information on how to become a sponsor, please contact: olaf@perlfoundation.org&lt;/p&gt;
    &lt;p&gt;"Fireworks" by colink. is licensed under CC BY-SA 2.0.&lt;/p&gt;
    &lt;p&gt;Tags&lt;/p&gt;
    &lt;head rend="h3"&gt;Olaf Alders&lt;/head&gt;
    &lt;p&gt;Dad, Perl hacker, guitar player and swimmer. Olaf is a founder of the MetaCPAN project.&lt;/p&gt;
    &lt;head rend="h5"&gt;Browse their articles&lt;/head&gt;
    &lt;head rend="h3"&gt;Feedback&lt;/head&gt;
    &lt;p&gt;Something wrong with this article? Help us out by opening an issue or pull request on GitHub&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.perl.com/article/duckduckgo-donates-25-000-to-the-perl-and-raku-foundation-v2025/"/><published>2025-10-01T16:42:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45439955</id><title>Codeberg Reaches 300k Projects</title><updated>2025-10-01T19:08:01.983504+00:00</updated><content/><link href="https://codeberg.org/"/><published>2025-10-01T16:48:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45440212</id><title>Why Tech Inevitability is Self-Defeating</title><updated>2025-10-01T19:08:01.760912+00:00</updated><content>&lt;doc fingerprint="2c389bc7ad46364b"&gt;
  &lt;main&gt;
    &lt;p&gt;My dear startup founder friend,&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;You told me that everything is “hyper-determined,” that “life is a big game because of that.” You’re not alone in thinking this: Silicon Valley loves to preach inevitability.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Silicon Valley paradox&lt;/head&gt;
    &lt;p&gt;“Technology has its own agenda.” — Kevin Kelly https://kk.org/thetechnium/the-unabomber-w/&lt;/p&gt;
    &lt;p&gt;“Technology happens because it is possible.” — Sam Altman https://www.nytimes.com/2023/03/31/technology/sam-altman-open-ai-chatgpt.html&lt;/p&gt;
    &lt;p&gt;“Probability that AI exceeds the intelligence of all humans combined by 2030 is ~100%.” — Elon Musk https://x.com/elonmusk/status/1871083864111919134?lang=fr&lt;/p&gt;
    &lt;p&gt;Notice the pattern: inevitability, inevitability, inevitability.&lt;/p&gt;
    &lt;p&gt;Precisely what you believe.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Yet in reality, they could have gone surfing, lived quietly, done nothing. However, they worked obsessively, raised money, built companies, and reshaped the world. That was their agency, fully deployed.&lt;/p&gt;
    &lt;p&gt;You’ll say, “They got lucky, it had to happen, if not them someone else. That’s why we all work ourselves to the bone.”&lt;/p&gt;
    &lt;p&gt;Now hear me out: I believe they worked hard to achieve this outcome and they deserve the credit. But I also believe that with success comes responsibility — responsibility that evaporates the moment y’all imply inevitability.&lt;/p&gt;
    &lt;p&gt;My goal here is to show you why the trope “it was inevitable” is misleading and it shifts responsibility away from those with power.&lt;/p&gt;
    &lt;p&gt;To be clear, versions of this reasoning have appeared often in “pop science” and academic writing so I’m not claiming originality, but I hope to convince you that treating inevitability as fact comes with important downsides and it’s better to manage your life as if you’d had agency.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;inevitability&lt;/head&gt;
    &lt;p&gt;What Silicon Valley calls inevitability is really a rhetorical posture: a way of talking about the future as if it could not be otherwise. It borrows from different philosophical schools of thought (fatalism, determinism, technological determinism) — each distinct, and often incompatible.&lt;/p&gt;
    &lt;p&gt;This patchwork coalesces around the idea that it is inevitable and I don’t matter as an individual (because someone else would have done it). “This will happen nevertheless (even if I choose to fight it).”&lt;/p&gt;
    &lt;p&gt;This is oddly compatible with their obsessive work: being the one who succeeds is merely a matter of luck and we help luck through hard work.&lt;/p&gt;
    &lt;head rend="h2"&gt;True Prophets and False Predictions&lt;/head&gt;
    &lt;p&gt;Tomorrow the sun will rise is a rock-solid prediction. Objects fall to the ground. Everyone dies. These sentences are so reliable we call them laws. We believe in them.&lt;/p&gt;
    &lt;p&gt;Belief is an act of faith: we trust the future will resemble the past, that the laws won’t suddenly change. Yes, that trust has been earned through millennia of confirmation, but it’s still a belief in regularity. Just ask the turkey on Thanksgiving Day, who believed sunrise meant good food and safety…&lt;/p&gt;
    &lt;p&gt;So even laws of nature require belief, and much more so for “social predictions” that are rooted in much fewer facts. When Sam is saying AI is inevitable, how can I tell if this prediction is a prophecy or an advertisement for his company?&lt;/p&gt;
    &lt;p&gt;More generally, when someone makes a prediction I can’t know if it’s true (otherwise I’d be the one making the prediction). Therefore to act on it, I need to believe in it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predictions as Communication&lt;/head&gt;
    &lt;p&gt;A prediction is, by definition, a communicative act and communication theory teaches us that these acts are made to influence us. And I should be cognizant and wary of them (i.e. don’t accept them at face value). My prediction about the sunrise could be here to build belief in the scientific method.&lt;/p&gt;
    &lt;p&gt;No prediction is neutral. Because they talk about the future, predictions are always either attempts to reshape the world, genuine forecasts, or a mix of both. And at a fundamental (epistemological) level, I can never know which is which. That is why inevitability talk is so powerful and so dangerous. They present themselves as a law of physics while it’s only an act of communication whose power comes from people believing in it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Fulfilling Prophecy&lt;/head&gt;
    &lt;p&gt;Since I can’t know whether a prediction is persuasion, foresight or a mix of the two, the important question becomes: how should I act?&lt;/p&gt;
    &lt;p&gt;Let’s analyze each position: “things are inevitable” or “if I fight it I might win.”&lt;/p&gt;
    &lt;p&gt;Assuming you think everything is inevitable, you go into a self-fulfilling prophecy and some circular logic:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“X is inevitable” → Stop trying to change it&lt;/item&gt;
      &lt;item&gt;Stop trying → No agency exercised&lt;/item&gt;
      &lt;item&gt;No agency exercised → X happens&lt;/item&gt;
      &lt;item&gt;Prophecy fulfilled → X was inevitable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can replace X by AGI or anything. E.g., there’s an opportunity to win a prize but you won’t work at it because you’re a loser and everything is determined. And it works positively too: e.g., “I am a future champion” → so I become one.&lt;/p&gt;
    &lt;p&gt;The other case is easier (and healthier): if you assume things are not inevitable then you can change the prediction by exercising your agency. You also accept some uncertainty in the process because it might not work out.&lt;/p&gt;
    &lt;p&gt;If you believe in inevitability, it makes sense not to act. And if you don’t, it makes sense to act.&lt;/p&gt;
    &lt;p&gt;Except, except… Using the previous paragraph, I explained it’s impossible for us to know if a prediction is inevitable or not. But if I use my agency, I have a higher chance of getting where I want things to be. Therefore, the only pragmatically rational stance is to embrace our agency.&lt;/p&gt;
    &lt;p&gt;So the informed position is to treat all predictions I disagree with as something I need to fight to change. Not accept them. Especially if they’re inevitable.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Agency’s Wager&lt;/head&gt;
    &lt;p&gt;Inevitability is rhetoric, not truth. Predictions aren’t laws of nature, they are acts of persuasion. And because no one can ever know how much is determined and how much is open, the only rational stance is to live as agents. Supersonic flight once looked inevitable until people stopped it. You, too, have already reshaped the world once; you can do it again. Don’t give away your power.&lt;/p&gt;
    &lt;p&gt;My dear CEO friend, if you want to change the world, the first step is to believe you can. You may win or lose. Success is never guaranteed. But one thing is certain: the surest way to lose is to not fight.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://deviantabstraction.com/2025/09/29/against-the-tech-inevitability/"/><published>2025-10-01T17:09:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45440387</id><title>Solar leads EU electricity generation as renewables hit 54%</title><updated>2025-10-01T19:08:01.679835+00:00</updated><content>&lt;doc fingerprint="af51f0535bbde596"&gt;
  &lt;main&gt;
    &lt;p&gt;More than half of the European Union’s (EU) electricity came from renewables in the second quarter of 2025, and solar is leading from the front.&lt;/p&gt;
    &lt;p&gt;According to new data from Eurostat, renewable energy sources generated 54% of the EU’s net electricity in Q2 2025, up from 52.7% year-over-year. The growth came mainly from solar, which produced 122,317 gigawatt-hours (GWh) – nearly 20% of the total electricity generation mix.&lt;/p&gt;
    &lt;p&gt;June 2025 was a milestone month: Solar became the EU’s single largest electricity source for the first time ever. It supplied 22% of all power that month, edging out nuclear (21.6%), wind (15.8%), hydro (14.1%), and natural gas (13.8%).&lt;/p&gt;
    &lt;p&gt;Some countries are already nearly 100% renewable. Denmark led with an impressive 94.7% share of renewables in net electricity generated, followed by Latvia (93.4%), Austria (91.8%), Croatia (89.5%), and Portugal (85.6%). At the other end of the spectrum, Slovakia (19.9%), Malta (21.2%), and the Czech Republic (22.1%) lagged behind.&lt;/p&gt;
    &lt;p&gt;In total, 15 EU countries saw their share of renewable generation rise year-over-year. Luxembourg (+13.5 percentage points) and Belgium (+9.1 pp) posted the most significant gains, driven largely by solar power growth.&lt;/p&gt;
    &lt;p&gt;Across the EU, solar made up 36.8% of renewable generation, followed by wind at 29.5%, hydro at 26%, biomass at 7.3%, and geothermal at 0.4%.&lt;/p&gt;
    &lt;p&gt;Read more: EIA: Solar and wind crush coal with 20% more power in 2025&lt;/p&gt;
    &lt;p&gt;The 30% federal solar tax credit is ending this year. If you’ve ever considered going solar, now’s the time to act. To make sure you find a trusted, reliable solar installer near you that offers competitive pricing, check out EnergySage, a free service that makes it easy for you to go solar. It has hundreds of pre-vetted solar installers competing for your business, ensuring you get high-quality solutions and save 20-30% compared to going it alone. Plus, it’s free to use, and you won’t get sales calls until you select an installer and share your phone number with them.&lt;/p&gt;
    &lt;p&gt;Your personalized solar quotes are easy to compare online and you’ll get access to unbiased Energy Advisors to help you every step of the way. Get started here.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electrek.co/2025/09/30/solar-leads-eu-electricity-generation-as-renewables-hit-54-percent/"/><published>2025-10-01T17:22:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45440431</id><title>OpenTSLM: Language Models That Understand Time-Series (Stanford, ETH, Google)</title><updated>2025-10-01T19:08:01.495615+00:00</updated><content>&lt;doc fingerprint="748d54ec193a388b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenTSLM&lt;/head&gt;
    &lt;p&gt;The Future of AI Delivered on Time&lt;/p&gt;
    &lt;p&gt;AI understands text, images, audio, and video.&lt;lb/&gt;But the real world runs on time.&lt;/p&gt;
    &lt;p&gt;Every heartbeat, price tick, sensor pulse, machine log, and user click is a temporal signal.&lt;lb/&gt;Current models can't reason about them.&lt;/p&gt;
    &lt;p&gt;We're changing that.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Class of Foundation Models&lt;/head&gt;
    &lt;p&gt;Time-Series Language Models (TSLMs) are multimodal foundation models with time series as a native modality, next to text, enabling direct reasoning, explanation, and forecasting over temporal data in natural language.&lt;/p&gt;
    &lt;p&gt;Our research shows order-of-magnitude gains in temporal reasoning while running on smaller, faster backbones. TSLMs are not an add-on. They're a new modality for AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Core, Frontier Edge&lt;/head&gt;
    &lt;p&gt;OpenTSLM: Lightweight base models trained on public data, released openly. They set the standard for temporal reasoning and power a global developer and research ecosystem.&lt;/p&gt;
    &lt;p&gt;Frontier TSLMs: Advanced proprietary models trained on specialized data, delivering enterprise-grade performance and powering APIs, fine-tuning, and vertical solutions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our Vision&lt;/head&gt;
    &lt;p&gt;We're building the temporal interface for AI - the layer that connects continuous real-world signals to intelligent decisions and autonomous agents.&lt;/p&gt;
    &lt;p&gt;A universal TSLM will power proactive healthcare, adaptive robotics, resilient infrastructure, and new forms of human-AI collaboration.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;OpenTSLM is a team of scientists, engineers, and builders from ETH, Stanford, Harvard, Cambridge, TUM, CDTM, Google, Meta, AWS, and beyond. We are the original authors of the OpenTSLM paper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.opentslm.com/"/><published>2025-10-01T17:25:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45441069</id><title>Jane Goodall has died</title><updated>2025-10-01T19:08:01.289205+00:00</updated><content>&lt;doc fingerprint="764fb86741a32728"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Jane Goodall, who transformed understanding of humankind by studying chimpanzees, dies at 91&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Share via&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Jane Goodall, the trailblazing naturalist whose intimate observations of chimpanzees in the African wild produced powerful insights that transformed basic conceptions of humankind, has died. She was 91.&lt;/p&gt;
    &lt;p&gt;A tireless advocate of preserving chimpanzees’ natural habitat, Goodall died on Wednesday morning in California of natural causes, the Jane Goodall Institute announced on its Instagram page.&lt;/p&gt;
    &lt;p&gt;“Dr. Goodall’s discoveries as an ethologist revolutionized science,” the Jane Goodall Institute said in a statement.&lt;/p&gt;
    &lt;p&gt;A protege of anthropologist Louis S.B. Leakey, Goodall made history in 1960 when she discovered that chimpanzees, humankind’s closest living ancestors, made and used tools, characteristics that scientists had long thought were exclusive to humans.&lt;/p&gt;
    &lt;p&gt;She also found that chimps hunted prey, ate meat, and were capable of a range of emotions and behaviors similar to those of humans, including filial love, grief and violence bordering on warfare.&lt;/p&gt;
    &lt;p&gt;In the course of establishing one of the world’s longest-running studies of wild animal behavior at what is now Tanzania’s Gombe Stream National Park, she gave her chimp subjects names instead of numbers, a practice that raised eyebrows in the male-dominated field of primate studies in the 1960s. But within a decade, the trim British scientist with the tidy ponytail was a National Geographic heroine, whose books and films educated a worldwide audience with stories of the apes she called David Graybeard, Mr. McGregor, Gilka and Flo.&lt;/p&gt;
    &lt;p&gt;“When we read about a woman who gives funny names to chimpanzees and then follows them into the bush, meticulously recording their every grunt and groom, we are reluctant to admit such activity into the big leagues,” the late biologist Stephen Jay Gould wrote of the scientific world’s initial reaction to Goodall.&lt;/p&gt;
    &lt;p&gt;But Goodall overcame her critics and produced work that Gould later characterized as “one of the Western world’s great scientific achievements.”&lt;/p&gt;
    &lt;p&gt;Tenacious and keenly observant, Goodall paved the way for other women in primatology, including the late gorilla researcher Dian Fossey and orangutan expert Birutė Galdikas. She was honored in 1995 with the National Geographic Society’s Hubbard Medal, which then had been bestowed only 31 times in the previous 90 years to such eminent figures as North Pole explorer Robert E. Peary and aviator Charles Lindbergh.&lt;/p&gt;
    &lt;p&gt;In her 80s she continued to travel 300 days a year to speak to schoolchildren and others about the need to fight deforestation, preserve chimpanzees’ natural habitat and promote sustainable development in Africa. She was in California as part of her speaking tour in the U.S. at the time of her death.&lt;/p&gt;
    &lt;p&gt;Jane Goodall brings “The Book of Hope” to the Los Angeles Times Book Club Feb. 25.&lt;/p&gt;
    &lt;p&gt;Goodall was born April 3, 1934, in London and grew up in the English coastal town of Bournemouth. The daughter of a businessman and a writer who separated when she was a child and later divorced, she was raised in a matriarchal household that included her maternal grandmother, her mother, Vanne, some aunts and her sister, Judy.&lt;/p&gt;
    &lt;p&gt;She demonstrated an affinity for nature from a young age, filling her bedroom with worms and sea snails that she rushed back to their natural homes after her mother told her they would otherwise die.&lt;/p&gt;
    &lt;p&gt;When she was about 5, she disappeared for hours to a dark henhouse to see how chickens laid eggs, so absorbed that she was oblivious to her family’s frantic search for her. She did not abandon her study until she observed the wondrous event.&lt;/p&gt;
    &lt;p&gt;“Suddenly with a plop, the egg landed on the straw. With clucks of pleasure the hen shook her feathers, nudged the egg with her beak, and left,” Goodall wrote almost 60 years later. “It is quite extraordinary how clearly I remember that whole sequence of events.”&lt;/p&gt;
    &lt;p&gt;Wildlife biologist Miguel Ordeñana was passionate about wild animals as a kid. In Jane Goodall, he found hope that would shape his life.&lt;/p&gt;
    &lt;p&gt;When finally she ran out of the henhouse with the exciting news, her mother did not scold her but patiently listened to her daughter’s account of her first scientific observation.&lt;/p&gt;
    &lt;p&gt;Later, she gave Goodall books about animals and adventure — especially the Doctor Dolittle tales and Tarzan. Her daughter became so enchanted with Tarzan’s world that she insisted on doing her homework in a tree.&lt;/p&gt;
    &lt;p&gt;“I was madly in love with the Lord of the Jungle, terribly jealous of his Jane,” Goodall wrote in her 1999 memoir, “Reason for Hope: A Spiritual Journey.” “It was daydreaming about life in the forest with Tarzan that led to my determination to go to Africa, to live with animals and write books about them.”&lt;/p&gt;
    &lt;p&gt;Her opportunity came after she finished high school. A week before Christmas in 1956 she was invited to visit an old school chum’s family farm in Kenya. Goodall saved her earnings from a waitress job until she had enough for a round-trip ticket.&lt;/p&gt;
    &lt;p&gt;She arrived in Kenya in 1957, thrilled to be living in the Africa she had “always felt stirring in my blood.” At a dinner party in Nairobi shortly after her arrival, someone told her that if she was interested in animals, she should meet Leakey, already famous for his discoveries in East Africa of man’s fossil ancestors.&lt;/p&gt;
    &lt;p&gt;She went to see him at what’s now the National Museum of Kenya, where he was curator. He hired her as a secretary and soon had her helping him and his wife, Mary, dig for fossils at Olduvai Gorge, a famous site in the Serengeti Plains in what is now northern Tanzania.&lt;/p&gt;
    &lt;p&gt;Leakey spoke to her of his desire to learn more about all the great apes. He said he had heard of a community of chimpanzees on the rugged eastern shore of Lake Tanganyika where an intrepid researcher might make valuable discoveries.&lt;/p&gt;
    &lt;p&gt;When Goodall told him this was exactly the kind of work she dreamed of doing, Leakey agreed to send her there.&lt;/p&gt;
    &lt;p&gt;It took Leakey two years to find funding, which gave Goodall time to study primate behavior and anatomy in London. She finally landed in Gombe in the summer of 1960.&lt;/p&gt;
    &lt;p&gt;On a rocky outcropping she called the Peak, Goodall made her first important observation. Scientists had thought chimps were docile vegetarians, but on this day about three months after her arrival, Goodall spied a group of the apes feasting on something pink. It turned out to be a baby bush pig.&lt;/p&gt;
    &lt;p&gt;Two weeks later, she made an even more exciting discovery — the one that would establish her reputation. She had begun to recognize individual chimps, and on a rainy October day in 1960, she spotted the one with white hair on his chin. He was sitting beside a mound of red earth, carefully pushing a blade of grass into a hole, then withdrawing it and poking it into his mouth.&lt;/p&gt;
    &lt;p&gt;When he finally ambled off, Goodall hurried over for a closer look. She picked up the abandoned grass stalk, stuck it into the same hole and pulled it out to find it covered with termites. The chimp she later named David Graybeard had been using the stalk to fish for the bugs.&lt;/p&gt;
    &lt;p&gt;“It was hard for me to believe what I had seen,” Goodall later wrote. “It had long been thought that we were the only creatures on earth that used and made tools. ‘Man the Toolmaker’ is how we were defined...” What Goodall saw challenged man’s uniqueness.&lt;/p&gt;
    &lt;p&gt;When she sent her report to Leakey, he responded: “We must now redefine man, redefine tool, or accept chimpanzees as human!”&lt;/p&gt;
    &lt;p&gt;Goodall’s startling finding, published in Nature in 1964, enabled Leakey to line up funding to extend her stay at Gombe. It also eased Goodall’s admission to Cambridge University to study ethology. In 1965, she became the eighth person in Cambridge history to earn a doctorate without first having a bachelor’s degree.&lt;/p&gt;
    &lt;p&gt;In the meantime, she had met and in 1964 married Hugo Van Lawick, a gifted filmmaker who had traveled to Gombe to make a documentary about her chimp project. They had a child, Hugo Eric Louis — later nicknamed Grub — in 1967.&lt;/p&gt;
    &lt;p&gt;Goodall later said that raising Grub, who lived at Gombe until he was 9, gave her insights into the behavior of chimp mothers. Conversely, she had “no doubt that my observation of the chimpanzees helped me to be a better mother.”&lt;/p&gt;
    &lt;p&gt;“So,” Brett Morgen began, “you’ve been telling your story for so many years.&lt;/p&gt;
    &lt;p&gt;She and Van Lawick were married for 10 years, divorcing in 1974. The following year she married Derek Bryceson, director of Tanzania National Parks. He died of colon cancer four years later.&lt;/p&gt;
    &lt;p&gt;Within a year of arriving at Gombe, Goodall had chimps literally eating out of her hands. Toward the end of her second year there, David Graybeard, who had shown the least fear of her, was the first to allow her physical contact. She touched him lightly and he permitted her to groom him for a full minute before gently pushing her hand away. For an adult male chimpanzee who had grown up in the wild to tolerate physical contact with a human was, she wrote in her 1971 book “In the Shadow of Man,” “a Christmas gift to treasure.”&lt;/p&gt;
    &lt;p&gt;Her studies yielded a trove of other observations on behaviors, including etiquette (such as soliciting a pat on the rump to indicate submission) and the sex lives of chimps. She collected some of the most fascinating information on the latter by watching Flo, an older female with a bulbous nose and an amazing retinue of suitors who was bearing children well into her 40s.&lt;/p&gt;
    &lt;p&gt;Her reports initially caused much skepticism in the scientific community. “I was not taken very seriously by many of the scientists. I was known as a [National] Geographic cover girl,” she recalled in a CBS interview in 2012.&lt;/p&gt;
    &lt;p&gt;Her unorthodox personalizing of the chimps was particularly controversial. The editor of one of her first published papers insisted on crossing out all references to the creatures as “he” or “she” in favor of “it.” Goodall eventually prevailed.&lt;/p&gt;
    &lt;p&gt;Her most disturbing studies came in the mid-1970s, when she and her team of field workers began to record a series of savage attacks.&lt;/p&gt;
    &lt;p&gt;The incidents grew into what Goodall called the four-year war, a period of brutality carried out by a band of male chimpanzees from a region known as the Kasakela Valley. The marauders beat and slashed to death all the males in a neighboring colony and subjugated the breeding females, essentially annihilating an entire community.&lt;/p&gt;
    &lt;p&gt;It was the first time a scientist had witnessed organized aggression by one group of non-human primates against another. Goodall said this “nightmare time” forever changed her view of ape nature.&lt;/p&gt;
    &lt;p&gt;“During the first 10 years of the study I had believed ... that the Gombe chimpanzees were, for the most part, rather nicer than human beings,” she wrote in “Reason for Hope: A Spiritual Journey,” a 1999 book co-authored with Phillip Berman. “Then suddenly we found that the chimpanzees could be brutal — that they, like us, had a dark side to their nature.”&lt;/p&gt;
    &lt;p&gt;Critics tried to dismiss the evidence as merely anecdotal. Others thought she was wrong to publicize the violence, fearing that irresponsible scientists would use the information to “prove” that the tendency to war is innate in humans, a legacy from their ape ancestors. Goodall persisted in talking about the attacks, maintaining that her purpose was not to support or debunk theories about human aggression but to “understand a little better” the nature of chimpanzee aggression.&lt;/p&gt;
    &lt;p&gt;“My question was: How far along our human path, which has led to hatred and evil and full-scale war, have chimpanzees traveled?”&lt;/p&gt;
    &lt;p&gt;Her observations of chimp violence marked a turning point for primate researchers, who had considered it taboo to talk about chimpanzee behavior in human terms. But by the 1980s, much chimp behavior was being interpreted in ways that would have been labeled anthropomorphism — ascribing human traits to non-human entities — decades earlier. Goodall, in removing the barriers, raised primatology to new heights, opening the way for research on subjects ranging from political coalitions among baboons to the use of deception by an array of primates.&lt;/p&gt;
    &lt;p&gt;Chimp change&lt;/p&gt;
    &lt;p&gt;Her concern about protecting chimpanzees in the wild and in captivity led her in 1977 to found the Jane Goodall Institute to advocate for great apes and support research and public education. She also established Roots and Shoots, a program aimed at youths in 130 countries, and TACARE, which involves African villagers in sustainable development.&lt;/p&gt;
    &lt;p&gt;She became an international ambassador for chimps and conservation in 1986 when she saw a film about the mistreatment of laboratory chimps. The secretly taped footage “was like looking into the Holocaust,” she told interviewer Cathleen Rountree in 1998. From that moment, she became a globe-trotting crusader for animal rights.&lt;/p&gt;
    &lt;p&gt;In the 2017 documentary “Jane,” the producer poured through 140 hours of footage of Goodall that had been hidden away in the National Geographic archives. The film won a Los Angeles Film Critics Assn. Award, one of many honors it received.&lt;/p&gt;
    &lt;p&gt;In a ranging 2009 interview with Times columnist Patt Morrison, Goodall mused on topics from traditional zoos — she said most captive environments should be abolished — to climate change, a battle she feared humankind was quickly losing, if not lost already. She also spoke about the power of what one human can accomplish.&lt;/p&gt;
    &lt;p&gt;“I always say, ‘If you would spend just a little bit of time learning about the consequences of the choices you make each day’ — what you buy, what you eat, what you wear, how you interact with people and animals — and start consciously making choices, that would be beneficial rather than harmful.”&lt;/p&gt;
    &lt;p&gt;As the years past, Goodall continued to track Gombe’s chimps, accumulating enough information to draw the arcs of their lives — from birth through sometimes troubled adolescence, maturity, illness and finally death.&lt;/p&gt;
    &lt;p&gt;She wrote movingly about how she followed Mr. McGregor, an older, somewhat curmudgeonly chimp, through his agonizing death from polio, and how the orphan Gilka survived to lonely adulthood only to have her babies snatched from her by a pair of cannibalistic female chimps.&lt;/p&gt;
    &lt;p&gt;Her reaction in 1972 to the death of Flo, a prolific female known as Gombe’s most devoted mother, suggested the depth of feeling that Goodall had for the animals. Knowing that Flo’s faithful son Flint was nearby and grieving, Goodall watched over the body all night to keep marauding bush pigs from violating her remains.&lt;/p&gt;
    &lt;p&gt;“People say to me, thank you for giving them characters and personalities,” Goodall once told CBS’s “60 Minutes.” “I said I didn’t give them anything. I merely translated them for people.”&lt;/p&gt;
    &lt;p&gt;Woo is a former Times staff writer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.latimes.com/obituaries/story/2025-10-01/jane-goodall-chimpanzees-dead"/><published>2025-10-01T18:10:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45441219</id><title>Announcing Tinker</title><updated>2025-10-01T19:08:01.151484+00:00</updated><content>&lt;doc fingerprint="1c5d0f35d932c938"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing Tinker&lt;/head&gt;
    &lt;p&gt;Today, we are launching Tinker, a flexible API for fine-tuning language models. It empowers researchers and hackers to experiment with models by giving them control over the algorithms and data while we handle the complexity of distributed training. Tinker advances our mission of enabling more people to do research on cutting-edge models and customize them to their needs.&lt;/p&gt;
    &lt;p&gt;Tinker lets you fine-tune a range of large and small open-weight models, including large mixture-of-experts models such as Qwen-235B-A22B. Switching from a small model to a large one is as simple as changing a single string in your Python code.&lt;/p&gt;
    &lt;p&gt;Tinker is a managed service that runs on our internal clusters and training infrastructure. We handle scheduling, resource allocation, and failure recovery. This allows you to get small or large runs started immediately, without worrying about managing infrastructure. We use LoRA so that we can share the same pool of compute between multiple training runs, lowering costs.&lt;/p&gt;
    &lt;p&gt;Tinker’s API gives you low-level primitives like &lt;code&gt;forward_backward&lt;/code&gt; and &lt;code&gt;sample&lt;/code&gt;, which can be used to express most common post-training methods. Even so, achieving good results requires getting many details right. That’s why we’re releasing an open-source library, the Tinker Cookbook, with modern implementations of post-training methods that run on top of the Tinker API.&lt;/p&gt;
    &lt;p&gt;Groups at Princeton, Stanford, Berkeley, and Redwood Research have already been using Tinker:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Princeton Goedel Team trained mathematical theorem provers&lt;/item&gt;
      &lt;item&gt;The Rotskoff Chemistry group at Stanford fine-tuned a model to complete chemistry reasoning tasks&lt;/item&gt;
      &lt;item&gt;Berkeley’s SkyRL group ran experiments on a custom async off-policy RL training loop with multi-agents and multi-turn tool-use.&lt;/item&gt;
      &lt;item&gt;Redwood Research used Tinker to RL Qwen3-32B on difficult AI control tasks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tinker is now in private beta for researchers and developers. You can sign up for the Tinker waitlist here. We will be onboarding users to the platform starting today.&lt;/p&gt;
    &lt;p&gt;If you’re an organization interested in using Tinker, please contact us here.&lt;/p&gt;
    &lt;p&gt;Tinker will be free to start. We will introduce usage-based pricing in the coming weeks.&lt;/p&gt;
    &lt;p&gt;We’re excited to see what you discover and make with Tinker!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thinkingmachines.ai/blog/announcing-tinker/"/><published>2025-10-01T18:20:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45441447</id><title>Decoding Netflix's AV1 Streams: Here are 10 things I found</title><updated>2025-10-01T19:08:00.958757+00:00</updated><content>&lt;doc fingerprint="675800970b045fa8"&gt;
  &lt;main&gt;
    &lt;p&gt;Alright, pull up a chair. We need to talk about AV1. The codec wars are a special kind of hell.&lt;/p&gt;
    &lt;p&gt;For anyone who’s ever worked in video engineering, you know what I’m talking about. It’s a multi-decade, billion-dollar street fight over bytes and pixels, waged in the esoteric battlegrounds of DCT blocks and entropy coding. We argue endlessly about H.264’s ubiquity, HEVC’s licensing nightmare, and the mythical potential of the “next big thing.”&lt;/p&gt;
    &lt;p&gt;For years, we’ve been promised a streaming messiah. A codec that would deliver pristine 4K, slay the buffering dragon, and magically shrink our data bills. The industry anointed AV1 as the chosen one, the open-source hero that would free us from the licensing labyrinth of H.265/HEVC.&lt;/p&gt;
    &lt;p&gt;Most of the time, a new codec offers a modest, almost boring improvement. You get a 15-20% bump in efficiency, you write some blog posts, and life goes on. It’s evolution, not revolution. Netflix, being Netflix, was one of the first to roll it out at scale. But is it actually any good? Or is it just another line item in a press release?&lt;/p&gt;
    &lt;p&gt;I decided to find out. Armed with a Fire TV, a MacBook, the Android Debug Bridge (ADB), and a level of patience only attainable after debugging a newborn’s requirement at 3 AM, I spent multiple weekends capturing and analyzing every bit Netflix sent my way.&lt;/p&gt;
    &lt;p&gt;The assumption is simple: AV1, being newer, must be more efficient. And it is. But the real story, the one that matters to engineers and anyone who hates seeing a spinning circle, is far more interesting. It turns out that for some content, AV1 is so obsessed with quality it actually uses more data than its predecessor.&lt;/p&gt;
    &lt;p&gt;Yes, you read that right. Let’s get into it.&lt;/p&gt;
    &lt;head rend="h1"&gt;TL;DR - The Numbers Don’t Lie&lt;/head&gt;
    &lt;p&gt;Before we dive deep, here’s the high-level summary.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Up to 55% bandwidth savings on blockbuster movies.&lt;/item&gt;
      &lt;item&gt;Peak bitrate spikes often cut in half during action scenes (goodbye, buffering).&lt;/item&gt;
      &lt;item&gt;50% more Netflix hours per month on your phone’s data plan.&lt;/item&gt;
      &lt;item&gt;Netflix’s wallet: My back-of-the-napkin math puts potential savings around $25M/year.&lt;/item&gt;
      &lt;item&gt;The catch: Not all content benefits equally—some animated shows use more data with AV1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The following table is your cheat sheet to the entire analysis. All metrics are relative to AV1&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Insight&lt;/cell&gt;
        &lt;cell role="head"&gt;Catch Me&lt;p&gt;If You Can&lt;/p&gt;&lt;p&gt;H.264&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Minions&lt;p&gt;H.264&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Peaky&lt;p&gt;Blinders&lt;/p&gt;&lt;p&gt;H.264&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Breaking&lt;p&gt;Bad&lt;/p&gt;&lt;p&gt;HEVC&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Bojack&lt;p&gt;Horseman&lt;/p&gt;&lt;p&gt;HEVC&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;% saved vs baseline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Instances &amp;gt;10 Mbps&lt;/cell&gt;
        &lt;cell&gt;AV10H.26416&lt;/cell&gt;
        &lt;cell&gt;AV12H.26416&lt;/cell&gt;
        &lt;cell&gt;AV10H.2649&lt;/cell&gt;
        &lt;cell&gt;AV128HEVC64&lt;/cell&gt;
        &lt;cell&gt;AV10HEVC0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;GB saved&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: All metrics are relative to AV1. For “Instances &amp;gt;10 Mbps,” lower is better.&lt;/p&gt;
    &lt;head rend="h1"&gt;How I Accidentally Became a Netflix Data Stalker&lt;/head&gt;
    &lt;p&gt;Look, I wasn’t planning to spend my evenings decoding Netflix streams like some kind of bandwidth detective. But after seeing the 47th “AV1 is the future” post without actual data, I got annoyed enough to do something about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Setup:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fire TV Stick (because it actually reports real bitrates) &lt;list rend="ul"&gt;&lt;item&gt;4K Max model (2024) for receiving AV1 streams&lt;/item&gt;&lt;item&gt;4K model (2021) for receiving H.264 and H.265 streams&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Python script leveraging Android Debug (ADB) Bridge capturing data every 5 seconds&lt;/item&gt;
      &lt;item&gt;5 viewing sessions per title to account for network conditions&lt;/item&gt;
      &lt;item&gt;30-second windows aggregated into min/max/average values&lt;/item&gt;
      &lt;item&gt;Zero chill about methodology&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s what the debug info looks like on FireTV&lt;/p&gt;
    &lt;head rend="h2"&gt;The Victims:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🍌 Minions (CGI animation) - AV1 vs H.264&lt;/item&gt;
      &lt;item&gt;🎬 Catch Me If You Can (live-action film) - AV1 vs H.264&lt;/item&gt;
      &lt;item&gt;🎩 Peaky Blinders S1E1 (live-action TV) - AV1 vs H.264&lt;/item&gt;
      &lt;item&gt;🧪 Breaking Bad S1E1 (grainy film) - AV1 vs H.265&lt;/item&gt;
      &lt;item&gt;🐴 Bojack Horseman (2D animation) - AV1 vs H.265&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Yes, I watched the Breaking Bad pilot ten times for science. My Netflix recommendations are permanently broken.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Part 1: The Foundation - Massive Efficiency Gains&lt;/head&gt;
    &lt;p&gt;First, let’s establish the baseline. Is AV1 actually more efficient? The data shows it’s not just an incremental improvement; it’s a demolition.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #1: The Headline Savings&lt;/head&gt;
    &lt;p&gt;AV1 is fighting a war on two fronts: against the old king, H.264, and the modern contender, H.265/HEVC. What I found was a knockout in both fights.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Codec &lt;p&gt;Battle&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Competitor&lt;p&gt;Avg. Bitrate&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;AV1&lt;p&gt;Avg. Bitrate&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;% Saved&lt;p&gt;vs Competitor&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎩 Peaky Blinders&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;3,632 kbps&lt;/cell&gt;
        &lt;cell&gt;1,884 kbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🍌 Minions&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;4,231 kbps&lt;/cell&gt;
        &lt;cell&gt;1,907 kbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎬 Catch Me If You Can&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;4,271 kbps&lt;/cell&gt;
        &lt;cell&gt;2,249 kbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🧪 Breaking Bad&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;7,369 kbps&lt;/cell&gt;
        &lt;cell&gt;5,081 kbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🐴 Bojack Horseman&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;1,027 kbps&lt;/cell&gt;
        &lt;cell&gt;1,096 kbps&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Let’s take Peaky Blinders as an example—a show famous for its cinematic style, complete with smoke and moody lighting. The average bitrate for the AV1 stream was 48.1% lower than H.264.&lt;/p&gt;
    &lt;p&gt;Let that sink in. This isn’t a minor tweak. This is a nearly 50% gain over the most ubiquitous codec on the planet. This was my first “holy shit” moment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #2: The Art of Silence&lt;/head&gt;
    &lt;p&gt;How does AV1 achieve such massive savings? Part of the secret is how ruthless it is during the boring parts. I call this the “efficiency floor”—the typical low-end bitrate a codec settles on during static, quiet scenes. To measure this, I analyzed the average of the minimum bitrates across all segments.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Codec &lt;p&gt;Battle&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Competitor&lt;p&gt;Avg. Minimum&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;AV1&lt;p&gt;Avg. Minimum&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;AV1 Advantage&lt;p&gt;% Lower&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🍌 Minions&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.246&lt;/cell&gt;
        &lt;cell&gt;2,825 kbps&lt;/cell&gt;
        &lt;cell&gt;1,155 kbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎬 Catch Me If You Can&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;2,905 kbps&lt;/cell&gt;
        &lt;cell&gt;1,381 kbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎩 Peaky Blinders&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;2,121 kbps&lt;/cell&gt;
        &lt;cell&gt;1,076 kbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🧪 Breaking Bad&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;5,081 kbps&lt;/cell&gt;
        &lt;cell&gt;2,718 kbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🐴 Bojack Horseman&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;637 kbps&lt;/cell&gt;
        &lt;cell&gt;641 kbps&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;AV1 is absolutely merciless during quiet moments, dropping to bitrates that would make H.264 engineers file bug reports. But it works—every saved bit during dialogue scenes funds the spectacular action sequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #3: The Payoff in Gigabytes&lt;/head&gt;
    &lt;p&gt;Percentages are boring. Here’s what AV1 actually saves you in tangible data:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🎬 Catch Me If You Can (141 min): 1.94 GB saved&lt;/item&gt;
      &lt;item&gt;🍌 Minions (91 min): 1.48 GB saved&lt;/item&gt;
      &lt;item&gt;🧪 Breaking Bad S1E1 (58 min): 0.93 GB saved&lt;/item&gt;
      &lt;item&gt;🎩 Peaky Blinders S1E1 (59 min): 0.7 GB saved&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a 10-episode season of Peaky Blinders, that’s 7 GB saved. Enough for three additional HD movies.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Netflix literally gave you a ‘buy 10 episodes, get 3 movies free’ deal and nobody noticed.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Part 2: The Secret Weapon - Thriving Under Pressure&lt;/head&gt;
    &lt;p&gt;Saving data is one thing, but AV1’s real genius lies in how it handles complexity. This is where we uncover the fascinating paradoxes that make AV1 a strategic masterpiece.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #4: The Action Scene Advantage&lt;/head&gt;
    &lt;p&gt;You’d think a codec saves data by cutting corners during chaotic action scenes. AV1 does the exact opposite. Its efficiency advantage often accelerates when the on-screen complexity ramps up.&lt;/p&gt;
    &lt;p&gt;We can prove this by segmenting each stream into four “quartiles” of complexity. Q1 represents the quietest 25% of scenes, while Q4 represents the most frantic 25%.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Complexity &lt;p&gt;Level&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;🍌 Minions&lt;/cell&gt;
        &lt;cell role="head"&gt;🎬 Catch Me&lt;/cell&gt;
        &lt;cell role="head"&gt;🎩 Peaky Blinders&lt;/cell&gt;
        &lt;cell role="head"&gt;🧪 Breaking Bad&lt;/cell&gt;
        &lt;cell role="head"&gt;🐴 Bojack&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Easiest 25%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Low‑Mid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Mid‑High&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Hardest 25%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notice how Peaky Blinders actually gets MORE efficient as scenes get harder? A perfect example of this in action is the CGI-animated film Minions. The chart below shows the H.264 stream (red) spiking wildly during action scenes, while the AV1 stream (green) remains remarkably composed.&lt;/p&gt;
    &lt;p&gt;When the screen is filled with smoke, rain, and fast action, AV1’s advantage widens. It thrives under pressure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #5: The Peak Bitrate Anomaly&lt;/head&gt;
    &lt;p&gt;Peak bitrates are the silent killers of a smooth streaming experience. But in my analysis, the single highest bitrate spike across all five titles didn’t come from an older, less efficient codec. It came from AV1.&lt;/p&gt;
    &lt;p&gt;The hall of fame (or shame):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AV1’s Wildest Moment: The AV1 stream for Breaking Bad hit a staggering 22,573 kbps spike.&lt;/item&gt;
      &lt;item&gt;HEVC’s Response: For the same scene, the HEVC stream topped out at a more controlled 19,428 kbps.&lt;/item&gt;
      &lt;item&gt;The Norm: In contrast, during the most chaotic action scenes in Minions, H.264 peaked at over 10 Mbps, while AV1 stayed calmly under 4 Mbps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This isn’t a bug. It’s a deliberate, almost fanatical, choice to prioritize visual quality over anything else, which leads us directly to the most interesting discovery of this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #6: The Breaking Bad Paradox&lt;/head&gt;
    &lt;p&gt;So, why would the “more efficient” codec throw a 22 Mbps tantrum? This brings us to the Breaking Bad Paradox. The behavior we see is a direct result of Netflix’s shot-based encoding strategy in action. Their systems analyze the show scene-by-scene and see that its signature 35mm film grain is a crucial, and very data-intensive, part of its artistic identity.&lt;/p&gt;
    &lt;p&gt;Faced with this, the encoder is given a clear directive: preserve that grain at all costs.&lt;/p&gt;
    &lt;p&gt;This results in a bizarre Jekyll-and-Hyde mode. Instead of just saving bits everywhere, the encoder acts like a savvy budget manager operating under Netflix’s quality-first philosophy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It robs bits from the data-poor scenes (quiet dialogue) by compressing them aggressively.&lt;/item&gt;
      &lt;item&gt;It then spends that budget on the data-rich scenes (intense grain), ensuring the show’s look is rendered with absolute fidelity, even if it causes a massive spike.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This reveals that the ultimate strategy isn’t just about the codec’s raw power; it’s about the intelligent instructions guiding it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #7: The Bojack Horseman Mystery&lt;/head&gt;
    &lt;p&gt;The Bojack Horseman result, where AV1 uses slightly more data, is the other side of the same coin: Netflix’s obsession with perceptual quality. The shot-based analysis for this show isn’t concerned with film grain, but with preserving the clean lines and solid colors of 2D animation.&lt;/p&gt;
    &lt;p&gt;Here’s what’s happening: the system recognizes that for this type of content, a tiny extra investment in bitrate can deliver a disproportionately large improvement in visual quality, eliminating subtle artifacts that older codecs might introduce around character outlines.&lt;/p&gt;
    &lt;p&gt;HEVC can hit “good enough” quality at 1.0 Mbps. But Netflix has directed AV1 to deliver “pristine” quality at 1.1 Mbps—a worthwhile trade-off to ensure the animation looks exactly as the creators intended. It’s the difference between a decent burger and a gourmet one for just a few cents more, a decision made possible by a platform that prioritizes quality over raw efficiency.&lt;/p&gt;
    &lt;head rend="h1"&gt;Part 3: The User Experience - A Smoother, More Stable Stream&lt;/head&gt;
    &lt;p&gt;All this technical wizardry is impressive, but what does it actually mean when you hit “play”?&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #8: The Buffer Battle Royale&lt;/head&gt;
    &lt;p&gt;Imagine a 10 Mbps internet connection. Let’s count how many times each stream would cause buffering by exceeding that limit.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Codec &lt;p&gt;Battle&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;H.264/HEVC &lt;p&gt;Danger Zones&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;AV1 &lt;p&gt;Danger Zones&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Your Frustration &lt;p&gt;Level&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎬 Catch Me If You Can&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;Zen master&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎩 Peaky Blinders&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;Zen master&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🐴 Bojack Horseman&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;Equally smooth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🧪 Breaking Bad&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;Still annoyed, but less&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🍌 Minions&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;times&lt;/cell&gt;
        &lt;cell&gt;Still annoyed, but less&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In Catch Me If You Can, the H.264 stream crossed into the danger zone 16 times. The AV1 stream? Zero. The chart below makes this crystal clear—watch how the red H.264 line constantly spikes while the green AV1 line stays composed.&lt;/p&gt;
    &lt;p&gt;AV1 literally eliminated buffering for two titles and significantly reduced it for the others. Your internet didn’t get better—the codec did.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #9: Faster Start-Up, Less Waiting&lt;/head&gt;
    &lt;p&gt;You click play, and those first 30 seconds determine whether you commit to watching or bounce to TikTok. A high initial bitrate can kill the experience on slower connections, leading to a frustrating wait before the video even starts.&lt;/p&gt;
    &lt;p&gt;Across the board, AV1 shows a massive advantage right from the very first frame.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Codec &lt;p&gt;Battle&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Competitor&lt;p&gt;Startup Bitrate&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;AV1&lt;p&gt;Startup Bitrate&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;AV1 Advantage&lt;p&gt;% Lower&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🍌 Minions&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;3.4 Mbps&lt;/cell&gt;
        &lt;cell&gt;1.3 Mbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🧪 Breaking Bad&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;4.0 Mbps&lt;/cell&gt;
        &lt;cell&gt;1.8 Mbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎬 Catch Me If You Can&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;2.0 Mbps&lt;/cell&gt;
        &lt;cell&gt;1.1 Mbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎩 Peaky Blinders&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;0.8 Mbps&lt;/cell&gt;
        &lt;cell&gt;0.5 Mbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🐴 Bojack Horseman&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;0.9 Mbps&lt;/cell&gt;
        &lt;cell&gt;0.7 Mbps&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Real-world impact: That hotel WiFi that barely loads your email? It might actually handle an AV1 Netflix stream from the get-go. By requiring less data to start, AV1 drastically reduces the initial buffering time, getting you to your show faster.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding #10: The “Safe Zone” Revolution&lt;/head&gt;
    &lt;p&gt;For users on mobile or congested Wi-Fi, the most important metric is the percentage of time a stream stays below a low-bitrate threshold (e.g., 2 Mbps).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Codec &lt;p&gt;Battle&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Competitor&lt;p&gt;% Time in Zone&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;AV1&lt;p&gt;% Time in Zone&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Threshold&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🍌 Minions&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 2 Mbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎬 Catch Me If You Can&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 2 Mbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🎩 Peaky Blinders&lt;/cell&gt;
        &lt;cell&gt;AV1 / H.264&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 2 Mbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;🧪 Breaking Bad&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 4 Mbps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🐴 Bojack Horseman&lt;/cell&gt;
        &lt;cell&gt;AV1 / HEVC&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 2 Mbps&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;AV1 makes HD streaming viable on connections that would have struggled with legacy codecs. Your mobile data plan just became way more useful.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Bottom Line: What This Actually Means&lt;/head&gt;
    &lt;p&gt;Let’s bring this all together.&lt;/p&gt;
    &lt;head rend="h2"&gt;For You, the Viewer&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On Your Phone: A 10GB data plan used to get you ~12 hours of Netflix. With AV1’s efficiency, that’s now closer to 20 hours. You can binge an entire extra season of a show on your commute, for free.&lt;/item&gt;
      &lt;item&gt;On Bad Wi-Fi: That hotel or airport Wi-Fi that always buffers? AV1’s lower bitrate floor and superior stability make streaming on it not just possible, but smooth.&lt;/item&gt;
      &lt;item&gt;On a Great Connection: You get a higher-quality, more detailed picture, especially in complex scenes, because the codec is using your bandwidth more intelligently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;For Netflix, the Business&lt;/head&gt;
    &lt;p&gt;Let’s talk money, because that’s what drives these decisions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My back-of-the-napkin math, using some conservative industry estimates for bandwidth costs, puts Netflix’s potential savings in the ballpark of $25M/year. That’s money they can invest in shows instead of paying to internet providers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;The Catch (There’s Always a Catch)&lt;/head&gt;
    &lt;p&gt;I’m not an AV1 evangelist who ignores reality. The codec has two major hurdles:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Encoding Cost: AV1 is a beast to encode. This isn’t just a one-time software update; it requires a massive, ongoing investment in server infrastructure for any streaming service.&lt;/item&gt;
      &lt;item&gt;Device Support: Hardware decoding for AV1 isn’t on every device yet. This is a classic chicken-and-egg problem that will take a few more years of device refresh cycles to fully resolve.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;The Verdict: A Revolution in Progress&lt;/head&gt;
    &lt;p&gt;Despite the hurdles, the data is clear. AV1 is not just another spec sheet with theoretical gains. In real-world streams, it delivers on its promise with staggering results.&lt;/p&gt;
    &lt;p&gt;It’s smarter in quiet moments, more composed during action scenes, and its efficiency advantage often accelerates the harder you push it. The transition will be slow and expensive, but it’s no longer a question of if AV1 will dominate streaming, but when.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We’re witnessing a quiet revolution, one Gigabyte at a time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Found this useful? Share it with anyone who’s ever cursed their internet connection during a Netflix binge. And yes, I realize I spent a few weekends proving that Netflix’s engineers know what they’re doing. No regrets.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://singhkays.com/blog/netflix-av1-decode/"/><published>2025-10-01T18:36:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45441580</id><title>Tinker by Thinking Machines</title><updated>2025-10-01T19:08:00.511047+00:00</updated><content>&lt;doc fingerprint="1d7d5d37b139c838"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Tinker is a training API for researchers&lt;/head&gt;&lt;p&gt;Control every aspect of model training and fine-tuning while we handle the infrastructure.&lt;/p&gt;&lt;head rend="h2"&gt;Your ideas in four functions&lt;/head&gt;&lt;head rend="h3"&gt;forward_backward&lt;/head&gt;&lt;p&gt;Performs a forward pass and a backward pass, accumulating the gradient.&lt;/p&gt;&lt;head rend="h3"&gt;optim_step&lt;/head&gt;&lt;p&gt;Updates the weights based on the accumulated gradient.&lt;/p&gt;&lt;head rend="h3"&gt;sample&lt;/head&gt;&lt;p&gt;Generate tokens for interaction, evaluation, or RL actions.&lt;/p&gt;&lt;head rend="h3"&gt;save_state&lt;/head&gt;&lt;p&gt;Save training progress for resumption.&lt;/p&gt;&lt;head rend="h2"&gt;Supported models&lt;/head&gt;&lt;head rend="h2"&gt;Tinker uses LoRA&lt;/head&gt;&lt;p&gt;LoRA fine-tunes models by training a small add-on instead of changing all the original weights.&lt;/p&gt;Read blog post&lt;head rend="h2"&gt;FAQs&lt;/head&gt;&lt;p&gt;Sign up for our waitlist here. If you're a university or organization looking for wide scale access, contact [email protected].&lt;/p&gt;&lt;p&gt;Tinker is a flexible API for efficiently fine-tuning open source models with LoRA. It's designed for researchers and developers who want flexibility and full control of their data and algorithms without worrying about infrastructure management.&lt;/p&gt;&lt;p&gt;LoRA is an efficient approach to fine-tuning that trains a streamlined adapter instead of updating all base model weights. Our research demonstrates that with the right setup, LoRA matches the learning performance of full fine-tuning while providing more flexibility and requiring less compute.&lt;/p&gt;&lt;p&gt;Tinker handles scheduling, tuning, resource management, and infrastructure reliability so you can focus on the training data and algorithms. Behind the scenes, Tinker orchestrates distributed training on powerful GPU clusters for efficient utilization.&lt;/p&gt;&lt;p&gt;A dataset of supervised learning examples or reinforcement learning environments. After picking a base model to train on, the Tinker API provides simple functions to compute gradients, update the weights, and sample outputs from the trained model. See our cookbook for examples to get started.&lt;/p&gt;&lt;p&gt;Tinker is currently available for a broad selection of open-source models, ranging from compact models like Llama-3.2-1B to large MoEs like Qwen3-235B-A22B-Instruct. We plan to expand our model lineup with even more choices soon.&lt;/p&gt;&lt;p&gt;You can download model weights throughout and following training.&lt;/p&gt;&lt;p&gt;Tinker will be free to start. We will introduce usage-based pricing in the coming weeks.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thinkingmachines.ai/tinker/"/><published>2025-10-01T18:44:47+00:00</published></entry></feed>