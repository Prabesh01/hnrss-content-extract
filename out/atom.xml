<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-13T05:45:53.297479+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46588837</id><title>LLVM: The bad parts</title><updated>2026-01-13T05:46:03.879156+00:00</updated><content>&lt;doc fingerprint="3a5898176125975c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;LLVM: The bad parts&lt;/head&gt;
    &lt;p&gt;A few years ago, I wrote a blog post on design issues in LLVM IR. Since then, one of these issues has been fixed fully (opaque pointers migration), one has been mostly fixed (constant expression removal), and one is well on the way towards being fixed (ptradd migration).&lt;/p&gt;
    &lt;p&gt;This time I’m going to be more ambitious and not stop at three issues. Of course, not all of these issues are of equal importance, and how important they are depends on who you ask. In the interest of brevity, I will mostly just explain what the problem is, and not discuss what possible solutions would be.&lt;/p&gt;
    &lt;p&gt;Finally, I should probably point out that this is written from my perspective as the lead maintainer of the LLVM project: This is not a list of reasons to not use LLVM, it’s a list of opportunities to improve LLVM.&lt;/p&gt;
    &lt;head rend="h2"&gt;High level issues&lt;/head&gt;
    &lt;head rend="h3"&gt;Review capacity&lt;/head&gt;
    &lt;p&gt;Unlike many other open-source projects, LLVM certainly does not suffer from a lack of contributors. There are thousands of contributors and the distribution is relatively flat (that is, it’s not the case that a small handful of people is responsible for the majority of contributions.)&lt;/p&gt;
    &lt;p&gt;What LLVM does suffer from is insufficient review capacity. There are a lot more people writing code than reviewing it. This is somewhat unsurprising, as code review requires more expertise than writing code, and may not provide immediate value1 to the person reviewing (or their employer).&lt;/p&gt;
    &lt;p&gt;Lack of review capacity makes for a bad contributor experience, and can also result in bad changes making their way into the codebase. The way this usually works out is that someone puts up a PR, then fails to get a qualified review for a long period of time, and then one of their coworkers (who is not a qualified reviewer for that area) ends up rubberstamping the PR.&lt;/p&gt;
    &lt;p&gt;A related problem is that LLVM has a somewhat peculiar contribution model where it’s the responsibility of the PR author to request reviewers. This is especially problematic for new contributors, who don’t know whom to request. Often relevant reviewers will become aware of the PR thanks to a label-based notification system, but this is not apparent from the UI, and it’s easy for PRs to fall through the cracks.&lt;/p&gt;
    &lt;p&gt;A potential improvement here would be a Rust-style PR assignment system.&lt;/p&gt;
    &lt;head rend="h3"&gt;Churn&lt;/head&gt;
    &lt;p&gt;Both the LLVM C++ API and LLVM IR are not stable and undergo frequent changes. This is simultaneously a great strength and weakness of LLVM. It’s a strength because LLVM does not stagnate and is willing to address past mistakes even at significant cost. It’s a weakness because churn imposes costs on users of LLVM.&lt;/p&gt;
    &lt;p&gt;Frontends are somewhat insulated from this because they can use the largely stable C API. However, it does not cover everything, and most major frontends will have additional bindings that use the unstable C++ API.&lt;/p&gt;
    &lt;p&gt;Users that integrate with LLVM more tightly (for example downstream backends) don’t have that option, and have to keep up with all API changes.&lt;/p&gt;
    &lt;p&gt;This is part of LLVM’s general development philosophy, which I’ll express somewhat pointedly as “upstream or GTFO”. LLVM is liberally licensed and does not require you to contribute changes upstream. However, if you do not upstream your code, then it will also not factor into upstream decision-making.&lt;/p&gt;
    &lt;p&gt;This point is somewhat unlike the rest, in that I’m not sure it’s possible to make things “strictly better” here. It’s possible that LLVM’s current point on the stability scale is not optimal, but moving it somewhere else would come with significant externalities. Making major changes in LLVM is already extremely hard due to the sheer scale of the project, without adding additional stability constraints on top.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build time&lt;/head&gt;
    &lt;p&gt;LLVM is a huge project. LLVM itself is &amp;gt;2.5 million lines of C++ and the entire monorepo is something like 9 million. C++ is not exactly known for fast build times, and compiling all that code takes time. This is bearable if you either have fast hardware or access to a build farm, but trying to build LLVM on a low-spec laptop is not going to be fun.&lt;/p&gt;
    &lt;p&gt;An additional complication is building with debug info (which I always recommend against), in which case you’ll add the extra gotchas of slow link times, high risk of OOM and massive disk usage. There are ways to avoid that (using shared libs or dylib build, using split dwarf, using lld), but it takes some expertise.&lt;/p&gt;
    &lt;p&gt;Promising changes in this area are the use of pre-compiled headers (which significantly improves build time), and changing to use a dylib build by default (which reduces disk usage and link time, esp. for debuginfo builds). Another is to reduce test overhead using daemonization (not strictly part of the “build time”, but relevant for the development cycle).&lt;/p&gt;
    &lt;head rend="h3"&gt;CI stability&lt;/head&gt;
    &lt;p&gt;LLVM CI consists of over 200 post-commit buildbots that test LLVM in lots of different configurations on lots of different hardware. Commits that turn a buildbot from green to red result in an email to the commit author.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this CI is never fully green, and flaky on top. This is in part due to flaky tests (typically in lldb or openmp), but can also be due to buildbot-specific issues. The end result is that it’s “normal” to get buildbot failure notifications for any given commit, even if it is perfectly harmless. This dilutes the signal, and makes it easier to miss the real failures.&lt;/p&gt;
    &lt;p&gt;The introduction of pre-merge testing on PRs did significantly improve the overall CI situation, but not the buildbot problem as such. I think we need to start taking flaky tests/buildbots more seriously before we can really make progress here.&lt;/p&gt;
    &lt;p&gt;Because someone is definitely going to mention how this is not rocket science, and we just need to start using bors / merge queues to guarantee an always-green build: It’s a problem of scale. There are &amp;gt;150 commits on a typical workday, which would be more than one commit every 10 minutes even if they were uniformly distributed. Many buildbots have multi-hour runs. This is hard to reconcile.2&lt;/p&gt;
    &lt;head rend="h3"&gt;End-to-end testing&lt;/head&gt;
    &lt;p&gt;In some respects, LLVM has very thorough test coverage. We’re quite pedantic about making sure that new optimizations have good coverage of both positive and negative tests. However, these tests are essentially unit tests for a single optimization pass or analysis.&lt;/p&gt;
    &lt;p&gt;We have only a small amount of coverage for the entire optimization pipeline (phase ordering tests), so optimizations sometimes regress due to pass interactions. Tests for the combination of the middle-end and backend pipelines are essentially nonexistent. There is likely room for improvement here, though it comes with tradeoffs.&lt;/p&gt;
    &lt;p&gt;However, what actually concerns me are end-to-end executable tests. LLVM’s test suite proper does not feature these at all. Executable tests are located in a separate llvm-test-suite repo, which is typically not used during routine development, but run by buildbots. It contains a lot of different code ranging from benchmarks to unit tests.&lt;/p&gt;
    &lt;p&gt;However, llvm-test-suite has quite few tests (compared to LLVM lit tests) and does not comprehensively cover basic operations. Things like testing operations on different float formats, on integers of different sizes, vectors of different sizes and element types, etc.&lt;/p&gt;
    &lt;p&gt;In part this is because of limitations of testing through C/C++, which is very heterogeneous in type support (C compilers don’t like exposing types that don’t have a defined psABI for the target). But that’s no excuse to delegate this testing to Zig instead (which exposes everything, everywhere, and has the corresponding test coverage).&lt;/p&gt;
    &lt;head rend="h3"&gt;Backend divergence&lt;/head&gt;
    &lt;p&gt;While LLVM’s middle-end is very unified, backend implementations are very heterogeneous, and there is a tendency to fix issues (usually performance, but sometimes even correctness) only for the backend you’re interested in.&lt;/p&gt;
    &lt;p&gt;This takes many forms, like implementing target-specific DAG combines instead of generic ones. Though my definite favorite is to introduce lots of target hooks for optimizations – not because the optimization is actually only beneficial for one target, but because the person introducing it just doesn’t want to deal with the fallout on other targets.&lt;/p&gt;
    &lt;p&gt;This is understandable – after all, they may lack the knowledge to evaluate a change for other targets, so it may require working with many other maintainers, which can slow progress a lot. But the end result is still increasing divergence and duplication.&lt;/p&gt;
    &lt;p&gt;Lack of end-to-end testing compounds this issue, because that would act as something of a forcing function that at least all operations compile without crashing and produce correct results for all tested targets.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compilation time&lt;/head&gt;
    &lt;p&gt;Because I’ve complained about this enough in the past, I’ll keep it short: LLVM is slow, which is an issue both for JIT use cases, and anything that tends to produce huge amounts of IR (like Rust or C++).&lt;/p&gt;
    &lt;p&gt;Since I’ve started tracking compile-times, the situation has significantly improved, both through targeted improvements and avoidance of regressions. However, there is still a lot of room for improvement: LLVM still isn’t fast, it’s just less slow.&lt;/p&gt;
    &lt;p&gt;One thing that LLVM is particularly bad at are &lt;code&gt;-O0&lt;/code&gt; compile-times. The architecture is optimized for optimization, and lots of costs remain even if no optimization takes place. The LLVM TPDE alternative backend shows that it’s possible to do better by an order of magnitude.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance tracking&lt;/head&gt;
    &lt;p&gt;The flip side of the compile-time coin is runtime performance. This is something that LLVM obviously cares a lot about. Which is why I find it rather surprising that LLVM does not have any “official” performance tracking infrastructure.&lt;/p&gt;
    &lt;p&gt;Of course, there are lots of organizations which track performance of LLVM downstream, on their own workloads. In some ways this is good, because it means there is more focus on real-world workloads than on synthetic benchmarks like SPEC. However, not having readily accessible, public performance tracking also makes it hard for contributors to evaluate changes.&lt;/p&gt;
    &lt;p&gt;To be fair, LLVM does have an LNT instance, but a) it’s currently broken, b) LNT is one of the worst UX crimes ever committed, c) little data gets submitted there, and d) it’s not possible to request a test run for a PR, or something like that.&lt;/p&gt;
    &lt;p&gt;This point is frankly just baffling to me. I don’t personally care about SPEC scores, but I know plenty of people do, so why there is no first-class tracking for this is a mystery to me.&lt;/p&gt;
    &lt;head rend="h2"&gt;IR design&lt;/head&gt;
    &lt;head rend="h3"&gt;Undef values&lt;/head&gt;
    &lt;p&gt;Undef values take an arbitrary value from a certain set. They are used to model uninitialized values, and have historically been used to model deferred undefined behavior. The latter role has been replaced by poison values, which have much simpler propagation rules and are more amenable to optimization. However, undef is still used for uninitialized memory to this day.&lt;/p&gt;
    &lt;p&gt;There are two main problems with undef values. The first is the multi-use problem: An undef value can take a different value at each use. This means that transforms that increase the use count are generally invalid, and care has to be taken when optimizing based on value equality. The mere existence of undef values prevents us from performing optimizations we want to do, or greatly increases their complexity.&lt;/p&gt;
    &lt;p&gt;The second issue is that undef is very hard to reason about. Humans have trouble understanding it, and for proof-checkers it is computationally expensive.&lt;/p&gt;
    &lt;p&gt;Most likely, uninitialized memory will be represented using poison values instead in the future, but this runs into the problem that LLVM currently is not capable of correctly treating poison in memory. Proper support for poison in memory requires additional IR features, like the byte type.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unsoundness and specification incompleteness&lt;/head&gt;
    &lt;p&gt;While most miscompilations (that is, correctness bugs) in LLVM are resolved quickly, there are quite a few that remain unfixed despite having been known for a long time. These issues usually combine the qualities of being largely theoretical (that is, appearing only in artificially constructed examples rather than real-world code) and running up against issues in LLVM’s IR design.&lt;/p&gt;
    &lt;p&gt;Some of them are cases where we have a good idea of how the IR design needs to change to address the issue, but these changes are complex and often require a lot of work to recover optimization parity. There is often a complexity cliff where you can do something that’s simple and nearly correct, or you can do something very complex that is fully correct.&lt;/p&gt;
    &lt;p&gt;Then there are other cases, where just deciding on how things should work is a hard problem. The provenance model is a prime example of this. The interaction of provenance with integer casts and type punning is a difficult problem with complex tradeoffs.&lt;/p&gt;
    &lt;p&gt;However, at some point these issues do need to be resolved. The recently formed formal specification working group aims to tackle these problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Constraint encoding&lt;/head&gt;
    &lt;p&gt;A key challenge for optimizing compilers is encoding of constraints (like “this value is non-negative” or “this add will not overflow”). This includes both frontend-provided constraints (based on language undefined behavior rules), but also compiler-generated ones.&lt;/p&gt;
    &lt;p&gt;In particular, there are many different analyses that can infer facts about the program, but keeping these up-to-date throughout optimization is challenging. One good way to handle this is to encode facts directly in the IR. Correctly updating or discarding these annotations then becomes part of transform correctness.&lt;/p&gt;
    &lt;p&gt;LLVM has many different ways to encode additional constraints (poison flags, metadata, attributes, assumes), and these all come with tradeoffs in terms of how much information can be encoded, how reliably it is retained during optimization and to what degree it can negatively affect optimization. Information from metadata is lost too often, while information from assumes is not lost often enough.&lt;/p&gt;
    &lt;head rend="h3"&gt;Floating-point semantics&lt;/head&gt;
    &lt;p&gt;There are various issues with floating-point (FP) semantics once we move outside the nice world of “strictly conforming IEEE 754 floats in the default environment”. A few that come to mind are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Handling of signaling NaN and FP exceptions, and non-default FP environment in general. LLVM represents this using constrained FP intrinsics. This is not ideal, as all the FP handling is split into two parallel universes.&lt;/item&gt;
      &lt;item&gt;Handling of denormals. LLVM has a function attribute to not assume IEEE denormal behavior, but this is only suitable for cases where flush to zero (FTZ) is used globally. It does not help with modeling cases like ARM, where scalar ops are IEEE, while vector ops use FTZ.&lt;/item&gt;
      &lt;item&gt;Handling of excess precision, in particular when using the x87 FPU.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Other technical issues&lt;/head&gt;
    &lt;head rend="h3"&gt;Partial migrations&lt;/head&gt;
    &lt;p&gt;LLVM is a very large project, and making any significant changes to it is hard and time consuming. Migrations often span years, where two different implementations of something coexist, until all code has been migrated. The two prime examples of this are:&lt;/p&gt;
    &lt;p&gt;New pass manager: The “new” pass manager was first introduced more than a decade ago. Then about five years ago, we started using it for the middle-end optimization pipeline by default, and support for the legacy PM was dropped.&lt;/p&gt;
    &lt;p&gt;However, the back-end is still using the legacy pass manager. There is ongoing work to support the new pass manager in codegen, and we’re pretty close to the point where it can be used end-to-end for a single target. However, I expect it will still take quite a while for all targets to be ported and the legacy pass manager to be completely retired.&lt;/p&gt;
    &lt;p&gt;GlobalISel: This is an even more extreme case. GlobalISel is the “new” instruction selector that is intended to replace SelectionDAG (and FastISel). It was introduced approximately one decade ago, and to this day, none of the targets that originally used SelectionDAG have been fully migrated to GlobalISel. There is one new target that’s GlobalISel-only, and there is one that uses GlobalISel by default for unoptimized builds. But otherwise, SelectionDAG is still the default everywhere.&lt;/p&gt;
    &lt;p&gt;There are two backends (AMDGPU and AArch64) that have somewhat complete GlobalISel support, but it’s not clear when/if they’ll be able to switch to using it by default. A big problem here is that new optimizations are continually being implemented on the SDAG side, so it’s hard to keep parity.&lt;/p&gt;
    &lt;head rend="h3"&gt;ABI / calling convention handling&lt;/head&gt;
    &lt;p&gt;Essentially everything about the handling of calling conventions in LLVM is a mess.&lt;/p&gt;
    &lt;p&gt;The responsibility for handling calling conventions is split between the frontend and the backend. There are good reasons why LLVM can’t do this by itself (LLVM IR sits at a too low level of abstraction to satisfy the extremely arcane ABI rules).&lt;/p&gt;
    &lt;p&gt;This is not a problem in itself – however, there is zero documentation of what the calling convention contract between the frontend and LLVM is, and the proper way to implement C FFI is essentially to look at what Clang does and copy that (invariably with errors, because the rules can be very subtle).&lt;/p&gt;
    &lt;p&gt;I’ve proposed to fix this by introducing an ABI lowering library and vortex73 has implemented a prototype for it as part of GSoC. So we’re well on the way to resolving this side of the problem.&lt;/p&gt;
    &lt;p&gt;There are more problems though. One that Rust has struggled with a lot is the interaction of target features with the calling convention. Enabling additional target features can change the call ABI, because additional float/vector registers start getting used for argument/return passing. This means that calls between functions with a feature enabled and disabled may be incompatible, because they assume different ABIs.&lt;/p&gt;
    &lt;p&gt;Ideally, ABI and target features would be orthogonal, and only coupled in that some ABIs require certain target features (e.g. you can’t have a hard float ABI without enabling FP registers). Target features are a per-function choice, while the ABI should be per-module.&lt;/p&gt;
    &lt;p&gt;Some of the newer architectures like Loongarch and RISC-V actually have proper ABI design, but most of the older ones don’t. For example, it’s currently not possible to target AArch64 with a soft float ABI but hard float implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Builtins / libcalls&lt;/head&gt;
    &lt;p&gt;Somewhat related to this is the handling of compiler builtins/libcalls, which are auxiliary functions that the compiler may emit for operations that are not natively supported by the target. This covers both libcalls provided by libc (or libm), and builtins provided by compiler runtime libraries like libgcc, compiler-rt or compiler-builtins.&lt;/p&gt;
    &lt;p&gt;There are two sources of truth for this, TargetLibraryInfo (TLI) and RuntimeLibcalls. The former is used by the middle-end, primarily to recognize and optimize C library calls (this mostly covers only libc, but not libgcc). The latter is used by the backend, primarily to determine which libcalls may be emitted by the compiler and how they are spelled (this covers libgcc, and the subset of libc covered by LLVM intrinsics).&lt;/p&gt;
    &lt;p&gt;A problem with RuntimeLibcalls is that it currently largely works off only the target triple, which means that we have to make “lowest common denominator” assumptions about which libcalls are available, where the lowest common denominator is usually libgcc. If &lt;code&gt;--rtlib=compiler-rt&lt;/code&gt; is used, LLVM does not actually know about that, and cannot make use of functions that are in compiler-rt but not libgcc.&lt;/p&gt;
    &lt;p&gt;This also means that we’re missing a customization point for other runtime libraries. For example, there is no way for Rust to say that it provides f128 suffix libcalls via compiler-builtins, overriding target-specific naming and availability assumptions based on which type &lt;code&gt;long double&lt;/code&gt; in C maps to.&lt;/p&gt;
    &lt;p&gt;There is a lot of ongoing work in this area (by arsenm), so the situation here will hopefully improve in the near-ish future.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context / module dichotomy&lt;/head&gt;
    &lt;p&gt;LLVM has two high-level data holders. A module corresponds to a compilation unit (e.g. pre-LTO, a single file in C/C++). The LLVM context holds various “global” data. There’s usually one context per thread, and multiple modules can (in principle) use a single context.&lt;/p&gt;
    &lt;p&gt;Things like functions and globals go into the module, while constants and types go into the context. The module also contains a data layout, which provides important type layout information like “how wide is a pointer”.&lt;/p&gt;
    &lt;p&gt;The fact that constants and types do not have access to the data layout is a constant source of friction. If you have a type, you cannot reliably tell its size without threading an extra parameter through everything. We have subsystems (like ConstantFold vs. ConstantFolding) that are separated entirely by whether data layout is available or not.&lt;/p&gt;
    &lt;p&gt;At the same time, I feel like this split is not actually buying us a lot. Having shared types and constants is somewhat convenient when it comes to module linking, because they can be directly shared, but I think performing explicit remapping in that one place would be better than having complexity everywhere else. Additionally, this would also allow cross-context linking, which is currently only possible by going through a bitcode roundtrip. In theory, the context could also allow some memory reuse when compiling multiple modules, but I think in practice there is usually a one-to-one correspondence between those.&lt;/p&gt;
    &lt;head rend="h3"&gt;LICM register pressure&lt;/head&gt;
    &lt;p&gt;This is getting a bit down in the weeds, but I’ll mention it anyway due to how often I’ve run across this in recent times.&lt;/p&gt;
    &lt;p&gt;LLVM considers loop invariant code motion (LICM) to be a canonicalization transform. This means that we always hoist instructions out of loops, without any target specific cost modelling. However, LICM can increase the live ranges of values, which can increase register pressure, which can lead to a large amount of spills and reloads.&lt;/p&gt;
    &lt;p&gt;The general philosophy behind this is that LICM hoists everything, all middle-end transforms can work with nicely loop invariant instructions, and then instructions will get sunk back into the loop by the backend, which can precisely model register pressure.&lt;/p&gt;
    &lt;p&gt;Except… that second part doesn’t actually happen. I believe that (for non-PGO builds) instructions only get sunk back into loops either through rematerialization in the register allocator, or specialized sinking (typically of addressing modes), but for anything not falling into those buckets, no attempt to sink into loops in order to reduce register pressure is made.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other&lt;/head&gt;
    &lt;p&gt;This list is not exhaustive. There’s more I could mention, but we’d get into increasingly narrow territory. I hope I covered most of the more important things – please do let me know what I missed!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;If you’re not concerned with overall project health, the primary value of reviews is reciprocity. People are more likely to review your PR, if you reviewed theirs. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The way Rust reconciles this is via a combination of “rollups” (where multiple PRs are merged as a batch, using human curation), and a substantially different contribution model. Where LLVM favors sequences of small PRs that do only one thing (and get squash merged), Rust favors large PRs with many commits (which do not get squashed). As getting an approved Rust PR merged usually takes multiple days due to bors, having large PRs is pretty much required to get anything done. This is not necessarily bad, just very different from what LLVM does right now. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.npopov.com/2026/01/11/LLVM-The-bad-parts.html"/><published>2026-01-12T14:18:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46588905</id><title>Show HN: Agent-of-empires: OpenCode and Claude Code session manager</title><updated>2026-01-13T05:46:02.928811+00:00</updated><content>&lt;doc fingerprint="e43055fb0d45e3b6"&gt;
  &lt;main&gt;
    &lt;p&gt;A terminal session manager for Linux and MacOS using tmux to aid in management and monitoring of AI coding agents, written in Rust.&lt;/p&gt;
    &lt;p&gt;Quick install (Linux &amp;amp; macOS):&lt;/p&gt;
    &lt;code&gt;curl -fsSL \
  https://raw.githubusercontent.com/njbrake/agent-of-empires/main/scripts/install.sh \
  | bash&lt;/code&gt;
    &lt;p&gt;Homebrew:&lt;/p&gt;
    &lt;code&gt;brew install njbrake/aoe/aoe&lt;/code&gt;
    &lt;p&gt;Update via &lt;code&gt;brew update &amp;amp;&amp;amp; brew upgrade aoe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/njbrake/agent-of-empires
cd agent-of-empires
cargo build --release&lt;/code&gt;
    &lt;p&gt;Agent of Empires (aoe) is a wrapper around tmux, the terminal multiplexer. Each AI coding session you create is actually a tmux session under the hood.&lt;/p&gt;
    &lt;p&gt;Once you attach to a session, you're working directly in tmux. Basic tmux knowledge helps:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;tmux Command&lt;/cell&gt;
        &lt;cell role="head"&gt;What It Does&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+b d&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Detach from session (return to Agent of Empires)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+b [&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enter scroll/copy mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;Ctrl+b n&lt;/code&gt; / &lt;code&gt;Ctrl+b p&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Next/previous window&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you're new to tmux, the key thing to remember is &lt;code&gt;Ctrl+b d&lt;/code&gt; to detach and return to the TUI, and that with Claude Code you'll need to enter scroll mode in order to scroll up in the Claude Code window (this isn't necessary when using opencode).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TUI Dashboard - Visual interface to manage all your AI coding sessions&lt;/item&gt;
      &lt;item&gt;Session Management - Create, attach, detach, and delete sessions&lt;/item&gt;
      &lt;item&gt;Group Organization - Organize sessions into hierarchical folders&lt;/item&gt;
      &lt;item&gt;Status Detection - Automatic status detection for Claude Code and OpenCode&lt;/item&gt;
      &lt;item&gt;tmux Integration - Sessions persist in tmux for reliability&lt;/item&gt;
      &lt;item&gt;Multi-profile Support - Separate workspaces for different projects&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Launch the TUI
aoe

# Or add a session directly from CLI
aoe add /path/to/project&lt;/code&gt;
    &lt;p&gt;Profiles let you maintain separate workspaces with their own sessions and groups. This is useful when you want to keep different contexts isolated—for example, work projects vs personal projects, or different client engagements.&lt;/p&gt;
    &lt;code&gt;aoe                 # Uses "default" profile
aoe -p work         # Uses "work" profile
aoe -p client-xyz   # Uses "client-xyz" profile&lt;/code&gt;
    &lt;p&gt;Each profile stores its own &lt;code&gt;sessions.json&lt;/code&gt; and &lt;code&gt;groups.json&lt;/code&gt;, so switching profiles gives you a completely different set of sessions.&lt;/p&gt;
    &lt;p&gt;Configuration is stored in &lt;code&gt;~/.agent-of-empires/&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;~/.agent-of-empires/
├── config.toml           # Global configuration
├── profiles/
│   └── default/
│       ├── sessions.json # Session data
│       └── groups.json   # Group structure
└── logs/                 # Session logs
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;AGENT_OF_EMPIRES_PROFILE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Default profile to use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;AGENT_OF_EMPIRES_DEBUG&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable debug logging&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Check code
cargo check

# Run tests
cargo test

# Format code
cargo fmt

# Lint
cargo clippy

# Run in debug mode
AGENT_OF_EMPIRES_DEBUG=1 cargo run

# Build release binary
cargo build --release&lt;/code&gt;
    &lt;p&gt;If you're connecting via SSH from a mobile app like Termius, you may encounter issues when attaching to sessions. The recommended approach is to run &lt;code&gt;aoe&lt;/code&gt; inside a tmux session:&lt;/p&gt;
    &lt;code&gt;# Start a tmux session first
tmux new-session -s main

# Then run aoe inside it
aoe&lt;/code&gt;
    &lt;p&gt;When you attach to an agent session, tmux will switch to that session. To navigate back to &lt;code&gt;aoe&lt;/code&gt; use the tmux command &lt;code&gt;Ctrl+b L&lt;/code&gt; to switch to last session (toggle back to aoe)&lt;/p&gt;
    &lt;p&gt;This is not an issue with &lt;code&gt;aoe&lt;/code&gt;: it's a known problem with Claude Code: anthropics/claude-code#1913&lt;/p&gt;
    &lt;p&gt;Inspired by agent-deck (Go + Bubble Tea).&lt;/p&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/njbrake/agent-of-empires"/><published>2026-01-12T14:23:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589658</id><title>Date is out, Temporal is in</title><updated>2026-01-13T05:46:02.294224+00:00</updated><content>&lt;doc fingerprint="8a417811e99f2f1c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Date is out, Temporal is in&lt;/head&gt;
    &lt;p&gt;Time makes fools of us all, and JavaScript is no slouch in that department either. Honestly, I’ve never minded the latter much — in fact, if you’ve taken JavaScript for Everyone or tuned into the newsletter, you already know that I largely enjoy JavaScript’s little quirks, believe it or not.&lt;/p&gt;
    &lt;p&gt;I like when you can see the seams; I like how, for as formal and iron-clad as the ES-262 specification might seem, you can still see all the good and bad decisions made by the hundreds of people who’ve been building the language in mid-flight, if you know where to look. JavaScript has character. Sure, it doesn’t necessarily do everything exactly the way one might expect, but y’know, if you ask me, JavaScript has a real charm once you get to know it!&lt;/p&gt;
    &lt;p&gt;There’s one part of the language where that immediately falls apart for me, though.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;// Numeric months are zero-indexed, but years and days are not: console.log( new Date(2026, 1, 1) ); // Result: Date Sun Feb 01 2026 00:00:00 GMT-0500 (Eastern Standard Time)&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;Date&lt;/code&gt; constructor.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;// A numeric string between 32 and 49 is assumed to be in the 2000s: console.log( new Date( "49" ) ); // Result: Date Fri Jan 01 2049 00:00:00 GMT-0500 (Eastern Standard Time) // A numeric string between 33 and 99 is assumed to be in the 1900s: console.log( new Date( "99" ) ); // Result: Date Fri Jan 01 1999 00:00:00 GMT-0500 (Eastern Standard Time) // ...But 100 and up start from year zero: console.log( new Date( "100" ) ); // Result: Date Fri Jan 01 0100 00:00:00 GMT-0456 (Eastern Standard Time)&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;I dislike &lt;code&gt;Date&lt;/code&gt; immensely.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;// A string-based date works the way you might expect: console.log( new Date( "2026/1/2" ) ); // Result: Date Fri Jan 02 2026 00:00:00 GMT-0500 (Eastern Standard Time) // A leading zero on the month? No problem; one is one, right? console.log( new Date( "2026/02/2" ) ); // Result: Date Mon Feb 02 2026 00:00:00 GMT-0500 (Eastern Standard Time) // Slightly different formatting? Sure! console.log( new Date( "2026-02-2" ) ); // Result: Date Mon Feb 02 2026 00:00:00 GMT-0500 (Eastern Standard Time) // A leading zero on the day? Of course; why wouldn't it work? console.log( new Date('2026/01/02') ); // Result: Date Fri Jan 02 2026 00:00:00 GMT-0500 (Eastern Standard Time) // Unless, of course, you separate the year, month, and date with hyphens. // Then it gets the _day_ wrong. console.log( new Date('2026-01-02') ); // Result: Date Thu Jan 01 2026 19:00:00 GMT-0500 (Eastern Standard Time)&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;Date&lt;/code&gt; sucks. It was hastily and shamelessly copied off of Java’s homework in the car on the way to school and it got all the same answers wrong, right down to the name at the top of the page: &lt;code&gt;Date&lt;/code&gt; doesn’t represent a date, it represents a time. Internally, dates are stored as number values called time values: Unix timestamps, divided into 1,000 milliseconds — which, okay, yes, a Unix time does also necessarily imply a date, sure, but still: Date represents a time, from which you can infer a date. Gross.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;// Unix timestamp for Monday, December 4, 1995 12:00:00 AM GMT-05 (the day JavaScript was announced): const timestamp = 818053200; console.log( new Date( timestamp * 1000 ) ); // Result: Date Mon Dec 04 1995 00:00:00 GMT-0500 (Eastern Standard Time)&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Words like “date” and “time” mean things, but, sure — whatever, JavaScript.&lt;/p&gt;
    &lt;p&gt;Java deprecated their &lt;code&gt;Date&lt;/code&gt; way back in 1997, only a few years after JavaScript’s &lt;code&gt;Date&lt;/code&gt; was turned loose on the unsuspecting world; meanwhile, we’ve been saddled with this mess ever since. It’s wildly inconsistent when it comes to parsing dates, as you’ve seen so far here. It has no sense of time zones beyond the local one and GMT, which is not ideal where “world-wide” is right there in the web’s name — and speaking-of, &lt;code&gt;Date&lt;/code&gt; only respects the Gregorian calendar model. It wholesale does not understand the concept of daylight savings time, which— I mean, okay, yeah, samesies, but I’m not made of computers. All these shortcomings make it exceptionally common to use a third-party library dedicated to working around it all, some of which are absolutely massive; a performance drain that has done real and measurable damage to the web.&lt;/p&gt;
    &lt;p&gt;None of these are my major issue with &lt;code&gt;Date&lt;/code&gt;. My complaint is about more than parsing or syntax or “developer ergonomics” or the web-wide performance impact of wholly necessary workarounds or even the definition of the word “date.” My issue with &lt;code&gt;Date&lt;/code&gt; is soul-deep. My problem with &lt;code&gt;Date&lt;/code&gt; is that using it means deviating from the fundamental nature of time itself.&lt;/p&gt;
    &lt;p&gt;All JavaScript’s primitives values are immutable, meaning that the values themselves cannot be changed. The number value &lt;code&gt;3&lt;/code&gt; can never represent anything but the concept of “three” — you can’t make &lt;code&gt;true&lt;/code&gt; mean anything other than “true.” These are values with concrete, iron-clad, real-world meanings. We know what three is. It can’t be some other non-three thing. These immutable data types are stored by value, meaning that a variable that represents the number value &lt;code&gt;3&lt;/code&gt; effectively “contains” — and thus behaves as — the number value &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When an immutable value is assigned to a variable, the JavaScript engine creates a copy of that value and stores the copy in memory:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theNumber = 3; console.log( theNumber ); // Result: 3&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;This fits the common mental model for “a variable” just fine: &lt;code&gt;theNumber&lt;/code&gt; “contains” &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When we initialize &lt;code&gt;theOtherNumber&lt;/code&gt; with the value bound to &lt;code&gt;theNumber&lt;/code&gt;, that mental model holds: once again a &lt;code&gt;3&lt;/code&gt; is created and stored in memory. &lt;code&gt;theOtherNumber&lt;/code&gt; can now be thought of as containing its own discrete &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theNumber = 3; const theOtherNumber = theNumber; console.log( theOtherNumber ); // Result: 3;&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;The value of &lt;code&gt;theNumber&lt;/code&gt; isn’t changed when we alter the value associated with &lt;code&gt;theOtherNumber&lt;/code&gt;, of course — again, we’re working with two discrete instances of &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theNumber = 3; let theOtherNumber = theNumber; theOtherNumber = 5; console.log( theOtherNumber ); // Result: 5; console.log( theNumber ); // Result: 3&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;When you change the value bound to &lt;code&gt;theOtherNumber&lt;/code&gt;, you’re not changing the &lt;code&gt;3&lt;/code&gt;, you’re creating a new, immutable number value and binding that in its place. Hence an error when you try to tinker with a variable declared using &lt;code&gt;const&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theNumber = 3; theNumber = 5; // Result: Uncaught TypeError: invalid assignment to const 'theNumber'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can’t change the binding of a &lt;code&gt;const&lt;/code&gt;, and you definitely can’t alter the meaning of &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Data types that can be changed after they’re created are mutable, meaning that the data value itself can be altered. Object values — any non-primitive value, like an array, map, or set — are mutable.&lt;/p&gt;
    &lt;p&gt;Variables (and object properties, function parameters, and elements in an array, set, or map) can’t “contain” an object, the way we might think of &lt;code&gt;theNumber&lt;/code&gt; in the example above as “containing” &lt;code&gt;3&lt;/code&gt;. A variable can contain either a primitive value or a reference value, the latter of which is a pointer to that object’s stored location in memory. When you assign an object to a variable, instead of creating a copy of that object, the identifier represents a reference to the object’s stored position in memory. That’s why an object bound to a variable declared with &lt;code&gt;const&lt;/code&gt; can still be altered: the reference value can’t be changed, but the values of the object can:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theObject = { theValue : 3 }; theObject.theValue++; console.log( theObject.theValue ); // Result: 4&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;You still can’t change the binding of a &lt;code&gt;const&lt;/code&gt;, but you can alter the object that binding references.&lt;/p&gt;
    &lt;p&gt;When a reference value is assigned from one variable to another, the JavaScript engine creates a copy of that reference value — not the object value itself, the way a discrete copy is made of a primitive value. Both identifiers point to the same object in memory — any changes made to that object by way of one reference will be reflected by the others, because they’re all referencing the same thing:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theObject = { theValue : 3 }; const theOtherObj = theObject; theOtherObj.theValue++; console.log( theOtherObj.theValue ); // Result: 4 console.log( theObject.theValue ); // Result: 4&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is what gets me about JavaScript’s date handling. Despite representing “point to it on a calendar” values, JavaScript’s date values are mutable — &lt;code&gt;Date&lt;/code&gt; is a constructor, invoking a constructor with &lt;code&gt;new&lt;/code&gt; necessarily results in an object, and all objects are inherently mutable:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theDate = new Date(); console.log( typeof theDate ); // Result: object&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Even though “January 1st, 2026” is as much an immutable real-world concept as “three” or “true,” the only way we have of representing that date is a with a mutable data structure.&lt;/p&gt;
    &lt;p&gt;This also means that any variable initialized with an instance of the &lt;code&gt;Date&lt;/code&gt; constructor contains a reference value, pointing to a data value in memory that can be changed by way of any reference to that value:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theDate = new Date(); console.log( theDate.toDateString() ); // Result: Tue Dec 30 2025 theDate.setMonth( 10 ); console.log( theDate.toDateString() ); // Result: Sun Nov 30 2025&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Again, we’re going to breeze right over the fact that month &lt;code&gt;10&lt;/code&gt; is November.&lt;/p&gt;
    &lt;p&gt;So despite real-world dates having set-in-stone meanings, the process of interacting with an instance of &lt;code&gt;Date&lt;/code&gt; that represents that real-world value can mean altering that instance in ways we didn’t necessarily intend:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = new Date(); const addDay = theDate =&amp;gt; { theDate.setDate( theDate.getDate() + 1 ); return theDate; }; console.log(`Today is ${ today.toLocaleDateString() }, tomorrow is ${ addDay( today ).toLocaleDateString() }.`); // Result: Today is 12/31/2025. Tomorrow is 1/1/2026.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Fine so far, right? Today is today, tomorrow is tomorrow; all is right in the world. You’d be forgiven for committing this to a codebase and moving on with your day. That is, unless we reordered the output slightly.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = new Date(); const addDay = theDate =&amp;gt; { theDate.setDate( theDate.getDate() + 1 ); return theDate; }; console.log(`Tomorrow will be ${ addDay( today ).toLocaleDateString() }. Today is ${ today.toLocaleDateString() }.`); // Result: Tomorrow will be 1/1/2026. Today is 1/1/2026.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;See what happened there? the variable &lt;code&gt;today&lt;/code&gt; represents a reference to the object created by &lt;code&gt;new Date()&lt;/code&gt;. When we provided &lt;code&gt;today&lt;/code&gt; as an argument to the &lt;code&gt;addDay&lt;/code&gt; function, the parameter &lt;code&gt;theDate&lt;/code&gt; now represents a copy of the reference value — not a copy of the value, but a second reference to the object that represents today’s date. When we manipulate that value to determine the date of the following day, we’re manipulating the mutable object in memory, not an immutable copy — today becomes tomorrow, the falcon has a hard time hearing the falconer, the center starts to look a little iffy vis-a-vis “holding,” and so on.&lt;/p&gt;
    &lt;p&gt;Now, by this point you can probably tell that I’m not here to praise &lt;code&gt;Date&lt;/code&gt;, but what you might not expect is that I’m here to bury it. That’s right: &lt;code&gt;Date&lt;/code&gt; is soon to be over, done, gone, as “deprecated” as any part of the web platform can be — which is to say, “around forever, but you shouldn’t use it anymore, if you can avoid it.” Soon we will — at long last — have an object that replaces &lt;code&gt;Date&lt;/code&gt; wholesale: &lt;code&gt;Temporal&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Temporal is not a constructor, it’s a namespace object&lt;/head&gt;
    &lt;p&gt;The sharp-eyed among you may have noticed that I said “an object that replaces &lt;code&gt;Date&lt;/code&gt;,” not “a constructor.” &lt;code&gt;Temporal&lt;/code&gt; is not a constructor, and your browser’s developer console will tell you the same if you attempt to invoke it as one:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = new Temporal(); // Uncaught TypeError: Temporal is not a constructor&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;Temporal&lt;/code&gt; is a way better name for something that pertains to time, if you ask me.&lt;/p&gt;
    &lt;p&gt;Instead, &lt;code&gt;Temporal&lt;/code&gt; is a namespace object — an ordinary object made up of static properties and methods, like the &lt;code&gt;Math&lt;/code&gt; object:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;console.log( Temporal ); /* Result (expanded): Temporal { … } Duration: function Duration() Instant: function Instant() Now: Temporal.Now { … } PlainDate: function PlainDate() PlainDateTime: function PlainDateTime() PlainMonthDay: function PlainMonthDay() PlainTime: function PlainTime() PlainYearMonth: function PlainYearMonth() ZonedDateTime: function ZonedDateTime() Symbol(Symbol.toStringTag): "Temporal" */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;I find this immediately understandable compared to &lt;code&gt;Date&lt;/code&gt;.  The classes and namespaces objects that &lt;code&gt;Temporal&lt;/code&gt; contains allow you to calculate durations between two points in time, represent a point in time with or without time zone specificity, or access the current moment in time via the &lt;code&gt;Now&lt;/code&gt; property. &lt;code&gt;Temporal.Now&lt;/code&gt; references a namespace object containing properties and methods of its own:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;console.log( Temporal.Now ); /* Result (expanded): Temporal.Now { … } instant: function instant() plainDateISO: function plainDateISO() plainDateTimeISO: function plainDateTimeISO() plainTimeISO: function plainTimeISO() timeZoneId: function timeZoneId() zonedDateTimeISO: function zonedDateTimeISO() Symbol(Symbol.toStringTag): "Temporal.Now" &amp;lt;prototype&amp;gt;: Object { … } */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;Temporal&lt;/code&gt; gives us a sensible, plain-language way to grab today’s date, a la raggedy old  &lt;code&gt;Date&lt;/code&gt;: the &lt;code&gt;Now&lt;/code&gt; property contains a &lt;code&gt;plainDateISO()&lt;/code&gt; method. Since we’re not specifying anything in the way of time zones (a thing we can do now, thanks to Temporal) that method gives us back today’s date in the current one — EST, in my case:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;console.log( Temporal.Now.plainDateISO() ); /* Result (expanded): Temporal.PlainDate 2025-12-31 &amp;lt;prototype&amp;gt;: Object { … } */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notice how &lt;code&gt;plainDateISO&lt;/code&gt; results in an already-formatted, date-only value? Stay tuned; that’ll come up again later.&lt;/p&gt;
    &lt;p&gt;—wait. That looks familiar:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const nowTemporal = Temporal.Now.plainDateISO(); const nowDate = new Date(); console.log( nowTemporal ); /* Result (expanded): Temporal.PlainDate 2025-12-31 &amp;lt;prototype&amp;gt;: Object { … } */ console.log( nowDate ); /* Result (expanded): Date Tue Dec 31 2025 11:05:52 GMT-0500 (Eastern Standard Time) &amp;lt;prototype&amp;gt;: Date.prototype { … } */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Could it be that—…&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const rightNow = Temporal.Now.instant(); console.log( typeof rightNow ); // object&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, we’re still working with a mutable object that represents the current date, I say in my spookiest voice, flashlight squarely beneath my chin. At a glance, this might not seem like it addresses my big complaint with &lt;code&gt;Date&lt;/code&gt; at all.&lt;/p&gt;
    &lt;p&gt;Well, we’re kind of at the mercy of the nature of the language, here: dates represent complex real-world values, complex data necessitates complex data structures, and for JavaScript, that means objects. The difference is in how we interact with these Temporal objects, as compared to instances of &lt;code&gt;Date&lt;/code&gt;, and — as is so often the case — the magic is in the prototype chain:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const nowTemporal = Temporal.Now.plainDateISO(); console.log( nowTemporal.__proto__ ); /* Result (expanded): Object { … } add: function add() calendarId: &amp;gt;&amp;gt; constructor: function PlainDate() day: &amp;gt;&amp;gt; dayOfWeek: &amp;gt;&amp;gt; dayOfYear: &amp;gt;&amp;gt; daysInMonth: &amp;gt;&amp;gt; daysInWeek: &amp;gt;&amp;gt; daysInYear: &amp;gt;&amp;gt; equals: function equals() era: &amp;gt;&amp;gt; eraYear: &amp;gt;&amp;gt; inLeapYear: &amp;gt;&amp;gt; month: &amp;gt;&amp;gt; monthCode: &amp;gt;&amp;gt; monthsInYear: &amp;gt;&amp;gt; since: function since() subtract: function subtract() toJSON: function toJSON() toLocaleString: function toLocaleString() toPlainDateTime: function toPlainDateTime() toPlainMonthDay: function toPlainMonthDay() toPlainYearMonth: function toPlainYearMonth() toString: function toString() toZonedDateTime: function toZonedDateTime() until: function until() valueOf: function valueOf() weekOfYear: &amp;gt;&amp;gt; with: function with() withCalendar: function withCalendar() year: &amp;gt;&amp;gt; yearOfWeek: &amp;gt;&amp;gt; Symbol(Symbol.toStringTag): "Temporal.PlainDate" &amp;lt;get calendarId()&amp;gt;: function calendarId() &amp;lt;get day()&amp;gt;: function day() &amp;lt;get dayOfWeek()&amp;gt;: function dayOfWeek() &amp;lt;get dayOfYear()&amp;gt;: function dayOfYear() &amp;lt;get daysInMonth()&amp;gt;: function daysInMonth() &amp;lt;get daysInWeek()&amp;gt;: function daysInWeek() &amp;lt;get daysInYear()&amp;gt;: function daysInYear() &amp;lt;get era()&amp;gt;: function era() &amp;lt;get eraYear()&amp;gt;: function eraYear() &amp;lt;get inLeapYear()&amp;gt;: function inLeapYear() */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Right away you’ll notice that there are a number of methods and properties devoted to accessing, formatting, and manipulating the details of the Temporal object we’re working with. No big surprises there — it means a little bit of a learning curve, sure, but nothing an occasional trip over to MDN couldn’t solve, and they all more-or-less do what they say on their respective tins. The big difference from working with &lt;code&gt;Date&lt;/code&gt; is how they do so, at a fundamental level:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const nowTemporal = Temporal.Now.plainDateISO(); // Current local date: console.log( nowTemporal ); /* Result (expanded): Temporal.PlainDate 2025-12-30 &amp;lt;prototype&amp;gt;: Object { … } */ // Current local year: console.log( nowTemporal.year ); // Result: 2025 // Current local date and time: console.log( nowTemporal.toPlainDateTime() ); /* Result (expanded): Temporal.PlainDateTime 2025-12-30T00:00:00 &amp;lt;prototype&amp;gt;: Object { … } */ // Specify that this date represents the Europe/London time zone: console.log( nowTemporal.toZonedDateTime( "Europe/London" ) ); /* Result (expanded): Temporal.ZonedDateTime 2025-12-30T00:00:00+00:00[Europe/London] &amp;lt;prototype&amp;gt;: Object { … } */ // Add a day to this date: console.log( nowTemporal.add({ days: 1 }) ); /* Temporal.PlainDate 2025-12-31 &amp;lt;prototype&amp;gt;: Object { … } */ // Add one month and one day to this date, and subtract two years: console.log( nowTemporal.add({ months: 1, days: 1 }).subtract({ years: 2 }) ); /* Temporal.PlainDate 2024-01-31 &amp;lt;prototype&amp;gt;: Object { … } */ console.log( nowTemporal ); /* Result (expanded): Temporal.PlainDate 2025-12-30 &amp;lt;prototype&amp;gt;: Object { … } */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notice how none of these transformations required us to manually spin up any new objects, and that the value of the object referenced by &lt;code&gt;nowTemporal&lt;/code&gt; remains unchanged? Unlike &lt;code&gt;Date&lt;/code&gt;,  the methods we use to interact with a Temporal object result in new Temporal objects, rather than requiring us to use them in the context of a new instance or to modify the instance we’re working with — which is how we’re able to chain the &lt;code&gt;add&lt;/code&gt; and &lt;code&gt;subtract&lt;/code&gt; methods together in &lt;code&gt;nowTemporal.add({ months: 1, days: 1 }).subtract({ years: 2 })&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Sure, we’re still working with objects, and that means we’re working with mutable data structures that represent real-world values:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const nowTemporal = Temporal.Now.plainDateISO(); nowTemporal.someProperty = true; console.log( nowTemporal ); /* Result (expanded): Temporal.PlainDate 2026-01-05 someProperty: true &amp;lt;prototype&amp;gt;: Object { … }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;…But the value represented by that Temporal object isn’t meant to be changed during the normal course of interacting with it — even though the object is still essentially mutable, we’re not stuck using that object in ways that could alter what it means in terms of real-world dates and times. I’ll take it.&lt;/p&gt;
    &lt;p&gt;So, let’s revisit that janky little “today is X, tomorrow is Y” script we wrote using &lt;code&gt;Date&lt;/code&gt; earlier. First, we’ll fix it by making sure we’re working with two discrete instances of &lt;code&gt;Date&lt;/code&gt; rather than modifying the instance that represents today’s date:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = new Date(); const addDay = theDate =&amp;gt; { const tomorrow = new Date(); tomorrow.setDate( theDate.getDate() + 1 ); return tomorrow; }; console.log(`Tomorrow will be ${ addDay( today ).toLocaleDateString() }. Today is ${ today.toLocaleDateString() }.`); // Result: Tomorrow will be 1/1/2026. Today is 12/31/2025.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thanks, I hate it.&lt;/p&gt;
    &lt;p&gt;Okay, fine. It gets the job done, just as it has since the day &lt;code&gt;Date&lt;/code&gt; first bumbled its way onto the web. We’re not unwittingly altering the value of &lt;code&gt;today&lt;/code&gt; since we’re spinning up a new instance of &lt;code&gt;Date&lt;/code&gt; inside our &lt;code&gt;addDay&lt;/code&gt; function — wordy, but it works, as it has for decades now. We add &lt;code&gt;1&lt;/code&gt; to it, which we have to just kind of know means add one day. Then in our template literal we need to keep nudging JavaScript to give us the date in a format that doesn’t include the current time, as a string. It’s functional, but verbose.&lt;/p&gt;
    &lt;p&gt;Now, let’s redo it using &lt;code&gt;Temporal&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = Temporal.Now.plainDateISO(); console.log(`Tomorrow will be ${ today.add({ days: 1 }) }. Today is ${ today }.`); // Result: Tomorrow will be 2026-01-01. Today is 2025-12-31.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Now we’re talking.&lt;/p&gt;
    &lt;p&gt;So much better. Leaner, meaner, and way less margin for error. We want today’s date without the time, and the object that results from invoking &lt;code&gt;plainDateISO&lt;/code&gt; (and any new Temporal objects created from it) will retain that formatting without being coerced to a string. Formatting: check.&lt;/p&gt;
    &lt;p&gt;We want to output a value that represents today’s date plus one day, and we want to do so in a way where we are unmistakably saying “add one day to it” with no parsing guesswork: check and check.&lt;/p&gt;
    &lt;p&gt;Most importantly, we don’t want to run the risk of having our original &lt;code&gt;today&lt;/code&gt; object altered unintentionally — because the result of calling the &lt;code&gt;add&lt;/code&gt; method will always be a new Temporal object: check.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Temporal&lt;/code&gt; is going to be a massive improvement over &lt;code&gt;Date&lt;/code&gt;, and I only say “going to be” because it still isn’t quite ready for prime-time usage. The draft specification for the proposed &lt;code&gt;Temporal&lt;/code&gt; object has reached stage three of the standardization process, meaning it is now officially “recommended for implementation” — not yet part of the standard that informs the ongoing development of JavaScript itself, but close enough that browsers can start tinkering with it. That means the results of that early experimentation may be used to further refine the specification, so nothing is set in stone just yet. Web standards are an iterative process, after all.&lt;/p&gt;
    &lt;p&gt;That’s where you and I come in. Now that &lt;code&gt;Temporal&lt;/code&gt; has landed in the latest versions of Chrome and Firefox — and others, soon — it’s time for us to get in there and kick the tires a little bit. We may not have had any say in &lt;code&gt;Date&lt;/code&gt;, but we get to experiment with &lt;code&gt;Temporal&lt;/code&gt; before the final implementations land.&lt;/p&gt;
    &lt;p&gt;Soon, JavaScript will have sensible, modern date handling, and we’ll finally be able to cram &lt;code&gt;Date&lt;/code&gt; way in the back of the junk drawer with the rubber bands, mismatched jar lids, mystery keys, and probably-half-empty AA batteries — still present, still an inexorable part of the web platform, but no longer our first, last, and only way of handling dates. And we only had to wait— well, hold on, let me just crunch the numbers real quick:&lt;/p&gt;
    &lt;head rend="h3"&gt;Try it out&lt;/head&gt;
    &lt;code&gt;const today = Temporal.Now.plainDateISO();
const jsShipped = Temporal.PlainDate.from( "1995-12-04" );
const sinceDate = today.since( jsShipped, { largestUnit: 'year' });

console.log( `${ sinceDate.years } years, ${ sinceDate.months } months, and ${ sinceDate.days } days.` );&lt;/code&gt;
    &lt;p&gt;Sure, the best time to replace &lt;code&gt;Date&lt;/code&gt; would’ve been back in 1995, but hey: the second best time is &lt;code&gt;Temporal.Now&lt;/code&gt;, right?&lt;/p&gt;
    &lt;p&gt;Enjoyed this article? You can support us by leaving a tip via Open Collective&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://piccalil.li/blog/date-is-out-and-temporal-is-in/"/><published>2026-01-12T15:20:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589675</id><title>Apple picks Gemini to power Siri</title><updated>2026-01-13T05:46:01.972320+00:00</updated><content>&lt;doc fingerprint="3d46b9ec932631f0"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple is joining forces with Google to power its artificial intelligence features, including a major Siri upgrade expected later this year.&lt;/p&gt;
    &lt;p&gt;The multiyear partnership will lean on Google's Gemini and cloud technology for future Apple foundational models, according to a joint statement obtained by CNBC's Jim Cramer.&lt;/p&gt;
    &lt;p&gt;"After careful evaluation, we determined that Google's technology provides the most capable foundation for Apple Foundation Models and we're excited about the innovative new experiences it will unlock for our users," Apple said in a statement Monday.&lt;/p&gt;
    &lt;p&gt;The models will continue to run on Apple devices and the company's private cloud compute, the companies added.&lt;/p&gt;
    &lt;p&gt;Apple declined to comment on the terms of the deal. Google referred CNBC to the joint statement.&lt;/p&gt;
    &lt;p&gt;In August, Bloomberg reported that Apple was in early talks with Google to use a custom Gemini model to power a new iteration of Siri. The news outlet later reported that Apple was planning to pay about $1 billion a year to utilize Google AI.&lt;/p&gt;
    &lt;p&gt;The deal is another major indicator of growing trust in Google's accelerating AI agenda and comeback against OpenAI. In 2025, the search giant logged its best year since 2009 and surpassed Apple in market capitalization last week for the first time since 2019.&lt;/p&gt;
    &lt;p&gt;Google already pays Apple billions each year to be the default search engine on iPhones. But that lucrative partnership briefly came into question after Google was found to hold an illegal internet search monopoly.&lt;/p&gt;
    &lt;p&gt;In September, a judge ruled against a worst-case scenario outcome that could have forced Google to divest its Chrome browser business.&lt;/p&gt;
    &lt;p&gt;The decision also allowed Google to continue to make deals such as the one with Apple.&lt;/p&gt;
    &lt;p&gt;Shares climbed following the news but later pulled back. Google briefly touched above a $4 trillion market value.&lt;/p&gt;
    &lt;p&gt;Apple has mostly stood on the sidelines of the AI frenzy that's swept up Wall Street since the launch of OpenAI's ChatGPT at the end of 2022.&lt;/p&gt;
    &lt;p&gt;Hyperscalers Amazon, Meta Platforms and Microsoft have shelled out billions on AI products, tools and infrastructure for their customers.&lt;/p&gt;
    &lt;p&gt;That's amped up the pressure on the iPhone maker to deliver an impressive Siri AI voice upgrade, which it delayed last year until 2026, despite running ads for the product.&lt;/p&gt;
    &lt;p&gt;"It's going to take us longer than we thought to deliver on these features and we anticipate rolling them out in the coming year," the company said in a statement at the time.&lt;/p&gt;
    &lt;p&gt;Apple currently partners with OpenAI to integrate ChatGPT into Siri and Apple Intelligence, specifically for complicated queries that can tap into the AI model's world knowledge. It's unclear what the Google partnership means for the ChatGPT integration in the future.&lt;/p&gt;
    &lt;p&gt;The iPhone maker told CNBC that it isn't making any changes to the agreement. OpenAI did not immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Google has made steady progress on its AI agenda, introducing its upgraded Gemini 3 model late last year.&lt;/p&gt;
    &lt;p&gt;In October, Google CEO Sundar Pichai said the company's cloud segment signed more deals worth over $1 billion through the third quarter of 2025 than the previous two years combined.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2026/01/12/apple-google-ai-siri-gemini.html"/><published>2026-01-12T15:22:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46590280</id><title>TimeCapsuleLLM: LLM trained only on data from 1800-1875</title><updated>2026-01-13T05:46:01.406763+00:00</updated><content>&lt;doc fingerprint="34089b5011be0c83"&gt;
  &lt;main&gt;
    &lt;head&gt;🌐 Language&lt;/head&gt;
    &lt;code&gt;  &amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;
    &lt;p&gt;A language model trained from scratch exclusively on data from certain places and time periods to reduce modern bias and emulate the voice, vocabulary, and worldview of the era.&lt;/p&gt;
    &lt;p&gt;Imagine if an AI model didnt just pretend to be historical but actually was.&lt;/p&gt;
    &lt;p&gt;v0 and v0.5 built on nanoGPT by Andrej Karpathy Core training scripts and model architecture are his work.&lt;/p&gt;
    &lt;p&gt;v1 built on Phi 1.5 by Microsoft&lt;/p&gt;
    &lt;p&gt;v2 built on llamaforcausallm&lt;/p&gt;
    &lt;p&gt;Early prompts show the model responding with 1800's language and behavior. Example: Prompt: "Who art Henry?" and it replied "I know that man, I have did not a black, the storm."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No mention of modern concepts&lt;/item&gt;
      &lt;item&gt;Mostly era-accurate vocabulary&lt;/item&gt;
      &lt;item&gt;Sentences are mostly incoherent (expected for ~187MB training data)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A significant improvement over v0.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Victorian writing style, proper punctuation, mostly grammatical sentences&lt;/item&gt;
      &lt;item&gt;Still high factual hallucination rate&lt;/item&gt;
      &lt;item&gt;OCR noise (“Digitized by Google”) still present in outputs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First model to recall and connect a real historical event with an actual figure from the dataset.&lt;/p&gt;
    &lt;p&gt;Example: Prompt: "It was the year of our Lord 1834"&lt;/p&gt;
    &lt;p&gt;The output: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity"&lt;/p&gt;
    &lt;p&gt;At first I assumed that a protest might have coincidentally taken place the same year but take a look at this:&lt;/p&gt;
    &lt;p&gt;Trained using a 15GB sample from v2's 90GB dataset. Model trained to 10K steps only.&lt;/p&gt;
    &lt;p&gt;There was an issue with tokenization that causes output to look like this:&lt;/p&gt;
    &lt;p&gt;PROMPT: Who is Charles Dickens?&lt;/p&gt;
    &lt;p&gt;W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht&lt;/p&gt;
    &lt;p&gt;Corrected:&lt;/p&gt;
    &lt;p&gt;"Who is Charles Dickens? Does that work more of his excellent stirring, in his plays, in the Great Company's farm? What I have yet to quote from Jack Pickett? Do you not know that they were a species of galloping, or sawing of their breasts, or what was to be done about the time when Jackson was looking on the window? What is the success of an Englishman, and which his son has not been discovering to me, whereby to accomplish such a weight? Did you ever make a passage into the old roadway, or to an anchor-breeze at the foot of our boat, which you must leave us? The fact is, that whether the wind would rise up from the plain on Saturday night or noontide, or till the north, or otherwise, we shall be compelled to describe a formidable barrier, with the same effects as the present. In this situation, at least, it is not too much to say that we have left that room. I believe there are three copies in the 'Five Hundred-fold,' to be referred to, as the first number of our readers who wish t[o]...”&lt;/p&gt;
    &lt;p&gt;Trained using the same 15GB sample used for training eval1. Model is again trained to 10K steps only.&lt;/p&gt;
    &lt;p&gt;PROMPT:Charles Darwin&lt;/p&gt;
    &lt;p&gt;Charles DarwinECCEMACY. Sir, — The following case is interesting to me : — I was in London a fortnight, and was much affected with an attack of rheumatism. The first attack of rheumatism was a week before I saw you, and the second when I saw you, and the third when I saw you, and the third in the same time. The second attack of gout, however, was not accompanied by any febrile symptoms, but was accompanied with an increased flow of urine, and with a more copious discharge of urine. The third attack was an hour after I saw you, and was succeeded by a return of a paroxysm of gout, and a more rapid return of the gout. The fourth attack was also accompanied by a fever, but was not always accompanied by any febrile symptoms. The third attack of gout was a fortnight after you had been ill, and the fourth was followed by a paroxysm of gout. The fourth attack was a fortnight after you were attacked, and was accompanied by a sense&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;90GB of 1800-1875 London texts&lt;/item&gt;
      &lt;item&gt;136,344 documents&lt;/item&gt;
      &lt;item&gt;The full 90GB is not avalaible yet as it hasn't been tokenized but you can find a 15GB sample here: https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Refer to v2 bias report for more info.&lt;/p&gt;
    &lt;p&gt;This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)&lt;/item&gt;
      &lt;item&gt;Keep them within your chosen time/place window&lt;/item&gt;
      &lt;item&gt;Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.&lt;/item&gt;
      &lt;item&gt;This will give you vocab.json and merges.txt&lt;/item&gt;
      &lt;item&gt;Thes files define vocab and merge rules for your model&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Refer to nanoGPT by Andrej Karpathy for the training process or your chosen architecture’s docs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.&lt;/p&gt;
    &lt;p&gt;For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1875.&lt;/p&gt;
    &lt;p&gt;I'm using books, legal documents, newspapers, and other writings from 1800–1875 London. The list I linked (for v0) has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents: https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt&lt;/p&gt;
    &lt;p&gt;Dataset sizes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;v0: ~187MB&lt;/item&gt;
      &lt;item&gt;v0.5: ~435MB&lt;/item&gt;
      &lt;item&gt;v1: ~6.25GB&lt;/item&gt;
      &lt;item&gt;v2mini-eval1: 15GB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;v0: 16M Parameters&lt;/p&gt;
    &lt;p&gt;v0.5 123M Parameters&lt;/p&gt;
    &lt;p&gt;v1: 700M Parameters&lt;/p&gt;
    &lt;p&gt;v2mini-eval1: 300M Parameters&lt;/p&gt;
    &lt;p&gt;GPU: Geforce rtx 4060 CPU: i5-13400F Ram: 16GB DDR5.&lt;/p&gt;
    &lt;p&gt;GPU: A100 SXM rented&lt;/p&gt;
    &lt;p&gt;GPU: A100 SXM rented&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/haykgrigo3/TimeCapsuleLLM"/><published>2026-01-12T16:04:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46591100</id><title>Show HN: AI in SolidWorks</title><updated>2026-01-13T05:46:01.171718+00:00</updated><content>&lt;doc fingerprint="b3c6fa5900f7aa44"&gt;
  &lt;main&gt;
    &lt;p&gt;1.1.0&lt;/p&gt;
    &lt;p&gt;01-11-2026&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add planning mode&lt;/item&gt;
      &lt;item&gt;Add macro writing/running&lt;/item&gt;
      &lt;item&gt;Detect and report sketch issues&lt;/item&gt;
      &lt;item&gt;Improve caching efficiency&lt;/item&gt;
      &lt;item&gt;Various AI context improvements and bug fixes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;LAD (Language-Aided Designer)&lt;/p&gt;
    &lt;p&gt;A SolidWorks add-in to design with natural language using AI&lt;/p&gt;
    &lt;p&gt;Describe your design in plain language and LAD will translate it into SolidWorks operations, creating sketches, features, and assemblies all through natural conversation. LAD uses screenshots and the feature tree to understand your model's current state, verifying operations were completed correctly and correcting mistakes.&lt;/p&gt;
    &lt;p&gt;Design from Documentation and Images&lt;/p&gt;
    &lt;p&gt;Provide documentation files, images, or examples of previous parts and assemblies, and LAD will intelligently read and use them.&lt;/p&gt;
    &lt;p&gt;Write and Run Macros&lt;/p&gt;
    &lt;p&gt;LAD can write and run VBA macros for reproducibility and niche functionality not covered by standard LAD tools. When writing macros, LAD searches SolidWorks documentation and examples to better understand the API.&lt;/p&gt;
    &lt;p&gt;Permissioning and Versioning&lt;/p&gt;
    &lt;p&gt;LAD stores checkpoints so you can revert unwanted changes, lets you control which commands run automatically, and uses rules you provide to guide the AI as it works.&lt;/p&gt;
    &lt;p&gt;LAD (Language-Aided Designer) is a system that integrates directly into SolidWorks as an Add-in and can create sketches, features, macros, and other CAD objects.&lt;/p&gt;
    &lt;p&gt;1.1.0&lt;/p&gt;
    &lt;p&gt;01-11-2026&lt;/p&gt;
    &lt;p&gt;1.0.0&lt;/p&gt;
    &lt;p&gt;01-05-2026&lt;/p&gt;
    &lt;p&gt;Try LAD now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.trylad.com"/><published>2026-01-12T16:56:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46591170</id><title>Carma (YC W24 clients, A in 6mo) Eng hiring: Replace $500B human fleet ops with AI</title><updated>2026-01-13T05:46:00.965962+00:00</updated><content>&lt;doc fingerprint="b98cc6fbbd9605ac"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;TL;DR: Live product used daily by Fortune 500 clients; post-revenue, real operations, solving a real problem today. We are growing fast, hiring founding engineers, $200K+ cash base + 3.00%+ equity, build with us in-person in SF.&lt;/p&gt;
      &lt;p&gt;We’re building an AI platform that makes fleet operations — maintenance, repairs, approvals, pricing — fully autonomous. This massive $500B industry still runs on phone calls, PDFs, and spreadsheets.&lt;/p&gt;
      &lt;p&gt;Today: - Live product already used by multiple Fortune 500 clients nationwide&lt;/p&gt;
      &lt;p&gt;- Signing new enterprise clients every month&lt;/p&gt;
      &lt;p&gt;- $5.5M seed raised, Series A on track in mid-2026&lt;/p&gt;
      &lt;p&gt;- Your code runs in live fleet ops and gets same-day feedback&lt;/p&gt;
      &lt;p&gt;Stack: React, NestJS, PostgreSQL, AWS Location: On-site, San Francisco&lt;/p&gt;
      &lt;p&gt;Roles: Founding Head of Engineering Comp: $250K + 5% equity https://www.ycombinator.com/companies/carma/jobs/5Mwg4jp-fou...&lt;/p&gt;
      &lt;p&gt;Founding Full-Stack Engineer Comp: $200K + 3% equity https://www.ycombinator.com/companies/carma/jobs/YLaym2M-fou...&lt;/p&gt;
      &lt;p&gt;If you want: real % ownership, work w/ a top 0.1% business team (all STEM degrees) closing brand-name clients nonstop who use what you build every day, and experience real PMF — this is one of those rare early roles.&lt;/p&gt;
      &lt;p&gt;Interested? Let's chat ASAP. Apply above or email me directly: muhammad at joincarma.com&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46591170"/><published>2026-01-12T17:00:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46591402</id><title>Message Queues: A Simple Guide with Analogies (2024)</title><updated>2026-01-13T05:46:00.822094+00:00</updated><content>&lt;doc fingerprint="e71cd8526aa188cb"&gt;
  &lt;main&gt;
    &lt;p&gt;I find stories and analogies very fascinating and — to explain message queues in a super approachable way, we will use some analogies: databases, warehouses and post offices.&lt;/p&gt;
    &lt;p&gt;Stay with me …&lt;/p&gt;
    &lt;p&gt;Databases are primarily used for data persistence — think Postgres or MongoDB. Like databases, message queues also perform some storage function. But why use message queues for data storage when there are databases? Think of databases and message queues in terms of warehouses and post offices.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Databases are like warehouses - they are designed to hold a lot of different things, most times, over a long period of time.&lt;/item&gt;
      &lt;item&gt;Message queues on the other hand are like post offices — Where letters and packages stop briefly on their way to being delivered. The packages don't stay there long; they're just sorted and sent off to where they need to go.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Essentially, databases are primarily designed for scenarios where you need to store and manage some state over a long period of time. In contrast, you would want to use a message queue for data that you do not want to keep around for very long— A message queue holds information just long enough to send it to the next stop.&lt;/p&gt;
    &lt;p&gt;If you look at message queues from this post office perspective, then you will begin to appreciate the fact that a message queue is simply a medium through which data flows from a source system to a destination system.&lt;/p&gt;
    &lt;p&gt;Looking at message queues as medium of communication is just one perspective, but it’s sufficient to help you get started with message queues — Let’s double down on that perspective.&lt;/p&gt;
    &lt;p&gt;A message queue is a technology that simply receives data, formally called messages in the message queueing world from a source system(s) (producer), lines up these messages in the order they arrive, then sends each message to some final destination, usually another system called the consumer.&lt;/p&gt;
    &lt;p&gt;Note that both the producer and consumer could also just be modules in the same application.&lt;/p&gt;
    &lt;p&gt;Now that we understand the core essence of message queues, let’s explore how they work.&lt;/p&gt;
    &lt;head rend="h2"&gt;How a Message Queue Works&lt;/head&gt;
    &lt;p&gt;Typically, producers and consumers would connect and communicate with a message queue via some protocol that the message queue supports.&lt;/p&gt;
    &lt;p&gt;In other words, a message queue would implement a protocol or some set of protocols. To communicate with a message queue, a producer or consumer would leverage some client library that also implements the protocol or one of the protocols supported by the broker.&lt;/p&gt;
    &lt;p&gt;Most message brokers commonly implement at least one of these protocols : AMQP, MQTT and STOMP. You can learn more about these protocols in our AMQP vs MQTT guide or the AMQP, MQTT and STOMP guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;When to Use a Message Queue&lt;/head&gt;
    &lt;p&gt;We’ve already seen how message queues allow messages to flow from a source system to a destination system. This inherent nature of message queues makes them perfect for communication between systems in a microservice architecture.&lt;/p&gt;
    &lt;p&gt;What is the microservice architecture? Again, let’s start with something you are familiar with — Monoliths.&lt;/p&gt;
    &lt;p&gt;A monolith is characterized by the entire codebase being inside one application. This is a great approach for smaller projects, and many new applications start out as a monolith. This is because on a smaller monoliths are faster to develop, easier to test, and easier to deploy.&lt;/p&gt;
    &lt;p&gt;However when an application starts to grow, the more problems you will see with this architecture. Even with a structured approach, the code often starts to feel messy and the development experience becomes inconvenient. Changes become more difficult to implement, and the risk of introducing bugs is higher.&lt;/p&gt;
    &lt;p&gt;Many times the solution to these problems is to break up your monolith application into microservices. And microservices are smaller, more modular services that focus on a single area of responsibility.&lt;/p&gt;
    &lt;p&gt;The microservice approach has some benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With microservices, there is fault isolation— if one service is buggy, that bug is isolated to just that service. This in turn makes your application more reliable compared to a monolith where a single component error could take down the entire application.&lt;/item&gt;
      &lt;item&gt;There is also the opportunity of being able to diversify the technology stack from service to service, which helps you optimize your services for its purpose. For example, a performance critical service has the chance to make certain performance trade-offs, without putting limits to the rest of the services.&lt;/item&gt;
      &lt;item&gt;Naturally, scaling becomes much easier because you can just scale one of your services instead of scaling the entire application and save a lot of resources.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that we understand what microservices are, let’s cycle back to: Using message queues for communication between systems in a microservice architecture.&lt;/p&gt;
    &lt;p&gt;But before we get to that, note that message queueing isn’t the only way to get services to communicate — There is one other common way:&lt;/p&gt;
    &lt;p&gt;Synchronous communication, where network requests are sent directly from one service to another via REST API calls, for example. Service A will initiate a request and then wait for Service B to finish handling the request and send a response back before it continues on with the activity it was doing.&lt;/p&gt;
    &lt;p&gt;With message queueing, the communication is asynchronous — In this case, Service A can send messages to a message broker and instead of waiting for Service B, it will receive a super quick acknowledgement back from the broker and then it can carry on doing what it was doing while Service B fetches the message from the queue and handles it.&lt;/p&gt;
    &lt;p&gt;This will save your service from overloading if there is a suddenly increased workload, instead the messages are buffered by the queue and your services can just handle them when they have the capacity.&lt;/p&gt;
    &lt;p&gt;There you have it, a very gentle introduction to message queues. Now, let’s do a recap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In summary, message queues are like post offices for your data, moving messages from one place to another. They work by receiving messages from producers, lining them up in the order they arrive, and sending them to consumers. This makes them perfect for situations where systems need to communicate without waiting— think microservice architectures.&lt;/p&gt;
    &lt;p&gt;Understanding how message queues work and when to use them can help you build more reliable and scalable applications.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cloudamqp.com/blog/message-queues-exaplined-with-analogies.html"/><published>2026-01-12T17:17:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46591708</id><title>Postal Arbitrage</title><updated>2026-01-13T05:46:00.656338+00:00</updated><content/><link href="https://walzr.com/postal-arbitrage"/><published>2026-01-12T17:41:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46592344</id><title>Show HN: Yolobox – Run AI coding agents with full sudo without nuking home dir</title><updated>2026-01-13T05:45:59.977169+00:00</updated><content>&lt;doc fingerprint="2c165c5c95e5fbae"&gt;
  &lt;main&gt;
    &lt;code&gt;██╗   ██╗ ██████╗ ██╗      ██████╗ ██████╗  ██████╗ ██╗  ██╗
╚██╗ ██╔╝██╔═══██╗██║     ██╔═══██╗██╔══██╗██╔═══██╗╚██╗██╔╝
 ╚████╔╝ ██║   ██║██║     ██║   ██║██████╔╝██║   ██║ ╚███╔╝
  ╚██╔╝  ██║   ██║██║     ██║   ██║██╔══██╗██║   ██║ ██╔██╗
   ██║   ╚██████╔╝███████╗╚██████╔╝██████╔╝╚██████╔╝██╔╝ ██╗
   ╚═╝    ╚═════╝ ╚══════╝ ╚═════╝ ╚═════╝  ╚═════╝ ╚═╝  ╚═╝
&lt;/code&gt;
    &lt;p&gt;Let your AI go full send. Your home directory stays home.&lt;/p&gt;
    &lt;p&gt;Run Claude Code, Codex, or any AI coding agent in "yolo mode" without nuking your home directory.&lt;/p&gt;
    &lt;p&gt;AI coding agents are incredibly powerful when you let them run commands without asking permission. But one misinterpreted prompt and &lt;code&gt;rm -rf ~&lt;/code&gt; later, you're restoring from backup (yea right, as if you have backups lol).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;yolobox&lt;/code&gt; runs your AI agent inside a container where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Your project directory is mounted at &lt;code&gt;/workspace&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;✅ The agent has full permissions and sudo inside the container&lt;/item&gt;
      &lt;item&gt;✅ Your home directory is NOT mounted (unless you explicitly opt in)&lt;/item&gt;
      &lt;item&gt;✅ Persistent volumes keep tools and configs across sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AI can go absolutely wild inside the sandbox. Your actual home directory? Untouchable.&lt;/p&gt;
    &lt;code&gt;# Install (requires Go)
curl -fsSL https://raw.githubusercontent.com/finbarr/yolobox/master/install.sh | bash

# Or clone and build
git clone https://github.com/finbarr/yolobox.git
cd yolobox
make install&lt;/code&gt;
    &lt;p&gt;Then from any project:&lt;/p&gt;
    &lt;code&gt;cd /path/to/your/project
yolobox&lt;/code&gt;
    &lt;p&gt;You're now in a sandboxed shell. Run &lt;code&gt;claude&lt;/code&gt; and let it rip.&lt;/p&gt;
    &lt;p&gt;The base image comes batteries-included:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI CLIs: Claude Code, Gemini CLI, OpenAI Codex, OpenCode (all pre-configured for full-auto mode!)&lt;/item&gt;
      &lt;item&gt;Node.js 22 + npm/yarn/pnpm&lt;/item&gt;
      &lt;item&gt;Python 3 + pip + venv&lt;/item&gt;
      &lt;item&gt;Build tools: make, cmake, gcc&lt;/item&gt;
      &lt;item&gt;Git + GitHub CLI&lt;/item&gt;
      &lt;item&gt;Common utilities: ripgrep, fd, fzf, jq, vim&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Need something else? You have sudo.&lt;/p&gt;
    &lt;p&gt;Inside yolobox, the AI CLIs are aliased to skip all permission prompts:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Expands to&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;claude&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;claude --dangerously-skip-permissions&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;codex&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;codex --dangerously-bypass-approvals-and-sandbox&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini --yolo&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;opencode&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;opencode&lt;/code&gt; (no yolo flag available yet)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;No confirmations, no guardrails—just pure unfiltered AI, the way nature intended.&lt;/p&gt;
    &lt;code&gt;yolobox                     # Drop into interactive shell
yolobox run &amp;lt;cmd...&amp;gt;        # Run a single command
yolobox run claude          # Run Claude Code in sandbox
yolobox upgrade             # Update binary and pull latest image
yolobox config              # Show resolved configuration
yolobox reset --force       # Delete volumes (fresh start)
yolobox version             # Show version
yolobox help                # Show help&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--runtime &amp;lt;name&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Use &lt;code&gt;docker&lt;/code&gt; or &lt;code&gt;podman&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--image &amp;lt;name&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Custom base image&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mount &amp;lt;src:dst&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Extra mount (repeatable)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--env &amp;lt;KEY=val&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set environment variable (repeatable)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--ssh-agent&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Forward SSH agent socket&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--no-network&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disable network access&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--readonly-project&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mount project read-only (outputs go to &lt;code&gt;/output&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--claude-config&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Copy host &lt;code&gt;~/.claude&lt;/code&gt; config into container&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These are automatically passed into the container if set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GITHUB_TOKEN&lt;/code&gt;/&lt;code&gt;GH_TOKEN&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;GEMINI_API_KEY&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create &lt;code&gt;~/.config/yolobox/config.toml&lt;/code&gt; for global defaults:&lt;/p&gt;
    &lt;code&gt;runtime = "docker"
image = "ghcr.io/finbarr/yolobox:latest"
ssh_agent = true&lt;/code&gt;
    &lt;p&gt;Or &lt;code&gt;.yolobox.toml&lt;/code&gt; in your project for project-specific settings:&lt;/p&gt;
    &lt;code&gt;mounts = ["../shared-libs:/libs:ro"]
env = ["DEBUG=1"]
no_network = true&lt;/code&gt;
    &lt;p&gt;Priority: CLI flags &amp;gt; project config &amp;gt; global config &amp;gt; defaults.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Setting&lt;/p&gt;&lt;code&gt;claude_config = true&lt;/code&gt;in your config will copy your host's Claude config on every container start, overwriting any changes made inside the container. Use the CLI flag&lt;code&gt;--claude-config&lt;/code&gt;for one-time syncs.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: Docker Desktop, OrbStack, or Colima&lt;/item&gt;
      &lt;item&gt;Linux: Docker or Podman&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Memory: Claude Code needs 4GB+ RAM allocated to Docker. Colima defaults to 2GB which will cause OOM kills. Increase with:&lt;/p&gt;
      &lt;code&gt;colima stop &amp;amp;&amp;amp; colima start --memory 8&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;yolobox uses container isolation (Docker or Podman) as its security boundary. When you run &lt;code&gt;yolobox&lt;/code&gt;, it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Starts a container with your project mounted at &lt;code&gt;/workspace&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Runs as user &lt;code&gt;yolo&lt;/code&gt;with sudo access inside the container&lt;/item&gt;
      &lt;item&gt;Does NOT mount your home directory (unless explicitly requested)&lt;/item&gt;
      &lt;item&gt;Uses Linux namespaces to isolate the container's filesystem, process tree, and network&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AI agent has full root access inside the container, but the container's view of the filesystem is restricted to what yolobox explicitly mounts.&lt;/p&gt;
    &lt;p&gt;The trust boundary is the container runtime (Docker/Podman). This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Protection against accidental &lt;code&gt;rm -rf ~&lt;/code&gt;or credential theft&lt;/item&gt;
      &lt;item&gt;✅ Protection against most filesystem-based attacks&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;NOT protection against container escapes — a sufficiently advanced exploit targeting kernel vulnerabilities could break out&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;NOT protection against a malicious AI deliberately trying to escape — this is defense against accidents, not adversarial attacks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're worried about an AI actively trying to escape containment, you need VM-level isolation (see "Hardening Options" below).&lt;/p&gt;
    &lt;p&gt;What yolobox protects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your home directory from accidental deletion&lt;/item&gt;
      &lt;item&gt;Your SSH keys, credentials, and dotfiles&lt;/item&gt;
      &lt;item&gt;Other projects on your machine&lt;/item&gt;
      &lt;item&gt;Host system files and configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What yolobox does NOT protect:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your project directory (it's mounted read-write by default)&lt;/item&gt;
      &lt;item&gt;Network access (use &lt;code&gt;--no-network&lt;/code&gt;to disable)&lt;/item&gt;
      &lt;item&gt;The container itself (the AI has root via sudo)&lt;/item&gt;
      &lt;item&gt;Against kernel exploits or container escape vulnerabilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Level 1: Basic (default)&lt;/p&gt;
    &lt;code&gt;yolobox  # Standard container isolation&lt;/code&gt;
    &lt;p&gt;Level 2: Reduced attack surface&lt;/p&gt;
    &lt;code&gt;yolobox run --no-network --readonly-project claude&lt;/code&gt;
    &lt;p&gt;Level 3: Rootless Podman (recommended for security-conscious users)&lt;/p&gt;
    &lt;code&gt;# Install podman and run rootless
yolobox --runtime podman&lt;/code&gt;
    &lt;p&gt;Rootless Podman runs the container without root privileges on the host, using user namespaces. This significantly reduces the impact of container escapes since the container's "root" maps to your unprivileged user on the host.&lt;/p&gt;
    &lt;p&gt;Level 4: VM isolation (maximum security)&lt;/p&gt;
    &lt;p&gt;For true isolation with no shared kernel, consider running yolobox inside a VM:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: Use a Linux VM via UTM, Parallels, or Lima&lt;/item&gt;
      &lt;item&gt;Linux: Use a Podman machine or dedicated VM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This adds significant overhead but eliminates kernel-level attack surface.&lt;/p&gt;
    &lt;p&gt;For users who want to prevent container access to the local network while preserving internet access:&lt;/p&gt;
    &lt;code&gt;# Rootless podman uses slirp4netns by default, which provides
# network isolation from the host network
podman run --network=slirp4netns:allow_host_loopback=false ...&lt;/code&gt;
    &lt;p&gt;yolobox doesn't currently expose this as a flag, but you can achieve it by running rootless Podman (the default network mode for rootless is slirp4netns).&lt;/p&gt;
    &lt;code&gt;make image&lt;/code&gt;
    &lt;p&gt;This builds &lt;code&gt;yolobox/base:latest&lt;/code&gt; locally.&lt;/p&gt;
    &lt;p&gt;Because you want to tell your AI agent "just do it" without consequences. YOLO, but in a box.&lt;/p&gt;
    &lt;code&gt;make build          # Build binary
make test           # Run tests
make lint           # Run linters
make image          # Build Docker image
make install        # Install to ~/.local/bin&lt;/code&gt;
    &lt;p&gt;Version is derived automatically from git tags via &lt;code&gt;git describe&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tagged commit: &lt;code&gt;v0.1.1&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;After tag: &lt;code&gt;v0.1.1-3-gead833b&lt;/code&gt;(3 commits after tag)&lt;/item&gt;
      &lt;item&gt;Uncommitted changes: adds &lt;code&gt;-dirty&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No files to edit for releases. The Makefile handles it.&lt;/p&gt;
    &lt;p&gt;To release a new version:&lt;/p&gt;
    &lt;code&gt;git tag v0.1.2
git push origin master --tags&lt;/code&gt;
    &lt;p&gt;That's it. GitHub Actions will automatically:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build binaries for linux/darwin × amd64/arm64&lt;/item&gt;
      &lt;item&gt;Create a GitHub release with binaries and checksums&lt;/item&gt;
      &lt;item&gt;Build and push Docker image to &lt;code&gt;ghcr.io/finbarr/yolobox&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Version policy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Patch bump (&lt;code&gt;0.1.x&lt;/code&gt;): Bug fixes, security fixes&lt;/item&gt;
      &lt;item&gt;Minor bump (&lt;code&gt;0.x.0&lt;/code&gt;): New features&lt;/item&gt;
      &lt;item&gt;Major bump (&lt;code&gt;x.0.0&lt;/code&gt;): Breaking changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/finbarr/yolobox"/><published>2026-01-12T18:34:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46592376</id><title>Show HN: Fall asleep by watching JavaScript load</title><updated>2026-01-13T05:45:59.081090+00:00</updated><content>&lt;doc fingerprint="af7827771bac338b"&gt;
  &lt;main&gt;
    &lt;p&gt;This little toy project progressively tells a bedtime story about Liora and its little companion, while intentionally making a JavaScript loader spin for no reason other than to help you get to sleep.&lt;/p&gt;
    &lt;p&gt;As you go through the story, the time the loader spins increases while the speed at which the text appears decreases, so that you ideally never reach the end (unless you really, really want to).&lt;/p&gt;
    &lt;p&gt;I came up with this one night when I couldn’t fall asleep and was looking for some data analysis results on a web platform, noticing that the spinner was actually making me sleepy.&lt;/p&gt;
    &lt;p&gt;I am using it from time to time, and it helps a bit. So I decided to make it public. Here it is! You can try it directly at http://bedtime.my.&lt;/p&gt;
    &lt;p&gt;If you add it to your phone’s home screen, it should display fullscreen.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/sarusso/bedtime"/><published>2026-01-12T18:36:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46592461</id><title>Clearspace (YC W23) Is Hiring an Applied Researcher (ML)</title><updated>2026-01-13T05:45:58.395077+00:00</updated><content>&lt;doc fingerprint="14939967bc01edd1"&gt;
  &lt;main&gt;
    &lt;p&gt;Eliminate compulsive phone usage&lt;/p&gt;
    &lt;p&gt;About Clearspace&lt;/p&gt;
    &lt;p&gt;Clearspace is building the intentionality layer of the internet. Our mission is to build technology as effective at protecting human attention as social media is at exploiting it (infinite scrolling, short-form feeds, manipulative notifications, etc). Our category defining mobile app has been featured on Huberman Lab, New York Times Wirecutter, NPR Marketplace, Forbes, TBPN.&lt;/p&gt;
    &lt;p&gt;People that want a better relationship with their devices have nowhere to turn except for willpower. We are building an agent that achieves this on all devices by processing and filtering network traffic based on natural language rules.&lt;/p&gt;
    &lt;p&gt;About The Role&lt;/p&gt;
    &lt;p&gt;We are looking for an ML-focused engineer that will be responsible for training and improving a model for classifying network traffic. You are great for this role if you are not only excited about the latest in AI and ML but are also a problem-solver in the data domain. You don’t just think about the model but “how can we get more data volume”; “how can we featurize the data intelligently”; “what are our data needs based on our task and desired model size”, and like building backwards from inference requirements.&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Nice to Have&lt;/p&gt;
    &lt;p&gt;At Clearspace we help people reduce compulsive phone usage.&lt;/p&gt;
    &lt;p&gt;We exist to protect people's attention from the exploits of modern technology platforms and make space for the things that matter to them most.&lt;/p&gt;
    &lt;p&gt;We believe the technology to protect someones attention should be just as sophisticated and effective as the tech that is exploiting it and are building a world-class engineering team to arm the world with a comprehensive attention protection stack.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/clearspace/jobs/GOWiDwp-research-engineer-at-clearspace"/><published>2026-01-12T18:41:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46593022</id><title>Cowork: Claude Code for the rest of your work</title><updated>2026-01-13T05:45:58.101007+00:00</updated><content>&lt;doc fingerprint="772d8cfe3f1786ef"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;January 12, 2026&lt;/item&gt;
      &lt;item&gt;5min&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we released Claude Code, we expected developers to use it for coding. They didâand then quickly began using it for almost everything else. This prompted us to build Cowork: a simpler way for anyoneânot just developersâto work with Claude in the very same way. Cowork is available today as a research preview for Claude Max subscribers on our macOS app, and we will improve it rapidly from here.&lt;/p&gt;
    &lt;p&gt;How is using Cowork different from a regular conversation? In Cowork, you give Claude access to a folder of your choosing on your computer. Claude can then read, edit, or create files in that folder. It can, for example, re-organize your downloads by sorting and renaming each file, create a new spreadsheet with a list of expenses from a pile of screenshots, or produce a first draft of a report from your scattered notes.Â&lt;/p&gt;
    &lt;p&gt;In Cowork, Claude completes work like this with much more agency than youâd see in a regular conversation. Once youâve set it a task, Claude will make a plan and steadily complete it, while looping you in on what itâs up to. If youâve used Claude Code, this will feel familiarâCowork is built on the very same foundations. This means Cowork can take on many of the same tasks that Claude Code can handle, but in a more approachable form for non-coding tasks.Â Â&lt;/p&gt;
    &lt;p&gt;When youâve mastered the basics, you can make Cowork more powerful still. Claude can use your existing connectors, which link Claude to external information, and in Cowork weâve added an initial set of skills that improve Claudeâs ability to create documents, presentations, and other files. If you pair Cowork with Claude in Chrome, Claude can complete tasks that require browser access, too.&lt;/p&gt;
    &lt;p&gt;Cowork is designed to make using Claude for new work as simple as possible. You donât need to keep manually providing context or converting Claudeâs outputs into the right format. Nor do you have to wait for Claude to finish before offering further ideas or feedback: you can queue up tasks and let Claude work through them in parallel. It feels much less like a back-and-forth and much more like leaving messages for a coworker.Â&lt;/p&gt;
    &lt;head rend="h2"&gt;Stay in control&lt;/head&gt;
    &lt;p&gt;In Cowork, you can choose which folders and connectors Claude can see: Claude canât read or edit anything you donât give it explicit access to. Claude will also ask before taking any significant actions, so you can steer or course-correct it as you need.Â&lt;/p&gt;
    &lt;p&gt;That said, there are still things to be aware of before you give Claude control. By default, the main thing to know is that Claude can take potentially destructive actions (such as deleting local files) if itâs instructed to. Since thereâs always some chance that Claude might misinterpret your instructions, you should give Claude very clear guidance around things like this.Â&lt;/p&gt;
    &lt;p&gt;You should also be aware of the risk of âprompt injectionsâ: attempts by attackers to alter Claudeâs plans through content it might encounter on the internet. Weâve built sophisticated defenses against prompt injections, but agent safetyâthat is, the task of securing Claudeâs real-world actionsâis still an active area of development in the industry.Â&lt;/p&gt;
    &lt;p&gt;These risks arenât new with Cowork, but it might be the first time youâre using a more advanced tool that moves beyond a simple conversation. We recommend taking precautions, particularly while you learn how it works. We provide more detail in our Help Center.Â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;This is a research preview. Weâre releasing Cowork early because we want to learn what people use it for, and how they think it could be better. We encourage you to experiment with what Cowork can do for you, and to try things you donât expect to work: you might be surprised! As we learn more from this preview, we plan to make lots of improvements (including by adding cross-device sync and bringing it to Windows), and weâll identify further ways to make it safer.Â&lt;/p&gt;
    &lt;p&gt;Claude Max subscribers can try Cowork now by downloading the macOS app, then clicking on âCoworkâ in the sidebar. If you're on another plan, you can join the waitlist for future access.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transform how your organization operates with Claude&lt;/head&gt;
    &lt;p&gt;Get the developer newsletter&lt;/p&gt;
    &lt;p&gt;Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://claude.com/blog/cowork-research-preview"/><published>2026-01-12T19:27:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46593395</id><title>The chess bot on Delta Air Lines will destroy you (2024) [video]</title><updated>2026-01-13T05:45:57.295624+00:00</updated><content>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=c0mLhHDcY3I"/><published>2026-01-12T19:57:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46593802</id><title>Fabrice Bellard's TS Zip (2024)</title><updated>2026-01-13T05:45:56.317358+00:00</updated><content>&lt;doc fingerprint="a5077a8aa4100a3b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;ts_zip: Text Compression using Large Language Models&lt;/head&gt; The &lt;code&gt;ts_zip&lt;/code&gt; utility can
compress (and hopefully decompress) text files using a Large Language
Model. The compression ratio is much higher than with other
compression tools. There are some caveats of course:
&lt;list rend="ul"&gt;&lt;item&gt;A GPU is necessary to get a reasonable speed. 4 GB of RAM is required. &lt;/item&gt;&lt;item&gt;It is slower than conventional compressors (compression and decompression speed: up to 1 MB/s on a RTX 4090).&lt;/item&gt;&lt;item&gt;Only text files are supported. Binary files won't be compressed much. The currently used language model (RWKV 169M v4) was trained mostly on English texts. Other languages are supported including source code.&lt;/item&gt;&lt;item&gt;It is experimental so no backward compability should be expected between the various versions.&lt;/item&gt;&lt;item&gt;See also ts_sms which is optimized for the compression of small messages.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Compression Ratio&lt;/head&gt;&lt;p&gt; The compression ratio is given in bits per byte (bpb). &lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell aria-sort="ascending" style="width:15em;" role="head"&gt;File &lt;/cell&gt;&lt;cell class="num" style="width:8em;" role="head"&gt;Original size&lt;lb/&gt;(bytes) &lt;/cell&gt;&lt;cell class="num" style="width:12em;" colspan="2" role="head"&gt;xz&lt;lb/&gt;(bytes) (bpb) &lt;/cell&gt;&lt;cell class="num" style="width:12em;" colspan="2" role="head"&gt;ts_zip&lt;lb/&gt;(bytes) (bpb) &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;alice29.txt &lt;/cell&gt;&lt;cell class="num"&gt;152089 &lt;/cell&gt;&lt;cell class="num"&gt;48492 &lt;/cell&gt;&lt;cell class="num"&gt;2.551 &lt;/cell&gt;&lt;cell class="num"&gt;21713 &lt;/cell&gt;&lt;cell class="num"&gt;1.142 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;book1 &lt;/cell&gt;&lt;cell class="num"&gt;768771 &lt;/cell&gt;&lt;cell class="num"&gt;261116 &lt;/cell&gt;&lt;cell class="num"&gt;2.717 &lt;/cell&gt;&lt;cell class="num"&gt;137477 &lt;/cell&gt;&lt;cell class="num"&gt;1.431 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;enwik8 &lt;/cell&gt;&lt;cell class="num"&gt;100000000 &lt;/cell&gt;&lt;cell class="num"&gt;24865244 &lt;/cell&gt;&lt;cell class="num"&gt;1.989 &lt;/cell&gt;&lt;cell class="num"&gt;13825741 &lt;/cell&gt;&lt;cell class="num"&gt;1.106 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;enwik9 &lt;/cell&gt;&lt;cell class="num"&gt;1000000000 &lt;/cell&gt;&lt;cell class="num"&gt;213370900 &lt;/cell&gt;&lt;cell class="num"&gt;1.707 &lt;/cell&gt;&lt;cell class="num"&gt;135443237 &lt;/cell&gt;&lt;cell class="num"&gt;1.084 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;linux-1.2.13.tar &lt;/cell&gt;&lt;cell class="num"&gt;9379840 &lt;/cell&gt;&lt;cell class="num"&gt;1689468 &lt;/cell&gt;&lt;cell class="num"&gt;1.441 &lt;/cell&gt;&lt;cell class="num"&gt;1196859 &lt;/cell&gt;&lt;cell class="num"&gt;1.021 &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Results and speed for other programs on enwik8 and enwik9 are available at the Large Text Compression Benchmark. &lt;/p&gt;&lt;head rend="h2"&gt;Download&lt;/head&gt;&lt;head rend="h2"&gt;Technical information&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;ts_zip&lt;/code&gt; uses
  the RWKV 169M v4
  language model which is a good compromise between speed and
  compression ratio. The model is quantized to 8 bits per parameter
    and evaluated using BF16 floating point numbers.&lt;/item&gt;&lt;item&gt;The language model predicts the probabilities of the next token. An arithmetic coder then encodes the next token according to the probabilities.&lt;/item&gt;&lt;item&gt;The model is evaluated in a deterministic and reproducible way. Hence the result does not depend on the exact GPU or CPU model nor on the number of configured threads. This key point ensures that a compressed file can be decompressed using a different hardware or software configuration.&lt;/item&gt;&lt;/list&gt;&lt;lb/&gt;Fabrice Bellard - https://bellard.org/&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bellard.org/ts_zip/"/><published>2026-01-12T20:26:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46595130</id><title>F2 (YC S25) Is Hiring</title><updated>2026-01-13T05:45:55.521979+00:00</updated><content>&lt;doc fingerprint="777b7af98957236a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Role Overview&lt;/head&gt;
        &lt;p&gt;We’re hiring a Product Designer to craft intuitive, elegant, and impactful user experiences across F2’s platform. You’ll partner with Product, Engineering, and Customer teams to define workflows, refine interaction patterns, and elevate the design language for a complex B2B AI product. As an early designer at F2, you’ll influence the product’s vision and have a direct impact on how investment professionals interact with cutting-edge AI capabilities.&lt;/p&gt;
        &lt;head rend="h3"&gt;What You’ll Do&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Lead end-to-end design across new and existing features — from user research and ideation through prototyping, interaction design, and final UI.&lt;/item&gt;
          &lt;item&gt;Collaborate closely with Product and Engineering to translate product strategy into intuitive experiences.&lt;/item&gt;
          &lt;item&gt;Define and maintain design systems and visual standards to ensure consistency and scalability.&lt;/item&gt;
          &lt;item&gt;Balance business goals with user needs, prioritizing work that drives adoption, engagement, and product delight.&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;You Might Be a Great Fit If&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;You have 2+ years of experience in product design, UX/UI, or related roles, ideally with SaaS products and/or AI-driven tools.&lt;/item&gt;
          &lt;item&gt;You have a strong portfolio that demonstrates design thinking, problem solving, and polished visual execution across complex workflows.&lt;/item&gt;
          &lt;item&gt;You’re comfortable leading design efforts in ambiguity, iterating quickly, and shipping with high quality.&lt;/item&gt;
          &lt;item&gt;You excel at cross-functional collaboration and communicate clearly with product, engineering, and business stakeholders.&lt;/item&gt;
          &lt;item&gt;You’re curious about how AI reshapes user interaction and excited to design experiences where human judgment and machine intelligence converge.&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;Preferred Qualifications&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Experience designing for data-intensive enterprise products or B2B platforms.&lt;/item&gt;
          &lt;item&gt;Background in UX research, usability testing, and incorporating user feedback into measurable product improvements.&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;Why F2?&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Join a fast-growing AI startup reshaping workflows in private markets and finance.&lt;/item&gt;
          &lt;item&gt;Be part of a small, high-impact team where your design decisions influence product strategy.&lt;/item&gt;
          &lt;item&gt;Competitive compensation with equity upside and benefits.&lt;/item&gt;
          &lt;item&gt;Collaborative, mission-driven culture that values craftsmanship, curiosity, and rapid iteration.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;p&gt;F2 is the AI platform for private markets investors. Purpose-built for private credit, private equity, and commercial banks, F2 helps teams evaluate deals over 70% faster. Already trusted across thousands of deals, F2 makes collaboration easy and retains historical context, so past work compounds into future wins.&lt;/p&gt;
      &lt;p&gt;F2 accelerates private market investors’ workflows, streamlining the screening, underwriting and portfolio management processes. It connects data from financial models, data rooms, and 3rd party data to automate complex analysis, surface insights faster, and generate investor-grade, IC ready materials in minutes.&lt;/p&gt;
      &lt;p&gt;Headquartered in New York City, F2 is backed by leading investors including NFX, Left Lane Capital, and Y Combinator.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/f2/jobs/cJsc7Fe-product-designer"/><published>2026-01-12T22:30:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46595393</id><title>A deep dive on agent sandboxes</title><updated>2026-01-13T05:45:55.221328+00:00</updated><content>&lt;doc fingerprint="34f9da11e0243ef0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A deep dive on agent sandboxes&lt;/head&gt;
    &lt;p&gt;The modern generation of coding agents are powerful in part because of their access permissions. They have a few built-in functions for the most obvious operations (writing files, editing strings) but other than that they mostly generate what they need adhoc. Their &lt;code&gt;bash&lt;/code&gt; tool is by far their most powerful because it's the most expressive. You can run some adhoc python to test out some logic, run a compilation pass, or curl a website. It's turning complete for gosh sake.&lt;/p&gt;
    &lt;p&gt;It's also by far the most dangerous. You probably wouldn't give your new intern access to the prod credentials. But an arbitrary bash session could certainly provide that permissions escalation; if not fully delete your home directory from disk.&lt;/p&gt;
    &lt;p&gt;The safest way to run any coding agent is within virtualization. Boot up a container with a limited scope, a git branched workspace of what you're working on, and all the tools it will need to get the job done. I'm willing to wadger a large sum that almost no one does that. &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt; is well named but most people just shrug and let it run.&lt;/p&gt;
    &lt;p&gt;One solution to balance permissions and danger are the command whitelist. You see this in Claude Code and Cursor when run in the normal mode. Every command execution asks you, a real human being, if you feel comfortable with the agent doing that same command in the future. A &lt;code&gt;swift build&lt;/code&gt;? Perfectly safe. An &lt;code&gt;rm -rf&lt;/code&gt;? Nah let's skip that one for now. But command whitelists are brittle and a bit annoying. If you need to run the command in a new directory, sometimes the &lt;code&gt;cd&lt;/code&gt; command will block because it hasn't been approved yet. It's fully impractical to run if you step away from your computer and you aren't available in the loop for the approval.&lt;/p&gt;
    &lt;p&gt;But the tides are a changing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Codex Permissions&lt;/head&gt;
    &lt;p&gt;I've been trying out the Codex CLI with &lt;code&gt;gpt-5-codex high&lt;/code&gt; for the last couple of weeks. So far I like it1. I was specifically having it iterate on a swift project to resolve some compilation issues by continuously running &lt;code&gt;swift build&lt;/code&gt; before it yielded back. The command was running but it kept failing with a clang permissions error. Clang was installed and working fine when I ran it manually so I wasn't immediately sure what codepath or environment variable was causing the issue.&lt;/p&gt;
    &lt;p&gt;Turns out Codex default launches in the mode where it only has access to the current folder. Running &lt;code&gt;/approvals&lt;/code&gt; shows you what mode you're actually in. It supports three different ones:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read Only - Codex can read files and answer questions. Requires approval for edits, commands, or network access&lt;/item&gt;
      &lt;item&gt;Auto (current) - Can read files, make edits, and run commands in the workspace. Requires approval for workspace-external or network access&lt;/item&gt;
      &lt;item&gt;Full Access - Can read files, make edits, and run commands with network access, without approval&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Read Only and Full Access are pretty obvious implementations. Read Only basically only allows access to &lt;code&gt;grep&lt;/code&gt; and &lt;code&gt;cat&lt;/code&gt;. And Full Access throws caution to the wind and just gives your Agent full access to the terminal.&lt;/p&gt;
    &lt;p&gt;Auto is the most interesting one. Limiting network access and workspace external access (when when running within a script itself) smelled like it was doing something more interesting, perhaps even containerized. But it's certainly not using Docker - so what's going on here?&lt;/p&gt;
    &lt;p&gt;Since everything's open sourced you can take a look yourself. When running on macOS it uses the native sandboxing APIs to enforce these restrictions2. The swift toolchain lives outside the project directory, so of course it couldn't access it. I spent some time digging into how this actually works.&lt;/p&gt;
    &lt;head rend="h2"&gt;The execution pipeline&lt;/head&gt;
    &lt;p&gt;Every tool call flows through a centralized execution system that decides whether to run commands raw, under macOS Seatbelt, or through a Linux seccomp helper. The CLI bootstraps this by wrapping the main entry point in &lt;code&gt;arg0_dispatch_or_else&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// 160:163:codex-rs/cli/src/main.rs
fn main() -&amp;gt; anyhow::Result&amp;lt;()&amp;gt; {
    arg0_dispatch_or_else(|codex_linux_sandbox_exe| async move {
        cli_main(codex_linux_sandbox_exe).await?;
        Ok(())
    })
}
&lt;/code&gt;
    &lt;p&gt;This captures the executable path and passes it to the async runtime, so downstream components can spawn the Linux sandbox helper when they need it. The dispatcher checks if the binary was invoked via the &lt;code&gt;codex-linux-sandbox&lt;/code&gt; alias:&lt;/p&gt;
    &lt;code&gt;// 48:49:codex-rs/arg0/src/lib.rs
if exe_name == LINUX_SANDBOX_ARG0 {
    // Safety: [`run_main`] never returns.
&lt;/code&gt;
    &lt;p&gt;If it was, it jumps straight into sandbox execution. Otherwise it loads environment variables, patches PATH, and calls the main CLI logic with the sandbox executable path available.&lt;/p&gt;
    &lt;p&gt;This architecture also makes sandboxing effectively. opt-out, not opt-in. Every command execution goes through &lt;code&gt;process_exec_tool_call&lt;/code&gt;, which routes based on &lt;code&gt;SandboxType&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// 92:99:codex-rs/core/src/exec.rs
let raw_output_result: std::result::Result&amp;lt;RawExecToolCallOutput, CodexErr&amp;gt; = match sandbox_type
{
    SandboxType::None =&amp;gt; exec(params, sandbox_policy, stdout_stream.clone()).await,
    SandboxType::MacosSeatbelt =&amp;gt; {
        let ExecParams {
            command, cwd, env, ..
        } = params;
        let child = spawn_command_under_seatbelt(
&lt;/code&gt;
    &lt;p&gt;By default, you're sandboxed.&lt;/p&gt;
    &lt;head rend="h2"&gt;An aside on OS sandboxing&lt;/head&gt;
    &lt;p&gt;There are a bunch of different ways to isolate processes on modern operating systems. Linux gives you the most options, while macOS keeps things simpler (but more limited).&lt;/p&gt;
    &lt;p&gt;Linux combines several kernel features that have been available for awhile now:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Landlock (Linux 5.13+): Capability-based filesystem access control that's actually pretty straightforward to use&lt;/item&gt;
      &lt;item&gt;Seccomp-BPF: System call filtering with programmable filters&lt;/item&gt;
      &lt;item&gt;Namespaces: Process, network, mount, and user isolation&lt;/item&gt;
      &lt;item&gt;Cgroups: Resource limiting and accounting&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS has two main approaches, both with pretty significant limitations for agents:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;App Sandbox: Apple's preferred approach, designed for App Store apps with static entitlements&lt;/item&gt;
      &lt;item&gt;Seatbelt: The older framework that App Sandbox is built on top of&lt;/item&gt;
      &lt;item&gt;Virtualization.framework: Actually implementing virtualization/container technologies yourself. This is what Docker uses as its default backend these days.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Despite Apple marking Seatbelt as deprecated, it's still pretty used across the ecosystem. This thread on HN was a good read. Some choice insights:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;the sandbox subsystem is what all of Apple's system software uses for sandboxing, as well as many security-conscious third-party programs such as web browsers. It's not going anywhere anytime soon.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Some other issues. I'm quoting here since I haven't spent a whole lot of time poking around the edges of Seatbelt myself:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Package management restrictions: Agents need to install dependencies on the fly. "If you use a native Mac sandbox then it'd need to ask permission to use homebrew. Maybe you can find a way to run homebrew inside a Mac sandbox but it won't be straightforward."&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Policy complexity leads to security holes: Custom Seatbelt policies are easy to mess up: "The bug I reported (in an AI agent sandbox using Seatbelt) was that it failed to properly block off access to the user's home directory dotfiles and ~/Library. This sort of mistake is easy to make with Seatbelt but harder to make with containers."&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Limited network control: Neither App Sandbox nor Seatbelt gives you the granular network controls you'd want. You can't easily restrict access to specific domains or protocols, or intercept traffic to black some bad actors. It's an all or nothing approach.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OS-level sandboxes work at the syscall level. They understand files, network sockets, and processes, but not higher-level concepts like "only allow HTTPS to api.openai.com" or "permit git operations but block arbitrary network access." So you're stuck making pretty coarse-grained decisions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Platform-specific implementations&lt;/head&gt;
    &lt;p&gt;Now back to Codex. The &lt;code&gt;SandboxPolicy&lt;/code&gt; struct drives both platform implementations. It specifies writable roots, network access, and other constraints that get translated into platform-specific restrictions.&lt;/p&gt;
    &lt;head rend="h3"&gt;macOS Seatbelt&lt;/head&gt;
    &lt;p&gt;On macOS, they're using Apple's Seatbelt framework. The implementation hardcodes &lt;code&gt;/usr/bin/sandbox-exec&lt;/code&gt; and injects environment variables to identify sandboxed execution:&lt;/p&gt;
    &lt;code&gt;// 19:38:codex-rs/core/src/seatbelt.rs
pub async fn spawn_command_under_seatbelt(
    command: Vec&amp;lt;String&amp;gt;,
    sandbox_policy: &amp;amp;SandboxPolicy,
    cwd: PathBuf,
    stdio_policy: StdioPolicy,
    mut env: HashMap&amp;lt;String, String&amp;gt;,
) -&amp;gt; std::io::Result&amp;lt;Child&amp;gt; {
    let args = create_seatbelt_command_args(command, sandbox_policy, &amp;amp;cwd);
    let arg0 = None;
    env.insert(CODEX_SANDBOX_ENV_VAR.to_string(), "seatbelt".to_string());
    spawn_child_async(
        PathBuf::from(MACOS_PATH_TO_SEATBELT_EXECUTABLE),
        args,
        arg0,
        cwd,
        sandbox_policy,
        stdio_policy,
        env,
    )
    .await
}
&lt;/code&gt;
    &lt;p&gt;They enumerate writable roots from the policy configuration but carve out &lt;code&gt;.git&lt;/code&gt; directories as read-only:&lt;/p&gt;
    &lt;code&gt;// 59:73:codex-rs/core/src/seatbelt.rs
for (index, wr) in writable_roots.iter().enumerate() {
    // Canonicalize to avoid mismatches like /var vs /private/var on macOS.
    let canonical_root = wr.root.canonicalize().unwrap_or_else(|_| wr.root.clone());
    let root_param = format!("WRITABLE_ROOT_{index}");
    cli_args.push(format!(
        "-D{root_param}={}",
        canonical_root.to_string_lossy()
    ));

    if wr.read_only_subpaths.is_empty() {
        writable_folder_policies.push(format!("(subpath (param \"{root_param}\"))"));
    } else {
        // Add parameters for each read-only subpath and generate
        // the `(require-not ...)` clauses.
        let mut require_parts: Vec&amp;lt;String&amp;gt; = Vec::new();
&lt;/code&gt;
    &lt;p&gt;Pretty clever. Agents can modify your workspace but can't mess up your git history.&lt;/p&gt;
    &lt;p&gt;Network access is straightforward - either you have it or you don't:&lt;/p&gt;
    &lt;code&gt;// 106:110:codex-rs/core/src/seatbelt.rs
let network_policy = if sandbox_policy.has_full_network_access() {
    "(allow network-outbound)\n(allow network-inbound)\n(allow system-socket)"
} else {
    ""
};
&lt;/code&gt;
    &lt;p&gt;When network is disabled, they just omit the network permissions. Seatbelt denies by default, so that's all you need.&lt;/p&gt;
    &lt;head rend="h3"&gt;Linux Landlock + seccomp&lt;/head&gt;
    &lt;p&gt;The Linux implementation is a bit more complex - they're using Landlock for filesystem restrictions and seccomp for system call filtering.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;codex-linux-sandbox&lt;/code&gt; helper is a separate binary that gets spawned as a subprocess. It parses the serialized policy and target command, applies restrictions, then exec's the actual command:&lt;/p&gt;
    &lt;code&gt;// 20:35:codex-rs/linux-sandbox/src/linux_run_main.rs
pub fn run_main() -&amp;gt; ! {
    let LandlockCommand {
        sandbox_policy_cwd,
        sandbox_policy,
        command,
    } = LandlockCommand::parse();

    if let Err(e) = apply_sandbox_policy_to_current_thread(&amp;amp;sandbox_policy, &amp;amp;sandbox_policy_cwd) {
        panic!("error running landlock: {e:?}");
    }

    if command.is_empty() {
        panic!("No command specified to execute.");
    }

    #[expect(clippy::expect_used)]
    let c_command =
&lt;/code&gt;
    &lt;p&gt;This keeps the sandbox restrictions isolated to just the child process.&lt;/p&gt;
    &lt;p&gt;They apply both filesystem and network restrictions before the exec:&lt;/p&gt;
    &lt;code&gt;// 30:50:codex-rs/linux-sandbox/src/landlock.rs
pub(crate) fn apply_sandbox_policy_to_current_thread(
    sandbox_policy: &amp;amp;SandboxPolicy,
    cwd: &amp;amp;Path,
) -&amp;gt; Result&amp;lt;()&amp;gt; {
    if !sandbox_policy.has_full_network_access() {
        install_network_seccomp_filter_on_current_thread()?;
    }

    if !sandbox_policy.has_full_disk_write_access() {
        let writable_roots = sandbox_policy
            .get_writable_roots_with_cwd(cwd)
            .into_iter()
            .map(|writable_root| writable_root.root)
            .collect();
        install_filesystem_landlock_rules_on_current_thread(writable_roots)?;
    }

    // TODO(ragona): Add appropriate restrictions if
    // `sandbox_policy.has_full_disk_read_access()` is `false`.

    Ok(())
}
&lt;/code&gt;
    &lt;p&gt;Landlock grants read access everywhere but restricts writes to whitelisted roots (plus &lt;code&gt;/dev/null&lt;/code&gt;). The key is applying these rules before &lt;code&gt;execvp&lt;/code&gt; - once the target command starts, it's already sandboxed.&lt;/p&gt;
    &lt;p&gt;For network isolation, they use seccomp to block outbound network sockets (except AF_UNIX for local IPC):&lt;/p&gt;
    &lt;code&gt;// 87:109:codex-rs/linux-sandbox/src/landlock.rs
fn install_network_seccomp_filter_on_current_thread() -&amp;gt; std::result::Result&amp;lt;(), SandboxErr&amp;gt; {
    // Build rule map.
    let mut rules: BTreeMap&amp;lt;i64, Vec&amp;lt;SeccompRule&amp;gt;&amp;gt; = BTreeMap::new();

    // Helper â insert unconditional deny rule for syscall number.
    let mut deny_syscall = |nr: i64| {
        rules.insert(nr, vec![]); // empty rule vec = unconditional match
    };

    deny_syscall(libc::SYS_connect);
    deny_syscall(libc::SYS_accept);
    deny_syscall(libc::SYS_accept4);
    deny_syscall(libc::SYS_bind);
    deny_syscall(libc::SYS_listen);
    deny_syscall(libc::SYS_getpeername);
    deny_syscall(libc::SYS_getsockname);
    deny_syscall(libc::SYS_shutdown);
    deny_syscall(libc::SYS_sendto);
    deny_syscall(libc::SYS_sendmsg);
    deny_syscall(libc::SYS_sendmmsg);
    // NOTE: allowing recvfrom allows some tools like: `cargo clippy` to run
    // with their socketpair + child processes for sub-proc management
&lt;/code&gt;
    &lt;p&gt;This is more granular than what you get with Seatbelt on macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Child process management&lt;/head&gt;
    &lt;p&gt;All sandboxed processes go through &lt;code&gt;spawn_child_async&lt;/code&gt;, which handles environment cleanup and stdio management. They completely clear the environment and rebuild it with only the variables you actually want:&lt;/p&gt;
    &lt;code&gt;// 38:61:codex-rs/core/src/spawn.rs
pub(crate) async fn spawn_child_async(
    program: PathBuf,
    args: Vec&amp;lt;String&amp;gt;,
    #[cfg_attr(not(unix), allow(unused_variables))] arg0: Option&amp;lt;&amp;amp;str&amp;gt;,
    cwd: PathBuf,
    sandbox_policy: &amp;amp;SandboxPolicy,
    stdio_policy: StdioPolicy,
    env: HashMap&amp;lt;String, String&amp;gt;,
) -&amp;gt; std::io::Result&amp;lt;Child&amp;gt; {
    trace!(
        "spawn_child_async: {program:?} {args:?} {arg0:?} {cwd:?} {sandbox_policy:?} {stdio_policy:?} {env:?}"
    );

    let mut cmd = Command::new(&amp;amp;program);
    #[cfg(unix)]
    cmd.arg0(arg0.map_or_else(|| program.to_string_lossy().to_string(), String::from));
    cmd.args(args);
    cmd.current_dir(cwd);
    cmd.env_clear();
    cmd.envs(env);

    if !sandbox_policy.has_full_network_access() {
        cmd.env(CODEX_SANDBOX_NETWORK_DISABLED_ENV_VAR, "1");
    }
&lt;/code&gt;
    &lt;p&gt;This prevents leaking sensitive environment variables. Network-disabled runs get tagged with &lt;code&gt;CODEX_SANDBOX_NETWORK_DISABLED=1&lt;/code&gt; so downstream tools know what's happening. Stdio can be either piped (for capturing output) or inherited (for interactive commands).&lt;/p&gt;
    &lt;p&gt;On Linux, they register a parent-death signal handler using &lt;code&gt;prctl(PR_SET_PDEATHSIG)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// 68:69:codex-rs/core/src/spawn.rs
#[cfg(target_os = "linux")]
unsafe {
&lt;/code&gt;
    &lt;p&gt;This ensures sandboxed children die if the main process gets killed - you don't want orphaned processes running around.&lt;/p&gt;
    &lt;head rend="h2"&gt;Command whitelisting&lt;/head&gt;
    &lt;p&gt;Before executing any command, they run it through &lt;code&gt;assess_command_safety&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// 81:99:codex-rs/core/src/safety.rs
pub fn assess_command_safety(
    command: &amp;amp;[String],
    approval_policy: AskForApproval,
    sandbox_policy: &amp;amp;SandboxPolicy,
    approved: &amp;amp;HashSet&amp;lt;Vec&amp;lt;String&amp;gt;&amp;gt;,
    with_escalated_permissions: bool,
) -&amp;gt; SafetyCheck {
    // A command is "trusted" because either:
    // - it belongs to a set of commands we consider "safe" by default, or
    // - the user has explicitly approved the command for this session
    //
    // Currently, whether a command is "trusted" is a simple boolean, but we
    // should include more metadata on this command test to indicate whether it
    // should be run inside a sandbox or not. (This could be something the user
    // defines as part of `execpolicy`.)
    //
    // For example, when `is_known_safe_command(command)` returns `true`, it
    // would probably be fine to run the command in a sandbox, but when
    // `approved.contains(command)` is `true`, the user may have approved it for
&lt;/code&gt;
    &lt;p&gt;This uses trust lists and approval policies to categorize commands: safe to auto-run, needs user approval, or needs to run unsandboxed.&lt;/p&gt;
    &lt;p&gt;The trust list is session-scoped - once you approve a command, it's trusted for the rest of that session.&lt;/p&gt;
    &lt;p&gt;If a sandboxed command fails due to permissions, the system asks if you want to retry unsandboxed. If you approve, it marks the command as trusted and re-executes with &lt;code&gt;SandboxType::None&lt;/code&gt;. Pretty reasonable escape hatch for when you actually need broader access.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging support&lt;/head&gt;
    &lt;p&gt;While I was poking around the code I also noticed that they expose some debuggers to help test sandboxing. &lt;code&gt;codex debug seatbelt&lt;/code&gt; and &lt;code&gt;codex debug landlock&lt;/code&gt; let you test arbitrary commands through the sandbox. These honor the same &lt;code&gt;--full-auto&lt;/code&gt; flags as the main CLI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This implementation is quite nice considering the main goal of Codex is improving agents and not containerization.3 OS-native enforcement lets them avoid a lot of overhead while still getting pretty good isolation.&lt;/p&gt;
    &lt;p&gt;The key design choices that stood out to me:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Platform-specific implementations unified behind a common policy abstraction&lt;/item&gt;
      &lt;item&gt;Default-sandbox execution with selective escalation when needed&lt;/item&gt;
      &lt;item&gt;Session-based trust lists that reduce approval fatigue&lt;/item&gt;
      &lt;item&gt;Debug tooling that actually helps you understand what's happening&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As more people start using agents to write and execute code, this kind of sandboxing is going to be pretty important. The alternative, really asking pretty please to avoid calling rm -rf your home directory, doesn't seem like a great long-term plan.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It's a SOTA architecture. All these models are basically in the same ballpark. â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Linux, flexible in all things, lets you do this customization with even lower level permissioning. Half the reason why Docker thrives on Linux. â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;But I suppose it's an acknowledgement that one is growing more tightly bound to the other. â©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pierce.dev/notes/a-deep-dive-on-agent-sandboxes"/><published>2026-01-12T23:02:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46595419</id><title>Google removes AI health summaries after investigation finds dangerous flaws</title><updated>2026-01-13T05:45:54.911948+00:00</updated><content>&lt;doc fingerprint="e02b01a36d005e2"&gt;
  &lt;main&gt;
    &lt;p&gt;On Sunday, Google removed some of its AI Overviews health summaries after a Guardian investigation found people were being put at risk by false and misleading information. The removals came after the newspaper found that Google’s generative AI feature delivered inaccurate health information at the top of search results, potentially leading seriously ill patients to mistakenly conclude they are in good health.&lt;/p&gt;
    &lt;p&gt;Google disabled specific queries, such as “what is the normal range for liver blood tests,” after experts contacted by The Guardian flagged the results as dangerous. The report also highlighted a critical error regarding pancreatic cancer: The AI suggested patients avoid high-fat foods, a recommendation that contradicts standard medical guidance to maintain weight and could jeopardize patient health. Despite these findings, Google only deactivated the summaries for the liver test queries, leaving other potentially harmful answers accessible.&lt;/p&gt;
    &lt;p&gt;The investigation revealed that searching for liver test norms generated raw data tables (listing specific enzymes like ALT, AST, and alkaline phosphatase) that lacked essential context. The AI feature also failed to adjust these figures for patient demographics such as age, sex, and ethnicity. Experts warned that because the AI model’s definition of “normal” often differed from actual medical standards, patients with serious liver conditions might mistakenly believe they are healthy and skip necessary follow-up care.&lt;/p&gt;
    &lt;p&gt;Vanessa Hebditch, director of communications and policy at the British Liver Trust, told The Guardian that a liver function test is a collection of different blood tests and that understanding the results “is complex and involves a lot more than comparing a set of numbers.” She added that the AI Overviews fail to warn that someone can get normal results for these tests when they have serious liver disease and need further medical care. “This false reassurance could be very harmful,” she said.&lt;/p&gt;
    &lt;p&gt;Google declined to comment on the specific removals to The Guardian. A company spokesperson told The Verge that Google invests in the quality of AI Overviews, particularly for health topics, and that “the vast majority provide accurate information.” The spokesperson added that the company’s internal team of clinicians reviewed what was shared and “found that in many instances, the information was not inaccurate and was also supported by high-quality websites.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/ai/2026/01/google-removes-some-ai-health-summaries-after-investigation-finds-dangerous-flaws/"/><published>2026-01-12T23:05:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46597023</id><title>Provenance Is the New Version Control</title><updated>2026-01-13T05:45:54.273180+00:00</updated><content>&lt;doc fingerprint="a7b6086aac1c8dd1"&gt;
  &lt;main&gt;
    &lt;p&gt;When code can be thrown away and recreated, the unit of change is no longer lines of code. It’s reasons. Version control has to follow.&lt;/p&gt;
    &lt;p&gt;Regenerable systems quietly invalidate an assumption that has underpinned software engineering for decades: that the text of the code is the best record of how and why a system came to be. Once an AI can reliably regenerate an implementation from specification, the code itself becomes an artifact of synthesis, not the locus of intent.&lt;/p&gt;
    &lt;p&gt;By regenerable, I mean: if you delete a component, you can recreate it from stored intent (requirements, constraints, and decisions) with the same behavior and integration guarantees.&lt;/p&gt;
    &lt;p&gt;In that world, version control doesn’t disappear, but it has to move upstream.&lt;/p&gt;
    &lt;head rend="h3"&gt;When Diffs Stop Representing Decisions&lt;/head&gt;
    &lt;p&gt;Traditional version control works because code edits are a reasonable proxy for human decisions. Someone typed this conditional. Someone refactored that loop. A diff is an imperfect but serviceable record of authorship.&lt;/p&gt;
    &lt;p&gt;AI-assisted generation severs that link.&lt;/p&gt;
    &lt;p&gt;When an agent reads a specification, reasons about constraints, chooses an approach, and emits code, the resulting text reflects outcomes, not decisions. A diff can show what changed in the artifact, but it cannot explain which requirement demanded the change, which constraint shaped it, or which tradeoff caused one structure to be chosen over another.&lt;/p&gt;
    &lt;p&gt;This is the sense in which code-first version control becomes a lossy history. Not because diffs are useless (they still matter operationally) but because they no longer represent the causal history of the system. They tell you what happened, not why it happened.&lt;/p&gt;
    &lt;p&gt;That distinction matters once code is no longer directly authored.&lt;/p&gt;
    &lt;head rend="h3"&gt;Specifications as Executable Intent&lt;/head&gt;
    &lt;p&gt;In a regenerable system, specifications are no longer descriptive documents. They are executable inputs.&lt;/p&gt;
    &lt;p&gt;If a component can be deleted and recreated at will, then whatever information is required to recreate it is, by definition, the source of truth. Specifications stop being explanatory prose and become causal inputs.&lt;/p&gt;
    &lt;p&gt;The same is true of an agent’s plan.&lt;/p&gt;
    &lt;p&gt;The plan that matters isn’t free-form thinking. It’s the decision record: chosen strategy, rejected alternatives, and the constraints that forced the choice. Even when the choice is wrong, it’s still the most useful artifact to preserve: it explains why the system looks like this. Treating this as throwaway reasoning discards information that is often more important than the final text.&lt;/p&gt;
    &lt;p&gt;The plan is not documentation. It is part of the implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Concrete Example: Email Validation&lt;/head&gt;
    &lt;p&gt;Consider a small component: a function that validates email addresses.&lt;/p&gt;
    &lt;p&gt;A specification might state:&lt;/p&gt;
    &lt;code&gt;The system must accept standard email addresses of the form  local@domain.&lt;/code&gt;
    &lt;code&gt;It must reject inputs without exactly one  @.&lt;/code&gt;
    &lt;quote&gt;It must not attempt full RFC compliance.&lt;/quote&gt;
    &lt;p&gt;An agent produces a plan:&lt;/p&gt;
    &lt;quote&gt;Use a simple regular expression.&lt;/quote&gt;
    &lt;quote&gt;Do not rely on external libraries.&lt;/quote&gt;
    &lt;quote&gt;Explicitly reject whitespace.&lt;/quote&gt;
    &lt;quote&gt;Favor readability over completeness.&lt;/quote&gt;
    &lt;p&gt;From this, code is generated.&lt;/p&gt;
    &lt;p&gt;Now the requirement changes:&lt;/p&gt;
    &lt;quote&gt;The system must accept internationalized domain names (IDN) in the domain portion.&lt;/quote&gt;
    &lt;p&gt;Nothing else changes.&lt;/p&gt;
    &lt;p&gt;In a code-centric workflow, you inspect the diff and infer intent after the fact. In an intent-centric workflow, a single requirement node changes, the dependent plan node(s) changes, and the generated code changes as a consequence. The unit of change is not “these lines,” but “this reason.”&lt;/p&gt;
    &lt;p&gt;You can now answer not just what changed, but why it had to.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Files to Intent Graphs&lt;/head&gt;
    &lt;p&gt;To support this, intent cannot live in a loose collection of documents. It needs structure.&lt;/p&gt;
    &lt;p&gt;The representation that works is a content-addressed graph. Individual requirements, constraints, plans, decisions, and environmental factors become nodes. Each node has a stable representation and a hash derived from its content. Edges express causality: this plan depends on that requirement; this decision exists because of that constraint.&lt;/p&gt;
    &lt;p&gt;In practice, each node needs at least: a type, canonical content, explicit dependencies, and evaluation artifacts (tests, constraints, budgets) that make regeneration checkable.&lt;/p&gt;
    &lt;p&gt;Even in the small example above, the graph is explicit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A requirement node: “accept standard email addresses”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A constraint node: “no RFC compliance”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A plan node: “use a regex, reject whitespace”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A generator node: “Claude-class model, email-validator template”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The code sits downstream of all four.&lt;/p&gt;
    &lt;p&gt;The “version” of the component is the root hash of this graph. Change a requirement and only the downstream nodes change. Regenerate with identical inputs and the root hash remains stable. Identity moves from files to intent.&lt;/p&gt;
    &lt;head rend="h3"&gt;What’s New and What Isn’t&lt;/head&gt;
    &lt;p&gt;None of these ideas exist in isolation.&lt;/p&gt;
    &lt;p&gt;Build systems like Bazel—and increasingly Nix-style systems—use hashed inputs and content-addressed caches to track which inputs produced which outputs. Formal methods have long pursued specifications with mathematical semantics precise enough to analyze and verify.&lt;/p&gt;
    &lt;p&gt;What’s new is the coupling.&lt;/p&gt;
    &lt;p&gt;Bazel tracks build causality. Formal specifications describe logical intent. Regenerable systems require generative provenance: a direct, machine-enforced link between intent and implementation. The specification graph doesn’t sit beside the system. It drives it.&lt;/p&gt;
    &lt;p&gt;Description can drift. Drivers cannot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Traceability Failed and Why It Might Not Now&lt;/head&gt;
    &lt;p&gt;Industries have attempted requirements traceability for decades, usually through tickets, spreadsheets, and process checklists. It often failed in mainstream software because humans were asked to maintain links that the system itself did not depend on.&lt;/p&gt;
    &lt;p&gt;Regenerable systems invert the incentives.&lt;/p&gt;
    &lt;p&gt;If a system can regenerate itself, it must already know what it’s doing. Provenance stops being overhead and becomes infrastructure. The links exist because generation requires them.&lt;/p&gt;
    &lt;p&gt;This does not describe how today’s AI tools work. Current generators do not emit stable, versionable plans or structured intent graphs. This is not a description of the present. It’s an argument about the direction forced by regeneration economics: the cost of re-deriving code keeps falling, while the cost of rediscovering intent does not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hard Problems and Failure Modes&lt;/head&gt;
    &lt;p&gt;This model raises real challenges.&lt;/p&gt;
    &lt;p&gt;Specifications expressed in natural language require canonicalization. Two nodes may be semantically equivalent but textually different, and we won’t always detect that reliably. Agents will make implicit assumptions that are not explicitly recorded. Non-deterministic generators may produce different code from identical intent graphs.&lt;/p&gt;
    &lt;p&gt;These are not reasons to abandon the approach. They are design constraints.&lt;/p&gt;
    &lt;p&gt;The model does not require perfect formalization. It requires tractability—and tractability improves as specifications become more structured, plans become explicit, and generators are forced to surface their decisions. Ambiguity becomes visible rather than hidden in diffs.&lt;/p&gt;
    &lt;p&gt;Even failure becomes diagnosable at the level that matters: intent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Versioning What Actually Matters&lt;/head&gt;
    &lt;p&gt;Git taught us how to version text.&lt;/p&gt;
    &lt;p&gt;Regenerable systems force us to version intent: the requirements, constraints, and decisions that caused a system to take its current shape. Code still matters but it becomes an artifact, not the record of authorship.&lt;/p&gt;
    &lt;p&gt;The tools to do this well don’t fully exist yet. But the pressure is already here. If code can be recreated at will, the question becomes unavoidable:&lt;/p&gt;
    &lt;p&gt;What, exactly, is worth preserving, and how would you know?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aicoding.leaflet.pub/3mcbiyal7jc2y"/><published>2026-01-13T03:26:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46597030</id><title>PawSense: Catproof Your Computer</title><updated>2026-01-13T05:45:53.971028+00:00</updated><content>&lt;doc fingerprint="dd1e1d1d218f6715"&gt;
  &lt;main&gt;
    &lt;p&gt;When cats walk or climb on your keyboard, they can enter random commands and data, damage your files, and even crash your computer. This can happen whether you are near the computer or have suddenly been called away from it.&lt;/p&gt;
    &lt;p&gt;PawSense is a software utility that helps protect your computer from cats. It quickly detects and blocks cat typing, and also helps train your cat to stay off the computer keyboard.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Every time your computer boots up, PawSense will automatically start up in the background to watch over your computer system.&lt;/item&gt;
      &lt;item&gt;Even while you use your other software, PawSense constantly monitors keyboard activity. PawSense analyzes keypress timings and combinations to distinguish cat typing from human typing. PawSense normally recognizes a cat on the keyboard within one or two pawsteps.&lt;/item&gt;
      &lt;item&gt; If a cat gets on the keyboard, PawSense makes a sound that annoys cats. &lt;lb/&gt;This teaches your cat that getting on the keyboard is bad even if humans aren't watching.&lt;/item&gt;
      &lt;item&gt;Once a cat has been recognized, PawSense blocks the cat's keyboard input. This keeps the cat from entering lots of commands to your programs or operating system.&lt;/item&gt;
      &lt;item&gt;PawSense also includes a screensaver mode, which features extra-sensitive cat detection, protection, and sonic training and a one-click escape button.&lt;/item&gt;
      &lt;item&gt;We make our software easier to use with a manual that you can view two different ways: you can view it on your computer, and you can print it.&lt;/item&gt;
      &lt;item&gt;Additional advanced features provide maximal flexibility and effectiveness.&lt;/item&gt;
      &lt;item&gt;Now available for Windows 2000, XP, Vista, 7, and 8. (See Windows 8 details.) (See Windows 10 details.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To buy PawSense, click here.&lt;/item&gt;
      &lt;item&gt;To find out more about using or configuring PawSense, please contact us.&lt;/item&gt;
      &lt;item&gt;For more information about buying PawSense, you can use email, telephone, or postal mail to contact us.&lt;/item&gt;
      &lt;item&gt;We have a "Frequently Asked Questions" list with answers.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://bitboost.com/pawsense/"/><published>2026-01-13T03:27:05+00:00</published></entry></feed>