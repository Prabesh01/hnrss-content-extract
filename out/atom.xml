<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-03T23:09:46.789329+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45798479</id><title>The Case Against PGVector</title><updated>2025-11-03T23:09:57.619865+00:00</updated><content>&lt;doc fingerprint="231d8b761d4497ae"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Everyone Loves pgvector (in theory)&lt;/head&gt;&lt;p&gt;If you‚Äôve spent any time in the vector search space over the past year, you‚Äôve probably read blog posts explaining why pgvector is the obvious choice for your vector database needs. The argument goes something like this: you already have Postgres, vector embeddings are just another data type, why add complexity with a dedicated vector database when you can keep everything in one place?&lt;/p&gt;&lt;p&gt;It‚Äôs a compelling story. And like most of the AI influencer bullshit that fills my timeline, it glosses over the inconvenient details.&lt;/p&gt;&lt;p&gt;I‚Äôm not here to tell you pgvector is bad. It‚Äôs not. It‚Äôs a useful extension that brings vector similarity search to Postgres. But after spending some time trying to build a production system on top of it, I‚Äôve learned that the gap between ‚Äúworks in a demo‚Äù and ‚Äúscales in production‚Äù is&amp;amp;mldr; significant.&lt;/p&gt;&lt;head rend="h2"&gt;Nobody‚Äôs actually run this in production&lt;/head&gt;&lt;p&gt;What bothers me most: the majority of content about pgvector reads like it was written by someone who spun up a local Postgres instance, inserted 10,000 vectors, ran a few queries, and called it a day. The posts are optimistic, the benchmarks are clean, and the conclusions are confident.&lt;/p&gt;&lt;p&gt;They‚Äôre also missing about 80% of what you actually need to know.&lt;/p&gt;&lt;p&gt;I‚Äôve read through dozens of these posts.&lt;/p&gt;They all cover the same ground: here‚Äôs how to install pgvector, here‚Äôs how to create a vector column, here‚Äôs a simple similarity search query. Some of them even mention that you should probably add an index.&lt;p&gt;What they don‚Äôt tell you is what happens when you actually try to run this in production.&lt;/p&gt;&lt;head rend="h2"&gt;Picking an index (there are no good options)&lt;/head&gt;&lt;p&gt;Let‚Äôs start with indexes, because this is where the tradeoffs start.&lt;/p&gt;&lt;p&gt;pgvector gives you two index types: IVFFlat and HNSW. The blog posts will tell you that HNSW is newer and generally better, which is&amp;amp;mldr; technically true but deeply unhelpful.&lt;/p&gt;&lt;head rend="h3"&gt;IVFFlat&lt;/head&gt;&lt;p&gt;IVFFlat (Inverted File with Flat quantization) partitions your vector space into clusters. During search, it identifies the nearest clusters and only searches within those.&lt;/p&gt;&lt;p&gt;The good:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Lower memory footprint during index creation&lt;/item&gt;&lt;item&gt;Reasonable query performance for many use cases&lt;/item&gt;&lt;item&gt;Index creation is faster than HNSW&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The bad:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Requires you to specify the number of lists (clusters) upfront&lt;/item&gt;&lt;item&gt;That number significantly impacts both recall and query performance&lt;/item&gt;&lt;item&gt;The commonly recommended formula (&lt;code&gt;rows / 1000&lt;/code&gt;) is a starting point at best&lt;/item&gt;&lt;item&gt;Recall can be&amp;amp;mldr; disappointing depending on your data distribution&lt;/item&gt;&lt;item&gt;New vectors get assigned to existing clusters, but clusters don‚Äôt rebalance without a full rebuild&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Image source: IVFFlat or HNSW index for similarity search? by Simeon Emanuilov&lt;/p&gt;&lt;head rend="h3"&gt;HNSW&lt;/head&gt;&lt;p&gt;HNSW (Hierarchical Navigable Small World) builds a multi-layer graph structure for search.&lt;/p&gt;&lt;p&gt;The good:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Better recall than IVFFlat for most datasets&lt;/item&gt;&lt;item&gt;More consistent query performance&lt;/item&gt;&lt;item&gt;Scales well to larger datasets&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The bad:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Significantly higher memory requirements during index builds&lt;/item&gt;&lt;item&gt;Index creation is slow‚Äîpainfully slow for large datasets&lt;/item&gt;&lt;item&gt;The memory requirements aren‚Äôt theoretical; they are real, and they‚Äôll take down your database if you‚Äôre not careful&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Image source: IVFFlat or HNSW index for similarity search? by Simeon Emanuilov&lt;/p&gt;&lt;p&gt;None of the blogs mention that building an HNSW index on a few million vectors can consume 10+ GB of RAM or more (depending on your vector dimensions and dataset size). On your production database. While it‚Äôs running. For potentially hours.&lt;/p&gt;&lt;head rend="h2"&gt;Real-time search is basically impossible&lt;/head&gt;&lt;p&gt;In a typical application, you want newly uploaded data to be searchable immediately. User uploads a document, you generate embeddings, insert them into your database, and they should be available in search results. Simple, right?&lt;/p&gt;&lt;head rend="h3"&gt;How index updates actually work&lt;/head&gt;&lt;p&gt;When you insert new vectors into a table with an index, one of two things happens:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;IVFFlat: The new vectors are inserted into the appropriate clusters based on the existing structure. This works, but it means your cluster distribution gets increasingly suboptimal over time. The solution is to rebuild the index periodically. Which means downtime, or maintaining a separate index and doing an atomic swap, or accepting degraded search quality.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;HNSW: New vectors are added to the graph structure. This is better than IVFFlat, but it‚Äôs not free. Each insertion requires updating the graph, which means memory allocation, graph traversals, and potential lock contention.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Neither of these is a deal-breaker in isolation. But here‚Äôs what happens in practice: you‚Äôre inserting vectors continuously throughout the day. Each insertion is individually cheap, but the aggregate load adds up. Your database is now handling your normal transactional workload, analytical queries, AND maintaining graph structures in memory for vector search.&lt;/p&gt;&lt;head rend="h3"&gt;Handling new inserts&lt;/head&gt;&lt;p&gt;Let‚Äôs say you‚Äôre building a document search system. Users upload PDFs, you extract text, generate embeddings, and insert them. The user expects to immediately search for that document.&lt;/p&gt;&lt;p&gt;Here‚Äôs what actually happens:&lt;/p&gt;&lt;p&gt;With no index: The insert is fast, the document is immediately available, but your searches do a full sequential scan. This works fine for a few thousand documents. At a few hundred thousand? Your searches start taking seconds. Millions? Good luck.&lt;/p&gt;&lt;p&gt;With IVFFlat: The insert is still relatively fast. The vector gets assigned to a cluster. But whoops, a problem. Those initial cluster assignments were based on the data distribution when you built the index. As you add more data, especially if it‚Äôs not uniformly distributed, some clusters get overloaded. Your search quality degrades. You rebuild the index periodically to fix this, but during the rebuild (which can take hours for large datasets), what do you do with new inserts? Queue them? Write to a separate unindexed table and merge later?&lt;/p&gt;&lt;p&gt;With HNSW: The graph gets updated on each insert through incremental insertion, which sounds great. But updating an HNSW graph isn‚Äôt free‚Äîyou‚Äôre traversing the graph to find the right place to insert the new node and updating connections. Each insert acquires locks on the graph structure. Under heavy write load, this becomes a bottleneck. And if your write rate is high enough, you start seeing lock contention that slows down both writes and reads.&lt;/p&gt;&lt;head rend="h3"&gt;The operational reality&lt;/head&gt;&lt;p&gt;Here‚Äôs the real nightmare: you‚Äôre not just storing vectors. You have metadata‚Äîdocument titles, timestamps, user IDs, categories, etc. That metadata lives in other tables (or other columns in the same table). You need that metadata and the vectors to stay in sync.&lt;/p&gt;&lt;p&gt;In a normal Postgres table, this is easy‚Äîtransactions handle it. But when you‚Äôre dealing with index builds that take hours, keeping everything consistent gets complicated. For IVFFlat, periodic rebuilds are basically required to maintain search quality. For HNSW, you might need to rebuild if you want to tune parameters or if performance has degraded.&lt;/p&gt;&lt;p&gt;The problem is that index builds are memory-intensive operations, and Postgres doesn‚Äôt have a great way to throttle them. You‚Äôre essentially asking your production database to allocate multiple (possibly dozens) gigabytes of RAM for an operation that might take hours, while continuing to serve queries.&lt;/p&gt;&lt;p&gt;You end up with strategies like:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Write to a staging table, build the index offline, then swap it in (but now you have a window where searches miss new data)&lt;/item&gt;&lt;item&gt;Maintain two indexes and write to both (double the memory, double the update cost)&lt;/item&gt;&lt;item&gt;Build indexes on replicas and promote them&lt;/item&gt;&lt;item&gt;Accept eventual consistency (users upload documents that aren‚Äôt searchable for N minutes)&lt;/item&gt;&lt;item&gt;Provision significantly more RAM than your ‚Äúworking set‚Äù would suggest&lt;/item&gt;&lt;/list&gt;&lt;p&gt;None of these are ‚Äúwrong‚Äù exactly. But they‚Äôre all workarounds for the fact that pgvector wasn‚Äôt really designed for high-velocity real-time ingestion.&lt;/p&gt;&lt;head rend="h2"&gt;Pre- vs. Post-Filtering (or: why you need to become a query planner expert)&lt;/head&gt;&lt;p&gt;Okay but let‚Äôs say you solve your index and insert problems. Now you have a document search system with millions of vectors. Documents have metadata‚Äîmaybe they‚Äôre marked as &lt;code&gt;draft&lt;/code&gt;, &lt;code&gt;published&lt;/code&gt;, or &lt;code&gt;archived&lt;/code&gt;. A user searches for something, and you only want to return published documents.&lt;/p&gt;&lt;code&gt;1SELECT * FROM documents
2WHERE status = 'published'
3ORDER BY embedding &amp;lt;-&amp;gt; query_vector
4LIMIT 10;
&lt;/code&gt;&lt;p&gt;Simple enough. But now you have a problem: should Postgres filter on status first (pre-filter) or do the vector search first and then filter (post-filter)?&lt;/p&gt;&lt;p&gt;This seems like an implementation detail. It‚Äôs not. It‚Äôs the difference between queries that take 50ms and queries that take 5 seconds. It‚Äôs also the difference between returning the most relevant results and&amp;amp;mldr; not.&lt;/p&gt;&lt;p&gt;Pre-filter works great when the filter is highly selective (1,000 docs out of 10M). It works terribly when the filter isn‚Äôt selective‚Äîyou‚Äôre still searching millions of vectors.&lt;/p&gt;&lt;p&gt;Post-filter works when your filter is permissive. Here‚Äôs where it breaks: imagine you ask for 10 results with &lt;code&gt;LIMIT 10&lt;/code&gt;. pgvector finds the 10 nearest neighbors, then applies your filter. Only 3 of those 10 are published. You get 3 results back, even though there might be hundreds of relevant published documents slightly further away in the embedding space.&lt;/p&gt;&lt;p&gt;The user searched, got 3 mediocre results, and has no idea they‚Äôre missing way better matches that didn‚Äôt make it into the initial k=10 search.&lt;/p&gt;&lt;p&gt;You can work around this by fetching more vectors (say, &lt;code&gt;LIMIT 100&lt;/code&gt;) and then filtering, but now:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You‚Äôre doing way more distance calculations than needed&lt;/item&gt;&lt;item&gt;You still don‚Äôt know if 100 is enough&lt;/item&gt;&lt;item&gt;Your query performance suffers&lt;/item&gt;&lt;item&gt;You‚Äôre guessing at the right oversampling factor&lt;/item&gt;&lt;/list&gt;&lt;p&gt;With pre-filter, you avoid this problem, but you get the performance problems I mentioned. Pick your poison.&lt;/p&gt;&lt;head rend="h3"&gt;Multiple filters&lt;/head&gt;&lt;p&gt;Now add another dimension: you‚Äôre filtering by user_id AND category AND date_range.&lt;/p&gt;&lt;code&gt;1SELECT * FROM documents
2WHERE user_id = 'user123'
3  AND category = 'technical'
4  AND created_at &amp;gt; '2024-01-01'
5ORDER BY embedding &amp;lt;-&amp;gt; query_vector
6LIMIT 10;
&lt;/code&gt;&lt;p&gt;What‚Äôs the right strategy now?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Apply all filters first, then search? (Pre-filter)&lt;/item&gt;&lt;item&gt;Search first, then apply all filters? (Post-filter)&lt;/item&gt;&lt;item&gt;Apply some filters first, search, then apply remaining filters? (Hybrid)&lt;/item&gt;&lt;item&gt;Which filters should you apply in which order?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The planner will look at table statistics, index selectivity, and estimated row counts and come up with a plan. That plan will probably be wrong, or at least suboptimal, because the planner‚Äôs cost model wasn‚Äôt built for vector similarity search.&lt;/p&gt;&lt;p&gt;And it gets worse: you‚Äôre inserting new vectors throughout the day. Your index statistics are outdated. The plans get increasingly suboptimal until you ANALYZE the table. But ANALYZE on a large table with millions of rows takes time and resources. And it doesn‚Äôt really understand vector data distribution in a meaningful way‚Äîit can tell you how many rows match &lt;code&gt;user_id = 'user123'&lt;/code&gt;, but not how clustered those vectors are in the embedding space, which is what actually matters for search performance.&lt;/p&gt;&lt;head rend="h3"&gt;Workarounds&lt;/head&gt;&lt;p&gt;You end up with hacks: query rewriting for different user types, partitioning your data into separate tables, CTE optimization fences to force the planner‚Äôs hand, or just fetching way more results than needed and filtering in application code.&lt;/p&gt;&lt;p&gt;None of these are sustainable at scale.&lt;/p&gt;&lt;head rend="h3"&gt;What vector databases do&lt;/head&gt;&lt;p&gt;Dedicated vector databases have solved this. They understand the cost model of filtered vector search and make intelligent decisions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Adaptive strategies: Some databases dynamically choose pre-filter or post-filter based on estimated selectivity&lt;/item&gt;&lt;item&gt;Configurable modes: Others let you specify the strategy explicitly when you know your data distribution&lt;/item&gt;&lt;item&gt;Specialized indexes: Some build indexes that support efficient filtered search (like filtered HNSW)&lt;/item&gt;&lt;item&gt;Query optimization: They track statistics specific to vector operations and optimize accordingly&lt;/item&gt;&lt;/list&gt;&lt;p&gt;OpenSearch‚Äôs k-NN plugin, for example, lets you specify pre-filter or post-filter behavior. Pinecone automatically handles filter selectivity. Weaviate has optimizations for common filter patterns.&lt;/p&gt;&lt;p&gt;With pgvector, you get to build all of this yourself. Or live with suboptimal queries. Or hire a Postgres expert to spend weeks tuning your query patterns.&lt;/p&gt;&lt;head rend="h2"&gt;Hybrid search? Build it yourself&lt;/head&gt;&lt;p&gt;Oh, and if you want hybrid search‚Äîcombining vector similarity with traditional full-text search‚Äîyou get to build that yourself too.&lt;/p&gt;&lt;p&gt;Postgres has excellent full-text search capabilities. pgvector has excellent vector search capabilities. Combining them in a meaningful way? That‚Äôs on you.&lt;/p&gt;&lt;p&gt;You need to:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Decide how to weight vector similarity vs. text relevance&lt;/item&gt;&lt;item&gt;Normalize scores from two different scoring systems&lt;/item&gt;&lt;item&gt;Tune the balance for your use case&lt;/item&gt;&lt;item&gt;Probably implement Reciprocal Rank Fusion or something similar&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Again, not impossible. Just another thing that many dedicated vector databases provide out of the box.&lt;/p&gt;&lt;head rend="h2"&gt;pgvectorscale (it doesn‚Äôt solve everything)&lt;/head&gt;&lt;p&gt;Timescale has released pgvectorscale, which addresses some of these issues. It adds:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;StreamingDiskANN, a new search backend that‚Äôs more memory-efficient&lt;/item&gt;&lt;item&gt;Better support for incremental index builds&lt;/item&gt;&lt;item&gt;Improved filtering performance&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is great! It‚Äôs also an admission that pgvector out of the box isn‚Äôt sufficient for production use cases.&lt;/p&gt;&lt;p&gt;pgvectorscale is still relatively new, and adopting it means adding another dependency, another extension, another thing to manage and upgrade. For some teams, that‚Äôs fine. For others, it‚Äôs just more evidence that maybe the ‚Äúkeep it simple, use Postgres‚Äù argument isn‚Äôt as simple as it seemed.&lt;/p&gt;&lt;p&gt;Oh, and if you‚Äôre running on RDS, pgvectorscale isn‚Äôt available. AWS doesn‚Äôt support it. So enjoy managing your own Postgres instance if you want these improvements, or just&amp;amp;mldr; keep dealing with the limitations of vanilla pgvector.&lt;/p&gt;&lt;p&gt;The ‚Äújust use Postgres‚Äù simplicity keeps getting simpler.&lt;/p&gt;&lt;head rend="h2"&gt;Just use a real vector database&lt;/head&gt;&lt;p&gt;I get the appeal of pgvector. Consolidating your stack is good. Reducing operational complexity is good. Not having to manage another database is good.&lt;/p&gt;&lt;p&gt;But here‚Äôs what I‚Äôve learned: for most teams, especially small teams, dedicated vector databases are actually simpler.&lt;/p&gt;&lt;head rend="h3"&gt;What you actually get&lt;/head&gt;&lt;p&gt;With a managed vector database (Pinecone, Weaviate, Turbopuffer, etc.), you typically get:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Intelligent query planning for filtered searches&lt;/item&gt;&lt;item&gt;Hybrid search built in&lt;/item&gt;&lt;item&gt;Real-time indexing without memory spikes&lt;/item&gt;&lt;item&gt;Horizontal scaling without complexity&lt;/item&gt;&lt;item&gt;Monitoring and observability designed for vector workloads&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;It‚Äôs probably cheaper than you think&lt;/head&gt;&lt;p&gt;Yes, it‚Äôs another service to pay for. But compare:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The cost of a managed vector database for your workload&lt;/item&gt;&lt;item&gt;vs. the cost of over-provisioning your Postgres instance to handle index builds&lt;/item&gt;&lt;item&gt;vs. the engineering time to tune queries and manage index rebuilds&lt;/item&gt;&lt;item&gt;vs. the opportunity cost of not building features because you‚Äôre fighting your database&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Turbopuffer starts at $64 month with generous limits.&lt;/p&gt;&lt;p&gt;For a lot of teams, the managed service is actually cheaper.&lt;/p&gt;&lt;head rend="h2"&gt;What I wish someone had told me&lt;/head&gt;&lt;p&gt;pgvector is an impressive piece of technology. It brings vector search to Postgres in a way that‚Äôs technically sound and genuinely useful for many applications.&lt;/p&gt;&lt;p&gt;But it‚Äôs not a panacea. Understand the tradeoffs.&lt;/p&gt;&lt;p&gt;If you‚Äôre building a production vector search system:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Index management is hard. Rebuilds are memory-intensive, time-consuming, and disruptive. Plan for this from day one.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Query planning matters. Filtered vector search is a different beast than traditional queries, and Postgres‚Äôs planner wasn‚Äôt built for this.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Real-time indexing has costs. Either in memory, in search quality, or in engineering time to manage it.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The blog posts are lying to you (by omission). They‚Äôre showing you the happy path and ignoring the operational reality.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Managed offerings exist for a reason. There‚Äôs a reason that Pinecone, Weaviate, Qdrant, and others exist and are thriving. Vector search at scale has unique challenges that general-purpose databases weren‚Äôt designed to handle.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The question isn‚Äôt ‚Äúshould I use pgvector?‚Äù It‚Äôs ‚Äúam I willing to take on the operational complexity of running vector search in Postgres?‚Äù&lt;/p&gt;&lt;p&gt;For some teams, the answer is yes. You have database expertise, you need the tight integration, you‚Äôre willing to invest the time.&lt;/p&gt;&lt;p&gt;For many teams‚Äîmaybe most teams‚Äîthe answer is probably no. Use a tool designed for the job. Your future self will thank you.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alex-jacobs.com/posts/the-case-against-pgvector/"/><published>2025-11-03T12:50:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798681</id><title>Why Nextcloud feels slow to use</title><updated>2025-11-03T23:09:56.837987+00:00</updated><content>&lt;doc fingerprint="57c20352a0823c6c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Nextcloud feels slow to use&lt;/head&gt;
    &lt;p&gt;Nextcloud. I really want to like it, but it‚Äôs making it really difficult.&lt;/p&gt;
    &lt;p&gt;I like what Nextcloud offers with its feature set and how easily it replaces a bunch of services under one roof (files, calendar, contacts, notes, to-do lists, photos etc.), but no matter how hard I try and how much I optimize its resources on my home server, it feels slow to use, even on hardware that is ranging from decent to good. Then I opened developer tools and found the culprit.&lt;/p&gt;
    &lt;p&gt;It‚Äôs the Javascript.&lt;/p&gt;
    &lt;p&gt;On a clean page load, you will be downloading about 15-20 MB of Javascript, which does compress down to about 4-5 MB in transit, but that is still a huge amount of Javascript. For context, I consider 1 MB of Javascript to be on the heavy side for a web page/app.&lt;/p&gt;
    &lt;p&gt;Yes, that Javascript will be cached in the browser for a while, but you will still be executing all of that on each visit to your Nextcloud instance, and that will take a long time due to the sheer amount of code your browser now has to execute on the page.&lt;/p&gt;
    &lt;p&gt;A significant contributor to this heft seems to be the &lt;code&gt;core-common.js&lt;/code&gt; bundle, which based on its name seems to provide
some common functionality that‚Äôs shared across different Nextcloud apps that one can install. It‚Äôs coming in at 4.71
MB at the time of writing.&lt;/p&gt;
    &lt;p&gt;Then you want notifications, right? &lt;code&gt;NotificationsApp.chunk.mjs&lt;/code&gt; is here to cover you, at 1.06 MB.&lt;/p&gt;
    &lt;p&gt;Then there are the app-specific views. The Calendar app is taking up 5.94 MB to show a basic calendar view.&lt;/p&gt;
    &lt;p&gt;Files app includes a bunch of individual scripts, such as &lt;code&gt;EditorOutline&lt;/code&gt; (1.77 MB), &lt;code&gt;previewUtils&lt;/code&gt; (1.17 MB),
&lt;code&gt;index&lt;/code&gt; (1.09 MB), &lt;code&gt;emoji-picker&lt;/code&gt; (0.9 MB which I‚Äôve never used!) and many smaller ones.&lt;/p&gt;
    &lt;p&gt;Notes app with its basic bare-bones editor? 4.36 MB for the &lt;code&gt;notes-main.js&lt;/code&gt;!&lt;/p&gt;
    &lt;p&gt;This means that even on an iPhone 13 mini, opening the Tasks app (to-do list), will take a ridiculously long time. Imagine opening your shopping list at the store and having to wait 5-10 seconds before you see anything, even with a solid 5G connection. Sounds extremely annoying, right?&lt;/p&gt;
    &lt;p&gt;I suspect that a lot of this is due to how Nextcloud is architected. There‚Äôs bound to be some hefty common libraries and tools that allow app developers to provide a unified experience, but even then there is something seriously wrong with the end result, the functionality to bundle size ratio is way off.&lt;/p&gt;
    &lt;p&gt;As a result, I‚Äôve started branching out some things from Nextcloud, such as replacing the Tasks app with using a private Vikunja instance, and Photos to a private Immich instance. Vikunja is not perfect, but its 1.5 MB of Javascript is an order of magnitude smaller compared to Nextcloud, making it feel incredibly fast in comparison.&lt;/p&gt;
    &lt;p&gt;However, with other functionality I have to admit that the convenience of Nextcloud is enough to dissuade me from replacing it elsewhere, due to the available feature set comparing well to alternatives.&lt;/p&gt;
    &lt;p&gt;For now.&lt;/p&gt;
    &lt;p&gt;I‚Äôm sure that there are some legitimate reasons behind the current state, and overworked development teams and volunteers are unfortunately the norm in the industry, but it doesn‚Äôt take away the fact that the user experience and accessibility suffers as a result.&lt;/p&gt;
    &lt;p&gt;I‚Äôd like to thank Alex Russell for writing about web performance and why it matters, with supporting evidence and actionable advice, it has changed how I view websites and web apps and has pushed me to be better in my own work. I highly suggest reading his content, starting with the performance inequality gap series. It‚Äôs educational, insightful and incredibly irritating once you learn how crap most things are and how careless a lot of development teams are towards performance and accessibility.&lt;/p&gt;
    &lt;p&gt;Subscribe to new posts via the RSS feed.&lt;/p&gt;
    &lt;p&gt;Not sure what RSS is, or how to get started? Check this guide!&lt;/p&gt;
    &lt;p&gt;You can reach me via e-mail or LinkedIn.&lt;/p&gt;
    &lt;p&gt;If you liked this post, consider sharing it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ounapuu.ee/posts/2025/11/03/nextcloud-slow/"/><published>2025-11-03T13:21:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798838</id><title>VimGraph</title><updated>2025-11-03T23:09:56.304925+00:00</updated><content>&lt;doc fingerprint="797c6f35ce5cddfb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wolfram Function Repository&lt;/head&gt;
    &lt;p&gt;Instant-use add-on functions for the Wolfram Language&lt;/p&gt;
    &lt;p&gt;Function Repository Resource:&lt;/p&gt;
    &lt;p&gt;Construct a graph of simple Vim-style movements in text&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ResourceFunction["VimGraph"][text]&lt;/p&gt;
          &lt;p&gt;returns a graph with letters as vertices and Vim-style movements as edges.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shortcut&lt;/cell&gt;
        &lt;cell&gt;Movement Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;h / l&lt;/cell&gt;
        &lt;cell&gt;Move one character left / right on the same line&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;k / j&lt;/cell&gt;
        &lt;cell&gt;Move one character up / down; jumps to end of target line if shorter than current horizontal position&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;w / b&lt;/cell&gt;
        &lt;cell&gt;Jump to the beginning of the next / previous word, across lines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;e&lt;/cell&gt;
        &lt;cell&gt;Jump to the end of the next word, across lines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;^/$&lt;/cell&gt;
        &lt;cell&gt;Move to the beginning/end of the current line&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Vim graph for the movements: up, right, and to the beginning of the next word, respectively:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[1]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[1]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The same, with nicer formatting:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[2]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[2]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Returns a minimal sequence of keystrokes needed to move from one letter to another:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[3]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[3]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Illustrates the relationship between the maximum keystroke distance required to navigate between two letters in a text and the number of randomly inserted newlines:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[4]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[4]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Use the "CustomPatterns" option to define new movements by passing a string pattern to "StringPattern", with optional shortcuts for jumping forward or backward to the nearest match:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[5]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[5]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Wolfram Language 13.0 (December 2021) or above&lt;/p&gt;
    &lt;p&gt;This work is licensed under a Creative Commons Attribution 4.0 International License&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://resources.wolframcloud.com/FunctionRepository/resources/VimGraph/"/><published>2025-11-03T13:40:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798871</id><title>Show HN: a Rust ray tracer that runs on any GPU ‚Äì even in the browser</title><updated>2025-11-03T23:09:55.690538+00:00</updated><content>&lt;doc fingerprint="6f9919590aaf4bcd"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;A high-performance path tracing renderer with CPU, GPU, and real-time interactive implementations&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Try it online: Live WebGPU Raytracer&lt;/p&gt;
    &lt;p&gt;Run locally:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/tchauffi/rust-rasterizer.git
cd rust-rasterizer
cargo run --bin live_raytracer --release&lt;/code&gt;
    &lt;p&gt;Use your mouse to rotate the camera and press &lt;code&gt;SPACE&lt;/code&gt; to toggle rendering modes!&lt;/p&gt;
    &lt;p&gt;This project demonstrates three different approaches to ray tracing - a rendering technique that simulates how light behaves in the real world to create photorealistic images:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;üñ•Ô∏è CPU Raytracer - Traditional software rendering with full path tracing&lt;/item&gt;
      &lt;item&gt;‚ö° GPU Raytracer - Hardware-accelerated offline rendering using compute shaders&lt;/item&gt;
      &lt;item&gt;üéÆ Live GPU Raytracer - Real-time interactive raytracer you can navigate and explore&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whether you're learning about ray tracing, exploring Rust graphics programming, or looking for a starting point for your own renderer, this project provides working examples of multiple rendering approaches.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple Rendering Backends: Choose between CPU, offline GPU, or real-time GPU rendering&lt;/item&gt;
      &lt;item&gt;Path Tracing: Physically-based lighting with multiple light bounces for realistic illumination&lt;/item&gt;
      &lt;item&gt;Mesh Support: Load and render complex .obj models with triangle meshes&lt;/item&gt;
      &lt;item&gt;HDR Environment Maps: Support for high dynamic range .exr skyboxes&lt;/item&gt;
      &lt;item&gt;Interactive Controls: Real-time camera navigation with mouse controls&lt;/item&gt;
      &lt;item&gt;WebAssembly Support: Runs in the browser using WebGPU&lt;/item&gt;
      &lt;item&gt;Multiple Materials: Diffuse, metallic surface properties&lt;/item&gt;
      &lt;item&gt;Acceleration Structures: BVH for efficient ray-mesh intersection&lt;/item&gt;
      &lt;item&gt;Progressive Rendering: Denoising and sample accumulation for high-quality images&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The CPU version renders scenes using traditional CPU-based raytracing and outputs to a PPM image file. Best for learning how raytracing works under the hood.&lt;/p&gt;
    &lt;code&gt;# Build and run (outputs to stdout, redirect to file)
cargo run --release &amp;gt; output.ppm

# Or build first, then run
cargo build --release
./target/release/rust-raytracer &amp;gt; output.ppm&lt;/code&gt;
    &lt;p&gt;Use this when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Learning raytracing fundamentals&lt;/item&gt;
      &lt;item&gt;Debugging rendering algorithms&lt;/item&gt;
      &lt;item&gt;No GPU compute support available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full path tracing with multiple bounces&lt;/item&gt;
      &lt;item&gt;Direct and indirect lighting&lt;/item&gt;
      &lt;item&gt;Mesh support (.obj files)&lt;/item&gt;
      &lt;item&gt;Sphere primitives&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The GPU version uses compute shaders to accelerate rendering, outputting to a PPM file. Significantly faster than CPU rendering for high-quality images.&lt;/p&gt;
    &lt;code&gt;# Build and run
cargo run --bin gpu_raytracer --release &amp;gt; output.ppm

# Or build separately
cargo build --bin gpu_raytracer --release
./target/release/gpu_raytracer &amp;gt; output.ppm&lt;/code&gt;
    &lt;p&gt;Use this when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rendering high-quality final images&lt;/item&gt;
      &lt;item&gt;Need faster rendering than CPU&lt;/item&gt;
      &lt;item&gt;Don't need real-time interaction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GPU-accelerated compute shader rendering&lt;/item&gt;
      &lt;item&gt;Same scene quality as CPU version&lt;/item&gt;
      &lt;item&gt;Significantly faster rendering times (10-100x speedup)&lt;/item&gt;
      &lt;item&gt;Hardware-accelerated ray-triangle intersection&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The live version provides a real-time interactive window where you can navigate the scene. Perfect for exploring scenes and iterating on designs.&lt;/p&gt;
    &lt;code&gt;# Run the live raytracer
cargo run --bin live_raytracer --release&lt;/code&gt;
    &lt;p&gt;Use this when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exploring and navigating scenes interactively&lt;/item&gt;
      &lt;item&gt;Setting up camera angles for final renders&lt;/item&gt;
      &lt;item&gt;Demonstrating raytracing in real-time&lt;/item&gt;
      &lt;item&gt;Debugging scene geometry&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Controls:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mouse: Click and drag to rotate the camera&lt;/item&gt;
      &lt;item&gt;SPACE: Toggle between raytracing and normals visualization modes&lt;/item&gt;
      &lt;item&gt;Window Title: Displays current mode and FPS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time GPU raytracing (30-60 FPS on modern GPUs)&lt;/item&gt;
      &lt;item&gt;Interactive camera controls&lt;/item&gt;
      &lt;item&gt;Two rendering modes: &lt;list rend="ul"&gt;&lt;item&gt;Raytracing: Full path tracing with lighting and shadows&lt;/item&gt;&lt;item&gt;Normals: Fast visualization showing surface normals (useful for debugging)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Live FPS counter in window title&lt;/item&gt;
      &lt;item&gt;WebAssembly support for browser-based rendering&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust: Latest stable version (install from rustup.rs)&lt;/item&gt;
      &lt;item&gt;GPU versions: A GPU with compute shader support &lt;list rend="ul"&gt;&lt;item&gt;Vulkan (Linux, Windows, Android)&lt;/item&gt;&lt;item&gt;Metal (macOS, iOS)&lt;/item&gt;&lt;item&gt;DirectX 12 (Windows)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Web version: A browser with WebGPU support (Chrome 113+, Edge 113+, Firefox with flag)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The project can be built for WebAssembly and deployed to GitHub Pages:&lt;/p&gt;
    &lt;code&gt;# Install trunk (if not already installed)
cargo install trunk

# Build for web
trunk build --release

# The output will be in the dist/ directory&lt;/code&gt;
    &lt;p&gt;Performance varies based on hardware and scene complexity:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Renderer&lt;/cell&gt;
        &lt;cell role="head"&gt;Resolution&lt;/cell&gt;
        &lt;cell role="head"&gt;FPS/Time&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CPU&lt;/cell&gt;
        &lt;cell&gt;1920x1080&lt;/cell&gt;
        &lt;cell&gt;~30-60s&lt;/cell&gt;
        &lt;cell&gt;Single-threaded, full quality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPU Offline&lt;/cell&gt;
        &lt;cell&gt;1920x1080&lt;/cell&gt;
        &lt;cell&gt;~3-5s&lt;/cell&gt;
        &lt;cell&gt;High-quality final render&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPU Live&lt;/cell&gt;
        &lt;cell&gt;800x600&lt;/cell&gt;
        &lt;cell&gt;30-60 FPS&lt;/cell&gt;
        &lt;cell&gt;Real-time, interactive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WebGPU&lt;/cell&gt;
        &lt;cell&gt;800x600&lt;/cell&gt;
        &lt;cell&gt;25-50 FPS&lt;/cell&gt;
        &lt;cell&gt;15-20% slower than native&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Benchmarks are approximate and measured on modern hardware (M1/M2 Mac or RTX 3000+ series GPU). Your results may vary.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Completed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sphere ray tracing&lt;/item&gt;
      &lt;item&gt;Direct and indirect lighting calculations&lt;/item&gt;
      &lt;item&gt;Triangle meshes and .obj loading&lt;/item&gt;
      &lt;item&gt;GPU acceleration with compute shaders&lt;/item&gt;
      &lt;item&gt;Shadows and reflections&lt;/item&gt;
      &lt;item&gt;Interactive real-time viewer&lt;/item&gt;
      &lt;item&gt;WebAssembly/WebGPU support&lt;/item&gt;
      &lt;item&gt;BVH acceleration structure for faster ray-mesh&lt;/item&gt;
      &lt;item&gt;Progressive rendering with denoising&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Planned:&lt;/p&gt;
    &lt;p&gt;intersection&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Texture mapping (UV coordinates and image textures)&lt;/item&gt;
      &lt;item&gt;Additional material types (glass, subsurface scattering)&lt;/item&gt;
      &lt;item&gt;Scene file format for easy scene definition&lt;/item&gt;
      &lt;item&gt;Camera controls (zoom, pan, dolly)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Built with Rust and powered by:&lt;/p&gt;
    &lt;p&gt;Made with ‚ù§Ô∏è by tchauffi&lt;/p&gt;
    &lt;p&gt;Star ‚≠ê this repository if you find it helpful!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tchauffi/rust-rasterizer"/><published>2025-11-03T13:45:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798881</id><title>Skyfall-GS ‚Äì Synthesizing Immersive 3D Urban Scenes from Satellite Imagery</title><updated>2025-11-03T23:09:55.419808+00:00</updated><content>&lt;doc fingerprint="ed199ac9cc8e47ba"&gt;
  &lt;main&gt;
    &lt;p&gt;Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose Skyfall-GS, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches.&lt;/p&gt;
    &lt;p&gt;Our method synthesizes immersive and free-flight navigable city-block scale 3D scenes solely from multi-view satellite imagery in two stages.&lt;/p&gt;
    &lt;p&gt;(a) Reconstruction Stage&lt;/p&gt;
    &lt;p&gt;(b) Synthesis Stage&lt;/p&gt;
    &lt;p&gt;Explore our 3D Gaussian Splatting results interactively. Click on the scene buttons below to switch between different urban scenes. Use your mouse to freely navigate within each scene, and use WASD keys for fly navigation. Click the information button in the viewer for more controls.&lt;/p&gt;
    &lt;p&gt;This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2 and 113-2628-EA49-023-. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan.&lt;/p&gt;
    &lt;code&gt;@article{lee2025SkyfallGS,
  title = {{Skyfall-GS}: Synthesizing Immersive {3D} Urban Scenes from Satellite Imagery},
  author = {Jie-Ying Lee and Yi-Ruei Liu and Shr-Ruei Tsai and Wei-Cheng Chang and Chung-Ho Wu and Jiewen Chan and Zhenjun Zhao and Chieh Hubert Lin and Yu-Lun Liu},
  journal = {arXiv preprint},
  year = {2025},
  eprint = {2510.15869},
  archivePrefix = {arXiv}
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://skyfall-gs.jayinnn.dev/"/><published>2025-11-03T13:46:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45799478</id><title>State of Terminal Emulators in 2025: The Errant Champions</title><updated>2025-11-03T23:09:55.138764+00:00</updated><content>&lt;doc fingerprint="b60308d6479438c4"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a follow-up to my previous article, Terminal Emulators Battle Royale √¢ Unicode Edition! from 2023, in which I documented Unicode support across terminal emulators. Since then, the ucs-detect tool and its supporting blessed library have been extended to automatically detect support of DEC Private Modes, sixel graphics, pixel size, and software version.&lt;/p&gt;
    &lt;p&gt;The ucs-detect program tests terminal cursor positioning by sending visible text followed by control sequences that request the cursor position. The terminal responds by writing the cursor location as simulated keyboard input. The ucs-detect program reads and compares these values against the Python wcwidth library result, logging any discrepancies.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Width Problem&lt;/head&gt;
    &lt;p&gt;Terminal emulators face a fundamental challenge: mapping the vast breadth of Unicode scripts into a fixed-width grid while maintaining legibility. A terminal must predict whether each character occupies one cell or two, whether combining marks overlay previous characters, and how emoji sequences collapse into single glyphs.&lt;/p&gt;
    &lt;p&gt;These predictions fail routinely. Zero-width joiners, variation selectors, and grapheme clustering compound in complexity. When terminals and CLI applications guess wrong, text becomes unreadable - cursors misalign and corrupt output and so then also corrupt the location of our input.&lt;/p&gt;
    &lt;p&gt;Our results share which terminals have the best "Unicode support" -- the least likely to exhibit these kinds of problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Gentleman Errant&lt;/head&gt;
    &lt;p&gt;Before presenting the latest results, Ghostty warrants particular attention, not only because it scored the highest among all terminals tested, but that it was publicly released only this year by Mitchell Hashimoto. It is a significant advancement. Developed from scratch in zig, the Unicode support implementation is thoroughly correct.&lt;/p&gt;
    &lt;p&gt;In 2023, Mitchell published Grapheme Clusters and Terminal Emulators, demonstrating a commitment to understanding and implementing the fundamentals. His recent announcement of libghostty provides a welcome alternative to libvte, potentially enabling a new generation of terminals on a foundation of strong Unicode support.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Errant Champion&lt;/head&gt;
    &lt;p&gt;Kovid Goyal's Kitty scored just as well, only outranked by the arbitrary weights that are not necessarily fair. More important than scoring is Kovid's publication of a text-splitting algorithm description that closely matches the Python wcwidth specification. This is unsurprising since both are derived from careful interpretation of Unicode.org standards and that it scores so highly in our test.&lt;/p&gt;
    &lt;p&gt;Kitty and Ghostty are the only terminals that correctly support Variation Selector 15, I have not written much about it because it is not likely to see any practical use, but, it will be added to a future release of Python wcwidth now that there are multiple standards and reference implementations in agreement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing Results&lt;/head&gt;
    &lt;p&gt;The first table, General Tabulated Summary describes unicode features of each terminal, then, a brief summary of DEC Private Modes, sixel support, and testing time.&lt;/p&gt;
    &lt;p&gt;The second table, DEC Private Modes Support (not pictured), contains the first feature capability matrix of DEC Private Modes for Terminals of any length. I hope this is useful most especially to developers of CLI libraries and applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Long Road&lt;/head&gt;
    &lt;p&gt;The most notable finding relates to performance. That many terminals perform so slowly was surprising, so I have included the elapsed time in the results.&lt;/p&gt;
    &lt;p&gt;iTerm2 and Extraterm consume a majority of the CPU and perform so slowly that the test parameters were reduced to finish within the hour what many other terminals manage in a few minutes.&lt;/p&gt;
    &lt;p&gt;GNOME Terminal and its VTE-based derivatives also perform too slowly for a full test, taking over 5 hours while consuming very little CPU. Many terminals exhibit stalls or inefficiencies in their event loops that result in slow automatic responses, but we should be forgiving; nobody really considered the need to handle hundreds of automatic sequence replies per second!&lt;/p&gt;
    &lt;p&gt;I expected Python wcwidth to consume the most CPU resources during testing, as it is frequently called and always the "highest-level" language in the mix, but it keeps up pretty well for most terminals.&lt;/p&gt;
    &lt;p&gt;Earlier this year, I dedicated effort to optimizing the Python wcwidth implementation using techniques including bit vectors, bloom filters, and varying sizes of LRU caches. The results confirmed that the existing implementation performed best: a binary search with a functools.lru_cache decorator.&lt;/p&gt;
    &lt;p&gt;The LRU cache is effective because human languages typically use a small, repetitive subset of Unicode. The ucs-detect tool tests hundreds of languages from the UDHR dataset, excluding only those without any interesting zero or wide characters. This dataset provides an extreme but practical demonstration of LRU cache benefits when processing Unicode.&lt;/p&gt;
    &lt;p&gt;I previously considered distributing a C module with Python wcwidth for greater performance, but the existing Python implementation keeps up well enough with the fastest terminals. When fully exhausted the text scroll speed is fast enough to produce screen tearing artifacts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tilting at Edges&lt;/head&gt;
    &lt;p&gt;Terminology produces inconsistent results between executions. Our tests are designed to be deterministic, so these kinds of results suggest possible state corruption. Despite this issue, Terminology offers interesting visual effects that would be a welcome feature in other terminals.&lt;/p&gt;
    &lt;p&gt;iTerm2 reports "supported, but disabled, and cannot be changed" status for all DEC Private Modes queried, including fictional modes like 9876543. For this reason, the summary of DEC Private Modes shows only those modes that are changeable.&lt;/p&gt;
    &lt;p&gt;Konsole does not reply to queries about DEC Private modes, but does support several modes when they are enabled. For this reason, ucs-detect cannot automatically infer which DEC Modes Konsole supports.&lt;/p&gt;
    &lt;p&gt;Similarly, ucs-detect reports "No DEC Private Mode Support" for Contour. I investigated this discrepancy because Contour's author also authored a Mode 2027 specification dependent on this functionality. The issue was that Contour responded with a different mode number than the one queried. While developing a fix, Contour's latest release from December 2024 presented an additional complication: a bad escape key configuration. Each instance of being stuck in vi required typing CTRL + [ as a workaround!&lt;/p&gt;
    &lt;p&gt;Terminals based on libvte with software version label VTE/7600 continue to show identical performance with low scores in our tests, unchanged from 2023.&lt;/p&gt;
    &lt;p&gt;My attempt to discuss improving Unicode support in libvte received substantial criticism. However, recent libvte project issue Support Emoji Sequences is a positive indicator for improved language and Emoji support in 2026.&lt;/p&gt;
    &lt;head rend="h2"&gt;On Mode 2027&lt;/head&gt;
    &lt;p&gt;I included DEC Private Mode 2027 in the results to accompany Mitchell's table from his article, Grapheme Clusters and Terminal Emulators, and to verify for myself that it has limited utility.&lt;/p&gt;
    &lt;p&gt;In theory, a CLI program can query this mode to classify a terminal as "reasonably supporting" unicode, but not which specific features or version level. Since other terminals with similar capabilities do not respond to Mode 2027 queries, this binary indicator has limited utility.&lt;/p&gt;
    &lt;p&gt;The only practical approach to determining Unicode support of a terminal is to interactively test for specific features, codepoints, and at the Unicode version levels of interest, as ucs-detect does.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond Fixed Widths&lt;/head&gt;
    &lt;p&gt;Terminals cannot reproduce many of the world's languages legibly when constrained to monospace cells. The measurements dictated by rapidly expanding Unicode standards and varying implementation levels create inherent tension.&lt;/p&gt;
    &lt;p&gt;The text sizing protocol published early this year represents a significant development. Kovid Goyal describes the motivation in a recent interview:&lt;/p&gt;
    &lt;quote&gt;And then my next windmill that I'm looking at is variable-sized text in the terminal. So when I'm catting a markdown file, I want to see the headings big.&lt;/quote&gt;
    &lt;p&gt;While this feature may enable more advanced typesetting-like capabilities in terminal apps, it also promises to increase accessibility. Allowing text to escape monospace constraints enables legible support of the diverse set of the world's languages.&lt;/p&gt;
    &lt;p&gt;For example, using Contour with ucs-detect --stop-at-error=lang, stopping to take a look at a result of the language Kh√É¬ºn:&lt;/p&gt;
    &lt;p&gt;In this case Contour and Python wcwidth disagree on measurement, but more important is the legibility. We can compare this given Kh√É¬ºn text to the Kate editor:&lt;/p&gt;
    &lt;p&gt;They are clearly different. I regret I cannot study it more carefully, but I suggest that terminals could more easily display complex scripts by switching to a variable size text mode, allowing the font engine to drive the text without careful processing of cell and cursor movement.&lt;/p&gt;
    &lt;p&gt;Although I have yet to experiment with it, I am encouraged to see some resolution to this problem by the progressive changes suggested by the text sizing protocol.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffquast.com/post/state-of-terminal-emulation-2025/"/><published>2025-11-03T14:40:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45800308</id><title>Robert Hooke's "Cyberpunk‚Äù Letter to Gottfried Leibniz</title><updated>2025-11-03T23:09:54.478593+00:00</updated><content>&lt;doc fingerprint="2d2b9d15aca48677"&gt;
  &lt;main&gt;
    &lt;p&gt;Cyberpunk is a genre of science fiction about high tech, urban sprawl, and do-it-yourself counterculture. It‚Äôs usually associated with the early days of computer hackers and AI. This is the first in a series of blog posts about how high tech, urban sprawl, and do-it-yourself counterculture were just as much a part of the rapid progress of 17th century natural science as they were of the rapid progress of 20th century computer science; and about what we can learn by drawing this comparison.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"If I were to choose a patron saint for cybernetics... I should have to choose Leibniz"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;-Norbert Wiener&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"if we could find characters or signs appropriate for expressing all our thoughts as definitely and as exactly as arithmetic expresses numbers or geometric analysis expresses lines, we could in all subjects in so far as they are amenable to reasoning accomplish what is done in arithmetic and geometry."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;-Gottfried Leibniz&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"...especially in all those subjects where use of [such a language] may be free and where interest and authority do not intercept, the regular exercise thereof which I conceive to be the great antagonists which may impede its progress..."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;-Robert Hooke&lt;/p&gt;
    &lt;p&gt;The last quote is from an archival text which I've been trying to transcribe on and off for the past few months. It comes from a scan of a letter which Robert Hooke wrote to Gottfried Leibniz in 1681. I am fascinated by this letter for a couple of reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I was pleasantly surprised to learn that Hooke and Leibniz had exchanged any letters at all. I found the letter by chance, when browsing through the Royal Society's online archives.&lt;/item&gt;
      &lt;item&gt;The letter is about one of my favourite topics: Leibniz's project to create a universal language of science, which could be mechanically applied to automate any piece of scientific reasoning (apart from the collection of new experimental data).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For these reasons, this letter from Hooke to Leibniz has become one of my favourite pieces of niche archival material. (The other two are this popular science article which Alan Turing wrote to introduce computational undecidability to a general audience, and this speech which Ove Arup gave to his employees to reassure them that the new industrial computer his engineering firm had bought wasn't going to replace them in their jobs).&lt;/p&gt;
    &lt;p&gt;I think my transcription is about 90% accurate so far. Hooke's handwriting is quite hard to read. The original section quoted looks like this:&lt;/p&gt;
    &lt;p&gt;But from what I have transcribed, I think this letter is a particularly nice example of the originality and prescience of Hooke's way of thinking about the world. Almost as fascinating as the letter itself, are the events surrounding the time in which it was written. I hope to write more soon about Hooke's life, about his relationship with cryptography, and about the way in which he bridged the gap between technician and scientist.&lt;/p&gt;
    &lt;p&gt;A common thread I find myself drawing across much of Hooke's work - albeit anachronistically - is an early expression of the hacker mindset which flourished among computer scientists in the second half of the 20th century, coincided with the explosion of computing innovations that took place during that period, and came to be romanticised in cyberpunk science fiction. And I think that if I had to pick one piece of Hooke's writing that expresses this attitude most clearly, I would have to pick this letter. To explain why, I'll first talk a bit more about why Hooke was writing to Leibniz in the first place.&lt;/p&gt;
    &lt;p&gt;You likely know Robert Hooke from studying his laws about the motion of springs in high-school physics, for his role in the foundation of the Royal Society, or for his beef with Isaac Newton. You likely know about Gottfried Leibniz from hearing about his philosophy of monads, for his role in the invention of calculus, or for his beef with Isaac Newton.&lt;/p&gt;
    &lt;p&gt;It turns out that, aside from their common interest in antagonising Isaac Newton, Hooke and Leibniz also shared an interest in mechanising scientific reasoning through the invention of a universal language for science. Leibniz called his project the "Characteristica Universalis". The philosopher Norbert Wiener credited this idea as a precursor to his own notion of ‚Äúcybernetics‚Äù ‚Äì which, incidentally, is the word he coined from which we get the ‚Äúcyber‚Äù in "cyberpunk". One thing I took away from reading Wiener was that you can think of the Characteristica Universalis as a kind of proto computer programming language. Hooke liked Leibniz‚Äô ideas on this topic so much that he sent him the above letter just to say so.&lt;/p&gt;
    &lt;p&gt;What makes Hooke‚Äôs letter cyberpunk as opposed to just cybernetic is that it adds to Leibniz‚Äôs worldview an explicit (and perhaps naively optimistic) hope that individual freedom might be enabled by rather than stifled by the proliferation of this early programming language. In particular he saw the effect of a language for mechanised scientific reasoning as especially useful when used by individuals to express, explore and test ideas freely, without interference from unjust authorities who might seek to censor or interfere with their work. In 1681, Hooke was already imagining the countercultural edge of cybernetic systems.&lt;/p&gt;
    &lt;p&gt;This should not be surprising, given Hooke's own politically uneasy upbringing, his tendency to skip lectures at university in order to tinker (in Robert Boyle's lab) with designs for experiments and novel instruments that went on to occupy his scientific career, or his habit of falling out with the interfering authorities of his time.&lt;/p&gt;
    &lt;p&gt;If my reading of this letter is accurate then - just as Wiener calls Leibniz the patron saint of cybernetics, we should call Hooke the patron saint of cyberpunk.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mynamelowercase.com/blog/robert-hookes-cyberpunk-letter-to-gottfried-leibniz/"/><published>2025-11-03T15:45:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45800464</id><title>Ask HN: Who wants to be hired? (November 2025)</title><updated>2025-11-03T23:09:53.649945+00:00</updated><content>&lt;doc fingerprint="301182753412e0e7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Share your information if you are looking for work. Please use this format:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  Location:
  Remote:
  Willing to relocate:
  Technologies:
  R√©sum√©/CV:
  Email:
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Please only post if you are personally looking for work. Agencies, recruiters, job boards, and so on, are off topic here.&lt;/p&gt;
      &lt;p&gt;Readers: please only email these addresses to discuss work opportunities.&lt;/p&gt;
      &lt;p&gt;There's a site for searching these posts at https://www.wantstobehired.com.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45800464"/><published>2025-11-03T16:00:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45800465</id><title>Ask HN: Who is hiring? (November 2025)</title><updated>2025-11-03T23:09:52.645994+00:00</updated><content>&lt;doc fingerprint="3741985a3402e664"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company‚Äîno recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss this other fine thread: Who wants to be hired? https://news.ycombinator.com/item?id=45800464&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45800465"/><published>2025-11-03T16:00:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45800777</id><title>Learning to read Arthur Whitney's C to become smart (2024)</title><updated>2025-11-03T23:09:51.846218+00:00</updated><content>&lt;doc fingerprint="2eb9d195bc6fba9c"&gt;
  &lt;main&gt;
    &lt;p&gt;Burger.&lt;/p&gt;
    &lt;head rend="h3"&gt;it's not working&lt;/head&gt;
    &lt;head rend="h5"&gt;Written January 19, 2024&lt;/head&gt;
    &lt;p&gt;Arthur Whitney is an esteemed computer scientist who led the design on a few well-known pieces of software:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The A, K, and Q programming languages&lt;/item&gt;
      &lt;item&gt;kdb, a high-performance database built on K used in fintech&lt;/item&gt;
      &lt;item&gt;Shakti, which is like kdb but faster, designed for trillion-row datasets.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I've never even seen a trillion numbers, much less calculated them, but kdb is apparently a standard tool on Wall Street. They probably care about money, so I'll assume kdb does its job well. His languages take significantly after APL, which was a very popular language for similar applications before the invention of (qwerty) keyboards.&lt;/p&gt;
    &lt;p&gt;But I'm not here to talk about boring things like "using software to make incomprehensible amounts of money in finance" or "human beings and their careers", I'm here to talk about how a guy writes C code weird. For a very simple version of the programming language K, there's a publicly available interpreter he wrote in a few days using about 50 lines of C to show the basics of interpreter writing. This is the C (specifically the January 16, 2024 version #2):&lt;/p&gt;
    &lt;head rend="h2"&gt;a.h&lt;/head&gt;
    &lt;code&gt;typedef char*s,c;s Q=(s)128;&amp;#13;
#define _(e...) ({e;})&amp;#13;
#define x(a,e...) _(s x=a;e)&amp;#13;
#define $(a,b) if(a)b;else&amp;#13;
#define i(n,e) {int $n=n;int i=0;for(;i&amp;lt;$n;++i){e;}}&amp;#13;
&amp;#13;
#define Q(e) if(Q==(e))return Q;&amp;#13;
#define Qs(e,s) if(e)return err(__func__,s);&amp;#13;
#define Qr(e) Qs(e,"rank")&amp;#13;
#define Qd(e) Qs(e,"domain")&amp;#13;
#define Qz(e) Qs(e,"nyi")&amp;#13;
&amp;#13;
#define _s(f,e,x...) s f(x){return _(e);}&amp;#13;
#define _i(f,e) _s(f,e,c x)&amp;#13;
#define f(f,e)  _s(f,e,s x)&amp;#13;
#define F(f,e)  _s(f,e,s a,s x)&amp;#13;
&amp;#13;
#define ax (256&amp;gt;x)&amp;#13;
#define ix (c)x&amp;#13;
#define nx x[-1]&amp;#13;
#define xi x[i]&amp;#13;
&amp;#13;
#define aa x(a,ax)&amp;#13;
#define ia x(a,ix)&amp;#13;
#define na x(a,nx)&amp;#13;
&amp;#13;
#define oo w("oo\n")&lt;/code&gt;
    &lt;head rend="h2"&gt;a.c&lt;/head&gt;
    &lt;code&gt;#include"a.h"//fF[+-!#,@] atom/vector 1byte(int/token) clang-13 -Os -oa a.c -w &amp;#13;
#define r(n,e) _(s r=m(n);i(n,r[i]=e)r)&amp;#13;
f(w,write(1,ax?&amp;amp;x:x,ax?1:strlen(x));x)F(err,w(a);w((s)58);w(x);w((s)10);Q)&amp;#13;
_i(wi,s b[5];sprintf(b,"%d ",x);w(b);0)&amp;#13;
f(W,Q(x)$(ax,wi(ix))i(nx,wi(xi))w(10);x)&amp;#13;
&amp;#13;
f(srt,Qz(1)0)f(uni,Qz(1)0)F(Cut,Qz(1)0)F(Drp,Qz(1)0)_i(m,s a=malloc(1+x);*a++=x;a)&amp;#13;
#define A(c) ((s)memchr(a,c,na)?:a+na)-a&amp;#13;
#define g(a,v) ax?255&amp;amp;a:r(nx,v)&amp;#13;
f(not,g(!ix,!xi))f(sub,g(-ix,-xi))F(At,Qr(aa)g(a[ix],a[xi]))F(_A,Qr(aa)g(A(ix),A(xi)))&amp;#13;
f(ind,Qr(!ax)0&amp;gt;ix?r(-ix,-ix-1-i):r(ix,i))F(Ind,Qr(!aa)Qd(1&amp;gt;ia)g(ix%ia,xi%ia))&amp;#13;
#define G(f,o) F(f,ax?aa?255&amp;amp;ia o ix:Ltn==f?f(sub(x),sub(a)):f(x,a):r(nx,(aa?ia:a[i])o xi))&amp;#13;
G(Ltn,&amp;lt;)G(Eql,==)G(Not,!=)G(Sum,+)G(Prd,*)G(And,&amp;amp;)G(Or,|)&amp;#13;
f(cat,Qr(!ax)r(1,ix))F(Cat,a=aa?cat(a):a;x=ax?cat(x):x;s r=m(na+nx);memcpy(r+na,x,nx);memcpy(r,a,na))&amp;#13;
f(at,At(x,0))f(rev,Qr(ax)At(x,ind(255&amp;amp;-nx)))f(cnt,Qr(ax)nx)&amp;#13;
F(Tak,Qr(!aa||ax)Qd(0&amp;gt;ia||ia&amp;gt;nx)At(x,ind(a)))F(Sub,Sum(a,sub(x)))F(Mtn,Ltn(x,a))f(qz,Qz(1)0)&amp;#13;
#define v(e) ((strchr(V,e)?:V)-V)&amp;#13;
s U[26],V=" +-*&amp;amp;|&amp;lt;&amp;gt;=~!@?#_^,",&amp;#13;
(*f[])()={0,abs,sub,qz ,qz,rev,qz ,qz, qz ,not,ind,at,uni,cnt,qz ,srt,cat},&amp;#13;
(*F[])()={0,Sum,Sub,Prd,And,Or,Ltn,Mtn,Eql,Not,Ind,At,_A ,Tak,Drp,Cut,Cat};&amp;#13;
_i(n,10u&amp;gt;x-48?x-48:26u&amp;gt;x-97?U[x-97]:0)&amp;#13;
f(e,s z=x;c i=*z++;!*z?n(i):v(i)?x(e(z),Q(x)f[v(i)](x)):x(e(z+1),Q(x)58==*z?U[i-97]=x:_(c f=v(*z);Qd(!f)F[f](n(i),x))))&amp;#13;
int main(){c b[99];while(1)if(w(32),b[read(0,b,99)-1]=0,*b)58==b[1]?e(b):W(e(b));}&lt;/code&gt;
    &lt;p&gt;This is the entire interpreter, and this is apparently how he normally writes code. Opinions on his coding style are divided, though general consensus seems to be that it's incomprehensible. As daunting as it is, I figured I should give it a chance for a few reasons.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;As I work on larger and larger codebases, scrolling up and down to track information has become a more common annoyance. Whitney's talked about coding the way he does to avoid exactly that: he wants to keep his logic on one screen. Perhaps learning to read code like this could give me ideas on writing my own code more compactly.&lt;/item&gt;
      &lt;item&gt;In a Hacker News comments section, somebody asked "would you rather spend 10 days reading 100,000 lines of code, or 4 days reading 1000?", and that raises a good point. The complexity of the code is because even a simple interpreter is pretty complex. Writing it in 500 lines wouldn't make the complexity go away, it just spreads it out. Does writing in this more compact format feel more daunting because you're exposed to more of the complexity at once? I think so. Does showing it all at once actually help you understand the whole thing faster? I don't know.&lt;/item&gt;
      &lt;item&gt;Reading code has become a more important part of my job than writing it, so I should challenge my reading skills, regardless.&lt;/item&gt;
      &lt;item&gt;It confuses people, and that's basically the same as being smart.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So I'm going to go line by line and explain my understanding. I tried to use the notes provided in the repo only when I was stuck, which was a few times early on, but by the end I could understand it pretty well.&lt;/p&gt;
    &lt;head rend="h2"&gt;A reading of a.h&lt;/head&gt;
    &lt;code&gt;typedef char*s,c;&lt;/code&gt;
    &lt;p&gt;This already shows some funky C. It defines &lt;code&gt;s&lt;/code&gt; as &lt;code&gt;char *&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt; as &lt;code&gt;char&lt;/code&gt;, because the &lt;code&gt;*&lt;/code&gt; attaches to the name, not the type. It's an oddity of C syntax that I've never been a fan of. Otherwise this is pretty straight forward: &lt;code&gt;s&lt;/code&gt; is for string, and &lt;code&gt;c&lt;/code&gt; is for character.&lt;/p&gt;
    &lt;code&gt;s Q=(s)128;&lt;/code&gt;
    &lt;p&gt;Fuck. Shit. He assigned 128 to a string named &lt;code&gt;Q&lt;/code&gt;. What does it mean? &lt;code&gt;s&lt;/code&gt; is &lt;code&gt;char *&lt;/code&gt;. Why is Q a pointer to the address 128? I thought I must have misunderstood, and &lt;code&gt;s&lt;/code&gt; was actually a character or something, but it's clearly specified as &lt;code&gt;char *&lt;/code&gt;. &lt;code&gt;s&lt;/code&gt; is for string!&amp;#13;
I couldn't figure out the meaning, so I soon gave up and looked at the annotated code. The &lt;code&gt;char *&lt;/code&gt; is &lt;code&gt;unsigned long long&lt;/code&gt; in other versions, and they explain that the type is used for both integers and pointers. The code operates on vectors of 8-bit integers, either as ASCII or numbers, so it makes some sense to use &lt;code&gt;char *&lt;/code&gt; from a memory layout perspective, but I don't use pointers as integers very often.&lt;/p&gt;
    &lt;code&gt;#define _(e...) ({e;})&amp;#13;
#define x(a,e...) _(s x=a;e)&amp;#13;
#define $(a,b) if(a)b;else&amp;#13;
#define i(n,e) {int $n=n;int i=0;for(;i&amp;lt;$n;++i){e;}}&lt;/code&gt;
    &lt;p&gt;These are all pretty straight forward, with one subtle caveat I only realized from the annotated code. They're all macros to make common operations more compact: wrapping an expression in a block, defining a variable &lt;code&gt;x&lt;/code&gt; and using it, conditional statements, and running an expression &lt;code&gt;n&lt;/code&gt; times.&lt;/p&gt;
    &lt;p&gt;The subtle thing the annotations point out is the first macro, &lt;code&gt;({e;})&lt;/code&gt;. The parentheses around curly brackets make this a statement expression, a non-standard C extension that allows you to treat a block of statements as a single expression, if the last statement is an expression that provides a value. In other words, &lt;code&gt;int x = ({int a = func1(); int b = func2(a); a+b;});&lt;/code&gt; sets &lt;code&gt;x&lt;/code&gt; to whatever &lt;code&gt;a+b&lt;/code&gt; is. This is used everywhere in the code after this.&lt;/p&gt;
    &lt;code&gt;#define Q(e) if(Q==(e))return Q;&amp;#13;
#define Qs(e,s) if(e)return err(__func__,s);&amp;#13;
#define Qr(e) Qs(e,"rank")&amp;#13;
#define Qd(e) Qs(e,"domain")&amp;#13;
#define Qz(e) Qs(e,"nyi")&lt;/code&gt;
    &lt;p&gt;These are error macros using that mysterious &lt;code&gt;Q&lt;/code&gt; defined earlier. &lt;code&gt;Q&lt;/code&gt; seems to have been used to represent errors, possibly short for "Quit". The &lt;code&gt;Qr/d/z&lt;/code&gt; functions seem to be types of errors. I have no idea what "nyi" means (I figure it out later).&lt;/p&gt;
    &lt;code&gt;#define _s(f,e,x...) s f(x){return _(e);}&amp;#13;
#define _i(f,e) _s(f,e,c x)&amp;#13;
#define f(f,e)  _s(f,e,s x)&amp;#13;
#define F(f,e)  _s(f,e,s a,s x)&lt;/code&gt;
    &lt;p&gt;These replace function declarations, and we can see that &lt;code&gt;_&lt;/code&gt; macro being used to add an implicit return.&amp;#13;
&lt;code&gt;_s&lt;/code&gt; could be used like&lt;/p&gt;
    &lt;code&gt;_s(my_function, puts("I rock!!!"); x*5+e, s x, int e)&lt;/code&gt;
    &lt;p&gt;, which would create basically this standard C:&lt;/p&gt;
    &lt;code&gt;char *my_function(char *x, int e) {&amp;#13;
	puts("I rock!!!");&amp;#13;
	return x*5+e;&amp;#13;
}&lt;/code&gt;
    &lt;p&gt;All the macros except the base &lt;code&gt;_s&lt;/code&gt; also add implicit arguments like &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; and you bet it's hard to tell them apart.&lt;/p&gt;
    &lt;code&gt;#define ax (256&amp;gt;x)&lt;/code&gt;
    &lt;p&gt;This was another one that baffled me until I looked at the annotations. Remember how I said &lt;code&gt;s&lt;/code&gt; values were either integers or pointers? 256 is the cutoff value for these integers, which the annotations call atoms, so ax means "is &lt;code&gt;x&lt;/code&gt; an atom?"&lt;/p&gt;
    &lt;code&gt;#define ix (c)x&amp;#13;
#define nx x[-1]&amp;#13;
#define xi x[i]&lt;/code&gt;
    &lt;p&gt;These aren't too confusing. &lt;code&gt;ix&lt;/code&gt; casts &lt;code&gt;x&lt;/code&gt; to a &lt;code&gt;char&lt;/code&gt;. &lt;code&gt;nx&lt;/code&gt; implies &lt;code&gt;x&lt;/code&gt; is some sort of fat pointer, meaning there's probably a length at &lt;code&gt;x[-1]&lt;/code&gt;, but we'll see. &lt;code&gt;xi&lt;/code&gt; just indexes &lt;code&gt;x&lt;/code&gt; as a normal pointer, using our implicitly defined &lt;code&gt;i&lt;/code&gt; from the &lt;code&gt;i(...)&lt;/code&gt; macro.&lt;/p&gt;
    &lt;code&gt;#define aa x(a,ax)&amp;#13;
#define ia x(a,ix)&amp;#13;
#define na x(a,nx)&lt;/code&gt;
    &lt;p&gt;These copy &lt;code&gt;ax&lt;/code&gt;, &lt;code&gt;ix&lt;/code&gt;, and &lt;code&gt;nx&lt;/code&gt; respectively to work on the &lt;code&gt;a&lt;/code&gt; variable, which is an implicit argument in functions defined using the &lt;code&gt;F(f,e)&lt;/code&gt; macro. You remembered the &lt;code&gt;x(name, expression)&lt;/code&gt; macro for assigning to a locally-scoped &lt;code&gt;x&lt;/code&gt;, right?&lt;/p&gt;
    &lt;code&gt;#define oo w("oo\n")&lt;/code&gt;
    &lt;p&gt;It prints &lt;code&gt;oo&lt;/code&gt;. It's not used anywhere.&lt;/p&gt;
    &lt;head rend="h2"&gt;a reading of a.c&lt;/head&gt;
    &lt;p&gt;I wound up not needing to refer to the annotated code at all to understand this. The C code is mostly using everything in the headers to build the interpreter.&lt;/p&gt;
    &lt;code&gt;#define r(n,e) _(s r=m(n);i(n,r[i]=e)r)&lt;/code&gt;
    &lt;p&gt;We create a vector &lt;code&gt;r&lt;/code&gt; from &lt;code&gt;m(n)&lt;/code&gt; (which is defined later (it's malloc)), fill &lt;code&gt;r&lt;/code&gt; with the results of &lt;code&gt;e&lt;/code&gt;, and return it out of the statement expression.&lt;/p&gt;
    &lt;code&gt;f(w,write(1,ax?&amp;amp;x:x,ax?1:strlen(x));x)&lt;/code&gt;
    &lt;p&gt;This defines &lt;code&gt;s w(s x)&lt;/code&gt;, which is our print function. If &lt;code&gt;x&lt;/code&gt; is an atom (&lt;code&gt;ax?&lt;/code&gt;), we print it as a single character by getting its address (&lt;code&gt;&amp;amp;x&lt;/code&gt;) and providing a length of 1. If it's a vector, we print it as a string using &lt;code&gt;strlen&lt;/code&gt; to calculate how long it is, so now we also know vectors must be null-terminated here.&amp;#13;
&lt;code&gt;write&lt;/code&gt; and &lt;code&gt;strlen&lt;/code&gt; are standard functions that we call without including the headers, because fuck headers. Let the linker figure it out.&lt;/p&gt;
    &lt;code&gt;F(err,w(a);w((s)58);w(x);w((s)10);Q)&lt;/code&gt;
    &lt;p&gt;Our fancy shmancy error printing function, &lt;code&gt;s err(s a, s x)&lt;/code&gt;. The confusing thing is that &lt;code&gt;:&lt;/code&gt; and the newline are represented by their ASCII numbers 58 and 10, respectively. This just prints a message in the format &lt;code&gt;{a}:{x}\n&lt;/code&gt; and returns our special error value &lt;code&gt;Q&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;_i(wi,s b[5];sprintf(b,"%d ",x);w(b);0)&lt;/code&gt;
    &lt;p&gt;Defines &lt;code&gt;s wi(c x)&lt;/code&gt;, which takes &lt;code&gt;x&lt;/code&gt; as a &lt;code&gt;char&lt;/code&gt;, formats it as an integer in up to &lt;code&gt;5*sizeof(char*)/sizeof(char)&lt;/code&gt; characters (40 on 64-bit machines), and writes that.&lt;/p&gt;
    &lt;code&gt;f(W,Q(x)$(ax,wi(ix))i(nx,wi(xi))w(10);x)&lt;/code&gt;
    &lt;p&gt;Another print function, &lt;code&gt;s W(s x)&lt;/code&gt; either writes &lt;code&gt;x&lt;/code&gt; as an integer or a list of integers. It also refuses to print the &lt;code&gt;Q&lt;/code&gt; vector.&lt;/p&gt;
    &lt;code&gt;f(srt,Qz(1)0) f(uni,Qz(1)0) F(Cut,Qz(1)0) F(Drp,Qz(1)0)&lt;/code&gt;
    &lt;p&gt;I figured out what &lt;code&gt;nyi&lt;/code&gt; means! It means "Not yet implemented", as we can see from these function definitions.&lt;/p&gt;
    &lt;code&gt;_i(m,s a=malloc(1+x);*a++=x;a)&lt;/code&gt;
    &lt;p&gt;And we find our previously-used function &lt;code&gt;s m(c x)&lt;/code&gt;, which allocates our buffer and returns a fat pointer (with the size at &lt;code&gt;x[-1]&lt;/code&gt;, hence the &lt;code&gt;1+x&lt;/code&gt; and &lt;code&gt;a++&lt;/code&gt;). &lt;code&gt;x&lt;/code&gt; is the length we're allocating here, which means our vectors are limited to 255 bytes. The repo suggests upgrading capacity as an exercise to the reader, which could be fun.&lt;/p&gt;
    &lt;code&gt;#define A(c) ((s)memchr(a,c,na)?:a+na)-a&lt;/code&gt;
    &lt;p&gt;This macro finds the first occurence of the character &lt;code&gt;c&lt;/code&gt; in our vector &lt;code&gt;a&lt;/code&gt; as an index into the string (hence the &lt;code&gt;-a&lt;/code&gt;, since &lt;code&gt;memchr&lt;/code&gt; returns a pointer). If the result is null, it just returns the length of the string (&lt;code&gt;a+na - a&lt;/code&gt;). We see another fun bit of non-standard syntax, &lt;code&gt;?:&lt;/code&gt;, which I had to look up. &lt;code&gt;a ?: b&lt;/code&gt; is equivalent to &lt;code&gt;a ? a : b&lt;/code&gt; without evaluating &lt;code&gt;a&lt;/code&gt; twice. Pretty snazzy!&lt;/p&gt;
    &lt;code&gt;#define g(a,v) ax?255&amp;amp;a:r(nx,v)&lt;/code&gt;
    &lt;p&gt;Strange little operation, I'll have to see it in action. If &lt;code&gt;x&lt;/code&gt; is an atom, it clamps &lt;code&gt;a&lt;/code&gt; to be an atom with a simple mask (&lt;code&gt;255&amp;amp;a&lt;/code&gt;), otherwise it creates a new vector the same size as &lt;code&gt;x&lt;/code&gt; filled with the result from &lt;code&gt;v&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;f(not,g(!ix,!xi)) f(sub,g(-ix,-xi)) F(At,Qr(aa)g(a[ix],a[xi])) F(_A,Qr(aa)g(A(ix),A(xi)))&lt;/code&gt;
    &lt;p&gt;Ah, I see now. &lt;code&gt;g(a, v)&lt;/code&gt; lets us define functions that work on both atoms and vectors. If &lt;code&gt;x&lt;/code&gt; is an atom, it returns the atom result clamped within the correct bounds. Otherwise it allocates a new vector and computes the other expression.&amp;#13;
All the above functions work either on &lt;code&gt;x&lt;/code&gt; as an integer (&lt;code&gt;ix&lt;/code&gt;), or on every element of &lt;code&gt;x&lt;/code&gt; (&lt;code&gt;xi&lt;/code&gt;).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;s not(s x)&lt;/code&gt;does boolean negation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;s sub(s x)&lt;/code&gt;does arithmetic negation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;s At(s a, s x)&lt;/code&gt;indexes into&lt;code&gt;a&lt;/code&gt;, either to get one value or to shuffle them into a new vector.&lt;code&gt;a&lt;/code&gt;has to be a vector.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;s _A(s a, s x)&lt;/code&gt;searches a vector&lt;code&gt;a&lt;/code&gt;for the value of&lt;code&gt;x&lt;/code&gt;and gives us the index, either one value or every value in the vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a lot of functionality in such a small bit of code.&lt;/p&gt;
    &lt;code&gt;f(ind,Qr(!ax)0&amp;gt;ix?r(-ix,-ix-1-i):r(ix,i))&amp;#13;
F(Ind,Qr(!aa)Qd(1&amp;gt;ia)g(ix%ia,xi%ia))&lt;/code&gt;
    &lt;p&gt;These are some atom-only functions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;s ind(s x)&lt;/code&gt;creates a vector of length |x| containing&lt;code&gt;0, 1... x-1&lt;/code&gt;if&lt;code&gt;x&lt;/code&gt;is positive, otherwise it contains&lt;code&gt;-x-1, -x-2... 0&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;s Ind(s a, s x)&lt;/code&gt;does&lt;code&gt;x&lt;/code&gt;modulo&lt;code&gt;a&lt;/code&gt;, either on&lt;code&gt;x&lt;/code&gt;as an integer or every value of the vector&lt;code&gt;x&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Honestly, I can buy that this method of coding produces fewer bugs, once you can actually write it, since you work only on small building blocks of the logic and reuse them. Like, where could a bug for &lt;code&gt;ind&lt;/code&gt; even be? Maybe an off-by-one in &lt;code&gt;-ix-1-i&lt;/code&gt;, but it's hard to miss what's happening once you can see the trees through the forest.&lt;/p&gt;
    &lt;code&gt;#define G(f,o) F(f,ax?aa?255&amp;amp;ia o ix:Ltn==f?f(sub(x),sub(a)):f(x,a):r(nx,(aa?ia:a[i])o xi))&lt;/code&gt;
    &lt;p&gt;That's too many trees! I can't understand this many nested ternary operators at the same time because I'm not the alien from Arrival. I process things in linear time. I have to chunk this up.&lt;/p&gt;
    &lt;code&gt;F(f, ax ?&amp;#13;
		aa ?&amp;#13;
			255 &amp;amp; ia o ix&amp;#13;
			: Ltn==f ? &amp;#13;
				f(sub(x),sub(a))&amp;#13;
				: f(x,a)&amp;#13;
		: r(nx,(aa?ia:a[i])o xi))&lt;/code&gt;
    &lt;p&gt;Okay, I see. It's 3 cases from 2 conditions: &lt;code&gt;x&lt;/code&gt; is an atom or a vector, and &lt;code&gt;a&lt;/code&gt; is an atom or a vector.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;x&lt;/code&gt;and&lt;code&gt;a&lt;/code&gt;are atoms: apply some operator&lt;code&gt;o&lt;/code&gt;to both values and clamp to 8 bits. I also didn't realize the bitwize and (&lt;code&gt;&amp;amp;&lt;/code&gt;) had a lower precedence than the operators this macro is used on, meaning it's always&lt;code&gt;`C 255 &amp;amp; (ia o ix)&lt;/code&gt;`.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;x&lt;/code&gt;is an atom and&lt;code&gt;a&lt;/code&gt;is a vector: run this function with the arguments swapped. If the function is&lt;code&gt;Ltn&lt;/code&gt;, also negate the arguments, since less-than depends on argument order.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;x&lt;/code&gt;is a vector: create a new vector, either applying&lt;code&gt;a&lt;/code&gt;or each value of&lt;code&gt;a&lt;/code&gt;to&lt;code&gt;x&lt;/code&gt;using the operator. It assumes vector&lt;code&gt;a&lt;/code&gt;is at least as long as&lt;code&gt;x&lt;/code&gt;, or it'll index past the end of&lt;code&gt;a&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;G(Ltn,&amp;lt;)G(Eql,==)G(Not,!=)G(Sum,+)G(Prd,*)G(And,&amp;amp;)G(Or,|)&lt;/code&gt;
    &lt;p&gt;Using our fancy new macro, we quickly define seven new functions for the vectors, where they're all element-wise applications of binary operators.&lt;/p&gt;
    &lt;code&gt;f(cat,Qr(!ax)r(1,ix)) F(Cat,a=aa?cat(a):a;x=ax?cat(x):x;s r=m(na+nx);memcpy(r+na,x,nx);memcpy(r,a,na))&lt;/code&gt;
    &lt;p&gt;I was confused by the first function, but I see now these are &lt;code&gt;cat&lt;/code&gt; as in "concatenate". For an atom, it creates a vector of length 1 containing that atom. &lt;code&gt;Cat&lt;/code&gt; does the more complex work of joining two vectors together, running &lt;code&gt;cat&lt;/code&gt; on each value if it's an atom to get a list.&lt;/p&gt;
    &lt;code&gt;f(at,At(x,0)) f(rev,Qr(ax)At(x,ind(255&amp;amp;-nx))) f(cnt,Qr(ax)nx)&lt;/code&gt;
    &lt;p&gt;Some more simple functions. Lil &lt;code&gt;at&lt;/code&gt; gets the first item of &lt;code&gt;x&lt;/code&gt;; &lt;code&gt;rev&lt;/code&gt; reverses the list using our &lt;code&gt;ind&lt;/code&gt; function to generate the list of indeces in reverse, which is a little overkill but vectors are 256 bytes at max and memory is never freed so who cares; and &lt;code&gt;cnt&lt;/code&gt; gets the size of &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;F(Tak,Qr(!aa||ax)Qd(0&amp;gt;ia||ia&amp;gt;nx)At(x,ind(a))) F(Sub,Sum(a,sub(x))) F(Mtn,Ltn(x,a)) f(qz,Qz(1)0)&lt;/code&gt;
    &lt;p&gt;Some more simple functions. &lt;code&gt;Tak&lt;/code&gt; returns the first &lt;code&gt;a&lt;/code&gt; characters from the vector &lt;code&gt;x&lt;/code&gt; as a new list; &lt;code&gt;Sub&lt;/code&gt; subtracts; &lt;code&gt;Mtn&lt;/code&gt; is "more than"; and &lt;code&gt;qz&lt;/code&gt; returns our "not yet implemented" error.&lt;/p&gt;
    &lt;code&gt;#define v(e) ((strchr(V,e)?:V)-V)&lt;/code&gt;
    &lt;p&gt;A shorthand to get the first occurence of a character from a string &lt;code&gt;V&lt;/code&gt;, returning an offset into the array or zero if it's not present. This seems ambiguous, since that's also the result if we match with the first character, but we'll see.&lt;/p&gt;
    &lt;code&gt;s U[26],V=" +-*&amp;amp;|&amp;lt;&amp;gt;=~!@?#_^,",&amp;#13;
(*f[])()={0,abs,sub,qz ,qz,rev,qz ,qz, qz ,not,ind,at,uni,cnt,qz ,srt,cat},&amp;#13;
(*F[])()={0,Sum,Sub,Prd,And,Or,Ltn,Mtn,Eql,Not,Ind,At,_A ,Tak,Drp,Cut,Cat};&lt;/code&gt;
    &lt;p&gt;Ah, we have seen. The first character of &lt;code&gt;V&lt;/code&gt; is a space, and it looks like the arrays of function pointers match up with the characters of &lt;code&gt;V&lt;/code&gt; to give us our functions, with space being null. However, I think &lt;code&gt;abs&lt;/code&gt; is from the standard library here, since it's not defined anywhere else? That seems like a bug. It'd work for atoms, but it'd break vectors.&amp;#13;
This also defines an array of 26 vectors, which I assume will be our variables. &lt;/p&gt;
    &lt;code&gt;_i(n, 10u&amp;gt;x-48? x-48 : 26u&amp;gt;x-97? U[x-97] : 0)&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;s n(c x)&lt;/code&gt; reads a char and returns one of several things. I'll have to consult an ASCII table.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;code&gt;x&lt;/code&gt;is between 48 and 57 (inclusive), we subtract 48 and return that. Meaning, if&lt;code&gt;x&lt;/code&gt;is an ASCII character representing 0-9, we subtract 48 so it's the integer 0-9, rather than the character. It's phrased strangely,&lt;code&gt;10u &amp;gt; x-48&lt;/code&gt;, instead of the more obvious&lt;code&gt;x&amp;gt;47&amp;amp;&amp;amp;58&amp;gt;x&lt;/code&gt;. Maybe it's because it's two characters shorter. Maybe Arthur wanted to show the length of the span of characters (10) more than the start and end, which this does once you understand it. Maybe he just thought the underflow trickery was neat.&lt;/item&gt;
      &lt;item&gt;If &lt;code&gt;x&lt;/code&gt;is between 97 and 122 (inclusive), it's a lowercase character of the alphabet in ASCII, in which case the function returns one of our variables from the&lt;code&gt;U&lt;/code&gt;array, mapping 'a' to 'z'.&lt;/item&gt;
      &lt;item&gt;Otherwise, the function returns 0.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So it looks like this function is specifically to read values, either numerals or variables, all of which are one character.&lt;/p&gt;
    &lt;code&gt;f(e,s z=x;c i=*z++;!*z?n(i):v(i)?x(e(z),Q(x)f[v(i)](x)):x(e(z+1),Q(x)58==*z?U[i-97]=x:_(c f=v(*z);Qd(!f)F[f](n(i),x))))&lt;/code&gt;
    &lt;p&gt;Uh, let's spread this out a little.&lt;/p&gt;
    &lt;code&gt;f(e, s z=x; c i=*z++; !*z ? n(i)&amp;#13;
	: v(i) ? x(e(z), Q(x) f[v(i)](x))&amp;#13;
		: x(e(z+1), Q(x) 58==*z ? U[i-97]=x &amp;#13;
			: _(c f=v(*z); Qd(!f) F[f](n(i),x))))&lt;/code&gt;
    &lt;p&gt;Okay, easy. It's a recursive function that checks each character of vector &lt;code&gt;x&lt;/code&gt;, called &lt;code&gt;i&lt;/code&gt;.&amp;#13;
If we're at the end of the string, we check if &lt;code&gt;i&lt;/code&gt; is a value and return it.&amp;#13;
Otherwise, we first check if it's an operator (from our &lt;code&gt;V&lt;/code&gt; string). If it is, we evaluate the rest of the string, check for errors, and then apply the operation to the result from the rest of the evaluation.&amp;#13;
If it wasn't an operation, we evaluate the rest of the string, skipping one character. If the skipped character is a colon (ASCII 58), we assign the result of the evaluation to one of the slots in &lt;code&gt;U&lt;/code&gt; (if the character &lt;code&gt;i&lt;/code&gt; is not a lowercase ASCII letter, this will corrupt memory, so don't write bugs).&lt;/p&gt;
    &lt;p&gt;I'm pretty sure spaces are a syntax error in every location, and I don't see code to create array literals. If the skipped character is an operator, we instead apply that binary operator on the evaluation result and &lt;code&gt;n(i)&lt;/code&gt;, which is either a variable or a number.&amp;#13;
This means code is executed from right to left, with no operator precedent, which is standard for APL-type languages from what I understand.&amp;#13;
Because this language has only single-character variable names, numbers, and operators, this is all the tokenizing, parsing, and evaluation we need.&lt;/p&gt;
    &lt;code&gt;C int main(){c b[99]; while(1) if(w(32),b[read(0,b,99)-1]=0,*b) 58==b[1] ? e(b) : W(e(b));}&lt;/code&gt;
    &lt;p&gt;And finally, &lt;code&gt;main&lt;/code&gt;.&amp;#13;
In an infinite loop, we read up to 99 characters from a user, and then write the evaluated result of that text.&lt;/p&gt;
    &lt;p&gt;Et voila. We have a tiny interpreter for a simple array language. It's not exactly production-ready, but it does quite a lot in its tiny footprint.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;I think I can say I understand this code pretty well, even more than most code I read. I don't know how much of that is because of the coding style, or because I spent eight hours writing several thousand words about fifty lines of code in a borderline-delusional fugue state brought on by drinking one small Starbucks‚Ñ¢ Frapuccino¬Æ (Mocha¬Æ Flavored*) I bought from a gas station at 10 PM on a Thursday.&lt;/p&gt;
    &lt;p&gt;I had some fun dreams about macros with one-character names applying operations on scalars and vectors that morning (I went to sleep at 6:40 AM).&lt;/p&gt;
    &lt;p&gt;All in all, it was a fun exercise. To summarize my thoughts:&lt;/p&gt;
    &lt;head rend="h3"&gt;Good ideas&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Well-considered primitives. I've read and written a lot of macro-ridden C, but this feels like a proper little language designed with composable, useful macros that removed enough repetition to make common operations, like iteration, easy to decipher. In other languages, higher-order functions and similar constructs could be used the same way.&lt;/item&gt;
      &lt;item&gt;Fewer lines. I didn't have to scroll so much when I forgot what a macro or a function did! I have a wide screen monitor, I don't need lines limited to 10-20 characters of actual code most of the time. If people can read English in compact paragraphs, why not code?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Bad ideas&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-semantic types. I was completely baffled by the &lt;code&gt;char *&lt;/code&gt;being treated as an integer at first, and simply assumed I was misunderstanding the code somehow until I checked the annotated version. In my own code, I use types as one of the core building blocks. What the data is defines how I use it. For a full interpreter, this would be a custom type anyway, since right now it assumes&lt;code&gt;malloc&lt;/code&gt;will never give it a pointer to an address less than 256, which is probably true but not guaranteed, and also the integer 128 is reserved for invalid results, which is probably the bigger limitation.&lt;/item&gt;
      &lt;item&gt;Code golf. I can understand the large gains to density like macros, and even the medium gains like short names, but there's a point where the code becomes significantly harder to follow for very minor gains, such as the use of ASCII codes like &lt;code&gt;58&lt;/code&gt;instead of character literals like&lt;code&gt;':'&lt;/code&gt;, or the use of&lt;code&gt;10u&amp;gt;x-48&lt;/code&gt;instead of&lt;code&gt;x&amp;gt;48&amp;amp;&amp;amp;58&amp;gt;x&lt;/code&gt;for checking if&lt;code&gt;x&lt;/code&gt;is within a range. And while I think more code can be put on a line, I'm not throwing out my space key just yet, epecially between function declarations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ideas I'm ambivalent about&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-standard syntax. There's some very interesting features in this code that utilize GCC-specific extensions, like &lt;code&gt;a ?: b&lt;/code&gt;ternaries and statement expressions. I don't typically need to compile for everything on earth, so learning the tricks of one reasonably cross-platform compiler isn't a bad idea. At the same time, even using clang, the same compiler Arthur was using, I had to include&lt;code&gt;stdio.h&lt;/code&gt;due to not linking&lt;code&gt;sprintf&lt;/code&gt;otherwise. If I wanted to build using Visual Studio I'd just have to rewrite a bunch of stuff. Also, compiling without&lt;code&gt;-w&lt;/code&gt;generates almost three compiler warnings per line of code. Those are useful sometimes, if they aren't flooded!&lt;/item&gt;
      &lt;item&gt;Implicit arguments. Many of the density gains in this code come from using the variables &lt;code&gt;x&lt;/code&gt;,&lt;code&gt;a&lt;/code&gt;, and&lt;code&gt;i&lt;/code&gt;in nearly every context, allowing macros to use them by default and skip listing other parameters. I didn't find it to be a problem after a brief adjustment, but it's also a very small codebase. Implicit arguments are a feature in many dynamically typed languages, and in "point-free" or "tacit" programming, but it's fallen out of style due to its difficulty to parse at first glance. Whitney's languages all being based around tacit programming is surely an influence on his tendency to make arguments implicit.&lt;/item&gt;
      &lt;item&gt;Short names. Beyond the very first introduction, there'd be virtually no benefit to renaming &lt;code&gt;ax&lt;/code&gt;as&lt;code&gt;vector_is_atom(x)&lt;/code&gt;,&lt;code&gt;_(...)&lt;/code&gt;as 'execute(...)', or most other primitives after they're introduced. Most are small enough that you can parse what it does right away. However, this doesn't lend any signal as to why it does. You have to figure out what it does from context. For code that's meant to be read by another person, there should probably be some explanation as to why a primitive does what it does, even if it has a short name. This is especially true of more complex operations, like the evaluation function&lt;code&gt;e&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Nested ternary operators. Too confusing for me to follow without parentheses, especially without whitespace.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This coding style feels most appropriate for "done" code, or code that's been worked out on paper or some other environment and is now being written down in a compact way. It's more like finalized mathematical notation than typical code. I don't see myself being able to effectively write code like this, since I tend to quickly jot down ideas, compile and run to validate, and edit what I've done based on the results. Making a small set of good primitives and building heavily on those requires basically having solved the problem before writing a single line. Otherwise you get bad primitives that cause more confusion than help, and adjusting those primitives would involve rewriting all the code that depends on them, which is all the code.&lt;/p&gt;
    &lt;p&gt;But I think that's my key takeaway: I tend to work out problems in the code, which can lead to messy results. I write the dumbest possible solution, and then have to try and refactor as I develop a better mental model of the problem. What I like most about this code isn't how few characters are used, or everything fitting in one screen. I like how it seems the code was well-understood before it was written. You can't refactor a 500-line jumble of C into code like this.&lt;/p&gt;
    &lt;p&gt;The real lesson is that I'm probably too quick to jump into coding things. I could spend more time working out how I want to model a problem in a more free-form way, like writing notation on paper, before jumping into the rigid world of programming syntax. With a clear mental model, I can then write code in terms of how I was thinking about the problem, instead of thinking about the problem in terms of how I wrote the code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next time&lt;/head&gt;
    &lt;p&gt;I think a fun exercise would be to extend this interpreter while maintaining its code style, to see how I fare actually working with it. The repository this came from has various suggested exercises, but my ideas would be adding the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swapping the bytes for float vectors&lt;/item&gt;
      &lt;item&gt;Vectors longer than 255 values&lt;/item&gt;
      &lt;item&gt;Multi-character numbers&lt;/item&gt;
      &lt;item&gt;Array literals (you can do it with comma, but that runs &lt;code&gt;cat&lt;/code&gt;on every value, which isn't ideal)&lt;/item&gt;
      &lt;item&gt;Whitespace insensitivity&lt;/item&gt;
      &lt;item&gt;Memory management&lt;/item&gt;
      &lt;item&gt;The unimplemented functions&lt;/item&gt;
      &lt;item&gt;Multi-character variable names&lt;/item&gt;
      &lt;item&gt;Multi-character operators&lt;/item&gt;
      &lt;item&gt;Indication for syntax errors&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://needleful.net/blog/2024/01/arthur_whitney.html"/><published>2025-11-03T16:23:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45800955</id><title>Why we migrated from Python to Node.js</title><updated>2025-11-03T23:09:51.286387+00:00</updated><content>&lt;doc fingerprint="26f311d55ce5b00f"&gt;
  &lt;main&gt;
    &lt;p&gt;We just did something crazy: we completely rewrote our backend from Python to Node just one week after our launch.&lt;/p&gt;
    &lt;p&gt;We did this so we can scale. Yes, scale. A week in.&lt;/p&gt;
    &lt;p&gt;In some ways, it's a good time right? The codebase is still small and we don't have too many users.&lt;/p&gt;
    &lt;p&gt;But on the other hand, it goes completely against the advice given to early-stage startups which is to just ship and sell, and worry about scale once you've hit product-market-fit. "Do things that don't scale", as PG put it.&lt;/p&gt;
    &lt;p&gt;You see, we didn't have a magical launch week that flooded us with users and force us to scale. And generally you can expect that any stack you pick should be able to scale reasonably well for a long time until you actually get to the point where you should consider changing frameworks or rewriting your backend in a different language (read: Rust).&lt;/p&gt;
    &lt;p&gt;So why do it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Python async sucks&lt;/head&gt;
    &lt;p&gt;I'm a big fan of Django. I was introduced to it at PostHog and it's become my go-to backend for most projects since. It gets you off the ground really fast, has great tooling and abstractions, and is still flexible enough to tweak to your needs.&lt;/p&gt;
    &lt;p&gt;So naturally, when I started writing our backend at Skald, I started us off with Django too.&lt;/p&gt;
    &lt;p&gt;Now, we make a lot of calls to LLM and embedding APIs at Skald, so we're generally doing a lot of network I/O that we'd like to be async. Not only that, we often want to fire a lot of requests concurrently, such as when need to generate vector embeddings for the various chunks of a document.&lt;/p&gt;
    &lt;p&gt;And things quickly got really messy in Django.&lt;/p&gt;
    &lt;p&gt;I'll preface this by saying that neither of us has a lot of experience writing Python async code (I've mostly worked on async-heavy services in Node) but I think this is partly the point here: it's really hard and unintuitive to write solid and performant Python async code. You need to go deep into the foundations of everything in order to be able to do so.&lt;/p&gt;
    &lt;p&gt;I'm actually really interested in spending proper time in becoming more knowledgeable with Python async, but in our context you a) lose precious time that you need to use to ship as an early-stage startup and b) can shoot yourself in the foot very easily in the process.&lt;/p&gt;
    &lt;p&gt;Nevertheless, I thought I was to blame. "Bad programmer! Bad programmer!" was what I was hearing in my head as I tried to grasp everything. But while more knowledgeable folks would certainly have a better time, we discovered that the foundations of Python async are actually a bit shaky too.&lt;/p&gt;
    &lt;p&gt;Unlike JavaScript, which had the event loop from the beginning, and Go, that created the concept of goroutines (both concurrency models that I quite like and have used in production), Python async support was patched on later, and that's where the difficulty lies.&lt;/p&gt;
    &lt;p&gt;Two blog posts that cover this really well are "Python has had async for 10 years -- why isn't it more popular?" and "Python concurrency: gevent had it right", both conveniently published not long before I started digging into all this.&lt;/p&gt;
    &lt;p&gt;As for us, we learned a few things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python doesn't have native async file I/O.&lt;/item&gt;
      &lt;item&gt;Django still doesn't have full async support. Async in the ORM is not done yet and the colored functions problem really shines here. You can technically use Django with async, but their docs on this have so many caveats that it should scare anyone.&lt;/item&gt;
      &lt;item&gt;You gotta write &lt;code&gt;sync_to_async&lt;/code&gt;and&lt;code&gt;async_to_sync&lt;/code&gt;everywhere.&lt;/item&gt;
      &lt;item&gt;All sorts of models have emerged to bring better async support to different parts of the Python ecosystem, but as they're not native they have their own caveats. For instance, aiofiles brings async API-compatible file operations but uses a thread pool under the hood, and Gevent with its greenlets is pretty cool but it literally patches the stdlib in order to work.&lt;/item&gt;
      &lt;item&gt;Due to a lot of async support in Python relying on layers that sit on top of the language rather than being native, you need to be careful about the async code you write as it will have different implications depending on e.g. the Gunicorn worker type you run (good luck learning much about those from the Gunicorn docs, btw).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Overall, just getting an equivalent of &lt;code&gt;Promise.all&lt;/code&gt; to work, while understanding all of its gotchas was not simple at all.&lt;/p&gt;
    &lt;p&gt;Faced with this, I went into the PostHog codebase.&lt;/p&gt;
    &lt;p&gt;I worked at PostHog for three years and we had no async in the Django codebase back then but they're a massive company and they have AI features now so they must have figured this out!&lt;/p&gt;
    &lt;p&gt;And what I realized was that they're still running WSGI (not ASGI) with Gunicorn Gthread workers (where the max concurrent requests you're able to handle is usually max 4x CPU cores), thus not getting much benefit from running things async. The codebase also has a lot of utils to make async work properly, like their own implementation of &lt;code&gt;async_to_sync&lt;/code&gt;. So I guess way they're handling a lot of load is probably just horizontal scaling.&lt;/p&gt;
    &lt;p&gt;There's simply no great way to run async in Django.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ok, now what?&lt;/head&gt;
    &lt;p&gt;We essentially concluded that Django was going to hurt us really soon, not just when we started to have a lot of load.&lt;/p&gt;
    &lt;p&gt;Without too many users we'd already need to start running multiple machines in order to not have terrible latency, plus we'd be writing clunky code that would be hard to maintain.&lt;/p&gt;
    &lt;p&gt;We could of course just "do things that don't scale" for now and just solve the problem with money (or AWS credits), but it didn't feel right. And being so early would make the migration to another framework much easier.&lt;/p&gt;
    &lt;p&gt;At this point, some people are probably screaming at their screens going: "just use FastAPI!" -- and we did indeed consider it.&lt;/p&gt;
    &lt;p&gt;FastAPI does have proper async support and is quite a loved framework said to be performant. And if you want an ORM with it you could use SQLAlchemy which also supports async.&lt;/p&gt;
    &lt;p&gt;Migrating to FastAPI would have probably saved us a day or two (our migration took 3 days) due to being able to reuse a lot of code without translating it, but at this point we weren't feeling great about the Python async ecosystem overall, and we had actually already written our background worker service in Node, so we thought it would be a good opportunity to go all-in on one ecosystem.&lt;/p&gt;
    &lt;p&gt;And so migrate to Node we did. We took a little time picking the framework + ORM combo but settled on Express + MikroORM.&lt;/p&gt;
    &lt;p&gt;Yeah sure Express is old but it's battle-tested and feels familiar. Coming over to the JS event loop was the main point of all this anyway.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we gained, what we lost&lt;/head&gt;
    &lt;head rend="h3"&gt;Gained: Efficiency&lt;/head&gt;
    &lt;p&gt;Our initial benchmarks show we've gained ~3x throughput out of the box and that's just with us running what is mostly sequential code in an async context. Being over on Node now, we're planning on doing a lot concurrent processing when chunking, embedding, reranking, and so on. This means this change should have an even greater payoff over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lost: Django&lt;/head&gt;
    &lt;p&gt;Losing Django hurts, and we've already found ourselves building a lot more middleware and utilities ourselves on the Express side. Adonis exists, which is a more fully-featured Node framework, but moving to a whole new ecosystem felt like more work to us than just using something minimal.&lt;/p&gt;
    &lt;p&gt;What I'm missing the most is the ORM, which in my opinion is really ergonomic. And while you always have to be careful with ORMs when looking to extract the best possible performance, the Django ORM does do some nice things under the hood in order to make it performant enough to write queries in Python, and I learned a bit more about this when migrating our Django models over to MikroORM entities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gained: MikroORM&lt;/head&gt;
    &lt;p&gt;MikroORM was a consolation prize in this whole migration. I still much prefer the Django ORM but at the same time different ecosystems call for different tooling.&lt;/p&gt;
    &lt;p&gt;I'd never used it before and was positively surprised to find Django-like lazy loading, a migrations setup that felt much better than Prisma's, as well as a reasonably ergonomic API (once you manually set up the foundations right).&lt;/p&gt;
    &lt;p&gt;Overall, we're early into this change, but currently happy to have picked MikroORM over the incumbent Prisma.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lost: The Python ecosystem&lt;/head&gt;
    &lt;p&gt;I think this is pretty self-explanatory. While most tools for building RAGs and agents have Python and TypeScript SDKs, Python still takes priority, and we're just talking about API wrappers here.&lt;/p&gt;
    &lt;p&gt;Once you want to actually get into ML stuff yourself, there's just no competition. I suspect that as we get more sophisticated we'll end up having a Python service, but for now we're ok.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gained: Unified codebase&lt;/head&gt;
    &lt;p&gt;We'd always realized that migrating to Node would mean we'd have two Node services instead of a Python one and a Node one, but it didn't occur to us until a day in that we could actually merge the codebases and that that would be extremely helpful.&lt;/p&gt;
    &lt;p&gt;There was a lot of duplicate logic across the Node worker and the Django server, and now we've unified the Express server and background worker into one codebase, which feels so much better. They can both use the ORM now (previously the worker was running raw SQL) and share a bunch of utils.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gained: Much better testing&lt;/head&gt;
    &lt;p&gt;This is not a &lt;code&gt;pytest&lt;/code&gt; vs &lt;code&gt;jest&lt;/code&gt; thing, it's just that in order to make sure everything was working as expected after migrating, we just wrote a ton more tests. This and some refactoring were welcome side benefits.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we did it&lt;/head&gt;
    &lt;p&gt;I think it's about time to wrap this post up, but here are some quick notes about the actual migration process.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It took us three days.&lt;/item&gt;
      &lt;item&gt;We barely used AI code generation at all until the final bits -- it felt important to us to understand the foundations of our new setup really well, particularly the inner workings of the new ORM. Once we had the foundations of everything down Claude Code was quite helpful in generating code for some less important endpoints, and also helped us in scanning the codebase for issues.&lt;/item&gt;
      &lt;item&gt;We almost quit multiple times. We were getting customer requests for new features and had some bugs in the Django code and it felt like we were wasting time migrating instead of serving customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Would we do it again?&lt;/head&gt;
    &lt;p&gt;Honestly, we're quite happy with our decision and would 100% do it again. Not only will this pay off in the long term but it's already paying off today.&lt;/p&gt;
    &lt;p&gt;We learned a lot of stuff in the process too, and if the whole point of this whole post is that someone comes to tell me that we're dumb and we should just have done X or Y, or comes to teach me about how Python async works, then that will honestly be great. For my part, I gladly recognize my inexperience with Python async and if I can learn more about it, that's a win.&lt;/p&gt;
    &lt;p&gt;And if you're interested in actually seeing the code, check out the following PRs:&lt;/p&gt;
    &lt;p&gt;Skald is an MIT-licensed RAG API platform, so if you have any thoughts or concerns, you can come yell at us on GitHub, or open a PR to rewrite the backend to your framework of choice :D&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.yakkomajuri.com/blog/python-to-node"/><published>2025-11-03T16:35:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45801416</id><title>Why engineers can't be rational about programming languages</title><updated>2025-11-03T23:09:51.047299+00:00</updated><content>&lt;doc fingerprint="321991316f69a6a2"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;How Benjamin Franklin Invented Machine Learning in 1720&lt;/head&gt;&lt;p&gt;Benjamin Franklin discovered gradient descent 250 years before we had the mathematics to describe it.&lt;/p&gt;&lt;p&gt;A programming language is the single most expensive choice a company makes, yet we treat it like a technical debate. After watching this mistake bankrupt dozens of companies and hurt hundreds more, I‚Äôve learned the uncomfortable truth: these decisions are rarely about technology. They‚Äôre about identity, emotion, and ego, and they‚Äôre destroying your velocity and budget in ways you can‚Äôt see until it‚Äôs too late.&lt;/p&gt;&lt;p&gt;Early in my career, I worked at Takkle, a promising social network. A sudden departure vaulted me from lead engineer to VP of Engineering, leading a team of 12. While we were delivering on all our goals, I was in my early 20s and lacked experience, a risk our board wanted to fix. They pressured our CEO to recruit a CTO with more experience. I looked forward to learning from him; he was a well known figure in the Perl community and arrived with a stack of O‚ÄôReilly ‚Äúcamel‚Äù books.&lt;/p&gt;&lt;p&gt;One of his first acts was to pronounce our language, PHP, the wrong choice. He decreed a switch to Perl. This decree happened after what felt to me like a sham analysis comparing PHP and Perl.&lt;/p&gt;&lt;p&gt;Our velocity collapsed. Our team had to not only learn a new language but rebuild from scratch, delaying our product by nine months. Our monthly burn rate jumped from $200K to $500K as we more than doubled our size to make up for the lost velocity while building the new Perl based system, which halved our runway.&lt;/p&gt;&lt;p&gt;Our CTO did deliver, at least on some of his promises. We built a beautiful system, one I was truly proud of. But it was too late. By the time we finally launched, the market opportunity had vanished. Facebook had now expanded beyond colleges, and we were at the end of our monitary runway. The increased spend had shortened our runway by half, and we didn‚Äôt have enough momentum with the new site to reach the milestones required to raise more money.&lt;/p&gt;&lt;p&gt;I‚Äôve often wondered: What if we had just stuck with PHP? We had a fine system and real momentum. We would have launched much earlier at a fraction of the cost. PHP was good enough for Facebook; why not us?&lt;/p&gt;&lt;p&gt;But the question that haunted me was: Why did such an experienced leader make such an terrible mistake?&lt;/p&gt;&lt;p&gt;Promises Made&lt;/p&gt;&lt;p&gt;Reality Delivered&lt;/p&gt;&lt;p&gt;As my career progressed I saw this same pattern over and over. As Languages Product Lead at Google, my group included C++, Java, Go, and Python. At MongoDB, I managed teams writing in 13 different languages. In both places I saw brilliant engineers arguing past each other, armed with conflicting data, all of it true, but none of it complete. At Google Cloud, I saw these same challenges across our customers.&lt;/p&gt;&lt;p&gt;Fast forward two decades from Takkle, and I had d√©j√† vu. I watched as a VP of Engineering presented to leadership why his team needed to build their next system in Rust. The presentation eerily paralleled that Takkle experience. In that old presentation, nearly every reason the CTO gave for Perl was truer of PHP at the time. Now, every single reason given for choosing Rust in this presentation, Go was objectively better at. As an example: they cited ‚Äúeasy build and deploy‚Äù as a Rust advantage. It‚Äôs true that this is a strength of Rust, but Go‚Äôs nearly instant cross compilation and single static binary is even stronger than Rust in this specic critera with Rust‚Äôs very long build times.&lt;/p&gt;&lt;p&gt;It‚Äôs not that I think they should have chosen Go, Go would have been the wrong choice for their situation, and I believe Rust was the right choice. But what struck me was how broken their reasoning was. If they were making a logical argument, surely they would have considered Go and in doing so with their presented criteria they would have realized Go was a better option and, at the very least, refined their critera.&lt;/p&gt;&lt;p&gt;I pulled the VP aside after the meeting. ‚ÄúWalk me through how you evaluated other language candidates,‚Äù I said. His face went blank. ‚ÄúWe&amp;amp;mldr; didn‚Äôt really look at any others,‚Äù he admitted. ‚ÄúEveryone‚Äôs talking about Rust.‚Äù There it was: a 50 million dollar decision made on hype, about to be green lit.&lt;/p&gt;&lt;p&gt;For me this was the moment of epiphany, finally an answer to the question for the beginning of my career. The presentation didn‚Äôt share an analysis, they hadn‚Äôt done one; it was a justification for a choice already made. This was a decision based purely on hype, emotion, and identity.&lt;/p&gt;&lt;p&gt;In every language discussion, two conversations are happening simultaneously.&lt;/p&gt;&lt;p&gt;The Visible Conversation: ‚ÄúRust has memory safety without garbage collection.‚Äù ‚ÄúGo has faster compile times and easier deployment.‚Äù ‚ÄúPython has the richest ML ecosystem.‚Äù&lt;/p&gt;&lt;p&gt;The Invisible Conversation: ‚ÄúI am a Rust programmer.‚Äù ‚ÄúI want to become a Rust programmer.‚Äù ‚ÄúI cannot imagine being someone who doesn‚Äôt choose Rust.‚Äù&lt;/p&gt;&lt;p&gt;If you just read that and thought ‚Äúwell, my last language choice was different, I was being rational,‚Äù your invisible conversation is running right now, defending itself while you read this sentence.&lt;/p&gt;&lt;p&gt;My CTO at Takkle was having the invisible conversation. Every point in his visible Perl analysis was technically true, but it felt like a sham because it was only there to cover the much deeper invisible conversation. He wasn‚Äôt evaluating languages. He was protecting an identity he‚Äôd spent a decade building. What our company paid $300K per month extra for wasn‚Äôt better architecture or faster hiring. We paid for the opportunity for him to be a Perl CTO instead of a PHP CTO. That was the real transaction. The rebuild was just the payment plan.&lt;/p&gt;&lt;p&gt;That VP‚Äôs Rust presentation listed ‚Äúeasy build and deploy‚Äù as an advantage, technically true, but Go is objectively better on that specific criterion. If they were truly having the visible conversation, they would have caught that. They would have at least considered Go in their analysis.&lt;/p&gt;&lt;p&gt;They hadn‚Äôt. Because they weren‚Äôt having that conversation at all. And they were about to spend $50 million on the invisible one.&lt;/p&gt;&lt;p&gt;In one of the most fascinating studies done in the past 20 years, researchers set out to understand why people cling to beliefs even when confronted with overwhelming contradictory evidence. What they discovered fundamentally changed our understanding of human decision making.&lt;/p&gt;&lt;p&gt;The researchers recruited participants and first identified which beliefs were central to each person‚Äôs identity‚Äîtheir core political views, their fundamental values, the beliefs that defined who they were. Then, while participants lay in an fMRI scanner, the researchers presented them with carefully constructed challenges to these identity based beliefs, alongside challenges to beliefs the participants held but weren‚Äôt central to their sense of self.&lt;/p&gt;&lt;p&gt;The brain scans revealed something remarkable: these two types of challenges activated completely different neural pathways.&lt;/p&gt;&lt;p&gt;When a peripheral belief was challenged, something the person believed but wasn‚Äôt core to their identity, the brain‚Äôs reasoning centers activated normally. Participants could consider the evidence, weigh the arguments, and sometimes even change their minds.&lt;/p&gt;&lt;p&gt;But when an identity based belief was challenged, the brain responded as if under physical attack. The amygdala, your threat detection system, the same system that fires when you encounter a predator or a physical danger, activates immediately. The insular cortex, which processes emotional pain and disgust, lit up with activity. Most tellingly, the brain‚Äôs Default Mode Network, the system that maintains your sense of self and personal narrative, went into defensive mode, working to protect the existing identity rather than evaluate the new information.&lt;/p&gt;&lt;p&gt;In other words, your brain wasn‚Äôt weighing evidence. It was defending itself from an existential threat.&lt;/p&gt;&lt;p&gt;Your brain can‚Äôt objectively evaluate challenges to identity based beliefs because doing so requires temporarily dismantling the neural architecture that defines who you are. It‚Äôs not a matter of being more rational or trying harder. The mechanism that would allow you to see the bias clearly is the same mechanism the bias has compromised.&lt;/p&gt;&lt;p&gt;Think about what this means in practice. Every time an engineer evaluates a language that isn‚Äôt ‚Äútheirs,‚Äù their brain is literally working against them. They‚Äôre not just analyzing technical trade offs, they‚Äôre contemplating a version of themselves that doesn‚Äôt exist yet, that feels threatening to the version that does. The Python developer reads case studies about Go‚Äôs performance and their amygdala quietly marks each one as a threat to be neutralized. The Rust advocate looks at identical problems and their Default Mode Network constructs narratives about why ‚Äúonly‚Äù Rust can solve them.&lt;/p&gt;&lt;p&gt;We‚Äôre not lying. We genuinely believe our reasoning is sound. That‚Äôs what makes identity based thinking so expensive, and so invisible.&lt;/p&gt;&lt;p&gt;We call ourselves Pythonistas, Gophers, Rustaceans, we wear these labels like badges, sometimes we even wear literal badges (t-shirts, stickers, etc). There‚Äôs a reason so many of our surnames come from people‚Äôs crafts: Potter, Smith, Brewer. What we do becomes who we are. What looks like decorative labels are really decision making frameworks that operate beneath conscious thought.&lt;/p&gt;&lt;p&gt;We‚Äôve built our entire industry around the visible conversation. We train engineers to debate technical merits. We create decision frameworks based on feature matrices. We think if we just gather enough benchmarks and case studies, we‚Äôll make the right choice.&lt;/p&gt;&lt;p&gt;But the invisible conversation is much stronger. It‚Äôs why my CTO chose Perl. It‚Äôs why that VP chose Rust. And it‚Äôs operating in your next language decision right now, invisible and unexamined.&lt;/p&gt;&lt;p&gt;The moment you hire a Rust developer to evaluate languages, you‚Äôve already chosen Rust. You‚Äôve just added a $2 million feasibility study to make the predetermined decision feel rational.&lt;/p&gt;&lt;p&gt;The question isn‚Äôt whether this bias exists, the science is conclusive. The real question is: can you afford to let it make your decisions?&lt;/p&gt;&lt;p&gt;Because the invisible conversation has a price tag. Industry research suggests that technology stack decisions account for 40-60% of total development costs over a product‚Äôs lifecycle. Research by Stripe found that developers spend 42% of their time on technical debt. When you let identity drive that decision you‚Äôre mortgaging your velocity, your budget, and your runway to pay for someone‚Äôs sense of self.&lt;/p&gt;&lt;p&gt;The visible conversation is about technology. The invisible conversation is about identity. And the invisible conversation always wins.&lt;/p&gt;&lt;p&gt;So how do we win, when the invisible conversation is constantly working against us? Change the conversation entirely.&lt;/p&gt;&lt;p&gt;Instead of asking ‚Äúwhich language is best?‚Äù we need to ask ‚Äúwhat is this language going to cost us?‚Äù Not just in salaries, but in velocity, in technical debt, in hiring difficulty, in operational complexity, in every dimension that actually determines whether you survive.&lt;/p&gt;&lt;p&gt;Reframe it from a technical debate to an economic one. And unlike identity, economics can be measured, compared, and decided without anyone‚Äôs ego being threatened.&lt;/p&gt;&lt;p&gt;Choosing a programming language is the single most expensive economic decision your company will make. It will define your culture, constrain your budget, determine your hiring pipeline, set your operational costs, and ultimately dictate whether you can move fast enough to win your market.&lt;/p&gt;&lt;p&gt;We need a framework that makes the invisible costs visible. One that lets us have the economic conversation instead of the identity conversation. One that works whether you‚Äôre choosing your first language or evaluating a migration.&lt;/p&gt;&lt;p&gt;Our industry has never really had that framework&amp;amp;mldr; Until now.&lt;/p&gt;&lt;p&gt;Up Next&lt;/p&gt;In my next post, I‚Äôll introduce the 9 Factors of a Language‚Äôs True Cost: a comprehensive framework for evaluating language decisions based on economics, not identity. You‚Äôll learn how to quantify the hidden costs, predict long term impact, and make defensible decisions that your team can align behind, regardless of their language preferences.&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spf13.com/p/the-hidden-conversation/"/><published>2025-11-03T17:08:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45802029</id><title>The Case That A.I. Is Thinking</title><updated>2025-11-03T23:09:50.591873+00:00</updated><content>&lt;doc fingerprint="2f41991cbdb78dfb"&gt;
  &lt;main&gt;
    &lt;p&gt;Dario Amodei, the C.E.O. of the artificial-intelligence company Anthropic, has been predicting that an A.I. ‚Äúsmarter than a Nobel Prize winner‚Äù in such fields as biology, math, engineering, and writing might come online by 2027. He envisions millions of copies of a model whirring away, each conducting its own research: a ‚Äúcountry of geniuses in a datacenter.‚Äù In June, Sam Altman, of OpenAI, wrote that the industry was on the cusp of building ‚Äúdigital superintelligence.‚Äù ‚ÄúThe 2030s are likely going to be wildly different from any time that has come before,‚Äù he asserted. Meanwhile, the A.I. tools that most people currently interact with on a day-to-day basis are reminiscent of Clippy, the onetime Microsoft Office ‚Äúassistant‚Äù that was actually more of a gadfly. A Zoom A.I. tool suggests that you ask it ‚ÄúWhat are some meeting icebreakers?‚Äù or instruct it to ‚ÄúWrite a short message to share gratitude.‚Äù Siri is good at setting reminders but not much else. A friend of mine saw a button in Gmail that said ‚ÄúThank and tell anecdote.‚Äù When he clicked it, Google‚Äôs A.I. invented a funny story about a trip to Turkey that he never took.&lt;/p&gt;
    &lt;p&gt;The rushed and uneven rollout of A.I. has created a fog in which it is tempting to conclude that there is nothing to see here‚Äîthat it‚Äôs all hype. There is, to be sure, plenty of hype: Amodei‚Äôs timeline is science-fictional. (A.I. models aren‚Äôt improving that fast.) But it is another kind of wishful thinking to suppose that large language models are just shuffling words around. I used to be sympathetic to that view. I sought comfort in the idea that A.I. had little to do with real intelligence or understanding. I even celebrated its shortcomings‚Äîrooting for the home team. Then I began using A.I. in my work as a programmer, fearing that if I didn‚Äôt I would fall behind. (My employer, a trading firm, has several investments in and partnerships with A.I. companies, including Anthropic.) Writing code is, by many accounts, the thing that A.I. is best at; code has more structure than prose does, and it‚Äôs often possible to automatically validate that a given program works. My conversion was swift. At first, I consulted A.I. models in lieu of looking something up. Then I gave them small, self-contained problems. Eventually, I gave them real work‚Äîthe kind I‚Äôd trained my whole career to do. I saw these models digest, in seconds, the intricate details of thousands of lines of code. They could spot subtle bugs and orchestrate complex new features. Finally, I was transferred to a fast-growing team that aims to make better use of A.I. tools, and to create our own.&lt;/p&gt;
    &lt;p&gt;The science-fiction author William Gibson is said to have observed that the future is already here, just not evenly distributed‚Äîwhich might explain why A.I. seems to have minted two cultures, one dismissive and the other enthralled. In our daily lives, A.I. ‚Äúagents‚Äù that can book vacations or file taxes are a flop, but I have colleagues who compose much of their code using A.I. and sometimes run multiple coding agents at a time. Models sometimes make amateur mistakes or get caught in inane loops, but, as I‚Äôve learned to use them effectively, they have allowed me to accomplish in an evening what used to take a month. Not too long ago, I made two iOS apps without knowing how to make an iOS app.&lt;/p&gt;
    &lt;p&gt;I once had a boss who said that a job interview should probe for strengths, not for the absence of weaknesses. Large language models have many weaknesses: they famously hallucinate reasonable-sounding falsehoods; they can be servile even when you‚Äôre wrong; they are fooled by simple puzzles. But I remember a time when the obvious strengths of today‚Äôs A.I. models‚Äîfluency, fluidity, an ability to ‚Äúget‚Äù what someone is talking about‚Äîwere considered holy grails. When you experience these strengths firsthand, you wonder: How convincing does the illusion of understanding have to be before you stop calling it an illusion?&lt;/p&gt;
    &lt;p&gt;On a brutally hot day this summer, my friend Max met up with his family at a playground. For some reason, a sprinkler for kids was switched off, and Max‚Äôs wife had promised everyone that her husband would fix it. Confronted by red-faced six- and seven-year-olds, Max entered a utility shed hoping to find a big, fat ‚ÄúOn‚Äù switch. Instead, he found a maze of ancient pipes and valves. He was about to give up when, on a whim, he pulled out his phone and fed a photo into ChatGPT-4o, along with a description of his problem. The A.I. thought for a second, or maybe didn‚Äôt think, but all the same it said that he was looking at a backflow-preventer system typical of irrigation setups. Did he see that yellow ball valve toward the bottom? That probably controlled the flow. Max went for it, and cheers rang out across the playground as the water turned on.&lt;/p&gt;
    &lt;p&gt;Was ChatGPT mindlessly stringing words together, or did it understand the problem? The answer could teach us something important about understanding itself. ‚ÄúNeuroscientists have to confront this humbling truth,‚Äù Doris Tsao, a neuroscience professor at the University of California, Berkeley, told me. ‚ÄúThe advances in machine learning have taught us more about the essence of intelligence than anything that neuroscience has discovered in the past hundred years.‚Äù Tsao is best known for decoding how macaque monkeys perceive faces. Her team learned to predict which neurons would fire when a monkey saw a specific face; even more strikingly, given a pattern of neurons firing, Tsao‚Äôs team could render the face. Their work built on research into how faces are represented inside A.I. models. These days, her favorite question to ask people is ‚ÄúWhat is the deepest insight you have gained from ChatGPT?‚Äù ‚ÄúMy own answer,‚Äù she said, ‚Äúis that I think it radically demystifies thinking.‚Äù&lt;/p&gt;
    &lt;p&gt;The most basic account of how we got here goes something like this. In the nineteen-eighties, a small team of cognitive psychologists and computer scientists tried to simulate thinking in a machine. Among the more famous of them were David Rumelhart, Geoffrey Hinton, and James McClelland, who went on to form a research group at U.C. San Diego. They saw the brain as a vast network in which neurons fire in patterns, causing other sets of neurons to fire, and so on; this dance of patterns is thinking. The brain learns by changing the strength of the connections between neurons. Crucially, the scientists mimicked this process by creating an artificial neural network, and by applying a simple algorithm called gradient descent to increase the accuracy of its predictions. (The algorithm could be compared to a hiker navigating from a mountaintop to a valley; a simple strategy for eventually finding one‚Äôs way is to insure that every step moves downhill.) The use of such algorithms in large networks is known as deep learning.&lt;/p&gt;
    &lt;p&gt;Other people in A.I. were skeptical that neural networks were sophisticated enough for real-world tasks, but, as the networks got bigger, they began to solve previously unsolvable problems. People would devote entire dissertations to developing techniques for distinguishing handwritten digits or for recognizing faces in images; then a deep-learning algorithm would digest the underlying data, discover the subtleties of the problem, and make those projects seem obsolete. Deep learning soon conquered speech recognition, translation, image captioning, board games, and even the problem of predicting how proteins will fold.&lt;/p&gt;
    &lt;p&gt;Today‚Äôs leading A.I. models are trained on a large portion of the internet, using a technique called next-token prediction. A model learns by making guesses about what it will read next, then comparing those guesses to whatever actually appears. Wrong guesses inspire changes in the connection strength between the neurons; this is gradient descent. Eventually, the model becomes so good at predicting text that it appears to know things and make sense. So that is something to think about. A group of people sought the secret of how the brain works. As their model grew toward a brain-like size, it started doing things that were thought to require brain-like intelligence. Is it possible that they found what they were looking for?&lt;/p&gt;
    &lt;p&gt;There is understandable resistance to such a simplistic and triumphant account of A.I. The case against it was well argued by Ted Chiang, who wrote an article for this magazine in early 2023 titled ‚ÄúChatGPT Is a Blurry JPEG of the Web.‚Äù He meant it in a more or less deflationary way: that‚Äôs all ChatGPT is. You feed the whole internet to a program and it regurgitates it back to you imperfectly, like a copy of a copy of a photograph‚Äîbut with just enough facility to fool you into believing that the program is intelligent. This spring, a similar argument was made in a book, ‚ÄúThe AI Con,‚Äù by Emily M. Bender, a linguist, and Alex Hanna, a sociologist. Bender is perhaps best known for describing L.L.M.s as ‚Äústochastic parrots.‚Äù ‚ÄúLarge language models do not, cannot, and will not ‚Äòunderstand‚Äô anything at all,‚Äù the writer Tyler Austin Harper declared in a book review in The Atlantic. Models ‚Äúproduce writing not by thinking but by making statistically informed guesses about which lexical item is likely to follow another.‚Äù Harper buttressed these technical arguments with moral ones. A.I. enriches the powerful, consumes enough energy to accelerate climate change, and marginalizes workers. He concluded that ‚Äúthe foundation of the AI industry is a scam.‚Äù&lt;/p&gt;
    &lt;p&gt;But the moral case against A.I. may ultimately be stronger than the technical one. ‚ÄúThe ‚Äòstochastic parrot‚Äô thing has to be dead at some point,‚Äù Samuel J. Gershman, a Harvard cognitive scientist who is no A.I. hype man, told me. ‚ÄúOnly the most hardcore skeptics can deny these systems are doing things many of us didn‚Äôt think were going to be achieved.‚Äù Jonathan Cohen, a cognitive neuroscientist at Princeton, emphasized the limitations of A.I., but argued that, in some cases, L.L.M.s seem to mirror one of the largest and most important parts of the human brain. ‚ÄúTo a first approximation, your neocortex is your deep-learning mechanism,‚Äù Cohen said. Humans have a much larger neocortex than other animals, relative to body size, and the species with the largest neocortices‚Äîelephants, dolphins, gorillas, chimpanzees, dogs‚Äîare among the most intelligent.&lt;/p&gt;
    &lt;p&gt;In 2003, the machine-learning researcher Eric B. Baum published a book called ‚ÄúWhat Is Thought?‚Äù (I stumbled upon it in my college‚Äôs library stacks, drawn by the title.) The gist of Baum‚Äôs argument is that understanding is compression, and compression is understanding. In statistics, when you want to make sense of points on a graph, you can use a technique called linear regression to draw a ‚Äúline of best fit‚Äù through them. If there‚Äôs an underlying regularity in the data‚Äîmaybe you‚Äôre plotting shoe size against height‚Äîthe line of best fit will efficiently express it, predicting where new points could fall. The neocortex can be understood as distilling a sea of raw experience‚Äîsounds, sights, and other sensations‚Äîinto ‚Äúlines of best fit,‚Äù which it can use to make predictions. A baby exploring the world tries to guess how a toy will taste or where food will go when it hits the floor. When a prediction is wrong, the connections between neurons are adjusted. Over time, those connections begin to capture regularities in the data. They form a compressed model of the world.&lt;/p&gt;
    &lt;p&gt;Artificial neural networks compress experience just like real neural networks do. One of the best open-source A.I. models, DeepSeek, is capable of writing novels, suggesting medical diagnoses, and sounding like a native speaker in dozens of languages. It was trained using next-token prediction on many terabytes of data. But when you download the model it is one six-hundredth of that. A distillation of the internet, compressed to fit on your laptop. Ted Chiang was right to call an early version of ChatGPT a blurry JPEG of the web‚Äîbut, in my view, this is the very reason these models have become increasingly intelligent. Chiang noted in his piece that, to compress a text file filled with millions of examples of arithmetic, you wouldn‚Äôt create a zip file. You‚Äôd write a calculator program. ‚ÄúThe greatest degree of compression can be achieved by understanding the text,‚Äù he wrote. Perhaps L.L.M.s are starting to do that.&lt;/p&gt;
    &lt;p&gt;It can seem unnatural, even repulsive, to imagine that a computer program actually understands, actually thinks. We usually conceptualize thinking as something conscious, like a Joycean inner monologue or the flow of sense memories in a Proustian daydream. Or we might mean reasoning: working through a problem step by step. In our conversations about A.I., we often conflate these different kinds of thinking, and it makes our judgments pat. ChatGPT is obviously not thinking, goes one argument, because it is obviously not having a Proustian reverie; ChatGPT clearly is thinking, goes another, because it can work through logic puzzles better than you can.&lt;/p&gt;
    &lt;p&gt;Something more subtle is going on. I do not believe that ChatGPT has an inner life, and yet it seems to know what it‚Äôs talking about. Understanding‚Äîhaving a grasp of what‚Äôs going on‚Äîis an underappreciated kind of thinking, because it‚Äôs mostly unconscious. Douglas Hofstadter, a professor of cognitive science and comparative literature at Indiana University, likes to say that cognition is recognition. Hofstadter became famous for a book about the mind and consciousness called ‚ÄúG√∂del, Escher, Bach: An Eternal Golden Braid,‚Äù which won a Pulitzer Prize in 1980. Hofstadter‚Äôs theory, developed through decades of research, is that ‚Äúseeing as‚Äù is the essence of thinking. You see one patch of color as a car and another as a key chain; you recognize the letter ‚ÄúA‚Äù no matter what font it is written in or how bad the handwriting might be. Hofstadter argued that the same process underlies more abstract kinds of perception. When a grand master examines a chess board, years of practice are channelled into a way of seeing: white‚Äôs bishop is weak; that endgame is probably a draw. You see an eddy in a river as a sign that it‚Äôs dangerous to cross. You see a meeting you‚Äôre in as an emperor-has-no-clothes situation. My nearly two-year-old son recognizes that late-morning stroller walks might be an opportunity for a croissant and makes demands accordingly. For Hofstadter, that‚Äôs intelligence in a nutshell.&lt;/p&gt;
    &lt;p&gt;Hofstadter was one of the original A.I. deflationists, and my own skepticism was rooted in his. He wrote that most A.I. research had little to do with real thinking, and when I was in college, in the two-thousands, I agreed with him. There were exceptions. He found the U.C.S.D. group interesting. And he admired the work of a lesser-known Finnish American cognitive scientist, Pentti Kanerva, who noticed some unusual properties in the mathematics of high-dimensional spaces. In a high-dimensional space, any two random points may be extremely far apart. But, counterintuitively, each point also has a large cloud of neighbors around it, so you can easily find your way to it if you get ‚Äúclose enough.‚Äù That reminded Kanerva of the way that memory works. In a 1988 book called ‚ÄúSparse Distributed Memory,‚Äù Kanerva argued that thoughts, sensations, and recollections could be represented as co√∂rdinates in high-dimensional space. The brain seemed like the perfect piece of hardware for storing such things. Every memory has a sort of address, defined by the neurons that are active when you recall it. New experiences cause new sets of neurons to fire, representing new addresses. Two addresses can be different in many ways but similar in others; one perception or memory triggers other memories nearby. The scent of hay recalls a memory of summer camp. The first three notes of Beethoven‚Äôs Fifth beget the fourth. A chess position that you‚Äôve never seen reminds you of old games‚Äînot all of them, just the ones in the right neighborhood.&lt;/p&gt;
    &lt;p&gt;Hofstadter realized that Kanerva was describing something like a ‚Äúseeing as‚Äù machine. ‚ÄúPentti Kanerva‚Äôs memory model was a revelation for me,‚Äù he wrote in a foreword to Kanerva‚Äôs book. ‚ÄúIt was the very first piece of research I had ever run across that made me feel I could glimpse the distant goal of understanding how the brain works as a whole.‚Äù Every kind of thinking‚Äîwhether Joycean, Proustian, or logical‚Äîdepends on the relevant thing coming to mind at the right time. It‚Äôs how we figure out what situation we‚Äôre in.&lt;/p&gt;
    &lt;p&gt;Kanerva‚Äôs book receded from view, and Hofstadter‚Äôs own star faded‚Äîexcept when he occasionally poked up his head to criticize a new A.I. system. In 2018, he wrote of Google Translate and similar technologies: ‚ÄúThere is still something deeply lacking in the approach, which is conveyed by a single word: understanding.‚Äù But GPT-4, which was released in 2023, produced Hofstadter‚Äôs conversion moment. ‚ÄúI‚Äôm mind-boggled by some of the things that the systems do,‚Äù he told me recently. ‚ÄúIt would have been inconceivable even only ten years ago.‚Äù The staunchest deflationist could deflate no longer. Here was a program that could translate as well as an expert, make analogies, extemporize, generalize. Who were we to say that it didn‚Äôt understand? ‚ÄúThey do things that are very much like thinking,‚Äù he said. ‚ÄúYou could say they are thinking, just in a somewhat alien way.‚Äù&lt;/p&gt;
    &lt;p&gt;L.L.M.s appear to have a ‚Äúseeing as‚Äù machine at their core. They represent each word with a series of numbers denoting its co√∂rdinates‚Äîits vector‚Äîin a high-dimensional space. In GPT-4, a word vector has thousands of dimensions, which describe its shades of similarity to and difference from every other word. During training, a large language model tweaks a word‚Äôs co√∂rdinates whenever it makes a prediction error; words that appear in texts together are nudged closer in space. This produces an incredibly dense representation of usages and meanings, in which analogy becomes a matter of geometry. In a classic example, if you take the word vector for ‚ÄúParis,‚Äù subtract ‚ÄúFrance,‚Äù and then add ‚ÄúItaly,‚Äù the nearest other vector will be ‚ÄúRome.‚Äù L.L.M.s can ‚Äúvectorize‚Äù an image by encoding what‚Äôs in it, its mood, even the expressions on people‚Äôs faces, with enough detail to redraw it in a particular style or to write a paragraph about it. When Max asked ChatGPT to help him out with the sprinkler at the park, the model wasn‚Äôt just spewing text. The photograph of the plumbing was compressed, along with Max‚Äôs prompt, into a vector that captured its most important features. That vector served as an address for calling up nearby words and concepts. Those ideas, in turn, called up others as the model built up a sense of the situation. It composed its response with those ideas ‚Äúin mind.‚Äù&lt;/p&gt;
    &lt;p&gt;A few months ago, I was reading an interview with an Anthropic researcher, Trenton Bricken, who has worked with colleagues to probe the insides of Claude, the company‚Äôs series of A.I. models. (Their research has not been peer-reviewed or published in a scientific journal.) His team has identified ensembles of artificial neurons, or ‚Äúfeatures,‚Äù that activate when Claude is about to say one thing or another. Features turn out to be like volume knobs for concepts; turn them up and the model will talk about little else. (In a sort of thought-control experiment, the feature representing the Golden Gate Bridge was turned up; when one user asked Claude for a chocolate-cake recipe, its suggested ingredients included ‚Äú1/4 cup dry fog‚Äù and ‚Äú1 cup warm seawater.‚Äù) In the interview, Bricken mentioned Google‚Äôs Transformer architecture, a recipe for constructing neural networks that underlies leading A.I. models. (The ‚ÄúT‚Äù in ChatGPT stands for ‚ÄúTransformer.‚Äù) He argued that the mathematics at the heart of the Transformer architecture closely approximated a model proposed decades earlier‚Äîby Pentti Kanerva, in ‚ÄúSparse Distributed Memory.‚Äù&lt;/p&gt;
    &lt;p&gt;Should we be surprised by the correspondence between A.I. and our own brains? L.L.M.s are, after all, artificial neural networks that psychologists and neuroscientists helped develop. What‚Äôs more surprising is that when models practiced something rote‚Äîpredicting words‚Äîthey began to behave in such a brain-like way. These days, the fields of neuroscience and artificial intelligence are becoming entangled; brain experts are using A.I. as a kind of model organism. Evelina Fedorenko, a neuroscientist at M.I.T., has used L.L.M.s to study how brains process language. ‚ÄúI never thought I would be able to think about these kinds of things in my lifetime,‚Äù she told me. ‚ÄúI never thought we‚Äôd have models that are good enough.‚Äù&lt;/p&gt;
    &lt;p&gt;It has become commonplace to say that A.I. is a black box, but the opposite is arguably true: a scientist can probe the activity of individual artificial neurons and even alter them. ‚ÄúHaving a working system that instantiates a theory of human intelligence‚Äîit‚Äôs the dream of cognitive neuroscience,‚Äù Kenneth Norman, a Princeton neuroscientist, told me. Norman has created computer models of the hippocampus, the brain region where episodic memories are stored, but in the past they were so simple that he could only feed them crude approximations of what might enter a human mind. ‚ÄúNow you can give memory models the exact stimuli you give to a person,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;The Wright brothers studied birds during their early efforts to build an airplane. They noted that birds take off into the wind, even though a reasonable person might have assumed they‚Äôd want the wind at their backs, and that they warp the tips of their wings for balance. These findings influenced their rudimentary glider designs. Then they built a six-foot-long wind tunnel, which allowed them to test a set of artificial wings under precisely controlled conditions. Their next round of glider flights was far more successful. Strangely, it was only well after they‚Äôd made a working flying machine that it became possible to understand exactly how the birds do it.&lt;/p&gt;
    &lt;p&gt;A.I. enables scientists to place thinking itself in a wind tunnel. For a paper provocatively titled ‚ÄúOn the Biology of a Large Language Model,‚Äù Anthropic researchers observed Claude responding to queries and described ‚Äúcircuits‚Äù‚Äîcascades of features that, together, perform complex computations. (Calling up the right memories is one step toward thinking; combining and manipulating them in circuits is arguably another.) One longstanding criticism of L.L.M.s has been that, because they must generate one token of their response at a time, they can‚Äôt plan or reason. But, when you ask Claude to finish a rhyming couplet in a poem, a circuit begins considering the last word of the new line, to insure that it will rhyme. It then works backward to compose the line as a whole. Anthropic researchers counted this as evidence that their models do engage in planning. Squint a little and you might feel, for the first time, that the inner workings of a mind are in view.&lt;/p&gt;
    &lt;p&gt;You really do have to squint, though. ‚ÄúThe worry I have is that people flipped the bit from ‚ÄòI‚Äôm really skeptical of this‚Äô to totally dropping their shields,‚Äù Norman, the Princeton neuroscientist, told me. ‚ÄúMany things still have to get figured out.‚Äù I‚Äôm one of the people that Norman is talking about. (Perhaps I am too easily moved by the seeming convergence of ‚ÄúSparse Distributed Memory‚Äù and an Anthropic model.) In the past year or two, I started to believe what Geoffrey Hinton, who recently won a Nobel Prize for his A.I. research, told the journalist Karen Hao in 2020: ‚ÄúDeep learning is going to be able to do everything.‚Äù But we have also seen that larger models aren‚Äôt always better models. Curves plotting model performance against size have begun flattening out. It‚Äôs becoming difficult to find high-quality data that the models haven‚Äôt already digested, and computing power is increasingly expensive. When GPT-5 came out, in August, it was a merely incremental improvement‚Äîand so profound a disappointment that it threatened to pop the A.I. investment bubble. The moment demands a middle kind of skepticism: one that takes today‚Äôs A.I. models seriously without believing that there are no hard problems left.&lt;/p&gt;
    &lt;p&gt;Perhaps the most consequential of these problems is how to design a model that learns as efficiently as humans do. It is estimated that GPT-4 was exposed to trillions of words in training; children need only a few million to become fluent. Cognitive scientists tell us that a newborn‚Äôs brain has certain ‚Äúinductive biases‚Äù that accelerate learning. (Of course, the brain is the result of millions of years of evolution‚Äîitself a sort of training data.) For instance, human babies have the expectation that the world is made of objects, and that other beings have beliefs and intentions. When Mama says ‚Äúbanana,‚Äù an infant connects that word to the entire yellow object she‚Äôs looking at‚Äînot just its tip or its peel. Infants perform little experiments: Can I eat this? How far can I throw that? They are motivated by emotions such as desire, curiosity, and frustration. Children are always trying to do something just beyond their ability. Their learning is efficient because it‚Äôs embodied, adaptive, deliberate, and continuous. Maybe truly understanding the world requires participating in it.&lt;/p&gt;
    &lt;p&gt;An A.I.‚Äôs experience, in comparison, is so impoverished that it can‚Äôt really be called ‚Äúexperience.‚Äù Large language models are trained on data that is already extraordinarily refined. ‚ÄúI think the reason they work is that they‚Äôre piggybacking on language,‚Äù Tsao, the U.C. Berkeley neuroscientist, told me. Language is like experience pre-chewed; other kinds of data are less dense with meaning. ‚ÄúWhy is it that we haven‚Äôt had a comparable revolution in terms of reasoning about video data?‚Äù Gershman, the Harvard cognitive scientist, asked. ‚ÄúThe kinds of vision models that we have still struggle with common-sense reasoning about physics.‚Äù A recent model from DeepMind can generate videos in which paints are mixed correctly and mazes are solved‚Äîbut they also depict a glass bouncing, instead of shattering, and ropes defying physics by being smooshed into a knot. Ida Momennejad, a cognitive neuroscientist who now works for Microsoft Research, has done experiments in which an L.L.M. is given a virtual walk-through of a building and then asked questions about routes and shortcuts‚Äîspatial inferences that come easily to humans. With all but the most basic setups, the A.I.s tend to fail or hallucinate nonexistent paths. ‚ÄúDo they really do planning?‚Äù she said. ‚ÄúNot really.‚Äù&lt;/p&gt;
    &lt;p&gt;In my conversations with neuroscientists, I sensed a concern that the A.I. industry is racing ahead somewhat thoughtlessly. If the goal is to make artificial minds as capable as human minds are, then ‚Äúwe‚Äôre not training the systems in the right way,‚Äù Brenden M. Lake, a cognitive scientist at Princeton, told me. When an A.I. is done training, the neural network‚Äôs ‚Äúbrain‚Äù is frozen. If you tell the model some facts about yourself, it doesn‚Äôt rewire its neurons. Instead, it uses a crude substitute: it writes down a bit of text‚Äî‚ÄúThe user has a toddler and is studying French‚Äù‚Äîand considers that before other instructions you give. The human brain updates itself continuously, and there‚Äôs a beautiful theory about one of its ways of doing so: when you sleep, selected snapshots from your episodic memory are replayed for your neocortex in order to train it. Your high-dimensional thought space gets dimpled by the replayed memories; you wake up with a slightly new way of seeing.&lt;/p&gt;
    &lt;p&gt;The A.I. community has become so addicted to‚Äîand so financially invested in‚Äîbreakneck progress that it sometimes pretends that advancement is inevitable and there‚Äôs no science left to do. Science has the inconvenient property of sometimes stalling out. Silicon Valley may call A.I. companies ‚Äúlabs,‚Äù and some employees there ‚Äúresearchers,‚Äù but fundamentally it has an engineering culture that does whatever works. ‚ÄúIt‚Äôs just so remarkable how little the machine-learning community bothers looking at, let alone respects, the history and cognitive science that precedes it,‚Äù Cohen said.&lt;/p&gt;
    &lt;p&gt;Today‚Äôs A.I. models owe their success to decades-old discoveries about the brain, but they are still deeply unlike brains. Which differences are incidental and which are fundamental? Every group of neuroscientists has its pet theory. These theories can be put to the test in a way that wasn‚Äôt possible before. Still, no one expects easy answers. The problems that continue to plague A.I. models ‚Äúare solved by carefully identifying ways in which the models don‚Äôt behave as intelligently as we want them to and then addressing them,‚Äù Norman said. ‚ÄúThat is still a human-scientist-in-the-loop process.‚Äù&lt;/p&gt;
    &lt;p&gt;In the nineties, billions of dollars poured into the Human Genome Project on the assumption that sequencing DNA might solve medicine‚Äôs most vexing problems: cancer, hereditary conditions, even aging. It was a time of bluster and confidence‚Äîthe era of Dolly the cloned sheep and ‚ÄúJurassic Park‚Äù‚Äîwhen biotech was ascendant and the commentariat reckoned with whether humans should be playing God. Biologists soon found that the reality was more complicated. We didn‚Äôt cure cancer or discover the causes of Alzheimer‚Äôs or autism. We learned that DNA tells just one part of the story of life. In fact, one could argue that biology got swept up in a kind of gene fever, fixating on DNA because we had the means to study and understand it.&lt;/p&gt;
    &lt;p&gt;Still, nobody would claim that Francis Crick was wrong when, on the day in 1953 that he helped confirm the structure of DNA, he walked into a Cambridge pub talking about having discovered the secret of life. He and his colleagues did more to demystify life than almost anyone, ever. The decades following their discovery were among the most productive and exciting in the history of science. DNA became a household term; every high schooler learns about the double helix.&lt;/p&gt;
    &lt;p&gt;With A.I., we once again find ourselves in a moment of bluster and confidence. Sam Altman talks about raising half a trillion dollars to build Stargate, a new cluster of A.I. data centers, in the U.S. People discuss the race for superintelligence with a gravitas and an urgency that can seem ungrounded, even silly. But I suspect the reason that the Amodeis and Altmans of the world are making messianic pronouncements is that they believe that the basic picture of intelligence has been worked out; the rest is just details.&lt;/p&gt;
    &lt;p&gt;Even some neuroscientists believe that a crucial threshold has been crossed. ‚ÄúI really think it could be the right model for cognition,‚Äù Uri Hasson, a colleague of Cohen‚Äôs, Norman‚Äôs, and Lake‚Äôs at Princeton, said of neural networks. This upsets him as much as it excites him. ‚ÄúI have the opposite worry of most people,‚Äù he said. ‚ÄúMy worry is not that these models are similar to us. It‚Äôs that we are similar to these models.‚Äù If simple training techniques can enable a program to behave like a human, maybe humans aren‚Äôt as special as we thought. Could it also mean that A.I. will surpass us not only in knowledge but also in judgment, ingenuity, cunning‚Äîand, as a result, power? To my surprise, Hasson told me that he is ‚Äúworried these days that we might succeed in understanding how the brain works. Pursuing this question may have been a colossal mistake for humanity.‚Äù He likened A.I. researchers to nuclear scientists in the nineteen-thirties: ‚ÄúThis is the most interesting time in the life of these people. And, at the same time, they know that what they are working on has grave implications for humanity. But they cannot stop because of the curiosity to learn.‚Äù&lt;/p&gt;
    &lt;p&gt;One of my favorite books by Hofstadter is a nerdy volume called ‚ÄúFluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought.‚Äù When I was in college, it electrified me. The premise was that a question such as ‚ÄúWhat is thinking?‚Äù was not merely philosophical but, rather, had a real answer. In 1995, when the book was published, Hofstadter and his research group could only gesture at what the answer might be. Thinking back on the book, I wondered whether Hofstadter would feel excited that A.I. researchers may have attained what he had yearned for: a mechanical account of the rudiments of thinking. When we spoke, however, he sounded profoundly disappointed‚Äîand frightened. Current A.I. research ‚Äúconfirms a lot of my ideas, but it also takes away from the beauty of what humanity is,‚Äù he told me. ‚ÄúWhen I was younger, much younger, I wanted to know what underlay creativity, the mechanisms of creativity. That was a holy grail for me. But now I want it to remain a mystery.‚Äù Perhaps the secrets of thinking are simpler than anyone expected‚Äîthe kind of thing that a high schooler, or even a machine, could understand. ‚ô¶&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.newyorker.com/magazine/2025/11/10/the-case-that-ai-is-thinking"/><published>2025-11-03T17:55:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45802290</id><title>Agent-o-rama: build, trace, evaluate, and monitor LLM agents in Java or Clojure</title><updated>2025-11-03T23:09:50.282773+00:00</updated><content>&lt;doc fingerprint="3d568e11483d85ab"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve just open-sourced Agent-o-rama, a library for building scalable and stateful LLM agents on the JVM. Agent-o-rama provides two first-class APIs, one for Java and one for Clojure, with feature parity between them.&lt;/p&gt;
    &lt;p&gt;AI tooling today is overwhelmingly centered on Python, and while the JVM ecosystem has seen growing support through libraries like LangChain4j, it lacks the kind of integrated tooling that lets developers evaluate, observe, and deploy LLM-based systems rigorously and at scale. Available tools are fragmented or complex to set up, and nothing handles the entire workflow from development to production with proper observability.&lt;/p&gt;
    &lt;p&gt;Agent-o-rama fills that gap. It brings the same ideas popularized by LangGraph and LangSmith ‚Äì structured agent graphs, tracing, datasets, experiments, evaluation ‚Äì but makes them native to Java and Clojure. LLMs are powerful but inherently unpredictable, so building applications with LLMs that are helpful and performant with minimal hallucination requires being rigorous about testing and monitoring.&lt;/p&gt;
    &lt;p&gt;Agents are defined as simple graphs of Java or Clojure functions that execute in parallel. Agent-o-rama automatically captures detailed traces and includes a web UI for offline experimentation, online evaluation, and time-series telemetry (e.g. model latency, token usage, database latency). It also supports streaming, with a simple client API to stream model calls or other outputs from nodes in real time. Agent-o-rama extends the ideas from LangGraph and LangSmith with far greater scalability, full parallel execution, and built-in high-performance data storage and deployment.&lt;/p&gt;
    &lt;p&gt;Agent-o-rama is deployed onto your own infrastructure on a Rama cluster. Rama is free to use for clusters up to two nodes and can scale to thousands with a commercial license. Every part of Agent-o-rama is built-in and requires no other dependency besides Rama. Agent-o-rama also integrates seamlessly with any other tool, such as databases, vector stores, external APIs, or anything else. Unlike hosted observability tools, all data and traces stay within your infrastructure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example agent&lt;/head&gt;
    &lt;p&gt;Let‚Äôs take a look at an example agent! This is a research agent from the examples/ directory in the project. In that directory you‚Äôll find equivalent Java and Clojure versions.&lt;/p&gt;
    &lt;p&gt;You‚Äôll need Java 21 installed and API keys for OpenAI and Tavily (Tavily‚Äôs free tier is sufficient). Put the API keys in environment variables like so:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;export OPENAI_API_KEY=your_openai_key_here&lt;/p&gt;
          &lt;p&gt;export TAVILY_API_KEY=your_tavily_key_here&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To run the agent, clone Agent-o-rama and follow these instructions (for Java or Clojure, whichever you prefer):&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
          &lt;p&gt;8&lt;/p&gt;
          &lt;p&gt;9&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;# Java instructions&lt;/p&gt;
          &lt;p&gt;cd examples/java&lt;/p&gt;
          &lt;p&gt;./run-example com.rpl.agent.research.ResearchAgentExample&lt;/p&gt;
          &lt;p&gt;# Clojure instructions&lt;/p&gt;
          &lt;p&gt;cd examples/clj&lt;/p&gt;
          &lt;p&gt;lein repl&lt;/p&gt;
          &lt;p&gt;(require '[com.rpl.agent.research-agent :as research-agent] :reload)&lt;/p&gt;
          &lt;p&gt;(research-agent/run-agent)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This runs Rama‚Äôs ‚Äúin-process cluster‚Äù (IPC) and launches the research agent on it. You‚Äôll get a prompt at the terminal to enter a research topic. The agent will generate a set of analyst personas to analyze the topic, and you‚Äôll be prompted again whether you want to give feedback on the generated analysts. Once you tell the agent you have no more feedback, it will spend a few minutes generating the report, including using information it finds through web searches and through Wikipedia, and then the final report will be printed.&lt;/p&gt;
    &lt;p&gt;As the report is being generated or when it‚Äôs finished, you can open the Agent-o-rama UI at &lt;code&gt;http://localhost:1974&lt;/code&gt;

.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs an example back and forth:&lt;/p&gt;
    &lt;p&gt;If you click on the research agent in the UI, you‚Äôll see this:&lt;/p&gt;
    &lt;p&gt;The invoke there is what we just ran. Clicking on it brings up the trace for the invoke:&lt;/p&gt;
    &lt;p&gt;On the right is aggregated statistics of everything that happened during the agent‚Äôs execution. You can see how many tokens it used, and if it did any database reads/writes you‚Äôd see stats about those too. If the agent invokes other agents, you can see a breakdown of stats by agent as well.&lt;/p&gt;
    &lt;p&gt;Clicking on the ‚Äúwrite-report‚Äù node brings up a detailed trace of what happened when that node executed:&lt;/p&gt;
    &lt;p&gt;This node did one LLM call, and you can see the arguments to that LLM, what was returned, and stats on the call in the ‚ÄúOperations‚Äù section. The code for this node is just this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
          &lt;p&gt;8&lt;/p&gt;
          &lt;p&gt;9&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;.node("write-report", "finish-report", (AgentNode agentNode, String sections, String topic) -&amp;gt; {&lt;/p&gt;
          &lt;p&gt;ChatModel openai = agentNode.getAgentObject("openai");&lt;/p&gt;
          &lt;p&gt;String instructions = String.format(REPORT_WRITER_INSTRUCTIONS, topic, sections);&lt;/p&gt;
          &lt;p&gt;List&amp;lt;ChatMessage&amp;gt; chatMessages = Arrays.asList(&lt;/p&gt;
          &lt;p&gt;new SystemMessage(instructions),&lt;/p&gt;
          &lt;p&gt;new UserMessage("Write a report based upon these memos."));&lt;/p&gt;
          &lt;p&gt;String report = openai.chat(chatMessages).aiMessage().text();&lt;/p&gt;
          &lt;p&gt;agentNode.emit("finish-report", "report", report);&lt;/p&gt;
          &lt;p&gt;})&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
          &lt;p&gt;8&lt;/p&gt;
          &lt;p&gt;9&lt;/p&gt;
          &lt;p&gt;10&lt;/p&gt;
          &lt;p&gt;11&lt;/p&gt;
          &lt;p&gt;12&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;(aor/node&lt;/p&gt;
          &lt;p&gt;"write-report"&lt;/p&gt;
          &lt;p&gt;"finish-report"&lt;/p&gt;
          &lt;p&gt;(fn [agent-node sections topic]&lt;/p&gt;
          &lt;p&gt;(let [openai (aor/get-agent-object agent-node "openai")&lt;/p&gt;
          &lt;p&gt;instr (report-writer-instructions topic sections)&lt;/p&gt;
          &lt;p&gt;text (chat-and-get-text&lt;/p&gt;
          &lt;p&gt;openai&lt;/p&gt;
          &lt;p&gt;[(SystemMessage. instr)&lt;/p&gt;
          &lt;p&gt;(UserMessage. "Write a report based upon these memos.")])]&lt;/p&gt;
          &lt;p&gt;(aor/emit! agent-node "finish-report" "report" text)&lt;/p&gt;
          &lt;p&gt;)))&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This code says that the node‚Äôs name is ‚Äúwrite-report‚Äù, the node emits to the node ‚Äúfinish-report‚Äù, and the node‚Äôs implementation is the given function. The &lt;code&gt;agentNode&lt;/code&gt;

/

&lt;code&gt;agent-node&lt;/code&gt;

argument is how you interact with the graph to return a result, emit to other nodes, or get agent objects like models, database connections, or anything else. When you emit to other nodes, you simply say what node you want to emit to and what arguments to pass to that node. Agent nodes run on virtual threads, so they can be efficiently written in a blocking style like this.&lt;/p&gt;
    &lt;p&gt;That‚Äôs most of what‚Äôs involved in programming agents with Agent-o-rama! There‚Äôs a bit more to learn with aggregation and how to declare agent objects, and this is all documented on the programming agents guide. The rest of using Agent-o-rama is creating and managing datasets, running experiments, setting up online evaluation and other actions on production runs, and analyzing agent telemetry.&lt;/p&gt;
    &lt;p&gt;Also, you can see from this code and the trace that model calls are automatically traced ‚Äì this node didn‚Äôt have to record any tracing info explicitly. Though you can include your own info in traces with a simple API (see this Javadoc and this Clojuredoc).&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take a look at running this on a real cluster! Let‚Äôs quickly set up a cluster locally by following these instructions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest Rama release from here.&lt;/item&gt;
      &lt;item&gt;Unpack the release somewhere.&lt;/item&gt;
      &lt;item&gt; Run: &lt;code&gt;./rama devZookeeper &amp;amp;&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt; Run: &lt;code&gt;./rama conductor &amp;amp;&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt; Run: &lt;code&gt;./rama supervisor &amp;amp;&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt; Visit: &lt;code&gt;http://localhost:8888&lt;/code&gt;. When the page loads, the cluster is ready.&lt;/item&gt;
      &lt;item&gt;Download the latest Agent-o-rama release from here.&lt;/item&gt;
      &lt;item&gt;Unpack it somewhere.&lt;/item&gt;
      &lt;item&gt; Run: &lt;code&gt;./aor --rama /path/to/rama-root &amp;amp;&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Next, to deploy you need to build a jar first. Here‚Äôs how to build either the Java or Clojure version from the Agent-o-rama project:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;# For Java version &lt;/p&gt;
          &lt;p&gt;cd examples/java&lt;/p&gt;
          &lt;p&gt;mvn clean package -Dmaven.test.skip=true&lt;/p&gt;
          &lt;p&gt;# For Clojure version&lt;/p&gt;
          &lt;p&gt;cd examples/clj&lt;/p&gt;
          &lt;p&gt;lein uberjar&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The Java version will build &lt;code&gt;target/java-examples-with-dependencies.jar&lt;/code&gt;

, and the Clojure version will build

&lt;code&gt;target/agent-o-rama-examples-1.0.0-SNAPSHOT-standalone.jar&lt;/code&gt;

.&lt;/p&gt;
    &lt;p&gt;Next, to deploy the module just run this command:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
          &lt;p&gt;8&lt;/p&gt;
          &lt;p&gt;9&lt;/p&gt;
          &lt;p&gt;10&lt;/p&gt;
          &lt;p&gt;11&lt;/p&gt;
          &lt;p&gt;12&lt;/p&gt;
          &lt;p&gt;13&lt;/p&gt;
          &lt;p&gt;14&lt;/p&gt;
          &lt;p&gt;15&lt;/p&gt;
          &lt;p&gt;16&lt;/p&gt;
          &lt;p&gt;17&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;# Deploy the module (Java uberjar)&lt;/p&gt;
          &lt;p&gt;./rama deploy \&lt;/p&gt;
          &lt;p&gt;--action launch \&lt;/p&gt;
          &lt;p&gt;--jar /path/to/java-examples-with-dependencies.jar \&lt;/p&gt;
          &lt;p&gt;--module com.rpl.agent.research.ResearchAgentModule \&lt;/p&gt;
          &lt;p&gt;--tasks 4 \&lt;/p&gt;
          &lt;p&gt;--threads 2 \&lt;/p&gt;
          &lt;p&gt;--workers 1&lt;/p&gt;
          &lt;p&gt;# Deploy the module (Clojure uberjar)&lt;/p&gt;
          &lt;p&gt;./rama deploy \&lt;/p&gt;
          &lt;p&gt;--action launch \&lt;/p&gt;
          &lt;p&gt;--jar /path/to/agent-o-rama-examples-1.0.0-SNAPSHOT-standalone.jar \&lt;/p&gt;
          &lt;p&gt;--module com.rpl.agent.research-agent/ResearchAgentModule \&lt;/p&gt;
          &lt;p&gt;--tasks 4 \&lt;/p&gt;
          &lt;p&gt;--threads 2 \&lt;/p&gt;
          &lt;p&gt;--workers 1&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Now it‚Äôs up and running! You can view the agent in the UI at &lt;code&gt;http://localhost:1974&lt;/code&gt;

and play with it. From the agent screen you can invoke the agent with the arguments

&lt;code&gt;["", {"topic": "your topic here"}]&lt;/code&gt;

. On the trace, you‚Äôll be able to see any human input prompts the agent makes and respond to them there.&lt;/p&gt;
    &lt;p&gt;Rama handles all of storage, deployment, and scaling. There are no other dependencies needed to run this. Setting up a production cluster is only slightly more work, and there are one-click deploys for AWS and for Azure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resources&lt;/head&gt;
    &lt;p&gt;Check out these resources to learn more or get involved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tour of Agent-o-rama&lt;/item&gt;
      &lt;item&gt;Quickstart&lt;/item&gt;
      &lt;item&gt;Full documentation&lt;/item&gt;
      &lt;item&gt;Javadoc&lt;/item&gt;
      &lt;item&gt;Clojuredoc&lt;/item&gt;
      &lt;item&gt;Mailing list&lt;/item&gt;
      &lt;item&gt;#rama channel on Clojurians&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Agent-o-rama lets developers gain the benefits of Rama without needing to learn it. Rama‚Äôs distributed programming model is powerful but has a learning curve: it introduces a rich dataflow API and uses compound data structures for indexing instead of fixed data models. Agent-o-rama abstracts away those concepts into a familiar API so developers can take advantage of Rama‚Äôs strengths for the specific domain of building LLM agents.&lt;/p&gt;
    &lt;p&gt;For those who want to learn how to program Rama directly, Agent-o-rama also serves as a great example of Rama in practice. The backend is about 15K lines of code and the front-end about 11K, yet together they form a complete, end-to-end distributed system with a diverse feature set. Along with our Twitter-scale Mastodon implementation, it shows the breadth of what can be built with Rama.&lt;/p&gt;
    &lt;p&gt;We‚Äôd love to hear what you build with Agent-o-rama. Join the rama-user mailing list or the #rama channel on the Clojurians Slack to ask questions, share feedback, or discuss ideas with others using Agent-o-rama.&lt;/p&gt;
    &lt;p&gt;If you‚Äôd like to talk directly with us about Agent-o-rama, whether to exchange ideas, get technical guidance, or explore working together on building an LLM agent, you can book a call with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.redplanetlabs.com/2025/11/03/introducing-agent-o-rama-build-trace-evaluate-and-monitor-stateful-llm-agents-in-java-or-clojure/"/><published>2025-11-03T18:16:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45802459</id><title>Gallery of wonderful drawings our little thermal printer received</title><updated>2025-11-03T23:09:49.710682+00:00</updated><content>&lt;doc fingerprint="59d03e8ff57de69d"&gt;
  &lt;main&gt;
    &lt;p&gt;Back to GoodEnough.us This is a gallery of all the wonderful drawings our little printer received. Draw us something! Rene The Robot (Self Portrait) December 10, 2024 172.70.115.89 imthatwhodrawthis December 1, 2024 162.158.79.18 Dan C. November 27, 2024 108.162.216.235 Florian November 26, 2024 162.158.78.234 Kabir Kumar November 20, 2024 172.69.194.121 Charmaine „ÉÑ November 20, 2024 172.69.6.8 Rafael Enes November 20, 2024 172.68.102.237 (.‚ùõ ·¥ó ‚ùõ.) November 19, 2024 172.70.135.108 Lu November 19, 2024 162.158.78.231 werty November 18, 2024 108.162.216.131 Daniel November 18, 2024 108.162.216.130 dz November 16, 2024 162.158.79.62 MAx from Rennes (France) November 16, 2024 162.158.79.28 Jake November 16, 2024 162.158.78.249 Time + Pressure = Diamonds November 10, 2024 162.158.146.55 Sofia October 27, 2024 162.158.78.184 Ka Wai October 24, 2024 108.162.216.150 Francis October 24, 2024 162.158.79.146 ZEEP ZORP October 23, 2024 162.158.78.135 Jesse Cooke October 23, 2024 108.162.216.236 Alberto Gallwgo October 23, 2024 172.71.222.163 Loading more choice prints...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://guestbook.goodenough.us"/><published>2025-11-03T18:27:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45803306</id><title>FreakWAN: A floor-routing WAN implementing a chat over bare-LoRa (no LoRaWAN)</title><updated>2025-11-03T23:09:49.080128+00:00</updated><content>&lt;doc fingerprint="bc0d98104829b1ca"&gt;
  &lt;main&gt;
    &lt;p&gt;FreakWAN is an effort to create a LoRa-based open WAN network, completely independent from Internet and the cellular phones networks. The network built with FreakWAN has two main goals:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;To provide both a plaintext and an encrypted distributed chat system, that can be used by technology amateurs, or in places where internet is not available and during disasters.&lt;/item&gt;
      &lt;item&gt;As a side effect of the first goal, to create a robust protocol over LoRa to support other applications, like sensors data collection, home automation applications, not having the usual range limitations of OOK/FSK communcation, and so forth.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our goal is to cover parts of the Sicily with such a network. The code will be freely available to anyone wanting to build their own LoRa WANs on top of this software. The main features of our implementation and protocol are the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A distributed network based on LoRa and broadcast routing.&lt;/item&gt;
      &lt;item&gt;Basic chat features, ability to send medias (like small images).&lt;/item&gt;
      &lt;item&gt;Different group chat or data channels (including one-to-one chats) using encryption to separate them.&lt;/item&gt;
      &lt;item&gt;Configurable number of retransmissions with random delays.&lt;/item&gt;
      &lt;item&gt;First-hop acknowledges of messages sent.&lt;/item&gt;
      &lt;item&gt;Symmetric encryption with AES in CBC mode, with support for integrity detection and multiple concurrent keys: each group of clients knowing a given key is part of a virtual group. The network is collaborative for encrypted messages: even nodes that are not able to decrypt a given message can broadcast it, since the encrypted part is not vital to perform relaying of messages.&lt;/item&gt;
      &lt;item&gt;Sensing of nearby nodes, via &lt;code&gt;HELLO&lt;/code&gt;messages (advertising).&lt;/item&gt;
      &lt;item&gt;Bandwidth usage mitigation features.&lt;/item&gt;
      &lt;item&gt;Duty cycle tracking.&lt;/item&gt;
      &lt;item&gt;Local storage of messages in the device flash, with automatic deletion of old messages.&lt;/item&gt;
      &lt;item&gt;Simple home-made driver for the SX1276 and SX1262 LoRa chip. In general, no external dependencies. Runs with vanilla MicroPython installs.&lt;/item&gt;
      &lt;item&gt;OLED terminal alike output. OLED burning pixels protection.&lt;/item&gt;
      &lt;item&gt;CLI interface via USB serial and Bluetooth LE.&lt;/item&gt;
      &lt;item&gt;IRC interface: the device can work as a bot over the IRC protocol.&lt;/item&gt;
      &lt;item&gt;Simple to understand, hackable code base.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This code is currently a functional work in progress, designed to work with the following ESP32-based devices:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LILYGO TTGO T3 v2 1.6 LoRa module.&lt;/item&gt;
      &lt;item&gt;LILYGO TTGO T3-S3 v1.2 LoRa module (2.4 Ghz version not tested/supported).&lt;/item&gt;
      &lt;item&gt;LILYGO TTGO T Beam LoRa module.&lt;/item&gt;
      &lt;item&gt;LILYGO T-WATCH S3.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However changing the pins in the configuration, to adapt it to other ESP32 modules that have an SX1276, SX1262 LoRa chips, and an SSD1306 or ST7789 display (or no dislay, in headless mode), should be very little work. T-ECHO devices are also supported, even if with less features, in the C port of FreanWAN, under the &lt;code&gt;techo-port&lt;/code&gt; directory, but the T-ECHO port is still alpha quality software.&lt;/p&gt;
    &lt;p&gt;FreakWAN is implemented in MicroPython, making use only of default libraries.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install MicroPython on your device. Follow this instructions to get the right MicroPython version:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;MicroPython versions &amp;gt; 1.19.1 and &amp;lt; 1.22.2 have buggy bluetooth stack, so make sure to use 1.22.2 (or greater) or if you want to stick with something old, use 1.19.1.&lt;/item&gt;
      &lt;item&gt;Don't bother installing a MicroPython specific for the LILYGO devices. Just grab the standard ESP32 image (but not for the T3-S3, read more).&lt;/item&gt;
      &lt;item&gt;The T3-S3 (and probably the T-BEAM S3, which we don't support at the moment, because we don't have the device) have a 4MB flash which is not compatible with the default MicroPython image for 8MB flash. You can find a working image directly inside this repository under the &lt;code&gt;device&lt;/code&gt;directory.&lt;/item&gt;
      &lt;item&gt;To flash your device, follow the MicroPython instructions in the download page of your device. For the T3-S3, don't forget to press the boot button while powering it up, otherwise you will not be able to flash it. Then, with &lt;code&gt;esptool.py&lt;/code&gt;, perform the&lt;code&gt;erase_flash&lt;/code&gt;command followed by the&lt;code&gt;write_flash&lt;/code&gt;with the parameters specified in the MicroPython download page.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Clone this repository, and edit&lt;/p&gt;&lt;code&gt;wan_config.py&lt;/code&gt;to set your nickname and status message, set the frequency according to your device. Warning: make sure to set the right frequency based on the LoRa module you own, and make sure your antenna is already installed before using the software, or you may damage your hardware, (but I would like to report that we started the device with the wrong freuqnecy several times and nothing happened: still, proceed at your own risk).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Copy one of the files inside the&lt;/p&gt;&lt;code&gt;devices&lt;/code&gt;folder in the main folder as&lt;code&gt;device_config.py&lt;/code&gt;, for instance if I have a T3 v2 1.6 device, I will do:&lt;p&gt;cp devices/device_config.t3_v2_1.6.py ./device_config.py&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Transfer all the&lt;/p&gt;&lt;code&gt;.py&lt;/code&gt;files in the root directory of this project in your device. To transfer the files, you can use mpremote (&lt;code&gt;pip3 install mpremote&lt;/code&gt;should be enough), or an alternative (and slower, but sometimes more compatible) tool that we wrote, called talk32. Talk32 is not as fast as mpremote at transferring files, but sometimes mpremote does not work with certain devices and talk32 does (and the other way around).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(NOTE: you need the &lt;code&gt;:&lt;/code&gt; below, is not an error)&lt;/p&gt;
    &lt;code&gt;mpremote cp *.py :
&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;talk32 /dev/tty.usbserial001 put *.py
&lt;/code&gt;
    &lt;p&gt;Please note that you don't need both the command lines. Just one depending on the tool you use.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart your device: you can either power it off/on, or use &lt;code&gt;mpremote repl&lt;/code&gt;and hit&lt;code&gt;Ctrl_D&lt;/code&gt;to trigger a soft reset. Sometimes devices also have a reset button. If everything is fine you will see the splash screen and then the program version.&lt;/item&gt;
      &lt;item&gt;If you are using a T-WATCH S3, or other recent Lyligo devices based on ESP32-S3, and your splash screen freezes (the waves should move and then the splash screen should disappear, if everything works well), please try to disable BLE from &lt;code&gt;wan_config.py&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The two simplest ways to send commands to the device, write messages that will be broadcasted and also receive messages sent by other users, it is to use the USB or Bluetooth serial.&lt;/p&gt;
    &lt;p&gt;To obtain a serial command line interface, make sure the device is connected via an USB cable with your computer. Than connect to the device serial with &lt;code&gt;talk32&lt;/code&gt;, &lt;code&gt;minicom&lt;/code&gt;, &lt;code&gt;screen&lt;/code&gt; or whatever serial terminal you want to use.
Normally the bound rate is 115200. Example of command lines and tools you could use:&lt;/p&gt;
    &lt;code&gt;mpremote repl
&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;talk32 /dev/tty.usbserial001 repl
&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;screen /dev/tty.usbserial001 115200
&lt;/code&gt;
    &lt;p&gt;Of course the name of the device is just an example. Try &lt;code&gt;ls /dev/tty*&lt;/code&gt; to see the list of possible device names in your computer.&lt;/p&gt;
    &lt;p&gt;Once you connect, you will see the device logs, but you will also be able to send bang commands or messages to the chat (see below).&lt;/p&gt;
    &lt;p&gt;It is possible to use the device via Bluetooth LE, using one of the following applications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Android: install one of the many BLE UART apps available. We recommend the Serial Bluetooth Terminal app. It works great out of the box, but for the best experience open the settings, go to the Send tab, and select clear input on send. An alternative is nRF Toolbox, select the UART utility service, connect to the device and send a text message or just &lt;code&gt;!help&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;iPhone: BLE Terminal HM-10 works well and is free. There are other more costly options.&lt;/item&gt;
      &lt;item&gt;Linux Desktop: install Freakble following the project README.&lt;/item&gt;
      &lt;item&gt;For MacOS, there is a BLE UART app directly inside this software distribution under the &lt;code&gt;osx-bte-cli&lt;/code&gt;directory. Please read the README file that is there.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using one of the above, you can talk with the device, and chat with other users around, sending CLI commands. If you just type some text, it will be sent as message in the network. Messages received from the network are also shown in the serial console. If you send a valid command starting with the &lt;code&gt;!&lt;/code&gt; character, it will be executed as a command, in order to show information, change the device configuration and so forth. For now you can use:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;!automsg&lt;/code&gt;[on/off] to disable enable automatic messages used for testing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!bat&lt;/code&gt;to show the battery level.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!preset &amp;lt;name&amp;gt;&lt;/code&gt;to set a LoRa preset. Each preset is a specific spreading, bandwidth and coding rate setup. To see all the available presets write&lt;code&gt;!preset help&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!sp&lt;/code&gt;,&lt;code&gt;!bw&lt;/code&gt;,&lt;code&gt;!cr&lt;/code&gt;to change the spreading, bandwidth and coding rate independently, if you wish.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!pw&lt;/code&gt;changes the TX power. Valid values are from 2 to 20 (dbms). Default is 17dbms.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!ls&lt;/code&gt;shows nodes around. This is the list of nodes that your node is able to sense, via HELLO messages.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!font big|small&lt;/code&gt;will change between an 8x8 and a 5x7 (4x6 usable area) font.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!image &amp;lt;image-file-name&amp;gt;&lt;/code&gt;send an FCI image (see later about images).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!last [&amp;lt;count&amp;gt;]&lt;/code&gt;show the last messages received, taking them from the local storage of the device.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!config [save|reset]&lt;/code&gt;to save (or reset) certain configuration parameters (LoRa radio parameters, automsg, irc, wifi, ...) that will be reloaded at startup.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!irc &amp;lt;stop|start&amp;gt;&lt;/code&gt;starts or stops the IRC interface.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!telegram &amp;lt;start|stop|token&amp;gt;&lt;/code&gt;starts, stops and sets the token of the Telegram bot.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!wifi help&lt;/code&gt;, to see all the WiFi configuration subcommands. Using this command you can add and remove WiFi networks, connect or disconnect the WiFi (required for the IRC and Telegram interface), and so forth.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!quiet &amp;lt;yes|no&amp;gt;&lt;/code&gt;, to enable quiet mode (default is off). In this mode, the device sends only the smallest amount of data needed, that is the data messages that we want to send. No ACKs are sent in reply to data messages, nor HELLO messages to advertise our presence in the network. Packets are not relayed in this mode, nor data is transmitted multiple times. Basically this mode is designed to save channel bandwidth, at the expense of advanced FreakWAN features, when there are many active devices and we want to make sure the LoRa channel is not continuously busy.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;!b0&lt;/code&gt;, this is the same as pressing the button 0 on the devices (if they have one). Will switch the device screen to the next view.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;New bang commands are added constantly, so try &lt;code&gt;!help&lt;/code&gt; to see what is available. We'll try to take this README in sync, especially after the first months of intense development will be finished.&lt;/p&gt;
    &lt;p&gt;When FreakWAN is located in some fixed location with WiFi access, it is possible to put it online as a Telegram bot. This way it is possible to receive the messages the device receives via LoRa as Telegram messages, and at the same time it is possible to send commands and messages writing to the bot.&lt;/p&gt;
    &lt;p&gt;To use this feature, follow the instructions below:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Create your bot using the Telegram @BotFather.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;After obtaining your bot token (it's basically the bot API key) use the following commands in the FreakWAN cli (either via USB or BLE) or alternatively edit&lt;/p&gt;&lt;code&gt;wan_config.py&lt;/code&gt;to set the same parameters.&lt;p&gt;!wifi add networkname password !wifi start networkname !telegram token !telegram start&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now use your Telegram application in order to sent &lt;code&gt;!help&lt;/code&gt; to the bot, and wait for the reply. If you can receive the reply correclty, Freakwan will also set the target of your messages, that is, the account you used to talk with the bot the first time. Now you are ready to save your configuration with:&lt;/p&gt;
    &lt;code&gt;!config save
&lt;/code&gt;
    &lt;p&gt;FreakWAN is able to join IRC, as a bot. It can receive messages and commands via IRC, and also show messages received via LoRa into an IRC channel. Edit &lt;code&gt;wan_config.py&lt;/code&gt; and enable IRC by setting the enabled flag to True, and configuring a WiFi network. Upload the modified file inside the device and restart it. Another way to enable IRC is to use bang commands via Bluetooth, like that:&lt;/p&gt;
    &lt;code&gt;!wifi add networkname password
!wifi start networkname
!irc start
!config save (only if you want to persist this configuration)
&lt;/code&gt;
    &lt;p&gt;The device, by default, will enter the &lt;code&gt;##Freakwan-&amp;lt;nickname&amp;gt;&lt;/code&gt; channel of &lt;code&gt;irc.libera.chat&lt;/code&gt; (please, note the two &lt;code&gt;#&lt;/code&gt; in the channel name), and will listen for commands there. The same commands you can send via Bluetooth are also available via IRC. Because of limitations with the ESP32 memory and the additional MicroPython memory usage, SSL is not available, so FreakWAN will connect to IRC via the TCP port 6667, which is not encrypted.&lt;/p&gt;
    &lt;p&gt;By default LoRa messages are sent in clear text and will reach every device that is part of the network, assuming it is configured with the same LoRa frequency, spreading, bandwidth and coding rate. However, it is possible to send encrypted messages that will reach only other users with a matching symmetric key. For instance, if Alice and Bob want to communicate in a private way, they will set the same key, let's say &lt;code&gt;abcd123&lt;/code&gt;, in their devices. Bob will do:&lt;/p&gt;
    &lt;code&gt;!addkey alice abcd123
&lt;/code&gt;
    &lt;p&gt;While Alice will do:&lt;/p&gt;
    &lt;code&gt;!addkey bob abcd123
&lt;/code&gt;
    &lt;p&gt;(Note: they need to use much longer and hard to guess key! A good way to generate a key is to to combine a number of words and numbers together, or just generate a random 256 bit hex string with any available tool).&lt;/p&gt;
    &lt;p&gt;Now, Alice will be able to send messages to Bob, or the other way around, just typing:&lt;/p&gt;
    &lt;code&gt;#bob some message
&lt;/code&gt;
    &lt;p&gt;Bob will see, in the OLED display of the device, and in the Android application, if connected via BTE, something like that:&lt;/p&gt;
    &lt;code&gt;#alice Alice&amp;gt; ... message ...
&lt;/code&gt;
    &lt;p&gt;Encrypted messages that are correctly received and decoded are shown as:&lt;/p&gt;
    &lt;code&gt;#&amp;lt;keyname&amp;gt; Nick&amp;gt; message
&lt;/code&gt;
    &lt;p&gt;Each device can have multiple keys. The device will try to decrypt each encrypted message with all the keys stored inside the key chain (the device flash memory, under the &lt;code&gt;keys&lt;/code&gt; directory -- warning! keys are not encrypted on the device storage).&lt;/p&gt;
    &lt;p&gt;If many users will set the same key with the same name, they are effectively creating something like an IRC channel over LoRa, a chat where all such individuals can talk together.&lt;/p&gt;
    &lt;p&gt;This is the set of commands to work with encrypted messages:&lt;/p&gt;
    &lt;code&gt;#&amp;lt;keyname&amp;gt; some message     -- send message with the given key.
!addkey keyname actual-key-as-a-string  -- add the specified key.
!delkey keyname             -- remove the specified key.
!keys                       -- list available keys.
!usekey keyname             -- set the specified key as default to send all the next messages, without the need to resort to the #key syntax.
!nokey                      -- undo !usekey, return back to plaintext.
&lt;/code&gt;
    &lt;p&gt;FreakWAN implements its own very small, losslessy compressed 1 bit images, as a proof of concept that we can send small media types over LoRa. Images are very useful in order to make the protocol more robust when working with very long packets (that have a very large time on air). Inside the &lt;code&gt;fci&lt;/code&gt; directory of this repository you will find the specification of the image format and run length compression used, and a tool to convert small PNG files (255x255 max) into FCI images. For now, only images that are up to 200 bytes compressed can be transmitted.&lt;/p&gt;
    &lt;p&gt;Once you have the FCI images, you should copy them into the &lt;code&gt;images&lt;/code&gt; directory inside your device (again, using ampy or talk32 or any other tool). Then you can send the images using the &lt;code&gt;!image&lt;/code&gt; command using the bluetooth CLI.&lt;/p&gt;
    &lt;p&gt;You can find a couple test FCI images under &lt;code&gt;fci/testfci&lt;/code&gt;. They are all less than 200 bytes, so it is possible to send them as FreakWAN messages.&lt;/p&gt;
    &lt;p&gt;Right now, for the LILYGO TTGO T3 device, we support reading the battery level and shutting down the device when the battery is too low, since the battery could be damaged if it is discharged over a certain limit. To prevent such issues, when voltage is too low, the device will go in deep sleep mode, and will flash the led 3 times every 5 seconds. Then, once connected again to the charger, when the battery charges a bit, it will restart again.&lt;/p&gt;
    &lt;p&gt;For the T Beam, work is in progress to provide the same feature. For now it is better to disable the power management at all, by setting &lt;code&gt;config['sleep_battery_perc']&lt;/code&gt; to 0 in the &lt;code&gt;wan_config.py&lt;/code&gt; file.&lt;/p&gt;
    &lt;p&gt;If you plan to power your device with a battery that is not 3.7v, probably it's better to disable this feature from the configuration, or the device may shut down because it is sensing a too low voltage, assuming the battery is low.&lt;/p&gt;
    &lt;p&gt;The rest of this document is useful for anybody wanting to understand the internals of FreakWAN. The kind of messages it sends, how messages are relayed in order to reach far nodes, the retransmission and acknowledge logic, and so forth.&lt;/p&gt;
    &lt;p&gt;The goals of the design is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Allow far nodes to communicate using intermediate nodes.&lt;/item&gt;
      &lt;item&gt;To employ techniques to mitigate missed messages due to the fact LoRa is inherently half-duplex, so can't hear messages when transmitting.&lt;/item&gt;
      &lt;item&gt;Do 1 and 2 considering the available data rate, which is very low.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The low level (layer 2) format is the one with the explicit header selected, so it is up to the chip to add a length, a CRC and so forth. This layer is not covered here, as from the SX1276 / SX1262 driver we directly get the clean bytes received. So this covers layer 3, that is the messages format implemented by FreakWAN.&lt;/p&gt;
    &lt;p&gt;The first byte is the message type byte. The following message types are defined:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MessageTypeData = 0&lt;/item&gt;
      &lt;item&gt;MessageTypeAck = 1&lt;/item&gt;
      &lt;item&gt;MessageTypeHello = 2&lt;/item&gt;
      &lt;item&gt;MessageTypeBulkStart = 3&lt;/item&gt;
      &lt;item&gt;MessageTypeBulkData = 4&lt;/item&gt;
      &lt;item&gt;MessageTypeBulkEND = 5&lt;/item&gt;
      &lt;item&gt;MessageTypeBulkReply = 6&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The second byte of messages of all the message types is the flag byte. Bits have the following meaning:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bit 0: &lt;code&gt;Relayed&lt;/code&gt;. Set if the message was repeated by some node that is not the originator of the message. Relayed messages are not acknowledged.&lt;/item&gt;
      &lt;item&gt;Bit 1: &lt;code&gt;PleaseRelay&lt;/code&gt;. If this flag is set, other receivers of the message will try to repeat the message, so that it can travel further in the WAN.&lt;/item&gt;
      &lt;item&gt;Bit 2: &lt;code&gt;Fragment&lt;/code&gt;. This flag means that this message is a fragment of many, that should be reassembled in order to retrieve the full Data message.&lt;/item&gt;
      &lt;item&gt;Bit 3: &lt;code&gt;Media&lt;/code&gt;. For message of type 'Data' this flag means that the message is not text, but some kind of media. See the Data messages section for more information.&lt;/item&gt;
      &lt;item&gt;Bit 4: &lt;code&gt;Encr&lt;/code&gt;. For messages of type 'Data' this flag means that the message is encrypted.&lt;/item&gt;
      &lt;item&gt;Bit 5-7: Reserved for future uses. Should be 0.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Currently not all the message types are implemented.&lt;/p&gt;
    &lt;p&gt;Format:&lt;/p&gt;
    &lt;code&gt;+--------+---------+---------------+-------+-----------+------------------//
| type:8 | flags:8 | message ID:32 | TTL:8 | sender:48 | Message string:...
+--------+---------+---------------+-------+-----------+------------------//
&lt;/code&gt;
    &lt;p&gt;Note that there is no message length, as it is implicitly encoded in the previous layer. The message string is in the following format:&lt;/p&gt;
    &lt;code&gt;+--------+---------------------//
| nlen:8 | nick+message text
+--------+---------------------//
&lt;/code&gt;
    &lt;p&gt;Where &lt;code&gt;nlen&lt;/code&gt; is a byte representing the nick name length inside the
message, as an unsigned 8 bit integer, and the rest is just the nickname
of the specified length concatenated with the actual message text, without
any separator, like in the following example:&lt;/p&gt;
    &lt;code&gt;"\x04AnnaHey how are you?"
&lt;/code&gt;
    &lt;p&gt;The TTL is set to 255 normally, and decreased at every retransmission. The sender ID is the HMAC returned by the device API, while the 32 bit message ID is generated randomly, and is used in order to mark a message as already processed, in order to discard duplicates (and there are many since the protocol uses broadcasted retransmissions in order to build the WAN).&lt;/p&gt;
    &lt;p&gt;Note that on retransmissions of the same message by other nodes, with the scope of reaaching the whole network, the message sender remains set to the same sender of the original message, that is, the device that created the message the first time. So there is no way to tell who sent a given retransmission of a given message.&lt;/p&gt;
    &lt;p&gt;Data messages may contain media in case this flag is set in the header of the message:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bit 3: &lt;code&gt;Media&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When this happens, the data inside the message is not some text in the form &lt;code&gt;nick&lt;/code&gt;+&lt;code&gt;message&lt;/code&gt;. Instead the first byte of the message is the media type ID, from 0 to 255. Right now only two media types are defined:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Media type 0: FreakWAN Compressed Image (FCI). Small 1 bit color image.&lt;/item&gt;
      &lt;item&gt;Media type 1: Sensor reading.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;+--------+------//------+-----------+-------------+----//
| type:8 | other fields | sender:48 | mediatype:8 | ... media data ...
+--------+------//------+-----------+-------------+----//
&lt;/code&gt;
    &lt;p&gt;Devices receiving this message should try to show the bitmap on the screen, if possible, or if the image is too big or they lack any graphical display ability, some text should be produced to make the user aware that the message contains an image.&lt;/p&gt;
    &lt;p&gt;The ACK message is used to acknowledge the sender that some nearby device actually received the message sent. ACKs are sent only when receiving messages of type: DATA, and only if the &lt;code&gt;Relayed&lt;/code&gt; flag is not set.&lt;/p&gt;
    &lt;p&gt;The goal of ACK messages are two:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;They inform the sender of the fact at least some near nodes (immediately connected hops) received the message. The sender can't know, just by ACKs, the total reach of the message, but it will know if the number of receivers is non-zero.&lt;/item&gt;
      &lt;item&gt;Each sender takes a list of directly connected nodes, via the HELLO messages (see later in this document). When a sender transmits some data, it will resend it multiple times, in order to make delivery more likely. To save channel time, when a sender receives an ACK from all the known neighbor nodes, it must suppress further retransmissions of the message. In practice this often means that, out of 3 different transmission attempts, only one will be performed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The reason why nodes don't acknowledge with ACKs messages that are relayed (and thus have the &lt;code&gt;Relayed&lt;/code&gt; flag set) is the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We can't waste channel time to make the sender aware of far nodes that received the message. For each message we would have to produce &lt;code&gt;N-1&lt;/code&gt;ACKs (with N being the number of nodes), and even worse such ACKs would be relayed as well to reach the sender. This does not make sense, in practice: LoRa bandwidth is tiny. So the only point of sending ACKs to relayed messages would be to suppress retransmissions of relayed messages: this, however, is used in the first hop (as described before) only because we want to inform the original sender of the fact somebody received the message. However using this mechanism to just suppress retransmissions is futile: often the ACKs would waste more channel bandwidth than the time saved.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Format:&lt;/p&gt;
    &lt;code&gt;+--------+---------+---------------+-----------------+---------------+
| type:8 | flags:8 | message ID:32 | 8 bits ack type | 48 bit sender |
+--------+---------+---------------+-----------------+---------------+
&lt;/code&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The type id is MessageTypeAck&lt;/item&gt;
      &lt;item&gt;Flags are set to 0. Ack messages should never be repeated.&lt;/item&gt;
      &lt;item&gt;The 32 bit message ID is the ID of the acknowledged message. ACKs don't have a message ID for the ACK itself, as they are fire and forget and it would not be useful.&lt;/item&gt;
      &lt;item&gt;The ACK type is the message type of the original message we are acknowledging.&lt;/item&gt;
      &lt;item&gt;Sender is the sender node, the one that is acknowledging the message, so this is NOT the sender of the original massage. The sender field is used so that who sent the acknowledged message can know which node acknowledged it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This message has the unique goal of advertising our presence to other devices in the network. This way, when a new device, part of the WAN, is powered on, it can tell if it is alone or surrounded by one or more other participants that are near enough to be received.&lt;/p&gt;
    &lt;p&gt;Hello messages are sent periodically, with a random period between 60000 and 120000 milliseconds (one to two minutes).&lt;/p&gt;
    &lt;p&gt;Devices receiving HELLO messages will compile a list of neighbors. A device is removed from the list if we don't receive a HELLO message from it for 10 minutes (this means we need to miss many successive hello messages, in order to remove a device -- this is an important point, since we need to account for the high probability of losing messages for being in TX mode while some other node broadcasts).&lt;/p&gt;
    &lt;p&gt;Format:&lt;/p&gt;
    &lt;code&gt;+--------+---------+---------------+--------+------------\\
| type:8 | flags:8 | 48 bit sender | seen:8 | status message
+--------+---------+---------------+--------+------------\\
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The type id is set to the HELLO message type.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flags are currently unused for the HELLO message.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The sender is the device ID of the sender.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seen is the number of devices this device is currently sensing, that is, the length of its neighbors list.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The status message is exactly like in the DATA message format: a string composed of one byte length of the nickname, and then the nickname of the owner and the message that was set as status message. Like:&lt;/p&gt;
        &lt;p&gt;"\x07antirezHi there! I'm part of FreakWAN."&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Data messages with the &lt;code&gt;PleaseRelay&lt;/code&gt; flag set are retransmitted by the nodes receiving them. The retransmission is the fundamental way in which the WAN is built. Imagine the following FreakWAN set of devices:&lt;/p&gt;
    &lt;code&gt;A &amp;lt;------ 10 km -------&amp;gt; B &amp;lt;----- 10km -----&amp;gt; C
&lt;/code&gt;
    &lt;p&gt;For a message sent by A to reach C, if we imagine a range of, for instance, 12 km, When B receives the messages created by A it must repeat the messages, so that C can also receive them.&lt;/p&gt;
    &lt;p&gt;To do so, FreakWAN uses the following mechanism:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A data message that has the &lt;code&gt;PleaseRelay&lt;/code&gt;bit set, when received, is retransmitted multiple times, assuming its TTL is still greater than 1. The TTL of the message is decremented by one, the&lt;code&gt;Relayed&lt;/code&gt;flag is set in the message, finally the message is sent again as it is, without changing the sender address, but maintaining the original one.&lt;/item&gt;
      &lt;item&gt;Devices may chose to avoid retransmitting messages with a too high RSSI, in order to avoid using precious channel time without a good reason. It is of little interest that two very nearby devices retransmit their messages.&lt;/item&gt;
      &lt;item&gt;Retransmitted messages have the &lt;code&gt;Relayed&lt;/code&gt;flag set, so ACKs are not transmitted by the receivers of those messages. FreakWAN ACKs only serve to inform the originator of the message that some neighbor device received the message, but are not used in order to notify of the final destinations of the message, as this would require a lot of channel time and is quite useless. For direct messages between users, when they will be implemented, the acknowledge of reception can be created on top of the messaging system itself, sending an explicit reply.&lt;/item&gt;
      &lt;item&gt;Each message received and never seen before is relayed N times, with N being a configuration inside the program defaulting to 3. However users may change it, depending on the network nodes density and other parameters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FreakWAN implementations are required to implement Listen Before Talk, in order to avoid sending messages if they detect some other valid LoRa transmission (either about FreakWAN or not) currently active. In this implementation, this feature is accomplished by reading the LoRa radio status register and avoiding transmitting if the set of bits reports an incoming packet being received.&lt;/p&gt;
    &lt;p&gt;LBT is a fundamental improvement for the performance of this protocol, since a major issue with this kind of routing, where every packet is sent and then relayed to everybody on the same frequency, is packet collisions.&lt;/p&gt;
    &lt;p&gt;FreakWAN default mode of operation is as unencrypted everybody-can-talk open network. In this mode, messages can be spoofed, and different persons (nicks) or nodes IDs can be impersonated by other devices very easily.&lt;/p&gt;
    &lt;p&gt;For this reason it is also possible to use encryption with pre-shared symmetric keys. Because of the device limitations and the standard library provided by MicroPython, we had to use an in-house encryption mode based on AES and SHA256.&lt;/p&gt;
    &lt;p&gt;Each device can store multiple symmetric keys, associated with a key name. Every time an encrypted message is received, all the keys are tested against the packet, and if a matching key is found (see later about the mechanism to validate the key) the message is correctly received, and displayed with the additional information of the key name, in order to make the user aware that this is an encrypted message that was decrypted with a specific key.&lt;/p&gt;
    &lt;p&gt;So, for example, if a key is shared between only two users, Alice and Bob, then Alice will store the &lt;code&gt;xyz&lt;/code&gt; key with the name "BoB", ad Bob
will store the same &lt;code&gt;xyz&lt;/code&gt; key with the name "Alice". Every time Alice
receives an encrypted message with such key, it will see:&lt;/p&gt;
    &lt;code&gt;#Bob bob&amp;gt; Hi Alice, how are you?
&lt;/code&gt;
    &lt;p&gt;Where the first part is &lt;code&gt;#&amp;lt;keyname&amp;gt;&lt;/code&gt;, and the rest of the message is
the normal message, nick and text, or media type, as normally displayed.&lt;/p&gt;
    &lt;p&gt;Similarly if the same key is shared among a group of users, the effect will be to participate into a group chat.&lt;/p&gt;
    &lt;p&gt;Keys must be shared using a protected channel: either via messaging systems like Whatsapp or Signal, or face to face with the interested users. Optionally the system may decide to encrypt local keys using a passphrase, so that keys can't be extracted from the device when it is non operational.&lt;/p&gt;
    &lt;p&gt;Only data messages are encrypted. ACKs, HELLO and other messages remain unencrypted.&lt;/p&gt;
    &lt;p&gt;The first four standard header fields of an encrypted packet are not encrypted at all: receivers, even without a key at hand, need to be able to check the message type and flags, the TTL (in case of relay), the message UID (to avoid reprocessing) and so forth. The only difference between the first 7 bytes (message type, flags, UID, TTL) of an ecrpyted and unencrypted message is that the &lt;code&gt;Encr&lt;/code&gt; flag is set. Then, if such
flag is set and the packet is thus encrypted, a 4 bytes initialization
vector (IV) follows. This is the unencrypted part of the packet. The
encrypted part is as the usual data as found in the DATA packet type: 6 bytes
of sender and the data payload itself. However, at the end of the packet,
there is an additional (not encrypted) 10 bytes of HMAC (truncated
HMAC-256), used to check integrity, and even to see if a given key is actually
decrypting a given packet correctly. The checksum computation is specified later.&lt;/p&gt;
    &lt;p&gt;This is the representation of the message described above:&lt;/p&gt;
    &lt;code&gt;+--------+---------+-------+-------+-------+-----------+--//--+------+
| type:8 | flags:8 | ID:32 | TTL:8 | IV:32 | sender:48 | data | HMAC |
+--------+---------+-------+-------+-------+-----------+--//--+------+
                                           |                  |
                                           `- Encrypted part -'
&lt;/code&gt;
    &lt;p&gt;The 'IV' is the initialization vector for the CBC mode of AES, that is the algorithm used for encryption. However it is used together with all the unencrypted header part, from the type field, at byte 0, to the last byte of the IV. So the initialization vector used is a total 11 bytes, of which at least 64 bits of pseudorandom data.&lt;/p&gt;
    &lt;p&gt;The final 10 bytes HMAC is computed using HMAC-SHA256, truncated to 10 bytes, but the last 4 bits are set to the padding length of the message, so actually of the 80 bits HMAC, only the first 76 are used. The last byte least significant four bits tell us how many bytes to discard from the decrypted payload, because of AES padding.&lt;/p&gt;
    &lt;p&gt;The AES key and the HMAC key are different, but derived from the same master key. The derivation is performed as such: Given the unique key &lt;code&gt;k&lt;/code&gt;, we derive the two keys with as:&lt;/p&gt;
    &lt;code&gt;aes-key = first 16 bytes of HMAC-SHA256(k,"AES14159265358979323846")
mac-key = HMAC-SHA256(k,"MAC26433832795028841971")
&lt;/code&gt;
    &lt;p&gt;To encrypt:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build the first four fields of the packet header as described above.&lt;/item&gt;
      &lt;item&gt;Renerate a random 4 bytes IV and append it.&lt;/item&gt;
      &lt;item&gt;Set TTL to 0 (but save the original value), and clear the &lt;code&gt;Relayed&lt;/code&gt;flag bit, also saving the original flag value.&lt;/item&gt;
      &lt;item&gt;Encrypt the payload with AES in CBC mode, using aes-key as key, and using as IV the first 11 bytes of the packet. Pad the packet with zeroes so that it is multiple of 16 bytes (AES block). Remember the padding used to reach the AES block size as "PADLEN". Append the encrypted payload to the packet.&lt;/item&gt;
      &lt;item&gt;Perform HMAC-SHA256(mac-key,packet) of all the packet built so far.&lt;/item&gt;
      &lt;item&gt;Append the first 10 bytes of the HMAC to the packet, but replace the last 4 bits with PADLEN as unsigned 4 bit integer.&lt;/item&gt;
      &lt;item&gt;Restore TTL and &lt;code&gt;Relayed&lt;/code&gt;flag.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Decrypting is very similar. When a packet arrives, we clear the TTL and &lt;code&gt;Relayed&lt;/code&gt; bit (saving the original values). We also store the last 4 bits
of the HMAC as PADLEN, and clear those bits. Then, for all the all the keys
we have in memory, one after the other, we see if we can find one for which
we can recompute the HMAC of the message (minus the last 10 bytes), trucate
it to 10 bytes, clearing the last 4 bits, and check if it matches with the
final 10 bytes of the packet.&lt;/p&gt;
    &lt;p&gt;If a match is found, we can decrpyt the message with AES in CBC mode, discard the final PADLEN zero bytes (checking they are actually zero as an additional check against implementation bugs), and process it.&lt;/p&gt;
    &lt;p&gt;The receiver of the packet has all the information required in order to relay the packet: we want the network to be collaborative even for messages that are not public. If the PleaseRelay flag is set, nodes should retransmit the message as usually, decrementing that TTL and setting the &lt;code&gt;Relayed&lt;/code&gt; flag: the TTL and this flag are not part of the CHECKSUM computation, nor of the IV (they are set to 0), so can be modified by intermediate nodes without invalidating the packet. Similarly, the message UID is exposed in the unencrypted header, so
nodes without a suitable key for the message can yet avoid re-processing the
message just saving its UID in the messages cache.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The encryption scheme described here was designed in order to use few bytes of additional space and the only encryption primitive built-in in MicroPython that was stable enough: the SHA256 hash and AES.&lt;/item&gt;
      &lt;item&gt;Because of the device and LoRa packets size and bandwidth limitations, the IV is shorter than one would hope. However it is partially compensated by the fact that the message UID is also part of the set of bytes used as initialization vector (see the encryption algorithm above). So the IV is actually at least 64 bits of pseudorandom data. For the attacker, it will be very hard to find two messages with the same IV, and even so the information disclosed would be minimal.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;sender&lt;/code&gt;field of the message is part of the encrypted part, thus encrypted messages don't discose nor the sender, that is encrypted, nor the received, that is implicit (has the key) of the message.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;LoRa packets are limited to 256 bytes. There is no way to go over such limitation (it is hardcoded in the hardware), and it would also be useless, since the long time on air means that, after a certain length, it is hard to take a good frequency reference: communication errors would be inevitable.&lt;/p&gt;
    &lt;p&gt;So the FreakWAN protocol supports fragmentation, for use cases when it is useful to transmit messages up to a few kilobytes at max. Fragmentation is only supported for DATA type message (the other message types don't need, to be larger than the LoRa packet size), and works both for clear text and encrypted messages.&lt;/p&gt;
    &lt;p&gt;In the sender size, when the total length of the packet would be more than &lt;code&gt;MAXPACKET&lt;/code&gt; total bytes (that may be configured inside the app), the data
payload is split in roughly equally sized packets. The last packet may have a
byte more in case the data length is not multiple of the number of packets.&lt;/p&gt;
    &lt;p&gt;Important: &lt;code&gt;MAXPACKET&lt;/code&gt; must be choosen so that it is always possible to transmit
two bytes more than its value, since the data section will have two additional
bytes used for the fragmentation metadata.&lt;/p&gt;
    &lt;p&gt;So, for instance, if the data section (nick + data o media) of a DATA packet is 1005 bytes, and &lt;code&gt;MAXPACKET&lt;/code&gt; is set to 200 bytes, the number of total packets
required is the integer result of the following division:&lt;/p&gt;
    &lt;code&gt;NUMPACKETS = (DATALEN+MAXPACKET-1) / MAXPACKET
&lt;/code&gt;
    &lt;p&gt;That is:&lt;/p&gt;
    &lt;code&gt;NUMPACKETS = (1005+199)/200 = 6
&lt;/code&gt;
    &lt;p&gt;Each of the packets will have the following size:&lt;/p&gt;
    &lt;code&gt;BASESIZE = 1005/6 = 167
&lt;/code&gt;
    &lt;p&gt;However &lt;code&gt;167*6&lt;/code&gt; = just 1002 bytes. So we also calculate a reminder size:&lt;/p&gt;
    &lt;code&gt;REM = DATALEN - BASESIZE*NUMPACKETS
REM = 1005 - (167*6) = 3
&lt;/code&gt;
    &lt;p&gt;And we add a single additional byte of data to the first REM packets we generate during the fragmentation, so the length of the packets will be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Packet 1: 168&lt;/item&gt;
      &lt;item&gt;Packet 2: 168&lt;/item&gt;
      &lt;item&gt;Packet 3: 168&lt;/item&gt;
      &lt;item&gt;Packet 4: 167&lt;/item&gt;
      &lt;item&gt;Packet 5: 167&lt;/item&gt;
      &lt;item&gt;Packet 6: 167&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That is a total of 1005 bytes. The strategy used attempts at generating packets of roughly the same size, so that each packet has a better probability of being transmitted correctly. For our use case, to transmit the first N-1 full packets at the maximum length, and then a final small packet, would not be optimal.&lt;/p&gt;
    &lt;p&gt;Each fragment will be sent as a DATA message is exactly like a normal DATA message, with the following differences:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The &lt;code&gt;Fragment&lt;/code&gt;flag is set in the header flags section.&lt;/item&gt;
      &lt;item&gt;At the end of the fragment data, there are two 8 bit unsigned integers, appended to the data itself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For instance, in the example above, the first packet would have 168 bytes of data terminated by two additional bytes:&lt;/p&gt;
    &lt;code&gt;//-----------------------+------------+-------------+
... 168 byts of data ... | frag-num:8 | tot-frags:8 |
/.-----------------------+------------+-------------+
&lt;/code&gt;
    &lt;p&gt;Where &lt;code&gt;frag-num&lt;/code&gt; is the number of the fragment, identifying its position
among all the fragmnets, and &lt;code&gt;tot-frags&lt;/code&gt; is the total number of
fragments. The total length of the data is implicit, and is not direcly
available.&lt;/p&gt;
    &lt;p&gt;The receiver will accumulate (for a given maximum time) message fragments having the same Message ID field value. Once all the fragments are received, the origianl message is generated from the fragments, where the &lt;code&gt;Fragment&lt;/code&gt; flag is cleared, the data sections of all the fragments are glued
together (but discarding the last two bytes of each packet), and finally the
message is passed for processing to the normal FreakWAN path inside the
FreakWAN stack. The software should make sure that fragments don't accumulate
forever in case some fragment is missing: after a given time, if no full
reassembly was possible, fargments should expire and the memory should be
freed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/antirez/freakwan"/><published>2025-11-03T19:28:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45803358</id><title>&lt;/&gt; Htmx ‚Äì The Fetch()ening</title><updated>2025-11-03T23:09:48.765771+00:00</updated><content>&lt;doc fingerprint="3e519854fda60ad0"&gt;
  &lt;main&gt;&lt;p&gt;OK, I said there would never be a version three of htmx.&lt;/p&gt;&lt;p&gt;But, technically, I never said anything about a version four‚Ä¶&lt;/p&gt;&lt;p&gt;In The Future of htmx I said the following:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We are going to work to ensure that htmx is extremely stable in both API &amp;amp; implementation. This means accepting and documenting the quirks of the current implementation.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Earlier this year, on a whim, I created fixi.js, a hyperminimalist implementation of the ideas in htmx. That work gave me a chance to get a lot more familiar with the &lt;code&gt;fetch()&lt;/code&gt; and, especially, the
async infrastructure
available in JavaScript.&lt;/p&gt;&lt;p&gt;In doing that work I began to wonder if that, while the htmx API is (at least reasonably) correct, maybe there was room for a more dramatic change of the implementation that took advantage of these features in order to simplify the library.&lt;/p&gt;&lt;p&gt;Further, changing from ye olde &lt;code&gt;XMLHttpRequest&lt;/code&gt;
(a legacy of htmx 1.0 IE support) to &lt;code&gt;fetch()&lt;/code&gt; would
be a pretty violent change, guaranteed to break at least some stuff.&lt;/p&gt;&lt;p&gt;So I began thinking: if we are going to consider moving to fetch, then maybe we should also use this update as a chance address at least some of the quirks &amp;amp; cruft that htmx has acquired over its lifetime.&lt;/p&gt;&lt;p&gt;So, eventually &amp;amp; reluctantly, I have changed my mind: there will be another major version of htmx.&lt;/p&gt;&lt;p&gt;However, in order to keep my word that there will not be a htmx 3.0, the next release will instead be htmx 4.0.&lt;/p&gt;&lt;p&gt;With htmx 4.0 we are rebuilding the internals of htmx, based on the lessons learned from fixi.js and five+ years of supporting htmx.&lt;/p&gt;&lt;p&gt;There are three major simplifying changes:&lt;/p&gt;&lt;p&gt;The biggest internal change is that &lt;code&gt;fetch()&lt;/code&gt; will replace &lt;code&gt;XMLHttpRequest&lt;/code&gt; as the core ajax infrastructure.  This
won‚Äôt actually have a huge effect on most usages of htmx except that the events model will necessarily change due
to the differences between &lt;code&gt;fetch()&lt;/code&gt; and &lt;code&gt;XMLHttpRequest&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;I feel that my biggest mistake in htmx 1.0 &amp;amp; 2.0 was making attribute inheritance implicit. I was inspired by CSS in doing this, and the results have been roughly the same as CSS: powerful &amp;amp; maddening.&lt;/p&gt;&lt;p&gt;In htmx 4.0, attribute inheritance will be explicit rather than implicit, via the &lt;code&gt;:inherited&lt;/code&gt; modifier:&lt;/p&gt;&lt;code&gt;  &amp;lt;div hx-target:inherited="#output"&amp;gt;
    &amp;lt;button hx-post="/up"&amp;gt;Like&amp;lt;/button&amp;gt;
    &amp;lt;button hx-post="/down"&amp;gt;Dislike&amp;lt;/button&amp;gt;
  &amp;lt;/div&amp;gt;
  &amp;lt;output id="output"&amp;gt;Pick a button...&amp;lt;/output&amp;gt;
&lt;/code&gt;&lt;p&gt;Here the &lt;code&gt;hx-target&lt;/code&gt; attribute is explicitly declared as &lt;code&gt;inherited&lt;/code&gt; on the enclosing &lt;code&gt;div&lt;/code&gt; and, if it wasn‚Äôt, the
&lt;code&gt;button&lt;/code&gt; elements would not inherit the target from it.&lt;/p&gt;&lt;p&gt;This will be the most significant upgrade change to deal with for most htmx users.&lt;/p&gt;&lt;p&gt;Another constant source of pain for both us and for htmx users is history support. htmx 2.0 stores history in local cache to make navigation faster. Unfortunately, snapshotting the DOM is often brittle because of third-party modifications, hidden state, etc. There is a terrible simplicity to the web 1.0 model of blowing everything away and starting over. There are also security concerns storing history information in session storage.&lt;/p&gt;&lt;p&gt;In htmx 2.0, we often end up recommending that people facing history-related issues simply disable the cache entirely, and that usually fixes the problems.&lt;/p&gt;&lt;p&gt;In htmx 4.0, history support will no longer snapshot the DOM and keep it locally. It will, rather, issue a network request for the restored content. This is the behavior of 2.0 on a history cache-miss, and it works reliably with little effort on behalf of htmx users.&lt;/p&gt;&lt;p&gt;We will offer an extension that enables history caching like in htmx 2.0, but it will be opt-in, rather than the default.&lt;/p&gt;&lt;p&gt;This tremendously simplifies the htmx codebase and should make the out-of-the-box behavior much more plug-and-play.&lt;/p&gt;&lt;p&gt;Most things.&lt;/p&gt;&lt;p&gt;The core functionality of htmx will remain the same, &lt;code&gt;hx-get&lt;/code&gt;, &lt;code&gt;hx-post&lt;/code&gt;,
&lt;code&gt;hx-target&lt;/code&gt;, &lt;code&gt;hx-boost&lt;/code&gt;, &lt;code&gt;hx-swap&lt;/code&gt;, &lt;code&gt;hx-trigger&lt;/code&gt;, etc.&lt;/p&gt;&lt;p&gt;Except for adding an &lt;code&gt;:inherited&lt;/code&gt; modifier on a few attributes, many htmx projects will ‚Äújust work‚Äù with htmx 4.&lt;/p&gt;&lt;p&gt;These changes will make the long term maintenance &amp;amp; sustainability of the project much stronger. It will also take pressure off the 2.0 releases, which can now focus on stability rather than contemplating new features.&lt;/p&gt;&lt;p&gt;That said, htmx 2.0 users will face an upgrade project when moving to 4.0 in a way that they did not have to in moving from 1.0 to 2.0.&lt;/p&gt;&lt;p&gt;I am sorry about that, and want to offer three things to address it:&lt;/p&gt;&lt;code&gt;latest&lt;/code&gt; and htmx 4.x is &lt;code&gt;next&lt;/code&gt;&lt;p&gt;Of course, it isn‚Äôt all bad. Beyond simplifying the implementation of htmx significantly, switching to fetch also gives us the opportunity to add some nice new features to htmx&lt;/p&gt;&lt;p&gt;By switching to &lt;code&gt;fetch()&lt;/code&gt;, we can take advantage of its support for
readable streams, which
allow for a stream of content to be swapped into the DOM, rather than a single response.&lt;/p&gt;&lt;p&gt;htmx 1.0 had Server Sent Event support integrated into the library. In htmx 2.0 we pulled this functionality out as an extension. It turns out that SSE is just a specialized version of a streaming response, so in adding streaming support, it‚Äôs an almost-free free two-fer to add that back into core as well.&lt;/p&gt;&lt;p&gt;This will make incremental response swapping much cleaner and well-supported in htmx.&lt;/p&gt;&lt;p&gt;Three years ago I had an idea for a DOM morphing algorithm that improved on the initial algorithm pioneered by morphdom.&lt;/p&gt;&lt;p&gt;The idea was to use ‚Äúid sets‚Äù to make smarter decisions regarding which nodes to preserve and which nodes to delete when merging changes into the DOM, and I called this idea ‚Äúidiomorph‚Äù. Idiomorph has gone on to be adopted by many other web project such as Hotwire.&lt;/p&gt;&lt;p&gt;We strongly considered including it in htmx 2.0, but I decided not too because it worked well as an extension and htmx 2.0 had already grown larger than I wanted.&lt;/p&gt;&lt;p&gt;In 4.0, with the complexity savings we achieved by moving to &lt;code&gt;fetch()&lt;/code&gt;, we can now comfortably fit a &lt;code&gt;morphInner&lt;/code&gt; and
&lt;code&gt;morphOuter&lt;/code&gt; swap into core, thanks to the excellent work of Michael West.&lt;/p&gt;&lt;p&gt;htmx has, since very early on, supported a concept of ‚ÄúOut-of-band‚Äù swaps: content that is removed from the main HTML response and swapped into the DOM elsewhere. I have always been a bit ambivalent about them, because they move away from Locality of Behavior, but there is no doubt that they are useful and often crucial for achieving certain UI patterns.&lt;/p&gt;&lt;p&gt;Out-of-band swaps started off very simply: if you marked an element as &lt;code&gt;hx-swap-oob='true'&lt;/code&gt;, htmx would swap the element
as the outer HTML of any existing element already in the DOM with that id.  Easy-peasy.&lt;/p&gt;&lt;p&gt;However, over time, people started asking for different functionality around Out-of-band swaps: prepending, appending, etc. and the feature began acquiring some fairly baroque syntax to handle all these needs.&lt;/p&gt;&lt;p&gt;We have come to the conclusion that the problem is that there are really two use cases, both currently trying to be filled by Out-of-band swaps:&lt;/p&gt;&lt;p&gt;Therefore, we are introducing the notion of &lt;code&gt;&amp;lt;partial&amp;gt;&lt;/code&gt;s in htmx 4.0&lt;/p&gt;&lt;p&gt;A partial element is, under the covers, a template element and, thus, can contain any sort of content you like. It specifies on itself all the standard htmx options regarding swapping, &lt;code&gt;hx-target&lt;/code&gt; and &lt;code&gt;hx-swap&lt;/code&gt; in particular, allowing
you full access to all the standard swapping behavior of htmx without using a specialized syntax.  This tremendously
simplifies the mental model for these sorts of needs, and dovetails well with the streaming support we intend to offer.&lt;/p&gt;&lt;p&gt;Out-of-band swaps will be retained in htmx 4.0, but will go back to their initial, simple focus of simply replacing an existing element by id.&lt;/p&gt;&lt;p&gt;htmx 2.0 has had View Transition support since April of 2023. In the interceding two years, support for the feature has grown across browsers (c‚Äômon, safari, you can do it) and we‚Äôve gained experience with the feature.&lt;/p&gt;&lt;p&gt;One thing that has become apparent to us while using them is that, to use them in a stable manner, it is important to establish a queue of transitions, so each can complete before the other begins. If you don‚Äôt do this, you can get visually ugly transition cancellations.&lt;/p&gt;&lt;p&gt;So, in htmx 4.0 we have added this queue which will ensure that all view transitions complete smoothly.&lt;/p&gt;&lt;p&gt;CSS transitions will continue to work as before as well, although the swapping model is again made much simpler by the async runtime.&lt;/p&gt;&lt;p&gt;We may enable View Transitions by default, the jury is still out on that.&lt;/p&gt;&lt;p&gt;A wonderful thing about &lt;code&gt;fetch()&lt;/code&gt; and the async support in general is that it is much easier to guarantee a stable
order of events.  By linearizing asynchronous code and allowing us to use standard language features like try/catch,
the event model of htmx should be much more predictable and comprehensible.&lt;/p&gt;&lt;p&gt;We are going to adopt a new standard for event naming to make things even clearer:&lt;/p&gt;&lt;p&gt;&lt;code&gt;htmx:&amp;lt;phase&amp;gt;:&amp;lt;system&amp;gt;[:&amp;lt;optional-sub-action&amp;gt;]&lt;/code&gt;&lt;/p&gt;&lt;p&gt;So, for example, &lt;code&gt;htmx:before:request&lt;/code&gt; will be triggered before a request is made.&lt;/p&gt;&lt;p&gt;Another opportunity we have is to take advantage of the &lt;code&gt;async&lt;/code&gt; behavior of &lt;code&gt;fetch()&lt;/code&gt; for much better performance in our
preload extension (where we issue a speculative (&lt;code&gt;GET&lt;/code&gt; only!) request in anticipation of an actual trigger).  We have
also added an optimistic update extension to the core extensions, again made easy by the new async features.&lt;/p&gt;&lt;p&gt;In general, we have opened up the internals of the htmx request/response/swap cycle much more fully to extension developers, up to and including allowing them to replace the &lt;code&gt;fetch()&lt;/code&gt; implementation used by htmx for a particular request.  There
should not be a need for any hacks to get the behavior you want out of htmx now: the events and the open ‚Äúcontext‚Äù object
should provide the ability to do almost anything.&lt;/p&gt;&lt;code&gt;hx-on&lt;/code&gt; Support&lt;p&gt;In htmx 2.0, I somewhat reluctantly added the &lt;code&gt;hx-on&lt;/code&gt; attributes to support light
scripting inline on elements.  I added this because HTML does not allow you to listen for arbitrary events via &lt;code&gt;on&lt;/code&gt;
attributes: only standard DOM events like &lt;code&gt;onclick&lt;/code&gt; can be responded to.&lt;/p&gt;&lt;p&gt;We hemmed and hawed about the syntax and so, unfortunately, there are a few different ways to do it.&lt;/p&gt;&lt;p&gt;In htmx 4.0 we will adopt a single standard for the &lt;code&gt;hx-on&lt;/code&gt; attributes: &lt;code&gt;hx-on:&amp;lt;event name&amp;gt;&lt;/code&gt;.  Additionally, we are
working to improve the htmx JavaScript API (especially around async operation support) and will make those features
available in &lt;code&gt;hx-on&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;&amp;lt;button hx-post="/like"
        hx-on:htmx:after:swap="await timeout('3s'); ctx.newContent[0].remove()"&amp;gt;
    Get A Response Then Remove It 3 Seconds Later
&amp;lt;/button&amp;gt;
&lt;/code&gt;
&lt;p&gt;htmx will never support a fully featured scripting mechanism in core, we recommend something like Alpine.js for that, but our hope is that we can provide a relatively minimalist API that allows for easy, light async scripting of the DOM.&lt;/p&gt;&lt;p&gt;I should note that htmx 4.0 will continue to work with &lt;code&gt;eval()&lt;/code&gt; disabled, but you will need to forego a few features like
&lt;code&gt;hx-on&lt;/code&gt; if you choose to do so.&lt;/p&gt;&lt;p&gt;All in all, our hope is that htmx 4.0 will feel an awful lot like 2.0, but with better features and, we hope, with fewer bugs.&lt;/p&gt;&lt;p&gt;As always, software takes as long as it takes.&lt;/p&gt;&lt;p&gt;However, our current planned timeline is:&lt;/p&gt;&lt;code&gt;htmx@4.0.0-alpha1&lt;/code&gt;&lt;code&gt;latest&lt;/code&gt; in early-2027ish&lt;p&gt;You can track our progress (and see quite a bit of dust flying around) in the &lt;code&gt;four&lt;/code&gt; branch on
github and at:&lt;/p&gt;&lt;p&gt;Thank you for your patience and pardon our dust!&lt;/p&gt;&lt;quote&gt;&lt;p&gt;‚ÄúWell, when events change, I change my mind. What do you do?‚Äù ‚ÄìPaul Samuelson or John Maynard Keynes&lt;/p&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://htmx.org/essays/the-fetchening/"/><published>2025-11-03T19:33:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45803427</id><title>Leverage Points: Places to Intervene in a System (1999)</title><updated>2025-11-03T23:09:47.892412+00:00</updated><content>&lt;doc fingerprint="a992980c2ee2879d"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;By Donella Meadows~&lt;/head&gt;
    &lt;p&gt;Folks who do systems analysis have a great belief in ‚Äúleverage points.‚Äù These are places within a complex system (a corporation, an economy, a living body, a city, an ecosystem) where a small shift in one thing can produce big changes in everything.&lt;/p&gt;
    &lt;p&gt;This idea is not unique to systems analysis ‚Äî it‚Äôs embedded in legend. The silver bullet, the trimtab, the miracle cure, the secret passage, the magic password, the single hero who turns the tide of history. The nearly effortless way to cut through or leap over huge obstacles. We not only want to believe that there are leverage points, we want to know where they are and how to get our hands on them. Leverage points are points of power.&lt;/p&gt;
    &lt;p&gt;The systems analysis community has a lot of lore about leverage points. Those of us who were trained by the great Jay Forrester at MIT have all absorbed one of his favorite stories. ‚ÄúPeople know intuitively where leverage points are,‚Äù he says. ‚ÄúTime after time I‚Äôve done an analysis of a company, and I‚Äôve figured out a leverage point ‚Äî in inventory policy, maybe, or in the relationship between sales force and productive force, or in personnel policy. Then I‚Äôve gone to the company and discovered that there‚Äôs already a lot of attention to that point. Everyone is trying very hard to push it IN THE WRONG DIRECTION!‚Äù&lt;/p&gt;
    &lt;p&gt;The classic example of that backward intuition was my own introduction to systems analysis, the world model. Asked by the Club of Rome to show how major global problems ‚Äî poverty and hunger, environmental destruction, resource depletion, urban deterioration, unemployment ‚Äî are related and how they might be solved, Forrester made a computer model and came out with a clear leverage point1: Growth. Not only population growth, but economic growth. Growth has costs as well as benefits, and we typically don‚Äôt count the costs ‚Äî among which are poverty and hunger, environmental destruction, etc. ‚Äî the whole list of problems we are trying to solve with growth! What is needed is much slower growth, much different kinds of growth, and in some cases no growth or negative growth.&lt;/p&gt;
    &lt;p&gt;The world‚Äôs leaders are correctly fixated on economic growth as the answer to virtually all problems, but they‚Äôre pushing with all their might in the wrong direction.&lt;/p&gt;
    &lt;p&gt;Another of Forrester‚Äôs classics was his urban dynamics study, published in 1969, which demonstrated that subsidized low-income housing is a leverage point.2 The less of it there is, the better off the city is ‚Äî even the low-income folks in the city. This model came out at a time when national policy dictated massive low-income housing projects, and Forrester was derided. Now those projects are being torn down in city after city.&lt;/p&gt;
    &lt;p&gt;Counterintuitive. That‚Äôs Forrester‚Äôs word to describe complex systems. Leverage points are not intuitive. Or if they are, we intuitively use them backward, systematically worsening whatever problems we are trying to solve.&lt;/p&gt;
    &lt;p&gt;The systems analysts I know have come up with no quick or easy formulas for finding leverage points. When we study a system, we usually learn where leverage points are. But a new system we‚Äôve never encountered? Well, our counterintuitions aren‚Äôt that well developed. Give us a few months or years and we‚Äôll figure it out. And we know from bitter experience that, because of counterintuitiveness, when we do discover the system‚Äôs leverage points, hardly anybody will believe us.&lt;/p&gt;
    &lt;p&gt;Very frustrating, especially for those of us who yearn not just to understand complex systems, but to make the world work better.&lt;/p&gt;
    &lt;p&gt;So one day I was sitting in a meeting about how to make the world work better ‚Äî actually it was a meeting about how the new global trade regime, NAFTA and GATT and the World Trade Organization, is likely to make the world work worse. The more I listened, the more I began to simmer inside. ‚ÄúThis is a HUGE NEW SYSTEM people are inventing!‚Äù I said to myself. ‚ÄúThey haven‚Äôt the SLIGHTEST IDEA how this complex structure will behave,‚Äù myself said back to me. ‚ÄúIt‚Äôs almost certainly an example of cranking the system in the wrong direction ‚Äî it‚Äôs aimed at growth, growth at any price!! And the control measures these nice, liberal folks are talking about to combat it ‚Äî small parameter adjustments, weak negative feedback loops ‚Äî are PUNY!!!‚Äù&lt;/p&gt;
    &lt;p&gt;Suddenly, without quite knowing what was happening, I got up, marched to the flip chart, tossed over to a clean page, and wrote:&lt;/p&gt;
    &lt;head rend="h3"&gt;PLACES TO INTERVENE IN A SYSTEM&lt;/head&gt;
    &lt;p&gt;(in increasing order of effectiveness)&lt;/p&gt;
    &lt;p&gt;9. Constants, parameters, numbers (subsidies, taxes, standards).&lt;lb/&gt; 8. Regulating negative feedback loops.&lt;lb/&gt; 7. Driving positive feedback loops.&lt;lb/&gt; 6. Material flows and nodes of material intersection.&lt;lb/&gt; 5. Information flows.&lt;lb/&gt; 4. The rules of the system (incentives, punishments, constraints).&lt;lb/&gt; 3. The distribution of power over the rules of the system.&lt;lb/&gt; 2. The goals of the system.&lt;lb/&gt; 1. The mindset or paradigm out of which the system ‚Äî its goals, power structure, rules, its culture ‚Äî arises.&lt;/p&gt;
    &lt;p&gt;Everyone in the meeting blinked in surprise, including me. ‚ÄúThat‚Äôs brilliant!‚Äù someone breathed. ‚ÄúHuh?‚Äù said someone else.&lt;/p&gt;
    &lt;p&gt;I realized that I had a lot of explaining to do.&lt;/p&gt;
    &lt;p&gt;I also had a lot of thinking to do. As with most of the stuff that come to me in boil-over mode, this list was not exactly tightly reasoned. As I began to share it with others, especially systems analysts who had their own lists and activists who wanted to put the list to immediate use, questions and comments came back that caused me to rethink, add and delete items, change the order, add caveats.&lt;/p&gt;
    &lt;p&gt;In a minute I‚Äôll go through the list I ended up with, explain the jargon, give examples and exceptions. The reason for this introduction is to place the list in a context of humility and to leave room for evolution. What bubbled up in me that day was distilled from decades of rigorous analysis of many different kinds of systems done by many smart people. But complex systems are, well, complex. It‚Äôs dangerous to generalize about them. What you are about to read is a work in progress. It‚Äôs not a recipe for finding leverage points. Rather it‚Äôs an invitation to think more broadly about system change.&lt;/p&gt;
    &lt;p&gt;Here, in the light of a cooler dawn, is a revised list:&lt;/p&gt;
    &lt;head rend="h3"&gt;PLACES TO INTERVENE IN A SYSTEM&lt;/head&gt;
    &lt;p&gt;(in increasing order of effectiveness)&lt;/p&gt;
    &lt;p&gt;12. Constants, parameters, numbers (such as subsidies, taxes, standards).&lt;lb/&gt; 11. The sizes of buffers and other stabilizing stocks, relative to their flows.&lt;lb/&gt; 10. The structure of material stocks and flows (such as transport networks, population age structures).&lt;lb/&gt; 9. The lengths of delays, relative to the rate of system change.&lt;lb/&gt; 8. The strength of negative feedback loops, relative to the impacts they are trying to correct against.&lt;lb/&gt; 7. The gain around driving positive feedback loops.&lt;lb/&gt; 6. The structure of information flows (who does and does not have access to information).&lt;lb/&gt; 5. The rules of the system (such as incentives, punishments, constraints).&lt;lb/&gt; 4. The power to add, change, evolve, or self-organize system structure.&lt;lb/&gt; 3. The goals of the system.&lt;lb/&gt; 2. The mindset or paradigm out of which the system ‚Äî its goals, structure, rules, delays, parameters ‚Äî arises.&lt;lb/&gt; 1. The power to transcend paradigms.&lt;/p&gt;
    &lt;p&gt;To explain parameters, stocks, delays, flows, feedback, and so forth, I need to start with a basic diagram.&lt;/p&gt;
    &lt;p&gt;The ‚Äústate of the system‚Äù is whatever standing stock is of importance ‚Äî amount of water behind the dam, amount of harvestable wood in the forest, number of people in the population, amount of money in the bank, whatever. System states are usually physical stocks, but they could be nonmaterial ones as well ‚Äî self-confidence, degree of trust in public officials, perceived safety of a neighborhood.&lt;/p&gt;
    &lt;p&gt;There are usually inflows that increase the stock and outflows that decrease it. Deposits increase the money in the bank; withdrawals decrease it. River inflow and rain raise the water behind the dam; evaporation and discharge through the spillway lower it. Births and immigrations increase the population, deaths and emigrations reduce it. Political corruption decreases trust in public officials; experience of a well-functioning government increases it.&lt;/p&gt;
    &lt;p&gt;Insofar as this part of the system consists of physical stocks and flows ‚Äî and they are the bedrock of any system ‚Äî it obeys laws of conservation and accumulation. You can understand its dynamics readily, if you can understand a bathtub with some water in it (the state of the system) and an inflowing faucet and outflowing drain. If the inflow rate is higher than the outflow rate, the stock gradually rises. If the outflow rate is higher than the inflow, the stock gradually goes down. The sluggish response of the water level to what could be sudden twists in the input and output valves is typical ‚Äî it takes time for flows to accumulate, just as it takes time for water to fill up or drain out of the tub.&lt;/p&gt;
    &lt;p&gt;The rest of the diagram is the information that causes the flows to change, which then cause the stock to change. If you‚Äôre about to take a bath, you have a desired water level in mind. You plug the drain, turn on the faucet and watch until the water rises to your chosen level (until the discrepancy between the desired and the actual state of the system is zero). Then you turn the water off.&lt;/p&gt;
    &lt;p&gt;If you start to get in the bath and discover that you‚Äôve underestimated your volume and are about to produce an overflow, you can open the drain for awhile, until the water goes down to your desired level.&lt;/p&gt;
    &lt;p&gt;Those are two negative feedback loops, or correcting loops, one controlling the inflow, one controlling the outflow, either or both of which you can use to bring the water level to your goal. Notice that the goal and the feedback connections are not visible in the system. If you were an extraterrestrial trying to figure out why the tub fills and empties, it would take awhile to figure out that there‚Äôs an invisible goal and a discrepancy-measuring process going on in the head of the creature manipulating the faucets. But if you watched long enough, you could figure that out.&lt;/p&gt;
    &lt;p&gt;Very simple so far. Now let‚Äôs take into account that you have two taps, a hot and a cold, and that you‚Äôre also adjusting for another system state ‚Äî temperature. Suppose the hot inflow is connected to a boiler way down in the basement, four floors below, so it doesn‚Äôt respond quickly. And you‚Äôre making faces at yourself in the mirror or distracted by studying the instructions and not paying close attention to the water level. And, of course, the inflow pipe is connected to a reservoir somewhere, which is connected to the whole planetary hydrological cycle. The system begins to get complex, and realistic, and interesting.&lt;/p&gt;
    &lt;p&gt;Mentally change the bathtub into your checking account. Write checks, make deposits, add a faucet that keeps dribbling in a little interest and a special drain that sucks your balance even drier if it ever goes dry. Attach your account to a thousand others and let the bank create loans as a function of your combined and fluctuating deposits, link a thousand of those banks into a federal reserve system ‚Äî and you begin to see how simple stocks and flows, plumbed together, make up systems way too complex to figure out.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why leverage points are not intuitive. And that‚Äôs enough systems theory to proceed to the list.&lt;/p&gt;
    &lt;head rend="h3"&gt;12. Constants, parameters, numbers (subsidies, taxes, standards).&lt;/head&gt;
    &lt;p&gt;‚ÄúParameters‚Äù in systems jargon means the numbers that determine how much of a discrepancy turns which faucet how fast. Maybe the faucet turns hard, so it takes awhile to get the water flowing or to turn it off. Maybe the drain is blocked and can allow only a small flow, no matter how open it is. Maybe the faucet can deliver with the force of a fire hose. These considerations are a matter of numbers, some of which are physically locked in and unchangeable, but most of which are popular intervention points.&lt;/p&gt;
    &lt;p&gt;Consider the national debt. It‚Äôs a negative bathtub, a money hole. The rate at which it sinks is called the annual deficit. Tax income makes it rise, government expenditures make it fall. Congress and the president spend most of their time arguing about the many, many parameters that open and close tax faucets and spending drains. Since those faucets and drains are connected to us, the voters, these are politically charged parameters. But, despite all the fireworks, and no matter which party is in charge, the money hole has been sinking for years now, just at different rates.&lt;/p&gt;
    &lt;p&gt;To adjust the dirtiness of the air we breathe, the government sets parameters called ambient air quality standards. To assure some standing stock of forest (or some flow of money to logging companies) it sets allowed annual cuts. Corporations adjust parameters such as wage rates and product prices, with an eye on the level in their profit bathtub ‚Äî the bottom line.&lt;/p&gt;
    &lt;p&gt;The amount of land we set aside for conservation. The minimum wage. How much we spend on AIDS research or Stealth bombers. The service charge the bank extracts from your account. All these are parameters, adjustments to faucets. So, by the way, is firing people and getting new ones, including politicians. Putting different hands on the faucets may change the rate at which the faucets turn, but if they‚Äôre the same old faucets, plumbed into the same old system, turned according to the same old information and goals and rules, the system isn‚Äôt going to change much. Electing Bill Clinton was definitely different from electing George Bush, but not all that different, given that every president is plugged into the same political system. (Changing the way money flows in that system would make much more of a difference ‚Äî but I‚Äôm getting ahead of myself on this list.)&lt;/p&gt;
    &lt;p&gt;Parameters are dead last on my list of powerful interventions. Diddling with the details, arranging the deck chairs on the Titanic. Probably 90, no 95, no 99 percent of our attention goes to parameters, but there‚Äôs not a lot of leverage in them.&lt;/p&gt;
    &lt;p&gt;Not that parameters aren‚Äôt important ‚Äî they can be, especially in the short term and to the individual who‚Äôs standing directly in the flow. People care deeply about parameters and fight fierce battles over them. But they RARELY CHANGE BEHAVIOR. If the system is chronically stagnant, parameter changes rarely kick-start it. If it‚Äôs wildly variable, they don‚Äôt usually stabilize it. If it‚Äôs growing out of control, they don‚Äôt brake it.&lt;/p&gt;
    &lt;p&gt;Whatever cap we put on campaign contributions, it doesn‚Äôt clean up politics. The Feds fiddling with the interest rate haven‚Äôt made business cycles go away. (We always forget that during upturns, and are shocked, shocked by the downturns.) After decades of the strictest air pollution standards in the world, Los Angeles air is less dirty, but it isn‚Äôt clean. Spending more on police doesn‚Äôt make crime go away.&lt;/p&gt;
    &lt;p&gt;Since I‚Äôm about to get into some examples where parameters ARE leverage points, let me stick in a big caveat here. Parameters become leverage points when they go into ranges that kick off one of the items higher on this list. Interest rates, for example, or birth rates, control the gains around positive feedback loops. System goals are parameters that can make big differences. Sometimes a system gets onto a chaotic edge, where the tiniest change in a number can drive it from order to what appears to be wild disorder.&lt;/p&gt;
    &lt;p&gt;These critical numbers are not nearly as common as people seem to think they are. Most systems have evolved or are designed to stay far out of critical parameter ranges. Mostly, the numbers are not worth the sweat put into them.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a story a friend sent me over the Internet to makes that point:3&lt;/p&gt;
    &lt;p&gt;When I became a landlord, I spent a lot of time and energy trying to figure out what would be a ‚Äúfair‚Äù rent to charge.&lt;/p&gt;
    &lt;p&gt;I tried to consider all the variables, including the relative incomes of my tenants, my own income and cash flow needs, which expenses were for upkeep and which were capital expenses, the equity versus the interest portion of the mortgage payments, how much my labor on the house was worth, etc.&lt;/p&gt;
    &lt;p&gt;I got absolutely nowhere. Finally I went to someone who specializes in giving money advice. She said: ‚ÄúYou‚Äôre acting as though there is a fine line at which the rent is fair, and at any point above that point the tenant is being screwed and at any point below that you are being screwed. In fact there is a large grey area in which both you and the tenant are getting a good, or at least a fair, deal. Stop worrying and get on with your life.‚Äù&lt;/p&gt;
    &lt;head rend="h3"&gt;11. The sizes of buffers and other stabilizing stocks, relative to their flows.&lt;/head&gt;
    &lt;p&gt;Consider a huge bathtub with slow in and outflows. Now think about a small one with very fast flows. That‚Äôs the difference between a lake and a river. You hear about catastrophic river floods much more often than catastrophic lake floods, because stocks that are big, relative to their flows, are more stable than small ones. In chemistry and other fields, a big, stabilizing stock is known as a buffer.&lt;/p&gt;
    &lt;p&gt;The stabilizing power of buffers is why you keep money in the bank rather than living from the flow of change through your pocket. It‚Äôs why stores hold inventory instead of calling for new stock just as customers carry the old stock out the door. It‚Äôs why we need to maintain more than the minimum breeding population of an endangered species. Soils in the eastern U.S. are more sensitive to acid rain than soils in the west, because they haven‚Äôt got big buffers of calcium to neutralize acid.&lt;/p&gt;
    &lt;p&gt;You can often stabilize a system by increasing the capacity of a buffer.4 But if a buffer is too big, the system gets inflexible. It reacts too slowly. And big buffers of some sorts, such as water reservoirs or inventories, cost a lot to build or maintain. Businesses invented just-in-time inventories, because occasional vulnerability to fluctuations or screw-ups is cheaper (for them, anyway) than certain, constant inventory costs ‚Äî and because small-to-vanishing inventories allow more flexible response to shifting demand.&lt;/p&gt;
    &lt;p&gt;There‚Äôs leverage, sometimes magical, in changing the size of buffers. But buffers are usually physical entities, not easy to change. The acid absorption capacity of eastern soils is not a leverage point for alleviating acid rain damage. The storage capacity of a dam is literally cast in concrete. So I haven‚Äôt put buffers very high on the list of leverage points.&lt;/p&gt;
    &lt;head rend="h3"&gt;10. The structure of material stocks and flows and nodes of intersection (such as transport networks, population age structures, flow of nitrogen through soil).&lt;/head&gt;
    &lt;p&gt;The plumbing structure, the stocks and flows and their physical arrangement, can have an enormous effect on how the system operates. When the Hungarian road system was laid out so all traffic from one side of the nation to the other has to pass through central Budapest, that determined a lot about air pollution and commuting delays that are not easily fixed by pollution control devices, traffic lights, or speed limits.&lt;/p&gt;
    &lt;p&gt;The only way to fix a system that is laid out wrong is to rebuild it, if you can. Amory Lovins does wonders of energy conservation by straightening out bent pipes and enlarging too-small ones. If we let him do energy retrofits on all the buildings of the nation,we could shut down at least half of our electric power plants.&lt;/p&gt;
    &lt;p&gt;But often physical rebuilding is the slowest and most expensive kind of change to make in a system. Some stock-and-flow structures are just plain unchangeable. The baby-boom swell in the U.S. population first caused pressure on the elementary school system, then high schools, then colleges, then jobs and housing, and now we‚Äôre looking forward to supporting its retirement. Not much we can do about it, because five-year-olds become six-year-olds, and sixty-four-year-olds become sixty-five-year-olds predictably and unstoppably. The same can be said for the lifetime of destructive CFC molecules in the ozone layer, for the rate at which contaminants get washed out of aquifers, for the fact that an inefficient car fleet takes 10-20 years to turn over.&lt;/p&gt;
    &lt;p&gt;Physical structure is crucial in a system, but rarely a leverage point, because changing it is rarely quick or simple. The leverage point is in proper design in the first place. After the structure is built, the leverage is in understanding its limitations and bottlenecks, using it with maximum efficiency, and refraining from fluctuations or expansions that strain its capacity.&lt;/p&gt;
    &lt;head rend="h3"&gt;9. The lengths of delays, relative to the rate of system changes.&lt;/head&gt;
    &lt;p&gt;Remember that bathtub on the fourth floor I mentioned, with the water heater in the basement? I actually experienced one of those once, in an old hotel in London. It wasn‚Äôt even a bathtub, it was a shower ‚Äî no buffering capacity. The water temperature took at least a minute to respond to my faucet twists. Guess what my shower was like.&lt;/p&gt;
    &lt;p&gt;Right, oscillations from hot to cold and back to hot, punctuated with expletives.&lt;/p&gt;
    &lt;p&gt;Delays in feedback loops are critical determinants of system behavior. They are common causes of oscillations. If you‚Äôre trying to adjust a system state to your goal, but you only receive delayed information about what the system state is, you will overshoot and undershoot. Same if your information is timely, but your response isn‚Äôt. For example, it takes several years to build an electric power plant, and then that plant lasts, say, thirty years. Those delays make it impossible to build exactly the right number of plants to supply a rapidly changing demand. Even with immense effort at forecasting, almost every electricity industry in the world experiences long oscillations between overcapacity and undercapacity. A system just can‚Äôt respond to short-term changes when it has long-term delays. That‚Äôs why a massive central-planning system, such as the Soviet Union or General Motors, necessarily functions poorly.&lt;/p&gt;
    &lt;p&gt;Because we know they‚Äôre important, we systems folks see delays wherever we look. The delay between the time when a pollutant is dumped on the land and when it trickles down to the groundwater. The delay between the birth of a child and the time when that child is ready to have a child. The delay between the first successful test of a new technology and the time when that technology is installed throughout the economy. The time it takes for a price to adjust to a supply-demand imbalance.&lt;/p&gt;
    &lt;p&gt;A delay in a feedback process is critical RELATIVE TO RATES OF CHANGE (growth, fluctuation, decay) IN THE STOCKS THAT THE FEEDBACK LOOP IS TRYING TO CONTROL. Delays that are too short cause overreaction, ‚Äúchasing your tail,‚Äù oscillations amplified by the jumpiness of the response. Delays that are too long cause damped, sustained, or exploding oscillations, depending on how much too long. At the extreme they cause chaos. Overlong delays in a system with a threshold, a danger point, a range past which irreversible damage can occur, cause overshoot and collapse.&lt;/p&gt;
    &lt;p&gt;I would list delay length as a high leverage point, except for the fact that delays are not often easily changeable. Things take as long as they take. You can‚Äôt do a lot about the construction time of a major piece of capital, or the maturation time of a child, or the growth rate of a forest. It‚Äôs usually easier to SLOW DOWN THE CHANGE RATE, so that inevitable feedback delays won‚Äôt cause so much trouble. That‚Äôs why growth rates are higher up on the leverage-point list than delay times.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs why slowing economic growth is a greater leverage point in Forrester‚Äôs world model than faster technological development or freer market prices. Those are attempts to speed up the rate of adjustment. But the world‚Äôs physical capital plant, its factories and boilers, the concrete manifestations of its working technologies, can only change so fast, even in the face of new prices or new ideas ‚Äî and prices and ideas don‚Äôt change instantly either, not through a whole global culture. There‚Äôs more leverage in slowing the system down so technologies and prices can keep up with it, than there is in wishing the delays away.&lt;/p&gt;
    &lt;p&gt;But if there is a delay in your system that can be changed, changing it can have big effects. Watch out! Be sure you change it in the right direction! (For example, the great push to reduce information and money transfer delays in financial markets is just asking for wild gyrations)&lt;/p&gt;
    &lt;head rend="h3"&gt;8. The strength of negative feedback loops, relative to the impacts they are trying to correct against.&lt;/head&gt;
    &lt;p&gt;Now we‚Äôre beginning to move from the physical part of the system to the information and control parts, where more leverage can be found.&lt;/p&gt;
    &lt;p&gt;Negative feedback loops are ubiquitous in systems. Nature evolves them and humans invent them as controls to keep important system states within safe bounds. A thermostat loop is the classic example. Its purpose is to keep the system state called ‚Äúroom temperature‚Äù fairly constant at a desired level. Any negative feedback loop needs a goal (the thermostat setting), a monitoring and signaling device to detect excursions from the goal (the thermostat), and a response mechanism (the furnace and/or air conditioner, fans, heat pipes, fuel, etc.).&lt;/p&gt;
    &lt;p&gt;A complex system usually has numerous negative feedback loops it can bring into play, so it can self-correct under different conditions and impacts. Some of those loops may be inactive much of the time ‚Äî like the emergency cooling system in a nuclear power plant, or your ability to sweat or shiver to maintain your body temperature ‚Äî but their presence is critical to the long-term welfare of the system.&lt;/p&gt;
    &lt;p&gt;One of the big mistakes we make is to strip away these ‚Äúemergency‚Äù response mechanisms because they aren‚Äôt often used and they appear to be costly. In the short term we see no effect from doing this. In the long term, we drastically narrow the range of conditions over which the system can survive. One of the most heartbreaking ways we do this is in encroaching on the habitats of endangered species. Another is in encroaching on our own time for rest, recreation, socialization, and meditation.&lt;/p&gt;
    &lt;p&gt;The ‚Äústrength‚Äù of a negative loop ‚Äî its ability to keep its appointed stock at or near its goal ‚Äî depends on the combination of all its parameters and links ‚Äî the accuracy and rapidity of monitoring, the quickness and power of response, the directness and size of corrective flows. Sometimes there are leverage points here.&lt;/p&gt;
    &lt;p&gt;Take markets, for example, the negative feedback systems that are all but worshipped by economists ‚Äî and they can indeed be marvels of self-correction, as prices vary to moderate supply and demand and keep them in balance. The more the price ‚Äî the central piece of information signaling both producers and consumers ‚Äî is kept clear, unambiguous, timely, and truthful, the more smoothly markets will operate. Prices that reflect full costs will tell consumers how much they can actually afford and will reward efficient producers. Companies and governments are fatally attracted to the price leverage point, of course, all of them determinedly pushing it in the wrong direction with subsidies, fixes, externalities, taxes, and other forms of confusion.&lt;/p&gt;
    &lt;p&gt;These folks are trying to weaken the feedback power of market signals by twisting information in their favor. The REAL leverage here is to keep them from doing it. Hence the necessity of anti-trust laws, truth-in-advertising laws, attempts to internalize costs (such as pollution taxes), the removal of perverse subsidies, and other ways of leveling market playing fields.&lt;/p&gt;
    &lt;p&gt;None of which get far these days, because of the weakening of another set of negative feedback loops ‚Äî those of democracy. This great system was invented to put self-correcting feedback between the people and their government. The people, informed about what their elected representatives do, respond by voting those representatives in or out of office. The process depends upon the free, full, unbiased flow of information back and forth between electorate and leaders. Billions of dollars are spent to limit and bias and dominate that flow. Give the people who want to distort market price signals the power to pay off government leaders, get the channels of communication to be self-interested corporate partners themselves, and none of the necessary negative feedbacks work well. Both market and democracy erode.&lt;/p&gt;
    &lt;p&gt;The strength of a negative feedback loop is important RELATIVE TO THE IMPACT IT IS DESIGNED TO CORRECT. If the impact increases in strength, the feedbacks have to be strengthened too. A thermostat system may work fine on a cold winter day ‚Äî but open all the windows and its corrective power will fail. Democracy worked better before the advent of the brainwashing power of centralized mass communications. Traditional controls on fishing were sufficient until radar spotting and drift nets and other technologies made it possible for a few actors to wipe out the fish. The power of big industry calls for the power of big government to hold it in check; a global economy makes necessary a global government and global regulations.&lt;/p&gt;
    &lt;p&gt;Here are some examples of strengthening negative feedback controls to improve a system‚Äôs self-correcting abilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;preventive medicine, exercise, and good nutrition to bolster the body‚Äôs ability to fight disease,&lt;/item&gt;
      &lt;item&gt;integrated pest management to encourage natural predators of crop pests,&lt;/item&gt;
      &lt;item&gt;the Freedom of Information Act to reduce government secrecy,&lt;/item&gt;
      &lt;item&gt;monitoring systems to report on environmental damage,&lt;/item&gt;
      &lt;item&gt;protection for whistleblowers,&lt;/item&gt;
      &lt;item&gt;impact fees, pollution taxes, and performance bonds to recapture the externalized public costs of private benefits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;7. The gain around driving positive feedback loops.&lt;/head&gt;
    &lt;p&gt;A negative feedback loop is self-correcting; a positive feedback loop is self-reinforcing. The more it works, the more it gains power to work some more. The more people catch the flu, the more they infect other people. The more babies are born, the more people grow up to have babies. The more money you have in the bank, the more interest you earn, the more money you have in the bank. The more the soil erodes, the less vegetation it can support, the fewer roots and leaves to soften rain and runoff, the more soil erodes. The more high-energy neutrons in the critical mass, the more they knock into nuclei and generate more.&lt;/p&gt;
    &lt;p&gt;Positive feedback loops are sources of growth, explosion, erosion, and collapse in systems. A system with an unchecked positive loop ultimately will destroy itself. That‚Äôs why there are so few of them. Usually a negative loop will kick in sooner or later. The epidemic will run out of infectable people ‚Äî or people will take increasingly strong steps to avoid being infected. The death rate will rise to equal the birth rate ‚Äî or people will see the consequences of unchecked population growth and have fewer babies. The soil will erode away to bedrock, and after a million years the bedrock will crumble into new soil ‚Äî or people will stop overgrazing, put up checkdams, plant trees, and stop the erosion.&lt;/p&gt;
    &lt;p&gt;In all those examples, the first outcome is what will happen if the positive loop runs its course, the second is what will happen if there‚Äôs an intervention to reduce its self-multiplying power. Reducing the gain around a positive loop ‚Äî slowing the growth ‚Äî is usually a more powerful leverage point in systems than strengthening negative loops, and much preferable to letting the positive loop run.&lt;/p&gt;
    &lt;p&gt;Population and economic growth rates in the world model are leverage points, because slowing them gives the many negative loops, through technology and markets and other forms of adaptation, all of which have limits and delays, time to function. It‚Äôs the same as slowing the car when you‚Äôre driving too fast, rather than calling for more responsive brakes or technical advances in steering.&lt;/p&gt;
    &lt;p&gt;Another example: there are many positive feedback loops in society that reward the winners of a competition with the resources to win even bigger next time. Systems folks call them ‚Äúsuccess to the successful‚Äù loops. Rich people collect interest; poor people pay it. Rich people pay accountants and lean on politicians to reduce their taxes; poor people can‚Äôt. Rich people give their kids inheritances and good educations; poor kids lose out. Anti-poverty programs are weak negative loops that try to counter these strong positive ones. It would be much more effective to weaken the positive loops. That‚Äôs what progressive income tax, inheritance tax, and universal high-quality public education programs are meant to do. (If rich people can buy government and weaken, rather than strengthen those of measures, the government, instead of balancing ‚Äúsuccess to the successful‚Äù loops, becomes just another instrument to reinforce them!)&lt;/p&gt;
    &lt;p&gt;The most interesting behavior that rapidly turning positive loops can trigger is chaos. This wild, unpredictable, unreplicable, and yet bounded behavior happens when a system starts changing much, much faster than its negative loops can react to it. For example, if you keep raising the capital growth rate in the world model, eventually you get to a point where one tiny increase more will shift the economy from exponential growth to oscillation. Another nudge upward gives the oscillation a double beat. And just the tiniest further nudge sends it into chaos.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt expect the world economy to turn chaotic any time soon (not for that reason, anyway). That behavior occurs only in unrealistic parameter ranges, equivalent to doubling the size of the economy within a year. Real-world systems can turn chaotic, however, if something in them can grow or decline very fast. Fast-replicating bacteria or insect populations, very infectious epidemics, wild speculative bubbles in money systems, neutron fluxes in the guts of nuclear power plants. These systems are hard to control, and control must involve slowing down the positive feedbacks.&lt;/p&gt;
    &lt;p&gt;In more ordinary systems, look for leverage points around birth rates, interest rates, erosion rates, ‚Äúsuccess to the successful‚Äù loops, any place where the more you have of something, the more you have the possibility of having more.&lt;/p&gt;
    &lt;head rend="h3"&gt;6. The structure of information flows (who does and does not have access to information).&lt;/head&gt;
    &lt;p&gt;There was this subdivision of identical houses, the story goes, except that for some reason the electric meter in some of the houses was installed in the basement and in others it was installed in the front hall, where the residents could see it constantly, going round faster or slower as they used more or less electricity. With no other change, with identical prices, electricity consumption was 30 percent lower in the houses where the meter was in the front hall.&lt;/p&gt;
    &lt;p&gt;We systems-heads love that story because it‚Äôs an example of a high leverage point in the information structure of the system. It‚Äôs not a parameter adjustment, not a strengthening or weakening of an existing loop. It‚Äôs a NEW LOOP, delivering feedback to a place where it wasn‚Äôt going before.&lt;/p&gt;
    &lt;p&gt;A more recent example is the Toxic Release Inventory ‚Äî the U.S. government‚Äôs requirement, instituted in 1986, that every factory releasing hazardous air pollutants report those emissions publicly every year. Suddenly every community could find out precisely what was coming out of the smokestacks in town. There was no law against those emissions, no fines, no determination of ‚Äúsafe‚Äù levels, just information. But by 1990 emissions dropped 40 percent. They‚Äôve continued to go down since, not so much because of citizen outrage as because of corporate shame. One chemical company that found itself on the Top Ten Polluters list reduced its emissions by 90 percent, just to ‚Äúget off that list.‚Äù&lt;/p&gt;
    &lt;p&gt;Missing feedback is one of the most common causes of system malfunction. Adding or restoring information can be a powerful intervention, usually much easier and cheaper than rebuilding physical infrastructure. The tragedy of the commons that is crashing the world‚Äôs commercial fisheries occurs because there is no feedback from the state of the fish population to the decision to invest in fishing vessels. (Contrary to economic opinion, the price of fish doesn‚Äôt provide that feedback. As the fish get more scarce and hence more expensive, it becomes all the more profitable to go out and catch them. That‚Äôs a perverse feedback, a positive loop that leads to collapse.)&lt;/p&gt;
    &lt;p&gt;It‚Äôs important that the missing feedback be restored to the right place and in compelling form. To take another tragedy of the commons, it‚Äôs not enough to inform all the users of an aquifer that the groundwater level is dropping. That could initiate a race to the bottom. It would be more effective to set a water price that rises steeply as the pumping rate begins to exceed the recharge rate.&lt;/p&gt;
    &lt;p&gt;Compelling feedback. Suppose taxpayers got to specify on their return forms what government services their tax payments must be spent on. (Radical democracy!) Suppose any town or company that puts a water intake pipe in a river had to put it immediately DOWNSTREAM from its own outflow pipe. Suppose any public or private official who made the decision to invest in a nuclear power plant got the waste from that plant stored on his/her lawn. Suppose (this is an old one) the politicians who declare war were required to spend that war in the front lines.&lt;/p&gt;
    &lt;p&gt;There is a systematic tendency on the part of human beings to avoid accountability for their own decisions. That‚Äôs why there are so many missing feedback loops ‚Äî and why this kind of leverage point is so often popular with the masses, unpopular with the powers that be, and effective, if you can get the powers that be to permit it to happen (or go around them and make it happen anyway).&lt;/p&gt;
    &lt;head rend="h3"&gt;5. The rules of the system (incentives, punishments, constraints).&lt;/head&gt;
    &lt;p&gt;The rules of the system define its scope, its boundaries, its degrees of freedom. Thou shalt not kill. Everyone has the right of free speech. Contracts are to be honored. The president serves four-year terms and cannot serve more than two of them. Nine people on a team, you have to touch every base, three strikes and you‚Äôre out. If you get caught robbing a bank, you go to jail.&lt;/p&gt;
    &lt;p&gt;Mikhail Gorbachev came to power in the USSR and opened information flows (glasnost) and changed the economic rules (perestroika), and look what happened.&lt;/p&gt;
    &lt;p&gt;Constitutions are the strongest examples of social rules. Physical laws such as the second law of thermodynamics are absolute rules, whether we understand them or not or like them or not. Laws, punishments, incentives, and informal social agreements are progressively weaker rules.&lt;/p&gt;
    &lt;p&gt;To demonstrate the power of rules, I like to ask my students to imagine different ones for a college. Suppose the students graded the teachers, or each other. Suppose there were no degrees: you come to college when you want to learn something, and you leave when you‚Äôve learned it. Suppose tenure were awarded to professors according to their ability to solve real-world problems, rather than to publish academic papers. Suppose a class got graded as a group, instead of as individuals.&lt;/p&gt;
    &lt;p&gt;As we try to imagine restructured rules like that and what our behavior would be under them, we come to understand the power of rules. They are high leverage points. Power over the rules is real power. That‚Äôs why lobbyists congregate when Congress writes laws, and why the Supreme Court, which interprets and delineates the Constitution ‚Äî the rules for writing the rules ‚Äî has even more power than Congress. If you want to understand the deepest malfunctions of systems, pay attention to the rules, and to who has power over them.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why my systems intuition was sending off alarm bells as the new world trade system was explained to me. It is a system with rules designed by corporations, run by corporations, for the benefit of corporations. Its rules exclude almost any feedback from any other sector of society. Most of its meetings are closed even to the press (no information flow, no feedback). It forces nations into positive loops ‚Äúracing to the bottom,‚Äù competing with each other to weaken environmental and social safeguards in order to attract corporate investment. It‚Äôs a recipe for unleashing ‚Äúsuccess to the successful‚Äù loops, until they generate enormous accumulations of power and huge centralized planning systems that will destroy themselves, just as the Soviet Union destroyed itself, and for similar systemic reasons.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. The power to add, change, evolve, or self-organize system structure.&lt;/head&gt;
    &lt;p&gt;The most stunning thing living systems and some social systems can do is to change themselves utterly by creating whole new structures and behaviors. In biological systems that power is called evolution. In human economies it‚Äôs called technical advance or social revolution. In systems lingo it‚Äôs called self-organization.&lt;/p&gt;
    &lt;p&gt;Self-organization means changing any aspect of a system lower on this list ‚Äî adding completely new physical structures, such as brains or wings or computers ‚Äî adding new negative or positive loops, or new rules. The ability to self-organize is the strongest form of system resilience. A system that can evolve can survive almost any change, by changing itself. The human immune system has the power to develop new responses to (some kinds of ) insults it has never before encountered. The human brain can take in new information and pop out completely new thoughts.&lt;/p&gt;
    &lt;p&gt;The power of self-organization seems so wondrous that we tend to regard it as mysterious, miraculous, manna from heaven. Economists often model technology as literal manna ‚Äî coming from nowhere, costing nothing, increasing the productivity of an economy by some steady percent each year. For centuries people have regarded the spectacular variety of nature with the same awe. Only a divine creator could bring forth such a creation.&lt;/p&gt;
    &lt;p&gt;Further investigation of self-organizing systems reveals that the divine creator, if there is one, does not have to produce evolutionary miracles. He, she, or it just has to write marvelously clever RULES FOR SELF-ORGANIZATION. These rules basically govern how, where, and what the system can add onto or subtract from itself under what conditions. As hundreds of self-organizing computer models have demonstrated, complex and delightful patterns can evolve from quite simple evolutionary algorithms. (That need not mean that real-world algorithms are simple, only that they can be.) The genetic code within the DNA that is the basis of all biological evolution contains just four different letters, combined into words of three letters each. That pattern, and the rules for replicating and rearranging it, has been constant for something like three billion years, during which it has spewed out an unimaginable variety of failed and successful self-evolved creatures.&lt;/p&gt;
    &lt;p&gt;Self-organization is basically a matter of an evolutionary raw material ‚Äî a highly variable stock of information from which to select possible patterns ‚Äî and a means for experimentation, for selecting and testing new patterns. For biological evolution the raw material is DNA, one source of variety is spontaneous mutation, and the testing mechanism is something like punctuated Darwinian selection. For technology the raw material is the body of understanding science has accumulated and stored in libraries and in the brains of its practitioners. The source of variety is human creativity (whatever THAT is) and the selection mechanism can be whatever the market will reward, or whatever governments and foundations will fund, or whatever meets human needs.&lt;/p&gt;
    &lt;p&gt;When you understand the power of system self-organization, you begin to understand why biologists worship biodiversity even more than economists worship technology. The wildly varied stock of DNA, evolved and accumulated over billions of years, is the source of evolutionary potential, just as science libraries and labs and universities where scientists are trained are the source of technological potential. Allowing species to go extinct is a systems crime, just as randomly eliminating all copies of particular science journals, or particular kinds of scientists, would be.&lt;/p&gt;
    &lt;p&gt;The same could be said of human cultures, of course, which are the store of behavioral repertoires, accumulated over not billions, but hundreds of thousands of years. They are a stock out of which social evolution can arise. Unfortunately, people appreciate the precious evolutionary potential of cultures even less than they understand the preciousness of every genetic variation in the world‚Äôs ground squirrels. I guess that‚Äôs because one aspect of almost every culture is the belief in the utter superiority of that culture.&lt;/p&gt;
    &lt;p&gt;Insistence on a single culture shuts down learning. Cuts back resilience. Any system, biological, economic, or social, that gets so encrusted that it cannot self-evolve, a system that systematically scorns experimentation and wipes out the raw material of innovation, is doomed over the long term on this highly variable planet.&lt;/p&gt;
    &lt;p&gt;The intervention point here is obvious, but unpopular. Encouraging variability and experimentation and diversity means ‚Äúlosing control.‚Äù Let a thousand flowers bloom and ANYTHING could happen! Who wants that? Let‚Äôs play it safe and push this leverage point in the wrong direction by wiping out biological, cultural, social, and market diversity!&lt;/p&gt;
    &lt;head rend="h3"&gt;3. The goals of the system.&lt;/head&gt;
    &lt;p&gt;Right there, the diversity-destroying consequence of the push for control, that demonstrates why the goal of a system is a leverage point superior to the self-organizing ability of a system. If the goal is to bring more and more of the world under the control of one particular central planning system (the empire of Genghis Khan, the world of Islam, the People‚Äôs Republic of China, Wal-Mart, Disney, whatever), then everything further down the list, physical stocks and flows, feedback loops, information flows, even self-organizing behavior, will be twisted to conform to that goal.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why I can‚Äôt get into arguments about whether genetic engineering is a ‚Äúgood‚Äù or a ‚Äúbad‚Äù thing. Like all technologies, it depends upon who is wielding it, with what goal. The only thing one can say is that if corporations wield it for the purpose of generating marketable products, that is a very different goal, a different selection mechanism, a different direction for evolution than anything the planet has seen so far.&lt;/p&gt;
    &lt;p&gt;As my little single-loop examples have shown, most negative feedback loops within systems have their own goals ‚Äî to keep the bathwater at the right level, to keep the room temperature comfortable, to keep inventories stocked at sufficient levels, to keep enough water behind the dam. Those goals are important leverage points for pieces of systems, and most people realize that. If you want the room warmer, you know the thermostat setting is the place to intervene. But there are larger, less obvious, higher-leverage goals, those of the entire system.&lt;/p&gt;
    &lt;p&gt;Even people within systems don‚Äôt often recognize what whole-system goal they are serving. To make profits, most corporations would say, but that‚Äôs just a rule, a necessary condition to stay in the game. What is the point of the game? To grow, to increase market share, to bring the world (customers, suppliers, regulators) more and more under the control of the corporation, so that its operations becomes ever more shielded from uncertainty. John Kenneth Galbraith recognized that corporate goal ‚Äî to engulf everything ‚Äî long ago.5 It‚Äôs the goal of a cancer too. Actually it‚Äôs the goal of every living population ‚Äî and only a bad one when it isn‚Äôt balanced by higher-level negative feedback loops that never let an upstart power-loop-driven entity control the world. The goal of keeping the market competitive has to trump the goal of each corporation to eliminate its competitors (and brainwash its customers and swallow its suppliers), just as in ecosystems, the goal of keeping populations in balance and evolving has to trump the goal of each population to reproduce without limit.&lt;/p&gt;
    &lt;p&gt;I said awhile back that changing the players in the system is a low-level intervention, as long as the players fit into the same old system. The exception to that rule is at the top, where a single player can have the power to change the system‚Äôs goal. I have watched in wonder as ‚Äî only very occasionally ‚Äî a new leader in an organization, from Dartmouth College to Nazi Germany, comes in, enunciates a new goal, and swings hundreds or thousands or millions of perfectly intelligent, rational people off in a new direction.&lt;/p&gt;
    &lt;p&gt;That‚Äôs what Ronald Reagan did, and we watched it happen. Not long before he came to office, a president could say ‚ÄúAsk not what government can do for you, ask what you can do for the government,‚Äù and no one even laughed. Reagan said over and over, the goal is not to get the people to help the government and not to get government to help the people, but to get government off our backs. One can argue, and I would, that larger system changes and the rise of corporate power over government let him get away with that. But the thoroughness with which the public discourse in the U.S. and even the world has been changed since Reagan is testimony to the high leverage of articulating, meaning, repeating, standing up for, insisting upon new system goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. The mindset or paradigm out of which the system ‚Äî its goals, structure, rules, delays, parameters ‚Äî arises.&lt;/head&gt;
    &lt;p&gt;Another of Jay Forrester‚Äôs famous systems sayings goes: it doesn‚Äôt matter how the tax law of a country is written. There is a shared idea in the minds of the society about what a ‚Äúfair‚Äù distribution of the tax load is. Whatever the rules say, by fair means or foul, by complications, cheating, exemptions or deductions, by constant sniping at the rules, actual tax payments will push right up against the accepted idea of ‚Äúfairness.‚Äù&lt;/p&gt;
    &lt;p&gt;The shared idea in the minds of society, the great big unstated assumptions ‚Äî unstated because unnecessary to state; everyone already knows them ‚Äî constitute that society‚Äôs paradigm, or deepest set of beliefs about how the world works. There is a difference between nouns and verbs. Money measures something real and has real meaning (therefore people who are paid less are literally worth less). Growth is good. Nature is a stock of resources to be converted to human purposes. Evolution stopped with the emergence of Homo sapiens. One can ‚Äúown‚Äù land. Those are just a few of the paradigmatic assumptions of our current culture, all of which have utterly dumfounded other cultures, who thought them not the least bit obvious.&lt;/p&gt;
    &lt;p&gt;Paradigms are the sources of systems. From them, from shared social agreements about the nature of reality, come system goals and information flows, feedbacks, stocks, flows and everything else about systems. No one has ever said that better than Ralph Waldo Emerson:&lt;/p&gt;
    &lt;p&gt;Every nation and every man instantly surround themselves with a material apparatus which exactly corresponds to ‚Ä¶ their state of thought. Observe how every truth and every error, each a thought of some man‚Äôs mind, clothes itself with societies, houses, cities, language, ceremonies, newspapers. Observe the ideas of the present day ‚Ä¶ see how timber, brick, lime, and stone have flown into convenient shape, obedient to the master idea reigning in the minds of many persons‚Ä¶. It follows, of course, that the least enlargement of ideas ‚Ä¶ would cause the most striking changes of external things.6&lt;/p&gt;
    &lt;p&gt;The ancient Egyptians built pyramids because they believed in an afterlife. We build skyscrapers, because we believe that space in downtown cities is enormously valuable. (Except for blighted spaces, often near the skyscrapers, which we believe are worthless.) Whether it was Copernicus and Kepler showing that the earth is not the center of the universe, or Einstein hypothesizing that matter and energy are interchangeable, or Adam Smith postulating that the selfish actions of individual players in markets wonderfully accumulate to the common good, people who have managed to intervene in systems at the level of paradigm have hit a leverage point that totally transforms systems.&lt;/p&gt;
    &lt;p&gt;You could say paradigms are harder to change than anything else about a system, and therefore this item should be lowest on the list, not second-to-highest. But there‚Äôs nothing physical or expensive or even slow in the process of paradigm change. In a single individual it can happen in a millisecond. All it takes is a click in the mind, a falling of scales from eyes, a new way of seeing. Whole societies are another matter ‚Äî they resist challenges to their paradigm harder than they resist anything else.&lt;/p&gt;
    &lt;p&gt;So how do you change paradigms? Thomas Kuhn, who wrote the seminal book about the great paradigm shifts of science,7 has a lot to say about that. In a nutshell, you keep pointing at the anomalies and failures in the old paradigm, you keep coming yourself, and loudly and with assurance from the new one, you insert people with the new paradigm in places of public visibility and power. You don‚Äôt waste time with reactionaries; rather you work with active change agents and with the vast middle ground of people who are open-minded.&lt;/p&gt;
    &lt;p&gt;Systems folks would say you change paradigms by modeling a system, which takes you outside the system and forces you to see it whole. We say that because our own paradigms have been changed that way.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. The power to transcend paradigms.&lt;/head&gt;
    &lt;p&gt;There is yet one leverage point that is even higher than changing a paradigm. That is to keep oneself unattached in the arena of paradigms, to stay flexible, to realize that NO paradigm is ‚Äútrue,‚Äù that every one, including the one that sweetly shapes your own worldview, is a tremendously limited understanding of an immense and amazing universe that is far beyond human comprehension. It is to ‚Äúget‚Äù at a gut level the paradigm that there are paradigms, and to see that that itself is a paradigm, and to regard that whole realization as devastatingly funny. It is to let go into Not Knowing, into what the Buddhists call enlightenment.&lt;/p&gt;
    &lt;p&gt;People who cling to paradigms (which means just about all of us) take one look at the spacious possibility that everything they think is guaranteed to be nonsense and pedal rapidly in the opposite direction. Surely there is no power, no control, no understanding, not even a reason for being, much less acting, in the notion or experience that there is no certainty in any worldview. But, in fact, everyone who has managed to entertain that idea, for a moment or for a lifetime, has found it to be the basis for radical empowerment. If no paradigm is right, you can choose whatever one will help to achieve your purpose. If you have no idea where to get a purpose, you can listen to the universe (or put in the name of your favorite deity here) and do his, her, its will, which is probably a lot better informed than your will.&lt;/p&gt;
    &lt;p&gt;It is in this space of mastery over paradigms that people throw off addictions, live in constant joy, bring down empires, get locked up or burned at the stake or crucified or shot, and have impacts that last for millennia.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Final Caution&lt;/head&gt;
    &lt;p&gt;Back from the sublime to the ridiculous, from enlightenment to caveats. There is so much that has to be said to qualify this list. It is tentative and its order is slithery. There are exceptions to every item that can move it up or down the order of leverage. Having had the list percolating in my subconscious for years has not transformed me into a Superwoman. The higher the leverage point, the more the system will resist changing it ‚Äî that‚Äôs why societies have to rub out truly enlightened beings.&lt;/p&gt;
    &lt;p&gt;Magical leverage points are not easily accessible, even if we know where they are and which direction to push on them. There are no cheap tickets to mastery. You have to work hard at it, whether that means rigorously analyzing a system or rigorously casting off your own paradigms and throwing yourself into the humility of Not Knowing. In the end, it seems that mastery has less to do with pushing leverage points than it does with strategically, profoundly, madly letting go.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;J.W. Forrester, World Dynamics, Portland OR, Productivity Press, 1971&lt;/item&gt;
      &lt;item&gt;J.W. Forrester, Urban Dynamics, Portland OR, Productivity Press, 1969.&lt;/item&gt;
      &lt;item&gt;Thanks to David Holmstrom of Santiago, Chile.&lt;/item&gt;
      &lt;item&gt;For an example, see Dennis Meadows‚Äôs model of commodity price fluctuations: D.L. Meadows, Dynamics of Commodity Production Cycles, Portland OR, 1970.&lt;/item&gt;
      &lt;item&gt;John Kenneth Galbraith, The New Industrial State, 1967.&lt;/item&gt;
      &lt;item&gt;Ralph Waldo Emerson, ‚ÄúWar,‚Äù (lecture delivered in Boston, March, 1838). Reprinted in Emerson‚Äôs Complete Works, vol. XI, Boston, Houghton, Mifflin &amp;amp; Co., 1887, p. 177.&lt;/item&gt;
      &lt;item&gt;Thomas Kuhn, The Structure of Scientific Revolutions, Chicago, University of Chicago Press, 1962.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;‚ÄîDownload Leverage Points: Places to Intervene in a System as a PDF‚Äî&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/"/><published>2025-11-03T19:37:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45804122</id><title>The Mack Super Pumper was a locomotive engined fire fighter (2018)</title><updated>2025-11-03T23:09:47.646566+00:00</updated><content>&lt;doc fingerprint="3a1ad86c4fc08ebb"&gt;
  &lt;main&gt;
    &lt;p&gt;(Lead image: Philip Goldstein) ‚Äì In the early 1960s the New York City Fire Department was facing a host of problems. The world around them was growing ever taller, ever more compact, and ever more dangerous with respect to fire. There were times when the very infrastructure that was supposed to be supplying them the water to extinguish a blaze simply stopped flowing, there were other times that the equipment they had proved to be woefully insufficient to stop a fire that should have been controlled, resulting in massive blazes that ate up homes, businesses, lives, and millions upon millions of dollars. In April of 1963, a massive fire on Staten Island taxed the city‚Äôs fire service to the absolute breaking point while destroying millions and millions in property.&lt;/p&gt;
    &lt;p&gt;The day is still referred to as Black Saturday by the people who lived through it. Due to the lack of suitable water supplies, the fire was far larger than it should have been. There was a drought that year and many of the sources that the firemen were used to pulling water from had literally run dry. This began a series of events that led to the construction of the most powerful land based fire fighting truck ever created, the Mack Super Pumper System. It was actually five trucks that worked as a brigade to battle the worst flaming disasters that the city could throw at it.&lt;/p&gt;
    &lt;p&gt;From 1965 through the early 1980s, the Mack Super Pumper System responded to more than 2,200 calls with more than 900 firefighters serving to operate it in some capacity. The five trucks that made up the super pumper system were a massive, locomotive-engined central pumping truck, a tender truck full of hoses, manifolds, and other gear, and three satellite trucks that looked like standard fire engines but were not equipped with their own pumps. It cost the city of New York $875,000 when it was new and we‚Äôll wager to say that it was probably the best money ever spent to keep Gotham safe. There‚Äôs never been anything else like it.&lt;/p&gt;
    &lt;p&gt;The pumping unit ‚Äì&lt;/p&gt;
    &lt;p&gt;The keystone of the whole operation was the massive central pumping unit that could draw water from eight hydrants at once, drop lines into bodies of water, supply a mind-boggling number of lines with water simultaneously, and flow over 10,000 gallons per minute at low pressures if the situation called for it. When the pressure was ramped up to to 350psi, it could move 8,800 GPM. This was enough to supply the other satellite trucks as well as feed a massive water cannon on the tender truck that could heave water over 600ft. That‚Äôs right, nearly and eight of a mile in whatever direction you wanted it to go. How was this possible? It was possible because of innovations in diesel engine technology during WWII. The grunt for the Super Pumper system came from a Napier-Deltic diesel engine. This was an engine designed by the British during WWII as a lightweight, high speed means to propel their ships. Making 2,400 horsepower and even more prodigious torque numbers, the engine was ‚Äúlight‚Äù enough to be mounted in a trailer behind a tractor and carted around. The Napier-Deltic was used to power locomotives and other massive land craft as well for a while. The engine‚Äôs design is interesting in the fact that it had three crankshafts and was an opposed piston style engine meaning that the pistons travel at each other. With turbochargers and a two stroke design, it was as mighty a compact piston powered engine the world had ever known to that point. It was thirsty and noisy as well. When working at full song, the engine would consume 137 gallons of diesel fuel per hour and the noise was so deafening that firemen near the truck had to wear strong ear protection to prevent hearing damage.&lt;/p&gt;
    &lt;p&gt;Mack was awarded the contract to build the truck in 1964 and by the end of the year, the unit was nearly ready to hit the streets of NYC. The tractor employed to drag the pumping unit around was a F715FSTP cab over that used a 255hp Mack END864 engine. The top speed of the whole rig was 42mph but since it was intended for responding to calls in the city, high mph was not as much a concern as maneuverability, and the ability to zip around at lower speeds happily. There were custom built PTOs to power the priming pump for the water pump and to to run an air compressor that needed 450psi to light off the pump engine.&lt;/p&gt;
    &lt;p&gt;The custom built trailer housed the engine and all of the stuff needed to keep it alive like the cooling system, fuel tanks, etc. At the rear of the trailer was the enormous six stage pump which was built by a company called DeLaval and that‚Äôs where the real magic happened. When the big Deltic was put to work turning that bad boy, fire, at least any on the first 60 stories of a building, didn‚Äôt stand a chance. The whole rig weighed in at 68,000lbs and for as much reading and research as we have done, there are no accounts of it ever faltering, failing, or leaving firefighters without the resources they needed to battle a fire. Often times we read about awesome machines like this and discover that they were unreliable or prone to fail but not this big guy.&lt;/p&gt;
    &lt;p&gt;Some pretty stunning facts about the truck and the pumper:&lt;/p&gt;
    &lt;p&gt;At 8,800 GPM it was throwing nearly 70,000lbs of water on a fire per minute.&lt;/p&gt;
    &lt;p&gt;During a fire in the Bronx, firemen laid 7,000ft of hose to get to a suitable water supply and the truck pumped as though it was dipping its feet into the ocean.&lt;/p&gt;
    &lt;p&gt;In 1967 the Super Pumper responded to a fire at a postal annex in NYC and managed to supply water to the massive gun on the tender truck, its three satellite units, two tower ladder trucks, and a portable manifold with multiple hand lines all by itself.&lt;/p&gt;
    &lt;p&gt;The hoses on the truck were pressure tested to 1,000psi of pressure but typically operated anywhere in the 350-800psi range depending on the situation. This is way higher (by several times!) what modern trucks use by our understanding. The hoses were a derivative of hoses developed by the Navy in WWII for high pressure applications and while incredibly heavy when compared to modern hoses, they were cutting edge at the time.&lt;/p&gt;
    &lt;p&gt;The truck still exists, living at a museum in Michigan and standing as a great reminder that human beings are amazingly inventive and creative beings when forced to find solutions to problems that endanger lots of lives or lots of valuable property!&lt;/p&gt;
    &lt;p&gt;SCROLL DOWN FOR MORE INFORMATION AND PHOTOS ON THIS MIGHTY TRUCK THAT SERVED NYC DUTIFULLY FOR MANY YEARS ‚Äì&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bangshift.com/bangshiftxl/mack-super-pumper-system-locomotive-engine-powered-pumper-extinguish-hell-often/"/><published>2025-11-03T20:37:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45804377</id><title>AI's Dial-Up Era</title><updated>2025-11-03T23:09:47.176197+00:00</updated><content>&lt;doc fingerprint="898080510eed85e8"&gt;
  &lt;main&gt;
    &lt;p&gt;It is 1995.&lt;/p&gt;
    &lt;p&gt;Your computer modem screeches as it tries to connect to something called the internet. Maybe it works. Maybe you try again.&lt;/p&gt;
    &lt;p&gt;For the first time in history, you can exchange letters with someone across the world in seconds. Only 2000-something websites exist1, so you could theoretically visit them all over a weekend. Most websites are just text on gray backgrounds with the occasional pixelated image2. Loading times are brutal. A single image takes a minute, a 1-minute video could take hours. Most people do not trust putting their credit cards online. The advice everyone gives: don‚Äôt trust strangers on the internet.&lt;/p&gt;
    &lt;p&gt;People split into two camps very soon.&lt;/p&gt;
    &lt;p&gt;Optimists predict grand transformations. Some believe digital commerce will overtake physical retail within years. Others insist we‚Äôll wander around in virtual reality worlds.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúI expect that within the next five years more than one in ten people will wear head-mounted computer displays while traveling in buses, trains, and planes.‚Äù - Nicholas Negroponte, MIT Professor, 1993&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Pessimists call the internet a fad and a bubble.&lt;/p&gt;
    &lt;p&gt;If you told someone in 1995 that within 25 years, we‚Äôd consume news from strangers on social media over newspapers, watch shows on-demand in place of cable TV, find romantic partners through apps more than through friends, and flip ‚Äúdon‚Äôt trust strangers on the internet‚Äù so completely that we‚Äôd let internet strangers pick us up in their personal vehicles and sleep in their spare bedrooms, most people would find that hard to believe.&lt;/p&gt;
    &lt;p&gt;We‚Äôre in 1995 again. This time with Artificial Intelligence.&lt;/p&gt;
    &lt;p&gt;And both sides of today‚Äôs debate are making similar mistakes.&lt;/p&gt;
    &lt;p&gt;One side warns that AI will eliminate entire professions and cause mass unemployment within a couple of years. The other claims that AI will create more jobs than it destroys. One camp dismisses AI as overhyped vaporware destined for a bubble burst, while the other predicts it will automate every knowledge task and reshape civilization within the decade.&lt;/p&gt;
    &lt;p&gt;Both are part right and part wrong.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Employment Paradox: Why Automation‚Äôs Impact Depends On The Industry&lt;/head&gt;
    &lt;p&gt;Geoffrey Hinton, who some call the Father of AI, warned in 2016 that AI would trigger mass unemployment. ‚ÄúPeople should stop training radiologists now,‚Äù he declared, certain that AI would replace them within years.&lt;/p&gt;
    &lt;p&gt;Yet as Deena Mousa, a researcher, shows in ‚ÄúThe Algorithm Will See You Now,‚Äù AI hasn‚Äôt replaced radiologists, despite predictions. It is thriving.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2025, American diagnostic radiology residency programs offered a record 1,208 positions across all radiology specialties, a four percent increase from 2024, and the field‚Äôs vacancy rates are at all-time highs. In 2025, radiology was the second-highest-paid medical specialty in the country, with an average income of $520,000, over 48 percent higher than the average salary in 2015.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Mousa identifies a few factors for why the prediction failed - real-world complexity, the job involves more than image recognition, and regulatory/insurance hurdles. Most critical she points is Jevons Paradox, which is the economic principle that a technological improvement in resource efficiency leads to an increase in the total consumption of that resource, rather than a decrease. Her argument is that as AI makes radiologists more productive, better diagnostics and faster turnaround at lower costs mean more people get scans. So employment doesn‚Äôt decrease. It increases.&lt;/p&gt;
    &lt;p&gt;This is also the Tech world‚Äôs consensus. Microsoft CEO Satya Nadella agrees, as does Box CEO Aaron Levie, who suggests:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúThe least understood yet most important concept in the world is Jevons Paradox. When we make a technology more efficient, demand goes well beyond the original level. AI is the perfect example of this‚Äîalmost anything that AI is applied to will see more demand, not less.‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;They‚Äôre only half right.&lt;/p&gt;
    &lt;p&gt;First, as Andrej Karpathy, the computer scientist who coined the term vibe coding, points out, radiology is not the right job to look for initial job displacements.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúRadiology is too multi-faceted, too high risk, too regulated. When looking for jobs that will change a lot due to AI on shorter time scales, I‚Äôd look in other places - jobs that look like repetition of one rote task, each task being relatively independent, closed (not requiring too much context), short (in time), forgiving (the cost of mistake is low), and of course automatable giving current (and digital) capability. Even then, I‚Äôd expect to see AI adopted as a tool at first, where jobs change and refactor (e.g. more monitoring or supervising than manual doing, etc).‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Second, the tech consensus that we will see increased employment actually depends on the industry. Specifically, how much unfulfilled demand can be unlocked in that industry, and whether this unfulfilled demand growth outpaces continued automation and productivity improvement.&lt;/p&gt;
    &lt;p&gt;To understand this better, look at what actually happened in three industries over a 200-year period from 1800 to 2000. In the paper Automation and jobs: when technology boosts employment, James Bessen, an economist, shows the employment, productivity, and demand data for textile, iron &amp;amp; steel, and motor vehicle industries.&lt;/p&gt;
    &lt;p&gt;After automation, both textile and iron/steel workers saw employment increase for nearly a century before experiencing a steep decline. Vehicle manufacturing, by contrast, holds steady and hasn‚Äôt seen the same steep decline yet.&lt;/p&gt;
    &lt;p&gt;To answer why those two industries saw sharp declines but motor vehicle manufacturing did not, first look at the productivity of workers in all three industries:&lt;/p&gt;
    &lt;p&gt;Then look at the demand across those three industries:&lt;/p&gt;
    &lt;p&gt;What the graphs show is a consistent pattern (note: the productivity and demand graphs are logarithmic, meaning productivity and demand grew exponentially). Early on, a service or product is expensive because many workers are needed to produce it. Most people can‚Äôt afford it or use them sparingly. For example, in the early 1800s, most people could only afford a pair of pants or shirt. Then automation makes workers dramatically more productive. A textile worker in 1900 could produce fifty times more than one in 1800. This productivity explosion crashes prices, which creates massive new demand. Suddenly everyone can afford multiple outfits instead of just one or two. Employment and productivity both surge (note: employment growth masks internal segment displacement and wage changes. See footnote3)&lt;/p&gt;
    &lt;p&gt;Once demand saturates, employment doesn‚Äôt further increase but holds steady at peak demand. But as automation continues and workers keep getting more productive, employment starts to decline. In textiles, mechanization enabled massive output growth but ultimately displaced workers once consumption plateaued while automation and productivity continued climbing. We probably don‚Äôt need infinite clothing. Similarly, patients will likely never need a million radiology reports, no matter how cheap they become and so radiologists will eventually hit a ceiling. We don‚Äôt need infinite food, clothing, tax returns, and so on.&lt;/p&gt;
    &lt;p&gt;Motor vehicles, in Bessen‚Äôs graphs, tell a different story because demand remains far from saturated. Most people globally still don‚Äôt own cars. Automation hasn‚Äôt completely conquered manufacturing either (Tesla‚Äôs retreat from full manufacturing automation proves the current technical limits). When both demand and automation potential remain high, employment can sustain or even grow despite productivity gains.&lt;/p&gt;
    &lt;p&gt;Software presents an even more interesting question. How many apps do you need? What about software that generates applications on demand, that creates entire software ecosystems autonomously? Until now, handcrafted software was the constraint. Expensive software engineers and their labor costs limited what companies could afford to build. Automation changes this equation by making those engineers far more productive. Both consumer and enterprise software markets suggest significant unmet demand because businesses have consistently left projects unbuilt4. They couldn‚Äôt justify the development costs or had to allocate limited resources to their top priority projects. I saw this firsthand at Amazon. Thousands of ideas went unfunded not because they lacked business value, but because of the lack of engineering resources to build them. If AI can produce software at a fraction of the cost, that unleashes enormous latent demand. The key question then is if and when that demand will saturate.&lt;/p&gt;
    &lt;p&gt;So to generalize, for each industry, employment hinges on a race between two forces:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The magnitude and growth of unmet market demand, and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Whether that demand growth outpaces productivity improvements from automation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Different industries will experience different outcomes depending on who‚Äôs winning that demand and productivity race.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Bubble: Irrational Exuberance Builds the Future&lt;/head&gt;
    &lt;p&gt;The second debate centers on whether this AI boom is a bubble waiting to burst.&lt;/p&gt;
    &lt;p&gt;The dotcom boom of the 1990s saw a wave of companies adding ‚Äú.com‚Äù to their name to ride the mania and watch their valuations soar. Infra companies poured billions into fiber optics and undersea cables - expensive projects only possible because people believed the hype5. All of this eventually burst in spectacular fashion in the dotcom crash in 2000-2001. Infrastructure companies like Cisco briefly became the most valuable in the world only to come tumbling down6. Pets.com served as the poster child of this exuberance raising $82.5 million in its IPO, spending millions on a Super Bowl ad only to collapse nine months later7.&lt;/p&gt;
    &lt;p&gt;But the dotcom bubble also got several things right. More importantly, it eventually bought us the physical infrastructure that made YouTube, Netflix, and Facebook possible. Sure, companies like Worldcom, NorthPoint, and Global Crossing making these investments went bankrupt, but they also laid the foundation for the future. Although the crash proved the skeptics right in the short term, it proved the optimists were directionally correct in the long term.&lt;/p&gt;
    &lt;p&gt;Today‚Äôs AI boom shows similar exuberance. Consider the AI startup founded by former OpenAI executive Mira Murati, which raised $2 billion at a $10 billion valuation, the largest seed round in history8. This despite having no product and declining to reveal what it‚Äôs building or how it will generate returns. Several AI wrappers have raised millions in seed funding with little to no moat.&lt;/p&gt;
    &lt;p&gt;Yet some investments will outlast the hype and will likely help future AI companies even if this is a bubble. For example, the annual capital expenditures of Hyperscalers9 that have more than doubled since ChatGPT‚Äôs release - Microsoft, Google, Meta, and Amazon are collectively spending almost half a trillion dollars on data centers, chips, and compute infrastructure. Regardless of which specific companies survive, this infrastructure being built now will create the foundation for our AI future - from inference capacity to the power generation needed to support it.&lt;/p&gt;
    &lt;p&gt;The infrastructure investments may have long-term value, but are we already in bubble territory? Azeem Azhar, a tech analyst and investor, provides an excellent practical framework to answer the AI bubble question. He benchmarks today‚Äôs AI boom using five gauges: economic strain (investment as a share of GDP), industry strain (capex to revenue ratios), revenue growth trajectories (doubling time), valuation heat (price-to-earnings multiples), and funding quality (the resilience of capital sources). His analysis shows that AI remains in a demand-led boom rather than a bubble, but if two of the five gauges head into red, we will be in bubble territory.&lt;/p&gt;
    &lt;p&gt;The demand is real. After all OpenAI is one of the fastest-growing companies in history10. But that alone doesn‚Äôt prevent bubbles. OpenAI will likely be fine given its product-market fit, but many other AI companies face the same unit economics questions that plagued dotcom companies in the 1990s. Pets.com had millions of users too (a then large portion of internet users), but as the tech axiom goes, you can acquire infinite customers and generate infinite revenue if you sell dollars for 85 cents11. So despite the demand, the pattern may rhyme with the 1990s. Expect overbuilding. Expect some spectacular failures. But also expect the infrastructure to outlast the hype cycle and enable things we can‚Äôt yet imagine.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Predictably Unpredictable Future&lt;/head&gt;
    &lt;p&gt;So where does this leave us?&lt;/p&gt;
    &lt;p&gt;We‚Äôre early in the AI revolution. We‚Äôre at that metaphorical screeching modem phase of the internet era. Just as infrastructure companies poured billions into fiber optics, hyperscalers now pour billions into compute. Startups add ‚Äú.ai‚Äù to their names like companies once added ‚Äú.com‚Äù as they seek higher valuations. The hype will cycle through both euphoria and despair. Some predictions will look laughably wrong. Some that seem crazy will prove conservative.&lt;/p&gt;
    &lt;p&gt;Different industries will experience different outcomes. Unlike what the Jevons optimists suggest, demand for many things plateaus once human needs are met. Employment outcomes in any industry depend on the magnitude and growth of unmet market demand and whether that demand growth outpaces productivity improvements from automation.&lt;/p&gt;
    &lt;p&gt;Cost reduction will unlock market segments. Aswath Damodaran, a finance professor, (in)famously undervalued Uber assuming it would only capture a portion of the existing taxi market12. He missed that making rides dramatically cheaper would expand the market itself as people took Ubers to destinations they‚Äôd never have paid taxi prices to reach. AI will similarly enable products and services currently too expensive to build with human intelligence. A restaurant owner might use AI to create custom supply chain software that say at $100,000 with human developers would never have been built. A non-profit might deploy AI to contest a legal battle that was previously unaffordable.&lt;/p&gt;
    &lt;p&gt;We can predict change, but we can‚Äôt predict the details. No one in 1995 predicted we‚Äôd date strangers from the internet, ride in their ubers, or sleep in their airbnbs. Or that a job called influencers would become the most sought-after career among young people. Human creativity generates outcomes we can‚Äôt forecast with our current mental models. Expect new domains and industries to emerge. AI has already helped us decode more animal communication in the last five years than in the last fifty. Can we predict what jobs a technology that allows us to have full-blown conversations with them will unlock? A job that doesn‚Äôt exist today will likely be the most sought-after job in 2050. We can‚Äôt name it because it hasn‚Äôt been invented yet.&lt;/p&gt;
    &lt;p&gt;Job categories will transform. Even as the internet made some jobs obsolete, it also transformed them and created new categories. Expect the same with AI. Karpathy ends with a question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;About 6 months ago, I was also asked to vote if we will have less or more software engineers in 5 years. Exercise left for the reader.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To answer this question, go back to 1995 and ask the same question but with journalists. You might have predicted more journalists because the internet would create more demand by enabling you to reach the whole world. You‚Äôd be right for 10 or so years as employment in journalism grew until the early 2000s. But 30 years later, the number of newspapers and the number of journalists both have declined, even though more ‚Äújournalism‚Äù happens than ever. Just not by people we call journalists. Bloggers, influencers, YouTubers, and newsletter writers do the work that traditional journalists used to do13.&lt;/p&gt;
    &lt;p&gt;The same pattern will play out with software engineers. We‚Äôll see more people doing software engineering work and in a decade or so, what ‚Äúsoftware engineer‚Äù means will have transformed. Consider the restaurant owner from earlier who uses AI to create custom inventory software that is useful only for them. They won‚Äôt call themselves a software engineer.&lt;/p&gt;
    &lt;p&gt;So just like in 1995, if the AI optimists today say that within 25 years, we‚Äôd prefer news from AI over social media influencers, watch AI-generated characters in place of human actors, find romantic partners through AI matchmakers more than through dating apps (or perhaps use AI romantic partners itself), and flip ‚Äúdon‚Äôt trust AI‚Äù so completely that we‚Äôd rely on AI for life-or-death decisions and trust it to raise our children, most people would find that hard to believe. Even with all the intelligence, both natural and artificial, no one can predict with certainty what our AI future will look like. Not the tech CEOs, not the AI researchers, and certainly not some random guy pontificating on the internet. But whether we get the details right or not, our AI future is loading.&lt;/p&gt;
    &lt;p&gt;Approximately 2,879 websites were established before 1995, expanding to 23,500 by June 1995. See https://en.wikipedia.org/wiki/List_of_websites_founded_before_1995&lt;/p&gt;
    &lt;p&gt;Historical data from the Industrial Revolution shows that even as aggregate textile employment grew, workers shifted between job types within the industry as some roles became redundant. And correspondingly, some jobs saw wages collapse while others saw increases. For example, domestic hand-loom weavers were displaced by power looms and saw their wages collapse, while self-acting mule spinners (a newly created role) and factory workers saw stable employment and steady compensation growth. Additionally, Britain‚Äôs deflationary period (1815-1850) saw food prices fall by half, meaning real purchasing power often rose even when nominal wages declined. Despite all this, the psychological reality was harsh. Even with falling prices, watching your paycheck shrink while neighbors lost jobs and debts grew harder to pay (lower wages but fixed obligations) created real instability regardless of aggregate statistics improvements. Also see Acemoglu &amp;amp; Johnson, 2024.&lt;/p&gt;
    &lt;p&gt;Ask any tech product leader about their roadmap planning, and they‚Äôll all universally report far more worthwhile projects than resources to build them, forcing ruthless prioritization to decide what gets built.&lt;/p&gt;
    &lt;p&gt;Hyperscalers are large cloud computing companies like Microsoft, Google, Meta, and Amazon that operate massive data centers and provide the computing infrastructure necessary to train and run AI models at scale.&lt;/p&gt;
    &lt;p&gt;Damodaran valued Uber at $6B, assuming a 10% market share of the then $100B taxi market. Uber‚Äôs market cap as of October 2025 is $190B.&lt;/p&gt;
    &lt;p&gt;Job transformation doesn‚Äôt guarantee comparable compensation. Much of this new ‚Äújournalism‚Äù or content creation happens for free or at rates far below what traditional news organizations paid, separating the work from stable employment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wreflection.com/p/ai-dial-up-era"/><published>2025-11-03T21:01:09+00:00</published></entry></feed>