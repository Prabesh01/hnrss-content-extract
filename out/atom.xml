<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-28T00:47:53.695000+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46065698</id><title>Coq: The World's Best Macro Assembler? (2013) [pdf]</title><updated>2025-11-28T00:48:01.502775+00:00</updated><content/><link href="https://nickbenton.name/coqasm.pdf"/><published>2025-11-27T04:34:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065817</id><title>Music eases surgery and speeds recovery, study finds</title><updated>2025-11-28T00:48:01.387169+00:00</updated><content>&lt;doc fingerprint="adb964f948bf9fcf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Music eases surgery and speeds recovery, Indian study finds&lt;/head&gt;
    &lt;p&gt;Under the harsh lights of an operating theatre in the Indian capital, Delhi, a woman lies motionless as surgeons prepare to remove her gallbladder.&lt;/p&gt;
    &lt;p&gt;She is under general anaesthesia: unconscious, insensate and rendered completely still by a blend of drugs that induce deep sleep, block memory, blunt pain and temporarily paralyse her muscles.&lt;/p&gt;
    &lt;p&gt;Yet, amid the hum of monitors and the steady rhythm of the surgical team, a gentle stream of flute music plays through the headphones placed over her ears.&lt;/p&gt;
    &lt;p&gt;Even as the drugs silence much of her brain, its auditory pathway remains partly active. When she wakes up, she will regain consciousness more quickly and clearly because she required lower doses of anaesthetic drugs such as propofol and opioid painkillers than patients who heard no music.&lt;/p&gt;
    &lt;p&gt;That, at least, is what a new peer-reviewed study from Delhi's Maulana Azad Medical College and Lok Nayak Hospital suggests. The research, published in the journal Music and Medicine, offers some of the strongest evidence yet that music played during general anaesthesia can modestly but meaningfully reduce drug requirements and improve recovery.&lt;/p&gt;
    &lt;p&gt;The study focuses on patients undergoing laparoscopic cholecystectomy, the standard keyhole operation to remove the gallbladder. The procedure is short - usually under an hour - and demands a particularly swift, "clear-headed" recovery.&lt;/p&gt;
    &lt;p&gt;To understand why the researchers turned to music, it helps to decode the modern practice of anaesthesia.&lt;/p&gt;
    &lt;p&gt;"Our aim is early discharge after surgery," says Dr Farah Husain, senior specialist in anaesthesia and certified music therapist for the study. "Patients need to wake up clear-headed, alert and oriented, and ideally pain-free. With better pain management, the stress response is curtailed."&lt;/p&gt;
    &lt;p&gt;Achieving that requires a carefully balanced mix of five or six drugs that together keep the patient asleep, block pain, prevent memory of the surgery and relax the muscles.&lt;/p&gt;
    &lt;p&gt;In procedures like laparoscopic gallbladder removal, anaesthesiologists now often supplement this drug regimen with regional "blocks" - ultrasound-guided injections that numb nerves in the abdominal wall.&lt;/p&gt;
    &lt;p&gt;"General anaesthesia plus blocks is the norm," says Dr Tanvi Goel, primary investigator and a former senior resident of Maulana Azad Medical College. "We've been doing this for decades."&lt;/p&gt;
    &lt;p&gt;But the body does not take to surgery easily. Even under anaesthesia, it reacts: heart rate rises, hormones surge, blood pressure spikes. Reducing and managing this cascade is one of the central goals of modern surgical care. Dr Husain explains that the stress response can slow recovery and worsen inflammation, highlighting why careful management is so important.&lt;/p&gt;
    &lt;p&gt;The stress starts even before the first cut, with intubation - the insertion of a breathing tube into the windpipe.&lt;/p&gt;
    &lt;p&gt;To do this, the anaesthesiologist uses a laryngoscope to lift the tongue and soft tissues at the base of the throat, obtain a clear view of the vocal cords, and guide the tube into the trachea. It's a routine step in general anaesthesia that keeps the airway open and allows precise control of the patient's breathing while they are unconscious.&lt;/p&gt;
    &lt;p&gt;"The laryngoscopy and intubation are considered the most stressful response during general anaesthesia," says Dr Sonia Wadhawan, director-professor of anaesthesia and intensive care at Maulana Azad Medical College and supervisor of the study.&lt;/p&gt;
    &lt;p&gt;"Although the patient is unconscious and will remember nothing, their body still reacts to the stress with changes in heart rate, blood pressure, and stress hormones."&lt;/p&gt;
    &lt;p&gt;To be sure, the drugs have evolved. The old ether masks have vanished. In their place are intravenous agents - most notably propofol, the hypnotic made infamous by Michael Jackson's death but prized in operating theatres for its rapid onset and clean recovery. "Propofol acts within about 12 seconds," notes Dr Goel. "We prefer it for short surgeries like laparoscopic cholecystectomy because it avoids the 'hangover' caused by inhalational gases."&lt;/p&gt;
    &lt;p&gt;The team of researchers wanted to know whether music could reduce how much propofol and fentanyl (an opioid painkiller) patients required. Less drugs means faster awakening, steadier vital signs and reduced side effects.&lt;/p&gt;
    &lt;p&gt;So they designed a study. A pilot involving eight patients led to a full 11-month trial of 56 adults, aged roughly 20 to 45, randomly assigned to two groups. All received the same five-drug regimen: a drug that prevents nausea and vomiting, a sedative, fentanyl, propofol and a muscle relaxant. Both groups wore noise-cancelling headphones - but only one heard music.&lt;/p&gt;
    &lt;p&gt;"We asked patients to select from two calming instrumental pieces - soft flute or piano," says Dr Husain. "The unconscious mind still has areas that remain active. Even if the music isn't explicitly recalled, implicit awareness can lead to beneficial effects."&lt;/p&gt;
    &lt;p&gt;The results were striking.&lt;/p&gt;
    &lt;p&gt;Patients exposed to music required lower doses of propofol and fentanyl. They experienced smoother recoveries, lower cortisol or stress-hormone levels and a much better control of blood pressure during the surgery. "Since the ability to hear remains intact under anaesthesia," the researchers write, "music can still shape the brain's internal state."&lt;/p&gt;
    &lt;p&gt;Clearly, music seemed to quieten the internal storm. "The auditory pathway remains active even when you're unconscious," says Dr Wadhawan. "You may not remember the music, but the brain registers it."&lt;/p&gt;
    &lt;p&gt;The idea that the mind behind the anaesthetic veil is not entirely silent has long intrigued scientists. Rare cases of "intraoperative awareness" show patients recalling fragments of operating-room conversation.&lt;/p&gt;
    &lt;p&gt;If the brain is capable of picking up and remembering stressful experiences during surgery - even when a patient is unconscious - then it might also be able to register positive or comforting experiences, like music, even without conscious memory.&lt;/p&gt;
    &lt;p&gt;"We're only beginning to explore how the unconscious mind responds to non-pharmacological interventions like music," says Dr Husain. "It's a way of humanising the operating room."&lt;/p&gt;
    &lt;p&gt;Music therapy is not new to medicine; it has long been used in psychiatry, stroke rehabilitation and palliative care. But its entry into the intensely technical, machine-governed world of anaesthesia marks a quiet shift.&lt;/p&gt;
    &lt;p&gt;If such a simple intervention can reduce drug use and speed recovery - even modestly - it could reshape how hospitals think about surgical wellbeing.&lt;/p&gt;
    &lt;p&gt;As the research team prepares its next study exploring music-aided sedation, building on earlier findings, one truth is already humming through the data: even when the body is still and the mind asleep, it appears a few gentle notes can help the healing begin.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c231dv9zpz3o"/><published>2025-11-27T04:55:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065955</id><title>Tell HN: Happy Thanksgiving</title><updated>2025-11-28T00:48:01.109757+00:00</updated><content>&lt;doc fingerprint="7f4ed38a148e83a2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I‚Äôve been a part of this community for fifteen years. Despite the yearly bemoaning of HN‚Äôs quality compared to its mythical past, I‚Äôve found that it‚Äôs the one community that has remained steadfast as a source of knowledge, cattiness, and good discussion.&lt;/p&gt;
      &lt;p&gt;Thank you @dang and @tomhow.&lt;/p&gt;
      &lt;p&gt;Here's to another year.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46065955"/><published>2025-11-27T05:21:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46066280</id><title>Linux Kernel Explorer</title><updated>2025-11-28T00:48:00.893946+00:00</updated><content>&lt;doc fingerprint="7391f92da42b0365"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;The kernel isn't a process‚Äîit's the system. It serves user processes, reacts to context, and enforces separation and control.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The Kernel Is Not a Process: It's the always-present authority bridging hardware and software.&lt;/item&gt;
          &lt;item&gt;Serving the Process: Orchestrates syscalls, interrupts, and scheduling to keep user tasks running.&lt;/item&gt;
          &lt;item&gt;System of Layers: Virtual, mapped, isolated, and controlled‚Äîstructure at runtime.&lt;/item&gt;
        &lt;/list&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt;üìö Study Files&lt;/head&gt;
          &lt;div&gt;
            &lt;p&gt;init/main.c&lt;/p&gt;
            &lt;p&gt;kernel/fork.c&lt;/p&gt;
            &lt;p&gt;include/linux/sched.h&lt;/p&gt;
            &lt;p&gt;arch/x86/kernel/entry_64.S&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;div&gt;
              &lt;p&gt;1. What is the fundamental difference between the kernel and a process?&lt;/p&gt;
              &lt;div&gt;
                &lt;p&gt;A.The kernel is a special process with elevated privileges&lt;/p&gt;
                &lt;p&gt;B.The kernel is not a process‚Äîit's the system itself that serves processes&lt;/p&gt;
                &lt;p&gt;C.The kernel is just a library that processes link against&lt;/p&gt;
                &lt;p&gt;D.There is no difference; they are the same thing&lt;/p&gt;
              &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
              &lt;p&gt;2. How does the kernel primarily serve user processes?&lt;/p&gt;
              &lt;div&gt;
                &lt;p&gt;A.By running as a background daemon&lt;/p&gt;
                &lt;p&gt;B.By orchestrating syscalls, interrupts, and scheduling&lt;/p&gt;
                &lt;p&gt;C.By providing a GUI interface&lt;/p&gt;
                &lt;p&gt;D.By compiling user code&lt;/p&gt;
              &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
              &lt;p&gt;3. What characterizes the kernel's system of layers?&lt;/p&gt;
              &lt;div&gt;
                &lt;p&gt;A.Physical, tangible, and direct&lt;/p&gt;
                &lt;p&gt;B.Simple and flat with no hierarchy&lt;/p&gt;
                &lt;p&gt;C.Virtual, mapped, isolated, and controlled&lt;/p&gt;
                &lt;p&gt;D.User-accessible and modifiable&lt;/p&gt;
              &lt;/div&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://reverser.dev/linux-kernel-explorer"/><published>2025-11-27T06:17:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46066522</id><title>Mixpanel Security Breach</title><updated>2025-11-28T00:48:00.577844+00:00</updated><content>&lt;doc fingerprint="35be0cd749786243"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Our response to a recent security incident&lt;/head&gt;
    &lt;p&gt;Out of transparency and our desire to share with our community, this blog post contains key information about a recent security incident that impacted a limited number of our customers. On November 8th, 2025, Mixpanel detected a smishing campaign and promptly executed our incident response processes. We took comprehensive steps to contain and eradicate unauthorized access and secure impacted user accounts. We engaged external cybersecurity partners to remediate and respond to the incident.&lt;/p&gt;
    &lt;p&gt;We proactively communicated with all impacted customers. If you have not heard from us directly, you were not impacted. We continue to prioritize security as a core tenet of our company, products and services. We are committed to supporting our customers and communicating transparently about this incident.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we did in response&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Secured affected accounts&lt;/item&gt;
      &lt;item&gt;Revoked all active sessions and sign-ins&lt;/item&gt;
      &lt;item&gt;Rotated compromised Mixpanel credentials for impacted accounts&lt;/item&gt;
      &lt;item&gt;Blocked malicious IP addresses&lt;/item&gt;
      &lt;item&gt;Registered IOCs in our SIEM platform&lt;/item&gt;
      &lt;item&gt;Performed global password resets for all Mixpanel employees&lt;/item&gt;
      &lt;item&gt;Engaged third-party forensics firm to advise on containment and eradication measures&lt;/item&gt;
      &lt;item&gt;Performed a forensic review of authentication, session, and export logs across impacted accounts&lt;/item&gt;
      &lt;item&gt;Implemented additional controls to detect and block similar activity going forward.&lt;/item&gt;
      &lt;item&gt;Engaged with law enforcement and external cybersecurity advisors&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What you should know&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you received a communication from us, please review it for the steps we have taken to secure your account, as well as next steps.&lt;/item&gt;
      &lt;item&gt;If you did not receive a communication from us, no action is required. Your accounts were not impacted.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have any questions about this incident, please contact support@mixpanel.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mixpanel.com/blog/sms-security-incident/"/><published>2025-11-27T07:02:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46066695</id><title>Ray Marching Soft Shadows in 2D (2020)</title><updated>2025-11-28T00:48:00.386959+00:00</updated><content>&lt;doc fingerprint="3fae98912ba42222"&gt;
  &lt;main&gt;
    &lt;p&gt;Disclaimer: the demos on this page use WebGL features that aren‚Äôt available on some mobile devices.&lt;/p&gt;
    &lt;p&gt;A couple of weeks ago I tweeted a video of a toy graphics project (below). It‚Äôs not done, but a lot of people liked it which was surprising and fun! A few people asked how it works, so that‚Äôs what this post is about.&lt;/p&gt;
    &lt;p&gt;Under the hood it uses something called a distance field. A distance field is an image like the one below that tells you how far each pixel is from your shape. Light grey pixels are close to the shape and dark grey pixels are far from it.&lt;/p&gt;
    &lt;p&gt;When the demo starts up, it draws some text on a 2D canvas and generates a distance field of it. It uses a library I wrote that generates distance fields really quickly. If you‚Äôre curious how the library works, I wrote about that here.&lt;/p&gt;
    &lt;p&gt;Our lighting scheme works like this: when processing a particular pixel we consider a ray from it to the light, like so‚Ä¶&lt;/p&gt;
    &lt;p&gt;If the ray intersects a glyph, the pixel we‚Äôre shading must be in shadow because there‚Äôs something between it and the light.&lt;/p&gt;
    &lt;p&gt;The simplest way to check this would be to move along the ray in 1px increments, starting from the pixel we‚Äôre shading and ending at the light, repeatedly asking the distance field if we‚Äôre distance 0 from a shape. This would work, but it‚Äôd be really slow.&lt;/p&gt;
    &lt;p&gt;We could pick some specific length like 30px and move in increments of that size, but then we risk jumping over glyphs that are smaller than 30px. We might think we‚Äôre not in shadow when we should be.&lt;/p&gt;
    &lt;p&gt;Ray marching‚Äôs core idea is this: the distance field tells you how far you are from the closest glyph. You can safely advance along your ray by that distance without skipping over any glyphs.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs walk through an example. We start as pictured above and ask the distance field how far we are from any glyph. Turns out in this case that the answer is 95px (pictured left). This means that we can move 95px along our ray without skipping over anything!&lt;/p&gt;
    &lt;p&gt;Now we‚Äôre a little closer to the light. We repeat the process until we hit the ascender of the b! If the b glyph weren‚Äôt there, we‚Äôd have kept going until we hit the light.&lt;/p&gt;
    &lt;p&gt;Below is a demo that shows the ray marching steps for a given pixel. The red box is the pixel we‚Äôre shading, and each circle along the ray represents a ray marching step and the distance from the scene at that step.&lt;/p&gt;
    &lt;p&gt;Try dragging the light and the pixel around to build an intuition for it.&lt;/p&gt;
    &lt;p&gt;Below is GLSL to implement this technique. It assumes you‚Äôve defined a function &lt;code&gt;getDistance&lt;/code&gt; that samples the distance field.&lt;/p&gt;
    &lt;code&gt;vec2 rayOrigin = ...;
vec2 rayDirection = ...;

float rayProgress = 0;
while (true) {
  if (rayProgress &amp;gt; distance(rayOrigin, lightPosition)) {
    // We hit the light! This pixel is not in shadow.
    return 1.;
  }

  float sceneDist = getDistance(
    rayOrigin + rayProgress * rayDirection);
  if (sceneDist &amp;lt;= 0.) {
    // We hit a shape! This pixel is in shadow.
    return 0.;
  }

  rayProgress += sceneDist;
}
&lt;/code&gt;
    &lt;p&gt;It turns out that some pixels are really expensive to process. So in practice we use a for-loop instead of a while loop ‚Äì that way we bail out if we‚Äôve done too many steps. A common ‚Äúslow case‚Äù in ray marching is when a ray is parallel to the edge of a shape in the scene‚Ä¶&lt;/p&gt;
    &lt;p&gt;The approach I‚Äôve described so far will get you a scene that looks like the one below.&lt;/p&gt;
    &lt;p&gt;It‚Äôs cool, but the shadows are sharp which doesn‚Äôt look very good. The shadows in the demo look more like this‚Ä¶&lt;/p&gt;
    &lt;p&gt;One big disclaimer is that they‚Äôre not physically realistic! Real shadows look like hard shadows where the edges have been fuzzed. This approach does something slightly different: all pixels that were previously in shadow are still fully in shadow. We‚Äôve just added a penumbra of partially shaded pixels around them.&lt;/p&gt;
    &lt;p&gt;The upside is that they‚Äôre pretty and fast to compute, and that‚Äôs what I care about! There are three ‚Äúrules‚Äù involved in computing them.&lt;/p&gt;
    &lt;p&gt;Rule 1: The closer a ray gets to intersecting a shape, the more its pixel should be shadowed. In the image below there are two similar rays (their distances to the shape pictured in yellow and green). We want the one that gets closer to touching the corner to be more shadowed.&lt;/p&gt;
    &lt;p&gt;This is cheap to compute because the variable &lt;code&gt;sceneDist&lt;/code&gt; tells us how far we are from the closest shape at each ray marching step. So the smallest value of &lt;code&gt;sceneDist&lt;/code&gt; across all steps is a good approximation for the yellow and green lines in the image above.&lt;/p&gt;
    &lt;p&gt;Rule 2: if the pixel we‚Äôre shading is far from the point where it almost intersects a shape, we want the shadow to spread out more.&lt;/p&gt;
    &lt;p&gt;Consider two pixels along the ray above. One is closer to the almost-intersection and is lighter (its distance is the green line). The other is farther and darker (its distance is the yellow line). In general: the further a pixel is from its almost intersection, the more ‚Äúin shadow‚Äù we should make it.&lt;/p&gt;
    &lt;p&gt;This is cheap to compute because the variable &lt;code&gt;rayProgress&lt;/code&gt; is the length of the green and yellow lines in the image above.&lt;/p&gt;
    &lt;p&gt;So: we previously returned &lt;code&gt;1.0&lt;/code&gt; for pixels that weren‚Äôt in shadow. To implement rules 1 and 2, we compute &lt;code&gt;sceneDist / rayProgress&lt;/code&gt; on each ray marching step, keep track of its minimum value, and return that instead.&lt;/p&gt;
    &lt;code&gt;vec2 rayOrigin = ...;
vec2 rayDirection = ...;
float rayProgress = 0.;
float stopAt = distance(samplePt, lightPosition);
float lightContribution = 1.;
for (int i = 0; i &amp;lt; 64; i++) {
  if (rayProgress &amp;gt; stopAt) {
    return lightContribution;
  }

  // `getDistance` samples our distance field texture.
  float sceneDist = getDistance(
    rayOrigin + rayProgress * rayDirection);
  if (sceneDist &amp;lt;= 0.) {
    // We hit a shape! This pixel is in shadow.
    return 0.;
  }

  lightContribution = min(
    lightContribution,
    sceneDist / rayProgress
  );

  rayProgress += sceneDist;
}

// Ray-marching took more than 64 steps!
return 0.;
&lt;/code&gt;
    &lt;p&gt;This ratio feels kind of magical to me because it doesn‚Äôt correspond to any physical value. So let‚Äôs build some intuition for it by thinking through why it might take on particular values‚Ä¶&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;If&lt;/p&gt;&lt;code&gt;sceneDist / rayProgress &amp;gt;= 1&lt;/code&gt;, then either&lt;code&gt;sceneDist&lt;/code&gt;is big or&lt;code&gt;rayProgress&lt;/code&gt;is small (relative to each other). In the former case we‚Äôre far from any shapes and we shouldn‚Äôt be in shadow, so a light value of&lt;code&gt;1&lt;/code&gt;makes sense. In the latter case, the pixel we‚Äôre shadowing is really close to an object casting a shadow and the shadow isn‚Äôt fuzzy yet, so a light value of&lt;code&gt;1&lt;/code&gt;makes sense.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The ratio is&lt;/p&gt;&lt;code&gt;0&lt;/code&gt;only when&lt;code&gt;sceneDist&lt;/code&gt;is&lt;code&gt;0&lt;/code&gt;. This corresponds to rays that intersect an object and whose pixels are in shadow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And here‚Äôs a demo of what we have so far‚Ä¶&lt;/p&gt;
    &lt;p&gt;Rule #3 is the most straightforward one: light gets weaker the further you get from it.&lt;/p&gt;
    &lt;p&gt;Instead of returning the minimum value of &lt;code&gt;sceneDist / rayProgress&lt;/code&gt; verbatim, we multiply it by a &lt;code&gt;distanceFactor&lt;/code&gt; which is &lt;code&gt;1&lt;/code&gt; right next to the light, &lt;code&gt;0&lt;/code&gt; far away from it, and gets quadratically smaller as you move away from it.&lt;/p&gt;
    &lt;p&gt;All together, the code for the approach so far looks like this‚Ä¶&lt;/p&gt;
    &lt;code&gt;vec2 rayOrigin = ...;
vec2 rayDirection = ...;
float rayProgress = 0.;
float stopAt = distance(samplePt, lightPosition);
float lightContribution = 1.;
for (int i = 0; i &amp;lt; 64; i++) {
  if (rayProgress &amp;gt; stopAt) {
    // We hit the light!
    float LIGHT_RADIUS_PX = 800.;

    // fadeRatio is 1.0 next to the light and 0. at
    // LIGHT_RADIUS_PX away.
    float fadeRatio =
      1.0 - clamp(stopAt / LIGHT_RADIUS_PX, 0., 1.);

    // We'd like the light to fade off quadratically instead of
    // linearly.
    float distanceFactor = pow(fadeRatio, 2.);
    return lightContribution * distanceFactor;
  }

  // `getDistance` samples our distance field texture.
  float sceneDist = getDistance(rayOrigin + rayProgress * rayDirection);
  if (sceneDist &amp;lt;= 0.) {
    // We hit a shape! This pixel is in shadow.
    return 0.;
  }

  lightContribution = min(
    lightContribution,
    sceneDist / rayProgress
  );

  rayProgress += sceneDist;
}

// Ray-marching took more than 64 steps!
return 0.;
&lt;/code&gt;
    &lt;p&gt;I forget where I found this soft-shadow technique, but I definitely didn‚Äôt invent it. Inigo Quilez has a great post on it where he talks about using it in 3D.&lt;/p&gt;
    &lt;p&gt;Inigo‚Äôs post also talks about a gotcha with this approach that you might have noticed in the demos above: it causes banding artifacts. This is because Rule 1 assumes that the smallest value of &lt;code&gt;sceneDist&lt;/code&gt; across all steps is a good approximation for the distance from a ray to the scene. This is not always true because we sometimes take very few ray marching steps.&lt;/p&gt;
    &lt;p&gt;So in my demo I use an improved approximation that Inigo writes about in his post. I also use another trick that is more effective but less performant: instead of advancing by &lt;code&gt;sceneDist&lt;/code&gt; on each ray marching step, I advance by something like &lt;code&gt;sceneDist * randomJitter&lt;/code&gt; where &lt;code&gt;randomJitter&lt;/code&gt; is between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This improves the approximation because we‚Äôre adding more steps to our ray march. But we could do that by advancing by &lt;code&gt;sceneDist * .3&lt;/code&gt;. The random jitter ensures that pixels next to each other don‚Äôt end up in the same band. This makes the result a little grainy which isn‚Äôt great. But I think looks better than banding‚Ä¶ This is an aspect of the demo that I‚Äôm still not satisfied with, so if you have ideas for how to improve it please tell me!&lt;/p&gt;
    &lt;p&gt;Overall my demo has a few extra tweaks that I might write about in future but this is the core of it. Thanks for reading! If you have questions or comments, let me know on Twitter.&lt;/p&gt;
    &lt;p&gt;_Thank you to Jessica Liu, Susan Wang, Matt Nichols and Kenrick Rilee for giving feedback on early drafts of this post!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.rykap.com/2020/09/23/distance-fields/"/><published>2025-11-27T07:31:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46068777</id><title>The current state of the theory that GPL propagates to AI models</title><updated>2025-11-28T00:48:00.042706+00:00</updated><content>&lt;doc fingerprint="2d531d13953a0592"&gt;
  &lt;main&gt;
    &lt;p&gt;When GitHub Copilot was launched in 2021, the fact that its training data included a vast amount of Open Source code publicly available on GitHub attracted significant attention, sparking lively debates regarding licensing. While there were issues concerning conditions such as attribution required by most licenses, there was a particularly high volume of discourse suggesting that the conditions of copyleft licenses, such as the GNU General Public License (GNU GPL), would propagate to the model itself, necessitating that the entire model be released under the same license. The propagation of the GPL is a concept that many modern software engineers have naturally accepted; thus, for an engineer with a straightforward sensibility, it is a perfectly natural progression to think that if GPL code is included in some form, copyleft applies and the license propagates.&lt;/p&gt;
    &lt;p&gt;However, as of 2025, the theory that the license of the source code propagates to AI models trained on Open Source code is not seen as frequently as it was back then. Although some ardent believers in software freedom still advocate for such theories, it appears they are being overwhelmed by the benefits of AI coding, which has overwhelmingly permeated the programming field. Amidst this trend, even I sometimes succumb to the illusion that such a theory never existed in the first place.&lt;/p&gt;
    &lt;p&gt;Has the theory that the license of training code propagates to such AI models been completely refuted?&lt;/p&gt;
    &lt;p&gt;Actually, it has not. This issue remains an indeterminate problem where lawsuits are still ongoing and the judgments of major national governments have not been made clear. In this article, I will explain the current situation of this license propagation theory, namely ‚ÄúGPL propagates to AI models trained on GPL code,‚Äù and connect it to points of discussion such as the legal positioning of models and the nature of the freedom we pursue in the AI domain.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The Current Standing in Two Lawsuits&lt;/item&gt;
      &lt;item&gt;Arguments Negating the Theory of License Propagation to Models&lt;/item&gt;
      &lt;item&gt;The Stance of OSI and FSF&lt;/item&gt;
      &lt;item&gt;Summary&lt;/item&gt;
      &lt;item&gt;References&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: This article is an English translation of a post originally written in Japanese. While it assumes a Japanese reader, I believe it may also be useful for an English-speaking audience.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Current Standing in Two Lawsuits&lt;/head&gt;
    &lt;p&gt;First, let us organize what the ‚ÄúGPL propagation theory to AI models‚Äù entails. This is the idea that when an AI model ingests GPL code as training data, the model itself constitutes a derivative work (derivative) of the GPL code; therefore, when distributing the model, the copyleft conditions of the GPL, such as the obligation to disclose source code, apply. In other words, it is not a question of whether the output of the model is similar to the GPL code, but a theory that ‚Äúsince the model itself is a derivative containing GPL code, the GPL extends to the model.‚Äù While there were many voices supporting this theory around 2021, as mentioned earlier, it is no longer the mainstream of the discussion today. However, two major ongoing lawsuits can be cited as grounds that this theory has not been completely denied. These are Doe v. GitHub (the Copilot class action) filed in the United States and GEMA v. OpenAI filed in Germany. I will explain the history and current status of each lawsuit below.&lt;/p&gt;
    &lt;head rend="h3"&gt;Doe v. GitHub (Copilot Class Action): The Persisting Claim of Open Source License Violation&lt;/head&gt;
    &lt;p&gt;In the Copilot class action filed at the end of 2022 in relation to GitHub Copilot, anonymous developers became plaintiffs and argued that GitHub, Microsoft, and OpenAI trained their models on source code from public repositories without permission, inviting massive license violations through Copilot. Specifically, they viewed it as problematic that when Copilot reproduces part of the code that served as the training source in its output, it does not perform the author attribution or copyright notice required by licenses such as MIT or Apache-2.0 at all, and furthermore, it indiscriminately trains on and outputs code under licenses that impose copyleft conditions like the GPL, thereby trampling on license clauses. The plaintiffs claimed this was a contractual violation of open source licenses and also sought damages and injunctions, asserting that it constituted a violation of the Digital Millennium Copyright Act (DMCA) under copyright law.&lt;/p&gt;
    &lt;p&gt;In this case, several decisions have already been handed down by the United States District Court for the Northern District of California, and many of the plaintiffs‚Äô claims have been dismissed. What were dismissed were mainly peripheral claims such as DMCA clause violations, privacy policy violations, unjust enrichment, and torts, but some DMCA violations and the claim of ‚Äúviolation of open source licenses‚Äù (breach of contract) are still alive. Regarding the latter specifically, the argument is that despite the plaintiffs‚Äô code being published under licenses like GPL or MIT, the defendants failed to comply with the author attribution or the obligation to publish derivatives under the same license, which constitutes a contractual violation. Although the court did not recognize claims for monetary damages because the plaintiffs could not demonstrate a specific amount of damage, it determined that there were sufficient grounds for the claim for injunctive relief against the license violation itself. As a result, the plaintiffs are permitted to continue the lawsuit seeking an order prohibiting the act of Copilot reproducing others‚Äô code without appropriate license indications.&lt;/p&gt;
    &lt;p&gt;As is clear from the above history, ‚Äúviolation of open source licenses in training data‚Äù is still being contested in court in the Copilot litigation, and this is one of the reasons why the theory of license propagation to models has not been completely denied. The plaintiffs‚Äô claim in this lawsuit does not directly demand the release of the model itself under the GPL, but it legally pursues the point that license conditions were ignored in the process of training and output; consequently, it suggests that ‚Äúif the handling does not follow the license of the training data, the act of providing the model could be illegal.‚Äù Furthermore, the court has not clearly rejected this logic at this stage and has indicated a judgment that the use of open source code is accompanied by license obligations, and providing tools that ignore this could constitute a tort subject to injunction.&lt;/p&gt;
    &lt;p&gt;However, it is necessary to note that the claims in the Copilot litigation are legally framed as breach of contract (license) or DMCA violation, and are not a direct copyright argument that ‚Äúthe model is a derivative work of GPL code.‚Äù No judgment has been shown stepping so far as to mandate the disclosure of the entire model under the GPL license. The actual judgment is conservative, stating ‚Äúmonetary damages have not been shown, but there is room for future injunctive relief,‚Äù and does not mention the obligation to disclose the model itself. In other words, at present, there is no judicial precedent directly addressing the ‚ÄúGPL propagation theory to models,‚Äù and the situation is one where the issue raised regarding license violation of the source code remains alive in the judicial arena.&lt;/p&gt;
    &lt;head rend="h3"&gt;GEMA v. OpenAI: The Theory Treating ‚ÄúMemory‚Äù in Models as Legal Reproduction&lt;/head&gt;
    &lt;p&gt;Another important lawsuit is the case where the German music copyright collective GEMA sued OpenAI. This is a copyright lawsuit concerning the unauthorized training and output of lyrics by an AI model, not AI code generation, but it carries significant theoretical implications related to ‚Äúlicense propagation to models‚Äù even if not directly related to GPL.&lt;/p&gt;
    &lt;p&gt;In November 2025, the Munich I Regional Court handed down a judgment on this lawsuit, indicating regarding the matter where the ChatGPT model had memorized and reproduced the lyrics of 9 famous German songs, that the act of ‚Äúmemory‚Äù inside the model itself falls under the act of reproduction under copyright law. According to the judgment, the lyrics under the plaintiff‚Äôs management were ‚Äúfixed‚Äù in the models of ChatGPT‚Äôs GPT-4 and 4o, and the situation was such that the lyrics were output almost verbatim just by the user giving a simple prompt. Based on this, the court determined that the model contains ‚Äúparameters that memorized the work‚Äù internally, and if it is possible to reproduce an expression substantially identical to the original work for a human by means of an appropriate prompt, that memory itself falls under ‚Äúreproduction‚Äù in Article 16 of the German Copyright Act. Furthermore, it determined that the act of actually outputting lyrics in response to a prompt is also a separate act of reproduction, and providing lyrics to the user falls under the act of making available to the public (public transmission). Also, it ruled that since all of these are done without the permission of the rights holder, they deviate from the scope justified by the TDM (Text and Data Mining) exception in the EU DSM Copyright Directive.&lt;/p&gt;
    &lt;p&gt;The important point of this judgment is that it clearly acknowledged that ‚Äúif a work is recorded inside the model in a reproducible form, that state itself can constitute copyright infringement.‚Äù The court cited the text of the EU InfoSoc Directive that ‚Äúreproduction includes copies in any form or manner, and does not need to be directly perceptible to humans,‚Äù and stated that in the spirit of this, even if the lyrics are encoded within the model‚Äôs parameters, it amounts to the creation of a reproduction. It went as far as to mention that ‚Äúencoding in the form of probabilistic weights does not prevent it from being considered a copy,‚Äù showing a strong recognition that differences in technical formats cannot avoid the nature of reproduction under copyright law. Also, since the fact that the model could output the lyrics was not coincidental but highly consistent, it was factually found that ‚Äúthe direct incorporation of the essential part of the training data‚Äù occurred rather than the result of statistical learning. As a result, the Munich District Court recognized OpenAI‚Äôs liability for injunction and damages regarding the output act of the lyrics in question, and further ordered the provision of information regarding training data and output content for the future. However, this judgment is the first instance, and since OpenAI has indicated an intention to appeal, it is expected to be a continuing dispute.&lt;/p&gt;
    &lt;p&gt;The noteworthy theory shown by this GEMA judgment is the extension of the concept of reproduction under copyright law to the interior of the model. That is, if the work used as training data remains within the model and can be reproduced with a simple operation, it means the model already contains a reproduction of that work. This theory is groundbreaking in that it deems ‚Äúthe model contains the source work,‚Äù and indeed, in a commentary by Osborne Clarke, it is evaluated that ‚Äúin contrast to the judgment of the English High Court in the Getty v. Stability AI case, the Munich District Court explicitly recognized the possibility that the AI model contains copies of the training material.‚Äù Standing on this view, the model is not merely a result of analysis, but depending on the case, can be evaluated as an aggregate of the training data itself.&lt;/p&gt;
    &lt;p&gt;However, it is necessary to keep in mind that this judgment is based on an extreme case where a complete match output was obtained with short text such as lyrics. The court itself stated, ‚ÄúNormally, temporary reproduction for learning remains within the purpose of analysis and does not infringe on the rights holder‚Äôs market, but in this case, the model holds the work in a restorable form and exceeds the scope of analysis,‚Äù emphasizing that the judgment is limited to ‚Äúcases where the model performs complete reproduction.‚Äù Also, as the UK case shows, judicial decisions vary by country, and a legal consensus on this issue has not yet been formed.&lt;/p&gt;
    &lt;p&gt;Nevertheless, the judgment this time, which declared that the recording of a work inside a model is a reproduction, can become a major basis supporting the license propagation theory. This is because, while the premise for discussing GPL propagation is ‚Äúwhether the model can be said to be a reproduction or derivative work of the GPL code,‚Äù the logic of the Munich District Court legally certified exactly that ‚Äúa model can be a reproduction of training data‚Äù.&lt;/p&gt;
    &lt;head rend="h3"&gt;Possibilities Derived from the Current Status of the Two Lawsuits&lt;/head&gt;
    &lt;p&gt;From the two lawsuits above, we can consider the path through which the theory of license propagation to AI models might be recognized in the future.&lt;/p&gt;
    &lt;p&gt;Let us assume the worst-case scenario from the perspective of AI operators, where these lawsuits are finalized with the plaintiffs winning. In the Copilot litigation, the judgment that ‚Äúmodel providers must comply with the license conditions of the training source code‚Äù would be established, and in the GEMA litigation, the legal principle that ‚Äúthe model encompasses reproductions of the work‚Äù would be established. When these two intersect, the conclusion that ‚Äúsince an AI model containing GPL code is a reproduction or derivative work of the GPL code, the conditions of the GPL directly apply to its provision‚Äù is theoretically derived. That is, the possibility emerges that the theory of GPL propagation to models is effectively ratified by the judiciary.&lt;/p&gt;
    &lt;p&gt;Specifically, if the model memorizes and contains GPL code fragments internally, the act of distributing or providing that model to a third party may be regarded as the distribution of a reproduction of GPL code; in that case, the act of distribution under conditions other than GPL would be evaluated as a GPL license violation. If a GPL violation is established, there would be room to argue for remedies such as injunctions and claims for damages, as well as forced GPL compliance demanding the disclosure of the entire model under the same license, just as in the case of ordinary software. In fact, the remedies GEMA sought from OpenAI included disclosure regarding training data and output content, and although this is in the context of musical works, this can be said to be a type of disclosure request to make transparent ‚Äúwhat the model learned and contains.‚Äù In the case of a GPL violation as well, the possibility cannot be denied that demands such as ‚Äúdisclosure of the GPL code parts contained inside the model‚Äù or ‚Äúsource disclosure in a form that allows reconstruction of the model‚Äù would emerge in seeking license compliance.&lt;/p&gt;
    &lt;p&gt;Even if not reaching such an extreme conclusion, an intermediate scenario could involve imposing certain restrictions on model providers. For example, the Copilot litigation might be settled or judged by taking measures such as ‚Äúattaching a license and author attribution at the time of output if existing code of a certain length or more is included in the generated code,‚Äù or technically mandating the implementation of filters so that GPL code fragments are not extracted or reproduced from the model. In fact, GitHub, the developer of Copilot, has already introduced an optional feature that ‚Äúexcludes from suggestions if the candidate code matches existing code on large-scale repositories,‚Äù attempting to reduce litigation risk. Also regarding OpenAI, there are reports that it strengthened filters so that ChatGPT does not output copyrighted lyrics as they are, in response to the GEMA judgment.&lt;/p&gt;
    &lt;p&gt;While these are not license propagation itself legally, in practice, they indicate that the industry is steering in the direction of ‚Äúensuring the model does not potentially infringe license conditions.‚Äù In the future, there is a possibility that guidelines for excluding data with specific license terms like GPL at the model training stage, or mechanisms and systems to guarantee that there is no license-infringing output by conducting output inspections after training, will be established.&lt;/p&gt;
    &lt;p&gt;In any case, until these two lawsuits are completely settled and the subsequent legislative response is determined, the ‚Äútheory of GPL propagation to models‚Äù has not completely disappeared. It is a scenario that could suddenly become realistic depending on future judgments, and even if the plaintiffs lose in the lawsuits, there is a possibility that support for this theory will reignite within the open source community. It is necessary to note that while it is currently an ‚Äúundetermined theory not shouted as loudly as before,‚Äù that does not mean it has been legally completely denied and resolved. As our community, we need to carefully consider countermeasures while observing these trends and taking into account the legal systems of each country and opposing arguments described in the latter half of this article.&lt;/p&gt;
    &lt;head rend="h3"&gt;Treatment under Japanese Law&lt;/head&gt;
    &lt;p&gt;Based on the trends of the overseas lawsuits mentioned above, I will also organize the relationship between AI models, copyrighted works, and licenses under Japanese law. In Japan, Article 30-4 of the Copyright Act, introduced by the 2018 amendment, exists as a provision that comprehensively legalizes reproduction acts associated with machine learning. Furthermore, in March 2024, the Copyright Division of the Council for Cultural Affairs of the Agency for Cultural Affairs published a guideline-like document titled ‚ÄúThought on AI and Copyright‚Äù (hereinafter ‚Äúthe Thought‚Äù), presenting a legal organization divided into the development/training stage and the generation/utilization stage of generative AI.&lt;/p&gt;
    &lt;p&gt;According to ‚Äúthe Thought,‚Äù reproduction performed basically for the purpose of AI training is legal as long as it satisfies ‚Äúinformation analysis not for the purpose of enjoying the thoughts or sentiments expressed in the work‚Äù as defined in Article 30-4. Therefore, acts of collecting and reproducing a wide range of data from the internet to create a training dataset for research and development purposes can be done without the permission of the rights holders in principle. However, what is important is whether an ‚Äúpurpose of enjoyment‚Äù is mixed into that training act. ‚ÄúThe Thought‚Äù states that if training is conducted with the purpose of ‚Äúintentionally reproducing all or part of the creative expression of a specific work in the training data as the output of generative AI,‚Äù it is evaluated as having a concurrent purpose of enjoying the work rather than mere information analysis, and thus lacks the application of Article 30-4. As a typical example of this, ‚Äúoverfitting‚Äù is cited, and acts such as making a model memorize specific groups of works through additional training to cause it to output something similar to those works are judged to have a purpose of enjoyment.&lt;/p&gt;
    &lt;p&gt;Furthermore, ‚Äúthe Thought‚Äù also mentions the legal treatment of trained models, stating first that ‚Äútrained models created by AI training cannot be said to be reproductions of the works used for training in many cases.‚Äù This is the view that since the model can generate outputs unrelated to the original in response to various inputs in a general-purpose manner, the model itself is not a copy of any specific work.&lt;/p&gt;
    &lt;p&gt;However, ‚Äúthe Thought‚Äù simultaneously acknowledges the possibility that, exceptionally, in cases where ‚Äúthe trained model is in a state of generating products with similarity to the work that was training data with high frequency,‚Äù the creative expression of the original work remains in the model, and it may be evaluated as a reproduction. It also points out that in such cases, the model is positioned as a machine for copyright infringement, and a claim for injunction may be recognized. In short, usually the model is merely statistical data and not the work itself, but if it has turned into a device for spewing out specific works almost as they are, it can be treated as an infringing item; this thinking shares parts with the content of the GEMA judgment.&lt;/p&gt;
    &lt;p&gt;It is necessary to note that the above organization is strictly a discussion of the scope of application of rights limitation provisions (exception provisions) under the Copyright Act, and does not touch upon the validity of contracts or license clauses. The Agency for Cultural Affairs document discusses from the perspective of ‚Äúwhether it is copyright infringement or not,‚Äù and does not deny that even if the training act is legal, contractual liability may arise if it violates terms of service or open source licenses separately. Also, no in-depth view has been shown regarding the propagation of copyleft clauses like the GPL. In Japan‚Äôs Copyright Act, there is no override provision where rights limitation provisions like Article 30-4 take precedence over contract conditions, and the ‚ÄúContract Guidelines on Utilization of AI and Data‚Äù by the Ministry of Economy, Trade and Industry suggests the possibility that if there is a contract prohibiting data use between parties, that contract takes precedence.&lt;/p&gt;
    &lt;p&gt;Therefore, if the license is regarded as a valid contract, even if ‚Äútraining is legal‚Äù under Article 30-4 of the Copyright Act, the risk remains that it becomes a ‚Äúviolation of license conditions‚Äù under contract law, and it can be said that at least there is no official view organizing the theory of GPL propagation to models. In other words, currently, while the legality of model training acts is recognized quite broadly under the Copyright Act, license violation is left to general civil theory, and there is no clear guideline on, for example, ‚Äúwhether the act of publicly distributing a model trained on GPL code constitutes a GPL license violation.‚Äù Overall, the legal organization in Japan is in a situation of ‚Äúsafe in principle at the copyright layer, but blank at the contract layer.‚Äù Hence, the discussion in Japan regarding the theory of GPL propagation to models relies on future judicial judgments and legislative trends, and at present, there is no choice but to consider operational guidelines carefully following the organization by the Agency for Cultural Affairs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Arguments Negating the Theory of License Propagation to Models&lt;/head&gt;
    &lt;p&gt;As seen in the previous sections, the theory of GPL propagation to models is not legally zero. However, many legal experts and engineers point out that this theory has serious detrimental effects. Here, I present representative arguments negating the theory of license propagation to models from the layers of copyright law, GPL text, technology, and practical policy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Arguments for Negation at the Copyright Law Layer&lt;/head&gt;
    &lt;p&gt;First, under copyright law, it is unreasonable to regard an AI model as a ‚Äúderivative work‚Äù or ‚Äúreproduction‚Äù of the training source works. In many cases, the expressions of specific works are not stored inside the model in a form recognizable to humans. The model merely holds statistical abstractions where text and code have been converted into weight parameters, and that itself is not a creative expression to humans at all. A ‚Äúderivative work‚Äù under copyright law refers to a creation that incorporates the essential features of the expression of the original work in a form that can be directly perceived, but one cannot directly perceive the creativity of the original code from the model‚Äôs weights. In other words, the model does not show the nature of a work directly enough to be evaluated as encompassing the original code. For example, the High Court of Justice in the UK stated in the judgment of the Getty v. Stability AI case that ‚Äúthe Stable Diffusion model itself is not an infringing copy of the training images,‚Äù showing a negative view on regarding the model itself as a reproduction of works. Thus, there are many cautious positions internationally regarding regarding the model itself as an accumulation of works or a compilation work.&lt;/p&gt;
    &lt;p&gt;Also, the output generated by the model involves probabilistic and statistical transformations, and in many cases, things that do not resemble the training source at all are output. Even if a match or similarity occurs by chance, it is difficult to prove whether it is a reproduction relying on the original or an accidental similarity. It is not realistic to conduct the certification of reliance and similarity required to discuss copyright infringement for the entire model. Ultimately, in the framework of copyright law, there is no choice but to judge ‚Äúwhether the model relies on a specific work‚Äù on a work-by-work basis, and recognizing uniform copyrightability or infringing nature for the model itself is a large leap. As organized in Japanese law where the model is not considered a reproduction in most cases, the schematic of model equals work is considered unreasonable under copyright law.&lt;/p&gt;
    &lt;head rend="h3"&gt;Arguments for Negation at the GPL Text Layer&lt;/head&gt;
    &lt;p&gt;Next, looking at the license text and intent of the GPL itself, doubts are cast on the interpretation that GPL propagates to AI models. For example, in the text of GPLv2, the target of copyleft is limited to ‚Äúderivative works‚Äù of the original code provided under GPL and ‚Äúworks that contain the Program.‚Äù Typically, this has been interpreted as software created by modifying or incorporating GPL code, or software combined (linked) with GPL code. In the case of an AI model, it is extremely unclear which part of the original GPL code the model ‚Äúcontains.‚Äù Even if the model could memorize fragments of the GPL code used for training, it is a tiny fraction when viewed from the entire model, and most parts are occupied by parameters unrelated to the GPL code. There is no clear assumption shown by the GPL drafters as to whether a statistical model that may partially encapsulate information derived from GPL code can be said to be ‚Äúa work containing the Program‚Äù.&lt;/p&gt;
    &lt;p&gt;Furthermore, GPLv3 requires the provision of software source code in a ‚Äúpreferred form for modification.‚Äù If an AI model is a GPL derivative, the problem arises as to what that preferred form for modification would be. The model weights themselves have low readability and editability for humans, and are hard to call a ‚Äúpreferred form for modification.‚Äù If we ask whether the training data is the source code, the original trained GPL code itself cannot be said to be the source of the model, nor is it clear if it refers to the entire vast and heterogeneous training dataset. It is difficult to define what should be disclosed to redistribute the model under GPL compliance, and it could lead to an extreme conclusion that all code and data used for model training must be disclosed. While this is what some freedom believers aim for, it can only be said to be unrealistic in reality, and it deviates from the point of the GPL‚Äôs intent to enable users to modify and build from source. Thus, existing GPL provisions are not designed to directly cover products like AI models, and forcing their application causes discrepancies in both text and operation.&lt;/p&gt;
    &lt;p&gt;In fact, in the ‚ÄúOpen Source AI Definition‚Äù compiled by the OSI (Open Source Initiative) in 2023, regarding ‚Äúinformation necessary for modification‚Äù of the model, it stopped at stating that sufficiently detailed information about the training data should be disclosed, and did not require the provision of the training data itself in its entirety. Also, it states that model weights and training code should be published under OSI-approved licenses.&lt;/p&gt;
    &lt;p&gt;In addition, the FSF (Free Software Foundation) itself does not believe that the current GPL interpretation alone can guarantee freedom in the AI domain, and announced in 2024 that it has started formulating ‚Äúconditions for machine learning applications to be free.‚Äù There, the directionality is shown that ‚Äúthe four freedoms should be guaranteed to users including not only software but also raw training data and model parameters,‚Äù but this conversely is a recognition that this is not guaranteed under current licenses. The FSF also points out that ‚Äúsince model parameters cannot be said to be source comprehensible to humans, modification through retraining is more realistic than direct editing,‚Äù and can be said to be cautious about treating models on the extension of existing GPL. Overall, claiming GPL propagation univocally to AI models that fall outside the wording and assumptions of GPL provisions is unreasonable from the perspective of interpretation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Arguments for Negation at the Technical Layer&lt;/head&gt;
    &lt;p&gt;There are also strong counterarguments from a technical perspective against the theory of GPL propagation to models. AI models, particularly those called large language models, basically hold huge statistical trends internally and do not store the original code or text as they are like a database. Returning a specific output for a specific input is merely generation according to a probability distribution, and it is not guaranteed that the same output as the training data is always obtained. If the model does not perform verbatim reproduction of training data except for a very small number of exceptional cases, evaluating it as ‚Äúcontaining GPL code‚Äù within the model does not fit the technical reality. In fact, the OpenAI side argued in the GEMA lawsuit that ‚Äúthe model does not memorize individual training data, but merely reflects knowledge learned from the entire dataset in parameters.‚Äù This argument was not accepted by the Munich District Court, but that was because there was a clear example of lyric reproduction; conversely, unless there is a clear example of reproduction, the view would be that ‚Äúthe model is a lump of statistical knowledge‚Äù.&lt;/p&gt;
    &lt;p&gt;Furthermore, although it has been confirmed that models can output fragments of training data, that proportion is considered extremely limited when viewed from the whole. Regarding the whole as a reproduction based on the existence of partial memory is like claiming the whole is a reproduction of a photograph just because it contains a tiny mosaic-like fragment in an image, which is an excessive generalization. Technically, it is difficult to quantitatively measure how far specific parameters of the model retain the influence of the original data, and the correspondence between the model and training data remains statistical and difficult to draw a line. Therefore, criteria such as ‚Äúhow similar must it be for GPL to propagate?‚Äù cannot be established in the first place. The judgment of infringement or not has to be done on an individual output basis, and this would not be consistent with the idea of applying a single license to the entire model. From the technical aspect, since the model is basically a statistical transformation and the majority is unrelated to GPL code, applying GPL collectively can be said to be irrational.&lt;/p&gt;
    &lt;head rend="h3"&gt;Practical and Policy Arguments for Negation&lt;/head&gt;
    &lt;p&gt;Finally, major demerits can be pointed out regarding the theory of license propagation to models from practical and policy perspectives. What would happen if this GPL propagation theory were legally recognized? As an extreme example, if 1 million code repositories were used for training a certain large-scale model, all the various licenses contained in them (GPL, MIT, Apache, proprietary, etc.) would ‚Äúpropagate‚Äù to the model, and the model provider would have to distribute the model in a form that complies with all 1 million license clauses. As a practical matter, there would be combinations where conditions contradict, such as GPLv2 and Apache-2.0, and attaching and managing a huge collection of copyright notices for one model is nothing but unrealistic. Applying all licenses to an AI model created from training data with mixed licenses is practically bankrupt, and eventually, the only thing that can be done to avoid it would be to exclude code with copyleft licenses like GPL from the training data from the start.&lt;/p&gt;
    &lt;p&gt;Is such a situation really desirable for our community? The spirit of the GPL is to promote the free sharing and development of software. However, if asserting excessive propagation to AI models causes companies to avoid using GPL code, and as a result, the value held by GPL software is not utilized in the AI era, it would be putting the cart before the horse. In the field of software development, many companies take a policy of not mixing GPL code into their own products, but similarly, if it becomes ‚Äúdo not include GPL in our AI training data,‚Äù GPL projects could lose value as data sources. Furthermore, the current legal battles surrounding AI are leaning more towards monetary compensation and regulatory rule-making, and the reality is that they are proceeding in a different vector from the direction of code sharing idealized by GPL. If only the theory of GPL propagation to models walks alone, in reality, only data exclusion and closing off to avoid litigation risks will progress, and there is a fear that it will not lead to the expansion of free software culture.&lt;/p&gt;
    &lt;p&gt;Policy-wise as well, governments of each country are carefully considering the use of copyrighted works in AI, but at present, there is no example establishing an explicit rule that ‚Äúlicense violation of training data generates legal liability for the model.‚Äù Even in the EU AI Act, while there are provisions regarding the quality and transparency of training data, it does not demand compliance with open source licenses. Rather, from the perspective of promoting open science and innovation, the movement to allow text and data mining under rights limitations is strong. In Japan as well, as mentioned earlier, the direction is to broadly recognize information analysis use under Article 30-4, and the policy of forcibly applying licenses to AI models is not mainstream in current international discussions.&lt;/p&gt;
    &lt;p&gt;Based on the above, the theory of license propagation to models is highly likely to cause disadvantages to open source on both practical and policy fronts, and can be said not to be a realistic solution. What is important is how to realize the ‚Äúfreedom of software,‚Äù which is the philosophy of open source, in the AI era; the opinion that this should be attempted through realistic means such as ensuring transparency and promoting open model development rather than extreme legal interpretations is potent, and this is something I have consistently argued as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Stance of OSI and FSF&lt;/head&gt;
    &lt;p&gt;I will also organize what stance major organizations in the open source (and free software) community are currently taking in relation to the theory of GPL propagation to AI models. Representative organizations are the Open Source Initiative (OSI) and the Free Software Foundation (FSF); while they share the goal of software freedom, they do not necessarily take the same approach regarding AI models and training data.&lt;/p&gt;
    &lt;p&gt;First, the OSI formulated the ‚ÄúOpen Source AI Definition‚Äù (OSAID) in 2024, defining the requirements for an AI system to be called open source. This definition states that the four freedoms (use, study, modify, redistribute) similar to software should be guaranteed for AI systems as well, and defines requirements regarding ‚Äúforms necessary for modification‚Äù to realize that, requiring the disclosure of the following three elements.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data Information: Provide sufficiently detailed information about the data used for training so that a skilled person can reconstruct an equivalent model. &lt;list rend="ul"&gt;&lt;item&gt;This does not make publishing the training data itself in its entirety mandatory, but requires disclosing the origin, scope, nature, and acquisition method if there is data that cannot be published, listing data that can be published, and providing information on data available from third parties.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Code: Publish the complete set of source code for training and running the model under an OSI-approved license.&lt;/item&gt;
      &lt;item&gt;Parameters: Publish the model weights (parameters) under OSI-approved conditions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It should be noted that while OSI states that information regarding the code used for training and training data is indispensable in addition to model weights to realize ‚ÄúOpen Source AI,‚Äù it does not require the complete disclosure of the training data itself. This is a flexible stance that, for example, if raw data cannot be published due to privacy or confidentiality, explaining the nature of the data by clarifying that fact can substitute. Also, the legal mechanism to ensure free use of model parameters is an issue to be clarified in the future, and at present, no conclusion has been reached on legal rights control (e.g., presence or absence of copyrightability) over parameters either.&lt;/p&gt;
    &lt;p&gt;As can be read from these, the OSI promotes opening up AI models at the level of the open source definition in principle, but keeps the handling of training data to requirements at the information disclosure level. Thereby, it can be said that the OSI avoids adopting the theory of license propagation to models to demand training data disclosure, and is exploring a realistic solution that first guarantees transparency and reproducibility. In principle, it could be said that the OSI denied the GPL propagation theory at the time of publishing the OSAID definition. Note that I am probably the one who sealed the mandatory argument for training data in the final stage of this definition‚Äôs formulation process, and I believe this was the correct judgment.&lt;/p&gt;
    &lt;p&gt;On the other hand, the FSF and FSF Europe (FSFE) take a stance more faithful to fundamental principles. FSFE declared as of 2021 that ‚Äúfor an AI application to be free, both its training code and training data must be published under a free software license.‚Äù That is, to modify or verify the model, one must be able to obtain it including the training data, and therefore both must be free. Also, the FSF itself stated in a 2024 statement, ‚ÄúUnder current understanding, for an ML application to be called free, all training data and the scripts processing it must satisfy the four freedoms,‚Äù trying to extend the requirements of freedom to data. Thus, FSF/FSFE stands on the position that a model with undisclosed training data is unfree as a whole even if the software part is free.&lt;/p&gt;
    &lt;p&gt;However, the FSF simultaneously states to the effect that ‚Äúwhether a non-free machine learning application is ethically unjust depends on the case,‚Äù mentioning that there can be ‚Äúlegitimate moral reasons‚Äù for not being able to publish training data (personal information) of a medical diagnosis AI, for example. In that case, it implies that although that AI is non-free, its use might be ethically permitted due to social utility. One can see an attitude of seeking a compromise between the FSF‚Äôs ideal and reality here, but in any case, there is no mistake that the FSF ultimately aims for freedom including training data.&lt;/p&gt;
    &lt;p&gt;So, does the FSF support the theory of GPL propagation to AI models? Not necessarily. Their claim is closer to an ethical standard or ideal image rather than legal enforceability, and they are not arguing that it applies to models as an interpretation of the current GPL license. Rather, as mentioned before, they are at the stage of trying to create new standards and agreements. Even in the white paper on the Copilot issue funded by the FSF, while legal points such as copyright and license violation are discussed, substantially it has a strong aspect of being told as a GPL compliance problem for users (downstream developers) concerned that they bear the risk of GPL violation if Copilot‚Äôs output contains GPL code fragments. This is a caution to developers using AI coding tools rather than GPL application to the model itself, and is different from an approach forcing GPL compliance directly on model providers.&lt;/p&gt;
    &lt;p&gt;The Software Freedom Conservancy (SFC) naturally has a strong interest in this issue but is also cautious in some respects. The SFC started the protest campaign ‚ÄúGive Up GitHub‚Äù against GitHub in 2022, condemning Copilot‚Äôs methods as contrary to the philosophy of open source, and is also involved in the Copilot class action. However, in an SFC blog post, regarding this lawsuit, it showed concern about ‚Äúthe risk of interpretations deviating from the principles of the open source community being brought in,‚Äù and called on the plaintiffs‚Äô side to comply with community-led GPL enforcement principles as well. The SFC also states that Copilot‚Äôs act is an ‚Äúunprecedented license violation,‚Äù and while not fully denying the GPL propagation theory, it can be interpreted as fearing that a judicial precedent undesirable for the community might be created depending on the result of the legal battle. The SFC might be said to be carefully balancing between the aspect of pursuing GPL propagation and the risk of entrusting it to the judiciary.&lt;/p&gt;
    &lt;p&gt;Finally, what is concerned as the free software camp is that excessive propagation of licenses might conversely invite results that impair freedom. Both OSI and FSF ultimately want to make AI something open that anyone can utilize, but they are carefully assessing whether increasing the purity of legal theory in demands for full data disclosure really leads to achieving the objective. Considering the demerits such as the avoidance of open data due to excessive propagation interpretation or the atrophy effect due to a flurry of lawsuits, I feel that the major organizations share a commonality in that it is essential not to lose sight of the big picture of spreading freedom. Rather than inciting GPL application to models, the pursuit of realistic solutions such as how to make models and data open and which parts should be relaxed in line with reality will likely continue in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;I have looked at the current state of the theory of GPL propagation to AI models above, and as a conclusion, this theory is in a halfway position where ‚Äúit is not touted as loudly as before, but it has not completely disappeared.‚Äù As a result of points such as license violation of training data and reproduction within the model beginning to be scrutinized in lawsuits like the Copilot class action and GEMA v. OpenAI, it even appears that the hurdle for infringement certification is lowering. In fact, the Munich District Court‚Äôs judgment deemed model memory as reproduction, and the claim of open source license violation survives in the Copilot litigation.&lt;/p&gt;
    &lt;p&gt;However, on the other hand, the hurdle for the propagation of licenses like GPL remains high. There is a large gap between infringement being recognized and the conclusion that the entire model must be disclosed under GPL etc. immediately. What the current lawsuits are seeking is also injunctions and damages, not the forced GPL-ization of the model. There are zero examples where the judiciary supported the theory of GPL propagation to models itself, and it is a legally uncharted territory. Even if that claim were attempted somewhere in the future, it would face the legal, technical, and practical counterarguments mentioned earlier.&lt;/p&gt;
    &lt;p&gt;However, the situation has fluid parts, and there is a possibility that the line will shift depending on the policies of each country and the trends of the community. For example, if pressure from rights holder groups strengthens in Europe, there is a possibility that guidelines including license compliance will be formulated. Also, if a consensus is formed within the community regarding the state of copyleft in the AI era, a new license might appear. If such changes occur, a phase where the theory of propagation to models is re-evaluated will also arrive.&lt;/p&gt;
    &lt;p&gt;To offer my personal opinion, what is important at this moment is the perspective of how to balance software freedom and freedom in the AI domain. Instead of blindly trying to apply the philosophy of copyleft to AI, it is necessary to think about what is best to maximize freedom while considering the technical nature and industrial structure peculiar to AI. Fortunately, solutions to practical problems such as the open publication of large-scale AI models, dataset cleaning methods, and automated attachment of license notices are already being explored by the open source community. Promoting such voluntary efforts and supporting them with legal frameworks as necessary will likely be the key to balancing freedom and development.&lt;/p&gt;
    &lt;p&gt;The theory of GPL propagation to models is a point where judgment is divided on whether it is an ideal to be pursued or a nightmare to be avoided. However, as stated in this article, seeing the situation in the current year of 2025, it is not a situation where it will become reality immediately, and the majority of the community is likely maintaining a cautious stance. Although it is speculated that trial and error will continue in the judicial, legislative, and technical aspects in the future, as our community, we need to continue exploring the point of compatibility between technological innovation and software freedom without jumping to hasty conclusions. That process itself can be said to be a new challenge in the AI era on the extension of the free software spirit.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub Copilot litigation: https://githubcopilotlitigation.com/&lt;/item&gt;
      &lt;item&gt;GEMA v. OpenAI Judgment text: https://aifray.com/wp-content/uploads/2025/11/42-O-14139-24-Endurteil.pdf&lt;/item&gt;
      &lt;item&gt;GEMA vs. OpenAI | AI memorisation is a reproduction relevant to copyright law, and the TDM exception does not help in LLM training, Munich I Regional Court holds: https://www.osborneclarke.com/insights/gema-vs-openai-ai-memorisation-reproduction-relevant-copyright-law-and-tdm-exception-does&lt;/item&gt;
      &lt;item&gt;Impressions on GEMA v. OpenAI (Munich I Regional Court) Judgment: https://shujisado.com/2025/11/15/gema-v-openai/&lt;/item&gt;
      &lt;item&gt;Draft thought on AI and Copyright, Agency for Cultural Affairs: https://www.bunka.go.jp/seisaku/bunkashingikai/chosakuken/pdf/94037901_01.pdf&lt;/item&gt;
      &lt;item&gt;Contract Guidelines on Utilization of AI and Data: https://www.meti.go.jp/policy/mono_info_service/connected_industries/sharing_and_utilization/20180615001-1.pdf&lt;/item&gt;
      &lt;item&gt;Open Source AI: https://opensource.org/ai&lt;/item&gt;
      &lt;item&gt;Is publication of complete training data necessary for AI models to be Open Source?: https://shujisado.com/2025/02/18/need_for_training_data_in_opensource_ai/&lt;/item&gt;
      &lt;item&gt;Controlling technology at the age of Artificial Intelligence: a Free Software perspective: https://fsfe.org/freesoftware/artificial-intelligence.en.html&lt;/item&gt;
      &lt;item&gt;FSF is working on freedom in machine learning applications: https://www.fsf.org/news/fsf-is-working-on-freedom-in-machine-learning-applications&lt;/item&gt;
      &lt;item&gt;Give Up GitHub!: https://sfconservancy.org/GiveUpGitHub/&lt;/item&gt;
      &lt;item&gt;On the filing of the Class Action Law Suit over GitHub‚Äôs Copilot: https://sfconservancy.org/news/2022/nov/04/class-action-lawsuit-filing-copilot/&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/"/><published>2025-11-27T12:48:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46069048</id><title>TPUs vs. GPUs and why Google is positioned to win AI race in the long term</title><updated>2025-11-28T00:47:59.381683+00:00</updated><content>&lt;doc fingerprint="3d3a95c811b6b1f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The chip made for the AI inference era ‚Äì the Google TPU&lt;/head&gt;
    &lt;p&gt;Hey everyone,&lt;/p&gt;
    &lt;p&gt;As I find the topic of Google TPUs extremely important, I am publishing a comprehensive deep dive, not just a technical overview, but also strategic and financial coverage of the Google TPU.&lt;/p&gt;
    &lt;p&gt;Topics covered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The history of the TPU and why it all even started?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The difference between a TPU and a GPU?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Performance numbers TPU vs GPU?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Where are the problems for the wider adoption of TPUs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google‚Äôs TPU is the biggest competitive advantage of its cloud business for the next 10 years&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How many TPUs does Google produce today, and how big can that get?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gemini 3 and the aftermath of Gemini 3 on the whole chip industry&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let‚Äôs dive into it.&lt;/p&gt;
    &lt;p&gt;The history of the TPU and why it all even started?&lt;/p&gt;
    &lt;p&gt;The story of the Google Tensor Processing Unit (TPU) begins not with a breakthrough in chip manufacturing, but with a realization about math and logistics. Around 2013, Google‚Äôs leadership‚Äîspecifically Jeff Dean, Jonathan Ross (the CEO of Groq), and the Google Brain team‚Äîran a projection that alarmed them. They calculated that if every Android user utilized Google‚Äôs new voice search feature for just three minutes a day, the company would need to double its global data center capacity just to handle the compute load.&lt;/p&gt;
    &lt;p&gt;At the time, Google was relying on standard CPUs and GPUs for these tasks. While powerful, these general-purpose chips were inefficient for the specific heavy lifting required by Deep Learning: massive matrix multiplications. Scaling up with existing hardware would have been a financial and logistical nightmare.&lt;/p&gt;
    &lt;p&gt;This sparked a new project. Google decided to do something rare for a software company: build its own custom silicon. The goal was to create an ASIC (Application-Specific Integrated Circuit) designed for one job only: running TensorFlow neural networks.&lt;/p&gt;
    &lt;p&gt;Key Historical Milestones:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;2013-2014: The project moved really fast as Google both hired a very capable team and, to be honest, had some luck in their first steps. The team went from design concept to deploying silicon in data centers in just 15 months‚Äîa very short cycle for hardware engineering.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2015: Before the world knew they existed, TPUs were already powering Google‚Äôs most popular products. They were silently accelerating Google Maps navigation, Google Photos, and Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2016: Google officially unveiled the TPU at Google I/O 2016.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This urgency to solve the ‚Äúdata center doubling‚Äù problem is why the TPU exists. It wasn‚Äôt built to sell to gamers or render video; it was built to save Google from its own AI success. With that in mind, Google has been thinking about the ¬ªcostly¬´ AI inference problems for over a decade now. This is also one of the main reasons why the TPU is so good today compared to other ASIC projects.&lt;/p&gt;
    &lt;p&gt;The difference between a TPU and a GPU?&lt;/p&gt;
    &lt;p&gt;To understand the difference, it helps to look at what each chip was originally built to do. A GPU is a ‚Äúgeneral-purpose‚Äù parallel processor, while a TPU is a ‚Äúdomain-specific‚Äù architecture.&lt;/p&gt;
    &lt;p&gt;The GPUs were designed for graphics. They excel at parallel processing (doing many things at once), which is great for AI. However, because they are designed to handle everything from video game textures to scientific simulations, they carry ‚Äúarchitectural baggage.‚Äù They spend significant energy and chip area on complex tasks like caching, branch prediction, and managing independent threads.&lt;/p&gt;
    &lt;p&gt;A TPU, on the other hand, strips away all that baggage. It has no hardware for rasterization or texture mapping. Instead, it uses a unique architecture called a Systolic Array.&lt;/p&gt;
    &lt;p&gt;The ‚ÄúSystolic Array‚Äù is the key differentiator. In a standard CPU or GPU, the chip moves data back and forth between the memory and the computing units for every calculation. This constant shuffling creates a bottleneck (the Von Neumann bottleneck).&lt;/p&gt;
    &lt;p&gt;In a TPU‚Äôs systolic array, data flows through the chip like blood through a heart (hence ‚Äúsystolic‚Äù).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It loads data (weights) once.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It passes inputs through a massive grid of multipliers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The data is passed directly to the next unit in the array without writing back to memory.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What this means, in essence, is that a TPU, because of its systolic array, drastically reduces the number of memory reads and writes required from HBM. As a result, the TPU can spend its cycles computing rather than waiting for data.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs new TPU design, also called Ironwood also addressed some of the key areas where a TPU was lacking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;They enhanced the SparseCore for efficiently handling large embeddings (good for recommendation systems and LLMs)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It increased HBM capacity and bandwidth (up to 192 GB per chip). For a better understanding, Nvidia‚Äôs Blackwell B200 has 192GB per chip, while Blackwell Ultra, also known as the B300, has 288 GB per chip.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved the Inter-Chip Interconnect (ICI) for linking thousands of chips into massive clusters, also called TPU Pods (needed for AI training as well as some time test compute inference workloads). When it comes to ICI, it is important to note that it is very performant with a Peak Bandwidth of 1.2 TB/s vs Blackwell NVLink 5 at 1.8 TB/s. But Google‚Äôs ICI, together with its specialized compiler and software stack, still delivers superior performance on some specific AI tasks.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key thing to understand is that because the TPU doesn‚Äôt need to decode complex instructions or constantly access memory, it can deliver significantly higher Operations Per Joule.&lt;/p&gt;
    &lt;p&gt;For scale-out, Google uses Optical Circuit Switch (OCS) and its 3D torus network, which compete with Nvidia‚Äôs InfiniBand and Spectrum-X Ethernet. The main difference is that OCS is extremely cost-effective and power-efficient as it eliminates electrical switches and O-E-O conversions, but because of this, it is not as flexible as the other two. So again, the Google stack is extremely specialized for the task at hand and doesn‚Äôt offer the flexibility that GPUs do.&lt;/p&gt;
    &lt;p&gt;Performance numbers TPU vs GPU?&lt;/p&gt;
    &lt;p&gt;As we defined the differences, let‚Äôs look at real numbers showing how the TPU performs compared to the GPU. Since Google isn‚Äôt revealing these numbers, it is really hard to get details on performance. I studied many articles and alternative data sources, including interviews with industry insiders, and here are some of the key takeaways.&lt;/p&gt;
    &lt;p&gt;The first important thing is that there is very limited information on Google‚Äôs newest TPUv7 (Ironwood), as Google introduced it in April 2025 and is just now starting to become available to external clients (internally, it is said that Google has already been using Ironwood since April, possibly even for Gemini 3.0.). And why is this important if we, for example, compare TPUv7 with an older but still widely used version of TPUv5p based on Semianalysis data:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 produces 4,614 TFLOPS(BF16) vs 459 TFLOPS for TPUv5p&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 has 192GB of memory capacity vs TPUv5p 96GB&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 memory Bandwidth is 7,370 GB/s vs 2,765 for v5p&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can see that the performance leaps between v5 and v7 are very significant. To put that in context, most of the comments that we will look at are more focused on TPUv6 or TPUv5 than v7.&lt;/p&gt;
    &lt;p&gt;Based on analyzing a ton of interviews with Former Google employees, customers, and competitors (people from AMD, NVDA &amp;amp; others), the summary of the results is as follows.&lt;/p&gt;
    &lt;p&gt;Most agree that TPUs are more cost-effective compared to Nvidia GPUs, and most agree that the performance per watt for TPUs is better. This view is not applicable across all use cases tho.&lt;/p&gt;
    &lt;p&gt;A Former Google Cloud employee:&lt;/p&gt;
    &lt;p&gt;¬ªIf it is the right application, then they can deliver much better performance per dollar compared to GPUs. They also require much lesser energy and produces less heat compared to GPUs. They‚Äôre also more energy efficient and have a smaller environmental footprint, which is what makes them a desired outcome.&lt;/p&gt;
    &lt;p&gt;The use cases are slightly limited to a GPU, they‚Äôre not as generic, but for a specific application, they can offer as much as 1.4X better performance per dollar, which is pretty significant saving for a customer that might be trying to use GPU versus TPUs.¬´&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;Similarly, a very insightful comment from a Former Unit Head at Google around TPUs materially lowering AI-search cost per query vs GPUs:&lt;/p&gt;
    &lt;p&gt;¬ªTPU v6 is 60-65% more efficient than GPUs, prior generations 40-45%¬´&lt;/p&gt;
    &lt;p&gt;This interview was in November 2024, so the expert is probably comparing the v6 TPU with the Nvidia Hopper. Today, we already have Blackwell vs V7.&lt;/p&gt;
    &lt;p&gt;Many experts also mention the speed benefit that TPUs offer, with a Former Google Head saying that TPUs are 5x faster than GPUs for training dynamic models (like search-like workloads).&lt;/p&gt;
    &lt;p&gt;There was also a very eye-opening interview with a client who used both Nvidia GPUs and Google TPUs as he describes the economics in great detail:&lt;/p&gt;
    &lt;p&gt;¬ªIf I were to use eight H100s versus using one v5e pod, I would spend a lot less money on one v5e pod. In terms of price point money, performance per dollar, you will get more bang for TPU. If I already have a code, because of Google‚Äôs help or because of our own work, if I know it already is going to work on a TPU, then at that point it is beneficial for me to just stick with the TPU usage.&lt;/p&gt;
    &lt;p&gt;In the long run, if I am thinking I need to write a new code base, I need to do a lot more work, then it depends on how long I‚Äôm going to train. I would say there is still some, for example, of the workload we have already done on TPUs that in the future because as Google will add newer generation of TPU, they make older ones much cheaper.&lt;lb/&gt;For example, when they came out with v4, I remember the price of v2 came down so low that it was practically free to use compared to any NVIDIA GPUs.&lt;/p&gt;
    &lt;p&gt;Google has got a good promise so they keep supporting older TPUs and they‚Äôre making it a lot cheaper. If you don‚Äôt really need your model trained right away, if you‚Äôre willing to say, ‚ÄúI can wait one week,‚Äù even though the training is only three days, then you can reduce your cost 1/5.¬´&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;Another valuable interview was with a current AMD employee, acknowledging the benefits of ASICs:&lt;/p&gt;
    &lt;p&gt;¬ªI would expect that an AI accelerator could do about probably typically what we see in the industry. I‚Äôm using my experience at FPGAs. I could see a 30% reduction in size and maybe a 50% reduction in power vs a GPU.¬´&lt;/p&gt;
    &lt;p&gt;We also got some numbers from a Former Google employee who worked in the chip segment:&lt;/p&gt;
    &lt;p&gt;¬ªWhen I look at the published numbers, they (TPUs) are anywhere from 25%-30% better to close to 2x better, depending on the use cases compared to Nvidia. Essentially, there‚Äôs a difference between a very custom design built to do one task perfectly versus a more general purpose design.¬´&lt;/p&gt;
    &lt;p&gt;What is also known is that the real edge of TPUs lies not in the hardware but in the software and in the way Google has optimized its ecosystem for the TPU.&lt;/p&gt;
    &lt;p&gt;A lot of people mention the problem that every Nvidia ¬ªcompetitor¬´ like the TPU faces, which is the fast development of Nvidia and the constant ¬ªcatching up¬´ to Nvidia problem. This month a former Google Cloud employee addressed that concern head-on as he believes the rate at which TPUs are improving is faster than the rate at Nvidia:&lt;/p&gt;
    &lt;p&gt;¬ªThe amount of performance per dollar that a TPU can generate from a new generation versus the old generation is a much significant jump than Nvidia¬´&lt;/p&gt;
    &lt;p&gt;In addition, the recent data from Google‚Äôs presentation at the Hot Chips 2025 event backs that up, as Google stated that the TPUv7 is 100% better in performance per watt than their TPUv6e (Trillium).&lt;/p&gt;
    &lt;p&gt;Even for hard Nvidia advocates, TPUs are not to be shrugged off easily, as even Jensen thinks very highly of Google‚Äôs TPUs. In a podcast with Brad Gerstner, he mentioned that when it comes to ASICs, Google with TPUs is a ¬ªspecial case¬´. A few months ago, we also got an article from the WSJ saying that after the news publication The Information published a report that stated that OpenAI had begun renting Google TPUs for ChatGPT, Jensen called Altman, asking him if it was true, and signaled that he was open to getting the talks back on track (investment talks). Also worth noting was that Nvidia‚Äôs official X account posted a screenshot of an article in which OpenAI denied plans to use Google‚Äôs in-house chips. To say the least, Nvidia is watching TPUs very closely.&lt;/p&gt;
    &lt;p&gt;Ok, but after looking at some of these numbers, one might think, why aren‚Äôt more clients using TPUs?&lt;/p&gt;
    &lt;p&gt;Where are the problems for the wider adoption of TPUs&lt;/p&gt;
    &lt;p&gt;The main problem for TPUs adoption is the ecosystem. Nvidia‚Äôs CUDA is engraved in the minds of most AI engineers, as they have been learning CUDA in universities. Google has developed its ecosystem internally but not externally, as it has used TPUs only for its internal workloads until now. TPUs use a combination of JAX and TensorFlow, while the industry skews to CUDA and PyTorch (although TPUs also support PyTorch now). While Google is working hard to make its ecosystem more supportive and convertible with other stacks, it is also a matter of libraries and ecosystem formation that takes years to develop.&lt;/p&gt;
    &lt;p&gt;It is also important to note that, until recently, the GenAI industry‚Äôs focus has largely been on training workloads. In training workloads, CUDA is very important, but when it comes to inference, even reasoning inference, CUDA is not that important, so the chances of expanding the TPU footprint in inference are much higher than those in training (although TPUs do really well in training as well ‚Äì Gemini 3 the prime example).&lt;/p&gt;
    &lt;p&gt;The fact that most clients are multi-cloud also poses a challenge for TPU adoption, as AI workloads are closely tied to data and its location (cloud data transfer is costly). Nvidia is accessible via all three hyperscalers, while TPUs are available only at GCP so far. A client who uses TPUs and Nvidia GPUs explains it well:&lt;/p&gt;
    &lt;p&gt;¬ªRight now, the one biggest advantage of NVIDIA, and this has been true for past three companies I worked on is because AWS, Google Cloud and Microsoft Azure, these are the three major cloud companies.&lt;/p&gt;
    &lt;p&gt;Every company, every corporate, every customer we have will have data in one of these three. All these three clouds have NVIDIA GPUs. Sometimes the data is so big and in a different cloud that it is a lot cheaper to run our workload in whatever cloud the customer has data in.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know if you know about the egress cost that is moving data out of one cloud is one of the bigger cost. In that case, if you have NVIDIA workload, if you have a CUDA workload, we can just go to Microsoft Azure, get a VM that has NVIDIA GPU, same GPU in fact, no code change is required and just run it there.&lt;/p&gt;
    &lt;p&gt;With TPUs, once you are all relied on TPU and Google says, ‚ÄúYou know what? Now you have to pay 10X more,‚Äù then we would be screwed, because then we‚Äôll have to go back and rewrite everything. That‚Äôs why. That‚Äôs the only reason people are afraid of committing too much on TPUs. The same reason is for Amazon‚Äôs Trainium and Inferentia.¬´&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;These problems are well known at Google, so it is no surprise that internally, the debate over keeping TPUs inside Google or starting to sell them externally is a constant topic. When keeping them internally, it enhances the GCP moat, but at the same time, many former Google employees believe that at some point, Google will start offering TPUs externally as well, maybe through some neoclouds, not necessarily with the biggest two competitors, Microsoft and Amazon. Opening up the ecosystem, providing support, etc., and making it more widely usable are the first steps toward making that possible.&lt;/p&gt;
    &lt;p&gt;A former Google employee also mentioned that Google last year formed a more sales-oriented team to push and sell TPUs, so it‚Äôs not like they have been pushing hard to sell TPUs for years; it is a fairly new dynamic in the organization.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs TPU is the biggest competitive advantage of its cloud business for the next 10 years&lt;/p&gt;
    &lt;p&gt;The most valuable thing for me about TPUs is their impact on GCP. As we witness the transformation of cloud businesses from the pre-AI era to the AI era, the biggest takeaway is that the industry has gone from an oligopoly of AWS, Azure, and GCP to a more commoditized landscape, with Oracle, Coreweave, and many other neoclouds competing for AI workloads. The problem with AI workloads is the competition and Nvidia‚Äôs 75% gross margin, which also results in low margins for AI workloads. The cloud industry is moving from a 50-70% gross margin industry to a 20-35% gross margin industry. For cloud investors, this should be concerning, as the future profile of some of these companies is more like that of a utility than an attractive, high-margin business. But there is a solution to avoiding that future and returning to a normal margin: the ASIC.&lt;/p&gt;
    &lt;p&gt;The cloud providers who can control the hardware and are not beholden to Nvidia and its 75% gross margin will be able to return to the world of 50% gross margins. And there is no surprise that all three AWS, Azure, and GCP are developing their own ASICs. The most mature by far is Google‚Äôs TPU, followed by Amazon‚Äôs Trainum, and lastly Microsoft‚Äôs MAIA (although Microsoft owns the full IP of OpenAI‚Äôs custom ASICs, which could help them in the future).&lt;/p&gt;
    &lt;p&gt;While even with ASICs you are not 100% independent, as you still have to work with someone like Broadcom or Marvell, whose margins are lower than Nvidia‚Äôs but still not negligible, Google is again in a very good position. Over the years of developing TPUs, Google has managed to control much of the chip design process in-house. According to a current AMD employee, Broadcom no longer knows everything about the chip. At this point, Google is the front-end designer (the actual RTL of the design) while Broadcom is only the backend physical design partner. Google, on top of that, also, of course, owns the entire software optimization stack for the chip, which makes it as performant as it is. According to the AMD employee, based on this work split, he thinks Broadcom is lucky if it gets a 50-point gross margin on its part.&lt;/p&gt;
    &lt;p&gt;Without having to pay Nvidia for the accelerator, a cloud provider can either price its compute similarly to others and maintain a better margin profile or lower costs and gain market share. Of course, all of this depends on having a very capable ASIC that can compete with Nvidia. Unfortunately, it looks like Google is the only one that has achieved that, as the number one-performing model is Gemini 3 trained on TPUs. According to some former Google employees, internally, Google is also using TPUs for inference across its entire AI stack, including Gemini and models like Veo. Google buys Nvidia GPUs for GCP, as clients want them because they are familiar with them and the ecosystem, but internally, Google is full-on with TPUs.&lt;/p&gt;
    &lt;p&gt;As the complexity of each generation of ASICs increases, similar to the complexity and pace of Nvidia, I predict that not all ASIC programs will make it. I believe outside of TPUs, the only real hyperscaler shot right now is AWS Trainium, but even that faces much bigger uncertainties than the TPU. With that in mind, Google and its cloud business can come out of this AI era as a major beneficiary and market-share gainer.&lt;/p&gt;
    &lt;p&gt;Recently, we even got comments from the SemiAnalysis team praising the TPU:&lt;/p&gt;
    &lt;p&gt;¬ªGoogle‚Äôs silicon supremacy among hyperscalers is unmatched, with their TPU 7th Gen arguably on par with Nvidia Blackwell. TPU powers the Gemini family of models which are improving in capability and sit close to the pareto frontier of $ per intelligence in some tasks¬´&lt;/p&gt;
    &lt;p&gt;source: SemiAnalysis&lt;/p&gt;
    &lt;p&gt;How many TPUs does Google produce today, and how big can that get?&lt;/p&gt;
    &lt;p&gt;Here are the numbers that I researched:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.uncoveralpha.com/p/the-chip-made-for-the-ai-inference"/><published>2025-11-27T13:28:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46069556</id><title>Show HN: Runprompt ‚Äì run .prompt files from the command line</title><updated>2025-11-28T00:47:58.672967+00:00</updated><content>&lt;doc fingerprint="1c7ffd5c6cb14e40"&gt;
  &lt;main&gt;
    &lt;p&gt;A single-file Python script for running .prompt files.&lt;/p&gt;
    &lt;p&gt;Quick start | Examples | Configuration | Providers&lt;/p&gt;
    &lt;code&gt;curl -O https://raw.githubusercontent.com/chr15m/runprompt/main/runprompt
chmod +x runprompt&lt;/code&gt;
    &lt;p&gt;Create &lt;code&gt;hello.prompt&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Run it:&lt;/p&gt;
    &lt;code&gt;export ANTHROPIC_API_KEY="your-key"
echo '{"name": "World"}' | ./runprompt hello.prompt&lt;/code&gt;
    &lt;p&gt;In addition to the following, see the tests folder for more example &lt;code&gt;.prompt&lt;/code&gt; files.&lt;/p&gt;
    &lt;code&gt;cat article.txt | ./runprompt summarize.prompt&lt;/code&gt;
    &lt;p&gt;The special &lt;code&gt;{{STDIN}}&lt;/code&gt; variable always contains the raw stdin as a string.&lt;/p&gt;
    &lt;p&gt;Extract structured data using an output schema:&lt;/p&gt;
    &lt;code&gt;echo "John is a 30 year old teacher" | ./runprompt extract.prompt
# {"name": "John", "age": 30, "occupation": "teacher"}&lt;/code&gt;
    &lt;p&gt;Fields ending with &lt;code&gt;?&lt;/code&gt; are optional. The format is &lt;code&gt;field: type, description&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Pipe structured output between prompts:&lt;/p&gt;
    &lt;code&gt;echo "John is 30" | ./runprompt extract.prompt | ./runprompt generate-bio.prompt&lt;/code&gt;
    &lt;p&gt;The JSON output from the first prompt becomes template variables in the second.&lt;/p&gt;
    &lt;p&gt;Override any frontmatter value from the command line:&lt;/p&gt;
    &lt;code&gt;./runprompt --model anthropic/claude-haiku-4-20250514 hello.prompt
./runprompt --name "Alice" hello.prompt&lt;/code&gt;
    &lt;p&gt;Set API keys for your providers:&lt;/p&gt;
    &lt;code&gt;export ANTHROPIC_API_KEY="..."
export OPENAI_API_KEY="..."
export GOOGLE_API_KEY="..."
export OPENROUTER_API_KEY="..."&lt;/code&gt;
    &lt;p&gt;Override any frontmatter value via environment variables prefixed with &lt;code&gt;RUNPROMPT_&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;export RUNPROMPT_MODEL="anthropic/claude-haiku-4-20250514"
./runprompt hello.prompt&lt;/code&gt;
    &lt;p&gt;This is useful for setting defaults across multiple prompt runs.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;-v&lt;/code&gt; to see request/response details:&lt;/p&gt;
    &lt;code&gt;./runprompt -v hello.prompt&lt;/code&gt;
    &lt;p&gt;Models are specified as &lt;code&gt;provider/model-name&lt;/code&gt;:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Provider&lt;/cell&gt;
        &lt;cell role="head"&gt;Model format&lt;/cell&gt;
        &lt;cell role="head"&gt;API key env var&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Anthropic&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;anthropic/claude-sonnet-4-20250514&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OpenAI&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;openai/gpt-4o&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Google AI&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;googleai/gemini-1.5-pro&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;OpenRouter&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;openrouter/anthropic/claude-sonnet-4-20250514&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;OpenRouter provides access to models from many providers (Anthropic, Google, Meta, etc.) through a single API key.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/chr15m/runprompt"/><published>2025-11-27T14:26:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46070668</id><title>Same-day upstream Linux support for Snapdragon 8 Elite Gen 5</title><updated>2025-11-28T00:47:58.217898+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qualcomm.com/developer/blog/2025/10/same-day-snapdragon-8-elite-gen-5-upstream-linux-support"/><published>2025-11-27T16:19:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46070868</id><title>The VanDersarl Bl√©riot: a 1911 airplane homebuilt by teenage brothers (2017)</title><updated>2025-11-28T00:47:57.987616+00:00</updated><content>&lt;doc fingerprint="3e50c640e82cc578"&gt;
  &lt;main&gt;
    &lt;p&gt;Fabricated by teenage brothers in 1911, this unique homebuilt is once again airworthy.&lt;/p&gt;
    &lt;p&gt;Despite its shortcomings, the Bl√©riot XI was one of the great designs of aviation‚Äôs early years. The successful fruit of numerous prior attempts‚Äîand failures‚Äîby French pioneer aviator Louis Bl√©riot, it was tricky and even dangerous to fly, largely because its horizontal stabilizer had an airfoil like the wing, which could cause the nose to suddenly pitch down during high-speed dives. When Bl√©riot piloted the shoulder-winged monoplane on a historic 23¬Ω-mile hop across the English Channel in July 1909, however, he won his design a worldwide stamp of approval beyond its inherent merits. From then on, aviators out to score more firsts in distance, speed, altitude or endurance, or simply out to experience the thrill of early flight for its own sake, wanted a Bl√©riot XI. Besides the examples Bl√©riot produced, a number of other companies on either side of the Atlantic manufactured it under license, while other budding fliers built their own planes based on its basic layout. It was in that last category that the VanDersarl brothers staked their modest claim to fame.&lt;/p&gt;
    &lt;p&gt;Little is now known about Jules ‚ÄúJ.J.‚Äù VanDersarl and his younger brother, Frank, except that they lived just outside Denver, Colo.; their mother worked as a housekeeper; and they barely made it through grade school. But both brothers proved to have innate mechanical talents that made them proficient at machining, carpentry and other skills. Given that, it‚Äôs not surprising these young men, like a good many others at the time, became enthralled with aviation. J.J. experimented with gliders at age 12, and later, a few months after Bl√©riot‚Äôs 1909 Channel flight, he and Frank got more ambitious. Obtaining all the publications and photographs they could, they used those references to build their own Bl√©riot XI in 1911‚Ä¶then learned to fly it.&lt;/p&gt;
    &lt;p&gt;According to Javier Arango, director of The Aeroplane Collection in Paso Robles, Calif., who now owns the VanDersarl Bl√©riot, the brothers ‚Äúmust have had some guidance and lots of information,‚Äù because the dimensions of their airplane are close to those of the original. Their homebuilt differs from the standard Bl√©riot XI in three respects, however. First and foremost, instead of the 25-hp Anzani 3-cylinder radial or Gnome rotary engine that normally powered Bl√©riots, the VanDersarls, using their general knowledge and machining skills, adapted a 4-cylinder inline air-cooled automobile engine with a reworked oil system to aerial use. Just what that engine was remains uncertain, though Arango said it was ‚Äúclose in dimensions‚Äù to the power plant used in the Metz, a car equipped with a liquid-cooled engine that the company had planned to adapt to aviation but which never quite materialized.&lt;/p&gt;
    &lt;p&gt;A second difference, Arango noted, was that ‚Äúsome of the structure around the empennage is placed slightly differently than in most Bl√©riots.‚Äù Finally, he said, ‚ÄúThe French Bl√©riots were built to a high quality, but our plane was built by teen agers in Colorado who just wanted to go fly‚Äîit‚Äôs a little rougher than pristine Bl√©riots.‚Äù&lt;/p&gt;
    &lt;p&gt;Even so, the handmade airplane worked remarkably well. ‚ÄúThere is a photo of the first flight, which ended up in a landing that broke the landing gear,‚Äù Arango reported. ‚ÄúBut it was repaired and flew again. Both brothers flew it.‚Äù&lt;/p&gt;
    &lt;p&gt;The VanDersarls went on to fly Curtiss JN-4 Jennys and Standards, and in the 1920s, Arango said, ‚ÄúFrank started an airport and barnstorming operation.‚Äù The most remarkable thing, though, is that Frank kept the homebuilt in which he and J.J. had first learned how to fly. ‚ÄúI‚Äôm so glad they kept it,‚Äù Arango remarked. ‚ÄúThis breed of airplane is quite rare. The Smithsonian Institution has only one such aircraft.‚Äù&lt;/p&gt;
    &lt;p&gt;In the 1960s Frank VanDersarl tried to restore the Bl√©riot, but he died before completing the project. After J.J. VanDersarl died in Raton, N.M., in November 1977, the monoplane was exhibited at the Museum of New Mexico. In 1994 it was bought by Joseph Gertler, who loaned it to Dowling College in Bayport, N.Y. There it was further restored by John Zale, Frankie Mineo, Russ Moore and the Bayport Aerodrome Society. Then in 2009 Arango‚Äôs C.C. Air Corporation purchased it and added it to The Aeroplane Collection, with the ultimate goal of making it airworthy for the first time in a century.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhen we got it the plane was minimally restored,‚Äù Arango explained. ‚ÄúIt was extremely authentic.‚Äù That meant it served as a useful reference toward the inevitable replacement of deteriorated material and components. ‚ÄúChuck Wentworth from Antique Aero, who is really the main character in the restoration project, inspected it and went through everything,‚Äù he said. ‚ÄúThe entire fuselage was in good shape. There were busted wires and turnbuckles that had to be reproduced and replaced to get it back to original condition. Chuck had to find parts of 1911 vintage to get the correct look, based on plans and photos. For example, they‚Äôd stuck a fake control wheel in the cockpit for display. We took all of that out.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe wings were difficult‚Äîthey were not the same age as the fuselage. They were probably damaged and were repaired or rebuilt by the VanDersarls. It took a lot of work with the wings to make them airworthy. The cotton covering was difficult to work with, and we even had to find the ‚Äòhoney-colored coating‚Äô the VanDersarls described. We used a varnish that was tinted to get the correct honey color.‚Äù&lt;/p&gt;
    &lt;p&gt;Though he considered obtaining an Anzani engine, Arango decided to heed the advice of the National Air and Space Museum and ‚Äúkeep it as it was‚Äù by reconstructing the original engine. Fortunately for the restoration team, the VanDersarls ‚Äúleft good data on the cylinders, the copper cooling fins‚Äîall the specifications we needed to build the engine from scratch. The engine was put together with help from period publications and photos of the original.‚Äù The most difficult part was getting period components, but they managed to obtain a 1905 Bosch magneto, a brass carburetor of 1909 vintage, a tachometer, a magneto switch and a 1910 automobile oil gauge. In 2011 Wentworth unveiled the Bl√©riot at the National Aviation Heritage Invitational in Reno, Nev. There on September 18 it won the event‚Äôs top award, the RollsRoyce Aviation Heritage Trophy.&lt;/p&gt;
    &lt;p&gt;Once the four-year project was completed, Arango and his team went through a systematic process toward getting it approved for flight by the Federal Aviation Administration. This presented some challenges, Arango said, since the Bl√©riot XI predated the Civil Aeronautics Administration, let alone the FAA, and ‚Äúthere is no certificate and no paperwork of the age to make it current.‚Äù After the FAA inspected the aircraft, however, it finally registered the VanDersarl Bl√©riot as an experimental airplane on August 16, 2012. This meant it could be flown under certain restrictions, such as not carrying a passenger for hire and with limits on the number of flights and travel radius around the airfield. ‚ÄúThat was fine by us,‚Äù said Arango, ‚Äúbecause we were happy to just fly, more or less in a straight line.‚Äù&lt;/p&gt;
    &lt;p&gt;Even with FAA approval, the VanDersarl Bl√©riot underwent testing, reinspection and taxiing trials before it finally got airborne for the first time in more than a century on November 3, 2012. Since then, Arango keeps its flight itinerary at Paso Robles under tight self-imposed restrictions. ‚ÄúIt‚Äôs a marginal airplane,‚Äù he explained, ‚Äúwith a 50-hp engine and very cambered wings that cause a lot of drag. It‚Äôs a good-flying airplane, but I‚Äôm not going to risk the airframe. It‚Äôs one of a kind, touched twice by its creators, and once by Chuck. I wanted it authentic to its own type.‚Äù&lt;/p&gt;
    &lt;p&gt;Originally published in the March 2014 issue of Aviation History. To subscribe, click here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.historynet.com/vandersarl-bleriot/"/><published>2025-11-27T16:38:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46070915</id><title>Pakistan says rooftop solar output to exceed grid demand in some hubs next year</title><updated>2025-11-28T00:47:57.621877+00:00</updated><content>&lt;doc fingerprint="2b11807f0bf4c538"&gt;
  &lt;main&gt;
    &lt;p&gt;BELEM, Brazil, Nov 22 (Reuters) - Pakistan's rooftop solar generation will for the first time exceed power demand on the country's electrical grid during daytime hours in some major industrial regions next year, a senior government official told Reuters.&lt;/p&gt;
    &lt;p&gt;The outlook reflects a record boom in the country's solar panel installations in recent years that has delivered lower emissions and reduced power bills for some, but also disrupted the finances of debt-laden utilities due to a protracted decline in demand for grid-based electricity.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;"Pakistan will experience negative grid-linked demand during certain daytime hours because behind-the-meter solar is offsetting grid consumption completely," Aisha Moriani, secretary of Pakistan's climate change ministry told Reuters on the sidelines of the COP30 climate conference in Brazil.&lt;/p&gt;
    &lt;p&gt;While regions in Europe and Australia sometimes experience negative electricity prices due to solar oversupply and low demand, Pakistan would be among the first major emerging markets where rooftop generation could exceed grid-linked demand in major areas entirely for lengthy periods.&lt;/p&gt;
    &lt;p&gt;"Negative demand" is likely in the northwestern city of Lahore, which has some of the country's highest solar penetration, followed by Faisalabad and Sialkot, where industrial areas are driving solar adoption, she said.&lt;/p&gt;
    &lt;p&gt;Power cuts and tariff hikes have pushed Pakistan's 250 million people to accelerate solar adoption and made it the world's third-largest panel importer, with solar's share in generation exceeding its neighbour China.&lt;/p&gt;
    &lt;p&gt;The south Asian nation will see more frequent negative-demand events, especially during bright summer afternoons, industrial holidays and moderate temperature days with high solar output, said Moriani, Pakistan's lead negotiator at COP30.&lt;/p&gt;
    &lt;p&gt;"Pakistan's challenge is not whether renewable energy will grow, it is how fast the grid, regulation, and market design can evolve to keep pace," she said.&lt;/p&gt;
    &lt;p&gt;The south Asian nation is planning to introduce new tariffs for large solar users, as well as changes to fee structures to ensure businesses with panels share equally in the costs of grid upkeep, she said.&lt;/p&gt;
    &lt;p&gt;Pakistan's grid-linked power demand is expected to grow 3-4% this year, slower than historical averages. Next year, consumption is expected to rise more steeply but could be impacted more by higher solar use, Moriani said.&lt;/p&gt;
    &lt;p&gt;The surge in solar use has also pushed Pakistan to renegotiate its LNG contracts with top supplier Qatar and cancel cargoes supplied by Italy's Eni, Moriani said.&lt;/p&gt;
    &lt;p&gt;Pakistan is looking for lower prices, flexible delivery schedules and potentially fewer cargoes, she said.&lt;/p&gt;
    &lt;p&gt;While there were no formal negotiations with Qatar at COP30, the event provided "diplomatic space for engagement with energy ministers and commercial representatives," she said.&lt;/p&gt;
    &lt;p&gt;"The key aim is to align Pakistan's gas import strategy with fiscal space, demand outlook, and seasonal patterns. Pakistan seeks stability and affordability, not expansion of LNG dependency."&lt;/p&gt;
    &lt;p&gt;Reporting by Sudarshan Varadhan; Editing by David Gregorio&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.reuters.com/sustainability/boards-policy-regulation/pakistan-says-rooftop-solar-output-exceed-grid-demand-some-hubs-next-year-2025-11-22/"/><published>2025-11-27T16:42:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46072786</id><title>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning [pdf]</title><updated>2025-11-28T00:47:57.133884+00:00</updated><content/><link href="https://github.com/deepseek-ai/DeepSeek-Math-V2/blob/main/DeepSeekMath_V2.pdf"/><published>2025-11-27T20:03:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46072988</id><title>LinkedIn is loud, and corporate is hell</title><updated>2025-11-28T00:47:56.979475+00:00</updated><content/><link href="https://ramones.dev/posts/linkedin-is-loud/"/><published>2025-11-27T20:30:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46073033</id><title>Underrated reasons to be thankful V</title><updated>2025-11-28T00:47:56.390558+00:00</updated><content>&lt;doc fingerprint="67ec982528b59eaf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Underrated reasons to be thankful V&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;That your dog, while she appears to love you only because she‚Äôs been adapted by evolution to appear to love you, really does love you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you‚Äôre a life form and you cook up a baby and copy your genes to them, you‚Äôll find that the genes have been degraded due to oxidative stress et al., which isn‚Äôt cause for celebration, but if you find some other hopefully-hot person and randomly swap in half of their genes, your baby will still be somewhat less fit compared to you and your hopefully-hot friend on average, but now there is variance, so if you cook up several babies, one of them might be as fit or even fitter than you, and that one will likely have more babies than your other babies have, and thus complex life can persist in a universe with increasing entropy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if we wanted to, we surely could figure out which of the 300-ish strains of rhinovirus are circulating in a given area at a given time and rapidly vaccinate people to stop it and thereby finally ‚Äúcure‚Äù the common cold, and though this is too annoying to pursue right now, it seems like it‚Äôs just a matter of time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you look back at history, you see that plagues went from Europe to the Americas but not the other way, which suggests that urbanization and travel are great allies for infectious disease, and these both continue today but are held in check by sanitation and vaccines even while we have lots of tricks like UVC light and high-frequency sound and air filtration and waste monitoring and paying people to stay home that we‚Äôve barely even put in play.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That while engineered infectious diseases loom ever-larger as a potential very big problem, we also have lots of crazier tricks we could pull out like panopticon viral screening or toilet monitors or daily individualized saliva sampling or engineered microbe-resistant surfaces or even dividing society into cells with rotating interlocks or having people walk around in little personal spacesuits, and while admittedly most of this doesn‚Äôt sound awesome, I see no reason this shouldn‚Äôt be a battle that we would win.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That clean water, unlimited, almost free.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That dentistry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That tongues.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That radioactive atoms either release a ton of energy but also quickly stop existing‚Äîa gram of Rubidium-90 scattered around your kitchen emits as much energy as ~200,000 incandescent lightbulbs but after an hour only 0.000000113g is left‚Äîor don‚Äôt put out very much energy but keep existing for a long time‚Äîa gram of Carbon-14 only puts out the equivalent of 0.0000212 light bulbs but if you start with a gram, you‚Äôll still have 0.999879g after a year‚Äîso it isn‚Äôt actually that easy to permanently poison the environment with radiation although Cobalt-60 with its medium energy output and medium half-life is unfortunate, medical applications notwithstanding I still wish Cobalt-60 didn‚Äôt exist, screw you Cobalt-60.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That while curing all cancer would only increase life expectancy by ~3 years and curing all heart disease would only increase life expectancy by ~3 years, and preventing all accidents would only increase life expectancy by ~1.5 years, if we did all of these at the same time and then a lot of other stuff too, eventually the effects would go nonlinear, so trying to cure cancer isn‚Äôt actually a waste of time, thankfully.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That the peroxisome, while the mitochondria and their stupid Krebs cycle get all the attention, when a fatty-acid that‚Äôs too long for them to catabolize comes along, who you gonna call.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That we have preferences, that there‚Äôs no agreed ordering of how good different things are, which is neat, and not something that would obviously be true for an alien species, and given our limited resources probably makes us happier on net.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That cardamom, it is cheap but tastes expensive, if cardamom cost 1000√ó more, people would brag about how they flew to Sri Lanka so they could taste chai made with fresh cardamom and swear that it changed their whole life.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That Gregory of Nyssa, he was right.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That Grandma Moses, it‚Äôs not too late.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That sleep, that probably evolution first made a low-energy mode so we don‚Äôt starve so fast and then layered on some maintenance processes, but the effect is that we live in a cycle and when things aren‚Äôt going your way it‚Äôs comforting that reality doesn‚Äôt stretch out before you indefinitely but instead you can look forward to a reset and a pause that‚Äôs somehow neither experienced nor skipped.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, glamorous or not, comfortable or not, cheap or not, carbon emitting or not, air travel is very safe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, for most of the things you‚Äôre worried about, the markets are less worried than you and they have the better track record, though not the issue of your mortality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That sexual attraction to romantic love to economic unit to reproduction, it‚Äôs a strange bundle, but who are we to argue with success.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That every symbolic expression recursively built from differentiable elementary functions has a derivative that can also be written as a recursive combination of elementary functions, although the latter expression may require vastly more terms.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That every expression graph built from differentiable elementary functions and producing a scalar output has a gradient that can itself be written as an expression graph, and furthermore that the latter expression graph is always the same size as the first one and is easy to find, and thus that it‚Äôs possible to fit very large expression graphs to data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, eerily, biological life and biological intelligence does not appear to make use of that property of expression graphs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you look at something and move your head around, you observe the entire light field, which is a five-dimensional function of three spatial coordinates and two angles, and yet if you do something fancy with lasers, somehow that entire light field can be stored on a single piece of normal two-dimensional film and then replayed later.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, as far as I can tell, the reason five-dimensional light fields can be stored on two-dimensional film simply cannot be explained without quite a lot of wave mechanics, a vivid example of the strangeness of this place and proof that all those physicists with their diffractions and phase conjugations really are up to something.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That disposable plastic, littered or not, harmless when consumed as thousands of small particles or not, is popular for a reason.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That disposable plastic, when disposed of correctly, is literally carbon sequestration, and that if/when air-derived plastic replaces dead-plankton-derived plastic, this might be incredibly convenient, although it must be said that currently the carbon in disposable plastic only represents a single-digit percentage of total carbon emissions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That rocks can be broken into pieces and then you can‚Äôt un-break the pieces but you can check that they came from the same rock, it‚Äôs basically cryptography.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That the deal society has made is that if you have kids then everyone you encounter is obligated to chip in a bit to assist you, and this seems to mostly work without the need for constant grimy negotiated transactions as Econ 101 would suggest, although the exact contours of this deal seem to be a bit murky.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That of all the humans that have ever lived, the majority lived under some kind of autocracy, with the rest distributed among tribal bands, chiefdoms, failed states, and flawed democracies, and only something like 1% enjoyed free elections and the rule of law and civil liberties and minimal corruption, yet we endured and today that number is closer to 10%, and so if you find yourself outside that set, do not lose heart.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you were in two dimensions and you tried to eat something then maybe your body would split into two pieces since the whole path from mouth to anus would have to be disconnected, so be thankful you‚Äôre in three dimensions, although maybe you could have some kind of jigsaw-shaped digestive tract so your two pieces would only jiggle around or maybe you could use the same orifice for both purposes, remember that if you ever find yourself in two dimensions, I guess.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Things to argue about over the holidays instead of politics III ¬∑ lists&lt;/p&gt;
    &lt;p&gt;Underrated reasons to be thankful IV ¬∑ lists&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dynomight.net/thanks-5/"/><published>2025-11-27T20:37:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46073817</id><title>A Programmer-Friendly I/O Abstraction Over io_uring and kqueue</title><updated>2025-11-28T00:47:56.055173+00:00</updated><content>&lt;doc fingerprint="7a1e315b29e0178f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Programmer-Friendly I/O Abstraction Over io_uring and kqueue&lt;/head&gt;
    &lt;p&gt;Consider this tale of I/O and performance. We√¢ll start with blocking I/O, explore io_uring and kqueue, and take home an event loop very similar to some software you may find familiar.&lt;/p&gt;
    &lt;p&gt;This is a twist on King√¢s talk at Software You Can Love Milan √¢22.&lt;/p&gt;
    &lt;p&gt;When you want to read from a file you might &lt;code&gt;open()&lt;/code&gt; and
then call &lt;code&gt;read()&lt;/code&gt; as many times as necessary to fill a
buffer of bytes from the file. And in the opposite direction, you call
&lt;code&gt;write()&lt;/code&gt; as many times as needed until everything is
written. It√¢s similar for a TCP client with sockets, but instead of
&lt;code&gt;open()&lt;/code&gt; you first call &lt;code&gt;socket()&lt;/code&gt; and then
&lt;code&gt;connect()&lt;/code&gt; to your server. Fun stuff.&lt;/p&gt;
    &lt;p&gt;In the real world though you can√¢t always read everything you want immediately from a file descriptor. Nor can you always write everything you want immediately to a file descriptor.&lt;/p&gt;
    &lt;p&gt;You can switch a file descriptor into non-blocking mode so the call won√¢t block while data you requested is not available. But system calls are still expensive, incurring context switches and cache misses. In fact, networks and disks have become so fast that these costs can start to approach the cost of doing the I/O itself. For the duration of time a file descriptor is unable to read or write, you don√¢t want to waste time continuously retrying read or write system calls.&lt;/p&gt;
    &lt;p&gt;So you switch to io_uring on Linux or kqueue on FreeBSD/macOS. (I√¢m skipping the generation of epoll/select users.) These APIs let you submit requests to the kernel to learn about readiness: when a file descriptor is ready to read or write. You can send readiness requests in batches (also referred to as queues). Completion events, one for each submitted request, are available in a separate queue.&lt;/p&gt;
    &lt;p&gt;Being able to batch I/O like this is especially important for TCP servers that want to multiplex reads and writes for multiple connected clients.&lt;/p&gt;
    &lt;p&gt;However in io_uring, you can even go one step further. Instead of having to call &lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; in userland
after a readiness event, you can request that the kernel do the
&lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; itself with a buffer you
provide. Thus almost all of your I/O is done in the kernel, amortizing
the overhead of system calls.&lt;/p&gt;
    &lt;p&gt;If you haven√¢t seen io_uring or kqueue before, you√¢d probably like an example! Consider this code: a simple, minimal, not-production-ready TCP echo server.&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const os = std.os;
const linux = os.linux;
const allocator = std.heap.page_allocator;

const State = enum{ accept, recv, send };
const Socket = struct {
: os.socket_t,
     handle: [1024]u8,
     buffer: State,
     state
 };
pub fn main() !void {
const entries = 32;
     const flags = 0;
     var ring = try linux.IO_Uring.init(entries, flags);
     defer ring.deinit();
     
var server: Socket = undefined;
     .handle = try os.socket(os.AF.INET, os.SOCK.STREAM, os.IPPROTO.TCP);
     serverdefer os.closeSocket(server.handle);
     
const port = 12345;
     var addr = std.net.Address.initIp4(.{127, 0, 0, 1}, port);
     var addr_len: os.socklen_t = addr.getOsSockLen();
     
try os.setsockopt(server.handle, os.SOL.SOCKET, os.SO.REUSEADDR, &amp;amp;std.mem.toBytes(@as(c_int, 1)));
     try os.bind(server.handle, &amp;amp;addr.any, addr_len);
     const backlog = 128;
     try os.listen(server.handle, backlog);
     
.state = .accept;
     server= try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
     _ 
while (true) {
     = try ring.submit_and_wait(1);
         _ 
while (ring.cq_ready() &amp;gt; 0) {
         const cqe = try ring.copy_cqe();
             var client = @intToPtr(*Socket, @intCast(usize, cqe.user_data));
             
if (cqe.res &amp;lt; 0) std.debug.panic("{}({}): {}", .{
             .state,
                 client.handle,
                 client@intToEnum(os.E, -cqe.res),
                 
             });
switch (client.state) {
             .accept =&amp;gt; {
                 = try allocator.create(Socket);
                     client .handle = @intCast(os.socket_t, cqe.res);
                     client.state = .recv;
                     client= try ring.recv(@ptrToInt(client), client.handle, .{.buffer = &amp;amp;client.buffer}, 0);
                     _ = try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
                     _ ,
                 }.recv =&amp;gt; {
                 const read = @intCast(usize, cqe.res);
                     .state = .send;
                     client= try ring.send(@ptrToInt(client), client.handle, client.buffer[0..read], 0);
                     _ ,
                 }.send =&amp;gt; {
                 .closeSocket(client.handle);
                     os.destroy(client);
                     allocator,
                 }
             }
         }
     } }&lt;/code&gt;
    &lt;p&gt;This is a great, minimal example. But notice that this code ties io_uring behavior directly to business logic (in this case, handling echoing data between request and response). It is fine for a small example like this. But in a large application you might want to do I/O throughout the code base, not just in one place. You might not want to keep adding business logic to this single loop.&lt;/p&gt;
    &lt;p&gt;Instead, you might want to be able to schedule I/O and pass a callback (and sometimes with some application context) to be called when the event is complete.&lt;/p&gt;
    &lt;p&gt;The interface might look like:&lt;/p&gt;
    &lt;code&gt;.dispatch({
 io_dispatch// some big struct/union with relevant fields for all event types
     , my_callback); }&lt;/code&gt;
    &lt;p&gt;This is great! Now your business logic can schedule and handle I/O no matter where in the code base it is.&lt;/p&gt;
    &lt;p&gt;Under the hood it can decide whether to use io_uring or kqueue depending on what kernel it√¢s running on. The dispatch can also batch these individual calls through io_uring or kqueue to amortize system calls. The application no longer needs to know the details.&lt;/p&gt;
    &lt;p&gt;Additionally, we can use this wrapper to stop thinking about readiness events, just I/O completion. That is, if we dispatch a read event, the io_uring implementation would actually ask the kernel to read data into a buffer. Whereas the kqueue implementation would send a √¢read√¢ readiness event, do the read back in userland, and then call our callback.&lt;/p&gt;
    &lt;p&gt;And finally, now that we√¢ve got this central dispatcher, we don√¢t need spaghetti code in a loop switching on every possible submission and completion event.&lt;/p&gt;
    &lt;p&gt;Every time we call io_uring or kqueue we both submit event requests and poll for completion events. The io_uring and kqueue APIs tie these two actions together in the same system call.&lt;/p&gt;
    &lt;p&gt;To sync our requests to io_uring or kqueue we√¢ll build a &lt;code&gt;flush&lt;/code&gt; function that submits requests and polls for
completion events. (In the next section we√¢ll talk about how the user of
the central dispatch learns about completion events.)&lt;/p&gt;
    &lt;p&gt;To make &lt;code&gt;flush&lt;/code&gt; more convenient, we√¢ll build a nice
wrapper around it so that we can submit as many requests (and process as
many completion events) as possible. To avoid accidentally blocking
indefinitely we√¢ll also introduce a time limit. We√¢ll call the wrapper
&lt;code&gt;run_for_ns&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally we√¢ll put the user in charge of setting up a loop to call this &lt;code&gt;run_for_ns&lt;/code&gt; function, independent of normal program
execution.&lt;/p&gt;
    &lt;p&gt;This is now your traditional event loop.&lt;/p&gt;
    &lt;p&gt;You may have noticed that in the API above we passed a callback. The idea is that after the requested I/O has completed, our callback should be invoked. But the question remains: how to track this callback between the submission and completion queue?&lt;/p&gt;
    &lt;p&gt;Thankfully, io_uring and kqueue events have user data fields. The user data field is opaque to the kernel. When a submitted event completes, the kernel sends a completion event back to userland containing the user data value from the submission event.&lt;/p&gt;
    &lt;p&gt;We can store the callback in the user data field by setting it to the callback√¢s pointer casted to an integer. When the completion for a requested event comes up, we cast from the integer in the user data field back to the callback pointer. Then, we invoke the callback.&lt;/p&gt;
    &lt;p&gt;As described above, the struct for &lt;code&gt;io_dispatch.dispatch&lt;/code&gt;
could get quite large handling all the different kinds of I/O events and
their arguments. We could make our API a little more expressive by
creating wrapper functions for each event type.&lt;/p&gt;
    &lt;p&gt;So if we wanted to schedule a read function we could call:&lt;/p&gt;
    &lt;code&gt;.read(fd, &amp;amp;buf, nBytesToRead, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;Or to write, similarly:&lt;/p&gt;
    &lt;code&gt;.write(fd, buf, nBytesToWrite, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;One more thing we need to worry about is that the batch we pass to io_uring or kqueue has a fixed size (technically, kqueue allows any batch size but using that might introduce unnecessary allocations). So we√¢ll build our own queue on top of our I/O abstraction to keep track of requests that we could not immediately submit to io_uring or kqueue.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;To keep this API simple we could allocate for each entry in the queue. Or we could modify the&lt;/p&gt;&lt;code&gt;io_dispatch.X&lt;/code&gt;calls slightly to accept a struct that can be used in an intrusive linked list to contain all request context, including the callback. The latter is what we do in TigerBeetle.&lt;/quote&gt;
    &lt;p&gt;Put another way: every time code calls &lt;code&gt;io_dispatch&lt;/code&gt;,
we√¢ll try to immediately submit the requested event to io_uring or
kqueue. But if there√¢s no room, we store the event in an overflow
queue.&lt;/p&gt;
    &lt;p&gt;The overflow queue needs to be processed eventually, so we update our &lt;code&gt;flush&lt;/code&gt; function (described in Callbacks and context above) to pull
as many events from our overflow queue before submitting a batch to
io_uring or kqueue.&lt;/p&gt;
    &lt;p&gt;We√¢ve now built something similar to libuv, the I/O library that Node.js uses. And if you squint, it is basically TigerBeetle√¢s I/O library! (And interestingly enough, TigerBeetle√¢s I/O code was adopted into Bun! Open-source for the win!)&lt;/p&gt;
    &lt;p&gt;Let√¢s check out how the Darwin version of TigerBeetle√¢s I/O library (with kqueue) differs from the Linux version. As mentioned, the complete &lt;code&gt;send&lt;/code&gt; call in the
Darwin implementation waits for file descriptor readiness (through
kqueue). Once ready, the actual &lt;code&gt;send&lt;/code&gt; call is made back in
userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) self.submit(
     ,
         context,
         callback,
         completion.send,
         .{
         .socket = socket,
             .buf = buffer.ptr,
             .len = @intCast(u32, buffer_limit(buffer.len)),
             ,
         }struct {
         fn do_operation(op: anytype) SendError!usize {
             return os.send(op.socket, op.buf[0..op.len], 0);
                 
             },
         }
     ); }&lt;/code&gt;
    &lt;p&gt;Compare this to the Linux version (with io_uring) where the kernel handles everything and there is no send system call in userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) .* = .{
     completion.io = self,
         .context = context,
         .callback = struct {
         fn wrapper(ctx: ?*anyopaque, comp: *Completion, res: *const anyopaque) void {
             
                 callback(@intToPtr(Context, @ptrToInt(ctx)),
                     ,
                     comp@intToPtr(*const SendError!usize, @ptrToInt(res)).*,
                     
                 );
             }.wrapper,
         }.operation = .{
         .send = .{
             .socket = socket,
                 .buffer = buffer,
                 ,
             },
         }
     };// Fill out a submission immediately if possible, otherwise adds to overflow buffer
     self.enqueue(completion);
      }&lt;/code&gt;
    &lt;p&gt;Similarly, take a look at &lt;code&gt;flush&lt;/code&gt; on Linux
and macOS
for event processing. Look at &lt;code&gt;run_for_ns&lt;/code&gt; on Linux
and macOS
for the public API users must call. And finally, look at what puts this
all into practice, the loop calling &lt;code&gt;run_for_ns&lt;/code&gt; in
src/main.zig.&lt;/p&gt;
    &lt;p&gt;We√¢ve come this far and you might be wondering √¢ what about cross-platform support for Windows? The good news is that Windows also has a completion based system similar to io_uring but without batching, called IOCP. And for bonus points, TigerBeetle provides the same I/O abstraction over it! But it√¢s enough to cover just Linux and macOS in this post. :)&lt;/p&gt;
    &lt;p&gt;In both this blog post and in TigerBeetle, we implemented a single-threaded event loop. Keeping I/O code single-threaded in userspace is beneficial (whether or not I/O processing is single-threaded in the kernel is not our concern). It√¢s the simplest code and best for workloads that are not embarrassingly parallel. It is also best for determinism, which is integral to the design of TigerBeetle because it enables us to do Deterministic Simulation Testing&lt;/p&gt;
    &lt;p&gt;But there are other valid architectures for other workloads.&lt;/p&gt;
    &lt;p&gt;For workloads that are embarrassingly parallel, like many web servers, you could instead use multiple threads where each thread has its own queue. In optimal conditions, this architecture has the highest I/O throughput possible.&lt;/p&gt;
    &lt;p&gt;But if each thread has its own queue, individual threads can become starved if an uneven amount of work is scheduled on one thread. In the case of dynamic amounts of work, the better architecture would be to have a single queue but multiple worker threads doing the work made available on the queue.&lt;/p&gt;
    &lt;p&gt;Hey, maybe we√¢ll split this out so you can use it too. It√¢s written in Zig so we can easily expose a C API. Any language with a C foreign function interface (i.e. every language) should work well with it. Keep an eye on our GitHub. :)&lt;/p&gt;
    &lt;p&gt;Additional resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/"/><published>2025-11-27T22:41:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46073855</id><title>250MWh 'Sand Battery' to start construction in Finland</title><updated>2025-11-28T00:47:55.750321+00:00</updated><content>&lt;doc fingerprint="b76f0758cbd3b7b"&gt;
  &lt;main&gt;
    &lt;p&gt;Technology provider Polar Night Energy and utility Lahti Energia have partnered for a large-scale project using Polar‚Äôs ‚ÄòSand Battery‚Äô technology for the latter‚Äôs district heating network in V√§√§ksy, Finland.&lt;/p&gt;
    &lt;p&gt;The project will have a heating power of 2MW and a thermal energy storage (TES) capacity of 250MW, making it a 125-hour system and the largest sand-based TES project once complete.&lt;/p&gt;
    &lt;p&gt;It will supply heat to Lahti Energia‚Äôs V√§√§ksy district heating network but is also large enough to participate in Fingrid‚Äôs reserve and grid balancing markets.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy‚Äôs technology works by heating a sand or a similar solid material using electricity, retaining that heat and then discharging that for industrial or heating use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Premium for just $1&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full premium access for the first month at only $1&lt;/item&gt;
      &lt;item&gt;Converts to an annual rate after 30 days unless cancelled&lt;/item&gt;
      &lt;item&gt;Cancel anytime during the trial period&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Premium Benefits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expert industry analysis and interviews&lt;/item&gt;
      &lt;item&gt;Digital access to PV Tech Power journal&lt;/item&gt;
      &lt;item&gt;Exclusive event discounts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Or get the full Premium subscription right away&lt;/head&gt;
    &lt;head rend="h3"&gt;Or continue reading this article for free&lt;/head&gt;
    &lt;p&gt;The project will cut fossil-based emissions in the V√§√§ksy district heating network by around 60% each year, by reducing natural gas use bu 80% and also decreasing wood chip consumption.&lt;/p&gt;
    &lt;p&gt;It follows Polar Night Energy completing and putting a 1MW/100MWh Sand Battery TES project into commercial operations this summer, for another utility Loviisan L√§mp√∂. That project uses soapstone as its storage medium, a byproduct of ceramics production.&lt;/p&gt;
    &lt;p&gt;This latest project will use locally available natural sand, held in a container 14m high and 15m wide. Lahti Energia received a grant for the project from state body Business Finland.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy will act as the main contractor for the construction project, with on-site work beginning in early 2026, and the Sand Battery will be completed in summer 2027.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe want to offer our customers affordable district heating and make use of renewable energy in our heat production. The scale of this Sand Battery also enables us to participate in Fingrid‚Äôs reserve and grid balancing markets. As the share of weather-dependent energy grows in the grid, the Sand Battery will contribute to balancing electricity supply and demand‚Äù, says Jouni Haikarainen, CEO of Lahti Energia.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.energy-storage.news/250mwh-sand-battery-to-start-construction-in-finland-for-both-heating-and-ancillary-services/"/><published>2025-11-27T22:48:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074111</id><title>Vsora Jotunn-8 5nm European inference chip</title><updated>2025-11-28T00:47:54.745970+00:00</updated><content>&lt;doc fingerprint="a495c47b155c4c8f"&gt;
  &lt;main&gt;
    &lt;p&gt;In modern data centers, success means deploying trained models with blistering speed, minimal cost, and effortless scalability. Designing and operating inference systems requires balancing key factors such as high throughput, low latency, optimized power consumption, and sustainable infrastructure. Achieving optimal performance while maintaining cost and energy efficiency is critical to meeting the growing demand for large-scale, real-time AI services across a variety of applications.&lt;/p&gt;
    &lt;p&gt;Unlock the full potential of your AI investments with our high-performance inference solutions. Engineered for speed, efficiency, and scalability, our platform ensures your AI models deliver maximum impact‚Äîat lower operational costs and with a commitment to sustainability. Whether you‚Äôre scaling up deployments or optimizing existing infrastructure, we provide the technology and expertise to help you stay competitive and drive business growth.&lt;/p&gt;
    &lt;p&gt;This is not just faster inference. It‚Äôs a new foundation for AI at scale.&lt;/p&gt;
    &lt;p&gt;In the world of AI data centers, speed, efficiency, and scale aren‚Äôt optional‚Äîthey‚Äôre everything. Jotunn8, our ultra-high-performance inference chip is built to deploy trained models with lightning-fast throughput, minimal cost, and maximum scalability. Designed around what matters most‚Äîperformance, cost-efficiency, and sustainability‚Äîthey deliver the power to run AI at scale, without compromise!&lt;/p&gt;
    &lt;p&gt;Why it matters: Critical for real-time applications like chatbots, fraud detection, and search.&lt;/p&gt;
    &lt;p&gt;Reasoning models, Generative AI and Agentic AI are increasingly being combined to build more capable and reliable systems. Generative AI provide flexibility and language fluency. Reasoning models provide rigor and correctness. Agentic frameworks provide autonomy and decision-making. The VSORA architecture enables smooth and easy integration of these algorithms, providing near-theory performance.&lt;/p&gt;
    &lt;p&gt;Why it matters: AI inference is often run at massive scale ‚Äì reducing cost per inference is essential for business viability.&lt;/p&gt;
    &lt;p&gt;Unmatched Performance at the Edge with Edge AI.&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V core to offload &amp;amp; run AI completely on-chip&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8: 1600 Tflops&lt;lb/&gt;fp16: 400 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8: 800 Tflops&lt;lb/&gt;fp16: 200 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8/int8: 50 Tflops&lt;lb/&gt;fp16/int16: 25 Tflops&lt;lb/&gt;fp32/int32: 12 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8/int8: 25 Tflops&lt;lb/&gt;fp16/int16: 12 Tflops&lt;lb/&gt;fp32/int32: 6 Tflops&lt;/p&gt;
    &lt;p&gt;Close to theory efficiency&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V cores to offload host &lt;lb/&gt;&amp;amp; run AI completely on-chip.&lt;/p&gt;
    &lt;p&gt;fp8: 3200 Tflops&lt;lb/&gt;fp16: 800 Tflops &lt;/p&gt;
    &lt;p&gt;fp8/int8: 100 Tflops&lt;lb/&gt;fp16/int16: 50 Tflops&lt;lb/&gt;fp32/int32: 25 Tflops&lt;lb/&gt;Close to theory efficiency&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vsora.com/products/jotunn-8/"/><published>2025-11-27T23:30:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074286</id><title>Bird flu viruses are resistant to fever, making them a major threat to humans</title><updated>2025-11-28T00:47:54.554654+00:00</updated><content/><link href="https://medicalxpress.com/news/2025-11-bird-flu-viruses-resistant-fever.html"/><published>2025-11-27T23:57:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074362</id><title>How Charles M Schulz created Charlie Brown and Snoopy (2024)</title><updated>2025-11-28T00:47:54.202378+00:00</updated><content>&lt;doc fingerprint="2e364174bde00afd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'You have to just draw something that you hope is funny': How Charles M Schulz created Charlie Brown and Snoopy&lt;/head&gt;
    &lt;p&gt;Charles M Schulz drew his beloved Peanuts strip for 50 years until his announcement on 14 December 1999 that ill health was forcing him to retire. In History looks at how an unassuming cartoonist built a billion-dollar empire out of the lives of a group of children, a dog and a bird.&lt;/p&gt;
    &lt;p&gt;Charles M Schulz's timeless creation Charlie Brown may have been as popular as any character in all of literature, but the cartoonist was modest about the scope of his miniature parables. In a 1977 BBC interview, he said: "I'm talking only about the minor everyday problems in life. Leo Tolstoy dealt with the major problems of the world. I'm only dealing with why we all have the feeling that people don't like us."&lt;/p&gt;
    &lt;p&gt;This did not mean that he felt as if he was dealing with trivial matters. He said: "I'm always very much offended when someone asks me, 'Do I ever do satire on the social condition?' Well, I do it almost every day. And they say, 'Well, do you ever do political things?' I say, 'I do things which are more important than politics. I'm dealing with love and hate and mistrust and fear and insecurity.'"&lt;/p&gt;
    &lt;p&gt;While Charlie Brown may have been the eternal failure, the universal feelings that Schulz channelled helped make Peanuts a global success. Born in 1922, Schulz drew every single Peanuts strip himself from 1950 until his death in February 2000. It was so popular that Nasa named two of the modules in its May 1969 Apollo 10 lunar mission after Charlie Brown and Snoopy. The strip was syndicated in more than 2,600 newspapers worldwide, and inspired films, music and countless items of merchandise.&lt;/p&gt;
    &lt;p&gt;Part of its success, according to the writer Umberto Eco, was that it worked on different levels. He wrote: "Peanuts charms both sophisticated adults and children with equal intensity, as if each reader found there something for himself, and it is always the same thing, to be enjoyed in two different keys. Peanuts is thus a little human comedy for the innocent reader and for the sophisticated."&lt;/p&gt;
    &lt;p&gt;Schulz's initial reason for focusing on children in the strip was strictly commercial. In 1990, he told the BBC: "I always hate to say it, but I drew little kids because this is what sold. I wanted to draw something, I didn't know what it was, but it just seemed as if whenever I drew children, these were the cartoons that editors seemed to like the best. And so, back in 1950, I mailed a batch of cartoons to New York City, to United Features Syndicate, and they said they liked them, and so ever since I've been drawing little kids."&lt;/p&gt;
    &lt;p&gt;IN HISTORY&lt;/p&gt;
    &lt;p&gt;In History is a series which uses the BBC's unique audio and video archive to explore historical events that still resonate today. Subscribe to the accompanying weekly newsletter.&lt;/p&gt;
    &lt;p&gt;Of Snoopy and Charlie Brown, he said: "I've always been a little bit intrigued by the fact that dogs apparently tolerate the actions of the children with whom they are playing. It's almost as if the dogs are smarter than the kids. I think also that the characters I have serve as a good outlet for any idea that I may come up with. I never think of an idea and then find that I have no way of using it. I can use any idea that I think of because I've got the right repertory company."&lt;/p&gt;
    &lt;p&gt;Schulz called upon some of his earliest experiences as a shy child to create the strip. As a teenager, he studied drawing by correspondence course because he was too reticent to attend art school in person. Speaking in 1977, he said: "I couldn't see myself sitting in a room where everyone else in the room could draw much better than I, and this way I was protected by drawing at home and simply mailing my drawings in and having them criticised. I wish I had a better education, but I think that my entire background made me well suited for what I do.&lt;/p&gt;
    &lt;p&gt;"If I could write better than I can, perhaps I would have tried to become a novelist, and I might have become a failure. If I could draw better than I can, I might have tried to become an illustrator or an artist and would have failed there, but my entire being seems to be just right for being a cartoonist."&lt;/p&gt;
    &lt;head rend="h2"&gt;Never give up&lt;/head&gt;
    &lt;p&gt;Peanuts remained remarkably consistent despite the relentless publishing schedule, and Schulz would not let the expectations of his millions of fans become a distraction. He said: "You have to kind of bend over the drawing board, shut the world out and just draw something that you hope is funny. Cartooning is still drawing funny pictures, whether they're just silly little things or rather meaningful political cartoons, but it's still drawing something funny, and that's all you should think about at that time ‚Äì keep kind of a light feeling.&lt;/p&gt;
    &lt;p&gt;"I suppose when a composer is composing well, the music is coming faster than he can think of it, and when I have a good idea I can hardly get the words down fast enough. I'm afraid that they will leave me before I get them down on the paper. Sometimes my hand will literally shake with excitement as I'm drawing it because I'm having a good time. Unfortunately, this does not happen every day."&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Julie Andrews on being 'teased' for Mary Poppins&lt;/p&gt;
    &lt;p&gt;Despite his modesty, Schulz insisted he was always confident that Peanuts would be a hit. He said: "I mean, when you sign up to play at Wimbledon, you expect to win. Obviously, there are a lot of things that I didn't anticipate, like Snoopy's going to the Moon and things like that, but I always had hopes it would become big."&lt;/p&gt;
    &lt;p&gt;Schulz generally worked five weeks in advance. On 14 December 1999, fans were dismayed to learn that he would be hanging up his pen because he had cancer. He said that his cartoon for 3 January 2000 would be the final daily release. It would be followed on 13 February with the final strip for a Sunday newspaper. He died one day before that last strip ran.&lt;/p&gt;
    &lt;p&gt;In it, Schulz wrote: "I have been grateful over the years for the loyalty of our editors and the wonderful support and love expressed to me by fans of the comic strip. Charlie Brown, Snoopy, Linus, Lucy... how can I ever forget them..."&lt;/p&gt;
    &lt;p&gt;Back in 1977, Schulz insisted that the cartoonist's role was mostly to point out problems rather than trying to solve them, but there was one lesson that people could take from his work. He said: "I suppose one of the solutions is, as Charlie Brown, just to keep on trying. He never gives up. And if anybody should give up, he should."&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For more stories and never-before-published radio scripts to your inbox, sign up to the In History newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook, X and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/culture/article/20241205-how-charles-m-schulz-created-charlie-brown-and-snoopy"/><published>2025-11-28T00:10:38+00:00</published></entry></feed>