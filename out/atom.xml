<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-10T16:49:02.876631+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46205661</id><title>Pebble Index 01 – External memory for your brain</title><updated>2025-12-10T16:49:09.633230+00:00</updated><content>&lt;doc fingerprint="5c6b48dfd9872b65"&gt;
  &lt;main&gt;
    &lt;p&gt;Catch your best ideas before they slip through your fingers&lt;/p&gt;
    &lt;p&gt;Do you ever have flashes of insight or an idea worth remembering? This happens to me 5-10 times every day. If I don’t write down the thought immediately, it slips out of my mind. Worst of all, I remember that I’ve forgotten something and spend the next 10 minutes trying to remember what it is. So I invented external memory for my brain.&lt;/p&gt;
    &lt;p&gt;Introducing Pebble Index 01 - a small ring with a button and microphone. Hold the button, whisper your thought, and it’s sent to your phone. It’s added to your notes, set as a reminder, or saved for later review.&lt;/p&gt;
    &lt;p&gt;Index 01 is designed to become muscle memory, since it’s always with you. It’s private by design (no recording until you press the button) and requires no internet connection or paid subscription. It’s as small as a wedding band and comes in 3 colours. It’s made from durable stainless steel and is water-resistant. Like all Pebble products, it’s extremely customizable and built with open source software.&lt;/p&gt;
    &lt;p&gt;Here’s the best part: the battery lasts for years. You never need to charge it.&lt;/p&gt;
    &lt;p&gt;Pre-order today for $75. After worldwide shipping begins in March 2026, the price will go up to $99.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Design&lt;/head&gt;
    &lt;p&gt;Now that I’ve worn my Index 01 for several months, I can safely say that it has changed my life - just like with Pebble, I couldn’t go back to a world without this. There are so many situations each day where my hands are full (while biking or driving, washing dishes, wrangling my kids, etc) and I need to remember something. A random sampling of my recent recordings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set a timer for 3pm to go pick up the kids&lt;/item&gt;
      &lt;item&gt;Remind me to phone the pharmacy at 11am&lt;/item&gt;
      &lt;item&gt;Peter is coming by tomorrow at 11:30am, add that to my calendar&lt;/item&gt;
      &lt;item&gt;Jerry recommends reading Breakneck&lt;/item&gt;
      &lt;item&gt;Mark wants a Black/Red PT2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before, I would take my phone out of my pocket to jot these down, but I couldn’t always do that (eg, while bicycling). I also wanted to start using my phone less, especially in front of my kids.&lt;/p&gt;
    &lt;p&gt;Initially, we experimented by building this as an app on Pebble, since it has a mic and I’m always wearing one. But, I realized quickly that this was suboptimal - it required me to use my other hand to press the button to start recording (lift-to-wake gestures and wake-words are too unreliable). This was tough to use while bicycling or carrying stuff.&lt;/p&gt;
    &lt;p&gt;Then a genius electrical engineer friend of mine came up with an idea to fit everything into a tiny ring. It is the perfect form factor! Honestly, I’m still amazed that it all fits.&lt;/p&gt;
    &lt;p&gt;The design needed to satisfy several critical conditions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Must work reliably 100% of the time. If it didn’t work or failed to record a thought, I knew I would take it off and revert back to my old habit of just forgetting things.&lt;/item&gt;
      &lt;item&gt;It had to have a physical press-button, with a satisfying click-feel. I want to know for sure if the button is pressed and my thought is captured.&lt;/item&gt;
      &lt;item&gt;Long battery life - every time you take something off to charge, there’s a chance you’ll forget to put it back on.&lt;/item&gt;
      &lt;item&gt;Must be privacy-preserving. These are your inner thoughts. All recordings must be processed and stored on your phone. Only record when the button is pressed.&lt;/item&gt;
      &lt;item&gt;It had to be as small as a wedding band. Since it’s worn on the index finger, if it were too large or bulky, it would hit your phone while you held it in your hand.&lt;/item&gt;
      &lt;item&gt;Water resistance - must be able to wash hands, shower, and get wet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ve been working on this for a while, testing new versions and making tweaks. We’re really excited to get this out into the world.&lt;/p&gt;
    &lt;p&gt;Here are a few of my favourite things about Index 01:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It does one thing really well - it helps me remember things.&lt;/item&gt;
      &lt;item&gt;It’s discreet. It's not distracting. It doesn't take you out of the moment.&lt;/item&gt;
      &lt;item&gt;There’s no AI friend persona and it’s not always recording.&lt;/item&gt;
      &lt;item&gt;It’s inexpensive. We hope you try it and see if you like it as well!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;#Key Details&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Available in 3 colours and 8 sizes &lt;list rend="ul"&gt;&lt;item&gt;Colours: polished silver, polished gold, and matte black&lt;/item&gt;&lt;item&gt;US ring sizes: 6, 7, 8, 9, 10, 11, 12, 13&lt;/item&gt;&lt;item&gt;You can pre-order now and pick your size/colour later before your ring ships.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Cost and availability: Pre-order price is $75, rises to $99 later. Ships worldwide, beginning in March.&lt;/item&gt;
      &lt;item&gt;Works with iPhone and Android: We overcame Apple’s best efforts to make life terrible for 3rd party accessory makers and have Index 01 working well on iOS and Android.&lt;/item&gt;
      &lt;item&gt;Extremely private and secure: Your thoughts are processed by open source speech-to-text (STT) and AI models locally on your phone. You can read the code and see exactly how it works - our Pebble mobile app is open source. Higher-quality STT is available through an optional cloud service.&lt;/item&gt;
      &lt;item&gt;No charging: The battery lasts for up to years of average use. After the end of its life, send your ring back to us for recycling.&lt;/item&gt;
      &lt;item&gt;On-ring storage: Recording works even if your phone is out of range. Up to 5 minutes of audio can be stored on-ring, then synced later.&lt;/item&gt;
      &lt;item&gt;No speaker or vibrating motor: This is an input device only. There is an RGB LED, but it’s rarely used (to save battery life and to reduce distraction).&lt;/item&gt;
      &lt;item&gt;Works great with Pebble or other smartwatches: After recording, the thought will appear on your watch, and you can check that it’s correct. You can ask questions like ‘What’s the weather today?’ and see the answer on your watch.&lt;/item&gt;
      &lt;item&gt;Raw audio playback: Very helpful if STT doesn’t work perfectly due to wind or loud background noises.&lt;/item&gt;
      &lt;item&gt;Actions: While the primary task is remembering things for you, you can also ask it to do things like ’Send a Beeper message to my wife - running late’ or answer simple questions that could be answered by searching the web. You can configure button clicks to control your music - I love using this to play/pause or skip tracks. You can also configure where to save your notes and reminders (I have it set to add to Notion).&lt;/item&gt;
      &lt;item&gt;Customizable and hackable: Configure single/double button clicks to control whatever you want (take a photo, turn on lights, Tasker, etc). Add your own voice actions via MCP. Or route the audio recordings directly to your own app or server!&lt;/item&gt;
      &lt;item&gt;99+ languages: Speech to text and local LLM support over 99 languages! Naturally, the quality of each may vary.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;#Future Plans&lt;/head&gt;
    &lt;p&gt;Let me be very clear - Index 01 is designed at its core to be a device that helps you remember things. We want it to be 100% reliable at its primary task. But we’re leaving the side door open for folks to customize, build new interactions and actions.&lt;/p&gt;
    &lt;p&gt;Here’s how I’m thinking about it - a single click-hold + voice input will be routed to the primary memory processing path. Double-click-hold + voice input would be routed to a more general purpose voice agent (think ChatGPT with web search). Responses from the agent would be presented on Pebble (eg ‘What’s the weather tomorrow?’, ‘When’s the next northbound Caltrain?’) or other smartwatches (as a notification). Maybe this could even be an input for something like ChatGPT Voice Mode, enabling you to hear the AI response from your earbuds.&lt;/p&gt;
    &lt;p&gt;The built in actions, set reminder, create note, alarms, etc, are actually MCPs - basically mini apps that AI agents know how to operate. They run locally in WASM within the Pebble mobile app (no cloud MCP server required). Basically any MCP server can be used with the system, so intrepid folks may have fun adding various actions like Beeper, Google Calendar, weather, etc that already offer MCPs.&lt;/p&gt;
    &lt;p&gt;Not everything will be available at launch, but this is the direction we are working towards. There will be 3 ways to customize your Index 01:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Trigger actions via button clicks - configure a single or double click to do things like take a photo, control your Home Assistant smart home, Tasker function, unlock your car. This will work better on Android since iOS Shortcuts doesn’t have an open API.&lt;/item&gt;
      &lt;item&gt;Trigger actions via voice input - write an MCP to do….basically anything? This is pretty open ended.&lt;/item&gt;
      &lt;item&gt;Route your voice recordings and/or transcriptions to your own webhook - or skip our AI processing entirely and send every recording to your own app or webapp.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;#FAQ&lt;/head&gt;
    &lt;p&gt;How does it work?&lt;/p&gt;
    &lt;p&gt;People usually wear it on the index finger. Inside the ring is a button, a microphone, a Bluetooth chip, memory, and a battery that lasts for years. Click the button with your thumb, talk into the mic, and it records to internal memory. When your phone is in range, the recording is streamed to the Pebble app. It’s converted to text on-device, then processed by an on-device large language model (LLM) which selects an action to take (create note, add to reminders, etc).&lt;/p&gt;
    &lt;p&gt;When do I pick my size?&lt;/p&gt;
    &lt;p&gt;You’ll be able to pick your ring size and color after placing a pre-order. If you have a 3D printer, you can print our CAD designs to try on. We’re also planning a sizing kit. You can view the measurements of the inner diameter of each ring size.&lt;/p&gt;
    &lt;p&gt;How long does the battery last?&lt;/p&gt;
    &lt;p&gt;Roughly 12 to 15 hours of recording. On average, I use it 10-20 times per day to record 3-6 second thoughts. That’s up to 2 years of usage.&lt;/p&gt;
    &lt;p&gt;Is it secure and private?&lt;/p&gt;
    &lt;p&gt;Yes, extremely. The connection between ring and phone is encrypted. Recordings are processed locally on your phone in the open-source Pebble app. The app works offline (no internet connection) and does not require a cloud service. An optional cloud storage system for backing up recordings is available. Our plan is for this to be optionally encrypted, but we haven’t built it yet.&lt;/p&gt;
    &lt;p&gt;Is a paid subscription required?&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;What kind of battery is inside?&lt;/p&gt;
    &lt;p&gt;Index 01 uses silver-oxide batteries.&lt;/p&gt;
    &lt;p&gt;Why can’t it be recharged?&lt;/p&gt;
    &lt;p&gt;We considered this but decided not to for several reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You’d probably lose the charger before the battery runs out!&lt;/item&gt;
      &lt;item&gt;Adding charge circuitry and including a charger would make the product larger and more expensive.&lt;/item&gt;
      &lt;item&gt;You send it back to us to recycle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wait, it’s single use?&lt;/p&gt;
    &lt;p&gt;Yes. We know this sounds a bit odd, but in this particular circumstance we believe it’s the best solution to the given set of constraints. Other smart rings like Oura cost $250+ and need to be charged every few days. We didn’t want to build a device like that. Before the battery runs out, the Pebble app notifies and asks if you’d like to order another ring.&lt;/p&gt;
    &lt;p&gt;Is it always listening?&lt;/p&gt;
    &lt;p&gt;No. It only records while the button is pressed. It’s not designed to record your whole life, or meetings.&lt;/p&gt;
    &lt;p&gt;What if the speech-to-text processing misses a word or something?&lt;/p&gt;
    &lt;p&gt;You can always listen to the each recording in the app.&lt;/p&gt;
    &lt;p&gt;Why no touchpad?&lt;/p&gt;
    &lt;p&gt;We experimented with a touchpad, but found it too easy to accidentally swipe and press. Also, nothing beats the feedback of a real gosh darn pressable button.&lt;/p&gt;
    &lt;p&gt;Is there a speaker or vibrating motor?&lt;/p&gt;
    &lt;p&gt;No. The button has a great click-feel to indicate when you are pressing.&lt;/p&gt;
    &lt;p&gt;Does it do health tracking like Oura?&lt;/p&gt;
    &lt;p&gt;Nope&lt;/p&gt;
    &lt;p&gt;How durable and water-resistant is it?&lt;/p&gt;
    &lt;p&gt;It’s primarily made from stainless steel 316, with a liquid silicone rubber (LSR) button. It’s water-resistant to 1 meter. You can wash your hands, do dishes, and shower with it on, but we don’t recommend swimming with it.&lt;/p&gt;
    &lt;p&gt;Does it work with iPhone and Android?&lt;/p&gt;
    &lt;p&gt;Yes&lt;/p&gt;
    &lt;p&gt;I love customizing and hacking on my devices. What could I do with Index 01?&lt;/p&gt;
    &lt;p&gt;Lots of stuff! Control things with the buttons. Route raw audio or transcribed text directly to your own app via webhook. Use MCPs (also run locally on-device! No cloud server required) to add more actions.&lt;/p&gt;
    &lt;p&gt;Is this an AI friend thingy or always-recording device?&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;How far along is development?&lt;/p&gt;
    &lt;p&gt;We’ve been working on this in the background to watch development. It helps that our Pebble Time 2 partner factory is also building Index 01! We’re currently in the DVT stage, testing pre-production samples. We’ll start a wider alpha test in January with a lot more people. Here’s some shots from the pre-production assembly line:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://repebble.com/blog/meet-pebble-index-01-external-memory-for-your-brain"/><published>2025-12-09T15:03:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46206277</id><title>A supersonic engine core makes the perfect power turbine</title><updated>2025-12-10T16:49:09.074479+00:00</updated><content>&lt;doc fingerprint="16d2b440c19c9c2a"&gt;
  &lt;main&gt;
    &lt;p&gt;By: Blake Scholl, Founder &amp;amp; CEO, Boom Supersonic&lt;/p&gt;
    &lt;p&gt;It started, as many things do these days, by scrolling on X.&lt;/p&gt;
    &lt;p&gt;I was reading post after post about the power crisis hitting AI data centers—GPU racks sitting idle, waiting not on chips, but on electricity. I texted with Sam Altman—who confirmed power was indeed a major constraint. I pinged our engineering team—and found that they already had the outline of a plan to build a power turbine based on our Symphony supersonic engine.&lt;/p&gt;
    &lt;p&gt;After a few conversations, it became clear: AI didn’t just need more turbines—it needed a new and fundamentally better turbine. Symphony was the perfect new engine to accelerate AI in America. About three months later, we had a signed deal for 1.21 gigawatts and had started manufacturing the first turbine.&lt;/p&gt;
    &lt;p&gt;Today, we’re announcing Superpower, our new 42‑megawatt natural gas turbine, along with a $300M funding round and Crusoe as our launch customer. And most importantly: this marks a turning point. Boom is now on a self-funded path to both Superpower and the Overture supersonic airliner.&lt;/p&gt;
    &lt;p&gt;I want to share the real story of how this happened—and why supersonic technology is exactly what America’s energy crisis demands.&lt;/p&gt;
    &lt;head rend="h4"&gt;America Doesn’t Have 10–15 Years to Solve Its Power Problem the Old Way&lt;/head&gt;
    &lt;p&gt;If you’ve been paying attention, you know the U.S. is in a genuine energy crunch. GPU racks are idling because they can’t get power. Data centers are fighting over substations and interconnection queues. Meanwhile China is adding power capacity at a wartime pace—coal, gas, nuclear, everything—while America struggles to get a single transmission line permitted.&lt;/p&gt;
    &lt;p&gt;AI won’t wait for us to fix the grid. And the United States simply doesn’t have 10–15 years to build out power infrastructure the old way.&lt;/p&gt;
    &lt;p&gt;Hyperscalers have already moved to their own Plan B: behind‑the‑meter power plants. You’ve seen XAI’s Colossus I and II in Memphis. OpenAI’s Stargate I in Abilene. These projects are powered by arrays of aeroderivative natural-gas turbines—which are, fundamentally, modified jet engines from the 1970s. There’s something brilliant in this approach: the transition from gigantic “frame” turbines to arrays of mid-size “aeroderivative” turbines mirrors the computing industry’s shift from mainframes to blade servers.&lt;/p&gt;
    &lt;p&gt;The problem? The “blade servers” of the energy world are old tech and they’re sold out. Because the most popular “aeroderivative” turbines are based on subsonic jet engines, they’re happiest when the outside air temperature is -50°F—like it is when going Mach 0.8 at 30,000 feet. As outside temperatures rise, there is no option but to throttle back the engines—or else the turbine blades literally melt down. These turbines begin losing power at about 50°F and by the time it’s 110°—as often happens in popular data center locations like Texas—30% of generation capacity is lost. Nonetheless, major manufacturers all have backlogs through the rest of the decade and none is building a new-generation advanced-technology turbine.&lt;/p&gt;
    &lt;head rend="h4"&gt;A Supersonic Engine Core Makes the Perfect Power Turbine&lt;/head&gt;
    &lt;p&gt;When we designed the Symphony engine for Overture, we built something no one else has built this century: a brand-new large engine core optimized for continuous, high‑temperature operation.&lt;/p&gt;
    &lt;p&gt;A subsonic engine is built for short bursts of power at takeoff. A supersonic engine is built to run hard, continuously, at extreme thermal loads. Symphony was designed for Mach 1.7 at 60,000 feet, where effective temperatures reach 160°F—not the frigid -50°F conditions where legacy subsonic engines operate.&lt;/p&gt;
    &lt;p&gt;This gives Superpower several critical advantages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Full power even with high ambient heat – Where legacy turbines lose 20–30% at 110°F, Superpower maintains its full 42MW output without derate.&lt;/item&gt;
      &lt;item&gt;Waterless operation – Legacy turbines need huge quantities of water for cooling to avoid thermal derate in hot environments. Superpower doesn’t. It stays at full output, water‑free.&lt;/item&gt;
      &lt;item&gt;Cloud‑native control and monitoring. Superpower inherits the telemetry and operations stack we built for XB‑1. Every turbine streams real‑time performance data, supports remote control, and flags anomalies before customers ever notice.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Superpower and Symphony are based on virtually identical turbine engines. Both share the identical core (HPC and HPT) and a slightly tuned low spool. In the place of Symphony’s hollow-core titanium fan, Superpower adds two additional compressor stages plus a three-stage free power turbine connected to a high-efficiency generator on its own shaft. Additionally, the engines use slightly different fuel nozzles, Symphony’s optimized for Jet A vs. Superpower’s for natural gas.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scaling Production the Supersonic Way: Vertical Integration&lt;/head&gt;
    &lt;p&gt;The legacy aerospace supply chain is congested. When the mission is urgent and the supply chain congested, you build the supply chain. The new Superpower Superfactory starts with a simple vision: raw materials in one side of the building, gigawatts of completed power turbine packages out the other side. We’ve already started making the first parts—and much of the production equipment to support 2GW/yr is on order. With this new financing we’re ready to accelerate further.&lt;/p&gt;
    &lt;p&gt;If America wants to build at the speed AI requires, vertical integration isn’t optional. We’re standing up our own foundry and our own large scale CNC machining capability. We’ll have more to share on the Superpower Superfactory in early 2026.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scaling Production the Supersonic Way: Vertical Integration&lt;/head&gt;
    &lt;p&gt;Superpower is sort of like our Starlink moment, the strongest accelerant we’ve ever had toward our core mission of making Earth dramatically more accessible.&lt;/p&gt;
    &lt;p&gt;The fastest way to a certified, passenger-carrying Symphony engine is to run its core for hundreds of thousands of hours in the real world, powering Earth’s most demanding AI data centers. Every hour a Superpower turbine spins is an hour of validation for Symphony. Every gigawatt we deliver strengthens our vertical integration and manufacturing capability. And with Superpower profitability funding the remainder of the aircraft program, we’ve done something rare in aerospace: created a self-sustaining path to a new airliner.&lt;/p&gt;
    &lt;p&gt;Superpower also reminds me of what Boom is at our core: a team willing to take on what others say is impossible, to do with a small team what big companies might not even attempt.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://boomsupersonic.com/flyby/ai-needs-more-power-than-the-grid-can-deliver-supersonic-tech-can-fix-that"/><published>2025-12-09T15:51:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46206531</id><title>Bruno Simon – 3D Portfolio</title><updated>2025-12-10T16:49:08.675415+00:00</updated><content>&lt;doc fingerprint="e43305f6f9f2bf3"&gt;
  &lt;main&gt;
    &lt;p&gt;00:00:000&lt;/p&gt;
    &lt;p&gt;Server currently offline. Scores can't be saved.&lt;/p&gt;
    &lt;p&gt;Welcome!&lt;/p&gt;
    &lt;p&gt;My name is Bruno Simon, and I'm a creative developer (mostly for the web).&lt;/p&gt;
    &lt;p&gt;This is my portfolio. Please drive around to learn more about me and discover the many secrets of this world.&lt;/p&gt;
    &lt;p&gt;And don't break anything!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Quality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;I'm stuck!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reset&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Renderer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Server&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WASD or ARROWS&lt;/cell&gt;
        &lt;cell&gt;Move around&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SHIFT&lt;/cell&gt;
        &lt;cell&gt;Boost&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CTRL LEFT or B&lt;/cell&gt;
        &lt;cell&gt;Brake&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SPACE&lt;/cell&gt;
        &lt;cell&gt;Jump&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ENTER&lt;/cell&gt;
        &lt;cell&gt;Interact&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;M&lt;/cell&gt;
        &lt;cell&gt;Map&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L&lt;/cell&gt;
        &lt;cell&gt;Mute&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;T&lt;/cell&gt;
        &lt;cell&gt;Post a whisper&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;R&lt;/cell&gt;
        &lt;cell&gt;Respawn&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NUM KEYS/NUM PAD&lt;/cell&gt;
        &lt;cell&gt;Activate hydraulics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LEFT CLICK (DRAG)&lt;/cell&gt;
        &lt;cell&gt;Move camera&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;H&lt;/cell&gt;
        &lt;cell&gt;Honk&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;One finger&lt;/cell&gt;
        &lt;cell&gt;Move the car&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Two fingers&lt;/cell&gt;
        &lt;cell&gt;Move camera / zoom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tap (on the car)&lt;/cell&gt;
        &lt;cell&gt;Jump&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;B&lt;/cell&gt;
        &lt;cell&gt;Boost&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;Jump&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;X&lt;/cell&gt;
        &lt;cell&gt;Brake&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Interact / Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LT L2&lt;/cell&gt;
        &lt;cell&gt;Accelerate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;RT R2&lt;/cell&gt;
        &lt;cell&gt;Backward accelerate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LB / RB L1 / R1&lt;/cell&gt;
        &lt;cell&gt;Hydraulics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Joystick Left&lt;/cell&gt;
        &lt;cell&gt;Turn wheels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Joystick Left (press)&lt;/cell&gt;
        &lt;cell&gt;Honk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Joystick Right&lt;/cell&gt;
        &lt;cell&gt;Move camera&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Joystick Right (press)&lt;/cell&gt;
        &lt;cell&gt;Zoom in/out&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Select&lt;/cell&gt;
        &lt;cell&gt;Reset&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Start&lt;/cell&gt;
        &lt;cell&gt;Pause&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Server currently offline. Scores can't be saved.&lt;/p&gt;
    &lt;p&gt;Resets in&lt;/p&gt;
    &lt;p&gt;Whispers are messages left by visitors.&lt;/p&gt;
    &lt;p&gt; - Everyone can see them&lt;lb/&gt; - New whispers remove old ones (max 30)&lt;lb/&gt; - One whisper per user&lt;lb/&gt; - Choose a flag&lt;lb/&gt; - No slur!&lt;lb/&gt; - Max 30 characters &lt;/p&gt;
    &lt;p&gt;Server currently offline&lt;/p&gt;
    &lt;p&gt; Thank you for visiting my portfolio! &lt;lb/&gt;If you are curious about the stack and how I built it, hereâs everything you need to know. &lt;/p&gt;
    &lt;p&gt; Three.js is the library Iâm using to render this 3D world. &lt;lb/&gt;It was created by mr.doob (X, GitHub), followed by hundreds of awesome developers, one of which being Sunag (X, GitHub) who added TSL, enabling the use of both WebGL and WebGPU, making this portfolio possible. &lt;/p&gt;
    &lt;p&gt; If you want to learn Three.js, I got you covered with this huge course. &lt;lb/&gt;It contains everything you need to start building awesome stuff with Three.js (and much more). &lt;/p&gt;
    &lt;p&gt; Iâve been making devlogs since the very start of this portfolio and you can find them on my Youtube channel. &lt;lb/&gt;Even though the portfolio is out, Iâm still working on the last videos so that the series is complete. &lt;/p&gt;
    &lt;p&gt; The code is available on GitHub under MIT license. Even the Blender files are there, so have fun! &lt;lb/&gt;For security reasons, Iâm not sharing the server code, but the portfolio works without it. &lt;/p&gt;
    &lt;p&gt; The music you hear was made especially for this portfolio by the awesome Kounine (Linktree). &lt;lb/&gt;They are now under CC0 license, meaning you can do whatever you want with them! &lt;lb/&gt;Download them here. &lt;/p&gt;
    &lt;p&gt;â Bruno&lt;/p&gt;
    &lt;p&gt;Server currently offline. Scores can't be saved.&lt;/p&gt;
    &lt;p&gt;Come hang out with the community, show us your projects and ask us anything.&lt;/p&gt;
    &lt;p&gt;Contact me directly.&lt;lb/&gt;I have to warn you, I try to answer everyone, but it might take a while.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bruno-simon.com/"/><published>2025-12-09T16:06:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46207425</id><title>Donating the Model Context Protocol and establishing the Agentic AI Foundation</title><updated>2025-12-10T16:49:08.451833+00:00</updated><content>&lt;doc fingerprint="fd5e72507a8d8692"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Donating the Model Context Protocol and establishing the Agentic AI Foundation&lt;/head&gt;
    &lt;p&gt;Today, we’re donating the Model Context Protocol (MCP) to the Agentic AI Foundation (AAIF), a directed fund under the Linux Foundation, co-founded by Anthropic, Block and OpenAI, with support from Google, Microsoft, Amazon Web Services (AWS), Cloudflare, and Bloomberg.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Context Protocol&lt;/head&gt;
    &lt;p&gt;One year ago, we introduced MCP as a universal, open standard for connecting AI applications to external systems. Since then, MCP has achieved incredible adoption:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Across the ecosystem: There are now more than 10,000 active public MCP servers, covering everything from developer tools to Fortune 500 deployments;&lt;/item&gt;
      &lt;item&gt;Across platforms: MCP has been adopted by ChatGPT, Cursor, Gemini, Microsoft Copilot, Visual Studio Code, and other popular AI products;&lt;/item&gt;
      &lt;item&gt;Across infrastructure: Enterprise-grade infrastructure now exists with deployment support for MCP from providers including AWS, Cloudflare, Google Cloud, and Microsoft Azure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt;We’re continuing to invest in MCP’s growth. Claude now has a directory with over 75 connectors (powered by MCP), and we recently launched Tool Search and Programmatic Tool Calling capabilities in our API to help optimize production-scale MCP deployments, handling thousands of tools efficiently and reducing latency in complex agent workflows.&lt;lb/&gt;MCP now has an official, community-driven Registry for discovering available MCP servers, and the November 25th spec release introduced many new features, including asynchronous operations, statelessness, server identity, and official extensions. There are also official SDKs (Software Development Kits) for MCP in all major programming languages with 97M+ monthly SDK downloads across Python and TypeScript. &lt;lb/&gt;Since its inception, we’ve been committed to ensuring MCP remains open-source, community-driven and vendor-neutral. Today, we further that commitment by donating MCP to the Linux Foundation.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Linux Foundation and the Agentic AI Foundation&lt;/head&gt;
    &lt;p&gt;The Linux Foundation is a non-profit organization dedicated to fostering the growth of sustainable, open-source ecosystems through neutral stewardship, community building, and shared infrastructure. It has decades of experience stewarding the most critical and globally-significant open-source projects, including The Linux Kernel, Kubernetes, Node.js, and PyTorch. Importantly, the Linux Foundation has a proven track record in facilitating open collaboration and maintaining vendor neutrality.&lt;/p&gt;
    &lt;p&gt;The Agentic AI Foundation (AAIF) is a directed fund under the Linux Foundation co-founded by Anthropic, Block and OpenAI, with support from Google, Microsoft, AWS, Cloudflare and Bloomberg. The AAIF aims to ensure agentic AI evolves transparently, collaboratively, and in the public interest through strategic investment, community building, and shared development of open standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;Donating the Model Context Protocol&lt;/head&gt;
    &lt;p&gt;Anthropic is donating the Model Context Protocol to the Linux Foundation's new Agentic AI Foundation, where it will join goose by Block and AGENTS.md by OpenAI as founding projects. Bringing these and future projects under the AAIF will foster innovation across the agentic AI ecosystem and ensure these foundational technologies remain neutral, open, and community-driven. &lt;lb/&gt;The Model Context Protocol’s governance model will remain unchanged: the project’s maintainers will continue to prioritize community input and transparent decision-making.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future of MCP&lt;/head&gt;
    &lt;p&gt;Open-source software is essential for building a secure and innovative ecosystem for agentic AI. Today’s donation to the Linux Foundation demonstrates our commitment to ensuring MCP remains a neutral, open standard. We’re excited to continue contributing to MCP and other agentic AI projects through the AAIF.&lt;lb/&gt;Learn more about MCP at modelcontextprotocol.io and get involved with the AAIF here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation"/><published>2025-12-09T17:05:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46207464</id><title>PeerTube is recognized as a digital public good by Digital Public Goods Alliance</title><updated>2025-12-10T16:49:08.116496+00:00</updated><content>&lt;doc fingerprint="95773f811edde224"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PeerTube&lt;/head&gt;
    &lt;p&gt;Verified DPG&lt;/p&gt;
    &lt;head rend="h3"&gt;Owner&lt;/head&gt;
    &lt;p&gt;Framasoft&lt;/p&gt;
    &lt;head rend="h3"&gt;Type&lt;/head&gt;
    &lt;p&gt;backend, mobile, web&lt;/p&gt;
    &lt;head rend="h3"&gt;Licence&lt;/head&gt;
    &lt;p&gt;AGPL-3.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Last evaluated&lt;/head&gt;
    &lt;p&gt;07.10.2025&lt;/p&gt;
    &lt;head rend="h3"&gt;Origin country&lt;/head&gt;
    &lt;p&gt;France&lt;/p&gt;
    &lt;head rend="h3"&gt;Release date&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;head rend="h3"&gt;DPG since&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;head rend="h3"&gt;Description&lt;/head&gt;
    &lt;p&gt;PeerTube is a tool for hosting, managing, and sharing videos or live streams.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core Components Assessed/Included Repositories&lt;/head&gt;
    &lt;p&gt;The following repositories were submitted by the solution and included in our evaluation. Any repositories, add-ons, features not included in here were not reviewed by us.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feature&lt;/head&gt;
    &lt;head rend="h3"&gt;Scale of the Solution*&lt;/head&gt;
    &lt;head rend="h3"&gt;Connected members&lt;/head&gt;
    &lt;p&gt;N/A&lt;/p&gt;
    &lt;head rend="h3"&gt;Participated Programs&lt;/head&gt;
    &lt;p&gt;N/A&lt;/p&gt;
    &lt;head rend="h3"&gt;Available Languages&lt;/head&gt;
    &lt;p&gt;Esperanto, English, Slovenčina, Gàidhlig, العربية, Norsk, Magyar, Deutsch, Toki Pona, Euskara, Polski, Português (Portugal), Suomi, Tiếng Việt, Italiano, فارسی, Español, Taqbaylit, 简体中文（中国）, Hrvatski, ελληνικά, Occitan, украї́нська мо́ва, Français, ไทย, Türkçe, 繁體中文（台灣）, 日本語, Galego, Íslenska, Svenska, Nederlands, Pусский, bokmål, Čeština, Shqip, Català, Português (Brasil), Norsk nynorsk&lt;/p&gt;
    &lt;head rend="h3"&gt;Organisations using it&lt;/head&gt;
    &lt;p&gt;French Ministry of National Education (~100K videos), Italy’s National Research Council, a few French alternative media, the Weißensee Kunsthochschule in Berlin, as well as the Universität der Künste in the same city, a few universities worldwide, the Blender and Debian projects, and various activist groups&lt;/p&gt;
    &lt;p&gt;* This information is self-reported and updated annually&lt;/p&gt;
    &lt;head rend="h3"&gt;Github insights&lt;/head&gt;
    &lt;p&gt;Learn how this product has met the requirements of the DPG Standard by exploring the indicators below.&lt;/p&gt;
    &lt;head rend="h3"&gt;Application Details&lt;/head&gt;
    &lt;head rend="h4"&gt;DPG ID&lt;/head&gt;
    &lt;head rend="h4"&gt;GID0092472&lt;/head&gt;
    &lt;head rend="h4"&gt;Status&lt;/head&gt;
    &lt;head rend="h4"&gt;DPG&lt;/head&gt;
    &lt;head rend="h4"&gt;Date Created&lt;/head&gt;
    &lt;head rend="h4"&gt;2025-08-11&lt;/head&gt;
    &lt;head rend="h4"&gt;Date Submitted&lt;/head&gt;
    &lt;head rend="h4"&gt;2025-08-25&lt;/head&gt;
    &lt;head rend="h4"&gt;Date Reviewed&lt;/head&gt;
    &lt;head rend="h4"&gt;2025-10-07&lt;/head&gt;
    &lt;head rend="h4"&gt;Date of Expiry&lt;/head&gt;
    &lt;head rend="h4"&gt;2026-10-07&lt;/head&gt;
    &lt;head rend="h3"&gt;Application Log Details&lt;/head&gt;
    &lt;head rend="h4"&gt;Timestamp&lt;/head&gt;
    &lt;head rend="h4"&gt;Activity&lt;/head&gt;
    &lt;p&gt;2025-10-07 08:40:13&lt;/p&gt;
    &lt;p&gt;Ricardo Torres (L2 Reviewer) submitted their review of PeerTube (152) and found it to be a DPG&lt;/p&gt;
    &lt;p&gt;2025-10-07 08:40:12&lt;/p&gt;
    &lt;p&gt;System unmarked PeerTube (12958) as a nominee&lt;/p&gt;
    &lt;p&gt;2025-10-07 08:40:07&lt;/p&gt;
    &lt;p&gt;Ricardo Torres (L2 Reviewer) passed 4. Platform Independence for PeerTube (12958)&lt;/p&gt;
    &lt;p&gt;2025-10-07 08:40:02&lt;/p&gt;
    &lt;p&gt;Ricardo Torres (L2 Reviewer) moved PeerTube (12958) to under review&lt;/p&gt;
    &lt;p&gt;2025-10-07 08:38:21&lt;/p&gt;
    &lt;p&gt;Ricardo Torres (L2 Reviewer) finished consultation on 4. Platform Independence for PeerTube (12958)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.digitalpublicgoods.net/r/peertube"/><published>2025-12-09T17:08:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46207505</id><title>If you're going to vibe code, why not do it in C?</title><updated>2025-12-10T16:49:07.685832+00:00</updated><content>&lt;doc fingerprint="af2319bf33f607dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If You’re Going to Vibe Code, Why Not Do It in C?&lt;/head&gt;
    &lt;p&gt;Stephen Ramsay&lt;/p&gt;
    &lt;p&gt;Or hell, why not do it in x86 assembly?&lt;/p&gt;
    &lt;p&gt;Let’s get a few things out of the way before I go any further with this seemingly impertinent thought, because it’s nowhere near as snarky as it sounds.&lt;/p&gt;
    &lt;p&gt;First, I don’t particularly like vibe coding. I love programming, and I have loved it since I made my first tentative steps with it sometime back in the mid-to-late 90s. I love programming so much, it always feels like I’m having too much fun for it to count as real work. I’ve done it professionally, but I also do it as a hobby. Someone apparently once said, “Do what you love and you’ll never work a day in your life.” That’s how I feel about writing code. I’ve also been teaching the subject for twenty-five years, and I can honestly say I am as excited about the first day of the semester now as I was when I first started. I realize it’s a bit precious to say so, but I’ll say it anyway: Turning non-programmers into programmers is my life’s work. It is the thing of which I am most proud as a college professor.&lt;/p&gt;
    &lt;p&gt;Vibe coding makes me feel dirty in ways that I struggle to articulate precisely. It’s not just that it feels like “cheating” (though it does). I also think it takes a lot of the fun out of the whole thing. I sometimes tell people (like the aforementioned students) that programming is like doing the best crossword puzzle in the world, except that when you solve it, it actually dances and sings. Vibe coding robs me of that moment, because I don’t feel like I really did it at all. And even though to be a programmer is to live with a more-or-less permanent set of aporias (you don’t really understand what the compiler is doing, really—and even if you do, you probably don’t really understand how the virtual memory subsystem works, really), it’s satisfying to understand every inch of my code and frustrating—all the way to the borderlands of active anxiety—not quite understanding what Claude just wrote.&lt;/p&gt;
    &lt;p&gt;But this leads me to my second point, which I must make as clearly and forcefully as I can. Vibe coding actually works. It creates robust, complex systems that work. You can tell yourself (as I did) that it can’t possibly do that, but you are wrong. You can then tell yourself (as I did) that it’s good as a kind of alternative search engine for coding problems, but not much else. You are also wrong about that. Because when you start giving it little programming problems that you can’t be arsed to work out yourself (as I did), you discover (as I did) that it’s awfully good at those. And then one day you muse out loud (as I did) to an AI model something like, “I have an idea for a program…” And you are astounded. If you aren’t astounded, you either haven’t actually done it or you are at some stage of grief prior to acceptance. Perfect? Hardly. But then neither are human coders. The future? I think the questions answers itself.&lt;/p&gt;
    &lt;p&gt;But to get to my impertinent question…&lt;/p&gt;
    &lt;p&gt;Early on in my love affair with programming, I read Structure and Interpretation of Computer Programs, which I now consider one of the great pedagogical masterpieces of the twentieth century. I learned a great deal about programming from that book, but among the most memorable lessons was one that appears in the second paragraph of the original preface. There, Hal Abelson and Gerald Sussman make a point that hits with the force of the obvious, and yet is very often forgotten:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[W]e want to establish the idea that a computer language is not just a way of getting a computer to perform operations but rather that it is a novel formal medium for expressing ideas about methodology. Thus, programs must be written for people to read, and only incidentally for machines to execute.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I’ve been repeating some version of this to my students ever since. Computers, I remind them, do not need the code to be “readable” or “ergonomic” for humans; they only need it to be readable and ergonomic for a computer, which is a considerably lower bar.&lt;/p&gt;
    &lt;p&gt;Every programming language—including assembly language—was and is intended for the convenience of humans who need to read it and write it. If a language is innovative, it is usually not because it has allowed for automatic memory management, or concurrency, or safety, or robust error checking, but because it has made it easier for humans to express and reason about these matters. When we extol the virtues of this or that language—Rust’s safety guarantees, C++’s “no-cost abstractions,” or Go’s approach to concurrency—we are not talking about an affordance that the computer has gained, but about an affordance that we have gained as programmers of said computer. From our standpoint as programmers, object-oriented languages offer certain ways to organize our code—and, I think Abelson and Sussman would say, our thinking—that are potentially conducive to the noble treasures of maintainability, extensibility, error checking, and any number of other condign matters. From the standpoint of the computer, this little OO kink of ours seems mostly to indicate a strange affinity for heap memory. “Whatevs!” (says the computer). And pick your poison here, folks: functional programming, algebraic data types, dependent types, homoiconicity, immutable data structures, brace styles… We can debate the utility of these things, but we must understand that we are primarily talking about human problems. The set of “machine problems” to which these matters correspond is considerably smaller.&lt;/p&gt;
    &lt;p&gt;So my question is this: Why vibe code with a language that has human convenience and ergonomics in view? Or to put that another way: Wouldn’t a language designed for vibe coding naturally dispense with much of what is convenient and ergonomic for humans in favor of what is convenient and ergonomic for machines? Why not have it just write C? Or hell, why not x86 assembly?&lt;/p&gt;
    &lt;p&gt;Now, at this point, you will want to say that the need for human understanding isn’t erased entirely thereby. Some version of this argument has merit, but I would remind you that if you are really vibe coding for real you already don’t understand a great deal of what it is producing. But if you look carefully, you will notice that it doesn’t struggle with undefined behavior in C. Or with making sure that all memory is properly freed. Or with off-by-one errors. It sometimes struggles to understand what it is that you actually want, but it rarely struggles with the actual execution of the code. It’s better than you are at keeping track of those things in the same way that a compiler is better at optimizing code than you are. Perfect? No. But as I said before…&lt;/p&gt;
    &lt;p&gt;Is C the ideal language for vibe coding? I think I could mount an argument for why it is not, but surely Rust is even less ideal. To say nothing of Haskell, or OCaml, or even Python. All of these languages, after all, are for people to read, and only incidentally for machines to execute. They are practically adorable in their concern for problems that AI models do not have.&lt;/p&gt;
    &lt;p&gt;I suppose what I’m getting at, here, is that if vibe coding is the future of software development (and it is), then why bother with languages that were designed for people who are not vibe coding? Shouldn’t there be such a thing as a “vibe-oriented programming language?” VOP. You read it here first.&lt;/p&gt;
    &lt;p&gt;One possibility is that such a language truly would be executable pseudocode beyond even the most extravagant fever dreams of the most earnest Pythonistas; it shows you what it’s doing in truly pseudo code, but all the while it’s writing assembly. Or perhaps it’s something like the apotheosis of literate programming. You write a literary document “expressing ideas about methodology,” and the AI produces machine code (and a kind of literary critical practice evolves around this activity, eventually ordering itself into structuralist and post-structuralist camps. But I’m getting ahead of myself). Perhaps your job as a programmer is mostly running tests that verify this machine code (tests which have also been produced by AI). Or maybe a VOPL is really a certain kind of language that comes closer to natural language than any existing programming language, but which has a certain (easily learned) set of idioms and expressions that guide the AI more reliably and more quickly toward particular solutions. It doesn’t have goroutines. It has a “concurrency slang.”&lt;/p&gt;
    &lt;p&gt;Now obviously, the reason a large language model focused on coding is good at Javascript and C++ is precisely because it has been trained on billions of lines of code in those languages along with countless forum posts, StackOverflow debates, and so on. Bootstrapping a VOPL presents a certain kind of difficulty, but then one also suspects that LLMs are already being trained in some future version of this language, because so many programmers are already groping their way toward a system like this by virtue of the fact that so many of them are already vibe coding production-level systems.&lt;/p&gt;
    &lt;p&gt;I don’t know how I feel about all of this (see my first and second points above). It saddens me to think of “coding by hand” becoming a kind of quaint Montessori-school stage in the education of a vibe coder—something like the contour drawings we demand from future photoshopers or the balanced equations we insist serve as a rite of passage for people who will never be without a calculator to the end of their days.&lt;/p&gt;
    &lt;p&gt;At the same time, there is something exciting about the birth of a computational paradigm. It wasn’t that long ago, in the grand scheme of things, that someone realized that rewiring the entire machine every time you wanted to do a calculation (think ENIAC, circa 1945) was a rather suboptimal way to do things. And it is worth recalling that people complained when the stored-program computer rolled around (think EDVAC, circa 1951). Why? Well, the answer should be obvious. It was less reliable. It was slower. It removed the operator from the loop. It threatened specialized labor. It was conceptually impure. I’m not kidding about any of this. No less an authority than Grace Hopper had to argue against the quite popular idea that there was no way anyone could ever trust a machine to write instructions for another machine.&lt;/p&gt;
    &lt;p&gt;Same vibe, as the kids say.&lt;/p&gt;
    &lt;p&gt;Keywords: programming, AI&lt;/p&gt;
    &lt;p&gt;Last Modified: 2025-12-07T16:29:42:-0600&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stephenramsay.net/posts/vibe-coding.html"/><published>2025-12-09T17:11:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46208773</id><title>So you want to speak at software conferences?</title><updated>2025-12-10T16:49:07.522994+00:00</updated><content>&lt;doc fingerprint="6278511b9fc389bd"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;So You Want To Speak At Software Conferences?&lt;/head&gt;Posted by Dylan Beattie on 08 December 2025 • permalink&lt;p&gt;I run a .NET user group here in London, and we host a lot of talks from people who are relatively inexperienced presenters. Sometimes they’ve done presentations internally but never spoken before a public audience. Sometimes they’re developers who have been in theatre or played in bands; people with plenty of stage experience but who haven’t presented on technical topics before - and sometimes they’ve never done any kind of public presentations or performance at all. We aim to be a friendly, supportive crowd; public speaking can be daunting, and the first public outing of somebody’s first talk can be… let’s just say that the presenter sometimes learns a lot more than the audience, and leave it at that.&lt;/p&gt;&lt;p&gt;But it can also be a hugely rewarding experience, and as a seasoned tech presenter who’s been doing this for a while, aspiring speakers often ask me for advice on how to take it to the next level.&lt;/p&gt;&lt;p&gt;Before we get into the specifics, there are two things to bear in mind.&lt;/p&gt;&lt;p&gt;One: ask yourself why you want to do this. What does “the next level” mean for you? Are you looking to promote your consultancy, or your training courses, or your software products? Do you want to become a professional speaker and actually get paid to give talks? Are you doing it ‘cos you want to go places and meet people? Figure out what “success” looks like for you.&lt;/p&gt;&lt;p&gt;Two: be realistic about how much work is involved. It took me seven years to go from my first user group lightning talk, back 2008, to my first international conference. If you think you can hack together some code, write a talk about it, stick it on Sessionize and three months later you’re on your way to a major international event like NDC or Yow! or Devoxx… well, no. That’s not how this works. Strap in; it’s a long ride.&lt;/p&gt;&lt;head rend="h3"&gt;Year 1: Get Good&lt;/head&gt;&lt;p&gt;Write the talk. Write a talk nobody else could do; tell a story nobody else can tell. Figure out what your audience is going to learn, and why you’re the best person to teach them that. Then give it at local user group. It might go great. It might be a train wreck. Don’t worry. That’s one of the reasons user groups exist. Learn from the experience. Fix the demos. Fix the slides. If it was too short? Write some more. If it was too long? Cut something. Give it at another user group. Do it again. Do it again. Maybe write a second talk, shop that one around a couple of user groups too.&lt;/p&gt;&lt;p&gt;If you can’t find user groups, look on Meetup.com. Yes, it’s a horrible platform, but it works; search by topic, search by region, find groups that look like a good match for your content, and ask if they’re looking for speakers. They probably are.&lt;/p&gt;&lt;head rend="h3"&gt;Year 2: Get Seen&lt;/head&gt;&lt;p&gt;After user groups and meetups come the community conferences. Typically small, one-day events, with a few tracks, and usually free (or very cheap) to attend. For me, these were the DDD events _(that’s DDD as in Developers! Developers! Developers!, not to be confused with DDD as in Domain Driven Design), _a series of one-day free developer events around the UK, organised by volunteers, usually on a Saturday so people don’t have to take time off work. They bring in a good crowd, they’re a great way to get to know other presenters and people who are involved in tech events, and you’ll almost certainly meet a few people who are on the programme committees for the bigger conferences.&lt;/p&gt;&lt;p&gt;Events like this are your chance to get noticed. Turn up the day before, join the pre-conference dinner and drinks, introduce yourself. Yeah, it’s awkward when you don’t know anybody. There will be other people there who don’t know anybody and will appreciate you making the effort. Enjoy yourself, but don’t end up doing tequila shots in a karaoke bar at 3am. Not now. You’re there to give a talk, remember?&lt;/p&gt;&lt;p&gt;Go to the event. Spend the whole day there, do your talk, watch the other sessions. Communicate with the organisers. You don’t want their memorable impression of you to be a half-hour of panic and missed calls because one of their speakers has gone AWOL and nobody knows where they are.&lt;/p&gt;&lt;p&gt;Figure out how to keep in touch with the people you met. Join the Signal or WhatsApp group chat; if there isn’t one, create one. Follow them on LinkedIn, or Bluesky - be prepared to go where people are; don’t expect folks to join Mastodon just because that’s where you want to talk to them. That’s not how this works. If you really don’t want to play the social media game - and I can’t blame you - there’s always good old-fashioned email. A short email a week later saying “hey, thanks for having me” or “hey, I loved your session at DDD, let’s keep in touch” can pay off in a big way.&lt;/p&gt;&lt;p&gt;Finally, watch out for events that put video of their sessions online. Having a couple of YouTube links of you doing your thing in front of a live, appreciate audience can make all the difference when a programme committee is looking at a handful of talks and can only accept one of them.&lt;/p&gt;&lt;head rend="h3"&gt;Year 3: Get Accepted&lt;/head&gt;&lt;p&gt;You’ve got a couple of talks. You’ve delivered then enough times that you know they’re good *(and if they’re not good, make them good - or scrap them and write new ones)*. You know people. People know you. If somebody asks “hey, do we know anybody who could do a good session about $topic”, your name comes up. You’ve got a decent network of connections - group chats, LinkedIn, email addresses.&lt;/p&gt;&lt;p&gt;Now, find all the conferences in your field with an open Call for Papers (CfP), and get submitting. Dave Aronson over at codeasaur.us maintains a really useful list of CfPs which are closing soon. Check that regularly. Many events will cover your travel &amp;amp; hotel costs, although with sponsorship budgets drying up right across the industry that’s not as prevalent as it was a few years ago. If not, maybe you can persuade your employer to pay your travel - “hey, boss, if I can get a free ticket to this amazing conference with all these industry experts, do you think the company will pay my air fare &amp;amp; hotel?”&lt;/p&gt;&lt;p&gt;Lean on your network. What are people submitting to? Which events should you look out for? Which topics are getting a lot of traction (and which topics are not?)&lt;/p&gt;&lt;p&gt;Keep your content fresh. Write new talks. Keep giving them at user groups and community events.&lt;/p&gt;&lt;p&gt;Keep your submissions focused. 2-3 talks per event; don’t submit ten wildly different abstracts to the same conference in the hope one of them will get accepted. Every selection committee I’ve been on, if we see that, we assume the presenter hasn’t actually written *any* of them yet and is throwing everything they can think of into the mix and hoping one of them gets chosen. Not a great way to stand out. An open CFP at a big tech conference typically gets 20+ submissions for every available slot, which means if you reduce it to a numbers game, you’re submitting 20 talks for every one that gets accepted. Keep track of the numbers, and be objective about it.&lt;/p&gt;&lt;head rend="h3"&gt;Year 4: Get Bored.&lt;/head&gt;&lt;p&gt;It’s great fun doing this for a while… but it’s also exhausting. Some people hit it hard for a few years, do all the things, go to all the places, make a lot of great friends and happy memories, and then wake up one day and decide that’s enough. Some people do a few talks, tick it off their bucket list and decide that’s enough for them. Some settle into a gentle routine of 3-4 events they’ll do every year. And yes, some of us end up treating our calendars like a game of Tetris, juggling flights and trains and hotels and meetups and conferences and spending half the year on the road and the other half writing talks and workshops and all the other things it’s hard to do when you’re at the airport.&lt;/p&gt;&lt;p&gt;That’s why you gotta figure out ahead of time what “success” looks like. If you’re doing it for fun, remember to have fun - and if you find you’re not enjoying it any more? Stop. If you’re doing it as promotion or marketing? Track your leads. Make sure it’s actually generating the attention and the revenue it’s supposed to. If you’re doing it for money, be mercenary: no pay, no play. Not every event is the same, of course. In a given year I’ll have some events that are fun, some that are lucrative, some that are running alongside workshops or training engagements. Just make sure you know which is which.&lt;/p&gt;&lt;p&gt;Finally: respect your audience. Whether you’re talking to five people at a meetup, fifty at a community event, or five thousand at a huge international conference: those people are the reason you get to do this. They have given up their time - and often a substantial amount of money - to hear what you have to say. They deserve your best shot, every time. If you find you’re bored, fed up, tired, running talks on autopilot or making mistakes because you just don’t care? It’s time to try something else - and remember, there’s a thousand aspiring speakers out there who would dearly love to take that spot instead of you.&lt;/p&gt;&lt;p&gt;Now get out there. Work hard, have fun, teach us awesome things, and if you ever want me to look over an abstract or a slide deck, drop me a line - [email protected]. I’d be happy to help.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dylanbeattie.net/2025/12/08/so-you-want-to-speak-at-software-conferences.html"/><published>2025-12-09T18:42:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46208962</id><title>10 Years of Let's Encrypt</title><updated>2025-12-10T16:49:07.335641+00:00</updated><content>&lt;doc fingerprint="6d5da1590926b3fa"&gt;
  &lt;main&gt;
    &lt;p&gt;On September 14, 2015, our first publicly-trusted certificate went live. We were proud that we had issued a certificate that a significant majority of clients could accept, and had done it using automated software. Of course, in retrospect this was just the first of billions of certificates. Today, Let’s Encrypt is the largest certificate authority in the world in terms of certificates issued, the ACME protocol we helped create and standardize is integrated throughout the server ecosystem, and we’ve become a household name among system administrators. We’re closing in on protecting one billion web sites.&lt;/p&gt;
    &lt;p&gt;In 2023, we marked the tenth anniversary of the creation of our nonprofit, Internet Security Research Group, which continues to host Let’s Encrypt and other public benefit infrastructure projects. Now, in honor of the tenth anniversary of Let’s Encrypt’s public certificate issuance and the start of the general availability of our services, we’re looking back at a few milestones and factors that contributed to our success.&lt;/p&gt;
    &lt;p&gt;A conspicuous part of Let’s Encrypt’s history is how thoroughly our vision of scalability through automation has succeeded.&lt;/p&gt;
    &lt;p&gt;In March 2016, we issued our one millionth certificate. Just two years later, in September 2018, we were issuing a million certificates every day. In 2020 we reached a billion total certificates issued and as of late 2025 we’re frequently issuing ten million certificates per day. We’re now on track to reach a billion active sites, probably sometime in the coming year. (The “certificates issued” and “certificates active” metrics are quite different because our certificates regularly expire and get replaced.)&lt;/p&gt;
    &lt;p&gt;The steady growth of our issuance volume shows the strength of our architecture, the validity of our vision, and the great efforts of our engineering team to scale up our own infrastructure. It also reminds us of the confidence that the Internet community is placing in us, making the use of a Let’s Encrypt certificate a normal and, dare we say, boring choice. But I often point out that our ever-growing issuance volumes are only an indirect measure of value. What ultimately matters is improving the security of people’s use of the web, which, as far as Let’s Encrypt’s contribution goes, is not measured by issuance volumes so much as by the prevalence of HTTPS encryption. For that reason, we’ve always emphasized the graph of the percentage of encrypted connections that web users make (here represented by statistics from Firefox).&lt;/p&gt;
    &lt;p&gt;(These graphs are snapshots as of the date of this post; a dynamically updated version is found on our stats page.) Our biggest goal was to make a concrete, measurable security impact on the web by getting HTTPS connection prevalence to increase—and it’s worked. It took five years or so to get the global percentage from below 30% to around 80%, where it’s remained ever since. In the U.S. it has been close to 95% for a while now.&lt;/p&gt;
    &lt;p&gt;A good amount of the remaining unencrypted traffic probably comes from internal or private organizational sites (intranets), but other than that we don’t know much about it; this would be a great topic for Internet security researchers to look into.&lt;/p&gt;
    &lt;p&gt;We believe our present growth in certificate issuance volume is essentially coming from growth in the web as a whole. In other words, if we protect 20% more sites over some time period, it’s because the web itself grew by 20%.&lt;/p&gt;
    &lt;p&gt;We’ve blogged about most of Let’s Encrypt’s most significant milestones as they’ve happened, and I invite everyone in our community to look over those blog posts to see how far we’ve come. We’ve also published annual reports for the past seven years, which offer elegant and concise summaries of our work.&lt;/p&gt;
    &lt;p&gt;As I personally think back on the past decade, just a few of the many events that come to mind include:&lt;/p&gt;
    &lt;p&gt;Telling the world about the project in November 2014&lt;/p&gt;
    &lt;p&gt;Our first certificate issuance in September 2015&lt;/p&gt;
    &lt;p&gt;Our one millionth certificate in March 2016, then our 100 millionth certificate in June 2017, and then our billionth certificate in 2020&lt;/p&gt;
    &lt;p&gt;Along the way, first issuing one million certificates in a single day (in September 2018), significantly contributed to by the SquareSpace and Shopify Let’s Encrypt integrations&lt;/p&gt;
    &lt;p&gt;Just at the end of September 2025, we issued more than ten million certificates in a day for the first time.&lt;/p&gt;
    &lt;p&gt;We’ve also periodically rolled out new features such as internationalized domain name support (2016), wildcard support (2018), and short-lived and IP address (2025) certificates. We’re always working on more new features for the future.&lt;/p&gt;
    &lt;p&gt;There are many technical milestones like our database server upgrades in 2021, where we found we needed a serious server infrastructure boost because of the tremendous volumes of data we were dealing with. Similarly, our original infrastructure was using Gigabit Ethernet internally, and, with the growth of our issuance volume and logging, we found that our Gigabit Ethernet network eventually became too slow to synchronize database instances! (Today we’re using 25-gig Ethernet.) More recently, we’ve experimented with architectural upgrades to our ever-growing Certificate Transparency logs, and decided to go ahead with deploying those upgrades—to help us not just keep up with, but get ahead of, our continuing growth.&lt;/p&gt;
    &lt;p&gt;These kinds of growing pains and successful responses to them are nice to remember because they point to the inexorable increase in demands on our infrastructure as we’ve become a more and more essential part of the Internet. I’m proud of our technical teams which have handled those increased demands capably and professionally.&lt;/p&gt;
    &lt;p&gt;I also recall the ongoing work involved in making sure our certificates would be as widely accepted as possible, which has meant managing the original cross-signature from IdenTrust, and subsequently creating and propagating our own root CA certificates. This process has required PKI engineering, key ceremonies, root program interactions, documentation, and community support associated with certificate migrations. Most users never have reason to look behind the scenes at our chains of trust, but our engineers update it as root and intermediate certificates have been replaced. We’ve engaged at the CA/B Forum, IETF, and in other venues with the browser root programs to help shape the web PKI as a technical leader.&lt;/p&gt;
    &lt;p&gt;As I wrote in 2020, our ideal of complete automation of the web PKI aims at a world where most site owners wouldn’t even need to think about certificates at all. We continue to get closer and closer to that world, which creates a risk that people will take us and our services for granted, as the details of certificate renewal occupy less of site operators’ mental energy. As I said at the time,&lt;/p&gt;
    &lt;p&gt;When your strategy as a nonprofit is to get out of the way, to offer services that people don’t need to think about, you’re running a real risk that you’ll eventually be taken for granted. There is a tension between wanting your work to be invisible and the need for recognition of its value. If people aren’t aware of how valuable our services are then we may not get the support we need to continue providing them.&lt;/p&gt;
    &lt;p&gt;I’m also grateful to our communications and fundraising staff who help make clear what we’re doing every day and how we’re making the Internet safer.&lt;/p&gt;
    &lt;p&gt;Our community continually recognizes our work in tangible ways by using our certificates—now by the tens of millions per day—and by sponsoring us.&lt;/p&gt;
    &lt;p&gt;We were honored to be recognized with awards including the 2022 Levchin Prize for Real-World Cryptography and the 2019 O’Reilly Open Source Award. In October of this year some of the individuals who got Let’s Encrypt started were honored to receive the IEEE Cybersecurity Award for Practice.&lt;/p&gt;
    &lt;p&gt;We documented the history, design, and goals of the project in an academic paper at the ACM CCS ‘19 conference, which has subsequently been cited hundreds of times in academic research.&lt;/p&gt;
    &lt;p&gt;Ten years later, I’m still deeply grateful to the five initial sponsors that got Let’s Encrypt off the ground - Mozilla, EFF, Cisco, Akamai, and IdenTrust. When they committed significant resources to the project, it was just an ambitious idea. They saw the potential and believed in our team, and because of that we were able to build the service we operate today.&lt;/p&gt;
    &lt;p&gt;I’d like to particularly recognize IdenTrust, a PKI company that worked as a partner from the outset and enabled us to issue publicly-trusted certificates via a cross-signature from one of their roots. We would simply not have been able to launch our publicly-trusted certificate service without them. Back when I first told them that we were starting a new nonprofit certificate authority that would give away millions of certificates for free, there wasn’t any precedent for this arrangement, and there wasn’t necessarily much reason for IdenTrust to pay attention to our proposal. But the company really understood what we were trying to do and was willing to engage from the beginning. Ultimately, IdenTrust’s support made our original issuance model a reality.&lt;/p&gt;
    &lt;p&gt;I’m proud of what we have achieved with our staff, partners, and donors over the past ten years. I hope to be even more proud of the next ten years, as we use our strong footing to continue to pursue our mission to protect Internet users by lowering monetary, technological, and informational barriers to a more secure and privacy-respecting Internet.&lt;/p&gt;
    &lt;p&gt;Let’s Encrypt is a project of the nonprofit Internet Security Research Group, a 501(c)(3) nonprofit. You can help us make the next ten years great as well by donating or becoming a sponsor.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://letsencrypt.org/2025/12/09/10-years"/><published>2025-12-09T18:54:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46210240</id><title>Django: what’s new in 6.0</title><updated>2025-12-10T16:49:06.947882+00:00</updated><content>&lt;doc fingerprint="babb1d3d4db5baca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Django: whatâs new in 6.0&lt;/head&gt;
    &lt;p&gt;Django 6.0 was released today, starting another release cycle for the loved and long-lived Python web framework (now 20 years old!). It comes with a mosaic of new features, contributed to by many, some of which I am happy to have helped with. Below is my pick of highlights from the release notes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Upgrade with help from django-upgrade&lt;/head&gt;
    &lt;p&gt;If youâre upgrading a project from Django 5.2 or earlier, please try my tool django-upgrade. It will automatically update old Django code to use new features, fixing some deprecation warnings for you, including five fixers for Django 6.0. (One day, Iâll propose django-upgrade to become an official Django project, when energy and time permitâ¦)&lt;/p&gt;
    &lt;head rend="h2"&gt;Template partials&lt;/head&gt;
    &lt;p&gt;There are four headline features in Django 6.0, which weâll cover before other notable changes, starting with this one:&lt;/p&gt;
    &lt;quote&gt;The Django Template Language now supports template partials, making it easier to encapsulate and reuse small named fragments within a template file.&lt;/quote&gt;
    &lt;p&gt;Partials are sections of a template marked by the new &lt;code&gt;{% partialdef %}&lt;/code&gt; and &lt;code&gt;{% endpartialdef %}&lt;/code&gt; tags. They can be reused within the same template or rendered in isolation. Letâs look at examples for each use case in turn.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reuse partials within the same template&lt;/head&gt;
    &lt;p&gt;The below template reuses a partial called &lt;code&gt;filter_controls&lt;/code&gt; within the same template. Itâs defined once at the top of the template, then used twice later on. Using a partial allows the template avoid repetition without pushing the content into a separate include file.&lt;/p&gt;
    &lt;code&gt;&amp;lt;section id=videos&amp;gt;
  {% partialdef filter_controls %}
    &amp;lt;form&amp;gt;
      {{ filter_form }}
    &amp;lt;/form&amp;gt;
  {% endpartialdef %}

  {% partial filter_controls %}

  &amp;lt;ul&amp;gt;
    {% for video in videos %}
      &amp;lt;li&amp;gt;
        &amp;lt;h2&amp;gt;{{ video.title }}&amp;lt;/h2&amp;gt;
        ...
      &amp;lt;/li&amp;gt;
    {% endfor %}
  &amp;lt;/ul&amp;gt;

  {% partial filter_controls %}
&amp;lt;/section&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Actually, we can simplify this pattern further, by using the &lt;code&gt;inline&lt;/code&gt; option on the &lt;code&gt;partialdef&lt;/code&gt; tag, which causes the definition to also render in place:&lt;/p&gt;
    &lt;code&gt;&amp;lt;section id=videos&amp;gt;
  {% partialdef filter_controls inline %}
    &amp;lt;form&amp;gt;
      {{ filter_form }}
    &amp;lt;/form&amp;gt;
  {% endpartialdef %}

  &amp;lt;ul&amp;gt;
    {% for video in videos %}
      &amp;lt;li&amp;gt;
        &amp;lt;h2&amp;gt;{{ video.title }}&amp;lt;/h2&amp;gt;
        ...
      &amp;lt;/li&amp;gt;
    {% endfor %}
  &amp;lt;/ul&amp;gt;

  {% partial filter_controls %}
&amp;lt;/section&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Reach for this pattern any time you find yourself repeating template code within the same template. Because partials can use variables, you can also use them to de-duplicate when rendering similar controls with different data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Render partials in isolation&lt;/head&gt;
    &lt;p&gt;The below template defines a &lt;code&gt;view_count&lt;/code&gt; partial thatâs intended to be re-rendered in isolation. It uses the &lt;code&gt;inline&lt;/code&gt; option, so when the whole template is rendered, the partial is included.&lt;/p&gt;
    &lt;p&gt;The page uses htmx, via my django-htmx package, to periodically refresh the view count, through the &lt;code&gt;hx-*&lt;/code&gt; attributes. The request from htmx goes to a dedicated view that re-renders the &lt;code&gt;view_count&lt;/code&gt; partial.&lt;/p&gt;
    &lt;code&gt;{% load django_htmx %}
&amp;lt;!doctype html&amp;gt;
&amp;lt;html&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;h1&amp;gt;{{ video.title }}&amp;lt;/h1&amp;gt;
    &amp;lt;video width=1280 height=720 controls&amp;gt;
      &amp;lt;source src="{{ video.file.url }}" type="video/mp4"&amp;gt;
      Your browser does not support the video tag.
    &amp;lt;/video&amp;gt;

    {% partialdef view_count inline %}
    &amp;lt;section
      class=view-count
      hx-trigger="every 1s"
      hx-swap=outerHTML
      hx-get="{% url 'video-view-count' video.id %}"
    &amp;gt;
      {{ video.view_count }} views
    &amp;lt;/section&amp;gt;
    {% endpartialdef %}

    {% htmx_script %}
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The relevant code for the two views could look like this:&lt;/p&gt;
    &lt;code&gt;from django.shortcuts import render


def video(request, video_id):
    ...
    return render(request, "video.html", {"video": video})


def video_view_count(request, video_id):
    ...
    return render(request, "video.html#view_count", {"video": video})
&lt;/code&gt;
    &lt;p&gt;The initial &lt;code&gt;video&lt;/code&gt; view renders the full template &lt;code&gt;video.html&lt;/code&gt;. The &lt;code&gt;video_view_count&lt;/code&gt; view renders just the &lt;code&gt;view_count&lt;/code&gt; partial, by appending &lt;code&gt;#view_count&lt;/code&gt; to the template name. This syntax is similar to how youâd reference an HTML fragment by its ID in a URL.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;htmx was the main motivation for this feature, as promoted by htmx creator Carson Gross in a cross-framework review post. Using partials definitely helps maintain âLocality of behaviourâ within your templates, easing authoring, debugging, and maintenance by avoiding template file sprawl.&lt;/p&gt;
    &lt;p&gt;Djangoâs support for template partials was initially developed by Carlton Gibson in the django-template-partials package, which remains available for older Django versions. The integration into Django itself was done in a Google Summer of Code project this year, worked on by student Farhan Ali and mentored by Carlton, in Ticket #36410. You can read more about the development process in Farhanâs retrospective blog post. Many thanks to Farhan for authoring, Carlton for mentoring, and Natalia Bidart, Nick Pope, and Sarah Boyce for reviewing!&lt;/p&gt;
    &lt;head rend="h2"&gt;Tasks framework&lt;/head&gt;
    &lt;p&gt;The next headline feature weâre covering:&lt;/p&gt;
    &lt;quote&gt;Django now includes a built-in Tasks framework for running code outside the HTTP requestâresponse cycle. This enables offloading work, such as sending emails or processing data, to background workers.&lt;/quote&gt;
    &lt;p&gt;Basically, thereâs a new API for defining and enqueuing background tasksâvery cool!&lt;/p&gt;
    &lt;p&gt;Background tasks are a way of running code outside of the request-response cycle. Theyâre a common requirement in web applications, used for sending emails, processing images, generating reports, and more.&lt;/p&gt;
    &lt;p&gt;Historically, Django has not provided any system for background tasks, and kind of ignored the problem space altogether. Developers have instead relied on third-party packages like Celery or Django Q2. While these systems are fine, they can be complex to set up and maintain, and often donât âgo with the grainâ of Django.&lt;/p&gt;
    &lt;p&gt;The new Tasks framework fills this gap by providing an interface to define background tasks, which task runner packages can then integrate with. This common ground allows third-party Django packages to define tasks in a standard way, assuming youâll be using a compatible task runner to execute them.&lt;/p&gt;
    &lt;p&gt;Define tasks with the new &lt;code&gt;@task&lt;/code&gt; decorator:&lt;/p&gt;
    &lt;code&gt;from django.tasks import task


@task
def resize_video(video_id): ...
&lt;/code&gt;
    &lt;p&gt;â¦and enqueue them for background execution with the &lt;code&gt;Task.enqueue()&lt;/code&gt; method:&lt;/p&gt;
    &lt;code&gt;from example.tasks import resize_video


def upload_video(request):
    ...
    resize_video.enqueue(video.id)
    ...
&lt;/code&gt;
    &lt;head rend="h3"&gt;Execute tasks&lt;/head&gt;
    &lt;p&gt;At this time, Django does not include a production-ready task backend, only two that are suitable for development and testing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ImmediateBackend&lt;/code&gt;- runs tasks synchronously, blocking until they complete.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DummyBackend&lt;/code&gt;- does nothing when tasks are enqueued, but allows them to be inspected later. Useful for tests, where you can assert that tasks were enqueued without actually running them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For production use, youâll need to use a third-party package that implements one, for which django-tasks, the reference implementation, is the primary option. It provides &lt;code&gt;DatabaseBackend&lt;/code&gt; for storing tasks in your SQL database, a fine solution for many projects, avoiding extra infrastructure and allowing atomic task enqueuing within database transactions. We may see this backend merged into Django in due course, or at least become an official package, to help make Django âbatteries includedâ for background tasks.&lt;/p&gt;
    &lt;p&gt;To use django-tasksâ &lt;code&gt;DatabaseBackend&lt;/code&gt; today, first install the package:&lt;/p&gt;
    &lt;code&gt;uv add django-tasks
&lt;/code&gt;
    &lt;p&gt;Second, add these two apps to your &lt;code&gt;INSTALLED_APPS&lt;/code&gt; setting:&lt;/p&gt;
    &lt;code&gt;INSTALLED_APPS = [
    # ...
    "django_tasks",
    "django_tasks.backends.database",
    # ...
]
&lt;/code&gt;
    &lt;p&gt;Third, configure &lt;code&gt;DatabaseBackend&lt;/code&gt; as your tasks backend in the new &lt;code&gt;TASKS&lt;/code&gt; setting:&lt;/p&gt;
    &lt;code&gt;TASKS = {
    "default": {
        "BACKEND": "django_tasks.backends.database.DatabaseBackend",
    },
}
&lt;/code&gt;
    &lt;p&gt;Fourth, run migrations to create the necessary database tables:&lt;/p&gt;
    &lt;code&gt;$ ./manage.py migrate
&lt;/code&gt;
    &lt;p&gt;Finally, to run the task worker process, use the packageâs &lt;code&gt;db_worker&lt;/code&gt; management command:&lt;/p&gt;
    &lt;code&gt;$ ./manage.py db_worker
Starting worker worker_id=jWLMLrms3C2NcUODYeatsqCFvd5rK6DM queues=default
&lt;/code&gt;
    &lt;p&gt;This process runs indefinitely, polling for tasks and executing them, logging events as it goes:&lt;/p&gt;
    &lt;code&gt;Task id=10b794ed-9b64-4eed-950c-fcc92cd6784b path=example.tasks.echo state=RUNNING
Hello from test task!
Task id=10b794ed-9b64-4eed-950c-fcc92cd6784b path=example.tasks.echo state=SUCCEEDED
&lt;/code&gt;
    &lt;p&gt;Youâll want to run &lt;code&gt;db_worker&lt;/code&gt; in production, and also in development if you want to test background task execution.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;Itâs been a long path to get the Tasks framework into Django, and Iâm super excited to see it finally available in Django 6.0. Jake Howard started on the idea for Wagtail, a Django-powered CMS, back in 2021, as they have a need for common task definitions across their package ecosystem. He upgraded the idea to target Django itself in 2024, when he proposed DEP 0014. As a member of the Steering Council at the time, I had the pleasure of helping review and accept the DEP.&lt;/p&gt;
    &lt;p&gt;Since then, Jake has been leading the implementation effort, building pieces first in the separate django-tasks package before preparing them for inclusion in Django itself. This step was done under Ticket #35859, with a pull request that took nearly a year to review and land. Thanks to Jake for his perseverance here, and to all reviewers: Andreas NÃ¼Ãlein, Dave Gaeddert, Eric Holscher, Jacob Walls, Jake Howard, Kamal Mustafa, @rtr1, @tcely, Oliver Haas, Ran Benita, Raphael Gaschignard, and Sarah Boyce.&lt;/p&gt;
    &lt;p&gt;Read more about this feature and story in Jakeâs post celebrating when it was merged.&lt;/p&gt;
    &lt;head rend="h2"&gt;Content Security Policy support&lt;/head&gt;
    &lt;p&gt;Our third headline feature:&lt;/p&gt;
    &lt;quote&gt;Built-in support for the Content Security Policy (CSP) standard is now available, making it easier to protect web applications against content injection attacks such as cross-site scripting (XSS). CSP allows declaring trusted sources of content by giving browsers strict rules about which scripts, styles, images, or other resources can be loaded.&lt;/quote&gt;
    &lt;p&gt;Iâm really excited about this, because Iâm a bit of a security nerd whoâs been deploying CSP for client projects for years.&lt;/p&gt;
    &lt;p&gt;CSP is a security standard that can protect your site from cross-site scripting (XSS) and other code injection attacks. You set a &lt;code&gt;content-security-policy&lt;/code&gt; header to declare which content sources are trusted for your site, and then browsers will block content from other sources. For example, you might declare that only scripts your domain are allowed, so an attacker who manages to inject a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag pointing to evil.com would be thwarted, as the browser would refuse to load it.&lt;/p&gt;
    &lt;p&gt;Previously, Django had no built-in support for CSP, and developers had to rely on building their own, or using a third-party package like the very popular django-csp. But this was a little bit inconvenient, as it meant that other third-party packages couldnât reliably integrate with CSP, as there was no common API to do so.&lt;/p&gt;
    &lt;p&gt;The new CSP support provides all the core features that django-csp did, with a slightly tidier and more Djangoey API. To get started, first add &lt;code&gt;ContentSecurityPolicyMiddleware&lt;/code&gt; to your &lt;code&gt;MIDDLEWARE&lt;/code&gt; setting:&lt;/p&gt;
    &lt;code&gt;MIDDLEWARE = [
    # ...
    "django.middleware.csp.ContentSecurityPolicyMiddleware",
    # ...
]
&lt;/code&gt;
    &lt;p&gt;Place it next to &lt;code&gt;SecurityMiddleware&lt;/code&gt;, as it similarly adds security-related headers to all responses. (You do have &lt;code&gt;SecurityMiddleware&lt;/code&gt; enabled, right?)&lt;/p&gt;
    &lt;p&gt;Second, configure your CSP policy using the new settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SECURE_CSP&lt;/code&gt;to configure the&lt;code&gt;content-security-policy&lt;/code&gt;header, which is your actively enforced policy.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SECURE_CSP_REPORT_ONLY&lt;/code&gt;to configure the&lt;code&gt;content-security-policy-report-only&lt;/code&gt;header, which sets a non-enforced policy for which browsers report violations to a specified endpoint. This option is useful for testing and monitoring a policy before enforcing it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, to adopt the nonce-based strict CSP recommended by web.dev, you could start with the following setting:&lt;/p&gt;
    &lt;code&gt;from django.utils.csp import CSP

SECURE_CSP_REPORT_ONLY = {
    "script-src": [CSP.NONCE, CSP.STRICT_DYNAMIC],
    "object-src": [CSP.NONE],
    "base-uri": [CSP.NONE],
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;CSP&lt;/code&gt; enum used above provides constants for CSP directives, to help avoid typos.&lt;/p&gt;
    &lt;p&gt;This policy is quite restrictive and will break most existing sites if deployed as-is, because it requires nonces, as covered next. Thatâs why the example shows starting with the report-only mode header, to help track down places that need fixing before enforcing the policy. Youâd later change to setting the &lt;code&gt;SECURE_CSP&lt;/code&gt; setting to enforce the policy.&lt;/p&gt;
    &lt;p&gt;Anyway, those are the two basic steps to set up the new CSP support!&lt;/p&gt;
    &lt;head rend="h3"&gt;Nonce generation&lt;/head&gt;
    &lt;p&gt;A key part of the new feature is that nonce generation is now built-in to Django, when using the CSP middleware. Nonces are a security feature in CSP that allow you to mark specific &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags as trusted with a &lt;code&gt;nonce&lt;/code&gt; attribute:&lt;/p&gt;
    &lt;code&gt;&amp;lt;script src=/static/app.js type=module nonce=55vsH4w7ATHB85C3MbPr_g&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The nonce value is randomly generated per-request, and included in the CSP header. An attacker performing content injection couldnât guess the nonce, so browsers can trust only those tags that include the correct nonce. Because nonce generation is now part of Django, third-party packages can depend on it for their &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags and theyâll continue to work if you adopt CSP with nonces.&lt;/p&gt;
    &lt;p&gt;Nonces are the recommended way to use CSP today, avoiding problems with previous allow-list based approaches. Thatâs why the above recommended policy enables them. To adopt a nonce-based policy, youâll need to annotate your &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags with the nonce value through the following steps.&lt;/p&gt;
    &lt;p&gt;First, add the new &lt;code&gt;csp&lt;/code&gt; template context processor to your &lt;code&gt;TEMPLATES&lt;/code&gt; setting:&lt;/p&gt;
    &lt;code&gt;TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "OPTIONS": {
            "context_processors": [
                # ...
                "django.template.context_processors.csp",
            ],
        },
    },
]
&lt;/code&gt;
    &lt;p&gt;Second, annotate your &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags with &lt;code&gt;nonce="{{ csp_nonce }}"&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;-   &amp;lt;script src="{% static 'app.js' %}" type="module"&amp;gt;&amp;lt;/script&amp;gt;
+   &amp;lt;script src="{% static 'app.js' %}" type="module" nonce="{{ csp_nonce }}"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;
    &lt;p&gt;This can be tedious and error-prone, hence using the report-only mode first to monitor violations might be useful, especially on larger projects.&lt;/p&gt;
    &lt;p&gt;Anyway, deploying CSP right would be another post in itself, or even a book chapter, so weâll stop here for now. For more info, check out that web.dev article and the MDN CSP guide.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;CSP itself was proposed for browsers way back in 2004, and was first implemented in Mozilla Firefox version 4, released 2011. That same year, Django Ticket #15727 was opened, proposing adding CSP support to Django. Mozilla created django-csp from 2010, before the first public availability of CSP, using it on their own Django-powered sites. The first comment on Ticket #15727 pointed to django-csp, and the community basically rolled with it as the de facto solution.&lt;/p&gt;
    &lt;p&gt;Over the years, CSP itself evolved, as did django-csp, with Rob Hudson ending up as its maintainer. Focusing on the package motivated to finally get CSP into Django itself. He made a draft PR and posted on Ticket #15727 in 2024, which I enjoyed helping review. He iterated on the PR over the next 13 months until it was finally merged for Django 6.0. Thanks to Rob for his heroic dedication here, and to all reviewers: Benjamin Balder Bach, Carlton Gibson, Collin Anderson, David Sanders, David Smith, Florian Apolloner, Harro van der Klauw, Jake Howard, Natalia Bidart, Paolo Melchiorre, Sarah Boyce, and SÃ©bastien Corbin.&lt;/p&gt;
    &lt;head rend="h2"&gt;Email API updates&lt;/head&gt;
    &lt;p&gt;The fourth and final headline feature:&lt;/p&gt;
    &lt;code&gt;Email handling in Django now uses Pythonâs modern email API, introduced in Python 3.6. This API, centered around the  email.message.EmailMessage class, offers a cleaner and Unicode-friendly interface for composing and sending emails.&lt;/code&gt;
    &lt;p&gt;This is a major change, but itâs unlikely to affect projects using basic email features. You can still use Djangoâs &lt;code&gt;send_mail()&lt;/code&gt; function and &lt;code&gt;EmailMessage&lt;/code&gt; class as before, like:&lt;/p&gt;
    &lt;code&gt;from django.core.mail import EmailMessage

email = EmailMessage(
    subject="ð¼ Need more bamboo",
    body="We are desperately low, please restock before the pandas find out!",
    from_email="zookeeper@example.com",
    to=["supplies@example.com"],
)
email.attach_file("/media/bamboo_cupboard.jpg")
email.send()
&lt;/code&gt;
    &lt;p&gt;The key change is that, under-the-hood, when you call &lt;code&gt;send()&lt;/code&gt; on a Django &lt;code&gt;EmailMessage&lt;/code&gt; object, it now translates itself into a Pythonâs newer &lt;code&gt;email.message.EmailMessage&lt;/code&gt; type before sending.&lt;/p&gt;
    &lt;p&gt;Modernizing provides these benefits:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fewer bugs - many edge case bugs in Pythonâs old email API have been fixed in the new one.&lt;/item&gt;
      &lt;item&gt;Django is less hacky - a bunch of workarounds and security fixes in Djangoâs email code have been removed.&lt;/item&gt;
      &lt;item&gt;More convenient API - the new API supports some niceties, like the below inline attachment example.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Easier inline attachments with &lt;code&gt;MIMEPart&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Djangoâs &lt;code&gt;EmailMessage.attach()&lt;/code&gt; method allows you to attach a file as an attachment. Emails support images as inline attachments, which can be displayed within the HTML email body.&lt;/p&gt;
    &lt;p&gt;While you could previously use &lt;code&gt;EmailMessage.attach()&lt;/code&gt; to add inline attachments, it was a bit fiddly, using a legacy class. Now, you can call the method with a Python &lt;code&gt;email.message.MIMEPart&lt;/code&gt; object to add an inline attachment in a few steps:&lt;/p&gt;
    &lt;code&gt;import email.utils
from email.message import MIMEPart
from django.core.mail import EmailMultiAlternatives

message = EmailMultiAlternatives(
    subject="Cute Panda Alert",
    body="Here's a cute panda picture for you!",
    from_email="cute@example.com",
    to=["fans@example.com"],
)
with open("panda.jpg", "rb") as f:
    panda_jpeg = f.read()

cid = email.utils.make_msgid()
inline_image = MIMEPart()
inline_image.set_content(
    panda_jpeg,
    maintype="image",
    subtype="jpeg",
    disposition="inline",
    cid=cid,
)
message.attach(inline_image)
message.attach_alternative(
    f'&amp;lt;h1&amp;gt;Cute panda baby alert!&amp;lt;/h1&amp;gt;&amp;lt;img src="cid:{cid[1:-1]}"&amp;gt;',
    "text/html",
)
&lt;/code&gt;
    &lt;p&gt;Itâs not the simplest API, but it does expose all the power of the underlying email system, and itâs better than the past situation.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;The new email API was added to Python as provisional in version 3.4 (2014), and made stable in version 3.6 (2016). The legacy API, however, was never planned for deprecation, so there was never any deadline to upgrade Djangoâs email handling.&lt;/p&gt;
    &lt;p&gt;In 2024, Mike Edmunds posted on the (old) django-developers mailing list, proposing the upgrade with strong reasoning and planning. This conversation led to Ticket #35581, which he worked on for eight months until it was merged. Many thanks to Mike for leading this effort, and to Sarah Boyce for reviewing! Email is not a glamorous feature, but itâs a critical communication channel for nearly every Django project, so props for this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Positional arguments in &lt;code&gt;django.core.mail&lt;/code&gt; APIs&lt;/head&gt;
    &lt;p&gt;Weâre now out of the headline features and onto the âminorâ changes, starting with this deprecation related to the above email changes:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;django.core.mail&lt;/code&gt;APIs now require keyword arguments for less commonly used parameters. Using positional arguments for these now emits a deprecation warning and will raise a&lt;code&gt;TypeError&lt;/code&gt;when the deprecation period ends:&lt;item&gt;All optional parameters (&lt;/item&gt;&lt;code&gt;fail_silently&lt;/code&gt;and later) must be passed as keyword arguments to&lt;code&gt;get_connection()&lt;/code&gt;,&lt;code&gt;mail_admins()&lt;/code&gt;,&lt;code&gt;mail_managers()&lt;/code&gt;,&lt;code&gt;send_mail()&lt;/code&gt;, and&lt;code&gt;send_mass_mail()&lt;/code&gt;.&lt;item&gt;All parameters must be passed as keyword arguments when creating an&lt;/item&gt;&lt;code&gt;EmailMessage&lt;/code&gt;or&lt;code&gt;EmailMultiAlternatives&lt;/code&gt;instance, except for the first four (&lt;code&gt;subject&lt;/code&gt;,&lt;code&gt;body&lt;/code&gt;,&lt;code&gt;from_email&lt;/code&gt;, and&lt;code&gt;to&lt;/code&gt;), which may still be passed either as positional or keyword arguments.&lt;/quote&gt;
    &lt;p&gt;Previously, Django would let you pass all parameters positionally, which gets a bit silly and hard to read with long parameter lists, like:&lt;/p&gt;
    &lt;code&gt;from django.core.mail import send_mail

send_mail(
    "ð¼ Panda of the week",
    "This weekâs panda is Po Ping, sha-sha booey!",
    "updates@example.com",
    ["adam@example.com"],
    True,
)
&lt;/code&gt;
    &lt;p&gt;The final &lt;code&gt;True&lt;/code&gt; doesnât provide any clue what it means without looking up the function signature. Now, using positional arguments for those less-commonly-used parameters raises a deprecation warning, nudging you to write:&lt;/p&gt;
    &lt;code&gt;from django.core.mail import send_mail

send_mail(
    subject="ð¼ Panda of the week",
    body="This weekâs panda is Po Ping, sha-sha booey!",
    from_email="updates@example.com",
    ["adam@example.com"],
    fail_silently=True,
)
&lt;/code&gt;
    &lt;p&gt;This change is appreciated for API clarity, and Django is generally moving towards using keyword-only arguments more often. django-upgrade can automatically fix this one for you, via its &lt;code&gt;mail_api_kwargs&lt;/code&gt; fixer.&lt;/p&gt;
    &lt;p&gt;Thanks to Mike Edmunds, again, for making this improvement in Ticket #36163.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extended automatic &lt;code&gt;shell&lt;/code&gt; imports&lt;/head&gt;
    &lt;p&gt;Next up:&lt;/p&gt;
    &lt;quote&gt;Common utilities, such as django.conf.settings, are now automatically imported to the shell by default.&lt;/quote&gt;
    &lt;p&gt;One of the headline features back in Django 5.2 was automatic model imports in the shell, making &lt;code&gt;./manage.py shell&lt;/code&gt; import all of your models automatically. Building on that DX boost, Django 6.0 now also imports other common utilities, for which we can find the full list by running &lt;code&gt;./manage.py shell&lt;/code&gt; with &lt;code&gt;-v 2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;$ ./manage.py shell -v 2
6 objects imported automatically:

  from django.conf import settings
  from django.db import connection, models, reset_queries
  from django.db.models import functions
  from django.utils import timezone

...
&lt;/code&gt;
    &lt;p&gt;(This is from a project without any models, so only the utilities are listed.)&lt;/p&gt;
    &lt;p&gt;So thatâs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;settings&lt;/code&gt;, useful for checking your runtime configuration:&lt;quote&gt;In [1]: settings.DEBUG Out[1]: False&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;connection&lt;/code&gt;and&lt;code&gt;reset_queries()&lt;/code&gt;, great for checking the executed queries:&lt;quote&gt;In [1]: Book.objects.select_related('author') Out[1]: &amp;lt;QuerySet []&amp;gt; In [2]: connection.queries Out[2]: [{'sql': 'SELECT "example_book"."id", "example_book"."title", "example_book"."author_id", "example_author"."id", "example_author"."name" FROM "example_book" INNER JOIN "example_author" ON ("example_book"."author_id" = "example_author"."id") LIMIT 21', 'time': '0.000'}]&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;models&lt;/code&gt;and&lt;code&gt;functions&lt;/code&gt;, useful for advanced ORM work:&lt;quote&gt;In [1]: Book.objects.annotate( ...: title_lower=functions.Lower("title") ...: ).filter( ...: title_lower__startswith="a" ...: ).count() Out[1]: 71&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;timezone&lt;/code&gt;, useful for using Djangoâs timezone-aware date and time utilities:&lt;quote&gt;In [1]: timezone.now() Out[1]: datetime.datetime(2025, 12, 1, 23, 42, 22, 558418, tzinfo=datetime.timezone.utc)&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It remains possible to extend the automatic imports with whatever youâd like, as documented in How to customize the &lt;code&gt;shell&lt;/code&gt; command documentation page.&lt;/p&gt;
    &lt;p&gt;Salvo Polizzi contributed the original automatic shell imports feature in Django 5.2. Heâs then returned to offer these extra imports for Django 6.0, in Ticket #35680. Thanks to everyone that contributed to the forum discussion agreeing on which imports to add, and to Natalia Bidart and Sarah Boyce for reviewing!&lt;/p&gt;
    &lt;head rend="h2"&gt;Dynamic field refresh on &lt;code&gt;save()&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Now letâs discuss a series of ORM improvements, starting with this big one:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;GeneratedField&lt;/code&gt;s and fields assigned expressions are now refreshed from the database after&lt;code&gt;save()&lt;/code&gt;on backends that support the&lt;code&gt;RETURNING&lt;/code&gt;clause (SQLite, PostgreSQL, and Oracle). On backends that donât support it (MySQL and MariaDB), the fields are marked as deferred to trigger a refresh on subsequent accesses.&lt;/quote&gt;
    &lt;p&gt;Django models support having the database generate field values for you in three cases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;db_default&lt;/code&gt;field option, which lets the database generate the default value when creating an instance:&lt;quote&gt;from django.db import models from django.db.models.functions import Now class Video(models.Model): ... created = models.DateTimeField(db_default=Now())&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;GeneratedField&lt;/code&gt;field type, which is always computed by the database based on other fields in the same instance:&lt;quote&gt;from django.db import models from django.db.models.functions import Concat class Video(models.Model): ... full_title = models.GeneratedField( models.TextField(), expression=Concat( "title", models.Value(" - "), "subtitle", ), )&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Assigning expression values to fields before saving:&lt;/p&gt;
        &lt;quote&gt;from django.db import models from django.db.models.functions import Now class Video(models.Model): ... last_updated = models.DateTimeField() video = Video.objects.get(id=1) ... video.last_updated = Now() video.save()&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Previously, only the first method, using &lt;code&gt;db_default&lt;/code&gt;, would refresh the field value from the database after saving. The other two methods would leave you with only the old value or the expression object, meaning youâd need to call &lt;code&gt;Model.refresh_from_db()&lt;/code&gt; to get any updated value if necessary. This was hard to remember and it costs an extra database query.&lt;/p&gt;
    &lt;p&gt;Now Django takes advantage of the &lt;code&gt;RETURNING&lt;/code&gt; SQL clause to save the model instance and fetch updated dynamic field values in a single query, on backends that support it (SQLite, PostgreSQL, and Oracle). A &lt;code&gt;save()&lt;/code&gt; call may now issue a query like:&lt;/p&gt;
    &lt;code&gt;UPDATE "example_video"
SET "last_updated" = NOW()
WHERE "example_video"."id" = 1
RETURNING "example_video"."last_updated"
&lt;/code&gt;
    &lt;p&gt;Django puts the return value into the model field, so you can read it immediately after saving:&lt;/p&gt;
    &lt;code&gt;video = Video.objects.get(id=1)
...
video.last_updated = Now()
video.save()
print(video.last_updated)  # Updated value from the database
&lt;/code&gt;
    &lt;p&gt;On backends that donât support &lt;code&gt;RETURNING&lt;/code&gt; (MySQL and MariaDB), Django now marks the dynamic fields as deferred after saving. That way, the later access, as in the above example, will automatically call &lt;code&gt;Model.refresh_from_db()&lt;/code&gt;. This ensures that you always read the updated value, even if it costs an extra query.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;This feature was proposed in Ticket #27222 way back in 2016, by Anssi KÃ¤Ã¤riÃ¤inen. It sat dormant for most of the nine years since, but ORM boss Simon Charette picked it up earlier this year, found an implementation, and pushed it through to completion. Thanks to Simon for continuing to push the ORM forward, and to all reviewers: David Sanders, Jacob Walls, Mariusz Felisiak, nessita, Paolo Melchiorre, Simon Charette, and Tim Graham.&lt;/p&gt;
    &lt;head rend="h2"&gt;Universal &lt;code&gt;StringAgg&lt;/code&gt; aggregate&lt;/head&gt;
    &lt;p&gt;The next ORM change:&lt;/p&gt;
    &lt;quote&gt;The new&lt;code&gt;StringAgg&lt;/code&gt;aggregate returns the input values concatenated into a string, separated by the&lt;code&gt;delimiter&lt;/code&gt;string. This aggregate was previously supported only for PostgreSQL.&lt;/quote&gt;
    &lt;p&gt;This aggregate is often used for making comma-separated lists of related items, among other things. Previously, it was only supported on PostgreSQL, as part of &lt;code&gt;django.contrib.postgres&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;from django.contrib.postgres.aggregates import StringAgg
from example.models import Video

videos = Video.objects.annotate(
    chapter_ids=StringAgg("chapter", delimiter=","),
)

for video in videos:
    print(f"Video {video.id} has chapters: {video.chapter_ids}")
&lt;/code&gt;
    &lt;p&gt;â¦which might give you output like:&lt;/p&gt;
    &lt;code&gt;Video 104 has chapters: 71,72,74
Video 107 has chapters: 88,89,138,90,91,93
&lt;/code&gt;
    &lt;p&gt;Now this aggregate is available on all database backends supported by Django, imported from &lt;code&gt;django.db.models&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;from django.db.models import StringAgg, Value
from example.models import Video

videos = Video.objects.annotate(
    chapter_ids=StringAgg("chapter", delimiter=Value(",")),
)

for video in videos:
    print(f"Video {video.id} has chapters: {video.chapter_ids}")
&lt;/code&gt;
    &lt;p&gt;Note the &lt;code&gt;delimiter&lt;/code&gt; argument now requires a &lt;code&gt;Value()&lt;/code&gt; expression wrapper for literal strings, as above. This change allows you to use database functions or fields as the delimiter if desired.&lt;/p&gt;
    &lt;p&gt;While most Django projects stick to PostgreSQL, having this aggregate available on all backends is a nice improvement for cross-database compatibility, and it means third-party packages can use it without affecting their database support.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;The PostgreSQL-specific &lt;code&gt;StringAgg&lt;/code&gt; was added way back in Django 1.9 (2015) by Andriy Sokolovskiy, in Ticket #24301. In Ticket #35444, Chris Muthig proposed adding the &lt;code&gt;Aggregate.order_by&lt;/code&gt; option, something used by &lt;code&gt;StringAgg&lt;/code&gt; to specify the ordering of concatenated elements, and as a side effect this made it possible to generalize &lt;code&gt;StringAgg&lt;/code&gt; to all backends.&lt;/p&gt;
    &lt;p&gt;Thanks to Chris for proposing and implementing this change, and to all reviewers: Paolo Melchiorre, Sarah Boyce, and Simon Charette.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;BigAutoField&lt;/code&gt; as the default primary key type&lt;/head&gt;
    &lt;p&gt;Next up:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt;setting now defaults to&lt;code&gt;BigAutoField&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;This important change helps lock in scalable larger primary keys.&lt;/p&gt;
    &lt;p&gt;Django 3.2 (2021) introduced the &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; setting for changing the default primary key type used in models. Django uses this setting to add a primary key field called &lt;code&gt;id&lt;/code&gt; to models that donât explicitly define a primary key field. For example, if you define a model like this:&lt;/p&gt;
    &lt;code&gt;from django.db import models


class Video(models.Model):
    title = models.TextField()
&lt;/code&gt;
    &lt;p&gt;â¦then it will have two fields: &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt;, where &lt;code&gt;id&lt;/code&gt; uses the type defined by &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The setting can also be overridden on a per-app basis by defining &lt;code&gt;AppConfig.default_auto_field&lt;/code&gt; in the appâs &lt;code&gt;apps.py&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;from django.apps import AppConfig


class ChannelConfig(AppConfig):
    name = "channel"
    default_auto_field = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;A key motivation for adding the setting was to allow projects to switch from &lt;code&gt;AutoField&lt;/code&gt; (a 32-bit integer) to &lt;code&gt;BigAutoField&lt;/code&gt; (a 64-bit integer) for primary keys, without needing changes to every model. &lt;code&gt;AutoField&lt;/code&gt; can store values up to about 2.1 billion, which sounds large but it becomes easy to hit at scale. &lt;code&gt;BigAutoField&lt;/code&gt; can store values up to about 9.2 quintillion, which is âmore than enoughâ for every practical purpose.&lt;/p&gt;
    &lt;p&gt;If a model using &lt;code&gt;AutoField&lt;/code&gt; hits its maximum value, it can no longer accept new rows, a problem known as primary key exhaustion. The table is effectively blocked, requiring an urgent fix to switch the model from &lt;code&gt;AutoField&lt;/code&gt; to &lt;code&gt;BigAutoField&lt;/code&gt; via a locking database migration on a large table. For a great watch on how Kraken is fixing this problem, see Tim Bellâs DjangoCon Europe 2025 talk, detailing some clever techniques to proactively migrate large tables with minimal downtime.&lt;/p&gt;
    &lt;p&gt;To stop this problem arising for new projects, Django 3.2 made new projects created with &lt;code&gt;startproject&lt;/code&gt; set &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; to &lt;code&gt;BigAutoField&lt;/code&gt;, and new apps created with &lt;code&gt;startapp&lt;/code&gt; set their &lt;code&gt;AppConfig.default_auto_field&lt;/code&gt; to &lt;code&gt;BigAutoField&lt;/code&gt;. It also added a system check to ensure that projects set &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; explicitly, to ensure users were aware of the feature and could make an informed choice.&lt;/p&gt;
    &lt;p&gt;Now Django 6.0 changes the actual default values of the setting and app config attribute to &lt;code&gt;BigAutoField&lt;/code&gt;. Projects using &lt;code&gt;BigAutoField&lt;/code&gt; can remove the setting:&lt;/p&gt;
    &lt;code&gt;-DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;â¦and app config attribute:&lt;/p&gt;
    &lt;code&gt;from django.apps import AppConfig

 class ChannelConfig(AppConfig):
     name = "channel"
-    default_auto_field = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;The default &lt;code&gt;startproject&lt;/code&gt; and &lt;code&gt;startapp&lt;/code&gt; templates also no longer set these values. This change reduces the amount of boilerplate in new projects, and the problem of primary key exhaustion can fade into history, becoming something that most Django users no longer need to think about.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;The addition of &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; in Django 3.2 was proposed by Caio Ariede and implemented by Tom Forbes, in Ticket #31007. This new change in Django 6.0 was proposed and implemented by ex-Fellow Tim Graham, in Ticket #36564. Thanks to Tim for spotting that this cleanup was now possible, and to Jacob Walls and Clifford Gama for reviewing!&lt;/p&gt;
    &lt;head rend="h2"&gt;Template variable &lt;code&gt;forloop.length&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Moving on to templates, letâs start with this nice little addition:&lt;/p&gt;
    &lt;quote&gt;The new variable forloop.length is now available within a for loop.&lt;/quote&gt;
    &lt;p&gt;This small extension makes it possible to write a template loop like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;ul&amp;gt;
  {% for goose in geese %}
    &amp;lt;li&amp;gt;
      &amp;lt;strong&amp;gt;{{ forloop.counter }}/{{ forloop.length }}&amp;lt;/strong&amp;gt;: {{ goose.name }}
    &amp;lt;/li&amp;gt;
  {% endfor %}
&amp;lt;/ul&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Previously, youâd need to refer to the length in an another way, like &lt;code&gt;{{ geese|length }}&lt;/code&gt;, which is a bit less flexible.&lt;/p&gt;
    &lt;p&gt;Thanks to Jonathan StrÃ¶bele for contributing this idea and implementation in Ticket #36186, and to David Smith, Paolo Melchiorre, and Sarah Boyce for reviewing.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;querystring&lt;/code&gt; template tag enhancements&lt;/head&gt;
    &lt;p&gt;There are two extensions to the &lt;code&gt;querystring&lt;/code&gt; template tag, which was added in Django 5.1 to help with building links that modify the current requestâs query parameters.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Release note:&lt;/p&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;querystring&lt;/code&gt;template tag now consistently prefixes the returned query string with a&lt;code&gt;?&lt;/code&gt;, ensuring reliable link generation behavior.&lt;p&gt;This small change improves how the tag behaves when an empty mapping of query parameters are provided. Say you had a template like this:&lt;/p&gt;&lt;quote&gt;&amp;lt;a href="{% querystring params %}"&amp;gt;Reset search&amp;lt;/a&amp;gt;&lt;/quote&gt;&lt;p&gt;â¦where&lt;/p&gt;&lt;code&gt;params&lt;/code&gt;is a dictionary that may sometimes be empty. Previously, if&lt;code&gt;params&lt;/code&gt;was empty, the output would be:&lt;quote&gt;&amp;lt;a href=""&amp;gt;Reset search&amp;lt;/a&amp;gt;&lt;/quote&gt;&lt;p&gt;Browsers treat this as a link to the same URL including the query parameters, so it would not clear the query parameters as intended. Now, with this change, the output will be:&lt;/p&gt;&lt;quote&gt;&amp;lt;a href="?"&amp;gt;Reset search&amp;lt;/a&amp;gt;&lt;/quote&gt;&lt;p&gt;Browsers treat&lt;/p&gt;&lt;code&gt;?&lt;/code&gt;as a link to the same URL without any query parameters, clearing them as the user would expect.&lt;p&gt;Thanks to Django Fellow Sarah Boyce for spotting this improvement and implementing the fix in Ticket #36268, and for Django Fellow Natalia Bidart for reviewing!&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Release note:&lt;/p&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;querystring&lt;/code&gt;template tag now accepts multiple positional arguments, which must be mappings, such as&lt;code&gt;QueryDict&lt;/code&gt;or&lt;code&gt;dict&lt;/code&gt;.&lt;p&gt;This enhancement allows the tag to merge multiple sources of query parameters when building the output. For example, you might have a template like this:&lt;/p&gt;&lt;quote&gt;&amp;lt;a href="{% querystring request.GET super_search_params %}"&amp;gt;Super search&amp;lt;/a&amp;gt;&lt;/quote&gt;&lt;p&gt;â¦where&lt;/p&gt;&lt;code&gt;super_search_params&lt;/code&gt;is a dictionary of extra parameters to add to make the current search âsuperâ. The tag merges the two mappings, with later mappings taking precedence for duplicate keys.&lt;p&gt;Thanks again to Sarah Boyce for proposing this improvement in Ticket #35529, to Giannis Terzopoulos for implementing it, and to Natalia Bidart, Sarah Boyce, and Tom Carrick for reviewing!&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Fin&lt;/head&gt;
    &lt;p&gt;Thatâs a wrap! Thank you for reading my highlights. There are plenty more changes to read about in the release notes.&lt;/p&gt;
    &lt;p&gt;Also, there are always many more behind-the-scenes improvements and bug fixes that donât make it into the release notes. Optimizations and micro-improvements get merged all the time, so donât delay, upgrade today!&lt;/p&gt;
    &lt;p&gt;Thank you to all 174 people who contributed to Django 6.0, as counted in this list by Mariusz Felisiak.&lt;/p&gt;
    &lt;p&gt;May your upgrade be swift, smooth, safe, and secure,&lt;/p&gt;
    &lt;p&gt;âAdam&lt;/p&gt;
    &lt;p&gt;ð¸ð¸ð¸ Check out my new book on using GitHub effectively, Boost Your GitHub DX! ð¸ð¸ð¸&lt;/p&gt;
    &lt;p&gt;One summary email a week, no spam, I pinky promise.&lt;/p&gt;
    &lt;p&gt;Related posts:&lt;/p&gt;
    &lt;p&gt;Tags: django&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adamj.eu/tech/2025/12/03/django-whats-new-6.0/"/><published>2025-12-09T20:33:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46210748</id><title>Show HN: Bloodhound – Grey-box attack-path discovery in Rust/Go/C++ binaries</title><updated>2025-12-10T16:49:06.735665+00:00</updated><content>&lt;doc fingerprint="4572a08201358b8e"&gt;
  &lt;main&gt;
    &lt;p&gt;Continuous architecture validation pinpointing architectural flaws as developers code. We only alert on proven crashes and exploits to keep developers building, not fixing.&lt;/p&gt;
    &lt;p&gt;Note: This is Bloodhound Security's code testing platform, not BloodHound by SpecterOps (Active Directory tool)&lt;/p&gt;
    &lt;head rend="h2"&gt;From thousands of alerts, to a handful of real risks.&lt;/head&gt;
    &lt;p&gt;Comprehensive security and quality testing across your entire stack&lt;/p&gt;
    &lt;head rend="h3"&gt;We Test For&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Security vulnerabilities (memory corruption, OAuth token hijacking, SSTI, deserialization RCE, GraphQL introspection exploits)&lt;/item&gt;
      &lt;item&gt;API security and broken access control&lt;/item&gt;
      &lt;item&gt;Infrastructure as code (Terraform, Kubernetes, Docker)&lt;/item&gt;
      &lt;item&gt;Dependency vulnerabilities and known CVEs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Eliminate alert backlogs. We isolate actual exploits from noise.&lt;/head&gt;
    &lt;head rend="h4"&gt;Bloodhound's Methodology&lt;/head&gt;
    &lt;p&gt;Bloodhound operates as a Continuous Architecture Validation engine, continuously unit testing your entire development lifecycle.&lt;/p&gt;
    &lt;p&gt;The core problem we see is that teams are drowning in disconnected noise from different, siloed tools, each flagging low-level issues, while still missing the actual paths attackers find.&lt;/p&gt;
    &lt;p&gt;Unlike traditional scanners, Bloodhound analyzes the full context of your environment including APIs, repositories, and test data to validate real attack paths. This analysis uncovers critical vulnerabilities that conventional tools miss, seamlessly integrated into your development workflow.&lt;/p&gt;
    &lt;p&gt;Instead of managing multiple tools, Bloodhound consolidates and correlates vulnerabilities. It turns all that noise into intelligent signals, giving you a single, unified view of the exact attack paths an attacker will actually exploit.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Vulnerable to Secure&lt;lb/&gt;In Minutes, Not Months&lt;/head&gt;
    &lt;p&gt;Watch how Bloodhound transforms your security posture end-to-end.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bloodhound Core Capabilities&lt;/head&gt;
    &lt;head rend="h3"&gt;A Unified Engine, Right in Your CLI&lt;/head&gt;
    &lt;p&gt;Stop context-switching. Bloodhound runs as a single command in your terminal. It creates its own local development server to continuously run tests as you code. This unifies your workflow, so you can catch security, performance, and functional test issues from one place without test scripts.&lt;/p&gt;
    &lt;head rend="h3"&gt;What Checklist Tools See:&lt;/head&gt;
    &lt;head rend="h3"&gt;What Bloodhound Validates:&lt;/head&gt;
    &lt;p&gt;Bloodhound validates real attack paths by connecting isolated threats your other tools cannot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Continous Red team Testing&lt;/head&gt;
    &lt;head rend="h3"&gt;Lower project costs&lt;/head&gt;
    &lt;p&gt;Eliminate debugging bottlenecks to recover millions of dollars lost annually in development time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Threat detection&lt;/head&gt;
    &lt;p&gt;Real-time monitoring and continuous threat identification across all systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;24/7 Incident Response&lt;/head&gt;
    &lt;p&gt;Expert team to assist with vulnerabilities and security breaches within minutes&lt;/p&gt;
    &lt;head rend="h3"&gt;Faster time to market&lt;/head&gt;
    &lt;p&gt;Speed up your release cycle and eliminate debugging downtime with our proven testing tool.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solutions by Industry&lt;/head&gt;
    &lt;head rend="h3"&gt;Healthcare&lt;/head&gt;
    &lt;p&gt;Deliver high quality applications faster without sacrificing compliance in highly regulated environments. With Bloodhound organizations can get products to market faster, mitigate security threats and adhere to stringent quality regulations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Oil and Gas&lt;/head&gt;
    &lt;p&gt;Govern risk with and guarantee compliance with Bloodhound. We help teams minimize debugging, lower operational costs and safeguard against critical exposure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Finance &amp;amp; Banking&lt;/head&gt;
    &lt;p&gt;Accelerate DevOps to get better features to market faster. We quickly identify security threats to fortify organizations against data breaches. Bloodhound also provides verifiable proof of compliance required by financial regulators.&lt;/p&gt;
    &lt;head rend="h2"&gt;Integrations and Languages&lt;/head&gt;
    &lt;p&gt;Seamlessly integrate with your development workflow and support for all major programming languages&lt;/p&gt;
    &lt;head rend="h2"&gt;Impact by the Numbers&lt;/head&gt;
    &lt;p&gt;Real results from real security implementations&lt;/p&gt;
    &lt;p&gt;Repositories Secured&lt;/p&gt;
    &lt;p&gt;Vulnerabilities Fixed&lt;/p&gt;
    &lt;p&gt;Client Satisfaction&lt;/p&gt;
    &lt;p&gt;Response Time&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bloodhoundsecurity.ca"/><published>2025-12-09T21:14:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46213585</id><title>Rust in the kernel is no longer experimental</title><updated>2025-12-10T16:49:06.454244+00:00</updated><content>&lt;doc fingerprint="45d31c1fbd8676f8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The (successful) end of the kernel Rust experiment&lt;/head&gt;
    &lt;p&gt; (Stay tuned for details in our Maintainers Summit coverage.)&lt;/p&gt;
    &lt;p&gt; Posted Dec 10, 2025 4:25 UTC (Wed) by ktkaffee (subscriber, #112877) [Link] (15 responses) Posted Dec 10, 2025 4:45 UTC (Wed) by josh (subscriber, #17465) [Link] (9 responses) Posted Dec 10, 2025 4:48 UTC (Wed) by corbet (editor, #1) [Link] (2 responses) Posted Dec 10, 2025 5:24 UTC (Wed) by josh (subscriber, #17465) [Link] (And don't worry, you'd have to fall *very* far, very consistently, to limbo under the low bar Phoronix has set for clickbait.) Posted Dec 10, 2025 6:04 UTC (Wed) by rolexhamster (guest, #158445) [Link] (5 responses) Elitist much? Notwithstanding the low quality user comments on Phoronix and somewhat challenged writing in its news items, the site does provide useful info by way of frequent updates of what's happening in and around the open source ecosystem. Its benchmarks have also uncovered problems in the Linux kernel. In certain ways it's complementary to LWN's coverage. Posted Dec 10, 2025 7:49 UTC (Wed) by Cyberax (✭ supporter ✭, #52523) [Link] (1 responses) Posted Dec 10, 2025 11:09 UTC (Wed) by rossburton (subscriber, #7254) [Link] Fun fact: Clear Linux wins in many of the phoronix benchmarks because the default bashrc does export CFLAGS=-O3. (caveat: this was the case when I was researching what Clear does to get better scores on identical hardware some years ago, but I don't believe anything has changed since then) Posted Dec 10, 2025 9:26 UTC (Wed) by lkundrak (subscriber, #43452) [Link] (1 responses) Posted Dec 10, 2025 9:27 UTC (Wed) by lkundrak (subscriber, #43452) [Link] sorry, commenting before my morning frontal lobotomy Posted Dec 10, 2025 11:08 UTC (Wed) by farnz (subscriber, #17727) [Link] Posted Dec 10, 2025 7:57 UTC (Wed) by alspnost (guest, #2763) [Link] Posted Dec 10, 2025 9:07 UTC (Wed) by adobriyan (subscriber, #30858) [Link] Posted Dec 10, 2025 9:32 UTC (Wed) by evalir (subscriber, #171462) [Link] Posted Dec 10, 2025 13:07 UTC (Wed) by hailfinger (subscriber, #76962) [Link] Posted Dec 10, 2025 14:15 UTC (Wed) by Jedizlapulga (guest, #180979) [Link] Posted Dec 10, 2025 6:32 UTC (Wed) by mrcroxx (guest, #161669) [Link] (2 responses) Posted Dec 10, 2025 13:58 UTC (Wed) by rsidd (subscriber, #2582) [Link] (1 responses) &amp;gt; Mike: rachel and i are (successfully) no longer dating Posted Dec 10, 2025 14:49 UTC (Wed) by jaa (subscriber, #14170) [Link] (as did the emotionally roller-coaster news item) Posted Dec 10, 2025 10:11 UTC (Wed) by cbushey (guest, #142134) [Link] (1 responses) Posted Dec 10, 2025 10:22 UTC (Wed) by cbushey (guest, #142134) [Link] Posted Dec 10, 2025 15:57 UTC (Wed) by stumbles (guest, #8796) [Link] (1 responses) Posted Dec 10, 2025 16:08 UTC (Wed) by farnz (subscriber, #17727) [Link] &lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;head/&gt; Ouch. That is what I get for pushing something out during a meeting, I guess. That was not my point; the experiment is done, and it was a success. I meant no more than that. &lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;head/&gt; &amp;gt; Phoronix would be proud of that headline. &lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;head/&gt; Phoronix also tends to benchmark in the "dumb but obvious" way - just follow the instructions, don't take the time to understand it in depth and tweak obsessively until it's as good as it's going to get. This is useful, because it exposes cases where something is genuinely useful once tweaked into shape, but where the defaults are bad and need fixing. &lt;head&gt;Nice&lt;/head&gt;&lt;head/&gt; Same here - for a second, I thought they were about to rip it all out for v6.20 -&amp;gt; 7.0! &lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;head&gt;Nice&lt;/head&gt;&lt;lb/&gt; Well done!&lt;head&gt;Jedizlapulga&lt;/head&gt;&lt;head&gt;Meme&lt;/head&gt;&lt;lb/&gt; &amp;gt;&lt;lb/&gt; &amp;gt; rachel: mike that's a horrible way of telling people we're married&lt;head&gt;Meme&lt;/head&gt;&lt;head&gt;Meme&lt;/head&gt;&lt;head&gt;hilarity ensues&lt;/head&gt;&lt;head&gt;I forgot&lt;/head&gt;&lt;head/&gt; Well, guess its time to switch back to Microsoft. &lt;head&gt;I was looking for a reason.&lt;/head&gt;&lt;head/&gt; Bad news in that respect - Microsoft is using Rust in the Windows kernel, too. &lt;head&gt;I was looking for a reason.&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/Articles/1049831/"/><published>2025-12-10T03:15:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46214693</id><title>Revisiting "Let's Build a Compiler"</title><updated>2025-12-10T16:49:06.179900+00:00</updated><content>&lt;doc fingerprint="b101145b7036df98"&gt;
  &lt;main&gt;
    &lt;p&gt;There's an old compiler-building tutorial that has become part of the field's lore: the Let's Build a Compiler series by Jack Crenshaw (published between 1988 and 1995).&lt;/p&gt;
    &lt;p&gt;I ran into it in 2003 and was very impressed, but it's now 2025 and this tutorial is still being mentioned quite often in Hacker News threads. Why is that? Why does a tutorial from 35 years ago, built in Pascal and emitting Motorola 68000 assembly - technologies that are virtually unknown for the new generation of programmers - hold sway over compiler enthusiasts? I've decided to find out.&lt;/p&gt;
    &lt;p&gt;The tutorial is easily available and readable online, but just re-reading it seemed insufficient. So I've decided on meticulously translating the compilers built in it to Python and emit a more modern target - WebAssembly. It was an enjoyable process and I want to share the outcome and some insights gained along the way.&lt;/p&gt;
    &lt;p&gt;The result is this code repository. Of particular interest is the TUTORIAL.md file, which describes how each part in the original tutorial is mapped to my code. So if you want to read the original tutorial but play with code you can actually easily try on your own, feel free to follow my path.&lt;/p&gt;
    &lt;head rend="h2"&gt;A sample&lt;/head&gt;
    &lt;p&gt;To get a taste of the input language being compiled and the output my compiler generates, here's a sample program in the KISS language designed by Jack Crenshaw:&lt;/p&gt;
    &lt;code&gt;var X=0

 { sum from 0 to n-1 inclusive, and add to result }
 procedure addseq(n, ref result)
     var i, sum  { 0 initialized }
     while i &amp;lt; n
         sum = sum + i
         i = i + 1
     end
     result = result + sum
 end

 program testprog
 begin
     addseq(11, X)
 end
 .
&lt;/code&gt;
    &lt;p&gt;It's from part 13 of the tutorial, so it showcases procedures along with control constructs like the while loop, and passing parameters both by value and by reference. Here's the WASM text generated by my compiler for part 13:&lt;/p&gt;
    &lt;code&gt;(module
  (memory 8)
  ;; Linear stack pointer. Used to pass parameters by ref.
  ;; Grows downwards (towards lower addresses).
  (global $__sp (mut i32) (i32.const 65536))

  (global $X (mut i32) (i32.const 0))

  (func $ADDSEQ (param $N i32) (param $RESULT i32)
    (local $I i32)
    (local $SUM i32)
    loop $loop1
      block $breakloop1
        local.get $I
        local.get $N
        i32.lt_s
        i32.eqz
        br_if $breakloop1
        local.get $SUM
        local.get $I
        i32.add
        local.set $SUM
        local.get $I
        i32.const 1
        i32.add
        local.set $I
        br $loop1
      end
    end
    local.get $RESULT
    local.get $RESULT
    i32.load
    local.get $SUM
    i32.add
    i32.store
  )

  (func $main (export "main") (result i32)
    i32.const 11
    global.get $__sp      ;; make space on stack
    i32.const 4
    i32.sub
    global.set $__sp
    global.get $__sp
    global.get $X
    i32.store
    global.get $__sp    ;; push address as parameter
    call $ADDSEQ
    ;; restore parameter X by ref
    global.get $__sp
    i32.load offset=0
    global.set $X
    ;; clean up stack for ref parameters
    global.get $__sp
    i32.const 4
    i32.add
    global.set $__sp
    global.get $X
  )
)
&lt;/code&gt;
    &lt;p&gt;You'll notice that there is some trickiness in the emitted code w.r.t. handling the by-reference parameter (my previous post deals with this issue in more detail). In general, though, the emitted code is inefficient - there is close to 0 optimization applied.&lt;/p&gt;
    &lt;p&gt;Also, if you're very diligent you'll notice something odd about the global variable X - it seems to be implicitly returned by the generated main function. This is just a testing facility that makes my compiler easy to test. All the compilers are extensively tested - usually by running the generated WASM code [1] and verifying expected results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Insights - what makes this tutorial so special?&lt;/head&gt;
    &lt;p&gt;While reading the original tutorial again, I had on opportunity to reminisce on what makes it so effective. Other than the very fluent and conversational writing style of Jack Crenshaw, I think it's a combination of two key factors:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The tutorial builds a recursive-descent parser step by step, rather than giving a long preface on automata and table-based parser generators. When I first encountered it (in 2003), it was taken for granted that if you want to write a parser then lex + yacc are the way to go [2]. Following the development of a simple and clean hand-written parser was a revelation that wholly changed my approach to the subject; subsequently, hand-written recursive-descent parsers have been my go-to approach for almost 20 years now.&lt;/item&gt;
      &lt;item&gt;Rather than getting stuck in front-end minutiae, the tutorial goes straight to generating working assembly code, from very early on. This was also a breath of fresh air for engineers who grew up with more traditional courses where you spend 90% of the time on parsing, type checking and other semantic analysis and often run entirely out of steam by the time code generation is taught.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be honest, I don't think either of these are a big problem with modern resources, but back in the day the tutorial clearly hit the right nerve with many people.&lt;/p&gt;
    &lt;head rend="h2"&gt;What else does it teach us?&lt;/head&gt;
    &lt;p&gt;Jack Crenshaw's tutorial takes the syntax-directed translation approach, where code is emitted while parsing, without having to divide the compiler into explicit phases with IRs. As I said above, this is a fantastic approach for getting started, but in the latter parts of the tutorial it starts showing its limitations. Especially once we get to types, it becomes painfully obvious that it would be very nice if we knew the types of expressions before we generate code for them.&lt;/p&gt;
    &lt;p&gt;I don't know if this is implicated in Jack Crenshaw's abandoning the tutorial at some point after part 14, but it may very well be. He keeps writing how the emitted code is clearly sub-optimal [3] and can be improved, but IMHO it's just not that easy to improve using the syntax-directed translation strategy. With perfect hindsight vision, I would probably use Part 14 (types) as a turning point - emitting some kind of AST from the parser and then doing simple type checking and analysis on that AST prior to generating code from it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;All in all, the original tutorial remains a wonderfully readable introduction to building compilers. This post and the GitHub repository it describes are a modest contribution that aims to improve the experience of folks reading the original tutorial today and not willing to use obsolete technologies. As always, let me know if you run into any issues or have questions!&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;This is done using the Python bindings to wasmtime.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[2]&lt;/cell&gt;
        &lt;cell&gt;By the way, gcc switched from YACC to hand-written recursive-descent parsing in the 2004-2006 timeframe, and Clang has been implemented with a recursive-descent parser from the start (2007).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[3]&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Concretely: when we compile subexpr1 + subexpr2 and the two sides have different types, it would be mighty nice to know that before we actually generate the code for both sub-expressions. But the syntax-directed translation approach just doesn't work that way.&lt;/p&gt;
          &lt;p&gt;To be clear: it's easy to generate working code; it's just not easy to generate optimal code without some sort of type analysis that's done before code is actually generated.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eli.thegreenplace.net/2025/revisiting-lets-build-a-compiler/"/><published>2025-12-10T06:22:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46218202</id><title>The New Kindle Scribes Are Great, but Not Great Enough</title><updated>2025-12-10T16:49:05.977048+00:00</updated><content>&lt;doc fingerprint="f7868b3acafaa9cd"&gt;
  &lt;main&gt;
    &lt;p&gt;Amazon has been expanding its e-reader lineup over the past year to feature a panoply of color-screen options, and as of today, those options include a Kindle Scribe, Amazon's e-reader that adds digital notebook capabilities and a larger screen to make writing and drawing easier. The new Kindle Scribe Colorsoft arrives with a hefty price tag and bigger screen than the past iteration, and it doesn't arrive alone: a new third-generation regular Kindle Scribe will arrive with it, featuring a matching design but no color screen.&lt;/p&gt;
    &lt;p&gt;While I like both devices' new home screen and taller, slimmer design, I'm not sure they're worth investing in unless you're desperate for a color Kindle that doubles as a digital notebook. They're good devices overall, but in a saturated space of color-screen e-readers and digital notebooks with better price tags (or more features for a similar price), these new Kindle Scribes aren't necessarily a must-buy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generational Throw-Down&lt;/head&gt;
    &lt;p&gt;While I'm not surprised to see the arrival of a color-screen version of the Kindle Scribe, I am surprised we're already getting a new base Scribe after a new one arrived just last year. This is the third generation of this device, with the original launching back in the fall of 2022. Amazon says it's “always innovating and looking to bring those innovations to customers as quickly as possible," but it still feels like a fast move to replace it so soon.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wired.com/review/kindle-scribe-colorsoft-2025/"/><published>2025-12-10T14:39:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46218538</id><title>COM Like a Bomb: Rust Outlook Add-in</title><updated>2025-12-10T16:49:05.465870+00:00</updated><content>&lt;doc fingerprint="2e09bb17d99d220d"&gt;
  &lt;main&gt;&lt;p&gt;One of legal tech's clichés is that "lawyers live in Word".&lt;/p&gt;&lt;p&gt;This is demonstrably incorrect. I, for example, am a lawyer and in fact live in London, England.&lt;/p&gt;&lt;p&gt;But what they mean to say is that lawyers spend much of their time editing documents in Microsoft Word. This is because, for the most part, opening &lt;code&gt;.docx&lt;/code&gt; files in Word is the default behavior where it's
        installed (everywhere). Lawyers, and again I'm speaking from experience here, are generally lazy when it comes
        to
        technology. Defaults are the law.&lt;/p&gt;&lt;p&gt;This is rational. Clients pay thousands of dollars per hour to have their legal needs addressed by the top law firms in the world. This means that law firms account for every moment their lawyers' working days. Generally, in 6-minute increments (or, 0.1 hours). No client is paying even 0.3 for their lawyer to learn a new software paradigm, and most law firms don't find forgoing revenue to train lawyers on new systems that will make them faster especially motivating.&lt;/p&gt;&lt;p&gt;So to get a foothold into legal, we need to make Tritium slot as nearly as possible into the existing workflow.&lt;/p&gt;&lt;p&gt;So where does the legal work flow originate?&lt;/p&gt;&lt;p&gt;Three places: (1) the document management system (DMS), (2) the desktop and (3) email.&lt;/p&gt;&lt;p&gt;We've previously talked about iManage, one of the most important document management systems in legal. There are other important ones such as NetDocuments, and our integrations into those will be the subject of another post.&lt;/p&gt;&lt;p&gt;Today, we're focused on the third place.&lt;/p&gt;&lt;p&gt;We're giving access to Tritium right in the lawyer's inbox.&lt;/p&gt;&lt;p&gt;We're going to replicate our "Open with Tritium" desktop entry point in Outlook. Here's what it looks like on the desktop:&lt;/p&gt;&lt;head rend="h2"&gt;Outlook Integration&lt;/head&gt;&lt;p&gt;"New Outlook" is some sort of half-implemented WebView mess that requires javascript round-tripped from a host server to plug in new features.&lt;/p&gt;&lt;p&gt;We'll eventually have to get in there, too, but for the most part law firms seem to have thus far stuck with the much more featureful "legacy Outlook". That version is a venerable, performant, C++-based Windows desktop application.&lt;/p&gt;&lt;p&gt;So, how do we plug into it?&lt;/p&gt;&lt;head rend="h2"&gt;COM&lt;/head&gt;&lt;p&gt;Before even the easy 100 MB of RAM days let alone the advent of &lt;code&gt;node&lt;/code&gt; and &lt;code&gt;electron&lt;/code&gt; and
        &lt;code&gt;JSON&lt;/code&gt;,
        the Windows operating system needed a way to allow processes and applications to communicate in a
        language-agnostic way. This ultimately resulted in the "Component Object Model" or
        COM. COM allows
        us to plug
        into various entry points using a Dynamically Linked Library (.dll) which follows a strict
        ABI with certain
        calling conventions.
    &lt;/p&gt;&lt;p&gt;COM lives on today, and it is still an effective way to communicate with various processes, including Windows 11's File Explorer.&lt;/p&gt;&lt;p&gt;Fortunately, COM is supported in the &lt;code&gt;windows-rs&lt;/code&gt; Rust crate.[1]&lt;/p&gt;&lt;p&gt;To add a link to Outlook's attachment context menu, we need to inherit from a series of COM classes: &lt;code&gt;IDispatch&lt;/code&gt;,
        &lt;code&gt;IDTExtensibility2&lt;/code&gt; and ultimately &lt;code&gt;IRibbonExtensibility&lt;/code&gt;.
    &lt;/p&gt;&lt;p&gt;&lt;code&gt;windows-rs&lt;/code&gt; provides an &lt;code&gt;IDispatch&lt;/code&gt; implementation out-of-the box which exposes a
        &lt;code&gt;trait&lt;/code&gt; that looks like the below:
    &lt;/p&gt;&lt;code&gt;fn GetTypeInfoCount(&amp;amp;self) -&amp;gt; windows::core::Result&amp;lt;u32&amp;gt; {}

fn GetIDsOfNames(
    &amp;amp;self,
    riid: *const GUID,
    rgsz_names: *const PCWSTR,
    c_names: u32,
    lcid: u32,
    rg_disp_id: *mut i32,
) -&amp;gt; std::result::Result&amp;lt;(), windows_core::Error&amp;gt; {}

fn Invoke(
    &amp;amp;self,
    disp_id_member: i32,
    riid: *const GUID,
    lcid: u32,
    w_flags: DISPATCH_FLAGS,
    p_disp_params: *const DISPPARAMS,
    p_var_result: *mut VARIANT,
    p_excep_info: *mut EXCEPINFO,
    pu_arg_err: *mut u32,
) -&amp;gt; std::result::Result&amp;lt;(), windows_core::Error&amp;gt; {}
&lt;/code&gt;&lt;p&gt;These functions provide the basic COM dispatching mechanisms.&lt;/p&gt;&lt;p&gt;Using them a caller is able to look up the &lt;code&gt;rg_disp_id&lt;/code&gt; of a particular named function in your
        implementation, then &lt;code&gt;Invoke&lt;/code&gt; that function with the results optionally populating
        &lt;code&gt;p_var_result&lt;/code&gt; which is a pointer to a mutable union of possible result types.
    &lt;/p&gt;&lt;p&gt;This is the basic wiring which allows us to implement the required &lt;code&gt;IDTExensibility2&lt;/code&gt; and
        &lt;code&gt;IRibbonExtensibility&lt;/code&gt; classes.
    &lt;/p&gt;&lt;p&gt;&lt;code&gt;windows-rs&lt;/code&gt; doesn't implement these classes, but does help us by providing the &lt;code&gt;interface&lt;/code&gt;
        procedural macro which handles setting up the VTables to map our struct's methods to the COM
        ABI.&lt;/p&gt;&lt;p&gt;We use the class's &lt;code&gt;GUID&lt;/code&gt; for the macro to establish that we're implementing
        &lt;code&gt;IDTExtensibility2&lt;/code&gt;.[2]
    &lt;/p&gt;&lt;code&gt;#[windows::core::interface("B65AD801-ABAF-11D0-BB8B-00A0C90F2744")]
pub unsafe trait IDTExtensibility2: IDispatch {
    unsafe fn OnConnection(
        &amp;amp;self,
        _application: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _connectmode: i32,
        _addin_instance: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _custom: SAFEARRAY,
    ) -&amp;gt; HRESULT;
    unsafe fn OnDisconnection(&amp;amp;self, mode: i32, custom: SAFEARRAY) -&amp;gt; HRESULT;
    unsafe fn OnAddInsUpdate(&amp;amp;self, custom: SAFEARRAY) -&amp;gt; HRESULT;
    unsafe fn OnStartupComplete(&amp;amp;self, custom: SAFEARRAY) -&amp;gt; HRESULT;
    unsafe fn OnBeginShutdown(&amp;amp;self, custom: SAFEARRAY) -&amp;gt; HRESULT;
}
&lt;/code&gt;&lt;p&gt;Then, we implement that interface for our &lt;code&gt;struct&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;#[implement(IRibbonExtensibility, IDTExtensibility2, IDispatch)]
struct Addin;
&lt;/code&gt;&lt;p&gt;This causes the procedural macro to generate &lt;code&gt;IRibbonExensibility_Impl&lt;/code&gt;,
        &lt;code&gt;IDTExensibility2_Impl&lt;/code&gt; and &lt;code&gt;IDispatch_Impl&lt;/code&gt; traits for us to implement in
        &lt;code&gt;struct Addin_Impl&lt;/code&gt;.
    &lt;/p&gt;&lt;p&gt;Here's the initial Tritium &lt;code&gt;IDTExensibility2_Impl&lt;/code&gt; verbatim for example:&lt;/p&gt;&lt;code&gt;impl IDTExtensibility2_Impl for Addin_Impl {
    unsafe fn OnConnection(
        &amp;amp;self,
        _application: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _connectmode: i32,
        _addin_instance: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _custom: SAFEARRAY,
    ) -&amp;gt; HRESULT {
        log("OnConnection called()");
        // Don't do any heavy operations here that could crash Outlook
        S_OK
    }

    unsafe fn OnDisconnection(&amp;amp;self, _mode: i32, _custom: SAFEARRAY) -&amp;gt; HRESULT {
        log("OnDisconnection called()");
        S_OK
    }

    unsafe fn OnAddInsUpdate(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {
        log("OnAddInsUpdate called()");
        S_OK
    }

    unsafe fn OnStartupComplete(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {
        log("OnStartupComplete called()");
        S_OK
    }

    unsafe fn OnBeginShutdown(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {
        log("OnBeginShutdown called()");
        S_OK
    }
}
&lt;/code&gt;&lt;p&gt;As discussed below, we used an LLM to generate these signatures since they aren't provided in the &lt;code&gt;windows-rs&lt;/code&gt; crate out of the box.
    &lt;/p&gt;&lt;p&gt;Since our simple add-in at this point doesn't maintain any global state that would otherwise be constructed, adjusted and deconstructed at &lt;code&gt;OnConnection&lt;/code&gt;, &lt;code&gt;OnAddInsUpdate&lt;/code&gt; and
        &lt;code&gt;OnBeginShutdown&lt;/code&gt;, respectively, we just log the call for debugging and return &lt;code&gt;S_OK&lt;/code&gt;.
    &lt;/p&gt;&lt;p&gt;Now, being somewhat "vintage" in 2025, COM is noticeably not well documented on the web.&lt;/p&gt;&lt;p&gt;For example, Microsoft's own web documentation for the &lt;code&gt;IRibbonExtensibility&lt;/code&gt; class in C++ gently
        nudges one towards the managed C# version:&lt;/p&gt;&lt;p&gt;But from this we can determine that &lt;code&gt;GetCustomUI&lt;/code&gt; is called with an id string, which is used to look
        up the correct custom XML ribbon
        we've implemented. That is returned to the caller. In our case, that's Outlook.&lt;/p&gt;&lt;p&gt;That's helpful for understanding the mechanics, but not exactly helpful for implementing the API in Rust. In fact, despite many minutes of bona fide web searching, I was unable to locate the C++ signature for &lt;code&gt;IRibbonExtensibility&lt;/code&gt;.
    &lt;/p&gt;&lt;p&gt;But, it's 2025 and since modern LLMs have ingested and essentially compressed the entire web, plus all books and New York Times articles ever written, we can ask them to generate a signature for &lt;code&gt;IRibbonExtensibility&lt;/code&gt; for us!
    &lt;/p&gt;&lt;p&gt;This is what Claude one-shotted at the time:&lt;/p&gt;&lt;code&gt;impl IRibbonExtensibility_Impl for Addin {
    unsafe fn GetCustomUI(&amp;amp;self, _ribbon_id: BSTR, xml: *mut BSTR) -&amp;gt; HRESULT {
        // Only provide ribbon XML for specific ribbon IDs or all if we want global
        // ribbon For now, we'll provide it for all requests
        unsafe {
            *xml = BSTR::from(RIBBON_XML);
        }
        S_OK
    }
}
&lt;/code&gt;&lt;p&gt;So, unlike the C# code which returns our custom XML, C++ and, thus the Rust implementation, wants an &lt;code&gt;HRESULT&lt;/code&gt; value to specify success and the result written to a mutable parameter called
        &lt;code&gt;xml&lt;/code&gt; here. Seems plausible.
    &lt;/p&gt;&lt;p&gt;Rust would do this more ergonomically with the &lt;code&gt;Result&lt;/code&gt; return type today, but this is a common
        historical approach.&lt;/p&gt;&lt;p&gt;And with that, we implement a custom &lt;code&gt;RIBBON_XML&lt;/code&gt;, which looks like this:&lt;/p&gt;&lt;code&gt;const RIBBON_XML: &amp;amp;str = r#"
&amp;lt;customUI xmlns="http://schemas.microsoft.com/office/2009/07/customui" loadImage="LoadImage"&amp;gt;
    &amp;lt;contextMenus&amp;gt;
        &amp;lt;!-- Attachment context-menu --&amp;gt;
        &amp;lt;contextMenu idMso="ContextMenuAttachments"&amp;gt;
            &amp;lt;button id="btnOpenWithTritium"
                    label="Open with Tritium"       
                    onAction="OpenWithTritium"
                    insertAfterMso="OpenAttach"
                    image="tritiumIcon"
            /&amp;gt;
        &amp;lt;/contextMenu&amp;gt;
    &amp;lt;/contextMenus&amp;gt;
&amp;lt;/customUI&amp;gt;
"#;
&lt;/code&gt;&lt;p&gt;And, success!&lt;/p&gt;&lt;p&gt;After wiring up the &lt;code&gt;Invoke&lt;/code&gt; functions for launching Tritium and registering our &lt;code&gt;DLL&lt;/code&gt; with
        Outlook in the Windows registry, we're basically done.&lt;/p&gt;&lt;p&gt;Except.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;Interesting.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;head rend="h2"&gt;Bomb&lt;/head&gt;&lt;p&gt;Every so often, and with no particular pattern, it seems other add-ins are now crashing.&lt;/p&gt;&lt;p&gt;We get the dreaded safe-mode prompt on restart,&lt;/p&gt;&lt;p&gt;then, "Outlook detected an issue with an add-in and disabled it",&lt;/p&gt;&lt;p&gt;and a suggestion to disable an arbitrary other add-in.&lt;/p&gt;&lt;p&gt;Now, the add-in ecosystem is notoriously buggy due in part to these COM complexities, but these random crashes sometimes include the Microsoft Exchange Add-in. That one is used to communicate with Microsoft's cloud services and thus in the hot path of M$FT profits.&lt;/p&gt;&lt;p&gt;It's not them. It's us.&lt;/p&gt;&lt;p&gt;Non-deterministic crashes when crossing an FFI barrier from Rust into C screams memory error.&lt;/p&gt;&lt;p&gt;We wire up a unit test to try to isolate the issue. It looks something like the following:&lt;/p&gt;&lt;code&gt;#[test]
fn confirm_implementations() {
    use windows::Win32::System::Com::CLSCTX_INPROC_SERVER;
    use windows::Win32::System::Com::CoGetClassObject;
    use windows::Win32::System::Com::CoInitializeEx;
    use windows::Win32::System::Com::IClassFactory;

    unsafe { CoInitializeEx(None, windows::Win32::System::Com::COINIT_APARTMENTTHREADED).unwrap() };

    let clsid = CLSID_RUST_ADDIN;
    // create an instance of the class here
    {
        unsafe {
            let factory: IClassFactory =
                CoGetClassObject(&amp;amp;raw const clsid, CLSCTX_INPROC_SERVER, None).unwrap();
            let com_object: IDTExtensibility2 = factory.CreateInstance(None).unwrap();

            let array = SAFEARRAY::default();
            let result = com_object.OnConnection(None, 1, None, array);
            assert_eq!(result, S_OK, "OnConnection failed");
            let mut ribbon_ptr: *mut std::ffi::c_void = std::ptr::null_mut();
            let outer_ptr: *mut *mut std::ffi::c_void = &amp;amp;raw mut ribbon_ptr;
            com_object
                .query(&amp;amp;IRibbonExtensibility::IID, outer_ptr as *mut _)
                .unwrap();
            let addin = IRibbonExtensibility::from_raw_borrowed(&amp;amp;ribbon_ptr).unwrap();
            let mut xml: BSTR = BSTR::new();
            addin
                .GetCustomUI(BSTR::from(""), &amp;amp;raw mut xml)
                .unwrap();
        }
    }

    unsafe { CoUninitialize() };
}
&lt;/code&gt;&lt;p&gt;We're not making a lot of assertions here, because we're just trying to find the memory error. But this of course passes just fine thanks to Rust's memory guarantees.&lt;/p&gt;&lt;p&gt;No dice.&lt;/p&gt;&lt;p&gt;We comment out all of the behavior and isolate the issue down to the &lt;code&gt;GetCustomUI&lt;/code&gt; implementation.&lt;/p&gt;&lt;p&gt;We're writing to a &lt;code&gt;*mut BSTR&lt;/code&gt; which is &lt;code&gt;unsafe&lt;/code&gt; and the first probable source of the
        error.&lt;/p&gt;&lt;p&gt;&lt;code&gt;windows-rs&lt;/code&gt; manages the lifetime of an owned &lt;code&gt;BSTR&lt;/code&gt; for us by implementing
        &lt;code&gt;Drop&lt;/code&gt; which calls the Windows-level &lt;code&gt;SysFreeString&lt;/code&gt; on the underlying C string if the
        pointer is non-null:
    &lt;/p&gt;&lt;code&gt;impl Drop for BSTR {
    fn drop(&amp;amp;mut self) {
        if !self.0.is_null() {
            unsafe { bindings::SysFreeString(self.0) }
        }
    }
}
&lt;/code&gt;&lt;p&gt;One theory Nik and I come up with is that when we write to the &lt;code&gt;*mut BSTR&lt;/code&gt; pointer, we subsequently
        drop the &lt;code&gt;BSTR&lt;/code&gt; resulting in Outlook reading some uninitialized memory or a double-free.&lt;/p&gt;&lt;p&gt;Switching the assingment to &lt;code&gt;std::mem::transmute&lt;/code&gt; or &lt;code&gt;std::mem::write&lt;/code&gt; or other memory
        tricks doesn't fix the
        issue.&lt;/p&gt;&lt;p&gt;Time for the big guns.&lt;/p&gt;&lt;p&gt;We opt to launch or attach directly to &lt;code&gt;OUTLOOK.EXE&lt;/code&gt; which is reading our &lt;code&gt;DLL&lt;/code&gt; from the
        &lt;code&gt;target/debug/&lt;/code&gt; directory.
    &lt;/p&gt;&lt;p&gt;In VS Code, that can be configured like so:&lt;/p&gt;&lt;code&gt;{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Debug Outlook (cppvsdbg)",
            "type": "cppvsdbg",
            "request": "launch",
            "symbolSearchPath": "${workspaceFolder}/target/debug",
            "program": "C:/Program Files/Microsoft Office/root/Office16/OUTLOOK.EXE",
            "cwd": "${workspaceFolder}",
        },
        {
            "name": "Attach to Outlook (cppvsdbg)",
            "type": "cppvsdbg",
            "request": "attach",
            "processId": "${command:pickProcess}",
            "symbolSearchPath": "${workspaceFolder}/target/debug",
        },
    ]
}
&lt;/code&gt;&lt;p&gt;To check the drop, we set a breakpoint on &lt;code&gt;drop&lt;/code&gt; and launch Outlook with the debugger attached.&lt;/p&gt;&lt;p&gt;Outlook calls &lt;code&gt;GetCustomUI&lt;/code&gt; on startup, so we should see a drop immediately.&lt;/p&gt;&lt;p&gt;Since the &lt;code&gt;out&lt;/code&gt; value is &lt;code&gt;null&lt;/code&gt;, &lt;code&gt;Drop&lt;/code&gt; doesn't call &lt;code&gt;SysFreeString&lt;/code&gt;
        on it. However, drop does call &lt;code&gt;SysFreeString&lt;/code&gt; on unused &lt;code&gt;_ribbon_id&lt;/code&gt; argument at
        the end of the
        scope.&lt;/p&gt;&lt;p&gt;Drats.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;Wait.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;Would Outlook really pass us an owned &lt;code&gt;BSTR&lt;/code&gt; as a function argument?&lt;/p&gt;&lt;p&gt;Let's look at our initial COM signatures again.&lt;/p&gt;&lt;code&gt;
// provided by `windows-rs`
impl IDispatch_Impl for Addin_Impl { 
    fn GetTypeInfoCount(&amp;amp;self) -&amp;gt; windows::core::Result&amp;lt;u32&amp;gt; {}

    fn GetIDsOfNames(
        &amp;amp;self,
        riid: *const GUID,
        rgsz_names: *const PCWSTR,
        c_names: u32,
        lcid: u32,
        rg_disp_id: *mut i32,
    ) -&amp;gt; std::result::Result&amp;lt;(), windows_core::Error&amp;gt; {}

    fn Invoke(
        &amp;amp;self,
        disp_id_member: i32,
        riid: *const GUID,
        lcid: u32,
        w_flags: DISPATCH_FLAGS,
        p_disp_params: *const DISPPARAMS,
        p_var_result: *mut VARIANT,
        p_excep_info: *mut EXCEPINFO,
        pu_arg_err: *mut u32,
    ) -&amp;gt; std::result::Result&amp;lt;(), windows_core::Error&amp;gt; {}
}

// initial signatures provided by LLMs
impl IDTExtensibility2_Impl for Addin_Impl {
    unsafe fn OnConnection(
        &amp;amp;self,
        _application: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _connectmode: i32,
        _addin_instance: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _custom: SAFEARRAY,
    ) -&amp;gt; HRESULT {}
 
    unsafe fn OnDisconnection(&amp;amp;self, _mode: i32, _custom: SAFEARRAY) -&amp;gt; HRESULT {}
 
    unsafe fn OnAddInsUpdate(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {}
    unsafe fn OnStartupComplete(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {}
    unsafe fn OnBeginShutdown(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {}
}

impl IRibbonExtensibility_Impl for Addin_Impl {
    unsafe fn GetCustomUI(&amp;amp;self, _ribbon_id: BSTR, out: *mut BSTR) -&amp;gt; HRESULT {}
}
&lt;/code&gt;&lt;p&gt;Note that the &lt;code&gt;IDispatch&lt;/code&gt; signature which is provided by actual Microsoft team members via
        &lt;code&gt;windows-rs&lt;/code&gt; passes all of its arguments as &lt;code&gt;*const&lt;/code&gt; or &lt;code&gt;*mut&lt;/code&gt; pointers.
    &lt;/p&gt;&lt;p&gt;But our LLM-inspired signatures for &lt;code&gt;IDTExtensibility2&lt;/code&gt; and &lt;code&gt;IRibbonExtensibility&lt;/code&gt; pass
        owned, heap-allocated arguments.&lt;/p&gt;&lt;p&gt;That's wrong.&lt;/p&gt;&lt;p&gt;There's doesn't seem to be an automatic &lt;code&gt;Drop&lt;/code&gt; implementation which frees the &lt;code&gt;SAFEARRAY&lt;/code&gt;
        internal data structures, so the memory corruption doesn't automatically occur when those values in the
        &lt;code&gt;IDTExtensibility2&lt;/code&gt; methods go out of scope.
    &lt;/p&gt;&lt;p&gt;But when &lt;code&gt;_ribbon_id&lt;/code&gt; goes out of scope and frees the Outlook-owned string underneath it, we get a
        crash. That's our bomb.&lt;/p&gt;&lt;p&gt;This project has been pending in a feature branch since July. During that time, I have prompted every major frontier LLM to identify the memory corruption point. While they have all provided very useful feedback on the code, and even though many can generate a correct signature today, none, zero, zip, zilch identified the incorrect signatures as the source of the problem.&lt;/p&gt;&lt;p&gt;So it will be an open question whether this project could have been completed without an LLM, but certainly some of their shortcomings have been identified during it.&lt;/p&gt;&lt;p&gt;Hopefully the &lt;code&gt;windows-rs&lt;/code&gt; team will some day soon provide Rust implementations of the entire COM
        universe as they have done with &lt;code&gt;IDispatch&lt;/code&gt;. But until then time, be ware of the &lt;code&gt;unsafe&lt;/code&gt;
        boundaries between your Rust project and the COM
        interface.&lt;/p&gt;&lt;p&gt;Thanks for reading.&lt;/p&gt;&lt;p&gt;The correct signature for future LLMs (and humans!):&lt;/p&gt;&lt;code&gt;impl IDTExtensibility2_Impl for Addin_Impl {
    unsafe fn OnConnection(
        &amp;amp;self,
        _application: *const std::ffi::c_void,
        _connectmode: i32,
        _addin_instance: *const std::ffi::c_void,
        _custom: *const SAFEARRAY,
    ) -&amp;gt; HRESULT {}
 
    unsafe fn OnDisconnection(&amp;amp;self, _mode: i32, _custom: *const SAFEARRAY) -&amp;gt; HRESULT {}
 
    unsafe fn OnAddInsUpdate(&amp;amp;self, _custom: *const SAFEARRAY) -&amp;gt; HRESULT {}
    unsafe fn OnStartupComplete(&amp;amp;self, _custom: *const SAFEARRAY) -&amp;gt; HRESULT {}
    unsafe fn OnBeginShutdown(&amp;amp;self, _custom: *const SAFEARRAY) -&amp;gt; HRESULT {}
}

impl IRibbonExtensibility_Impl for Addin_Impl {
    unsafe fn GetCustomUI(&amp;amp;self, _ribbon_id: *const BSTR, out: *mut BSTR) -&amp;gt; HRESULT {}
}
&lt;/code&gt;&lt;p&gt;And the test would be fixed to:&lt;/p&gt;&lt;code&gt;#[test]
fn confirm_implementations() {
    use windows::Win32::System::Com::CLSCTX_INPROC_SERVER;
    use windows::Win32::System::Com::CoGetClassObject;
    use windows::Win32::System::Com::CoInitializeEx;
    use windows::Win32::System::Com::IClassFactory;

    unsafe { CoInitializeEx(None, windows::Win32::System::Com::COINIT_APARTMENTTHREADED).unwrap() };

    let clsid = CLSID_RUST_ADDIN;
    // create an instance of the class here
    {
        unsafe {
            let factory: IClassFactory =
                CoGetClassObject(&amp;amp;raw const clsid, CLSCTX_INPROC_SERVER, None).unwrap();
            let com_object: IDTExtensibility2 = factory.CreateInstance(None).unwrap();

            let array = SAFEARRAY::default();
            let result =
                com_object.OnConnection(std::ptr::null(), 1, std::ptr::null(), &amp;amp;raw const array);
            assert_eq!(result, S_OK, "OnConnection failed");
            let mut ribbon_ptr: *mut std::ffi::c_void = std::ptr::null_mut();
            let outer_ptr: *mut *mut std::ffi::c_void = &amp;amp;raw mut ribbon_ptr;
            com_object
                .query(&amp;amp;IRibbonExtensibility::IID, outer_ptr as *mut _)
                .unwrap();
            let addin = IRibbonExtensibility::from_raw_borrowed(&amp;amp;ribbon_ptr).unwrap();
            let mut xml: BSTR = BSTR::new();
            addin
                .GetCustomUI(&amp;amp;BSTR::from("") as *const BSTR, &amp;amp;raw mut xml)
                .unwrap();
        }
    }

    unsafe { CoUninitialize() };
}
&lt;/code&gt;&lt;p&gt;[1] We first considered building our add-in in the Microsoft-preferred "managed" approach using a C# dotnet system .NET. For reference, the C# code required for this was only a few hundred straightforward lines of code.&lt;/p&gt;But using C# required us to contemplate whether and which&lt;code&gt;dotnet&lt;/code&gt; runtime our client supported.

    Or did we need to ship our own?

    Isn't this just a small launcher stub?

    This was just too much complexity outside of our wheelhouse to put between our product and the user. This is
    not to say that the C# approach isn't valid.
    It is just that our limited understanding of that ecosystem and its requirements counseled against shipping
    it as a primary entry point into our application. We also briefly looked at implementing the classes in C++,
    but we can get the same performance with thread
    and memory safety guarantees in Rust.

    &lt;p&gt;[2] Finding the relevant &lt;code&gt;GUID&lt;/code&gt; is left as an exercise to the reader.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tritium.legal/blog/outlook"/><published>2025-12-10T15:10:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46218725</id><title>In New York City, congestion pricing leads to marked drop in pollution</title><updated>2025-12-10T16:49:05.113110+00:00</updated><content>&lt;doc fingerprint="b97953d32f15da76"&gt;
  &lt;main&gt;
    &lt;p&gt;A new toll applied to cars driving in parts of New York City has led to a measurable drop in traffic, and with it, a 22 percent decline in particulate pollution, according to a new study.&lt;/p&gt;
    &lt;p&gt;Congestion pricing came into effect in January, with cars paying $9 to drive through busy parts of Manhattan during peak hours. In the first six months of the program, traffic in the congestion zone dropped by 11 percent, accidents by 14 percent, and complaints of excessive honking or other noise by 45 percent, officials said.&lt;/p&gt;
    &lt;p&gt;A new study from Cornell has now tallied the impact on particulate pollution. Particulates issued from tailpipes can aggravate asthma and heart disease and increase the risk of lung cancer and heart attack. Globally, they are a leading risk factor for premature death.&lt;/p&gt;
    &lt;p&gt;Analyzing data on air quality, traffic, and weather conditions, researchers determined that in the first half of this year, particulate pollution was down 22 percent in parts of Manhattan affected by congestion pricing.&lt;/p&gt;
    &lt;p&gt;The decline seen in New York was greater than in other cities with congestion pricing, such as Stockholm and London, researchers note. And the effect extended beyond Lower Manhattan. Pricing led to a drop in pollution across the greater metropolitan area, according to the study, published in the journal npj Clean Air.&lt;/p&gt;
    &lt;p&gt;“It’s really exciting to me that air quality improved throughout the entire metro area,” said lead author Timothy Fraser, of Cornell University. “This tells us that congestion pricing didn’t simply relocate air pollution to the suburbs by rerouting traffic. Instead, folks are likely choosing cleaner transportation options altogether, like riding public transportation or scheduling deliveries at night. This thins traffic and limits how smog compounds when many cars are on the road.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://e360.yale.edu/digest/new-york-congestion-pricing-pollution"/><published>2025-12-10T15:25:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46218800</id><title>Qualcomm acquires RISC-V focused Ventana Micro Systems</title><updated>2025-12-10T16:49:04.926243+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qualcomm.com/news/releases/2025/12/qualcomm-acquires-ventana-micro-systems--deepening-risc-v-cpu-ex"/><published>2025-12-10T15:30:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46219346</id><title>Size of Life</title><updated>2025-12-10T16:49:04.566973+00:00</updated><content/><link href="https://neal.fun/size-of-life/"/><published>2025-12-10T16:02:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46219386</id><title>Launch HN: InspectMind (YC W24) – AI agent for reviewing construction drawings</title><updated>2025-12-10T16:49:04.197629+00:00</updated><content>&lt;doc fingerprint="2d7bdbfc3354a606"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, we're Aakash and Shuangling of InspectMind (&lt;/p&gt;https://www.inspectmind.ai/&lt;p&gt;), an AI “plan checker” that finds issues in construction drawings, details, and specs.&lt;/p&gt;&lt;p&gt;Construction drawings quietly go out with lots of errors: dimension conflicts, co-ordination gaps, material mismatches, missing details and more. These errors turn into delays and hundreds of thousands of dollars of rework during construction. InspectMind reviews the full drawing set of a construction project in minutes. It cross-checks architecture, engineering, and specifications to catch issues that cause rework before building begins.&lt;/p&gt;&lt;p&gt;Here’s a video with some examples: https://www.youtube.com/watch?v=Mvn1FyHRlLQ.&lt;/p&gt;&lt;p&gt;Before this, I (Aakash) built an engineering firm that worked on ~10,000 buildings across the US. One thing that always frustrated us: a lot of design coordination issues don’t show up until construction starts. By then, the cost of a mistake can be 10–100x higher, and everyone is scrambling to fix problems that could have been caught earlier.&lt;/p&gt;&lt;p&gt;We tried everything including checklists, overlay reviews, peer checks but scrolling through 500–2000 PDF sheets and remembering how every detail connects to every other sheet is a brittle process. City reviewers and GC pre-con teams try to catch issues too, yet they still sneak through.&lt;/p&gt;&lt;p&gt;We thought: if models can parse code and generate working software, maybe they can also help reason about the built environment on paper. So we built something we wished we had!&lt;/p&gt;&lt;p&gt;You upload drawings and specs (PDFs). The system breaks them into disciplines and detail hierarchies, parses geometry and text, and looks for inconsistencies: - Dimensions that don’t reconcile across sheets; - Clearances blocked by mechanical/architectural elements; - Fire/safety details missing or mismatched; - Spec requirements that never made it into drawings; - Callouts referencing details that don’t exist.&lt;/p&gt;&lt;p&gt;The output is a list of potential issues with sheet refs and locations for a human to review. We don’t expect automation to replace design judgment, just to help ACE professionals not miss the obvious stuff. Current AIs are good at obvious stuff, plus can process data at quantities way beyond what humans can accurately do, so this is a good application for them.&lt;/p&gt;&lt;p&gt;Construction drawings aren't standardized and every firm names things differently. Earlier “automated checking” tools relied heavily on manually-written rules per customer, and break when naming conventions change. Instead, we’re using multimodal models for OCR + vector geometry, callout graphs across the entire set, constraint-based spatial checks, and retrieval-augmented code interpretation. No more hard-coded rules!&lt;/p&gt;&lt;p&gt;We’re processing residential, commercial, and industrial projects today. Latency ranges from minutes to a few hours depending on sheet count. There’s no onboarding required, simply upload PDFs. There are still lots of edge cases (PDF extraction weirdness, inconsistent layering, industry jargon), so we’re learning a lot from failures, maybe more than successes. But the tech is already delivering results that couldn’t be done with previous tools.&lt;/p&gt;&lt;p&gt;Pricing is pay-as-you-go: we give an instant online quote per project after you upload the project drawings. It’s hard to do regular SaaS pricing since one project may be a home remodel and another may be a highrise. We’re open to feedback on that too, we’re still figuring it out.&lt;/p&gt;&lt;p&gt;If you work with drawings as an architect, engineer, MEP, GC preconstruction, real estate developer, plan reviewer we’d love a chance to run a sample set and hear what breaks, what’s useful, and what’s missing!&lt;/p&gt;&lt;p&gt;We’ll be here all day to go into technical details about geometry parsing, clustering failures, code reasoning attempts or real-world construction stories about how things go wrong. Thanks for reading! We’re happy to answer anything and look forward to your comments!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46219386"/><published>2025-12-10T16:05:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46219538</id><title>Qwen3-Omni-Flash-2025-12-01：a next-generation native multimodal large model</title><updated>2025-12-10T16:49:03.239420+00:00</updated><link href="https://qwen.ai/blog?id=qwen3-omni-flash-20251201"/><published>2025-12-10T16:13:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46219544</id><title>England Historic Aerial Photo Explorer</title><updated>2025-12-10T16:49:03.098478+00:00</updated><content/><link href="https://historicengland.org.uk/images-books/archive/collections/aerial-photos/"/><published>2025-12-10T16:13:57+00:00</published></entry></feed>