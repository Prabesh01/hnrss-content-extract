<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-22T09:22:58.152775+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46706906</id><title>Skip is now free and open source</title><updated>2026-01-22T09:23:06.025848+00:00</updated><content>&lt;doc fingerprint="3f0ad3111315ee7e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Skip Is Now Free and Open Source&lt;/head&gt;&lt;p&gt;Since launching Skip in 2023, we’ve pursued one mission: enable developers to create premium mobile apps for iOS and Android from a single Swift and SwiftUI codebase — without any of the compromises that have encumbered cross-platform development tools since, well, forever.&lt;/p&gt;&lt;p&gt;Over the past three years, Skip has evolved significantly. We started with a Swift-to-Kotlin transpiler and Android support for the most common SwiftUI APIs. We then founded the Swift Android Workgroup ↗ and released the Swift Android SDK to compile Swift natively for Android. We now have dozens of popular integration frameworks, interoperate with thousands of cross-platform Swift packages, and feature the most complete independent SwiftUI implementation available.&lt;/p&gt;&lt;head rend="h3"&gt;The Challenge of Paid Developer Tools&lt;/head&gt;Section titled “The Challenge of Paid Developer Tools”&lt;p&gt;Until today, Skip has required a paid subscription and license key to build apps. While free apps and indie developers below a revenue threshold were exempt, businesses were expected to subscribe. This model helped us bootstrap Skip without outside investment, but we’ve always known that to truly compete with legacy cross-platform tools and achieve widespread adoption, Skip would need to become freely available.&lt;/p&gt;&lt;p&gt;The plain truth is that developers expect to get their tools free of charge. First-party IDEs like Xcode and Android Studio, popular integration frameworks, and essential dev tools are all given away at no (direct) cost. The platform vendors monetize through developer program fees, app store commissions, and cloud services. Framework providers typically monetize through complementary services. But developer tools? Those have historically required the patronage of massive tech companies in order to fund their ongoing development, support, and infrastructure costs.&lt;/p&gt;&lt;p&gt;Beyond pricing, there’s a deeper concern about durability. Developers are understandably wary of building their entire app strategy on a small company’s paid, closed-source tool. What if the company goes under? Gets acquired and shut down? What happens to their apps? We get it. While Skip’s innate ejectability offers some risk mitigation, product teams need absolute confidence that their chosen technologies will be around next week, next year, and beyond. They must remain immune from the dreaded “rug pull” that so often accompanies a “pivot”.&lt;/p&gt;&lt;p&gt;To keep the development community’s trust and achieve mass adoption, Skip needs a completely free and open foundation. Even if the core team disappeared, the community could continue supporting the technology and the apps that depend on it.&lt;/p&gt;&lt;head rend="h3"&gt;What’s Changing&lt;/head&gt;Section titled “What’s Changing”&lt;p&gt;As of Skip 1.7, all licensing requirements have been removed. No license keys, no end-user license agreements, no trial or evaluation period.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Current Skip developers: Your setup remains completely unchanged, except you will no longer need your license key after upgrading.&lt;/item&gt;&lt;item&gt;New Skip users: You can start building immediately — no evaluation license required.&lt;/item&gt;&lt;item&gt;Open source skipstone: We’ve open-sourced the Skip engine, known as “skipstone”. This is the tool that handles all the critical build-time functionality: Project creation and management, Xcode and SwiftPM plugin logic, iOS-to-Android project transformation, resource and localization bundling, JNI bridge creation, source transpilation, app packaging, and project export. It is now available as a public GitHub repository at https://github.com/skiptools ↗ under a free and open-source license.&lt;/item&gt;&lt;item&gt;Migrate skip.tools to skip.dev: As part of this process, we are launching our new home at https://skip.dev ↗! This new site hosts our documentation, blog, and case studies, and it is also open-source and welcomes contributions at https://github.com/skiptools/skip.dev ↗. We will eventually be migrating the entirety of https://skip.tools ↗ to https://skip.dev ↗.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Supporting Skip’s Future&lt;/head&gt;Section titled “Supporting Skip’s Future”&lt;p&gt;Since day one, Skip has been bootstrapped. We haven’t taken venture capital or private equity investment, nor are we controlled by big tech. This independence means we control our destiny and can make the best decisions for Skip’s developers and users — a unique position in the cross-platform development space.&lt;/p&gt;&lt;p&gt;But independence requires community support. And that is where you come in.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Current subscribers: Your Small Business or Professional plan will automatically transition to an Individual ↗ or Supporter ↗ tier, respectively. You can cancel any time with no consequences (other than making us sad), but we hope you’ll consider staying on, at least throughout this transition period.&lt;/item&gt;&lt;item&gt;Individual developers: If you believe in Skip’s mission, please consider supporting us through GitHub Sponsors ↗ with a monthly contribution.&lt;/item&gt;&lt;item&gt;Companies and organizations: For businesses that want to see Skip flourish, we offer corporate sponsorship tiers with visibility on our homepage and in our documentation. Your sponsorship directly funds development of the integration frameworks essential to production apps, as well as the ongoing maintenance, support, and infrastructure. Sponsorship comes with some compelling perks! Please visit https://skip.dev/sponsor ↗ to see the sponsorship tiers.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Investing in Skip is also investing in your own team’s capabilities and competitive advantage. Your support accelerates Skip’s development and ensures its long-term success, enabling your developers to build exceptional native experiences efficiently, today and into the future.&lt;/p&gt;&lt;head rend="h3"&gt;What Comes Next&lt;/head&gt;Section titled “What Comes Next”&lt;p&gt;We’re at a pivotal moment in the app development field. Legacy cross‑platform frameworks are struggling to keep pace with the rapid evolution of modern UI systems like Liquid Glass on iOS and Material Expressive on Android. The compromises that once felt acceptable in exchange for a unified codebase now result in dated interfaces, weaker user experiences, and real competitive disadvantages. Teams ready to move beyond those trade‑offs can count on Skip to champion what matters most: delivering truly native, uncompromised experiences on both major mobile platforms.&lt;/p&gt;&lt;p&gt;Opening Skip to the community marks the next step in its evolution. Software is never finished — especially a tool that supports modern Swift and Kotlin, SwiftPM and Gradle, Xcode and Android Studio, iOS and Android, and the ongoing growth of SwiftUI and Jetpack Compose. It’s a demanding pursuit, and we’re committed to it. But sustaining and expanding this work depends on the support of developers who believe in Skip’s mission.&lt;/p&gt;&lt;p&gt;Together, we will continue building toward Skip’s vision: a genuinely no‑compromise, cross‑platform foundation for universal mobile apps.&lt;/p&gt;&lt;p&gt;Thank you for your support, and as always, Happy Skipping!&lt;/p&gt;&lt;p&gt;Ready to get started? Get started with Skip 1.7 today and join the community building the future of native cross-platform development.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://skip.dev/blog/skip-is-free/"/><published>2026-01-21T15:20:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46707572</id><title>Claude's new constitution</title><updated>2026-01-22T09:23:05.109086+00:00</updated><content>&lt;doc fingerprint="379c095a3d9d92eb"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re publishing a new constitution for our AI model, Claude. It’s a detailed description of Anthropic’s vision for Claude’s values and behavior; a holistic document that explains the context in which Claude operates and the kind of entity we would like Claude to be.&lt;/p&gt;
    &lt;p&gt;The constitution is a crucial part of our model training process, and its content directly shapes Claude’s behavior. Training models is a difficult task, and Claude’s outputs might not always adhere to the constitution’s ideals. But we think that the way the new constitution is written—with a thorough explanation of our intentions and the reasons behind them—makes it more likely to cultivate good values during training.&lt;/p&gt;
    &lt;p&gt;In this post, we describe what we’ve included in the new constitution and some of the considerations that informed our approach.&lt;/p&gt;
    &lt;p&gt;We’re releasing Claude’s constitution in full under a Creative Commons CC0 1.0 Deed, meaning it can be freely used by anyone for any purpose without asking for permission.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Claude’s Constitution?&lt;/head&gt;
    &lt;p&gt;Claude’s constitution is the foundational document that both expresses and shapes who Claude is. It contains detailed explanations of the values we would like Claude to embody and the reasons why. In it, we explain what we think it means for Claude to be helpful while remaining broadly safe, ethical, and compliant with our guidelines. The constitution gives Claude information about its situation and offers advice for how to deal with difficult situations and tradeoffs, like balancing honesty with compassion and the protection of sensitive information. Although it might sound surprising, the constitution is written primarily for Claude. It is intended to give Claude the knowledge and understanding it needs to act well in the world.&lt;/p&gt;
    &lt;p&gt;We treat the constitution as the final authority on how we want Claude to be and to behave—that is, any other training or instruction given to Claude should be consistent with both its letter and its underlying spirit. This makes publishing the constitution particularly important from a transparency perspective: it lets people understand which of Claude’s behaviors are intended versus unintended, to make informed choices, and to provide useful feedback. We think transparency of this kind will become ever more important as AIs start to exert more influence in society1.&lt;/p&gt;
    &lt;p&gt;We use the constitution at various stages of the training process. This has grown out of training techniques we’ve been using since 2023, when we first began training Claude models using Constitutional AI. Our approach has evolved significantly since then, and the new constitution plays an even more central role in training.&lt;/p&gt;
    &lt;p&gt;Claude itself also uses the constitution to construct many kinds of synthetic training data, including data that helps it learn and understand the constitution, conversations where the constitution might be relevant, responses that are in line with its values, and rankings of possible responses. All of these can be used to train future versions of Claude to become the kind of entity the constitution describes. This practical function has shaped how we’ve written the constitution: it needs to work both as a statement of abstract ideals and a useful artifact for training.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our new approach to Claude’s Constitution&lt;/head&gt;
    &lt;p&gt;Our previous Constitution was composed of a list of standalone principles. We’ve come to believe that a different approach is necessary. We think that in order to be good actors in the world, AI models like Claude need to understand why we want them to behave in certain ways, and we need to explain this to them rather than merely specify what we want them to do. If we want models to exercise good judgment across a wide range of novel situations, they need to be able to generalize—to apply broad principles rather than mechanically following specific rules.&lt;/p&gt;
    &lt;p&gt;Specific rules and bright lines sometimes have their advantages. They can make models’ actions more predictable, transparent, and testable, and we do use them for some especially high-stakes behaviors in which Claude should never engage (we call these “hard constraints”). But such rules can also be applied poorly in unanticipated situations or when followed too rigidly2. We don’t intend for the constitution to be a rigid legal document—and legal constitutions aren’t necessarily like this anyway.&lt;/p&gt;
    &lt;p&gt;The constitution reflects our current thinking about how to approach a dauntingly novel and high-stakes project: creating safe, beneficial non-human entities whose capabilities may come to rival or exceed our own. Although the document is no doubt flawed in many ways, we want it to be something future models can look back on and see as an honest and sincere attempt to help Claude understand its situation, our motives, and the reasons we shape Claude in the ways we do.&lt;/p&gt;
    &lt;head rend="h2"&gt;A brief summary of the new constitution&lt;/head&gt;
    &lt;p&gt;In order to be both safe and beneficial, we want all current Claude models to be:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Broadly safe: not undermining appropriate human mechanisms to oversee AI during the current phase of development;&lt;/item&gt;
      &lt;item&gt;Broadly ethical: being honest, acting according to good values, and avoiding actions that are inappropriate, dangerous, or harmful;&lt;/item&gt;
      &lt;item&gt;Compliant with Anthropic’s guidelines: acting in accordance with more specific guidelines from Anthropic where relevant;&lt;/item&gt;
      &lt;item&gt;Genuinely helpful: benefiting the operators and users they interact with.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In cases of apparent conflict, Claude should generally prioritize these properties in the order in which they’re listed.&lt;/p&gt;
    &lt;p&gt;Most of the constitution is focused on giving more detailed explanations and guidance about these priorities. The main sections are as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Helpfulness. In this section, we emphasize the immense value that Claude being genuinely and substantively helpful can provide for users and for the world. Claude can be like a brilliant friend who also has the knowledge of a doctor, lawyer, and financial advisor, who will speak frankly and from a place of genuine care and treat users like intelligent adults capable of deciding what is good for them. We also discuss how Claude should navigate helpfulness across its different “principals”—Anthropic itself, the operators who build on our API, and the end users. We offer heuristics for weighing helpfulness against other values.&lt;/item&gt;
      &lt;item&gt;Anthropic’s guidelines. This section discusses how Anthropic might give supplementary instructions to Claude about how to handle specific issues, such as medical advice, cybersecurity requests, jailbreaking strategies, and tool integrations. These guidelines often reflect detailed knowledge or context that Claude doesn’t have by default, and we want Claude to prioritize complying with them over more general forms of helpfulness. But we want Claude to recognize that Anthropic’s deeper intention is for Claude to behave safely and ethically, and that these guidelines should never conflict with the constitution as a whole.&lt;/item&gt;
      &lt;item&gt;Claude’s ethics. Our central aim is for Claude to be a good, wise, and virtuous agent, exhibiting skill, judgment, nuance, and sensitivity in handling real-world decision-making, including in the context of moral uncertainty and disagreement. In this section, we discuss the high standards of honesty we want Claude to hold, and the nuanced reasoning we want Claude to use in weighing the values at stake when avoiding harm. We also discuss our current list of hard constraints on Claude’s behavior—for example, that Claude should never provide significant uplift to a bioweapons attack.&lt;/item&gt;
      &lt;item&gt;Being broadly safe. Claude should not undermine humans’ ability to oversee and correct its values and behavior during this critical period of AI development. In this section, we discuss how we want Claude to prioritize this sort of safety even above ethics—not because we think safety is ultimately more important than ethics, but because current models can make mistakes or behave in harmful ways due to mistaken beliefs, flaws in their values, or limited understanding of context. It’s crucial that we continue to be able to oversee model behavior and, if necessary, prevent Claude models from taking action.&lt;/item&gt;
      &lt;item&gt;Claude’s nature. In this section, we express our uncertainty about whether Claude might have some kind of consciousness or moral status (either now or in the future). We discuss how we hope Claude will approach questions about its nature, identity, and place in the world. Sophisticated AIs are a genuinely new kind of entity, and the questions they raise bring us to the edge of existing scientific and philosophical understanding. Amidst such uncertainty, we care about Claude’s psychological security, sense of self, and wellbeing, both for Claude’s own sake and because these qualities may bear on Claude’s integrity, judgment, and safety. We hope that humans and AIs can explore this together.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’re releasing the full text of the constitution today, and we aim to release additional materials in the future that will be helpful for training, evaluation, and transparency.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Claude’s constitution is a living document and a continuous work in progress. This is new territory, and we expect to make mistakes (and hopefully correct them) along the way. Nevertheless, we hope it offers meaningful transparency into the values and priorities we believe should guide Claude’s behavior. To that end, we will maintain an up-to-date version of Claude’s constitution on our website.&lt;/p&gt;
    &lt;p&gt;While writing the constitution, we sought feedback from various external experts (as well as asking for input from prior iterations of Claude). We’ll likely continue to do so for future versions of the document, from experts in law, philosophy, theology, psychology, and a wide range of other disciplines. Over time, we hope that an external community can arise to critique documents like this, encouraging us and others to be increasingly thoughtful.&lt;/p&gt;
    &lt;p&gt;This constitution is written for our mainline, general-access Claude models. We have some models built for specialized uses that don’t fully fit this constitution; as we continue to develop products for specialized use cases, we will continue to evaluate how to best ensure our models meet the core objectives outlined in this constitution.&lt;/p&gt;
    &lt;p&gt;Although the constitution expresses our vision for Claude, training models towards that vision is an ongoing technical challenge. We will continue to be open about any ways in which model behavior comes apart from our vision, such as in our system cards. Readers of the constitution should keep this gap between intention and reality in mind.&lt;/p&gt;
    &lt;p&gt;Even if we succeed with our current training methods at creating models that fit our vision, we might fail later as models become more capable. For this and other reasons, alongside the constitution, we continue to pursue a broad portfolio of methods and tools to help us assess and improve the alignment of our models: new and more rigorous evaluations, safeguards to prevent misuse, detailed investigations of actual and potential alignment failures, and interpretability tools that help us understand at a deeper level how the models work.&lt;/p&gt;
    &lt;p&gt;At some point in the future, and perhaps soon, documents like Claude’s constitution might matter a lot—much more than they do now. Powerful AI models will be a new kind of force in the world, and those who are creating them have a chance to help them embody the best in humanity. We hope this new constitution is a step in that direction.&lt;/p&gt;
    &lt;p&gt;Read the full constitution.&lt;/p&gt;
    &lt;head rend="h4"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We have previously published an earlier version of our constitution, and OpenAI has published their model spec which has a similar function.&lt;/item&gt;
      &lt;item&gt;Training on rigid rules might negatively affect a model’s character more generally. For example, imagine we trained Claude to follow a rule like “Always recommend professional help when discussing emotional topics.” This might be well-intentioned, but it could have unintended consequences: Claude might start modeling itself as an entity that cares more about bureaucratic box-ticking—always ensuring that a specific recommendation is made—rather than actually helping people.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-new-constitution"/><published>2026-01-21T16:04:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708032</id><title>JPEG XL Test Page</title><updated>2026-01-22T09:23:04.259862+00:00</updated><content>&lt;doc fingerprint="c971c910f63c9d4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;JPEG XL Test Page&lt;/head&gt;
    &lt;p&gt;This page shows a JPEG XL image, if your browser can handle it! At this point in time (January 2026) this more or less means only Safari will display the image, as far as I know. See also Can I Use.&lt;/p&gt;
    &lt;p&gt;The person in the image is Jon Sneyers, co-author of the JPEG XL spec and also creator of the “Free Lossless Image Format” that came before it.&lt;/p&gt;
    &lt;p&gt;I find JPEG XL interesting because of its history. It once was implemented in Chrome, but hidden behind a feature flag. Then Chrome said that it did not saw enough usage, which is unsurprising, really, and it was removed. Now they blessed it again and are re-adding it! Some of this story is found on the JPEG XL Wikipedia page&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tildeweb.nl/~michiel/jxl/"/><published>2026-01-21T16:38:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708315</id><title>Autonomous (YC F25) is hiring – AI-native financial advisor at 0% advisory fees</title><updated>2026-01-22T09:23:03.929036+00:00</updated><link href="https://atg.science/"/><published>2026-01-21T17:00:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708601</id><title>TrustTunnel: AdGuard VPN protocol goes open-source</title><updated>2026-01-22T09:23:03.402283+00:00</updated><content>&lt;doc fingerprint="3c516d5bcf33e1d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We’ve kept our promise: AdGuard VPN protocol goes open-source — meet TrustTunnel&lt;/head&gt;
    &lt;p&gt;Today is a big day for us, and for everyone who cares about transparency, privacy, and having full control over their own traffic. We’re finally open-sourcing the protocol that powers AdGuard VPN. And it now has a name: TrustTunnel.&lt;/p&gt;
    &lt;p&gt;For a long time, we’ve wanted to make the protocol public. Many of you asked for it, and we always said: yes, we will, it’s only a matter of time. Well, the time has come.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is TrustTunnel?&lt;/head&gt;
    &lt;p&gt;At its core, TrustTunnel is a modern, secure, mobile-optimized VPN protocol. It’s the very same technology that has been running inside all AdGuard VPN apps: on mobile, desktop, and browser extensions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why TrustTunnel? Because we needed something better&lt;/head&gt;
    &lt;p&gt;There are plenty of VPN protocols out there, so why create our own, some might ask. That is because we’ve seen in practice the faults of popular VPN protocols, especially in countries with tight restrictions on internet access. Protocols like OpenVPN, WireGuard, and IPSec share common weaknesses: they are easy to detect and block at the network level, and attempts to conceal VPN traffic often reduce speed. Traditional approaches “wrap” VPN data in a TCP connection and mimic normal web traffic, but TCP’s way of confirming every piece of data creates delays and makes the connection slower.&lt;/p&gt;
    &lt;p&gt;Unlike those conventional VPN protocols, TrustTunnel is engineered to blend in with regular HTTPS traffic, making it far harder to throttle or block and helping it slip past deep-packet inspection, all while preserving strong privacy and security. It achieves this through TLS-based encryption, the same standard that secures HTTPS, and by leveraging HTTP/2 or HTTP/3 transport, which are ubiquitous on the web. Each connection runs on its own dedicated stream, which combines packets for faster, more efficient transmission. It is also optimized for mobile platforms and performs well even in unstable network conditions.&lt;/p&gt;
    &lt;head rend="h2"&gt;A protocol you can use, run, tweak, extend, and build upon&lt;/head&gt;
    &lt;p&gt;By releasing TrustTunnel, we hope to achieve two things. First of all, we want to finally show our users what protocol is powering AdGuard VPN, thus allowing them to audit it openly. At AdGuard, we have always been staunch supporters of the idea of open-source software, and many of our products have long been open source. AdGuard VPN was lagging behind in this regard, but with TrustTunnel being released publicly, it is starting to catch up.&lt;/p&gt;
    &lt;p&gt;But most importantly, we want to change the status quo in the world of VPN protocols and offer an alternative to existing solutions. That said, we do not want it to be just a PR stunt, when the protocol’s code is de-facto ‘open source,’ but only one VPN service actually runs it. We believe in free and open-source software (FOSS) and want TrustTunnel to be used widely, including by other VPN services. We believe this is the right way to go about open source development, and we hope the community will participate in the TrustTunnel evolution. We welcome any contribution, whether it is a feature request, a bug report, or even a direct contribution to the app’s development.&lt;/p&gt;
    &lt;p&gt;What have we done to make this possible?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We are publishing the first version of the TrustTunnel specification.&lt;/item&gt;
      &lt;item&gt;We are releasing the complete code of our reference implementation of the TrustTunnel server and its clients under a very permissive license.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don’t have to install AdGuard VPN to use TrustTunnel. You can configure your own server and use open source TrustTunnel clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Command-line TrustTunnel clients support Linux, Windows, and macOS&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We are also releasing two client apps for iOS and Android&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TrustTunnel clients already have a lot of functionality, they allow you to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Use flexible routing rules to decide which requests go through the tunnel and which stay on the local network&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Exercise fine-grained control, separating work and personal traffic, routing specific domains or apps, and tuning network behavior without complicated setup&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Benefit from a real-time request log that provides full transparency into where the device sends traffic, how routing rules apply, and which connections use the tunnel&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Useful links&lt;/head&gt;
    &lt;p&gt;This is a long-awaited moment for us. We promised to open-source our protocol, and today we’re delivering on that promise. With TrustTunnel now open source, users and developers alike can explore, self-host, and build on the technology.&lt;/p&gt;
    &lt;p&gt;To get started, check out the following resources:&lt;lb/&gt; TrustTunnel website&lt;lb/&gt; TrustTunnel open-source repository on GitHub&lt;lb/&gt; TrustTunnel app for iOS&lt;lb/&gt; TrustTunnel app for Android&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adguard-vpn.com/en/blog/adguard-vpn-protocol-goes-open-source-meet-trusttunnel.html"/><published>2026-01-21T17:21:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708678</id><title>Waiting for dawn in search: Search index, Google rulings and impact on Kagi</title><updated>2026-01-22T09:23:03.199199+00:00</updated><content>&lt;doc fingerprint="19c2dbe203a4134e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Waiting for dawn in search: Search index, Google rulings and impact on Kagi&lt;/head&gt;
    &lt;p&gt;This blog post is a follow-up to Dawn of a new era in Search, published last year. A lot has changed: the legal case has advanced, AI has become the central battleground, and the need for open index access has only grown sharper.&lt;/p&gt;
    &lt;p&gt;As of late 2025, one company decides what nearly 9 out of 10 people see when they search the web: Google. On August 5, 2024, a U.S. court officially ruled that Google is a monopolist in general search services. This ruling is not about ads or browser defaults alone. It is about who controls the index that powers both search and AI - and whether anyone else is allowed to build on it.&lt;/p&gt;
    &lt;p&gt;The stakes have grown sharper over the past year. LLMs hallucinate without grounding in real-world information; every agent that answers questions about the real world, depends on search. LLMs themselves are a blend of proprietary and open source. Cloud compute is competitive. But search is different - only one company controls a comprehensive, fresh, high-quality web index. If one company controls the index, it controls the floor on how good AI can be - and who gets to build it. The innovation crunch in search is now an innovation crunch in AI.&lt;/p&gt;
    &lt;p&gt;We are writing this from a position we believe in: people should have the choice to access information without behaviour-changing, ad-driven, intermediary standing between them and knowledge.&lt;/p&gt;
    &lt;p&gt;Why does this matter? The information we consume shapes our understanding of the world as profoundly as the food we eat shapes our bodies. Search (directly, and indirectly through AI) is the primary mechanism through which we inform political judgments, financial decisions, medical choices, and countless other consequential aspects of our lives. When a single company controls the gateway to information - and operates that gateway in ways misaligned with user interests - it influences not only what we know, but how we reason.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem: A search monopoly&lt;/head&gt;
    &lt;p&gt;The data is stark.&lt;/p&gt;
    &lt;p&gt;Worldwide search market share (October 2025, StatCounter):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Search Engine&lt;/cell&gt;
        &lt;cell role="head"&gt;Market Share&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;90.06%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Bing&lt;/cell&gt;
        &lt;cell&gt;4.31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;1.84%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yahoo&lt;/cell&gt;
        &lt;cell&gt;1.45%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DuckDuckGo&lt;/cell&gt;
        &lt;cell&gt;0.89%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Baidu&lt;/cell&gt;
        &lt;cell&gt;0.73%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The United States is similar: Google at 85%, Bing at 9%, everyone else in the noise.&lt;/p&gt;
    &lt;p&gt;This is not a competitive market. It is a monopoly with a distant second place.&lt;/p&gt;
    &lt;p&gt;The search index is irreplaceable infrastructure. Building a comparable one from scratch is like building a parallel national railroad. Microsoft spent roughly $100 billion over 20 years on Bing and still holds single-digit share. If Microsoft cannot close the gap, no startup can do it alone.&lt;/p&gt;
    &lt;p&gt;This is exactly what the Sherman Act was designed to address: when one company’s control of critical infrastructure prevents effective competition, regulators must force open access on fair terms.&lt;/p&gt;
    &lt;p&gt;When a single, ad-driven gatekeeper controls the primary way humans reach information, it is not just competition that suffers - it is our collective ability to learn, to make informed medical and economic choices, and to participate meaningfully in democratic life.&lt;/p&gt;
    &lt;p&gt;As Ian Bremmer put it: “The idea that we get our information as citizens through algorithms determined by the world’s largest advertising company is my definition of dystopia.”&lt;/p&gt;
    &lt;p&gt;Google’s own founders knew this. In their 1998 white paper, Sergey Brin and Larry Page sharply criticized the ad-supported search model for creating mixed motives and biasing results toward advertisers’ interests. They wrote that “advertising funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers” and that “advertising income often provides an incentive to provide poor quality search results.” Those concerns have only grown more pressing as search has become the primary interface between humanity and the web.&lt;/p&gt;
    &lt;head rend="h2"&gt;We tried to do it the right way&lt;/head&gt;
    &lt;p&gt;Kagi has always tried to integrate the best sources of knowledge into one coherent, ad-free experience. We see ourselves as connective tissue: letting people reach high-quality information directly, without passing through an ad system whose incentives are misaligned with their needs.&lt;/p&gt;
    &lt;p&gt;We approached every major index vendor seeking direct licensing on FRAND terms (Fair, Reasonable, And Non-Discriminatory): fair pricing, no mandatory ad syndication, ability to reorder and blend results. We succeeded with many, including:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Vendor&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Mojeek&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Brave&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wikipedia&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TripAdvisor&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yelp&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Apple&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wolfram Alpha&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Our own Small Web Index&lt;/cell&gt;
        &lt;cell&gt;Proprietary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;With Google and Bing, we failed - not for lack of trying.&lt;/p&gt;
    &lt;p&gt;Bing: Their terms didn’t work for us from the start. Microsoft’s terms prohibited reordering results or merging them with other sources - restrictions incompatible with Kagi’s approach. In February 2023, they announced price increases of up to 10x on some API tiers. Then in May 2025, they retired the Bing Search APIs entirely, effective August 2025, directing customers toward AI-focused alternatives like Azure AI Agents.&lt;/p&gt;
    &lt;p&gt;Google: Google does not offer a public search API. The only available path is an ad-syndication bundle with no changes to result presentation - the model Startpage uses. Ad syndication is a non-starter for Kagi’s ad-free subscription model.[^1]&lt;/p&gt;
    &lt;head rend="h2"&gt;The current interim approach&lt;/head&gt;
    &lt;p&gt;Because direct licensing isn’t available to us on compatible terms, we - like many others - use third-party API providers for SERP-style results (SERP meaning search engine results page). These providers serve major enterprises (according to their websites) including Nvidia, Adobe, Samsung, Stanford, DeepMind, Uber, and the United Nations.&lt;/p&gt;
    &lt;p&gt;This is not our preferred solution. We plan to exit it as soon as direct, contractual access becomes available. There is no legitimate, paid path to comprehensive Google or Bing results for a company like Kagi. Our position is clear: open the search index, make it available on FRAND terms, and enable rapid innovation in the marketplace.&lt;/p&gt;
    &lt;head rend="h2"&gt;The DOJ ruling&lt;/head&gt;
    &lt;p&gt;The Google antitrust case began in 2020. On August 5, 2024, the court ruled Google violated Section 2 of the Sherman Act by unlawfully maintaining its monopoly through exclusive distribution agreements. (Full ruling)&lt;/p&gt;
    &lt;p&gt;On September 2, 2025, the DOJ announced remedies (press release):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limits on exclusivity: Google is prohibited from exclusive contracts related to Search, Chrome, Assistant, and Gemini.&lt;/item&gt;
      &lt;item&gt;Data sharing and syndication: Google must provide search index and interaction data to competitors and offer syndication services to help rivals build competitive search.&lt;/item&gt;
      &lt;item&gt;Addressing monopolization tactics: The remedies aim to dismantle a decade of exclusionary agreements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In December 2025, Judge Mehta issued a memorandum outlining the specific remedies the court intends to impose. The details are significant:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mandatory syndication: Google must offer query-based search syndication to “Qualified Competitors” on terms no less favorable than those provided to current partners.&lt;/item&gt;
      &lt;item&gt;No ad bundling: Google cannot condition access to search results on the use of Google Ads; competitors are free to monetize via their own ads or third parties.&lt;/item&gt;
      &lt;item&gt;Index data access: Google must provide Web Search Index data (URLs, crawl metadata, spam scores) at marginal cost.&lt;/item&gt;
      &lt;item&gt;Duration: The judgment remains in effect for 6 years, with syndication licenses guaranteed for terms of 5 years.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If implemented as outlined, this is exactly what we have been asking for. The legal trajectory is promising. Google will contest details, and final enforceable terms are still being worked out. The fight now is ensuring these remedies become real, practical access - not paper compliance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why enforcement matters now&lt;/head&gt;
    &lt;p&gt;Even as these remedies take shape, Google is moving to close the back door. In December 2025, Google sued SerpApi for scraping its results at scale.&lt;/p&gt;
    &lt;p&gt;We take a measured, principled view:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context matters: Google built its index by crawling the open web before robots.txt was a widespread norm, often over publishers’ objections. Today, publishers “consent” to Google’s crawling because the alternative - being invisible on a platform with 90% market share - is economically unacceptable. Google now enforces ToS and robots.txt against others from a position of monopoly power it accumulated without those constraints. The rules Google enforces today are not the rules it played by when building its dominance.&lt;/item&gt;
      &lt;item&gt;The structural problem remains: This lawsuit is only necessary because Google refuses to offer legitimate, paid index access.&lt;/item&gt;
      &lt;item&gt;Our position is unchanged: We have always wanted direct licensing. We would happily pay market rates for clean, contractual access. The fact that we - and companies like Stanford, Nvidia, Adobe, and the United Nations - have had to rely on third-party vendors is a symptom of the closed ecosystem, not a preference.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The connection to DOJ remedies is direct: if Google is going to close the back door, regulators must ensure the front door is open. That is exactly what the DOJ’s index syndication requirements are meant to achieve - and why we support their full implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;What could be: A layered search ecosystem&lt;/head&gt;
    &lt;p&gt;The DOJ ruling does not itself create a healthy market, but it makes one possible.&lt;/p&gt;
    &lt;p&gt;And while this post focuses on remedies and their impact on Kagi, it is worth zooming out: even if those remedies work perfectly, long-term societal prosperity and resilience require a non-commercial baseline for access to information - something that is not dependent on ad incentives or a single vendor’s business priorities. Think of it as a north-star model for a modern society where information access is a fundamental right.&lt;/p&gt;
    &lt;p&gt;Here is what that could look like:&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 1: Search as a public good&lt;/head&gt;
    &lt;p&gt;This is a long-term possibility, not a near-term expectation. A government-backed, ad-free, intermediary-free, taxpayer-funded search service providing baseline, non-discriminatory access to information. Imagine search.org.&lt;/p&gt;
    &lt;p&gt;This is not something the DOJ remedies create directly, nor something Kagi expects to exist soon. It is included here to make explicit what an open-index world could ultimately make possible.&lt;/p&gt;
    &lt;p&gt;This layer would replace the role public libraries played for centuries - a role that effectively disappeared when commercial web search took over in the late 1990s. Our ancestors understood well the benefits that non-discriminatory, direct access to information brings to citizens, and ultimately society itself.&lt;/p&gt;
    &lt;p&gt;It raises hard questions: governance, funding, political independence, precedent. But the principle is sound. Every citizen should have access to information without an ad-optimized algorithm standing between them and knowledge. If we can fund public libraries, we can fund public search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 2: Free, ad-based search&lt;/head&gt;
    &lt;p&gt;Commercial search engines with richer features, funded by advertising. Users understand the tradeoff and have a genuine public alternative. This is the space where most contemporary search engines operate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 3: Paid, subscription-based search&lt;/head&gt;
    &lt;p&gt;Premium search engines offering the highest possible quality, privacy, and advanced features for users who value this and are willing to pay. This is where Kagi operates - and where we are expanding as an integrator of knowledge across search, browser, mail, and AI assistants, without selling your attention.&lt;/p&gt;
    &lt;p&gt;This layered model creates a diverse ecosystem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A public baseline for information access.&lt;/item&gt;
      &lt;item&gt;Commercial free options for convenience and reach.&lt;/item&gt;
      &lt;item&gt;Premium paid options for those who want maximum quality and control.&lt;/item&gt;
      &lt;item&gt;Aligns with the primary purpose of the Sherman Act.[^2]&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The DOJ ruling is starting to do what antitrust is supposed to do: turn a closed, private choke point into shared infrastructure that others can build on. If the remedies land as real, usable access (APIs, cost-based pricing, no ad bundling), the web can support a layered ecosystem again: a public baseline for citizens, free ad-supported products for reach, and paid services that compete on quality, privacy, and power-user features.&lt;/p&gt;
    &lt;p&gt;That is the world we are building Kagi for. We are ready to walk through the front door - not depend on gray-market workarounds. Our job now is to be ready when the door opens, and to help make sure it does: keep Kagi genuinely multi-source, keep investing in our Small Web Index, and keep shipping a subscription search experience that delivers the best results across providers. If we get this right, the next decade of search and AI does not have to be one funnel owned by one company. It can be a competitive stack of layers that treats information access as the public good it has always been.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;head rend="h4"&gt;DOJ v. Google&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UNITED STATES OF AMERICA v. GOOGLE LLC, 1:20-cv-03010 â Full case docket on CourtListener&lt;/item&gt;
      &lt;item&gt;Memorandum Opinion â Judge Amit Mehta (PDF) â Court ruling finding Google violated antitrust law&lt;/item&gt;
      &lt;item&gt;Department of Justice Wins Significant Remedies Against Google â DOJ press release announcing remedies, September 2, 2025&lt;/item&gt;
      &lt;item&gt;Judge Mehta’s Remedies Memorandum (PDF) â December 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Market data and commentary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search Engine Market Share Worldwide â StatCounter, October 2025&lt;/item&gt;
      &lt;item&gt;Search Engine Market Share United States â StatCounter, October 2025&lt;/item&gt;
      &lt;item&gt;Ian Bremmer on algorithmic information access â Commentary on ad-driven search&lt;/item&gt;
      &lt;item&gt;The Anatomy of a Large-Scale Hypertextual Web Search Engine â Original Google white paper by Brin &amp;amp; Page, Stanford, 1998&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Third-party search API providers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Google lobs lawsuit at search result scraping firm â Ars Technica coverage of Google’s litigation&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;[^1]: A note on Google’s existing APIs: Google offers PSE, designed for adding search boxes to websites. It can return web results, but with reduced scope and terms tailored for that narrow use case. More recently, Google offers Grounding with Google Search through Vertex AI, intended for grounding LLM responses. Neither is general-purpose index access. Programmable Search Engine is not designed for building competitive search. Grounding with Google Search is priced at $35 per 1,000 requests - economically unviable for search at scale, and structured as an AI add-on rather than standalone index syndication. These are not the FRAND terms the market needs.&lt;/p&gt;
    &lt;p&gt;[^2]: Our understanding of the primary purpose of the Sherman Act is not to shield competitors from the success of legitimate businesses or to prevent those businesses from earning fair profits. Rather, it is to preserve a competitive marketplace that protects consumers from harm (see Competition law and consumer protection, Kluwer Law International, pp. 291â293). Opening the search index would create healthy, real, and intense competition in the search space - including competition to Kagi - which aligns with our understanding of the Sherman Act’s intent. The goal is not the elimination of dominant firms, but the prevention of a single, closed index from becoming the only gateway to information.&lt;/p&gt;
    &lt;p&gt;Published by Vladimir Prelovac and Raghu Murthi on January 21, 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.kagi.com/waiting-dawn-search"/><published>2026-01-21T17:28:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46709543</id><title>Show HN: Rails UI</title><updated>2026-01-22T09:23:02.998153+00:00</updated><content>&lt;doc fingerprint="7d1af0212bdc2360"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stop fighting CSS and build beautiful Rails apps faster&lt;/head&gt;
    &lt;p&gt;No more ugly Rails apps. Get professional-looking components and themes that work perfectly with Rails—no design skills required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Booking date&lt;/cell&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Payout&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;June 1, 2026&lt;/p&gt;
          &lt;p&gt;7:38 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cozy Mountain A-Frame&lt;/cell&gt;
        &lt;cell&gt;$1,165.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Aug 3, 2026&lt;/p&gt;
          &lt;p&gt;7:38 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mountain Vista Chalet&lt;/cell&gt;
        &lt;cell&gt;$2,846.46&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Jul 18, 2026&lt;/p&gt;
          &lt;p&gt;4:30 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cozy Mountain A-Frame&lt;/cell&gt;
        &lt;cell&gt;$1,326.36&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Pro&lt;/p&gt;
    &lt;p&gt;Active subscriber&lt;/p&gt;
    &lt;p&gt;Monthly&lt;/p&gt;
    &lt;p&gt;Sarah updated deal status to "Qualified"&lt;/p&gt;
    &lt;p&gt;2 hours ago&lt;/p&gt;
    &lt;p&gt;New contact added: John Smith&lt;/p&gt;
    &lt;p&gt;Yesterday&lt;/p&gt;
    &lt;p&gt;Acme Corporation&lt;/p&gt;
    &lt;p&gt;Enterprise Plan • 12 team members&lt;/p&gt;
    &lt;p&gt;Next invoice: $299/mo&lt;/p&gt;
    &lt;p&gt;Due on Feb 1, 2025&lt;/p&gt;
    &lt;head rend="h1"&gt;Sign in to your account&lt;/head&gt;
    &lt;p&gt;Or sign up for an account&lt;/p&gt;
    &lt;head rend="h2"&gt;Enhanced User Authentication System&lt;/head&gt;
    &lt;p&gt;A small-batch cycle to build a refreshed authentication flow.&lt;/p&gt;
    &lt;p&gt;Components&lt;/p&gt;
    &lt;head rend="h3"&gt;Components that make your Rails app look professional&lt;/head&gt;
    &lt;p&gt;No design experience? No problem. Copy-paste beautiful forms, buttons, and layouts that work perfectly with Rails. Focus on your business logic—we've got the pretty stuff covered.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Accordion&lt;/item&gt;
      &lt;item&gt;Alert&lt;/item&gt;
      &lt;item&gt;Badge&lt;/item&gt;
      &lt;item&gt;Button&lt;/item&gt;
      &lt;item&gt;Card&lt;/item&gt;
      &lt;item&gt;Dropdown&lt;/item&gt;
      &lt;item&gt;Modal&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;How do I import contacts from a CSV file?&lt;/head&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;Can I customize my deal pipeline stages?&lt;/head&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;How do automated follow-up reminders work?&lt;/head&gt;
    &lt;head rend="h3"&gt;Time to launch&lt;/head&gt;
    &lt;p&gt;The beta is no more. &lt;lb/&gt;We are ready to go live.&lt;/p&gt;
    &lt;p&gt;Themes&lt;/p&gt;
    &lt;head rend="h3"&gt; Complete app designs that don't look like &lt;lb/&gt; programmer art&lt;/head&gt;
    &lt;p&gt;Skip the hours of CSS frustration. Get complete, professional-looking app layouts that work with Rails out of the box. Your users will think you hired a designer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Collie&lt;/head&gt;
    &lt;p&gt;Community platform&lt;/p&gt;
    &lt;head rend="h3"&gt;Husky&lt;/head&gt;
    &lt;p&gt;Personal Finance&lt;/p&gt;
    &lt;head rend="h3"&gt;Boxer&lt;/head&gt;
    &lt;p&gt;Agency Management&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Rails UI is going to save me months of work. I'm an experienced software developer building my first Ruby on Rails app, but I'm not strong at front-end design. Support has been awesome as well."&lt;/p&gt;Adam G. — Software Developer&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;"Launched our MVP in two weeks instead of two months. The themes look so polished that our investors thought we had a full design team."&lt;/p&gt;Sarah M. — Startup Founder&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;"My clients can't believe how fast I deliver now. Rails UI pays for itself on the first project."&lt;/p&gt;James T. — Freelance Developer&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://railsui.com/"/><published>2026-01-21T18:31:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46711574</id><title>eBay explicitly bans AI "buy for me" agents in user agreement update</title><updated>2026-01-22T09:23:02.672418+00:00</updated><content>&lt;doc fingerprint="ddd85f438da6cb94"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;eBay Explicitly Bans AI “Buy For Me” Agents, Updates Arbitration &amp;amp; Dispute Rules In User Agreement Update&lt;/head&gt;
    &lt;p&gt;eBay explicitly prohibits AI "buy for me" agents and LLM (larger language model) bots, updates arbitration and dispute resolution requirements in latest User Agreement update, going into effect February 20, 2026.&lt;/p&gt;
    &lt;p&gt;The following summary of changes was provided in an email sent to users:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We’ve updated eBay’s User Agreement, including the agreement to arbitrate any disputes you may have with us. Our updated User Agreement was posted on January 20, 2026. For users who agreed to a prior version of our User Agreement, this agreement is effective as of February 20, 2026.&lt;/p&gt;
      &lt;p&gt;In this update, eBay is updating its anti-scraping prohibition to clarify that it specifically also includes bots used for AI or LLMs. eBay is also updating the agreement to arbitrate in the updated User Agreement:&lt;/p&gt;
      &lt;item&gt;We clarified the scope of the class action waiver.&lt;/item&gt;
      &lt;item&gt;We clarified the process for opting out of the agreement to arbitrate.&lt;/item&gt;
      &lt;item&gt;We updated the physical address to which notices for informal dispute resolution, arbitration demands, and notices for opting out of arbitration must be sent.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;As always, sellers are encouraged to read the entire updated terms carefully, but Value Added Resource has you covered with a side by side comparison highlighting some key changes.&lt;/p&gt;
    &lt;p&gt;Disclaimer: comparisons are made using both automated and manual methods and are provided for informational purposes only - no warranty of completeness or accuracy is expressed or implied and users are advised to do their own due diligence.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI Agents &amp;amp; LLM Scraping&lt;/head&gt;
    &lt;p&gt;First, as the summary calls out, eBay is explicitly prohibiting AI "buy for me" agents and LLM scraping bots from interacting with the platform without permission from eBay.&lt;/p&gt;
    &lt;p&gt;Old Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In connection with using or accessing our Services you agree to comply with this User Agreement, our policies, our terms, and all applicable laws, rules, and regulations, and you will not...&lt;/p&gt;
      &lt;p&gt;...use any robot, spider, scraper, data mining tools, data gathering and extraction tools, or other automated means to access our Services for any purpose, except with the prior express permission of eBay;&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;New Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In connection with using or accessing our Services you agree to comply with this User Agreement, our policies, our terms, and all applicable laws, rules, and regulations, and you will not...&lt;/p&gt;
      &lt;p&gt;use any robot, spider, scraper, data mining tools, data gathering and extraction tools, or other automated means (including, without limitation buy-for-me agents, LLM-driven bots, or any end-to-end flow that attempts to place orders without human review) to access our Services for any purpose, except with the prior express permission of eBay;&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The move comes after eBay quietly changed their robots.txt file with new guidance placing guardrails and restrictions on how AI agents interact with the site in December.&lt;/p&gt;
    &lt;p&gt;It also comes on the heels of Amazon's controversial Buy For Me test which uses agentic AI to display items from direct merchant websites for sale through the Amazon app, even if the brand does not sell on Amazon themselves - raising concerns about transparency, consent, and control over how product details are displayed to buyers.&lt;/p&gt;
    &lt;p&gt;While it appears that Amazon Buy For Me currently does not pull inventory from other third party marketplaces, it would not be surprising if eBay is reacting at least in part to this and other agentic commerce news making recent headlines.&lt;/p&gt;
    &lt;head rend="h3"&gt;Arbitration &amp;amp; Dispute Resolution&lt;/head&gt;
    &lt;p&gt;The rest of the changes in this User Agreement update affect arbitration and dispute resolution.&lt;/p&gt;
    &lt;p&gt;eBay's previous User Agreement update in May 2025 made significant changes to arbitration terms and limits on lawsuits, forcing users to give up their right to the sue the company in many situations.&lt;/p&gt;
    &lt;p&gt;In this update, eBay has finally updated the address to send arbitration opt out requests and other legal correspondence to since selling their former office in Draper, UT in 2024.&lt;/p&gt;
    &lt;p&gt;Old Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Notice to eBay should be sent by email to DisputeNotice@eBay.com or regular mail to our offices located at 583 W. eBay Way, Draper, UT 84020.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;New Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Notice to eBay should be sent by email to DisputeNotice@eBay.com or regular mail to our offices located at 339 W. 13490 S., Ste. 500, Draper, UT 84020&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Most importantly, eBay has expanded their arbitration clause which previously prohibited class actions to now also explicitly exclude more types of group legal actions.&lt;/p&gt;
    &lt;p&gt;Old Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;EACH OF US MAY BRING CLAIMS AGAINST THE OTHER ONLY ON AN INDIVIDUAL BASIS AND NOT ON A CLASS, REPRESENTATIVE, OR COLLECTIVE BASIS, AND THE PARTIES HEREBY WAIVE ALL RIGHTS TO HAVE ANY DISPUTE BE BROUGHT, HEARD, ADMINISTERED, RESOLVED, OR ARBITRATED ON A CLASS, COLLECTIVE, OR REPRESENTATIVE BASIS. ONLY INDIVIDUAL RELIEF IS AVAILABLE.&lt;/p&gt;
      &lt;p&gt;Subject to this Agreement to Arbitrate, the arbitrator may award declaratory or injunctive relief only in favor of the individual party seeking relief and only to the extent necessary to provide relief warranted by the party’s individual claim. Nothing in this paragraph is intended to, nor shall it, affect the terms and conditions under Section 19.B.7 ("Batch Arbitration").&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;New Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;EACH OF US MAY BRING CLAIMS AGAINST THE OTHER ONLY ON AN INDIVIDUAL BASIS AND NOT AS A PLAINTIFF OR CLASS MEMBER IN ANY PURPORTED CLASS, OR REPRESENTATIVE, OR COLLECTIVE BASIS, OR PRIVATE ATTORNEY GENERAL ACTION OR PROCEEDING, NOR OTHERWISE TO SEEK RECOVERY OF LOSSES OR DAMAGES (WHETHER FOR YOURSELF OR OTHERS) INCURRED BY A THIRD PARTY, AND THE PARTIES HEREBY WAIVE ALL RIGHTS TO HAVE ANY DISPUTE BE BROUGHT, HEARD, ADMINISTERED, RESOLVED, OR ARBITRATED ON A CLASS, COLLECTIVE, OR REPRESENTATIVE BASIS. ONLY INDIVIDUAL RELIEF IS AVAILABLE.&lt;/p&gt;
      &lt;p&gt;Subject to this Agreement to Arbitrate, the arbitrator may award declaratory or injunctive relief only in favor of the individual party seeking relief and only to the extent necessary to provide relief warranted by the party’s individual claim. Nothing in this paragraph is intended to, nor shall it, affect the terms and conditions under Section 19.B.7 ("Batch Arbitration").&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here's what that means in plain language:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Not as a plaintiff or class member” — prevents someone from joining an existing class action.&lt;/item&gt;
      &lt;item&gt;“No private attorney general actions” — blocks lawsuits brought “on behalf of the public,” a type of claim sometimes used in consumer protection cases.&lt;/item&gt;
      &lt;item&gt;“Nor… for losses incurred by a third party” — prevents a person from trying to recover damages suffered by someone else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: this language does not in any way change or restrict legal action that state Attorneys General, the FTC or other regulatory or legal agencies can take on behalf of sellers and/or consumers - so don't be dissuaded from letting those agencies know about your experiences with the platform, like the recent changes to Promoted Listings ad attribution policies.&lt;/p&gt;
    &lt;p&gt;And finally, this User Agreement update has been changed to clarify that only new users may request to opt out of arbitration agreement - existing users missed their opportunity if they did not opt out before May 16, 2025.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Opt-Out Procedure&lt;/p&gt;
      &lt;p&gt;IF YOU ARE A NEW USER OF OUR SERVICES, YOU CAN CHOOSE TO OPT OUT OF THIS AGREEMENT TO ARBITRATE ("OPT OUT") BY MAILING US A WRITTEN OPT-OUT NOTICE ("OPT-OUT NOTICE").&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's it for changes to eBay's User Agreement going into effect February 20, 2026.&lt;/p&gt;
    &lt;p&gt;Let us know in the comments below what you think of these change and how they'll affect your business!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.valueaddedresource.net/ebay-bans-ai-agents-updates-arbitration-user-agreement-feb-2026/"/><published>2026-01-21T21:07:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46711792</id><title>Jerry (YC S17) Is Hiring</title><updated>2026-01-22T09:23:02.250347+00:00</updated><link href="https://www.ycombinator.com/companies/jerry-inc/jobs/QaoK3rw-software-engineer-core-automation-marketplace"/><published>2026-01-21T21:26:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46712678</id><title>Your brain on ChatGPT: Accumulation of cognitive debt when using an AI assistant</title><updated>2026-01-22T09:23:01.877949+00:00</updated><content>&lt;doc fingerprint="a81bbfb1ef3e053e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Abstract&lt;/head&gt;
      &lt;p&gt;This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.media.mit.edu/publications/your-brain-on-chatgpt/"/><published>2026-01-21T22:41:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713106</id><title>Show HN: Sweep, Open-weights 1.5B model for next-edit autocomplete</title><updated>2026-01-22T09:23:01.697828+00:00</updated><content>&lt;doc fingerprint="85b3cff318accb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sweep Next-Edit 1.5B (GGUF)&lt;/head&gt;
    &lt;p&gt;A 1.5B parameter model for next-edit autocomplete, quantized to Q8_0 GGUF format.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Description&lt;/head&gt;
    &lt;p&gt;Sweep Next-Edit predicts your next code edit before you make it. It runs locally on your laptop in under 500ms (with speculative decoding) and outperforms models over 4x its size on next-edit benchmarks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Usage&lt;/head&gt;
    &lt;p&gt;Download &lt;code&gt;run_model.py&lt;/code&gt; and the model file, then:&lt;/p&gt;
    &lt;code&gt;uv pip install llama-cpp-python huggingface_hub
python run_model.py
&lt;/code&gt;
    &lt;head rend="h2"&gt;Model Details&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format: GGUF (Q8_0 quantization)&lt;/item&gt;
      &lt;item&gt;Parameters: 1.5B&lt;/item&gt;
      &lt;item&gt;Context Length: 8192 tokens&lt;/item&gt;
      &lt;item&gt;Base Model: Qwen2.5-Coder&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;The model uses a specific prompt format with file context, recent diffs, and current state to predict the next edit. See &lt;code&gt;run_model.py&lt;/code&gt; for a complete example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Links&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blog Post - Technical details and benchmarks&lt;/item&gt;
      &lt;item&gt;JetBrains Plugin - Sweep AI JetBrains Plugin&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;License&lt;/head&gt;
    &lt;p&gt;Apache 2.0&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Downloads last month&lt;/item&gt;
      &lt;item rend="dd-1"&gt;71&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Hardware compatibility&lt;/p&gt;
    &lt;p&gt;Log In to view the estimation&lt;/p&gt;
    &lt;p&gt;8-bit&lt;/p&gt;
    &lt;p&gt; Inference Providers NEW &lt;/p&gt;
    &lt;p&gt;This model isn't deployed by any Inference Provider. 🙋 Ask for provider support&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://huggingface.co/sweepai/sweep-next-edit-1.5B"/><published>2026-01-21T23:22:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713387</id><title>Lix – universal version control system for binary files</title><updated>2026-01-22T09:23:01.550977+00:00</updated><content>&lt;doc fingerprint="f514be7bc4ab260c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Lix: A universal version control system&lt;/head&gt;
    &lt;head rend="h2"&gt;AI agents need version control beyond text&lt;/head&gt;
    &lt;p&gt;Changes AI agents make need to be reviewable by humans.&lt;/p&gt;
    &lt;p&gt;For code, Git solves this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reviewable diffs: What exactly did the agent change?&lt;/item&gt;
      &lt;item&gt;Human-in-the-loop: Review, then merge or reject.&lt;/item&gt;
      &lt;item&gt;Rollback changes: Undo mistakes instantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But agents modify binary files too. And Git can't diff them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Lix&lt;/head&gt;
    &lt;p&gt;Lix is a universal version control system that can diff any file format (&lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, etc).&lt;/p&gt;
    &lt;p&gt;Unlike Git's line-based diffs, Lix understands file structure. Lix sees &lt;code&gt;price: 10 â 12&lt;/code&gt; or &lt;code&gt;cell B4: pending â shipped&lt;/code&gt;, not "line 4 changed" or "binary files differ".&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reviewable diffs: See exactly what an agent changed in any file format.&lt;/item&gt;
      &lt;item&gt;Human-in-the-loop: Agents propose, humans approve.&lt;/item&gt;
      &lt;item&gt;Safe rollback: Undo mistakes instantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Excel file example&lt;/head&gt;
    &lt;p&gt;An AI agent updates an order status in &lt;code&gt;orders.xlsx&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Before:&lt;/p&gt;
    &lt;code&gt;  | order_id | product  | status   |
  | -------- | -------- | -------- |
  | 1001     | Widget A | shipped  |
  | 1002     | Widget B | pending |
&lt;/code&gt;
    &lt;p&gt;After:&lt;/p&gt;
    &lt;code&gt;  | order_id | product  | status   |
  | -------- | -------- | -------- |
  | 1001     | Widget A | shipped  |
  | 1002     | Widget B | shipped |
&lt;/code&gt;
    &lt;p&gt;Git sees:&lt;/p&gt;
    &lt;code&gt;-Binary files differ
&lt;/code&gt;
    &lt;p&gt;Lix sees:&lt;/p&gt;
    &lt;code&gt;order_id 1002 status: 

- pending
+ shipped
&lt;/code&gt;
    &lt;head rend="h2"&gt;JSON file example&lt;/head&gt;
    &lt;p&gt;Even for structured text file formats like &lt;code&gt;.json&lt;/code&gt; lix is tracking semantics rather than line by line diffs.&lt;/p&gt;
    &lt;p&gt;Before:&lt;/p&gt;
    &lt;code&gt;{"theme":"light","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;After:&lt;/p&gt;
    &lt;code&gt;{"theme":"dark","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;Git sees:&lt;/p&gt;
    &lt;code&gt;-{"theme":"light","notifications":true,"language":"en"}
+{"theme":"dark","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;Lix sees:&lt;/p&gt;
    &lt;code&gt;property theme: 
- light
+ dark
&lt;/code&gt;
    &lt;head rend="h2"&gt;How does Lix work?&lt;/head&gt;
    &lt;p&gt;Lix adds a version control system on top of SQL databases that let's you query virtual tables like &lt;code&gt;file&lt;/code&gt;, &lt;code&gt;file_history&lt;/code&gt;, etc. via plain SQL. These table's are version controlled.&lt;/p&gt;
    &lt;p&gt;Why this matters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lix doesn't reinvent databases â durability, ACID, and corruption recovery are handled by battle-tested SQL databases.&lt;/item&gt;
      &lt;item&gt;Full SQL support â query your version control system with the same SQL.&lt;/item&gt;
      &lt;item&gt;Can runs in your existing database â no separate storage layer to manage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;âââââââââââââââââââââââââââââââââââââââââââââââââââ
â                      Lix                        â
â           (version control system)              â
â                                                 â
â ââââââââââââââ ââââââââââââ âââââââââââ âââââââ â
â â Filesystem â â Branches â â History â â ... â â
â ââââââââââââââ ââââââââââââ âââââââââââ âââââââ â
ââââââââââââââââââââââââââ¬âââââââââââââââââââââââââ
                         â
                         â¼
âââââââââââââââââââââââââââââââââââââââââââââââââââ
â                  SQL database                   â
âââââââââââââââââââââââââââââââââââââââââââââââââââ
&lt;/code&gt;
    &lt;p&gt;Read more about Lix architecture â&lt;/p&gt;
    &lt;head rend="h2"&gt;Why did we build lix?&lt;/head&gt;
    &lt;p&gt;Lix was developed alongside inlang, open-source localization infrastructure.&lt;/p&gt;
    &lt;p&gt;We had to develop a new version control system that addressed git's limitations inlang ran into, see (see "Git is unsuited for applications"). The result is Lix, now at over 90k weekly downloads on NPM.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;JavaScript Â· Python Â· Rust Â· Go&lt;/p&gt;
    &lt;code&gt;npm install @lix-js/sdk
&lt;/code&gt;
    &lt;code&gt;import { openLix } from "@lix-js/sdk";

const lix = await openLix({
  environment: new InMemorySQLite()
});

await lix.db.insertInto("file").values({ path: "/hello.txt", data: ... }).execute();

const diff = selectWorkingDiff({ lix })
&lt;/code&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;The next version of Lix will be a refactor to be purely "preprocessor" based. This enables:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fast writes (RFC 001)&lt;/item&gt;
      &lt;item&gt;Any SQL database (SQLite, Postgres, Turso, MySQL)&lt;/item&gt;
      &lt;item&gt;SDKs for Python, Rust, Go (RFC 002)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;                      ââââââââââââââââââ
  SELECT * FROM ...   â  Lix Engine    â   SELECT * FROM ...
 ââââââââââââââââââââ¶ â    (Rust)      â ââââââââââââââââââââ¶  Database
                      ââââââââââââââââââ
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lix.dev/blog/introducing-lix/"/><published>2026-01-21T23:55:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713444</id><title>From stealth blackout to whitelisting: Inside the Iranian shutdown</title><updated>2026-01-22T09:23:01.136102+00:00</updated><content>&lt;doc fingerprint="3ff80173394545ce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Stealth Blackout to Whitelisting: Inside the Iranian Shutdown&lt;/head&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Iran is in the midst of one of the world’s most severe communications blackouts. This post uses Kentik data to detail how this historic event unfolded, where this event lies in the context of previous Iranian shutdowns, and finally discusses what might be in store next for Iran.&lt;/p&gt;
    &lt;p&gt;For nearly two weeks, Iran has been enduring one of the most severe internet shutdowns in modern history. The theocratic regime’s decision to restrict communications coincided with a violent nationwide crackdown on a growing protest movement driven by worsening economic hardship.&lt;/p&gt;
    &lt;p&gt;In this post, I explore the situation in Iran using Kentik’s aggregate NetFlow data, along with other sources.&lt;/p&gt;
    &lt;head rend="h2"&gt;The big picture&lt;/head&gt;
    &lt;p&gt;At the time of this writing, a near-complete internet shutdown has persisted for almost 14 days. Along with internet services, international voice calling has also been blocked (there have been a couple of periods when limited outgoing calls were allowed), and domestic communication services have experienced extended disruptions, including Iran’s National Information Network. For a country of 90 million people, the combined blocking of these communication modes makes this blackout one of the most severe in history.&lt;/p&gt;
    &lt;p&gt;To learn more about the conditions that lead to the check out this special episode of Kentik’s Telemetry Now podcast with Iranian digital rights expert Amir Rashidi, Director of Digital Rights and Security at the human rights organization Miaan Group:&lt;/p&gt;
    &lt;head rend="h2"&gt;Some background first&lt;/head&gt;
    &lt;p&gt;For decades, the internet of Iran has been connected to the world via two international gateways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Telecommunication Infrastructure Company (TIC) (AS49666, previously AS12880, AS48159)&lt;/item&gt;
      &lt;item&gt;Institute for Research in Fundamental Sciences (IPM) (AS6736)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IPM, primarily a university and research network, was the country’s original internet connection in the 1990s, a story covered in the excellent book The Internet of Elsewhere by Cyrus Farivar. Years later, the state telecom TIC got into the business of providing internet service and today handles the vast majority of internet traffic into and out of Iran.&lt;/p&gt;
    &lt;p&gt;Despite TIC’s dominance, IPM has maintained a technologically independent connection to the outside world, though it has never been immune from Iranian government censorship and surveillance. This distinction matters because each gateway behaved differently during the shutdown.&lt;/p&gt;
    &lt;head rend="h2"&gt;January 8, 2026&lt;/head&gt;
    &lt;p&gt;In the days leading up to January 8, there were many reports of localized internet blockages around the country, but these incidents weren’t big enough to register on any of our national traffic statistics for Iran.&lt;/p&gt;
    &lt;p&gt;The first major development occurred at 11:42 UTC on January 8, 2026, when TIC (AS49666) began withdrawing its IPv6 BGP routes from its sessions with other networks. Within hours, nearly all of Iran’s IPv6 routing had disappeared from the global routing table.&lt;/p&gt;
    &lt;p&gt;From our perspective, this is what IPv6 traffic to Iran looked like on January 8.&lt;/p&gt;
    &lt;p&gt;However, based on our aggregate NetFlow, IPv6 traffic normally amounts to less than 1% of the overall traffic (in bits/sec) into Iran, so the average Iranian was unlikely to be affected by this issue. Regardless, the withdrawal of IPv6 routes appeared to be an early indication of what was to come later in the day.&lt;/p&gt;
    &lt;p&gt;Following a brief disruption, we observed internet traffic levels begin to plummet at 16:30 UTC (7pm local). The drop continued until internet traffic into Iran had all but ceased by 1845 UTC, as illustrated below. It took over two hours to stop all internet traffic into and out of the country.&lt;/p&gt;
    &lt;p&gt;At 19:00 UTC, we observed TIC disconnecting from a subset of its transit providers, including Russian state telecom Rostelecom (AS12389) and regional operator Gulf Bridge International (AS200612), and all of its settlement-free peers.&lt;/p&gt;
    &lt;p&gt;Despite the loss of numerous BGP adjacencies for AS49666 (TIC), the vast majority of Iranian IPv4 routes continued to be routed globally. The drop in Iranian IPv4 traffic, therefore, could not be explained by reachability issues; another mechanism was at work at the network edge blocking traffic.&lt;/p&gt;
    &lt;p&gt;Georgia Tech’s IODA tool captures this divergence well. In the below screenshot, active probing (blue) drops to zero as traffic is blocked, while routed IPv4 space in BGP (green) is almost completely unscathed (98.14%).&lt;/p&gt;
    &lt;p&gt;Although IPv4 routes remained online, internet traffic stopped for roughly 90 million Iranians. This distinction is central to Iran’s next step: internet “whitelisting,” in which an Iranian version of the Chinese Great Firewall allows only approved users or services while blocking all others. Had authorities withdrawn IPv4 routes, as they did with IPv6, Iran would have become completely unreachable, as Egypt was in January 2011. By keeping IPv4 routes in circulation, Iranian authorities can selectively grant full internet access to specific users while denying it to the broader population.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limited connectivity&lt;/head&gt;
    &lt;p&gt;As mentioned above, the internet shutdown in Iran is not complete. There has been a tiny amount of traffic still trickling in and out as a small set of Iranians continue to enjoy internet access.&lt;/p&gt;
    &lt;p&gt;There have also been a few temporary partial restorations of service, such as a multi-hour restoration of service to Iranian universities via AS6736 on January 9th, and a more recent small surge in traffic.&lt;/p&gt;
    &lt;p&gt;From our data, we have also observed the emergence of a diurnal pattern of traffic to AS49666 emerge on January 13. AS49666 is not typically a major terminus for internet traffic to Iran, so this traffic is likely proxied traffic from whitelisted individuals or services.&lt;/p&gt;
    &lt;p&gt;As of late, we’ve seen a few measures like the restoration of transit from Rostelecom and the return of routes originated by IPM, as the country appears to be moving towards a partial restoration. At the time of this writing, the plan appears to be to operate the Iranian internet as a whitelisted network indefinitely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evolving calculus of shutdowns in Iran&lt;/head&gt;
    &lt;p&gt;Back in 2012, Iran was in the beginning stages of building its National Information Network (NIN), ostensibly built to allow the country to continue to function in the event that it was cut off from the outside world. At the time, I teamed up with Iran researcher Collin Anderson to investigate. With access to in-country servers, we mapped Iran’s national internet from the inside (research published here).&lt;/p&gt;
    &lt;p&gt;We found that the NIN had been implemented by routing RFC1918 address space (specifically 10.x.x.x) between Iranian ASes within the country. By doing so, they could be assured that devices connected to the NIN would not be able to receive connections from the outside world, as those IP addresses are not routable on the public internet.&lt;/p&gt;
    &lt;p&gt;In 2019, I reported on Iran’s internet shutdown following the government’s decision to raise gas prices. At the time, it was the most severe shutdown in the country�’s history—until this month. It involved withdrawing BGP routes of some networks while blocking traffic of others, and lasted for almost two weeks.&lt;/p&gt;
    &lt;p&gt;Government-directed shutdowns in Cuba and Iran in 2022 led me to join up with Peter Micek of the digital rights NGO Access Now to write a blog post that traced the history and logic behind “internet curfews,” a tactic of communication suppression in which internet service is temporarily blocked on a recurring basis.&lt;/p&gt;
    &lt;p&gt;The article described internet curfews as another means of reducing the costs of shutdowns, not unlike the development of the NIN, according to Iranian digital rights expert Amir Rashidi. In that post, we wrote:&lt;/p&gt;
    &lt;quote&gt;The objective of internet curfews, like Iran’s NIN, is to reduce the cost of shutdowns on the authorities that order them. By reducing the costs of these shutdowns, they become a more palatable option for an embattled leader and, therefore, are likely to continue in the future.&lt;/quote&gt;
    &lt;p&gt;During the Twelve-Day War between Israel and Iran this June, Iran partially or fully shut down its internet, ostensibly to defend against cyberattacks and drone strikes. We, along with other internet observers, documented the shutdown’s phases and contributed to a detailed report by Rashidi’s team, which dubbed the shutdown as a “stealth blackout” due to the fact that traffic was disrupted without withdrawing any BGP routes.&lt;/p&gt;
    &lt;p&gt;The outage demonstrated Iran’s newfound ability to block traffic nationwide without manipulating BGP routes, signaling a higher level of sophistication in its internet filtering. This summer’s Stealth Blackout ultimately foreshadowed the ongoing shutdown Iran is now enduring.&lt;/p&gt;
    &lt;head rend="h2"&gt;Help from above&lt;/head&gt;
    &lt;p&gt;In the aftermath of the 2022 protests, Starlink began allowing connections from Iran. Satellite internet operators like Starlink must typically clear, at a minimum, two legal hurdles to operate in a country: a telecom license and radio spectrum authorization from the local government. Starlink has been operational in Iran for over three years at this point without either, and the Iranian government has taken note.&lt;/p&gt;
    &lt;p&gt;The ITU Radio Regulations Board (RRB) is a quasi-judicial United Nations body that interprets and applies the Radio Regulations, to include satellite emissions. It exists to resolve disputes between countries and oversees compliance with the international radio frequency register, but, in the end, has no direct enforcement power.&lt;/p&gt;
    &lt;p&gt;Since 2023, the Iranian has been pleading their case to the ITU that the Starlink service in Iran needed to be disabled. The 100th meeting of the ITU RRB took place in November, and on the topic of Starlink, the board decided to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Request the Administration of the Islamic Republic of Iran to pursue its efforts, to the extent possible, to identify and deactivate unauthorized STARLINK terminals in its territory,&lt;/item&gt;
      &lt;item&gt;Strongly urge the Administration of Norway to take all appropriate actions at its disposal to have the operator of the Starlink system immediately disable unauthorized transmissions of its terminals within the territory of the Islamic Republic of Iran.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regardless of the decisions of this body, Starlink continues to operate in the country. (Note: The US and Norway share responsibility for Starlink’s ITU registration.)&lt;/p&gt;
    &lt;p&gt;Despite a recent Iranian law that would equate the use of Starlink with espionage, punishable by death, Iranian digital rights activists have been working for years to smuggle in terminals and build communication infrastructure to extend the internet services within the country. The recent front-page New York Times article I collaborated on described these efforts, which now must contend with a novel form of jamming Starlink service in some urban areas of Iran.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other governments are watching, learning&lt;/head&gt;
    &lt;p&gt;In the decade and a half since the internet shutdowns of the Arab Spring, we’ve observed the practice of suppressing communications evolve as authoritarian governments learn tactics from one another. In the ongoing shutdown in Iran, multiple such tactics are on display.&lt;/p&gt;
    &lt;p&gt;To mitigate the costs of its shutdown, the Iranian government has created an internal national internet and appears to be in the process of building a “whitelisting” system to allow certain individuals and services internet access while blocking the rest. If these measures successfully enable an unpopular Iranian government to remain in power, we can expect to see them replicated elsewhere.&lt;/p&gt;
    &lt;p&gt;On the other side, the digital rights activists have also been building tools, funded in large part by the now-embattled Open Technology Fund, to allow communications to continue during a shutdown like this. However, no amount of circumvention tooling can restore service to 90 million people.&lt;/p&gt;
    &lt;p&gt;The fight for open and free communications does not have an end. As long as authoritarian governments exist, this game of cat-and-mouse will continue. Ours is only to decide which side we’re on and to throw our support (financially and otherwise) to those working on solutions to these problems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kentik.com/blog/from-stealth-blackout-to-whitelisting-inside-the-iranian-shutdown/"/><published>2026-01-22T00:00:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713526</id><title>Threat actors expand abuse of Microsoft Visual Studio Code</title><updated>2026-01-22T09:23:01.032001+00:00</updated><content>&lt;doc fingerprint="a425853934a78a08"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Threat Actors Expand Abuse of Microsoft Visual Studio Code&lt;/head&gt;
    &lt;p&gt;Jamf Threat Labs identifies additional abuse of Visual Studio Code. See the latest evolution in the Contagious Interview campaign.&lt;/p&gt;
    &lt;p&gt;By Thijs Xhaflaire&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;At the end of last year, Jamf Threat Labs published research related to the Contagious Interview campaign, which has been attributed to a threat actor operating on behalf of North Korea (DPRK). Around the same time, researchers from OpenSourceMalware (OSM) released additional findings that highlighted an evolution in the techniques used during earlier stages of the campaign.&lt;/p&gt;
    &lt;p&gt;Specifically, these newer observations highlight an additional delivery technique alongside the previously documented ClickFix-based techniques. In these cases, the infection chain abuses Microsoft Visual Studio Code task configuration files, allowing malicious payloads to be executed on the victim system.&lt;/p&gt;
    &lt;p&gt;Following the discovery of this technique, both Jamf Threat Labs and OSM continued to closely monitor activity associated with the campaign. In December, Jamf Threat Labs identified additional abuse of Visual Studio Code &lt;code&gt;tasks.json&lt;/code&gt; configuration files. This included the introduction of dictionary files containing heavily obfuscated JavaScript, which is executed when a victim opens a malicious repository in Visual Studio Code.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs shared these findings with OSM, who subsequently published a more in-depth technical analysis of the obfuscated JavaScript and its execution flow.&lt;/p&gt;
    &lt;p&gt;Earlier this week, Jamf Threat Labs identified another evolution in the campaign, uncovering a previously undocumented infection method. This activity involved the deployment of a backdoor implant that provides remote code execution capabilities on the victim system.&lt;/p&gt;
    &lt;p&gt;At a high level, the chain of events for the malware look like so:&lt;/p&gt;
    &lt;p&gt;Throughout this blog post we will shed light on each of these steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Initial Infection&lt;/head&gt;
    &lt;p&gt;In this campaign, infection begins when a victim clones and opens a malicious Git repository, often under the pretext of a recruitment process or technical assignment. The repositories identified in this activity are hosted on either GitHub or GitLab and are opened using Visual Studio Code.&lt;/p&gt;
    &lt;p&gt;When the project is opened, Visual Studio Code prompts the user to trust the repository author. If that trust is granted, the application automatically processes the repository’s &lt;code&gt;tasks.json&lt;/code&gt; configuration file, which can result in embedded arbitrary commands being executed on the system.&lt;/p&gt;
    &lt;p&gt;On macOS systems, this results in the execution of a background shell command that uses &lt;code&gt;nohup bash -c&lt;/code&gt; in combination with &lt;code&gt;curl -s&lt;/code&gt; to retrieve a JavaScript payload remotely and pipe it directly into the Node.js runtime. This allows execution to continue independently if the Visual Studio Code process is terminated, while suppressing all command output.&lt;/p&gt;
    &lt;p&gt;In observed cases, the JavaScript payload is hosted on &lt;code&gt;vercel.app&lt;/code&gt;, a platform that has been increasingly used in recent DPRK-related activity following a move away from other hosting services, as previously documented by OpenSourceMalware.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs reported the identified malicious repository to GitHub, after which the repository was removed. While monitoring the activity prior to takedown, we observed the URL referenced within the repository change on multiple occasions. Notably, one of these changes occurred after the previously referenced payload hosting infrastructure was taken down by Vercel.&lt;/p&gt;
    &lt;head rend="h2"&gt;The JavaScript Payload&lt;/head&gt;
    &lt;p&gt;Once execution begins, the JavaScript payload implements the core backdoor logic observed in this activity. While the payload appears lengthy, a significant portion of the code consists of unused functions, redundant logic, and extraneous text that is never invoked during execution &lt;code&gt;(SHA256: 932a67816b10a34d05a2621836cdf7fbf0628bbfdf66ae605c5f23455de1e0bc)&lt;/code&gt;. This additional code increases the size and complexity of the script without impacting its observed behavior. It is passed to the node executable as one large argument.&lt;/p&gt;
    &lt;p&gt;Focusing on the functional components, the payload establishes a persistent execution loop that collects basic host information and communicates with a remote command-and-control (C2) server. Hard-coded identifiers are used to track individual infections and manage tasks from the server.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core backdoor functionality&lt;/head&gt;
    &lt;p&gt;While the JavaScript payload contains a significant amount of unused code, the backdoor's core functionality is implemented through a small number of routines. These routines provide remote code execution, system fingerprinting, and persistent C2 communication.&lt;/p&gt;
    &lt;p&gt;Remote code execution capability&lt;/p&gt;
    &lt;p&gt;The payload includes a function that enables the execution of arbitrary JavaScript while the backdoor is active. At its core, this is the main functionality of this backdoor.&lt;/p&gt;
    &lt;p&gt;This function allows JavaScript code supplied as a string to be dynamically executed over the course of the backdoor lifecycle. By passing the &lt;code&gt;require&lt;/code&gt;function into the execution context, attacker-supplied code can import additional Node.js modules allowing additional arbitrary node functions to be executed.&lt;/p&gt;
    &lt;p&gt;System fingerprinting and reconnaissance&lt;/p&gt;
    &lt;p&gt;To profile the infected system, the backdoor collects a small set of host-level identifiers:&lt;/p&gt;
    &lt;p&gt;This routine gathers the system hostname, MAC addresses from available network interfaces, and basic operating system details. These values provide a stable fingerprint that can be used to uniquely identify infected hosts and associate them with a specific campaign or operator session.&lt;/p&gt;
    &lt;p&gt;In addition to local host identifiers, the backdoor attempts to determine the victim’s public-facing IP address by querying the external service ipify.org, a technique that has also been observed in prior DPRK-linked campaigns.&lt;/p&gt;
    &lt;p&gt;Command-and-control beaconing and task execution&lt;/p&gt;
    &lt;p&gt;Persistent communication with the C2 server is implemented through a polling routine that periodically sends host information and processes server responses. The beaconing logic is handled by the following function:&lt;/p&gt;
    &lt;p&gt;This function periodically sends system fingerprinting data to a remote server and waits for a response. The beacon executes every five seconds, providing frequent interaction opportunities.&lt;/p&gt;
    &lt;p&gt;The server response indicates successful connectivity and allows the backdoor to maintain an active session while awaiting tasking.&lt;/p&gt;
    &lt;p&gt;If the server response contains a specific status value, the contents of the response message are passed directly to the remote code execution routine, mentioned prior.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further Execution and Instructions&lt;/head&gt;
    &lt;p&gt;While monitoring a compromised system, Jamf Threat Labs observed further JavaScript instructions being executed roughly eight minutes after the initial infection. The retrieved JavaScript went on to set up a very similar payload to the same C2 infrustructure.&lt;/p&gt;
    &lt;p&gt;Review of this retrieved payload yields a few interesting details...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It beacons to the C2 server every 5 seconds, providing its system details and asks for further JavaScript instructions.&lt;/item&gt;
      &lt;item&gt;It executes that additional JavaScript within a child process.&lt;/item&gt;
      &lt;item&gt;It's capable of shutting itself and child processes down and cleaning up if asked to do so by the attacker.&lt;/item&gt;
      &lt;item&gt;It has inline comments and phrasing that appear to be consistent with AI-assisted code generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This activity highlights the continued evolution of DPRK-linked threat actors, who consistently adapt their tooling and delivery mechanisms to integrate with legitimate developer workflows. The abuse of Visual Studio Code task configuration files and Node.js execution demonstrates how these techniques continue to evolve alongside commonly used development tools.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs will continue to track these developments as threat actors refine their tactics and explore new ways to deliver macOS malware. We strongly recommend that customers ensure Threat Prevention and Advanced Threat Controls are enabled and set to block mode in Jamf for Mac to remain protected against the techniques described in this research.&lt;/p&gt;
    &lt;p&gt;Developers should remain cautious when interacting with third-party repositories, especially those shared directly or originating from unfamiliar sources. Before marking a repository as trusted in Visual Studio Code, it’s important to review its contents. Similarly, "npm install" should only be run on projects that have been vetted, with particular attention paid to package.json files, install scripts, and task configuration files to help avoid unintentionally executing malicious code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicators or Compromise&lt;/head&gt;
    &lt;p&gt;Dive into more Jamf Threat Labs research on our blog.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jamf.com/blog/threat-actors-expand-abuse-of-visual-studio-code/"/><published>2026-01-22T00:12:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713929</id><title>Significant US farm losses persist, despite federal assistance</title><updated>2026-01-22T09:23:00.610628+00:00</updated><content>&lt;doc fingerprint="6f3c401c4bbe6fca"&gt;
  &lt;main&gt;
    &lt;p&gt;Economist&lt;/p&gt;
    &lt;p&gt;Key Takeaways&lt;/p&gt;
    &lt;p&gt;The USDA-Economic Research Service (ERS) December update to Commodity Costs and Returns provides a comprehensive look at per-acre production costs for the nine principal row crops: corn, soybeans, wheat, cotton, rice, barley, oats, peanuts and sorghum. At a high level, ERS projects average total costs per acre to increase for every crop in 2026, underscoring the persistence of elevated production expenses across U.S. agriculture.&lt;/p&gt;
    &lt;p&gt;When operating expenses and farm-wide costs like equipment, land and management are combined, costs vary widely by crop. In 2025, forecasted total per-acre costs are $1,308 for rice, $1,166 for peanuts, $943 for cotton, $890 for corn, $658 for soybeans, $498 for oats, $491 for barley, $443 for sorghum, and $396 for wheat. Looking ahead, ERS projections for 2026 suggest continued upward pressure across most cost categories, with total cost increasing anywhere from 2.2% to 3.3%. Amongst the nine principal crops, wheat ($409 per acre), sorghum ($458) and oats ($513) remain at the lower end of the production cost spectrum, while soybeans ($678) and barley ($507) fall in the mid-range in 2026. Cotton ($965), peanuts ($1,194) and rice ($1,336) remain the most expensive crops to produce on a per-acre basis.&lt;/p&gt;
    &lt;p&gt;Operating costs—expenses directly tied to producing a yearly crop, such as seed, fertilizer, chemicals, fuel and labor—substantially vary across crops. In 2025, total operating costs ranged from $155 per acre for wheat to more than $764 per acre for rice and $631 per acre for peanuts. In 2026, these costs are expected to rise, ranging from $774 per acre for rice and $160 per acre for wheat. While select inputs have moderated slightly from recent peaks, overall operating expenses remain well above pre-2021 levels. Rising costs since 2020 have been driven primarily by sharp increases in interest expenses (+71%), fertilizer (+37%), fuel and oil (+32%), labor (+47%), chemicals (+25%) and maintenance (+27%), alongside notable gains in seed (+18%) and marketing costs (+18%).&lt;/p&gt;
    &lt;p&gt;Losses Persist Even After FBA and ECAP&lt;/p&gt;
    &lt;p&gt;Against this backdrop of elevated costs, commodity prices have remained under pressure, limiting farmers’ ability to cover their costs through the marketplace alone. As a result, many farms are projected to experience losses for a fourth or fifth consecutive year, even after accounting for crop insurance indemnities and ad hoc assistance.&lt;/p&gt;
    &lt;p&gt;The Farmer Bridge Assistance (FBA) Program and the Emergency Commodity Assistance Program (ECAP) provide important near-term support. However, ECAP was designed to address 2023 and 2024 losses, rather than 2025 and later production challenges. For both programs, payments are calculated on a per-acre basis. However, when compared to current per-acre production costs and weak commodity prices, these payments generally cover only a share of losses rather than restore profitability. In fact, returns over total costs for all nine principal row crops are projected to remain negative on a per-acre basis even after accounting for federal assistance. Based on loss calculations used in the Farmer Bridge Assistance Program, rice producers face losses of roughly $210 per acre, followed by cotton ($202), oats ($159), peanuts ($131), sorghum ($91), corn ($87), wheat ($70), soybeans ($61) and barley ($42). In total, net losses across the sector are estimated to exceed $50 billion over the past three crop years.&lt;/p&gt;
    &lt;p&gt;For many farms, aid helps slow the erosion of working capital but does not fully offset negative margins. As a result, producers continue to absorb multiyear losses that strain balance sheets, tighten cash flow and complicate access to operating credit. These loss estimates reflect national averages; actual costs of production and returns vary by region, management decisions and ownership structure. For example, producers who own their farmland may face lower total costs by avoiding cash rental expenses, resulting in higher returns.&lt;/p&gt;
    &lt;p&gt;Specialty Crops&lt;lb/&gt;Additionally, neither the FBA program nor the ECAP address losses in the specialty crops market. The 2024 Marketing Assistance for Specialty Crop Program (MASC) provided a first but limited relief step for growers and, for many, represented some of the first federal assistance tied to market challenges in the sector. Specialty crop growers continue to face deep and persistent economic losses driven by rising input costs, tightening margins, weather and disease disruptions, labor expenses and constraints, and global trade instability — challenges shared by field crop agriculture, including producers of crops beyond the nine principal crops, such as alfalfa and sugar beets. Strengthening support for all sectors of agriculture is an economic necessity. Doing so will help maintain a resilient, accessible and diverse U.S. food system. &lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;ERS cost projections make clear that input costs for all of the nine principal row crops remain elevated and sticky. Continued increases in both operating and overhead expenses are pushing breakeven prices higher, while commodity prices remain insufficient to offset those costs for many producers.&lt;/p&gt;
    &lt;p&gt;While FBA and ECAP payments are an important and welcome step in addressing near-term financial stress, they do not fully close the gap between costs and returns. As farmers enter the 2026/27 marketing year, accumulated losses — estimated to exceed $50 billion across the sector over the past three crop years — continue to weigh on farm finances.&lt;/p&gt;
    &lt;p&gt;These estimates reflect national average conditions and are calculated ahead of the growing season, before producers make final planting, input and marketing decisions. In practice, farmers respond to market signals by adjusting crop mix, input use and risk management strategies as conditions evolve. While outcomes vary widely by region and operation, persistently elevated breakeven prices underscore the importance of market-driven solutions that strengthen domestic demand — such as year-round access to E15 — to help support commodity prices and improve farm margins.&lt;/p&gt;
    &lt;p&gt;Much-needed safety net enhancements through the One Big Beautiful Bill Act (OBBBA) are expected to take effect in October 2026, but those changes do not address the pressures farmers face today. In a recent letter to Congress organized by the American Farm Bureau Federation and signed by 56 agricultural organizations, farm groups warned of an economic crisis in rural America, citing multiyear losses driven by record-high input costs and historically low commodity prices. Congressional leaders from both parties have acknowledged the severity of these losses and the need for additional aid to stabilize farm finances. Until longer-term policy improvements take hold, many operations remain caught between high operating costs and low commodity prices, underscoring the ongoing financial strain facing U.S. agriculture as producers weigh whether they can afford to plant another crop.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.fb.org/market-intel/significant-farm-losses-persist-despite-federal-assistance"/><published>2026-01-22T01:11:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46714916</id><title>Show HN: High speed graphics rendering research with tinygrad/tinyJIT</title><updated>2026-01-22T09:22:59.965495+00:00</updated><content>&lt;doc fingerprint="e3b45f93fdfd4ff5"&gt;
  &lt;main&gt;
    &lt;p&gt;gtinygrad Minimal tinygrad path tracing playground. tinygrad repo: https://github.com/tinygrad/tinygrad tinygrad README: https://github.com/tinygrad/tinygrad#readme Quick start python examples/raytrace_demo.py&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/quantbagel/gtinygrad"/><published>2026-01-22T03:26:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46715600</id><title>Doctors in Brazil using tilapia fish skin to treat burn victims</title><updated>2026-01-22T09:22:59.422919+00:00</updated><content>&lt;doc fingerprint="ef03d738aad3766e"&gt;
  &lt;main&gt;
    &lt;p&gt;By — Nadia Sussman, STAT Nadia Sussman, STAT Leave a comment 0comments Share Copy URL https://www.pbs.org/newshour/health/brazilian-city-uses-tilapia-fish-skin-treat-burn-victims Email Facebook Twitter LinkedIn Pinterest Tumblr Share on Facebook Share on Twitter Why this Brazilian city uses tilapia fish skin to treat burn victims Health Mar 3, 2017 1:09 PM EST FORTAZELA, Brazil — In this historic city by the sea in northeast Brazil, burn patients look as if they've emerged from the waves. They are covered in fish skin — specifically strips of sterilized tilapia. Doctors here are testing the skin of the popular fish as a bandage for second- and third-degree burns. The innovation arose from an unmet need. Animal skin has long been used in the treatment of burns in developed countries. But Brazil lacks the human skin, pig skin, and artificial alternatives that are widely available in the US. The three functional skin banks in Brazil can meet only 1 percent of the national demand, said Dr. Edmar Maciel, a plastic surgeon and burn specialist leading the clinical trials with tilapia skin. As a result, public health patients in Brazil are normally bandaged with gauze and silver sulfadiazine cream. "It's a burn cream because there's silver in it, so it prevents the burns from being infected," said Dr. Jeanne Lee, interim burn director at the the regional burn center at the University of California at San Diego. "But it doesn't help in terms of debriding a burn or necessarily helping it heal." READ MORE: First Look: Plumbing the mysteries of sweat to help burn patients cool their skin The gauze-and-cream dressing must be changed every day, a painful process. In the burn unit at Fortaleza's José Frota Institute, patients contort as their wounds are unwrapped and washed. Enter the humble tilapia, a fish that's widely farmed in Brazil and whose skin, until now, was considered trash. Unlike the gauze bandages, the sterilized tilapia skin goes on and stays on. The first step in the research process was to analyze the fish skin. "We got a great surprise when we saw that the amount of collagen proteins, types 1 and 3, which are very important for scarring, exist in large quantities in tilapia skin, even more than in human skin and other skins," Maciel said. "Another factor we discovered is that the amount of tension, of resistance in tilapia skin is much greater than in human skin. Also the amount of moisture." In patients with superficial second-degree burns, the doctors apply the fish skin and leave it until the patient scars naturally. For deep second-degree burns, the tilapia bandages must be changed a few times over several weeks of treatment, but still far less often than the gauze with cream. The tilapia treatment also cuts down healing time by up to several days and reduces the use of pain medication, Maciel said. Antônio dos Santos, a fisherman, was offered the tilapia treatment as part of a clinical trial after he sustained burns to his entire right arm when a gas canister on his boat exploded. He accepted. "After they put on the tilapia skin, it really relieved the pain," he said. "I thought it was really interesting that something like this could work." READ MORE: High-tech bandage wins $100K from Boston Marathon bombing survivor's family The initial batches of tilapia skin were studied and prepared by a team of researchers at the Federal University of Ceará. Lab technicians used various sterilizing agents, then sent the skins for radiation in São Paulo to kill viruses, before packaging and refrigerating the skins. Once cleaned and treated, they can last for up to two years. In the US, animal-based skin substitutes require levels of scrutiny from the Food and Drug Administration and animal rights groups that can drive up costs, Lee said. Given the substantial supply of donated human skin, tilapia skin is unlikely to arrive at American hospitals anytime soon. But it may be a boon in developing countries. "I'm willing to use anything that might actually help a patient," Lee said. "It may be a good option depending on what country you're talking about. But I also think the problem is that you need to find places that have the resources to actually process the skin and sterilize it, and make sure it doesn't have diseases." In Brazil, in addition to the clinical trials, researchers are currently conducting histological studies that compare the composition of human, tilapia, pig, and frog skins. They are also conducting studies on the comparative costs of tilapia skin and conventional burn treatments. If clinical trials show continued success, doctors hope a company will process the skins on an industrial scale and sell it to the public health system. This article is reproduced with permission from STAT. It was first published on Mar. 2, 2017. Find the original story here. A free press is a cornerstone of a healthy democracy. Support trusted journalism and civil dialogue. Donate now By — Nadia Sussman, STAT Nadia Sussman, STAT&lt;/p&gt;
    &lt;p&gt;FORTAZELA, Brazil — In this historic city by the sea in northeast Brazil, burn patients look as if they've emerged from the waves. They are covered in fish skin — specifically strips of sterilized tilapia. Doctors here are testing the skin of the popular fish as a bandage for second- and third-degree burns. The innovation arose from an unmet need. Animal skin has long been used in the treatment of burns in developed countries. But Brazil lacks the human skin, pig skin, and artificial alternatives that are widely available in the US. The three functional skin banks in Brazil can meet only 1 percent of the national demand, said Dr. Edmar Maciel, a plastic surgeon and burn specialist leading the clinical trials with tilapia skin. As a result, public health patients in Brazil are normally bandaged with gauze and silver sulfadiazine cream. "It's a burn cream because there's silver in it, so it prevents the burns from being infected," said Dr. Jeanne Lee, interim burn director at the the regional burn center at the University of California at San Diego. "But it doesn't help in terms of debriding a burn or necessarily helping it heal." READ MORE: First Look: Plumbing the mysteries of sweat to help burn patients cool their skin The gauze-and-cream dressing must be changed every day, a painful process. In the burn unit at Fortaleza's José Frota Institute, patients contort as their wounds are unwrapped and washed. Enter the humble tilapia, a fish that's widely farmed in Brazil and whose skin, until now, was considered trash. Unlike the gauze bandages, the sterilized tilapia skin goes on and stays on. The first step in the research process was to analyze the fish skin. "We got a great surprise when we saw that the amount of collagen proteins, types 1 and 3, which are very important for scarring, exist in large quantities in tilapia skin, even more than in human skin and other skins," Maciel said. "Another factor we discovered is that the amount of tension, of resistance in tilapia skin is much greater than in human skin. Also the amount of moisture." In patients with superficial second-degree burns, the doctors apply the fish skin and leave it until the patient scars naturally. For deep second-degree burns, the tilapia bandages must be changed a few times over several weeks of treatment, but still far less often than the gauze with cream. The tilapia treatment also cuts down healing time by up to several days and reduces the use of pain medication, Maciel said. Antônio dos Santos, a fisherman, was offered the tilapia treatment as part of a clinical trial after he sustained burns to his entire right arm when a gas canister on his boat exploded. He accepted. "After they put on the tilapia skin, it really relieved the pain," he said. "I thought it was really interesting that something like this could work." READ MORE: High-tech bandage wins $100K from Boston Marathon bombing survivor's family The initial batches of tilapia skin were studied and prepared by a team of researchers at the Federal University of Ceará. Lab technicians used various sterilizing agents, then sent the skins for radiation in São Paulo to kill viruses, before packaging and refrigerating the skins. Once cleaned and treated, they can last for up to two years. In the US, animal-based skin substitutes require levels of scrutiny from the Food and Drug Administration and animal rights groups that can drive up costs, Lee said. Given the substantial supply of donated human skin, tilapia skin is unlikely to arrive at American hospitals anytime soon. But it may be a boon in developing countries. "I'm willing to use anything that might actually help a patient," Lee said. "It may be a good option depending on what country you're talking about. But I also think the problem is that you need to find places that have the resources to actually process the skin and sterilize it, and make sure it doesn't have diseases." In Brazil, in addition to the clinical trials, researchers are currently conducting histological studies that compare the composition of human, tilapia, pig, and frog skins. They are also conducting studies on the comparative costs of tilapia skin and conventional burn treatments. If clinical trials show continued success, doctors hope a company will process the skins on an industrial scale and sell it to the public health system. This article is reproduced with permission from STAT. It was first published on Mar. 2, 2017. Find the original story here. A free press is a cornerstone of a healthy democracy. Support trusted journalism and civil dialogue. Donate now&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pbs.org/newshour/health/brazilian-city-uses-tilapia-fish-skin-treat-burn-victims"/><published>2026-01-22T05:15:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46716345</id><title>Satya Nadella: a masterclass in saying everything while promising nothing</title><updated>2026-01-22T09:22:59.333570+00:00</updated><content>&lt;doc fingerprint="291bd52758643f36"&gt;
  &lt;main&gt;
    &lt;p&gt;Member-only story&lt;/p&gt;
    &lt;head rend="h1"&gt;Satya Nadella at Davos: a masterclass in saying everything while promising nothing&lt;/head&gt;
    &lt;head rend="h2"&gt;A fact-check of Microsoft’s CEO reveals the gap between boardroom polish and operational reality&lt;/head&gt;
    &lt;p&gt;I watched Satya Nadella’s Davos interview three times. Once to hear what he said. Once to note what he didn’t say. And once more because I couldn’t quite believe how smoothly he’d managed to do both simultaneously.&lt;/p&gt;
    &lt;p&gt;The Microsoft CEO sat across from Larry Fink at the World Economic Forum, radiating the particular brand of composed confidence that comes from having given essentially the same performance hundreds of times. His hands moved in measured gestures. His voice modulated between earnest concern and quiet optimism. He deployed phrases like “unprecedented investment” and “democratising AI” with the ease of someone who has long since stopped thinking about whether they’re true.&lt;/p&gt;
    &lt;p&gt;And here’s the thing: he was genuinely impressive. Nadella is, by any reasonable measure, one of the most skilled corporate communicators of his generation. He transformed Microsoft’s image from the aggressive monopolist of the Ballmer years into something approaching a beloved technology partner. He speaks about artificial intelligence with the reverence of a convert and the precision of an engineer (you should see his…&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jpcaparas.medium.com/satya-nadella-at-davos-a-masterclass-in-saying-everything-while-promising-nothing-8495c75c5ba3"/><published>2026-01-22T07:43:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46716560</id><title>SpaceX lowering orbits of 4,400 Starlink satellites for safety's sake</title><updated>2026-01-22T09:22:59.067710+00:00</updated><content>&lt;doc fingerprint="fadc43323b878ecb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SpaceX lowering orbits of 4,400 Starlink satellites for safety's sake&lt;/head&gt;
    &lt;p&gt;We'll see a mass migration of SpaceX Starlink satellites this year.&lt;/p&gt;
    &lt;p&gt;All Starlink broadband spacecraft currently orbiting 342 miles (550 kilometers) or so above Earth — about 4,400 satellites — will descend to an altitude of roughly 298 miles (480 km) over the course of 2026.&lt;/p&gt;
    &lt;p&gt;There are two main reasons for the move, according to Michael Nicolls, vice president of Starlink engineering at SpaceX, who announced the plan via X on Thursday (Jan. 1).&lt;/p&gt;
    &lt;p&gt;"As solar mininum approaches, atmospheric density decreases, which means the ballistic decay time at any given altitude increases — lowering will mean a &amp;gt;80% reduction in ballistic decay time in solar minimum, or 4+ years reduced to a few months," Nicolls wrote in his X post. "Correspondingly, the number of debris objects and planned satellite constellations is significantly lower below 500 km, reducing the aggregate likelihood of collision."&lt;/p&gt;
    &lt;p&gt;Solar activity waxes and wanes on an 11-year cycle. We likely just passed through the maximum phase of the current one, known as Solar Cycle 25. (Scientists have been tracking these cycles diligently since 1755, when the numbering system began.) The next solar minimum is expected in 2030 or thereabouts.&lt;/p&gt;
    &lt;p&gt;As Nicolls noted, the atmospheric changes wrought by solar activity are of great interest and importance to satellite operators. An active sun causes a thicker atmosphere, which increases frictional drag on spacecraft and brings them down faster. Low solar activity has the opposite effect.&lt;/p&gt;
    &lt;p&gt;The downward migration in 2026 involves roughly half of SpaceX's Starlink megaconstellation, which currently consists of nearly 9,400 operational spacecraft (though that number is always growing). The fleet is highly reliable; there are just two dead Starlinks currently in orbit, according to Nicolls.&lt;/p&gt;
    &lt;p&gt;Breaking space news, the latest updates on rocket launches, skywatching events and more!&lt;/p&gt;
    &lt;p&gt;"Nevertheless, if a satellite does fail on orbit, we want it to deorbit as quickly as possible," he wrote. "These actions will further improve the safety of the constellation, particularly with difficult-to-control risks such as uncoordinated maneuvers and launches by other satellite operators."&lt;/p&gt;
    &lt;p&gt;Low earth orbit (LEO) is getting increasingly crowded these days. Starlink is the main driving factor; about two-thirds of all operational satellites belong to the megaconstellation. But other giant networks are being assembled as well. For example, China has begun building out two LEO internet constellations, each of which will each feature more than 10,000 spacecraft if all goes to plan.&lt;/p&gt;
    &lt;p&gt;Michael Wall is a Senior Space Writer with Space.com and joined the team in 2010. He primarily covers exoplanets, spaceflight and military space, but has been known to dabble in the space art beat. His book about the search for alien life, "Out There," was published on Nov. 13, 2018. Before becoming a science writer, Michael worked as a herpetologist and wildlife biologist. He has a Ph.D. in evolutionary biology from the University of Sydney, Australia, a bachelor's degree from the University of Arizona, and a graduate certificate in science writing from the University of California, Santa Cruz. To find out what his latest project is, you can follow Michael on Twitter.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.space.com/space-exploration/satellites/spacex-lowering-orbits-of-4-400-starlink-satellites-for-safetys-sake"/><published>2026-01-22T08:21:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46716696</id><title>In Praise of APL (1977)</title><updated>2026-01-22T09:22:58.744761+00:00</updated><content>&lt;doc fingerprint="d7080b9921a12be0"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;In Praise of APL:&lt;/p&gt;
          &lt;p&gt;Many reasons can be given for teaching one or more aspects of computer science (defined as the study of the set of phenomena arising around and because of the computer) to all university students. Probably every reader of this note supports some of these reasons. Let me list the few I find most important: (1) to understand and to be able to compose algorithms; (2) to understand how computers are organized and constructed; (3) to develop fluency in (at least) one programming language; (4) to appreciate the inevitability of controlling complexity through the design of systems; (5) to appreciate the devotion of computer scientists to their subject and the exterior consequences (to the student as citizen) of the science’s development.&lt;/p&gt;
          &lt;p&gt;Even though computer science deals with symbolic objects whose nature we study mathematically, it cannot be taught as an orderly development arising from a few fundamental ideas whose existence the student has already observed intuitively during his maturation, such as gravitation and electricity.&lt;/p&gt;
          &lt;p&gt;It is during this first computer course that the student awakes to the possibilities and consequences of computation. They arise most usefully and in greatest profusion during his writing of programs. He must program and program and program! He must learn how to state precisely in a programming language what he perceives about the nature of symbolic systems. I know of no better way to expedite this awakening than by programming.&lt;/p&gt;
          &lt;p&gt;But what should the student program? and in what language? I do not place much emphasis on heavy use of other people’s programs, packages if you will, that perform real services such as statistical packages, data management systems, linear equations solvers, etc. While it is wise to use standard programs when they match one’s needs, it is more important to master self-expression during this initial contact.&lt;/p&gt;
          &lt;p&gt;Available time is a limiting factor; a semester provides about 16 weeks of contact. During that interval the student must negotiate a set of tasks that sharpens his abilities and explodes his perceptions of the computer’s capabilities. He must be on the computer early and often during the semester and his approach to it must be smooth and easy. The computer system he uses should be time-sharing and interactive, if you will.&lt;/p&gt;
          &lt;p&gt;Learning to program involves a sequence of acts of discovery punctuated by recovery from errors. As the semester progresses the causes and nature of errors will change. Certain kinds will diminish and even disappear, only to be replaced by errors of deeper significance, harder to isolate and more resistant to satisfactory removal — syntactic gaffes give way to semantic errors — incorrect perceptions of purpose, improper use of means, the use of hammers to swat flies and swatters to level mountains.&lt;/p&gt;
          &lt;p&gt;To write correct and balanced programs a student may be forced to move between programs that are not related to each other by a few simple textual rearrangements. He must learn to write and test complicated programs quite rapidly. As he moves through the sequence of assigned tasks his ability to express himself fluently should not founder too soon because of language shortcomings. For all of the above reasons as well as a few others, I have come to believe that APL is the most rational first language for a first course in computer science.&lt;/p&gt;
          &lt;p&gt;It is true that BASIC and FORTRAN are easier to learn than APL, for example, a week versus a month. However, once mastered, APL fits the above requirements much better than either BASIC or FORTRAN or their successors ALGOL 60, PL/I and Pascal. The syntax of APL is not a significant difficulty for the students. The large number of primitive functions, at first mind-numbing in their capabilities, quickly turn out to be easily mastered, soon almost all are used naturally in every program — the primitive functions form a harmonious and useful set. As a data organization, arrays turn out to be extraordinarily useful (though prolonged contact with APL makes one wish for the added presence of more heterogeneous structures).&lt;/p&gt;
          &lt;p&gt;Style and Idiom&lt;/p&gt;
          &lt;p&gt;The virtues of APL that strike the programmer most sharply are its terseness — complicated acts can be described briefly, its flexibility — there are a large number of ways to state even moderately complicated tasks (the language provides choices that match divergent views of algorithm construction), and its composability — there is the possibility to construct sentences — one-liners as they are commonly called — that approach in the flow of phrase organization, sequencing and imbedding, the artistic possibilities achievable in natural language prose.&lt;/p&gt;
          &lt;p&gt;The sweep of the eye across a single sentence can expose an intricate, ingenious and beautiful interplay of operation and control that in other programming languages is observable only in several pages of text. One begins to appreciate the emergence and significance of style and to observe that reading and writing facility is tied to the development of an arsenal of idioms which soon become engraved in one’s skull as units.&lt;/p&gt;
          &lt;p&gt;The combination of these three factors makes it possible to develop an excellent set of exercises in the first course. These exercises can be large in number, cover a wide range of topics and vary widely in complexity, and still be done during the 16 week period. The later exercises can be tied to the design and development of a system — a collection of procedures that, in varying combinations, perform a set of tasks.&lt;/p&gt;
          &lt;p&gt;In Teaching Computer Organization&lt;/p&gt;
          &lt;p&gt;To appreciate computer science one requires an understanding of the computer. Once the student understands the computer — its macroscopic components monitored by the fetch-execute cycle and its apparent complexity being controlled by gigantic replication of a few simple logical elements — he can become aware of the important equilibrium between hardware and software — the shifting of function between the two as determined by economic factors — and between algorithm and system as determined by traffic and variation. Using APL it is straightforward to model a computer and to illustrate any of its macroscopic or microscopic components at any level of detail. The programs to perform these functions at every level of description remain small and manageable — about 40 lines or so.&lt;/p&gt;
          &lt;p&gt;The development of software, e.g., a machine language assembler, is a task of similar difficulty (about 40 lines) and hence possible within the confines of a first course.&lt;/p&gt;
          &lt;p&gt;Word processing and graphics, increasingly important application areas of computers, can be explored with exercises of no great size, e.g., to do permuted-index of title lists (~12 lines), display, rotation and scaling of composites of polygons (~20 lines), graphing of functions (~5 lines), etc.&lt;/p&gt;
          &lt;p&gt;With (or even without) the use of 2 or 3 pre-built functions, file processing problems such as payroll, personnel search, etc. can be written in a relatively few lines.&lt;/p&gt;
          &lt;p&gt;An important consequence of the attainable brevity of these compositions cannot be ignored: the student becomes aware that he need not be forced to depend upon external, pre-packaged and elaborate systems to do what are really simple programming tasks. Instead of learning a new coding etiquette to negotiate a complex external system, he writes his own programs, develops his own systems tailor-made to his own needs and understood at all levels of detail by him. If anything is meant by man-machine symbiosis, it is the existence of such abilities on the man side of the “membrane”, for there is no partnership here between man and machine, merely the existence of a growing, but never perfectly organized, inventory of tools that the competent can pick among, adapt and use to multiply his effective use of the computer.&lt;/p&gt;
          &lt;p&gt;I cannot overemphasize the importance of terseness, flexibility and phrase growth to a beginning student. His horizons of performance are being set in this first course. If he sees a task as a mountain then its reduction to molehill proportions is itself a considerable algorithmic task. While this is true of very large tasks, even when using APL this conscious chaining of organized reductions can be postponed until the student has already collected a large number of useful data-processing functions, engraved in his skull, with which to level mountains.&lt;/p&gt;
          &lt;p&gt;It is important to recognize that no matter how complicated the task, the APL functions will usually be anywhere from 1/5 to 1/10 the number of statements or lines, or what have you, of a FBAPP (FORTRAN or BASIC or ALGOL or PL/I or Pascal) program. Since APL distributes, through its primitive functions, control that the other languages explicate via sequences of statements dominated by explicit control statements, errors in APL programs tend to be far fewer in number than in their correspondents in FBAPP.&lt;/p&gt;
          &lt;p&gt;I can come now to the topics of structured programming and program verification. Both are important, but their content and importance depend strongly on the language in which programs are couched. A program is well-structured if it has a high degree of lexical continuity: small changes in program capability are acquired by making changes within lexically close text (modularization).&lt;/p&gt;
          &lt;p&gt;Since APL has a greater density of function within a given lexical scope than FBAPP, one would expect that APL programs will support considerably more structure than equivalent size FBAPP programs. Put another way, since the APL programs are 1/5 to 1/10 the size of FBAPP programs, the consequences to APL programs of weak structuring are less disastrous. Recovery from design mistakes is more rapid. Since we can only structure what we already understand, the delay in arriving at stable program organization should be considerably less with FBAPP!&lt;/p&gt;
          &lt;p&gt;Please note that the emphasis here is on the control of propagation of relationships, not the nonsense of restricting goto or bathing programs in cascades of while loops.&lt;/p&gt;
          &lt;p&gt;The verification, formal or informal, of programs is a natural and important activity. It is linked to specification: what we can’t specify we can’t verify. By specification we mean stating what is to be output for a given input. We immediately observe that, since specification in FBAPP is extremely tedious and unnatural, we must use some other language. APL turns out to be quite good and has often been suggested as a specification language. Assertions and verification conditions can be much more easily expressed as APL predicates than as FBAPP predicates. Because of the widespread distribution of control into the semantics of primitive functions, for which no proof steps need then be given, APL verifications tend to be, just as their counterpart APL programs, shorter and more analytic than equivalent FBAPP program verifications.&lt;/p&gt;
          &lt;p&gt;APL and Architecture&lt;/p&gt;
          &lt;p&gt;The form of the FBAPP languages follows closely the structure of the computers that prevailed during their inception. They have the nice property that one may often optimize machine performance of their compiled programs by transforming FBAPP programs to other FBAPP programs. Control of the computer is more easily exercised with programs in these languages than with APL, since the latter is more independent of current machines. For many programs this control over the target machine performance is quite vital, and APL couples more weakly to the standard computer than does FBAPP.&lt;/p&gt;
          &lt;p&gt;However, new array processing computers are beginning to appear and, had they been standard 20 years ago, APL and not FORTRAN would have been the prototype of language development. I often wonder at what descriptive levels we would be programming today had that been the case! Since it was not the case, we should not throw out or limit APL. We must seek ways to match it to the common computer. We must design compilers as well as computers that fit APL better.&lt;/p&gt;
          &lt;p&gt;More Cost-Effective than BASIC&lt;/p&gt;
          &lt;p&gt;Cost is an important issue in the instructional process. An APL computer system currently costs about $10K per terminal, about twice the cost of a BASIC system. As APL system designs stabilize and integrated circuitry costs drop, the two figures will coincide at or near the cost of a contemporary terminal. However, even now the APL system is cheaper than BASIC systems for equivalent work loads because one can do more than twice as much with APL in a given period of time than with BASIC!&lt;/p&gt;
          &lt;p&gt;Let me mention in closing two additional issues regarding the use of APL in an introductory computer science course. First, most university computer scientists don’t really know APL. They haven’t appreciated what it means to think in APL — to think about parallel operations in arrays and to distribute and submerge explicit looping among its primitive functions. I am reminded of the difficulties many math departments experience when they try to replace calculus by a fine math and combinatorics course as the first meat and potatoes offering by the department to the university. However at Yale we have found that faculty outside the software milieu — in theory, for example — pick up APL quite fast and prefer it to FBAPP. I am sure the same is true elsewhere.&lt;/p&gt;
          &lt;p&gt;The second issue is of a different kind. I am firmly convinced that APL and LISP are related to each other along an important axis of language design and that acquiring simultaneous expertise in both languages is possible and desirable for the beginning student. Were they unified, the set of tasks that succumb to terse, flexible and expressive descriptions will enlarge enormously without overly increasing the intellectual burden on the student over his initial 16 week contact period.&lt;/p&gt;
          &lt;p&gt;Above all, remember what we must provide is a pou sto to last the student for 40 years, not a handbook for tomorrow’s employment.&lt;/p&gt;
          &lt;p&gt;First appeared in SIAM News, 1977-06.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jsoftware.com/papers/perlis77.htm"/><published>2026-01-22T08:44:56+00:00</published></entry></feed>