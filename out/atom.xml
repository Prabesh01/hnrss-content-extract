<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-29T06:56:23.124947+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46414723</id><title>Software engineers should be a little bit cynical</title><updated>2025-12-29T06:56:30.553945+00:00</updated><content>&lt;doc fingerprint="2c3f911c3e160190"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Software engineers should be a little bit cynical&lt;/head&gt;
    &lt;p&gt;A lot of my readers call me a cynic when I say things like “you should do things that make your manager happy” or “big tech companies get to decide what projects you work on”. Alex Wennerberg put the “Sean Goedecke is a cynic” case well in his post Software Engineers Are Not Politicians. Here are some excerpts:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have no doubt that [Sean’s] advice is quite effective for navigating the upper levels of an organization dedicated to producing a large, mature software product. But what is lost is any sort of conception of value. Is it too naive to say that engineers are more than “tools in a political game”, they are specialized professionals whose role is to apply their expertise towards solving meaningful problems?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;The irony is that this kind of thinking destroys a company’s ability to actually make money … the idea that engineers should begin with a self-conception of doing what their manager tells them to is, to me, very bleak. It may be a good way to operate smoothly within a bureaucratic organization, and of course, one must often make compromises and take direction, but it is a bad way to do good work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I can see why people would think this way. But I love working in big tech companies! I do see myself as a professional solving meaningful problems. And I think navigating the organization to put real features or improvements in the hands of users is an excellent way - maybe the best way - to do good work.&lt;/p&gt;
    &lt;p&gt;Why do I write such cynical posts, then? Well, I think that a small amount of cynicism is necessary in order to think clearly about how organizations work, and to avoid falling into the trap of being overly cynical. In general, I think good engineers ought to be a little bit cynical.&lt;/p&gt;
    &lt;head rend="h3"&gt;The idealist view is more cynical than idealists think&lt;/head&gt;
    &lt;p&gt;One doctrinaire “idealist” view of software engineering goes something like this. I’m obviously expressing it in its most lurid form, but I do think many people believe this more or less literally:1&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We live in a late-stage-capitalist hellscape, where large companies are run by aspiring robber barons who have no serious convictions beyond desiring power. All those companies want is for obedient engineering drones to churn out bad code fast, so they can goose the (largely fictional) stock price. Meanwhile, end-users are left holding the bag: paying more for worse software, being hassled by advertisements, and dealing with bugs that are unprofitable to fix. The only thing an ethical software engineer can do is to try and find some temporary niche where they can defy their bosses and do real, good engineering work, or to retire to a hobby farm and write elegant open-source software in their free time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When you write it all out, I think it’s clear to see that this is incredibly cynical. At the very least, it’s a cynical way to view your coworkers and bosses, who are largely people like you: doing a job, balancing a desire to do good work with the need to please their own bosses. It’s a cynical way to view the C-staff of a company. I think it’s also inaccurate: from my limited experience, the people who run large tech companies really do want to deliver good software to users.&lt;/p&gt;
    &lt;p&gt;It’s idealistic only in the sense that it does not accept the need for individual software engineers to compromise. According to this view, you never need to write bad software. No matter how hard the company tells you to compromise and just get something out, you’re morally required to plant your feet and tell them to go to hell. In fact, by doing so, you’re taking a stand against the general degeneration of the modern software world. You’re protecting - unsung, like Batman - the needs of the end-user who will never know you exist.&lt;/p&gt;
    &lt;p&gt;I can certainly see the appeal of this view! But I don’t think it’s an idealistic appeal. It comes from seeing the world as fundamentally corrupted and selfish, and believing that real positive change is impossible. In other words, I think it’s a cynical appeal.&lt;/p&gt;
    &lt;head rend="h3"&gt;The cynical view is more idealistic than idealists think&lt;/head&gt;
    &lt;p&gt;I don’t see a hard distinction between engineers being “tools in a political game” and professionals who solve meaningful problems. In fact, I think that in practice almost all meaningful problems are solved by playing political games.&lt;/p&gt;
    &lt;p&gt;There are very few problems that you can solve entirely on your own. Software engineers encounter more of these problems than average, because the nature of software means that a single engineer can have huge leverage by sitting down and making a single code change. But in order to make changes to large products - for instance, to make it possible for GitHub’s 150M users to use LaTeX in markdown - you need to coordinate with many other people at the company, which means you need to be involved in politics.&lt;/p&gt;
    &lt;p&gt;It is just a plain fact that software engineers are not the movers and shakers in large tech organizations. They do not set the direction of the company. To the extent that they have political influence, it’s in how they translate the direction of the company into specific technical changes. But that is actually quite a lot of influence!&lt;/p&gt;
    &lt;p&gt;Large tech companies serve hundreds of millions (or billions) of users. Small changes to these products can have a massive positive or negative effect in the aggregate. As I see it, choosing to engage in the messy, political process of making these changes - instead of washing your hands of it as somehow impure - is an act of idealism.&lt;/p&gt;
    &lt;p&gt;I think the position of a software engineer in a large tech company is similar to people who go into public service: idealistically hoping that they can do some good, despite knowing that they themselves will never set the broad strokes of government policy.&lt;/p&gt;
    &lt;p&gt;Of course, big-tech software engineers are paid far better, so many people who go into this kind of work in fact are purely financially-motivated cynics. But I’m not one of them! I think it’s possible, by doing good work, to help steer the giant edifice of a large tech company for the better.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cynicism as inoculation&lt;/head&gt;
    &lt;p&gt;Cynical writing is like most medicines: the dose makes the poison. A healthy amount of cynicism can serve as an inoculation from being overly cynical.&lt;/p&gt;
    &lt;p&gt;If you don’t have an slightly cynical explanation for why engineers write bad code in large tech companies - such as the one I write about here - you risk adopting an overly cynical one. For instance, you might think that big tech engineers are being deliberately demoralized as part of an anti-labor strategy to prevent them from unionizing, which is nuts. Tech companies are simply not set up to engage in these kind of conspiracies.&lt;/p&gt;
    &lt;p&gt;If you don’t have a slightly cynical explanation for why large tech companies sometimes make inefficient decisions - such as this one - you risk adopting an overly cynical one. For instance, you might think that tech companies are full of incompetent losers, which is simply not true. Tech companies have a normal mix of strong and weak engineers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;Idealist writing is massively over-represented in writing about software engineering. There is no shortage of books or blog posts (correctly) explaining that we ought to value good code, that we ought to be kind to our colleagues, that we ought to work on projects with positive real-world impact, and so on. There is a shortage of writing that accurately describes how big tech companies operate.&lt;/p&gt;
    &lt;p&gt;Of course, cynical writing can harm people: by making them sad, or turning them into bitter cynics. But idealist writing can harm people too. There’s a whole generation of software engineers who came out of the 2010s with a factually incorrect model of how big tech companies work, and who are effectively being fed into the woodchipper in the 2020s. They would be better off if they internalized a correct model of how these companies work: not just less likely to get into trouble, but better at achieving their own idealist goals2.&lt;/p&gt;
    &lt;p&gt;edit: this post got some traction on Hacker News, with many comments. Some commenters said that it’s incoherent to say “what I do is good, actually” when my employer is engaged in various unethical activity. Fair enough! But this post isn’t about whether it’s ethical to work for Microsoft or not. It’s a followup to How good engineers write bad code at big companies - the main cynicism I’m interested in here is not “big tech is evil”, but “big tech is incompetent”.&lt;/p&gt;
    &lt;p&gt;Some other commenters challenged my claim that C-staff want to deliver good software by pointing out that they’re not willing to trade off their personal success to do so. Sure, I agree with that. The kind of person willing to sacrifice their career for things doesn’t typically make it to a C-level position. But it’s not always zero-sum. Good software makes money for software companies, after all.&lt;/p&gt;
    &lt;p&gt;I also saw two commenters link this as an example of big tech companies actually being engaged in conspiracies against their employees. I’m not convinced. Companies are structurally set up to collude on salaries, but they’re not set up to deliberately make their employees sad - they just don’t have that kind of fine-grained control over the culture! To the extent they have any control, they try to make their employees happy so they’ll work for less money and not leave.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;I don’t think I’m strawmanning here - I’ve seen many people make all of these points in the past, and I suspect at least some readers will be genuinely nodding along to the following paragraph. If you’re one of those readers (or if you only agree with about 50%), consider doing me a favor and emailing me to let me know! If I don’t get any emails I will probably rewrite this.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For some concrete details on this, see my post How I influence tech company politics as a staff software engineer. Also, if you’re interested, I wrote a much less well-developed version of this post right at the start of 2024, called Is it cynical to do what your manager wants?.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News. Here's a preview of a related post that shares tags with this one.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;You can't design software you don't work on&lt;/p&gt;&lt;p&gt;Only the engineers who work on a large software system can meaningfully participate in the design process. That’s because you cannot do good software design without an intimate understanding of the concrete details of the system. In other words, generic software design advice is typically useless for most practical software design problems.&lt;/p&gt;&lt;p&gt;What is generic software design? It’s “designing to the problem”: the kind of advice you give when you have a reasonable understanding of the domain, but very little knowledge of the existing codebase. Unfortunately, this is the only kind of advice you’ll read in software books and blog posts. Engineers love giving generic software design advice for the same reason that all technical professionals love “talking shop”. However, you should be very careful about applying generic advice to your concrete day-to-day work problems.&lt;/p&gt;&lt;lb/&gt;Continue reading...&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/a-little-bit-cynical/"/><published>2025-12-28T21:29:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46414819</id><title>Unity's Mono problem: Why your C# code runs slower than it should</title><updated>2025-12-29T06:56:29.795133+00:00</updated><content>&lt;doc fingerprint="f319d3cb995267ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Unity's Mono problem: Why your C# code runs slower than it should&lt;/head&gt;
    &lt;p&gt;Execution of C# code in Unityâs Mono runtime is slow by todayâs standards, much slower than you might expect! Our game runs 2-3x faster on modern .NET compared to Unityâs Mono, and in a few small benchmarks I measured speedups of up to 15x. Iâve spent some time investigating whatâs going on and in this article I will present my findings and why everyone should want Unityâs .NET modernization to become production-ready as soon as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;How did we get here&lt;/head&gt;
    &lt;p&gt;Unity uses the Mono framework to run C# programs and back in 2006 it was one of the only viable multi-platform implementations of .NET. Mono is also open-source, allowing Unity to do some tweaks to better suit game development.&lt;/p&gt;
    &lt;p&gt;An interesting twist happened nearly 10 years later. In 2014, Microsoft began open-sourcing .NET (notably .NET Core later that year) and in June 2016, .NET Core 1.0 shipped with official cross-platform support. Since then, the .NET ecosystem gained momentum and lots of improvements have been made, including the Roslyn compiler platform, a new JIT (just-in-time compiler), performance improvements, more features, etc.&lt;/p&gt;
    &lt;p&gt;In 2018, Unity engineers discussed that they are working on porting the engine to .NET CoreCLR, the multi-platform version of Common Language Runtime (CLR), a component that runs .NET programs. Their main motivations behind this project were performance and convergence. In their post they said:&lt;/p&gt;
    &lt;quote&gt;...CoreCLR could be great for Unity game developers, as it will provide a significant boost in performance, by an order of 2x to 5x compare to the Mono runtime sometimes up to x10 on some workload!&lt;/quote&gt;
    &lt;p&gt;Unfortunately, now itâs the end of 2025 and we still canât run games on CoreCLR.&lt;/p&gt;
    &lt;head rend="h2"&gt;The performance gap&lt;/head&gt;
    &lt;p&gt;We donât hear about the performance gap between Mono and .NET much, likely because it is not possible to run games written for Unity under modern .NET. But we can still do a direct comparison with code that does not depend on Unity directly.&lt;/p&gt;
    &lt;p&gt;Our game has a unique architecture â we strictly separate the game simulation code (business logic) from rendering. So much so that the simulation code does not depend on Unityâs libraries and can be compiled and run under any .NET version.&lt;/p&gt;
    &lt;p&gt;One day I was debugging an issue in map generation and it was time-consuming because it was taking over 2 minutes to start a game. To make debugging faster, Iâve written a unit test, hoping to cut down on the turn-around time since Unity takes 15+ seconds just to crunch new DLLs and reload the domain before the game can be launched and it also initializes rendering stuff that I did not care about. When I ran the test, it finished in 40 seconds. I was quite surprised that it was more than 3x faster, so I started digging deeper.&lt;/p&gt;
    &lt;p&gt;Long story short, Figure 1 shows traces from a profiler showing the difference between the game launching in Unity running under Mono vs. a unit test running under .NET.&lt;/p&gt;
    &lt;p&gt;Note that all shown benchmarks are using either Unity 6.0 or .NET 10.&lt;/p&gt;
    &lt;p&gt;So our benchmark shows that loading a save file, generating a map, and initializing the simulation takes 100 seconds in Unity/Mono but only 38 seconds in .NET. This result alone is already something that may raise eyebrows and has real consequences of how you may want to approach debugging and testing.&lt;/p&gt;
    &lt;p&gt;I also know from experience with Unity that Release mode running as a standalone executable (without the Unity editor) is much faster, so I decided to test that next.&lt;/p&gt;
    &lt;head rend="h2"&gt;.NET vs. Mono in standalone Release mode&lt;/head&gt;
    &lt;p&gt;Debug mode slowness is not great, but even non-optimized C++ code can be slow. To compare the real performance gap between Mono and .NET, letâs run the same benchmark as above but in release mode, standalone executable.&lt;/p&gt;
    &lt;p&gt;First up: Unity. Iâve run our deploy script to get an optimized executable and run it directly. Unsurprisingly, optimized standalone executable is beating Unity editor by a big margin, more than 3x faster. Next, the same code running under .NET in Release mode. Figure 2 shows the results.&lt;/p&gt;
    &lt;p&gt;Yep. 12 seconds. Itâs actually mind-boggling how much work is being done in these 12 seconds and when I saw this for the first time, I was not only shocked, but also impressed. Just so you know, a 4k Ã 4k map is being generated using all available threads out of hundreds of combined noise functions in like 3 seconds. Figure 3 shows the trace expanded.&lt;/p&gt;
    &lt;p&gt;If you are interested in seeing the actual x86 assembly generated by Mono and .NET JITs, see the Extras section at the end of this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;As you can see from the presented benchmarks, Mono is massively behind .NET in terms of performance. This is primarily due to differences in runtime optimizations and JIT that generates unoptimized assembly. The actual speedup surely depends on the code itself, but from my research, 1.5-3x speedup of C# execution is very likely for most projects.&lt;/p&gt;
    &lt;p&gt;If you are a game developer using Unity, or even a player, you can now understand that CoreCLR would be a massive boost to performance of games and even the Unity editor. Unfortunately, for the past 8 years, Unity leadership was more interested in âother thingsâ and did not give .NET modernization the attention it deserves.&lt;/p&gt;
    &lt;p&gt;Some view .NET modernization as support for new language features in C#, but that is just a cherry on top. New C# adds some handy features, but the new JIT can deliver multi-x speedups.&lt;/p&gt;
    &lt;p&gt;At this year's Unite conference, Unity announced that CoreCLR is still ongoing but it wonât be production ready in 2026. The good news is that it now seems to be on the Unity 6.x roadmap, and not left for later versions as suggested by 2024âs Unite presentation.&lt;/p&gt;
    &lt;p&gt;Moreover, CoreCLR is not just new JIT and C#, it unlocks broader and better-optimized support for things like Span&amp;lt;T&amp;gt;-style APIs, hardware intrinsics, and newer SIMD paths that devs cannot use these days. These features could add another multiplier to the performance gains for some classes of code. For example, our map generator heavily uses 2D and 3D simplex noise. I bet that having access to new runtime features in CoreCLR could speed up the map generation by another 2x.&lt;/p&gt;
    &lt;p&gt;Unity has a technology called Burst that automatically converts marked C# methods to optimized native assembly via the LLVM compiler. This sounds neat as it can avoid the poor JIT performance, but the downside is that Burst has strict limitations on what can be converted and supports only subset of C#. I believe that CoreCLR with modern JIT will have very similar performance characteristics to Burst. I am curious what would happen in a universe where Unity invested all the time and effort in CoreCLR support and high-performance C#, instead of developing and maintaining Burst.&lt;/p&gt;
    &lt;p&gt;Another interesting consequence of CoreCLR support is the ability to pre-compile the .NET intermediate assembly to machine code using ahead-of-time compilation (AOT). AOT can further improve startup time and is essential on platforms where JIT is restricted (notably iOS). Nowadays, Unity solves this with IL2CPP that takes the intermediate code and compiles it to C++ which is then optimized and compiled to native assembly. However, according to RichardFine (Unity staff), using CoreCLR AOT is not planned and IL2CPP is here to stay:&lt;/p&gt;
    &lt;quote&gt;AOT for IL2CPP is completely independent of AOT for CoreCLR (which we have no plans to adopt anyway). GC behaviour on IL2CPP improves when we upgrade the GC there, itâs not really affected by CoreCLR at all.&lt;/quote&gt;
    &lt;p&gt;In conclusion, CoreCLR wonât magically fix every bottleneck in a Unity game, but it does fix many of the code generation inefficiencies and allows writing higher-performance code. The benchmark presented in this article is meant to illustrate that modern .NET has spent years squeezing more work into fewer CPU cycles, and Unity users are largely locked out of those gains today.&lt;/p&gt;
    &lt;p&gt;If Unity can deliver production-ready CoreCLR support, it wonât just mean ânewer C#â. It will mean faster runtime performance, faster iteration times, more performance headroom, no domain reload, better GC behavior, and maybe even more managed code and less native code. Until then, the gap will remain an invisible tax on every Unity project that leans on managed code.&lt;/p&gt;
    &lt;p&gt;Iâm cheering for you, Unity devs, CoreCLR for the win!&lt;/p&gt;
    &lt;head rend="h2"&gt;Extras: Comparison of x86 assembly&lt;/head&gt;
    &lt;p&gt;I have actually dug much deeper into the performance aspects of Mono vs .NET but for the sake of this article not being too long, here is a brief summary.&lt;/p&gt;
    &lt;p&gt;Code listing 1 shows the testing code. It does some basic summing of custom structs that are wrappers around ints. This is an interesting example because Mono is very bad at inlining and simplifying expressions, even obvious ones, and we have plenty of structs like these in our code base (e.g. Quantity, MechPower, Tile2i, etc).&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;static class Program { static void Main() { Console.WriteLine(RunTest(int.MaxValue)); } public static TestStruct RunTest(int iterations) { TestStruct value1 = new TestStruct(iterations % 2); TestStruct value2 = new TestStruct(iterations % 7); TestStruct value3 = new TestStruct(iterations % 13); TestStruct result = default; for (int i = 0; i &amp;lt; iterations; ++i) { result += value1 + value2; result += value1 + value3; } return result; } } readonly struct TestStruct { public readonly int Value; public TestStruct(int value) { Value = value; } public static TestStruct operator +(TestStruct lhs, TestStruct rhs) { return new TestStruct(lhs.Value + rhs.Value); } public override string ToString() =&amp;gt; Value.ToString(); }&lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt;To obtain assembly code, Iâve compiled the code in Release mode and ran it as a standalone executable. Then, I attached a debugger to the running process. An easy way to find this loop was to make it long/infinite and just break the program at any time, it would end up in that loop.&lt;/p&gt;
    &lt;p&gt;First, letâs take a look at .NET. Here is the x64 assembly of the for-loop section of the code.&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 8 9 10 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;add r8d,edx add edx,r10d 00007FFDEC338E88: mov r10d,r8d add r9d,r10d mov r10d,edx add r9d,r10d inc ecx cmp ecx,eax jl 00007FFDEC338E88&lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt; In both cases, the full loop of &lt;code&gt;int.MaxValue&lt;/code&gt; iterations took around 750 ms on my machine.
&lt;/p&gt;
    &lt;p&gt; This looks neat. Even if you donât read assembly, you can see that there are two add instructions, one decrement, and one jump. It seems that the JIT hoisted the invariant sums &lt;code&gt;a = value1 + value2&lt;/code&gt; and &lt;code&gt;b = value1 + value3&lt;/code&gt; out of the loop and then just accumulates them.
&lt;/p&gt;
    &lt;p&gt;I also tested x86 assembly, and it looks very similar:&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;082E18D0: lea ebx,[esi+edi] add eax,ebx lea ebx,[esi+edx] add eax,ebx dec ecx jne 082E18D0&lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt;Interestingly, the loop direction was reversed, counting down. This saves one instruction as comparison to zero and conditional jump can be done as one instruction.&lt;/p&gt;
    &lt;p&gt;Now letâs look at Monoâs x64 assembly.&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;1E87D2F3E20: movsxd rax,dword ptr [rsp+0C0h] mov dword ptr [rsp+40h],eax movsxd rax,dword ptr [rsp+0B8h] mov dword ptr [rsp+38h],eax movsxd rax,dword ptr [rsp+40h] mov dword ptr [rsp+0A0h],eax movsxd rax,dword ptr [rsp+38h] mov dword ptr [rsp+98h],eax movsxd rax,dword ptr [rsp+0A0h] movsxd rcx,dword ptr [rsp+98h] add eax,ecx mov dword ptr [rsp+90h],0 mov dword ptr [rsp+90h],eax mov dword ptr [rsp+30h],eax movsxd rax,dword ptr [rsp+0A8h] mov dword ptr [rsp+88h],eax movsxd rax,dword ptr [rsp+30h] mov dword ptr [rsp+80h],eax movsxd rax,dword ptr [rsp+88h] movsxd rcx,dword ptr [rsp+80h] add eax,ecx mov dword ptr [rsp+78h],0 mov dword ptr [rsp+78h],eax mov dword ptr [rsp+0A8h],eax mov dword ptr [rsp+28h],eax movsxd rax,dword ptr [rsp+0C0h] mov dword ptr [rsp+20h],eax movsxd rax,dword ptr [rsp+0B0h] mov dword ptr [rsp+18h],eax movsxd rax,dword ptr [rsp+20h] mov dword ptr [rsp+70h],eax movsxd rax,dword ptr [rsp+18h] mov dword ptr [rsp+68h],eax movsxd rax,dword ptr [rsp+70h] movsxd rcx,dword ptr [rsp+68h] add eax,ecx mov dword ptr [rsp+60h],0 mov dword ptr [rsp+60h],eax mov dword ptr [rsp+10h],eax movsxd rax,dword ptr [rsp+28h] mov dword ptr [rsp+58h],eax movsxd rax,dword ptr [rsp+10h] mov dword ptr [rsp+50h],eax movsxd rax,dword ptr [rsp+58h] movsxd rcx,dword ptr [rsp+50h] add eax,ecx mov dword ptr [rsp+48h],0 mov dword ptr [rsp+48h],eax mov dword ptr [rsp+0A8h],eax inc esi cmp esi,7FFFFFFFh jl 1E87D2F3E20&lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt; As you can see just from the number of instructions, this code will run way slower. The full loop of &lt;code&gt;int.MaxValue&lt;/code&gt; iterations took around 11500 ms, thatâs ~15x slower.
&lt;/p&gt;
    &lt;p&gt;In the assembly you can see the four add instructions in the loop, the âinefficientâ increment + comparison + jump (instead of decrement + conditional jump), and most importantly a sea of mov instructions, which are just memory copies from inefficient inlining of the struct fields. Basically Mono is just tossing values around memory.&lt;/p&gt;
    &lt;p&gt;I have also tested assembly compiled in Debug mode running in the Unity editor and itâs even worse. The full loop takes 67 seconds (67000 ms)! In Unity Editor, the JIT likely switches to far less optimized codegen and includes additional checks/sequence-point overhead, which balloons runtime.&lt;/p&gt;
    &lt;p&gt;Takeaway: modern .NETâs JIT can scalarize tiny value types and hoist invariant work so the hot loop becomes a handful of register ops, while Mono often fails to do so and ends up shuffling values through memory, exactly the kind of gap that shows up as slowdowns in real simulation-heavy code.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://marekfiser.com/blog/mono-vs-dot-net-in-unity/"/><published>2025-12-28T21:41:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46414837</id><title>Why I Disappeared – My week with minimal internet in a remote island chain</title><updated>2025-12-29T06:56:29.649853+00:00</updated><content>&lt;doc fingerprint="5df6d2b4112c141d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Disappeared&lt;/head&gt;
    &lt;head rend="h3"&gt;My week with minimal internet in a remote island chain&lt;/head&gt;
    &lt;p&gt;I’m writing this on my flight back from a weeklong trip to the Galapagos Islands, where for the first time in years I was largely without access to the Internet. For someone as congenitally online as I am, this at first seemed a curse. It turned out to be far from that and has me questioning a lot of what I thought I knew about media, politics and, well, people.&lt;/p&gt;
    &lt;p&gt;My Galapagos excursion took place on a boat with over a dozen other travelers. They were young and older, professors and small business owners (even an Army colonel!), Republicans and Democrats, from big cities and small towns. People from wildly different backgrounds readily shared sunscreen, snacks, even life advice. And while it wasn’t partisan, it also wasn’t apolitical: issues like the outrageous cost of housing, healthcare and childcare came up. Yet absent was the political vitriol that the national security state says necessitates a new domestic war on terrorism.&lt;/p&gt;
    &lt;p&gt;Contrary to the national security threat machine’s picture of a country at war with itself, we all got along so swimmingly that the idea of a civil war or anything like it struck me as laughable, as did the notion that the statistically insignificant number of politically-motivated killings, though real, said anything at all about the vast majority of real-world Americans. I say ‘real-world’ to distinguish from the Internet, where anonymity and disembodied reality can lead to people saying things they never would in real life.&lt;/p&gt;
    &lt;p&gt;The Galapagos are unique because its extreme remoteness has insulated various species from outside predators, providing Charles Darwin a controlled setting to observe and later theorize evolution. That same remoteness — in my case from the constant intravenous drip of the internet and social media for the first time in maybe my entire adult life — left me with the staggering realization that what passes for news is mostly just noise.&lt;/p&gt;
    &lt;p&gt;Where Darwin prized the islands for its incubation of remarkably distinct animal lineages, I too benefited from the bewitching remoteness of the Galapagos. During the brief, intermittent moments that I had time to check the news (rather than living through it moment by moment), I realized how utterly forgettable and meaningless most of it was.&lt;/p&gt;
    &lt;p&gt;Not watching every twist and turn about, say, the latest Epstein transparency failure, I noticed how little these news cycles ultimately produce — a very different picture than the 24/7 cycle creates. When I saw that Washington media had dogpiled Trump’s chief of staff Susie Wiles for offering some mildly critical remarks about Elon Musk and other administration figures, it occurred to me that no one would give a shit about or even remember any of this a week from now.&lt;/p&gt;
    &lt;p&gt;I came away shocked and sad at how much the media traffics in fake urgency as a result of its quest for the click. Combine that with national security’s constant drumbeat of civil war, disinformation, terrorism, violence, and the threat from within and you can see why people disengage from the news. It’s not they don’t care, lack “media literacy,” or any of the usual explanations. They look at the hurricane of sensational headlines and aren’t sure what they’re supposed to do with any of it or if they should even believe it. And they’re right.&lt;/p&gt;
    &lt;p&gt;When another traveler on the boat, an academic, remarked that she didn’t really follow the news, it occurred to me that if even a very intelligent, well-educated and thoughtful person feels this way, the media has a much deeper problem than supposedly lazy audiences.&lt;/p&gt;
    &lt;p&gt;None of this is to say that there isn’t important news out there. But as with evolution, change is usually more gradual, like the shifting of tectonic plates, than the 24/7 cycle suggests.&lt;/p&gt;
    &lt;p&gt;One of the most striking features of Galapagos is its record number of “endemic” species, or those that can only be found in this one place. That’s why the islands have an almost otherworldly feeling with such unique creatures as the giant Galapagos tortoise, the blue-footed booby, and the Galapagos penguin. The takeaway here is that sometimes evolution requires insulation from outside forces. And if I’m honest, as much as I love social media, it makes it difficult to develop the intellectual equivalent of Galapagos’ delicate species because these gargantuan apps foster a ruthless competition for your attention that becomes a race to the bottom.&lt;/p&gt;
    &lt;p&gt;In other words, social media is such a free-for-all of attention-grabbing stunts that it’s hard to see through the blizzard of posts to what actually matters — which is supposed to be the whole point of journalism.&lt;/p&gt;
    &lt;p&gt;What the incentives for now, now, now creates is a perverse system of survival of the dumbest. Just take a look at the insane number of hoax Epstein documents and allegations swirling around social media right now. A doctored video of Jeffrey Epstein committing suicide in his cell has generated millions of views on X alone, despite the hoax clip having first circulated in 2020.&lt;/p&gt;
    &lt;p&gt;As I return to celebrate Christmas, I realize that I didn’t really miss much during my week without social media. What passes for the news though, at its breakneck speed, zooms here and there, initially making it feel like I missed so much, that it’s all so overwhelming, that I can’t possibly keep up and should instead retreat to my family and friends, my reading, my hobbies, my team, my town. Nothing encourages me to be involved. And though I know that those in charge don’t have everything under control, don’t have a plan, and are grossly committed to survival of the fittest, I also am reminded that they love it this way. The blizzard of false urgency leaves us, the public, confused — just how the elite like it. We are prey.&lt;/p&gt;
    &lt;p&gt;But I have a plan. In this next year I’ll be focusing more on stories that actually matter instead of chasing the flash-in-the-pan ephemera that nobody remembers the next week. And most importantly, I’m also going to create my own island, by launching a new kind of website apart from Substack and the social media maelstrom. My dream is to create a home for news that is truly endemic to the site — that you can’t find anywhere else — by incubating stories that evince the kind of uniqueness as the magical creatures of the Galapagos.&lt;/p&gt;
    &lt;p&gt;Help me protect the fragile species that isn’t predatory, clickbait journalism by becoming a paid subscriber below (or via our GoFundMe if you prefer a one-off contribution.)&lt;/p&gt;
    &lt;p&gt;— Edited by William M. Arkin&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kenklippenstein.com/p/why-i-disappeared"/><published>2025-12-28T21:45:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415129</id><title>Researchers discover molecular difference in autistic brains</title><updated>2025-12-29T06:56:29.241810+00:00</updated><content>&lt;doc fingerprint="f7fe1347089b1bdd"&gt;
  &lt;main&gt;
    &lt;p&gt;Yale School of Medicine (YSM) scientists have discovered a molecular difference in the brains of autistic people compared to their neurotypical counterparts.&lt;/p&gt;
    &lt;p&gt;Autism is a neurodevelopmental condition associated with behavioral differences including difficulties with social interaction, restrictive or intense interests, and repetitive movements or speech. But it’s not clear what makes autistic brains different.&lt;/p&gt;
    &lt;p&gt;Now, a new study in The American Journal of Psychiatry has found that brains of autistic people have fewer of a specific kind of receptor for glutamate, the most common excitatory neurotransmitter in the brain. The reduced availability of these receptors may be associated with various characteristics linked to autism.&lt;/p&gt;
    &lt;p&gt;“We have found this really important, never-before-understood difference in autism that is meaningful, has implications for intervention, and can help us understand autism in a more concrete way than we ever have before,” says James McPartland, PhD, Harris Professor of Child Psychiatry and Psychology in the Child Study Center at YSM and the study’s co-principal investigator.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://medicine.yale.edu/news-article/molecular-difference-in-autistic-brains/"/><published>2025-12-28T22:23:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415225</id><title>What an unprocessed photo looks like</title><updated>2025-12-29T06:56:28.661716+00:00</updated><content>&lt;doc fingerprint="e049a755b7543a7"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;What an unprocessed photo looks like:&lt;/head&gt;(Photography)&lt;p&gt;Here’s a photo of a Christmas tree, as my camera’s sensor sees it:&lt;/p&gt;&lt;p&gt;It’s not even black-and-white, it’s gray-and-gray.&lt;/p&gt;&lt;p&gt;This is becuase while the camera’s analog-to-digital converter (ADC) output can theoretically output values from 0 to 16382, the data doesn’t cover that whole range:&lt;/p&gt;&lt;p&gt;The real range of ADC values is ~2110 to ~136000. Let’s set those values as the white and black in the image:&lt;/p&gt;&lt;p&gt;Vnew = (Vold - Black)/(White - Black)&lt;/p&gt;&lt;p&gt;Much better, but it’s still more monochromatic then I remember the tree being. Camera sensors aren’t actually able to see color: They only measure how much light hit each pixel.&lt;/p&gt;&lt;p&gt;In a color camera, the sensor is covered by a grid of alternating color filters:&lt;/p&gt;&lt;p&gt;Let’s color each pixel the same as the filter it’s looking through:&lt;/p&gt;&lt;p&gt;This version is more colorful, but each pixel only has one third of it’s RGB color. To fix this, I just averaged the values each pixel with it’s neighbors:&lt;/p&gt;&lt;p&gt;Applying this process to the whole photo gives the lights some color:&lt;/p&gt;&lt;p&gt;However, the image is still very dark. This is because monitors don’t have as much dynamic range as the human eye, or a camera sensor: Even if you are using an OLED, the screen still has some ambient light reflecting off of it and limiting how black it can get.&lt;/p&gt;&lt;p&gt;There’s also another, sneakier factor causing this:&lt;/p&gt;&lt;p&gt;Our perception of brightness is non-linear.&lt;/p&gt;&lt;p&gt;If brightness values are quantized, most of the ADC bins will be wasted on nearly identical shades of white while every other tone is crammed into the bottom. Because this is an inefficient use of memory, most color spaces assign extra bins to darker colors:&lt;/p&gt;&lt;p&gt;As a result of this, if the linear data is displayed directly, it will appear much darker then it should be.&lt;/p&gt;&lt;p&gt;Both problems can be solved by applying a non-linear curve to each color channel to brighten up the dark areas… but this doesn’t quite work out:&lt;/p&gt;&lt;p&gt;Some of this green cast is caused by the camera sensor being intrinsically more sensitive to green light, but some of it is my fault: There are twice as many green pixels in the filter matrix. When combined with my rather naive demosaicing, this resulted in the green channel being boosted even higher.&lt;/p&gt;&lt;p&gt;In either case, it can fixed with proper white-balance: Equalize the channels by multipling each one with a constant.&lt;/p&gt;&lt;p&gt;However, because the image is now non-linear, I have to go back a step to do this. Here’s the dark image from before with all the values temporarily scaled up so I can see the problem:&lt;/p&gt;&lt;p&gt;… here’s that image with the green taken down to match the other channels:&lt;/p&gt;&lt;p&gt;… and after re-applying the curve:&lt;/p&gt;&lt;p&gt;This is really just the bare minimum: I haven’t done any color calibration, the white balance isn’t perfect, the black points are too high, there’s lots of noise that needs to be cleaned up…&lt;/p&gt;&lt;p&gt;Additionally, applying the curve to each color channel accidentally desaturated the highlights. This effect looks rather good — and is what we’ve come to expect from film — but it’s has de-yellowed the star. It’s possible to separate the luminance and curve it while preserving color. On it’s own, this would make the LED Christmas lights into an overstaturated mess, but combining both methods can produce nice results.&lt;/p&gt;&lt;p&gt;For comparison, here’s the image my camera produced from the same data:&lt;/p&gt;&lt;p&gt;Far from being an “unedited” photo: there’s a huge amount of math that’s gone into making an image that nicely represents what the subject looks like in person.&lt;/p&gt;&lt;p&gt;There’s nothing that happens when you adjust the contrast or white balance in editing software that the camera hasn’t done under the hood. The edited image isn’t “faker” then the original: they are different renditions of the same data.&lt;/p&gt;&lt;p&gt;In the end, replicating human perception is hard, and it’s made harder when constrained to the limitations of display technology or printed images. There’s nothing wrong with tweaking the image when the automated algorithms make the wrong call.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maurycyz.com/misc/raw_photo/"/><published>2025-12-28T22:35:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415338</id><title>As AI gobbles up chips, prices for devices may rise</title><updated>2025-12-29T06:56:28.351137+00:00</updated><content>&lt;doc fingerprint="8a32d49868b221ec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Memory loss: As AI gobbles up chips, prices for devices may rise&lt;/head&gt;
    &lt;head rend="h4"&gt;Memory loss: As AI gobbles up chips, prices for devices may rise&lt;/head&gt;
    &lt;p&gt;The world has a memory problem, thanks to artificial intelligence.&lt;/p&gt;
    &lt;p&gt;The explosion in AI-related cloud computing and data centers has led to so much demand for certain types of memory chips that now there's a shortage. The imbalance is expected to start affecting prices of all sorts of products powered by technology.&lt;/p&gt;
    &lt;p&gt;"I keep telling everybody that if you want a device, you buy it now," said Avril Wu, a senior research vice president at TrendForce, a Taiwan-based consultancy that tracks markets for computer components. "I myself bought an iPhone 17 already,"&lt;/p&gt;
    &lt;p&gt;The chips are known as RAM, or random access memory, and are crucial to making sure that things like smartphones, computers and game consoles run smoothly. Chips allow you to keep multiple tabs open in browsers, for instance, or watch videos without them being choppy.&lt;/p&gt;
    &lt;p&gt;Wu said TrendForce's data indicates that demand for RAM chips exceeds supply by 10% – and it's growing so fast that manufacturers are having to shell out a lot more to buy them each month.&lt;/p&gt;
    &lt;p&gt;Wu said this quarter alone, they're paying 50% more than the previous quarter for the most common type of RAM, known as DRAM – dynamic random access memory. And if producers want the chips sooner, they're paying two to three times more.&lt;/p&gt;
    &lt;p&gt;Wu expects DRAM prices to rise another 40% in the coming quarter, and she doesn't expect the prices to go down in 2026.&lt;/p&gt;
    &lt;head rend="h3"&gt;How AI is gobbling up memory&lt;/head&gt;
    &lt;p&gt;AI data centers require huge amounts of memory to accompany their cutting-edge graphics processing unit (GPU) microprocessors that train and operate AI models.&lt;/p&gt;
    &lt;p&gt;"AI workloads are built around memory," said Sanchit Vir Gogia, CEO of the tech advisory firm Greyhound Research.&lt;/p&gt;
    &lt;p&gt;What's more, AI companies are spending billions of dollars constructing data centers at warp speed around the world. It's the reason why Gogia says the demand for these chips isn't just a cyclical blip.&lt;/p&gt;
    &lt;p&gt;"AI has changed the nature of demand itself," he said. "Training and inference systems require large, persistent memory footprints, extreme bandwidth, and tight proximity to compute. You cannot dial this down without breaking performance."&lt;/p&gt;
    &lt;head rend="h3"&gt;More chips for AI means fewer chips for other products&lt;/head&gt;
    &lt;p&gt;Idaho-based Micron Technology is one of the world's top makers of RAM and it's benefited from this increase in demand. It reported better-than-expected quarterly earnings last week on the back of higher memory chip prices.&lt;/p&gt;
    &lt;p&gt;CEO Sanjay Mehrotra said the company expected the market to remain strong, as the AI boom continues apace. "We believe that the aggregate industry supply will remain substantially short of the demand for the foreseeable future," he said on a webcast after the earnings report.&lt;/p&gt;
    &lt;p&gt;Chipmakers like Micron have shifted production to meet as much of the lucrative AI-related demand for high-end memory as they can, according to analysts. That translates into fewer chips for other segments of the market – personal computers, mobile phones, games and consumer products like TVs.&lt;/p&gt;
    &lt;p&gt;And that means higher costs. Dell Technologies Chief Operating Officer Jeff Clarke noted the higher costs on an earnings call on Nov. 25. For PC's, he said "I don't see how this will certainly not make its way into the customer base."&lt;/p&gt;
    &lt;p&gt;Analysts say there is no short-term fix.&lt;/p&gt;
    &lt;p&gt;Tech consultant Wu said the memory chip industry faces a significant bottleneck. By the end of 2026, she said, chip makers will have maxed out how much they can expand production in their current facilities.&lt;/p&gt;
    &lt;p&gt;She said the next new factory expected to come online is being built by Micron in Idaho. The company says it will be operational in 2027.&lt;/p&gt;
    &lt;p&gt;Expect suppliers to keep raising prices for the foreseeable future, Wu said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram"/><published>2025-12-28T22:52:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415426</id><title>62 years in the making: NYC's newest water tunnel nears the finish line</title><updated>2025-12-29T06:56:27.862675+00:00</updated><content>&lt;doc fingerprint="aac0a95e8b65c6b6"&gt;
  &lt;main&gt;
    &lt;p&gt;Turn on the tap, and water flows without a second thought. But deep beneath New York City, hundreds of feet below street level, workers are finishing a project that’s been under construction for more than half a century — a massive water tunnel that will help keep that simple act possible for generations to come.&lt;/p&gt;
    &lt;p&gt;Tunnel No. 3, as it’s known, is one of the most ambitious infrastructure projects in the city’s history.&lt;/p&gt;
    &lt;p&gt;When complete, it will ensure New Yorkers continue to receive clean water from upstate reservoirs — some more than 125 miles away — while allowing long-overdue maintenance on the city’s two older tunnels, built in 1917 and 1936.&lt;/p&gt;
    &lt;p&gt;City Department of Environmental Protection Commissioner Rohit Aggarwala and DEP Portfolio Manager Lauren D’Attile recently took an elevator nearly 800 feet down to see the progress for themselves.&lt;/p&gt;
    &lt;p&gt;“It’s not quite as far down as the Empire State Building is tall, but it’s getting there,” Aggarwala said during the 10-minute descent.&lt;/p&gt;
    &lt;p&gt;Down below, flashlights cut through the darkness as water dripped from the rock walls. Workers stood in waterproof boots along the cool, damp concrete — the result of decades of digging, drilling and sealing off bare rock to create a watertight tunnel system.&lt;/p&gt;
    &lt;p&gt;“When this tunnel was originally constructed, it was built by a tunnel boring machine, which is a very large piece of equipment with cutter heads on the front,” said D’Attile. “We drill the tunnel and after that we line that bare rock with a couple of feet of concrete — so that’s what you’re seeing now, because this tunnel is complete.”&lt;/p&gt;
    &lt;p&gt;Construction on Tunnel No. 3 began in 1970.&lt;/p&gt;
    &lt;p&gt;The Bronx and Manhattan already receive water from it, and the final phase — extending service to Brooklyn and Queens — is expected to be completed by 2032.&lt;/p&gt;
    &lt;p&gt;“The project started in 1970, it will be finished in 2032 — that’s 62 years to build this thing,” Aggarwala said. “But a project like this is going to serve New York for two, three hundred years, who knows how much longer than that. Seems worth it. Totally worth it. It’s what makes the city work because we are constantly investing in our future.”&lt;/p&gt;
    &lt;p&gt;When it’s complete, the DEP will finally be able to take the older tunnels offline for repairs — a step city engineers have waited decades to take.&lt;/p&gt;
    &lt;p&gt;Above ground, New Yorkers will keep turning on their faucets, washing dishes, and filling glasses — rarely thinking about the billion gallons of water flowing through the underground arteries that make city life possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ny1.com/nyc/all-boroughs/news/2025/11/09/water--dep--tunnels-"/><published>2025-12-28T23:05:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415448</id><title>Slaughtering Competition Problems with Quantifier Elimination (2021)</title><updated>2025-12-29T06:56:27.762712+00:00</updated><content>&lt;doc fingerprint="747111628645eb5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Slaughtering Competition Problems with Quantifier Elimination&lt;/head&gt;&lt;head rend="h3"&gt;22 Dec 2021 - Tags: sage , featured&lt;/head&gt;&lt;p&gt;Anytime I see questions on mse that ask something “simple”, I feel a powerful urge to chime in with “a computer can do this for you!”. Obviously if you’re a researching mathematician you shouldn’t waste your time with something a computer can do for you, but when you’re still learning techniques (or, as is frequently the case on mse, solving homework problems), it’s not a particularly useful comment (so I usually abstain). The urge is particularly powerful when it comes to the contrived inequalities that show up in a lot of competition math, and today I saw a question that really made me want to say something about this! I still feel like it would be a bit inappropriate for mse, but thankfully I have a blog where I can talk about whatever I please :P So today, let’s see how to hit these problems with the proverbial nuke that is quantifier elimination!&lt;/p&gt;&lt;p&gt;I want this to be a fairly quick post, so I won’t go into too much detail. The gist is the following powerful theorem from model theory:&lt;/p&gt;&lt;p&gt;Tarski-Seidenberg Theorem1&lt;/p&gt;&lt;p&gt;If $\varphi$ is any formula of the form&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;$p(\overline{x}) = 0$, for $p \in \mathbb{R}[\overline{x}]$&lt;/item&gt;&lt;item&gt;$p(\overline{x}) \lt 0$, for $p \in \mathbb{R}[\overline{x}]$&lt;/item&gt;&lt;item&gt;combinations of the above using $\lor$, $\land$, $\lnot$, $\to$&lt;/item&gt;&lt;item&gt;combinations of the above using $\exists$ and $\forall$&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Then $\varphi$ is equivalent to a formula without quantifiers.&lt;/p&gt;&lt;p&gt;I’m legally required to give the following example:&lt;/p&gt;&lt;p&gt;The formula $\exists x . a x^2 + bx + c = 0$ (which has a quantifier) is equivalent to the formula $b^2 - 4ac \geq 0$&lt;/p&gt;&lt;p&gt;As a more complicated example, we have&lt;/p&gt;\[\forall x . \exists y . (x &amp;gt; 0 \to ax + by + xy &amp;gt; c)\]&lt;p&gt;is equivalent to&lt;/p&gt;\[b \geq 0 \ \lor \ c + ab \lt 0\]&lt;p&gt;Of course, this means if we want to know whether the above formula really is true for some choice of $a,b,c \in \mathbb{R}$, we can just plug into this quantifier free formula and check!&lt;/p&gt;&lt;p&gt;Now, you might be wondering: “How did you find this quantifier free expression?”, and the answer is, of course, sage! Sage has interfaces with a lot of pre-existing software, and for us the relevant interface is to QEPCAD, which will actually do quantifier elimination for us!&lt;/p&gt;&lt;p&gt;To get started, you have to make sure you have qepcad installed in a way that sage can access. You’ll want to run &lt;code&gt;sage -i qepcad&lt;/code&gt; just in case
(it wasn’t installed for me).&lt;/p&gt;&lt;p&gt;Next, let’s see how we eliminated quantifiers from the “more complicted example” above!&lt;/p&gt;&lt;p&gt;Yup. It’s that easy!&lt;/p&gt;&lt;p&gt;If you’re interested in reading more about this, you should check out the documentation, but I’m also going to give a handful of examples in the next section2!&lt;/p&gt;&lt;p&gt;So then, let’s slaughter some competition problems3!&lt;/p&gt;&lt;p&gt;First, the problem that made me write this post in the first place (here is the mse link again)&lt;/p&gt;&lt;p&gt;Let $a,b \geq 0$ with $a^4 + b^4 = 17$.&lt;/p&gt;&lt;p&gt;Prove $15(a+b) \geq 17 + 14 \sqrt{2ab}$&lt;/p&gt;&lt;p&gt;This isn’t given to us as polynomials, but of course it’s easy for us to fix that by rewriting it as&lt;/p&gt;\[(15(a+b) - 17)^2 \geq 14^2 \cdot 2ab\]&lt;p&gt;then we simply ask sage4:&lt;/p&gt;&lt;p&gt;We can extend this too. The asker conjectures that $(1,2)$ and $(2,1)$ are the only choices of $(a,b)$ for which we get equality. As a bonus, we can check this:&lt;/p&gt;&lt;p&gt;So we see that these really are the only points where equality holds!&lt;/p&gt;&lt;p&gt;Let’s take another example I remember seeing recently (the original mse link is here):&lt;/p&gt;&lt;p&gt;Let $x,y,z$ be positive real numbers. Show&lt;/p&gt;&lt;p&gt;\(\left ( x + \frac{1}{x} \right ) \left ( y + \frac{1}{y} \right ) \left ( z + \frac{1}{z} \right ) \geq \left ( x + \frac{1}{y} \right ) \left ( y + \frac{1}{z} \right ) \left ( z + \frac{1}{x} \right )\)&lt;/p&gt;&lt;p&gt;Again, we cannot plug this into sage directly, because it’s not a polynomial inequality. But multiplying through by $xyz$ on both sides solves that issue.&lt;/p&gt;&lt;p&gt;and even though the asker doesn’t mention it, one thing that Steele makes very clear in the (excellent) book The Cauchy-Schwarz Masterclass is that whenever working with a new inequality, we should ask where it’s sharp.&lt;/p&gt;&lt;p&gt;So we ask sage!&lt;/p&gt;&lt;p&gt;It turns out this inequality is sharp at infinitely many points, so instead of asking for the list of all points, we ask for a geometric description of the solution set.&lt;/p&gt;&lt;p&gt;Now, this admits some simplification, since we know that $x = y$ by the second line. I’ll leave it to you to figure out exactly what set this is if you’re interested.&lt;/p&gt;&lt;p&gt;As an exercise, you should be on the lookout for places to use this tool!&lt;/p&gt;&lt;p&gt;Next time somebody is asking about some wacky inequality, or really any question about sets definable by polynomial (in)equations in $\mathbb{R}^n$, you should think about whether you can slaughter the problem without much thought by asking a computer!&lt;/p&gt;&lt;p&gt;As a more concrete exercise to show the flexibility of this method, pick your favorite theorem in euclidean geometry. Rephrase it using coordinates, then ask sage if it’s true!&lt;/p&gt;&lt;p&gt;As a very concrete exercise, can you do this with Ptolemy’s Theorem?&lt;/p&gt;&lt;p&gt;Also, if you find yourself using this, definitely come back and let me know! I would love to hear about places where this comes up in the wild!&lt;/p&gt;&lt;p&gt;Also also, if you have other (possibly surprising) uses for sage or other programs that automatically answer ceretain problems, definitely let me know! This is one of the parts of mathematical logic that I get most geeky about!&lt;/p&gt;&lt;p&gt;See you next time ^_^&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;As a cute thought exercise, you should try to provide geometric meaning to this claim. It’s telling us that if you take the solution set of polynomial inequations, then project from $\mathbb{R}^{n+m}$ down to $\mathbb{R}^n$, the resulting set is still definable by polynomial inequations!&lt;/p&gt;&lt;p&gt;This should sound somewhat miraculous, and it’s worth trying out some&lt;/p&gt;&lt;p&gt;Thankfully, by the end of the post, you’ll have all the tools you need in order to work out some examples on your own ^_^. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;In fact, there are $\sim \star \sim$ bonus quantifiers $\sim \star \sim$ built into QEPCAD! For instance, we can ask for&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;“there exist exactly 5 $x$ so that $\varphi(x)$” (and obviously there’s nothing special about $5$)&lt;/item&gt;&lt;item&gt;“there exist infinitely many $x$ so that $\varphi(x)$”&lt;/item&gt;&lt;item&gt;“the set of $x$ so that $\varphi(x)$ is a connected set”&lt;/item&gt;&lt;/list&gt;&lt;p&gt;and while these aren’t going to be useful for the purposes of this post, I still wanted to mention them! ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I know that I’m currently treating this like a kind of party trick, but being able to ask a computer whether an implication between polynomial inequalities is true (and being able to find a counterexample if it isn’t) is super useful in practice! In fact, André Platzer at CMU crucially uses this machinery in order to automatically prove that robots will not bump into each other (etc.). See, for instance, his book Logical Foundations of Cyber-Physical Systems, as well as the accompanying lectures on youtube. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;For some reason typing this out wasn’t working for me, but using the constructors directly got things going… There’s probably something to do with the parsing that I don’t understand, and this isn’t too much of a hassle! ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://grossack.site/2021/12/22/qe-competition.html"/><published>2025-12-28T23:10:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415458</id><title>Spherical Cow</title><updated>2025-12-29T06:56:27.687454+00:00</updated><content/><link href="https://lib.rs/crates/spherical-cow"/><published>2025-12-28T23:11:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415522</id><title>How to complain (2024)</title><updated>2025-12-29T06:56:27.560631+00:00</updated><content>&lt;doc fingerprint="676bdb193ae38898"&gt;
  &lt;main&gt;
    &lt;quote&gt;Foo is bad, and bar is better; here is why ...&lt;/quote&gt;
    &lt;p&gt;Or, at least, be very careful about writing such things.&lt;/p&gt;
    &lt;p&gt;Why? Because inevitably, somebody will respond: ‘wait, I was confused, but I think I’ve figured it out: what you’re calling a “bar” I know as a “frobnicated foo”’.&lt;/p&gt;
    &lt;p&gt;A frobnicated foo is obviously a type of foo. So writing things like that alienates a core part of your audience: the people who have strong opinions on frobnicated foos and thing they’re great. That is, the people who agree with you. But they will be put off when they read that foos are bad, and it will be difficult to win them back.&lt;/p&gt;
    &lt;quote&gt;Often, when people try to solve a problem, they employ a particular approach. This approach is prone to problems; here is why an alternate approach does not run into those problems.&lt;/quote&gt;
    &lt;p&gt;Names are difficult, and people frequently disagree on their meanings. Replace pesky names with descriptions.&lt;/p&gt;
    &lt;p&gt;And absolute statements (‘foo is always better than bar’) are quite strong, and require an equally strong defense. It’s not necessary to explicitly state an absolute, even if you think it holds. ‘It might be better to use bar than foo sometimes’ is easier to defend than ‘bar is better than foo’, and it’s usually more true.&lt;/p&gt;
    &lt;p&gt;If somebody already knows what a foo is, isn’t it redundant, even patronising, to make them read a description of the problems foo solves and how it solves them? It can be, but it doesn’t have to be. The purpose of a description in this case isn’t just to be a definition-in-place-of-a-name. It’s to frame the problem, in a way that sets up the rest of your argument, and helps people avoid preconceptions they may have about related names. Your argument should be made in reference to your specific framing of the problem, not just in reference to the things that you expect people to know about foos.&lt;/p&gt;
    &lt;p&gt;Providing context to your argument also means that it can be read and understood by more people, making it more accessible.&lt;/p&gt;
    &lt;p&gt;Simply spewing negativity into the void is not a good enough reason to publish a complaint. There is enough negativity in the world as is. A complaint should have a good reason for existing. In particular, if that reason is to convince people that they should agree with you, then an overly acerbic tone may be unhelpful. And empathy always helps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://outerproduct.net/trivial/2024-03-25_complain.html"/><published>2025-12-28T23:23:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415570</id><title>Fast CVVDP implementation in C</title><updated>2025-12-29T06:56:27.098537+00:00</updated><content>&lt;doc fingerprint="7636b6ae2b57b990"&gt;
  &lt;main&gt;
    &lt;p&gt;A fast C implementation of the CVVDP metric (arXiv) from the University of Cambridge. More information about how CVVDP works according to this implementation is provided here.&lt;/p&gt;
    &lt;p&gt;Benchmarked using &lt;code&gt;poop&lt;/code&gt; on Linux, Core i7
13700k. Note that fcvvdp runs with one CPU thread here while cvvdp uses multiple
threads. This is a current limitation of fcvvdp, which does not yet support
multithreading.&lt;/p&gt;
    &lt;code&gt;poop "cvvdp -r fm360p.y4m -t fm360p_x264.y4m --display standard_fhd" "./fcvvdp -m fhd fm360p.y4m fm360p_x264.y4m"
Benchmark 1 (3 runs): cvvdp -r fm360p.y4m -t fm360p_x264.y4m --display standard_fhd
  measurement          mean ± σ            min … max           outliers         delta
  wall_time          19.6s  ±  568ms    19.2s  … 20.2s           0 ( 0%)        0%
  peak_rss           1.00GB ± 28.1MB     979MB … 1.03GB          0 ( 0%)        0%
  cpu_cycles          747G  ± 8.54G      741G  …  757G           0 ( 0%)        0%
  instructions        362G  ± 1.20G      361G  …  363G           0 ( 0%)        0%
  cache_references   2.77G  ± 46.9M     2.71G  … 2.81G           0 ( 0%)        0%
  cache_misses        899M  ± 11.7M      890M  …  912M           0 ( 0%)        0%
  branch_misses       107M  ± 1.80M      105M  …  109M           0 ( 0%)        0%
Benchmark 2 (3 runs): ./fcvvdp -m fhd fm360p.y4m fm360p_x264.y4m
  measurement          mean ± σ            min … max           outliers         delta
  wall_time          16.1s  ± 56.2ms    16.0s  … 16.1s           0 ( 0%)        ⚡- 17.9% ±  4.7%
  peak_rss           86.7MB ±  109KB    86.6MB … 86.8MB          0 ( 0%)        ⚡- 91.4% ±  4.5%
  cpu_cycles         82.8G  ± 80.9M     82.8G  … 82.9G           0 ( 0%)        ⚡- 88.9% ±  1.8%
  instructions        255G  ± 30.0M      255G  …  255G           0 ( 0%)        ⚡- 29.6% ±  0.5%
  cache_references   1.49G  ± 6.43M     1.49G  … 1.50G           0 ( 0%)        ⚡- 46.1% ±  2.7%
  cache_misses        369M  ± 2.84M      365M  …  371M           0 ( 0%)        ⚡- 59.0% ±  2.2%
  branch_misses      8.50M  ± 62.3K     8.45M  … 8.57M           0 ( 0%)        ⚡- 92.1% ±  2.7%
&lt;/code&gt;
    &lt;p&gt;fcvvdp uses 91% less RAM, 88% fewer CPU cycles, and is almost 18% faster in terms of wall clock time. In terms of user time, fcvvdp is ~15x more efficient.&lt;/p&gt;
    &lt;p&gt;Compilation requires:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ensure all dependencies are installed&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;zig build --release=fast&lt;/code&gt;(add&lt;code&gt;-Dflto=true&lt;/code&gt;for FLTO)&lt;/item&gt;
      &lt;item&gt;Your &lt;code&gt;fcvvdp&lt;/code&gt;binary will be in&lt;code&gt;zig-out/bin/&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fcvvdp by Halide Compression, LLC | [version]

usage: fcvvdp [options] &amp;lt;reference.(png|y4m)&amp;gt; &amp;lt;distorted.(png|y4m)&amp;gt;

compare two images/videos using the CVVDP perceptual quality metric

options:
  -m, --model &amp;lt;name&amp;gt;
      display model to use (fhd, 4k, hdr_pq, hdr_hlg, hdr_linear,
      hdr_dark, hdr_zoom); default: fhd
  -v, --verbose
      show verbose output with display parameters
  -j, --json
      output result as JSON
  -h, --help
      show this help message&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ensure all dependencies are installed&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;zig build --release=fast&lt;/code&gt;(add&lt;code&gt;-Dflto=true&lt;/code&gt;for FLTO)&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;libcvvdp&lt;/code&gt;library will be in&lt;code&gt;zig-out/lib/&lt;/code&gt;, alongside&lt;code&gt;cvvdp.h&lt;/code&gt;in&lt;code&gt;zig-out/include/&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Library usage is clearly defined in &lt;code&gt;cvvdp.h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;fcvvdp&lt;/code&gt; is under the Apache 2.0 License. &lt;code&gt;fcvvdp&lt;/code&gt; is developed by
Halide Compression.&lt;/p&gt;
    &lt;p&gt;Special thanks to Vship, from which this implementation was derived. Vship is under the MIT license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/halidecx/fcvvdp"/><published>2025-12-28T23:30:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46415819</id><title>Show HN: My app just won best iOS Japanese learning tool of 2025 award (blog)</title><updated>2025-12-29T06:56:26.789504+00:00</updated><content>&lt;doc fingerprint="2c2508572948f6d5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Best Japanese Learning Tools 2025 Award Show 🏆&lt;/head&gt;&lt;p&gt;Welcome to the award show everyone! Hosted by your favourite bee... Bee! 🥳&lt;/p&gt;&lt;p&gt;I wanted to summarise the best tools etc out there in 2025, and what better way then to put on a fake award show!&lt;/p&gt;&lt;p&gt;And like all true award shows and Christmas themed events, let's get into the spirit of giving.&lt;/p&gt;&lt;head rend="h1"&gt;Best Overall&lt;/head&gt;&lt;p&gt;This category features 3 tools.&lt;/p&gt;&lt;p&gt;If I could only pick 3 to learn Japanese with, it would be these 3.&lt;/p&gt;&lt;p&gt;The best overall winner of the 2025 Japanese Learning Awards is....&lt;/p&gt;&lt;head rend="h2"&gt;🏆Yomitan&lt;/head&gt;&lt;p&gt;Yomitan is the go-to dictionary application.&lt;/p&gt;&lt;p&gt;It works in all browsers (Chrome, Firefox, Edge) and even on mobile browsers.&lt;/p&gt;&lt;p&gt;You install it easily and just select your language and some dictionaries&lt;/p&gt;&lt;p&gt;It supports:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Many dictionaries across many languages&lt;/item&gt;&lt;item&gt;Anki&lt;/item&gt;&lt;item&gt;Native audio&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Even if you don't use any of the fancy features, having a dictionary you can use at the click of a button is useful.&lt;/p&gt;&lt;head rend="h2"&gt;Anki&lt;/head&gt;&lt;p&gt;If you use Yomitan, you must also use Anki too.&lt;/p&gt;&lt;p&gt;Anki is the premier flashcard software.&lt;/p&gt;&lt;p&gt;You see a word you don't know, and create a flashcard for it in Anki.&lt;/p&gt;&lt;p&gt;Anki solves the issue of forgetting, mostly. You will still forget things, but significantly less.&lt;/p&gt;&lt;head rend="h2"&gt;GSM&lt;/head&gt;&lt;p&gt;https://github.com/bpwhelan/GameSentenceMiner&lt;/p&gt;&lt;p&gt;Game Sentence Miner (GSM) is an all-in-one toolkit to turn any visual media into Anki flashcards.&lt;/p&gt;&lt;p&gt;Use the overlay to directly look words up (using Yomitan) in your game, anime, or manga without needing to go to another website to look it up.&lt;/p&gt;&lt;p&gt;Create flashcards in one click with the real audio used, and a gif of what happened on screen.&lt;/p&gt;&lt;p&gt;Analyse your statistics to help you learn to read better, over 30+ graphs and extensive goal planning.&lt;/p&gt;&lt;p&gt;Best of all? It's 100% free, works offline, and works for many other languages – not just Japanese!&lt;/p&gt;&lt;p&gt;GSM can take in text from anywhere, create statistics based on it and enhance your Anki cards with gifs + audio along with an overlay dictionary.&lt;/p&gt;&lt;p&gt;For this reason you'll see it come up a lot... It's a well loved tool, and for good reason!&lt;/p&gt;&lt;p&gt;GSM's main problem is the barrier to entry can be high, it's got a lot of features and many settings. Thankfully the author has created many, many blog posts and YouTube videos on how to use it.&lt;/p&gt;&lt;head rend="h1"&gt;Best Phone Apps&lt;/head&gt;&lt;head rend="h2"&gt;🏆Renshuu - Overall Winner&lt;/head&gt;&lt;p&gt;Renshuu wins the best app of 2025!&lt;/p&gt;&lt;p&gt;It's like Duolingo but better in every way.&lt;/p&gt;&lt;p&gt;It can work out your level and adjust the difficulty of words or sentences&lt;/p&gt;&lt;p&gt;It gives you varied practice. Writing kanji, flipping flashcards, and fun games.&lt;/p&gt;&lt;p&gt;If you're looking for an easy app to replace the Green Owl™️ but actually be somewhat effective, this is it!&lt;/p&gt;&lt;head rend="h3"&gt;Best Android Apps&lt;/head&gt;&lt;p&gt;Let's split this up into two, IOS and Android.&lt;/p&gt;&lt;head rend="h2"&gt;🏆Jidoujisho - Overall Android Winner&lt;/head&gt;&lt;p&gt;This is an everything-in-one kinda app.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Supports Ankidroid&lt;/item&gt;&lt;item&gt;Dictionary lookups similar to Yomitan&lt;/item&gt;&lt;item&gt;Watch videos or listen to audio, and make flashcards from them&lt;/item&gt;&lt;item&gt;Read books and make flashcards from them&lt;/item&gt;&lt;item&gt;Read Manga!&lt;/item&gt;&lt;item&gt;Play video games, visual novels etc.&lt;/item&gt;&lt;item&gt;Instantly look up the lyrics of the song you're listening to, and make flashcards&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If you do not have access to a computer, this is perhaps the best app to do everything on Android.&lt;/p&gt;&lt;p&gt;But! It does require some time to setup and learn how it all works.&lt;/p&gt;&lt;head rend="h4"&gt;Poe&lt;/head&gt;&lt;p&gt;Poe is Yomitan for Android&lt;/p&gt;&lt;p&gt;It supports Anki, native audio and pitch accent.&lt;/p&gt;&lt;p&gt;I wrote more about this here:&lt;/p&gt;&lt;head rend="h3"&gt;iOS&lt;/head&gt;&lt;p&gt;Now let's look at the options on IOS, albeit limited options.&lt;/p&gt;&lt;head rend="h2"&gt;🏆Manabi Reader - Overall IOS Winner&lt;/head&gt;&lt;p&gt;Manabi Reader is a way to read on IOS, similar to Jidoujisho but with less features. Not their fault, mostly IOS has a lot of walls.&lt;/p&gt;&lt;p&gt;You can look words up in dictionaries and send things to Anki.&lt;/p&gt;&lt;p&gt;See breakdown of sentences, how many words in a sentence do you know?&lt;/p&gt;&lt;p&gt;You can read books and webpages and get full comprehension statistics about that page.&lt;/p&gt;&lt;p&gt;You can also look up words using OCR or by pasting the text.&lt;/p&gt;&lt;p&gt;The author is working on a bunch of new features as they told me:&lt;/p&gt;&lt;p&gt;Here are some exclusive behind the scenes screenshots of the new Manabi Reader, coming soon!&lt;/p&gt;&lt;head rend="h4"&gt;Shiori Reader&lt;/head&gt;&lt;p&gt;This is another "look things up and make anki cards" app, but this time it focusses on reading books.&lt;/p&gt;&lt;head rend="h1"&gt;Best Anki Decks&lt;/head&gt;&lt;p&gt;Since we've talked so much about Anki, one of the big questions people have who begun using it is "what decks do I use?"&lt;/p&gt;&lt;head rend="h2"&gt;🏆Kaishi - Overall Best Anki Deck&lt;/head&gt;&lt;p&gt;This is the definitive Anki deck for people just getting into learning Japanese with Anki.&lt;/p&gt;&lt;p&gt;The idea is that this teaches the most common words found in media, not necessarily the words you'll come across ordering food in Japan.&lt;/p&gt;&lt;p&gt;Once you finish this deck you then know enough Japanese to read books / immerse. You will still struggle, but it won't be as bad as starting from 0.&lt;/p&gt;&lt;head rend="h2"&gt;Japanese Proper Nouns&lt;/head&gt;&lt;p&gt;Do you have problems reading city names? What about names of people?&lt;/p&gt;&lt;p&gt;The proper nouns Anki deck is designed to teach you all the important proper nouns you'll encounter, and then pretty much every proper noun ever.&lt;/p&gt;&lt;head rend="h1"&gt;Best Anki Notetypes&lt;/head&gt;&lt;quote&gt;"okay bee, I finished Kaishi. I want to use Yomitan to make my own Anki deck but it wants a note type... what do I use?"&lt;/quote&gt;&lt;p&gt;I hear you say! probably....&lt;/p&gt;&lt;head rend="h2"&gt;🏆Kiku&lt;/head&gt;&lt;p&gt;Kiku came out swinging towards the end of 2025 as the go to Anki note type.&lt;/p&gt;&lt;p&gt;Other note types were static, but Kiku harnessed the power of Javascript in Anki.&lt;/p&gt;&lt;p&gt;View similar Kanji, and view other flashcards that you made that use that kanji!&lt;/p&gt;&lt;p&gt;Sometimes you come across a word used in a really nice context, but you already have a flashcard for it!&lt;/p&gt;&lt;p&gt;You want to make another flashcard because you love this context, but it's just not possible without duplicating them or deleting your old card 🫠&lt;/p&gt;&lt;p&gt;Kiku solves this by allowing you to have multiple contexts in one card.&lt;/p&gt;&lt;p&gt;Most Anki card themes come in either light mode or dark mode.&lt;/p&gt;&lt;p&gt;Kiku has over 35 themes.&lt;/p&gt;&lt;p&gt;Kiku also has a settings page and a plugins system to really customise it for yourself.&lt;/p&gt;&lt;p&gt;Here's a bullet pointed list of my favourite features:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Fade out the front of the card after 3 seconds, encouraging you to answer faster.&lt;/item&gt;&lt;item&gt;Blur images which are tagged NSFW&lt;/item&gt;&lt;item&gt;Only blur them between 9 - 5pm workdays... In case you want to see said images when you're at home :)&lt;/item&gt;&lt;item&gt;Display extra fields, such as SentenceTranslation.&lt;/item&gt;&lt;item&gt;Randomise the font, so you learn the word in any font not just the main one you use.&lt;/item&gt;&lt;item&gt;Add external links to your cards to easily see the card in Jisho, Nadeshiko etc.&lt;/item&gt;&lt;item&gt;Hover over Kanji in your cards and see it broken down.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Lapis&lt;/head&gt;&lt;p&gt;Lapis is made by the same person who made Kaishi.&lt;/p&gt;&lt;p&gt;It's very similar to Kiku but without all the fancy features (Kiku is based on Lapis).&lt;/p&gt;&lt;p&gt;If you want a less Javascript heavy card, this is great!&lt;/p&gt;&lt;head rend="h2"&gt;Best Anki Addons&lt;/head&gt;&lt;p&gt;Now you use Anki, another common question people have is:&lt;/p&gt;&lt;quote&gt;What Anki addons can I use to maximise it?&lt;/quote&gt;&lt;head rend="h2"&gt;🏆Priority reorder&lt;/head&gt;&lt;p&gt;When you make Anki cards, they kinda go into a semi random order.&lt;/p&gt;&lt;p&gt;Not every word in Japanese is equally important.&lt;/p&gt;&lt;p&gt;Migaku, who ran an analysis on Netflix found these statistics.&lt;/p&gt;&lt;p&gt;If you select a word at random, there is a 10% chance that word is one of these three:&lt;/p&gt;&lt;p&gt;50% chance it will be one of 45 words:&lt;/p&gt;&lt;p&gt;Words are repeated, often. Just learning the top 1500 words or so means you can understand 80% of all words in a show.&lt;/p&gt;&lt;p&gt;Therefore it makes sense to learn your Anki cards in the order of most frequent first.&lt;/p&gt;&lt;p&gt;Priority Reorder does this.&lt;/p&gt;&lt;p&gt;But not all media is equal. One Piece has a lot of pirate talk, but you won't find that in other media.&lt;/p&gt;&lt;p&gt;Wouldn't it be cool to learn the most frequent words in One Piece if your goal is to watch it?&lt;/p&gt;&lt;p&gt;Priority reorder does that.&lt;/p&gt;&lt;p&gt;Finally, you have a short term memory. Flashcards you made today will stick better than flashcards made 50 days ago.&lt;/p&gt;&lt;p&gt;Wouldn't it be cool to also prioritise recently made flashcards that appear frequently in One Piece?&lt;/p&gt;&lt;p&gt;Priority Reorder does this!&lt;/p&gt;&lt;p&gt;Wouldn't it be cool to mine words that have a high frequency?&lt;/p&gt;&lt;head rend="h2"&gt;Kanji Grid&lt;/head&gt;&lt;p&gt;Looking to take the JLPT or similar and wondering "god, do I really know all the kanji in that exam?"&lt;/p&gt;&lt;p&gt;Or wanting to just see how you progress in terms of Kanji?&lt;/p&gt;&lt;p&gt;The Kanji Grid addon is for you!&lt;/p&gt;&lt;p&gt;https://ankiweb.net/shared/info/1610304449&lt;/p&gt;&lt;head rend="h2"&gt;Local Audio Server&lt;/head&gt;&lt;p&gt;This is an addon that works with Yomitan or similar tools.&lt;/p&gt;&lt;p&gt;It lets you listen to native audio in Yomitan, and even add that to your Anki cards.&lt;/p&gt;&lt;p&gt;It takes a bit to set up, but once you do you don't have to mess with it. You can now have native audio on all of your Anki cards!&lt;/p&gt;&lt;head rend="h1"&gt;Best paid solution&lt;/head&gt;&lt;quote&gt;This is all too much setup! I wish there was some sort of company I could pay to do this all for me&lt;/quote&gt;&lt;p&gt;Not to worry, there is!&lt;/p&gt;&lt;head rend="h2"&gt;🏆Migaku&lt;/head&gt;&lt;p&gt;Migaku is an all-in-one solution.&lt;/p&gt;&lt;p&gt;They aim to do everything mentioned here already, albeit imperfectly and for a price.&lt;/p&gt;&lt;p&gt;They have courses which teach you the top 1500 words, Kanji and grammar designed to help you immerse as soon as possible similar to Kaishi.&lt;/p&gt;&lt;p&gt;They have their own SRS alternative to Anki, so you don't need addons etc to make anything work.&lt;/p&gt;&lt;p&gt;You can watch Netflix and look up all the words you want. They'll even highlight good words you should make flashcards out of.&lt;/p&gt;&lt;p&gt;They can tell you how much of a specific video you know in terms of words, what is your expected comprehension of it:&lt;/p&gt;&lt;p&gt;You can:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Read books&lt;/item&gt;&lt;item&gt;Watch videos locally&lt;/item&gt;&lt;item&gt;Study Netflix / YouTube videos&lt;/item&gt;&lt;item&gt;Generate subtitles if none exist&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If you are looking for an alright solution to learning Japanese and you don't mind spending money, in my opinion this is it.&lt;/p&gt;&lt;p&gt;For me personally, messing with tools is one of my little joys so I don't mind it.&lt;/p&gt;&lt;head rend="h1"&gt;Best for Games:&lt;/head&gt;&lt;p&gt;Japanese games are the greatest, let's look at options to learn Japanese from them.&lt;/p&gt;&lt;p&gt;The only real option is to use OCR, which is a fancy word to mean "the computer will read the text on the screen and give you the sentence so you can copy it / look it up".&lt;/p&gt;&lt;head rend="h2"&gt;🏆Game Sentence Miner - Winner of Best For Games&lt;/head&gt;&lt;p&gt;After winning overall earlier, it does make sense that game sentence miner is the best for games.&lt;/p&gt;&lt;p&gt;Once you setup OCR, you can then setup the overlay to be able to look words up directly in the game.&lt;/p&gt;&lt;p&gt;It takes around 1 second to go from "text appearing on screen" to "being able to look up the text".&lt;/p&gt;&lt;p&gt;If you have a GPU it could be even less time, around 0.5 seconds or so.&lt;/p&gt;&lt;p&gt;https://github.com/AuroraWright/owocr&lt;/p&gt;&lt;p&gt;You can then click the plus icon to make a flashcard, and GSM will make it all in the background. You don't have to constantly switch between enjoying a game and making flashcards.&lt;/p&gt;&lt;head rend="h2"&gt;Meikipop&lt;/head&gt;&lt;p&gt;This is a really fast OCR that works anywhere on Windows, Linux or Mac.&lt;/p&gt;&lt;p&gt;It's super simple to setup and use, and it works similar to GSM's "hover over the word to see the meaning"&lt;/p&gt;&lt;p&gt;The only downside is that you can't mine to Anki with it, however it is extremely simple to use, fast, and works on anything on your screen (even Windows settings) so for that reason it's winning second place.&lt;/p&gt;&lt;head rend="h3"&gt;Yomininja&lt;/head&gt;&lt;p&gt;Yomininja is another tool similar to GSM.&lt;/p&gt;&lt;p&gt;It uses OCR to scan the screen and lets you look things up:&lt;/p&gt;&lt;p&gt;It's a lot simpler than GSM, but in my opinion it's not as pretty.&lt;/p&gt;&lt;p&gt;GSM doesn't highlight boxes red by default, and you can hover over the words and see the definition above them as you read it.&lt;/p&gt;&lt;p&gt;With Yomininja there's this extra box on the side you have to read.&lt;/p&gt;&lt;p&gt;Not to mention the fact that GSM lets you easily make flashcards with the audio and a gif from the game itself.&lt;/p&gt;&lt;p&gt;Still, Yomininja is extremely easy to use and a fan favourite.&lt;/p&gt;&lt;head rend="h1"&gt;Best for visual novels&lt;/head&gt;&lt;head rend="h2"&gt;🏆GSM - Best for Visual Novels&lt;/head&gt;&lt;p&gt;GSM is really, really good for visual media on a computer.&lt;/p&gt;&lt;p&gt;But when it comes to visual novels, we can use texthookers.&lt;/p&gt;&lt;p&gt;A texthooker grabs this text and gives it to you, letting you look things up without OCR.&lt;/p&gt;&lt;p&gt;I'll talk more about this next!&lt;/p&gt;&lt;p&gt;Texthookers work with the overlay just like OCR does with games.&lt;/p&gt;&lt;p&gt;Let's explore an under-rated feature in GSM, as our next tool will have this too – stats.&lt;/p&gt;&lt;p&gt;GSM has over 35 charts related to statistics about everything you read, designed to help you answer questions such as:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Do I read better in the morning or evening?&lt;/item&gt;&lt;item&gt;Do I read faster reading horror or slice of life?&lt;/item&gt;&lt;item&gt;Do I play more games or visual novels? Which one is better for me in terms of learning?&lt;/item&gt;&lt;item&gt;Am I improving?&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;JL&lt;/head&gt;&lt;p&gt;JL is an alternative Japanese dictionary program.&lt;/p&gt;&lt;p&gt;It works really, really well for visual novels.&lt;/p&gt;&lt;p&gt;I wrote extensively about it here:&lt;/p&gt;&lt;p&gt;But in short:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It's really fast&lt;/item&gt;&lt;item&gt;It has amazing features specifically for Japanese&lt;/item&gt;&lt;item&gt;It requires a texthooker and is Windows only :(&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I like how you can put the textbox over the visual novel, which lets you do something similar to GSM's overlay.&lt;/p&gt;&lt;p&gt;JL also has some stats:&lt;/p&gt;&lt;p&gt;And even more excitedly they have stats on how many times you looked up a word, something that no other dictionary app has.&lt;/p&gt;&lt;head rend="h1"&gt;Best texthookers&lt;/head&gt;&lt;p&gt;Texthookers are lil programs that "hook" into visual novels or some games, take the text on your screen and give it to you.&lt;/p&gt;&lt;p&gt;They are faster and more accurate than OCR, but sometimes awkward to use.&lt;/p&gt;&lt;head rend="h2"&gt;🏆LunaTranslator&lt;/head&gt;&lt;p&gt;By far the best texthooker out there.&lt;/p&gt;&lt;p&gt;It works on everything I try in terms of visual novels.&lt;/p&gt;&lt;p&gt;It can hook emulated devices such as PSP, PS2, and the Nintendo Switch.&lt;/p&gt;&lt;p&gt;If you find a "bad" hook (one that has a lot of junk) there's a million things you can do to make it more normal.&lt;/p&gt;&lt;p&gt;Like filtering out curly braces, filtering out non Japanese text etc.&lt;/p&gt;&lt;p&gt;If this doesn't help you, you can even write a Python file to preprocess the text!&lt;/p&gt;&lt;p&gt;On top of this, Luna supports much more than just hooking.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;OCR&lt;/item&gt;&lt;item&gt;Speech Recognition (it listens to the sound the game is making, and tries to turn those sounds into sentences)&lt;/item&gt;&lt;item&gt;You can install Yomitan and use it similarly to JL (it wont overlay like GSM does though)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;But there are some rumours that the author has stolen the code of other people and rebranded it as their own (I have not found evidence of this, please let me know in the comments if you have proof).&lt;/p&gt;&lt;p&gt;1. The license is MIT. Legally you're allowed to take the code and not credit. But, morally it's right to credit.&lt;/p&gt;&lt;p&gt;2. The work is transformative, they took code in one language and rewrote it to work with C++ for Luna.&lt;/p&gt;&lt;p&gt;3. Agent (talked about next) also has its own drama. It's not open source, and it encrypts the Nintendo Switch hooks to prevent competition from getting them.&lt;/p&gt;&lt;p&gt;All in all, it's really confusing and messy. Decide for yourself.&lt;/p&gt;&lt;p&gt;Also, when writing code people often tell you what has changed since the last release.&lt;/p&gt;&lt;p&gt;The Luna author does not do this, it's kind of confusing to figure out what's been added or removed.&lt;/p&gt;&lt;p&gt;This led to people not trusting them.&lt;/p&gt;&lt;p&gt;On top of this, it supports a lot. Like translation, yomitan, Japanese parsing etc.&lt;/p&gt;&lt;p&gt;This led to someone forking the code and creating their own version, removing all of this and keeping just the hooking part.&lt;/p&gt;&lt;head rend="h2"&gt;Agent&lt;/head&gt;&lt;p&gt;Agent is a much simpler texthooking program.&lt;/p&gt;&lt;p&gt;It has a database of hooks, and you click on the game or visual novel you want to play.&lt;/p&gt;&lt;p&gt;You then have a perfect hook, it's not dirty and works first time without much issue.&lt;/p&gt;&lt;p&gt;The problem is that it doesn't have hooks for all games and visual novels, yet.&lt;/p&gt;&lt;head rend="h2"&gt;Chen's Textractor&lt;/head&gt;&lt;p&gt;Chen's textractor works similarly to Luna and Agent, closer to Luna in the sense that you find the hook yourself.&lt;/p&gt;&lt;p&gt;It's most similar to Textractor (it's a fork) which is an older texthooking program, so many people love this as its similar to what they already use and love.&lt;/p&gt;&lt;head rend="h2"&gt;Texthooking Pages&lt;/head&gt;&lt;p&gt;Let's say you want to use Yomitan to mine from games / visual novels without GSM / JL or Yomininja etc.&lt;/p&gt;&lt;p&gt;Yomitan is browser only.&lt;/p&gt;&lt;p&gt;You need to take the text from OCR / Texthooking and place it onto a webpage to look up words.&lt;/p&gt;&lt;p&gt;There's some opinions about these, so let's list the most popular ones.&lt;/p&gt;&lt;head rend="h2"&gt;🏆Kizuna&lt;/head&gt;&lt;p&gt;Kizuna is a texthooking paged based on another one by Renji.&lt;/p&gt;&lt;p&gt;Everytime your texthooker receives a line of text, it sends it to this page.&lt;/p&gt;&lt;p&gt;Here you can use Yomitan to look up words.&lt;/p&gt;&lt;p&gt;Kizuna is special because it's a social texthooking page.&lt;/p&gt;&lt;p&gt;It records characters read and time spent per visual novel:&lt;/p&gt;&lt;p&gt;And you can create "rooms" with your friends to compare your stats together and motivate each other:&lt;/p&gt;&lt;p&gt;It's three main downsides are that it is focused primarily on Japanese visual novels, it's not open source and it doesn't let you export your data.&lt;/p&gt;&lt;head rend="h2"&gt;Renji's Texthooking Page&lt;/head&gt;&lt;p&gt;Renji's page inspired Kizuna's.&lt;/p&gt;&lt;p&gt;It's a simple texthooking page that can be downloaded and ran entirely locally, with its source code published online.&lt;/p&gt;&lt;p&gt;It looks pretty much exactly the same:&lt;/p&gt;&lt;p&gt;Many people use Renji's because it's lightweight and has many settings to allow you to configure things.&lt;/p&gt;&lt;p&gt;Maybe too many settings.&lt;/p&gt;&lt;p&gt;Because Renji's texthooker is open source it is often bundled into other software like Game Sentence Miner which has added a few specific features.&lt;/p&gt;&lt;p&gt;While Kizuna has stats, Renji's does not other than characters read and time spent.&lt;/p&gt;&lt;p&gt;The author, Renji, actually suggests people use GameSentenceMiner for stats with their texthooker:&lt;/p&gt;&lt;head rend="h2"&gt;ExStatic&lt;/head&gt;&lt;p&gt;You may have noticed in the previous paragraph someone mentioned ExStatic.&lt;/p&gt;&lt;p&gt;This is another texthooking page but with a lot more stats.&lt;/p&gt;&lt;p&gt;But in terms of functionality of the page itself, it's lacking somethings that Renji's has.&lt;/p&gt;&lt;p&gt;Still, many people use ExStatic as an easy way to get beautiful stats. It is open source and can be run entirely locally, without an internet connection.&lt;/p&gt;&lt;p&gt;Other stat apps form opinions, like what is Japanese? Do English letters count? Like does the T in "Ｔシャツ" count?&lt;/p&gt;&lt;p&gt;By giving you raw data, you can decide for yourself what counts and change the stats at any time.&lt;/p&gt;&lt;p&gt;Only ExStatic, Renji's and GSM support storing raw lines of data.&lt;/p&gt;&lt;head rend="h1"&gt;best for manga&lt;/head&gt;&lt;head rend="h2"&gt;🏆Mokuro + Mokuro Reader - Joint 1st&lt;/head&gt;&lt;p&gt;Mokuro is a file format for manga. It's basically a HTML overlay over a bunch of Manga images that let you look up the words in that manga panel using Yomitan.&lt;/p&gt;&lt;p&gt;It does this by OCRing the manga to generate this.&lt;/p&gt;&lt;p&gt;Mokuro Reader is the app that lets you read these files:&lt;/p&gt;&lt;p&gt;The main problem with this is finding manga.&lt;/p&gt;&lt;p&gt;You have to find a way to buy the manga, get the raw images, and process it. There are less than legal ways to do this, but I won't talk about that here.&lt;/p&gt;&lt;p&gt;You can store all of your manga in the cloud along with progress etc and easily read from any device (you can read from any device, and use Yomitan on Android if you want to mine):&lt;/p&gt;&lt;p&gt;When you read manga, since it is in the browser, you can use Yomitan to look things up:&lt;/p&gt;&lt;p&gt;For reading it has some cool features, like a night mode to block out blue light to make night reading easier:&lt;/p&gt;&lt;p&gt;And a million other minor settings to alter how you read.&lt;/p&gt;&lt;p&gt;Of course it also has Anki support. Make a word card using Yomitan and then&lt;/p&gt;&lt;p&gt;Double click the screen to grab an image.&lt;/p&gt;&lt;p&gt;There's even stats!&lt;/p&gt;&lt;p&gt;In the manga itself you can see how long it'd take to read your current volume:&lt;/p&gt;&lt;p&gt;The stats are visually appealing&lt;/p&gt;&lt;p&gt;With cool per volume / series data:&lt;/p&gt;&lt;p&gt;My main complaints are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The stats are precomputed, it does not store your raw data. This means if you change your mind about something (like I don't want to count English letters, I don't want to count repeated things like aaaaaa, or I want to set my AFK timer to be lower) it's impossible to change.&lt;/item&gt;&lt;item&gt;You have to use Mokuro'd manga, which is very hard to find and painful to convert if you don't have a powerful computer.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;🏆MangaTan - Joint 1st&lt;/head&gt;&lt;p&gt;MangaTan was born out of anger.&lt;/p&gt;&lt;p&gt;Anger at Mokuro files being terrible to create.&lt;/p&gt;&lt;p&gt;Anger at current manga websites shoving 10 ads / second down your throat.&lt;/p&gt;&lt;p&gt;Hear it from the creator:&lt;/p&gt;&lt;quote&gt;We can just drag and drop light novels into ttsu, hook into visual novels with ease, or load up anime with subtitles in ASB or Memento.&lt;lb/&gt;But manga? Manga has always been the exception.&lt;lb/&gt;The process often meant needing a high-end GPU just to run tools like Mokuro, and then waiting for it to finish&lt;lb/&gt;And don't get me started with those terrible websites that use the worst hosts in existence.&lt;lb/&gt;Who doesn't miss the old days of simply reading manga in bed? Before we were hardcore weebs, we didn't have to deal with our tenth ad on a limited 100kbit-speed-limited hoster while using Tachiyomi. But you can cast aside that&lt;/quote&gt;&lt;p&gt;If it was Mokuro (the converter) vs Mangatan, Mangatan would win. I once spent 27 hours converting a single volume of manga into Mokuro.... and the OCR sucked. A lot.&lt;/p&gt;&lt;p&gt;Mangatan aims to get rid of all the painpoints of Mokuro:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You don't have to spend 20+ hours converting manga to mokuro files&lt;/item&gt;&lt;item&gt;Or searching for Mokuro files across the web&lt;/item&gt;&lt;item&gt;It works with over 1200+ sites, the second a new volume of manga is released you can read it 🥳&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The setup requires some computer skills but once its done, it works alright.&lt;/p&gt;&lt;code&gt;.exe&lt;/code&gt;, which would make Mangatan extremely easy to use.&lt;p&gt;Update: After I joined their Discord thread and talked about this, there is now a one-click&lt;/p&gt;&lt;code&gt;.exe&lt;/code&gt; you can use!&lt;p&gt;Mangatan is my favourite. I don't want to update this, as it happened after my imaginary award show. But future readers: use Mangatan.&lt;/p&gt;&lt;p&gt;Use this fork https://github.com/KolbyML/Mangatan&lt;/p&gt;&lt;p&gt;Every time you load a page it will OCR it and let you look things up.&lt;/p&gt;&lt;p&gt;You can even crop images to send to Anki:&lt;/p&gt;&lt;p&gt;It's very lightweight once its running. All it does is OCR the manga and crop images for anki cards.&lt;/p&gt;&lt;p&gt;From the author themselves:&lt;/p&gt;&lt;p&gt;There's also a bunch of settings, for example you can use your own OCR server if you want:&lt;/p&gt;&lt;p&gt;I like setting the colour to purple and font colour to black, as it makes it look nicer to me:&lt;/p&gt;&lt;p&gt;There's no stats, but in this case it's a good thing. Mangatan is extremely lightweight.&lt;/p&gt;&lt;p&gt;It also works on Android devices.&lt;/p&gt;&lt;head rend="h2"&gt;GSM&lt;/head&gt;&lt;p&gt;You can also use GSM's OCR to read manga if you so wish.&lt;/p&gt;&lt;p&gt;Set up the OCR, and then open the overlay and you can create flashcards etc similar to how Mangatan does it.&lt;/p&gt;&lt;p&gt;GSM even has stats for manga:&lt;/p&gt;&lt;p&gt;Because GSM stores the raw data, you can also use it to calculate how many times you've seen a kanji, or search to see the first time you encountered a word or kanji. You can do anything you want.&lt;/p&gt;&lt;p&gt;But the downsides are that GSM isn't made for manga.&lt;/p&gt;&lt;p&gt;When it takes a screenshot, it can't crop the manga like Mokuro does (due to it being built for games / full screen things).&lt;/p&gt;&lt;p&gt;Look at this Anki card:&lt;/p&gt;&lt;p&gt;The image has black bars around it, whereas if it was cropped it would not.&lt;/p&gt;&lt;p&gt;If you already have a good GSM setup, it makes sense to use this. But if you want a really good manga setup, maybe this isn't so good.&lt;/p&gt;&lt;p&gt;It's also not as automatic as Mangatan, if you want nice stats you need to tell GSM you're reading something new.&lt;/p&gt;&lt;p&gt;Mangatan can only use Suwayomi whereas GSM can OCR any type of manga. But, Suwayomi has all the manga already so it's not that much of an improvement.&lt;/p&gt;&lt;p&gt;Also, GSM does not work on Android unlike Mokuro Reader and Mangatan.&lt;/p&gt;&lt;head rend="h1"&gt;Best Video Players&lt;/head&gt;&lt;p&gt;Most people get into learning Japanese because of anime, so now let's talk about the best video players out there!&lt;/p&gt;&lt;head rend="h2"&gt;🏆Migaku&lt;/head&gt;&lt;p&gt;Despite being a paid for product, I believe Migaku offers the best service for watching videos.&lt;/p&gt;&lt;p&gt;Firstly let's look at this.&lt;/p&gt;&lt;p&gt;Migaku shows you an estimated comprehension score for the video you want to watch. It uses the frequency of the words and your known words to calculate this.&lt;/p&gt;&lt;p&gt;It's not as simple as "you know these words, you don't know these" – it uses an algorithm to work out the average frequency of words you do and don't know and uses that to calculate how hard a media is.&lt;/p&gt;&lt;p&gt;For example if the words you don't know are very high frequency, it will be a lower difficulty than a video with a bunch of words you don't know.&lt;/p&gt;&lt;p&gt;If the show doesn't have subtitles, you can also generate them using the top bar.&lt;/p&gt;&lt;p&gt;Migaku has about a million different presets for you to watch videos, or you can make your own:&lt;/p&gt;&lt;p&gt;As well as keyboard shortcuts so you don't even have to use a mouse:&lt;/p&gt;&lt;p&gt;If the UI is too cluttered you can just hide everything apart from this tab:&lt;/p&gt;&lt;p&gt;Migaku will highlight words in good sentences to mine with high frequency, and you can even tell it to include a lil single definition under the word to help you read the sentence:&lt;/p&gt;&lt;p&gt;Migaku works on Netflix and Disney+, but all of these screenshots use their local player. This is just a DVD I'm playing 😃&lt;/p&gt;&lt;p&gt;Its main downside is that it costs money, but I believe Migaku is easily the best video player out there in terms of features and ease of use.&lt;/p&gt;&lt;p&gt;But... It does cost money and do you really need those extra features? It's up to you.&lt;/p&gt;&lt;head rend="h2"&gt;ASB Player&lt;/head&gt;&lt;p&gt;ASB Player is the GOAT of free video players for Japanese.&lt;/p&gt;&lt;p&gt;It's offline.&lt;/p&gt;&lt;p&gt;It works with Netflix, YouTube etc.&lt;/p&gt;&lt;p&gt;It extracts subtitles from them.&lt;/p&gt;&lt;p&gt;You can mine from it just like Migaku.&lt;/p&gt;&lt;p&gt;There's really not much to it. It plays videos well, has great subtitle support and lets you mine from it.&lt;/p&gt;&lt;p&gt;Which is why it's so good and well loved, it does one thing and does it well.&lt;/p&gt;&lt;p&gt;However, compared to Migaku it is missing a few features some people may want:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Ability to generate subtitles&lt;/item&gt;&lt;item&gt;Highlighting words you should mine&lt;/item&gt;&lt;/list&gt;&lt;p&gt;But if you don't care for those features, ASB Player is great.&lt;/p&gt;&lt;p&gt;While I haven't tried this, it sounds cool. I would love gifs of anime scenes on my Anki cards 😄&lt;/p&gt;&lt;p&gt;Update 2: I tried this. The ASB Player websocket is for controlling the player, not for subtitles ☹️&lt;/p&gt;&lt;p&gt;but you can use MPV with MPV Web Socket (which does do subtitles) to get pretty gifs&lt;/p&gt;&lt;p&gt;However you need to download files locally and sync the subtitles which is a bit of a pain....&lt;/p&gt;&lt;head rend="h2"&gt;Yomine&lt;/head&gt;&lt;p&gt;Yomine is a relatively new player in the field, and not actually a video player but something that supports video players.&lt;/p&gt;&lt;p&gt;To use their words:&lt;/p&gt;&lt;quote&gt;A Japanese vocabulary mining tool designed to help language learners extract and study words from subtitle files. It integrates with ASBPlayer and MPV for timestamp navigation, ranks terms by frequency, and supports Anki integration to filter out known words.&lt;/quote&gt;&lt;p&gt;So it's kinda of like the Migaku "you should mine this word" feature, but for ASB Player and free.&lt;/p&gt;&lt;p&gt;It basically extracts all the words from a video, checks to see if you have them in Anki and sorts them by a frequency.&lt;/p&gt;&lt;p&gt;From this you can then click a button to mine that word.&lt;/p&gt;&lt;p&gt;Here are some interesting ways people use it, which may not be obvious:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;If you saw a word you want to mine, but you were too busy to mine it you can use Yomine to search your favourite anime etc to find the word and mine it later on.&lt;/item&gt;&lt;item&gt;If you want to play a video game or visual novel, you could download a Let's Play of it and generate a comprehension score or pre-mine words you need to know.&lt;list rend="ul"&gt;&lt;item&gt;This is different than downloading Anki decks from Jiten and friends, because it's specifically words you need to know.&lt;/item&gt;&lt;item&gt;Also you get to watch a Let's Play which is like double immersion points 😯&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Using GSM to generate a Long Play (basically a large &lt;code&gt;mp4&lt;/code&gt;recording of a game you're playing with a subtitles file), loading it into Yomine and mining all the words you didn't mine during that playthrough. Useful if you want to play now, mine later.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h1"&gt;Best Websites&lt;/head&gt;&lt;p&gt;Now let's look at some of the best websites out there.&lt;/p&gt;&lt;head rend="h2"&gt;🏆Jiten&lt;/head&gt;&lt;p&gt;Jiten is a website that stores thousands of Japanese media and tells you a rough difficulty for them, among other things.&lt;/p&gt;&lt;p&gt;They support all sorts of media, not just visual novels or anime.&lt;/p&gt;&lt;p&gt;In Jiten you can upload a list of vocaburary you know (syncs with Anki and other tools):&lt;/p&gt;&lt;p&gt;Jiten can then rank how many words in the media you know, and tell you a personalised coverage score.&lt;/p&gt;&lt;p&gt;Something I like to do is find visual novels with &amp;gt;80% external rating (external rating == reviews on vndb, anilist etc) which I have &amp;gt;80% coverage for (meaning I will understand most of it).&lt;/p&gt;&lt;p&gt;Once you've found a piece of media you like hit "statistics" and see a cool graph.&lt;/p&gt;&lt;p&gt;For example, in this anime to understand 95% of it you just need to know the most frequency 1944 words that appear in it.&lt;/p&gt;&lt;p&gt;To understand 99%, you need to know an extra 3200ish.&lt;/p&gt;&lt;p&gt;Learning 1944 of the words in this anime seems great, but how do we actually learn them?&lt;/p&gt;&lt;p&gt;No worries, Jiten lets you download Anki decks with the exact freq order of that show. Learn the words you need to know for the media you want to watch.&lt;/p&gt;&lt;p&gt;If doing premade Anki decks isn't your thing, download the "occurrences" dictionary to get a frequency list you can use in Yomitan to tell you how often a word appears in the show.&lt;/p&gt;&lt;p&gt;That way I can mine high frequent words in shows I want to watch, without watching them yet. Almost like I'm prepping for it.&lt;/p&gt;&lt;p&gt;Jiten also has a dictionary you can use. Search a word to see its frequency, and all the media that word is in:&lt;/p&gt;&lt;p&gt;Speaking of dictionaries, you can download global frequency lists on Jiten too:&lt;/p&gt;&lt;p&gt;And finally, Jiten has a lot of data and every single week it is improving.&lt;/p&gt;&lt;head rend="h2"&gt;Yokubi - Morg Grammar Guide&lt;/head&gt;&lt;p&gt;If you have spent any amount of time in Japanese spaces online you may have heard of "Morg". Especially on Discord or Reddit.&lt;/p&gt;&lt;p&gt;He's a really nice guy who knows a lot about grammar and wants to help you learn Japanese.&lt;/p&gt;&lt;p&gt;This year he took it upon himself to improve the Sakubi grammar guide, and ended up writing Yokubi.&lt;/p&gt;&lt;p&gt;It's a really succinct grammar guide designed to get you immersing ASAP.&lt;/p&gt;&lt;head rend="h1"&gt;Best for books&lt;/head&gt;&lt;head rend="h2"&gt;🏆LumieReader&lt;/head&gt;&lt;p&gt;Lumie Reader came out hitting this year with a single premise:&lt;/p&gt;&lt;quote&gt;What if we made ttsu but good?&lt;/quote&gt;&lt;p&gt;It's entirely offline, very fast and supports a lot of features.&lt;/p&gt;&lt;p&gt;It has some features over other readers like:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Bookmarks&lt;/item&gt;&lt;item&gt;Extensive statistics for reading&lt;/item&gt;&lt;item&gt;Cloud sync (this is the biggest one)&lt;/item&gt;&lt;item&gt;Social features, like sharing what you are reading&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can tell I don't read books can't ya...&lt;/p&gt;&lt;head rend="h1"&gt;Conclusion&lt;/head&gt;&lt;p&gt;This year has been amazing for Japanese learning tools.&lt;/p&gt;&lt;p&gt;If you want to give back to the community but don't want to code, many of these devs have donation links listed.&lt;/p&gt;&lt;p&gt;What's your favourite tool? Tell me in the comments :) &amp;lt;3&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://skerritt.blog/best-japanese-learning-tools-2025-award-show/"/><published>2025-12-29T00:01:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46416945</id><title>You can make up HTML tags</title><updated>2025-12-29T06:56:26.165877+00:00</updated><content>&lt;doc fingerprint="8706dbfe498098bd"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;You can make up HTML tags:&lt;/head&gt;(Programming)&lt;p&gt;Instead of writing HTML like this:&lt;/p&gt;&lt;code&gt;&amp;lt;div class=cool-thing&amp;gt;
Hello, World!
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;p&gt;… you can write HTML like this:&lt;/p&gt;&lt;code&gt;&amp;lt;cool-thing&amp;gt;
Hello, World!
&amp;lt;/cool-thing&amp;gt;
&lt;/code&gt;&lt;p&gt;… and CSS like this:&lt;/p&gt;&lt;code&gt;cool-thing {
	display: block;
	font-weight: bold;
	text-align: center;
	filter: drop-shadow(0 0 0.5em #ff0);
	color: #ff0;
}
&lt;/code&gt;&lt;p&gt;Browsers handle unrecognized tags by treating them as a generic element, with no effect beyond what’s specified in the CSS. This isn’t just a weird quirk, but is standardized behavior. If you include hyphens in the name, you can guarantee that your tag won’t appear in any future versions of HTML.&lt;/p&gt;&lt;p&gt;While you should use descriptive built-in tags if they exist, if it’s a choice between &amp;lt;div&amp;gt; and &amp;lt;span&amp;gt;, making up your own tag provides better readability then using a bunch of class names.&lt;/p&gt;&lt;p&gt;As an example, if you have a bunch of nested tags:&lt;/p&gt;&lt;code&gt;&amp;lt;div class=article&amp;gt;
&amp;lt;div class=article-header&amp;gt;
&amp;lt;div class=article-quote&amp;gt;
&amp;lt;div class=quote-body&amp;gt;
... a bunch more HTML ...
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;p&gt;Good luck trying to insert something inside of “article-heading” but after “article-quote” on the first try. This problem vanishes if you use descriptive tag names — no &amp;lt;/div&amp;gt; counting required:&lt;/p&gt;&lt;code&gt;&amp;lt;main-article&amp;gt;
&amp;lt;article-header&amp;gt;
&amp;lt;article-quote&amp;gt;
&amp;lt;quote-body&amp;gt;
... a bunch more HTML ...
&amp;lt;/quote-body&amp;gt;
&amp;lt;/article-quote&amp;gt;
&amp;lt;!-- here! --&amp;gt;
&amp;lt;/article-header&amp;gt;
&amp;lt;/main-article&amp;gt;
&lt;/code&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maurycyz.com/misc/make-up-tags/"/><published>2025-12-29T02:47:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417227</id><title>Fast GPU Interconnect over Radio</title><updated>2025-12-29T06:56:25.897795+00:00</updated><content>&lt;doc fingerprint="adc0404b0a9d96e5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Data Centers Demand More Than Copper Can Deliver&lt;/head&gt;
    &lt;p&gt;Radio and terahertz links could be better, faster, and cheaper&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In data-center terms, scaling out involves linking computers, while scaling up packs more GPUs into a computer, challenging copper’s physical limits.&lt;/item&gt;
      &lt;item&gt;Copper cables face a phenomenon at high data rates at high data rates that necessitate wider wires and more power, complicating a data center’s dense connections.&lt;/item&gt;
      &lt;item&gt;Point2 and AttoTude propose radio-based cables, offering longer reach, lower power consumption, and narrower cables than copper, without the cost and complexity of optics.&lt;/item&gt;
      &lt;item&gt;Startups aim to directly integrate radio cables with GPUs, easing cooling needs and enhancing data-center efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How fast you can train gigantic new AI models boils down to two words: up and out.&lt;/p&gt;
    &lt;p&gt;In data-center terms, scaling out means increasing how many AI computers you can link together to tackle a big problem in chunks. Scaling up, on the other hand, means jamming as many GPUs as possible into each of those computers, linking them so that they act like a single gigantic GPU, and allowing them to do bigger pieces of a problem faster.&lt;/p&gt;
    &lt;p&gt;The two domains rely on two different physical connections. Scaling out mostly relies on photonic chips and optical fiber, which together can sling data hundreds or thousands of meters. Scaling up, which results in networks that are roughly 10 times as dense, is the domain of much simpler and less costly technology—copper cables that often span no more than a meter or two.&lt;/p&gt;
    &lt;p&gt;This article is part of our special report Top Tech 2026.&lt;/p&gt;
    &lt;p&gt;But the increasingly high GPU-to-GPU data rates needed to make more powerful computers work are coming up against the physical limits of copper. As the bandwidth demands on copper cables approach the terabit-per-second realm, physics demands that they be made shorter and thicker, says David Kuo, vice president of product marketing and business development at the data-center-interconnect startup Point2 Technology. That’s a big problem, given the congestion inside computer racks today and the fact that Nvidia, the leading AI hardware company, plans an eightfold increase in the maximum number of GPUs per system, from 72 to 576 by 2027.&lt;/p&gt;
    &lt;p&gt;“We call it the copper cliff,” says Kuo.&lt;/p&gt;
    &lt;p&gt;The industry is working on ways to unclog data centers by extending copper’s reach and bringing slim, long-reaching optical fiber closer to the GPUs themselves. But Point2 and another startup, AttoTude, advocate for a solution that’s simultaneously in between the two technologies and completely different from them. They claim the tech will deliver the low cost and reliability of copper as well as some of the narrow gauge and distance of optical—a combination that will handily meet the needs of future AI systems.&lt;/p&gt;
    &lt;p&gt;Their answer? Radio.&lt;/p&gt;
    &lt;p&gt;Later this year, Point2 will begin manufacturing the chips behind a 1.6-terabit-per-second cable consisting of eight slender polymer waveguides, each capable of carrying 448 gigabits per second using two frequencies, 90 gigahertz and 225 GHz. At each end of the waveguide are plug-in modules that turn electronic bits into modulated radio waves and back again. AttoTude is planning essentially the same thing, but at terahertz frequencies and with a different kind of svelte, flexible cable.&lt;/p&gt;
    &lt;p&gt;Both companies say their technologies can easily outdo copper in reach—spanning 10 to 20 meters without significant loss, which is certainly long enough to handle Nvidia’s announced scale-up plans. And in Point2’s case, the system consumes one-third of optical’s power, costs one-third as much, and offers as little as one-thousandth the latency.&lt;/p&gt;
    &lt;p&gt;According to its proponents, radio’s reliability and ease of manufacturing compared with those of optics mean that it might beat photonics in the race to bring low-energy processor-to-processor connections all the way to GPU, eliminating some copper even on the printed circuit board.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s wrong with copper?&lt;/head&gt;
    &lt;p&gt;So, what’s wrong with copper? Nothing, so long as the data rate isn’t too high and the distance it has to go isn’t too far. At high data rates, though, conductors like copper fall prey to what’s called the skin effect.&lt;/p&gt;
    &lt;p&gt;A 1.6-terabit-per-second e-Tube cable has half the area of a 32-gauge copper cable and has up to 20 times the reach. Point2 Technology&lt;/p&gt;
    &lt;p&gt;The skin effect occurs because the signal’s rapidly changing current leads to a changing magnetic field that tries to counter the current. This countering force is concentrated at the middle of the wire, so most of the current is confined to flowing at the wire’s outer edge—the “skin”—which increases resistance. At 60 hertz—the mains frequency in many countries—most of the current is in the outer 8 millimeters of copper. But at 10 GHz, the skin is just 0.65 micrometers deep. So to push high-frequency data through copper, the wire needs to be wider, and you need more power. Both requirements work against packing more and more connections into a smaller space to scale up computing.&lt;/p&gt;
    &lt;p&gt;To counteract the skin effect and other signal-degrading issues, companies have developed copper cables with specialized electronics at either end. With the most promising, called active electrical cables, or AECs, the terminating chip is called a retimer (pronounced “re-timer”). This IC cleans up the data signal and the clock signal as they arrive from the processor. The circuit then retransmits them down the copper cable’s typically eight pairs of wires, or lanes. (There is a second set for transmitting in the other direction.) At the other end, the chip’s twin takes care of any noise or clock issues that accumulate during the journey and sends the data on to the receiving processor. Thus, at the cost of electronic complexity and power consumption, an AEC can extend the distance that copper can reach.&lt;/p&gt;
    &lt;p&gt;Don Barnetson, senior vice president and head of product at Credo, which provides network hardware to data centers, says his company has developed an AEC that can deliver 800 Gb/s as far as 7 meters—a distance that’s likely needed as computers hit 500 to 600 GPUs and span multiple racks. The first use of AECs will probably be to link individual GPUs to the network switches that form the scale-out network. This first stage in the scale-out network is important, says Barnetson, because “it’s the only nonredundant hop in the network.” Losing that link, even momentarily, can cause an AI training run to collapse.&lt;/p&gt;
    &lt;p&gt;But even if retimers manage to push the copper cliff a bit farther into the future, physics will eventually win. Point2 and AttoTude are betting that point is coming soon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Terahertz radio’s reach&lt;/head&gt;
    &lt;p&gt;AttoTude grew out of founder and CEO Dave Welch’s deep investigations into photonics. A cofounder of Infinera, an optical telecom–equipment maker purchased by Nokia in 2025, Welch developed photonic systems for decades. He knows the technology’s weaknesses well: It consumes too much power (about 10 percent of a data center’s compute budget, according to Nvidia); it’s extremely sensitive to temperature; getting light into and out of photonics chips requires micrometer-precision manufacturing; and the technology’s lack of long-term reliability is notorious. (There’s even a term for it: “link flap.”)&lt;/p&gt;
    &lt;p&gt;“Customers love fiber. But what they hate is the photonics,” says Welch. “Electronics have been demonstrated to be inherently more reliable than optics.”&lt;/p&gt;
    &lt;p&gt;Fresh off Nokia’s US $2.3 billion purchase of Infinera, Welch asked himself some fundamental questions as he contemplated his next startup, beginning with “If I didn’t have to be at [an optical wavelength], where should I be?” The answer was the highest frequency that’s achievable purely with electronics—the terahertz regime, 300 to 3,000 GHz.&lt;/p&gt;
    &lt;p&gt;“You start with passive copper, and you do everything you can to run in passive copper as long as you can.” —Don Barnetson, Credo&lt;/p&gt;
    &lt;p&gt;So Welch and his team set about building a system that consists of a digital component to interface with the GPU, a terahertz-frequency generator, and a mixer to encode the data on the terahertz signal. An antenna then funnels the signal into a narrow, flexible waveguide.&lt;/p&gt;
    &lt;p&gt;As for the waveguide, it’s made of a dielectric at the center, which channels the terahertz signal, surrounded by cladding. One early version was just a narrow, hollow copper tube. Welch says that the second-generation cable—made up of fibers only about 200 µm across— points to a system with losses down to 0.3 decibels per meter—a small fraction of the loss from a typical copper cable carrying 224 Gb/s.&lt;/p&gt;
    &lt;p&gt;Welch predicts this waveguide will be able to carry data as far as 20 meters. That “happens to be a beautiful distance for scale-up in data centers,” he says.&lt;/p&gt;
    &lt;p&gt;So far, AttoTude has made the individual components—the digital data chip, the terahertz-signal generator, the circuit that mixes the two—along with a couple generations of waveguides. But the company hasn’t yet integrated them into a single pluggable form. Still, Welch says, the combination delivers enough bandwidth for at least 224 Gb/s transmission, and the startup demonstrated 4-meter transmission at 970 GHz last April at the Optical Fiber Communications Conference, in San Francisco.&lt;/p&gt;
    &lt;head rend="h2"&gt;Radio’s reach in the data center&lt;/head&gt;
    &lt;p&gt;Point2 has been aiming to bring radio to the data center longer than AttoTude has. Formed nine years ago by veterans of Marvell, Nvidia, and Samsung, the startup has pulled in $55 million in venture funding, most notably from computer cables and connections maker Molex. The latter’s backing “is critical, because they’re a major part of the cable-and-connector ecosystem,” says Kuo. Molex has already shown that it can make Point2’s cable without modifying its existing manufacturing lines, and now Foxconn Interconnect Technology, which makes cables and connectors, is partnering with the startup. The support could be a big selling point for the hyperscalers who would be Point2’s customers.&lt;/p&gt;
    &lt;p&gt;Nvidia’s GB200 NVL72 rack-scale computer relies on many copper cables to link its 72 processors together.NVIDIA&lt;/p&gt;
    &lt;p&gt;Each end of the Point2 cable, called an e-Tube, consists of a single silicon chip that converts the incoming digital data into modulated millimeter-wave frequencies and an antenna that radiates into the waveguide. The waveguide itself is a plastic core with metal cladding, all wrapped in a metal shield. A 1.6-Tb/s cable, called an active radio cable (ARC), is made up of eight e-Tube cores. At 8.1 millimeters across, that cable takes up half the volume of a comparable AEC cable.&lt;/p&gt;
    &lt;p&gt;One of the benefits of operating at RF frequencies is that the chips that handle them can be made in a standard silicon foundry, says Kuo. A collaboration between engineers at Point2 and the Korea Advanced Institute of Science and Technology, reported this year in the IEEE Journal of Solid-State Circuits, used 28-nanometer CMOS technology, which hasn’t been cutting edge since 2010.&lt;/p&gt;
    &lt;head rend="h2"&gt;The scale-up network market&lt;/head&gt;
    &lt;p&gt;As promising as their tech sounds, Point2 and AttoTude will have to overcome the data-center industry’s long history with copper. “You start with passive copper,” says Credo’s Barnetson. “And you do everything you can to run in passive copper as long as you can.”&lt;/p&gt;
    &lt;p&gt;The boom in liquid cooling for data-center computing is evidence of that, he says. “The entire reason people have gone to liquid cooling is to keep [scaling up] in passive copper,” Barnetson says. To connect more GPUs in a scale-up network with passive copper, they must be packed in at densities too high for air cooling alone to handle. Getting the same kind of scale-up from a more spread-out set of GPUs connected by millimeter-wave ARCs would ease the need for cooling, suggests Kuo.&lt;/p&gt;
    &lt;p&gt;Meanwhile, both startups are also chasing a version of the technology that will attach directly to the GPU.&lt;/p&gt;
    &lt;p&gt;Nvidia and Broadcom recently deployed optical transceivers that live inside the same package as a processor, separating the electronics and optics by micrometers rather than centimeters or meters. Right now, the technology is limited to the network-switch chips that connect to a scale-out network. But big players and startups alike are trying to extend its use all the way to the GPU.&lt;/p&gt;
    &lt;p&gt;Both Welch and Kuo say their companies’ technologies could have a big advantage over optical tech in such a transceiver-processor package. Nvidia and Broadcom—separately—had to do a mountain of engineering to make their systems possible to manufacture and reliable enough to exist in the same package as a very expensive processor. One of the many challenges is how to attach an optical fiber to a waveguide on a photonic chip with micrometer accuracy. Because of its short wavelength, infrared laser light must be lined up very precisely with the core of an optical fiber, which is only around 10 µm across. By contrast, millimeter-wave and terahertz signals have a much longer wavelength, so you don’t need as much precision to attach the waveguide. In one demo system it was done by hand, says Kuo.&lt;/p&gt;
    &lt;p&gt;Pluggable connections will be the technology’s first use, but radio transceivers co-packaged with processors are “the real prize,” says Welch.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/rf-over-fiber"/><published>2025-12-29T03:39:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417252</id><title>Why I think Valve’s retiring the Steam Deck LCD</title><updated>2025-12-29T06:56:25.812810+00:00</updated><content/><link href="https://gardinerbryant.com/why-valves-retiring-the-steam-deck-lcd/"/><published>2025-12-29T03:44:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417676</id><title>My First Meshtastic Network</title><updated>2025-12-29T06:56:25.492102+00:00</updated><content>&lt;doc fingerprint="ae48c852da0933e2"&gt;
  &lt;main&gt;
    &lt;p&gt;I first heard about Meshtastic from a blog post that made the rounds on Hacker News.&lt;lb/&gt; The author lived on a boat and used Meshtastic radios to stay in touch without cellular networks. Meshtastic allows you to send short text messages (around 200 characters) over long ranges without cell towers or satellites. It works by creating a mesh network of low-power LoRa devices that relay messages on behalf of peers. Because it uses license-free radio frequencies (in the ~915 MHz ISM band), no ham license is required.&lt;/p&gt;
    &lt;head rend="h2"&gt;My First Radio&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; I ordered a pair of Heltec V3 LoRa radios (the ones I bought), which are small devices based on the ESP32 microcontroller with a LoRa modem. These radios didn't come with GPS, which in hindsight I regret because Meshtastic can share your location if a GPS is present. I also picked up a third-party antenna upgrade, since the community warned that the cheap antennas bundled with these devices are nearly useless (yet another thing I learned in hindsight)&lt;/p&gt;
    &lt;p&gt;Out of the box, the devices had outdated firmware and wouldn't communicate with current Meshtastic apps. Fortunately, flashing the latest firmware was straightforward using the official Meshtastic Web Flasher (a browser-based tool at flasher.meshtastic.org). By connecting the device via USB and using Chrome (which supports the WebSerial API), I flashed the newest Meshtastic firmware without installing any software.&lt;/p&gt;
    &lt;head rend="h2"&gt;Initial Setup&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; With fresh firmware, I could configure and manage the radios using the Meshtastic mobile app (available for Android/iOS) over Bluetooth. There's also a web client (client.meshtastic.org) that works over USB or Wi-Fi. One quirk I learned: many Meshtastic devices can work over both Wi-Fi and Bluetooth, but you typically use one interface at a time for management. On my device, it was not possible to use both at the same time, which led to some confusion.&lt;/p&gt;
    &lt;p&gt;After setup, I had my two devices chatting with each other. Sending a message from one device would pop up on the other in a few seconds. Meshtastic uses a mesh protocol where every node repeats messages, so two devices in direct range will communicate one-to-one, and if more nodes are around they can hop messages further. I noticed that if I tried sending when only one device was on, the app would show &lt;quote&gt;Waiting to be acknowledged...&lt;/quote&gt; and eventually &lt;quote&gt;Maximum retransmission reached.&lt;/quote&gt; In other words, my message went nowhere because no other node heard it.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Contact&lt;/head&gt;
    &lt;p&gt;Up to this point, I hadn't heard any traffic besides my own test messages. I suspected I was the only Meshtastic user in my immediate area (the far west suburbs of Chicago). I left one radio powered on and placed it by a second-story window facing toward the city. The next morning, I was surprised to see that it had logged messages from a handful of unknown nodes overnight.&lt;/p&gt;
    &lt;p&gt;Looking up the node identifiers online, I discovered a website called MeshMap that shows public Meshtastic nodes on a map. Sure enough, some of the node names I saw appeared on a community mesh map of the Chicagoland area. A few even had labels referencing &lt;quote&gt;ChiMesh&lt;/quote&gt;. There's an active group of Meshtastic enthusiasts in Chicago, Chicagoland Mesh, and somehow my little device had picked up their transmissions from roughly 40-50 miles away. This was an early sign that mesh networking can extend beyond line-of-sight with the help of intermediate nodes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Joining a Local Community&lt;/head&gt;
    &lt;p&gt;Excited by this discovery, I joined the Chicagoland Mesh (ChiMesh) Discord server and introduced myself. To my surprise, there was another member only a mile or two from my house. We coordinated a simple experiment: he sent a test message from his device at home, and I was able to receive it on mine. However, when I tried to reply, he never saw my message. It became clear that while I could &lt;quote&gt;hear&lt;/quote&gt; the network, my transmission range was too short for others to hear me.&lt;/p&gt;
    &lt;p&gt;Community members quickly pointed to the antenna as the culprit. The stock rubber-duck antenna that came with my Heltec radio was likely low-quality. I switched to the high-gain antenna I bought and tried again. This time my messages started getting through. Antenna quality (and placement) makes a huge difference in radio range.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expanding the Network&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; My success rekindled interest among a couple of local makers. Some fellow members of my local makerspace had dabbled with Meshtastic earlier but stopped due to the lack of active users. With my second device to spare, we set it up as a relay node at the makerspace, which is a few miles from my house. Positioned near a roofline, this node acted like a little tower, rebroadcasting messages between my home and the other member's location.&lt;lb/&gt; It took a bit of fiddling with placement and settings, but eventually we managed to pass messages between our homes via the makerspace node. It wasn't instantaneous or foolproof, but messages eventually hopped from my device to the relay and then to my friend's device, reaching farther than any single link could.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; To better understand and improve our coverage, we started playing with the Meshtastic Site Planner. This is a web tool that lets you simulate radio coverage on a map given a node's location, antenna, and power. Being in a river valley, our area has some challenging terrain that limits range. The planner helped confirm that putting a node on higher ground (a tall building) could dramatically extend reach.&lt;/p&gt;
    &lt;p&gt;In the coming months, we plan to upgrade to better antennas (perhaps an outdoor mounted one on a mast) and add more nodes at strategic spots. I'm also interested in experimenting with Meshtastic's other capabilities. For example, it can interface with sensors and send telemetry. A fun project idea is an off-grid weather station broadcasting its data over the mesh network.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continuing the Exploration&lt;/head&gt;
    &lt;p&gt;Working with Meshtastic has been fun. It's impressive how a few inexpensive devices can form a communications network covering many miles. The system is limited, but within those constraints it feels magical to send a message into the ether and have it hop across a county line to a stranger.&lt;/p&gt;
    &lt;p&gt;Meshtastic isn't very useful alone, but as more people join, the mesh becomes stronger and more useful for everyone. If you're in the Illinois Fox Valley area and interested, feel free to reach out or drop by our makerspace meetup - we'd love to grow the network. And if you're elsewhere, consider looking up Meshtastic groups in your region. I hope to see you on the air.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rickcarlino.com/notes/electronics/my-first-meshtastic-network.html"/><published>2025-12-29T05:12:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417748</id><title>Show HN: My not-for-profit search engine with no ads, no AI, &amp; all DDG bangs</title><updated>2025-12-29T06:56:25.134010+00:00</updated><content>&lt;doc fingerprint="5fcfdfe9c315cacb"&gt;
  &lt;main&gt;
    &lt;p&gt;nilch No AI, no ads, just search.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nilch.org"/><published>2025-12-29T05:25:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417791</id><title>Binaries</title><updated>2025-12-29T06:56:25.048227+00:00</updated><content>&lt;doc fingerprint="a804ba32052587f4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Huge binaries&lt;/head&gt;
    &lt;p&gt;Published 2025-12-28 on Farid Zakaria's Blog&lt;/p&gt;
    &lt;p&gt;A problem I experienced when pursuing my PhD and submitting academic articles was that I had built solutions to problems that required dramatic scale to be effective and worthwhile. Responses to my publication submissions often claimed such problems did not exist; however, I had observed them during my time within industry, such as at Google, but I couldn’t cite it!&lt;/p&gt;
    &lt;p&gt;One problem that is only present at these mega-codebases is massive binaries. What’s the largest binary (ELF file) you’ve ever seen? I had observed binaries beyond 25GiB, including debug symbols. How is this possible? These companies prefer to statically build their services to speed up startup and simplify deployment. Statically including all code in some of the world’s largest codebases is a recipe for massive binaries.&lt;/p&gt;
    &lt;p&gt;Similar to the sound barrier, there is a point at which code size becomes problematic and we must re-think how we link and build code. For x86_64, that is the 2GiB “Relocation Barrier.”&lt;/p&gt;
    &lt;p&gt;Why 2GiB? 🤔&lt;/p&gt;
    &lt;p&gt;Well let’s take a look at how position independent code is put-together.&lt;/p&gt;
    &lt;p&gt;Let’s look at a simple example.&lt;/p&gt;
    &lt;code&gt;extern void far_function();

int main() {
    far_function();
    return 0;
}
&lt;/code&gt;
    &lt;p&gt;If we compile this &lt;code&gt;gcc -c simple-relocation.c -o simple-relocation.o&lt;/code&gt; we can inspect it with &lt;code&gt;objdump&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;&amp;gt; objdump -dr simple-relocation.o

0000000000000000 &amp;lt;main&amp;gt;:
   0:	55                   	push   %rbp
   1:	48 89 e5             	mov    %rsp,%rbp
   4:	b8 00 00 00 00       	mov    $0x0,%eax
   9:	e8 00 00 00 00       	call   e &amp;lt;main+0xe&amp;gt;
			a: R_X86_64_PLT32	far_function-0x4
   e:	b8 00 00 00 00       	mov    $0x0,%eax
  13:	5d                   	pop    %rbp
  14:	c3                   	ret
&lt;/code&gt;
    &lt;p&gt;There’s a lot going on here, but one important part is &lt;code&gt;e8 00 00 00 00&lt;/code&gt;. &lt;code&gt;e8&lt;/code&gt; is the &lt;code&gt;CALL&lt;/code&gt; opcode [ref] and it takes a 32bit signed relative offset, which happens to be 0 (four bytes of 0) right now. &lt;code&gt;objdump&lt;/code&gt; also lets us know there is a “relocation” necessary to fixup this code when we finalize it. We can view this relocation with &lt;code&gt;readelf&lt;/code&gt; as well.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note If you are wondering why we need&lt;/p&gt;&lt;code&gt;-0x4&lt;/code&gt;, it’s because the offset is relative to the instruction-pointer which has already moved to the next instruction. The 4 bytes is the operand it has skipped over.&lt;/quote&gt;
    &lt;code&gt;&amp;gt; readelf -r simple-relocation.o -d

Relocation section '.rela.text' at offset 0x170 contains 1 entry:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
00000000000a  000400000004 R_X86_64_PLT32    0000000000000000 far_function - 4
&lt;/code&gt;
    &lt;p&gt;This is additional information embedded in the binary which tells the linker in susbsequent stages that it has code that needs to be fixed. Here we see the address &lt;code&gt;00000000000a&lt;/code&gt;, and &lt;code&gt;a&lt;/code&gt; is 9 + 1, which is the offset of the start of the operand for our &lt;code&gt;CALL&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;Let’s now create the C file for our missing function.&lt;/p&gt;
    &lt;code&gt;void far_function() {
}
&lt;/code&gt;
    &lt;p&gt;We will now compile it and link the two object files together using our linker.&lt;/p&gt;
    &lt;code&gt;&amp;gt; gcc simple-relocation.o far-function.o -o simple-relocation
&lt;/code&gt;
    &lt;p&gt;Let’s now inspect that same callsite and see what it has.&lt;/p&gt;
    &lt;code&gt;&amp;gt; objdump -dr simple-relocation

0000000000401106 &amp;lt;main&amp;gt;:
  401106:	55                   	push   %rbp
  401107:	48 89 e5             	mov    %rsp,%rbp
  40110a:	b8 00 00 00 00       	mov    $0x0,%eax
  40110f:	e8 07 00 00 00       	call   40111b &amp;lt;far_function&amp;gt;
  401114:	b8 00 00 00 00       	mov    $0x0,%eax
  401119:	5d                   	pop    %rbp
  40111a:	c3                   	ret

000000000040111b &amp;lt;far_function&amp;gt;:
  40111b:	55                   	push   %rbp
  40111c:	48 89 e5             	mov    %rsp,%rbp
  40111f:	90                   	nop
  401120:	5d                   	pop    %rbp
  401121:	c3                   	ret
&lt;/code&gt;
    &lt;p&gt;We can see that the linker did the right thing with the relocation and calculated the relative offset of our symbol &lt;code&gt;far_function&lt;/code&gt; and fixed the &lt;code&gt;CALL&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;Okay cool…🤷 What does this have to do with huge binaries?&lt;/p&gt;
    &lt;p&gt;Notice that this call instruction, &lt;code&gt;e8&lt;/code&gt;, only takes 32bits signed which means it’s limited to 2^31 bits. This means a callsite can only jump roughly 2GiB forward or 2GiB backward. The “2GiB Barrier” represents the total reach of a single relative jump.&lt;/p&gt;
    &lt;p&gt;What happens if our callsite is over 2GiB away?&lt;/p&gt;
    &lt;p&gt;Let’s build a synthetic example by asking our linker to place &lt;code&gt;far_function&lt;/code&gt; really really far away. We can do this using a “linker script”.&lt;/p&gt;
    &lt;code&gt;SECTIONS
{
    /* 1. Start with standard low-address sections */
    . = 0x400000;
    
    /* Catch everything except our specific 'far' object */
    .text : { 
        simple-relocation.o(.text.*) 
    }
    .rodata : { *(.rodata .rodata.*) }
    .data   : { *(.data .data.*) }
    .bss    : { *(.bss .bss.*) }

    /* 2. Move the cursor for the 'far' island */
    . = 0x120000000; 
    
    .text.far : { 
        far-function.o(.text*) 
    }
}
&lt;/code&gt;
    &lt;p&gt;If we now try to link our code we will a “relocation overflow”. I used &lt;code&gt;lld&lt;/code&gt; from LLVM because the error messages are a bit prettier.&lt;/p&gt;
    &lt;code&gt;&amp;gt; gcc simple-relocation.o far-function.o -T overflow.lds -o simple-relocation-overflow -fuse-ld=lld

ld.lld: error: &amp;lt;internal&amp;gt;:(.eh_frame+0x6c):
relocation R_X86_64_PC32 out of range:
5364513724 is not in [-2147483648, 2147483647]; references section '.text'
ld.lld: error: simple-relocation.o:(function main: .text+0xa):
relocation R_X86_64_PLT32 out of range:
5364514572 is not in [-2147483648, 2147483647]; references 'far_function'
&amp;gt;&amp;gt;&amp;gt; referenced by simple-relocation.c
&amp;gt;&amp;gt;&amp;gt; defined in far-function.o
&lt;/code&gt;
    &lt;p&gt;When we hit this problem what solutions do we have? Well this is a complete other subject on “code models”, and it’s a little more nuanced depending on whether we are accessing data (i.e. static variables) or code that is far away. A great blog post that goes into this is the following by @maskray who wrote &lt;code&gt;lld&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The simplest solution however is to use &lt;code&gt;-mcmodel=large&lt;/code&gt; which changes all the relative &lt;code&gt;CALL&lt;/code&gt; instructions to absolute &lt;code&gt;JMP&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;&amp;gt; gcc simple-relocation.o far-function.o -T overflow.lds -o simple-relocation-overflow

&amp;gt; gcc -c simple-relocation.c -o simple-relocation.o -mcmodel=large -fno-asynchronous-unwind-tables

&amp;gt; gcc simple-relocation.o far-function.o -T overflow.lds -o simple-relocation-overflow

./simple-relocation-overflow
&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note I needed to add&lt;/p&gt;&lt;code&gt;-fno-asynchronous-unwind-tables&lt;/code&gt;to disable some additional data that might cause overflow for the purpose of this demonstration.&lt;/quote&gt;
    &lt;p&gt;What does the disassembly look like now?&lt;/p&gt;
    &lt;code&gt;&amp;gt; objdump -dr simple-relocation-overflow 

0000000120000000 &amp;lt;far_function&amp;gt;:
   120000000:	55                   	push   %rbp
   120000001:	48 89 e5             	mov    %rsp,%rbp
   120000004:	90                   	nop
   120000005:	5d                   	pop    %rbp
   120000006:	c3                   	ret

00000000004000e6 &amp;lt;main&amp;gt;:
  4000e6:	55                   	push   %rbp
  4000e7:	48 89 e5             	mov    %rsp,%rbp
  4000ea:	b8 00 00 00 00       	mov    $0x0,%eax
  4000ef:	48 ba 00 00 00 20 01 	movabs $0x120000000,%rdx
  4000f6:	00 00 00 
  4000f9:	ff d2                	call   *%rdx
  4000fb:	b8 00 00 00 00       	mov    $0x0,%eax
  400100:	5d                   	pop    %rbp
  400101:	c3                   	ret
&lt;/code&gt;
    &lt;p&gt;There is no longer a sole &lt;code&gt;CALL&lt;/code&gt; instruction, it has become &lt;code&gt;MOVABS&lt;/code&gt; &amp;amp; &lt;code&gt;CALL&lt;/code&gt; 😲. This changed the instructions from 5 (opcode + 4 bytes for 32bit relative offset) to a whopping 12 bytes (2 bytes for &lt;code&gt;ABS&lt;/code&gt; opcode + 8 bytes for absolute 64 bit address + 2 bytes for &lt;code&gt;CALL&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;This has notable downsides among others:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Instruction Bloat: We’ve gone from 5 bytes per call to 12. In a binary with millions of callsites, this can add up.&lt;/item&gt;
      &lt;item&gt;Register Pressure: We’ve burned a general-purpose register, &lt;code&gt;%rdx&lt;/code&gt;, to perform the jump.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Caution I had a lot of trouble building a benchmark that demonstrated a worse lower IPC (instructions per-cycle) for the large&lt;/p&gt;&lt;code&gt;mcmodel&lt;/code&gt;, so let’s just take my word for it. 🤷&lt;/quote&gt;
    &lt;p&gt;We would like to keep our small code-model. What other strategies can we pursue?&lt;/p&gt;
    &lt;p&gt;More to come in subsequent writings.&lt;/p&gt;
    &lt;p&gt; Improve this page @ f0a4dce &lt;lb/&gt; The content for this site is CC-BY-SA. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fzakaria.com/2025/12/28/huge-binaries"/><published>2025-12-29T05:35:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417815</id><title>Show HN: Z80-μLM, a 'Conversational AI' That Fits in 40KB</title><updated>2025-12-29T06:56:24.413731+00:00</updated><content>&lt;doc fingerprint="2da11bd4dafe81a"&gt;
  &lt;main&gt;
    &lt;p&gt;Z80-μLM is a 'conversational AI' that generates short character-by-character sequences, with quantization-aware training (QAT) to run on a Z80 processor with 64kb of ram.&lt;/p&gt;
    &lt;p&gt;The root behind this project was the question: how small can we go while still having personality, and can it be trained or fine-tuned easily? With easy self-hosted distribution?&lt;/p&gt;
    &lt;p&gt;The answer is Yes! And a 40kb .com binary (including inference, weights &amp;amp; a chat-style UI) running on a 4MHz processor from 1976.&lt;/p&gt;
    &lt;p&gt;It won't pass the Turing test, but it might make you smile at the green screen.&lt;/p&gt;
    &lt;p&gt;For insight on how to best train your own model, see TRAINING.md.&lt;/p&gt;
    &lt;p&gt;Two pre-built examples are included:&lt;/p&gt;
    &lt;p&gt;A conversational chatbot trained on casual Q&amp;amp;A pairs. Responds to greetings, questions about itself, and general banter with terse personality-driven answers.&lt;/p&gt;
    &lt;code&gt;&amp;gt; hello
HI
&amp;gt; are you a robot
YES
&amp;gt; do you dream
MAYBE
&lt;/code&gt;
    &lt;p&gt;A 20 Questions game where the model knows a secret topic and answers YES/NO/MAYBE to your questions. Guess correctly to WIN.&lt;/p&gt;
    &lt;code&gt;&amp;gt; is it alive
YES
&amp;gt; is it big
YES
&amp;gt; does it have a trunk
YES
&amp;gt; is it grey
MAYBE
&amp;gt; elephant
WIN
&lt;/code&gt;
    &lt;p&gt;Includes tools for generating training data with LLMs (Ollama or Claude API) and balancing class distributions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trigram hash encoding: Input text is hashed into 128 buckets - typo-tolerant, word-order invariant&lt;/item&gt;
      &lt;item&gt;2-bit weight quantization: Each weight is {-2, -1, 0, +1}, packed 4 per byte&lt;/item&gt;
      &lt;item&gt;16-bit integer inference: All math uses Z80-native 16-bit signed arithmetic&lt;/item&gt;
      &lt;item&gt;~40KB .COM file: Fits in CP/M's Transient Program Area (TPA)&lt;/item&gt;
      &lt;item&gt;Autoregressive generation: Outputs text character-by-character&lt;/item&gt;
      &lt;item&gt;No floating point: Everything is integer math with fixed-point scaling&lt;/item&gt;
      &lt;item&gt;Interactive chat mode: Just run &lt;code&gt;CHAT&lt;/code&gt;with no arguments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model doesn't understand you. But somehow, it gets you.&lt;/p&gt;
    &lt;p&gt;Your input is hashed into 128 buckets via trigram encoding - an abstract "tag cloud" representation. The model responds to the shape of your input, not the exact words:&lt;/p&gt;
    &lt;code&gt;"hello there"  →  [bucket 23: 64, bucket 87: 32, ...]
"there hello"  →  [bucket 23: 64, bucket 87: 32, ...]  (same!)
"helo ther"    →  [bucket 23: 32, bucket 87: 32, ...]  (similar - typo tolerant)
&lt;/code&gt;
    &lt;p&gt;This is semantically powerful for short inputs, but there's a limit: longer or order-dependent sentences blur together as concepts compete for the same buckets. "Open the door and turn on the lights" will likely be too close to distringuish from "turn on the door and open the lights."&lt;/p&gt;
    &lt;p&gt;A 1-2 word response can convey surprising nuance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;OK&lt;/code&gt;- acknowledged, neutral&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;WHY?&lt;/code&gt;- questioning your premise&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;R U?&lt;/code&gt;- casting existential doubt&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MAYBE&lt;/code&gt;- genuine uncertainty&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AM I?&lt;/code&gt;- reflecting the question back&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This isn't necessarily a limitation - it's a different mode of interaction. The terse responses force you to infer meaning from context or ask probing direct yes/no questions to see if it understands or not (e.g. 'are you a bot', 'are you human', 'am i human' displays logically consistent memorized answers)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Short, varied inputs with consistent categorized outputs&lt;/item&gt;
      &lt;item&gt;Fuzzy matching (typos, rephrasing, word order)&lt;/item&gt;
      &lt;item&gt;Personality through vocabulary choice&lt;/item&gt;
      &lt;item&gt;Running on constrianed 8-bit hardware&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A chatbot that generates novel sentences&lt;/item&gt;
      &lt;item&gt;Something that tracks multi-turn context deeply&lt;/item&gt;
      &lt;item&gt;A parser that understands grammar&lt;/item&gt;
      &lt;item&gt;Anything approaching general intelligence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It's small, but functional. And sometimes that's exactly what you need&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Input: 128 query trigram buckets + 128 context buckets&lt;/item&gt;
      &lt;item&gt;Hidden layers: Configurable depth/width, e.g., 256 → 192 → 128&lt;/item&gt;
      &lt;item&gt;Output: One neuron per character in charset&lt;/item&gt;
      &lt;item&gt;Activation: ReLU between hidden layers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Z80 is an 8-bit CPU, but we use its 16-bit register pairs (HL, DE, BC) for activations and accumulators. Weights are packed 4-per-byte (2-bit each) and unpacked into 8-bit signed values for the multiply-accumulate.&lt;/p&gt;
    &lt;p&gt;The 16-bit accumulator gives us numerical stability (summing 256 inputs without overflow), but the model's expressiveness is still bottlenecked by the 2-bit weights, and naive training may overflow or act 'weirdly' without QAT.&lt;/p&gt;
    &lt;p&gt;The core of inference is a tight multiply-accumulate loop. Weights are packed 4-per-byte:&lt;/p&gt;
    &lt;code&gt;; Unpack 2-bit weight from packed byte
ld a, (PACKED)      ; Get packed weights
and 03h             ; Mask bottom 2 bits
sub 2               ; Map 0,1,2,3 → -2,-1,0,+1
ld (WEIGHT), a

; Rotate for next weight
ld a, (PACKED)
rrca
rrca
ld (PACKED), a
&lt;/code&gt;
    &lt;p&gt;The multiply-accumulate handles the 4 possible weight values:&lt;/p&gt;
    &lt;code&gt;MULADD:
    or a
    jr z, DONE       ; weight=0: skip entirely
    jp m, NEG        ; weight&amp;lt;0: subtract
    ; weight=+1: add activation
    ld hl, (ACC)
    add hl, de
    ld (ACC), hl
    ret
NEG:
    cp 0FFh
    jr z, NEG1       ; weight=-1
    ; weight=-2: subtract twice
    ld hl, (ACC)
    sbc hl, de
    sbc hl, de
    ld (ACC), hl
    ret
NEG1:
    ; weight=-1: subtract once
    ld hl, (ACC)
    sbc hl, de
    ld (ACC), hl
    ret
&lt;/code&gt;
    &lt;p&gt;After each layer, arithmetic right-shift by 2 to prevent overflow:&lt;/p&gt;
    &lt;code&gt;sra h        ; Shift right arithmetic (preserves sign)
rr l
sra h
rr l         ; ACC = ACC / 4
&lt;/code&gt;
    &lt;p&gt;That's the entire neural network: unpack weight, multiply-accumulate, shift. Repeat ~100K times per character generated.&lt;/p&gt;
    &lt;p&gt;License: MIT or Apache-2.0 as you see fit.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/HarryR/z80ai"/><published>2025-12-29T05:41:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417844</id><title>Staying ahead of censors in 2025</title><updated>2025-12-29T06:56:23.725549+00:00</updated><content>&lt;doc fingerprint="be971c778814a0c2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; by meskio and shelikhoo | December 3, 2025 &lt;/p&gt;
      &lt;p&gt;From internet blackouts in Iran to Russia's evolving censorship tactics, 2025 has tested Tor's anti-censorship tools like never before. These are the moments where the work of Tor's anti-censorship team is more important than ever, to fulfill our mission of preserving connectivity between users in affected regions and the rest of the world.&lt;/p&gt;
      &lt;p&gt;In this blog post, we want to talk about what we've learned, how we've adapted, and what other internet users can do to keep Tor users connected.&lt;/p&gt;
      &lt;head rend="h2"&gt;Iran&lt;/head&gt;
      &lt;p&gt;In June, during the war between Iran and Israel, the censorship in Iran intensified up to a point where internet was disconnected for few days. Presumably to impede espionage-related communication while simultaneously consolidating political power.&lt;/p&gt;
      &lt;head rend="h3"&gt;Monitoring the censorship landscape&lt;/head&gt;
      &lt;p&gt;During this period, we were constantly monitoring the situation using our in-region vantage-point system. This vantage-point system is a network of monitoring locations inside Iran that provides more recent and accurate information about censorship than is available from public data.&lt;/p&gt;
      &lt;p&gt;One clear example is domain-fronting data. Domain-fronting is a technique that makes Tor traffic look like other popular, harder-to-block websites (like major cloud services). To determine which domain-fronting configurations perform best across the most locations, we deployed an automated testing tool that detects and reports the accessibility of the Snowflake broker and the Moat service for each domain-fronting configuration at each of our vantage points. This information is then aggregated by the log collector and subsequently used to monitor the domain-fronting configurations currently in use and to select the configurations to use in the future.&lt;/p&gt;
      &lt;head rend="h3"&gt;Strengthening Snowflake&lt;/head&gt;
      &lt;p&gt;Snowflake is the most used network traffic obfuscation tool in Iran. Over the past year we have been working on improving it to ensure that it remains strong and accessible to users.&lt;/p&gt;
      &lt;p&gt;We have upgraded the web extension to Manifest Version 3 (the latest browser extension standard), to be compatible with modern browsers. We improved the NAT checking logic which helps us figure out what kind of network setup each user has. This way, the proxies are more accurately assigned to the clients depending on their network capabilities. And we enhanced the metrics reported by the standalone proxy, providing better tooling for proxy operators to assist what is happening with their proxies.&lt;/p&gt;
      &lt;p&gt;Under the hood we have created a staging server for Snowflake, so we have a robust infrastructure to stress test new features making sure they're fit for real deployment. This will help us bring big changes in the coming year to improve the efficiency of the protocol where networks are severely disrupted and to create better mechanisms to prevent censors from blocking Snowflake.&lt;/p&gt;
      &lt;head rend="h3"&gt;Deploying Conjure&lt;/head&gt;
      &lt;p&gt;Censorship agencies like those in Iran often attempt to block bridges by obtaining bridge information in bulk and then inputing the network address of these bridges to their censorship gateways to block them. That's why we developed Conjure.&lt;/p&gt;
      &lt;p&gt;Conjure is a pluggable transport designed to stay ahead of proxy-listing-based blocking by leveraging unused address space within cooperating ISP networks, thereby limiting the damage caused by blocking individual network addresses. Think of it like the act of generating temporary email addresses to avoid spam emails, by making sure the address is temporary and easy to regenerate, anything blocked at that address won't affect your ability to get new ones.&lt;/p&gt;
      &lt;p&gt;We are working on distributing Conjure in places with strong censorship. To make it hard for censors, we have improved Tor's implementation of Conjure by extending the protocols used both for bootstrapping the connection and transport the data. We added multiple registration methods (DNS and AMP-cache), making the bootstrap of the conjure connection more censorship-resistant and the connection will look as if the user is connecting to widely used service. We also integrated additional transports from upstream (DTLS and prefix) that makes the Tor traffic look like common protocols–meaning regular internet traffic.&lt;/p&gt;
      &lt;head rend="h2"&gt;Russia&lt;/head&gt;
      &lt;p&gt;Another region that has experienced many changes this year is Russia. With continued conflict and attrition, internet censorship has intensified, including increased allowlist-based censorship and address-block-based censorship.&lt;/p&gt;
      &lt;p&gt;Last year, we introduced WebTunnel as a new pluggable transport. We have seen this year how WebTunnel has become a key tool for users in Russia, thanks to its ability to blend into regular web traffic. As the severity of censorship in Russia has increased, WebTunnel has also received several fixes, such as SNI imitation and safe non-WebPKI certificate support with certificate-chain pinning to ensure it can withstand more kinds of censorship, including SNI allowlisting and the rapid blocking of distributed bridges.&lt;/p&gt;
      &lt;p&gt;Many of these improvements come from volunteers or are shaped by user feedback. Our community of users and supporters makes all this work possible and helps us stay ahead at Tor. Thanks to our Tor community team, we have first-hand insights into what works and what doesn't. This gives us access to the best information in the region. Additionally, through the community team's work with people on the ground, we receive support in testing and identifying the best technology for each censorship scenario.&lt;/p&gt;
      &lt;head rend="h3"&gt;Experimenting with bridge distribution&lt;/head&gt;
      &lt;p&gt;When we started distributing WebTunnel bridges in December they were a very useful tool to connect to Tor. They worked well for months, and Tor Browser users got them configured automatically over Connect Assist if they were located in Russia. However, in June, the Russian censors began listing most of our WebTunnel bridges, prompting us to shift strategies.&lt;/p&gt;
      &lt;p&gt;In recent history, our Telegram distributor has proven to be a useful tool in Russia, as the censor has a harder time extracting all the bridges from it. This is why we have now added support for WebTunnel in our Telegram distributor. We are always trying to meet our users where they are, and while Telegram might not be the safest place for your online communications, many users in Russia already uses it. And is not only useful for Russian users, but also for Iranian ones that are currently using webtunnel bridges distributed over Telegram.&lt;/p&gt;
      &lt;p&gt;All these fast changes of bridges distribution are possible thanks to rdsys, Tor's new bridge distribution system that we introduced last year. This year we kept improving rdsys adding a staging server, so we can stress-test it in a similar environments to the ones used in production. For our censored users that means that by the time new and updated anticensorship features arrive, we have already been able to fix many stability issues.&lt;/p&gt;
      &lt;head rend="h2"&gt;Where do we go from here?&lt;/head&gt;
      &lt;p&gt;Supporting our users to continue fighting censorship is what our work is all about. Making it possible to connect to the Tor network on censored networks–whatever they are. Whether it is your university, your internet service provider, or your government trying to keep you from getting the information you are entitled to. Next year we'll start rolling out Conjure, keep improving WebTunnel,and prepare Snowflake for the next big censorship events.&lt;/p&gt;
      &lt;p&gt;You too can help us fight censorship today, by sharing your bandwidth and running your own Snowflake. The easiest way is to install a snowflake plugin in your browser to help others access the Tor network. And if you have a website consider running a webtunnel bridge.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forum.torproject.org/t/staying-ahead-of-censors-in-2025-what-weve-learned-from-fighting-censorship-in-iran-and-russia/20898"/><published>2025-12-29T05:47:40+00:00</published></entry></feed>