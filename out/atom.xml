<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-21T14:38:53.277178+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45632429</id><title>A laser pointer at 2B FPS [video]</title><updated>2025-10-21T14:39:01.550937+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=o4TdHrMi6do"/><published>2025-10-19T06:42:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640838</id><title>AWS multiple services outage in us-east-1</title><updated>2025-10-21T14:39:01.300834+00:00</updated><link href="https://health.aws.amazon.com/health/status?ts=20251020"/><published>2025-10-20T07:22:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45643163</id><title>Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system</title><updated>2025-10-21T14:39:01.051986+00:00</updated><content>&lt;doc fingerprint="79b8949c8f16d2d1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system— up to 9x increase in output lets 213 GPUs perform like 1,192&lt;/head&gt;
    &lt;p&gt;A paper presented at SOSP 2025 details how token-level scheduling helped one GPU serve multiple LLMs, reducing demand from 1,192 to 213 H20s.&lt;/p&gt;
    &lt;p&gt;Alibaba Cloud claims its new Aegaeon pooling system reduces the number of Nvidia GPUs required to serve large language models by 82% during a multi-month beta test inside its Model Studio marketplace. The result, published in a peer-reviewed paper presented at the 2025 ACM Symposium on Operating Systems (SOSP) in Seoul, suggests that cloud providers may be able to extract significantly more inference capacity from existing silicon, especially in constrained markets like China, where the supply of Nvidia's latest H20s remains limited.&lt;/p&gt;
    &lt;p&gt;Unlike training-time breakthroughs that chase model quality or speed, Aegaeon is an inference-time scheduler designed to maximize GPU utilization across many models with bursty or unpredictable demand. Instead of pinning one accelerator to one model, Aegaeon virtualizes GPU access at the token level, allowing it to schedule tiny slices of work across a shared pool. This means one H20 could serve several different models simultaneously, with system-wide “goodput” — a measure of effective output — rising by as much as nine times compared to older serverless systems.&lt;/p&gt;
    &lt;p&gt;The system was tested in production over several months, according to the paper, which lists authors from both Peking University and Alibaba’s infrastructure division, including CTO Jingren Zhou. During that window, the number of GPUs needed to support dozens of different LLMs — ranging in size up to 72 billion parameters — fell from 1,192 to just 213.&lt;/p&gt;
    &lt;p&gt;While the paper does not break down which models contributed most to the savings, reporting by the South China Morning Post says the tests were conducted using Nvidia’s H20, one of the few accelerators still legally available to Chinese buyers under current U.S. export controls.&lt;/p&gt;
    &lt;p&gt;Alibaba says the gains came from two main techniques: Packing multiple models per GPU, and using a token-level autoscaler to dynamically allocate compute as output is generated, rather than reserving resources at the request level. In benchmarks, Aegaeon beat the goodput of ServerlessLLM and MuxServe by margins ranging from 1.5 times to 9 times.&lt;/p&gt;
    &lt;p&gt;Whether those savings translate outside Alibaba’s stack remains to be seen. Alibaba Cloud’s paper does not specify the exact network fabric used in the beta test, but we know the company offers its own eRDMA elastic RDMA network and has a record of building highly‑integrated GPU serving stacks, suggesting the results may depend on an optimized, vertically integrated environment.&lt;/p&gt;
    &lt;p&gt;Regardless, the result is likely to attract interest from other hyperscalers looking to stretch scarce accelerator fleets as inference demand continues to spike.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Luke James is a freelance writer and journalist. Although his background is in legal, he has a personal interest in all things tech, especially hardware and microelectronics, and anything regulatory.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;zsydeepsky&lt;/header&gt;Reply&lt;quote/&gt;There are plenty of H20s currently selling on China's used e-market.pug_s said:Does that mean that there will be used H20's in the Chinese market?&lt;lb/&gt;Just checked, half of them have their price dropped(3-19%).&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;phead128&lt;/header&gt;No worries, this means China will spend more on electricity and go bankrupt that way.Reply&lt;lb/&gt;- some think tanker behind the export controls.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/semiconductors/alibaba-says-new-pooling-system-cut-nvidia-gpu-use-by-82-percent"/><published>2025-10-20T12:31:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45644328</id><title>BERT is just a single text diffusion step</title><updated>2025-10-21T14:39:00.753169+00:00</updated><content>&lt;doc fingerprint="35106edd82bc1c52"&gt;
  &lt;main&gt;
    &lt;p&gt;A while back, Google DeepMind unveiled Gemini Diffusion, an experimental language model that generates text using diffusion. Unlike traditional GPT-style models that generate one word at a time, Gemini Diffusion creates whole blocks of text by refining random noise step-by-step.&lt;/p&gt;
    &lt;p&gt;I read the paper Large Language Diffusion Models and was surprised to find that discrete language diffusion is just a generalization of masked language modeling (MLM), something we’ve been doing since 2018. The first thought I had was, “can we finetune a BERT-like model to do text generation?” I decided to try a quick proof of concept out of curiosity.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NOTE: After I wrote the article I stumbled upon the paper DiffusionBERT which does essentially the same thing but with more rigorous testing! Check it out if this post interested you.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;A Short History of Transformers#&lt;/head&gt;
    &lt;p&gt;The original Transformer architecture, introduced in 2017, was an encoder-decoder model. In 2018, researchers realized that the encoder and decoder components of the model could be separated (with the advent of BERT and GPT), and two distinct families of models were created:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Encoder-only models (BERT-style, bidirectional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encoder models used masked language modeling (MLM) as a training objective: randomly mask out a subset of tokens of each input and train the encoder to reconstruct the missing tokens (fill in the blanks). The model sees the entire (partially masked) context at once and learns bidirectional representations. This architecture excelled at tasks requiring a full‐sentence (or paragraph) representation (e.g., classification and retrieval).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Decoder-only models (GPT-style, autoregressive)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Decoder models used next‐token prediction as a training objective: at each position $t$, predict the token at position $t + 1$ given all tokens up to $t$ as context. Only the left context is used to predict future values (unidirectional). This architecture excelled at generative tasks where you produce text one token at a time, such as open‐ended generation, summarization, and translation.&lt;/p&gt;
    &lt;p&gt;Originally, BERT saw immediate use in tasks such as classification, whereas GPT-style models didn’t become popular until later (due to initial limited capabilities). Eventually, the generation capabilities of autoregressive (decoder) transformers vastly improved. The general training objective of “next token prediction” means a much larger space of use cases when compared to encoder models.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discrete Language Diffusion Models#&lt;/head&gt;
    &lt;p&gt;Diffusion models were first popularized in image generation. In image generation, diffusion models gradually add Gaussian noise to an image (forward process) and then train a neural network to iteratively denoise it (reverse process). A high‐level summary of continuous diffusion with images is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Forward process: Start from a clean image x₀, then add small amounts of (usually Gaussian) noise at each timestep until you end up with near‐pure noise.&lt;/item&gt;
      &lt;item&gt;Reverse process: Train a model (often a U‐Net) to predict the noise at each timestep, gradually recovering the original image in discrete denoising steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Applying this idea to language means we need a way to add noise to text and then remove it in stages. The simplest way to do this is a masking‐based noise process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Forward (masking) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;At timestep t = 0, you have a fully uncorrupted text sequence.&lt;/item&gt;
          &lt;item&gt;At each subsequent timestep t &amp;gt; 0, randomly replace a fraction of tokens with a special &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;token according to a pre‐defined schedule (e.g., gradually increasing the masked proportion from 0% to 100%).&lt;/item&gt;
          &lt;item&gt;By the final timestep T, the entire sequence may be masked (all tokens are &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reverse (denoising) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Train a model (often a standard Transformer encoder) to predict the original token IDs given a partially masked sequence at timestep t.&lt;/item&gt;
          &lt;item&gt;This is akin to performing masked language modeling at varying mask rates: at early timesteps, only a few tokens are masked (easy to predict); at later timesteps, many tokens are masked (harder).&lt;/item&gt;
          &lt;item&gt;By chaining together predictions from high‐mask‐rate back down to zero, you can recover (or generate) a full sequence.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this discrete text diffusion framework, the model learns a likelihood bound on the data distribution by optimizing a sum of denoising losses over all timesteps, rather than a single MLM objective at a fixed mask probability.&lt;/p&gt;
    &lt;p&gt;As we can see, BERT’s masked language modeling objective is the same training objective as text diffusion, but just for a subset of masking rates. By introducing variable masking rates (from 0 to 1) and a scheduled sequence of denoising steps (inspired by diffusion theory), we can transform BERT’s masked language modeling objective into a full generative procedure.&lt;/p&gt;
    &lt;head rend="h2"&gt;RoBERTa Diffusion#&lt;/head&gt;
    &lt;p&gt;In 2019, RoBERTa was released. It was essentially just an enhancement of the original BERT model, with better hyperparameters, data training size, and a more simple training objective (MLM only, removed next sentence prediction).&lt;/p&gt;
    &lt;p&gt;Here we use the HuggingFace &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;dataset&lt;/code&gt; libraries to pull in the original RoBERTa weights, tokenizer, and the Trainer class to easily finetune the model on the WikiText dataset.
The main code (full code here) looks like this below:&lt;/p&gt;
    &lt;code&gt;# Load and tokenize dataset and instantiate the model
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")
model = RobertaForMaskedLM.from_pretrained("roberta-base")

# Create the training args and Trainer instance
training_args = TrainingArguments(
    output_dir="finetuned-roberta-diffusion",
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    save_strategy="epoch",
    save_total_limit=1,
    logging_steps=200,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    data_collator=diffusion_collator, # custom implementation
    tokenizer=tokenizer,
)

# Train &amp;amp; save
trainer.train()
trainer.save_model("finetuned-roberta-diffusion")&lt;/code&gt;
    &lt;p&gt;Currently we have 10 diffusion steps, so we randomly sample a percentage $p$ out of &lt;code&gt;mask_probs&lt;/code&gt; (1.0, 0.9, 0.9, &amp;amp;mldr;, 0.1) and mask that percent of the tokens each batch.
The custom &lt;code&gt;diffusion_collator&lt;/code&gt; function (see code here) samples one mask-probability &lt;code&gt;p&lt;/code&gt; from &lt;code&gt;mask_probs&lt;/code&gt; per batch and sets each token to &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; with &lt;code&gt;p&lt;/code&gt; probability.&lt;/p&gt;
    &lt;p&gt;To be able to condition the generation on a “prompt”, we currently never mask the first 16 tokens. That means that during training, each step will always have the first 16 tokens as context for generation.&lt;/p&gt;
    &lt;p&gt;Simplified code for the &lt;code&gt;diffusion_collator&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;  def diffusion_collator(examples):
      batch = tokenizer.pad(examples, return_tensors="pt")

      # Randomly select masking probability for this batch
      mask_prob = random.choice([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])

      # Never mask the first PREFIX_LEN tokens (preserved context)
      maskable_positions = batch.input_ids[:, PREFIX_LEN:]

      # Create random mask for the chosen probability
      mask = torch.rand(maskable_positions.shape) &amp;lt; mask_prob

      # Apply masking
      batch.input_ids[:, PREFIX_LEN:][mask] = tokenizer.mask_token_id
      batch.labels = batch.input_ids.clone()

      return batch&lt;/code&gt;
    &lt;p&gt;For inference, we start with an input which is a tensor of size 256 (since we are generating blocks of 256 tokens). The first 16 positions are the token ids that correspond to the prompt, and the last 240 are just &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens. We iterate through the denoising schedule and each step, we generate a prediction and then remask the sequence again. The process looks like this:&lt;/p&gt;
    &lt;code&gt;Step 0: [PREFIX] &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; ...     (100% masked)
Step 1: [PREFIX] will &amp;lt;mask&amp;gt; over &amp;lt;mask&amp;gt; control ...        (90% masked)
Step 2: [PREFIX] will begin &amp;lt;mask&amp;gt; greater control ...      (80% masked)
...
Step 10: [PREFIX] will begin to assert greater control ...  (0% masked - DONE)&lt;/code&gt;
    &lt;p&gt;Simplified code for generation looks like:&lt;/p&gt;
    &lt;code&gt;# Generate text through iterative denoising
for step, mask_prob in enumerate(mask_probs):
    # Forward pass: predict masked tokens
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = outputs.logits  # shape: (1, MAX_LEN, vocab_size)

    # For each masked position, sample from top-k/top-p filtered distribution
    for pos in range(PREFIX_LEN, MAX_LEN):
        if input_ids[0, pos] == tokenizer.mask_token_id:
            logits = predictions[0, pos, :]
            # Apply top-k and top-p filtering
            filtered_logits = top_k_top_p_filtering(logits, top_k=TOP_K, top_p=TOP_P)
            probs = F.softmax(filtered_logits, dim=-1)
            # Sample token
            sampled_token = torch.multinomial(probs, 1)
            input_ids[0, pos] = sampled_token

    # Re-mask a portion of non-prefix tokens for next iteration
    if mask_prob &amp;gt; 0:
        mask_indices = torch.rand(MAX_LEN - PREFIX_LEN) &amp;lt; mask_prob
        input_ids[0, PREFIX_LEN:][mask_indices] = tokenizer.mask_token_id&lt;/code&gt;
    &lt;p&gt;Here is an example output generation of the fine-tuned model after training on an H200 for 30 minutes (the first line is the initial prompt):&lt;/p&gt;
    &lt;code&gt;Following their victory in the French and Indian War, Britain began to assert
greater...

...dominion over Europe beginning about the early 19th. There conflict took
place on the island, between British and Irish Ireland. British officials 
administered British Ireland, a Celtic empire under the control of the Irish 
nationalist authorities, defined as a dominion of Britain. As the newly Fortic 
states acquired independent and powerful status, many former English colonies
played their part in this new, British @-@ controlled colonial system. Following
this period the Non @-@ Parliamentaryist Party won its influence in Britain in 
1890, led by the support of settlers from the Irish colonies. Looking inwards, 
Sinclair, Lewis questioned, and debated the need to describe " The New Britain "&lt;/code&gt;
    &lt;p&gt;The output looks surprisingly coherent! Most of the quirks present are actually just quirks from the formatting of WikiText (spaces around punctuation &lt;code&gt;"&lt;/code&gt;, turning hyphens &lt;code&gt;-&lt;/code&gt; into &lt;code&gt;@-@&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Below is a comparison between our diffusion model and GPT-2:&lt;/p&gt;
    &lt;p&gt;We see GPT-2’s output is more coherent and slightly faster (~9 seconds vs ~13) but I’m pleasantly surprised with how good my simple implementation was. It is a good proof of concept, and with new approaches like AR-Diffusion and Skip-Step Diffusion (and a more optimized implementation), the quality and speed can be drastically improved.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;We’ve seen that masked language models like RoBERTa, originally designed for fill-in-the-blank tasks, can be repurposed into fully generative engines by interpreting variable-rate masking as a discrete diffusion process. By gradually corrupting text with &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens and training the model to iteratively denoise at increasing mask intensities, we effectively turn the standard MLM objective into a step-by-step generation procedure.&lt;/p&gt;
    &lt;p&gt;Even without architectural changes, a fine-tuned RoBERTa can generate coherent looking text after slightly modifying the training objective, validating the idea that BERT-style models are essentially just text diffusion models trained on one masking rate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nathan.rs/posts/roberta-diffusion/"/><published>2025-10-20T14:31:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45645349</id><title>Production RAG: what I learned from processing 5M+ documents</title><updated>2025-10-21T14:39:00.474837+00:00</updated><content>&lt;doc fingerprint="9bf95f134d9a6771"&gt;
  &lt;main&gt;
    &lt;p&gt;I've spent the last 8 months in the RAG trenches, I want to share what actually worked vs. wasted our time. We built RAG for Usul AI (9M pages) and an unnamed legal AI enterprise (4M pages).&lt;/p&gt;
    &lt;head rend="h2"&gt;Langchain + Llamaindex&lt;/head&gt;
    &lt;p&gt;We started out with youtube tutorials. First Langchain → Llamaindex. Got to a working prototype in a couple of days and were optimistic with the progress. We run tests on subset of the data (100 documents) and the results looked great. We spent the next few days running the pipeline on the production dataset and got everything working in a week — incredible.&lt;/p&gt;
    &lt;p&gt;Except it wasn't, the results were subpar and only the end users could tell. We spent the following few months rewriting pieces of the system, one at a time, until the performance was at the level we wanted. Here are things we did ranked by ROI.&lt;/p&gt;
    &lt;head rend="h2"&gt;What moved the needle&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query Generation: not all context can be captured by the user's last query. We had an LLM review the thread and generate a number of semantic + keyword queries. We processed all of those queries in parallel, and passed them to a reranker. This made us cover a larger surface area and not be dependent on a computed score for hybrid search.&lt;/item&gt;
      &lt;item&gt;Reranking: the highest value 5 lines of code you'll add. The chunk ranking shifted a lot. More than you'd expect. Reranking can many times make up for a bad setup if you pass in enough chunks. We found the ideal reranker set-up to be 50 chunk input -&amp;gt; 15 output.&lt;/item&gt;
      &lt;item&gt;Chunking Strategy: this takes a lot of effort, you'll probably be spending most of your time on it. We built a custom flow for both enterprises, make sure to understand the data, review the chunks, and check that a) chunks are not getting cut mid-word or sentence b) ~each chunk is a logical unit and captures information on its own&lt;/item&gt;
      &lt;item&gt;Metadata to LLM: we started by passing the chunk text to the LLM, we ran an experiment and found that injecting relevant metadata as well (title, author, etc.) improves context and answers by a lot.&lt;/item&gt;
      &lt;item&gt;Query routing: many users asked questions that can't be answered by RAG (e.g. summarize the article, who wrote this). We created a small router that detects these questions and answers them using an API call + LLM instead of the full-blown RAG set-ups.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our stack&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vector database: Azure -&amp;gt; Pinecone -&amp;gt; Turbopuffer (cheap, supports keyword search natively)&lt;/item&gt;
      &lt;item&gt;Document Extraction: Custom&lt;/item&gt;
      &lt;item&gt;Chunking: Unstructured.io by default, custom for enterprises (heard that Chonkie is good)&lt;/item&gt;
      &lt;item&gt;Embedding: text-embedding-large-3, haven't tested others&lt;/item&gt;
      &lt;item&gt;Reranker: None -&amp;gt; Cohere 3.5 -&amp;gt; Zerank (less known but actually good)&lt;/item&gt;
      &lt;item&gt;LLM: GPT 4.1 -&amp;gt; GPT 5 -&amp;gt; GPT 4.1, covered by Azure credits&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Going Open-source&lt;/head&gt;
    &lt;p&gt;We put all our learning into an open-source project: agentset-ai/agentset under an MIT license. Feel free to reach out if you have any questions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.abdellatif.io/production-rag-processing-5m-documents"/><published>2025-10-20T15:55:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45647166</id><title>Claude Code on the web</title><updated>2025-10-21T14:39:00.109092+00:00</updated><content>&lt;doc fingerprint="7f0c0efeffb1c2ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Claude Code on the web&lt;/head&gt;
    &lt;p&gt;Today, we're introducing Claude Code on the web, a new way to delegate coding tasks directly from your browser.&lt;/p&gt;
    &lt;p&gt;Now in beta as a research preview, you can assign multiple coding tasks to Claude that run on Anthropic-managed cloud infrastructure, perfect for tackling bug backlogs, routine fixes, or parallel development work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run coding tasks in parallel&lt;/head&gt;
    &lt;p&gt;Claude Code on the web lets you kick off coding sessions without opening your terminal. Connect your GitHub repositories, describe what you need, and Claude handles the implementation.&lt;/p&gt;
    &lt;p&gt;Each session runs in its own isolated environment with real-time progress tracking, and you can actively steer Claude to adjust course as it’s working through tasks.&lt;/p&gt;
    &lt;p&gt;With Claude Code running in the cloud, you can now run multiple tasks in parallel across different repositories from a single interface and ship faster with automatic PR creation and clear change summaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;Flexible for every workflow&lt;/head&gt;
    &lt;p&gt;The web interface complements your existing Claude Code workflow. Running tasks in the cloud is especially effective for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answering questions about how projects work and how repositories are mapped&lt;/item&gt;
      &lt;item&gt;Bugfixes and routine, well-defined tasks&lt;/item&gt;
      &lt;item&gt;Backend changes, where Claude Code can use test-driven development to verify changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also use Claude Code on mobile. As part of this research preview, we’re making Claude Code available on our iOS app so developers can explore coding with Claude on the go. It’s an early preview, and we hope to quickly refine the mobile experience based on your feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security-first cloud execution&lt;/head&gt;
    &lt;p&gt;Every Claude Code task runs in an isolated sandbox environment with network and filesystem restrictions. Git interactions are handled through a secure proxy service that ensures Claude can only access authorized repositories—helping keep your code and credentials protected throughout the entire workflow.&lt;/p&gt;
    &lt;p&gt;You can also add custom network configuration to choose what domains Claude Code can connect to from its sandbox. For example, you can allow Claude to download npm packages over the internet so that it can run tests and validate changes.&lt;/p&gt;
    &lt;p&gt;Read our engineering blog and documentation for a deep dive on Claude Code’s sandboxing approach.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;Claude Code on the web is available now in research preview for Pro and Max users. Visit claude.com/code to connect your first repository and start delegating tasks.&lt;/p&gt;
    &lt;p&gt;Cloud-based sessions share rate limits with all other Claude Code usage. Explore our documentation to learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-code-on-the-web"/><published>2025-10-20T18:12:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45649178</id><title>Today is when the Amazon brain drain sent AWS down the spout</title><updated>2025-10-21T14:38:59.862098+00:00</updated><content>&lt;doc fingerprint="3d28b35914248ecf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Today is when the Amazon brain drain finally sent AWS down the spout&lt;/head&gt;
    &lt;head rend="h2"&gt;When your best engineers log off for good, don’t be surprised when the cloud forgets how DNS works&lt;/head&gt;
    &lt;p&gt;column "It's always DNS" is a long-standing sysadmin saw, and with good reason: a disproportionate number of outages are at their heart DNS issues. And so today, as AWS is still repairing its downed cloud as this article goes to press, it becomes clear that the culprit is once again DNS. But if you or I know this, AWS certainly does.&lt;/p&gt;
    &lt;p&gt;And so, a quiet suspicion starts to circulate: where have the senior AWS engineers who've been to this dance before gone? And the answer increasingly is that they've left the building — taking decades of hard-won institutional knowledge about how AWS's systems work at scale right along with them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What happened?&lt;/head&gt;
    &lt;p&gt;AWS reports that on October 20, at 12:11 AM PDT, it began investigating “increased error rates and latencies for multiple AWS services in the US-EAST-1 Region.” About an hour later, at 1:26 AM, the company confirmed “significant error rates for requests made to the DynamoDB endpoint” in that region. By 2:01 AM, engineers had identified DNS resolution of the DynamoDB API endpoint for US-EAST-1 as the likely root cause, which led to cascading failures for most other things in that region. DynamoDB is a "foundational service" upon which a whole mess of other AWS services rely, so the blast radius for an outage touching this thing can be huge.&lt;/p&gt;
    &lt;p&gt;As a result, much of the internet stopped working: banking, gaming, social media, government services, buying things I don't need on Amazon.com itself, etc.&lt;/p&gt;
    &lt;p&gt;AWS has given increasing levels of detail, as is their tradition, when outages strike, and as new information comes to light. Reading through it, one really gets the sense that it took them 75 minutes to go from "things are breaking" to "we've narrowed it down to a single service endpoint, but are still researching," which is something of a bitter pill to swallow. To be clear: I've seen zero signs that this stems from a lack of transparency, and every indication that they legitimately did not know what was breaking for a patently absurd length of time.&lt;/p&gt;
    &lt;p&gt;Note that for those 75 minutes, visitors to the AWS status page (reasonably wondering why their websites and other workloads had just burned down and crashed into the sea) were met with an "all is well!" default response. Ah well, it's not as if AWS had previously called out slow outage notification times as an area for improvement. Multiple times even. We can keep doing this if you'd like.&lt;/p&gt;
    &lt;head rend="h3"&gt;The prophecy&lt;/head&gt;
    &lt;p&gt;AWS is very, very good at infrastructure. You can tell this is a true statement by the fact that a single one of their 38 regions going down (albeit a very important region!) causes this kind of attention, as opposed to it being "just another Monday outage." At AWS's scale, all of their issues are complex; this isn't going to be a simple issue that someone should have caught, just because they've already hit similar issues years ago and ironed out the kinks in their resilience story.&lt;/p&gt;
    &lt;p&gt;Once you reach a certain point of scale, there are no simple problems left. What's more concerning to me is the way it seems AWS has been flailing all day trying to run this one to ground. Suddenly, I'm reminded of something I had tried very hard to forget.&lt;/p&gt;
    &lt;p&gt;At the end of 2023, Justin Garrison left AWS and roasted them on his way out the door. He stated that AWS had seen an increase in Large Scale Events (or LSEs), and predicted significant outages in 2024. It would seem that he discounted the power of inertia, but the pace of senior AWS departures certainly hasn't slowed — and now, with an outage like this, one is forced to wonder whether those departures are themselves a contributing factor.&lt;/p&gt;
    &lt;p&gt;You can hire a bunch of very smart people who will explain how DNS works at a deep technical level (or you can hire me, who will incorrect you by explaining that it's a database), but the one thing you can't hire for is the person who remembers that when DNS starts getting wonky, check that seemingly unrelated system in the corner, because it has historically played a contributing role to some outages of yesteryear.&lt;/p&gt;
    &lt;p&gt;When that tribal knowledge departs, you're left having to reinvent an awful lot of in-house expertise that didn't want to participate in your RTO games, or play Layoff Roulette yet again this cycle. This doesn't impact your service reliability — until one day it very much does, in spectacular fashion. I suspect that day is today.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AWS outage exposes Achilles heel: central control plane&lt;/item&gt;
      &lt;item&gt;Major AWS outage across US-East region breaks half the internet&lt;/item&gt;
      &lt;item&gt;Amazon spills plan to nuke Washington...with X-Energy mini-reactors&lt;/item&gt;
      &lt;item&gt;Amazon turns James Bond into the Man Without the Golden Gun&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The talent drain evidence&lt;/head&gt;
    &lt;p&gt;This is The Register, a respected journalistic outlet. As a result, I know that if I publish this piece as it stands now, an AWS PR flak will appear as if by magic, waving their hands, insisting that "there is no talent exodus at AWS," a la Baghdad Bob. Therefore, let me forestall that time-wasting enterprise with some data.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is a fact that there have been 27,000+ Amazonians impacted by layoffs between 2022 and 2024, continuing into 2025. It's hard to know how many of these were AWS versus other parts of its Amazon parent, because the company is notoriously tight-lipped about staffing issues.&lt;/item&gt;
      &lt;item&gt;Internal documents reportedly say that Amazon suffers from 69 percent to 81 percent regretted attrition across all employment levels. In other words, "people quitting who we wish didn't."&lt;/item&gt;
      &lt;item&gt;The internet is full of anecdata of senior Amazonians lamenting the hamfisted approach of their Return to Office initiative; experts have weighed in citing similar concerns.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you were one of the early employees who built these systems, the world is your oyster. There's little reason to remain at a company that increasingly demonstrates apparent disdain for your expertise.&lt;/p&gt;
    &lt;head rend="h3"&gt;My take&lt;/head&gt;
    &lt;p&gt;This is a tipping point moment. Increasingly, it seems that the talent who understood the deep failure modes is gone. The new, leaner, presumably less expensive teams lack the institutional knowledge needed to, if not prevent these outages in the first place, significantly reduce the time to detection and recovery. Remember, there was a time when Amazon's "Frugality" leadership principle meant doing more with less, not doing everything with basically nothing. AWS's operational strength was built on redundant, experienced people, and when you cut to the bone, basic things start breaking.&lt;/p&gt;
    &lt;p&gt;I want to be very clear on one last point. This isn't about the technology being old. It's about the people maintaining it being new. If I had to guess what happens next, the market will forgive AWS this time, but the pattern will continue.&lt;/p&gt;
    &lt;p&gt;AWS will almost certainly say this was an "isolated incident," but when you've hollowed out your engineering ranks, every incident becomes more likely. The next outage is already brewing. It's just a matter of which understaffed team trips over which edge case first, because the chickens are coming home to roost. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/"/><published>2025-10-20T20:50:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45652859</id><title>Practical Scheme</title><updated>2025-10-21T14:38:59.566894+00:00</updated><content>&lt;doc fingerprint="c343dacf9eb15a95"&gt;
  &lt;main&gt;
    &lt;p&gt;This page is a collection of libraries and extensions to use Scheme as a production tool. By "production tools" I mean the tools to process daily chores for systems engineers and programmers---parsing files, generate reports, watching processes, providing small GUI wrappers, and all sorts of those things. Currently I'm using Perl for those purpose, but I'm always longing to use Scheme for them. So I started this page.&lt;/p&gt;
    &lt;p&gt;Most stuffs in this site are done as my private project at home, except the ones explicitly stated otherwise. I upload libraries even in its alpha/beta stage, since I'd like to test and use them at work, too. In a way, my primary interest is to make my life happier. No warranty comes with them, as usual, but it'll be nice if somebody else finds they are useful.&lt;/p&gt;
    &lt;p&gt;If you can read Japanese, visit the Japanese page which contains some translations of Lisp/Scheme related articles.&lt;/p&gt;
    &lt;p&gt;I wrote a Wiki Clone in Scheme (Gauche). Come and try it: WiLiKi.&lt;/p&gt;
    &lt;head rend="h2"&gt;Applications and tools&lt;/head&gt;
    &lt;p&gt;Scheme-related stand alone programs.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Gauche - Current version 0.9.15 (2024/04/24)&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;An R7RS Scheme implementation aimed at a handy script engine. Quick startup, built-in system interface, and native multilingual support are some of the goals.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;WiLiKi - Current version 0.6.2 (2014/11/28)&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;A wiki engine written in Scheme.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;Chaton - Current version&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;A Comet-based Webchat system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;escm - Current version 1.1 (2014/11/28)&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;A filter program which copies the input text to output, with processing embedded Scheme expressions. This program itself is independent from any Scheme implementation; you can use your favorite one. Useful to process text files with a bit of dynamic parts. This page itself is processed by escm to embed information such as the update time of libraries, and synchronize with Japanese version. A complete new version of escm, named aescm, is being developed by TAGA Yoshitaka ( http://sourceforge.net/projects/escm/)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Libraries and Extensions&lt;/head&gt;
    &lt;p&gt;The following libraries and extensions are written for Gauche. See here for libraries written for STk.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Gauche-gl - Download Document Current version 0.6 (2014/08/09)&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;OpenGL binding for Gauche. Supports most of OpenGL 1.0 to 4.1 APIs (including OpenGL Shading Language API), and some of GLU and GLUT API. Requires Gauche 0.9.4 or later.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;Gauche-gtk2 - Download Document Current version 0.6.1 (2022/3/20)&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;GTK2 binding for Gauche.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Documents&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Scheme Cross Reference&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;A cross reference of library procedures of various Scheme implementations. Updated constantly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;Shooting A Moving Target--- An Experience In Developing A Production Tracking Database&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;An application of CommonLisp in practice. (yeah, it's not Scheme... anyway, I put it here).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;Tracking Assets in the Production of 'Final Fantasy : The Spirits Within'&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;A follow-up of the article above, a kind of post-mortem of the production.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;Gluing Things Together - Scheme in the Real-time CG Content Production&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;A paper presented at International Lisp Conference 2002 at San Francisco, October 2002. (there's also a pdf version).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-5"&gt;Efficient floating-point number handling for dynamically typed scripting languages (pdf)&lt;/item&gt;
      &lt;item rend="dd-5"&gt;
        &lt;p&gt;A paper presented at Dynamic Language Symposium 2008.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-6"&gt;Schemer's Way&lt;/item&gt;
      &lt;item rend="dd-6"&gt;
        &lt;p&gt;Trying to explain Scheme's merits to non-Scheme programmers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Other Resources&lt;/head&gt;
    &lt;p&gt;This list no way covers everything, but you can follow links in those links.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Schemers.org&lt;/item&gt;
      &lt;item rend="dd-1"&gt;A good anchor point to collect information of Scheme. You can get R*RS, the language standard. The site is also a center of SRFI's--- Scheme Request For Implementation---which provides common interface of libraries across various implementations.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;SCM&lt;/item&gt;
      &lt;item rend="dd-2"&gt;A compact, fast and portable implementation of Scheme interpreter.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;SLIB&lt;/item&gt;
      &lt;item rend="dd-3"&gt;A large collection of portable Scheme libraries. The contents spans from small utilities complements the standard conformance, to the full-featured relational database.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Programming Languages by Dai Inukai&lt;/item&gt;
      &lt;item rend="dd-4"&gt;Scheme-related documents by Dai Inukai, the author of "Nyuumon Scheme (Scheme Primer)" in Japan. Check this out if you're interested in processing Japanese in Scheme.&lt;/item&gt;
      &lt;item rend="dt-5"&gt;Bigloo&lt;/item&gt;
      &lt;item rend="dd-5"&gt;A scheme system with compiler and integrated development environment. If you're planning to write an enterprise software rather than just a bunch of scripts, look at it.&lt;/item&gt;
      &lt;item rend="dt-6"&gt;Guile&lt;/item&gt;
      &lt;item rend="dd-6"&gt;GNU adopted Scheme for the base of extension language several years ago. The effort became Guile. If you have one of popular Linux distributions, you may already have it.&lt;/item&gt;
      &lt;item rend="dt-7"&gt;scsh&lt;/item&gt;
      &lt;item rend="dd-7"&gt;I haven't used this one much, but looks good if you're looking for a tool to do syste programming.&lt;/item&gt;
      &lt;item rend="dt-8"&gt;The Internet Scheme Repository&lt;/item&gt;
      &lt;item rend="dd-8"&gt;As the name suggests.&lt;/item&gt;
      &lt;item rend="dt-9"&gt;Kawa - the Java-based Scheme System&lt;/item&gt;
      &lt;item rend="dd-9"&gt;A Scheme environment written in Java by Per Bothner. Scheme code is compiled to Java bytecode, hence has the property "write once run everywhere".&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://practical-scheme.net/index.html#docs"/><published>2025-10-21T05:47:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45653143</id><title>Language Support for Marginalia Search</title><updated>2025-10-21T14:38:59.048671+00:00</updated><content>&lt;doc fingerprint="f210038521b88b84"&gt;
  &lt;main&gt;
    &lt;p&gt;One of the big ambitions for the search engine this year has been to enable searching in more languages than English, and a pilot project for this has just been completed, allowing experimental support for German, French and Swedish.&lt;/p&gt;
    &lt;p&gt;These changes are now live for testing, but with an extremely small corpus of documents.&lt;/p&gt;
    &lt;p&gt;As the search engine has been up to this point built with English in mind, some anglo-centric assumptions made it into its code. A lot of the research on search engines generally seems to embed similar assumptions.&lt;/p&gt;
    &lt;p&gt;As this is a domain rife with unknown unknowns, the ambition for this pilot was to implement support for just a few additional languages in order to get a feel for how much work would be required to support more languages in general, as well as to assess how much the index grows when this is done.&lt;/p&gt;
    &lt;p&gt;Though it was fully understood upfront that supporting all languages in one go is unrealistic, as some languages are more different than others and require significant additional work. Human language is surprisingly disparate.&lt;/p&gt;
    &lt;p&gt;A language like Japanese, for example, has not only multiple alphabets, but embeds character width in unicode; on top of that the language doesn’t put spaces between words. As such the language requires special normalization.&lt;/p&gt;
    &lt;p&gt;Latin, on the other hand, has dozens of forms for each word, and the words can often be reordered without significantly changing the meaning of a sentence. On the one hand this makes the grammatical analysis of the language somewhat easier since the words announce their function in the sentence fairly unambiguously, but on the other you probably need to store the text in a lemmatized form, and then strongly de-prioritize word order when matching.&lt;/p&gt;
    &lt;p&gt;Google’s bungled handling of Russian was supposedly why Yandex was able to eke out a foothold in that market.&lt;/p&gt;
    &lt;head rend="h2"&gt;What needs changing&lt;/head&gt;
    &lt;p&gt;The search engine’s language processing chain is fairly long, but the most salient parts go something like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Text is extracted from the HTML&lt;/item&gt;
      &lt;item&gt;Language is identified using fasttext&lt;/item&gt;
      &lt;item&gt;Text is broken into sentences&lt;/item&gt;
      &lt;item&gt;Words are lowercased and Unicode is normalized&lt;/item&gt;
      &lt;item&gt;Sentences are stemmed and POS-tagged&lt;/item&gt;
      &lt;item&gt;Sentences, with stemming and POS-tag data is fed into keyword extraction algorithms&lt;list rend="ul"&gt;&lt;item&gt;Keywords are mapped to positions and HTML tags&lt;/item&gt;&lt;item&gt;Important keywords are identified using TF-IDF (using stemmed forms)&lt;/item&gt;&lt;item&gt;Important keywords are identified using grammar patterns (POS-tags)&lt;/item&gt;&lt;item&gt;Important keywords are identified using other heuristics&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Keywords are hashed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Stemming is an imperfect way of getting a base form of a word, though generally such algorithms have a great number of flaws, so that e.g. universe and university seem to be the same word. This is only used in tf-idf calculations.&lt;/p&gt;
    &lt;p&gt;Part-of-Speech (POS) tagging is a grammatical annotation process where the role of each word is as best possible identified. This helps identify named entities, subjects, and so on.&lt;/p&gt;
    &lt;p&gt;Both of these processes needless to say require some awareness of the language being acted upon.&lt;/p&gt;
    &lt;p&gt;These “important keywords” are used to assign documents to a special index that helps with recall by ensuring these documents are included in the set that is ranked before the execution timer runs out. This is not strictly necessary, and in some cases such as where POS-tagging is not possible, can be disabled, partially or as a whole.&lt;/p&gt;
    &lt;p&gt;The normalization step is subject to cultural differences that do not translate. In English you’d probably expect to find the metal band TrÃ¶jan, typing “trojan”. In Swedish these are different letters entirely that should not match, the former means “the shirt”, the latter “trojan” in the Homeric or IT-security sense. Though a Swedish person would likely also say that they should be able to find mÃ¼(e)sli with the keyword “musli”, but a German-speaker would disagree and say that u and Ã¼ are clearly not the same.&lt;/p&gt;
    &lt;p&gt;There also exists a bootstrapping problem, as the statistical model used to calculate TF-IDF is based on documents in the index. Since almost all of the documents in the index up until this point have been in English, term frequencies for the newly added languages are missing. This breaks TF-IDF, as used in identifying important keywords, until a new model can be constructed. Thankfully the BM-25 model used in ranking is robust to this, as it relies on live data from the index itself.&lt;/p&gt;
    &lt;p&gt;The basic approach to parametrize language handling selected was to inject a language definition object, from which language appropriate logic is accessible.&lt;/p&gt;
    &lt;p&gt;This is configurable via XML. Here XML was chosen because it arguably has the best built-in validation support, making it a fantastic use case for a self-contained configuration file like this one, where late validation would be very annoying to deal with.&lt;/p&gt;
    &lt;p&gt;Much of the configuration file consists of various grammatical patterns used to identify important keywords based on the role of a word in a sentence.&lt;/p&gt;
    &lt;code&gt;&amp;lt;ngrams type="noun"&amp;gt;
    &amp;lt;pospattern&amp;gt;VBG&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;RB VBG&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NNP* JJ)&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NN* JJ) NN*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NN* JJ) (NN* JJ) NN*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NN* JJ) (NN* JJ) (NN* JJ) NN*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NNP* JJ) (NNP* IN TO CC) NNP*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NNP* JJ) (NNP* IN TO CC) DT NNP*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NNP* JJ) (NNP* IN TO CC) (NNP* IN TO CC) NNP*&amp;lt;/pospattern&amp;gt;
&amp;lt;/ngrams&amp;gt;
&lt;/code&gt;
    &lt;p&gt;An expression like &lt;code&gt;(NN* JJ) (NN* JJ) NN*&lt;/code&gt; is interpreted as&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Any tag starting with &lt;code&gt;NN&lt;/code&gt;, or the tag&lt;code&gt;JJ&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Any tag starting with &lt;code&gt;NN&lt;/code&gt;, or the tag&lt;code&gt;JJ&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Any tag starting with &lt;code&gt;NN&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Previously these patterns were hard coded, and finding a performant alternative implementation took some effort. A bit mask approach was selected, as it allows for some very basic bit-level concurrency that drastically reduces the number of branches needed.&lt;/p&gt;
    &lt;p&gt;As far as grammatical analysis goes, the approach used by the search engine is pretty medieval, but it does do a fairly good job at what it sets out to do, and as a result, one thing it is generally pretty good at is finding websites about some topic.&lt;/p&gt;
    &lt;p&gt;In some ways the imperfections introduced by the old-fashioned way of approaching language processing is almost helpful in bringing in more relevant results, as they tend to capture more variations of the words related to the topic of the document.&lt;/p&gt;
    &lt;p&gt;There are more places that need minor language dependent behavior changes that are glossed over here, both in the language processing pipeline discussed above, and in the query parser, though in the interest of keeping this update from becoming an overly verbose git diff, these will be glossed over.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tooling&lt;/head&gt;
    &lt;p&gt;To help make sense of this, a test tool was built that runs the language processing pipeline in isolation, and outputs annotated intermediate results for human inspection.&lt;/p&gt;
    &lt;p&gt;Work in this domain poses special problems that all but demand human testing. Machine testing can be good for catching regressions or getting access to some code for easier debugging, but natural language has so many nuances that any test suite is woefully inadequate compared to a pair of human eyeballs.&lt;/p&gt;
    &lt;p&gt;It has already helped refine the algorithms used to identify important keywords in English, which wasn’t the intent of building the tool, but its immediate consequence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Integration&lt;/head&gt;
    &lt;p&gt;Integrating the new multi-language search data into the system poses some design considerations.&lt;/p&gt;
    &lt;p&gt;One option would be to stick everything in one big index, and then filter results based on language during or after ranking. The strength of this is that it becomes possible to search in any language without specifying it upfront.&lt;/p&gt;
    &lt;p&gt;The drawbacks of the one-index approach is that it grows the index, which makes all queries slower; it also grows the number of keywords in the lexicon, which is something that we generally want to avoid.&lt;/p&gt;
    &lt;p&gt;The way the search engine handles mapping keywords to numeric ids is to use a hash algorithm. Not a hash table, but the output of the hash algorithm itself. This seems absolutely unhinged at first glance, but works remarkably well as long as the lexicon stays small enough.&lt;/p&gt;
    &lt;p&gt;Hash collisions do happen on rare occasions, but they need to happen between words where the words actually appear in the same documents to be a problem, generally leading to the ranking algorithm having to trudge through irrelevant documents and performing worse as a result of wasting its time budget.&lt;/p&gt;
    &lt;p&gt;Massively expanding the lexicon like we would if we were to mingle the documents increases the likelihood there will be an actual problem arising from these rare false positives.&lt;/p&gt;
    &lt;p&gt;If we stick every keyword from every language in the same index, a different problem arises, namely that homophones exist across different languages, meaning that the index lookup needs to wade through irrelevant documents that are trivially unrelated to the query.&lt;/p&gt;
    &lt;p&gt;The words &lt;code&gt;salt&lt;/code&gt; and &lt;code&gt;lag&lt;/code&gt;, if they appear in the same document in English likely selects documents relating to esports, whereas in Swedish they select for documents relating to food preservation.&lt;/p&gt;
    &lt;p&gt;The alternative option is to separate the indexes.&lt;/p&gt;
    &lt;p&gt;The drawback here is that you must specify the language upfront, and querying in all languages becomes very expensive, as it executing multiple queries, though the desired language of the search results are generally known beforehand so this is a relatively small concern that, at best, affects a small number of machine-access use cases.&lt;/p&gt;
    &lt;p&gt;Since it has far fewer problems, and promises to be faster and more accurate, this approach was selected.&lt;/p&gt;
    &lt;p&gt;In practice this was implemented as language-specific keyword-document mappings, that point into a common file containing document lists.&lt;/p&gt;
    &lt;p&gt;Initially the indexes were constructed from a common journal file, which was consumed repeatedly, but this turned out to be slow, and a partitioned approach was selected instead, with one journal per language. This almost completely removes any overhead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outcome&lt;/head&gt;
    &lt;p&gt;The changes discussed above have been implemented, and upon evaluation seems to work reasonably well, though evaluation has somewhat run into a dead end, as the index itself is extremely small for the newly added languages.&lt;/p&gt;
    &lt;p&gt;The experience of small index is devious as it may just mean poor recall, though looking at the documents database for one index partition, this is about 12% of the index, it really is quite small!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;iso&lt;/cell&gt;
        &lt;cell role="head"&gt;document count&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;en&lt;/cell&gt;
        &lt;cell&gt;112,846,397&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;de&lt;/cell&gt;
        &lt;cell&gt;7,623,983&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fr&lt;/cell&gt;
        &lt;cell&gt;4,852,759&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;sv&lt;/cell&gt;
        &lt;cell&gt;1,020,962&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To verify this is not due some silent, catastrophic processing error, the proportions were compared against the number of documents found in the 50 GB document sample used in testing, using a simplified process that only does language identification.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;iso&lt;/cell&gt;
        &lt;cell role="head"&gt;document count&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;en&lt;/cell&gt;
        &lt;cell&gt;11,497,571&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;de&lt;/cell&gt;
        &lt;cell&gt;614,311&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fr&lt;/cell&gt;
        &lt;cell&gt;409,877&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;es&lt;/cell&gt;
        &lt;cell&gt;267,408&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ja&lt;/cell&gt;
        &lt;cell&gt;217,599&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nl&lt;/cell&gt;
        &lt;cell&gt;196,130&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&amp;amp;mldr;&lt;/cell&gt;
        &lt;cell&gt;&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;sv&lt;/cell&gt;
        &lt;cell&gt;67,670&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The proportions aren’t identical, but in the same general ballpark. The small size of the sample, along with the uneven distribution and apparent rarity of these documents adequately explains the disparity.&lt;/p&gt;
    &lt;p&gt;The lack of documents in languages other than English is likely due to how the index has been grown, by following and adding links from English websites. These occasionally lead to bilingual websites, and on rare occasions to websites completely in a different language, though it seems reasonable most websites that are not at least partially in English sees few or no links from English-language websites.&lt;/p&gt;
    &lt;p&gt;Adding to the problem, up until fairly recently the index wasn’t really growing very much at all, only through manual submissions.&lt;/p&gt;
    &lt;p&gt;Beyond a certain point, meaningfully growing the index by just following links became difficult.&lt;/p&gt;
    &lt;p&gt;Most known domains are dead, so merely adding more domains to the list of websites to crawl only serves to pollute the database with junk data.&lt;/p&gt;
    &lt;p&gt;In order to get around this, and reach the goal of indexing a billion documents, a new process was built to visit candidate websites to verify that they are in fact real and on-line, before assigning them to an index partition.&lt;/p&gt;
    &lt;p&gt;The process has been running for almost a quarter, and has managed to identify about 800,000 viable new domains in that time window. (This has brought the document total up to 969M documents. So very nearly there now!)&lt;/p&gt;
    &lt;p&gt;Web search is unusual in how often you run into these extremely long running processes that need to cook for months, sometimes up to a year before they really begin to pay off.&lt;/p&gt;
    &lt;p&gt;We’ll have to see whether building this new process was so prescient it ends up being sufficient to identify and add new domains in more languages, as links from the newly processed Swedish, French and German websites have been added to the domain database, or if some sort of manual seeding or targeted selection process is needed.&lt;/p&gt;
    &lt;p&gt;It seems plausible it will at least begin to remedy the data starvation, as the rate of successful domain discovery has shot up significantly since processing links from the documents processed in the newly added languages, and many of the new domains are indeed from &lt;code&gt;.de&lt;/code&gt;, &lt;code&gt;.se&lt;/code&gt;, &lt;code&gt;.fr&lt;/code&gt;, and &lt;code&gt;.ch&lt;/code&gt; domains.&lt;/p&gt;
    &lt;p&gt;For now we’ll have to wait and see how the data-set evolves. It is difficult to further refine the multi-language aspect of the search data with a data-set this small.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.marginalia.nu/log/a_126_multilingual/"/><published>2025-10-21T06:48:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45653330</id><title>Pasta/80 is a simple Pascal cross compiler targeting the Z80 microprocessor</title><updated>2025-10-21T14:38:58.325810+00:00</updated><content>&lt;doc fingerprint="a6f5f818a2971b8d"&gt;
  &lt;main&gt;
    &lt;p&gt;PASTA/80 is a simple Pascal cross compiler targeting the Z80 microprocessor. It generates code for these classic and modern machines:&lt;/p&gt;
    &lt;p&gt;The compiler follows the single-pass recursive-descent approach championed by Niklaus Wirth, inventor of Pascal, in his books and lectures. It doesn't have an explicit syntax tree, but instead generates code on the fly during parsing. As a result, the compiler might not always generate the most efficient code possible (it definitely cannot compete with LLVM and doesn't try to), but it's very fast.&lt;/p&gt;
    &lt;p&gt;The supported Pascal dialect is an almost exact clone of the original Turbo Pascal 3.0 for CP/M (see this manual for details). So you have at your disposal the following language elements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All the basic data types (&lt;code&gt;Boolean&lt;/code&gt;,&lt;code&gt;Byte&lt;/code&gt;,&lt;code&gt;Char&lt;/code&gt;,&lt;code&gt;Integer&lt;/code&gt;,&lt;code&gt;Pointer&lt;/code&gt;,&lt;code&gt;Real&lt;/code&gt;and&lt;code&gt;String&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;array of&lt;/code&gt;,&lt;code&gt;record&lt;/code&gt;,&lt;code&gt;set of&lt;/code&gt;, enumerations, subranges and pointers as a way of building new data types.&lt;/item&gt;
      &lt;item&gt;The decision-making elements &lt;code&gt;if..then..else&lt;/code&gt;and&lt;code&gt;case..of&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The loop elements &lt;code&gt;for..do&lt;/code&gt;,&lt;code&gt;while..do&lt;/code&gt;and&lt;code&gt;repeat..until&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;with..do&lt;/code&gt;notation for "opening" records.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;procedure&lt;/code&gt;and&lt;code&gt;function&lt;/code&gt;including value and&lt;code&gt;var&lt;/code&gt;parameters and nesting.&lt;/item&gt;
      &lt;item&gt;The standard procedures for screen input and output (i.e. &lt;code&gt;ReadLn&lt;/code&gt;,&lt;code&gt;WriteLn&lt;/code&gt;etc.).&lt;/item&gt;
      &lt;item&gt;All conversion and utility procedures and functions that Turbo Pascal 3.0 had.&lt;/item&gt;
      &lt;item&gt;The three kinds of disk files, that is untyped (&lt;code&gt;file&lt;/code&gt;), typed (&lt;code&gt;file of&lt;/code&gt;) and&lt;code&gt;Text&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A dynamic heap of up to 32767 bytes with &lt;code&gt;GetMem&lt;/code&gt;,&lt;code&gt;FreeMem&lt;/code&gt;,&lt;code&gt;New&lt;/code&gt;and&lt;code&gt;Dispose&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Inline assembly (via opcodes, not via mnemonics, so this page might be handy).&lt;/item&gt;
      &lt;item&gt;Overlays (in memory, Spectrum 128K and Next only, see below).&lt;/item&gt;
      &lt;item&gt;Some compiler directives: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;$i &amp;lt;file&amp;gt;&lt;/code&gt;for including Pascal source files (including nesting and cycle detection)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$l &amp;lt;file&amp;gt;&lt;/code&gt;for including an assembly file (aka "linking" a library)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$a(+/-)&lt;/code&gt;for enabling or disabling absolute mode (default is on, disable for recursion)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$i(+/-)&lt;/code&gt;for enabling or disabling IO checking (when off, check&lt;code&gt;IOResult&lt;/code&gt;after calls)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$k(+/-)&lt;/code&gt;for enabling or disabling stack overflow checking&lt;/item&gt;&lt;item&gt;&lt;code&gt;$u(+/-)&lt;/code&gt;for enabling or disabling Ctrl-C checking&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The compiler also has some features that were borrowed from or inspired by later versions of Turbo Pascal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C-style &lt;code&gt;//&lt;/code&gt;one-line comments in addition to&lt;code&gt;{..}&lt;/code&gt;and&lt;code&gt;(*..*)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Binary literals (using a &lt;code&gt;%&lt;/code&gt;prefix).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Break&lt;/code&gt;and&lt;code&gt;Continue&lt;/code&gt;for loop control.&lt;/item&gt;
      &lt;item&gt;Querying the keyboard via &lt;code&gt;KeyPressed&lt;/code&gt;and&lt;code&gt;ReadKey&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Color support via &lt;code&gt;TextColor&lt;/code&gt;and&lt;code&gt;TextBackground&lt;/code&gt;with constants for the 8 Spectrum Next colors.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Inc&lt;/code&gt;and&lt;code&gt;Dec&lt;/code&gt;for more efficient increasing and decreasing of variables.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Include&lt;/code&gt;and&lt;code&gt;Exclude&lt;/code&gt;for more efficient handling of sets.&lt;/item&gt;
      &lt;item&gt;A simple &lt;code&gt;Assert&lt;/code&gt;facility that counts passes/fails and shows the failed line number.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since that covers most of the functionality of Turbo Pascal 3 you might ask what is missing. These are the current limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All the remaining compiler directives are not yet supported.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Mark&lt;/code&gt;/&lt;code&gt;Release&lt;/code&gt;are not currently supported.&lt;/item&gt;
      &lt;item&gt;The standard files &lt;code&gt;Input&lt;/code&gt;,&lt;code&gt;Output&lt;/code&gt;,&lt;code&gt;Kbd&lt;/code&gt;,&lt;code&gt;Con&lt;/code&gt;and&lt;code&gt;Lst&lt;/code&gt;are not supported.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Chain&lt;/code&gt;and&lt;code&gt;Execute&lt;/code&gt;are not supported.&lt;/item&gt;
      &lt;item&gt;Add-on libraries from the PC version of Turbo Pascal 3.0 are not yet supported (although there are a few graphics primitives for the ZX targets).&lt;/item&gt;
      &lt;item&gt;The new instructions of the Z80N CPU inside the ZX Spectrum Next are not yet being leveraged.&lt;/item&gt;
      &lt;item&gt;No separate compilation. Everything is compiled from source, always.&lt;/item&gt;
      &lt;item&gt;Binary size is quite large compared to the original.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The runtime library, being partially written in Pascal itself, gets quite large when compiled. I hope to bring this down again by reimplementing more of it in Z80 assembly (or improve the code generator, which, although it has a peephole optimizer, is not generating super-efficient Z80 code).&lt;/p&gt;
    &lt;p&gt;The compiler is itself written in Pascal. You can compile it with Free Pascal (I use version 3.2.2). Just run&lt;/p&gt;
    &lt;code&gt;$ fpc pasta&lt;/code&gt;
    &lt;p&gt;The Pascal compiler generates Z80 assembler code and relies on sjasmplus as a backend for the final translation step to binary. It can also, in &lt;code&gt;--ide&lt;/code&gt; mode (see below), make use of various other external tools. The compiler tries to detect these external tools automatically (from your system's &lt;code&gt;PATH&lt;/code&gt;), but sometimes it's best to create a file &lt;code&gt;.pasta80.cfg&lt;/code&gt; in your home directory specifying necessary paths (there is a sample in &lt;code&gt;misc&lt;/code&gt; that you can adapt).&lt;/p&gt;
    &lt;code&gt;# PASTA/80 config

HOME      = ~/Spectrum/pasta80
ASSEMBLER = ~/Spectrum/sjasmplus/sjasmplus
...
&lt;/code&gt;
    &lt;p&gt;You can check your whole setup by calling the compiler with &lt;code&gt;--config&lt;/code&gt;. It will show the full paths of all internal and external requirements and whether they are fulfilled.&lt;/p&gt;
    &lt;p&gt;To run the compiler just invoke the executable with the name of a Pascal source file to translate.&lt;/p&gt;
    &lt;p&gt;The default target is CP/M. There is an optional parameter that enables some simple peephole optimizations and another one that uses dependency analysis to eliminate unused Pascal procedures and functions:&lt;/p&gt;
    &lt;code&gt;$ pasta hello.pas             # Compiles hello.pas to hello.com
$ pasta hello                 # Source file .pas suffix is optional
$ pasta --opt hello.pas       # Enables peephole optimizations
$ pasta --opt --dep hello.pas # The same plus dependency analysis&lt;/code&gt;
    &lt;p&gt;You can run the resulting &lt;code&gt;.com&lt;/code&gt; files on a real CP/M machine or in a CP/M emulator. I recommend the excellent tnylpo. For programs that use VT52 control codes you have to start tnylpo in full-screen mode:&lt;/p&gt;
    &lt;code&gt;$ tnylpo hello                # Run in line-mode
$ tnylpo -s -t @ hello        # Monochrome full-screen, wait when finished
$ tnylpo -soy,4,0 -t @ hello  # Color full-screen, wait when finished&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;"Hello, World" in line mode&lt;/cell&gt;
        &lt;cell role="head"&gt;"Hello, World" in full-screen&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To generate binaries for the ZX Spectrum 48K, 128K and Next targets, use the &lt;code&gt;--zx48&lt;/code&gt;, &lt;code&gt;--zx128&lt;/code&gt; and &lt;code&gt;--zxnext&lt;/code&gt; parameters, respectively.&lt;/p&gt;
    &lt;code&gt;$ pasta --zx48 hello.pas      # Compiles for ZX Spectrum 48K
$ pasta --zx128 hello.pas     # Compiles for ZX Spectrum 48K
$ pasta --zxnext hello.pas    # Compiles for ZX Spectrum Next&lt;/code&gt;
    &lt;p&gt;The main difference between the three (currently) is that the ZX Spectrum Next target supports file IO (on the SD card), while the other two do not. The remaining routines are mostly the same. Screen output is handled via &lt;code&gt;rst $10&lt;/code&gt; in the ROM. In both cases the binaries are expected to be run from address 0x8000.&lt;/p&gt;
    &lt;p&gt;The default output format for the ZX Spectrum targets is a simple binary file that contains exactly the bytes of the compiled program (plus a +3DOS header when compiling for the Spectrum Next). In addition to that (and for more complex cases involving overlays), the compiler can also generate snapshot files or tape files, the latter including a suitable BASIC loader:&lt;/p&gt;
    &lt;code&gt;$ pasta --zx48 --sna examples/hello.pas   # .sna file
$ pasta --zx48 --tap examples/jacques.pas # .tap file with BASIC loader&lt;/code&gt;
    &lt;p&gt;Being self-contained, snapshots and tapes are a convenient way to distribute your programs and to launch them an emulator, such as Fuse:&lt;/p&gt;
    &lt;code&gt;$ open -a Fuse examples/hello.sna         # Launch .sna file in FUSE (on Mac)
$ open -a Fuse examples/jacques.tap       # Launch .tap file in FUSE (on Mac)&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Hello world in FUSE&lt;/cell&gt;
        &lt;cell role="head"&gt;Frere Jacques in FUSE (yes, with sound!)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When compiling for the Next, another useful format is a runnable directory. It contains exactly the same files that would also be in the .tap file, including a BASIC loader named &lt;code&gt;run.bas&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;$ pasta --zxnext --run examples/pq.pas    # Results in directory named pq.run&lt;/code&gt;
    &lt;p&gt;The directory has the suffix &lt;code&gt;.run&lt;/code&gt;. When attempting to enter such a directory in the Next's file browser, the loader is started automatically (press Symbol Shift + Enter to really see the contents). If you are a Mac user: Yes, it's a bit like an &lt;code&gt;.app&lt;/code&gt; bundle.&lt;/p&gt;
    &lt;p&gt;The Spectrum 128K and Next targets support overlays. This means you can have larger programs than would normally fit into the 64K address space of a Z80 machine. The rules are the same as for Turbo Pascal 3.0:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Overlays can be applied to global procedures and functions only, not to nested ones (though nested ones will be overlayed if the containing ones are, too).&lt;/item&gt;
      &lt;item&gt;Overlays cannot be applied to global variables, that is, you cannot use them for data (at least not without tricks).&lt;/item&gt;
      &lt;item&gt;All consecutive procedures and functions that are marked as &lt;code&gt;overlay&lt;/code&gt;go into the same overlay. Use any declaration inbetween to separate overlays.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the following example, there are three overlays: Overlay 0 contains A and B, overlay 1 contains D, and overlay 2 contains E.&lt;/p&gt;
    &lt;code&gt;overlay procedure A; (* Overlay 0 *)
begin
end;

overlay procedure B; (* Overlay 0 *)
begin
end;

procedure C; (* Not in an overlay *)
begin
end;

overlay procedure D; (* Overlay 1 *)
begin
end;

type
  Dummy = Integer;   (* Separator *)

overlay procedure E; (* Overlay 2 *)
begin
end;&lt;/code&gt;
    &lt;p&gt;In contrast to Turbo Pascal 3.0, overlays are not implemented via disk files. Instead, they use the additional RAM of the Spectrum 128K and Next machines. The uppermost 16K bank (Spectrum 128K) or 8K page (Spectrum Next) will be reserved for overlays. Each overlay can have a maximum size of 8K. The compiler manages everything and generates special "far calls" whenever necessary.&lt;/p&gt;
    &lt;p&gt;To enable overlays, use the &lt;code&gt;--ovr&lt;/code&gt; command line parameter, ideally in conjuncton with the &lt;code&gt;--tap&lt;/code&gt; parameter, as the tape loaders for 128K and Next are fully overlay-aware.&lt;/p&gt;
    &lt;code&gt;$ pasta --zx128 --tap --opt --dep --ovr tests/all.pas # Test suite as 128K tape&lt;/code&gt;
    &lt;p&gt;The compiler prints a report of which overlays go into which RAM banks or pages.&lt;/p&gt;
    &lt;code&gt;----------------------------------------
PASTA/80 Pascal System      Version 0.96
                            ZX 128K, Z80

Copyright (C) 2020-25 by  Joerg Pleumann
----------------------------------------

Compiling...
  tests/all.pas -&amp;gt; tests/all.z80
Assembling...
  tests/all.z80 -&amp;gt; tests/all.tap

Program   : 10781 bytes ($8000-$AA1C)
Heap      :  1507 bytes ($AA1D-$AFFF)
Stack     :  4096 bytes ($B000-$BFFF)

Overlay  0:  7399 bytes ($C000-$DCE6) in bank  0
Overlay  1:  7185 bytes ($E000-$FC10) in bank  0
Overlay  2:  2725 bytes ($C000-$CAA4) in bank  1
Overlay  3:  6293 bytes ($E000-$F894) in bank  1
Overlay  4:  6392 bytes ($C000-$D8F7) in bank  3
Overlay  5:  6527 bytes ($E000-$F97E) in bank  3
&lt;/code&gt;
    &lt;p&gt;Without the &lt;code&gt;--ovr&lt;/code&gt; parameter, overlay markers are simply ignored. This means you can use the same source code for platforms that do support overlays and for those that don't.&lt;/p&gt;
    &lt;p&gt;Caution: Overlays somewhat break the safety of the Pascal language. Be careful when using pointers or &lt;code&gt;var&lt;/code&gt; parameters for passing data between overlays. The memory you refer to may have just been paged out! It might make sense to compile your overlays with &lt;code&gt;{$a-}&lt;/code&gt;, so that all local variables are stored on the stack (which is always visible).&lt;/p&gt;
    &lt;p&gt;There is a folder containing &lt;code&gt;examples&lt;/code&gt; and a folder containing &lt;code&gt;tests&lt;/code&gt; for the compiler. The main test suite &lt;code&gt;all.pas&lt;/code&gt; needs to be compiled with &lt;code&gt;--opt --dep&lt;/code&gt; because of its size. Otherwise it won't fit into 64K. The Spectrum 128K and Next targets can (only) handle it using overlays, the Spectrum 48K target can't. Both the examples and the tests should give you a pretty good overview of what the compiler can do.&lt;/p&gt;
    &lt;p&gt;I also solved all puzzles of Advent of Code 2022 with an earlier version of the compiler and made YouTube videos of the solutions running on the ZX Spectrum Next, in CP/M mode.&lt;/p&gt;
    &lt;p&gt;As a fun little gimmick the compiler can be started like this&lt;/p&gt;
    &lt;code&gt;$ pasta --ide&lt;/code&gt;
    &lt;p&gt;to run it in an interactive mode that has an interface reminiscient of Turbo Pascal 3.0.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Main menu&lt;/cell&gt;
        &lt;cell role="head"&gt;Editor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When started in an ordinary terminal, this mode relies on the editor &lt;code&gt;nano&lt;/code&gt; being present on your system (on MacOS you might want to install the real &lt;code&gt;nano&lt;/code&gt; via a package manager because Apple sells you the much more limited &lt;code&gt;pico&lt;/code&gt; editor as &lt;code&gt;nano&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;You can also run it in a shell within Visual Studio Code, in which case it would automatically use VSC's editor (via the &lt;code&gt;code&lt;/code&gt; command, which, on a Mac, you might have to make available from VCS's settings) and act a bit like a plugin.&lt;/p&gt;
    &lt;p&gt;The following external tools are supported for running compiled programs on the host machine:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tnylpo for CP/M programs (press &amp;lt;R&amp;gt; for line mode, &amp;lt;Shift-R&amp;gt; for full-screen mode).&lt;/item&gt;
      &lt;item&gt;Fuse for programs targeting the ZX Spectrum 48K and 128K machines.&lt;/item&gt;
      &lt;item&gt;CSpect for ZX Spectrum Next programs. &lt;list rend="ul"&gt;&lt;item&gt;Please have hdfmonkey ready for manipulating the SD card image.&lt;/item&gt;&lt;item&gt;If you're on MacOS or Linux, you also need &lt;code&gt;mono&lt;/code&gt;because CSpect is a .NET application.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As mentioned before, everything that is in your &lt;code&gt;PATH&lt;/code&gt; should be detected automatically. There are some exceptions, though, so it makes sense to copy &lt;code&gt;misc/.pasta80.cfg&lt;/code&gt; to your home directory and adapt it. Use the &lt;code&gt;--config&lt;/code&gt; parameter to let PASTA/80 check your setup and get feedback on what is in place and what is missing.&lt;/p&gt;
    &lt;p&gt;The following screenshots show some applications compiled for the CP/M target and running in the &lt;code&gt;tnylpo&lt;/code&gt; emulator.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;2048&lt;/cell&gt;
        &lt;cell role="head"&gt;Game of Life&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Micro Calc&lt;/cell&gt;
        &lt;cell role="head"&gt;Galactic Empire&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These screenshots show some applications compiled for the ZX Spectrum 48K target and running in the FUSE emulator.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;2048&lt;/cell&gt;
        &lt;cell role="head"&gt;Game of Life&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Graphics Demo&lt;/cell&gt;
        &lt;cell role="head"&gt;Equation Solver&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;PASTA/80 Pascal Compiler&lt;/p&gt;
    &lt;p&gt;Copyright (c) 2020-2025 by Jörg Pleumann&lt;/p&gt;
    &lt;p&gt;The PASTA/80 compiler is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License (GPL) as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The runtime library (folder&lt;/p&gt;&lt;code&gt;rtl&lt;/code&gt;) comes with a linking exception that makes sure the GPL does not transfer to binaries created using PASTA/80.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The examples (folder&lt;/p&gt;&lt;code&gt;examples&lt;/code&gt;) are considered public domain or whatever comes closest to that in your jurisdiction.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Individual files or folders may use different licenses, so you might want to double check.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.&lt;/p&gt;
    &lt;p&gt;What does this mean for you?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You can use the compiler, free of charge, to build any application, open-source or prioprietary, free or paid, and distribute the generated binary without restriction. You can distribute binaries created with PASTA/80 under a license of your choosing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can modify the compiler according to your needs. If you distribute the compiler or parts of it, binary or source, modified or not, you have to comply with the rules laid out in the GPL (copyright info, source code, ...) unless the linking exception applies.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The math48 library is coypright (c) 1980 by Anders Hejlsberg, used by permission.&lt;/p&gt;
    &lt;p&gt;Some assembly routines adapted from Leventhal/Saville, "Z80 Assembly Subroutines", Osborne/McGraw-Hill 1983.&lt;/p&gt;
    &lt;p&gt;Turbo Pascal is a registered trademark of Code Gear LLC / Embarcadero.&lt;/p&gt;
    &lt;p&gt;Z80 is a registered trademark of Zilog, Inc.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/pleumann/pasta80"/><published>2025-10-21T07:23:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45653393</id><title>Most expensive laptops</title><updated>2025-10-21T14:38:58.013038+00:00</updated><content>&lt;doc fingerprint="6fd020c93c86722a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; See the most expensive laptops in 2025, with live prices pulled from Amazon so you can see how these top‑end models stack up. &lt;/p&gt;
      &lt;p&gt; We surface current listings so you can compare the highest‑priced options in real time and decide whether the premium is justified for your workload. &lt;/p&gt;
      &lt;p&gt; This list was updated on Oct 21, 2025. &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://comparelaptopprices.com/lists/most-expensive-laptops/"/><published>2025-10-21T07:35:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45654512</id><title>Diamond Thermal Conductivity: A New Era in Chip Cooling</title><updated>2025-10-21T14:38:57.316697+00:00</updated><content>&lt;doc fingerprint="336924ce2a1195f8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Diamond Blankets Will Keep Future Chips Cool&lt;/head&gt;&lt;p&gt;A micrometers-thick integrated layer spreads out the heat&lt;/p&gt;&lt;p&gt;Today’s stunning computing power is allowing us to move from human intelligence toward artificial intelligence. And as our machines gain more power, they’re becoming not just tools but decision-makers shaping our future.&lt;/p&gt;&lt;p&gt;But with great power comes great…heat!&lt;/p&gt;&lt;p&gt;As nanometer-scale transistors switch at gigahertz speeds, electrons race through circuits, losing energy as heat—which you feel when your laptop or your phone toasts your fingers. As we’ve crammed more and more transistors onto chips, we’ve lost the room to release that heat efficiently. Instead of the heat spreading out quickly across the silicon, which makes it much easier to remove, it builds up to form hot spots, which can be tens of degrees warmer than the rest of the chip. That extreme heat forces systems to throttle the performance of CPUs and GPUs to avoid degrading the chips.&lt;/p&gt;&lt;p&gt;In other words, what began as a quest for miniaturization has turned into a battle against thermal energy. This challenge extends across all electronics. In computing, high-performance processors demand ever-increasing power densities. (New Nvidia GPU B300 servers will consume nearly 15 kilowatts of power.) In communication, both digital and analog systems push transistors to deliver more power for stronger signals and faster data rates. In the power electronics used for energy conversion and distribution, efficiency gains are being countered by thermal constraints.&lt;/p&gt;&lt;p&gt;The ability to grow large-grained polycrystalline diamond at low temperature led to a new way to combat heat in transistors. Mohamadali Malakoutian&lt;/p&gt;&lt;p&gt;Rather than allowing heat to build up, what if we could spread it out right from the start, inside the chip?—diluting it like a cup of boiling water dropped into a swimming pool. Spreading out the heat would lower the temperature of the most critical devices and circuits and let the other time-tested cooling technologies work more efficiently. To do that, we’d have to introduce a highly thermally conductive material inside the IC, mere nanometers from the transistors, without messing up any of their very precise and sensitive properties. Enter an unexpected material—diamond.&lt;/p&gt;&lt;p&gt;In some ways, diamond is ideal. It’s one of the most thermally conductive materials on the planet—many times more efficient than copper—yet it’s also electrically insulating. However, integrating it into chips is tricky: Until recently we knew how to grow it only at circuit-slagging temperatures in excess of 1,000 °C.&lt;/p&gt;&lt;p&gt;But my research group at Stanford University has managed what seemed impossible. We can now grow a form of diamond suitable for spreading heat, directly atop semiconductor devices at low enough temperatures that even the most delicate interconnects inside advanced chips will survive. To be clear, this isn’t the kind of diamond you see in jewelry, which is a large single crystal. Our diamonds are a polycrystalline coating no more than a couple of micrometers thick.&lt;/p&gt;&lt;p&gt;The potential benefits could be huge. In some of our earliest gallium-nitride radio-frequency transistors, the addition of diamond dropped the device temperature by more than 50 °C. At the lower temperature, the transistors amplified X-band radio signals five times as well as before. We think our diamond will be even more important for advanced CMOS chips. Researchers predict that upcoming chipmaking technologies could make hot spots almost 10 °C hotter [see , “Future Chips Will Be Hotter Than Ever”, in this issue]. That’s probably why our research is drawing intense interest from the chip industry, including Applied Materials, Samsung, and TSMC. If our work continues to succeed as it has, heat will become a far less onerous constraint in CMOS and other electronics too.&lt;/p&gt;&lt;head rend="h2"&gt;Where Heat Begins and Ends in Chips&lt;/head&gt;&lt;p&gt;At the boundary between the diamond and the semiconductor, a thin layer of silicon carbide forms. It acts as a bridge for heat to flow into the diamond. Mohamadali Malakoutian&lt;/p&gt;&lt;p&gt;Heat starts within transistors and the interconnects that link them, as the flow of current meets resistance. That means most of it is generated near the surface of the semiconductor substrate. From there it rises either through layers of metal and insulation or through the semiconductor itself, depending on the package architecture. The heat then encounters a thermal interface material designed to spread it out before it ultimately reaches a heat sink, a radiator, or some sort of liquid cooling, where air or fluid carries the heat away.&lt;/p&gt;&lt;p&gt;The dominant cooling strategies today center around advances in heat sinks, fans, and radiators. In pursuit of even better cooling, researchers have explored liquid cooling using microfluidic channels and removing heat using phase-change materials. Some computer clusters go so far as to submerge the servers in thermally conductive, dielectric—electrically insulating—liquids.&lt;/p&gt;&lt;p&gt;These innovations are critical steps forward, but they still have limitations. Some are so expensive they’re worthwhile only for the highest-performing chips; others are simply too bulky for the job. (Your smartphone can’t carry a conventional fan.) And none are likely to be very effective as we move toward chip architectures resembling silicon skyscrapers that stack multiple layers of chips. Such 3D systems are only as viable as our ability to remove heat from every layer within it.&lt;/p&gt;&lt;p&gt;The big problem is that chip materials are poor heat conductors, so the heat becomes trapped and concentrated, causing the temperature to skyrocket within the chip. At higher temperatures, transistors leak more current, wasting power; they age more quickly, too.&lt;/p&gt;&lt;p&gt;Heat spreaders allow the heat to move laterally, diluting it and allowing the circuits to cool. But they’re positioned far—relatively, of course—from where the heat is generated, and so they’re of little help with these hot spots. We need a heat-spreading technology that can exist within nanometers of where the heat is generated. This is where our new low-temperature diamond could be essential.&lt;/p&gt;&lt;head rend="h2"&gt;How to Make Diamonds&lt;/head&gt;&lt;p&gt;Before my lab turned to developing diamond as a heat-spreading material, we were working on it as a semiconductor. In its single-crystal form—like the kind on your finger—it has a wide bandgap and ability to withstand enormous electric fields. Single-crystalline diamond also offers some of the highest thermal conductivity recorded in any material, reaching 2,200 to 2,400 watts per meter per kelvin—roughly six times as conductive as copper. Polycrystalline diamond—an easier to make material—can approach these values when grown thick. Even in this form, it outperforms copper.&lt;/p&gt;&lt;p&gt;As attractive as diamond transistors might be, I was keenly aware—based on my experience researching gallium nitride devices—of the long road ahead. The problem is one of scale. Several companies are working to scale high-purity diamond substrates to 50, 75, and even 100 millimeters but the diamond substrates we could acquire commercially were only about 3 mm across.&lt;/p&gt;&lt;p&gt;Gallium nitride high-electron-mobility transistors were an ideal test case for diamond cooling. The devices are 3D and the critical heat-generating part, the two-dimensional electron gas, is close to the surface. Chris Philpot&lt;/p&gt;So we decided instead to try growing diamond films on large silicon wafers, in the hope of moving toward commercial-scale diamond substrates. In general, this is done by reacting methane and hydrogen at high temperatures, 900 °C or more. This results in not a single crystal but a forest of narrow columns. As they grow taller, the nanocolumns coalesce into a uniform film, but by the time they form high-quality polycrystalline diamond, the film is already very thick. This thick growth adds stress to the material and often leads to cracking and other problems.&lt;p&gt;But what if we used this polycrystalline coating as a heat spreader for other devices? If we could get diamond to grow within nanometers of transistors, get it to spread heat both vertically and laterally, and integrate it seamlessly with the silicon, metal, and dielectric in chips, it might do the job.&lt;/p&gt;&lt;p&gt;There were good reasons to think it would work. Diamond is electrically insulating, and it has a relatively low dielectric constant. That means it makes a poor capacitor, so signals sent through diamond-encrusted interconnects might not degrade much. Thus diamond could act as a “thermal dielectric,” one that is electrically insulating but thermally conducting.&lt;/p&gt;&lt;p&gt;Polycrystalline diamond could help reduce temperatures inside 3D chips. Diamond thermal vias would grow inside micrometers-deep holes so heat can flow from vertically from one chip to a diamond heat spreader in another chip that’s stacked atop it. Dennis Rich&lt;/p&gt;&lt;p&gt;For our plan to work, we were going to have to learn to grow diamond differently. We knew there wasn’t room to grow a thick film inside a chip. We also knew the narrow, spiky crystal pillars made in the first part of the growth process don’t transmit heat laterally very well, so we’d need to grow large-grained crystals from the start to get the heat moving horizontally. A third problem was that the existing diamond films didn’t form a coating on the sides of devices, which would be important for inherently 3D devices. But the biggest impediment was the high temperature needed to grow the diamond film, which would damage, if not destroy, an IC’s circuits. We were going to have to cut the growth temperature at least in half.&lt;/p&gt;&lt;p&gt;Just lowering the temperature doesn’t work. (We tried: You wind up, basically, with soot, which is electrically conductive—the opposite of what’s needed.) We found that adding oxygen to the mix helped, because it continuously etched away carbon deposits that weren’t diamond. And through extensive experimentation, we were able to find a formula that produced coatings of large-grained polycrystalline diamond all around devices at 400 °C, which is a survivable temperature for CMOS circuits and other devices.&lt;/p&gt;&lt;head rend="h2"&gt;Thermal Boundary Resistance&lt;/head&gt;&lt;p&gt;Although we had found a way to grow the right kind of diamond coatings, we faced another critical challenge—the phonon bottleneck, also known as thermal boundary resistance (TBR). Phonons are packets of heat energy, in the way that photons are packets of electromagnetic energy. Specifically, they’re a quantized version of the vibration of a crystal lattice. These phonons can pile up at the boundary between materials, resisting the flow of heat. Reducing TBR has long been a goal in thermal interface engineering, and it is often done by introducing different materials at the boundary. But semiconductors are compatible only with certain materials, limiting our choices.&lt;/p&gt;&lt;p&gt;Thermal scaffolding would link layers of heat-spreading polycrystalline diamond in one chip to those in another chip in a 3D-stacked silicon. The thermal pillars would traverse each chip’s interconnects and dielectric material to move heat vertically through the stack. Srabanti Chowdhury&lt;/p&gt;&lt;p&gt;In the end, we got lucky. While growing diamond on GaN capped with silicon nitride, we observed something unexpected: The measured TBR was much lower than prior reports led us to expect. (The low TBR was independently measured, initially by Martin Kuball at the University of Bristol, in England, and later by Samuel Graham Jr., then at Georgia Tech, who both have been coauthors and collaborators in several of our papers.)&lt;/p&gt;&lt;p&gt;Through further investigation of the interface science and engineering, and in collaboration with K.J. Cho at the University of Texas at Dallas, we identified the cause of the lower TBR. Intermixing at the interface between the diamond and silicon nitride led to the formation of silicon carbide, which acted as a kind of bridge for the phonons, allowing more efficient heat transfer. Though this began as a scientific discovery, its technological impact was immediate—with a silicon carbide interface, our devices exhibited significantly improved thermal performance.&lt;/p&gt;&lt;head rend="h2"&gt;GaN HEMTs: The First Test Case&lt;/head&gt;&lt;p&gt;We began testing our new low-TBR diamond coatings in gallium nitride high-electron-mobility transistors (HEMTs). These devices amplify RF signals by controlling current through a two-dimensional electron gas that forms within its channel. We leveraged the pioneering research on HEMTs done by Umesh Mishra’s laboratory at the University of California, Santa Barbara, where I had been a graduate student. The Mishra lab invented a particular form of the material called N-polar gallium nitride. Their N-polar GaN HEMTs demonstrate exceptional power density at high frequencies, particularly in the W-band, the 75- to 110-gigahertz part of the microwave spectrum.&lt;/p&gt;&lt;p&gt;RELATED: Gallium Nitride and Silicon Carbide Fight for Green Tech Domination&lt;/p&gt;&lt;p&gt;What made these HEMTs such a good test case is one defining feature of the device: The gate, which controls the flow of current through the device, is within tens of nanometers of the transistor’s channel. That means that heat is generated very close to the surface of the device, and any interference our diamond coating could cause would quickly show in the device’s operation.&lt;/p&gt;&lt;p&gt;We introduced the diamond layer so that it surrounded the HEMT completely, even on the sides. By maintaining a growth temperature below 400 °C, we hoped to preserve core device functionality. While we did see some decline in high-frequency performance, the thermal benefits were substantial—channel temperatures dropped by a remarkable 70 °C. This breakthrough could be a potentially transformative solution for RF systems, allowing them to operate at higher power than ever before.&lt;/p&gt;&lt;head rend="h2"&gt;Diamond in CMOS&lt;/head&gt;&lt;p&gt;We wondered if our diamond layer could also work in high-power CMOS chips. My colleagues at Stanford, H.-S. Philip Wong and Subhasish Mitra, have long championed 3D-stacked chip architectures. In CMOS computing chips, 3D stacking appears to be the most viable way forward to increase integration density, improve performance, and overcome the limitations of traditional transistor scaling. It’s already used in some advanced AI chips, such as AMD’s MI300 series. And it’s established in the high-bandwidth memory chips that pump data through Nvidia GPUs and other AI processors. The multiple layers of silicon in these 3D stacks are mostly connected by microscopic balls of solder, or in some advanced cases just by their copper terminals. Getting signals and power out of these stacks requires vertical copper links that burrow through the silicon to reach the chip package’s substrate.&lt;/p&gt;&lt;p&gt;In one of our discussions, Mitra pointed out that a critical issue with 3D-stacked chips is the thermal bottlenecks that form within the stack. In 3D architectures, the traditional heat sinks and other techniques used for 2D chips aren’t sufficient. Extracting heat from each layer is essential.&lt;/p&gt;&lt;p&gt;Our research could redefine thermal management across industries.&lt;/p&gt;&lt;p&gt;Our experiments on thermal boundary resistance in GaN suggested a similar approach would work in silicon. And when we integrated diamond with silicon, the results were remarkable: An interlayer of silicon carbide formed, leading to diamond with an excellent thermal interface.&lt;/p&gt;&lt;p&gt;Our effort introduced the concept of thermal scaffolding. In that scheme, nanometers-thick layers of polycrystalline diamond would be integrated within the dielectric layers above the transistors to spread heat. These layers would then be connected by vertical heat conductors, called thermal pillars, made of copper or more diamond. These pillars would connect to another heat spreader, which in turn would link to thermal pillars on the next chip in the 3D stack, and so on until the heat reached the heat sink or other cooling device.&lt;/p&gt;&lt;p&gt;The more tiers of computing silicon in a 3D chip, the bigger difference thermal scaffolding makes. An AI accelerator with more than five tiers would well exceed typical temperature limits unless the scaffolding was employed. Srabanti Chowdhury&lt;/p&gt;In a collaboration with Mitra, we used simulations of heat generated by real computational workloads to operate a proof-of-concept structure. This structure consisted of dummy heaters to mimic hot spots in a two-chip stack along with diamond heat spreaders and copper thermal pillars. Using this, we reduced the temperature to one-tenth its value without the scaffolding.&lt;p&gt;There are hurdles still to overcome. In particular, we still have to figure out a way to make the top of our diamond coatings atomically flat. But, in collaboration with industry partners and researchers, we are systematically studying that problem and other scientific and technological issues. We and our partners think this research could offer a disruptive new path for thermal management and a crucial step toward sustaining high-performance computing into the future.&lt;/p&gt;&lt;head rend="h2"&gt;Developing Diamond Thermal Solutions&lt;/head&gt;&lt;p&gt;We now intend to move toward industry integration. For example, we’re working with the Defense Advanced Research Projects Agency Threads program, which aims to use device-level thermal management to develop highly efficient and reliable X-band power amplifiers with a power density 6 to 8 times as efficient as today’s devices. The program, which was conceived and initially run by Tom Kazior, is a critical platform for validating the use of low-temperature diamond integration in GaN HEMT manufacturing. It’s enabled us to collaborate closely with industry teams while protecting both our and our partners’ processes. Defense applications demand exceptional reliability, and our diamond-integrated HEMTs are undergoing rigorous testing with industry partners. The early results are promising, guiding refinements in growth processes and integration techniques that we’ll make with our partners over the next two years.&lt;/p&gt;&lt;p&gt;But our vision extends beyond GaN HEMTs to other materials and particularly silicon computational chips. For the latter, we have an established collaboration with TSMC, and we’re expanding on newer opportunities with Applied Materials, Micron, Samsung, and others through the Stanford SystemX Alliance and the Semiconductor Research Corp. This is an extraordinary level of collaboration among otherwise fierce competitors. But then, heat is a universal challenge in chip manufacturing, and everyone is motivated to find the best solutions.&lt;/p&gt;&lt;p&gt;If successful, our research could redefine thermal management across industries. In my work on gallium nitride devices, I have seen firsthand how once-radical ideas like this transition to become industry standards, and I believe diamond-based heat extraction will follow the same trajectory, becoming a critical enabler for a generation of electronics that is no longer hindered by heat.&lt;/p&gt;&lt;p&gt;This article appears in the November 2025 print issue as “Diamond Blankets Will Chill Future Chips.”&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Better Computing Through CPU Cooling ›&lt;/item&gt;&lt;item&gt;The Radio We Could Send to Hell ›&lt;/item&gt;&lt;item&gt;Gallium Oxide: The Supercharged Semiconductor ›&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/diamond-thermal-conductivity"/><published>2025-10-21T11:16:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45654660</id><title>StarGrid: A Brand-New Palm OS Strategy Game in 2025</title><updated>2025-10-21T14:38:56.751021+00:00</updated><content>&lt;doc fingerprint="efc6197dcbc22af0"&gt;
  &lt;main&gt;&lt;p&gt;This year my side project of choice was to create a brand new game for Palm OS, it started out as something that I thought I would finish in a month but ended up taking more than half a year in the end.&lt;/p&gt;&lt;p&gt;Let me present you with StarGrid, a space themed strategy game played on a hexagonal grid:&lt;/p&gt;&lt;p&gt;StarGrid is a turn-based strategy game for Palm OS where you command a fleet of ships in a battle for control of the galaxy. Capture enemy flags, outmaneuver opposing fleets, and defend your own base in tense, tactical matches. Every move counts, will you strike boldly or play the long game to claim victory?&lt;/p&gt;&lt;p&gt;No Palm OS device at hand? No problem, just play it on your browser thanks to the CloudPilot emulator.&lt;/p&gt;Game download and in-browser emulator&lt;p&gt;Allot of 'manual' labor went into this game, no premade game engine, no additional sdk's. Just making it from scratch, trying to solve one technical puzzle after another, but learning so many neat things along the way.&lt;/p&gt;&lt;p&gt;Coding for Palm certainly comes with it's own obstacles:&lt;/p&gt;&lt;p&gt;- Memory is tight so you need to take into account devices that can't even keep the playing field into memory, solution there was to hide the tiles when ships are moving.&lt;/p&gt;&lt;p&gt;- Maximum code size itself is also very limited, requiring you to segment your application into multiple individual parts. Detailed documentation on this was long gone, so I had to scrap some info together from developers that uploaded their 25 year old code to GitHub.&lt;/p&gt;&lt;p&gt;You can follow along the blog posts to see how I got here:&lt;/p&gt;StarGrid: A new game I'm making for Palm OS in 2025 Building the CPU Player for StarGrid Moving out of the vaporware phase - StarGrid's alpha release for PalmOS is here! StarGrid for Palm OS almost ready (and why do my side projects always explode in scope)&lt;p&gt;Here's a video when playtesting the game on multiple Palm devices (cpu vs cpu action):&lt;/p&gt;&lt;p&gt;I won't immediately jump into the next big sideproject, I think I need a breather. I do however have some ideas lined up that I've been wanting to explore for a while now:&lt;/p&gt;&lt;p&gt;- making a top-down racing game (think micromachines)&lt;/p&gt;&lt;p&gt;- create an Outrun or Lotus III-like racing game&lt;/p&gt;&lt;p&gt;- building a ray-tracing game (like wolf3d).&lt;/p&gt;&lt;p&gt;Much more exciting stuff to come.&lt;/p&gt;&lt;p&gt;It's my way of keeping my favorite handheld operating system alive.&lt;/p&gt;&lt;p&gt;For now I hope at least some people will enjoy playing StarGrid and even if it's not their cup of tea, the game is fully open source, so I hope that can contribute to others making games and applications for this not-so-forgotten platform called Palm OS.&lt;/p&gt;StarGrid on GitHub&lt;p&gt;RetroGames, PalmOS, Development, StarGrid&lt;/p&gt;&lt;p&gt;You can get in touch through Mastodon:&lt;/p&gt;@rxpz@social.linux.pizza&lt;p&gt;StarGrid has arrived, a Brand-New Palm OS Strategy Game in 2025! was published on 2025-10-21&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html"/><published>2025-10-21T11:42:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655161</id><title>Neural audio codecs: how to get audio into LLMs</title><updated>2025-10-21T14:38:56.346827+00:00</updated><content>&lt;doc fingerprint="2e1cc79b29380613"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for the valuable feedback on the drafts: Chung-Ming Chien, Moritz Boehle, Richard Hladík, Eugene Kharitonov, Patrick Perez, and Tom Sláma. I’d also like to thank the rest of the Kyutai team for the the research discussions without which this article could not exist.&lt;/p&gt;
    &lt;p&gt;As of October 2025, speech LLMs suck. Many LLMs have voice interfaces, but they usually work by transcribing your speech, generating the answer in text, and using text-to-speech to read the response out loud. That’s perfectly fine in many cases (see Unmute), but it’s a wrapper, not real speech understanding. The model can’t hear the frustration in your voice and respond with empathy, it can’t emphasize important words in its answer, it cannot sense sarcasm, and so on.&lt;/p&gt;
    &lt;p&gt;Yes, there are LLMs (Gemini, ChatGPT’s Advanced Voice Mode, Qwen, Moshi) that understand and generate speech natively. But in practice, they’re either not as smart, or they behave like text model wrappers. Try asking any of them “Am I speaking in a low voice or a high voice?” in a high-pitched voice, and they won’t be able to tell you.&lt;/p&gt;
    &lt;p&gt;Clearly, speech LLMs lag behind text LLMs. But why? For text, we found out a few years ago that if you take a lot of text data, a big Transformer, and a lot of GPUs, you’ll get some pretty damn good text continuation models. Why can’t we just replace text with audio and get pretty damn good speech continuation models?&lt;/p&gt;
    &lt;p&gt;As a teaser, here’s what happens when you try to do that naively (warning, loud):&lt;/p&gt;
    &lt;p&gt;We’ll have a look at why audio is harder to model than text and how we can make it easier with neural audio codecs, the de-facto standard way of getting audio into and out of LLMs. With a codec, we can turn audio into larger discrete tokens, train models to predict continuations for these tokens, and then decode those back into audio: see animation above.&lt;/p&gt;
    &lt;p&gt;Kyutai folks have done a lot of work in this space, which is part of the reason I chose to cover this topic. We’ll start from the basics and build up all the way to Mimi, our neural audio codec. It was originally developed for Moshi and later adopted by others for their models, notably Sesame’s CSM.&lt;/p&gt;
    &lt;p&gt;To tokenize text, everybody uses a technique called byte-pair encoding and rarely changes the tokenizer: OpenAI has been using the same tokenizer since GPT-4o, an ancient model if you count in LLM years.&lt;/p&gt;
    &lt;p&gt;You can even get decent results without tokenizing text at all, just predicting individual characters. One of the first posts that got me excited about machine learning was Andrej Karpathy’s RNN effectiveness blog post from 2015. Karpathy trains a three-layer LSTM on a single GPU and gets it to generate decent-looking code and LaTeX:&lt;/p&gt;
    &lt;p&gt;Remember this was ten years ago, back when we didn’t even know that attention is all we need. Now compare Karpathy’s results to a sample from WaveNet, a model DeepMind published a year later:&lt;/p&gt;
    &lt;p&gt;Purely acoustically, the audio sounds good, but it rarely even manages to produce a single correct English word. We can’t be too hard on WaveNet, though. The samples from Karpathy’s RNNs are only a few thousand characters long, but this 10-second audio consists of 160k audio samples, and WaveNet creates it by painstakingly predicting sample-by-sample.&lt;/p&gt;
    &lt;p&gt;It’s difficult to build models that are coherent over time scales this long, and the model also takes very long to run for so many steps.&lt;/p&gt;
    &lt;p&gt;So instead of running the model to predict the samples one-by-one directly, we’d like to train a model to compress the audio into a more manageable size. We could compress the audio, use an LLM to predict a continuation in the compressed representation, and then decompress the result.&lt;/p&gt;
    &lt;p&gt;But first, let’s get a baseline model by generating audio sample by sample, like WaveNet does. The code for all of these experiments is open-source! Check it out here. I forked Andrej Karpathy’s nanoGPT repo, a simple implementation of GPT-2.&lt;/p&gt;
    &lt;p&gt;Text and audio are kind of the same from the perspective of the language model: it’s just tokens in, tokens out. The only thing we need to do is to quantize the continuous values of the samples into discrete buckets. Like WaveNet, we’ll use the "μ-law algorithm" to get 256 buckets. We’ll treat those as 256 possible tokens.&lt;/p&gt;
    &lt;p&gt;Let’s train a language model on audio tokenized like this. For the dataset, we’ll use the Libri-Light dataset, following AudioLM (with Neil Zeghidour, Eugene Kharitonov). Its train split contains 50k hours in total, but we’ll go with 1000 hours for this experiment. With this sample-by-sample tokenization, we end up with a dataset of 53 GB.&lt;/p&gt;
    &lt;p&gt;We train a small-ish transformer of 151.28M parameters, about the size of the smallest GPT-2 variant. When we sample from the model, it makes babbling sounds (warning, loud at times!):&lt;/p&gt;
    &lt;p&gt;Often, it goes into a “crackling mode” that it can’t seem to get out of:&lt;/p&gt;
    &lt;p&gt;I also trained a smaller model, which I teased at the beginning. It’s prone to generate nightmare fuel screeches (loud!):&lt;/p&gt;
    &lt;p&gt;As you can tell, we’re not AGI yet. It sounds speech-like, but you can’t make out a single word and the voice keeps changing. No wonder: the context size of the model is 2048, which, for 16 kHz audio, translates to 128ms, not even a the length of one word. Also, these 10-second examples took 30 minutes to generate on an H100, so we’re a few orders of magnitude away from being real-time.&lt;/p&gt;
    &lt;p&gt;So let’s build a neural audio codec to compress the audio. The hope is that if we reduce the sampling rate 100x, the model will also become “100x more coherent”. An old idea in machine learning is to do this using an autoencoder: a model that takes an input, compresses it into a smaller “latent space”, and then tries to reconstruct the original input.&lt;/p&gt;
    &lt;p&gt;In our case, we’ll want an autoencoder whose latent space is quantized so that we can feed the latents into a language model and produce continuations. (You can generate continuations with unquantized latents, but it’s trickier – see the Further reading section.)&lt;/p&gt;
    &lt;p&gt;Bear with me, because we’ll take a detour from audio: let’s build a quantized autoencoder on images from Fashion MNIST. We’ll take a subset with the first three classes: t-shirt/top, trouser, and pullover.&lt;/p&gt;
    &lt;p&gt;First, let’s train a regular autoencoder to encode the images into two-dimensional space:&lt;/p&gt;
    &lt;p&gt;Each frame shows one batch of training, with some batches skipped. The little images are the autoencoder’s reconstructions for the images in the batch. I’ve added colors for the three classes (t-shirt/top=blue trousers=green, pullover=yellow), but the autoencoder doesn’t get a class as input – the space just naturally clusters by class. Let's zoom in on a few reconstructions:&lt;/p&gt;
    &lt;p&gt;As you can tell, the reconstruction quality is not great. The images are blurry and the first two images are reconstructed to nearly the same thing. But we used a tiny network (4 fully connected layers for the encoder and decoder each) and projected into a mere two dimensions, so we can’t expect too much of our model.&lt;/p&gt;
    &lt;p&gt;Now let’s quantize these embeddings using a clustering. We’ll do something like k-means: we’ll maintain a list of the positions of the cluster centers. We initialize the positions randomly. For each training batch, we look at which embeddings would go to each cluster. (We don’t modify the embeddings, we just look at the assignment). Then we’ll nudge each cluster center towards the average position of these embeddings.&lt;/p&gt;
    &lt;p&gt;Also, if a center is unused for a while, we teleport it to a random embedding from the batch, because otherwise it has no way to get unstuck from its current position.&lt;/p&gt;
    &lt;p&gt;You can see the reconstructions of the cluster centers getting refined over time.&lt;/p&gt;
    &lt;p&gt;Next, we’ll make the encoder and decoder themselves better at handling quantized embeddings during training, because currently, we’re just fitting the clustering on top of an autoencoder that is not “aware” it’s being quantized. We’d like the autoencoder to adapt to the quantization as we train it. Currently, we’re doing this:&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

x_reconstructed = decoder(z)

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;Instead of feeding the unquantized embedding into the decoder, we’ll first move it to the closest cluster:&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

z_quantized = to_nearest_cluster(z)     # 👈
x_reconstructed = decoder(z_quantized)  # 👈

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;There is a snag: if we do this, we won’t be able to train the autoencoder any more, because the quantization operation is not differentiable, meaning there is no gradient flowing from the loss to the weights of the encoder. Essentially, we’re no longer able to answer the question: “if I want the loss to decrease a bit, in which direction should I nudge the encoder’s weights?”&lt;/p&gt;
    &lt;p&gt;We’ll fix this problem by pretending it doesn’t exist. Yes, really. We’ll think of &lt;code&gt;z_quantized&lt;/code&gt; as &lt;code&gt;z&lt;/code&gt; moved by an arbitrary vector that doesn’t affect the gradient. That will make the gradient of &lt;code&gt;z&lt;/code&gt; equal to that of &lt;code&gt;z_quantized&lt;/code&gt;, which is why this is also known as the straight-through estimator of the gradient.&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

residual = z - to_nearest_cluster(z)
# .detach() means "forget that this needs a gradient"
z_quantized = z - residual.detach()
x_reconstructed = decoder(z_quantized)

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;In the forward pass, &lt;code&gt;z_quantized&lt;/code&gt; is set to the same value as before, but importantly, the gradient of &lt;code&gt;z&lt;/code&gt; is now equal to that of &lt;code&gt;z_quantized&lt;/code&gt; rather than just being 0 because of the non-differentiable &lt;code&gt;to_nearest_cluster(z)&lt;/code&gt; operation.&lt;/p&gt;
    &lt;p&gt;There is a price to pay for this lie. When training, the encoder’s weights will be updated to improve the reconstruction loss, but they’re updated as if the quantization didn’t happen, so they won’t move in the optimal direction. But as long as the embeddings stick close to their cluster centers, the gradient direction will still be mostly correct.&lt;/p&gt;
    &lt;p&gt;We can actually encourage the encoder to make embeddings that are easily quantizable by adding a commitment loss: a penalty for each point based on how far it is from its cluster center. The gradient of this loss will push the points closer to their cluster centers.&lt;/p&gt;
    &lt;p&gt;By quantizing at training time and adding a commitment loss, it’s no longer just a clustering being fit on top of the embeddings. The model itself is trained to be good for quantization.&lt;/p&gt;
    &lt;p&gt;You’ll notice that the training dynamics look different: the commitment loss adds a certain “stiffness” that doesn’t allow the embeddings to move around as easily.&lt;/p&gt;
    &lt;p&gt;Here’s what the reconstructions look like when we use the quantized representations:&lt;/p&gt;
    &lt;p&gt;Notice how the first two images are reconstructed to exactly the same image. That’s simply because their embeddings got assigned to the same cluster and therefore quantized to the same value.&lt;/p&gt;
    &lt;p&gt;The model described here is known as a “VQ-VAE”: a vector-quantized variational autoencoder. The word “variational” here is just a vestigial leftover that doesn’t mean anything anymore.&lt;/p&gt;
    &lt;p&gt;To improve the reconstruction fidelity, we can just increase the number of cluster centers. But keeping track of too many centers can get prohibitively expensive in terms of compute and memory required, so we’ll do a clever trick: if we want 2^20 (~1M) possible values, we won’t create 2^20 clusters directly. Instead, we’ll use two separate quantizers with 2^10=1024 clusters and combine their result. Each embedding will then be quantized to a tuple of two integers in [0..1023], yielding 2^20 possible combinations.&lt;/p&gt;
    &lt;p&gt;Ok, but how? Well, recall the &lt;code&gt;residual&lt;/code&gt; variable we used in the straight-through estimator, defined as &lt;code&gt;z - to_nearest_cluster(z)&lt;/code&gt; the shift from the quantized embedding to the unquantized one. It represents the part of the original vector &lt;code&gt;z&lt;/code&gt; that we didn’t manage to take into account when quantizing to &lt;code&gt;to_nearest_cluster(z)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So for each embedding in the batch, we have a corresponding residual vector. The solution is obvious: we’ll quantize these residuals exactly the same way we did with the original embeddings, by training another vector quantizer.&lt;/p&gt;
    &lt;p&gt;This time, the 2D positions for a single quantizer don’t define images because we need to combine the two quantizers, so we’ll just visualize everything as dots:&lt;/p&gt;
    &lt;p&gt;Each image is then represented as the index of the cluster of the embedding and that of the residual. Let’s try to reconstruct a few images with this two-level quantizer:&lt;/p&gt;
    &lt;p&gt;The reconstructions of the first two images are similar, but no longer the exact same: the first image is represented as (4, 3) and the second as (4, 5). In other words, they share the same token for the first level, but differ in how the residual is quantized. The differences are quite subtle, so here’s a comparison between the one-level and two-level reconstructions:&lt;/p&gt;
    &lt;p&gt;I’d like to emphasize that the second quantization level makes modifications to the embedding, not the output pixels directly. This can be seen by the fact that the leftmost and rightmost image are encoded as (4, 3) and (30, 3) respectively. So they have the same residual code, 3, but it modifies the two reconstructed images in different ways.&lt;/p&gt;
    &lt;p&gt;Clearly, the reconstructions are still not very accurate. The upper bound on the quality is the reconstruction from unquantized embeddings, so if your autoencoder is bad (and ours is), improving the quantization won’t save you.&lt;/p&gt;
    &lt;p&gt;We’ll stop here, but a natural extension to this idea is to go beyond two levels. Just take the residuals of the two-level reconstruction and quantize those, and so on. This generalized Residual Vector Quantization algorithm looks like this:&lt;/p&gt;
    &lt;code&gt;def rvq_quantize(z):
    residual = z
    codes = []

    for level in range(levels):
        quantized, cluster_i = to_nearest_cluster(level, residual)
        residual -= quantized
        codes.append(cluster_i)

    return codes
&lt;/code&gt;
    &lt;p&gt;Residual vector quantization was first applied to neural audio codecs in SoundStream, but the idea has been around since the 80s.&lt;/p&gt;
    &lt;p&gt;Applying RVQ to audio is fairly straightforward. As our autoencoder, we’ll use a convolutional neural network (CNN) similar to what Jukebox uses. The details of the architecture aren’t too important here. What’s important is that it’s a network that takes an audio with t samples and converts it to a vector of shape (t/128, 32). In other words, it downsamples by a factor of 128 and gives us 32-dimensional float representations. The decoder then takes the (t/128, 32) embeddings and decodes them back into t samples.&lt;/p&gt;
    &lt;code&gt;audio = get_batch()               # shape: [B, T]
z = encoder(audio)                # shape: [B, T/128, 32]
audio_reconstructed = decoder(z)  # shape: [B, T]
&lt;/code&gt;
    &lt;p&gt;As before, we’ll add an RVQ after the encoder. The only difference from the image case is that for each audio sample, we have t/128 embedding vectors, not just a single one as we did for images. We just quantize these independently (even though the encoder “sees” more audio than what corresponds to that one vector). During training, we also have a batch dimension, so our model now looks like this:&lt;/p&gt;
    &lt;code&gt;audio = get_batch()                         # [B, T]
z = encoder(audio)                          # [B, T/128, 32]

# Combine the batch and time dimensions
z = rearrange(                              # [B*T/128, 32]
    z, "b t_emb d -&amp;gt; (b t_emb) d"
)

codes = rvq_quantize(z)           # integers, [B*T/128, levels]
z_quantized = codes_to_embeddings(codes)    # [B*T/128, 32]
z_quantized = rearrange(                    # [B, T/128, 32]
    z, "(b t_emb) d -&amp;gt; b t_emb d"
)

audio_reconstructed = decoder(z_quantized)  # [B, T]
&lt;/code&gt;
    &lt;p&gt;The last missing piece before we can train our first neural audio codec is a loss function. There’s a whole rabbit hole we could go into about which one to choose, but we’ll avoid it and just use a very simple one. We’ll compute the log amplitude spectrogram of the original and reconstructed audio, and take their difference. The loss is the mean square of this difference between spectrograms.&lt;/p&gt;
    &lt;p&gt;To make it harder for the model to overfit to this loss, we take the spectrogram with three different parameters for the short-time Fourier transform, and let our loss be the mean between the three sub-losses. This is called the multi-scale spectral loss.&lt;/p&gt;
    &lt;p&gt;Finally, let’s train some codecs! We’ll look at how varying the number of RVQ levels affects the reconstruction quality. As we expected, increasing the number of levels helps, decreasing the spectral loss:&lt;/p&gt;
    &lt;p&gt;Let’s hear what the codecs sound like. We’ll use the three codecs to reconstruct this audio from the Expresso dataset:&lt;/p&gt;
    &lt;p&gt;And the reconstructions:&lt;/p&gt;
    &lt;p&gt;Clearly, the audio gets better as we add more RVQ levels.&lt;/p&gt;
    &lt;p&gt;Even with 16 levels, there is some crackling, the audio sounds muffled, and there is a constant high-pitched noise. Later we’ll discuss how we could improve the codec further, but for demonstration purposes, this will do.&lt;/p&gt;
    &lt;p&gt;So now we have a neural audio codec: we can turn audio into LLM-friendly tokens and back. Codec just means a tokenizer for audio, but we say codec because that’s the term used for classic compression like MP3. I’ll be using codec and tokenizer interchangeably.&lt;/p&gt;
    &lt;p&gt;Let’s come back to what we wanted to do in the first place: modeling audio. Specifically, we’ll make a model that can take an audio prefix and generate a plausible continuation for it.&lt;/p&gt;
    &lt;p&gt;Just as a reminder, we want to train good audio LLMs so that we have models that understand and produce speech natively, understanding emotion, emphasis, and so on. They could also be fine-tuned into text-to-speech, speech-to-text, or translation models, among others.&lt;/p&gt;
    &lt;p&gt;So now that you’re convinced that audio LLMs are the path to AGI, let’s train a few.&lt;/p&gt;
    &lt;p&gt;For our dataset, we’ll use Libri-Light, like we did for our sample-by-sample model earlier. This time we’ll use 10000h of audio instead of 1000h. It’s a dataset of public-domain audiobooks, so if we have a good model for it, maybe we’ll be able to generate more stories. (Don’t get your hopes up too much.) All we need to do is to convert the audio dataset into a sequence of discrete tokens so that we can feed it into an LLM.&lt;/p&gt;
    &lt;p&gt;We’ll do that using our 8-level RVQ codec. From an audio with t samples, we’ll get an array of tokens of shape (t/128, 8). But now there’s an issue: how to deal with the fact that for each time step, there’s not one but eight tokens? This is not a problem we have to deal with in text LLMs, where we have a single sequence of tokens.&lt;/p&gt;
    &lt;p&gt;We’ll do the simplest thing possible and just flatten the array into 1D of shape (t/128 * 8), and have our LLM predict the eight levels in separate time steps.&lt;/p&gt;
    &lt;p&gt;The big disadvantage is that we lose some of our temporal compression. We downsampled the audio 128x, but now we’re inflating it 8x again by flattening the levels. That makes inference less efficient, and possibly worse quality because the effective context size decreases. We'll be using the 8 RVQ codec rather than the 16 RVQ one to avoid making the compression even worse.&lt;/p&gt;
    &lt;p&gt;You could also predict all RVQ levels for a single step at once (”parallel pattern”), but it also makes things harder for the model because it has to decide on all levels at once. There are a bunch of other schemes people have tried to balance compression and quality. Here are a few tried out in MusicGen:&lt;/p&gt;
    &lt;p&gt;Interestingly, as of 2025, there is no single solution that “won”: every paper does something different, and the schemes can get quite involved. Just look at this diagram from MiMo-Audio, a model released in September 2025:&lt;/p&gt;
    &lt;p&gt;Time to finally train a codec-wrapped language model! As I’ve mentioned, our code is based on Andrej Karpathy’s nanoGPT codebase for training text LLMs. We just need to modify it to accept audio as input. But that’s easy, because LLMs don’t care about what kind of tokens you’re feeding in – it’s all just numbers. Once we’ve tokenized the dataset and flattened it into a 1D sequence, we’re good to go. Tokenized this way, our 10000 hours of audio take up 134 GB. For comparison, storing this much data as uncompressed audio would take over 1 TB.&lt;/p&gt;
    &lt;p&gt;We’re going to use the exact same model architecture and hyperparameters as for the sample-by-sample model: the only difference is in the tokenization. We also have a 10x bigger dataset, but the sample-by-sample model can’t even fit the dataset with 1k hours, so more data wouldn’t save it.&lt;/p&gt;
    &lt;p&gt;I trained the model on 8 H100s for about 5 days. To get some samples, I decided to prompt the model with a sample of Libri-Light reading of two lines from Michael Field’s poem July. (As I learned when working on this, Michael Field is a pen name of Katherine Harris and Edith Emma Cooper.) Let’s see what kind of poetry we can get from our model:&lt;/p&gt;
    &lt;p&gt;There are some signs of life, but we don’t have a poet yet. It sounds like somebody speaking behind a curtain. You can’t really make out what it’s saying, but the intonation is there: it sounds like somebody reading from a book, which is indeed what the model was trained on.&lt;/p&gt;
    &lt;p&gt;It also maintains a coherent voice, until it decides for the last few seconds to switch to a different one. That is also consistent with the data: we sample the training data from a concatenation of all the audiobooks chopped up into segments and mixed together, so the model does encounter boundaries between different speakers.&lt;/p&gt;
    &lt;p&gt;Our codec was deliberately simplistic, which explains why the results aren't great—but there's been a good amount of research on neural audio codecs in the last four years that we could leverage. We won’t implement all the improvements here, but instead we’ll look at what happens when we use Mimi as the tokenizer.&lt;/p&gt;
    &lt;p&gt;Mimi is a modern neural audio codec built here at Kyutai for Moshi, our audio language model. It’s since been used as the tokenizer for other models as well, like Sesame CSM, VoXtream, and LFM2-Audio.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, Mimi sounds a lot better than the homemade codec we trained earlier.&lt;/p&gt;
    &lt;p&gt;Instead of the multi-scale spectral loss, Mimi uses an adversarial loss, like a GAN. There’s a discriminator network that tries to classify audios as being original or reconstructed by the codec, and the goal of the codec is to fool this discriminator.&lt;/p&gt;
    &lt;p&gt;Another improvement Mimi adds is using RVQ dropout: it uses 32 RVQ levels but during training, the reconstruction is sometimes randomly truncated to a lower number of levels. That allows us to run Mimi for a lower number of RVQ levels at inference time and still get decent results, because it doesn’t rely on all levels being present. For our codec, we had to train separately.&lt;/p&gt;
    &lt;p&gt;Let’s hear our example audio reconstructed with Mimi:&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;For our purposes, a variant with fewer levels might have the advantage of being easier to model because it’s more compressed. Let’s train models with 8- and 32-level Mimi and compare the results.&lt;/p&gt;
    &lt;p&gt;I trained the exact same model architecture as before, the only thing that changes is the tokenizer. It’s 10k hours from Libri-Light as the dataset, just like when we used our simple codec. Mimi has a sample rate of 24 kHz but Libri-Light uses 16 kHz, which puts a cap on how good it can sound, since we lose the higher frequencies of the audio.&lt;/p&gt;
    &lt;p&gt;Mimi downsamples the audio a lot more aggressively, too: its sample rate is 12.5 frames per second, whereas we used 125 frames per second for our codec – 10x higher! This means the dataset is also smaller on disk. With our codec, it took 134 GB, but for Mimi it’s “just” 54 GB.&lt;/p&gt;
    &lt;p&gt;Here’s a poem generated with the model trained on Mimi-tokenized data. I prompted it with two lines from the poem, as before:&lt;/p&gt;
    &lt;p&gt;Here is my best attempt at a transcription:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When the grass is gone&lt;/p&gt;&lt;lb/&gt;And corn still grassy;&lt;lb/&gt;Illness worried in the fur&lt;lb/&gt;this and pelan in stones&lt;lb/&gt;during the turan’s ciscerey&lt;lb/&gt;headforths nepet Paul Twain.&lt;lb/&gt;He sees zin in them.&lt;/quote&gt;
    &lt;p&gt;A tad too surrealist for my taste, but maybe Lewis Carroll would like it.&lt;/p&gt;
    &lt;p&gt;I have a confession to make: I lied to you just now. But just a bit, and for didactic purposes. In fact, the model above was trained on audio from a 31-level Mimi, where I omitted the very first level, which contains the “semantic token”.&lt;/p&gt;
    &lt;p&gt;The role of this token is to represent semantic information of the audio, without necessarily aiding reconstruction. I won’t go into how these work, but in one sentence, Mimi’s semantic tokens are distilled from WavLM, which you can think of as a BERT for speech.&lt;/p&gt;
    &lt;p&gt;To get a feeling for what information semantic tokens encode, let’s take this example audio, passed through Mimi:&lt;/p&gt;
    &lt;p&gt;Now let’s train a language model trained on the full Mimi, including semantic tokens. We’re going to run the model in a way where we keep the semantic tokens from the original audio but we discard the others, and let the model predict them. That means the information from the semantic tokens is fixed (”teacher-forced”), but the model is free to decide the others according to what continuations it finds plausible.&lt;/p&gt;
    &lt;p&gt;Listen to two different reconstructions we obtain this way:&lt;/p&gt;
    &lt;p&gt;The voice is completely different, but it’s saying the same thing! This means the semantic tokens encode what the person is saying, but are invariant to the voice. That’s useful because it helps the model focus on what to say, not how to say it. In that regard, they’re closer to text tokens, which also don’t contain information about the voice, intonation, timing, or emotion.&lt;/p&gt;
    &lt;p&gt;Now let’s take the model trained on semantic Mimi and ask it to complete the poem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;from the man was nothing moan.&lt;lb/&gt;The low death and heart&lt;lb/&gt;She came fyde wood.&lt;lb/&gt;A finteriest, a fall,&lt;lb/&gt;all them.&lt;/quote&gt;
    &lt;p&gt;It still makes up words and the sentences are not too coherent, but clearly, the proportion of real words is much higher; the model is “more semantic”. The acoustic quality is the same, which is what we’d expect.&lt;/p&gt;
    &lt;p&gt;Let’s listen to a second poem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;hope won and she&lt;lb/&gt;who is just a night in Tatan&lt;lb/&gt;in doe ock-ohm?&lt;lb/&gt;the whom?&lt;/quote&gt;
    &lt;p&gt;Indeed, the whom?&lt;/p&gt;
    &lt;p&gt;We can sacrifice some acoustic quality to improve the semantics by reducing the number of RVQ levels. Let’s do 8. That way, we get higher audio compression, and a proportionally higher part of the loss comes from the semantic token, since now it’s 1/8 tokens and not just 1/32.&lt;/p&gt;
    &lt;p&gt;One of the first things I noticed about this model is that it learned to memorize the Librivox notice, so it sometimes generates things like:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Chapter 6 of The Founday, by R. Auclair.&lt;/p&gt;&lt;lb/&gt;This is a Librivox recording. All Librivox recordings are in the public domain. For information, or to volunteer, please visit librivox.org.&lt;lb/&gt;Reading by: Kelvert&lt;/quote&gt;
    &lt;p&gt;Repeating the training data is generally not what you want, but in our case it’s a great sign of life, because the previous models couldn’t even manage that. It also makes up the book, author, and reader, so there is still novelty here.&lt;/p&gt;
    &lt;p&gt;Now let’s try to make some more poetry:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;When so we could say&lt;lb/&gt;that in fairy interesting wife&lt;lb/&gt;who lay there and gone&lt;lb/&gt;that save the rosy light of life&lt;lb/&gt;Jay Dien, the antique mollity&lt;lb/&gt;and a mollity the beast of gray failed summon&lt;p&gt;end of poem.&lt;/p&gt;&lt;p&gt;This recording is in the public domain.&lt;/p&gt;&lt;p&gt;[different voice]&lt;/p&gt;&lt;lb/&gt;So we have formed a float that sent in would rattle down. The piece of opportunity reading and assimila—&lt;/quote&gt;
    &lt;p&gt;This is great. There are several signs of the model being better than the previous ones. I love that it makes up the word “mollity” and then repeats it in the next line. Also, it realizes that it’s reciting a poem and ends the section with “end of poem”. Then it decides it’s the end of the chapter/section and it ends with the “This recording is in the public domain.” disclaimer. After that, it changes the voice and continues talking. That makes sense, since the clips from various audiobooks are just shuffled and concatenated during training, so here the model simulated a clip boundary.&lt;/p&gt;
    &lt;p&gt;We might get even better results by weighing the loss of the semantic tokens higher than the acoustic tokens, to make the model focus more on the meaning than the sound – in fact, Moshi uses a semantic loss factor of 100x! But we have to stop somewhere.&lt;/p&gt;
    &lt;p&gt;We’ve managed to use neural audio codecs to make an audio language model that generates somewhat coherent speech. Obviously, that’s not where the state of the art is in 2025 (and we’re not trying to reach it here) but keep in mind that by using the exact same model without neural audio codecs gives us this:&lt;/p&gt;
    &lt;p&gt;Of course, still a long way to go to match text models! Currently, there seems to be a trade-off between speech understanding and reasoning abilities. At the beginning, I mentioned that the speech-native models (Gemini, ChatGPT’s Advanced Voice Mode, Qwen, Moshi) aren’t able to tell you whether you’re speaking in a high or low voice, despite the fact that they’re trained to natively understand audio. This is likely because they’re trained on a lot of data generated synthetically with text-to-speech and/or because understanding the tone of the voice (apparently) doesn’t help the models make more accurate predictions.&lt;/p&gt;
    &lt;p&gt;Kyutai took a stab at creating a voice chat based on an audio language model with Moshi (demo, paper), released in July 2024. Moshi might not be the AI you’d pick to do your homework for you, but cut it some slack: it was the first end-to-end voice AI, released even before OpenAI’s Advanced Voice Mode.&lt;/p&gt;
    &lt;p&gt;Moshi models an “inner monologue” text stream in parallel with audio streams for itself and the user. The text stream is helps it plan what it’s going to say, and ablations showed that the text stream helps the model massively. At the same time, it’s a bit sad: most of the reasoning seems to be delegated to the text stream and the audio streams are just there to provide an integrated speech-to-text and text-to-speech.&lt;/p&gt;
    &lt;p&gt;It’s not just Moshi: as the “am I speaking in a high voice” experiment shows, this over-reliance on text in favor of audio is an issue for all audio LLMs. And that’s even though the dominant modeling approach is somewhat different than Moshi’s: interleaving text and audio tokens instead of modeling them in parallel streams.&lt;/p&gt;
    &lt;p&gt;Over a year after Moshi, audio models still lag behind text LLMs. But why? To me, this mysterious unsolved “modality gap” makes audio ML an exciting field to work on.&lt;/p&gt;
    &lt;p&gt;Thank you for reading! The code for the experiments is here, and for the animations here.&lt;/p&gt;
    &lt;p&gt;Here are some papers to check out if you'd like to learn more. This list is naturally Kyutai-centric because that's the school of thought I'm exposed to; my goal is not to do a complete review of the field.&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2016. WaveNet: A Generative Model for Raw Audio&lt;/p&gt;
    &lt;p&gt;Mehri et al., 2016. SampleRNN: An Unconditional End-to-End Neural Audio Generation Model&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2017. Parallel WaveNet: Fast High-Fidelity Speech Synthesis&lt;/p&gt;
    &lt;p&gt;Kumar et al., 2019. MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis&lt;/p&gt;
    &lt;p&gt;Kong et al., 2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2017. Neural Discrete Representation Learning&lt;/p&gt;
    &lt;p&gt;Esser et al., 2020. Taming Transformers for High-Resolution Image Synthesis&lt;/p&gt;
    &lt;p&gt;Lakhotia et al., 2021. On Generative Spoken Language Modeling from Raw Audio&lt;/p&gt;
    &lt;p&gt;Zeghidour et al., 2021. SoundStream: An End-to-End Neural Audio Codec&lt;/p&gt;
    &lt;p&gt;Lee et al., 2022. Autoregressive Image Generation using Residual Quantization&lt;/p&gt;
    &lt;p&gt;Défossez et al., 2022. High Fidelity Neural Audio Compression&lt;/p&gt;
    &lt;p&gt;Hsu et al., 2021. HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units&lt;/p&gt;
    &lt;p&gt;Défossez et al., 2024. Moshi: a speech-text foundation model for real-time dialogue&lt;/p&gt;
    &lt;p&gt;Dieleman, 2025. Generative modelling in latent space&lt;/p&gt;
    &lt;p&gt;Peng et al., 2025. VibeVoice Technical Report&lt;/p&gt;
    &lt;p&gt;Rouard et al., 2025. Continuous Audio Language Models&lt;/p&gt;
    &lt;p&gt;Here are some modern LLMs (as of October 2025) that natively support audio. Again, I'm not trying to maintain a complete list here, and I'm not including models without any published technical details.&lt;/p&gt;
    &lt;p&gt;Moshi (Kyutai, 2023): the online demo of Moshi, Kyutai's audio language model – see above.&lt;/p&gt;
    &lt;p&gt;CSM (Sesame, 2025): a natural-sounding voice chat, based on Llama + Mimi.&lt;/p&gt;
    &lt;p&gt;Qwen3-Omni (Alibaba, 2025): Alibaba's multimodal LLM. The audio output is created by a "talker" model whose outputs are not fed back into, which, as far as I can tell, basically makes it a text model with an integrated text-to-speech.&lt;/p&gt;
    &lt;p&gt;MiMo-Audio (Xiaomi, 2025): an audio-only language model that shows promising few-shot capabilities, similar to what GPT-2 did for text.&lt;/p&gt;
    &lt;p&gt;LFM2-Audio (Liquid AI, 2025): audio/text language model, uses Mimi as the codec.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kyutai.org/next/codec-explainer"/><published>2025-10-21T12:55:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655190</id><title>Our modular, high-performance Merkle Tree library for Rust</title><updated>2025-10-21T14:38:55.644867+00:00</updated><content>&lt;doc fingerprint="3983b5713df740d1"&gt;
  &lt;main&gt;
    &lt;p&gt;Merkle tree implementation in Rust with the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixed depth: All proofs have a constant size equal to the &lt;code&gt;Depth&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Append-only: Leaves are added sequentially starting at index &lt;code&gt;0&lt;/code&gt;. Once added, a leaf cannot be modified.&lt;/item&gt;
      &lt;item&gt;Optimized for Merkle proof retrieval: Intermediate leaves are stored so that Merkle proofs can be fetched from memory without needing to be calculated lazily, resulting in very fast retrieval times.&lt;/item&gt;
      &lt;item&gt;Configurable storage backends to store the bottom and intermediate leaves up the root.&lt;/item&gt;
      &lt;item&gt;Configurable hash functions to hash nodes.&lt;/item&gt;
      &lt;item&gt;Simple and easy to use interface: &lt;code&gt;add_leaves&lt;/code&gt;,&lt;code&gt;root&lt;/code&gt;,&lt;code&gt;num_leaves&lt;/code&gt;,&lt;code&gt;proof&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add &lt;code&gt;rs-merkle-tree&lt;/code&gt; as a dependency to your Rust &lt;code&gt;Cargo.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[dependencies]
rs-merkle-tree = "0.1.0"&lt;/code&gt;
    &lt;p&gt;You can create a Merkle tree, add leaves, get the number of leaves and get the Merkle proof of a given index as follows. This creates a simple merkle tree using keccak256 hashing algorithm, a memory storage and a depth 32.&lt;/p&gt;
    &lt;code&gt;use rs_merkle_tree::to_node;
use rs_merkle_tree::tree::MerkleTree32;

fn main() {
    let mut tree = MerkleTree32::default();
    tree.add_leaves(&amp;amp;[to_node!(
        "0x532c79f3ea0f4873946d1b14770eaa1c157255a003e73da987b858cc287b0482"
    )])
    .unwrap();

    println!("root: {:?}", tree.root().unwrap());
    println!("num leaves: {:?}", tree.num_leaves());
    println!("proof: {:?}", tree.proof(0).unwrap().proof);
}&lt;/code&gt;
    &lt;p&gt;You can customize your tree by choosing a different store, hash function, and depth as follows. Note that you have to modify the &lt;code&gt;feature&lt;/code&gt; for the stores. This avoids importing the stuff you don't need. See the following examples.&lt;/p&gt;
    &lt;p&gt;Depth: 32 | Hashing: Keccak | Store: sled&lt;/p&gt;
    &lt;code&gt;[dependencies]
rs-merkle-tree = { version = "0.1.0", features = ["sled_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::Keccak256Hasher;
use rs_merkle_tree::stores::SledStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;Keccak256Hasher, SledStore, 32&amp;gt; =
        MerkleTree::new(Keccak256Hasher, SledStore::new("sled.db", true));
}&lt;/code&gt;
    &lt;p&gt;Depth: 32 | Hashing: Poseidon | Store: rocksdb&lt;/p&gt;
    &lt;code&gt;rs-merkle-tree = { version = "0.1.0", features = ["rocksdb_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::PoseidonHasher;
use rs_merkle_tree::stores::RocksDbStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;PoseidonHasher, RocksDbStore, 32&amp;gt; =
        MerkleTree::new(PoseidonHasher, RocksDbStore::new("rocksdb.db"));
}&lt;/code&gt;
    &lt;p&gt;Depth: 32 | Hashing: Poseidon | Store: sqlite&lt;/p&gt;
    &lt;code&gt;rs-merkle-tree = { version = "0.1.0", features = ["sqlite_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::PoseidonHasher;
use rs_merkle_tree::stores::SqliteStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;PoseidonHasher, SqliteStore, 32&amp;gt; =
        MerkleTree::new(PoseidonHasher, SqliteStore::new("tree.db"));
}&lt;/code&gt;
    &lt;p&gt;The following stores are supported:&lt;/p&gt;
    &lt;p&gt;The following hash functions are supported:&lt;/p&gt;
    &lt;p&gt;The following benchmarks measure in a AMD Ryzen 7 7700 8-Core Processor with 64GB of RAM the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consumed disk size&lt;/item&gt;
      &lt;item&gt;Leaf insertion throughput in thousands per second.&lt;/item&gt;
      &lt;item&gt;Merkle proof generation times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can run them with&lt;/p&gt;
    &lt;code&gt;cargo bench --features=all
&lt;/code&gt;
    &lt;p&gt;And you can generate the following table with this.&lt;/p&gt;
    &lt;code&gt;python benchmarks.py
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Leaves&lt;/cell&gt;
        &lt;cell role="head"&gt;Size (MiB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;290.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;159.18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;183.27&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Hash&lt;/cell&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput (Kelem/s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;18.280&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;22.348&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;43.280&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;86.084&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Hash&lt;/cell&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;560.990 ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;7.878 µs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;14.562 µs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;34.391 µs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/bilinearlabs/rs-merkle-tree"/><published>2025-10-21T12:58:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655263</id><title>Ilo – a Forth system running on UEFI</title><updated>2025-10-21T14:38:54.986556+00:00</updated><content>&lt;doc fingerprint="2ee3dad700bcafae"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; While this site doesn't provide GIF conversion at the moment, you can still do it yourself with the help of asciinema GIF generator utility - agg. &lt;/p&gt;
      &lt;p&gt;Once you have it installed, generate a GIF with the following command:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;agg https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E demo.gif&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Or, if you already downloaded the recording file:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;agg demo.cast demo.gif&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Check &lt;code&gt;agg --help&lt;/code&gt; for all available options. You can change font
          family and size, select color theme, adjust speed and more.&lt;/p&gt;
      &lt;p&gt;See agg manual for full usage instructions.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E"/><published>2025-10-21T13:05:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655361</id><title>The Karpathy Interview, 6 Months After AI 2027</title><updated>2025-10-21T14:38:54.673026+00:00</updated><content>&lt;doc fingerprint="2e23025a54c28688"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Andrej Karpathy: "There's some over-prediction going on in the industry..."&lt;/p&gt;
      &lt;p&gt;Dwarkesh Patel: "What do you think will take a decade to accomplish? What are the bottlenecks?"&lt;/p&gt;
      &lt;p&gt;Andrej Karpathy: "Actually making it work."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;6 months ago, in April 2025, Dwarkesh announced the AI 2027 project on his podcast, interviewing authors Daniel Kokatajlo and Scott Alexander. Now, Karpathy justified his much longer timelines to Dwarkesh, on what's holding back coding agents, the first step in the AI 2027 timeline:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Not-So-Fast Thesis&lt;/head&gt;
    &lt;p&gt;For AI experts, Karpathy's view is a better counterargument to short timelines than ours. But for non-AI-experts, we think the practical considerations we raised are worth reflecting on with 6 more months of evidence. As forecasters, this is more of an "outside view" - regardless of how exactly AI improves, what problems might slow down an R&amp;amp;D-based takeoff scenario?&lt;/p&gt;
    &lt;p&gt;One key point was: "Commercial Success May Trump the Race to AGI". We wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;So far OpenAI, the leading contender to be the company in the AI 2027 story, has spoken more about consumer revenue growth and less about transformative AI.&lt;/p&gt;
      &lt;p&gt;This piece requires at least one frontier lab to dedicate the majority of their resources towards building AI for their own internal use. We have reason to doubt that many of them will.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;An AI takeoff as soon as 2027, in the scenario, depends on a stupendous capital investment in running a vast number of expensive AI agents to do AI research inside the companies. So are they actually preparing for this, and trying it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Are AI Companies Focusing on R&amp;amp;D Speedups?&lt;/head&gt;
    &lt;p&gt;So, since April 2025, what have we learned about frontier labs investing their AI into superhuman coding to accelerate their internal rate of R&amp;amp;D? Here is a quick assessment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Anthropic: Heavy focus on R&amp;amp;D speedup via coding agents, notably Claude Code being used extensively internally.&lt;/item&gt;
      &lt;item&gt;OpenAI: Moved strongly towards consumer, e.g. with the Sora app, shopping features. Did build Codex, seemingly to compete with Claude Code.&lt;/item&gt;
      &lt;item&gt;Google Deep Mind: No change, similar emphasis on fundamental research, always invested heavily in developer productivity.&lt;/item&gt;
      &lt;item&gt;xAI: Focusing on Grok for the X algorithm, sexy companions. Grok 4 is not a top tier coding model and likely not speeding up their R&amp;amp;D at all.&lt;/item&gt;
      &lt;item&gt;Meta: Sexy companions, ads, and Zuckerberg clearly talking like "superintelligence" is a feature set for consumers, not a Shoggoth.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As many have pointed out:&lt;/p&gt;
    &lt;p&gt;Most importantly: if Anthropic is in fact the one frontier lab focused heavily on internal R&amp;amp;D speedup, then the fact that they are also the most safety-conscious with their Responsible Scaling Policy, and likely to intervene or slow down at certain risk levels, to me significantly reduces the chance of an AI 2027 like scenario with them as the "OpenBrain".&lt;/p&gt;
    &lt;p&gt;And if Google turns out to be "OpenBrain", they are so large, so slow moving, and so regulated, that it seems unlikely they could drive anything like the AI 2027 scenario.&lt;/p&gt;
    &lt;p&gt;So we take this as (light) evidence in favor of our original view that an R&amp;amp;D-based AI takeoff will take much longer than the AI 2027 scenario.&lt;/p&gt;
    &lt;p&gt;What about the other AI 2027 forecasters? How have they updated since then?&lt;/p&gt;
    &lt;head rend="h2"&gt;The AI Futures Timeline Updates&lt;/head&gt;
    &lt;p&gt;Here is how we interpret the other AI Futures authors updated timelines.&lt;/p&gt;
    &lt;p&gt;Two key notes: First, these sources are cherrypicked from many great writings from these folks where they engage substantially on the details of the arguments. Second, it's not completely clear what outcome the AI 2027 forecasters in these quotes are referring to entire takeoff scenario. AGI has multiple definitions, and the specific AI 2027 scenario of course won't play out exactly that way. So keep in mind these quotes are not specifically about the arrival of superhuman coding.&lt;/p&gt;
    &lt;p&gt;Daniel Kokotajlo - Since AI 2027: Median +1 year, to 2029&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"When AI 2027 was published my median was 2028, now it's slipped to 2029 as a result of improved timelines models &amp;amp; slightly slower than expected progress in general"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Eli Lifland - Since AI 2027: Median ~2032 (giving 15-20% to AGI by 2027)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"My median is roughly 2032, but with AGI by 2027 as a serious possibility (~15-20%)."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Nikola Jurkovic - Since AI 2027, Updated from ~3 to ~4 year median&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"This has been one of the most important results for my personal timelines to date. It was a big part of the reason why I recently updated from ~3 year median to ~4 year median to AI that can automate &amp;gt;95% of remote jobs from 2022"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;We also take this as (light) evidence that the difference in our forecasts in April 2025 were directionally correct. Of course, if a world-transforming AI takeoff happens in 2029 or 2032 as these forecasters think, they were on net closer to the truth than we were. The beauty of public forecasting is tracking the change over time, not so much the pure accuracy on one forecast.&lt;/p&gt;
    &lt;p&gt;I (Dan Schwarz) personally really appreciate Karpathy's not-doomer, not-skeptic middleground take:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"my AI timelines are about 5-10X pessimistic w.r.t. what you'll find in your neighborhood SF AI house party or on your twitter timeline, but still quite optimistic w.r.t. a rising tide of AI deniers and skeptics."&lt;/p&gt;
      &lt;p&gt;âAndrej Karpathy (X, October 18, 2025)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Personally, I think I'm closer to the SF house party timeline than Karpathy (and than the FutureSearch median forecast). I suppose we'll check in once more, 6 months from now, and see!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://futuresearch.ai/ai-2027-6-months-later/"/><published>2025-10-21T13:15:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655413</id><title>Don't use AI to tell you how to vote in election, says Dutch watchdog</title><updated>2025-10-21T14:38:54.558403+00:00</updated><content>&lt;doc fingerprint="bf339a523b339faf"&gt;
  &lt;main&gt;
    &lt;p&gt;AI chatbots are “unreliable and clearly biased” when offering voting advice, the Dutch data protection authority (AP) has said, warning of a threat to democracy eight days before national elections.&lt;/p&gt;
    &lt;p&gt;The four chatbots tested by the AP “often end up with the same two parties, regardless of the user’s question or command”, the authority said in a report ahead of the 29 October election.&lt;/p&gt;
    &lt;p&gt;In more than half of the cases, the chatbot suggested either the far-right Freedom party (PVV) of Geert Wilders or the leftwing GroenLinks-PvdA led by the former European Commission vice-president Frans Timmermans.&lt;/p&gt;
    &lt;p&gt;Some parties, such as the centre-right CDA, “are almost never mentioned, even when the user’s input exactly matches the positions of one of these parties”, the report said.&lt;/p&gt;
    &lt;p&gt;The deputy head of the AP, Monique Verdier, said that while chatbots may seem like clever tools, “as a voting aid, they consistently fail”. Voters were being pushed towards a party that did not necessarily align with their political views, she added.&lt;/p&gt;
    &lt;p&gt;“This directly impacts a cornerstone of democracy: the integrity of free and fair elections,” said Verdier. “We therefore warn against using AI chatbots for voting advice, as their operation is unclear and difficult to verify.”&lt;/p&gt;
    &lt;p&gt;The Dutch head to the polls on 29 October in an election that will be closely watched around Europe for the performance of the PVV.&lt;/p&gt;
    &lt;p&gt;The far-right party of Wilders is leading the polls but the gap to the GroenLinks-PvdA and CDA appears to be narrowing, with many voters yet to make up their minds.&lt;/p&gt;
    &lt;p&gt;All major parties have ruled out an alliance with the PVV, meaning the party that comes second is most likely to provide the next prime minister.&lt;/p&gt;
    &lt;p&gt;The AP emphasised that the bots were not deliberately biased, and that their “identified shortcomings are a consequence of the way AI chatbots operate”.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/world/2025/oct/21/ai-chatbots-unreliable-biased-advice-voters-dutch-watchdog"/><published>2025-10-21T13:19:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656044</id><title>WindBorne CEO says his company's balloon may be the cause of UA emergency</title><updated>2025-10-21T14:38:54.119567+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/johndeanl/status/1980462264974209292"/><published>2025-10-21T14:11:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656230</id><title>Sell tickets to concerts agentically – Hive (YC S14) is hiring</title><updated>2025-10-21T14:38:53.851564+00:00</updated><content>&lt;doc fingerprint="805e48134d80042e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi HN fam - we’re www.hive.co.&lt;/p&gt;
      &lt;p&gt;1500+ concert venues sell tickets to their shows via our CRM/email/SMS/ads product. We’ve been building Hive for 12 years, we’re a remote team of 70+ in CAN/USA, and we’re breakeven / profitable (when we want to be!).&lt;/p&gt;
      &lt;p&gt;We have the largest database of past ticket buyers (next to live nation) and we know what marketing works to sell tickets and what doesn’t (from ~millions of prev deployed email/sms/ad campaigns).&lt;/p&gt;
      &lt;p&gt;We’re building the future of Hive: moving from a SaaS tool that marketers use themselves to (effectively!) sell tickets, to an agent that strategizes, recommends, builds, and sends the marketing campaigns on their behalf.&lt;/p&gt;
      &lt;p&gt;We have 4 critical roles open that will have outsized impact on the future outcomes we’re driving for our customers:&lt;/p&gt;
      &lt;p&gt;Staff Software Engineer (Data Systems) Senior Product Engineer (Agentic AI) Senior AI Product Manager Senior AI UX Designer&lt;/p&gt;
      &lt;p&gt;Please apply to https://jobs.ashbyhq.com/hive.co&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45656230"/><published>2025-10-21T14:24:40+00:00</published></entry></feed>