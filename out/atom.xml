<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-29T17:39:54.825077+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45745566</id><title>Show HN: Learn German with Games</title><updated>2025-10-29T17:40:01.191027+00:00</updated><content>&lt;doc fingerprint="2ce7b8cb946d6548"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Choose Your Learning Adventure&lt;/head&gt;
    &lt;p&gt;Select a game below and start mastering German in an engaging, interactive way!&lt;/p&gt;
    &lt;head rend="h3"&gt;Numbers to Words Game&lt;/head&gt;
    &lt;p&gt;See a number and type the German word - perfect for learning German number vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;Words to Numbers Game&lt;/head&gt;
    &lt;p&gt;Practice recognizing German number words and converting them to digits&lt;/p&gt;
    &lt;head rend="h3"&gt;German Time Game&lt;/head&gt;
    &lt;p&gt;Learn to tell time in German by reading analog clocks and typing time expressions&lt;/p&gt;
    &lt;head rend="h3"&gt;Time Short Form Game&lt;/head&gt;
    &lt;p&gt;Practice German time with short forms: nach, vor, halb, viertel, and punkt&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Artikel&lt;/head&gt;
    &lt;p&gt;Master German artikels (der, die, das) by guessing the correct artikel for each noun&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Word&lt;/head&gt;
    &lt;p&gt;Translate German nouns to English - see a German word with its article and type the English meaning&lt;/p&gt;
    &lt;head rend="h3"&gt;English Nouns to German&lt;/head&gt;
    &lt;p&gt;See an English word and type the German translation with its artikel&lt;/p&gt;
    &lt;head rend="h3"&gt;Verb Conjugation&lt;/head&gt;
    &lt;p&gt;Practice conjugating German verbs in present tense for all persons - ich, du, er/sie/es, wir, ihr, sie/Sie&lt;/p&gt;
    &lt;head rend="h3"&gt;German Verbs to English&lt;/head&gt;
    &lt;p&gt;See a German verb and type its English meaning - perfect for building vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;English Verbs to German&lt;/head&gt;
    &lt;p&gt;See an English verb meaning and type the German infinitive form - reverse translation practice&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.learngermanwithgames.com/"/><published>2025-10-29T11:50:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746478</id><title>From VS Code to Helix</title><updated>2025-10-29T17:40:00.815559+00:00</updated><content>&lt;doc fingerprint="7708325733a03a9d"&gt;
  &lt;main&gt;
    &lt;p&gt;I created the website you‚Äôre reading with VS Code. Behind the scenes I use Astro, a static site generator that gets out of the way while providing nice conveniences.&lt;/p&gt;
    &lt;p&gt;Using VS Code was a no-brainer: everyone in the industry seems to at least be familiar with it, every project can be opened with it, and most projects can get enhancements and syntactic helpers in a few clicks. In short: VS Code is free, easy to use, and widely adopted.&lt;/p&gt;
    &lt;p&gt;A Rustacean colleague kept singing Helix‚Äôs praises. I discarded it because he‚Äôs much smarter than I am, and I only ever use vim when I need to fiddle with files on a server. I like when things ‚ÄúJust Work‚Äù and didn‚Äôt want to bother learning how to use Helix nor how to configure it.&lt;/p&gt;
    &lt;p&gt;Today it has become my daily driver. Why did I change my mind? What was preventing me from using it before? And how difficult was it to get there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Automation is a double-edged sword&lt;/head&gt;
    &lt;p&gt;Automation and technology make work easier, this is why we produce technology in the first place. But it also means you grow more dependent on the tech you use. If the tech is produced transparently by an international team or a team you trust, it‚Äôs fine. But if it‚Äôs produced by a single large entity that can screw you over, it‚Äôs dangerous.&lt;/p&gt;
    &lt;p&gt;VS Code might be open source, but in practice it‚Äôs produced by Microsoft. Microsoft has a problematic relationship to consent and is shoving AI products down everyone‚Äôs throat. I‚Äôd rather use tools that respect me and my decisions, and I‚Äôd rather not get my tools produced by already monopolistic organizations.&lt;/p&gt;
    &lt;p&gt;Microsoft is also based in the USA, and the political climate over there makes me want to depend as little as possible on American tools. I know that‚Äôs a long, uphill battle, but we have to start somewhere.&lt;/p&gt;
    &lt;p&gt;I‚Äôm not advocating for a ban against American tech in general, but for more balance in our supply chain. I‚Äôm also not advocating for European tech either: I‚Äôd rather get open source tools from international teams competing in a race to the top, rather than from teams in a single jurisdiction. What is happening in the USA could happen in Europe too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I feared using Helix&lt;/head&gt;
    &lt;p&gt;I‚Äôve never found vim particularly pleasant to use but it‚Äôs everywhere, so I figured I might just get used to it. But one of the things I never liked about vim is the number of moving pieces. By default, vim and neovim are very bare bones. They can be extended and completely modified with plugins, but I really don‚Äôt like the idea of having extremely customize tools.&lt;/p&gt;
    &lt;p&gt;I‚Äôd rather have the same editor as everyone else, with a few knobs for minor preferences. I am subject to choice paralysis, so making me configure an editor before I‚Äôve even started editing is the best way to tank my productivity.&lt;/p&gt;
    &lt;p&gt;When my colleague told me about Helix, two things struck me as improvements over vim.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Helix‚Äôs philosophy is that everything should work out of the box. There are a few configs and themes, but everything should work similarly from one Helix to another. All the language-specific logic is handled in Language Servers that implement the Language Server Protocol standard.&lt;/item&gt;
      &lt;item&gt;In Helix, first you select text, and then you perform operations onto it. So you can visually tell what is going to be changed before you apply the change. It fits my mental model much better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But there are major drawbacks to Helix too:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;After decades of vim, I was scared to re-learn everything. In practice this wasn‚Äôt a problem at all because of the very visual way Helix works.&lt;/item&gt;
      &lt;item&gt;VS Code ‚ÄúJust Works‚Äù, and Helix sounded like more work than the few clicks from VS Code‚Äôs extension store. This is true, but not as bad as I had anticipated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After a single week of usage, Helix was already very comfortable to navigate. After a few weeks, most of the wrinkles have been ironed out and I use it as my primary editor. So how did I overcome those fears?&lt;/p&gt;
    &lt;head rend="h2"&gt;What Helped&lt;/head&gt;
    &lt;head rend="h3"&gt;Just Do It&lt;/head&gt;
    &lt;p&gt;I tried Helix. It can sound silly, but the very first step to get into Helix was not to overthink it. I just installed it on my mac with &lt;code&gt;brew install helix&lt;/code&gt; and gave it a go. I was not too familiar with it, so I looked up the official documentation and noticed there was a tutorial.&lt;/p&gt;
    &lt;p&gt;This tutorial alone is what convinced me to try harder. It‚Äôs an interactive and well written way to learn how to move and perform basic operations in Helix. I quickly learned how to move around, select things, surround them with braces or parenthesis. I could see what I was about to do before doing it. This has been epiphany. Helix just worked the way I wanted.&lt;/p&gt;
    &lt;p&gt;Better: I could get things done faster than in VS Code after a few minutes of learning. Being a lazy person, I never bothered looking up VS Code shortcuts. Because the learning curve for Helix is slightly steeper, you have to learn those shortcuts that make moving around feel so easy.&lt;/p&gt;
    &lt;p&gt;Not only did I quickly get used to Helix key bindings: my vim muscle-memory didn‚Äôt get in the way at all!&lt;/p&gt;
    &lt;head rend="h3"&gt;Better docs&lt;/head&gt;
    &lt;p&gt;The built-in tutorial is a very pragmatic way to get started. You get results fast, you learn hands on, and it‚Äôs not that long. But if you want to go further, you have to look for docs. Helix has officials docs. They seem to be fairly complete, but they‚Äôre also impenetrable as a new user. They focus on what the editor supports and not on what I will want to do with it.&lt;/p&gt;
    &lt;p&gt;After a bit of browsing online, I‚Äôve stumbled upon this third-party documentation website. The domain didn‚Äôt inspire me a lot of confidence, but the docs are really good. They are clearly laid out, use-case oriented, and they make the most of Astro Starlight to provide a great reading experience. The author tried to upstream these docs, but that won‚Äôt happen. It looks like they are upstreaming their docs to the current website. I hope this will improve the quality of upstream docs eventually.&lt;/p&gt;
    &lt;p&gt;After learning the basics and finding my way through the docs, it was time to ensure Helix was set up to help me where I needed it most.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the most of Markdown and Astro in Helix&lt;/head&gt;
    &lt;p&gt;In my free time, I mostly use my editor for three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write notes in markdown&lt;/item&gt;
      &lt;item&gt;Tweak my website with Astro&lt;/item&gt;
      &lt;item&gt;Edit yaml to faff around my Kubernetes cluster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Helix is a ‚Äústupid‚Äù text editor. It doesn‚Äôt know much about what you‚Äôre typing. But it supports Language Servers that implement the Language Server Protocol. Language Servers understand the document you‚Äôre editing. They explain to Helix what you‚Äôre editing, whether you‚Äôre in a TypeScript function, typing a markdown link, etc. With that information, Helix and the Language Server can provide code completion hints, errors &amp;amp; warnings, and easier navigation in your code.&lt;/p&gt;
    &lt;p&gt;In addition to Language Servers, Helix also supports plugging code formatters. Those are pieces of software that will read the document and ensure that it is consistently formatted. It will check that all indentations use spaces and not tabs, that there is a consistent number of space when indenting, that brackets are on the same line as the function, etc. In short: it will make the code pretty.&lt;/p&gt;
    &lt;head rend="h3"&gt;Markdown&lt;/head&gt;
    &lt;p&gt;Markdown is not really a programming language, so it might seem surprising to configure a Language Server for it. But if you remember what we said earlier, Language Servers can provide code completion, which is useful when creating links for example. Marksman does exactly that!&lt;/p&gt;
    &lt;p&gt;Since Helix is pre-configured to use marksman for markdown files we only need to install marksman and make sure it‚Äôs in our &lt;code&gt;PATH&lt;/code&gt;. Installing it with homebrew is enough.&lt;/p&gt;
    &lt;p&gt;We can check that Helix is happy with it with the following command&lt;/p&gt;
    &lt;p&gt;But Language Servers can also help Helix display errors and warnings, and ‚Äúcode suggestions‚Äù to help fix the issues. It means Language Servers are a perfect fit for‚Ä¶ grammar checkers! Several grammar checkers exist. The most notable are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LTEX+, the Language Server used by Language Tool. It supports several languages must is quite resource hungry.&lt;/item&gt;
      &lt;item&gt;Harper, a grammar checker Language Server developed by Automattic, the people behind WordPress, Tumblr, WooCommerce, Beeper and more. Harper only support English and its variants, but they intend to support more languages in the future.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I mostly write in English and want to keep a minimalistic setup. Automattic is well funded, and I‚Äôm confident they will keep working on Harper to improve it. Since grammar checker LSPs can easily be changed, I‚Äôve decided to go with Harper for now.&lt;/p&gt;
    &lt;p&gt;To install it, homebrew does the job as always:&lt;/p&gt;
    &lt;p&gt;Then I edited my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to add Harper as a secondary Language Server in addition to marksman&lt;/p&gt;
    &lt;p&gt;Finally I can add a markdown linter to ensure my markdown is formatted properly. Several options exist, and markdownlint is one of the most popular. My colleagues recommended the new kid on the block, a Blazing Fast equivalent: rumdl.&lt;/p&gt;
    &lt;p&gt;Installing rumdl was pretty simple on my mac. I only had to add the repository of the maintainer, and install rumdl from it.&lt;/p&gt;
    &lt;p&gt;After that I added a new &lt;code&gt;language-server&lt;/code&gt; to my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; and added it to the language servers to use for the markdown &lt;code&gt;language&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since my website already contained a &lt;code&gt;.markdownlint.yaml&lt;/code&gt; I could import it to the rumdl format with&lt;/p&gt;
    &lt;p&gt;You might have noticed that I‚Äôve added a little quality of life improvement: soft-wrap at 80 characters.&lt;/p&gt;
    &lt;p&gt;Now if you add this to your own &lt;code&gt;config.toml&lt;/code&gt; you will notice that the text is completely left aligned. This is not a problem on small screens, but it rapidly gets annoying on wider screens.&lt;/p&gt;
    &lt;p&gt;Helix doesn‚Äôt support centering the editor. There is a PR tackling the problem but it has been stale for most of the year. The maintainers are overwhelmed by the number of PRs making it their way, and it‚Äôs not clear if or when this PR will be merged.&lt;/p&gt;
    &lt;p&gt;In the meantime, a workaround exists, with a few caveats. It is possible to add spaces to the left gutter (the column with the line numbers) so it pushes the content towards the center of the screen.&lt;/p&gt;
    &lt;p&gt;To figure out how many spaces are needed, you need to get your terminal width with &lt;code&gt;stty&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;In my case, when in full screen, my terminal is 243 characters wide. I need to remove the content column with from it, and divide everything by 2 to get the space needed on each side. In my case for a 243 character wide terminal with a text width of 80 characters:&lt;/p&gt;
    &lt;p&gt;As is, I would add 203 spaces to my left gutter to push the rest of the gutter and the content to the right. But the gutter itself has a width of 4 characters, that I need to remove from the total. So I need to subtract them from the total, which leaves me with &lt;code&gt;76&lt;/code&gt; characters to add.&lt;/p&gt;
    &lt;p&gt;I can open my &lt;code&gt;~/.config/helix/config.toml&lt;/code&gt; to add a new key binding that will automatically add or remove those spaces from the left gutter when needed, to shift the content towards the center.&lt;/p&gt;
    &lt;p&gt;Now when in normal mode, pressing Space then t then z will add/remove the spaces. Of course this workaround only works when the terminal runs in full screen mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Astro&lt;/head&gt;
    &lt;p&gt;Astro works like a charm in VS Code. The team behind it provides a Language Server and a TypeScript plugin to enable code completion and syntax highlighting.&lt;/p&gt;
    &lt;p&gt;I only had to install those globally with&lt;/p&gt;
    &lt;p&gt;Now we need to add a few lines to our &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to tell it how to use the language server&lt;/p&gt;
    &lt;p&gt;We can check that the Astro Language Server can be used by helix with&lt;/p&gt;
    &lt;p&gt;I also like to get a formatter to automatically make my code consistent and pretty for me when I save a file. One of the most popular code formaters out there is Prettier. I‚Äôve decided to go with the fast and easy formatter dprint instead.&lt;/p&gt;
    &lt;p&gt;I installed it with&lt;/p&gt;
    &lt;p&gt;Then in the projects I want to use dprint in, I do&lt;/p&gt;
    &lt;p&gt;I might edit the &lt;code&gt;dprint.json&lt;/code&gt; file to my liking. Finally, I configure Helix to use dprint globally for all Astro projects by appending a few lines in my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;One final check, and I can see that Helix is ready to use the formatter as well&lt;/p&gt;
    &lt;head rend="h3"&gt;YAML&lt;/head&gt;
    &lt;p&gt;For yaml, it‚Äôs simple and straightforward: Helix is preconfigured to use &lt;code&gt;yaml-language-server&lt;/code&gt; as soon as it‚Äôs in the PATH. I just need to install it with&lt;/p&gt;
    &lt;head rend="h2"&gt;Is it worth it?&lt;/head&gt;
    &lt;p&gt;Helix really grew on me. I find it particularly easy and fast to edit code with it. It takes a tiny bit more work to get the language support than it does in VS Code, but it‚Äôs nothing insurmountable. There is a slightly steeper learning curve than for VS Code, but I consider it to be a good thing. It forced me to learn how to move around and edit efficiently, because there is no way to do it inefficiently. Helix remains intuitive once you‚Äôve learned the basics.&lt;/p&gt;
    &lt;p&gt;I am a GNOME enthusiast, and I adhere to the same principles: I like when my apps work out of the box, and when I have little to do to configure them. This is a strong stance that often attracts a vocal opposition. I like products that follow those principles better than those who don‚Äôt.&lt;/p&gt;
    &lt;p&gt;With that said, Helix sometimes feels like it is maintained by one or two people who have a strong vision, but who struggle to onboard more maintainers. As of writing, Helix has more than 350 PRs open. Quite a few bring interesting features, but the maintainers don‚Äôt have enough time to review them.&lt;/p&gt;
    &lt;p&gt;Those 350 PRs mean there is a lot of energy and goodwill around the project. People are willing to contribute. Right now, all that energy is gated, resulting in frustration both from the contributors who feel like they‚Äôre working in the void, and the maintainers who feel like there at the receiving end of a fire hose.&lt;/p&gt;
    &lt;p&gt;A solution to make everyone happier without sacrificing the quality of the project would be to work on a Contributor Ladder. CHAOSS‚Äô Dr Dawn Foster published a blog post about it, listing interesting resources at the end.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ergaster.org/posts/2025/10/29-vscode-to-helix/"/><published>2025-10-29T13:19:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746726</id><title>Recreating a Homebrew Game System from 1987</title><updated>2025-10-29T17:40:00.725923+00:00</updated><content>&lt;doc fingerprint="345a297f4904c8c5"&gt;
  &lt;main&gt;
    &lt;p&gt;The specifications are as follows:&lt;/p&gt;
    &lt;p&gt;Controller input and audio output are handled by either an Intel 8255 or Zilog Z80PIO I/O controller. There are two sockets on the PCB for either controller, depending on which is easier for you to obtain. These two ICs have to be controlled slightly differently by software, but it's possible to write games that are compatible with both, as demonstrated by the games written by Inufuto.&lt;/p&gt;
    &lt;p&gt;The controller interface is designed for two-button Sega Master System controllers and will also work with Mega Drive/Genesis controllers. Standard one-button joysticks will also work, aside from the lack of a second button.&lt;/p&gt;
    &lt;p&gt;The composite sync signal is generated with an EPROM, an unconventional method of simplifying the circuitry. Different ROMs have different data access times, so you may need to experiment with one or two models of ROM before you'll find one that produces a glitchless video signal, due to the high speed at which the raster generator steps through the ROM's address bus.&lt;/p&gt;
    &lt;p&gt;Fortunately, any size of ROM between 4KB (2732) and 64KB (27512) can be used, so long as the 4KB binary data file (available for download further down this page) is written to the upper 4KB of higher capacity ROMs. During testing, I found that a 150ns ROM worked well, while a 450ns ROM was too slow.&lt;/p&gt;
    &lt;p&gt;If the prospect of making a lot of cartridges doesn't appeal to you, I've designed a multi-cartridge that holds sixteen 32KB games on one 27C040 ROM. Game selection on the multi-cartridge is performed with DIP switches.&lt;/p&gt;
    &lt;p&gt;The maximum file size for games is 32KB, but I've designed an experimental bank-switching cartridge PCB (not tested yet!) that should allow games of up to 256KB to be accessed through two configurable 16KB page registers on the cartridge.&lt;/p&gt;
    &lt;p&gt;8255:&lt;/p&gt;
    &lt;p&gt;There's a selection of tools available for programming the Z80 TV Game in C:&lt;/p&gt;
    &lt;p&gt; Schematic - Console &lt;lb/&gt; PDF document, 941 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - Console &lt;lb/&gt; ZIP archive, 744 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - Console &lt;lb/&gt; ZIP archive, 1.33 MB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - 32KB ROM Cartridge &lt;lb/&gt; HTML document, 338 KB &lt;/p&gt;
    &lt;p&gt; Schematic - 32KB ROM Cartridge &lt;lb/&gt; PDF document, 127 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - 32KB ROM Cartridge &lt;lb/&gt; ZIP archive, 170 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - 32KB ROM Cartridge &lt;lb/&gt; ZIP archive, 532 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - 32KB x 16 Multi-Cartridge &lt;lb/&gt; HTML document, 350 KB &lt;/p&gt;
    &lt;p&gt; Schematic - 32KB x 16 Multi-Cartridge &lt;lb/&gt; PDF document, 151 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - 32KB x 16 Multi-Cartridge &lt;lb/&gt; ZIP archive, 191 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - 32KB x 16 Multi-Cartridge &lt;lb/&gt; ZIP archive, 564 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - Experimental 256KB ROM Cartridge &lt;lb/&gt; HTML document, 316 KB &lt;/p&gt;
    &lt;p&gt; Schematic - Experimental 256KB ROM Cartridge &lt;lb/&gt; PDF document, 221 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - Experimental 256KB ROM Cartridge &lt;lb/&gt; ZIP archive, 209 KB - Please note that the 256KB cartridge hasn't yet been tested! &lt;/p&gt;
    &lt;p&gt; KiCad Files - Experimental 256KB ROM Cartridge &lt;lb/&gt; ZIP archive, 603 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Custom Fonts &lt;lb/&gt; ZIP archive, 8.90 MB - Custom fonts used for the KiCad files. Only needed if you want to modify these files. &lt;/p&gt;
    &lt;p&gt; Original Schematics &lt;lb/&gt; ZIP archive, 1.14 MB - Mr. Isizu's original schematics for the Z80 TV Game, with the 74LS122 timing circuit corrected. Includes the 1980's hand-drawn schematic, which has a different memory map to the 2000's CAD schematic that this PCB, emulators, C devtools, etc. are based on. &lt;/p&gt;
    &lt;p&gt; Game ROMs &lt;lb/&gt; ZIP archive, 922 KB - All the games I know to exist for the Z80 TV Game thus far. Includes two combined ROMs for those who would rather have all 26 games on 2 multi-cartridges. If you know of any games that aren't mentioned on this page (or you've written a new game), please let me know! My email address is on the home page. &lt;/p&gt;
    &lt;p&gt; 32KB Cartridge Dimensions &lt;lb/&gt; PDF document, 61.3 KB - Useful for designing a 3D printed cartridge enclosure. Note that the standard PCB thickness used by most manufacturers is 1.6mm. &lt;/p&gt;
    &lt;p&gt; 32KB x 16 Multi-Cartridge Dimensions &lt;lb/&gt; PDF document, 67.1 KB - Useful for designing a 3D printed cartridge enclosure. Note that the standard PCB thickness used by most manufacturers is 1.6mm. &lt;/p&gt;
    &lt;p&gt; Z80 TV Game Logo (1920 x 846) (Variant 1) &lt;lb/&gt; PNG image, 1.21 MB - The logo seen at the top of the page in full resolution. &lt;/p&gt;
    &lt;p&gt; Z80 TV Game Logo (1920 x 846) (Variant 2) &lt;lb/&gt; PNG image, 1.08 MB - The logo seen at the top of the page in full resolution. &lt;/p&gt;
    &lt;p&gt;Inufuto: Developer of Cate, a multi-platform compiler that can generate software for the Z80 TV Game. All 20 of the games he has created with it thus far have Z80 TV Game versions. Inufuto has also designed a PCB version of the Z80 TV Game that outputs VGA video via a Raspberry Pi Pico.&lt;/p&gt;
    &lt;p&gt;Takeda Toshiya: Developer of eZ80TVGAME, a Z80 TV Game emulator for Windows.&lt;/p&gt;
    &lt;p&gt;lsluk: Developer of vdmgr, a multi-platform emulator for Windows that supports the Z80 TV Game.&lt;/p&gt;
    &lt;p&gt;Last updated on Oct 26, 2025. &lt;lb/&gt;This page was first uploaded on Oct 26, 2025. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alex-j-lowry.github.io/z80tvg.html"/><published>2025-10-29T13:42:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747018</id><title>Kafka is Fast ‚Äì I'll use Postgres</title><updated>2025-10-29T17:40:00.319746+00:00</updated><content>&lt;doc fingerprint="6f5090de6e0009c6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intro&lt;/head&gt;
    &lt;p&gt;I feel like the tech world lives in two camps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;One camp chases buzzwords.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp tends to adopt whatever‚Äôs popular without thinking hard about whether it‚Äôs appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.&lt;/p&gt;
    &lt;p&gt;You see this everywhere in the Kafka world: Streaming Lakehouse‚Ñ¢Ô∏è, Kappa‚Ñ¢Ô∏è Architecture, Streaming AI Agents1.&lt;/p&gt;
    &lt;p&gt;This phenomenon is sometimes known as resume-driven design. Modern practices actively encourage this. Consultants push ‚Äúinnovative architectures‚Äù stuffed with vendor tech via ‚Äúinsight‚Äù reports2. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you‚Äôre interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack‚Ñ¢Ô∏è, not for being resourceful.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The other camp chases common sense&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp is far more pragmatic. They strip away unnecessary complexity and steer clear of overengineered solutions. They reason from first principles before making technology choices. They resist marketing hype and approach vendor claims with healthy skepticism.&lt;/p&gt;
    &lt;p&gt;Historically, it has felt like Camp 1 definitively held the upper hand in sheer numbers and noise. Today, it feels like the pendulum may be beginning to swing back, at least a tiny bit. Two recent trends are on the side of Camp 2:&lt;/p&gt;
    &lt;p&gt;Trend 1 - the ‚ÄúSmall Data‚Äù movement. People are realizing two things - their data isn‚Äôt that big and their computers are becoming big too. You can rent a 128-core, 4 TB of RAM instance from AWS. AMD just released 192-core CPUs this summer. That ought to be enough for anybody.3&lt;/p&gt;
    &lt;p&gt;Trend 2 - the Postgres Renaissance. The space is seeing incredible growth and investment4. In the last 2 years, the phrase ‚ÄúJust Use Postgres (for everything)‚Äù has gained a ton of popularity. The basic premise is that you shouldn‚Äôt complicate things with new tech when you don‚Äôt need to, and that Postgres alone solves most problems pretty well. Postgres competes with purpose-built solutions like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Elasticsearch (functionality supported by Postgres‚Äô &lt;code&gt;tsvector&lt;/code&gt;/&lt;code&gt;tsquery&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;MongoDB (&lt;code&gt;jsonb&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Redis (&lt;code&gt;CREATE UNLOGGED TABLE&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;AI Vector Databases (&lt;code&gt;pgvector&lt;/code&gt;,&lt;code&gt;pgai&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Snowflake (&lt;code&gt;pg_mooncake&lt;/code&gt;,&lt;code&gt;pg_duckdb&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and‚Ä¶ Kafka (this blog).&lt;/p&gt;
    &lt;p&gt;The claim isn‚Äôt that Postgres is functionally equivalent to any of these specialized systems. The claim is that it handles 80%+ of their use cases with 20% of the development effort. (Pareto Principle)&lt;/p&gt;
    &lt;p&gt;When you combine the two trends, the appeal becomes obvious. Postgres is a battle-tested, well-known system that is simple, scalable and reliable. Pair it with today‚Äôs powerful hardware and you quickly begin to realize that, more often than not, you do not need the state-of-the-art highly optimized and complex distributed system in order to handle your organization‚Äôs scale.&lt;/p&gt;
    &lt;p&gt;Despite being somebody who is biased towards Kafka, I tend to agree. Kafka is similar to Postgres in that it‚Äôs stable, mature, battle-tested and boasts a strong community. It also scales a lot further. Despite that, I don‚Äôt think it‚Äôs the right choice for a lot of cases. Very often I see it get adopted where it doesn‚Äôt make sense.&lt;/p&gt;
    &lt;p&gt;A 500 KB/s workload should not use Kafka. There is a scalability cargo cult in tech that always wants to choose ‚Äúthe best possible‚Äù tech for a problem - but this misses the forest for the trees. The ‚Äúbest possible‚Äù solution frequently isn‚Äôt a technical question - it‚Äôs a practical one. Adriano makes an airtight case for why you should opt for simple tech in his PG as Queue blog (2023) that originally inspired me to write this.&lt;/p&gt;
    &lt;p&gt;Enough background. In this article, we will do three simple things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for pub/sub messaging - # PG as a Pub/Sub&lt;/item&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for queueing - # PG as a Queue&lt;/item&gt;
      &lt;item&gt;Concisely touch upon when Postgres can be a fit for these use cases - # Should You Use Postgres?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I am not aiming for an exhaustive in-depth evaluation. Benchmarks are messy af. Rather, my goal is to publish some reasonable data points which can start a discussion.&lt;/p&gt;
    &lt;p&gt;(while this article is for Postgres, feel free to replace it with your database of choice)&lt;/p&gt;
    &lt;head rend="h1"&gt;Results TL;DR&lt;/head&gt;
    &lt;p&gt;If you‚Äôd like to skip straight to the results, here they are:&lt;/p&gt;
    &lt;head&gt;üî• The Benchmark Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;‚úçÔ∏è Write&lt;/cell&gt;
        &lt;cell role="head"&gt;üìñ Read&lt;/cell&gt;
        &lt;cell role="head"&gt;üî≠ e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1√ó c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;4.8 MiB/s&lt;p&gt;5036 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.6 MiB/s&lt;p&gt;25 183 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;60 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3√ó c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;4.9 MiB/s&lt;p&gt;5015 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.5 MiB/s&lt;p&gt;25 073 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;~65 % CPU; cross-AZ RF‚âà2.5; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1√ó c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;238 MiB/s&lt;p&gt;243,000 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.16 GiB/s&lt;p&gt;1,200,000 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;853 ms&lt;/cell&gt;
        &lt;cell&gt;~10 % CPU (idle); 30 partitions&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Queue Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;üì¨ Throughput (read + write)&lt;/cell&gt;
        &lt;cell role="head"&gt;üî≠ e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1√ó c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;2.81 MiB/s&lt;p&gt;2885 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;17.7 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; read-client bottleneck&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3√ó c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;2.34 MiB/s&lt;p&gt;2397 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;920 ms ‚ö†Ô∏è6&lt;/cell&gt;
        &lt;cell&gt;replication lag inflated E2E latency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1√ó c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;19.7 MiB/s&lt;p&gt;20,144 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;930 ms ‚ö†Ô∏è6&lt;/cell&gt;
        &lt;cell&gt;~50 % CPU; single-table bottleneck&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Make sure to at least read the last section of the article where we philosophize - # Should You Use Postgres?&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Pub/Sub&lt;/head&gt;
    &lt;p&gt;There are dozens of blogs out there using Postgres as a queue, but interestingly enough I haven‚Äôt seen one use it as a pub-sub messaging system.&lt;/p&gt;
    &lt;p&gt;A quick distinction between the two because I often see them get confused:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Queues are meant for point-to-point communication. They‚Äôre widely used for asynchronous background jobs: worker apps (clients) process a task in the queue like sending an e-mail or pushing a notification. The event is consumed once and it‚Äôs done with. A message is immediately deleted (popped) off the queue once it‚Äôs consumed. Queues do not have strict ordering guarantees7.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pub-sub messaging differs from the queue in that it is meant for one-to-many communication. This inherently means there is a large read fanout - more than one reader client is interested in any given message. Good pub-sub systems decouple readers from writers by storing data on disks. This allows them to not impose a max queue depth limit - something in-memory queues need to do in order to prevent them from going OOM.&lt;/p&gt;
        &lt;p&gt;There is also a general expectation that there is strict order - events should be read in the same order that they arrived in the system.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres‚Äô main competitor here is Kafka, which is the standard in pub-sub today. Various (mostly-proprietary) alternatives exist.8&lt;/p&gt;
    &lt;p&gt;Kafka uses the Log data structure to hold messages. You‚Äôll see my benchmark basically reconstructs a log from Postgres primitives.&lt;/p&gt;
    &lt;p&gt;Postgres doesn‚Äôt seem to have any popular libraries for pub-sub9 use cases, so I had to write my own. The Kafka-inspired workflow I opted for is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Writers produce batches of messages per statement10 (&lt;code&gt;INSERT INTO&lt;/code&gt;). Each transaction carries one batch insert and targets a single&lt;code&gt;topicpartition&lt;/code&gt;table11&lt;/item&gt;
      &lt;item&gt;Each writer is sticky to one table, but in aggregate they produce to multiple tables.&lt;/item&gt;
      &lt;item&gt;Each message has a unique monotonically-increasing offset number. A specific row in a special &lt;code&gt;log_counter&lt;/code&gt;table denotes the latest offset for a given&lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Write transactions atomically update both the &lt;code&gt;topicpartition&lt;/code&gt;data and the&lt;code&gt;log_counter&lt;/code&gt;row. This ensures consistent offset tracking across concurrent writers.&lt;/item&gt;
      &lt;item&gt;Readers poll for new messages. They consume the &lt;code&gt;topicpartition&lt;/code&gt;table(s) sequentially, starting from the lowest offset and progressively reading up.&lt;/item&gt;
      &lt;item&gt;Readers are split into consumer groups. Each group performs separate, independent reads and makes progress on the &lt;code&gt;topicpartition&lt;/code&gt;tables.&lt;/item&gt;
      &lt;item&gt;Each group contains 1 reader per &lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Readers store their progress in a &lt;code&gt;consumer_offsets&lt;/code&gt;table, with a row for each&lt;code&gt;topicpartition,group&lt;/code&gt;pair.&lt;/item&gt;
      &lt;item&gt;Each reader updates the latest processed offset (claiming the records), selects the records and processes them inside a single transaction.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This ensures Kafka-like semantics - gapless, monotonically-increasing offsets and at-least-once/at-most-once processing. This test in particular uses at-least-once semantics, but neither choice should impact the benchmark results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;The benchmark runs &lt;code&gt;N&lt;/code&gt; writer goroutines. These represent writer clients.
Each one loops and atomically inserts &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records while updating the latest offset:&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;The benchmark also runs &lt;code&gt;N&lt;/code&gt; reader goroutines. Each reader is assigned a particular consumer group and partition. The group as a whole reads all partitions while each reader in the group reads only one partition at a time.&lt;/p&gt;
    &lt;p&gt;The reader loops, opens a transaction, optimistically claims &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records (by advancing the offset mark beyond them), selects them and processes the records.
If successful, it commits the transaction and through that advances the offset for the group.&lt;/p&gt;
    &lt;p&gt;It is a pull-based read (just like Kafka), rather than push-based. If the reader has no records to poll, it sleeps for a bit.&lt;/p&gt;
    &lt;p&gt;First it opens a transaction:&lt;/p&gt;
    &lt;p&gt;Then it claims the offsets:&lt;/p&gt;
    &lt;p&gt;Followed by selecting the claimed records:&lt;/p&gt;
    &lt;p&gt;Finally, the data gets processed by the business logic (no-op in our benchmark) and the transaction is closed:&lt;/p&gt;
    &lt;p&gt;If you‚Äôre wondering ‚Äúwhy no &lt;code&gt;NOTIFY/LISTEN&lt;/code&gt;?‚Äù - my understanding of that feature is that it‚Äôs an optimization and cannot be fully relied upon, so polling is required either way12. Given that, I just copied Kafka‚Äôs relatively simple design.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;p&gt;The full code and detailed results are all published on GitHub at stanislavkozlovski/pg-queue-pubsub-benchmark. I ran three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;mostly default Postgres settings (synchronous commit, fsync); &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5036 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.8 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 38.7ms p99 / 6.2ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,183 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.6 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 27.3ms p99 (varied 8.9ms-47ms b/w runs); 4.67ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 60ms p99 / 10.6ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~60% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are pretty good results. It‚Äôs funny to think that the majority of people run a complex distributed system like Kafka for similar workloads13.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;Now, a replicated setup to more accurately mimic the durability and availability guarantees of Kafka.&lt;/p&gt;
    &lt;p&gt;The average of two 5-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5015 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.9 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 153.45ms p99 / 6.8ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,073 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.5 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 57ms p99; 4.91ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 186ms p99 / 12ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~65% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now these are astonishing results! Throughput was not impacted at all. Latency increased but not extremely. Our p99 e2e latency 3x‚Äôd (60ms vs 185ms), but the p95 barely moved from 10.6ms to 12ms.&lt;/p&gt;
    &lt;p&gt;This shows that a simple 3-node Postgres cluster can pretty easily sustain what is a very common Kafka workload - 5 MB/s ingest and 25 MB/s egress. Not only that, but for a cheap cost too. Just $11,514 per year.16&lt;/p&gt;
    &lt;p&gt;Typically, you‚Äôd expect Postgres to run more expensive than Kafka at a certain scale, simply because it wasn‚Äôt designed to be efficient for this use case. Not here though. Running Kafka yourself would cost the same. Running the same workload through a Kafka vendor will cost you at least $50,000 a year. ü§Ø&lt;/p&gt;
    &lt;p&gt;By the way, in Kafka it‚Äôs customary to apply client-side compression on your data. If we assume your messages were 5 KB in size and your clients applied a pretty regular compression ratio of 4x17 - Postgres is actually handling 20 MB/s ingress and 100 MB/s egress.&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;Ok, let‚Äôs see how far Postgres will go.&lt;/p&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge (96 vCPU, 192 GiB RAM) Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;30 topicpartition tables&lt;/item&gt;
      &lt;item&gt;100 writers (~3.33 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;150 reader clients total (5 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 200 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 243,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 238 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 138ms p99 / 47ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 1,200,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 1.16 GiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 24.6ms p99&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 853ms p99 / 242ms p95 / 23.4ms p50&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~10% CPU (basically idle);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;bottleneck: The bottleneck was the write rate per partition. It seems like the test wasn‚Äôt able to write at a higher rate than 8 MiB/s (8k msg/s) per table with this design. I didn‚Äôt push further, but I do wonder now as I write this - how far would writes have scaled?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reads were trivial to scale. Adding more consumer groups was trivial - I tried with 10x fanout and still ran at low CPU. I didn‚Äôt include it because I didn‚Äôt feel the need to push to an unrealistic read-fanout extreme.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;240 MiB/s ingress and 1.16 GiB/s egress are pretty good! The 96 vCPU machine was overkill for this test - it could have done a lot more, or we could have simply opted for a smaller machine. For what it‚Äôs worth, I do think it‚Äôs worth it to deploy a separate Kafka cluster at this scale. Kafka can save you a lot of money here because it can be more efficient in how it handles cross-zone network traffic with features like Diskless Kafka.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pub-Sub Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here ‚Üí üëâ stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;These tests seem to show that Postgres is pretty competitive with Kafka at low scale.&lt;/p&gt;
    &lt;p&gt;You may have noticed none of these tests were particularly long-running. From my understanding, the value in longer-running tests is to test table vacuuming in Postgres, as that can have negative performance effects. In the pub-sub section, vacuuming doesn‚Äôt apply because the tables are append-only. My other reasoning for running shorter tests was to keep costs in check and not spend too much time18.&lt;/p&gt;
    &lt;p&gt;In any case, no benchmark is perfect. My goal wasn‚Äôt to indisputably prove &lt;code&gt;$MY_CLAIM&lt;/code&gt;. Rather, I want to start a discussion by showing that what‚Äôs possible is likely larger than what most people assume. I certainly didn‚Äôt assume I‚Äôd get such good numbers, especially with the pub-sub part.&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Queue&lt;/head&gt;
    &lt;p&gt;In Postgres, a queue can be implemented with &lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;. This command selects an unlocked row and locks it. It also skips reading already-locked rows. That‚Äôs how mutual exclusion is achieved - a worker can‚Äôt get other workers‚Äô jobs.&lt;/p&gt;
    &lt;p&gt;Postgres has a very popular pgmq library that offers a slick queue API. To keep it simple and understand the end-to-end flow better, I decided to write my own queue. The basic version of it is very easy. My workflow is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;add job (&lt;code&gt;INSERT&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;lock row &amp;amp; take job (&lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;process job (&lt;code&gt;{your business logic}&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;mark job as ‚Äúdone‚Äù (&lt;code&gt;UPDATE&lt;/code&gt;a field or&lt;code&gt;DELETE &amp;amp; INSERT&lt;/code&gt;the row into a separate table)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres competes with RabbitMQ, AWS SQS, NATS, Redis19 and to an extent Kafka20 here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;p&gt;We use a simple &lt;code&gt;queue&lt;/code&gt; table. When an element is processed off the queue, it‚Äôs moved into the archive table.&lt;/p&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;N&lt;/code&gt; writer client goroutines.
Each one simply loops and sequentially inserts a single random item into the table:&lt;/p&gt;
    &lt;p&gt;It only inserts one message per statement, which is pretty inefficient at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;M&lt;/code&gt; reader client goroutines. Each reader loops and processes one message.
The processing is done inside a database transaction.&lt;/p&gt;
    &lt;p&gt;Each reader again only works with one message at a time per transaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Results&lt;/head&gt;
    &lt;p&gt;I again ran the same three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of two 15-minute tests. I also ran three 2-minute tests. They all performed similarly. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;all default Postgres settings21&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2885 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.81 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 2.46ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 4.2ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 17.72ms p99&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What I found Postgres wasn‚Äôt good at was handling client count. The bottleneck in this setup was the read clients. Each client could not read more than ~192 messages a second because of its median read latency and sequential read nature.&lt;/p&gt;
    &lt;p&gt;Increasing client count boosted throughput but violated my ~60% CPU target. Trying to run 50 write and 50 read clients got to 4000 msg/s without increasing the queue depth but pegged the server‚Äôs CPU to 100%. I wanted to keep the benchmarks realistic for what you may run in production, rather than maxing out what a machine can do. This would be easily alleviated with a connection pooler (standard across all prod PG deployments) or a larger machine.&lt;/p&gt;
    &lt;p&gt;Another thing worth mentioning is that the workload could sustain a lot more writes than reads. If I didn‚Äôt throttle the benchmark, it would write at 12,000 msg/s and read at 2,800 msg/s. In the spirit of simplicity, I didn‚Äôt debug further and instead throttled my writes to see at what point I could get a stable 1:1 workload.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;A single 10-minute test. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2397 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.34 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 3.3ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 7.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 920ms p99 ‚ö†Ô∏è6; 536ms p95; 7ms p50&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As expected, throughput and latency were impacted somewhat. But not that much. It‚Äôs still over 2000 messages a second, which is pretty good for an HA queue!&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;100 writer clients, 200 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 20,144 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 19.67 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 9.42ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 22.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency: 930ms p99 ‚ö†Ô∏è6; 709ms p95; 12.6ms p50&lt;/item&gt;
      &lt;item&gt;server at 40-60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This run wasn‚Äôt that impressive. There is some bottleneck in the single-table queue approach at this scale which I didn‚Äôt bother figuring out. I figured that it wasn‚Äôt important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one. The 96 vCPU instance would likely scale far further were we to run a few separate queue tables in parallel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Queue Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here ‚Üí üëâ stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;Even a modest Postgres node can durably push thousands of queue ops/sec, which already covers the scale 99% of companies ever hit with a single queue. As I said earlier, the last 2 years have seen the Just Use Postgres slogan become mainstream. The &lt;code&gt;pgmq&lt;/code&gt; library‚Äôs star history captures this trend perfectly:
&lt;/p&gt;
    &lt;head rend="h1"&gt;Should You Use Postgres?&lt;/head&gt;
    &lt;p&gt;Most of the time - yes. You should always default to Postgres until the constraints prove you wrong.&lt;/p&gt;
    &lt;p&gt;Kafka is obviously better optimized for pub-sub workloads. Queue systems are obviously better optimized for queue workloads. The point is that picking a technology based on technical optimization alone is a flawed approach. To throw an analogy:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a Formula One car is optimized to drive faster, but I still use a sedan to go to work. I am way more comfortable driving my sedan than an F1 car.&lt;/p&gt;
      &lt;p&gt;(seriously, see the steering wheel on these things)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Postgres sedan comes with many quality-of-life comforts that the F1 Kafka does not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ability to debug messages with regular SQL&lt;/item&gt;
      &lt;item&gt;ability to delete, re-order or edit messages in place&lt;/item&gt;
      &lt;item&gt;ability to join pub-sub data with regular tables&lt;/item&gt;
      &lt;item&gt;ability to trivially read specific data via rich SQL queries (&lt;code&gt;ID=54&lt;/code&gt;,&lt;code&gt;name="John"&lt;/code&gt;,&lt;code&gt;cost&amp;gt;1000&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Giving up these comforts is a justified sacrifice for your F1 car to go at 378 kmh (235 mph), but masochistic if you plan on driving at 25kmh (15 mph).&lt;/p&gt;
    &lt;p&gt;Donald Knuth warned us in 1974 - premature optimization is the root of all evil. Deploying Kafka at small scale is premature optimization. The point of this article is to show you that this ‚Äúsmall scale‚Äù number has grown further than what people remember it to be - it can comfortably mean many megabytes per second.&lt;/p&gt;
    &lt;p&gt;We are in a Postgres Renaissance for a reason: Postgres is frequently good enough. Modern NVMEs and cheap RAM allow it to scale absurdly high.&lt;/p&gt;
    &lt;p&gt;What‚Äôs the alternative?&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Solutions for Everything?&lt;/head&gt;
    &lt;p&gt;Naive engineers tend to adopt a specialized technology at the slightest hint of a need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Need a cache? Redis, of course!&lt;/item&gt;
      &lt;item&gt;Search? Let‚Äôs deploy Elasticsearch!&lt;/item&gt;
      &lt;item&gt;Offline data analysis? BigQuery or Snowflake - that‚Äôs what our data analysts used at their last job.&lt;/item&gt;
      &lt;item&gt;No schemas? We need a NoSQL database like MongoDB.&lt;/item&gt;
      &lt;item&gt;Have to crunch some numbers on S3? Let‚Äôs use Spark!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A good engineer thinks through the bigger picture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Does this new technology move the needle?&lt;/item&gt;
      &lt;item&gt;Is shaving a few milliseconds off our query worth the extra organizational complexity introduced with the change?&lt;/item&gt;
      &lt;item&gt;Will our users notice?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At small scale, these systems hurt you more than they benefit you. Distributed systems - both in terms of node count and system cardinality - should be respected, feared, avoided and employed only as a weapon of last resort against particularly gnarly problems. Everything with a distributed system becomes more challenging and time-consuming.&lt;/p&gt;
    &lt;p&gt;The problem is the organizational overhead. The organizational overhead of adopting a new system, learning its nuances, configs, establishing monitoring, establishing processes around deployments and upgrades, attaining operational expertise on how to manage it, creating runbooks, testing it, debugging it, adopting its clients and API, using its UI, keeping up with its ecosystem, etc.&lt;/p&gt;
    &lt;p&gt;All of these are real organizational costs that can take months to get right, even if the system in question isn‚Äôt difficult (a lot are). Managed SaaS offerings trade off some of the organizational overhead for greater financial costs - but they still don‚Äôt remove it all. And until you reach the scale where the technology is necessary, you pay these extra {financial, organizational} costs for zero significant gain.&lt;/p&gt;
    &lt;p&gt;If the same can be done with tech for which you‚Äôve already paid the organizational costs for (e.g Postgres), adopting something else prematurely is most definitely an anti-pattern. You don‚Äôt need web-scale technologies when you don‚Äôt have web-scale problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;MVI (a better alternative)&lt;/head&gt;
    &lt;p&gt;What I think is a better approach is to search for the minimum viable infrastructure (MVI): build the smallest amount of system while still providing value.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;choose good-enough technology your org is already familiar with &lt;list rend="ul"&gt;&lt;item&gt;good-enough == meets your users‚Äô needs without being too slow/expensive/insecure&lt;/item&gt;&lt;item&gt;familiar == your org has prior experience, has runbooks/ops setups, monitoring, UI, etc&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;solve a real problem with it&lt;/item&gt;
      &lt;item&gt;use the minimum set of features &lt;list rend="ul"&gt;&lt;item&gt;the fewer features you use, the more flexibility you have to move off the infra in question in the future (e.g if locked in with a vendor)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bonus points if that technology:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;is widely adopted so finding good engineers for it is trivial (Postgres - check)&lt;/item&gt;
      &lt;item&gt;has a strong and growing network effect (Postgres - check)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MVI approach reduces the surface area of your infra. The fewer moving parts you have, the fewer failure modes you worry about and the less glue code you have to maintain.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it‚Äôs human nature to go against this. Just like startups suffer due to MVP bloat (one more feature!), infra teams suffer due to MVI bloat (one more system!)&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are we like this?&lt;/head&gt;
    &lt;p&gt;I won‚Äôt pretend to be able to map out the exact path-dependent outcome, but my guess is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;the zero interest rate era gave us abundant speculative money that was invested in any company that could grow fast&lt;/item&gt;
      &lt;item&gt;a lot of viral internet companies were growing at speeds that led old infra to become obsolete fast&lt;/item&gt;
      &lt;item&gt;this prompted the next wave of ZIRP investment - specialized data infrastructure companies (in a gold rush, sell shovels!); some of these data infra startups spun off directly from the high-growth companies themselves&lt;/item&gt;
      &lt;item&gt;each well-funded data infra vendor is financially motivated to evangelize their product and have you adopt it even when you don‚Äôt need to (Everyone is Talking Their Book). They had deep pockets for marketing and used them.&lt;/item&gt;
      &lt;item&gt;innovative infrastructure software got engineered. It was exciting - so engineers got nerd-sniped into it&lt;/item&gt;
      &lt;item&gt;a web-scale craze/cargo cult developed, where everybody believed they need to be able to scale from zero to millions of RPS because they may go viral any day.&lt;/item&gt;
      &lt;item&gt;a trend developed to copy whatever solutions the most successful, largest digital-native companies were using (Amazon, Google, Uber, etc.)&lt;/item&gt;
      &lt;item&gt;the trend became a self-perpetuating prophecy: these technologies became a sought-after skill on resumes &lt;list rend="ul"&gt;&lt;item&gt;system design interview questions were adapted to test for knowledge of these systems&lt;/item&gt;&lt;item&gt;within an organization, engineers (knowingly or not) pushed for projects that are exciting and helped build their resumes;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This trend continues to grow while there is no strong competing force that is sufficiently motivated to push the opposite view. Even engineers inside a company, who ought to be motivated to keep things simple, have strong incentives to pursue extra complexity. It benefits their career by giving them a project to use as ammo for their next promotion and improves their resume (cool tech/story on there) for their next job-hop. Plus it‚Äôs simply more fun.&lt;/p&gt;
    &lt;p&gt;This is why I think we, as an industry, don‚Äôt always use the simplest solution available.&lt;/p&gt;
    &lt;p&gt;In most cases, Postgres is that simplest solution that is available.&lt;/p&gt;
    &lt;head rend="h2"&gt;But It Won‚Äôt Scale!&lt;/head&gt;
    &lt;p&gt;I want to wrap this article up, but one rebuttal I can‚Äôt miss addressing is the ‚Äúit won‚Äôt scale argument‚Äù.&lt;/p&gt;
    &lt;p&gt;The argument goes something like this: ‚Äúin today‚Äôs age we can go viral at a moment‚Äôs notice; these viral moments are very valuable for our business so we need to aggressively design in a way that keeps our app stable under traffic spikes‚Äù&lt;/p&gt;
    &lt;p&gt;I have three arguments against this:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Postgres Scales&lt;/head&gt;
    &lt;p&gt;As of 2025, OpenAI still uses an unsharded Postgres architecture with only one primary instance for writes22. OpenAI is the poster-child of rapid viral growth. They hold the record for the fastest startup to reach 100 million users.&lt;/p&gt;
    &lt;p&gt;Bohan Zhang, a member of OpenAI‚Äôs infrastructure team and co-founder of OtterTune (a Postgres tuning service), can be quoted as saying23:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúAt OpenAI, we utilize an unsharded architecture with one writer and multiple readers, demonstrating that PostgreSQL can scale gracefully under massive read loads.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúThe main message of my talk was that if you are not too write heavy, you can scale Postgres to a very high read throughput with read replicas using only a single master! That is exactly the message that needs to be spelled out as that covers the vast majority of apps.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúPostgres is probably the default choice for developers right now. You can use Postgres for a very long time. If you are building a startup with read-heavy workloads, just start with Postgres. If you hit a scalability issue, increase the instance size. You can scale it to a very large scale. If in the future the database becomes a bottleneck, congratulations. You have built a successful startup. It‚Äôs a good problem to have.‚Äù&lt;/p&gt;
      &lt;p&gt;(slightly edited for clarity and grammar)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite their rapid growth to a user base of more than 800 million, OpenAI has still NOT opted for a web-scale distributed database. If they haven‚Äôt‚Ä¶ why does your unproven project need to?&lt;/p&gt;
    &lt;head rend="h3"&gt;2. You Have More Time To Scale Than You Think&lt;/head&gt;
    &lt;p&gt;Let‚Äôs say it‚Äôs a good principle to design/test for ~10x your scale. Here are the years of consistent growth rate it takes to get to 10x your current scale:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;annual growth&lt;/cell&gt;
        &lt;cell role="head"&gt;years to hit 10√ó scale&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10 %&lt;/cell&gt;
        &lt;cell&gt;24.16 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25 %&lt;/cell&gt;
        &lt;cell&gt;10.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50 %&lt;/cell&gt;
        &lt;cell&gt;5.68 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;75 %&lt;/cell&gt;
        &lt;cell&gt;4.11 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;100 %&lt;/cell&gt;
        &lt;cell&gt;3.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;150 %&lt;/cell&gt;
        &lt;cell&gt;2.51 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;200 %&lt;/cell&gt;
        &lt;cell&gt;2.10 y&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It goes to show that even at extreme growth levels, you still have years to migrate between solutions. The majority of developers, though, work at companies in the 0-50% growth rate. They are more likely to have moved on to another job by the time the solution needs to change (if ever).&lt;/p&gt;
    &lt;head rend="h3"&gt;3. It‚Äôs Overdesign&lt;/head&gt;
    &lt;p&gt;In an ideal world, you would build for scale and any other future problem you may hit in 10 years.&lt;/p&gt;
    &lt;p&gt;In the real world, you have finite bandwidth, so you have to build for the most immediate, highest ROI problem.&lt;/p&gt;
    &lt;p&gt;Commenter snej on lobste.rs captured it well:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Planning your infrastructure around being able to handle that is sort of like buying a huge Marshall stack as your first guitar amp because your garage band might get invited to open for Coldplay.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Just use Postgres until it breaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Title inspiration comes from a great recent piece - ‚ÄúRedis is fast - I‚Äôll cache in Postgres‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I‚Äôm a complete Postgres noob. You may see a lot of dumb mistakes here. Feel free to call me out on them - I‚Äôm happy to learn. I used AI to help a lot with some of the PG tools to use. This both shows how inexperienced I am in the context and how easy it is to start. I am generally skeptical of AI‚Äôs promise (in the short-term), but there‚Äôs no denying it has made a large dent in democratizing niche/low-level knowledge.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you‚Äôd like to reach out to me, you can find me on LinkedIn or X (Twitter).&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Don‚Äôt worry if you don‚Äôt fully understand these terms. I work full-time in the industry that spews these things and I don‚Äôt have a great grasp either. It‚Äôs marketing slop. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gartner and others push embarrassing recommendations that aren‚Äôt tech driven. It‚Äôs frequently the opposite - they‚Äôre profit driven. Gartner makes $6.72B purely off a consulting service that charges organizations $50k per seat solely for access to reports that recommend these slop architectures. It‚Äôs not crazy to believe, hence many people are converging with the idea that it is a pay-to-win racket model. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seriously, the improvement in hardware is something I find most senior engineers haven‚Äôt properly appreciated. Newest gen AMD CPUs boast 192 cores. Modern SSDs can do 5.5 million random reads a second, or ~28GB/s sequential reads. Both are a 10-20x improvement over the last 10 years alone. Single nodes are more powerful than ever. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Just in the last 6 months - Snowflake acquired Crunchy Data for ~$250M, Databricks acquired Neon for ~$1 billion; In the last 12 months, Supabase more than 5x‚Äôd its valuation from ($900M to $5B), raising $380M across three series (!!!). Within a single year! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;End-to-end latency here is defined as&lt;/p&gt;&lt;code&gt;now() - event_create_time&lt;/code&gt;; In essence, it tracks how long a brand new persisted event takes to get consumed. It helps show cases where queue lag spikes like when consumers temporarily fall behind the write rate. ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some queue tests showed higher E2E latencies which I believe was due to a bug. In the pub-sub tests, I ensured readers start before the writers via a 1000ms sleep. For the queue tests, though, I didn‚Äôt do this. The result is that queue tests immediately spike queue depth at startup because the writers manage to get a head start before the readers. I believe the E2E latency is artificially high because of this flaw in the test. ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actually, things are ordered in the happy path. Only during retries can you get out of order processing. e.g at t=0, client A reads task N; At t=1, client B reads task N+1 and processes it successfully; At t=2, A fails and is unable to process task N; At t=3, client B takes the next available task - which is N. B therefore executes the tasks in order [N+1, N], whereas proper order would have been [N, N+1] ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open-source projects include Apache Pulsar (open source), RedPanda (source-available), AutoMQ (a fork of Kafka) and a lot of proprietary offerings - AWS Kinesis, Google Pub/Sub, Azure Event Hubs, Confluent Kora, Confluent WarpStream, Bufstream to name a few. What‚Äôs common in 90% of these projects is that they all implement the Apache Kafka API, making Kafka undoubtedly the protocol standard in the space. There‚Äôs also an open-source project which exposes a Kafka API on top of a pluggable Postgres or S3 backend - Tansu (Rust, btw :] ) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The most popular library I could find is pg-pubsub with 106 stars as of writing (Oct 2025). Its last commit was 3 months ago. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Batching messages per client is very important for scalability here. It is one of Kafka‚Äôs least-talked-about performance ‚Äútricks‚Äù. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;These tables act as different log data structures. You can see them as separate topics, or partitions of one topic (shards). ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Postgres stores all&lt;/p&gt;&lt;code&gt;NOTIFY&lt;/code&gt;events in a single, global queue. If this queue becomes full, transactions calling&lt;code&gt;NOTIFY&lt;/code&gt;will fail when committing. (src) ‚Ü©&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A report by RedPanda found that ~55% of respondents use Kafka for &amp;lt; 1 MB/s. Kafka-vendor Aiven similarly shared that 50% of their Kafka deployments have an ingest rate of below 10 MB/s. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This replication is equivalent to RF=2 in Kafka with one extra non-synchronous replica. Call it RF=2.5. The client receives a response when the one&lt;/p&gt;&lt;code&gt;sync&lt;/code&gt;replica confirms the change. The other&lt;code&gt;potential&lt;/code&gt;replica is replicating asynchronously without blocking the write path. It will become promoted to&lt;code&gt;sync&lt;/code&gt;if the other one was to die. ‚Ü© ‚Ü©2&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The tests didn‚Äôt direct any read traffic to the standbys. This caused extra load on the primary - most production workloads would read from the standbys. Despite that, the results were still good! In my tests, I found that the extra read workload didn‚Äôt seem to have a negative effect on the database - it seems such tail reads were served exclusively from cache. ‚Ü© ‚Ü©2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The node and its disk cost $1826 per year. Since we run three of those, it‚Äôs $5478/yr. The networking in AWS costs $0.02/GB and our setup is replicating 4.9MB/s across two instances - that results in 294.74TB cross-zone networking per year. That‚Äôs $6036 per year in replication networking. Assuming your clients are in the same zone as the database they‚Äôre writing to / reading from, that networking is free. That results in an annual cost of $11,514. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can realistically achieve a 10x+ compression ratio if operating on compressible data like logs (something Kafka is used for frequently). The only gotcha is that we need to compress larger batches - eg 25KB+ - so that requires a bit of a re-design in the pub-sub data model. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I had already spent enough business days working on this benchmark and re-running tests numerous, numerous times as I iterated on the benchmark and the methodology. On the larger instances, the cost accumulates fast and running longer tests at high MB/s rates requires deploying much larger and more expensive disks in order to store all the accumulated data. The effort spent matches the goal I have with the article. If any Postgres vendor wants to sponsor a more thorough investigation - let me know! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Surprisingly (to me), Redis is a very popular queue-like backend choice for background jobs. Most popular open-source libraries use it. Although I‚Äôm sure Postgres can do just as good a job, many devs will prefer to use an established library rather than build one from scratch or use something less well-maintained. I do think PG-backed libraries should get developed though! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kafka has historically never been a queue. To use it as one, you had to develop some difficult workarounds. Today, however, it is in the middle of implementing a first-class Queue-like interface (currently in Preview) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most importantly, synchronous commit and fsync are both on. This means every write is durably persisted to disk. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The optimizations they did to support this scale are cool, but not novel. See these two talks at a) PGConf.dev 2025 (my transcript) and b) POSETTE (my transcript) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the talks PGConf.dev 2025 (my transcript) and POSETTE (my transcript) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks"/><published>2025-10-29T14:06:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747112</id><title>I made a 10¬¢ MCU Talk</title><updated>2025-10-29T17:39:59.981235+00:00</updated><content>&lt;doc fingerprint="a50cd3b339220607"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;TLDR: Yes, you can fit about 7 seconds of audio into 16K of flash and still have room for code. And you can even play LPC encoded audio on a 10 cent MCU.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There‚Äôs quite a lot more detail in this video (and of course you can hear the audio!).&lt;/p&gt;
    &lt;p&gt;In the previous project, I had this ultra-cheap CH32V003 microcontroller playing simple tunes on a tiny SMD buzzer. It was just toggling a GPIO pin at musical note frequencies ‚Äì 1-bit audio output ‚Äì and it sounded surprisingly decent. That was a fun start, but now it‚Äôs time to push this little $0.10 MCU even further: can we make it actually talk?&lt;/p&gt;
    &lt;p&gt;Spoiler: Yes, we can! (well, there wouldn‚Äôt be much of a blog post if we couldn‚Äôt) This 8-pin RISC-V chip is now producing sampled audio data and spoken words. We‚Äôre really stretching the limits of what you can fit in 16 KB of flash.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Beeps to Actual Audio&lt;/head&gt;
    &lt;p&gt;Moving from simple beeps to real audio meant using the microcontroller‚Äôs PWM output as a rudimentary DAC. Instead of just on/off beeping, I‚Äôm driving a waveform at an 8 kHz sample rate using a high-frequency PWM on the output pin. The hardware is the same tiny board as before ‚Äì but I‚Äôve swapped the small SMD buzzer for a small speaker. The buzer works too, but it‚Äôs quieter and very tinny.&lt;/p&gt;
    &lt;p&gt;The sample I wanted to test with is just over 6 seconds in length - it‚Äôs the iconic ‚ÄúOpen the pod bay doors HAL‚Ä¶‚Äù sequence from 2001.&lt;/p&gt;
    &lt;p&gt;If we keep this audio at 16-bit PCM, 8kHZ, we‚Äôd need about 96KB ‚Äì way beyond our 16 KB flash! And remember, that 16 KB has to hold both the audio data and our playback code. Clearly some aggressive compression is required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Bits/Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Fits in 16KB?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CD Quality&lt;/cell&gt;
        &lt;cell&gt;44.1 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;529 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå 33√ó too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Phone Quality&lt;/cell&gt;
        &lt;cell&gt;16 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;192 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå 12√ó too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Basic PCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;8-bit&lt;/cell&gt;
        &lt;cell&gt;48 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå 3√ó too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;4-bit ADPCM (IMA)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;4-bit&lt;/cell&gt;
        &lt;cell&gt;24 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå 1.5√ó too big&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;QOA (Quite OK Audio)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;3.2-bit&lt;/cell&gt;
        &lt;cell&gt;19 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå Still too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2-bit ADPCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;2-bit&lt;/cell&gt;
        &lt;cell&gt;12 KB&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Fits!&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I considered a few encoding options for compressing the audio.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8-bit PCM: Simply using 8-bit samples at 8 kHz cuts size in half (to ~47 KB for 6s), but that‚Äôs still about 3√ó too large for our flash.&lt;/item&gt;
      &lt;item&gt;4-bit ADPCM: Adaptive Differential PCM is a simple lossy compression that could quarter the size. In theory 6 seconds would be ~24 KB ‚Äì much closer to fitting,&lt;/item&gt;
      &lt;item&gt;‚ÄúQuite OK Audio‚Äù (QOA): This is nice codec that packs audio into about 3.2 bits per sample (roughly 1/5 the size of 16-bit PCM)&lt;/item&gt;
      &lt;item&gt;2-bit ADPCM: Going even further with ADPCM, using only 2 bits per sample gives a 4:1 compression relative to 8-bit audio ‚Äì that‚Äôs 75% storage savings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2-bit ADPCM is definitely the winner here. Our 6-second clip shrinks to under 12 KB, which comfortably fits in flash with room for code. This looked like the winner, provided the audio quality was acceptable. The decoder for 2-bit ADPCM is also very lightweight (my implementation compiled to under just over 1K of code - 1340 bytes!). It‚Äôs definitely low quality - but it actually sounds surprisingly ok.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does 2-bit ADPCM work?&lt;/head&gt;
    &lt;p&gt;It‚Äôs actually a very simple algorithm. Both the encoder and decoder maintain a predicted signal value and a step size index into a predefined table. Each 2-bit code tells the decoder how to adjust the current prediction and the step size index. In essence, we‚Äôre coding the difference between the real audio and our prediction, with only four possible levels (since 2 bits gives 4 values). After each sample, the algorithm adapts: if the prediction error was large, we move to a bigger step size (to allow larger changes); if the error was small, we use a smaller step size for finer resolution. This adaptive step is what makes it ADPCM (Adaptive Differential PCM).&lt;/p&gt;
    &lt;p&gt;Our codes are as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;00&lt;/code&gt;(0): Go down by 1 step - subtract the step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01&lt;/code&gt;(1): Go up by 1 step - add the step size to our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;10&lt;/code&gt;(2): Go down by 2 steps - subtract the 2 x step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;11&lt;/code&gt;(3): Go up by 2 steps - add the 2 x step size to our current prediction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with this very high level of compression, the predicted waveform manages to track the original audio surprisingly well. The above graph shows a small snippet of the audio: the blue line is the original waveform and the yellow line is the ADPCM decoder‚Äôs output.&lt;/p&gt;
    &lt;p&gt;They‚Äôre not identical (and we wouldn‚Äôt expect them to be), but the general shape is preserved. When you play it back through the little speaker, it‚Äôs recognizable and surprisingly good.&lt;/p&gt;
    &lt;p&gt;To make my life easier, I built a quick conversion tool to encode WAV files into this 2-bit ADPCM format. The tool lets me drag-and-drop a WAV, and it gives you the files with the data that can ve dropped into the firmware code.&lt;/p&gt;
    &lt;head rend="h2"&gt;LPC Speech Synthesis&lt;/head&gt;
    &lt;p&gt;Six seconds of audio is cool, but what about longer phrases or even arbitrary speech? Storing anything much longer with raw or ADPCM audio would quickly fill the 16K of flash.&lt;/p&gt;
    &lt;p&gt;For my second experiment, I tried something different: instead of recorded waveform audio, I used an old-school speech synthesis approach. This leverages the fact that spoken language can be encoded very compactly by modeling the human voice, rather than storing the raw sound. Specifically, I integrated a library called Talkie.&lt;/p&gt;
    &lt;p&gt;Talkie is a software implementation of the Texas Instruments LPC speech synthesis architecture from the late 1970s. This was implemented in a variety of chips, most commonly the TMS5220 and TMS5100 speech chips.&lt;/p&gt;
    &lt;p&gt;These were used in things like the original Speak &amp;amp; Spell, arcade games like early Star Wars, and speech add-ons for home computers (e.g. the BBC Micro).&lt;/p&gt;
    &lt;p&gt;The Talkie library (originally by Peter Knight, later added to by Adafruit) comes with a big set of examples and vocabulary. It‚Äôs also possible to extract examples from old ROMs from arcade games.&lt;/p&gt;
    &lt;p&gt;Each phrase or word only takes a few hundred bytes or even less, so you can fit quite a lot of speech into a few kilobytes of flash. The trade-off is that the voice has a very computer-esque timbre ‚Äì think of the Speak &amp;amp; Spell‚Äôs voice. It‚Äôs clearly synthetic, but still understandable.&lt;/p&gt;
    &lt;p&gt;To say custom sentences not in the library, you either concatenate the available words/phonemes (which can be clunky), or you need to generate new LPC data. The original tools for this are a bit obscure ‚Äì there‚Äôs BlueWizard (a classic Mac app) and PythonWizard (a command-line tool with TK GUI) which can analyze WAV files and produce LPC data.&lt;/p&gt;
    &lt;p&gt;I gave both a try with some success (and a few headaches setting them up). In the end, I cheated a bit and used an AI coding assistant to help me create a streamlined online tool for this.&lt;/p&gt;
    &lt;p&gt;The result is a little web app where I can upload a recording of, say, my own voice, and it outputs the LPC data. It even lets me play back the synthesized voice in-browser to check it.&lt;/p&gt;
    &lt;p&gt;So there we have it ‚Äì our 10¬¢ microcontroller now has a voice! By using 2-bit ADPCM compression, we can store short audio clips (up to around 8 seconds) even in 16 KB of flash, and play them back via PWM with decent fidelity.&lt;/p&gt;
    &lt;p&gt;And with the Talkie LPC speech synthesis, we can make the device ‚Äúspeak‚Äù lots of words and phrases with only a tiny memory footprint.&lt;/p&gt;
    &lt;p&gt;If you want to hear it for yourself, check out the video demo linked at the top of this post. In the video, you‚Äôll see (and hear) the WarGames clip and the Star Wars quotes running on the hardware. It‚Äôs honestly amazing what these cheap little MCUs can do. We‚Äôre really pushing the boundaries of cheap hardware here.&lt;/p&gt;
    &lt;p&gt;You can find all my code on GitHub in this repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.atomic14.com/2025/10/29/CH32V003-talking"/><published>2025-10-29T14:12:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747402</id><title>Show HN: HUD-like live annotation and sketching app for macOS</title><updated>2025-10-29T17:39:59.800609+00:00</updated><content>&lt;doc fingerprint="354456b7533f15ed"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Annotate, explain, create - anywhere on your screen&lt;/head&gt;&lt;p&gt;Draw Over It gives presenters, trainers, and any professional an always-ready overlay for live markups. Open it with a hotkey, sketch directly on top of any app, and jump back to work without leaving a trace.&lt;/p&gt;Mac App Store&lt;head rend="h2"&gt;Always-on overlay for live sessions&lt;/head&gt;&lt;p&gt;Stay in flow while you mark up anything on screen. Draw Over It floats above your desktop and hides with a single hotkey.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Overlay the canvas on any app instantly with the menu bar controls or just √¢√¢¬•√¢D combination.&lt;/item&gt;&lt;item&gt;Access the HUD toolkit easily with a right-click.&lt;/item&gt;&lt;item&gt;With just a click, blur everything else to keep attention on your key points.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Tools built for clear annotations&lt;/head&gt;&lt;p&gt;Summon the SwiftUI HUD with a right-click and reach pens, highlighters, shapes, blur, and session controls without leaving the overlay.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Swap between pens, highlighters, rectangles, and circles with one click.&lt;/item&gt;&lt;item&gt;Configure pen widths from 1√¢64 pt, opacity, fill modes, and color presets.&lt;/item&gt;&lt;item&gt;Use quick presets for Pen sizes, highlight box, and highlighter to stay in rhythm.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Capture your ideas close to the source&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Save the canvas at any time so you can get back to that state quickly.&lt;/item&gt;&lt;item&gt;Export the canvas state as transparent PNGs ready for sharing.&lt;/item&gt;&lt;item&gt;The entire application is available in 14 languages.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Switch tools, tweak width or opacity, and pick presets in one floating palette.&lt;/p&gt;&lt;p&gt;Easily annotate any document, presentation or codebase&lt;/p&gt;&lt;p&gt;Export your canvas to a PNG with a single click&lt;/p&gt;&lt;p&gt;Visualise your ideas close to the source&lt;/p&gt;&lt;p&gt;Easy to use tooling always at your fingertips&lt;/p&gt;&lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;&lt;head rend="h3"&gt;How do I toggle the overlay?&lt;/head&gt;&lt;p&gt;Use √¢√¢¬•√¢D or the status bar menu. The overlay floats above every window until you hide it.&lt;/p&gt;&lt;head rend="h3"&gt;How do I open the tool HUD?&lt;/head&gt;&lt;p&gt;Right-click the overlay or press √¢√¢¬•√¢H. The HUD appears near your pointer for quick adjustments.&lt;/p&gt;&lt;head rend="h3"&gt;Can I erase or undo mistakes?&lt;/head&gt;&lt;p&gt;Hold Option to switch to the eraser or press √¢√¢¬•√¢Z to undo. Redo lives in the HUD and app menus.&lt;/p&gt;&lt;head rend="h3"&gt;Where are exports saved?&lt;/head&gt;&lt;p&gt;Transparent PNG exports live in ~/Library/Application Support/DrawOverIt/Exports inside the app sandbox.&lt;/p&gt;&lt;head rend="h3"&gt;Does the app remember my annotations?&lt;/head&gt;&lt;p&gt;Yes. Sessions persist between launches, and you can save or restore snapshots manually. Enable Ephemeral Canvas if you prefer a fresh slate every time.&lt;/p&gt;&lt;head rend="h3"&gt;Which macOS versions are supported?&lt;/head&gt;&lt;p&gt;Draw Over It targets macOS 13.5 or later with universal binaries for Apple Silicon and Intel.&lt;/p&gt;&lt;head rend="h2"&gt;Ready to draw over anything?&lt;/head&gt;&lt;p&gt;Try it yourself and see how easy visualising your ideas becomes!&lt;/p&gt;Mac App Store&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://draw.wrobele.com/"/><published>2025-10-29T14:36:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747804</id><title>Collins Aerospace: Sending text messages to the cockpit with test:test</title><updated>2025-10-29T17:39:59.272564+00:00</updated><content>&lt;doc fingerprint="ef98aafde223bee7"&gt;
  &lt;main&gt;
    &lt;p&gt;Informed parties: RTX Corporation and Department of Defense Cyber Crime Center (on September 21, 2025)&lt;/p&gt;
    &lt;p&gt;Using the credentials test:test, it was possible to log in at the ARINC OpCenter Message Browser (Screenshot) as U.S. Navy Fleet Logistics Support Wing.&lt;/p&gt;
    &lt;p&gt;Via this web service, text messages can be sent to the cockpit. For obvious reasons, we did not try that. Sent messages could be viewed.&lt;/p&gt;
    &lt;p&gt;Unfortunately, RTX did not respond to our vulnerability report. The account was disabled.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ccc.de/en/disclosure/collins-aerospace-mit-test-test-textnachrichten-bis-ins-cockpit-senden"/><published>2025-10-29T15:07:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748186</id><title>Hosting SQLite Databases on GitHub Pages (2021)</title><updated>2025-10-29T17:39:59.155132+00:00</updated><content>&lt;doc fingerprint="3d0f8f5e6fa86b83"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Hosting SQLite databases on Github Pages&lt;/head&gt;&lt;p&gt;(or IPFS or any static file hoster)&lt;/p&gt;‚Ä¢ Last Update&lt;p&gt;I was writing a tiny website to display statistics of how much sponsored content a Youtube creator has over time when I noticed that I often write a small tool as a website that queries some data from a database and then displays it in a graph, a table, or similar. But if you want to use a database, you either need to write a backend (which you then need to host and maintain forever) or download the whole dataset into the browser (which is not so great when the dataset is more than 10MB).&lt;/p&gt;&lt;p&gt;In the past when I‚Äôve used a backend server for these small side projects at some point some external API goes down or a key expires or I forget about the backend and stop paying for whatever VPS it was on. Then when I revisit it years later, I‚Äôm annoyed that it‚Äôs gone and curse myself for relying on an external service - or on myself caring over a longer period of time.&lt;/p&gt;&lt;p&gt;Hosting a static website is much easier than a "real" server - there‚Äôs many free and reliable options (like GitHub, GitLab Pages, Netlify, etc), and it scales to basically infinity without any effort.&lt;/p&gt;&lt;p&gt;So I wrote a tool to be able to use a real SQL database in a statically hosted website!&lt;/p&gt;&lt;p&gt;Here‚Äôs a demo using the World Development Indicators dataset - a dataset with 6 tables and over 8 million rows (670 MiByte total).&lt;/p&gt;&lt;code&gt;select country_code, long_name from wdi_country limit 3;&lt;/code&gt;&lt;p&gt;As you can see, we can query the &lt;code&gt;wdi_country&lt;/code&gt; table while fetching only 1kB of data!&lt;/p&gt;&lt;p&gt;This is a full SQLite query engine. As such, we can use e.g. the SQLite JSON functions:&lt;/p&gt;&lt;code&gt;select json_extract(arr.value, '$.foo.bar') as bar
  from json_each('[{"foo": {"bar": 123}}, {"foo": {"bar": "baz"}}]') as arr&lt;/code&gt;&lt;p&gt;We can also register JS functions so they can be called from within a query. Here‚Äôs an example with a &lt;code&gt;getFlag&lt;/code&gt; function that gets the flag emoji for a country:&lt;/p&gt;&lt;code&gt;function getFlag(country_code) {
  // just some unicode magic
  return String.fromCodePoint(...Array.from(country_code||"")
    .map(c =&amp;gt; 127397 + c.codePointAt()));
}

await db.create_function("get_flag", getFlag)
return await db.query(`
  select long_name, get_flag("2-alpha_code") as flag from wdi_country
    where region is not null and currency_unit = 'Euro';
`)&lt;/code&gt;&lt;p&gt;Press the Run button to run the following demos. You can change the code in any way you like, though if you make a query too broad it will fetch large amounts of data ;)&lt;/p&gt;&lt;p&gt;Note that this website is 100% hosted on a static file hoster (GitHub Pages).&lt;/p&gt;&lt;p&gt;So how do you use a database on a static file hoster? Firstly, SQLite (written in C) is compiled to WebAssembly. SQLite can be compiled with emscripten without any modifications, and the sql.js library is a thin JS wrapper around the wasm code.&lt;/p&gt;&lt;p&gt;sql.js only allows you to create and read from databases that are fully in memory though - so I implemented a virtual file system that fetches chunks of the database with HTTP Range requests when SQLite tries to read from the filesystem: sql.js-httpvfs. From SQLite‚Äôs perspective, it just looks like it‚Äôs living on a normal computer with an empty filesystem except for a file called &lt;code&gt;/wdi.sqlite3&lt;/code&gt; that it can read from. Of course it can‚Äôt write to this file, but a read-only database is still very useful.&lt;/p&gt;&lt;p&gt;Since fetching data via HTTP has a pretty large overhead, we need to fetch data in chunks and find some balance between the number of requests and the used bandwidth. Thankfully, SQLite already organizes its database in "pages" with a user-defined page size (4 KiB by default). I‚Äôve set the page size to 1 KiB for this database.&lt;/p&gt;&lt;p&gt;Here‚Äôs an example of a simple index lookup query:&lt;/p&gt;&lt;code&gt;select indicator_code, long_definition from wdi_series where indicator_name
    = 'Literacy rate, youth total (% of people ages 15-24)'&lt;/code&gt;&lt;p&gt;Run the above query and look at the page read log. SQLite does 7 page reads for that query.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Three page reads are just some to get some schema information (these are already cached)&lt;/item&gt;&lt;item&gt;Two page reads are the index lookup in the index &lt;code&gt;on wdi_series (indicator_name)&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Two page reads are on the &lt;code&gt;wdi_series&lt;/code&gt;table data (the first to find the row value by primary key, the second to get the text data from an overflow page)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Both the index as well as the table reads are B-Tree lookups.&lt;/p&gt;&lt;p&gt;A more complex query: What are the countries with the lowest youth literacy rate, based on the newest data from after 2010?&lt;/p&gt;&lt;code&gt;with newest_datapoints as (
  select country_code, indicator_code, max(year) as year from wdi_data
  join wdi_series using (indicator_code)
  where
    indicator_name = 'Literacy rate, youth total (% of people ages 15-24)'
    and year &amp;gt; 2010
  group by country_code
)
select c.short_name as country, printf('%.1f %%', value) as "Youth Literacy Rate"
from wdi_data
  join wdi_country c using (country_code)
  join newest_datapoints using (indicator_code, country_code, year)
order by value asc limit 10&lt;/code&gt;&lt;p&gt;The above query should do 10-20 GET requests, fetching a total of 130 - 270KiB, depending on if you ran the above demos as well. Note that it only has to do 20 requests and not 270 (as would be expected when fetching 270 KiB with 1 KiB at a time). That‚Äôs because I implemented a pre-fetching system that tries to detect access patterns through three separate virtual read heads and exponentially increases the request size for sequential reads. This means that index scans or table scans reading more than a few KiB of data will only cause a number of requests that is logarithmic in the total byte length of the scan. You can see the effect of this by looking at the "Access pattern" column in the page read log above.&lt;/p&gt;&lt;p&gt;All of this only works well when we have indices in the database that match the queries well. For example, the index used in the above query is a &lt;code&gt;INDEX ON wdi_data (indicator_code, country_code, year, value)&lt;/code&gt;. If that index did not include the value column, the SQLite engine would have to do another random access (unpredictable) read and thus HTTP request to retrieve the actual value for every data point. If the index was ordered &lt;code&gt;country_code, indicator_code, ...&lt;/code&gt;, then we would be able to quickly get all indicators for a single country, but not all country values of a single indicator.&lt;/p&gt;&lt;p&gt;We can also make use of the SQLite FTS module so we can do a full-text search on the more text-heavy information in the database - in this case there are over 1000 human development indicators in the database with longer descriptions.&lt;/p&gt;&lt;code&gt;select * from indicator_search
where indicator_search match 'educatio* femal*'
order by rank limit 10&lt;/code&gt;&lt;p&gt;The total amount of data in the &lt;code&gt;indicator_search&lt;/code&gt; FTS table is around 8 MByte. The above query should only fetch around 70 KiB. You can see how it is constructed here.&lt;/p&gt;&lt;p&gt;And finally, here‚Äôs a more complete demonstration of the usefulness of this system - here‚Äôs an interactive graph showing the development of a few countries over time, for any countries you want using any indicator from the dataset:&lt;/p&gt;&lt;head&gt;Extra information about this indicator&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Indicator Code&lt;/item&gt;&lt;item rend="dd-1"&gt;IT.NET.USER.ZS&lt;/item&gt;&lt;item rend="dt-2"&gt;Long definition&lt;/item&gt;&lt;item rend="dd-2"&gt;Internet users are individuals who have used the Internet (from any location) in the last 3 months. The Internet can be used via a computer, mobile phone, personal digital assistant, games machine, digital TV etc.&lt;/item&gt;&lt;item rend="dt-3"&gt;Statistical concept and methodology&lt;/item&gt;&lt;item rend="dd-3"&gt;The Internet is a world-wide public computer network. It provides access to a number of communication services including the World Wide Web and carries email, news, entertainment and data files, irrespective of the device used (not assumed to be only via a computer - it may also be by mobile phone, PDA, games machine, digital TV etc.). Access can be via a fixed or mobile network. For additional/latest information on sources and country notes, please also refer to: https://www.itu.int/en/ITU-D/Statistics/Pages/stat/default.aspx&lt;/item&gt;&lt;item rend="dt-4"&gt;Development relevance&lt;/item&gt;&lt;item rend="dd-4"&gt;The digital and information revolution has changed the way the world learns, communicates, does business, and treats illnesses. New information and communications technologies (ICT) offer vast opportunities for progress in all walks of life in all countries - opportunities for economic growth, improved health, better service delivery, learning through distance education, and social and cultural advances. Today's smartphones and tablets have computer power equivalent to that of yesterday's computers and provide a similar range of functions. Device convergence is thus rendering the conventional definition obsolete. Comparable statistics on access, use, quality, and affordability of ICT are needed to formulate growth-enabling policies for the sector and to monitor and evaluate the sector's impact on development. Although basic access data are available for many countries, in most developing countries little is known about who uses ICT; what they are used for (school, work, business, research, government); and how they affect people and businesses. The global Partnership on Measuring ICT for Development is helping to set standards, harmonize information and communications technology statistics, and build statistical capacity in developing countries. However, despite significant improvements in the developing world, the gap between the ICT haves and have-nots remains.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note that many indicators are only available for some countries, for example the indicator "Women who believe a husband is justified in beating his wife when she burns the food" is based on surveys only conducted in lower-developed countries.&lt;/p&gt;&lt;head rend="h2"&gt;Bonus: DOM as a database&lt;/head&gt;&lt;p&gt;Since we‚Äôre already running a database in our browser, why not use our browser as a database using a virtual table called &lt;code&gt;dom&lt;/code&gt;?&lt;/p&gt;&lt;code&gt;select count(*) as number_of_demos from dom
  where selector match '.content div.sqlite-httpvfs-demo';
select count(*) as sqlite_mentions from dom
  where selector match '.content p' and textContent like '%SQLite%';&lt;/code&gt;&lt;p&gt;We can even insert elements directly into the DOM:&lt;/p&gt;&lt;code&gt;insert into dom (parent, tagName, textContent)
    select 'ul#outtable1', 'li', short_name
    from wdi_country where currency_unit = 'Euro'&lt;/code&gt;&lt;p&gt;Output:&lt;/p&gt;&lt;p&gt;And update elements in the DOM:&lt;/p&gt;&lt;code&gt;update dom set textContent =
  get_flag("2-alpha_code") || ' ' || textContent
from wdi_country
where selector match 'ul#outtable1 &amp;gt; li'
  and textContent = wdi_country.short_name&lt;/code&gt;&lt;p&gt;Of course, everything here is open source. The main implementation of the sqlite wrapper is in sql.js-httpvfs. The source code of this blog post is a pandoc markdown file, with the demos being a custom "fenced code block" React component.&lt;/p&gt;&lt;head rend="h2"&gt;Update 2023: Future Work&lt;/head&gt;&lt;p&gt;Since I wrote this article in 2021, a lot of interesting things have happened in this space, many inspired by this proof of concept! Here‚Äôs some highlights:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;libgen-ipfs: implements a search engine for the Library Genesis based on IPFS and sql.js-httpvfs.&lt;/item&gt;&lt;item&gt;absurd-sql: implements a backend for sql.js (sqlite3 compiled for the web) that treats IndexedDB like a disk and stores data in blocks there. Inspired by this article.&lt;/item&gt;&lt;item&gt;sqlite itself has now added official support for wasm, citing absurd-sql as related work!&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://phiresky.github.io/blog/2021/hosting-sqlite-databases-on-github-pages/"/><published>2025-10-29T15:32:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748195</id><title>The end of the rip-off economy: consumers use LLMs against information asymmetry</title><updated>2025-10-29T17:39:59.063608+00:00</updated><content/><link href="https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy"/><published>2025-10-29T15:32:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748570</id><title>Tell HN: Twilio support replies with hallucinated features</title><updated>2025-10-29T17:39:58.641790+00:00</updated><content>&lt;doc fingerprint="fe03d1565042ad40"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I was investigating some bug with our voice system and asked support where I can find some debugging information and event logs.&lt;/p&gt;
      &lt;p&gt;They told me where I should go in the interface to see that and reassured that they checked logs and this event exist.&lt;/p&gt;
      &lt;p&gt;It turned out these features and information doesn't exist anywhere in the interface and impossible to retrieve in any way. The support message with hallucinated features is mostly AI written.&lt;/p&gt;
      &lt;p&gt;CEOs tell us AGI is around the corner but in reality it just unreliable information and AI can't even restock the vending machine.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748570"/><published>2025-10-29T15:54:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748608</id><title>AirTips ‚Äì Alternative to Bento.me/Linktree</title><updated>2025-10-29T17:39:57.933678+00:00</updated><content>&lt;doc fingerprint="7b1933f95ea4aa9d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;acoffee&lt;/head&gt;
    &lt;p&gt;NO.0002Pro&lt;/p&gt;
    &lt;p&gt;Links and projects by acoffee ‚Äî compact, restrained, and a bit playful.&lt;/p&gt;
    &lt;p&gt;TIPS&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;tips received&lt;/p&gt;
    &lt;p&gt; Compact Customizable API‚Äëready &lt;/p&gt;
    &lt;p&gt; Invite‚Äëonly &lt;/p&gt;
    &lt;head rend="h2"&gt;Gallery&lt;/head&gt;
    &lt;p&gt;A.coffee/becool&lt;/p&gt;
    &lt;p&gt;Maker&lt;/p&gt;
    &lt;head rend="h2"&gt;Modules&lt;/head&gt;
    &lt;p&gt;TEXT&lt;/p&gt;
    &lt;head rend="h3"&gt;New note&lt;/head&gt;
    &lt;p&gt;This is a text module.&lt;/p&gt;
    &lt;p&gt;Number&lt;/p&gt;
    &lt;p&gt;1024&lt;/p&gt;
    &lt;head rend="h3"&gt;Custom HTML&lt;/head&gt;
    &lt;p&gt;Build playful, precise UI with pure HTML.&lt;/p&gt;
    &lt;p&gt; ‚ö° Compact üéõÔ∏è Customizable üîó API‚Äëready &lt;/p&gt;
    &lt;p&gt;LAYOUT&lt;/p&gt;
    &lt;p&gt;Responsive grids, badges, cards.&lt;/p&gt;
    &lt;p&gt;CONTENT&lt;/p&gt;
    &lt;p&gt;Headings, lists, emojis, inline SVG.&lt;/p&gt;
    &lt;p&gt;SAFE&lt;/p&gt;
    &lt;p&gt;No scripts. Pure, secure markup.&lt;/p&gt;
    &lt;head class="cursor-pointer text-sm font-medium text-slate-800"&gt;See a tiny JSON payload&lt;/head&gt;
    &lt;code&gt;{
    "items": [
      { "kind": "module", "type": "text", "id": "mod-1" },
      { "kind": "module", "type": "image", "id": "mod-2" }
    ]
  }&lt;/code&gt;
    &lt;p&gt;PROJECTS&lt;/p&gt;
    &lt;p&gt;IMAGE&lt;/p&gt;
    &lt;p&gt;Metric1&lt;/p&gt;
    &lt;p&gt;9383&lt;/p&gt;
    &lt;p&gt;Metric2&lt;/p&gt;
    &lt;p&gt;2.6k&lt;/p&gt;
    &lt;p&gt;Metri3&lt;/p&gt;
    &lt;p&gt;1234.5&lt;/p&gt;
    &lt;p&gt;This is a link&lt;/p&gt;
    &lt;p&gt;x.com/becool_me&lt;/p&gt;
    &lt;p&gt;LINKS&lt;/p&gt;
    &lt;p&gt;Custom HTML with Tailwind classes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://a.coffee/"/><published>2025-10-29T15:57:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748725</id><title>Cursor Composer: Building a fast frontier model with RL</title><updated>2025-10-29T17:39:57.518666+00:00</updated><content>&lt;doc fingerprint="3d5aedd9e03a0a1a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Composer: Building a fast frontier model with RL&lt;/head&gt;
    &lt;p&gt;Composer is our new agent model designed for software engineering intelligence and speed. On our benchmarks, the model achieves frontier coding results with generation speed four times faster than similar models.&lt;/p&gt;
    &lt;p&gt;We achieve these results by training the model to complete real-world software engineering challenges in large codebases. During training, Composer is given access to a set of production search and editing tools and tasked with efficiently solving a diverse range of difficult problems. The final result is a large-scale model optimized for high-speed use as an agent in Cursor.&lt;/p&gt;
    &lt;p&gt;Our motivation comes from our experience developing Cursor Tab, our custom completion model. We found that often developers want the smartest model that can support interactive use, keeping them in the flow of coding. In our development process, we experimented with a prototype agent model, codenamed Cheetah, to better understand the impact of faster agent models. Composer is a smarter version of this model that keeps coding delightful by being fast enough for an interactive experience.&lt;/p&gt;
    &lt;p&gt;Composer is a mixture-of-experts (MoE) language model supporting long-context generation and understanding. It is specialized for software engineering through reinforcement learning (RL) in a diverse range of development environments. At each iteration of training, the model is given a problem description and instructed to produce the best response, be it a code edit, a plan, or an informative answer. The model has access to simple tools, like reading and editing files, and also more powerful ones like terminal commands and codebase-wide semantic search.&lt;/p&gt;
    &lt;p&gt;To measure progress, we constructed an evaluation that measures a model's usefulness to a software developer as faithfully as possible. Our benchmark, Cursor Bench, consists of real agent requests from engineers and researchers at Cursor, along with hand-curated optimal solutions to these requests. The resulting evaluation measures not just the agent‚Äôs correctness, but also its adherence to a codebase's existing abstractions and software engineering practices.&lt;/p&gt;
    &lt;p&gt;Reinforcement learning allows us to actively specialize the model for effective software engineering. Since response speed is a critical component for interactive development, we incentivize the model to make efficient choices in tool use and to maximize parallelism whenever possible. In addition, we train the model to be a helpful assistant by minimizing unnecessary responses and claims made without evidence. We also find that during RL, the model learns useful behaviors on its own like performing complex searches, fixing linter errors, and writing and executing unit tests.&lt;/p&gt;
    &lt;p&gt;Efficient training of large MoE models requires significant investment into building infrastructure and systems research. We built custom training infrastructure leveraging PyTorch and Ray to power asynchronous reinforcement learning at scale. We natively train our models at low precision by combining our MXFP8 MoE kernels with expert parallelism and hybrid sharded data parallelism, allowing us to scale training to thousands of NVIDIA GPUs with minimal communication cost. Additionally, training with MXFP8 allows us to deliver faster inference speeds without requiring post-training quantization.&lt;/p&gt;
    &lt;p&gt;During RL, we want our model to be able to call any tool in the Cursor Agent harness. These tools allow editing code, using semantic search, grepping strings, and running terminal commands. At our scale, teaching the model to effectively call these tools requires running hundreds of thousands of concurrent sandboxed coding environments in the cloud. To support this workload, we adapted existing infrastructure we built for Background Agents, rewriting our virtual machine scheduler to support the bursty nature and scale of training runs. This enabled seamless unification of RL environments with production environments.&lt;/p&gt;
    &lt;p&gt;Cursor builds tools for software engineering, and we make heavy use of the tools we develop. A motivation of Composer development has been developing an agent we would reach for in our own work. In recent weeks, we have found that many of our colleagues were using Composer for their day-to-day software development. With this release, we hope that you also find it to be a valuable tool.&lt;/p&gt;
    &lt;p&gt;‚Äî&lt;/p&gt;
    &lt;p&gt;¬π Benchmarked on an internal benchmark in the Cursor tool harness. We group models into classes based on score and report the best model in each class. "Fast Frontier" includes models designed for efficient inference such as Haiku 4.5 and Gemini Flash 2.5. "Best Open" includes recent open weight model releases such as Qwen Coder and GLM 4.6. "Frontier 7/2025" is the best model available in July of this year. "Best Frontier" includes GPT-5 and Sonnet 4.5, which both outperform Composer. For the Tokens per Second calculation, tokens are standardized across models to the latest Anthropic tokenizer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/composer"/><published>2025-10-29T16:04:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748756</id><title>Azure major outage: Portal, Front Door and global regions down</title><updated>2025-10-29T17:39:57.166960+00:00</updated><content>&lt;doc fingerprint="fe4cae6ccb282b57"&gt;
  &lt;main&gt;
    &lt;p&gt;Just started ~5 minutes ago: multiple global regions report that Azure is offline ‚Äî portal won‚Äôt load (even cached), Front Door services unreachable, CDN failing ‚Äî US East/Central, West US, Netherlands, UK, Italy, NZ all reporting issues. Status pages still show green. Heads up to anyone on Azure. Last week AWS, now this.&lt;/p&gt;
    &lt;p&gt;High availability is touted as a reason for their high prices, but I swear I read about major cloud outages far more than I experience any outages at Hetzner.&lt;/p&gt;
    &lt;p&gt;The real reason is that outages are not your fault. Its the new version of "nobody ever got fired for buying IBM" - later it became MS, and now its any big cloud provider.&lt;/p&gt;
    &lt;p&gt;DO has been shockingly reliable for me. I shut down a neglected box almost 900 days uptime the other day. In that time AWS has randomly dropped many of my boxes with no warning requiring a manual stop/start action to recover them... But everybody keeps telling me that DO isn't "as reliable" as the big three are.&lt;/p&gt;
    &lt;p&gt;we use front door (as does miccrosoft.com) and our website was down, I was able to change the DNS records to point directly to our server and will leave it like that for a few hours until everything is green&lt;/p&gt;
    &lt;p&gt;They added a message at the same time as your comment:&lt;/p&gt;
    &lt;p&gt;"We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly."&lt;/p&gt;
    &lt;p&gt;For us, it looks like most services are still working (eastus and eastus2). Our AKS cluster is still running and taking requests. Failures seem limited to management portal.&lt;/p&gt;
    &lt;p&gt;On our end, our VMs are still working, so our gitlab instance is still up. Our services using Azure App Services are available through their provided url. However, Front Door is failing to resolve any domains that it was responsible for.&lt;/p&gt;
    &lt;p&gt;I'd say DNS/Front Door (or some carrier interconnect) is the thing affected, since I can auth just fine in a few places. (I'm at MS, but not looped into anything operational these days, so I'm checking my personal subscription).&lt;/p&gt;
    &lt;p&gt;Apologies, but this just reads like a low effort critique of big things.&lt;/p&gt;
    &lt;p&gt;To be clear, they should get criticism. They should be held liable for any damage they cause.&lt;/p&gt;
    &lt;p&gt;But that they remain the biggest cloud offering out there isn't something you'd expect to change from a few outages that, by most all evidence, potential replacements have, as well? More, a lot of the outages potential replacements have are often more global in nature.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748756"/><published>2025-10-29T16:05:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748799</id><title>Azure Outage</title><updated>2025-10-29T17:39:56.363520+00:00</updated><content>&lt;doc fingerprint="765891140d3625f7"&gt;
  &lt;main&gt;
    &lt;p&gt;Do Microsoft still say "If the government has a broader voluntary national security program to gather customer data, we don't participate in it" today (which PRISM proved very false), or are they at least acknowledging they're participating in whatever NSA has deployed today?&lt;/p&gt;
    &lt;p&gt;This is funny but also possibly true because: business/MBA types see these outages as a way to prove how critical some services are, leading to investors deciding to load up on the vendor's stock.&lt;/p&gt;
    &lt;p&gt;I may or may not have been known to temporarily take a database down in the past to make a point to management about how unreliable some old software is.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.&lt;/p&gt;
    &lt;p&gt;We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Update: 16:35 UTC:&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. We suspect that an inadvertent configuration change as the trigger event for this issue. We are taking two concurrent actions where we are blocking all changes to the AFD services and at the same time rolling back to our last known good state.&lt;/p&gt;
    &lt;p&gt;We have failed the portal away from Azure Front Door (AFD) to mitigate the portal access issues. Customers should be able to access the Azure management portal directly.&lt;/p&gt;
    &lt;p&gt;We do not have an ETA for when the rollback will be completed, but we will update this communication within 30 minutes or when we have an update.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 17:17 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;It's pretty unlikely. AWS published a public 'RCA' https://aws.amazon.com/message/101925/. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.&lt;/p&gt;
    &lt;p&gt;I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".&lt;/p&gt;
    &lt;p&gt;That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.&lt;/p&gt;
    &lt;p&gt;Whilst the status message acknowledge's the issue with Front Door (AFD), it seems as though the rest of the actions are about how to get Portal/internal services working without relying on AFD. For those of us using Front Door does that mean we're in for a long haul?&lt;/p&gt;
    &lt;p&gt;yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.&lt;/p&gt;
    &lt;p&gt;Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D&lt;/p&gt;
    &lt;p&gt;We already had to do it for large files served from Blob Storage since they would cap out at 2MB/s when not in cache of the nearest PoP. If you‚Äôve ever experienced slow Windows Store or X-Box downloads it‚Äôs probably the same problem.&lt;/p&gt;
    &lt;p&gt;I had a support ticket open for months about this and in the end the agent said ‚Äúthis is to be expected and we don‚Äôt plan on doing anything about it‚Äù.&lt;/p&gt;
    &lt;p&gt;We‚Äôve moved to Cloudflare and not only is the performance great, but it costs less.&lt;/p&gt;
    &lt;p&gt;Only thing I need to move off Front Door is a static website for our docs served from Blob Storage, this incident will make us do it sooner rather than later.&lt;/p&gt;
    &lt;p&gt;we are considering the same but because our website uses APEX domain we would need to move all DNS resolver to cloudfront right ? Does it have as a nice "rule set builder" as azure ?&lt;/p&gt;
    &lt;p&gt;Yeah just took down the prod site for one of our clients since we host the front-end out of their CDN. Just got wrapped up panic hosting it somewhere else for the past hour, very quickly reminds you about the pain of cookies...&lt;/p&gt;
    &lt;p&gt;The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.&lt;/p&gt;
    &lt;p&gt;How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.&lt;/p&gt;
    &lt;p&gt;A lot of money and years of marketing the cloud as the responsible business decision led us here. Now that the cloud providers have vendor lock-in, few will leave, and customers will continue to wildly overpay for cloud services.&lt;/p&gt;
    &lt;p&gt;A natural monopoly is a monopoly in an industry in which high infrastructure costs and other barriers to entry relative to the size of the market give the largest supplier in an industry, often the first supplier in a market, an overwhelming advantage over potential competitors. Specifically, an industry is a natural monopoly if a single firm can supply the entire market at a lower long-run average cost than if multiple firms were to operate within it. In that case, it is very probable that a company (monopoly) or a minimal number of companies (oligopoly) will form, providing all or most of the relevant products and/or services.&lt;/p&gt;
    &lt;p&gt;I think the response lies in the surrounding ecosystem.&lt;/p&gt;
    &lt;p&gt;If you have a company it's easier to scale your team if you use AWS (or any other established ecosystem). It's way easier to hire 10 engineers that are competent with AWS tools than it is to hire 10 engineers that are competent with the IBM tools.&lt;/p&gt;
    &lt;p&gt;And from the individuals perspective it also make sense to bet on larger platforms. If you want to increase your odds of getting a new job, learning the AWS tools gives you a better ROI than learning the IBM tools.&lt;/p&gt;
    &lt;p&gt;Consolidation is the inevitable outcome of free unregulated markets.&lt;/p&gt;
    &lt;p&gt;In our highly interconnected world, decentralization paradoxically requires a central authority to enforce decentralization by restricting M&amp;amp;A, cartels, etc.&lt;/p&gt;
    &lt;p&gt;I noticed that Starbucks mobile ordering was down and thought ‚Äúwelp, I guess I‚Äôll order a bagel and coffee on Grubhub‚Äù, then GrubHub was down. My next stop was HN to find the common denominator, and y‚Äôall did not disappoint.&lt;/p&gt;
    &lt;p&gt;I‚Äôve been migrating our services off of Azure slowly for the past couple of years. The last internet facing things remaining are a static assets bucket and an analytics VM running Matomo. Working with Front Door has been an abysmal experience, and today was the push I needed to finally migrate our assets to Cloudflare.&lt;/p&gt;
    &lt;p&gt;I feel pretty justified in my previous decisions to move away from Azure. Using it feels like building on quicksand‚Ä¶&lt;/p&gt;
    &lt;p&gt;And querying https://www.microsoft.com/ results in HTTP 200 on the root document, but the page elements return errors (a 504 on the .css/.js documents, a 404 on some fonts, Name Not Resolved on scripts.clarity.ms, Connection Timed Out on wcpstatic.microsoft.com and mem.gfx.ms). That many different kinds of errors is actually kind of impressive.&lt;/p&gt;
    &lt;p&gt;I'm gonna say this was a networking/routing issue. The CDN stayed up, but everything else non-CDN became unroutable, and different requests traveled through different paths/services, but each eventually hit the bad network path, and that's what created all the different responses. Could also have been a bad deploy or a service stopped running and there's different things trying to access that service in different ways, leading to the weird responses... but that wouldn't explain the failed DNS propagation.&lt;/p&gt;
    &lt;p&gt;There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.&lt;/p&gt;
    &lt;p&gt;You'd think any SeriousBusiness would have a backup way to take customers' money. This is the one thing you always want to be able to do: accept payment. If they made it so they can't do that, they deserve the hit to their revenue. People should just walk out of the store with the goods if they're not being charged.&lt;/p&gt;
    &lt;p&gt;Why doesn't someone in the store at least have one of those manual kachunk-kachunk carbon copy card readers in the back that they can resuscitate for a few days until the technology is turned back on? Did they throw them all away?&lt;/p&gt;
    &lt;p&gt;The kachunk-kachunk credit card machines need raised digits on the cards, and I don't think most banks have been issuing those for years at this point. Mine have been smooth for at least 10 years.&lt;/p&gt;
    &lt;p&gt;2-3%, bit higher on perishables. Though i'd just ask lump sum payments in cash since it likely has to no go through corporate (as in, avoid the corporation).&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.&lt;/p&gt;
    &lt;p&gt;I especially like how Nadella speaks of layoffs as some kind of uncontrollable natural disaster, like a hurricane, caused by no-one in particular. A kind of "God works in mysterious ways".&lt;/p&gt;
    &lt;p&gt;&amp;gt; ‚ÄúMicrosoft is being recognized and rewarded at levels never seen before,‚Äù Nadella wrote. ‚ÄúAnd yet, at the same time, we‚Äôve undergone layoffs. This is the enigma of success in an industry that has no franchise value.‚Äù &amp;gt; Nadella explained the disconnect between thriving financials and layoffs by stating that ‚Äúprogress isn‚Äôt linear‚Äù and that it is ‚Äúsometimes dissonant, and always demanding.‚Äù&lt;/p&gt;
    &lt;p&gt;I've read the whole memo and it's actually worse than those excerpts. Nadella doesn't even claim these were low performers:&lt;/p&gt;
    &lt;p&gt;&amp;gt; These decisions are among the most difficult we have to make. They affect people we‚Äôve worked alongside, learned from, and shared countless moments with‚Äîour colleagues, teammates, and friends.&lt;/p&gt;
    &lt;p&gt;Ok, so Microsoft is thriving, these were friends and people "we've learned from", but they must go because... uh... "progress isn't linear". Well, thanks Nadella! That explains so much!&lt;/p&gt;
    &lt;p&gt;We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven‚Äôt had weekly or even monthly issues. I can think of maybe three issues this year.&lt;/p&gt;
    &lt;p&gt;Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.&lt;/p&gt;
    &lt;p&gt;Same playbook for AWS. When they admitted that Dynamo was inaccessible, they failed to provide context that their internal services are heavily dependent on Dynamo&lt;/p&gt;
    &lt;p&gt;It's only after the fact they are transparent about the impact&lt;/p&gt;
    &lt;p&gt;That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.&lt;/p&gt;
    &lt;p&gt;SSO is down, Azure Portal Down and more, seems like a major outage. Already a lot of services seem to be affected: banks, airlines, consumer apps, etc.&lt;/p&gt;
    &lt;p&gt;The portal is up for me and their status page confirms they did a failover for it. Definitely not disputing that its reach is wide, but a lot of smaller setups probably aren't using Front Door.&lt;/p&gt;
    &lt;p&gt;I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.&lt;/p&gt;
    &lt;p&gt;They admit in their update blurb azure front door is having issues but still report azure front door as having no issues on their status page.&lt;/p&gt;
    &lt;p&gt;And it's very clear from these updates that they're more focused on the portal than the product, their updates haven't even mentioned fixing it yet, just moving off of it, as if it's some third party service that's down.&lt;/p&gt;
    &lt;p&gt;Unsubstantiated idea: So the support contract likely says there is a window between each reporting step and the status page is the last one and the one in the legal documents giving them several more hours before the clauses trigger.&lt;/p&gt;
    &lt;p&gt;I absolutely love the utility aspect of LLMs but part of me is curious if moving faster by using AI is going to make these sorts of failure more and more often.&lt;/p&gt;
    &lt;p&gt;Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking? Or is that the info that the malefactors are seeking?&lt;/p&gt;
    &lt;p&gt;Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.&lt;/p&gt;
    &lt;p&gt;It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.&lt;/p&gt;
    &lt;p&gt;I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.&lt;/p&gt;
    &lt;p&gt;Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).&lt;/p&gt;
    &lt;p&gt;FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.&lt;/p&gt;
    &lt;p&gt;Hate to say it, but DNS is looking like it's still the undisputed champ.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748799"/><published>2025-10-29T16:08:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748879</id><title>Minecraft removing obfuscation in Java Edition</title><updated>2025-10-29T17:39:55.969338+00:00</updated><content>&lt;doc fingerprint="370203ca08b7cced"&gt;
  &lt;main&gt;
    &lt;p&gt;Do you like to mod Java, tinker with builds, or take deep dives into Minecraft‚Äôs code? Then this article is for you!&lt;/p&gt;
    &lt;p&gt;For a long time, Java Edition has used obfuscation (hiding parts of the code) ‚Äì a common practice in the gaming industry. Now we‚Äôre changing how we ship Minecraft: Java Edition to remove obfuscation completely. We hope that, with this change, we can pave a future for Minecraft: Java Edition where it‚Äôs easier to create, update, and debug mods.&lt;/p&gt;
    &lt;head rend="h2"&gt;An obfuscated history&lt;/head&gt;
    &lt;p&gt;Minecraft: Java Edition has been obfuscated since its release. This obfuscation meant that people couldn‚Äôt see our source code. Instead, everything was scrambled ‚Äì and those who wanted to mod Java Edition had to try and piece together what every class and function in the code did.&lt;/p&gt;
    &lt;p&gt;But we encourage people to get creative both in Minecraft and with Minecraft ‚Äì so in 2019 we tried to make this tedious process a little easier by releasing ‚Äúobfuscation mappings‚Äù. These mappings were essentially a long list that allowed people to match the obfuscated terms to un-obfuscated terms. This alleviated the issue a little, as modders didn‚Äôt need to puzzle out what everything did, or what it should be called anymore. But why stop there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Removing obfuscation in Java Edition&lt;/head&gt;
    &lt;p&gt;To make things even easier ‚Äì and remove these intermediary steps ‚Äì we‚Äôre removing obfuscation altogether! Starting with the first snapshot following the complete Mounts of Mayhem launch, we will no longer obfuscate Minecraft: Java Edition. This means that this build (and all future builds) will have all of our original names* ‚Äì now with variable names and other names ‚Äì included by default to make modding even easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition"/><published>2025-10-29T16:12:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749017</id><title>Tailscale Peer Relays</title><updated>2025-10-29T17:39:55.838208+00:00</updated><content>&lt;doc fingerprint="66f1d873751076c5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they‚Äôre managed entirely by the customer, peer relays are less throughput-constrained than Tailscale‚Äôs managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.&lt;/p&gt;
    &lt;p&gt;In testing with early design partners, we‚Äôve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale‚Äôs managed DERP fleet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving past hard NAT&lt;/head&gt;
    &lt;p&gt;Over the past few weeks, you‚Äôve heard us talk about improvements we‚Äôve made to Network Address Translation (NAT) traversal techniques, so that Tailscale can establish direct connections wherever possible (hint: it‚Äôs over 90% of the time). However, we‚Äôve also outlined places where this isn‚Äôt possible or desirable today for a variety of reasons, especially in cloud environments. And, we‚Äôve postulated a bit about where we think the industry is going.&lt;/p&gt;
    &lt;p&gt;While we‚Äôve been keeping your network reliably connected for years with DERP, we‚Äôve heard from customers that the throughput and performance aspects of a QoS-aware managed relay fleet makes deployments in certain environments difficult or untenable. Furthermore, customers have noted that it‚Äôs non-trivial to deploy and manage custom DERP fleets (which run as a separate service and binary).&lt;/p&gt;
    &lt;p&gt;DERP provides an incredibly valuable service, setting up reliable connections between Tailscale clients anywhere in the world (including negotiating connections through peer relays). But often, DERP‚Äôs focus as a reliability and NAT traversal tool results in performance tradeoffs.&lt;/p&gt;
    &lt;p&gt;By contrast, Tailscale Peer Relays is designed as a performant connectivity tool, and can perform at a level rivaling direct connections. Peer relays can be placed right next to the resources they serve, and peer relays also run on top of UDP, both characteristics beneficial to lower latency and resource overhead. And, they are built into the Tailscale client itself for ease of deployment.&lt;/p&gt;
    &lt;p&gt;We want to move past even more hard NATs, and put Tailscale‚Äôs relaying technology in our customers‚Äô hands, so they can use Tailscale at scale, anywhere, with ease. We believe our new Tailscale Peer Relays connectivity option‚Äîunique to Tailscale‚Äîgives customers the best performance and flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Peer relays are configured with a single UDP port that must be available to both sides of a connection. Tailscale Peer Relays is built right into the Tailscale client, and can be enabled with a simple command, using the &lt;code&gt;tailscale set --relay-server-port&lt;/code&gt; flag from the Tailscale CLI. Once enabled via the steps in our documentation, clients can connect to infrastructure in hard NAT environments over the peer relay.&lt;/p&gt;
    &lt;p&gt;And don‚Äôt worry, we still prefer to fly direct. Tailscale prefers direct connections wherever possible. Clients can then fall back to available peer relays, and finally leverage Tailscale‚Äôs managed DERP fleet, or any customer-deployed custom DERPs, to ensure you have connectivity wherever you need it. All of this traffic, over any connection, is still end-to-end encrypted via WireGuard¬Æ.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays is designed for the real world, based on the feedback we‚Äôve received from customers and our own hard-earned networking expertise. It allows customers to make just one firewall exception for connections only coming from their tailnet. Peer relays scale across regions, are resilient to real-world network conditions, and graciously fall back to DERP (Tailscale‚Äôs or custom). Your network maintains its shape, but gains all kinds of flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;Connectivity, everywhere, at warp speed&lt;/head&gt;
    &lt;p&gt;Customers can now maintain performance benchmarks even where direct connections aren‚Äôt possible, by enabling Tailscale Peer Relays to build a deterministic and high-throughput relay topology.&lt;/p&gt;
    &lt;p&gt;We‚Äôve had customers use peer relays to provide access into unmanaged networks, allowing their partners or customers to provide a controllable and auditable connectivity path without sacrificing performance.&lt;/p&gt;
    &lt;p&gt;In strict private networks, customers can build predictable access paths. Tailscale Peer Relays can be placed in public subnets with VPC peering to private subnets, allowing security teams to efficiently segment their private network infrastructure, while enabling networking teams to roll Tailscale out in full-mesh mode across the subnet.&lt;/p&gt;
    &lt;p&gt;Today, customers are using peer relays to establish relayed connections at near-direct speeds across a variety of environments and settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable high-throughput traffic through cloud NATs, like AWS Managed NAT Gateways: Applications and services behind a Managed NAT Gateway can leverage peer relays to relay traffic that can‚Äôt establish direct connections.&lt;/item&gt;
      &lt;item&gt;Relay through network firewalls: Workloads running in strictly firewalled environments can predictably expose a single UDP port, limiting the Tailscale surface area and fast-tracking the approval process for firewall exceptions.&lt;/item&gt;
      &lt;item&gt;Offload from Custom and Managed DERP: Minimize data-in-transit through Tailscale‚Äòs managed DERP network, and remove the need for customer-maintained DERP servers.&lt;/item&gt;
      &lt;item&gt;Provide access to locked down customer networks: Data plane traffic can be relayed through predictable endpoints in customer networks, so that they only need to open minimal numbers of ports to facilitate cross network connections.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;It‚Äôs not perfect, but we‚Äôre getting there&lt;/head&gt;
    &lt;p&gt;Tailscale Peer Relays is available today as a public beta. We‚Äôve yet to establish all the connectivity paths we want to, and there‚Äôs still visibility and debugging improvements to work through. However, we‚Äôve reliably seen our early design partners move to peer relay deployments with relative ease, and we‚Äôre ready for you to give it a try on your tailnet.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays can be enabled on all plans, including free (it‚Äôs our little way of working through the kinks of the modern internet with our customers). All customers can use two peer relays, for free, forever. As your needs scale, so will the number of available peer relays. To add even more peer relays to your tailnet, come have a chat with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tailscale.com/blog/peer-relays-beta"/><published>2025-10-29T16:21:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749019</id><title>Floss Before Brushing</title><updated>2025-10-29T17:39:55.711087+00:00</updated><content>&lt;doc fingerprint="a689e3519fa44c96"&gt;
  &lt;main&gt;
    &lt;p&gt;A dental hygienist we met recently told us we should floss before brushing instead of after.&lt;/p&gt;
    &lt;p&gt;‚ÄúNo way‚Äù was my first reaction. I thought I had my dental hygiene routine down.&lt;/p&gt;
    &lt;p&gt;It turns out she was right. Flossing first loosens and removes food particles and plaque between your teeth that your toothbrush can‚Äôt reach.&lt;/p&gt;
    &lt;p&gt;Once the spaces between teeth are clean, toothpaste‚Äôs fluoride can reach more surfaces and protect them better.&lt;/p&gt;
    &lt;p&gt;This logic made sense. And it has shown up in studies too (like this one).&lt;/p&gt;
    &lt;p&gt;3 takeaways ‚Äì&lt;/p&gt;
    &lt;p&gt;(1) It is amazing how you can do something every day for so many years and realize there is a better way to do it.&lt;/p&gt;
    &lt;p&gt;(2) It is more important to floss than worry about the order. So that‚Äôs the first habit to nail.&lt;/p&gt;
    &lt;p&gt;(3) And assuming you do, floss before brushing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alearningaday.blog/2025/10/29/floss-before-brushing/"/><published>2025-10-29T16:21:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749161</id><title>AOL to be sold to Bending Spoons for roughly $1.5B</title><updated>2025-10-29T17:39:55.607988+00:00</updated><content/><link href="https://www.axios.com/2025/10/29/aol-bending-spoons-deal"/><published>2025-10-29T16:28:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749690</id><title>Does brand advertising work? Upwave (YC S12) is hiring engineers to answer that</title><updated>2025-10-29T17:39:55.465990+00:00</updated><content>&lt;doc fingerprint="355c16590c952eaa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Senior Software Engineer&lt;/head&gt;
    &lt;p&gt;Upwave: The Brand Outcomes Measurement Platform&lt;/p&gt;
    &lt;p&gt;Upwave is a leading measurement company entirely focused on measuring and optimizing upper funnel campaigns.. The world‚Äôs leading advertisers, agencies, and media partners trust Upwave‚Äôs robust, AI-driven platform to bring science to the top of the funnel.&lt;/p&gt;
    &lt;p&gt;With Upwave, marketers maximize the effectiveness of brand spend. Upwave measures Brand Lift, validates Brand Reach, and surfaces Brand Optimization opportunities in one, dynamic platform with cross-channel brand measurement for CTV, Digital, Social, Linear, Addressable, Retail Media, Streaming Audio and more.&lt;/p&gt;
    &lt;p&gt;We‚Äôre a profitable, growth-stage company backed by leading venture investors (Y Combinator, Uncork Capital, Bloomberg Beta, Initialized Capital, PivotNorth, Ridge Ventures, Industry Ventures, Conductive Ventures,) and leading AdTechfounders &amp;amp; CEOs.&lt;/p&gt;
    &lt;p&gt;We‚Äôre a humble but ambitious team that takes its work seriously but never ourselves. Come join us.&lt;/p&gt;
    &lt;p&gt;As a Senior Software Engineer at Upwave, you‚Äôll be a full-stack problem solver with a backend focus‚Äîbuilding the APIs, data pipelines, and systems that power our brand measurement platform. Your work will process billions of ad impressions, manage complex data workflows, and deliver insights that inform marketing decisions for the world‚Äôs biggest brands.&lt;/p&gt;
    &lt;p&gt;You‚Äôll collaborate across engineering, product, and data science to ship high-impact features end-to-end, scale our platform for the next phase of growth, and help define the next generation of brand measurement.&lt;/p&gt;
    &lt;p&gt;What you will do:&lt;/p&gt;
    &lt;p&gt;Build AI-powered customer experiences ‚Äî integrate LLMs and advanced causal inference techniques into production workflows that automatically generate data visualizations, synthesize campaign performance into natural language insights, and help enterprise customers understand and optimize their advertising through our AI analyst "Bayes."&lt;/p&gt;
    &lt;p&gt;Design and build scalable backend systems ‚Äîdevelop microservices and RESTful APIs that power the analytics platform behind the world‚Äôs top brand campaigns.&lt;/p&gt;
    &lt;p&gt;Contribute across the stack ‚Äî work from backend APIs to Python analytics services to React frontends, delivering complete features that combine sophisticated data analysis with intuitive user experiences.&lt;/p&gt;
    &lt;p&gt;Engineer data pipelines at scale ‚Äî design and operate systems that process massive volumes of ad and survey data with MySQL, DynamoDB, and AWS (S3, Lambda, EMR, Kinesis Firehose).&lt;/p&gt;
    &lt;p&gt;Improve reliability and performance ‚Äî deploy services on Kubernetes and AWS, automate deployments via CI/CD, monitor with DataDog and Sentry, and continuously raise the bar for operational excellence&lt;/p&gt;
    &lt;p&gt;Collaborate deeply ‚Äî work closely with Product and Data Science to productionize statistical models, integrate advanced analytics into customer-facing tools, and bring cutting-edge AI capabilities to enterprise customers.&lt;/p&gt;
    &lt;p&gt;Deliver insights that move millions ‚Äî enable brand lift analytics and real-time campaign insights by building reliable, high-throughput systems. Multi-million dollar advertising decisions hinge on our recommendations.&lt;/p&gt;
    &lt;p&gt;About you:&lt;/p&gt;
    &lt;p&gt;You‚Äôre an experienced engineer (5+ years) who thrives on solving complex problems across APIs, data systems, and distributed infrastructure. You care about clean architecture, reliable systems, and measurable customer impact.&lt;/p&gt;
    &lt;p&gt;You‚Äôve built powerful, intuitive, API-driven products for professional users.. You‚Äôre comfortable across the stack, with experience in RDBMS-backed backends using Spring Boot, Django, Rails, or Express, and single-page frontends built in React, Vue, or Angular.&lt;/p&gt;
    &lt;p&gt;You understand and enjoy programming. You‚Äôre fluent in the modern landscape of UI frameworks, API and microservice architectures, databases, and cloud platforms‚Äîand know when to use the right tool for the job.&lt;/p&gt;
    &lt;p&gt;You embrace modern AI-powered development tools to move faster and code smarter. You use technologies like Claude Code, Cursor, and GitHub CoPilot to automate routine work, explore ideas quickly, and focus your time on higher-value system design and innovation.&lt;/p&gt;
    &lt;p&gt;You value structured software development practices‚Äîtesting, documentation, CI/CD, and code review‚Äîand care about building maintainable systems that scale.&lt;/p&gt;
    &lt;p&gt;You believe developers should operate what they build. You think about observability, cost, and reliability from day one, and design systems that are easy to deploy and maintain. You‚Äôve built in the cloud and know both its power and pitfalls.&lt;/p&gt;
    &lt;p&gt;You like turning ideas into tools that make real customers more effective. You collaborate closely with Product to design features that solve real-world problems and delight users.&lt;/p&gt;
    &lt;p&gt;You mentor others, share knowledge freely, and understand that healthy human systems are the foundation of healthy technical systems. Teammates look to you for guidance.&lt;/p&gt;
    &lt;p&gt;You want to understand how things work and why. You care more about the best idea winning than whose idea it is.&lt;/p&gt;
    &lt;p&gt;You take responsibility, move quickly to fix problems, and take pride in establishing areas of deep expertise in a fast-changing environment.&lt;/p&gt;
    &lt;p&gt;You believe high-trust, inclusive teams outperform individuals. You communicate clearly and compassionately, and contribute to a culture where people enjoy working together.&lt;/p&gt;
    &lt;p&gt;Bonus points:&lt;/p&gt;
    &lt;p&gt;Have worked with modern backend ecosystems like Java/Kotlin/Groovy (Spring Boot or Grails) and know how to design APIs that scale elegantly.&lt;/p&gt;
    &lt;p&gt;Are fluent with data systems such as MySQL, DynamoDB, and Presto, and understand the tradeoffs between relational and NoSQL storage.&lt;/p&gt;
    &lt;p&gt;Have built cloud-native applications on AWS, especially using Kubernetes and Terraform for automation and scalability.&lt;/p&gt;
    &lt;p&gt;Know your way around modern front-end frameworks like React/Redux and enjoy collaborating up and down the stack.&lt;/p&gt;
    &lt;p&gt;Have startup DNA‚Äîyou‚Äôre comfortable with ambiguity, iterate fast, and make pragmatic technical decisions.&lt;/p&gt;
    &lt;p&gt;Bring experience from AdTech, MarTech, or measurement platforms, or are excited to learn how AI and large-scale data intersect in this space.&lt;/p&gt;
    &lt;p&gt;Why You‚Äôll Like Working Here:&lt;/p&gt;
    &lt;p&gt;Engineering-first company: Upwave‚Äôs success depends on high-velocity innovation, and we believe high velocity comes from high efficiency, not high effort. We set priorities rather than deadlines, we don‚Äôt crunch, we work reasonable hours, and engineers actually take vacations.&lt;/p&gt;
    &lt;p&gt;Modern tech stack: Python analytics, Kotlin/Java APIs, event streaming (100k+ RPM), DynamoDB, Kubernetes, AWS, Terraform, LLM orchestration.&lt;/p&gt;
    &lt;p&gt;Impact at scale: Your code processes billions of advertising events and directly influences multi-million dollar decisions by Fortune 500 brands.&lt;/p&gt;
    &lt;p&gt;Autonomy and ownership: Our engineers lead projects from design through deployment and monitoring.&lt;/p&gt;
    &lt;p&gt;Ambitious but humble culture: We take our work seriously but never ourselves. Upwavers collaborate hard and support each other generously. We screen for people who are both exceptionally talented and genuinely kind.&lt;/p&gt;
    &lt;p&gt;Remote-first team: Our diverse team spans half the globe (but only one half, to ensure everyone can talk live when we need to). We balance synchronous core hours with flexibility to create a work environment that enables both deep collaboration and deep work.&lt;/p&gt;
    &lt;p&gt;Additional Information:&lt;/p&gt;
    &lt;p&gt;The annual base salary range for this role is $150,000 - $175,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for the new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits.&lt;/p&gt;
    &lt;p&gt;Upwave is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.upwave.com/job/8228849002/"/><published>2025-10-29T17:00:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749763</id><title>A Year of Fast Apply ‚Äì Our Path to 10k Tokens per Second</title><updated>2025-10-29T17:39:55.155303+00:00</updated><content>&lt;doc fingerprint="d5137f0a11a0a9b"&gt;
  &lt;main&gt;
    &lt;p&gt;A year ago today, we released our first Fast Apply model publicly. Since then, we‚Äôve learned a lot about how to fine-tune small, specialized models for code-specific tasks.&lt;/p&gt;
    &lt;p&gt;Today, we‚Äôre open-sourcing what we've learned in training this series of models ‚Äî dataset curation, training methods, and inference techniques that led to Relace Apply 3, our best model yet, capable of running at 10k+ tokens per second while maintaining state-of-the-art accuracy.&lt;/p&gt;
    &lt;p&gt;Error rate comparison on 500 randomly sampled production merge requests. For breakdown of error categories, see section on evaluating merges.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;When making edits to a codebase, it doesn't make sense to have an expensive LLM regenerate all the unchanged, preexisting code.&lt;/p&gt;
    &lt;p&gt;Editing a thousand line file (~10k tokens) by rewriting it from scratch with Claude 4.5 Sonnet takes over 100 seconds and costs at least $0.18. For coding agents, this is infeasible from a product perspective.&lt;/p&gt;
    &lt;p&gt;The solution is to have the frontier model output a diff that minimally expresses the changes to make (i.e. the hard tokens), and use a lightweight algorithm to efficiently apply the diff back into the preexisting code. Not only does this save on cost, but if the merging algorithm is fast, you also significantly speed up end-to-end generation time.&lt;/p&gt;
    &lt;p&gt;For a long time, LLMs were incapable of reliably producing diff formats that were mergeable by a fixed algorithm like string replace or UDiff.&lt;/p&gt;
    &lt;p&gt;Cursor pioneered a flexible workaround to this problem ‚Äî let the frontier model produce "lazy" diffs and use a small, fast apply model as the merging algorithm. However, Cursor never made their model available for other companies to use outside of the IDE.&lt;/p&gt;
    &lt;p&gt;We decided to train our own apply model that anyone could use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Use an LLM as the Merge Algorithm?&lt;/head&gt;
    &lt;p&gt;In practice, frontier models can produce a wide variety of pathological diffs. Any closed-form algorithm you write to perform the merge will be susceptible to edge cases. In agentic settings, where there is often no human oversight, these errors compound and produce incorrect code.&lt;/p&gt;
    &lt;p&gt;Example of a pathological initial_code, diff pair that is hard to address with a closed-form merging algorithm.&lt;/p&gt;
    &lt;p&gt;In the code above, the user starts with a function called &lt;code&gt;handler&lt;/code&gt;. The intent
of the diff on the right is to rename the handler function to &lt;code&gt;messageHandler&lt;/code&gt;
and add a parameter for the &lt;code&gt;name&lt;/code&gt; column in the database query.&lt;/p&gt;
    &lt;p&gt;It would be difficult to write an algorithm that anticipates these two steps. A standard merging algorithm would likely just add a new function called &lt;code&gt;messageHandler&lt;/code&gt;, duplicating the original function.&lt;/p&gt;
    &lt;p&gt;The advantage of using an LLM as the merge algorithm is that it can flexibly infer the intent of the diff. Pretraining on trillions of code tokens bakes in pattern recognition robust to the many edge cases you see in production.&lt;/p&gt;
    &lt;p&gt;Also, by splitting the task into two steps ‚Äî diff generation and merge ‚Äî you can use a much smaller, fast model to focus entirely on the merge. This separation of work based on difficulty is a pattern we exploit a lot at Relace to improve overall performance of coding agents.&lt;/p&gt;
    &lt;p&gt;To achieve good results without needing to pretrain a model, we fine tune off-the-shelf small models on a high quality dataset that matches the distribution in production.&lt;/p&gt;
    &lt;head rend="h2"&gt;Producing the Dataset&lt;/head&gt;
    &lt;p&gt;A training set for fast apply contains three components: &lt;code&gt;initial_code&lt;/code&gt;, &lt;code&gt;diff&lt;/code&gt;,
and &lt;code&gt;merged_code&lt;/code&gt;. The initial code and diff are passed in as model inputs,
and the merged code is the output we train the model to produce.&lt;/p&gt;
    &lt;p&gt;For fine tuning, you need a small set of high quality examples. We found the size of the dataset to be much less important than the diversity and quality of merge data. Our first model was trained with only 30k data points, and we saw marginal gains beyond 100k data points.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inputs&lt;/head&gt;
    &lt;p&gt;Early on, Kortix AI released an open-source dataset by post-processing data scraped from public GitHub repos. Given some initial code, they prompt a frontier model like Claude to (1) come up with a change to make and (2) produce the "lazy" diff for it.&lt;/p&gt;
    &lt;p&gt;This turns out to be the wrong approach, as it doesn't reflect the actual distribution of data in production environments. The rich variety of edge cases in diffs comes from context overload. The LLM must adhere to instructions in long system prompts while simultaneously inferring intent from noisy conversations with non-technical users.&lt;/p&gt;
    &lt;p&gt;To get high-quality, complex merges with plenty of edge cases into the training set, we partnered with prompt-to-app companies. We took snapshots of the real context for LLM coding tasks and reran them with additional instructions to produce the "lazy" edits. This allowed us to sample directly from the true distribution of &lt;code&gt;initial_code&lt;/code&gt; and &lt;code&gt;diff&lt;/code&gt; that would be seen in production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Output&lt;/head&gt;
    &lt;p&gt;To generate the correct &lt;code&gt;merged_code&lt;/code&gt;, we use distillation with rejection
sampling. The idea is to feed the initial code and diff into a well-prompted
frontier ‚Äúteacher‚Äù model, guided by a set of rules for how merges should be
handled.&lt;/p&gt;
    &lt;p&gt;This approach lets you produce a large amount of candidate data quickly, but it‚Äôs crucial to filter out the teacher‚Äôs mistakes. When done correctly, the trained student model can actually end up outperforming the teacher.&lt;/p&gt;
    &lt;p&gt;However, filtering correctly is hard. We developed a multi-stage process to build a high quality LLM-as-a-judge that could scale up to thousands of datapoints.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluating Merges&lt;/head&gt;
    &lt;p&gt;We began by manually reviewing 500 randomly sampled examples to create a set of ironclad ground truths. For our first Apply model, these datapoints were fully synthetic, but we later repeated the process using real production data from earlier iterations of the hosted model.&lt;/p&gt;
    &lt;p&gt;To streamline the process while maintaining quality, we built our own internal evaluation tool: a Git-style diff viewer with annotation tools for categorizing merge outcomes.&lt;/p&gt;
    &lt;p&gt;A screenshot of Relace's internal merge evaluation tool.&lt;/p&gt;
    &lt;p&gt;Even with this tool, it took us over 40 hours to painstakingly ensure 100% correctness. Patterns started to emerge in the process, and we broke down merges into 6 categories:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Correct Merge: Model correctly implemented the intent of the diff.&lt;/item&gt;
      &lt;item&gt;Non-functional Error: Model implemented the intent of the diff except for inconsequential details such as variation in comments and formatting (e.g. function definitions in different orders).&lt;/item&gt;
      &lt;item&gt;Smoothing: Model fixed an error incorporated by the diff that would have resulted in uncompilable or broken code.&lt;/item&gt;
      &lt;item&gt;Functional/Merge Error: Model did not carry out the intent of the diff (e.g. omitting parts of code and inserting code in incorrect places).&lt;/item&gt;
      &lt;item&gt;Hallucination Error: Model added or changed code not specified by the diff (may be unsuccessful attempts at smoothing).&lt;/item&gt;
      &lt;item&gt;Truncation Errors: Model performed an incomplete merge (with parts of final code cut out) leading to uncompilable code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Categories 1-3 are considered correct merges, while 4-6 are considered incorrect. We keep the six-class system for more nuanced evaluations, but collapse to a binary classification when creating the LLM judge.&lt;/p&gt;
    &lt;head rend="h3"&gt;Aside on Smoothing&lt;/head&gt;
    &lt;p&gt;We often found that frontier LLMs produced diffs leading to slightly incorrect code. The most common example is when the diff uses a new library in the code without actually importing it.&lt;/p&gt;
    &lt;p&gt;This raised a question: should a fast apply model strictly follow the diff, or fix the error? In practice, we found customers building coding agents prefer that the model auto-corrects small mistakes to reduce friction for end users.&lt;/p&gt;
    &lt;p&gt;The hallucination category accounts for cases where the apply model overstepped and introduced more incorrect code to try and fix errors.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLM-as-a-Judge&lt;/head&gt;
    &lt;p&gt;Once we had this set of 500 categorized merge examples we could trust, the next step was to align an LLM-as-a-judge to our human-annotated seed dataset. It would be completely infeasible to hand-evaluate enough code merges to create the full training dataset of 100k+ examples.&lt;/p&gt;
    &lt;p&gt;The LLM-as-a-judge technique exploits the generation/verification gap. i.e. It's easier for an LLM to evaluate whether an answer is correct than to actually generate the answer. Still, the LLM often makes mistakes, and it's important to align it properly with a human evaluation first.&lt;/p&gt;
    &lt;p&gt;As with any binary classification task, there are two error modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;False positive - a bad merge is incorrectly classified as good.&lt;/item&gt;
      &lt;item&gt;False negative - a good merge is incorrectly classified as bad.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since we have a large synthetic dataset, false positives are the more problematic error mode. It's much worse for bad merges to remain in the dataset than to reduce dataset yield by throwing away a few false negatives.&lt;/p&gt;
    &lt;p&gt;We used Claude 4 Sonnet as the judge and iteratively tuned the prompt until we hit a false positive rate of ~1%. For comparison, the initial naive judge with no tuning had a false positive rate of ~16%!&lt;/p&gt;
    &lt;head rend="h3"&gt;Scaling Up&lt;/head&gt;
    &lt;p&gt;With the aligned LLM judge, it's possible to filter the rest of the data at scale.&lt;/p&gt;
    &lt;p&gt;For Relace Apply 3, we started with 200k sets of &lt;code&gt;initial_code&lt;/code&gt;, &lt;code&gt;diff&lt;/code&gt;, and
synthetically generated &lt;code&gt;merged_code&lt;/code&gt;. We chose a representative distribution of
examples across dozens of languages, but focused predominantly on
TypeScript/Javascript, Markdown, Python, Ruby, and HTML.&lt;/p&gt;
    &lt;p&gt;To further cut down on mistakes, we added an extra post-processing step using a combination of static analysis tools ‚Äî syntax verification with a code parser, deduplication, and regex-based filtering for common undesirable behaviors identified through customer feedback.&lt;/p&gt;
    &lt;p&gt;After all the filtering, we were left with a high-confidence training set of ~145k data points.&lt;/p&gt;
    &lt;head rend="h2"&gt;Training with LoRA&lt;/head&gt;
    &lt;p&gt;For small models, we've consistently found that data quality is the most important ingredient. With a clean, in-distribution dataset, the training just boils down to specializing a high quality base model for the merging task without catastrophic forgetting on general coding.&lt;/p&gt;
    &lt;p&gt;We trained our apply models using Supervised Fine-Tuning (SFT) on top of open-source coding models in the 3‚Äì8 billion parameter range. This gave us the right balance of expressiveness, inference speed, and cost efficiency.&lt;/p&gt;
    &lt;p&gt;We built our own training pipeline on top of the HuggingFace &lt;code&gt;transformers&lt;/code&gt;
library, but you can also just use out-of-the-box libraries like &lt;code&gt;axolotl&lt;/code&gt; or
&lt;code&gt;unsloth&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since post-training typically uses much smaller datasets than pretraining, updating every parameter in the model is wasteful. Instead of retraining all billions of weights, we use Low-rank adaptation (LoRA) ‚Äî a lightweight fine-tuning method that adds a small number of trainable ‚Äúadapter‚Äù matrices on top of the frozen base model.&lt;/p&gt;
    &lt;p&gt;This lets us specialize the model for merge tasks without erasing its coding intuition. The base model stays intact, while the adapters learn the merging algorithm we care about.&lt;/p&gt;
    &lt;p&gt;We ran a series of grid searches to tune the adapter size (rank), scaling factor (alpha), and learning rate, and landed on the configuration below, which produced the best evaluation loss and convergence speed.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;LoRA Rank&lt;/cell&gt;
        &lt;cell role="head"&gt;LoRA alpha&lt;/cell&gt;
        &lt;cell role="head"&gt;Learning Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Optimizer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;128&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;5e-5&lt;/cell&gt;
        &lt;cell&gt;AdamW&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Interestingly, we independently validated the optimal hyperparameters for LoRA published in the recent Thinking Machines blog post.&lt;/p&gt;
    &lt;p&gt;Using LoRA allowed us to train Relace Apply 3 on all ~145k data points using a single Nvidia H200 GPU on Modal with context length up to 64k tokens.&lt;/p&gt;
    &lt;p&gt;Using Modal allowed us to launch parallel training runs on H200 GPUs without having to wrestle with complicated cloud providers. Even though small model training lets you get way with batch size 1 on a single GPU, running a large hyperparameter sweep would normally take a lot of manual setup. With Modal, we ran our Python scripts and let it handle the scaling for us ‚Äî no need to ssh to new GPU instances every time.&lt;/p&gt;
    &lt;p&gt;After training in BF16, we convert the model weights to FP8 using the &lt;code&gt;llm-compressor&lt;/code&gt; library from
vLLM. This conversion step is
crucial ‚Äî by leveraging the FP8 cores on newer Nvidia GPUs, we achieve a
substantial jump in throughput without sacrificing precision.&lt;/p&gt;
    &lt;p&gt;To confirm that the quantization process was effectively lossless, we evaluated the resulting model against our 500 held-out ground-truth examples to validate that the outputs were the same.&lt;/p&gt;
    &lt;head rend="h2"&gt;10k tok/s with Speculative Decoding&lt;/head&gt;
    &lt;p&gt;With the model trained, our goal was to make it feel less like a slow language model and more like a closed-form algorithm for merging code. To reach that user expereince, we needed to push inference speed as far as possible ‚Äî which meant turning to speculative decoding.&lt;/p&gt;
    &lt;p&gt;LLMs normally generate tokens sequentially, where each new token depends on the ones that came before it. This dependency makes inference inherently slow, since every step requires a full forward pass through the model.&lt;/p&gt;
    &lt;p&gt;Speculative decoding takes advantage of the fact that forward passes are heavily memory bound. i.e. Shuttling the LLM weights into the GPU sRAM takes much longer than actually performing the matrix multiplication with the tensor cores. By guessing a sequence of k tokens, we can process many tokens in parallel (like prefill) and move more towards to the compute-bound regime.&lt;/p&gt;
    &lt;p&gt;After the forward pass, the model checks which of the predicted tokens match the "true" next tokens and keeps the ones that are correct. The better the "guess", the more tokens you accept, and the more you accelerate the model.&lt;/p&gt;
    &lt;p&gt;Animation demonstrating how speculative decoding parallelizes inference.&lt;/p&gt;
    &lt;p&gt;For code merging, large sections of the &lt;code&gt;initial_code&lt;/code&gt; and &lt;code&gt;diff&lt;/code&gt; are nearly
identical to what appears in the &lt;code&gt;merged_code&lt;/code&gt;. We can use this strong prior to
get long, high quality guesses for what tokens the model should output and get
huge speed ups.&lt;/p&gt;
    &lt;p&gt;However, speed and accuracy are tightly coupled for speculative decoding. Each hallucinated token interrupts the guessed sequence, resetting the verification chain and wasting computation.&lt;/p&gt;
    &lt;p&gt;By training on the meticulously cleaned dataset, we minimized these breaks and were able to push Relace Apply 3 to 10k tok/s:&lt;/p&gt;
    &lt;p&gt;Distribution of Relace Apply 3 throughput speed after first token (measured in tokens/second)&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;We tested Relace Apply on two datasets: (1) our manually reviewed benchmark of 500 examples, and (2) a second dataset of pathological merges collected from customer feedback.&lt;/p&gt;
    &lt;p&gt;Across both, Relace Apply 3 achieves state-of-the-art merge accuracy. Substantial improvements were made over the previous generation of Apply models thanks to targetted dataset tuning based on customer feedback.&lt;/p&gt;
    &lt;p&gt;Comparison of merge accuracy between Relace Apply 2 and Relace Apply 3 on a dataset of pathological merge requests.&lt;/p&gt;
    &lt;p&gt;Previous generations of fast apply struggled when edit snippets contained multiple diff formats in one (e.g. combining &lt;code&gt;\\ ... existing code ...&lt;/code&gt; format
with UDiff edits). We included subsets of UDiff and String Replace edits in the
training data for Relace Apply 3 allowing it to act as a universal merger.&lt;/p&gt;
    &lt;p&gt;Relace Apply 3 also introduces native support for 256k context, allowing it to handle very large files without degradation in performance. Combined with the 10k tok/s throughput, this makes Relace Apply 3 the fastest, most accurate, and longest-context model on the market.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast Apply, One Year Later&lt;/head&gt;
    &lt;p&gt;When we released our first Fast Apply model a year ago, LLMs were notoriously bad at outputting valid diff formats. Deterministic approaches like Search-and-Replace or UDiff were brittle, model-specific, and required extensive prompt engineering.&lt;/p&gt;
    &lt;p&gt;Diff formatting accuracy became such a bottleneck that Aider‚Äôs polyglot leaderboard tracked it as a separate metric ‚Äî one column for accuracy, another for performance.&lt;/p&gt;
    &lt;p&gt;Fast Apply changed that. It was the first model to make structured code edits feel reliable, and our customers felt the difference immediately.&lt;/p&gt;
    &lt;p&gt;Today, frontier models have become much better at diff formatting through heavy reinforcement learning on string-edit tools, but they‚Äôre still not perfect. Companies that exclusively use deterministic strategies, like Cline, need a supplmentary merge algorithm to help further boost the diff edit accuracy.&lt;/p&gt;
    &lt;p&gt;Schematic reconstructed from an X post by the Cline team. Even deterministic approaches require fallback logic to handle edge cases.&lt;/p&gt;
    &lt;p&gt;The best models now reach roughly 96% edit success, but models without this kind of tuning still fail around 10% of the time.&lt;/p&gt;
    &lt;p&gt;So while we expect that, over time, apply models may be phased out as diff accuracy improves, the underlying philosophy that drove it remains central to new projects.&lt;/p&gt;
    &lt;p&gt;Fast Apply proved that small, specialized models can deliver SoTA results when trained with high quality, task-specific datasets. The methods we developed are now guiding our broader work on accelerating codegen with small agentic models for utility tasks like search, merge conflict resolution, and refactoring.&lt;/p&gt;
    &lt;p&gt;Stay tuned for more releases soon!&lt;/p&gt;
    &lt;head rend="h2"&gt;We're Hiring&lt;/head&gt;
    &lt;p&gt;If you have gotten this far, chances are you found this interesting!&lt;/p&gt;
    &lt;p&gt;We‚Äôre hiring pragmatic researchers (Physics/Math/CS/ML) and exceptional engineers to ship models like this that real product teams rely on. Check out our careers page, and join us!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.relace.ai/blog/relace-apply-3"/><published>2025-10-29T17:04:48+00:00</published></entry></feed>