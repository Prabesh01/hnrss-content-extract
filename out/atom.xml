<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-17T20:12:28.681564+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45952614</id><title>Things I don't like in configuration languages</title><updated>2025-11-17T20:12:38.976469+00:00</updated><content>&lt;doc fingerprint="b7382afd84a3b21c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Things I Don't Like in Configuration Languages&lt;/head&gt;
    &lt;p&gt;Spoiler: I created my own configuration language maml.dev&lt;/p&gt;
    &lt;p&gt;Here will be a list of all markup languages/configuration languages I found on the internet, and things I don't like about them.&lt;/p&gt;
    &lt;head rend="h3"&gt;YAML&lt;/head&gt;
    &lt;code&gt;- - 1
  - 2
- - 3
  - 4&lt;/code&gt;
    &lt;p&gt;YAML 1.2 is better than YAML 1.0. But still, the YAML specification is monstrous, and I don't get how people trying to implement it are not going insane. YAML contains too many features.&lt;/p&gt;
    &lt;head rend="h3"&gt;XML&lt;/head&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;
&amp;lt;catalog xmlns="http://ex.com/catalog"
         xmlns:cat="http://ex.com/catalog"
         xmlns:old="http://ex.com/oldcatalog"&amp;gt;
  &amp;lt;item id="0042"&amp;gt;
    &amp;lt;name&amp;gt;R&amp;amp;amp;D Handbook&amp;lt;/name&amp;gt;
    &amp;lt;notes priority="1 2 3"&amp;gt;n/a&amp;lt;/notes&amp;gt;
  &amp;lt;/item&amp;gt;
&amp;lt;/catalog&amp;gt;&lt;/code&gt;
    &lt;p&gt;The era of XML is in the past. I remember the hype around XML, how it was thought to be a universal format, and people created a lot of stuff around it. But now it's dead.&lt;/p&gt;
    &lt;head rend="h3"&gt;JSON&lt;/head&gt;
    &lt;code&gt;{"name": "John", "age": 30, "car": null}&lt;/code&gt;
    &lt;p&gt;JSON is nice and JSON won. It's a universal, data-interchange format for the Web and applications. This is why I based my MAML on top of JSON. I fixed things that were a little bit annoying for me inside JSON.&lt;/p&gt;
    &lt;head rend="h3"&gt;TOML&lt;/head&gt;
    &lt;code&gt;[[servers.web]]
name = "frontend"
ip = "192.168.1.10"

[[servers]]
region = "eu-central"

[[servers.web]]
name = "backend"
ip = "192.168.1.11"&lt;/code&gt;
    &lt;p&gt;I don't get why the &lt;code&gt;[table]&lt;/code&gt; is more readable, and I don't understand &lt;code&gt;[[array of tables]]&lt;/code&gt; out of order.
And the lack of &lt;code&gt;null&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;JSON5&lt;/head&gt;
    &lt;code&gt;{
  lineBreaks: 'Look, Mom! \
No \\n\'s!',
  hexadecimal: 0xdecaf,
  a: +.8675309, b: +8675309.,
}&lt;/code&gt;
    &lt;p&gt;Too many unnecessary features. Object key-value pairs are still unordered. No distinction between integers and floats.&lt;/p&gt;
    &lt;head rend="h3"&gt;HJSON&lt;/head&gt;
    &lt;code&gt;{
  Hjson: a string
  RegEx: \s+
  md:
    '''
    First line.
    Second line.
      This line is indented by two spaces.
    '''  
}&lt;/code&gt;
    &lt;p&gt;Unquoted strings and three different types of comments. Multi-line strings with significant indentation.&lt;/p&gt;
    &lt;head rend="h3"&gt;JSONH&lt;/head&gt;
    &lt;code&gt;{
    // use #, // or /**/ comments
    keys: without quotes,
    isn\'t: {
        that: cool? # yes
    }
}&lt;/code&gt;
    &lt;p&gt;JSONH is same as HJSON but different.&lt;/p&gt;
    &lt;head rend="h3"&gt;RJSON&lt;/head&gt;
    &lt;code&gt;{ 
  shopping-list: [ milk butter /* don't forget the beer */ beer ]
  a\ space : t\:em item\ with\ spaces
}&lt;/code&gt;
    &lt;p&gt;Strings without quotes, but only if they don't contain spaces or commas. No specification.&lt;/p&gt;
    &lt;head rend="h3"&gt;JSONC&lt;/head&gt;
    &lt;code&gt;{
    /*
      This is a multi-line comment
      that spans multiple lines
    */
    "name": "Jane Doe",
    "age": 25,
}&lt;/code&gt;
    &lt;p&gt;There are so many implementations and differences in implementations of JSON with comments. For example, this one has trailing commas. Still allows duplicate keys, still no integers, unordered key-value objects.&lt;/p&gt;
    &lt;head rend="h3"&gt;HCL&lt;/head&gt;
    &lt;code&gt;service "aws_ami" "ubuntu" {
  most_recent = true
  instance_type = var.instance_type != "" ? var.instance_type : "t3.micro"
}&lt;/code&gt;
    &lt;p&gt;I don't like nesting with &lt;code&gt;service "http" "web_proxy"&lt;/code&gt; or ability to specify &lt;code&gt;service&lt;/code&gt; multiple times in different parts
of the file.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pkl&lt;/head&gt;
    &lt;code&gt;class Bird {
  name: String
  hidden taxonomy: Taxonomy
}

pigeon: Bird = new {
  name = "Common wood pigeon"
  taxonomy {
    species = "Columba palumbus"
  }
}&lt;/code&gt;
    &lt;p&gt;This is not a markup language. This is a full-blown programming language. I honestly would just use TypeScript instead of this one.&lt;/p&gt;
    &lt;head rend="h3"&gt;RON&lt;/head&gt;
    &lt;code&gt;GameConfig( // optional struct name
    window_size: (800, 600),
    key_bindings: {
        "up": Up,
        "down": Down,
        "left": Left,
        "right": Right,
    },
)&lt;/code&gt;
    &lt;p&gt;Very Rust-centric.&lt;/p&gt;
    &lt;head rend="h3"&gt;EDN&lt;/head&gt;
    &lt;code&gt;#:demo{:id #uuid "de305d54-75b4-431b-adb2-eb6b9e546014"
 :ts #inst "2025-11-04T09:00:00Z"
 :nums [1 2 3 4/2 5.5M]
 :set #{:a :b :c}
 :rx #"[A-Z]+"
 :q #queue ["x" "y"]
 :tag #user{:name "Zed"}}&lt;/code&gt;
    &lt;p&gt;I guess Clojure developers love it. I'm not one of them.&lt;/p&gt;
    &lt;head rend="h3"&gt;HOCON&lt;/head&gt;
    &lt;code&gt;include required("foo.conf")
a : [ 1, 2 ] [ 3, 4 ]
data-center-east = ${data-center-generic} { name = "east" }
{ foo include : 42 }&lt;/code&gt;
    &lt;p&gt;Too many features. Is it some sort of programming language? And it looks like it's very focused on just one project.&lt;/p&gt;
    &lt;head rend="h3"&gt;NestedText&lt;/head&gt;
    &lt;code&gt;default repository: home
report style: tree
compact format: {repo}: {size:{fmt}}.  Last back up: {last_create:ddd, MMM DD}.
date format: D MMMM YYYY
size format: .2b&lt;/code&gt;
    &lt;p&gt;YAML variant with only strings. But I need booleans!&lt;/p&gt;
    &lt;head rend="h3"&gt;KDL&lt;/head&gt;
    &lt;code&gt;author "Alex Monad" email=alex@example.com active=#true
scripts {
  message """
    hello
    world
    """
}&lt;/code&gt;
    &lt;p&gt;KDL is much closer to XML with properties on nodes. But I like JSON-like data structures. They are easier to understand. Also, I don't like significant indentation on multi-line strings.&lt;/p&gt;
    &lt;head rend="h3"&gt;SDLang&lt;/head&gt;
    &lt;code&gt;// Trailing semicolons are optional
prop true;

anotherprod on; author "Peter Parker"

this-is.1_valid$Tag-Name

renderer:options "invisible"
physics:options "nocollide"

title \
    "Some title"&lt;/code&gt;
    &lt;p&gt;They do have &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;false&lt;/code&gt;. Why adding &lt;code&gt;on&lt;/code&gt; and &lt;code&gt;off&lt;/code&gt;? By default, integers are 32 bits long. Suffix &lt;code&gt;10L&lt;/code&gt; is
needed for 64-bits.&lt;/p&gt;
    &lt;head rend="h3"&gt;CUE&lt;/head&gt;
    &lt;code&gt;import "math"

piPlusOne: math.Pi + 1

"quoted field names": {
    "four^four":         math.Pow(4, 4)
}&lt;/code&gt;
    &lt;p&gt;I would just use TypeScript instead of this programming language. Too many features; the specification is too big.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dhall&lt;/head&gt;
    &lt;code&gt;let makeUser = \(user : Text) -&amp;gt;
      let home       = "/home/${user}"
      let privateKey = "${home}/.ssh/id_ed25519"
      let publicKey  = "${privateKey}.pub"
      in  { home, privateKey, publicKey }
    {- Add another user to this list -}
in  [ makeUser "bill"
    , makeUser "jane"
    ]&lt;/code&gt;
    &lt;p&gt;Again, it is a programming language. So I guess only one implementation of Dhall exists?&lt;/p&gt;
    &lt;head rend="h3"&gt;Jsonnet&lt;/head&gt;
    &lt;code&gt;local my_function(x, y=10) = x + y;
{
  person1: {
    name: "Alice",
    welcome: "Hello " + self.name + "!",
  },
  person2: self.person1 { name: "Bob" },
  len: [
    std.length('hello'),
    std.length([1, 2, 3]),
  ]
}&lt;/code&gt;
    &lt;p&gt;Well, this is some sort of programming language with dynamic types. But there are so many good programming languages, so I don't know why this one needs to be used.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nickel&lt;/head&gt;
    &lt;code&gt;let conf = {
  name = "NiCl",
  version  = "0.0.1$",
  description = "My cool app!"
} in

let SemanticVersion = fun label value =&amp;gt;
  let pattern = "^\\d{1,2}\\.\\d{1,2}(\\.\\d{1,2})?$" in
  if std.string.is_match pattern value then
    value
  else
    let msg = "invalid version number" in
    std.contract.blame_with_message msg label
  in

let AppSchema = {
  name | String,
  version | SemanticVersion,
  description | String,
} in

conf | AppSchema&lt;/code&gt;
    &lt;p&gt;Nice programming language. Not a markup language.&lt;/p&gt;
    &lt;head rend="h3"&gt;Starlark&lt;/head&gt;
    &lt;code&gt;def fizz_buzz(n):
    """Print Fizz Buzz numbers from 1 to n."""
    for i in range(1, n + 1):
        s = ""
        if i % 3 == 0:
            s += "Fizz"
        if i % 5 == 0:
            s += "Buzz"
        print(s if s else i)

fizz_buzz(20)&lt;/code&gt;
    &lt;p&gt;Starlark is a programming language. Good lack porting/reimplementing Starlark in another programming language.&lt;/p&gt;
    &lt;head rend="h3"&gt;UCG&lt;/head&gt;
    &lt;code&gt;let tuple = {
    inner = {
        field = "value",
    },
    list = [1, 2] + [3],
    "quoted field" = "quoted value",
};&lt;/code&gt;
    &lt;p&gt;This is just an implementation of a language. Not portable to other languages.&lt;/p&gt;
    &lt;head rend="h3"&gt;UCL&lt;/head&gt;
    &lt;code&gt;.include "${CURDIR}/path.conf"
.macro_name(param={key=value}) "something";
section "blah" "foo" {
    key = value;
}&lt;/code&gt;
    &lt;p&gt;Okay. An implementation of a configuration language. No specification. No other implementations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Confetti&lt;/head&gt;
    &lt;code&gt;probe-device eth0 eth1

user * {
    login anonymous
    password "${ENV:ANONPASS}"
    machine 167.89.14.1
    proxy {
        try-ports 582 583 584
    }
}&lt;/code&gt;
    &lt;p&gt;Nice logo! But other than that, I found it difficult to understand what's going on in this example. What about escaping in strings?&lt;/p&gt;
    &lt;head rend="h3"&gt;Ziggy&lt;/head&gt;
    &lt;code&gt;{
  .name = "ziggy",
  .version = @v("1.0.0"),
  .license = @spdx("MIT"),
  .dependencies = {
    "react": "next",
    "leftpad": "^1.0.0",
  },
  .repository = Git {
    // "type" is now the struct name
    .url = "https://github.com",
  },
  .description = 
    \\# Ziggy
    \\
    \\A Data Serialization Language.
  ,
}&lt;/code&gt;
    &lt;p&gt;I actually liked the idea of differentiating structs and maps. But this is just an implementation of a tool. No specification. And I don't like Zig-style multiline strings.&lt;/p&gt;
    &lt;head rend="h3"&gt;HUML&lt;/head&gt;
    &lt;code&gt;website::
  ports:: 80, 443
  enabled: true
  factor: 3.14
  props:: mime_type: "text/html", encoding: "gzip"
  tags::
    - "markup"
    - "webpage"
    - "schema"&lt;/code&gt;
    &lt;p&gt;Nice idea to make YAML less worse. But it's still a YAML, with the same problems and significant indentation.&lt;/p&gt;
    &lt;head rend="h3"&gt;MAML&lt;/head&gt;
    &lt;code&gt;{
  project: "MAML"
  tags: [
    "minimal"
    "readable"
  ]

  # A simple nested object
  spec: {
    version: 1
    author: "Anton Medvedev"
  }

  # Array of objects
  examples: [
    {
      name: "JSON", born: 2001
    }
    {
      name: "MAML", born: 2025
    }
  ]

  notes: """
This is a multiline raw strings.
Keeps formatting as-is.
"""
}
&lt;/code&gt;
    &lt;p&gt;So, I decided to create a specification for my own language. And I wanted a distinctive name, not JSON something. I wanted a nice name and a strict specification. All languages have trade-offs. MAML as well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://medv.io/blog/things-i-dont-like-in-configuration-languages"/><published>2025-11-17T11:14:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45952890</id><title>Are you stuck in movie logic?</title><updated>2025-11-17T20:12:38.813654+00:00</updated><content/><link href="https://usefulfictions.substack.com/p/are-you-stuck-in-movie-logic"/><published>2025-11-17T12:06:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45953452</id><title>FreeMDU: Open-source Miele appliance diagnostic tools</title><updated>2025-11-17T20:12:38.660915+00:00</updated><content>&lt;doc fingerprint="af8dedd4595f4be1"&gt;
  &lt;main&gt;
    &lt;p&gt;The FreeMDU project provides open hardware and software tools for communicating with Miele appliances via their optical diagnostic interface. It serves as a free and open alternative to the proprietary Miele Diagnostic Utility (MDU) software, which is only available to registered service technicians.&lt;/p&gt;
    &lt;p&gt;Most Miele devices manufactured after 1996 include an optical infrared-based diagnostic interface, hidden behind one of the indicator lights on the front panel. On older appliances, this interface is marked by a Program Correction (PC) label.&lt;/p&gt;
    &lt;p&gt;Until now, communication with this interface required an expensive infrared adapter sold exclusively by Miele, along with their closed-source software. The goal of FreeMDU is to make this interface accessible to everyone for diagnostic and home automation purposes.&lt;/p&gt;
    &lt;p&gt;The project is split into three main components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protocol: core protocol library and device implementations&lt;/item&gt;
      &lt;item&gt;TUI: terminal-based device diagnostic and testing tool&lt;/item&gt;
      &lt;item&gt;Home: communication adapter firmware with MQTT integration for Home Assistant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details about the proprietary diagnostic interface and the reverse-engineering process behind this project can be found in this blog post.&lt;/p&gt;
    &lt;p&gt;Caution&lt;/p&gt;
    &lt;p&gt;This project is highly experimental and can cause permanent damage to your Miele devices if not used responsibly. Proceed at your own risk.&lt;/p&gt;
    &lt;p&gt;When a connection is established via the diagnostic interface, the appliance responds with its software ID, a 16-bit number that uniquely identifies the firmware version running on the device's microcontroller. However, this ID does not directly correspond to a specific model or board type, so it's impossible to provide a comprehensive list of supported models.&lt;/p&gt;
    &lt;p&gt;The following table lists the software IDs and device/board combinations that have been confirmed to work with FreeMDU:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Software ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Device&lt;/cell&gt;
        &lt;cell role="head"&gt;Board&lt;/cell&gt;
        &lt;cell role="head"&gt;Microcontroller&lt;/cell&gt;
        &lt;cell role="head"&gt;Optical interface location&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;360&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 223-A&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38078MC-065FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;419&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 206&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M37451MC-804FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;605&lt;/cell&gt;
        &lt;cell&gt;G 651 I PLUS-3&lt;/cell&gt;
        &lt;cell&gt;EGPL 542-C&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38027M8&lt;/cell&gt;
        &lt;cell&gt;Salt (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;629&lt;/cell&gt;
        &lt;cell&gt;W 2446&lt;/cell&gt;
        &lt;cell&gt;EDPL 126-B&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38079MF-308FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If your appliance is not listed here but has a model number similar to one of the above, it might already be compatible. In all other cases, determining the software ID is the first step toward adding support for new devices.&lt;/p&gt;
    &lt;p&gt;Details for adding support for new devices will be provided soon.&lt;/p&gt;
    &lt;p&gt;Before using any FreeMDU components, make sure you have the Rust toolchain installed on your system.&lt;/p&gt;
    &lt;p&gt;Next, you'll need to build a communication adapter to interface with your Miele device. Once the adapter is ready, choose the appropriate use case from the options below:&lt;/p&gt;
    &lt;p&gt;If you want to repair or test your appliance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the TUI application on your desktop computer.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to integrate your appliance into Home Assistant or another home automation system:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Flash the home firmware in standalone mode onto your communication adapter and attach it to your device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to develop your own software to communicate with Miele devices:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use the protocol crate to implement your custom software.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is an independent, open-source project and is not affiliated with, endorsed by, or sponsored by Miele &amp;amp; Cie. KG or its affiliates. All product names and trademarks are the property of their respective owners. References to Miele appliances are for descriptive purposes only and do not imply any association with Miele.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/medusalix/FreeMDU"/><published>2025-11-17T13:40:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45953568</id><title>The time has finally come for geothermal energy</title><updated>2025-11-17T20:12:38.418062+00:00</updated><content>&lt;doc fingerprint="254a117fb8a4a7aa"&gt;
  &lt;main&gt;
    &lt;p&gt;When I arrived in Reykjav√≠k, Iceland, last March, a gravel barrier, almost thirty feet at its highest point, had been constructed to keep lava from the Reykjanes volcano from inundating a major geothermal power station not far from downtown. So far, it had worked, but daily volcano forecasts were being broadcast on a small television at the domestic airport where I was waiting to take a short flight to Akureyri, a town on the north coast about an hour‚Äôs drive from one of the country‚Äôs oldest geothermal plants, the Krafla Geothermal Station. Until the early nineteen-seventies, Iceland relied on imported fossil fuels for nearly three-quarters of its energy. The resources of the country‚Äîa landscape of hot springs, lava domes, and bubbling mud pots‚Äîwere largely untapped. ‚ÄúIn the past, people here in the valley lacked most things now considered essential to human life, except for a hundred thousand million tons of boiling-hot water,‚Äù the Icelandic Nobelist Halld√≥r Laxness wrote in ‚ÄúA Parish Chronicle,‚Äù his 1970 novel. ‚ÄúFor a hundred thousand years this water, more valuable than all coal mines, ran in torrents out to sea.‚Äù The oil crisis of 1973, when prices more than tripled, proved a useful emergency. Among other efforts to develop local energy, public-investment funds provided loans for geothermal projects, whose upfront costs were considerable. By the early eighties, almost all the country‚Äôs homes were heated geothermally; in Reykjav√≠k, a subterranean geothermal-powered system is in place to melt snow and ice off sidewalks and roads. Today, more than a quarter of the country‚Äôs electricity comes from geothermal sources, a higher proportion than in almost any other nation. Most of the rest is from hydropower.&lt;/p&gt;
    &lt;p&gt;In some ways, the process of harnessing geothermal energy is simple. The deeper you dig, the hotter the temperatures get. For direct heating, you dig relatively shallow wells (typically several hundred metres deep), to access natural reservoirs of hot water or steam, which can be piped into a structure. For electricity, wells are dug farther down, to where temperatures are above a hundred and fifty degrees Celsius. (In Iceland, this temperature is reached at around one thousand to two thousand metres deep.) Pressurized steam spins a turbine that in turn spins a generator. Thermal energy (steam) is translated into mechanical energy (the spinning turbine), which is translated into electrical energy (via the generator). Geothermal energy is essentially carbon-free, it is available at any time of day and in any weather, and it leaves a small‚Äîalbeit very deep‚Äîfootprint on the landscape.&lt;/p&gt;
    &lt;p&gt;In 2008, Iceland‚Äôs three largest energy companies collaborated on a research project to drill down even farther, at a site near Krafla, for steam that was even hotter, some four hundred degrees Celsius. Such ‚Äúsupercritical‚Äù steam is water that is so hot and pressurized that it has passed into a fourth state, beyond gas. The hotter a well the better, typically: it will produce more energy more efficiently. The Iceland Deep Drilling Project (I.D.D.P.) engineers had planned to dig down some four kilometres‚Äîbut their drill got stuck at around two kilometres. Bits of black glass shot up from the well. After some disbelief, the team concluded that they had hit magma. This oops-ing into magma was at first received as ‚Äúvery bad news,‚Äù Bjarni Palsson, a chief project manager on the I.D.D.P. and now an executive vice-president of the energy company Landsvirkjun, told me. Many people thought that drilling into magma might trigger a volcanic eruption. ‚ÄúThen we started to see: What actually do we have here?‚Äù Palsson said. They put a wellhead on their work to measure the flow rates of steam. ‚ÄúWhat happened next was remarkable,‚Äù Palsson continued. The magma was about nine hundred degrees Celsius. The steam flow was such that it could produce ten times more energy than a regular well. They had created the hottest and most powerful geothermal well in the world.&lt;/p&gt;
    &lt;p&gt;There was no security check before boarding the plane. I was told by one of my companions, Hilmar M√°r Einarsson, a youthful project manager with Landsvirkjun, that people sometimes stowed their hunting rifles in the overhead luggage compartments. On the drive from Akureyri to Krafla, we passed Lake M√Ωvatn, home to a kind of arctic char that lives only there. We also passed Icelandic horses, a diminutive breed famed for its distinctive gaits: in addition to walking, trotting, and galloping, it has a ‚Äúflying pace‚Äù and a rhythmic four-beat gait known as t√∂lt. Amid the expansive greens and yellows of northeast Iceland, we arrived at the Krafla Geothermal Station, where steam has been spinning two Mitsubishi turbines continuously for decades.&lt;/p&gt;
    &lt;p&gt;The station provides power to commercial buildings and heating to homes in the district. A rust-red building in the shape of a giant barn stood across from silvery cooling towers capped by cloud-white steam. Construction began in 1974 but took four years to finish, working around the Krafla fires, a series of volcanic eruptions that went on for years. (In Icelandic folklore, the region is where the Devil landed after being expelled from Heaven.) Around the station is a volcanic valley of green vegetation and basalt rock, with patches of snow. The wellhouses appeared as igloo-size aluminum geodesic domes; like the main power station, they were rust red. Einarsson opened one for me. Visible within was a thick horizontal pipe joined to a vertical one, with what looked like a ship‚Äôs steering wheel attached. Not visible was the well itself, which extended belowground like a long metal straw. (Krafla‚Äôs geothermal wells are about seven inches across, notably narrower than many oil and gas wells.) ‚ÄúSome wells last twenty years, some last two‚Äîyou can‚Äôt know for certain,‚Äù Einarsson explained. The temperature and permeability of the rock, as well as the amount of fluid flowing across it, affect a well‚Äôs performance. Also, Iceland has myths about ‚Äúhidden people‚Äù‚Äîhulduf√≥lk. It is said that building on their land brings bad luck, so there‚Äôs that, too.&lt;/p&gt;
    &lt;p&gt;We stopped by the station‚Äôs canteen, taking our shoes off before entering. Lunch had ended, but there was homemade apple cake, dried apricots, and skyr available. Workmen in neon-yellow suits, who had traded their boots for slippers, were having tea. Einarsson then took me to the I.D.D.P. site, not far from the Krafla plant, where a sign marked a snow-covered depression about the size of a modest pond. Compared with the turbines and steam towers and the idyllic orderliness of the canteen, the site was underwhelming. Two years after the well was dug, the extreme pressure and heat began to corrode the metal casing of the well itself. Black smoke poured out each time the well was reopened. Soon, it had to be shut down permanently. In 2017, another research well, I.D.D.P.-2, was drilled down four and a half kilometres, where temperatures reached at least four hundred and twenty-six degrees Celsius‚Äîbut this time the well failed after only six months. ‚ÄúOne thing we learned is that you don‚Äôt open and close and open and close the well‚Äîyou just leave it open,‚Äù Palsson had told me, explaining that such actions made the well more brittle.&lt;/p&gt;
    &lt;p&gt;Landsvirkjun, which had paid for most of the I.D.D.P. work, decided that it needed financial support to drill more exploratory wells. ‚ÄúWe said, ‚ÄòWe‚Äôre just a small energy company in Iceland,‚Äô ‚Äù Palsson told me. But it made its research available to the international scientific community, and there has been intermittent interest from the U.K., Germany, Canada, and New Zealand. ‚ÄúThat‚Äôs where we are now, trying to fund it as a science project that can also benefit the energy industry,‚Äù Palsson said.&lt;/p&gt;
    &lt;p&gt;Driving back to the airport, we saw snow ptarmigans and cairns of black stones marking trails that stretched beyond view. Iceland‚Äôs transition into a country powered nearly completely by renewables can seem fantastical, and the landscape furthers this impression. Because Iceland is singular in so many ways‚Äîthat lonely arctic-char species! those small horses with their t√∂lt!‚Äîyou can get the feeling that geothermal energy is a niche endeavor, as opposed to one that is technically and economically feasible in places where volcanic eruptions aren‚Äôt part of the daily forecast. But that feeling is outdated and misleading.&lt;/p&gt;
    &lt;p&gt;Geothermal is underdeveloped, and its upfront costs can be high, but it‚Äôs always on and, once it‚Äôs set up, it is cheap and enduring. The dream of geothermal energy is to meet humanity‚Äôs energy demands affordably, without harnessing horses for horsepower, slaughtering whales for their oil, or burning fossil fuels. The planet‚Äôs heat could be used to pasteurize milk or heat dorm rooms or light up a baseball stadium for a night game.&lt;/p&gt;
    &lt;p&gt;At more than five thousand degrees Celsius, the Earth‚Äôs core is roughly as hot as the surface of the sun. At the Earth‚Äôs surface, the temperature is about fourteen degrees. But in some places, like Iceland, the ground underfoot is much warmer. Hot springs, geysers, and volcanoes are surface-level signs of the Earth‚Äôs inferno. Dante‚Äôs description of Hell is said by some to have been inspired by the landscape of sulfurous steam plumes found in Devil‚Äôs Valley in Tuscany.&lt;/p&gt;
    &lt;p&gt;Snow monkeys and humans have been using Earth-heated waters as baths for ages. In the Azores, a local dish, cozido de las furnas, is cooked by burying a clay pot in hot volcanic soil; in Iceland, bread is still sometimes baked this way. The first geothermal power generator was built in Devil‚Äôs Valley, in 1904, by Prince Piero Ginori Conti of Trevignano, who had been extracting borax from the area and thought to make use of the steam emerging from the mining borehole. The generator initially powered five light bulbs. Not long afterward, it powered central Italy‚Äôs railway system and a few villages. The geothermal complex is still in operation today, providing one to two per cent of Italy‚Äôs energy. In the United States, the first geothermal plant was built in 1921, in Northern California, in a geyser-filled area that a surveyor described as the gates of Hell. That plant powered a nearby resort hotel and is also still in use.&lt;/p&gt;
    &lt;p&gt;There aren‚Äôt gates of Hell just anywhere. A kilometre below ground in Kamchatka is considerably hotter than a kilometre below ground in Kansas. There is also readily accessible geothermal energy in Kenya (where it provides almost fifty per cent of the country‚Äôs energy), New Zealand (about twenty per cent), and the Philippines (about fifteen per cent)‚Äîall volcanic areas along tectonic rifts. But in less Hadean landscapes the costs and uncertainties of drilling deep in search of sufficient heat have curtailed development. This partly explains why, in the field of clean energy, geothermal is often either not on the list or mentioned under the rubric of ‚Äúother.‚Äù For decades, both private and government investment in geothermal energy was all but negligible.&lt;/p&gt;
    &lt;p&gt;That has now changed. In the past five years, in North America, more than a billion and a half dollars have gone into geothermal technologies. This is a small amount for the energy industry, but it‚Äôs also an exponential increase. In May, 2021, Google signed a contract with the Texas-based geothermal company Fervo to power its data centers and infrastructure in Nevada; Meta signed a similar deal with Texas-based Sage for a data center east of the Rocky Mountains, and with a company called XGS for one in New Mexico. Microsoft is co-developing a billion-dollar geothermal-powered data center in Kenya; Amazon installed geothermal heating at its newly built fulfillment center in Japan. (Geothermal energy enables companies to avoid the uncertainties of the electrical grid.) Under the Biden Administration, the geothermal industry finally received the same kind of tax credits given to wind and solar, and under the current Trump Administration it has received the same kind of fast-track permitting given to oil and gas. Donald Trump‚Äôs Secretary of Energy, Chris Wright, spoke at a geothermal conference and declared, in front of a MAGA-like sign that read ‚ÄúMAGMA (Making America Geothermal: Modern Advances),‚Äù that although geothermal hasn‚Äôt achieved ‚Äúliftoff yet, it should and it can.‚Äù Depending on whom you speak with, either it‚Äôs weird that suddenly everyone is talking about geothermal or it‚Äôs weird that there is a cost-competitive energy source with bipartisan appeal that no one is talking about.&lt;/p&gt;
    &lt;p&gt;Scientific work that has been discarded or forgotten can return‚Äîsometimes through unknowing repetition, at other times through deliberate recovery. In the early nineteen-seventies, the U.S. government funded a program at Los Alamos that looked into developing geothermal energy systems that didn‚Äôt require proximity to geysers or volcanoes. Two connected wells were built: in one, water was sent down into fractured hot, dry rock; from the other, the steam that resulted from the water meeting the rock emerged. In 1973, Richard Nixon announced Project Independence, which aimed to develop energy sources outside of fossil fuels. ‚ÄúBut when Reagan came into office, he changed things,‚Äù Jefferson Tester, a professor of sustainable energy systems at Cornell University, who was involved in the Los Alamos project, told me. The price of oil had come down, and support for geothermal dissipated. ‚ÄúPeople got this impression that it was a failure,‚Äù Tester said. ‚ÄúI think if they looked a little closer, they would see that a lot of the knowledge gained in those first years could have been used to leverage what is happening now.‚Äù&lt;/p&gt;
    &lt;p&gt;Tester went on to help establish the M.I.T. Energy Lab (now called the Energy Initiative), which focusses on advancing clean-energy solutions. He and his colleagues felt that students needed to know the history of the research into diverse energy sources, so they put together a course and a textbook called ‚ÄúSustainable Energy: Choosing Among Options.‚Äù In 2005, the Department of Energy, under George W. Bush, commissioned a group consisting of Tester and some seventeen other experts and researchers‚Äîincluding drilling engineers, energy economists, and power-plant builders‚Äîto investigate what it would take for the U.S. to produce a hundred thousand megawatts of geothermal energy, a bit more than one-fifth of the energy the U.S. had consumed that year. (Geothermal energy production in the U.S. at that time was around three or four thousand megawatts.) The experts avoided framing their support for geothermal in environmental terms. ‚ÄúThe feeling was that you weren‚Äôt supposed to talk about carbon, because then it would be perceived as about climate change,‚Äù Tester said.&lt;/p&gt;
    &lt;p&gt;In 2006, Tester and his colleagues published their report, ‚ÄúThe Future of Geothermal Energy.‚Äù One finding was that new drilling technology employed by the oil-and-gas industry was changing the economics of geothermal power generation. Latent ideas‚Äîlike those from the Los Alamos project‚Äîhad met their moment. ‚ÄúI was called to testify a few times before Congress. It was a relatively modest investment that was needed, and people were excited,‚Äù Tester told me. ‚ÄúBut then we submitted the report to the Department of Energy. And they did nothing. It was crazy.‚Äù He was still visibly dismayed.&lt;/p&gt;
    &lt;p&gt;One explanation for the lack of action is that, around that time, the U.S. went from being an oil importer to an oil exporter. This turnaround was largely due to the innovations of George Mitchell, a second-generation Greek American in Galveston, Texas, who spent years trying to extract oil and gas from the Barnett Shale formation, in North Texas, in an economically viable way. His approach synthesized hydraulic fracturing, or fracking, with horizontal drilling. Fracking involves injecting fluid down a well at high pressures, which cracks the subsurface, and the horizontal drilling augments the area of cracking. Eventually, Mitchell‚Äôs company, helped by generous tax incentives, made the economics work. Vast oil reserves became accessible. Fortunes were made. Fracking overwhelmed the renewed interest in geothermal power. But a couple of decades later there was a reversal: fracking accelerated geothermal power.&lt;/p&gt;
    &lt;p&gt;Tim Latimer, the thirty-five-year-old C.E.O. of Fervo Energy, a geothermal company founded in 2017, grew up in Riesel, Texas, a small town about fifteen miles outside Waco. After graduating from the University of Tulsa with a degree in mechanical engineering, Latimer wanted a well-paid engineering job close to home. ‚ÄúMy adviser was just, like, ‚ÄòHave you ever heard of the oil-and-gas industry?‚Äô ‚Äù he said, smiling.&lt;/p&gt;
    &lt;p&gt;As a greenhorn drilling engineer with the international mining company BHP Billiton, Latimer was put on a fracking project in the Eagle Ford Shale, in South Texas. The shale, which is a Cretaceous-era formation dense with marine fossils from when the area was an inland sea, is relatively hard and hot. ‚ÄúThe motors in our drill systems were failing early,‚Äù Latimer said. His supervisors suspected that this was because of the wells‚Äô unusually high temperatures, around a hundred and seventy-five degrees Celsius. ‚ÄúThey said, ‚ÄòCan you research what tools we could use to deal with the fact that these drilling temperatures are really high?‚Äô ‚Äù Latimer told me.&lt;/p&gt;
    &lt;p&gt;Much of the relevant work Latimer came across turned up in papers about geothermal energy. ‚ÄúI‚Äôd never heard of geothermal before,‚Äù he said. ‚ÄúI was, like, ‚ÄòWell, this seems pretty cool.‚Äô ‚Äù When Latimer read the 2006 ‚ÄúFuture of Geothermal Energy‚Äù report, including its description of the Los Alamos geothermal project, he saw parallels to his work in oil and gas. The report described two big technical challenges that were standing in the way of affordable, bountiful clean energy. One was getting drilling costs down‚Äîan area that oil and gas had made great progress in. The other was getting water flowing through hot rock that isn‚Äôt sufficiently permeable, like shale, so that you can generate steam. ‚ÄúAnd I‚Äôm just looking at the rig, being, like, ‚ÄòThis is a solved problem.‚Äô ‚Äù Generating flow where there isn‚Äôt much naturally‚Äîthat‚Äôs what hydraulic fracturing does.&lt;/p&gt;
    &lt;p&gt;Latimer reported what he had found to BHP. The shale drilling started working again, but Latimer‚Äôs imagination had shifted. In 2014, he applied to Stanford Business School with the goal of using fracking technology in geothermal wells. ‚ÄúGeothermal is an industry that, frankly, at that point in time, people had given up on as forgotten,‚Äù he said. ‚ÄúI didn‚Äôt think that was right. I was, like, ‚ÄòI‚Äôm a drilling engineer. I actually have a skill that can make a direct impact on this.‚Äô ‚Äù&lt;/p&gt;
    &lt;p&gt;In 2017, Latimer and his Stanford colleague Jack Norbeck co-founded Fervo Energy. ‚ÄúFracking‚Äù is an unpopular word. Fervo describes itself as a ‚Äúnext-generation geothermal energy developer.‚Äù Just as the fusion-energy industry avoids the phrase ‚Äúnuclear fusion,‚Äù and the term ‚Äúnatural gas‚Äù is now used for what is mostly methane, geothermal systems involving hydraulic fracturing tend to be referred to as ‚Äúenhanced‚Äù geothermal systems, or E.G.S.&lt;/p&gt;
    &lt;p&gt;In 2023, Fervo drilled a pair of demonstration wells in Nevada, proving its ideas in anticipation of scaling them up. The goal is to begin operating a five-hundred-megawatt geothermal power plant in Cape Station, Utah, with a hundred megawatts going online in 2026. This past June, Fervo drilled a four-and-a-half-kilometre appraisal well‚Äîa well for confirming predicted subsurface conditions before going all in on a site‚Äîthat reached temperatures of two hundred and seventy degrees Celsius. The well was drilled in sixteen days, remarkably fast, and faster drill times mean lowered costs. Fervo‚Äôs well design and drilling technologies are central to its hopes, and have helped it raise more than eight hundred million dollars in investment capital. Most everyone I spoke to seems to be rooting for Fervo, albeit with some skepticism. The Utah site is far away from a source for the large amounts of water that will be required, for instance. There are more technical points of concern, too. Fracking can induce seismic activity, so the siting of wells is an important consideration. Enthusiasts see these as solvable problems. And Fervo is not alone in showing promise in the use of fracking to access geothermal power. At the end of October, Mazama Energy demonstrated a pilot E.G.S. in Newberry, Oregon, that works at an even higher temperature: above three hundred degrees Celsius. For now, though, E.G.S. is still a kind of wildcat proposition. ‚ÄúI think the big question is: Who are the next nine Fervos?‚Äù Roland Horne, a professor of energy science and engineering at Stanford, who has studied geothermal energy for about fifty years, said. ‚ÄúFervo has expanded tremendously, they‚Äôre a nearly two-hundred-person company, but they don‚Äôt have the wherewithal to do a gigawatt project yet.‚Äù&lt;/p&gt;
    &lt;p&gt;Fervo pitches itself as a landing place for oil-and-gas workers. ‚ÄúI‚Äôve spent a lot of my life and career in small towns where the largest economic driver is oil and gas,‚Äù Latimer said. Geothermal means jobs in drilling: engineers, geologists, project managers. Barry Smitherman, who has worked as an oil-and-gas regulator in Texas and as the head of a utility company, told me, ‚ÄúWe‚Äôve been drilling oil and gas wells in Texas for over a hundred years. We‚Äôve drilled over a million wells. We know what the world looks like below the surface in Texas.‚Äù&lt;/p&gt;
    &lt;p&gt;In February, 2021, Winter Storm Uri left most of Texas without power for days. Not long afterward, Smitherman was asked to speak to state legislators about what went wrong and what needed to change. Soon he got a call from a foundation set up by George Mitchell and his wife, Cynthia. (George, who died in 2013, is known as the ‚Äúfather of fracking.‚Äù) The Mitchell Foundation wanted Smitherman to help start a local organization that would advocate on behalf of geothermal energy. He co-founded the Texas Geothermal Energy Alliance in 2022. During his long career in energy, Smitherman said, ‚Äúwe never had a conversation about geothermal. No one had brought it to my attention.‚Äù Smitherman had a series of meetings with people from geothermal startups, oil-and-gas companies, the Sierra Club, and utility companies. ‚ÄúWhat we‚Äôve always said around energy is that you need three legs‚Äîreliable, clean, and cheap. Those are the three legs of the stool. The old saying was ‚ÄòI can give you two of three, but I can‚Äôt give you all three,‚Äô ‚Äù he said. ‚ÄúBut, as we began to look at geothermal, it really began to look like it had all three‚Äîlow to no carbon, 24/7, and, as the cost curve comes down, eventually, cheap. It really began to look like this unicorn resource.‚Äù&lt;/p&gt;
    &lt;p&gt;Tester now teaches at Cornell, near the Finger Lakes, where in the winter Buttermilk and Taughannock Falls turn to blue ribbons of ice. ‚ÄúIf we look at the country and say our goal is ultimately to be much more sustainable with respect to our carbon footprint, you can‚Äôt ignore heating,‚Äù Tester said. Around thirty per cent of New York State‚Äôs carbon footprint can be attributed to heating and cooling buildings, a figure that is not far from the worldwide average. ‚ÄúA lot of it is for space heating and water heating, but also for low-temperature food processing, things like that,‚Äù Tester added. The excitement about geothermally generated electricity can obscure the thing that geothermal technology is, arguably, best suited to provide. ‚ÄúIt‚Äôs a little bit apples and oranges, and we need both electricity and heating,‚Äù Tester told me. But he went on to explain that using electricity for heating is not nearly as efficient as using heat for heating. And geothermal wells for heating, which can be relatively shallow, can work in places with no hot springs or volcanoes.&lt;/p&gt;
    &lt;p&gt;Midtown Manhattan, for example: as part of a major renovation completed in 2017, ten geothermal wells were dug beneath St. Patrick‚Äôs Cathedral. Some of the wells are less than a hundred metres deep, while others extend more than six hundred and fifty metres, more than ten times deeper than Manhattan‚Äôs deepest subway tunnel‚Äîand yet much shallower than the wells needed for a geothermal power plant. These wells carry warmth into the cathedral in winter, and out of the cathedral in summer, and do so with less noise and vibration than typical HVAC systems. The main issue is the upfront cost. When the cathedral‚Äôs system was built, it felt radical. One of the lead engineers on the project, Paul Boyce, of P. W. Grosser Consulting, told me that the demand for geothermal heating systems has grown dramatically since then. P.W.G.C.‚Äôs current geothermal projects include the Mastercard headquarters, in Purchase, New York, and the Obama Presidential Center, in Chicago. In Greenpoint, Brooklyn, an eight-hundred-and-thirty-four-unit apartment complex that‚Äôs under construction has its heating and cooling provided through three hundred boreholes, none much deeper than about a hundred and fifty metres. The system was put in by Geosource Energy, a geothermal company started in 2004.&lt;/p&gt;
    &lt;p&gt;But those projects provide geothermal energy building by building, not district by district. ‚ÄúI wish we were looking at how we plan our cities,‚Äù Tester said. ‚ÄúIt‚Äôs crazy that heating, electricity, cable, water‚Äîthese are all managed separately.‚Äù He is now in the midst of a research project that aims to demonstrate the feasibility of an ambitious geothermal system to serve Cornell‚Äôs seven-hundred-and-forty-five-acre campus, something close to what downtown Reykjav√≠k has‚Äîbut without the aid of close-to-the-surface magma. In the summer of 2022, a rig set up not far from Cornell‚Äôs School of Veterinary Medicine drilled for sixty-five days through layers of shale, limestone, and sandstone, passing beyond the geologic time of the dinosaurs to a crystalline basement dating to the Proterozoic eon, more than five hundred million years ago. This created the Cornell University Borehole Observatory (CUBO). In Iceland, if you dig down this deep, the temperatures could easily be four hundred degrees Celsius; in New York, the rocks are cooler, but the Cornell project needs to reach only eighty to ninety degrees Celsius. As CUBO was drilled, rock samples from each depth were analyzed, and the surrounding natural fracture systems were mapped. If CUBO secures more funding, the next stage will be to drill a pair of wells, with one for injecting water to make an underground reservoir and the other to bring the heated water up.&lt;/p&gt;
    &lt;p&gt;In other geographies, geothermal energy for district heating and cooling has been accomplished with shallower wells. Mieres, Spain, a historic mining town, uses warm water from the now closed mines to supply heat to the region. Nijar, also in Spain but closer to a volcano, uses an underground fluid reservoir to heat its greenhouses. Hayden, Colorado, a former coal town, is working with Bedrock Energy, a Texas-based company started in 2022, to construct a municipal geothermal district, in the hope that reduced energy bills will attract businesses. In Framingham, Massachusetts, activists and a local energy company collaborated on a geothermal heating-and-cooling network, and near Austin, Texas, the neighborhood of Whisper Valley is putting in a similar grid. Several companies, including Bedrock and Dig Energy, are aiming to bring drilling costs down by half or more. Geothermal systems for heating and cooling individual homes remain somewhat pricey to install, but they last for decades, reduce energy bills by twenty-five to fifty per cent, and avoid reliance on the ever more burdened electrical grid. Most people I spoke with in the geothermal industry make their case for it by focussing on cost savings; the unspoken climate benefits are known to those disposed to care, and potentially off-putting to those who are not.&lt;/p&gt;
    &lt;p&gt;Some environmentalists argue that the resources given to geothermal‚Äîor to small modular nuclear plants, or to fusion‚Äîwould be better spent elsewhere. Why not just go all in on solar, wind, and batteries, which are proven, scalable technologies? To invest in more speculative solutions, the argument goes, is a moral hazard, and a cynical or na√Øve distraction that obscures the solutions available now. But this line of thinking rests on the assumption that the people or nations or agencies that would fund one kind of energy would equally fund some other kind. This tends not to be true‚Äîfunding is rarely fungible, and always capricious. One geothermal-startup founder spoke of receiving a call from a potential investor‚Äôs adviser, who said, Sorry, the managing partner wants to invest in a blimp company instead. ‚ÄúGeothermal is the least moral hazard-y of the clean-energy technologies,‚Äù Gernot Wagner, a climate economist at Columbia Business School, said. ‚ÄúAnd we are still subsidizing nuclear a thousand times more than geothermal.‚Äù&lt;/p&gt;
    &lt;p&gt;An energy future without hydrocarbons will require working flexibly with the many variables of resources, geography, and politics. ‚ÄúWe can get maybe ninety per cent of the way with solar, batteries, wind,‚Äù Leah Stokes, a professor of environmental politics at the University of California, Santa Barbara, told me. ‚ÄúBut geothermal is one of the things that can fill that gap.‚Äù Investment follows fashion‚Äîand geothermal has become fashionable‚Äîbut it‚Äôs not only investors who appear confident about geothermal. Wagner called this ‚Äúthe moment when Ph.D.s meet M.B.A.s.‚Äù&lt;/p&gt;
    &lt;p&gt;The role of geothermal becomes easier to see when looking beyond the local noise of discussions in America. ‚ÄúYou know, there‚Äôs this thing called the curse of abundance,‚Äù Agnelli Kafuwe, the principal energy officer for the Zambian government, told me. Typically, the phrase refers to countries driven into corruption and misery by their oil endowments, but Kafuwe was referring to Zambia‚Äôs seemingly boundless supply of hydroelectric energy, from power stations such as one at the Zambezi River‚Äôs Mosi-oa-Tunya, the natural feature known to most Americans as Victoria Falls. For many years, hydropower met practically all of Zambia‚Äôs energy needs, even powering its lucrative copper mines.&lt;/p&gt;
    &lt;p&gt;But the country‚Äôs population grew rapidly, and in 2015 a severe drought hit, forcing Zambia to turn to diesel to make up the shortfall in hydropower. Mosi-oa-Tunya looked less like a world-renowned cataract than like dry, rocky cliffs. There wasn‚Äôt enough water to keep the hydropower plants running properly. Lengthy blackouts became common. In 2024, a new drought arrived‚Äîthe worst in at least a century‚Äîand power was cut off for eighteen to twenty hours a day. As in many countries, the leadership had thought about geothermal in the nineteen-seventies but had lost interest; Zambia hadn‚Äôt needed it enough then.&lt;/p&gt;
    &lt;p&gt;In addition to copper mining, extensive salt mining occurs in northern Zambia. ‚ÄúThese mining companies, they would drill down maybe fifty metres, and guess what comes up?‚Äù Kafuwe said. ‚ÄúGeothermal steam, of a very high, very good temperature.‚Äù The country‚Äôs mining history also meant that subsurface maps of its territory already existed‚Äîuseful for planning geothermal wells. One former mining-company head, Peter Vivian-Neal, now heads Kalahari GeoEnergy, a company he founded after seeing an egg being boiled in a natural hot spring while he was on safari. The company has drilled exploratory wells, done flow tests, well tests, and modelling‚Äîit aims to have a demonstration power plant running soon. Vivian-Neal is optimistic that a successful demonstration will bring in more investment. ‚ÄúWe could not have got to where we got today if my family hadn‚Äôt put in the money to start with,‚Äù he told me. ‚ÄúBut I‚Äôm quite sure that the next person will find it easier. They‚Äôll say, ‚ÄòOh, yes, look, Kalahari has made this a success, therefore we‚Äôre going to make it a success, and we‚Äôll do it even faster.‚Äô ‚Äù ‚ô¶&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.newyorker.com/magazine/2025/11/24/why-the-time-has-finally-come-for-geothermal-energy"/><published>2025-11-17T13:55:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45953702</id><title>Replicate is joining Cloudflare</title><updated>2025-11-17T20:12:38.088127+00:00</updated><content>&lt;doc fingerprint="3552db724b9eaa89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Replicate is joining Cloudflare&lt;/head&gt;
    &lt;p&gt;Big news: We‚Äôre joining Cloudflare.&lt;/p&gt;
    &lt;p&gt;Replicate‚Äôs going to carry on as a distinct brand, and all that‚Äôll happen is that it‚Äôs going to get way better. It‚Äôll be faster, we‚Äôll have more resources, and it‚Äôll integrate with the rest of Cloudflare‚Äôs Developer Platform.&lt;/p&gt;
    &lt;p&gt;The API isn‚Äôt changing. The models you‚Äôre using today will keep working. If you‚Äôve built something on Replicate, it‚Äôll keep running just like it does now.&lt;/p&gt;
    &lt;p&gt;So, why are we doing this?&lt;/p&gt;
    &lt;p&gt;At Replicate, we‚Äôre building the primitives for AI: the tools and abstractions that let software developers use AI without having to understand all the complex stuff underneath.&lt;/p&gt;
    &lt;p&gt;We started with Cog, an open-source tool which defines a standard format for what a model is. Then, we created Replicate, a platform where people can share models and run them with an API. We‚Äôve defined what a model is, how you publish it, how you run it, how you get data in and out.&lt;/p&gt;
    &lt;p&gt;These abstractions are like the low-level primitives of an operating system. But what‚Äôs interesting is that these primitives are running in the cloud. They have to ‚Äî they need specialized GPUs and clusters to scale up in production. It‚Äôs like a distributed operating system for AI, running in the cloud. In other words, the network is the computer.&lt;/p&gt;
    &lt;p&gt;Who has the best network? Cloudflare.&lt;/p&gt;
    &lt;p&gt;Cloudflare has built so many other parts of this operating system. Workers is the perfect platform for running agents and glue code. Durable Objects for managing state, R2 for storing files, WebRTC for streaming media.&lt;/p&gt;
    &lt;p&gt;Now that we‚Äôve got these low-level abstractions, we can build higher-level abstractions. Ways to orchestrate models and build agents. Ways to run real-time models, or run models on the edge.&lt;/p&gt;
    &lt;p&gt;This is why we‚Äôre joining Cloudflare.&lt;/p&gt;
    &lt;p&gt;For my whole career, I‚Äôve looked up to Cloudflare. How they built a product for developers, and turned that into a huge enterprise business. It‚Äôs the only public company that actually gets developers and knows how to build good products for them.&lt;/p&gt;
    &lt;p&gt;Cloudflare is the default for building web apps. From day one of Replicate, when we were building a prototype to apply to Y Combinator, we put Cloudflare in front of it.&lt;/p&gt;
    &lt;p&gt;Together, we‚Äôre going to become the default for building AI apps.&lt;/p&gt;
    &lt;p&gt;Check out the official announcement on Cloudflare‚Äôs blog for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://replicate.com/blog/replicate-cloudflare"/><published>2025-11-17T14:11:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45953977</id><title>Show HN: ESPectre ‚Äì Motion detection based on Wi-Fi spectre analysis</title><updated>2025-11-17T20:12:37.599860+00:00</updated><content>&lt;doc fingerprint="149c507545b3b9c1"&gt;
  &lt;main&gt;
    &lt;p&gt;Motion detection system based on Wi-Fi spectre analysis (CSI), with Home Assistant integration.&lt;/p&gt;
    &lt;p&gt;üì∞ Featured Article: Read the complete story behind ESPectre on Medium üáÆüáπ Italian, üá¨üáß English&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In 3 Points&lt;/item&gt;
      &lt;item&gt;Mathematical Approach&lt;/item&gt;
      &lt;item&gt;What You Need&lt;/item&gt;
      &lt;item&gt;Quick Start&lt;/item&gt;
      &lt;item&gt;How It Works&lt;/item&gt;
      &lt;item&gt;What You Can Do With It&lt;/item&gt;
      &lt;item&gt;Sensor Placement Guide&lt;/item&gt;
      &lt;item&gt;System Architecture&lt;/item&gt;
      &lt;item&gt;FAQ&lt;/item&gt;
      &lt;item&gt;Security and Privacy&lt;/item&gt;
      &lt;item&gt;Technical Deep Dive&lt;/item&gt;
      &lt;item&gt;Future Evolutions&lt;/item&gt;
      &lt;item&gt;References&lt;/item&gt;
      &lt;item&gt;Changelog&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
      &lt;item&gt;Author&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What it does: Detects movement at home using Wi-Fi (no cameras, no microphones)&lt;/item&gt;
      &lt;item&gt;What you need: A ~‚Ç¨10 device (ESP32-S3) + Home Assistant or MQTT server + ESP-IDF development tools&lt;/item&gt;
      &lt;item&gt;Setup time: 30-45 minutes (first time, including ESP-IDF setup)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project currently does NOT use Machine Learning models. Instead, it employs a mathematical approach that extracts 10 features from CSI (Channel State Information) data using statistical and signal processing techniques.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ No ML training required: Works out-of-the-box with mathematical algorithms&lt;/item&gt;
      &lt;item&gt;‚úÖ 10 extracted features: Statistical, spatial, and temporal features&lt;/item&gt;
      &lt;item&gt;‚úÖ Real-time processing: Low latency detection on ESP32-S3 hardware&lt;/item&gt;
      &lt;item&gt;‚úÖ Foundation for ML: These features can serve as the basis for collecting labeled datasets to train ML models for advanced tasks (people counting, activity recognition, gesture detection)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The mathematical approach provides excellent movement detection without the complexity of ML model training, while the extracted features offer a solid foundation for future ML-based enhancements.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ 2.4GHz Wi-Fi Router (the one you already have at home works fine)&lt;/item&gt;
      &lt;item&gt;‚úÖ ESP32-S3 DevKit bundle with external antennas (~‚Ç¨10) - Available on Amazon, AliExpress, or electronics stores&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ESP32-S3 DevKit with external antennas (recommended for better reception)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ MQTT Broker (required for operation): &lt;list rend="ul"&gt;&lt;item&gt;Home Assistant with built-in MQTT broker (on Raspberry Pi, PC, NAS, or cloud)&lt;/item&gt;&lt;item&gt;OR standalone Mosquitto MQTT server (can run on any device, including Raspberry Pi)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;‚úÖ ESP-IDF v6.1 (development framework for building firmware)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Basic command line knowledge required for building and flashing firmware&lt;/item&gt;
      &lt;item&gt;‚ùå NO router configuration needed&lt;/item&gt;
      &lt;item&gt;‚úÖ Follow the setup guide in SETUP.md&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Setup time: ~30-45 minutes (first time)&lt;lb/&gt; Difficulty: Intermediate (requires ESP-IDF setup)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Setup &amp;amp; Installation: Follow the complete guide in SETUP.md&lt;/item&gt;
      &lt;item&gt;Calibration &amp;amp; Tuning: Optimize for your environment with CALIBRATION.md&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When someone moves in a room, they "disturb" the Wi-Fi waves traveling between the router and the sensor. It's like when you move your hand in front of a flashlight and see the shadow change.&lt;/p&gt;
    &lt;p&gt;The ESP32-S3 device "listens" to these changes and understands if there's movement.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ No cameras (total privacy)&lt;/item&gt;
      &lt;item&gt;‚úÖ No wearables needed (no bracelets or sensors to wear)&lt;/item&gt;
      &lt;item&gt;‚úÖ Works through walls (Wi-Fi passes through walls)&lt;/item&gt;
      &lt;item&gt;‚úÖ Very cheap (~‚Ç¨10 total)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;üìö Technical Explanation (click to expand)&lt;/head&gt;
    &lt;p&gt;Channel State Information (CSI) represents the physical characteristics of the wireless communication channel between transmitter and receiver. Unlike simple RSSI (Received Signal Strength Indicator), CSI provides rich, multi-dimensional data about the radio channel.&lt;/p&gt;
    &lt;p&gt;Per-subcarrier information:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amplitude: Signal strength for each OFDM subcarrier (up to 64)&lt;/item&gt;
      &lt;item&gt;Phase: Phase shift of each subcarrier&lt;/item&gt;
      &lt;item&gt;Frequency response: How the channel affects different frequencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Environmental effects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multipath propagation: Reflections from walls, furniture, objects&lt;/item&gt;
      &lt;item&gt;Doppler shifts: Changes caused by movement&lt;/item&gt;
      &lt;item&gt;Temporal variations: How the channel evolves over time&lt;/item&gt;
      &lt;item&gt;Spatial patterns: Signal distribution across antennas/subcarriers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When a person moves in an environment, they:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alter multipath reflections (new signal paths)&lt;/item&gt;
      &lt;item&gt;Change signal amplitude and phase&lt;/item&gt;
      &lt;item&gt;Create temporal variations in CSI patterns&lt;/item&gt;
      &lt;item&gt;Modify the electromagnetic field structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These changes are detectable even through walls, enabling privacy-preserving presence detection without cameras, microphones, or wearable devices.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üè† Home security: Get an alert if someone enters while you're away&lt;/item&gt;
      &lt;item&gt;üë¥ Elderly care: Monitor activity to detect falls or prolonged inactivity&lt;/item&gt;
      &lt;item&gt;üí° Smart automation: Turn on lights/heating only when someone is present&lt;/item&gt;
      &lt;item&gt;‚ö° Energy saving: Automatically turn off devices in empty rooms&lt;/item&gt;
      &lt;item&gt;üë∂ Child monitoring: Alert if they leave the room during the night&lt;/item&gt;
      &lt;item&gt;üå°Ô∏è Climate control: Heat/cool only occupied zones&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Optimal sensor placement is crucial for reliable movement detection.&lt;/p&gt;
    &lt;p&gt;Optimal range: 3-8 meters&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Distance&lt;/cell&gt;
        &lt;cell role="head"&gt;Signal&lt;/cell&gt;
        &lt;cell role="head"&gt;Multipath&lt;/cell&gt;
        &lt;cell role="head"&gt;Sensitivity&lt;/cell&gt;
        &lt;cell role="head"&gt;Noise&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommendation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;&amp;lt; 2m&lt;/cell&gt;
        &lt;cell&gt;Too strong&lt;/cell&gt;
        &lt;cell&gt;Minimal&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;‚ùå Too close&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;3-8m&lt;/cell&gt;
        &lt;cell&gt;Strong&lt;/cell&gt;
        &lt;cell&gt;Good&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Optimal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&amp;gt; 10-15m&lt;/cell&gt;
        &lt;cell&gt;Weak&lt;/cell&gt;
        &lt;cell&gt;Variable&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;‚ùå Too far&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;‚úÖ Position sensor in the area to monitor (not necessarily in direct line with router)&lt;lb/&gt; ‚úÖ Height: 1-1.5 meters from ground (desk/table height)&lt;lb/&gt; ‚úÖ External antenna: Use IPEX connector for better reception&lt;lb/&gt; ‚ùå Avoid metal obstacles between router and sensor (refrigerators, metal cabinets)&lt;lb/&gt; ‚ùå Avoid corners or enclosed spaces (reduces multipath diversity)&lt;/p&gt;
    &lt;p&gt;ESPectre uses a streamlined processing pipeline:&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CSI Data   ‚îÇ  Raw Wi-Fi Channel State Information
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇSegmentation ‚îÇ  Moving Variance Segmentation (MVS)
‚îÇ  (2-state)  ‚îÇ  IDLE ‚Üî MOTION (operates on RAW CSI)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                     ‚îÇ
       ‚ñº                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    IDLE     ‚îÇ      ‚îÇ    MOTION    ‚îÇ
‚îÇ  (no feat.) ‚îÇ      ‚îÇ  (optional   ‚îÇ
‚îÇ             ‚îÇ      ‚îÇ   features)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ   Filters   ‚îÇ  Butterworth, Wavelet,
                     ‚îÇ             ‚îÇ  Hampel, Savitzky-Golay
                     ‚îÇ             ‚îÇ  (applied to features only)
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ  Features   ‚îÇ  10 mathematical features
                     ‚îÇ (if enabled)‚îÇ  (filtered CSI data)
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                                         ‚îÇ
       ‚ñº                                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    MQTT     ‚îÇ  Publish state + metrics ‚îÇ    MQTT     ‚îÇ
‚îÇ   (IDLE)    ‚îÇ                          ‚îÇ  (MOTION)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Key Points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2-state system: IDLE or MOTION (no intermediate states)&lt;/item&gt;
      &lt;item&gt;Segmentation-based: Uses Moving Variance Segmentation (MVS) on raw CSI data&lt;/item&gt;
      &lt;item&gt;Filters applied to features only: Segmentation uses unfiltered data to preserve motion sensitivity&lt;/item&gt;
      &lt;item&gt;Optional features: Feature extraction only during MOTION state (configurable)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇESP32-S3 ‚îÇ  ‚îÇESP32-S3 ‚îÇ  ‚îÇESP32-S3 ‚îÇ
‚îÇ Room 1  ‚îÇ  ‚îÇ Room 2  ‚îÇ  ‚îÇ Room 3  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ            ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚îÇ MQTT
                  ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ    MQTT Broker     ‚îÇ
         ‚îÇ (Home Assistant    ‚îÇ
         ‚îÇ  built-in or any   ‚îÇ
         ‚îÇ  MQTT Server)      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Home Assistant ‚îÇ
          ‚îÇ   MQTT Sensors ‚îÇ
          ‚îÇ  &amp;amp; Automations ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Each sensor publishes to its own topic:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;home/espectre/kitchen&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;home/espectre/bedroom&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;home/espectre/living&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Home Assistant can then:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor each room independently&lt;/item&gt;
      &lt;item&gt;Create group sensors for whole-house occupancy&lt;/item&gt;
      &lt;item&gt;Implement zone-based automations&lt;/item&gt;
      &lt;item&gt;Track movement patterns across rooms&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Click to expand FAQ&lt;/head&gt;
    &lt;p&gt;Q: Do I need programming knowledge to use it?&lt;lb/&gt; A: Basic command line skills are needed to build and flash the firmware using ESP-IDF. Follow the step-by-step guide in SETUP.md.&lt;/p&gt;
    &lt;p&gt;Q: Does it work with my router?&lt;lb/&gt; A: Yes, if your router has 2.4GHz Wi-Fi (virtually all modern routers have it).&lt;/p&gt;
    &lt;p&gt;Q: How much does it cost in total?&lt;lb/&gt; A: Hardware: ~‚Ç¨10 for the ESP32-S3 device. Software: All free and open source. You'll also need a device to run the MQTT broker (Home Assistant or Mosquitto), which can be a Raspberry Pi (~‚Ç¨35-50) or any existing PC/NAS you already have (free).&lt;/p&gt;
    &lt;p&gt;Q: Do I need to modify anything on the router?&lt;lb/&gt; A: No! The router works normally. The sensor "listens" to Wi-Fi signals without modifying anything.&lt;/p&gt;
    &lt;p&gt;Q: Can I try it without Home Assistant?&lt;lb/&gt; A: Yes, you can use any MQTT server (e.g., Mosquitto) or even just view data via serial port.&lt;/p&gt;
    &lt;p&gt;Q: Does it work through walls?&lt;lb/&gt; A: Yes, the 2.4GHz Wi-Fi signal penetrates drywall. Reinforced concrete walls reduce sensitivity but detection remains possible at reduced distances.&lt;/p&gt;
    &lt;p&gt;Q: How many sensors are needed for a house?&lt;lb/&gt; A: It depends on size. One sensor can monitor ~50 m¬≤. For larger homes, use multiple sensors (1 sensor every 50-70 m¬≤ for optimal coverage).&lt;/p&gt;
    &lt;p&gt;Q: Can it distinguish between people and pets?&lt;lb/&gt; A: The system uses a 2-state segmentation model (IDLE/MOTION) that identifies generic movement without distinguishing between people, pets, or other moving objects. For more sophisticated classification (people vs pets, activity recognition, gesture detection), trained AI/ML models would be required (see Future Evolutions section).&lt;/p&gt;
    &lt;p&gt;Q: Does it consume a lot of Wi-Fi bandwidth?&lt;lb/&gt; A: No, MQTT traffic is minimal. With smart publishing disabled (default), the system publishes all detection updates. When smart publishing is enabled, the system only sends data on significant changes or every 5 seconds as a heartbeat, resulting in ~0.2-0.5 KB/s per sensor during idle periods and up to ~1 KB/s during active movement. Network impact is negligible.&lt;/p&gt;
    &lt;p&gt;Q: Does it work with mesh Wi-Fi networks?&lt;lb/&gt; A: Yes, it works normally. Make sure the ESP32 connects to the 2.4 GHz band.&lt;/p&gt;
    &lt;p&gt;Q: Is a dedicated server necessary?&lt;lb/&gt; A: No, Home Assistant can run on Raspberry Pi, NAS, or cloud. Alternatively, just an MQTT broker (Mosquitto) on any device is sufficient.&lt;/p&gt;
    &lt;p&gt;Q: How accurate is the detection?&lt;lb/&gt; A: Detection accuracy is highly environment-dependent and requires proper tuning. Factors affecting performance include: room layout, wall materials, furniture placement, distance from router (optimal: 3-8m), and interference levels. In optimal conditions with proper tuning, the system provides reliable movement detection. Adjust the &lt;code&gt;segmentation_threshold&lt;/code&gt; parameter to tune sensitivity for your specific environment.&lt;/p&gt;
    &lt;p&gt;Q: What's the power consumption?&lt;lb/&gt; A: ~500mW typical during continuous operation. The firmware includes support for power optimization, and deep sleep modes can be implemented for battery-powered deployments, though this would require custom modifications to the code.&lt;/p&gt;
    &lt;p&gt;Q: If it doesn't work, can I get help?&lt;lb/&gt; A: Yes, open an Issue on GitHub or contact me via email.&lt;/p&gt;
    &lt;head&gt;üîê Privacy, Security &amp;amp; Ethical Considerations (click to expand)&lt;/head&gt;
    &lt;p&gt;The system collects anonymous data related to the physical characteristics of the Wi-Fi radio channel:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amplitudes and phases of OFDM subcarriers&lt;/item&gt;
      &lt;item&gt;Statistical signal variances&lt;/item&gt;
      &lt;item&gt;NOT collected: personal identities, communication contents, images, audio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CSI data represents only the properties of the transmission medium and does not contain direct identifying information.&lt;/p&gt;
    &lt;p&gt;‚úÖ No cameras: Respect for visual privacy&lt;lb/&gt; ‚úÖ No microphones: No audio recording&lt;lb/&gt; ‚úÖ No wearables: Doesn't require wearable devices&lt;lb/&gt; ‚úÖ Aggregated data: Only statistical metrics, not raw identifying data&lt;/p&gt;
    &lt;p&gt;WARNING: Despite the intrinsic anonymity of CSI data, this system can be used for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-consensual monitoring: Detecting presence/movement of people without their explicit consent&lt;/item&gt;
      &lt;item&gt;Behavioral profiling: With advanced AI models, inferring daily life patterns&lt;/item&gt;
      &lt;item&gt;Domestic privacy violation: Tracking activities inside private homes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The user is solely responsible for using this system and must:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;‚úÖ Obtain explicit consent from all monitored persons&lt;/item&gt;
      &lt;item&gt;‚úÖ Respect local regulations (GDPR in EU, local privacy laws)&lt;/item&gt;
      &lt;item&gt;‚úÖ Clearly inform about the presence of the sensing system&lt;/item&gt;
      &lt;item&gt;‚úÖ Limit use to legitimate purposes (home security, personal home automation)&lt;/item&gt;
      &lt;item&gt;‚úÖ Protect data with encryption and controlled access&lt;/item&gt;
      &lt;item&gt;‚ùå DO NOT use for illegal surveillance, stalking, or violation of others' privacy&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Moving Variance Segmentation (MVS) analysis: baseline graphs (top) show quiet state, while bottom graphs show motion detection with turbulence signal, adaptive threshold, and state transitions&lt;/p&gt;
    &lt;head&gt;üî¨ Signal Processing Pipeline (click to expand)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native ESP32 CSI API captures Wi-Fi Channel State Information via callback&lt;/item&gt;
      &lt;item&gt;Extracts amplitude and phase data from OFDM subcarriers (up to 64 subcarriers)&lt;/item&gt;
      &lt;item&gt;Typical capture rate: ~10-100 packets/second depending on Wi-Fi traffic&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spatial turbulence calculation: Standard deviation of subcarrier amplitudes (raw CSI data)&lt;/item&gt;
      &lt;item&gt;Moving Variance Segmentation (MVS): Real-time motion segment extraction&lt;/item&gt;
      &lt;item&gt;Adaptive threshold: Based on moving variance of turbulence signal&lt;/item&gt;
      &lt;item&gt;Segment features: Duration, average turbulence, maximum turbulence&lt;/item&gt;
      &lt;item&gt;Circular buffer: Maintains up to 10 recent segments for analysis&lt;/item&gt;
      &lt;item&gt;Foundation for ML: Segments can be labeled and used for activity classification&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Segmentation operates on raw, unfiltered CSI data to preserve motion sensitivity. Filters are not applied to the turbulence signal used for segmentation.&lt;/p&gt;
    &lt;p&gt;Advanced filters applied to CSI data before feature extraction (configurable via MQTT):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Butterworth Low-Pass: Removes high-frequency noise &amp;gt;8Hz (environmental interference) - Enabled by default&lt;/item&gt;
      &lt;item&gt;Wavelet db4: Removes low-frequency persistent noise using Daubechies wavelet transform&lt;/item&gt;
      &lt;item&gt;Hampel Filter: Outlier removal using MAD (Median Absolute Deviation)&lt;/item&gt;
      &lt;item&gt;Savitzky-Golay Filter: Polynomial smoothing (enabled by default)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Filter Pipeline: Raw CSI ‚Üí Butterworth (high freq) ‚Üí Wavelet (low freq) ‚Üí Hampel ‚Üí Savitzky-Golay ‚Üí Features&lt;/p&gt;
    &lt;p&gt;Note: Filters are applied only to feature extraction, not to segmentation. Segmentation uses raw CSI data to preserve motion sensitivity.&lt;/p&gt;
    &lt;p&gt;When enabled (default: on), extracts 10 mathematical features from filtered CSI data during MOTION state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Statistical (5): Variance, Skewness, Kurtosis, Entropy, IQR&lt;/item&gt;
      &lt;item&gt;Spatial (3): Spatial variance, correlation, gradient across subcarriers&lt;/item&gt;
      &lt;item&gt;Temporal (2): Delta mean, delta variance (changes between consecutive packets)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Feature extraction can be disabled to reduce CPU usage if only basic motion detection is needed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Publishes JSON payload every 1 second (configurable)&lt;/item&gt;
      &lt;item&gt;QoS level 0 (fire-and-forget) for low latency&lt;/item&gt;
      &lt;item&gt;Retained message option for last known state&lt;/item&gt;
      &lt;item&gt;Automatic reconnection on connection loss&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MQTT Sensor subscribes to topic and creates entity&lt;/item&gt;
      &lt;item&gt;State: Primary &lt;code&gt;movement&lt;/code&gt;value (0.0-1.0)&lt;/item&gt;
      &lt;item&gt;Attributes: All other metrics available for conditions&lt;/item&gt;
      &lt;item&gt;History: Automatic logging to database for graphs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;üìä Optional Feature Extraction (click to expand)&lt;/head&gt;
    &lt;p&gt;ESPectre can optionally extract 10 mathematical features from CSI data during MOTION state:&lt;/p&gt;
    &lt;p&gt;Statistical properties of the CSI signal distribution:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Variance - Signal variability, increases significantly with movement&lt;/item&gt;
      &lt;item&gt;Skewness - Distribution asymmetry, detects irregular movement patterns&lt;/item&gt;
      &lt;item&gt;Kurtosis - Distribution "tailedness", identifies outliers and sudden changes&lt;/item&gt;
      &lt;item&gt;Entropy - Signal randomness/disorder, increases when environment changes&lt;/item&gt;
      &lt;item&gt;IQR (Interquartile Range) - Robust spread measure (Q3-Q1), resistant to outliers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Characteristics across OFDM subcarriers (frequency domain):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spatial Variance - Variability across subcarriers, indicates multipath diversity&lt;/item&gt;
      &lt;item&gt;Spatial Correlation - Correlation between adjacent subcarriers, affected by movement&lt;/item&gt;
      &lt;item&gt;Spatial Gradient - Rate of change across subcarriers, highly sensitive to movement&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Changes between consecutive CSI packets:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Temporal Delta Mean - Average absolute difference from previous packet&lt;/item&gt;
      &lt;item&gt;Temporal Delta Variance - Variance of differences from previous packet&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feature extraction is enabled by default but can be disabled to reduce CPU usage. Note: Features are only extracted during MOTION state, not during IDLE, to optimize performance.&lt;/p&gt;
    &lt;head&gt;üìã Technical Specifications (click to expand)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Board: ESP32-S3-DevKitC-1 N16R8&lt;/item&gt;
      &lt;item&gt;Flash: 16MB&lt;/item&gt;
      &lt;item&gt;PSRAM: 8MB&lt;/item&gt;
      &lt;item&gt;Wi-Fi: 802.11 a/g/n (2.4 GHz only)&lt;/item&gt;
      &lt;item&gt;Antenna: Built-in PCB antenna + IPEX connector for external&lt;/item&gt;
      &lt;item&gt;Power: USB-C 5V or 3.3V via pins&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Framework: ESP-IDF v6.1&lt;/item&gt;
      &lt;item&gt;Language: C&lt;/item&gt;
      &lt;item&gt;Build System: CMake&lt;/item&gt;
      &lt;item&gt;Flash Tool: esptool.py&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CSI Capture Rate: 10-100 packets/second&lt;/item&gt;
      &lt;item&gt;Processing Latency: &amp;lt;50ms per packet&lt;/item&gt;
      &lt;item&gt;MQTT Publish Rate: Smart publishing (only on significant changes + 5s heartbeat)&lt;/item&gt;
      &lt;item&gt;MQTT Bandwidth: ~0.2-1 KB/s depending on activity&lt;/item&gt;
      &lt;item&gt;Power Consumption: ~500mW typical&lt;/item&gt;
      &lt;item&gt;Detection Range: 3-8 meters optimal&lt;/item&gt;
      &lt;item&gt;Detection Accuracy: Environment-dependent, requires tuning&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Works only on 2.4 GHz band (ESP32-S3 hardware limitation)&lt;/item&gt;
      &lt;item&gt;Sensitivity dependent on: wall materials, antenna placement, distances, interference&lt;/item&gt;
      &lt;item&gt;Not suitable for environments with very high Wi-Fi traffic&lt;/item&gt;
      &lt;item&gt;Cannot distinguish between people, pets, or objects (generic motion detection)&lt;/item&gt;
      &lt;item&gt;Cannot count people or recognize specific activities (without ML models)&lt;/item&gt;
      &lt;item&gt;Reduced performance through metal obstacles or thick concrete walls&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;üìö Machine Learning and Deep Learning (click to expand)&lt;/head&gt;
    &lt;p&gt;The current implementation uses an advanced mathematical approach with 10 features and multi-criteria detection to identify movement patterns. While this provides excellent results without requiring ML training, scientific research has shown that Machine Learning and Deep Learning techniques can extract even richer information from CSI data for complex tasks like people counting, activity recognition, and gesture detection.&lt;/p&gt;
    &lt;p&gt;Classification or regression models can estimate the number of people present in an environment by analyzing complex patterns in CSI.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wang et al. (2017) - "Device-Free Crowd Counting Using WiFi Channel State Information" - IEEE INFOCOM&lt;/item&gt;
      &lt;item&gt;Xi et al. (2016) - "Electronic Frog Eye: Counting Crowd Using WiFi" - IEEE INFOCOM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Neural networks (CNN, LSTM, Transformer) can classify human activities like walking, falling, sitting, sleeping.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wang et al. (2015) - "Understanding and Modeling of WiFi Signal Based Human Activity Recognition" - ACM MobiCom&lt;/item&gt;
      &lt;item&gt;Yousefi et al. (2017) - "A Survey on Behavior Recognition Using WiFi Channel State Information" - IEEE Communications Magazine&lt;/item&gt;
      &lt;item&gt;Zhang et al. (2019) - "WiFi-Based Indoor Robot Positioning Using Deep Neural Networks" - IEEE Access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Deep learning algorithms can estimate position and trajectory of moving people.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wang et al. (2016) - "CSI-Based Fingerprinting for Indoor Localization: A Deep Learning Approach" - IEEE Transactions on Vehicular Technology&lt;/item&gt;
      &lt;item&gt;Chen et al. (2018) - "WiFi CSI Based Passive Human Activity Recognition Using Attention Based BLSTM" - IEEE Transactions on Mobile Computing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Models trained on CSI temporal sequences can recognize hand gestures for touchless control.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Abdelnasser et al. (2015) - "WiGest: A Ubiquitous WiFi-based Gesture Recognition System" - IEEE INFOCOM&lt;/item&gt;
      &lt;item&gt;Jiang et al. (2020) - "Towards Environment Independent Device Free Human Activity Recognition" - ACM MobiCom&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UT-HAR: Human Activity Recognition dataset (University of Texas)&lt;/item&gt;
      &lt;item&gt;Widar 3.0: Gesture recognition dataset with CSI&lt;/item&gt;
      &lt;item&gt;SignFi: Sign language recognition dataset&lt;/item&gt;
      &lt;item&gt;FallDeFi: Fall detection dataset&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;üõú Standardized Wi-Fi Sensing (IEEE 802.11bf) (click to expand)&lt;/head&gt;
    &lt;p&gt;Currently, only a limited number of Wi-Fi chipsets support CSI extraction, which restricts hardware options for Wi-Fi sensing applications. However, the IEEE 802.11bf (Wi-Fi Sensing) standard should significantly improve this situation by making CSI extraction a standardized feature.&lt;/p&gt;
    &lt;p&gt;The 802.11bf standard was officially published on September 26, 2025, introducing Wi-Fi Sensing as a native feature of the Wi-Fi protocol. Main characteristics:&lt;/p&gt;
    &lt;p&gt;üîπ Native sensing: Detection of movements, gestures, presence, and vital signs&lt;lb/&gt; üîπ Interoperability: Standardized support across different vendors&lt;lb/&gt; üîπ Optimizations: Specific protocols to reduce overhead and power consumption&lt;lb/&gt; üîπ Privacy by design: Privacy protection mechanisms integrated into the standard&lt;lb/&gt; üîπ Greater precision: Improvements in temporal and spatial granularity&lt;lb/&gt; üîπ Existing infrastructure: Works with already present Wi-Fi infrastructure&lt;/p&gt;
    &lt;p&gt;Market: The Wi-Fi Sensing market is in its early stages and is expected to experience significant growth in the coming years as the 802.11bf standard enables native sensing capabilities in consumer devices.&lt;/p&gt;
    &lt;p&gt;Hardware availability:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;Consumer routers: Currently there are no widely available consumer routers with native 802.11bf support&lt;/item&gt;
      &lt;item&gt;üè¢ Commercial/industrial: Experimental devices and integrated solutions already in use&lt;/item&gt;
      &lt;item&gt;üîß Hardware requirements: Requires multiple antennas, Wi-Fi 6/6E/7 support, and AI algorithms for signal processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Expected timeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-2026: First implementations in enterprise and premium smart home devices&lt;/item&gt;
      &lt;item&gt;2027-2028: Diffusion in high-end consumer routers&lt;/item&gt;
      &lt;item&gt;2029+: Mainstream adoption in consumer devices&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When 802.11bf is widely adopted, applications like this project will become:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More accessible: No need for specialized hardware or modified firmware&lt;/item&gt;
      &lt;item&gt;More reliable: Standardization ensures predictable behavior&lt;/item&gt;
      &lt;item&gt;More efficient: Protocols optimized for continuous sensing&lt;/item&gt;
      &lt;item&gt;More secure: Privacy mechanisms integrated at the standard level&lt;/item&gt;
      &lt;item&gt;More powerful: Ability to detect even vital signs (breathing, heartbeat)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Perspective: In the next 3-5 years, routers and consumer devices will natively support Wi-Fi Sensing, making projects like this implementable without specialized hardware or firmware modifications. This will open new possibilities for smart home, elderly care, home security, health monitoring, and advanced IoT applications.&lt;/p&gt;
    &lt;p&gt;For now: Solutions like this project based on ESP32 CSI API remain the most accessible and economical way to experiment with Wi-Fi Sensing.&lt;/p&gt;
    &lt;p&gt;This project builds upon extensive research in Wi-Fi sensing and CSI-based movement detection. The following academic works and theses provide valuable insights into mathematical signal processing approaches for human activity recognition using Wi-Fi Channel State Information:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Wi-Fi Sensing per Human Identification attraverso CSI&lt;/p&gt;&lt;lb/&gt;University thesis (in Italian) covering CSI data collection for human recognition through Wi-Fi signal analysis, with in-depth exploration of mathematical signal processing methods.&lt;lb/&gt;üìÑ Read thesis&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Channel State Information (CSI) Features Collection in Wi-Fi&lt;/p&gt;&lt;lb/&gt;Detailed analysis of CSI feature collection and processing in Wi-Fi environments, with methods for extraction and analysis suitable for mathematical processing.&lt;lb/&gt;üìÑ Read thesis&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Indoor Motion Detection Using Wi-Fi Channel State Information (2018)&lt;/p&gt;&lt;lb/&gt;Scientific article describing indoor movement detection using CSI with approaches based on signal mathematics and physics, minimizing the use of machine learning models.&lt;lb/&gt;üìÑ Read paper&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;WiFi Motion Detection: A Study into Efficacy and Performance (2019)&lt;/p&gt;&lt;lb/&gt;Study using CSI data collected from standard devices to detect movements, with analysis of signal processing methods to extract movement events without relying on ML.&lt;lb/&gt;üìÑ Read paper&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;CSI-HC: A WiFi-Based Indoor Complex Human Motion Recognition Using Channel State Information (2020)&lt;/p&gt;&lt;lb/&gt;Recognition of complex indoor movements through CSI with methods based on mathematical signal features, ideal for projects with signal-based analysis without advanced ML.&lt;lb/&gt;üìÑ Read paper&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Location Intelligence System for People Estimation in Indoor Environment During Emergency Operation (2022)&lt;/p&gt;&lt;lb/&gt;Demonstrates the use of ESP32 with wavelet filtering (Daubechies db4) for people detection in emergency scenarios. This paper directly influenced ESPectre's wavelet filter implementation, showing that wavelet denoising outperforms traditional filters on ESP32 hardware.&lt;lb/&gt;üìÑ Read paper&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These references demonstrate that effective Wi-Fi sensing can be achieved through mathematical and statistical approaches, which is the foundation of ESPectre's design philosophy.&lt;/p&gt;
    &lt;p&gt;For a detailed history of changes, new features, and improvements, see the CHANGELOG.md.&lt;/p&gt;
    &lt;p&gt;This project is released under the GNU General Public License v3.0 (GPLv3).&lt;/p&gt;
    &lt;p&gt;GPLv3 ensures that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ The software remains free and open source&lt;/item&gt;
      &lt;item&gt;‚úÖ Anyone can use, study, modify, and distribute it&lt;/item&gt;
      &lt;item&gt;‚úÖ Modifications must be shared under the same license&lt;/item&gt;
      &lt;item&gt;‚úÖ Protects end-user rights and software freedom&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See LICENSE for the full license text.&lt;/p&gt;
    &lt;p&gt;Francesco Pace&lt;lb/&gt; üìß Email: francesco.pace@gmail.com&lt;lb/&gt; üíº LinkedIn: linkedin.com/in/francescopace&lt;lb/&gt; üõú Project: ESPectre&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/francescopace/espectre"/><published>2025-11-17T14:40:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954140</id><title>Aldous Huxley predicts Adderall and champions alternative therapies</title><updated>2025-11-17T20:12:37.345827+00:00</updated><content>&lt;doc fingerprint="d71d2a7e38230707"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Aldous Huxley Predicts Adderall and Champions Alternative Therapies&lt;/head&gt;
    &lt;p&gt;If delivered today, the last of Huxley‚Äôs 7-part lecture series at MIT would probably be categorised under motivational talks or self-help strategies. It surveys the various under-explored non-pharmacological means to realise the best versions of ourselves. Or, as he calls it, actualising our desirable potentialities.&lt;/p&gt;
    &lt;p&gt;Some fairly well known means for self-actualisation that Huxley discusses are Alexander technique and Gestalt therapy. While the former is considered a pseudoscientific therapyI am not using this as a pejorative, for a change., Huxley tells us of the influential educator John Dewey‚Äôs admiration of F.M. Alexander‚Äôs work. He paraphrases Dewey‚Äôs foreword in one of Alexander‚Äôs books:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Alexander‚Äôs technique is to education what education is to life, in general. It proposes an ideal and provides means whereby that ideal can be realised.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While this is mentioned much later in his lecture, a Huxley-like figure today might need to lead their lecture with this part to convey the import and validity of such approaches. Huxley doesn‚Äôt really go into the details of why or how this is true but admittedly admires Alexander‚Äôs contribution. I have a friend who swears by it for enhancing their dance practice though I have not been able to grok what it does so far‚Äîit sounds a lot like what meditation does in terms of raising awareness.&lt;/p&gt;
    &lt;p&gt;Huxley believes that such practices are effective at psychologically breeding in desirable qualities in a person instead of: genetically breeding out undesirable ones; or pharmacologically enhancing our intellectual abilities‚Äîi.e., improved attention spans or reduced sleep‚Äîto increase our mental efficiency. Here, Huxley predicts the emergence of Adderall though I was less impressed by his forecasting euphoric pharmaceuticals. After all, this lecture was delivered several years after the publication of The Doors of Perception.&lt;/p&gt;
    &lt;p&gt;The underlying efficiency gains from these psychological approaches happen, he claims, because they train humans into being fundamentally happier; something he felt pharma-euphorics might also achieve one day. The reason such therapies are effective is that they do not provide a homogeneous training; instead, they can be adapted to individual personalities and their intrinsic differences, allowing each individual to actualise their latent potentialities via different means. This recognition that there is no single ideal version of a human is quite old; Huxley finds the most realistic (or complete) ideals in the Bhagavad Gita‚Äôs Three Yogas. The ways of devotion (Bhakti), selfless action (Karma), and contemplation (Jnana) can all lead to enlightenment, i.e., the actualisation of desirable qualities. He sees a correspondence between these yogas and the more recent Western categorisation of human beings by William Sheldon‚Äôs somatotypes‚Äîquite a problematic take when I read the traits listed in this table. While I do admire his capacity to form connections through historyWhether I see them or not is less important., I don‚Äôt see the relationship between these two beyond the fact that these are categories. They‚Äôre by no means comparable so maybe I missed the point of this comparison.&lt;/p&gt;
    &lt;p&gt;He highlights parallels between the positive outcomes of training one‚Äôs imagination via Gestalt therapy and those seen in Richard DeMille‚Äôs strategies in Children‚Äôs Imagination Games: children get more fun out of life by, for example, visualising adversarial or intimidating situations with adults in a more playful manner so that things feel less serious than they need to beThat is how I understood this section.. The examples Huxley gives here reminded me of those given to nervous interviewees and public speakers, like ‚ÄúImagine your audience is naked‚Äù, to take the edge off.&lt;/p&gt;
    &lt;p&gt;As an educator I am very sympathetic to Huxley‚Äôs grand idea in this lecture that we must develop new methods of education that adapt to personality variations; the current strategy of pigeonholing students into the identical training-and-testing modalities remains inappropriate, especially as technological advancements‚Äîwhich academia struggles to keep up with‚Äîcould enable more personalised and expressive learning. He doesn‚Äôt imagine one-to-one therapy as the scalable solution to actualisation; instead, he suggests building upon the pre-existing categories of humans into three or more groups to test out other means and potentially develop new ones based on past practices.&lt;/p&gt;
    &lt;p&gt;While I think the whole lecture is delivered eloquently, I am unsure if it has more of a thesis than that; it‚Äôs more a survey of techniques that rely on anecdotal evidence or name-dropping to convey their effectiveness.&lt;/p&gt;
    &lt;p&gt;Tomorrow‚Äôs post will unpack how he sees the role of the humanities in helping us actualise our desirable potentialities, which Huxley discussed in his lecture. It will also include my own concluding thoughts on his lecture. Maybe I will have some semblance of a thesis from it as I contemplate his words overnight.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://angadh.com/inkhaven-7"/><published>2025-11-17T14:57:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954210</id><title>WeatherNext 2: Our most advanced weather forecasting model</title><updated>2025-11-17T20:12:36.938231+00:00</updated><content>&lt;doc fingerprint="3ff8f453711c2ba8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WeatherNext 2: Our most advanced weather forecasting model&lt;/head&gt;
    &lt;p&gt;The weather affects important decisions we make everyday ‚Äî from global supply chains and flight paths to your daily commute. In recent years, artificial intelligence (AI) has dramatically enhanced what‚Äôs possible in weather forecasting and the ways in which we can use it.&lt;/p&gt;
    &lt;p&gt;Today, Google DeepMind and Google Research are introducing WeatherNext 2, our most advanced and efficient forecasting model. WeatherNext 2 can generate forecasts 8x faster and with resolution up to 1-hour. This breakthrough is enabled by a new model that can provide hundreds of possible scenarios. Using this technology, we‚Äôve supported weather agencies in making decisions based on a range of scenarios through our experimental cyclone predictions.&lt;/p&gt;
    &lt;p&gt;We're now taking our research out of the lab and putting it into the hands of users. WeatherNext 2's forecast data is now available in Earth Engine and BigQuery. We‚Äôre also launching an early access program on Google Cloud‚Äôs Vertex AI platform for custom model inference.&lt;/p&gt;
    &lt;p&gt;By incorporating WeatherNext technology, we‚Äôve now upgraded weather forecasts in Search, Gemini, Pixel Weather and Google Maps Platform‚Äôs Weather API. In the coming weeks, it will also help power weather information in Google Maps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting more possible scenarios&lt;/head&gt;
    &lt;p&gt;From a single input, we use independently trained neural networks and inject noise in function space to create coherent variability in weather forecast predictions.&lt;/p&gt;
    &lt;p&gt;Weather predictions need to capture the full range of possibilities ‚Äî including worst case scenarios, which are the most important to plan for.&lt;/p&gt;
    &lt;p&gt;WeatherNext 2 can predict hundreds of possible weather outcomes from a single starting point. Each prediction takes less than a minute on a single TPU; it would take hours on a supercomputer using physics-based models.&lt;/p&gt;
    &lt;p&gt;Our model is also highly skillful and capable of higher-resolution predictions, down to the hour. Overall, WeatherNext 2 surpasses our previous state-of-the-art WeatherNext model on 99.9% of variables (e.g. temperature, wind, humidity) and lead times (0-15 days), enabling more useful and accurate forecasts.&lt;/p&gt;
    &lt;p&gt;This improved performance is enabled by a new AI modelling approach called a Functional Generative Network (FGN), which injects ‚Äònoise‚Äô directly into the model architecture so the forecasts it generates remain physically realistic and interconnected.&lt;/p&gt;
    &lt;p&gt;This approach is particularly useful for predicting what meteorologists refer to as ‚Äúmarginals‚Äù and ‚Äújoints.‚Äù Marginals are individual, standalone weather elements: the precise temperature at a specific location, the wind speed at a certain altitude or the humidity. What's novel about our approach is that the model is only trained on these marginals. Yet, from that training, it learns to skillfully forecast 'joints' ‚Äî large, complex, interconnected systems that depend on how all those individual pieces fit together. This 'joint' forecasting is required for our most useful predictions, such as identifying entire regions affected by high heat, or expected power output across a wind farm.&lt;/p&gt;
    &lt;p&gt;Continuous Ranked Probability Score (CRPS) comparing WeatherNext 2 to WeatherNext Gen&lt;/p&gt;
    &lt;head rend="h2"&gt;From research to reality&lt;/head&gt;
    &lt;p&gt;With WeatherNext 2, we're translating cutting edge research into high-impact applications. We‚Äôre committed to advancing the state of the art of this technology and making our latest tools available to the global community.&lt;/p&gt;
    &lt;p&gt;Looking ahead, we‚Äôre actively researching capabilities to improve our models, including integrating new data sources, and expanding access even further. By providing powerful tools and open data, we hope to accelerate scientific discovery and empower a global ecosystem of researchers, developers and businesses to make decisions on today‚Äôs most complex problems and build for the future.&lt;/p&gt;
    &lt;p&gt;To learn more about geospatial platforms and AI work at Google, check out Google Earth, Earth Engine, AlphaEarth Foundations, and Earth AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learn more about WeatherNext 2&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read our paper&lt;/item&gt;
      &lt;item&gt;WeatherNext developer documentation&lt;/item&gt;
      &lt;item&gt;Explore the Earth Engine Data Catalog&lt;/item&gt;
      &lt;item&gt;Query forecast data in BigQuery&lt;/item&gt;
      &lt;item&gt;Sign up to the early access program for Cloud Vertex AI&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/google-deepmind/weathernext-2/"/><published>2025-11-17T15:04:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954472</id><title>Show HN: Bsub.io ‚Äì zero-setup batch execution for command-line tools</title><updated>2025-11-17T20:12:36.486135+00:00</updated><content>&lt;doc fingerprint="350a1b57a9271cb0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I built bsub because I was tired of wiring up Docker images, Python environments, GPUs, sandboxing, and resource limits every time I needed to run heavy command-line tools from web apps. I wanted: send files -&amp;gt; run job in the cloud -&amp;gt; get output -&amp;gt; done.&lt;/p&gt;
      &lt;p&gt;https://www.bsub.io&lt;/p&gt;
      &lt;p&gt;bsub lets you execute tools like Whisper, Typst, Pandoc, Docling, and FFmpeg as remote batch jobs with no environment setup. You can try them locally via the CLI or integrate via a simple REST API.&lt;/p&gt;
      &lt;p&gt;Example (PDF extraction):&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  bsubio submit -w pdf/extract \*.pdf
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Works like running the tool locally, but the compute and isolation happen in the cloud.&lt;/p&gt;
      &lt;p&gt;Technical details: - Each job runs in an isolated container with defined CPU/GPU/RAM limits. - Files are stored ephemerally for the duration of the job and deleted after completion. - REST API returns job status, logs, and results. - Cold start for light processors (Typst, Pandoc) is low; Whisper/FFmpeg take longer due to model load/encoding time. - Backend scales horizontally; more workers can be added during load spikes.&lt;/p&gt;
      &lt;p&gt;Current processors:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  SST/Whisper -- speech-to-text

  Typography -- Typst, Pandoc

  PDF extraction -- Docling

  Video transcoding -- FFmpeg
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; More coming; suggestions welcome for tools that are painful to set up locally.&lt;/p&gt;
      &lt;p&gt;Looking for testers! CLI is open source: https://github.com/bsubio/cli. Installers available for Linux/macOS; Windows testing is in progress. Free during early testing; pricing TBD.&lt;/p&gt;
      &lt;p&gt;If you‚Äôre on Windows, feedback is especially helpful: contact@bsub.io&lt;/p&gt;
      &lt;p&gt;If you try it, I‚Äôd appreciate feedback on API design, latency, missing processors, or anything rough around the edges.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45954472"/><published>2025-11-17T15:34:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954560</id><title>Google is killing the open web, part 2</title><updated>2025-11-17T20:12:34.156463+00:00</updated><content>&lt;doc fingerprint="3741915498261dcc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google is killing the open web, part 2&lt;/head&gt;
    &lt;p&gt;Do not comply in advance.&lt;/p&gt;
    &lt;p&gt;I wrote a few months ago about the proxy war by Google against the open web by means of XSLT. Unsurprisingly, Google has been moving forward on the deprecation, still without providing a solid justification on the reasons why other than √¢we've been leeching off a FLOSS library for which we've finally found enough security bugs to use as an excuse√¢. They do not explain why they haven't decided to fix the security issues in the library instead, or adopt a more modern library written in a safe language, taking the opportunity to upgrade XSLT support to a more recent, powerful and easier-to-use revision of the standard.&lt;/p&gt;
    &lt;p&gt;Instead, what they do is to provide a √¢polyfill√¢, a piece of JavaScript that can allegedly used to supplant the functionality. Curiously, however, they do not plan to ship such alternative in-browser, which would allow a transparent transition without even a need to talk about XSLT at all. No, they specifically refuse to do it, and instead are requesting anyone still relying on XSLT to replace the invocation of the XSLT with a non-standard invocation of the JavaScript polyfill that should replace it.&lt;/p&gt;
    &lt;p&gt;This means that at least one of these two things are true:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;the polyfill is not, in fact, sufficient to cover all the use cases previously covered by the built-in support for XSLT, and insofar as it's not, they (Google) do not intend to invest resources in maintaining it, meaning that the task is being dumped on web developers (IOW, Google is removing a feature that is going to create more work for web developers just to provide the same functionality that they used to have from the browsers);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;insofar as the polyfill is sufficient to replace the XSLT support in the browser, the policy to not ship it as a replacement confirms that the security issues in the XSLT library used in Chrome were nothing more than excuses to give the final blow to RSS and any other XML format that is still the backbone of an independent web.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As I have mentioned in the Fediverse thread I wrote before this long-form article, there's an obvious parallel here with the events that I already mentioned in my previous article: when Mozilla bent over to Google's pressure to kill off RSS by removing the √¢Live Bookmarks√¢ features from the browser, they did this on presumed technical grounds (citing as usual security and maintenance costs, but despite paid lip service to their importance for an open and interoperable web, they didn't provide any official replacement for the functionality, directing users instead to a number of add-ons that provided similar functionality, none of which are written or supported by Mozilla. Compare and contrast with their Pocket integration that they force-installed everywhere before ultimately killing the service&lt;/p&gt;
    &lt;p&gt;Actions, as they say, speak louder than words. When a company claims that a service or feature they are removing can be still accessed by other means, but do not streamline such access said alternative, and instead require their users to do the work necessary to access it, you can rest assured that beyond any word of support they may coat their actions with there is a plain and direct intent at sabotaging said feature, and you can rest assured that any of the excuses brought forward to defend the choice are nothing but lies to cover a vested interest in sabotaging the adoption of the service or feature: the intent is for you to not use that feature at all, because they have a financial interest in you not using it.&lt;/p&gt;
    &lt;p&gt;And the best defense against that is to attack, and push the use of that feature even harder.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do. Not. Comply.&lt;/head&gt;
    &lt;p&gt;This is the gist of my Fediverse thread.&lt;/p&gt;
    &lt;p&gt;Do not install the polyfill. Do not change your XML files to load it. Instead, flood their issue tracker with requests to bring back in-browser XSLT support. Report failed support for XSLT as a broken in browsers, because this is not a website issue.&lt;/p&gt;
    &lt;p&gt;I will not comply. As I have for years continued using MathML, SVG and SMIL (sometimes even all together) despite Google's intent on their deprecation, I will keep using XSLT, and in fact will look for new opportunities to rely on it. At most, I'll set up an infobox warning users reading my site about their browser's potential brokenness and inability to follow standards, just like I've done for MathML and SMIL (you can see such infoboxes in the page I linked above). And just like ultimately I was proven right (after several years, Google ended up fixing both their SMIL and their MathML support in Chrome), my expectation is that, particularly with more of us pushing through, the standards will once again prevail.&lt;/p&gt;
    &lt;p&gt;Remember: there is not technical justification for Google's choice. This is not about a lone free software developer donating their free time to the community and finding they do not have the mental or financial resources to provide a particular feature. This is a trillion-dollar ad company who has been actively destroying the open web for over a decade and finally admitting to it as a consequence of the LLM push and intentional [enshittification of web search]404mediaSearch.&lt;/p&gt;
    &lt;p&gt;The deprecation of XSLT is entirely political, fitting within the same grand scheme of the parasitic corporation killing the foundations of its own success in an effort to grasp more and more control of it. And the fact that the WebKit team at Apple and the Firefox team at Mozilla are intentioned to follow along on the same destructive path is not a counterpoint, but rather an endorsement of the analysis, as neither of those companies is interested in providing a User Agent as much as a surveillance capitalism tool that you happen to use.&lt;/p&gt;
    &lt;p&gt;(Hence why Mozilla, a company allegedly starved for resources, is wasting them implementing LLM features nobody wants instead of fixing much-voted decade-old bugs with several duplicates. Notice how the bug pertains the (mis)treatment of XML-based formats √¢like RSS.)&lt;/p&gt;
    &lt;p&gt;If you have to spend any time at all to confront the Chrome push to deprecate XSLT, your time is much better spent inventing better uses of XSLT and reporting broken rendering if/when they start disabling it, than caving to their destructive requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;The WHATWG is not a good steward of the open web&lt;/head&gt;
    &lt;p&gt;I've mentioned it before, but the WHATWG, even assuming the best of intentions at the time it was founded, is not a good steward of the open web. It is more akin to the corrupt takeover you see in regulatory capture, except that instead of taking over the W3C they just decided to get the ball and run with it, taking advantage of the fact that, as implementors, they had the final say on what counted as √¢standard√¢ (de facto if not de jure): exactly the same attitude with which Microsoft tried taking over the web through Internet Explorer at the time of the First browser war, an attitude that was rightly condemned at the time √¢even as many of those who did, have so far failed to acknowledge the problem with Google's no less detrimental approach.&lt;/p&gt;
    &lt;p&gt;The key point here is that, whatever the WHATWG was (or was intended to be) when it was founded by Opera and Mozilla developers, it is now manifestly a corporate monster. Their corporate stakeholder have a very different vision of what the Web should be compared to the vision on which the Web was founded, the vision promoted by the W3C, and the vision that underlies a truly open and independent web.&lt;/p&gt;
    &lt;p&gt;The WHATWG aim is to turn the Web into an application delivery platform, a profit-making machine for corporations where the computer (and the browser through it) are a means for them to make money off you rather than for you to gain access to services you may be interested in. Because of this, the browser in their vision is not a User Agent anymore, but a tool that sacrifices privacy, actual security and user control at the behest of the corporations √¢on the other side of the wire√¢ √¢and of their political interests (refs. for Apple, Google, and a more recent list with all of them together).&lt;/p&gt;
    &lt;p&gt;Such vision is in direct contrast with that of the Web as a repository of knowledge, a vast vault of interconnected documents whose value emerges from organic connections, personalization, variety, curation and user control. But who in the WHATWG today would defend such vision?&lt;/p&gt;
    &lt;head rend="h2"&gt;A new browser war?&lt;/head&gt;
    &lt;p&gt;Maybe what we need is a new browser war. Not one of corporation versus corporation √¢doubly more so when all currently involved parties are allied in their efforts to enclose the Web than in fostering an open and independent one√¢ but one of users versus corporations, a war to take back control of the Web and its tools.&lt;/p&gt;
    &lt;p&gt;It's kind of ironic that in a time when hosting has become almost trivial, the fight we're going to have to fight is going to be on the client side. But the biggest question is: who do we have as champions on our side?&lt;/p&gt;
    &lt;p&gt;I would have liked to see browsers like Vivaldi, the spiritual successor to my beloved classic Opera browser, amongst our ranks, but with their dependency on the Blink rendering engine, controlled by Google, they won't be able to do anything but cave, as will all other FLOSS browsers relying on Google's or Apple's engines, none of which I foresee spending any significant efforts rolling back the extensive changes that these deprecations will involve. (We see this already when it comes to JPEG√Ç XL support, but it's also true that e.g. Vivaldi has made RSS feeds first-class documents, so who knows, maybe they'll find a way for XSLT through the polyfill that was mentioned above, or something like that?)&lt;/p&gt;
    &lt;p&gt;Who else is there? There is Servo, the rendering engine that was being developed at Mozilla to replace Gecko, and that turned into an independent project when its team was fired en masse in 2020; but they don't support XSLT yet, and I don't see why they would prioritize its implementation over, say, stuff like MathML or SVG animations with SMIL (just to name two of my pet peeves), or optimizing browsing speed (seriously, try opening the home page of this site and scrolling through).&lt;/p&gt;
    &lt;p&gt;What we're left with at the moment is basically just Firefox forks, and two of these (LibreWolf and WaterFox) are basically just √¢Firefox without the most egregious privacy-invasive misfeatures√¢, which leaves the question open about what they will be willing to do when Mozilla helps Google kill XSLT, and only the other one, Pale√Ç Moon, has grown into its own independent fork (since such an old version of Firefox, in fact, that it doesn't support WebExtensions-based plugins, such as the most recent versions of crucial plugins like uBlock Origin or Privacy Badger, although it's possible to install community-supported forks of these plugins designed for legacy versions of Firefox and forks like Pale√Ç Moon).&lt;/p&gt;
    &lt;p&gt;(Yes, I am aware that there are other minor independent browser projects, like Dillo and Ladybird, but the former is in no shape of being a serious contender for general use on more sophisticated pages √¢just see it in action on this site, as always√¢ and the latter is not even in alpha phase, just in case the questionable √¢no politics√¢ policies √¢which consistently prove to be weasel words for √¢we're right-wingers but too chicken to come out as such√¢√¢ weren't enough to stay away from it.)&lt;/p&gt;
    &lt;p&gt;Periodically, I go through them (the Firefox forks, that is) to check if they are good enough for me to become my daily drivers. Just for you (not really: just for me, actually), I just tested them again. They're not ready yet, at least not for me, although I must say that I'm seeing clear improvements since my last foray into the matter, that wasn't even that long ago. In some cases, I can attest that they are even better than Firefox: for example, Pale√Ç Moon and WaterFox have good JPEG√Ç XL support (including transparency and animation support, which break in LibreWolf as they do in the latest nightly version of Firefox I tried), and Pale√Ç Moon still has first-class support for RSS, from address bar indicator to rendering even in the absence of a stylesheet (be it CSS or XSLT).&lt;/p&gt;
    &lt;p&gt;(A suggestion? Look into more microformats support. An auxiliary bar with previous/&lt;/p&gt;
    &lt;p&gt;An interesting difference is that the user interface of these browsers is perceivably less refined than Firefox'. It's a bit surprising, given the common roots, but it emerges in several more and less apparent details, from the spacing between menu items to overlapping text and icons in context menus, passing through incomplete support for dark themes and other little details that all add up, giving these otherwise quite valid browsers and amateurish feeling.&lt;/p&gt;
    &lt;p&gt;And I get it: UI design is hard, and I myself suck at it, so I'm the last person that should be giving recommendations, but I'm still able to differentiate between more curated interfaces and ones that need some work; and if even someone like me who distinctly prefers function over form finds these little details annoying, I can imagine how much worse this may feel to users who care less about the former and more about the latter. Sadly, if a new browser war is to be fought to wrestle control from the corporate-controlled WHATWG, this matters.&lt;/p&gt;
    &lt;p&gt;In the end, I find myself in a √¢waiting√¢ position. How long will it take for Firefox to kill their XSLT support? What will its closest forks (WaterFox in particular is the one I'm eyeing) be able to do about it? Or will Pale√Ç Moon remain the only modern broser with support for it, as a hard fork that has since long gone its own way? Will they have matured enough to become my primary browsers? We'll see in time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another web?&lt;/head&gt;
    &lt;p&gt;There's more to the Internet than the World Wide Web built around the HTTP protocol and the HTML file format. There used to be a lot of the Internet beyond the Web, and while much of it still remains as little more than a shadow of the past, largely eclipsed by the Web and what has been built on top of it (not all of it good) outside of some modest revivals, there's also new parts of it that have tried to learn from the past, and build towards something different.&lt;/p&gt;
    &lt;p&gt;This is the case for example of the so-called √¢Gemini Space√¢, a small corner of the Internet that has nothing to do with the LLM Google is trying to shove down everyone's throat, and in fact not only predates it, as I've mentioned already, but is intentionally built around dfferent technology to stay away from the influence of Google and the like.&lt;/p&gt;
    &lt;p&gt;The Gemini protocol is designed to be conceptually simpler than HTTP, while providing modern features like built-in transport-level security and certificate-based client-side authentication, and its own √¢native√¢ document format, the so-called gemtext.&lt;/p&gt;
    &lt;p&gt;As I said in my aforementioned Fediverse thread:&lt;/p&gt;
    &lt;p&gt;There's something to be said about not wanting to share your environment with the poison that a large part of the web has become, but at the same time, there's also something to be said about throwing away the baby with the bathwater. The problem with the web isn't technical, it's social. The tech itself is fine.&lt;/p&gt;
    &lt;p&gt;I'm not going to write up an extensive criticism of the Gemini Space: you can find here an older opinion by the author of &lt;code&gt;curl&lt;/code&gt;,
(although it should be kept in mind that things have changed quite a bit since:
for example, the specification of the different components has been separated,
as suggested by Daniel),
and some criticism about how gemtext is used.&lt;/p&gt;
    &lt;p&gt;I'm not going to sing the praises of the Gemini protocol or gemtext either, even though I do like the idea of a web built on lightweight markup formats: I would love it if browsers had native support for formats like Markdown or AsciiDoc (and gemtext, for the matter): it's why I keep the AsciiDoctor Browser Extension installed.&lt;/p&gt;
    &lt;p&gt;But more in general, the Web (or at least its user agents) should not differentiate. It should not differentiate by protocol, and it should not differentiate by format. We've seen it with image formats like MNG being unfairly excluded, with [motivations based on alleged code bloat][nomng] that today are manifest in their idiocy (and yes, it hasn't escaped my that even Pale√Ç Moon doesn't support the format), and we're seeing it today with JPEG√Ç XL threatened with a similar fate, without even gracing us with a ridiculous excuse. On the upside, we have browsers shipping with a full-fledged PDF reader, which is a good step towards the integration of this format with the greater Web.&lt;/p&gt;
    &lt;p&gt;In an ideal world, browsers would have not deprecated older protocols like Gopher or FTP, and would just add support for new ones like Gemini, as they would have introduced support for new (open) document formats as they came along.&lt;/p&gt;
    &lt;p&gt;(Why insist on the open part? In another Fediverse thread about the XSLT deprecation I had an interesting discussion with the OP about SWF, the interactive multimedia format for the Web at the turn of the century. The Adobe Flash Player ultimately fell out of favour, arguably due to the advent of mobile Internet: it has been argued that the iPhone killed Flash, and while there's some well-deserved criticism of hypocrisy levelled against Steve Jobs infamous Thoughts on Flash letter, it is true that what ultimatelly truly killed the format was it being proprietary and not fully documented. And while we might not want to cry about the death of a proprietary format, it remains true even today that the loss of even just legacy suport for it has been a significant loss to culture and art, as argued by @whiteshark√¢@mastodon.social.)&lt;/p&gt;
    &lt;head rend="h2"&gt;A Web of interconnected software?&lt;/head&gt;
    &lt;p&gt;It shouldn't be up to the User Agent to determine which formats the user is able to access, and through which protocol. (If I had any artistic prowess (and willpower), I'd hack the √¢myth of consensual X√¢ meme representing the user and the server saying √¢I consent√¢, and the browser saying √¢I don't√¢.) I do appreciate that there is a non-trivial maintenance cost that grows with the number of formats and protocols, but we know from classic Opera that it is indeed quite possible to ship a full Internet suite in a browser packaging.&lt;/p&gt;
    &lt;p&gt;In the old days, browser developers were well-aware that a single vendor couldn't √¢cover all bases√¢, which is how interfaces like the once ubiquituous NPAPI were born. The plug-in interface has been since removed from most browsers, an initiative again promoted by Google, announced in 2013 and completed in 2015 (I should really add this to my previous post on Google killing the open web, but I also really don't feel like touching that anymore; here will have to suffice), with the other major browsers quickly following suit, and its support is now relegated only to independent browsers like Pale√Ç Moon.&lt;/p&gt;
    &lt;p&gt;And even if it can be argued that the NPAPI specifically was indeed mired with unfixable security and portability issues and it had to go, its removal without a clear cross-browser upgrade path has been a significant loss for the evolution of the web, destroying the primary √¢escape hatch√¢ to solve the chicken-and-egg problem of client-side format support versus server-side format adoption. By the way, it was also responsible for the biggest W3C blunder, the standardization of DRM for the web through the so-called Encrypted Media Extensions, a betrayal of the W3C own mission statement.&lt;/p&gt;
    &lt;head rend="h3"&gt;The role of multimedia streaming in the death of the User Agent&lt;/head&gt;
    &lt;p&gt;The timeline here is quite interesting, and correlates with the already briefly mentioned history of Flash, and its short-lived Microsoft Silverlight competitor, that were largely responsible for the early expansive growth of multimedia streaming services in the early years of the XXI century: with the tension between Apple's effort to kill Flash and the need of emerging streaming services like Netflix' and Hulu's to support in-browser multimedia streaming, there was a need to improve support for multimedia formats in the nascent HTML5 specification, but also a requirement from the MAFIAA partners that such a support would allow enforcing the necessary restrictions that would, among other things, prevent users from saving a local copy of the stream, something that could be more easily enforced within the Flash players the industries had control over than in a User Agent controlled by the user.&lt;/p&gt;
    &lt;p&gt;This is where the development of EME came in in 2013: this finally allowed a relatively quick phasing out of the Flash plugin, and a posteriori of the plugin interface that allowed its integration with the browsers: by that time, the Flash plugin was by and large the plugin the API existed for, and the plugin itself was indeed still supported by the browsers for some time after support for the API was otherwise discontinued (sometimes through alternative interfaces such as the PPAPI, other times by keeping the NPAPI support around, but only enabled for the Flash plugin).&lt;/p&gt;
    &lt;p&gt;There are several interesting consideration that emerge from this little glimpse at the history of Flash and the EME.&lt;/p&gt;
    &lt;p&gt;First of all, this is one more piece of history that goes to show how pivotal the year 2013 was for the enshittification of the World Wide Web, as discussed already.&lt;/p&gt;
    &lt;p&gt;Secondly, it shows how the developers of major browsers are more than willing to provide a smooth transition path with no user intervention, at least when catering to the interests of major industries. This indicates that when they don't, it's not because they can't: it's because they have a vested interest in not doing it. Major browser development is now (and has been for over a decade at least) beholden not to the needs and wants of their own users, but to those of other industries. But I repeat myself.&lt;/p&gt;
    &lt;p&gt;And thirdly, it's an excellent example, for the good and the bad, of how the plugin interface has helped drive the evolution of the web, as I was saying.&lt;/p&gt;
    &lt;head rend="h3"&gt;Controlled evolution&lt;/head&gt;
    &lt;p&gt;The removal of NPAPI support, followed a few years later by the removal of the (largely Chrome-specific) PPAPI interface (that was supposed to be the √¢safer, more portable√¢ evolution of NPAPI), without providing any alternative, is a very strong indication of the path that browser development has taken in the last √¢decade plus√¢: a path where the Web is entirely controlled by what Google, Apple and Microsoft (hey look, it's GAFAM all over again!) decide about what is allowed on it, and what is not allowed to not be on it (to wit, ads and other user tracking implements).&lt;/p&gt;
    &lt;p&gt;In this perspective, the transition from plugins to browser extensions cannot be viewed (just) as a matter of security and portability, but √¢more importantly, in fact√¢ as a matter of crippled functionality: indeed, extensions maintain enough capabilities to be a vector of malware and adware, but not enough to circumvent unwanted browser behavior, doubly more so with the so-called Extension Manifest V3 specifically designed to thwart ad blocking as I've already mentioned in the previous post of the series.&lt;/p&gt;
    &lt;p&gt;With plugins, anything could be integrated in the World Wide Web, and such integration would be close to as efficient as could be. Without plugins, such integration, when possible at all, becomes clumsier and more expensive.&lt;/p&gt;
    &lt;p&gt;As an example, there are browser extensions that can introduce support for JPEG√Ç XL to browsers that don't have native support. This provides a workaround to display such images in said browsers, but when a picture with multiple formats is offered (which is what I do e.g. to provide a PNG fallback for the JXL images I provide), this results in both the PNG and JXL formats being downloaded, increasing the amount of data transferred instead of decreasing it (one of the many benefits of JXL over PNG). By contrast, a plugin could register itself a handler for the JPEG√Ç XL format, and the browser would then be able to delegate rendering of the image to the plugin, only falling back to the PNG in case of failure, thus maximizing the usefulness of the format pending a built-in implementation.&lt;/p&gt;
    &lt;p&gt;The poster child of this lack of efficiency is arguably MathJax, that has been carrying for nearly two decades the burden of bringing math to the web while browser implementors slacked off on their MathML support. And while MathJax does offer more than just MathML support for browers without native implementations, there is little doubt that it would be more effective in delivering the services it delivers if it could be a plugin rather than a multi-megabyte (any efforts to minimize its size notwithstanding) JavaScript library each math-oriented website needs to load.&lt;/p&gt;
    &lt;p&gt;(In fact, it is somewhat surprising that there isn't a browser extesion version of MathJax that I can find other than a GreaseMonkey user script with convoluted usage requirements, but I guess this is the cost we have to pay for the library flexibility, and the sandboxing requirements enforced on JavaScript in modern browsers.)&lt;/p&gt;
    &lt;p&gt;Since apparently √¢defensive writing√¢ is a thing we need when jotting down an article such as this (as if it even matters, given how little attention people give to what they read √¢if they read it at all√¢ before commenting), I should clarify that I'm not necessarily for a return to NPAPI advocating. We have decades of experience about what could be considered the actual technical issues with that interface, and how they can be improved upon (which is for example what PPAPI allegedly did, before Google decided it would be better off to kill plugins entirely and thus gain full control of the Web as a platform), as we do about sandboxing external code running in browsers (largely through the efforts to sandbox JavaScript). A better plugin API could be designed.&lt;/p&gt;
    &lt;p&gt;It's not going to happen. It is now apparent that the major browsers explicitly and intentionally do not want to allow the kind of flexibility that plugins would allow, hiding their controlling efforts behind security excuses. It would thus be up to the minority browsers to come up with such an interface (or actually multiple ones, at least one for protocols and one for document types), but with most of them beholden to the rendering engines controlled by Google (for the most part), Apple (some, still using WebKit over Blink), and Mozilla (the few Firefox forks), they are left with very little leeway, if any at all, in terms of what they can support.&lt;/p&gt;
    &lt;p&gt;But even if, by some miraculous convergence, they did manage to agree on and implement support for such an API, would there actually be an interest by third party to develop plugins for it? I can envision this as a way for browsers to share coding efforts in supporting new protocols and formats before integrating them as first-class (for example, the already mentioned Gemini protocol and gemtext format could be implemented first as a plugin to the benefit of any browsers supporting such hypothetical interfaces) but there be any interest in developing for it, rather tha just trying to get the feature implemented in the browsers themselves?&lt;/p&gt;
    &lt;head rend="h3"&gt;A mesh of building blocks&lt;/head&gt;
    &lt;p&gt;Still, let me dream a bit of something like this, a browser made up of composable components, protocol handlers separate from primary document renderers separate from attachment handlers.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;A new protocol comes out?&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Implement a plugin to handle that, and you can test it by delivering the same content over it, and see it rendered just the same from the other components in the chain.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;A new document format comes out?&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Implement a plugin to handle that, and it will be used to render documents in the new format.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;A new image format comes out?&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Implement a plugin to handle that, and any image in the new format will be visible.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;A new scripting language comes out?&lt;/item&gt;
      &lt;item rend="dd-4"&gt;You guessed it: implement a plugin to handle that√Ç √¢¬¶&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How much tech would have had a real chance at proving itself in the field if this had been the case, or would have survived being ousted not by technical limitations, but by unfriendly corporate control? Who knows, maybe RSS and Atom integration would still be trivially at everybody's hand; nobody would have had to fight with the long-standing bugs in PNG rendering from Internet Explorer, MNG would have flourished, JPEG√Ç XL would have become ubiquituous six months after the specification had been finalized; we would have seen HTML+SMIL provide declarative interactive documents without JavaScript as far back as 2008; XSLT 2 and 3 would have long superseded XSLT 1 as the templating languages for the web, or XSLT would have been supplanted by the considerably more accessible XQuery; XHTML2 would have lived and grown alongside HTML5, offering more sensible markup for many common features, and much-wanted capabilities such as client-side includes.&lt;/p&gt;
    &lt;p&gt;The web would have been very different from what it is today, and most importantly we would never would have had to worry about a single corporation getting to dictate what is and what isn't allowed on the Web.&lt;/p&gt;
    &lt;p&gt;But the reality is much harsher and darker. Google has control, and we do need to wrestle it out of their hands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resist&lt;/head&gt;
    &lt;p&gt;So, do not comply.&lt;lb/&gt; Resist.&lt;lb/&gt; Force the unwanted tech through.&lt;lb/&gt; Use RSS.&lt;lb/&gt; Use XSLT.&lt;lb/&gt; Adopt JPEG√Ç XL as your primary image format.&lt;lb/&gt; And report newly broken sites for what they are:&lt;lb/&gt; a browser fault, not a content issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Post scriptum&lt;/head&gt;
    &lt;p&gt;I would like to add here any pi√É¬®ces de r√É¬©sistance for XSLT.&lt;/p&gt;
    &lt;p&gt;I'm going to inaugurate with a link I've just discovered thanks to JWZ:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;xslt.rip (best viewed with a browser that supports XSLT; viewing the source is highly recommended);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;and last but not least (yeah I know, doesn't make much sense with the current short list, but still), a shameless plug of my own website, of course, because of the idea to use XSLT not to produce HTML, but to produce SVG.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Made the news (and other related articles)&lt;/head&gt;
    &lt;p&gt;I've apparently √¢made the news√¢ (again).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hacker News has a a thread;&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I have read the comments, and little has changed since last time. The only comment worth responding is from the user that prefers EME to the mess of plugins we used to have. I understand where they are coming from, but I disagree on a matter of principle: DRM shouldn't exist, and it should never have been standardized in violation of the W3C mission statement; as a plus, and the more cumbersome it is for the user, the better it represents its negative nature.&lt;/p&gt;
    &lt;p&gt;Aside from that, the comments keep missing the point, and weight their personal dislike for XSLT more than the important role it plays on the open and indie web. Just seeing how many people follow the corporate recommendation to apply it server-side and distribute the √¢rendered√¢ HTML shows these are people who have no idea what they're talking about: just as an example, my sparklines are still 10√É smaller as XML data plus XSLT than as the rendered SVG, with benefits amortizing over multiple sparklines per page (due to common XSLT), and over time (data changes, XSLT does not; also, the actual ratio of SVG to XML data is much higher that 10√É, and as data increases, the ratio tends to that). For smaller, cheaper and/or home hosting, XSLT remains a clear winner.&lt;/p&gt;
    &lt;p&gt;You don't like the syntax? Fine. But use that energy to pressure for XQuery to be available instead of towing the corporate line about the demise of XSLT.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wok.oblomov.eu/tecnologia/google-killing-open-web-2/"/><published>2025-11-17T15:41:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954638</id><title>How to escape the Linux networking stack</title><updated>2025-11-17T20:12:33.704542+00:00</updated><content>&lt;doc fingerprint="a391685bd82399c5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;There is a theory which states that if ever anyone discovers exactly what the Linux networking stack does and why it does it, it will instantly disappear and be replaced by something even more bizarre and inexplicable.&lt;/p&gt;
      &lt;p&gt;There is another theory which states that Git was created to track how many times this has already happened.&lt;/p&gt;
      &lt;p&gt;Many products at Cloudflare aren√¢t possible without pushing the limits of network hardware and software to deliver improved performance, increased efficiency, or novel capabilities such as soft-unicast, our method for sharing IP subnets across data centers. Happily, most people do not need to know the intricacies of how your operating system handles network and Internet access in general. Yes, even most people within Cloudflare.&lt;/p&gt;
      &lt;p&gt;But sometimes we try to push well beyond the design intentions of Linux√¢s networking stack. This is a story about one of those attempts.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Hard solutions for soft problems&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;My previous blog post about the Linux networking stack teased a problem matching the ideal model of soft-unicast with the basic reality of IP packet forwarding rules. Soft-unicast is the name given to our method of sharing IP addresses between machines. You may learn about all the cool things we do with it, but as far as a single machine is concerned, it has dozens to hundreds of combinations of IP address and source-port range, any of which may be chosen for use by outgoing connections.&lt;/p&gt;
      &lt;p&gt;The SNAT target in iptables supports a source-port range option to restrict the ports selected during NAT. In theory, we could continue to use iptables for this purpose, and to support multiple IP/port combinations we could use separate packet marks or multiple TUN devices. In actual deployment we would have to overcome challenges such as managing large numbers of iptables rules and possibly network devices, interference with other uses of packet marks, and deployment and reallocation of existing IP ranges.&lt;/p&gt;
      &lt;p&gt;Rather than increase the workload on our firewall, we wrote a single-purpose service dedicated to egressing IP packets on soft-unicast address space. For reasons lost in the mists of time, we named it SLATFATF, or √¢fish√¢ for short. This service√¢s sole responsibility is to proxy IP packets using soft-unicast address space and manage the lease of those addresses.&lt;/p&gt;
      &lt;p&gt;WARP is not the only user of soft-unicast IP space in our network. Many Cloudflare products and services make use of the soft-unicast capability, and many of them use it in scenarios where we create a TCP socket in order to proxy or carry HTTP connections and other TCP-based protocols. Fish therefore needs to lease addresses that are not used by open sockets, and ensure that sockets cannot be opened to addresses leased by fish.&lt;/p&gt;
      &lt;p&gt;Our first attempt was to use distinct per-client addresses in fish and continue to let Netfilter/conntrack apply SNAT rules. However, we discovered an unfortunate interaction between Linux√¢s socket subsystem and the Netfilter conntrack module that reveals itself starkly when you use packet rewriting.&lt;/p&gt;
      &lt;p&gt;Suppose we have a soft-unicast address slice, 198.51.100.10:9000-9009. Then, suppose we have two separate processes that want to bind a TCP socket at 198.51.100.10:9000 and connect it to 203.0.113.1:443. The first process can do this successfully, but the second process will receive an error when it attempts to connect, because there is already a socket matching the requested 5-tuple.&lt;/p&gt;
      &lt;p&gt;Instead of creating sockets, what happens when we emit packets on a TUN device with the same destination IP but a unique source IP, and use source NAT to rewrite those packets to an address in this range?&lt;/p&gt;
      &lt;p&gt;If we add an nftables √¢snat√¢ rule that rewrites the source address to 198.51.100.10:9000-9009, Netfilter will create an entry in the conntrack table for each new connection seen on fishtun, mapping the new source address to the original one. If we try to forward more connections on that TUN device to the same destination IP, new source ports will be selected in the requested range, until all ten available ports have been allocated; once this happens, new connections will be dropped until an existing connection expires, freeing an entry in the conntrack table.&lt;/p&gt;
      &lt;p&gt;Unlike when binding a socket, Netfilter will simply pick the first free space in the conntrack table. However, if you use up all the possible entries in the table you will get an EPERM error when writing an IP packet. Either way, whether you bind kernel sockets or you rewrite packets with conntrack, errors will indicate when there isn√¢t a free entry matching your requirements.&lt;/p&gt;
      &lt;p&gt;Now suppose that you combine the two approaches: a first process emits an IP packet on the TUN device that is rewritten to a packet on our soft-unicast port range. Then, a second process binds and connects a TCP socket with the same addresses as that IP packet:&lt;/p&gt;
      &lt;p&gt;The first problem is that there is no way for the second process to know that there is an active connection from 198.51.100.10:9000 to 203.0.113.1:443, at the time the &lt;code&gt;connect() &lt;/code&gt;call is made. The second problem is that the connection is successful from the point of view of that second process.&lt;/p&gt;
      &lt;p&gt;It should not be possible for two connections to share the same 5-tuple. Indeed, they don√¢t. Instead, the source address of the TCP socket is silently rewritten to the next free port.&lt;/p&gt;
      &lt;p&gt;This behaviour is present even if you use conntrack without either SNAT or MASQUERADE rules. It usually happens that the lifetime of conntrack entries matches the lifetime of the sockets they√¢re related to, but this is not guaranteed, and you cannot depend on the source address of your socket matching the source address of the generated IP packets.&lt;/p&gt;
      &lt;p&gt;Crucially for soft-unicast, it means conntrack may rewrite our connection to have a source port outside of the port slice assigned to our machine. This will silently break the connection, causing unnecessary delays and false reports of connection timeouts. We need another solution.&lt;/p&gt;
      &lt;p&gt;For WARP, the solution we chose was to stop rewriting and forwarding IP packets, instead to terminate all TCP connections within the server and proxy them to a locally-created TCP socket with the correct soft-unicast address. This was an easy and viable solution that we already employed for a portion of our connections, such as those directed at the CDN, or intercepted as part of the Zero Trust Secure Web Gateway. However, it does introduce additional resource usage and potentially increased latency compared to the status quo. We wanted to find another way (to) forward.&lt;/p&gt;
      &lt;p&gt;If you want to use both packet rewriting and bound sockets, you need to decide on a single source of truth. Netfilter is not aware of the socket subsystem, but most of the code that uses sockets and is also aware of soft-unicast is code that Cloudflare wrote and controls. A slightly younger version of myself therefore thought it made sense to change our code to work correctly in the face of Netfilter√¢s design.&lt;/p&gt;
      &lt;p&gt;Our first attempt was to use the Netlink interface to the conntrack module, to inspect and manipulate the connection tracking tables before sockets were created. Netlink is an extensible interface to various Linux subsystems and is used by many command-line tools like ip and, in our case, conntrack-tools. By creating the conntrack entry for the socket we are about to bind, we can guarantee that conntrack won√¢t rewrite the connection to an invalid port number, and ensure success every time. Likewise, if creating the entry fails, then we can try another valid address. This approach works regardless of whether we are binding a socket or forwarding IP packets.&lt;/p&gt;
      &lt;p&gt;There is one problem with this √¢ it√¢s not terribly efficient. Netlink is slow compared to the bind/connect socket dance, and when creating conntrack entries you have to specify a timeout for the flow and delete the entry if your connection attempt fails, to ensure that the connection table doesn√¢t fill up too quickly for a given 5-tuple. In other words, you have to manually reimplement tcp_tw_reuse option to support high-traffic destinations with limited resources. In addition, a stray RST packet can erase your connection tracking entry. At our scale, anything like this that can happen, will happen. It is not a place for fragile solutions.&lt;/p&gt;
      &lt;p&gt;Instead of creating conntrack entries, we can abuse kernel features for our own benefit. Some time ago Linux added the TCP_REPAIR socket option, ostensibly to support connection migration between servers e.g. to relocate a VM. The scope of this feature allows you to create a new TCP socket and specify its entire connection state by hand.&lt;/p&gt;
      &lt;p&gt;An alternative use of this is to create a √¢connected√¢ socket that never performed the TCP three-way handshake needed to establish that connection. At least, the kernel didn√¢t do that √¢ if you are forwarding the IP packet containing a TCP SYN, you have more certainty about the expected state of the world.&lt;/p&gt;
      &lt;p&gt;However, the introduction of TCP Fast Open provides an even simpler way to do this: you can create a √¢connected√¢ socket that doesn√¢t perform the traditional three-way handshake, on the assumption that the SYN packet √¢ when sent with its initial payload √¢ contains a valid cookie to immediately establish the connection. However, as nothing is sent until you write to the socket, this serves our needs perfectly.&lt;/p&gt;
      &lt;p&gt;You can try this yourself:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;TCP_FASTOPEN_CONNECT = 30
TCP_FASTOPEN_NO_COOKIE = 34
s = socket(AF_INET, SOCK_STREAM)
s.setsockopt(SOL_TCP, TCP_FASTOPEN_CONNECT, 1)
s.setsockopt(SOL_TCP, TCP_FASTOPEN_NO_COOKIE, 1)
s.bind(('198.51.100.10', 9000))
s.connect(('1.1.1.1', 53))&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Binding a √¢connected√¢ socket that nevertheless corresponds to no actual socket has one important feature: if other processes attempt to bind to the same addresses as the socket, they will fail to do so. This satisfies the problem we had at the beginning to make packet forwarding coexist with socket usage.&lt;/p&gt;
      &lt;p&gt;While this solves one problem, it creates another. By default, you can√¢t use an IP address for both locally-originated packets and forwarded packets.&lt;/p&gt;
      &lt;p&gt;For example, we assign the IP address 198.51.100.10 to a TUN device. This allows any program to create a TCP socket using the address 198.51.100.10:9000. We can also write packets to that TUN device with the address 198.51.100.10:9001, and Linux can be configured to forward those packets to a gateway, following the same route as the TCP socket. So far, so good.&lt;/p&gt;
      &lt;p&gt;On the inbound path, TCP packets addressed to 198.51.100.10:9000 will be accepted and data put into the TCP socket. TCP packets addressed to 198.51.100.10:9001, however, will be dropped. They are not forwarded to the TUN device at all.&lt;/p&gt;
      &lt;p&gt;Why is this the case? Local routing is special. If packets are received to a local address, they are treated as √¢input√¢ and not forwarded, regardless of any routing you think should apply. Behold the default routing rules:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;cbranch@linux:~$ ip rule
cbranch@linux:~$ ip rule
0:√Ç¬† √Ç¬† √Ç¬† √Ç¬† from all lookup local
32766:√Ç¬† √Ç¬† from all lookup main
32767:√Ç¬† √Ç¬† from all lookup default&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;The rule priority is a nonnegative integer, the smallest priority value is evaluated first. This requires some slightly awkward rule manipulation to √¢insert√¢ a lookup rule at the beginning that redirects marked packets to the packet forwarding service√¢s TUN device; you have to delete the existing rule, then create new rules in the right order. However, you don√¢t want to leave the routing rules without any route to the √¢local√¢ table, in case you lose a packet while manipulating these rules. In the end, the result looks something like this:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;ip rule add fwmark 42 table 100 priority 10
ip rule add lookup local priority 11
ip rule del priority 0
ip route add 0.0.0.0/0 proto static dev fishtun table 100&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;As with WARP, we simplify connection management by assigning a mark to packets coming from the √¢fishtun√¢ interface, which we can use to route them back there. To prevent locally-originated TCP sockets from having this same mark applied, we assign the IP to the loopback interface instead of fishtun, leaving fishtun with no assigned address. But it doesn√¢t need one, as we have explicit routing rules now.&lt;/p&gt;
      &lt;p&gt;While testing this last fix, I ran into an unfortunate problem. It did not work in our production environment.&lt;/p&gt;
      &lt;p&gt;It is not simple to debug the path of a packet through Linux√¢s networking stack. There are a few tools you can use, such as setting nftrace in nftables or applying the LOG/TRACE targets in iptables, which help you understand which rules and tables are applied for a given packet.&lt;/p&gt;
      &lt;p&gt;Schematic for the packet flow paths through Linux networking and *tables by Jan Engelhardt&lt;/p&gt;
      &lt;p&gt;Our expectation is that the packet will pass the prerouting hook, a routing decision is made to send the packet to our TUN device, then the packet will traverse the forward table. By tracing packets originating from the IP of a test host, we could see the packets enter the prerouting phase, but disappear after the √¢routing decision√¢ block.&lt;/p&gt;
      &lt;p&gt;While there is a block in the diagram for √¢socket lookup√¢, this occurs after processing the input table. Our packet doesn√¢t ever enter the input table; the only change we made was to create a local socket. If we stop creating the socket, the packet passes to the forward table as before.&lt;/p&gt;
      &lt;p&gt;It turns out that part of the √¢routing decision√¢ involves some protocol-specific processing. For IP packets, routing decisions can be cached, and some basic address validation is performed. In 2012, an additional feature was added: early demux. The rationale being, at this point in packet processing we are already looking up something, and the majority of packets received are expected to be for local sockets, rather than an unknown packet or one that needs to be forwarded somewhere. In this case, why not look up the socket directly here and save yourself an extra route lookup?&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The workaround at the end of the universe&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Unfortunately for us, we just created a socket and didn√¢t want it to receive packets. Our adjustment to the routing table is ignored, because that routing lookup is skipped entirely when the socket is found. Raw sockets avoid this by receiving all packets regardless of the routing decision, but the packet rate is too high for this to be efficient. The only way around this is disabling the early demux feature. According to the patch√¢s claims, though, this feature improves performance: how far will performance regress on our existing workloads if we disable it?&lt;/p&gt;
      &lt;p&gt;This calls for a simple experiment: set the net.ipv4.tcp_early_demux syscall to 0 on some machines in a datacenter, let it run for a while, then compare the CPU usage with machines using default settings and the same hardware configuration as the machines under test.&lt;/p&gt;
      &lt;p&gt;The key metrics are CPU usage from /proc/stat. If there is a performance degradation, we would expect to see higher CPU usage allocated to √¢softirq√¢ √¢ the context in which Linux network processing occurs √¢ with little change to either userspace (top) or kernel time (bottom). The observed difference is slight, and mostly appears to reduce efficiency during off-peak hours.&lt;/p&gt;
      &lt;p&gt;While we tested different solutions to IP packet forwarding, we continued to terminate TCP connections on our network. Despite our initial concerns, the performance impact was small, and the benefits of increased visibility into origin reachability, fast internal routing within our network, and simpler observability of soft-unicast address usage flipped the burden of proof: was it worth trying to implement pure IP forwarding and supporting two different layers of egress?&lt;/p&gt;
      &lt;p&gt;So far, the answer is no. Fish runs on our network today, but with the much smaller responsibility of handling ICMP packets. However, when we decide to tunnel all IP packets, we know exactly how to do it.&lt;/p&gt;
      &lt;p&gt;A typical engineering role at Cloudflare involves solving many strange and difficult problems at scale. If you are the kind of goal-focused engineer willing to try novel approaches and explore the capabilities of the Linux kernel despite minimal documentation, look at our open positions √¢ we would love to hear from you!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/so-long-and-thanks-for-all-the-fish-how-to-escape-the-linux-networking-stack/"/><published>2025-11-17T15:49:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954640</id><title>Project Gemini</title><updated>2025-11-17T20:12:33.248255+00:00</updated><content>&lt;doc fingerprint="7b91a15fbb3f9295"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Gemini&lt;/head&gt;
    &lt;head rend="h2"&gt;Gemini in 100 words&lt;/head&gt;
    &lt;p&gt;Gemini is a new internet technology supporting an electronic library of interconnected text documents. That's not a new idea, but it's not old fashioned either. It's timeless, and deserves tools which treat it as a first class concept, not a vestigial corner case. Gemini isn't about innovation or disruption, it's about providing some respite for those who feel the internet has been disrupted enough already. We're not out to change the world or destroy other technologies. We are out to build a lightweight online space where documents are just documents, in the interests of every reader's privacy, attention and bandwidth. &lt;/p&gt;
    &lt;p&gt; If you'd like to know more, read our FAQ&lt;/p&gt;
    &lt;p&gt; Or, if you'd prefer, here's a video overview &lt;/p&gt;
    &lt;head rend="h2"&gt;Official resources&lt;/head&gt;
    &lt;p&gt; Project Gemini news&lt;/p&gt;
    &lt;p&gt; Project Gemini documentation&lt;/p&gt;
    &lt;p&gt; Project Gemini history&lt;/p&gt;
    &lt;p&gt; Known Gemini software &lt;/p&gt;
    &lt;p&gt;All content at geminiprotocol.net is CC BY-NC-ND 4.0 licensed unless stated otherwise:&lt;/p&gt;
    &lt;p&gt; CC Attribution-NonCommercial-NoDerivs 4.0 International &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://geminiprotocol.net/"/><published>2025-11-17T15:50:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955424</id><title>Israeli-founded app preloaded on Samsung phones is attracting controversy</title><updated>2025-11-17T20:12:33.040635+00:00</updated><content>&lt;doc fingerprint="1d0acc58f9b554c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Early Black Friday deals, Galaxy S25 FE, Fold 7, S25 Ultra. Follow us on YouTube, TikTok, or Instagram&lt;/p&gt;
    &lt;p&gt;Last updated: November 17th, 2025 at 07:11 UTC+01:00&lt;/p&gt;
    &lt;p&gt;SamMobile has affiliate and sponsored partnerships, we may earn a commission.&lt;/p&gt;
    &lt;p&gt;App's ties to Israel are the main cause of concern, but not the only one.&lt;/p&gt;
    &lt;p&gt;Reading time: 3 minutes&lt;/p&gt;
    &lt;p&gt;For years, Samsung has shipped its Galaxy M, F, and A series smartphones in India with a little-known app called AppCloud. Despite its name, AppCloud isn‚Äôt a cloud storage service. It‚Äôs essentially an app-installer that surfaces third-party app recommendations during device setup.&lt;/p&gt;
    &lt;p&gt;On new Galaxy devices in these lineups, AppCloud appears as part of the initial onboarding and forces users to choose whether they want to install certain apps before setup can be completed. You can postpone this by choosing the ‚Äúlater‚Äù option, but the app continues to push a persistent notification until you finish the selection process or disable it entirely.&lt;/p&gt;
    &lt;p&gt;For most users, AppCloud has long been regarded as little more than nuisance bloatware, a side effect of Samsung‚Äôs need to generate revenue beyond hardware margins while competing with aggressive Chinese smartphone brands in India.&lt;/p&gt;
    &lt;p&gt;But findings by the non-profit SMEX from earlier this year suggest AppCloud may not be as harmless as once assumed.&lt;/p&gt;
    &lt;p&gt;Since 2022, Samsung has also been preloading AppCloud on its A and M series phones in several West Asian and North African (WANA) markets. This rollout has triggered privacy concerns due to AppCloud‚Äôs ties to ironSource, a company founded in Israel and now owned by US-based Unity.&lt;/p&gt;
    &lt;p&gt;While AppCloud can be disabled, it is difficult to remove without root access. Furthermore, its privacy policy is not easily available online, raising questions about transparency, user consent, and what kind of data the app may collect.&lt;/p&gt;
    &lt;p&gt;ironSource itself has a controversial track record. The company previously operated an ‚ÄúInstallCore‚Äù program that became infamous for installing software without clear user permission and for bypassing security warnings, behavior that resulted in widespread criticism and blacklisting by several anti-malware tools.&lt;/p&gt;
    &lt;p&gt;The presence of an Israeli-origin technology component on Samsung phones in WANA countries poses additional problems. Several nations in this region legally bar Israeli companies from operating, and in light of the ongoing Israel‚ÄìPalestine conflict, the preload of an app tied to such a company becomes even more contentious.&lt;/p&gt;
    &lt;p&gt;ironSource‚Äôs Aura technology, which ‚Äúoptimizes device experiences‚Äù by surfacing apps, content, and services directly on smartphones, has been used on Samsung devices in Europe, Russia, and Southeast Asia, and by telecom operators in the US; it also appears to do something similar to AppCloud. However, AppCloud itself is not listed anywhere on ironSource‚Äôs website, which appears to be the major cause for concern, even though the app is now owned by a US company.&lt;/p&gt;
    &lt;p&gt;While there‚Äôs no concrete evidence that AppCloud engages in questionable data practices today, the lack of an accessible privacy policy and ironSource's past reputation are causing anxiety among users.&lt;/p&gt;
    &lt;p&gt;Consumer advocates and privacy-focused users are urging Samsung to take immediate steps, like providing a clear opt-out for AppCloud during setup, making its privacy policy public and accessible, and to stop preloading the app entirely in sensitive regions.&lt;/p&gt;
    &lt;p&gt;With concerns rising across multiple markets, Samsung will likely need to issue a statement to reassure customers. We have reached out to the company for comment and will update this story once we hear back.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sammobile.com/news/israeli-app-app-cloud-samsung-phones-controversy/"/><published>2025-11-17T16:54:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955565</id><title>How when AWS was down, we were not</title><updated>2025-11-17T20:12:32.704756+00:00</updated><content>&lt;doc fingerprint="765090565d2694dd"&gt;
  &lt;main&gt;
    &lt;p&gt;For help understanding this article or how you can implement auth and similar security architectures in your services, feel free to reach out to me via the community server.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬® AWS us-east-1 is down!√¢&lt;/head&gt;
    &lt;p&gt;One of the most massive AWS incidents transpired on October 20th. The long story short is that the DNS for DynamoDB was impacted for &lt;code&gt;us-east-1&lt;/code&gt;, which created a health event for the entire region. It's the worst incident we've seen in a decade. Disney+, Lyft, McDonald'ss, New York Times, Reddit, and the list goes on were lining up to claim their share too of the spotlight. And we've been watching because our product is part of our customers critical infrastructure. This one graph of the event says it all:&lt;/p&gt;
    &lt;p&gt;The AWS post-incident report indicates that at 7:48 PM UTC DynamoDB had "increased error rates". But this article isn't about AWS, and instead I want to share how exactly we were still up when when AWS was down.&lt;/p&gt;
    &lt;p&gt;Now you might be thinking: why are you running infra in us-east-1?&lt;/p&gt;
    &lt;p&gt;And it's true, almost no one should be using us-east-1, unless, well, of course, you are us. And that's because we end up running our infrastructure where our customers are. In theory, practice and theory are the same, but in practice they differ. And if our (or your) customers chose &lt;code&gt;us-east-1&lt;/code&gt; in AWS, then realistically, that means you are also choosing us-east-1 √∞¬Ö.&lt;/p&gt;
    &lt;p&gt;During this time, us-east-1 was offline, and while we only run a limited amount of infrastructure in the region, we have to run it there because we have customers who want it there. And even without a direct dependency on &lt;code&gt;us-east-1&lt;/code&gt;, there are critical services in AWS √¢ CloudFront, Certificate Manager, Lambda@Edge, and IAM √¢ that all have their control planes in that region. Attempting to create distributions or roles at that time were also met with significant issues.&lt;/p&gt;
    &lt;p&gt;Since there are plenty of articles in the wild talking about what actually happened, why it happened, and why it will continue to happen, I don't need to go into it here. Instead, I'm going to share a dive about exactly what we've built to avoid these exact issues, and what you can do for your applications and platforms as well. In this article, I'll review how we maintain a high SLI to match our SLA reliability commitment even when the infrastructure and services we use don't.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ What is reliability?√¢&lt;/head&gt;
    &lt;p&gt;Before I get to the part where I share how we built one of the most reliable auth solutions available. I want to define reliability. And for us, that's an SLA of five nines. I think that's so extraordinary that the question I want you to keep in mind through this article is: is that actually possible? Is it really achievable to have a service with a five nines SLA? When I say five nines, I mean that 99.999% of the time, our service is up and running as expected by our customers. And to put this into perspective, the red, in the sea of blue, represents just how much time we can be down.&lt;/p&gt;
    &lt;p&gt;And if you can't see it, it's hiding inside this black dot. It amounts to just five minutes and 15 seconds per year. This pretty much means we have to be up all the time, providing responses and functionality exactly as our customers expect.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ But why?√¢&lt;/head&gt;
    &lt;p&gt;To put it into perspective, it's important to share for a moment, the specific challenges that we face, why we built what we built, and of course why that's relevant. To do that, I need to include some details about what we're building √¢ what Authress actually does. Authress provides login and access control for the software applications that you write √¢ It generates JWTs for your applications. This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;User authentication and authorization&lt;/item&gt;
      &lt;item&gt;User identities&lt;/item&gt;
      &lt;item&gt;Granular role and resource-based authorization (ReBAC, ABAC, TBAC, RBAC, etc...)&lt;/item&gt;
      &lt;item&gt;API keys for your technical customers to interact with your own APIs&lt;/item&gt;
      &lt;item&gt;Machine to machine authentication, or services √¢ if you have a microservice architecture.&lt;/item&gt;
      &lt;item&gt;Audit trails to track the permission changes within your services or expose this to your customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And there are of course many more components, that help complete full auth-platform, but they aren't totally relevant to this article, so I'm going to skip over them.&lt;/p&gt;
    &lt;p&gt;With that, you may already start to be able to see why uptime is so critical for us. We're on the critical path for our customers. It's not inherently true for every single platform, but it is for us. So if our solution is down, then our customer applications are down as well.&lt;/p&gt;
    &lt;p&gt;If we put the reliability part in the back corner for one second and just think about the features, we can theorize about a potential initial architecture. That is, an architecture that just focuses on the features, how might you build this out as simple as possible? I want to do this, so I can help explain all the issues that we would face with the simple solution.&lt;/p&gt;
    &lt;p&gt;Maybe you've got a single region, and in that region you have some sort of HTTP router that handles requests and they forward to some compute, serverless, container, or virtual machine, or, and I'm very sorry for the scenario √¢ if you have to use bare metal. Lastly, you're interacting with some database, NoSQL, SQL, or something else, file storage, and maybe there's some async components.&lt;/p&gt;
    &lt;p&gt;If you take a look at this, it's probably obvious to you (and everyone else) that there is no way it is going to meet our reliability needs. But we have to ask, just exactly how often will there actually be a problem with this architecture? Just building out complexity doesn't directly increase reliability, we need to focus on why this architecture would fail. For us, we use AWS, so I look to the Amazon CTO for guidance, and he's famously quoted as saying, Everything fails all the time.&lt;/p&gt;
    &lt;p&gt;And AWS's own services are no exception to this. Over the last decade, we've seen numerous incidents:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2014 - Ireland (Partial) - Hardware - Transformer failed - EC2, EBS, and RDS&lt;/item&gt;
      &lt;item&gt;2016 - Sydney (Partial) - Severe Weather - Power Loss - All Services&lt;/item&gt;
      &lt;item&gt;2017 - All Regions - Human error - S3 critical servers deleted - S3&lt;/item&gt;
      &lt;item&gt;2018 - Seoul Region - Human error - DNS resolvers impacted - EC2&lt;/item&gt;
      &lt;item&gt;2021 - Virginia - Traffic Scaling - Network Control Plane outage - All Services&lt;/item&gt;
      &lt;item&gt;2021 - California - Traffic Scaling - Network Control Plane outage - All Services&lt;/item&gt;
      &lt;item&gt;2021 - Frankfurt (Partial) - Fire - Fire Suppression System issues - All Services&lt;/item&gt;
      &lt;item&gt;2023 - Virginia - Kinesis issues - Scheduling Lambda Invocations impact - Lambda&lt;/item&gt;
      &lt;item&gt;2023 - Virginia - Networking issues - Operational issue - Lambda, Fargate, API Gateway√¢¬¶&lt;/item&gt;
      &lt;item&gt;2023 - Oregon (Partial) - Error rates - Dynamodb + 48 services&lt;/item&gt;
      &lt;item&gt;2024 - Singapore (Partial) - EC2 Autoscaling - EC2&lt;/item&gt;
      &lt;item&gt;2024 - Virginia (Partial) - Describe API Failures ECS - ECS + 4 services&lt;/item&gt;
      &lt;item&gt;2024 - Brazil - ISP issues - CloudFront connectivity - CloudFront&lt;/item&gt;
      &lt;item&gt;2024 - Global - Network connectivity - STS Service&lt;/item&gt;
      &lt;item&gt;2024 - Virginia - Message size overflow - Kinesis down - Lambda, S3, ECS, CloudWatch, Redshift&lt;/item&gt;
      &lt;item&gt;2025 - Virginia - Dynamo DB DNS - DynamoDB down - All Services&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And any one of these would have caused major problems for us and therefore our customers. And the frequency of incident is actually increasing in time. This shouldn't be a surprise, right? Cloud adoption is increasing over time. The number of services AWS is offering is also increasing. But how impactful are these events? Would single one of them have been a problem for us to actually reach our SLA promise? What would happen if we just trusted AWS and used that to pass through our commitments? Would it be sufficient to achieve 99.999% SLA uptime? Well, let's take a look.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬∞√Ø¬∏ AWS SLA Commitments√¢&lt;/head&gt;
    &lt;head rend="h4"&gt;The AWS Lambda SLA is below 5 nines√¢&lt;/head&gt;
    &lt;head rend="h4"&gt;The API Gateway SLA is below 5 nines√¢&lt;/head&gt;
    &lt;head rend="h4"&gt;The AWS SQS SLA is below 5 nines√¢&lt;/head&gt;
    &lt;p&gt;Okay, so when it comes to trusting AWS SLAs, it isn't sufficient. At. All.&lt;/p&gt;
    &lt;p&gt;We can't just use the components that are offered by AWS, and go from there. We fundamentally need to do something more than that. So the question becomes, what exactly must a dependency's reliability be in order for us to utilize it? To answer that question, it's time for a math lesson. Or more specifically, everyone's favorite topic, probabilities.&lt;/p&gt;
    &lt;p&gt;Let's quickly get through this &lt;del&gt;torture&lt;/del&gt; exercise. Fundamentally, you have endpoints in your service, and you get in an HTTP request, and it interacts with some third-party component or API, and then you write the result to a database. For us, this could be an integration such as logging in with Google or with Okta for our customers' enterprise customers.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬ª Calculating the allowed failure rate√¢&lt;/head&gt;
    &lt;p&gt;So if we want to meet a 5-nines reliability promise, how unreliable could this third-party component actually be? What happens if this component out of the box is only 90% reliable? We'll design a strategy for getting around that.&lt;/p&gt;
    &lt;p&gt;Uptime is a product of all of the individual probabilities:&lt;/p&gt;
    &lt;p&gt;For the sake of this example, we'll just assume that every other component in our architecture is 100% reliable √¢ That's every line of code, no bugs ever written in our library dependencies, or transitive library dependencies, or the dependencies' dependencies' dependencies, and everything always works exactly as we expect.&lt;/p&gt;
    &lt;p&gt;So we can actually rewrite our uptime promise as a result of the failure rate of that third-party component.&lt;/p&gt;
    &lt;p&gt;And the only way that we can actually increase the success rate of the uptime based off of failures is to retry. And so we can multiply out the third-party failure rate and retry multiple times.&lt;/p&gt;
    &lt;p&gt;Logically that makes a lot of sense. When a component fails, if you retry again, and again, the likelihood it will be down every single time approaches zero. And we can generate a really nasty equation from this to actually determine how many exact times do we need to retry.&lt;/p&gt;
    &lt;p&gt;How many exactly can it? Rather than guessing whether or not we should retry four times or five times, or put it in a &lt;code&gt;while(true)&lt;/code&gt; loop, we can figure it out exactly. So we take this equation and extend it out a little bit. Plugging in our 90% reliable third-party component:&lt;/p&gt;
    &lt;p&gt;We find that our retry count actually must be greater than or equal to five. We can see that this adds up to our uptime expectation:&lt;/p&gt;
    &lt;p&gt;Is this the end of the story? Just retry a bunch of times and you're good? Well, not exactly. Remember this equation?&lt;/p&gt;
    &lt;p&gt;We do really need to consider every single component that we utilize. And specifically when it comes to the third-party component, we had to execute it by utilizing a retry handler. So we need to consider the addition of the retry handler into our equation. Going back to the initial architecture, instead of what we had before, when there's a failure in that third-party component, now we will automatically execute some sort of asynchronous retries or in-process retries. And every time that third-party component fails, we execute the retry handler and retry again.&lt;/p&gt;
    &lt;p&gt;This means we need to consider the reliability of that retry handler.&lt;/p&gt;
    &lt;p&gt;Let's assume we have a really reliable retry handler and that it's even more reliable than our service. I think that's reasonable, and actually required. A retry handler that is less reliable than our stated SLA by default is just as faulty as the third-party component.&lt;/p&gt;
    &lt;p&gt;Let's consider one with five and a half nines √¢ that's half a nine more reliable than our own SLA.&lt;/p&gt;
    &lt;p&gt;But how reliable does it really need to be? Well, we can pull in our original equation and realize that our total uptime is the unreliability or the reliability of the third-party component multiplied by the reliability of our retry handler.&lt;/p&gt;
    &lt;p&gt;From here, we add in the retries to figure out what the result should be:&lt;/p&gt;
    &lt;p&gt;We have a reliable retry handler, but it's not perfect. And with a retry handler that has reliability of five and a half nines, we can retry a maximum two times. Because remember, it has to be reliable every single time we utilize it, as it is a component which can also fail. Which means left with this equation:&lt;/p&gt;
    &lt;p&gt;I don't think comes as a surprise to anyone that in fact five is greater than two. What is the implication here?&lt;/p&gt;
    &lt;p&gt;The number of retries required for that unreliable third-party component to be utilized by us exceeds the number of retries actually allowed by our retry handler.&lt;/p&gt;
    &lt;p&gt;That's a failure, the retry handler can only retry twice before itself violates our SLA, but we need to retry five times in order to raise the third-party component reliably up. We can actually figure out what the minimum reliability of a third-party component is allowed to be, when using our retry handler:&lt;/p&gt;
    &lt;p&gt;Which in turn validates that it's actually impossible for us to utilize that component. &lt;code&gt;99.7%&lt;/code&gt;. &lt;code&gt;99.7%&lt;/code&gt; is the minimum allowed reliability for any third-party component in order for us to meet our required 5-nines SLA. This third-party component is so unreliable (&lt;code&gt;~90%&lt;/code&gt;), that even using a highly reliable retry handler, we still can't make it reliable enough without the retry handler itself compromising our SLA. We fundamentally need to consider this constraint, when we're building out our architecture. &lt;/p&gt;
    &lt;p&gt;That means we drop this third-party component. Done.&lt;/p&gt;
    &lt;p&gt;And then, let's assume we get rid of every flaky component, everything that don't have a high enough reliability for us. At this point, it's good to think, is this sufficient to achieve our 5-nines SLA? Well, it isn't just third-party components we have to be concerned about. We also have to be worried about those AWs infrastructure failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬©√Ø¬∏ Infrastructure Failures√¢&lt;/head&gt;
    &lt;p&gt;So let's flashback to our initial architecture again:&lt;/p&gt;
    &lt;p&gt;We can have issues at the database layer, right? There could be any number of problems here. Maybe it's returning 500s, there are some slow queries, maybe things are timing out. Or there could be a problem with our compute. Maybe it's not scaling up fast enough. We're not getting new infrastructure resources. Sometimes, even AWS is out of bare metal machines when you don't reserve them, request them get them on demand, and the list go on.&lt;/p&gt;
    &lt;p&gt;Additionally, there could also be some sort of network issue, where requests aren't making it through to us or even throw a DNS resolution error on a request from our users.&lt;/p&gt;
    &lt;p&gt;In many of these cases, I think the answer is obvious. We just have to declare the whole region as down. And you are probably thinking, well, this is where we failover to somewhere else. No surprise, yeah, this is exactly what we do:&lt;/p&gt;
    &lt;p&gt;However, this means we have to have all the data and all the infrastructure components duplicated to another region in order to do this. And since Authress has six primary regions around the world, that also means we need multiple backup regions to be able to support the strategy. But this comes with significant wasted resources and wasted compute that we're not even getting to use. Costly! But I'll get to that later.&lt;/p&gt;
    &lt;p&gt;Knowing a redundant architecture is required is a great first step, but that leaves us having to solve for: how do we actually make the failover happen in practice?&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬ß The Failover Routing Strategy√¢&lt;/head&gt;
    &lt;p&gt;Simply put √¢ our strategy is to utilize DNS dynamic routing. This means requests come into our DNS and it automatically selects between one of two target regions, the primary region that we're utilizing or the failover region in case there's an issue. The critical component of the infrastructure is to switch regions during an incident:&lt;/p&gt;
    &lt;p&gt;In our case, when using AWS, this means using the Route 53 health checks and the Route 53 failover routing policy.&lt;/p&gt;
    &lt;p&gt;We know how we're gonna do it, but the long pole in the tent is actually knowing that there is even a problem in the first place. A partial answer is to say Have a health check, so of course there is health check here. But the full answer is: have a health check that validates both of the regions, checking if the region is up, or is there an incident? And if it is, reports the results to the DNS router.&lt;/p&gt;
    &lt;p&gt;We could be utilizing the default provided handler from AWS Route 53 or a third-party component which pings our website, but that's not accurate enough from a standpoint of correctly and knowing for certain that our services are in fact down.&lt;/p&gt;
    &lt;p&gt;It would be devastating for us to fail over when a secondary region is having worse problems than our primary region. Or what if there's an issue with with network traffic. We wouldn't know if that's an issue of communication between AWS's infrastructure services, or an issue with the default Route 53 health check endpoint, or some entangled problem with how those specifically interact with our code that we're actually utilizing. So it became a requirement to built something ourselves, custom, to actually execute exactly what we need to check.&lt;/p&gt;
    &lt;p&gt;Here is a representation of what we're doing. It's not exactly what we are doing, but it's close enough to be useful. Health check request come in from the Route 53 Health Check. They call into our APIGW or Load Balancer as a router. The requests are passed to our compute which can interact and validate logic, code, access, and data in the database:&lt;/p&gt;
    &lt;p&gt;The health check executes this code on request that allows us to validate if the region is in fact healthy:&lt;/p&gt;
    &lt;code&gt;import Authorizer from './authorizer.js';&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We start a profiler to know how long our requests are taking.&lt;/item&gt;
      &lt;item&gt;Then we interact with our databases, as well as validate some secondary components, such as SQS. While issues with secondary components may not always be a reason to failover, they can cause impacts to response time, and those indicators can be used to predict incoming incidents.&lt;/item&gt;
      &lt;item&gt;From there, we check whether or not the most critical business logic is working correctly. In our case, that's interactions with DynamoDB as well as core authorizer logic. Compared to a simple unit test, this accounts for corruption in a deployment package, as well instances where some subtle differences between regions interact with our code base. We can catch those sorts of problems here, and know that the primary region that we're utilizing, one of the six, is having a problem and automatically update the DNS based on this.&lt;/item&gt;
      &lt;item&gt;When we're done, we return success or failure so the health check can track changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;√∞¬ø Improving the Failover Strategy√¢&lt;/head&gt;
    &lt;p&gt;And we don't stop here with our infrastructure failover however. With the current strategy, it's good, in some cases, even sufficient. But it isn't that great. For starters, we have to completely failover. If there's just one component that's problematic, we can't just swap that one out easily, it's all or nothing with the Route 53 health check. So when possible, we push for an edge-optimized architecture. In AWS, this means utilizing AWS CloudFront with AWS Lambda@Edge for compute. This not only helps reduce latency for our customers and their end users depending where they are around the world, as a secondary benefit, fundamentally, it is an improved failover strategy.&lt;/p&gt;
    &lt;p&gt;And that looks like this:&lt;/p&gt;
    &lt;p&gt;Using CloudFront gives us a highly reliable CDN, which routes requests to the locally available compute region. From there, we can interact with the local database. When our database in that region experiences a health incident, we automatically failover, and check the database in a second adjacent region. And when there's a problem there as well, we do it again to a third region. We can do that because when utilizing DynamoDB we have Global Tables configured for authorization configuration. In places where we don't need the data duplicated, we just interact with the table in a different region without replication.&lt;/p&gt;
    &lt;p&gt;After a third region with an issue, we stop.&lt;/p&gt;
    &lt;p&gt;And maybe you're asking why three and not four or five or six? Aren't you glad we did the probabilities exercise earlier? Now you can actually figure out why it's three here. But, I'll leave that math as an exercise for you.&lt;/p&gt;
    &lt;p&gt;As a quick recap, this handles the problems with at the infrastructure level and with third-party components. And if we solve those, is that sufficient for us to achieve our goal the 5-nines SLA?&lt;/p&gt;
    &lt;p&gt;For us the answer is No, and you might have guessed, if you peaked at the scrollbar or table contents that there are still quite some additional components integrated into our solution. One of them is knowing that at some point, there's going to be a bug in our code, unfortunately.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬ª Application level failures√¢&lt;/head&gt;
    &lt;p&gt;And that bug will get committed to production, which means we're going to end up with an application failure. It should be obvious that it isn't achievable to write completely bug-free code. Maybe there is someone out there that thinks that, and maybe even that's you, and I believe you that you believe that. However, I know it's not me, and realistically, I don't want to sit around and pray that it's also my fellow team members. The risk is too high, because in the case something does get into production, that means it can impact some of our customers. So instead, let's assume that will happen and design a strategy around it.&lt;/p&gt;
    &lt;p&gt;So when it does happen, we of course have to trigger our incident response. For us, we send out an email, we post a message on our community and internal communication workspaces, and start an on-call alert. The technology here isn't so relevant, but tools like AWS SES, SQS, SNS, Discord, and emails are involved.&lt;/p&gt;
    &lt;p&gt;Incidents wake an engineer up, so someone can start to take look at the incident, and most likely the code.&lt;/p&gt;
    &lt;p&gt;But by the time they even respond to the alert, let alone actually investigate and fix the cause of the incident, we would long violated our SLA. So an alert is not sufficient for us. We need to also implement automation to automatically remediate any of these problems. Now, I'm sure you're thinking, yeah, okay, test automation. You might even be thinking about an LLM agent that can automatically create PRs. (Side note: LLM code generation, doesn't actually work for us, and I'll get to that a little further down) Instead, we have to rely on having sufficient testing in place. And yes, of course we do. We test before deployment. There is no better time to test.&lt;/p&gt;
    &lt;p&gt;This seems simple and an obvious answer, and I hope that for anyone reading this article it is. Untested code never goes to production. Every line of code is completely tested before it is merged to production, even if it is enabled on some flag. Untested code is never released, it is far too dangerous. Untested code never makes it to production behind some magic flag. Abusing feature flags to make that happen could not be a worse decision for us. And that's because we can need to be as confident as possible before those changes actually get out in front of our customers. The result is √¢ we don't focus on test coverage percentage, but rather test value. That is, which areas provide most value, that are most risky, that we care about being the most reliable for our customers. Those are the ones we focus on testing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Root Cause Analysis (RCA)√¢&lt;/head&gt;
    &lt;p&gt;Every incident could have been prevented if we just had one more test. The trick though is actually having that right test, before the incident.&lt;/p&gt;
    &lt;p&gt;And in reality, that's not actually possible. Having every right test for a service that is constantly changing, while new features are being added, is just unmaintainable. Every additional test we write increases the maintenance burden of our service. Attempting to achieve 100% complete test coverage would require an infinite amount of time. This is known as the Pareto Principle, more commonly the 80-20 rule. If it takes 20% of the time to deliver 80% of the tests, it takes an infinite amount of time to achieve all the tests, and that assumes that the source code isn't changing.&lt;/p&gt;
    &lt;p&gt;The result is we'll never be able to catch everything. So we can't just optimize for prevention. We also need to optimize for recovery. This conclusion for us means also implementing tests against our deployed production code. One example of this are validation tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ Validation Tests√¢&lt;/head&gt;
    &lt;p&gt;A validation test is where you have some data in one format and data in another format and you use those two different formats to ensure referential consistency. (Side note: There are many different kinds of tests, and I do a deep dive in the different types of tests and how they're relevant in building secure and reliable systems). One concrete example could be you have a request that comes in, you end up logging the request data and the response, then you can compare that logged data to what's actually saved in your database.&lt;/p&gt;
    &lt;p&gt;In our scenario, which focuses on the authorization and permissions enforcement checks, we have multiple databases with similar data. In one case, there's the storage of permissions as well as the storage of the expected checks and the audit trail tracking the creation of those permissions. So we actually have multiple opportunities to compare the data between our databases asynchronously outside of customer critical path usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Running the Validation√¢&lt;/head&gt;
    &lt;p&gt;On a schedule, via an AWS CloudWatch Scheduled Rule, we load the data from our different databases and we compare them against each other to make sure it is consistent. If there is a problem, then if this fires off an incident before any of our customers notice, so that we can actually go in and check what's going on.&lt;/p&gt;
    &lt;p&gt;This sounds bad on the surface that it could ever happen. But the reality of the situation is that a discrepancy can show up as a result of any number of mechanisms. For instance, the infrastructure from AWS could have corrupted one of the database shards and what is written to the databases is inconsistent. We know that this can happen as there is no 100% guarantee on database durability, even from AWS. AWS does not guarantee Database Durability, are you assuming they do, because we don't! So actually reading the data back and verifying its internal consistency is something that we must do.&lt;/p&gt;
    &lt;p&gt;While it might not seem that this could reduce the probability of there being an incident. Consider that a requested user permission check whose result doesn't match our customer's expectation is an incident. It might not always be one that anyone identifies or even becomes aware of, but it nonetheless a problem, just like a publicly exposed S3 is technically an issue, even if no one has exfiltrated the data yet, it doesn't mean the bucket isn'is sufficiently secured.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬Ø Incident Impact√¢&lt;/head&gt;
    &lt;p&gt;There are two parts to the actual risk of an incident. The probability and the impact. Everything in this article I've discuss until now talks about reducing the probability of an incident, that is √¢ the likelihood of it happening. But since we know that we can't avoid ever having an incident, we also have to reduce the impact when it happens.&lt;/p&gt;
    &lt;p&gt;One way we do that is by utilizing an incremental rollout. Hopefully everyone knows what incremental rollout is, so I'll instead jump straight into how we accomplish it utilizing AWS. And for that we focus again on our solution integrating with CloudFront and our edge architecture.&lt;/p&gt;
    &lt;p&gt;The solution for us is what I call Customer Deployment Buckets. We bucket individual customers into separate buckets and then deploy to each of the buckets sequentially. If the deployment rolls out without a problem, and it's all green, that is everything works correctly, then we go on to the second bucket and then deploy our code to there, and then the third bucket, and so on and so forth until every single customer has the new version.&lt;/p&gt;
    &lt;p&gt;If there is an issue, we stop the rollout and we go and investigate what's actually going on. While we can't prevent the issue from happening to the earlier buckets, we are able to stop that issue from propagating to more customers, having an impact on everyone, and thus reduce the impact of the incident.&lt;/p&gt;
    &lt;p&gt;As I mentioned before the biggest recurring issue isn't executing an operations process during an incident, it's identifying there is a real incident in the first place. So, How do we actually know that there's an issue?&lt;/p&gt;
    &lt;p&gt;If it was an easy problem to solve, you would have written a unit task or integration test or service level test and thus already discovered it, right? So adding tests can't, by design, help us. Maybe there's an issue with the deployment itself or during infrastructure creation, but likely that's not what's happening.&lt;/p&gt;
    &lt;p&gt;Now, I know you're thinking, When is he going to get to AI?&lt;/p&gt;
    &lt;p&gt;Whether or not we'll ever truly have AI is a separate &lt;code&gt;&amp;lt;rant /&amp;gt;&lt;/code&gt; that I won't get into here, so this is the only section on it, I promise. What we actually do is better called anomaly detection. Historically anomaly detection, was what AI always meant, true AI, rather than an LLM or agent in any way.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ AI: Anomaly Detection√¢&lt;/head&gt;
    &lt;p&gt;This is a graph of our detection analysis:&lt;/p&gt;
    &lt;p&gt;You might notice that it's not tracking 400s or 500s, which are in reality relatively easy to detect. But in fact don't actually tell us meaningfully what's wrong with our service or whether or not there really is a problem. Impact is measured by business value, not technical protocol level analytics, so we need to have a business-focused metric.&lt;/p&gt;
    &lt;p&gt;And for us, at Authress, the business-focussed metric we use to identify meaningful incidents we call: The Authorization Ratio. That is the ratio of successful logins and authorizations to ones that are blocked, rejected, timeout or are never completed for some reason.&lt;/p&gt;
    &lt;p&gt;The above CloudWatch metric display contains this exact ratio, and here in this timeframe represents an instance not too long ago where we got really close to firing off our alert.&lt;/p&gt;
    &lt;p&gt;Here, there was a slight elevation of errors soon after a deployment. The expected ratio was outside of our allowance span for a short period of time. However not long enough to trigger an incident. We still investigated, but it wasn't something that required immediate remediation. And it's a good reminder that identifying problems in any production software isn't so straightforward. To achieve high reliability, we've needed an AI or in this case anomaly detection to actually identify additional problems. And realistically, even with this level of sophistication in place, we still can never know with 100% certainty that there is actually an incident at any moment. And that's because "what is an incident", is actually a philosophical question...&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬π Does it smell like an incident?√¢&lt;/head&gt;
    &lt;p&gt;Our anomaly detection said √¢ almost an incident, and we determined the result √¢ no incident. But does that mean there wasn't an incident? What makes an incident, how do I define an incident? And is that exact definition ubiquitous, for every system, every engineer, every customer?&lt;/p&gt;
    &lt;p&gt;Obviously not, and one look at the AWS Health Status Dashboard is all you need to determine that the identification of incidents is based on subjective perspective, rather than objective criteria. What's actually more important is the synthesis of our perspective on the situation and what our customers believe. To see what I mean, let's do a comparison:&lt;/p&gt;
    &lt;p&gt;I'm going to use Authress as an example. So I've got the product services perspective on one side and our customer's perspective on the other.&lt;/p&gt;
    &lt;head rend="h3"&gt;Incident Alignment√¢&lt;/head&gt;
    &lt;p&gt;In the top left corner we have alignment. If we believe that our system is up and working and our customers do, too, then success, all good. Everything's working as expected.&lt;/p&gt;
    &lt;p&gt;Inversely in the opposite corner, maybe there is a problem. We believe that one of our services is having an issue, and successfully, we're able to identify it. Most importantly, our customers say√¢yes, there is an issue for us.&lt;/p&gt;
    &lt;p&gt;It's not great that there's an incident, but as I've identified incidents will absolutely happen, and the fact we've correctly aligned with our customers on the problem's existence independently allows us to deploy automation to automatically remediate the issue. That's a success! If it's a new problem that we haven't seen before, we can even design new automation to fix this. Correctly identifying incidents is challenging, so doing that step correctly, leads itself very well to automation for remediation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Perspective Mismatch√¢&lt;/head&gt;
    &lt;p&gt;One interesting corner is when our customers believe that there's nothing wrong, there have been no incidents reported, but all our alerts are saying √¢ RED ALERT √¢ someone has to go look at this!&lt;/p&gt;
    &lt;p&gt;In this case, our alerts have identified a problem that no one cares about. This often happens in scenarios where our customers are in one region, Switzerland for example, with local region users, a health care, manufacturing, or e-commerce app, is a good example, rather than global, who are likely asleep at 2:00 AM. And that means an incident at the moment, could be an issue affecting some customers. But if they aren't around to experience it, is it actually happening?&lt;/p&gt;
    &lt;p&gt;You are probably wincing at that idea. There's a bug, it must be fixed! And sure that's a problem, it's happening and we should take note of what's going on. But we don't need to respond in real time. That's a waste of our resources where we could be investing in other things. Why wake up our engineers based on functionality that no one is using?&lt;/p&gt;
    &lt;p&gt;I think one of the most interesting categories is in the top right-hand corner where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;our customers say, "hey, your service is down"&lt;/item&gt;
      &lt;item&gt;But we say, "Wait, really, is it?"_&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is known as a gray failure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gray Failures√¢&lt;/head&gt;
    &lt;p&gt;And it can happen for any number of reasons. Maybe there is something in our knowledge base that tells our customers to do something one way and it's confusing and they've interpreted it in a different way. So there's a different expectation here. That expectation can get codified into customer processes and product services.&lt;/p&gt;
    &lt;p&gt;Or maybe our customer is running different tests from us, ones that are of course, valuable for their business, but not ones that we consider. Or more likely they are just using a less resilient cloud provider.&lt;/p&gt;
    &lt;p&gt;Most fundamentally, there could really be an incident, something that we haven't detected yet, but they have. And if we don't respond to that, it could grow, and left unchecked, escalate, and eventually impact all our customers. This means we need to give our customers an easy way to report incidents to us, which we can immediately follow up with.&lt;/p&gt;
    &lt;p&gt;For us, every single incident, every single customer support ticket that comes into our platform, we immediately and directly send it to our engineering team. Now, I often get pushback on this from other leaders. I'm sure, even you might be thinking something like √¢ I don't want to be on call for customer support incidents. But if you throw additional tiers in your organization between your engineering teams and your customers, that means you're increasing the time to actually start investigating and resolving those problems. If you have two tiers before your engineering team and each tier has its own SLA of 10 minutes to triage the issue, that means you've already gone through 20 minutes before an engineer even knows about it and can go and look at it. That violates our SLA by fourfold before investigation and remediation can even begin.&lt;/p&gt;
    &lt;p&gt;Instead, in those scenarios, what I actually recommend thinking about is how might you reduce the number of support tickets you receive in aggregate? This is the much more appropriate way to look at the problem. If you are getting support tickets that don't make sense, then you've got to investigate, why did we get this ticket? Do the root cause analysis on the ticket, not just the issue mentioned in it √¢ why the ticket was even created in the first place.&lt;/p&gt;
    &lt;p&gt;A ticket means: Something is broken. From there, we can figure out, OK, maybe we need to improve our documentation. Or we need to change what we're doing on one of our endpoints. Or we need to change the response error message we're sending. But you can always go deeper.&lt;/p&gt;
    &lt;head rend="h3"&gt;The customer support advantage√¢&lt;/head&gt;
    &lt;p&gt;And going deeper, means customer support is critical for us. We consider customer support to be the lifeline of our service level agreement (SLA). If we didn't have that advantage, then we might not have been able to deliver our commitment at all. So much so that we report some of our own CloudWatch custom metrics to our customers so they can have an aggregate view of both what they know internally and what we believe. We do this through our own internal dashboard in our application management UIs.&lt;/p&gt;
    &lt;p&gt;Helping our users identify incidents benefits us; because we can't catch everything. It's just not possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ Negligence and Malice√¢&lt;/head&gt;
    &lt;p&gt;To this point, we've done the math on reliability of third-party components. We've implemented an automatic region failover and added incremental rollout. And we have a core customer support focus. Is that sufficient to achieve 5-nines of reliability?&lt;/p&gt;
    &lt;p&gt;If you think yes, then you'd expect the meme pictures now. And, I wish I could say it was enough, but it's not. That's because we also have to deal with negligence and malice.&lt;/p&gt;
    &lt;p&gt;We're in a privileged position to have numerous security researchers out there on the internet constantly trying to find vulnerabilities within our service. For transparency, I have some of those reports I want to share:&lt;/p&gt;
    &lt;head rend="h3"&gt;√¢Real√¢ Vulnerability Reports√¢&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;I am a web security researcher enthusiast. Do you give a monetary reward?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Okay, this isn't starting out that great. What else have we received?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I found some vulnerabilities in your website. Do you offer rewards for ethical hackers?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Well, maybe, but I think you would actually need to answer for us, what the problem actually is. And you also might notice this went to our spam. It didn't even get to our inbox. So a lot of help they might be providing. Actually we ignore any √¢security√¢ email sent from a non-custom domain.&lt;/p&gt;
    &lt;p&gt;This one was really interesting. We had someone attempting to phish our engineering team by creating a support ticket and putting in some configuration trying to get us to provide them our own credentials to one of our third-party dependencies. Interestingly enough, our teams don't even have access to those credentials directly.&lt;/p&gt;
    &lt;p&gt;And, we know this was malicious because the credentials that they are referencing in the support request are from our honey pot, stuck in our UI to explicitly catch these sorts of things. The only way to get these credentials is if they hacked around our UI application and pulled out of the HTML. They aren't readily available any other way. So it was very easy for us to detect that this √¢report√¢ was actually a social engineering attack.&lt;/p&gt;
    &lt;p&gt;And this is one of my favorites, and I can't make this up:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have found many security loophole. How much will you pay if you want to working with me like project?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That's the exact quote, I don't even know what that means. Unfortunately, LLMs will actually start to make all of these future "vulnerability reports" sound more appealing to read in the future, for better or worse. However, at the end of the day, the truth is that these are harmless. And we actually do have a security disclosure program that anyone can go and submit problems for. I hope the message to white-hat hackers is please use that process, and the legitimate reports usually do go through it. Do not send us emails. Those are going to go into the abyss. Alternatively, you can follow our security.txt public page or go to the disclosure form, but with email, the wrong people are going to get that and we can't triage effectively.&lt;/p&gt;
    &lt;p&gt;Vulnerabilities in our services can result in production incidents for our customers. That means security is part of our SLA. Don't believe me, I'll show you how:&lt;/p&gt;
    &lt;head rend="h3"&gt;Multitenant considerations√¢&lt;/head&gt;
    &lt;p&gt;It's relevant for us, that Authress is a multitenant solution. So some of the resources within our service are in fact shared between customers.&lt;/p&gt;
    &lt;p&gt;Additionally, customers could have multiple services in a microservice architecture or multiple components. And one of these services could theoretically consume all of the resources that we've allocated for that customer. In that scenario, that would cause an incident for that customer. So we need to protect against resource exhaustion Intra-Tenant. Likewise, we have multiple customers. One of those customers could be consuming more resources than we've allocated to the entire tenant. And that could cause an incident across Inter-Tenant and cause an incident across our platform and impact other customers.&lt;/p&gt;
    &lt;p&gt;Lastly, we have to be worried about our customers, our customers' customers, and our customers' customers' customers, because any one of those could be malicious and consume their resources and so on and so forth, thus causing a cascading failure. A failure due to lack of resources is an incident. The only solution that makes sense for this is, surprise, rate limiting.&lt;/p&gt;
    &lt;head rend="h3"&gt;Helpful Rate Limiting√¢&lt;/head&gt;
    &lt;p&gt;So we need to rate-limit these requests at different levels for different kinds of clients, different kinds of users, and we do that within our architecture, at different fundamental levels within our infrastructure.&lt;/p&gt;
    &lt;p&gt;Primarily there are protections at our compute level, as well at the region level, and also place protections at a global level. In AWS, this of course means using a web application firewall or WAF. I think our WAF configuration is interesting and in some ways novel.&lt;/p&gt;
    &lt;p&gt;Fundamentally, one of the things that we love to use is the AWS managed IP reputation list.&lt;/p&gt;
    &lt;p&gt;The reputation list is list of IP addresses that have been associated with malicious activity outside of our service throughout other customers at AWS and other providers out there in the world where a problem has been detected. That means before those attacks even get to our service or to our customers' instances of Authress, we can already know to block them, and the WAF does that. This is great, and most importantly, has a very low false positive rate.&lt;/p&gt;
    &lt;p&gt;However, the false positive rate is an important metric for consideration of counter measures against malicious attacks or negligent accidental abuse of resources, and something that prevents us from using any other managed rules from AWS or external providers. There's two problems with managed rules, fundamentally:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Number one is the false positive rate. If that is even a little bit more than, it couldn't be sustainable, and would result in us blocking legitimate requests coming for a customer. This means it is a problem, and it's an incident for them if some of their users can't utilize their software because of something we did. False positives are customer incidents.&lt;/item&gt;
      &lt;item&gt;The second one is that managed rules are gratuitously expensive. Lots of companies are building these just to charge you lots of money, and the ROI just doesn't seem to be there. We don't see useful blocks from them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the truth is, we need to do something more than just the reputation list rule.&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling Requests at Scale√¢&lt;/head&gt;
    &lt;p&gt;And the thing that we've decided to do is √¢ add blocking for sufficiently high requests. By default, any Authress account's service client that goes above 2,000 requests per second (RPS), we just immediately terminate. Now, this isn't every customer, as there are some out there for us that do require such a high load or even higher (as 2k isn't that high). But for the majority of them, if you get to this number and they haven't talked to us about their volume, then it is probably malicious in some way. You don't magically go from zero to 2,000 one day, unless it is an import job.&lt;/p&gt;
    &lt;p&gt;Likewise, we can actually learn about a problem long before it gets to that scale. We have milestones, and we start reporting loads from clients at 100, 200, 500, 1,000, et cetera. If we see clients hitting these load milestones, we can already start to respond and create an incident for us to investigate before they reach a point where they're consuming all of the resources in our services for that customer. And we do this by adding alerts on the COUNT of requests for WAF metrics.&lt;/p&gt;
    &lt;p&gt;However, we also get attacks at a smaller scale. Just because we aren't being DDoS-ed doesn't mean there isn't attack. And those requests will still get through because they don't meet our blocking limits. They could be malicious in nature, but only identifiable in aggregate. So while single request might seem fine, if you see the same request 10 times a second, 100 times a second, something is probably wrong. Or if you have request urls that end in &lt;code&gt;.php?admin&lt;/code&gt;, when no one has run WordPress in decades, you also know that there's a problem. We catch these by logging all of the blocked requests.&lt;/p&gt;
    &lt;p&gt;We have automation in place to query those results and update our rules, but a picture is worth a thousand words:&lt;/p&gt;
    &lt;p&gt;Here you can see a query based off of the IP addresses from the client that are being utilized and sorted by frequency. When we get these requests that look non-malicious individually, we execute a query such as this one and we check to see if the results match a pattern. You can use ip address matching or more intelligently, something called the JA3 or JA4 fingerprints of those requests There are actually lots of options available, I'm not going to get into exactly what they are, there are some great articles on the topic. And there are more mechanisms to actually track these used throughout the security industry, and utilizing them let's you instantly identify: Hey, you know what? This request violates one of our patterns, maybe we should block all the requests from that client.&lt;/p&gt;
    &lt;p&gt;And so, rather than waiting for them to get to the point where an attacker is consuming 2,000 requests per second worth of resources, you can stop there right away. In the cases where we can't make a conclusive decision, this technology gives us another tool that we can utilize to improve our patterns for the future. Maybe it goes without saying, but of course because we've running our technology to many regions around the world, we have to work on deploying this infrastructure in all these places and push it out to the edge where possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ The Conclusion√¢&lt;/head&gt;
    &lt;p&gt;I said a lot of things, so I to quickly want to quickly summarize our architecture that we have in place:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Third-party component reliability reviews. I can't stress this enough. Don't just assume that you can utilize something. And sometimes in order to achieve 5-nines, you actually have to remove components from your infrastructure. Some things are just not able to be utilized no matter what. Now maybe you can put it in some sort of async background, but it can't be on the critical path for your endpoints.&lt;/item&gt;
      &lt;item&gt;DNS failover and health checks. For places where you have an individual region or availability zone or cluster, having a full backup with a way to conclusively determine what's up and automatically failover is critical.&lt;/item&gt;
      &lt;item&gt;Edge compute where possible. There's a whole network out there of services that are running on top of the cloud providers, which help guarantee your capability to run as close to as possible to where your users are and reduce latency.&lt;/item&gt;
      &lt;item&gt;Incremental rollout for when you want to reduce the impact as much as possible.&lt;/item&gt;
      &lt;item&gt;The Web Application Firewall for handling those malicious requests.&lt;/item&gt;
      &lt;item&gt;Having a Customer Support Focus to enable escalating issues that outside your area of detection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And through seven years or so that we've been doing this and building up this architecture, there's a couple of things that we've learned:&lt;/p&gt;
    &lt;head rend="h3"&gt;Murphy's Law√¢&lt;/head&gt;
    &lt;p&gt;Everything fails all the time. There absolutely will be failures everywhere. Every line of code, every component you pull in, every library, there's guaranteed to be a problem in each and everyone of those. And you will for sure have to deal with it, at some point. So being prepared to handle that situation, is something you have to be thinking through in your design.&lt;/p&gt;
    &lt;head rend="h3"&gt;DNS√¢&lt;/head&gt;
    &lt;p&gt;DNS, yeah, AWS will say it, everyone out there will say, and now we get to say it. The global DNS architecture is pretty good and reliable for a lot of scenarios, but I worry that it's still a single point of failure in a lot of ways.&lt;/p&gt;
    &lt;head rend="h3"&gt;Infrastructure as Code (IAC)√¢&lt;/head&gt;
    &lt;p&gt;The last thing is infrastructure as code challenges. We deploy primary regions, but then there's also the backup regions, which are slightly different from the primary regions, and then there are edge compute, which are, again, even more slightly different. And then sometimes, we do this ridiculous thing, where we deploy infrastructure dedicated to one customers. And in doing so, we're running some sort of IaC to deploy those resources.&lt;/p&gt;
    &lt;p&gt;It is almost exactly the same architecture. Almost! Because it isn't exactly the same there are quite the opportunities for challenges to sneak it. That's problematic with even Open Tofu or CloudFormation, and often these tools make it more difficult, not less. And good luck to you, if you're still using some else that hasn't been modernized. With those, it's even easier to run into problems and not get it exactly correct.&lt;/p&gt;
    &lt;p&gt;The last thing I want to leave you with is, well, With all of these, is that actually sufficient to achieve five nines?&lt;/p&gt;
    &lt;p&gt;No. Our commitment is 5-nines, what we do is in defense of that, just because you do all these things doesn't automatically mean your promise of 5-nines in guaranteed. And you know what, you too can promise a 5-nines SLA without doing anything. You'll likely break your promise, but for us our promise is important, and so this is our defense.&lt;/p&gt;
    &lt;p&gt;For help understanding this article or how you can implement auth and similar security architectures in your services, feel free to reach out to me via the community server.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://authress.io/knowledge-base/articles/2025/11/01/how-we-prevent-aws-downtime-impacts"/><published>2025-11-17T17:07:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955900</id><title>Azure hit by 15 Tbps DDoS attack using 500k IP addresses</title><updated>2025-11-17T20:12:31.006038+00:00</updated><content>&lt;doc fingerprint="5537b26832532216"&gt;
  &lt;main&gt;
    &lt;p&gt;On October 24, 2025, Azure DDOS Protection automatically detected and mitigated a multi-vector DDoS attack measuring 15.72 Tbps and nearly 3.64 billion packets per second (pps). This was the largest DDoS attack ever observed in the cloud and it targeted a single endpoint in Australia.&lt;/p&gt;
    &lt;p&gt;By utilizing Azure‚Äôs globally distributed DDoS Protection infrastructure and continuous detection capabilities, mitigation measures were initiated. Malicious traffic was effectively filtered and redirected, maintaining uninterrupted service availability for customer workloads.&lt;/p&gt;
    &lt;p&gt;The attack originated from Aisuru botnet. Aisuru is a Turbo Mirai-class IoT botnet that frequently causes record-breaking DDoS attacks by exploiting compromised home routers and cameras, mainly in residential ISPs in the United States and other countries.&lt;/p&gt;
    &lt;p&gt;The attack involved extremely high-rate UDP floods targeting a specific public IP address, launched from over 500,000 source IPs across various regions. These sudden UDP bursts had minimal source spoofing and used random source ports, which helped simplify traceback and facilitated provider enforcement.&lt;/p&gt;
    &lt;p&gt;Attackers are scaling with the internet itself. As fiber-to-the-home speeds rise and IoT devices get more powerful, the baseline for attack size keeps climbing.&lt;/p&gt;
    &lt;p&gt;As we approach the upcoming holiday season, it is essential to confirm that all internet-facing applications and workloads are adequately protected against DDOS attacks. Additionally, do not wait for an actual attack to assess your defensive capabilities or operational readiness‚Äîconduct regular simulations to identify and address potential issues proactively.&lt;/p&gt;
    &lt;p&gt;Learn more about Azure DDOS Protection at Azure DDoS Protection Overview | Microsoft Learn&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcommunity.microsoft.com/blog/azureinfrastructureblog/defending-the-cloud-azure-neutralized-a-record-breaking-15-tbps-ddos-attack/4470422"/><published>2025-11-17T17:39:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955904</id><title>DESI's Dizzying Results</title><updated>2025-11-17T20:12:30.847261+00:00</updated><content>&lt;doc fingerprint="2fc81306fdbe4349"&gt;
  &lt;main&gt;
    &lt;p&gt;In March of 2024 the DESI collaboration dropped a bombshell on the cosmological community: slim but significant evidence that dark energy might be getting weaker with time. This was a stunning result delivered after years of painstaking analysis. It√¢s not a bullet-proof result, but it doesn√¢t have to be to make our lives more interesting.&lt;/p&gt;
    &lt;p&gt;I know I√¢m late to the party on discussing this. And it√¢s okay, because 1) there√¢s a lot to unpack in this kind of result and I wanted to take my time, and 2) it√¢s not like this result is going to get revised or even updated anytime soon, so we√¢ve got plenty of room to play with this.&lt;/p&gt;
    &lt;p&gt;Let√¢s start with the results themselves and how they got there. DESI stands for Dark Energy Spectroscopic Instrument. It√¢s a roughly 4-meter telescope mounted on Kitt Peak in southeastern Arizona. It√¢s a galaxy survey, and to accomplish this survey they have 5,000 robotically controlled fiber optic cables underneath the telescope. Every night, the telescope selects a patch of sky to observe, the robots position the fiber optic cables to align with the positions of galaxies within that patch, and the instrument records detailed information for each and every single one. Then they do the same thing the next night, and then the next, and then the next.&lt;/p&gt;
    &lt;p&gt;So far they have amassed a catalog of over 13 million galaxies, providing the largest and comprehensive survey of galaxy positions in history. And they√¢re not even done! They√¢re aiming for 50 million galaxies once the survey is complete.&lt;/p&gt;
    &lt;p&gt;And let me tell you, those robotically controlled fiber optic cables are a huge game changer. In many ways DESI is the successor to an older survey, the Sloan Digital Sky Survey. That survey had a similar setup, except that instead of robots to move all those fibers every night, they had to use grad students. Probably cheaper, but still less efficient. (Note that I was never one of those unlucky √¢volunteers√¢ but I did hear horror stories.)&lt;/p&gt;
    &lt;p&gt;Sure, the DESI survey is less than 1% of all the galaxies in the observable volume of the cosmos, but it√¢s still pretty sizable. So what do you do with a map of a decent chunk of the entire universe?&lt;/p&gt;
    &lt;p&gt;I√¢m glad you didn√¢t ask, because I√¢m happy to answer. The arrangements of galaxies on very large scales tells us a lot about the universe. And one of the key things used in this new DESI analysis is a feature of the large-scale universe goes by the ungainly but super nerdy name of baryon acoustic oscillations, or BAO for short.&lt;/p&gt;
    &lt;p&gt;Check this out. Long ago the universe was much smaller, hotter, and denser than it is today. If you√¢re ever asked what the big bang theory is all about, that√¢s pretty much it in a nutshell. In fact, billions of years ago, when the universe was only a few hundred thousand years old, it was so hot and dense (for those of you keeping score at home, a million times smaller than its present volume and thousands of degrees hotter) that all the matter was crammed together in the form of an energized plasma. This is the same state of matter as the body of the Sun or a lightning bolt, and it literally filled the universe.&lt;/p&gt;
    &lt;p&gt;Like any dense material, there were sound waves √¢ waves of pressure that crisscrossed the universe. Many of these sound waves were triggered by a competition between gravity and radiation. Dense clumps of matter would try to collapse under their own gravity, but then those clumps would get hot and the radiation they emitted would push them back out.&lt;/p&gt;
    &lt;p&gt;This seesawing effect went on and on, back and forth, until the plasma cooled down so much that the light was released. This meant that radiation could no longer play the game, and the back-and-forth sound waves got stuck mid seesaw. Wherever they were, they acted as a source of additional gravitation, a shell of slightly higher density.&lt;/p&gt;
    &lt;p&gt;In fact we even have pictures of these features, which are the baryon acoustic oscillations (or √¢super hot sound waves√¢ if you prefer). The light that was emitted when this process stopped still exists today, and we can take pictures of it. It√¢s called the cosmic microwave background, and a decade ago when a bunch of my friends were plugging away their fiber optic cables, I was a member of the Planck collaboration, which was a satellite to map the microwave background.&lt;/p&gt;
    &lt;p&gt;These shells of extra matter didn√¢t just go away. They stuck around, and slowly slowly slowly over billions of years more matter accumulated on those shells than the surrounding regions. Today, we see the imprint of the BAO in the form of shells of matter roughly 800 million light-years in diameter.&lt;/p&gt;
    &lt;p&gt;The cool part about all this is that the shells are what√¢s called a standard ruler. We know how big the shells are supposed to be √¢ it√¢s a relatively straightforward calculation to transport the images we see in the microwave background to their sizes in the present day. And we can compare that expected value to how big they appear on the sky. And how big they appear on the sky depends on cosmology: on the properties, history, and evolution of the universe.&lt;/p&gt;
    &lt;p&gt;The new finding is that the BAO shells found by DESI are a little off. Their sizes don√¢t quite fit with our usual picture of cosmology. And they seem to fit better a picture of the universe where dark energy is evolving.&lt;/p&gt;
    &lt;p&gt;But what the heck is dark energy, and why is it so interesting that it might be evolving?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.universetoday.com/articles/desis-dizzying-results"/><published>2025-11-17T17:39:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955921</id><title>EEG-based neurofeedback in athletes and non-athletes</title><updated>2025-11-17T20:12:30.121891+00:00</updated><content>&lt;doc fingerprint="a5b1b1b2902b05ea"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Background: Electroencephalography (EEG) is a non-invasive technique that records millisecond-scale cortical electrical activity using scalp electrodes. In EEG-based neurofeedback (NFB), these signals are processed to provide real-time feedback that supports self-regulation of targeted brain rhythms; evidence suggests improvements in cognitive and neurophysiological performance in athletes and non-athletes. However, methodological inconsistencies‚Äîsuch as limited blinding, poor sham control, and outdated approaches to EEG spectral analysis‚Äîrestrict reproducibility and hinder cumulative progress in the field. Methods: This scoping review aimed to identify and analyze the methodological characteristics, outcome measures, and reproducibility gaps in EEG-based NFB studies involving athletes and non-athletes. Following PRISMA-ScR guidelines, we systematically searched academic databases (PubMed, Embase, Scopus, Web of Science, PsycINFO, and Cochrane Library), as well as gray literature sources (ProQuest Dissertations, LILACS, Tripdatabase, and Google Scholar). Of 48 included studies, 44 were published in international peer-reviewed journals and 4 in regional journals. Data were extracted on study design, participant population, NFB protocols, targeted EEG rhythms, cognitive and neurophysiological outcomes, and methodological rigor. Results: The review revealed substantial heterogeneity in targeted rhythms, protocols, and reporting standards. None of the studies employed modern spectral parameterization methods (e.g., FOOOF), while only 29% used active sham protocols and 6% employed inert sham conditions. Reporting blinding procedures and follow-up assessments was limited or absent in most studies. Discussion: This review highlights critical methodological shortcomings that may bias interpretations of NFB effects in sport and cognitive domains. To strengthen future research, studies should rigorously implement sham and blinding procedures, ensure transparent reporting of EEG metrics, and adopt open-science practices, including modern approaches to spectral parameterization.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Introduction&lt;/head&gt;
    &lt;p&gt;Electroencephalogram-based neurofeedback (EEG-NFB) has emerged as a promising non-invasive intervention to enhance cognitive and psychophysiological functioning, including attention, emotion regulation, and motor preparation [,,,]. In sports, NFB is increasingly applied to improve performance under pressure and support resilience across disciplines such as football, archery, judo, and swimming [].&lt;/p&gt;
    &lt;p&gt;Despite encouraging findings, such as improvements in attention, emotion regulation, and athletic performance reported in previous EEG-NFB studies [,,,], the current evidence is constrained by major methodological limitations. In the sports context, EEG-NFB has been increasingly applied to enhance attentional focus, optimize sensorimotor rhythm regulation, and support stress management during competition. Studies have reported improvements in accuracy (e.g., archery, shooting), faster reaction times, and decision-making in football, highlighting its potential to strengthen both cognitive and motor domains in athletes [,,,,,,,,,,,].&lt;/p&gt;
    &lt;p&gt;Studies differ in protocol duration, targeted brain regions, and outcome measures, while most lack rigorous sham or double-blind designs, raising concerns about expectancy and placebo effects [,]. Such inconsistencies limit the attribution of NFB-induced changes to genuine neurophysiological mechanisms.&lt;/p&gt;
    &lt;p&gt;To address these challenges, the CRED-nf checklist [] established standards for study design and reporting, including preregistration, detailed feedback specifications, sham controls (active and inert), and transparent reporting of outcomes. However, adherence remains inconsistent, and many studies still rely on closed-source analysis pipelines. Proprietary implementations of Fast Fourier Transform (FFT) parameters‚Äîsuch as window length or artifact rejection‚Äîare rarely disclosed, undermining reproducibility [,].&lt;/p&gt;
    &lt;p&gt;Notably, none of the reviewed studies employed modern spectral parameterization approaches, such as Fitting Oscillations &amp;amp; One-Over-F (FOOOF []), which separate periodic and aperiodic components to strengthen neurophysiological validity. This methodological gap is especially critical for sports applications, where subtle cognitive and performance-related changes demand precise measurement [,].&lt;/p&gt;
    &lt;p&gt;Finally, the drive for ecological validity‚Äîdefined as the extent to which experimental findings can be generalized to real-world contexts‚Äîhas led to portable EEG systems and semi-natural group protocols [,,,]. These approaches aim to capture cognitive and neurophysiological processes in more authentic environments, such as practice and competition, thereby increasing the applicability of research findings. While promising, they also introduce procedural challenges, including greater susceptibility to noise and artifacts, which require careful methodological control.&lt;/p&gt;
    &lt;p&gt;The aim of this scoping review is to systematically map methodological and analytical gaps in EEG-NFB studies, evaluate the current state of interventions in both athletes and non-athletes, and identify priorities for advancing transparency, reproducibility, and neurophysiological validity in future research.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Materials and Methods&lt;/head&gt;
    &lt;head rend="h3"&gt;2.1. Protocol and Registration&lt;/head&gt;
    &lt;p&gt;This scoping review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR []). The protocol was developed a priori, following PRISMA-P guidelines [] and methodological recommendations from the Joanna Briggs Institute [].&lt;/p&gt;
    &lt;p&gt;To ensure transparency, reproducibility, and methodological rigor, the review was prospectively registered on the Open Science Framework (OSF). The protocol defines the eligibility criteria, outlines procedures for study selection, data extraction, and synthesis, and specifies the use of Rayyan software for independent screening by two reviewers [].&lt;/p&gt;
    &lt;p&gt;The complete protocol is publicly accessible under the DOI registration: https://doi.org/10.17605/OSF.IO/XCUWY (accessed on 22 October 2025).&lt;/p&gt;
    &lt;head rend="h3"&gt;2.2. Eligibility Criteria&lt;/head&gt;
    &lt;p&gt;The research question was developed using the PCC framework‚ÄîPopulation, Concept, and Context []. The target population included adults ‚â• 18 years, divided into three groups: (i) elite athletes, training and competing at professional or international levels, with weekly physical activity typically &amp;gt;9 METs (vigorous-intensity []; (ii) amateur athletes, engaged in regular but non-professional sports practice, typically 3‚Äì9 METs; and (iii) non-athletes, healthy adults without organized sport participation, typically &amp;lt;3 METs. These classifications were based on the Compendium of Physical Activities [].&lt;/p&gt;
    &lt;p&gt;The concept focused on EEG-NFB as the primary intervention. Only studies reporting objective neurophysiological or cognitive outcomes were eligible (e.g., event-related potentials (ERP), quantitative electroencephalography (qEEG), low-resolution electromagnetic tomography (LORETA); or validated measures of attention, working memory, reaction time). Studies based solely on self-reported questionnaires or satisfaction ratings were excluded.&lt;/p&gt;
    &lt;p&gt;The context included sports and laboratory settings. Eligible studies could come from any country, year, or language, provided full-text access and accurate translation into English or Portuguese.&lt;/p&gt;
    &lt;p&gt;Only original empirical studies were included randomized controlled trials (RCTs), quasi-experimental, cohort, observational, qualitative, or mixed-method studies with explicit NFB interventions. Exclusions comprised systematic reviews, meta-analyses, theoretical papers, editorials, and conference abstracts‚Äîalthough these were screened for additional references.&lt;/p&gt;
    &lt;p&gt;Studies were excluded if they: (i) involved clinical populations, (ii) failed to report detailed NFB protocols or outcomes, (iii) assessed only non-specific effects (e.g., expectancy, placebo), or (iv) lacked peer-review. Full-text availability was mandatory.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.3. Information Sources and Search Strategy&lt;/head&gt;
    &lt;p&gt;The research team conducted an extensive literature search across seven academic databases: PubMed/MEDLINE, Embase, Scopus, Web of Science, PsycINFO, Cochrane Library, and LILACS. Both controlled vocabulary terms (MeSH, DeCS) and free-text terms were used to link NFB with EEG, cognitive performance, ERP, qEEG, and LORETA. Boolean operators and truncations were adapted for each database.&lt;/p&gt;
    &lt;p&gt;To minimize publication bias, grey literature sources were also searched, including Google Scholar, ProQuest Dissertations &amp;amp; Theses, Trip Database, and Dissertations Citation Index. Reference lists of included studies were hand-searched to identify additional articles.&lt;/p&gt;
    &lt;p&gt;No restrictions were applied regarding year, language, or country of origin, provided accurate translation could be ensured. References were deduplicated in EndNote X9 (Clarivate Analytics), and records were screened independently by two reviewers using Rayyan [].&lt;/p&gt;
    &lt;p&gt;The complete database search strategies, including the detailed list of keywords applied, are reported in the Supplementary Materials (Appendix S1).&lt;/p&gt;
    &lt;head rend="h3"&gt;2.4. Study Selection Process&lt;/head&gt;
    &lt;p&gt;The Rayyan system (Qatar Computing Research Institute) was employed to conduct the study selection process in two phases. In the first phase, two independent reviewers (RZ and TB) screened titles and abstracts against the eligibility criteria. In the second phase, the same reviewers conducted a full-text evaluation of studies that passed the initial screening. Disagreements were resolved through discussion, and when consensus could not be reached, a third reviewer (IBB) acted as arbitrator.&lt;/p&gt;
    &lt;p&gt;Additionally, the reference lists of included studies were manually reviewed to identify further eligible records. The entire selection process was documented through the PRISMA-ScR flow diagram, including reasons for exclusion during the full-text stage. To enhance methodological rigor, the procedure underwent independent double verification, thereby ensuring transparency, reliability, and reproducibility.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.5. Data Charting Process and Data Items&lt;/head&gt;
    &lt;p&gt;The first reviewer (RZ) independently performed the data charting process using a structured extraction form based on the PCC framework. A second reviewer (TB) verified all extracted data, while the first reviewer (RZ) and a third reviewer (IBB) resolved any discrepancies through discussion.&lt;/p&gt;
    &lt;p&gt;The extraction process included study characteristics (authors, year, country, study design), population details (type of participants: elite athletes, amateur athletes, or non-athletes; age range; gender distribution; and level of competition), characteristics of the NFB intervention (protocol type, frequency band, number and duration of sessions), neurophysiological assessment tools (ERP, qEEG, LORETA), cognitive outcome measures, and methodological aspects. In addition, although not pre-specified in the initial charting form, all studies were systematically reviewed for the use of modern spectral parameterization methods (e.g., FOOOF []) and for transparency and reproducibility practices (e.g., pre-registration, data sharing, code availability, detailed reporting of analysis pipelines; cf. [,]). These exploratory assessments were included to provide further insight into the analytical and methodological rigor of NFB research in sport.&lt;/p&gt;
    &lt;p&gt;The evaluation also focused on methodological strength through an assessment of sham controls (none, Active, or Inert), blinding procedures, and statistical approaches. Following [,,], sham controls were operationally categorized as Active Sham‚Äînon-contingent but plausible feedback (e.g., pre-recorded EEG or randomized signals)‚Äîand Inert Sham, fully decoupled from participants‚Äô physiological activity (e.g., random tones or pre-recorded videos). This classification allows for a clearer evaluation of methodological rigor, reducing the risk of conflating non-specific engagement effects with genuine NFB-related changes. A summary of the distribution of sham control types across studies is presented in the Section 3 (Figure 4) to providing a visual overview of this critical methodological factor.&lt;/p&gt;
    &lt;p&gt;When any essential information was unclear or unavailable, the corresponding study authors were contacted by email. However, response rates were limited, and missing data were coded as ‚Äúnot reported‚Äù.&lt;/p&gt;
    &lt;p&gt;Finally, the entire process was piloted on five studies to ensure consistency and clarity in the data extraction procedure, with particular emphasis on identifying the presence and type of sham controls as a critical methodological factor.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.6. Synthesis of Results&lt;/head&gt;
    &lt;p&gt;The research findings will be summarized in tables that organize data by study design, population type (elite athletes, amateur athletes, non-athletes), and characteristics of the NFB protocols, as well as neurophysiological and cognitive outcomes.&lt;/p&gt;
    &lt;p&gt;A narrative synthesis will be conducted to highlight methodological trends, outcome patterns, and evidence gaps across studies. The synthesis will remain descriptive in nature, consistent with the scoping review methodology.&lt;/p&gt;
    &lt;p&gt;The analysis will also quantify the frequency of key methodological variables, including the presence and type of sham controls, blinding procedures, and EEG analysis techniques (e.g., qEEG, ERP, LORETA). These distributions will be reported to provide a structured overview of methodological rigor and transparency across the included studies.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Results&lt;/head&gt;
    &lt;p&gt;The scoping review analyzed 48 studies that examined EEG-based NFB interventions among athletes competing in various sports and at different competitive levels. The PRISMA 2020 flow diagram (Figure 1) illustrates the study selection process from identification through screening to final inclusion.&lt;/p&gt;
    &lt;p&gt; Figure 1. PRISMA-ScR flow diagram showing the identification, screening, and inclusion of the studies included in this scoping review. * Records identified from the listed electronic databases. ** Records excluded after title and abstract screening. &lt;/p&gt;
    &lt;p&gt;The included studies investigated athletes from a wide range of sports (e.g., archery, golf, gymnastics, swimming, soccer, judo, and chess) across multiple countries and competitive levels (elite, amateur, and novice).&lt;/p&gt;
    &lt;p&gt;The study characteristics are summarized in Table 1, which provides detailed information on authors, publication years, sample sizes, demographic characteristics, sport disciplines, study designs, electrode placement protocols, control group types, outcome measures, and reported intervention effects.&lt;/p&gt;
    &lt;p&gt; Table 1. (A) Studies with Active Sham. Presents studies with Active Sham (feedback from pre-recorded EEG or randomized signals). (B) Studies with Inert Sham. Shows studies that used feedback completely unrelated to EEG (Inert Sham). (C) Studies without Sham. Includes studies with no form of sham (only passive control groups or pre-post designs). &lt;/p&gt;
    &lt;p&gt;The extensive information presented in Table 1 serves as the primary reference for understanding the diversity and methodological scope of the included studies.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.1. Selection of Sources of Evidence&lt;/head&gt;
    &lt;p&gt;The initial database search retrieved 3516 records, supplemented by an additional 240 records from gray literature and other sources. After removing 1737 duplicates, 1779 records remained for title and abstract screening. Of these, 1729 records were excluded based on eligibility criteria.&lt;/p&gt;
    &lt;p&gt;The full-text evaluation was conducted for 70 articles, of which 48 studies met the inclusion criteria and were included in the review. This corresponds to approximately 2.6% of the initially retrieved records.&lt;/p&gt;
    &lt;p&gt;The detailed selection process is presented in the PRISMA 2020 flow diagram (Figure 1). Reasons for full-text exclusion are documented in Supplementary Materials (Appendix S2), covering the 22 excluded studies.&lt;/p&gt;
    &lt;p&gt;Additionally, the distribution of electrode sites and frequency bands across the included studies is summarized in Figure 2.&lt;/p&gt;
    &lt;p&gt;Reason for exclusion at full-text stage:&lt;/p&gt;
    &lt;p&gt;Reason 1. Studies involving clinical populations (e.g., neurological or psychiatric diagnoses).&lt;/p&gt;
    &lt;p&gt;Reason 2. Studies lacking methodological detail on the neurofeedback protocol or outcomes.&lt;/p&gt;
    &lt;p&gt;Reason 3. Studies relying exclusively on subjective outcomes (e.g., self-perceived performance).&lt;/p&gt;
    &lt;p&gt;Reason 4. Non-peer-reviewed publications (e.g., conference abstracts, opinion articles, technical reports).&lt;/p&gt;
    &lt;p&gt;Reason 5. Full text not available or no response from corresponding authors after three contact attempts (within a three-week period).&lt;/p&gt;
    &lt;p&gt;The study selection process followed the PRISMA 2020 guidelines [], as illustrated in Figure 1.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.2. Characteristics of Included Studies&lt;/head&gt;
    &lt;p&gt;The included studies were conducted across 18 countries, with Poland contributing the largest share (24%), followed by Iran (18%) and Taiwan (12%). Other countries, including Germany, Portugal, and Canada, provided smaller but noteworthy contributions (Figure 3A,B).&lt;/p&gt;
    &lt;p&gt; Figure 3. Characteristics of included studies: (A) Geographic distribution; (B) Country contributions by percentage; (C) Study design classification. &lt;/p&gt;
    &lt;p&gt;In terms of research design, randomized controlled trials (RCTs) accounted for 60% of the studies, followed by quasi-experimental designs (29%) and case or single-subject approaches (11%) (Figure 3C). Most studies recruited participants ranging from novice to elite athletes, with males representing 77% of the total sample.&lt;/p&gt;
    &lt;p&gt;Control group strategies showed considerable variability: Active Sham conditions were used in 29% of studies, passive controls with no intervention in 33%, and no-control designs (e.g., pre‚Äìpost or single-subject studies) in 38% (Figure 4 and Figure 5).&lt;/p&gt;
    &lt;p&gt; Figure 4. Proportion of studies using different sham control types (no sham, active sham, and inert sham). &lt;/p&gt;
    &lt;p&gt; Figure 5. Distribution of control group types across included studies (sham feedback, passive control, no control). &lt;/p&gt;
    &lt;p&gt;As shown in Figure 6, SMR-based training (12‚Äì15 Hz) was the most frequently applied protocol, followed by theta/beta and alpha-based modulation. This pattern underscores the predominance of SMR approaches in sports-related EEG-NFB research, reflecting their established association with motor control and attentional regulation. At the same time, the relatively lower prevalence of infra-low frequency, mu, and customized alpha- or ERP-based protocols highlights emerging directions that remain underrepresented in the current literature.&lt;/p&gt;
    &lt;p&gt; Figure 6. Neurofeedback protocols used across included studies (SMR, theta/beta, alpha, ILF, um rhythm, custom). &lt;/p&gt;
    &lt;head rend="h3"&gt;3.3. Neurophysiological Outcomes&lt;/head&gt;
    &lt;p&gt;Neurophysiological outcomes were reported in 52% (n = 25) of the included studies. The reported effects encompassed EEG spectral power changes, such as sensorimotor rhythm (SMR) enhancement at Cz (located at the vertex of the scalp, approximately over the sensorimotor cortex), and at C3 and C4 (positioned over the left and right primary cortices, respectively). Other studies examined ERPs, particularly components such as P3 and N2 [], as well as coherence and connectivity measures derived from source localization techniques, including LORETA and sLORETA [].&lt;/p&gt;
    &lt;p&gt;Studies that combined neurophysiological measures with behavioral assessments frequently reported associations between EEG changes and improvements in motor or cognitive performance [,]. The distribution of studies focusing on neurophysiological outcomes, compared with those relying exclusively on behavioral or cognitive assessments, is illustrated in Figure 7.&lt;/p&gt;
    &lt;p&gt; Figure 7. Proportion of studies reporting neurophysiological outcomes versus behavioral-only measures. &lt;/p&gt;
    &lt;head rend="h3"&gt;3.4. Cognitive Outcomes&lt;/head&gt;
    &lt;p&gt;Cognitive outcomes were reported in 89% (n = 43) of the included studies. The research primarily targeted three cognitive domains: attention, working memory, and executive functions. These were assessed through standardized paradigms such as inhibition tasks (e.g., Stroop test), working memory updating (e.g., N-back task), and cognitive flexibility/set-shifting (e.g., Oddball paradigm).&lt;/p&gt;
    &lt;p&gt;Standardized neuropsychological assessments‚Äîparticularly the N-back task, Stroop test, and Oddball paradigm‚Äîwere frequently complemented with sport-specific tasks, including reaction time tests in archery or golf putting performance, to evaluate cognitive improvements in real-world contexts.&lt;/p&gt;
    &lt;p&gt;Overall, the evidence indicated consistent cognitive benefits of neurofeedback training. For instance, studies highlighted improvements in attentional control [,], while others reported enhanced stress regulation and self-perceived mental readiness [,].&lt;/p&gt;
    &lt;head rend="h3"&gt;3.5. Methodological Features&lt;/head&gt;
    &lt;p&gt;The increasing focus on methodological rigor is reflected in the gradual adoption of randomized controlled trials (RCTs). Nevertheless, only 29% of studies (n = 14) included Active Sham feedback as a placebo control [,]. The majority of these Active Sham protocols (n = 15) relied on pre-recorded EEG data or randomized signals, which may still introduce unspecific neuroplastic changes [,]. Only three studies applied Inert Sham protocols that fully separated neural activity from feedback [,,], representing the methodological gold standard for identifying neurofeedback-specific effects. Overall, approximately 40% of studies did not implement any sham control, relying on passive or no-control designs.&lt;/p&gt;
    &lt;p&gt;Beyond sham design, most studies did not report participant or evaluator blinding, and long-term follow-up assessments were rare (exceptions include [,]). These limitations further underscore the need for methodological consistency and transparency in EEG-NFB research.&lt;/p&gt;
    &lt;p&gt;Another critical issue concerns EEG spectral analysis. None of the 48 included studies applied modern spectral parameterization techniques such as FOOOF; []. Instead, all relied on conventional band-power approaches based on fixed frequency bands (e.g., SMR, alpha, theta, beta), typically calculated via Fast Fourier Transform (FFT). Some studies applied visual or manual inspection of EEG signals, and reporting of spectral analysis parameters was often incomplete. This reliance on traditional band-power metrics prevents separation of periodic oscillatory activity from the aperiodic 1/f background, which may bias the interpretation of NFB effects.&lt;/p&gt;
    &lt;p&gt;This methodological heterogeneity underscores the challenges of synthesizing evidence across studies and highlights the importance of adopting standardized protocols and transparent reporting practices. Figure 5, Figure 6 and Figure 7 illustrate these methodological inconsistencies, emphasizing the lack of sham standardization, limited neurophysiological outcome reporting, and the predominance of outdated spectral approaches.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.6. Transparency and Reproducibility&lt;/head&gt;
    &lt;p&gt;The analysis of the 48 included studies revealed a systemic absence of open science practices. None of the studies provided data sharing, code availability, or preregistration. A single exception was noted in [], which reported protocol approval by a local ethics committee prior to data collection; however, this does not constitute preregistration in the open science sense, as it lacked public accessibility and methodological detail.&lt;/p&gt;
    &lt;p&gt;Although most studies described their training protocols (e.g., electrode sites, frequency bands, session structures), independent replication remained unfeasible due to reliance on proprietary hardware/software and closed-source algorithms. In addition, statistical transparency was limited: the majority of studies reported only p-values, with rare mentions of effect sizes or confidence intervals, thereby constraining interpretability.&lt;/p&gt;
    &lt;p&gt;With respect to EEG analysis, all studies relied on traditional band-power metrics in fixed frequency bands. None applied modern spectral parameterization methods such as FOOOF [], which separate periodic oscillatory activity from the aperiodic 1/f background. This reliance on fixed-band approaches‚Äîoften embedded in commercial systems‚Äîfurther restricts the neurophysiological validity of reported outcomes.&lt;/p&gt;
    &lt;p&gt;Taken together, these findings align with concerns raised by [,], highlighting the urgent need for open data, shared code, preregistration, and transparent reporting of analytic pipelines to ensure reproducibility and credibility in EEG-NFB research.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Discussion&lt;/head&gt;
    &lt;p&gt;This scoping review synthesized 48 studies examining EEG-based NFB interventions across athletic and non-athletic populations. The evidence generally supports the potential of NFB to modulate neurophysiological activity and improve cognitive and performance-related outcomes. However, the review also exposes substantial methodological heterogeneity and reproducibility gaps that complicate interpretation and cross-study comparison. The following sections discuss these findings considering previous literature, highlighting consistent trends, discrepancies, and future research needs.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.1. Neurophysiological and Cognitive Outcomes&lt;/head&gt;
    &lt;p&gt;Across the analyzed studies, NFB training most frequently targeted SMR and alpha bands, with reported increases in EEG power often corresponding to improvements in reaction time, attention, and motor precision. These findings align with early work by [,], who demonstrated that modulating SMR and alpha activity could facilitate motor preparation and cognitive stability. Similarly, more recent studies‚Äîsuch as [,]‚Äîconfirmed enhanced motor accuracy and balance following SMR- and theta/beta-based training, supporting the link between neural regulation and performance optimization.&lt;/p&gt;
    &lt;p&gt;Nevertheless, not all evidence converges. Some experiments, such as [], reported null effects on reaction time or inconsistent EEG modulation, suggesting that task specificity, participant expertise, and feedback parameters critically influence outcomes. Cognitive measures‚Äîparticularly attention, working memory, and executive control‚Äîwere the most frequently improved domains, in line with systematic syntheses by [,]. Yet, the diversity of testing paradigms (e.g., Stroop, N-back, Oddball) and the predominance of short-term assessments limit the generalization of these results. Overall, the current evidence indicates that EEG-NFB can induce measurable neural and behavioral adaptations, though magnitude and persistence remain uncertain due to methodological inconsistency.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.2. The Role of Sham Controls&lt;/head&gt;
    &lt;p&gt;A central concern identified in this review involves the design and implementation of sham controls. As defined in Section 2.5, Active Sham refers to non-contingent but plausible feedback, whereas Inert Sham is fully decoupled from participants‚Äô physiological activity. Only 29% of studies employed active sham feedback, and a mere 6% applied inert sham protocols‚Äîthe methodological gold standard for isolating true NFB-specific effects. This distinction aligns with the CRED-nf recommendations [] and prior methodological reviews [], which emphasize the need for transparent reporting of sham procedures in EEG-NFB research. These proportions mirror the shortcomings previously highlighted by [], who emphasized that expectancy and engagement effects may inflate apparent efficacy in NFB research. The scarcity of inert sham conditions observed here suggests that many studies risk conflating neurophysiological change with non-specific psychological factors.&lt;/p&gt;
    &lt;p&gt;Furthermore, a large subset of studies lacked participant or assessor blinding and relied solely on pre‚Äìpost comparisons. Such designs increase susceptibility to placebo effects and Type III statistical errors, as discussed by []. When properly implemented, double-blind randomized trials‚Äîsuch as those by [] or []‚Äîdemonstrated more controlled evidence for EEG modulation and performance enhancement. Future investigations should therefore integrate both active and inert sham conditions, coupled with rigorous blinding, to strengthen internal validity and permit clearer attribution of causal effects.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.3. Electrode and Frequency Variability in Neurofeedback Protocols&lt;/head&gt;
    &lt;p&gt;The diversity of electrode montages and targeted frequency bands across the reviewed studies reflects the absence of standardized NFB protocols in sport settings. Central sites (Cz, C3, C4) were predominant in SMR-based interventions, consistent with their functional relevance to motor preparation and attention. However, frontal and parietal placements targeting alpha, beta, or theta activity were also frequent, often motivated by exploratory aims rather than established neurophysiological models. Comparable variability was reported by [,], who noted that inconsistency in training loci and spectral ranges impedes replication and cumulative synthesis.&lt;/p&gt;
    &lt;p&gt;This heterogeneity complicates the interpretation of EEG changes and performance outcomes. Even when similar behavioral gains were reported, underlying neural mechanisms may differ due to protocol divergence. Standardized reporting through frameworks such as the CRED-nf checklist [] and adoption of modern spectral parameterization tools like FOOOF [] would allow more accurate separation of oscillatory and aperiodic components, thereby improving cross-study comparability and theoretical precision.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.4. Ecological Validity and Implementation Challenges&lt;/head&gt;
    &lt;p&gt;Recent studies increasingly integrate portable EEG systems and field-based protocols to enhance ecological validity and bridge laboratory findings with real-world athletic contexts. Investigations by [,] illustrate this trend, demonstrating that brief, on-site SMR training sessions can positively influence golf and soccer performance. These developments parallel broader efforts in applied neuroscience to situate cognitive training within authentic performance environments.&lt;/p&gt;
    &lt;p&gt;However, ecological validity introduces methodological complexity. Field-based EEG is inherently vulnerable to motion artifacts, environmental noise, and fatigue effects that can compromise data quality and mask the specific contribution of NFB. Moreover, most reviewed studies relied on short-term pre‚Äìpost designs without longitudinal follow-up, precluding conclusions about retention or transfer of NFB benefits. Sustained improvements, as observed in long-term follow-ups by [], remain rare but essential for verifying whether NFB-induced adaptations persist beyond initial training phases. Future research should therefore combine controlled laboratory paradigms with extended, ecologically grounded interventions, using multimodal outcome measures (EEG, qEEG, ERP, behavioral, and psychophysiological indices) to achieve comprehensive evaluation.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.5. Considerations for Future Research&lt;/head&gt;
    &lt;p&gt;To consolidate the evidence base for EEG-NFB in sport, future investigations must emphasize methodological rigor, analytical transparency, and ecological realism. Randomized controlled trials incorporating both active and inert sham conditions are imperative to distinguish genuine neurofeedback effects from non-specific influences. Protocol standardization regarding electrode placement, targeted frequency bands, and session parameters will facilitate replication and meta-analytic synthesis.&lt;/p&gt;
    &lt;p&gt;Equally crucial is the transition toward open-science practices. None of the reviewed studies preregistered protocols or shared data and analysis code, reflecting a broader reproducibility gap in applied neuroscience [,]. Adopting preregistration, data sharing, and transparent reporting of analytic pipelines will markedly enhance credibility and cumulative progress. Aligned with the CRED-nf checklist [], future EEG-NFB studies should explicitly preregister core components of their experimental design, including hypotheses, primary and secondary outcomes, session parameters, and planned statistical analyses. Minimal datasets‚Äîsuch as pre-processed EEG spectra, behavioral measures, and analytic scripts‚Äîshould be made openly available in public repositories (e.g., OSF, Zenodo, OpenNeuro). Moreover, the transparent reporting of key signal-processing parameters (e.g., FFT settings, filter characteristics, artifact rejection thresholds, and reinforcement schedules) will facilitate methodological reproducibility and cross-study comparability. Collectively, these practices will transform general calls for transparency into concrete, actionable standards for advancing open science in EEG-NFB research.&lt;/p&gt;
    &lt;p&gt;Finally, research should expand participant diversity‚Äîaddressing gender balance and sport variety‚Äîand include longitudinal follow-ups to determine the durability and ecological transfer of NFB-induced performance gains. By integrating these methodological and conceptual refinements, future studies can transform EEG-NFB from a promising experimental approach into a reproducible, evidence-based tool for optimizing human performance.&lt;/p&gt;
    &lt;p&gt;Another potential methodological concern involves the partial overlap of samples across studies conducted by the same research groups (e.g., [,,,,]). Such overlap may inflate the apparent evidence base and reduce the effective sample diversity, particularly when similar participant cohorts are repeatedly analyzed under slightly modified protocols. This limitation should be considered when interpreting the overall findings, as it may bias outcome generalizability and overestimate the robustness of specific training effects. Future reviews should therefore apply stricter data-source screening procedures and explicitly report instances of potential sample duplication to enhance the transparency and reproducibility of evidence synthesis in EEG-NFB research.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Conclusions&lt;/head&gt;
    &lt;p&gt;EEG-based NFB demonstrates meaningful potential to enhance both neurophysiological regulation and cognitive-motor performance in athletes. Yet, this promise remains constrained by inconsistent methodology, limited sham control, and insufficient transparency. The field now requires rigorously designed, double-blind randomized trials using validated sham procedures and standardized spectral analyses to establish causal validity.&lt;/p&gt;
    &lt;p&gt;Future progress depends equally on adopting open-science principles‚Äîpreregistration, data and code sharing, and clear protocol reporting‚Äîto ensure replicability and comparability across studies. Long-term, ecologically valid designs will clarify whether short-term NFB effects translate into sustainable performance benefits. Strengthening methodological rigor and transparency will not only improve scientific reproducibility but also enable NFB to fulfill its potential as a practical tool in sport neuroscience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Supplementary Materials&lt;/head&gt;
    &lt;p&gt;The following supporting information can be downloaded at: https://www.mdpi.com/article/10.3390/bioengineering12111202/s1.&lt;/p&gt;
    &lt;head rend="h2"&gt;Author Contributions&lt;/head&gt;
    &lt;p&gt;Conceptualization, R.M.G.Z.; methodology, R.M.G.Z. and D.T.B.; validation, R.M.G.Z., D.T.B. and I.B.-B.; formal analysis, R.M.G.Z.; investigation, R.M.G.Z.; resources, J.M.C.; data curation, R.M.G.Z.; writing‚Äîoriginal draft preparation, R.M.G.Z.; writing‚Äîreview and editing, J.M.C., D.T.B., I.B.-B. and S.N.d.J.; visualization, R.M.G.Z.; supervision, J.M.C., I.B.-B. and S.N.d.J.; project administration, J.M.C.; funding acquisition, I.B.-B. All authors have read and agreed to the published version of the manuscript.&lt;/p&gt;
    &lt;head rend="h2"&gt;Funding&lt;/head&gt;
    &lt;p&gt;This research was funded by the European Union‚Äôs Horizon Europe Programme, Grant Agreement No. 101089757‚ÄîSEA-EU 2.0: The European University of the Seas Alliance navigating towards modern and co-transformative intercampus life; people-driven, planet-friendly, and knowledge-based progress for all, funded by the European Union. Additional financial support from the University of Gda≈Ñsk is gratefully acknowledged.&lt;/p&gt;
    &lt;head rend="h2"&gt;Institutional Review Board Statement&lt;/head&gt;
    &lt;p&gt;Not applicable. This study is a scoping review that analyzed and synthesized data from previously published studies; therefore, ethical approval was not required.&lt;/p&gt;
    &lt;head rend="h2"&gt;Informed Consent Statement&lt;/head&gt;
    &lt;p&gt;Not applicable. This study did not involve humans or animals.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data Availability Statement&lt;/head&gt;
    &lt;p&gt;All data supporting the findings of this study are included within the article and its Supplementary Materials.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;The authors acknowledge the use of artificial intelligence tools exclusively to support language clarity and text organization. All scientific content, methodological design, analysis, and interpretation are the sole responsibility of the authors.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conflicts of Interest&lt;/head&gt;
    &lt;p&gt;The authors declare no conflicts of interest.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Schomer, D.L.; Da Silva, F.H.L. (Eds.) Niedermeyer‚Äôs Electroencephalography: Basic Principles, Clinical Applications, and Related Fields, 7th ed.; Oxford University Press: New York, NY, USA, 2018. [Google Scholar]&lt;/item&gt;
      &lt;item&gt;Gruzelier, J.H. EEG-neurofeedback for optimising performance. I: A review of cognitive and affective outcome in healthy participants. Neurosci. Biobehav. Rev. 2014, 44, 124‚Äì141. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Hammond, D.C. What is Neurofeedback: An Update. J. Neurother. 2011, 15, 305‚Äì336. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Tosti, B.; Corrado, S.; Mancone, S.; Di Libero, T.; Carissimo, C.; Cerro, G.; Rodio, A.; Furtado da Silva, V.; Coimbra, D.R.; Andrade, A.; et al. Neurofeedback training protocols in sports: A systematic review of recent advances in performance, anxiety, and emotional regulation. Brain Sci. 2024, 14, 1036. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Rydzik, ≈Å.; WsƒÖcz, W.; Ambrozy, T.; Javdanekh, N.; Rydzak, K.; Koparska, M. The use of neurofeedback in sports training: Systematic review. Brain Sci. 2023, 13, 660. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Landers, D.M.; Petruzzello, S.J.; Salazar, W.; Crews, D.J.; Kubitz, K.A.; Gannon, T.L.; Han, M. The influence of electrocortical biofeedback on performance in pre-elite archers. Med. Sci. Sports Exerc. 1991, 23, 123‚Äì129. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Pakta≈ü, Y. The effect of neurofeedback training on the perceptual-motor abilities of basketball athletes. Pak. J. Med. Health Sci. 2021, 15, 791‚Äì793. [Google Scholar]&lt;/item&gt;
      &lt;item&gt;Go≈Ça≈õ, A.; Nitychoruk, M.; ≈ªak, M.; Kowalczyk, M.; Ignatjeva, A.; Maszczyk, A. Optimizing visual processing efficiency using neurofeedback training in judo athletes. Arch. Budo Sci. Martial Arts Extrem. Sports 2019, 15, 105‚Äì112. [Google Scholar]&lt;/item&gt;
      &lt;item&gt;Krawczyk, M.; Kowalczyk, M.; ≈ªak, M.; Daros, K.; Gozdowski, P. Zmiany aktywno≈õci fal m√≥zgowych pod wp≈Çywem treningu neurofeedback u zawodnik√≥w judo. Ogrody Nauk. I Szt. 2019, 9, 388‚Äì399. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Maszczyk, A.; Dobrakowski, P.; Nitychoruk, M.; Zak, M.; Kowalczyk, M.; Toborek, M. The Effect of Neurofeedback Training on the Visual Processing Efficiency in Judo Athletes. J. Hum. Kinet. 2020, 71, 219‚Äì227. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Rijken, N.H.; Soer, R.; de Maar, E.; Prins, H.; Teeuw, W.B.; Peuscher, J.; Oosterveld, F.G. Increasing Performance of Professional Soccer Players and Elite Track and Field Athletes with Peak Performance Training and Biofeedback: A Pilot Study. Appl. Psychophysiol. Biofeedback 2016, 41, 421‚Äì430. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Liu, Y.S.; Subramaniam, S.C.H.; Sourina, O.; Shah, E.; Chua, J.; Ivanov, K. Neurofeedback training for rifle shooters to improve cognitive ability. In Proceedings of the 2017 International Conference on Cyberworlds (CW), Chester, UK, 20‚Äì22 September 2017; pp. 186‚Äì189. [Google Scholar]&lt;/item&gt;
      &lt;item&gt;Mikicin, M.; Mroz, A.; Karczewska-Lindinger, M.; Malinowska, K.; Mastalerz, A.; Kowalczyk, M. Effect of the Neurofeedback-EEG Training During Physical Exercise on the Range of Mental Work Performance and Individual Physiological Parameters in Swimmers. Appl. Psychophysiol. Biofeedback 2020, 45, 49‚Äì55. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Assadourian, S.; Branco Lopes, A.; Saj, A. Improvement in peripheral visual attentional performance in professional soccer players following a single neurofeedback training session. Rev. Neuropsychol. 2022, 14, 133‚Äì138. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Lo, L.C.; Hatfield, B.D.; Janjigian, K.; Wang, Y.S.; Fong, D.Y.; Hung, T.M. The Effect of Left Temporal EEG Neurofeedback Training on Cerebral Cortical Activity and Precision Cognitive-Motor Performance. Res. Q. Exerc. Sport 2024, 96, 486‚Äì496. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Hosseini, F.; Norouzi, E. Effect of neurofeedback training on self-talk and performance in elite and non-elite volleyball players. Med. Dello Sport 2017, 70, 344‚Äì353. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Salimnejad, Z.; Zandi, H.; Arsham, S. Effect of Bio-Neural Feedback Exercises on the Performance of Female Rugby Players. Int. J. Mot. Control Learn. 2019, 1, 10‚Äì18. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Bussalb, A.; Congedo, M.; Barth√©lemy, Q.; Ojeda, D.; Acquaviva, E.; Delorme, R.; Mayaud, L. Clinical and experimental factors influencing the efficacy of neurofeedback in ADHD: A meta-analysis. Front. Psychiatry 2019, 10, 35. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Thibault, R.; Raz, A. The psychology of neurofeedback: Clinical intervention even if applied placebo. Am. Psychol. 2017, 72, 679‚Äì688. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Ros, T.; Enriquez-Geppert, S.; Zotev, V.; Young, K.D.; Wood, G.; Whitfield-Gabrieli, S.; Wan, F.; Vuilleumier, P.; Vialatte, F.; Van De Ville, D.; et al. Consensus on the reporting and experimental design of clinical and cognitive-behavioural neurofeedback studies (CRED-nf checklist). Brain 2020, 143, 1674‚Äì1685. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Nichols, T.; Das, S.; Eickhoff, S.; Evans, A.; Glatard, T.; Hanke, M.; Kriegeskorte, N.; Milham, M.; Poldrack, R.; Poline, J.-B.; et al. Best practices in data analysis and sharing in neuroimaging using MRI. Nat. Neurosci. 2017, 20, 299‚Äì303. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Poldrack, R.; Baker, C.; Durnez, J.; Gorgolewski, K.; Matthews, P.; Munafo, M.; Nichols, T.; Poline, J.-B.; Vul, E.; Yarkoni, T. Scanning the horizon: Towards transparent and reproducible neuroimaging research. Nat. Rev. Neurosci. 2017, 18, 115‚Äì126. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Donoghue, T.; Haller, M.; Peterson, E.; Varma, P.; Sebastian, P.; Gao, R.; Noto, T.; Lara, A.; Wallis, J.; Knight, R.; et al. Parameterizing neural power spectra into periodic and aperiodic components. Nat. Neurosci. 2020, 23, 1655‚Äì1665. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Ring, C.; Cooke, A.; Kavussanu, M.; McIntyre, D.; Masters, R. Investigating the efficacy of neurofeedback training for expediting expertise and excellence in sport. Psychol. Sport Exerc. 2015, 16, 118‚Äì127. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;van Boxtel, G.J.M.; Denissen, A.; de Groot, J.A.; Neleman, M.S.; Vellema, J.; Hart de Ruijter, E.M. Alpha Neurofeedback Training in Elite Soccer Players Trained in Groups. Appl. Psychophysiol. Biofeedback 2024, 49, 589‚Äì602. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Wu, J.H.; Chueh, T.Y.; Yu, C.L.; Wang, K.P.; Kao, S.C.; Gentili, R.J.; Hatfield, B.D.; Hung, T.M. Effect of a single session of sensorimotor rhythm neurofeedback training on the putting performance of professional golfers. Scand. J. Med. Sci. Sports 2024, 34, e14540. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Wu, J.H.; Tu, Y.C.; Chang, C.Y.; Chueh, T.Y.; Gentili, R.J.; Hatfield, B.D.; Hung, T.M. A single session of sensorimotor rhythm neurofeedback enhances long-game performance in professional golfers. Biol. Psychol. 2024, 192, 108844. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Tricco, A.C.; Lillie, E.; Zarin, W.; O‚ÄôBrien, K.K.; Colquhoun, H.; Levac, D.; Moher, D.; Peters, M.D.J.; Horsley, T.; Weeks, L.; et al. PRISMA Extension for Scoping Reviews (PRISMA-ScR): Checklist and Explanation. Ann. Intern. Med. 2018, 169, 467‚Äì473. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Moher, D.; Shamseer, L.; Clarke, M.; Ghersi, D.; Liberati, A.; Petticrew, M.; Shekelle, P.; Stewart, L.A.; Group, P.-P. Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement. Syst. Rev. 2015, 4, 1. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Peters, M.D.J.; Marnie, C.; Tricco, A.C.; Pollock, D.; Munn, Z.; Alexander, L.; McInerney, P.; Godfrey, C.M.; Khalil, H. Updated methodological guidance for the conduct of scoping reviews. JBI Evid. Synth. 2020, 18, 2119‚Äì2126. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Ouzzani, M.; Hammady, H.; Fedorowicz, Z.; Elmagarmid, A. Rayyan‚ÄîA web and mobile app for systematic reviews. Syst. Rev. 2016, 5, 210. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Ainsworth, B.E.; Haskell, W.L.; Herrmann, S.D.; Meckes, N.; Bassett, D.R., Jr.; Tudor-Locke, C.; Greer, J.L.; Vezina, J.; Whitt-Glover, M.C.; Leon, A.S. 2011 Compendium of Physical Activities: A second update of codes and MET values. Med. Sci. Sports Exerc. 2011, 43, 1575‚Äì1581. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Rogala, J.; Jurewicz, K.; Paluch, K.; Kublik, E.; Cetnarski, R.; Wr√≥bel, A. The Do‚Äôs and Don‚Äôts of Neurofeedback Training: A Review of the Controlled Studies Using Healthy Adults. Front. Hum. Neurosci. 2016, 10, 301. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Trullinger, M.; Novian, A.; Russell-Chapin, L.; Pradhan, D. Perspectives on Type III Statistical Errors: Exaggerating the Effects of Placebo in Neurofeedback. NeuroRegulation 2019, 6, 38‚Äì41. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Dekker, M.K.; van den Berg, B.R.; Denissen, A.J.; Sitskoorn, M.M.; van Boxtel, G.J. Feasibility of eyes open alpha power training for mental enhancement in elite gymnasts. J. Sports Sci. 2014, 32, 1550‚Äì1560. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Cheng, M.Y.; Huang, C.J.; Chang, Y.K.; Koester, D.; Schack, T.; Hung, T.M. Sensorimotor Rhythm Neurofeedback Enhances Golf Putting Performance. J. Sport Exerc. Psychol. 2015, 37, 626‚Äì636. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Maszczyk, A.; Golas, A.; Pietraszewski, P.; Kowalczyk, M.; Cieszczyk, P.; Kochanowicz, A.; Smolka, W.; Zajac, A. Neurofeedback for the enhancement of dynamic balance of judokas. Biol. Sport 2018, 35, 99‚Äì102. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Kober, S.E.; Ninaus, M.; Witte, M.; Buchrieser, F.; Grossinger, D.; Fischmeister, F.P.S.; Neuper, C.; Wood, G. Triathletes are experts in self-regulating physical activity‚ÄîBut what about self-regulating neural activity? Biol. Psychol. 2022, 173, 108406. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Afrash, S.; Saemi, E.; Gong, A.; Doustan, M. Neurofeedback training and motor learning: The enhanced sensorimotor rhythm protocol is better or the suppressed alpha or the suppressed mu? BMC Sports Sci. Med. Rehabil. 2023, 15, 93. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Chen, T.-T.; Wang, K.-P.; Chang, W.-H.; Kao, C.-W.; Hung, T.-M. Effects of the function-specific instruction approach to neurofeedback training on frontal midline theta waves and golf putting performance. Psychol. Sport Exerc. 2022, 61, 102211. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Mottola, F.; Blanchfield, A.; Hardy, J.; Cooke, A. EEG neurofeedback improves cycling time to exhaustion. Psychol. Sport Exerc. 2021, 55, 101944. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Pourbehbahani, Z.; Saemi, E.; Cheng, M.Y.; Dehghan, M.R. Both Sensorimotor Rhythm Neurofeedback and Self-Controlled Practice Enhance Motor Learning and Performance in Novice Golfers. Behav. Sci. 2023, 13, 65. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Mirifar, A.; Keil, A.; Beckmann, J.; Ehrlenspiel, F. No Effects of Neurofeedback of Beta Band Components on Reaction Time Performance. J. Cogn. Enhanc. 2019, 3, 251‚Äì260. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Wang, K.P.; Frank, C.; Hung, T.M.; Schack, T. Neurofeedback training: Decreases in Mu rhythm lead to improved motor performance in complex visuomotor skills. Curr. Psychol. 2023, 42, 20860‚Äì20871. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Horvath, D.; Negyesi, J.; Racz, M.; Gyori, T.; Matics, Z.; Puskin, A.; Csipor, J.; Racz, L. Feasibility of a novel neurofeedback system: A parallel randomized single-blinded pilot study. Sci. Rep. 2023, 13, 17353. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Wang, K.P.; Cheng, M.Y.; Elbanna, H.; Schack, T. A new EEG neurofeedback training approach in sports: The effects function-specific instruction of Mu rhythm and visuomotor skill performance. Front. Psychol. 2023, 14, 1273186. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Arns, M.; Kleinnijenhuis, M.; Fallahpour, K.; Breteler, R. Golf Performance Enhancement and Real-Life Neurofeedback Training Using Personalized Event-Locked EEG Profiles. J. Neurother. 2008, 11, 11‚Äì18. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Zi√≥≈Çkowski, A.; Graczyk, M.; Strza≈Çkowska, A.; W≈Çodarczyk, P.; Zara≈Ñska, B. Neuronal, cognitive and social indicators for the control of aggressive behaviors in sport. Acta Neuropsychol. 2012, 10, 537‚Äì546. [Google Scholar]&lt;/item&gt;
      &lt;item&gt;Graczyk, M.; Pachalska, M.; Ziolkowski, A.; Manko, G.; Lukaszewska, B.; Kochanowicz, K.; Mirski, A.; Kropotov, I.D. Neurofeedback training for peak performance. Ann. Agric. Environ. Med. 2014, 21, 871‚Äì875. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Kao, S.-C.; Huang, C.-J.; Hung, T.-M. Neurofeedback Training Reduces Frontal Midline Theta and Improves Putting Performance in Expert Golfers. J. Appl. Sport Psychol. 2014, 26, 271‚Äì286. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Mikicin, M.; Orzechowski, G.; Jurewicz, K.; Paluch, K.; Kowalczyk, M.; Wr√≥bel, A. Brain-training for physical performance: A study of EEG-neurofeedback and alpha relaxation training in athletes. Acta Neurobiol. Exp. 2015, 75, 434‚Äì445. [Google Scholar]&lt;/item&gt;
      &lt;item&gt;Mikicin, M. State of mind as a subjective mental sensation results from objective brain activity following neurofeedback-EEG and relaxation trainings. Acta Neuropsychol. 2016, 14, 17‚Äì33. [Google Scholar]&lt;/item&gt;
      &lt;item&gt;Szczypi≈Ñska, M.; Mikicin, M. Does attention training induce any changes in the level of the selected cognitive processes in handball players? J. Phys. Educ. Sport 2019, 19, 1445‚Äì1452. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Domingos, C.; Alves, C.; Sousa, E.; Rosa, A.; Pereira, J. Does Neurofeedback Training Improve Performance in Athletes? NeuroRegulation 2020, 7, 8‚Äì17. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Domingos, C.; da Silva Caldeira, H.; Miranda, M.; Melicio, F.; Rosa, A.C.; Pereira, J.G. The Influence of Noise in the Neurofeedback Training Sessions in Student Athletes. Int. J. Environ. Res. Public Health 2021, 18, 13223. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Domingos, C.; Peralta, M.; Prazeres, P.; Nan, W.; Rosa, A.; Pereira, J.G. Session frequency matters in neurofeedback training of athletes. Appl. Psychophysiol. Biofeedback 2021, 46, 195‚Äì204. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Domingos, C.; Silva, C.M.D.; Antunes, A.; Prazeres, P.; Esteves, I.; Rosa, A.C. The Influence of an Alpha Band Neurofeedback Training in Heart Rate Variability in Athletes. Int. J. Environ. Res. Public Health 2021, 18, 12579. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Mikicin, M.; Orzechowski, G. Neuronal Activity in the Brain Changes During Exercise in Attention States, Warm-up, Submaximal Effort, and Recovery, After Neurofeedback-Eeg Training in Motion. Acta Neuropsychol. 2022, 20, 175‚Äì186. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Fuentes-Garcia, J.P.; Villafaina, S. Psychophysiological and Performance Effects of Biofeedback and Neurofeedback Interventions in a Top 100 Female Chess Player. Behav. Sci. 2024, 14, 1044. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Bakhtafrooz, S.; Kavyani, M.; Farsi, A.; Alboghebeish, S. The effect of infra low frequency-neurofeedback training on pistol shooting performance and attention in semi-skilled players. Front. Hum. Neurosci. 2025, 19, 1487737. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Paul, M.; Ganesan, S.; Sandhu, J.; Simon, J. Effect of sensory motor rhythm neurofeedback on psycho-physiological, electro-encephalographic measures and performance of archery players. Ibnosina J. Med. Biomed. Sci. 2012, 4, 32‚Äì39. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Rostami, R.; Sadeghi, H.; Karami, K.A.; Abadi, M.N.; Salamati, P. The Effects of Neurofeedback on the Improvement of Rifle Shooters‚Äô Performance. J. Neurother. 2012, 16, 264‚Äì269. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Strizhkova, O.; Cherapkina, L.; Strizhkova, T. Neurofeedback course applying of high skilled gymnasts in competitive period. J. Hum. Sport Exerc. 2012, 7, S185‚ÄìS193. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Christie, S.; Bertollo, M.; Werthner, P. The Effect of an Integrated Neurofeedback and Biofeedback Training Intervention on Ice Hockey Shooting Performance. J. Sport Exerc. Psychol. 2020, 42, 34‚Äì47. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Shokri, A.; Nosratabadi, M. Comparison of Biofeedback and Combined Interventions on Athlete‚Äôs Performance. Appl. Psychophysiol. Biofeedback 2021, 46, 227‚Äì234. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Fardinia, M.; Shojaei, M.; Rahimi, A. The effect of neurofeedback training on the anxiety of elite female swimmers. Ann. Biol. Res. 2012, 3, 1020‚Äì1028. [Google Scholar]&lt;/item&gt;
      &lt;item&gt;Page, M.J.; McKenzie, J.E.; Bossuyt, P.M.; Boutron, I.; Hoffmann, T.C.; Mulrow, C.D.; Shamseer, L.; Tetzlaff, J.M.; Akl, E.A.; Brennan, S.E.; et al. The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. BMJ 2021, 372, n71. [Google Scholar] [CrossRef]&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Disclaimer/Publisher‚Äôs Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;¬© 2025 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.mdpi.com/2306-5354/12/11/1202"/><published>2025-11-17T17:40:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45956111</id><title>Show HN: Building WebSocket in Apache Iggy with Io_uring and Completion Based IO</title><updated>2025-11-17T20:12:29.832635+00:00</updated><content>&lt;doc fingerprint="4692d24dedb3fbb2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building WebSocket Protocol in Apache Iggy using io_uring and Completion Based I/O Architecture&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction√¢&lt;/head&gt;
    &lt;p&gt;In our 0.5.0 release blog post, we announced that work was underway on a complete rewrite of Apache Iggy's core architecture using io_uring with a thread-per-core, shared nothing design. This architectural redesign aims to further improve performance, reduce tail latecies and lower resource usage by leveraging io_uring's completion based I/O model.&lt;/p&gt;
    &lt;p&gt;As part of this rewrite, we migrated from Tokio to compio, a completion-based async runtime that allows us to better utilize io_uring capabilities. However, it also presents different challenges when integrating with the wider Rust ecosystem.&lt;/p&gt;
    &lt;p&gt;We came across one such challenge when we needed to add WebSocket support to Iggy server. WebSockets are useful for browser clients and streaming dashboards. The Rust ecosystem has excellent WebSocket libraries like tungstenite and tokio-tungstenite but they are made when poll-based IO was the dominanat IO paradigm. They expect shared buffers and readiness-based I/O, fundamentally incompatible with compio's completion-based model that requires owned buffers.&lt;/p&gt;
    &lt;p&gt;Here we describe our journey of building compio-ws, a WebSocket implementation for the compio async runtime, and the engineering challenges we faced bridging two fundamentally different I/O models and it finally lead to us contributing to &lt;code&gt;compio&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding the architectural divide (poll vs completion)√¢&lt;/head&gt;
    &lt;p&gt;Let's see why poll-based libraries can't easily work with completion based runtimes by examining the traits of compio, tungstenite.&lt;/p&gt;
    &lt;code&gt;// The Rust std::io::Read trait (what tungstenite expects)&lt;/code&gt;
    &lt;p&gt;The Read trait assumes you borrow a buffer. Compio's AsyncRead needs to take ownership of the buffer, submit it to the kernel, and return it later.&lt;/p&gt;
    &lt;code&gt;// Tungstenite's WebSocket structure&lt;/code&gt;
    &lt;p&gt;There's no way to make this work directly with compio's owned-buffer model. The abstractions are fundamentally incompatible.&lt;/p&gt;
    &lt;p&gt;When we need to add WebSocket support, we can't just drop in tokio-tungstenite, which uses &lt;code&gt;future_util::io::AsyncRead&lt;/code&gt; trait. We need an adapter that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Works with compio's owned-buffer async I/O&lt;/item&gt;
      &lt;item&gt;Presents synchronous Read/Write traits to tungstenite&lt;/item&gt;
      &lt;item&gt;Efficiently bridges sync and async world&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Challenge with Tungstenite√¢&lt;/head&gt;
    &lt;p&gt;Tungstenite is the de-facto Rust WebSocket protocol implementation. It handles all the WebSocket protocol logic:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frame parsing and generation&lt;/item&gt;
      &lt;item&gt;Message fragmentation&lt;/item&gt;
      &lt;item&gt;Control frames (ping, pong, close)&lt;/item&gt;
      &lt;item&gt;Protocol violations and error handling&lt;/item&gt;
      &lt;item&gt;Text encoding validation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our initial thought was to contribute to async-tungstenite, which provides runtime-agnostic WebSocket support. It has adapters for multiple async runtimes through a trait-based abstraction. But as we dug deeper into adapting it for compio we realized a fundamental incompatibility&lt;/p&gt;
    &lt;p&gt;The realization: async-tungstenite is strongly coupled to poll-based IO&lt;/p&gt;
    &lt;p&gt;The incompatibility becomes clear when we examine the AsyncRead traits of compio and Future's AsyncRead&lt;/p&gt;
    &lt;code&gt;// Compio's async read - can't be made into a poll-based trait&lt;/code&gt;
    &lt;p&gt;These are fundamentally different programming models that don't compose cleanly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Decision: A separate crate√¢&lt;/head&gt;
    &lt;p&gt;After realizing the incompatibility, we've decided to create &lt;code&gt;compio-ws&lt;/code&gt; as a standalone crate. The architecture would be:&lt;/p&gt;
    &lt;code&gt;                                √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢&lt;/code&gt;
    &lt;p&gt;The key insight: we need a bridge layer that provides synchronous Read/Write traits to tungstenite while internally using compio's async owned-buffer I/O.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Attempt: &lt;code&gt;SyncStream&lt;/code&gt;√¢&lt;/head&gt;
    &lt;p&gt;Compio already provides &lt;code&gt;SyncStream&lt;/code&gt; in the &lt;code&gt;compio-io::compat&lt;/code&gt; module specifically for interoperating with libraries that expect synchronous I/O traits. It's a clever structure that maintains internal buffers to bridge the async/sync boundary:&lt;/p&gt;
    &lt;code&gt;pub struct SyncStream&amp;lt;S&amp;gt; {&lt;/code&gt;
    &lt;p&gt;The default &lt;code&gt;DEFAULT_BUF_SIZE&lt;/code&gt; is 8KB, but you can specify any capacity you want. Once created, the buffer capacity is fixed. It never grows. This can be a problem, which we will discuss below, read along.&lt;/p&gt;
    &lt;p&gt;Here's how it works:&lt;/p&gt;
    &lt;p&gt;Reading (sync to async):&lt;/p&gt;
    &lt;code&gt;impl&amp;lt;S&amp;gt; Read for SyncStream&amp;lt;S&amp;gt; {&lt;/code&gt;
    &lt;p&gt;Writing (sync to async):&lt;/p&gt;
    &lt;code&gt;impl&amp;lt;S&amp;gt; Write for SyncStream&amp;lt;S&amp;gt; {&lt;/code&gt;
    &lt;p&gt;The pattern is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sync &lt;code&gt;read()&lt;/code&gt;/&lt;code&gt;write()&lt;/code&gt;operate on internal buffers&lt;/item&gt;
      &lt;item&gt;Return &lt;code&gt;WouldBlock&lt;/code&gt;when buffer is empty/full&lt;/item&gt;
      &lt;item&gt;Call async &lt;code&gt;fill_read_buf()&lt;/code&gt;/&lt;code&gt;flush_write_buf()&lt;/code&gt;to service buffers&lt;/item&gt;
      &lt;item&gt;Retry the sync operation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This allows tungstenite to work with compio streams:&lt;/p&gt;
    &lt;code&gt;let stream = TcpStream::connect("127.0.0.1:8080").await?;&lt;/code&gt;
    &lt;head rend="h2"&gt;The problem with fixed buffer size in &lt;code&gt;SyncStream&lt;/code&gt;√¢&lt;/head&gt;
    &lt;p&gt;The initial implementation worked perfectly for messages smaller than the buffer. WebSocket handshakes completed, ping/pong frames exchanged, text messages flowed. Everything seemed fine.&lt;/p&gt;
    &lt;p&gt;Then we tested with larger messages, and the performance collapsed.&lt;/p&gt;
    &lt;p&gt;The Problem Scenario: Sending a 16MB binary message through WebSocket with the default 8KB SyncStream buffer:&lt;/p&gt;
    &lt;p&gt;Here's what happens inside:&lt;/p&gt;
    &lt;code&gt;Message: 16MB&lt;/code&gt;
    &lt;p&gt;The measurements were bad:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Time: Over 100 seconds to send 16MB&lt;/item&gt;
      &lt;item&gt;Memory: Excessive allocations from repeated buffer handling. At times this lead to the websocket process being OOM killed by the OS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each &lt;code&gt;WouldBlock&lt;/code&gt; -&amp;gt; async call -&amp;gt; retry cycle involved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Saving state in tungstenite&lt;/item&gt;
      &lt;item&gt;Suspending the sync call&lt;/item&gt;
      &lt;item&gt;Executing async flush&lt;/item&gt;
      &lt;item&gt;Resuming the sync call&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The sync trait contract requires immediate success or &lt;code&gt;WouldBlock&lt;/code&gt;. There's no way to say "I need a bigger buffer" or "give me partial progress while I grow the buffer." Each &lt;code&gt;WouldBlock&lt;/code&gt; forces a complete async round trip.&lt;/p&gt;
    &lt;p&gt;We tried the obvious fix of increasing the &lt;code&gt;SyncStream&lt;/code&gt; buffer size to 1MB. Which worked perfectly and compio-ws passed all tests in the standard autobahn testsuite. But this solution is still fragile when the user doesn't know the peak memory usage of their workload and this can lead to overprovisioning and wastage of server resources.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Solution: &lt;code&gt;GrowableSyncStream&lt;/code&gt;√¢&lt;/head&gt;
    &lt;p&gt;Design goals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dynamic growth: Buffers start small and grow as needed&lt;/item&gt;
      &lt;item&gt;Bounded maximum: Prevent memory exhaustion with configurable limits&lt;/item&gt;
      &lt;item&gt;Automatic shrinking: Return to base capacity based on a threshold&lt;/item&gt;
      &lt;item&gt;Minimal round trips: Handle large messages without constant &lt;code&gt;WouldBlock&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Architecture Overview:&lt;/p&gt;
    &lt;code&gt;pub struct GrowableSyncStream&amp;lt;S&amp;gt; {&lt;/code&gt;
    &lt;p&gt;Buffer growth strategy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start with a small sized buffer (128 KB matching tungstenite default buffer size)&lt;/item&gt;
      &lt;item&gt;Grow in predictable increments of &lt;code&gt;base_capacity&lt;/code&gt;sized chunks&lt;/item&gt;
      &lt;item&gt;Cap growth at reasonable limit (64MB default)&lt;/item&gt;
      &lt;item&gt;Shrink back to &lt;code&gt;base_capacity&lt;/code&gt;when all data is consumed and current size &amp;gt; 4x base_capacity&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key operations:&lt;/p&gt;
    &lt;p&gt;Read path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compacts buffer when needed (moves unread data to front)&lt;/item&gt;
      &lt;item&gt;Grows in &lt;code&gt;base_capacity&lt;/code&gt;increments during&lt;code&gt;fill_read_buf()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Shrinks back to base when all data consumed and buffer &amp;gt; 4√É base&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Write path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Vec::extend_from_slice&lt;/code&gt;handles growth automatically up to&lt;code&gt;max_buffer_size&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Returns &lt;code&gt;WouldBlock&lt;/code&gt;only when approaching max limit&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;flush_write_buf()&lt;/code&gt;handles progressive flushing and shrinks back to base&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With these changes compio-ws successfully passes the all the tests in &lt;code&gt;autobahn-testsuite&lt;/code&gt; that &lt;code&gt;tokio-tungstenite&lt;/code&gt; passes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarks√¢&lt;/head&gt;
    &lt;head rend="h3"&gt;Benchmark Methodology√¢&lt;/head&gt;
    &lt;p&gt;To understand the real-world performance characteristics of compio-ws, we ran Apache Iggy's pinned producer benchmark and pinned consumer benchmark&lt;/p&gt;
    &lt;p&gt;The following benchmarks are run using AWS i3en.3xlarge instance using 24.04 Ubuntu OS.&lt;/p&gt;
    &lt;p&gt;Hardware Specifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: Intel Xeon Platinum 8259CL @ 2.50GHz (Cascade Lake)&lt;/item&gt;
      &lt;item&gt;Cores: 6 physical cores with hyperthreading (12 vCPUs total)&lt;/item&gt;
      &lt;item&gt;Cache: 36 MB L3 cache&lt;/item&gt;
      &lt;item&gt;Memory: 96 GB RAM&lt;/item&gt;
      &lt;item&gt;Storage: Local NVMe SSD&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common benchmark setting used:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;enable fsync and fsync every single message (&lt;code&gt;export IGGY_SYSTEM_PARTITION_ENFORCE_FSYNC=true&lt;/code&gt;and&lt;code&gt;export IGGY_SYSTEM_PARTITION_MESSAGES_REQUIRED_TO_SAVE=1&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Use huge pages in linux.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo sysctl -w vm.swappiness=10&lt;/code&gt;
    &lt;p&gt;Workload characteristics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;4 Producers (one per CPU core)&lt;/item&gt;
      &lt;item&gt;4 Streams, 1 Topic per Stream, 1 Partition per Topic&lt;/item&gt;
      &lt;item&gt;1000 messages per batch&lt;/item&gt;
      &lt;item&gt;40,000 batches&lt;/item&gt;
      &lt;item&gt;1000 bytes per message&lt;/item&gt;
      &lt;item&gt;Total: 40M messages, 40GB of data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We compared two scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TCP: Direct TCP connection (baseline)&lt;/item&gt;
      &lt;item&gt;WebSocket: compio-ws with GrowableSyncStream&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Results: TCP vs WebSocket√¢&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Percentile&lt;/cell&gt;
        &lt;cell role="head"&gt;TCP&lt;/cell&gt;
        &lt;cell role="head"&gt;WebSocket&lt;/cell&gt;
        &lt;cell role="head"&gt;Difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Average&lt;/cell&gt;
        &lt;cell&gt;2.61 ms&lt;/cell&gt;
        &lt;cell&gt;3.43 ms&lt;/cell&gt;
        &lt;cell&gt;+0.82 ms (31.4% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Median (P50)&lt;/cell&gt;
        &lt;cell&gt;2.58 ms&lt;/cell&gt;
        &lt;cell&gt;3.34 ms&lt;/cell&gt;
        &lt;cell&gt;+0.76 ms (29.5% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P95&lt;/cell&gt;
        &lt;cell&gt;2.94 ms&lt;/cell&gt;
        &lt;cell&gt;4.00 ms&lt;/cell&gt;
        &lt;cell&gt;+1.06 ms (36.1% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;3.63 ms&lt;/cell&gt;
        &lt;cell&gt;4.56 ms&lt;/cell&gt;
        &lt;cell&gt;+0.93 ms (25.6% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P999&lt;/cell&gt;
        &lt;cell&gt;3.84 ms&lt;/cell&gt;
        &lt;cell&gt;5.27 ms&lt;/cell&gt;
        &lt;cell&gt;+1.43 ms (37.2% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P9999&lt;/cell&gt;
        &lt;cell&gt;5.27 ms&lt;/cell&gt;
        &lt;cell&gt;9.48 ms&lt;/cell&gt;
        &lt;cell&gt;+4.21 ms (79.9% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Max&lt;/cell&gt;
        &lt;cell&gt;11.63 ms&lt;/cell&gt;
        &lt;cell&gt;16.88 ms&lt;/cell&gt;
        &lt;cell&gt;+5.25 ms (45.1% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Min&lt;/cell&gt;
        &lt;cell&gt;1.93 ms&lt;/cell&gt;
        &lt;cell&gt;2.60 ms&lt;/cell&gt;
        &lt;cell&gt;+0.67 ms (34.7% higher)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Benchmark graphs:&lt;/p&gt;
    &lt;head rend="h4"&gt;TCP√¢&lt;/head&gt;
    &lt;head rend="h4"&gt;WebSocket√¢&lt;/head&gt;
    &lt;p&gt;Next we look at pinned-consumer benchmark.&lt;/p&gt;
    &lt;p&gt;Workload characteristics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;4 Consumers (one per CPU core)&lt;/item&gt;
      &lt;item&gt;4 Streams, 1 Topic per Stream, 1 Partition per Topic&lt;/item&gt;
      &lt;item&gt;1000 messages per batch&lt;/item&gt;
      &lt;item&gt;40,000 batches&lt;/item&gt;
      &lt;item&gt;1000 bytes per message&lt;/item&gt;
      &lt;item&gt;Total: 40M messages, 40GB of data&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Percentile&lt;/cell&gt;
        &lt;cell role="head"&gt;TCP&lt;/cell&gt;
        &lt;cell role="head"&gt;WebSocket&lt;/cell&gt;
        &lt;cell role="head"&gt;Difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Average&lt;/cell&gt;
        &lt;cell&gt;0.70 ms&lt;/cell&gt;
        &lt;cell&gt;1.44 ms&lt;/cell&gt;
        &lt;cell&gt;+0.74 ms (105.7% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Median (P50)&lt;/cell&gt;
        &lt;cell&gt;0.68 ms&lt;/cell&gt;
        &lt;cell&gt;1.41 ms&lt;/cell&gt;
        &lt;cell&gt;+0.73 ms (107.4% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P95&lt;/cell&gt;
        &lt;cell&gt;0.85 ms&lt;/cell&gt;
        &lt;cell&gt;1.73 ms&lt;/cell&gt;
        &lt;cell&gt;+0.88 ms (103.5% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;1.00 ms&lt;/cell&gt;
        &lt;cell&gt;1.95 ms&lt;/cell&gt;
        &lt;cell&gt;+0.95 ms (95.0% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P999&lt;/cell&gt;
        &lt;cell&gt;1.32 ms&lt;/cell&gt;
        &lt;cell&gt;2.26 ms&lt;/cell&gt;
        &lt;cell&gt;+0.94 ms (71.2% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P9999&lt;/cell&gt;
        &lt;cell&gt;1.54 ms&lt;/cell&gt;
        &lt;cell&gt;2.52 ms&lt;/cell&gt;
        &lt;cell&gt;+0.98 ms (63.6% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Max&lt;/cell&gt;
        &lt;cell&gt;2.02 ms&lt;/cell&gt;
        &lt;cell&gt;10.44 ms&lt;/cell&gt;
        &lt;cell&gt;+8.42 ms (416.8% higher)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Min&lt;/cell&gt;
        &lt;cell&gt;0.55 ms&lt;/cell&gt;
        &lt;cell&gt;1.14 ms&lt;/cell&gt;
        &lt;cell&gt;+0.59 ms (107.3% higher)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Benchmark graphs:&lt;/p&gt;
    &lt;head rend="h4"&gt;TCP√¢&lt;/head&gt;
    &lt;head rend="h4"&gt;WebSocket√¢&lt;/head&gt;
    &lt;head rend="h3"&gt;Analysis:√¢&lt;/head&gt;
    &lt;p&gt;The results show measurable but reasonable overhead from the WebSocket layer:&lt;/p&gt;
    &lt;p&gt;Producer latency: WebSocket adds ~0.8-1.0ms across most percentiles (30-40% higher than TCP). Even at P9999, we achieve 9.48ms latency - impressive for a durable workload with fsync-per-message enabled.&lt;/p&gt;
    &lt;p&gt;Consumer latency: WebSocket shows roughly 2√É the latency of raw TCP, adding ~0.7-1.0ms. The P9999 of 2.52ms demonstrates consistent performance even at high percentiles.&lt;/p&gt;
    &lt;p&gt;Under durability constraints, achieving single-digit millisecond latencies at P9999 for producers and sub-3ms for consumers is quite good.&lt;/p&gt;
    &lt;p&gt;Adapter layer cost: The current implementation uses GrowableSyncStream as a bridge between sync and async I/O models. The buffer grows in linear increments, which can be suboptimal for large messages. However, for this first implementation enabling WebSocket support in a completion-based runtime, the performance is acceptable.&lt;/p&gt;
    &lt;p&gt;WebSocket protocol overhead:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WebSocket framing: each message needs frame headers.&lt;/item&gt;
      &lt;item&gt;Masking: XOR operation on payload&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tail latency consistency: GrowableSyncStream maintains roughly proportional overhead even at high percentiles. The P9999 shows larger absolute differences but percentage-wise remains consistent with lower percentiles, indicating predictable behavior under load.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Work√¢&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Smarter Buffer Growth&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Current: Linear &lt;code&gt;base_capacity&lt;/code&gt;increments&lt;/item&gt;
          &lt;item&gt;Better: Exponential growth (like &lt;code&gt;Vec::reserve&lt;/code&gt;)&lt;/item&gt;
          &lt;item&gt;Impact: Reduce reallocation overhead for large messages&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Current: Linear &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Buffer Pooling&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Current: Allocate/deallocate per message&lt;/item&gt;
          &lt;item&gt;Better: Per-core buffer pools with object reuse&lt;/item&gt;
          &lt;item&gt;Impact: Reduce allocator pressure&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebSocket implementation with native owned buffers&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Implement WebSocket protocol directly with owned buffers from scratch.&lt;/item&gt;
          &lt;item&gt;Direct integration with compio to use io_uring capabilities.&lt;/item&gt;
          &lt;item&gt;Could increase performance significantly.&lt;/item&gt;
          &lt;item&gt;This is a significant undertaking due to full RFC compliance.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We contributed &lt;code&gt;compio-ws&lt;/code&gt; to &lt;code&gt;compio&lt;/code&gt; and anyone interested in improving &lt;code&gt;compio-ws&lt;/code&gt; can contribute to &lt;code&gt;compio&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion√¢&lt;/head&gt;
    &lt;p&gt;Current state in Iggy: WebSocket support is currently implemented with long polling on the consumer side and push-based notifications is planned for future releases. This has different implications for producers and consumers.&lt;/p&gt;
    &lt;p&gt;For consumers (receiving messages): The current long polling implementation means consumers must repeatedly request new messages, even when none are available. This is inefficient, especially for low-power edge clients.&lt;/p&gt;
    &lt;p&gt;For producers (sending messages): WebSocket provides immediate benefits. Instead of establishing new connections or using inefficient long polling, producers can maintain a persistent WebSocket connection and push messages directly to the server. This is particularly valuable for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browser-based producers sending events or telemetry&lt;/item&gt;
      &lt;item&gt;Edge devices reporting sensor data or metrics&lt;/item&gt;
      &lt;item&gt;Dashboard applications sending commands or configuration updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What's next: While compio-ws with GrowableSyncStream enables WebSocket support today, significant optimization potential remains. A native WebSocket implementation designed for owned buffers from the ground up could eliminate the adapter layer overhead and unlock the full performance potential of io_uring's zero-copy capabilities. We invite the Rust community to contribute:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimize &lt;code&gt;GrowableSyncStream&lt;/code&gt;: Implement exponential buffer growth and pooling&lt;/item&gt;
      &lt;item&gt;Owned buffer WS protocol implementation: Build WebSocket from scratch with owned buffers&lt;/item&gt;
      &lt;item&gt;Push-based consumers: Help implement server-push notifications in Iggy&lt;/item&gt;
      &lt;item&gt;Edge client libraries: Create WebSocket SDKs optimized for resource-constrained devices&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://iggy.apache.org/blogs/2025/11/17/websocket-io-uring/"/><published>2025-11-17T18:00:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45956176</id><title>An official atlas of North Korea</title><updated>2025-11-17T20:12:29.208356+00:00</updated><content>&lt;doc fingerprint="4f675bef84e5fad8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An official atlas of North Korea&lt;/head&gt;
    &lt;head rend="h3"&gt;A North Korean atlas that shows the world from the perspective of one of the most isolated countries on the planet.&lt;/head&gt;
    &lt;p&gt;What I bring you today is a real gem that I have been searching for a long time and have finally managed to get my hands on a copy1. It is a collection of 672 maps found in the Great Korean Encyclopaedia2, North Korea‚Äôs reference encyclopaedia. More specifically, it is an electronic edition published on CD in the first decade of the 2000s3.&lt;/p&gt;
    &lt;p&gt;Work on this encyclopaedia began in 1964, when Kim Il Sung established a compilation committee with the aim of bringing together all the knowledge and guidelines that a good Korean should follow4. The work spanned several decades, and the result was published in thirty volumes between 1995 and 2002. The encyclopaedia contained more than 100,000 words, 25,000 images and photographs, and 5,200 historical figures.&lt;/p&gt;
    &lt;p&gt;And maps, lots of maps.&lt;/p&gt;
    &lt;head rend="h3"&gt;The maps of Korea&lt;/head&gt;
    &lt;p&gt;According to the prevailing narrative in North Korea, the war was won by the communists and since then, the entire Korean peninsula has remained united under the rule of the Korean Workers‚Äô Party. Therefore, when looking at the maps in this atlas, it should come as no surprise that Korea is always shown as one country, with no reference to the other country that exists at the southern tip of the peninsula.&lt;/p&gt;
    &lt;p&gt;In addition to generic maps of Korea, like any good atlas, the encyclopaedia also includes detailed maps of each of the provinces and counties that make them up. Again, it makes no distinction between north and south, maintaining that narrative of unity.&lt;/p&gt;
    &lt;head rend="h3"&gt;World maps&lt;/head&gt;
    &lt;p&gt;When we delve into the global vision of North Korean cartography, things become even more interesting. So I‚Äôll start simply with the world map.&lt;/p&gt;
    &lt;p&gt;This North Korean world map is centred on the Pacific Ocean, which gives Korea a privileged position on the global stage. This is nothing new, but what is new is how North Korea depicts its enemies. Can you spot them?&lt;/p&gt;
    &lt;p&gt;Yes, they are the only two countries painted in dark grey: the United States and Japan. This pattern can also be seen on the political map of Korea5, and is consistent on virtually all the political maps in the atlas. In the map of Europe, that you can see below, this colour is also used for the United Kingdom and France, but in this case, it is not consistent through all the political maps.&lt;/p&gt;
    &lt;p&gt;The representation of the continents is also of some interest. Apart from the idea of showing enemies in a consistent colour, I like the choice of projections. Instead of opting for the classic projections seen in Western cartography, the authors of these maps choose projections that better balance the shape and size of different countries. They take advantage of the fact that only one region of the globe needs to be represented.&lt;/p&gt;
    &lt;p&gt;The encyclopaedia also includes maps of all the oceans, which also incorporate ocean current patterns.&lt;/p&gt;
    &lt;head rend="h3"&gt;Country maps&lt;/head&gt;
    &lt;p&gt;This collection would not be complete without country maps. Here, once again, we find a strong emphasis on the geopolitical situation and North Korea‚Äôs view of the world. This is consistent with the political maps of each continent, but when viewed separately, it is even more evident.&lt;/p&gt;
    &lt;p&gt;First, the maps dedicated to enemies.&lt;/p&gt;
    &lt;p&gt;Beyond these obvious things, there are more subtle issues that can be understood by looking at the complete list. The only country that does not have a dedicated map is Israel. In fact, Israel does not appear under that name on any map, but the territory of Israel appears as Palestine on the map of Asia and on all maps of surrounding countries. In the one for Jordan, it is also clarified that Palestine is a territory under Israeli occupation.&lt;/p&gt;
    &lt;p&gt;Another curious detail is that the atlas includes a country with limited international recognition6: the Sahrawi Arab Democratic Republic of Western Sahara.&lt;/p&gt;
    &lt;p&gt;And well, although it may not be of general interest, as most of the readers are from countries with large English population, here you can see how some of these countries are represented.&lt;/p&gt;
    &lt;p&gt;If you are interested in any other map, please let me know in the comments and I can update the article.&lt;/p&gt;
    &lt;p&gt;Acknowledgement: I owe today‚Äôs article entirely to Pedro Zurita, the man behind the Mapoteca de pZZ, thanks to whom I got a copy of this fabulous atlas. I recommend that you follow him on Instagram, where he posts many maps, especially of Mexico, and on TikTok or YouTube, where he posts interesting videos on cartography and geography (in Spanish).&lt;/p&gt;
    &lt;p&gt;A few weeks ago, Pedro Zurita got me a copy. And it made me very, very happy. At the end of the article, I have included information so that you can follow him on social media.&lt;/p&gt;
    &lt;p&gt;Ï°∞ÏÑ†ÎåÄÎ∞±Í≥ºÏÇ¨Ï†Ñ, by its title in Korean.&lt;/p&gt;
    &lt;p&gt;While researching, I noticed that there is a digital edition from 2001, although it is possible that the copy I have was published later. Considering that there are maps with Timor-Leste and Montenegro as independent countries, but South Sudan still appears united with Sudan, it is plausible that the collection of maps is from an edition between 2007 and 2010.&lt;/p&gt;
    &lt;p&gt;This is understood to be North Korean, but within North Korea‚Äôs narrative, Korea remains united.&lt;/p&gt;
    &lt;p&gt;On the second map of Korea, the political map, in the lower right-hand corner, part of Japan is painted in the same dark grey colour. This is a deliberate choice, as it clearly differs from the colour chosen for China and Russia on the same map.&lt;/p&gt;
    &lt;p&gt;No European country currently recognises Western Sahara as an independent country, and very few in Asia do. Most of the countries that recognise its independence are in Africa and Latin America. And, of course, North Korea. You can see the details here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cartographerstale.com/p/an-official-atlas-of-north-korea"/><published>2025-11-17T18:07:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45957073</id><title>Implementing Rust newtype for errors in axum</title><updated>2025-11-17T20:12:28.998731+00:00</updated><content>&lt;doc fingerprint="1e86ea1ceac861ce"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;TL;DR: GITHUB REPO. Tired of writing the same verbose error handling boilerplate in your Axum handlers? Me too! √∞ By creating a custom&lt;/p&gt;&lt;code&gt;AppError&lt;/code&gt;newtype that wraps&lt;code&gt;anyhow::Error&lt;/code&gt;and implements&lt;code&gt;IntoResponse&lt;/code&gt;+&lt;code&gt;From&amp;lt;E&amp;gt;&lt;/code&gt;, you can ditch all those ugly match statements and embrace the beautiful&lt;code&gt;?&lt;/code&gt;operator. Your handler functions go from messy error-matching shenanigans to clean, readable code that automatically converts any error into proper HTTP responses. It√¢s like magic, but with more crabs! √∞¬¶&lt;/quote&gt;&lt;p&gt;Recently I√¢ve been digging a lot into the axum crate for any of my Rust web projects. There are plenty of options out there for Rust web applications, but it seems that we have all settled on Axum as the go to crate. Before you even start reading this, if you have not checked it out yet - do so now √¢¬¶ I√¢ll wait.&lt;/p&gt;&lt;p&gt;Okay, you√¢re back! Love it? YES! √∞¬•¬≥ Now, let√¢s talk about a learning project I am working on. Ever since I√¢ve discovered htmx I√¢ve been immersing myself in the world of web development with HATEOAS as my priority. Just the neatness of using hypermedia gives me a rush, and helps me with all that JavaScript fatigue. I do suggest you go and give hypermedia systems a read, a thing I have been doing over the past couple of weeks.&lt;/p&gt;&lt;p&gt;So, in the spirit of this book, I was following along with building some hypermedia driven system. But instead of using Python and Flask as stated in the book, I√¢ve opted to put my crab hat on, and do it in Rust. Like the big boy I am. In this post I wont explain how I did that (link to a post from the future will go here), but rather explain how I used the amazing features of Rust to eliminate a lot of boilerplate code on my side.&lt;/p&gt;&lt;p&gt;In order to be a good web builder, you want to make sure to return the proper HTTP status codes. (I√¢m looking at you &lt;code&gt;200 OK&lt;/code&gt; with an error message). So in my handler functions I make sure to explicitly return the Status code as part of my return tuple. Something like so:&lt;/p&gt;&lt;p&gt;This would signal back to the user (and axum) that the request was either &lt;code&gt;200 OK&lt;/code&gt;, and here is the HTML, or &lt;code&gt;500 Internal Server Error&lt;/code&gt; and an angry string. Nifty!&lt;/p&gt;&lt;p&gt;With the glory of Rust√¢s Result enum, we are equipped to handle any errors our back end may throw at us. So, I just &lt;code&gt;match&lt;/code&gt; on call that can fail, and return something back depending on that &lt;code&gt;Result&lt;/code&gt;.
In practice, say in this handler function that finds a &lt;code&gt;Contact&lt;/code&gt; by its ID in the database, it would look something like this:&lt;/p&gt;&lt;p&gt;That is a lot of boilerplate code. While I do enjoy the verbosity of Rust (makes me feel all safe and cozy), this gets real old real fast. Especially when you have multiple handler functions, that invoke multiple different calls that can fail. Let√¢s bring in another amazing feature of rust the newtype pattern, and simplify this √∞&lt;/p&gt;&lt;p&gt;I wont go too much into newtypes, as there is an excellent guide that I encourage all of you to read. Simply put, they are thin wrappers that allow you to extend functionality of existing Types not native to your crate. And I am gonna use it to extend the implement the &lt;code&gt;IntoResponse&lt;/code&gt; trait into a type I dubbed &lt;code&gt;AppError&lt;/code&gt;. And then allow it (what ever the Error is) to be converted into anyhow::Error.&lt;/p&gt;&lt;p&gt;Let√¢s first create this wrapper newtype:&lt;/p&gt;&lt;p&gt;Here I am wrapping &lt;code&gt;anyhow::Error&lt;/code&gt; into a new type called &lt;code&gt;AppError&lt;/code&gt;. I could do the same for any other type, and just create a wrapper around it (ie. a &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; wrapper: &lt;code&gt;struct Wrapper(Vec&amp;lt;T&amp;gt;)&lt;/code&gt;). Now comes the fun part, implementing certain traits.&lt;/p&gt;&lt;p&gt;To implement the &lt;code&gt;IntoResponse&lt;/code&gt; trait from axum into this new type, we only need to implement the &lt;code&gt;into_response&lt;/code&gt; function, which needs to return a &lt;code&gt;Response&amp;lt;Body&amp;gt;&lt;/code&gt; type. Let√¢s look at some code:&lt;/p&gt;&lt;p&gt;And just like that we√¢ve implemented our own way of returning an error response from a handler function. Let me explain the code a bit:&lt;/p&gt;&lt;code&gt;IntoResponse&lt;/code&gt; for &lt;code&gt;AppError&lt;/code&gt;&lt;code&gt;IntoResponse&lt;/code&gt; and we are simply returning a &lt;code&gt;Response&lt;/code&gt; from the axum crate&lt;code&gt;StatusCode&lt;/code&gt; and a &lt;code&gt;String&lt;/code&gt; coming from element 0 of the &lt;code&gt;AppError&lt;/code&gt; struct. Oh, and convert all that into a &lt;code&gt;Response&lt;/code&gt;&lt;p&gt;Here is a bit more complicated version of the same code, this one uses a templating engine to return some nicely formatted web pages. This version just expands the above, but should demonstrate that this all works really nicely with the rest of your codebase.&lt;/p&gt;&lt;p&gt;Okay, there is a lot more going on here. Most of it is the same as before, but let me break it down:&lt;/p&gt;&lt;code&gt;Error5xxTemplate&lt;/code&gt; struct that I use with the askama templating crate&lt;code&gt;500&lt;/code&gt; error and the string back.&lt;p&gt;Now that we have an &lt;code&gt;IntoResponse&lt;/code&gt; implemented. Let√¢s give our &lt;code&gt;AppError&lt;/code&gt; the ability to take errors from anywhere.&lt;/p&gt;&lt;p&gt;To make &lt;code&gt;AppError&lt;/code&gt; a bit more flexible, I wanted to be able to automatically convert any* (*I√¢ll come back to this in a bit) error type. Let√¢s look at some code and make sense of it:&lt;/p&gt;&lt;p&gt;We are taking advantage of a very powerful crate here, anyhow. Which allows us to work with errors in Rust way more efficiently. In our case we are using it√¢s popularity and other crates ability to convert into this Error type.&lt;/p&gt;&lt;p&gt;Let me explain this line by line:&lt;/p&gt;&lt;code&gt;From&amp;lt;E&amp;gt;&lt;/code&gt; for &lt;code&gt;AppError&lt;/code&gt;. By using the generic type &lt;code&gt;E&lt;/code&gt; we can have a wider implementation. This would be the equivalent of &lt;code&gt;impl From&amp;lt;sqxl::Error&amp;gt; for AppError&lt;/code&gt;. Which converts the &lt;code&gt;sqlx::Error&lt;/code&gt; type into &lt;code&gt;AppError&lt;/code&gt;&lt;code&gt;where&lt;/code&gt; clause, we only allow the generic type &lt;code&gt;E&lt;/code&gt; to be the ones that already support the &lt;code&gt;Into&lt;/code&gt; trait from &lt;code&gt;anyhow::Error&lt;/code&gt;. Basically limiting us to types that already support the conversion into this error type.&lt;code&gt;from&lt;/code&gt; function that takes an error and returns itself back. By using the support mentioned above, we can just take the error and run an &lt;code&gt;.into()&lt;/code&gt; conversion.&lt;p&gt;By using this generic trait approach we are essentially creating the following Rust code:&lt;/p&gt;&lt;p&gt;And so on √¢¬¶ You get the picture!&lt;/p&gt;&lt;p&gt;Okay, so we have our newtype &lt;code&gt;AppError&lt;/code&gt;, it has all the things it needs to be converted to our Error type. How does it actually work? Well, let√¢s go back to our &lt;code&gt;get_edit_contact&lt;/code&gt; handler function from before, and see what has changed:&lt;/p&gt;&lt;p&gt;Whoa, that is way tighter than before. Yes, we are using the &lt;code&gt;?&lt;/code&gt; operator we propagate the errors up the call stack. Meaning the value of the &lt;code&gt;Result&lt;/code&gt; returned by both &lt;code&gt;Contact::find_by_id&lt;/code&gt; and &lt;code&gt;.render()&lt;/code&gt; are returned back to axum as &lt;code&gt;AppError&lt;/code&gt; newtypes.&lt;/p&gt;&lt;p&gt;This means we no longer have to deal with error handling within the function itself, and we are just returning the same error type back. Since it is the same error type, both function handler and axum are happy with receiving it! √∞¬•¬≥ Huzzah!&lt;/p&gt;&lt;p&gt;If you want to see the full codebase in action, you can check out my GitHub repo here. And please ignore the mess, this is just a learning repo! √∞&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rup12.net/posts/learning-rust-custom-errors/"/><published>2025-11-17T19:22:06+00:00</published></entry></feed>