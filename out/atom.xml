<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-14T19:32:38.269471+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45574393</id><title>DDoS Botnet Aisuru Blankets US ISPs in Record DDoS</title><updated>2025-10-14T19:32:45.990549+00:00</updated><content>&lt;doc fingerprint="c70581cbb810e0c3"&gt;
  &lt;main&gt;
    &lt;p&gt;The world’s largest and most disruptive botnet is now drawing a majority of its firepower from compromised Internet-of-Things (IoT) devices hosted on U.S. Internet providers like AT&amp;amp;T, Comcast and Verizon, new evidence suggests. Experts say the heavy concentration of infected devices at U.S. providers is complicating efforts to limit collateral damage from the botnet’s attacks, which shattered previous records this week with a brief traffic flood that clocked in at nearly 30 trillion bits of data per second.&lt;/p&gt;
    &lt;p&gt;Since its debut more than a year ago, the Aisuru botnet has steadily outcompeted virtually all other IoT-based botnets in the wild, with recent attacks siphoning Internet bandwidth from an estimated 300,000 compromised hosts worldwide.&lt;/p&gt;
    &lt;p&gt;The hacked systems that get subsumed into the botnet are mostly consumer-grade routers, security cameras, digital video recorders and other devices operating with insecure and outdated firmware, and/or factory-default settings. Aisuru’s owners are continuously scanning the Internet for these vulnerable devices and enslaving them for use in distributed denial-of-service (DDoS) attacks that can overwhelm targeted servers with crippling amounts of junk traffic.&lt;/p&gt;
    &lt;p&gt;As Aisuru’s size has mushroomed, so has its punch. In May 2025, KrebsOnSecurity was hit with a near-record 6.35 terabits per second (Tbps) attack from Aisuru, which was then the largest assault that Google’s DDoS protection service Project Shield had ever mitigated. Days later, Aisuru shattered that record with a data blast in excess of 11 Tbps.&lt;/p&gt;
    &lt;p&gt;By late September, Aisuru was publicly flexing DDoS capabilities topping 22 Tbps. Then on October 6, its operators heaved a whopping 29.6 terabits of junk data packets each second at a targeted host. Hardly anyone noticed because it appears to have been a brief test or demonstration of Aisuru’s capabilities: The traffic flood lasted less only a few seconds and was pointed at an Internet server that was specifically designed to measure large-scale DDoS attacks.&lt;/p&gt;
    &lt;p&gt;Aisuru’s overlords aren’t just showing off. Their botnet is being blamed for a series of increasingly massive and disruptive attacks. Although recent assaults from Aisuru have targeted mostly ISPs that serve online gaming communities like Minecraft, those digital sieges often result in widespread collateral Internet disruption.&lt;/p&gt;
    &lt;p&gt;For the past several weeks, ISPs hosting some of the Internet’s top gaming destinations have been hit with a relentless volley of gargantuan attacks that experts say are well beyond the DDoS mitigation capabilities of most organizations connected to the Internet today.&lt;/p&gt;
    &lt;p&gt;Steven Ferguson is principal security engineer at Global Secure Layer (GSL), an ISP in Brisbane, Australia. GSL hosts TCPShield, which offers free or low-cost DDoS protection to more than 50,000 Minecraft servers worldwide. Ferguson told KrebsOnSecurity that on October 8, TCPShield was walloped with a blitz from Aisuru that flooded its network with more than 15 terabits of junk data per second.&lt;/p&gt;
    &lt;p&gt;Ferguson said that after the attack subsided, TCPShield was told by its upstream provider OVH that they were no longer welcome as a customer.&lt;/p&gt;
    &lt;p&gt;“This was causing serious congestion on their Miami external ports for several weeks, shown publicly via their weather map,” he said, explaining that TCPShield is now solely protected by GSL.&lt;/p&gt;
    &lt;p&gt;Traces from the recent spate of crippling Aisuru attacks on gaming servers can be still seen at the website blockgametracker.gg, which indexes the uptime and downtime of the top Minecraft hosts. In the following example from a series of data deluges on the evening of September 28, we can see an Aisuru botnet campaign briefly knocked TCPShield offline.&lt;/p&gt;
    &lt;p&gt;Paging through the same uptime graphs for other network operators listed shows almost all of them suffered brief but repeated outages around the same time. Here is the same uptime tracking for Minecraft servers on the network provider Cosmic (AS30456), and it shows multiple large dips that correspond to game server outages caused by Aisuru.&lt;/p&gt;
    &lt;head rend="h2"&gt;BOTNETS R US&lt;/head&gt;
    &lt;p&gt;Ferguson said he’s been tracking Aisuru for about three months, and recently he noticed the botnet’s composition shifted heavily toward infected systems at ISPs in the United States. Ferguson shared logs from an attack on October 8 that indexed traffic by the total volume sent through each network provider, and the logs showed that 11 of the top 20 traffic sources were U.S. based ISPs.&lt;/p&gt;
    &lt;p&gt;AT&amp;amp;T customers were by far the biggest U.S. contributors to that attack, followed by botted systems on Charter Communications, Comcast, T-Mobile and Verizon, Ferguson found. He said the volume of data packets per second coming from infected IoT hosts on these ISPs is often so high that it has started to affect the quality of service that ISPs are able to provide to adjacent (non-botted) customers.&lt;/p&gt;
    &lt;p&gt;“The impact extends beyond victim networks,” Ferguson said. “For instance we have seen 500 gigabits of traffic via Comcast’s network alone. This amount of egress leaving their network, especially being so US-East concentrated, will result in congestion towards other services or content trying to be reached while an attack is ongoing.”&lt;/p&gt;
    &lt;p&gt;Roland Dobbins is principal engineer at Netscout. Dobbins said Ferguson is spot on, noting that while most ISPs have effective mitigations in place to handle large incoming DDoS attacks, many are far less prepared to manage the inevitable service degradation caused by large numbers of their customers suddenly using some or all available bandwidth to attack others.&lt;/p&gt;
    &lt;p&gt;“The outbound and cross-bound DDoS attacks can be just as disruptive as the inbound stuff,” Dobbin said. “We’re now in a situation where ISPs are routinely seeing terabit-per-second plus outbound attacks from their networks that can cause operational problems.”&lt;/p&gt;
    &lt;p&gt;“The crying need for effective and universal outbound DDoS attack suppression is something that is really being highlighted by these recent attacks,” Dobbins continued. “A lot of network operators are learning that lesson now, and there’s going to be a period ahead where there’s some scrambling and potential disruption going on.”&lt;/p&gt;
    &lt;p&gt;KrebsOnSecurity sought comment from the ISPs named in Ferguson’s report. Charter Communications pointed to a recent blog post on protecting its network, stating that Charter actively monitors for both inbound and outbound attacks, and that it takes proactive action wherever possible.&lt;/p&gt;
    &lt;p&gt;“In addition to our own extensive network security, we also aim to reduce the risk of customer connected devices contributing to attacks through our Advanced WiFi solution that includes Security Shield, and we make Security Suite available to our Internet customers,” Charter wrote in an emailed response to questions. “With the ever-growing number of devices connecting to networks, we encourage customers to purchase trusted devices with secure development and manufacturing practices, use anti-virus and security tools on their connected devices, and regularly download security patches.”&lt;/p&gt;
    &lt;p&gt;A spokesperson for Comcast responded, “Currently our network is not experiencing impacts and we are able to handle the traffic.”&lt;/p&gt;
    &lt;head rend="h2"&gt;9 YEARS OF MIRAI&lt;/head&gt;
    &lt;p&gt;Aisuru is built on the bones of malicious code that was leaked in 2016 by the original creators of the Mirai IoT botnet. Like Aisuru, Mirai quickly outcompeted all other DDoS botnets in its heyday, and obliterated previous DDoS attack records with a 620 gigabit-per-second siege that sidelined this website for nearly four days in 2016.&lt;/p&gt;
    &lt;p&gt;The Mirai botmasters likewise used their crime machine to attack mostly Minecraft servers, but with the goal of forcing Minecraft server owners to purchase a DDoS protection service that they controlled. In addition, they rented out slices of the Mirai botnet to paying customers, some of whom used it to mask the sources of other types of cybercrime, such as click fraud.&lt;/p&gt;
    &lt;p&gt;Dobbins said Aisuru’s owners also appear to be renting out their botnet as a distributed proxy network that cybercriminal customers anywhere in the world can use to anonymize their malicious traffic and make it appear to be coming from regular residential users in the U.S.&lt;/p&gt;
    &lt;p&gt;“The people who operate this botnet are also selling (it as) residential proxies,” he said. “And that’s being used to reflect application layer attacks through the proxies on the bots as well.”&lt;/p&gt;
    &lt;p&gt;The Aisuru botnet harkens back to its predecessor Mirai in another intriguing way. One of its owners is using the Telegram handle “9gigsofram,” which corresponds to the nickname used by the co-owner of a Minecraft server protection service called Proxypipe that was heavily targeted in 2016 by the original Mirai botmasters.&lt;/p&gt;
    &lt;p&gt;Robert Coelho co-ran Proxypipe back then along with his business partner Erik “9gigsofram” Buckingham, and has spent the past nine years fine-tuning various DDoS mitigation companies that cater to Minecraft server operators and other gaming enthusiasts. Coelho said he has no idea why one of Aisuru’s botmasters chose Buckingham’s nickname, but added that it might say something about how long this person has been involved in the DDoS-for-hire industry.&lt;/p&gt;
    &lt;p&gt;“The Aisuru attacks on the gaming networks these past seven day have been absolutely huge, and you can see tons of providers going down multiple times a day,” Coelho said.&lt;/p&gt;
    &lt;p&gt;Coelho said the 15 Tbps attack this week against TCPShield was likely only a portion of the total attack volume hurled by Aisuru at the time, because much of it would have been shoved through networks that simply couldn’t process that volume of traffic all at once. Such outsized attacks, he said, are becoming increasingly difficult and expensive to mitigate.&lt;/p&gt;
    &lt;p&gt;“It’s definitely at the point now where you need to be spending at least a million dollars a month just to have the network capacity to be able to deal with these attacks,” he said.&lt;/p&gt;
    &lt;head rend="h2"&gt;RAPID SPREAD&lt;/head&gt;
    &lt;p&gt;Aisuru has long been rumored to use multiple zero-day vulnerabilities in IoT devices to aid its rapid growth over the past year. XLab, the Chinese security company that was the first to profile Aisuru’s rise in 2024, warned last month that one of the Aisuru botmasters had compromised the firmware distribution website for Totolink, a maker of low-cost routers and other networking gear.&lt;/p&gt;
    &lt;p&gt;“Multiple sources indicate the group allegedly compromised a router firmware update server in April and distributed malicious scripts to expand the botnet,” XLab wrote on September 15. “The node count is currently reported to be around 300,000.”&lt;/p&gt;
    &lt;p&gt;Aisuru’s operators received an unexpected boost to their crime machine in August when the U.S. Department Justice charged the alleged proprietor of Rapper Bot, a DDoS-for-hire botnet that competed directly with Aisuru for control over the global pool of vulnerable IoT systems.&lt;/p&gt;
    &lt;p&gt;Once Rapper Bot was dismantled, Aisuru’s curators moved quickly to commandeer vulnerable IoT devices that were suddenly set adrift by the government’s takedown, Dobbins said.&lt;/p&gt;
    &lt;p&gt;“Folks were arrested and Rapper Bot control servers were seized and that’s great, but unfortunately the botnet’s attack assets were then pieced out by the remaining botnets,” he said. “The problem is, even if those infected IoT devices are rebooted and cleaned up, they will still get re-compromised by something else generally within minutes of being plugged back in.”&lt;/p&gt;
    &lt;head rend="h2"&gt;BOTMASTERS AT LARGE&lt;/head&gt;
    &lt;p&gt;XLab’s September blog post cited multiple unnamed sources saying Aisuru is operated by three cybercriminals: “Snow,” who’s responsible for botnet development; “Tom,” tasked with finding new vulnerabilities; and “Forky,” responsible for botnet sales.&lt;/p&gt;
    &lt;p&gt;KrebsOnSecurity interviewed Forky in our May 2025 story about the record 6.3 Tbps attack from Aisuru. That story identified Forky as a 21-year-old man from Sao Paulo, Brazil who has been extremely active in the DDoS-for-hire scene since at least 2022. The FBI has seized Forky’s DDoS-for-hire domains several times over the years.&lt;/p&gt;
    &lt;p&gt;Like the original Mirai botmasters, Forky also operates a DDoS mitigation service called Botshield. Forky declined to discuss the makeup of his ISP’s clientele, or to clarify whether Botshield was more of a hosting provider or a DDoS mitigation firm. However, Forky has posted on Telegram about Botshield successfully mitigating large DDoS attacks launched against other DDoS-for-hire services.&lt;/p&gt;
    &lt;p&gt;In our previous interview, Forky acknowledged being involved in the development and marketing of Aisuru, but denied participating in attacks launched by the botnet.&lt;/p&gt;
    &lt;p&gt;Reached for comment earlier this month, Forky continued to maintain his innocence, claiming that he also is still trying to figure out who the current Aisuru botnet operators are in real life (Forky said the same thing in our May interview).&lt;/p&gt;
    &lt;p&gt;But after a week of promising juicy details, Forky came up empty-handed once again. Suspecting that Forky was merely being coy, I asked him how someone so connected to the DDoS-for-hire world could still be mystified on this point, and suggested that his inability or unwillingness to blame anyone else for Aisuru would not exactly help his case.&lt;/p&gt;
    &lt;p&gt;At this, Forky verbally bristled at being pressed for more details, and abruptly terminated our interview.&lt;/p&gt;
    &lt;p&gt;“I’m not here to be threatened with ignorance because you are stressed,” Forky replied. “They’re blaming me for those new attacks. Pretty much the whole world (is) due to your blog.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://krebsonsecurity.com/2025/10/ddos-botnet-aisuru-blankets-us-isps-in-record-ddos/"/><published>2025-10-13T23:21:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45575391</id><title>Don’t Look Up: Sensitive internal links in the clear on GEO satellites [pdf]</title><updated>2025-10-14T19:32:45.642391+00:00</updated><content/><link href="https://satcom.sysnet.ucsd.edu/docs/dontlookup_ccs25_fullpaper.pdf"/><published>2025-10-14T01:48:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45576502</id><title>Copy-and-Patch: A Copy-and-Patch Tutorial</title><updated>2025-10-14T19:32:45.383753+00:00</updated><content>&lt;doc fingerprint="e6e4b238e00f7a0a"&gt;
  &lt;main&gt;
    &lt;code&gt;int add_a_b(int a, int b) {
    return a + b
}&lt;/code&gt;
    &lt;head rend="h1"&gt;A Copy-and-Patch Tutorial&lt;/head&gt;
    &lt;p&gt;Copy-and-patch Compilation is a fascinating way of constructing a baseline JIT[1]. It permits incredibly fast runtime compilation of code fragments in a very easy to maintain fashion, requires barely any actual understanding of assembly code, and produces native code of sufficient quality to be within the same range as traditional, hand-written baseline JITs. [1]: Baseline JIT, as in a JIT whose goal is primarily to generate code quickly and gain performance by removing interpretation overhead than generating well optimized code itself. Baseline JITs can be paired with optimizing JITs, like V8’s Liftoff baseline JIT for WASM allowing tiering up into V8’s Crankshaft optimizing JIT.&lt;/p&gt;
    &lt;p&gt;Copy-and-patch works by writing stencils, minimal C functions that implement the desired individual operations such that they compile to concatenate native code fragments. At JIT compile time, one can copy the pre-compiled fragment for each operation back-to-back, patching them change embedded constants or addresses as needed..&lt;/p&gt;
    &lt;p&gt;As an adventure into understanding how copy-and-patch works, our goal will be to create the function&lt;/p&gt;
    &lt;p&gt;But specialized at runtime to compute &lt;code&gt;1 + 2&lt;/code&gt;. We’ll be doing this by first breaking it down into some bytecode-sized operations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;const_int_reg1:&lt;/p&gt;
        &lt;code&gt;a = 1;&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;const_int_reg2:&lt;/p&gt;
        &lt;code&gt;b = 2;&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;add_int1_int2:&lt;/p&gt;
        &lt;code&gt;c = a + b;&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;return_int1:&lt;/p&gt;
        &lt;code&gt;return c;&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And to define our copy-and-patch JIT, we’ll take each of these and:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Implement the operation in C with relocation holes to be later patched to form our stencil.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compile the stencil into native code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Copy-paste the native code back into a C file with functions to emit it to a buffer and patch any relocations.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then we can write our little JIT compilation engine to concatenate our stencils and execute the generated function. Let’s get started!&lt;/p&gt;
    &lt;head rend="h2"&gt;Stencils&lt;/head&gt;
    &lt;p&gt;Our first step is to define our stencils:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdint.h&amp;gt;

#define STENCIL_FUNCTION __attribute__((preserve_none))

extern char cnp_value_hole[65536];
extern void cnp_func_hole(void) STENCIL_FUNCTION;

#define STENCIL_HOLE(type) \
  (type)((uintptr_t)&amp;amp;cnp_value_hole)
#define DECLARE_STENCIL_OUTPUT(...) \
  typedef void(*stencil_output_fn)(__VA_ARGS__) STENCIL_FUNCTION; \
  stencil_output_fn stencil_output = (stencil_output_fn)&amp;amp;cnp_func_hole;

STENCIL_FUNCTION void load_int_reg1() {
  int a = STENCIL_HOLE(int);
  DECLARE_STENCIL_OUTPUT(int);
  stencil_output(a);
}

STENCIL_FUNCTION void load_int_reg2(int a) {
  int b = STENCIL_HOLE(int);
  DECLARE_STENCIL_OUTPUT(int, int);
  stencil_output(a, b);
}

STENCIL_FUNCTION void add_int1_int2(int a, int b) {
  int c = a + b;
  DECLARE_STENCIL_OUTPUT(int);
  stencil_output(c);
}

STENCIL_FUNCTION int return_int1(int a) {
  return a;
}&lt;/code&gt;
    &lt;p&gt;We compile this with &lt;code&gt;clang -O3 -mcmodel=medium -c stencils.c&lt;/code&gt;, and examine the generated code via &lt;code&gt;objdump -d -Mintel,x86-64 --disassemble --reloc stencils.o&lt;/code&gt;.  This yields:&lt;/p&gt;
    &lt;code&gt;0000000000000000 &amp;lt;load_int_reg1&amp;gt;:
   0:	41 bc 00 00 00 00    	mov    r12d,0x0
			2: R_X86_64_32	cnp_value_hole
   6:	e9 00 00 00 00       	jmp    b &amp;lt;load_int_reg1+0xb&amp;gt;
			7: R_X86_64_PLT32	cnp_func_hole-0x4
   b:	0f 1f 44 00 00       	nop    DWORD PTR [rax+rax*1+0x0]

0000000000000010 &amp;lt;load_int_reg2&amp;gt;:
  10:	41 bd 00 00 00 00    	mov    r13d,0x0
			12: R_X86_64_32	cnp_value_hole
  16:	e9 00 00 00 00       	jmp    1b &amp;lt;load_int_reg2+0xb&amp;gt;
			17: R_X86_64_PLT32	cnp_func_hole-0x4
  1b:	0f 1f 44 00 00       	nop    DWORD PTR [rax+rax*1+0x0]

0000000000000020 &amp;lt;add_int1_int2&amp;gt;:
  20:	45 01 ec             	add    r12d,r13d
  23:	e9 00 00 00 00       	jmp    28 &amp;lt;add_int1_int2+0x8&amp;gt;
			24: R_X86_64_PLT32	cnp_func_hole-0x4
  28:	0f 1f 84 00 00 00 00 	nop    DWORD PTR [rax+rax*1+0x0]
  2f:	00

0000000000000030 &amp;lt;return_int1&amp;gt;:
  30:	44 89 e0             	mov    eax,r12d
  33:	c3                   	ret&lt;/code&gt;
    &lt;p&gt;(The NOP’s aren’t actually a part of the function, they’re just padding added so that each function starts with 16 byte alignment.)&lt;/p&gt;
    &lt;p&gt;For each of these stencils, we fill in a template to form our stencil generation library to use during JITing.&lt;/p&gt;
    &lt;code&gt;uint8_t cnp_stencil_&amp;lt;OP&amp;gt;_code[] = {
  // Copy the bytes from the top of the function until the jmp.
};

uint8_t* cnp_copy_&amp;lt;OP&amp;gt;(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_&amp;lt;OP&amp;gt;_code);
  memcpy(stencil_start, cnp_stencil_&amp;lt;OP&amp;gt;_code, stencil_size);
  return stencil_start + stencil_size;
}

// If any relocations exist for the stencil, fill in the values.
// If not, just skip writing this function.
void cnp_patch_&amp;lt;OP&amp;gt;(uint8_t* stencil_start, /* ... */ ) {
  memcpy(stencil_start + /*relocation_offset*/, &amp;amp;value, /* relocation_size */);
}&lt;/code&gt;
    &lt;p&gt;So let’s get started!&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdint.h&amp;gt;

uint8_t cnp_stencil_load_int_reg1_code[] = {
   0x41, 0xbc, 0x00, 0x00, 0x00, 0x00, // mov r12d,0x0
};
uint8_t* cnp_copy_load_int_reg1(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_load_int_reg1_code);
  memcpy(stencil_start, cnp_stencil_load_int_reg1_code, stencil_size);
  return stencil_start + stencil_size;
}
void cnp_patch_load_int_reg1(uint8_t* stencil_start, int value) {
  // 2: R_X86_64_32 cnp_value_hole  -&amp;gt;  0x02 offset
  memcpy(stencil_start + 0x2, &amp;amp;value, sizeof(value));
}

uint8_t cnp_stencil_load_int_reg2_code[] = {
   0x41, 0xbd, 0x00, 0x00, 0x00, 0x00, // mov r13d,0x0
};
uint8_t* cnp_copy_load_int_reg2(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_load_int_reg2_code);
  memcpy(stencil_start, cnp_stencil_load_int_reg2_code, stencil_size);
  return stencil_start + stencil_size;
}
void cnp_patch_load_int_reg2(uint8_t* stencil_start, int value) {
  // 12: R_X86_64_32 cnp_value_hole  -&amp;gt;  0x12 - 0x10 base = 0x2
  memcpy(stencil_start + 0x2, &amp;amp;value, sizeof(value));
}

uint8_t cnp_stencil_add_int1_int2_code[] = {
  0x45, 0x01, 0xec, // add r12d,r13d
};
uint8_t* cnp_copy_add_int1_int2(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_add_int1_int2_code);
  memcpy(stencil_start, cnp_stencil_add_int1_int2_code, stencil_size);
  return stencil_start + stencil_size;
}
// No patching needed

uint8_t cnp_stencil_return_int1_code[] = {
  0x44, 0x89, 0xe0, // mov eax,r12d
  0xc3,             // ret
};
uint8_t* cnp_copy_return_int1(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_return_int1_code);
  memcpy(stencil_start, cnp_stencil_return_int1_code, stencil_size);
  return stencil_start + stencil_size;
}
// No patching needed&lt;/code&gt;
    &lt;p&gt;In a fully automated setup, all of this work will happen as part of the build system. The stencil compilation and transforming them into a library of copy functions and patch functions happens as part running &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your First JIT&lt;/head&gt;
    &lt;p&gt;With our stencil library in place, we can use our code generation functions to build our runtime specialized adder:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;assert.h&amp;gt;
#include &amp;lt;stdint.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;

//#include "cnp_stencils.h"
uint8_t* cnp_copy_load_int_reg1(uint8_t* stencil_start);
void cnp_patch_load_int_reg1(uint8_t* stencil_start, int value);
uint8_t* cnp_copy_load_int_reg2(uint8_t* stencil_start);
void cnp_patch_load_int_reg2(uint8_t* stencil_start, int value);
uint8_t* cnp_copy_add_int1_int2(uint8_t* stencil_start);
uint8_t* cnp_copy_return_int1(uint8_t* stencil_start);

typedef int(*jit_func)() __attribute__((preserve_none));

jit_func create_add_1_2() {
  // Most systems mark memory as non-executable by default
  // and mprotect() to set memory as executable needs
  // to be run against mmap-allocated memory.  We start
  // by allocating it as read/write, and then switch it
  // to write/execute once we're done writing the code.
  uint8_t* codedata = mmap(NULL, 256, PROT_READ | PROT_WRITE,
      MAP_PRIVATE | MAP_ANONYMOUS | MAP_POPULATE, -1, 0);
  assert (codedata != MAP_FAILED);
  jit_func ret = (jit_func)codedata;

  // Concatenate our program together, while saving the
  // locations that need to be patched.
  uint8_t* load_int_reg1_location = codedata;
  codedata = cnp_copy_load_int_reg1(codedata);
  uint8_t* load_int_reg2_location = codedata;
  codedata = cnp_copy_load_int_reg2(codedata);
  codedata = cnp_copy_add_int1_int2(codedata);
  codedata = cnp_copy_return_int1(codedata);

  // Overwrite the zero value placeholders with our intended
  // specialized values: 1 and 2.
  cnp_patch_load_int_reg1(load_int_reg1_location, 1);
  cnp_patch_load_int_reg2(load_int_reg2_location, 2);

  // Now that we're done writing, remove write access and
  // allow execution from this page instead.
  int rc = mprotect(ret, 256, PROT_READ | PROT_EXEC);
  if (rc) {
    perror("mprotect");
  }
  return ret;
}

int main() {
  jit_func add_1_2 = create_add_1_2();
  int result = add_1_2();
  printf("JIT'd 1 + 2 = %d\n", result);
  return 0;
}&lt;/code&gt;
    &lt;p&gt;And now we can compile and run that!&lt;/p&gt;
    &lt;quote&gt;$ clang cnp_jit.c cnp_stencils.c -o cnp_jit $ ./cnp_jit JIT'd 1 + 2 = 3&lt;/quote&gt;
    &lt;p&gt;We’ve successfully built runtime code generation, while letting clang do the hard work of actually writing the assembly code, and our JIT compiler is just a bunch of memcpy calls!&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Yourself&lt;/head&gt;
    &lt;p&gt;Here’s a header to offer some macros to make declaring relocation holes easier:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdint.h&amp;gt;

#define STENCIL_FUNCTION __attribute__((preserve_none))

extern void cnp_stencil_output(void) STENCIL_FUNCTION;

#define STENCIL_HOLE32(ordinal, type) \
  (type)((uintptr_t)&amp;amp;cnp_small_value_hole_##ordinal)
#define STENCIL_HOLE64(ordinal, type) \
  (type)((uintptr_t)&amp;amp;cnp_large_value_hole_##ordinal)
#define STENCIL_FN_NEAR(ordinal, type) \
  (type)&amp;amp;cnp_near_func_hole_##ordinal
#define STENCIL_FN_FAR(ordinal, type) \
  ({ uint64_t _cnp_addr_as_int = (uint64_t)((uintptr_t)&amp;amp;cnp_far_func_hole_##ordinal); \
  asm volatile("" : "+r" (_cnp_addr_as_int) : : "memory"); \
  (type)_cnp_addr_as_int; })
#define DECLARE_STENCIL_OUTPUT(...) \
  typedef void(*stencil_output_fn)(__VA_ARGS__) STENCIL_FUNCTION; \
  stencil_output_fn stencil_output = (stencil_output_fn)&amp;amp;cnp_stencil_output;

#define DECLARE_EXTERN_HOLES(ordinal) \
extern char cnp_large_value_hole_##ordinal[100000]; \
extern char cnp_small_value_hole_##ordinal[8]; \
extern void cnp_near_func_hole_##ordinal(void) STENCIL_FUNCTION; \
extern char cnp_far_func_hole_##ordinal[100000];&lt;/code&gt;
    &lt;p&gt;(If you’re interested in the details of why these macros are the way they are, see the next post in the series!)&lt;/p&gt;
    &lt;p&gt;Then you can declare as complex of a stencil as you need:&lt;/p&gt;
    &lt;code&gt;#include "cnp_stencils.h"

// Declare up to the maximum number of holes you need of one type
// in a function:
DECLARE_EXTERN_HOLES(1);
DECLARE_EXTERN_HOLES(2);

STENCIL_FUNCTION
void fused_multiply_add_sqrt_ifnotzero() {
  uint32_t a = STENCIL_HOLE32(1, uint32_t);
  uint32_t b = STENCIL_HOLE32(2, int32_t);
  uint64_t c = STENCIL_HOLE64(1, uint64_t);

  uint64_t fma = a * b + c;

  if (fma == 0) {
    void (*div_trap)(void) = STENCIL_FN_NEAR(1, void(*)(void));
    div_trap();
  }

  uint64_t (*sqrt)(uint64_t) = STENCIL_FN_FAR(1, uint64_t(*)(uint64_t));
  uint64_t result = sqrt(c);

  DECLARE_STENCIL_OUTPUT(uint64_t);
  stencil_output(result);
}&lt;/code&gt;
    &lt;p&gt;Which just for completeness sake, compiles into:&lt;/p&gt;
    &lt;quote&gt;0000000000000000 &amp;lt;fused_multiply_add_sqrt_ifnotzero&amp;gt;: 0: 50 push rax 1: b8 00 00 00 00 mov eax,0x0 2: R_X86_64_32 cnp_small_value_hole_2 6: b9 00 00 00 00 mov ecx,0x0 7: R_X86_64_32 cnp_small_value_hole_1 b: 0f af c8 imul ecx,eax e: 48 b8 00 00 00 00 00 movabs rax,0x0 15: 00 00 00 10: R_X86_64_64 cnp_large_value_hole_1 18: 48 01 c8 add rax,rcx 1b: 75 05 jne 22 &amp;lt;fused_multiply_add_sqrt_ifnotzero+0x22&amp;gt; 1d: e8 00 00 00 00 call 22 &amp;lt;fused_multiply_add_sqrt_ifnotzero+0x22&amp;gt; 1e: R_X86_64_PLT32 cnp_near_func_hole_1-0x4 22: 48 b8 00 00 00 00 00 movabs rax,0x0 29: 00 00 00 24: R_X86_64_64 cnp_far_func_hole_1 2c: 48 bf 00 00 00 00 00 movabs rdi,0x0 33: 00 00 00 2e: R_X86_64_64 cnp_large_value_hole_1 36: ff d0 call rax 38: 49 89 c4 mov r12,rax 3b: 58 pop rax 3c: e9 00 00 00 00 jmp 41 &amp;lt;fused_multiply_add_sqrt_ifnotzero+0x41&amp;gt; 3d: R_X86_64_PLT32 cnp_stencil_output-0x4&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://transactional.blog/copy-and-patch/tutorial"/><published>2025-10-14T05:14:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45578117</id><title>KDE celebrates the 29th birthday and kicks off the yearly fundraiser</title><updated>2025-10-14T19:32:44.963374+00:00</updated><content>&lt;doc fingerprint="abd5cf5cba4a215f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Happy Birthday to us&lt;/head&gt;
    &lt;p&gt;This week is KDEâs 29th anniversary. It may not be a nice round number like 25 or 30, but whenever another birthday rolls around for an independent project the size and scope of KDE â powered by the goodwill of its contributors and users â thatâs really quite something!&lt;/p&gt;
    &lt;p&gt;This year weâre celebrating by kicking off our yearly fundraiser. Letâs raise at least â¬50,000 before the end of the year!&lt;/p&gt;
    &lt;head rend="h3"&gt;Donated (updated daily)&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;â¬50000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;*Stretch goal coming soon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Make KDEâs Birthday Wishes Come True&lt;/head&gt;
    &lt;head rend="h3"&gt;Producing first-class software&lt;/head&gt;
    &lt;p&gt;KDE is on the verge of something big, and the popularity of its free software is on the rise. Itâs increasingly being adopted by gamers, artists, professionals, companies, and public institutions. But the costs associated with developing and maintaining that software are also growing.&lt;/p&gt;
    &lt;p&gt;Your donation keeps KDE âin businessâ and our software sustainable for generations to come.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping you in control&lt;/head&gt;
    &lt;p&gt;A core aim of KDE is keeping you in control of your digital life, and we do it by providing high-quality and privacy-conscious free software. But we can only keep doing it by preserving our own financial independence, so that we never become too dependent on any single source of support.&lt;/p&gt;
    &lt;p&gt;Your donation makes KDE truly independent. Funding from the people allows us keep KDE developed by the people, of the people, and for the people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cleaning up the world&lt;/head&gt;
    &lt;p&gt;This week is also International E-Waste Day, and KDE wants a clean planet too! We started the End of 10 campaign because big tech corporations continue pushing everyone to chase the new shinyâ¦ in the process filling landfills with perfectly functional devices that become terrible sources of pollution when junked*.&lt;/p&gt;
    &lt;p&gt;Your donation allows us to inform everybody about how they can help stave off these environmental disasters.&lt;/p&gt;
    &lt;p&gt;* Case in point: Microsoft is stopping free support for Windows 10 on hundreds of millions of computers this very week. Many of these old yet perfectly usable devices will not be able to upgrade because of spurious hardware requirements. Microsoftâs solution? âThrow away your computer and pollute the planet because we want to make even more money.â&lt;/p&gt;
    &lt;head rend="h3"&gt;Reaching people the tech industry left behind&lt;/head&gt;
    &lt;p&gt;Many arenât in a position to replace their devices every few years, or download hundreds of gigabyes of data from an always-on internet connection. KDE produces software that doesnât need the latest hardware or an always-on internet connection, allowing everybody find their space in the digitized world.&lt;/p&gt;
    &lt;p&gt;Your donation helps KDE serve those who are ignored by the industry, and bring marginalized users into the community so they can help the project grow for everyone.&lt;/p&gt;
    &lt;head rend="h3"&gt;Helping public institutions adopt free software&lt;/head&gt;
    &lt;p&gt;The governments of the world are starting to realize that using public funds to lock themselves into proprietary closed-source software has been a strategic geopolitical mistake.&lt;/p&gt;
    &lt;p&gt;Free software is publicly owned, representing the safest option for governments that want full control over their machines and safety for their citizensâ data. But often the standards required for software approval by public institutions is very high, and their needs very specific.&lt;/p&gt;
    &lt;p&gt;Your donation helps KDE adapt our software to what public institutions require, clearing the way for your tax dollars to fund KDE, not some big foreign companies.&lt;/p&gt;
    &lt;p&gt;Images "Konqi opens the magic box", ""Katie and Konqi make software", "Katie and Konqi take on the public administration" CC-BY-SA-4.0 license by Arctaxia. "Katie &amp;amp; Konqi recyle" CC-BY-SA-4.0 license by Nezumi CafunÃ©.&lt;/p&gt;
    &lt;head rend="h2"&gt;Goodies&lt;/head&gt;
    &lt;p&gt;Donât forget to download your goodies after you donated! Get digital badges, printable cards, and more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kde.org/fundraisers/yearend2025/"/><published>2025-10-14T09:54:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45578383</id><title>ADS-B Exposed</title><updated>2025-10-14T19:32:44.504952+00:00</updated><content>&lt;doc fingerprint="5bd73eafb8c37a0a"&gt;
  &lt;main&gt;
    &lt;p&gt;Proudly made with ClickHouse open-source database Picture from Wikipedia, © the details at the corresponding page. 👁&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adsb.exposed/"/><published>2025-10-14T10:38:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45578467</id><title>Nexperia – Update on Company Developments</title><updated>2025-10-14T19:32:44.279144+00:00</updated><content>&lt;doc fingerprint="7f545dc481b558e7"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Update on company developments&lt;/head&gt;October 14, 2025&lt;p&gt;Nijmegen -- Please find below an update on recent developments that concern Nexperia’s governance and operations.&lt;/p&gt;&lt;head rend="h3"&gt;Enterprise chamber ruling&lt;/head&gt;&lt;p&gt;Following an emergency hearing, the Dutch Enterprise Chamber on 7 October 2025 concluded provisionally that there are valid reasons to doubt sound management at Nexperia under the leadership of former CEO Zhang Xuezheng. The Enterprise Chamber ruled that, as an immediate measure, Zhang Xuezheng has been suspended as a director. Accordingly, Zhang Xuezheng is no longer acting as CEO of Nexperia.&lt;/p&gt;&lt;p&gt;Furthermore, the Enterprise Chamber ruled that substantially all voting rights on the shares in Nexperia, indirectly held by Wingtech Technology Co. Ltd. ("Wingtech"), have been placed under management of an independent administrator appointed by the Enterprise Chamber.&lt;/p&gt;&lt;p&gt;Going forward, CFO Stefan Tilger will be acting as interim CEO, Achim Kempe will continue to be the COO and CLO Ruben Lichtenberg will function as the statutory director of Nexperia Holding B.V. and Nexperia B.V., together with Guido Dierick, who has been appointed by the Enterprise Chamber as non-executive director.&lt;/p&gt;&lt;head rend="h3"&gt;Dutch government order&lt;/head&gt;&lt;p&gt;Due to the same serious managerial shortcomings, the Dutch Ministry of Economic Affairs observed that Nexperia's operations in Europe were being compromised in an unacceptable manner. This situation raised broader concerns for the Dutch government about the availability of semiconductor products critical to the European industry.&lt;/p&gt;&lt;p&gt;The combination of Zhang Xuezheng's behaviour as CEO and (indirect) shareholder, as well as concerns about the semiconductor product availability in the Netherlands and Europe, ultimately led to the Dutch government to intervene with an exceptional emergency order on the basis of the Goods Availability Act (Wbg).&lt;/p&gt;&lt;p&gt;Under this order, Nexperia is prohibited by the Dutch government from relocating company parts, firing existing executives and/or making other decisions without explicit permission from the Dutch government for a period of a year. This order is intended to prevent the goods manufactured by Nexperia from becoming unavailable, thus protecting Dutch and European economic security. The order is focused on ensuring business continuity. As such, Nexperia is positive that day-to-day operations can continue.&lt;/p&gt;&lt;head rend="h3"&gt;Export control measures&lt;/head&gt;&lt;p&gt;On 29 September 2025, the United States Bureau of Industry and Security ("BIS") issued a rule extending US export control restrictions to entities at least 50% owned by one or more entities on the US Entity List. Although Nexperia is not explicitly mentioned, the company is affected due to its status as a wholly owned subsidiary of Wingtech Technology Co., Ltd. (“Wingtech”), that was put on the BIS entity list last December. While Nexperia has made sufficient preparations to ensure business continuity – and as the BIS rule provides for a 60-day grace period – we are confident that a solution will be found.&lt;/p&gt;&lt;p&gt;On 4 October 2025, the Chinese Ministry of Commerce issued an export control notice prohibiting Nexperia China and its subcontractors from exporting specific finished components and sub-assemblies manufactured in China. Nexperia is actively engaging with the Chinese authorities to obtain an exemption from these restrictions and has deployed all available resources to that end. Nexperia is in close dialogue with all relevant national and local government authorities to mitigate the impact of this measure.&lt;/p&gt;&lt;head rend="h3"&gt;About Nexperia&lt;/head&gt;&lt;p&gt;Headquartered in the Netherlands, Nexperia is a global semiconductor company with a rich European history and over 12,500 employees across Europe, Asia, and the United States. As a leading expert in the development and production of essential semiconductors, Nexperia’s components enable the basic functionality of virtually every commercial electronic design in the world – from automotive and industrial to mobile and consumer applications.&lt;lb/&gt; The company serves a global customer base, shipping more than 110 billion products annually. These products are recognized as benchmarks in efficiency – in process, size, power, and performance. Nexperia's commitment to innovation, efficiency, sustainability, and stringent industry requirements is evident in its extensive IP portfolio, its expanding product range, and its certification to IATF 16949, ISO 9001, ISO 14001 and ISO 45001 standards.&lt;/p&gt;&lt;p&gt;For press information, please contact:&lt;/p&gt;&lt;p&gt;Hannes van Raemdonck, Head of Advocacy and Alliances&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nexperia.com/about/news-events/press-releases/update-on-company-developments"/><published>2025-10-14T10:52:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45578540</id><title>Zoo of Array Languages</title><updated>2025-10-14T19:32:44.200082+00:00</updated><content>&lt;doc fingerprint="378169306b47e15c"&gt;
  &lt;main&gt;
    &lt;quote&gt;•ktye/k run src intro apl360 pdp11 tokenize halfkey xk fem flow •the k incunabulum •zoo of array languages APL\360 ngn/apl APL\iv BQN KAP incunabulum APL\? j4.2 jstack oK ktye/k2 ktye/k.w klong ngn/k k7 k9 ktye/k lil •j stack language •edit&lt;/quote&gt;
    &lt;quote&gt;ktye/k ktye.github.io/k.html + flp add ' ech pri both bin - neg sub / ovr fix echright * fst mul \ scn fix eachleft % sqr div / join decode ! til key mod \ split encode &amp;amp; wer min $[a;b;...] cond | rev max while[c;a;b;d;e;..] &amp;lt; asc les f:{x+y} [bl;o;ck] &amp;gt; dsc mor "chars" c = grp eql 01234567 1 2 3 i ~ not mtc :+-*%&amp;amp;| .4 5 6. f , enl cat &amp;lt;&amp;gt;=~!,^# 2a300 z ^ srt cut _$?@. (1;2 3) L # cnt tak `a`b!5 6 D _ flr drp t,d t,t t,'t join $ str cst k!t key ? unq fnd in k?t group @ typ atx @[x;i;+;y] amend . val cal .[x;i;+;y] dmend abs sin cos exp log find angle imag conj types:cisfzLDTvcdlx ?n(uniform) ?-n(normal) ?z(bi) n?n(with) random -n?n(w/o)&lt;/quote&gt;
    &lt;quote&gt;╔═[■]════════ TURBO.K ════════[↑]═╗ ║ ║ ║ ║ ║ K ║ ║ ║ ║ ║ ║ ║ ║ ASM ╠════════════════════╣ ║ ║ ║ ║ ║ C ║ ║ ║ ║ ║ ║ ║ ╠════════════╩════════════════════╣ ║ ║ ║ WATCH ║ ║ ║ ╚══1:1════════════════════════════╝• jtye/k: k in fifty functions&lt;/quote&gt;
    &lt;quote&gt;+ type add ' each prior bin `js` - neg sub / over right join dec * sqr mul \ scan left split enc % sqrt div inv idiv mod &amp;amp; flip min atom | rev max atomic &amp;lt; up less curry &amp;gt; down more rec = freq eql ~ not match . value parse ! til dict token key where @ first at amend ? uniq find rand ^ sort cut while()[;;] # count take if()[;;;;;] _ floor drop do[]while() , list cat for(;;)[;;] $ string try[]catch(e)[]&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ktye.github.io/"/><published>2025-10-14T11:01:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45578990</id><title>Kyber (YC W23) Is Hiring an Enterprise AE</title><updated>2025-10-14T19:32:43.272256+00:00</updated><content>&lt;doc fingerprint="f15468be0e97206f"&gt;
  &lt;main&gt;
    &lt;p&gt;Instantly draft, review, and send complex regulatory notices.&lt;/p&gt;
    &lt;p&gt;At Kyber, we're building the next-generation document platform for enterprises. Today, our AI-native solution transforms regulatory document workflows, enabling insurance claims organizations to consolidate 80% of their templates, spend 65% less time drafting, and compress overall communication cycle times by 5x. Our vision is for every enterprise to seamlessly leverage AI templates to generate every document.&lt;/p&gt;
    &lt;p&gt;Over the past 9 months, we’ve:&lt;/p&gt;
    &lt;p&gt;Kyber is backed by top Silicon Valley VCs, including Y Combinator and Fellows Fund.&lt;/p&gt;
    &lt;p&gt;We’re now looking for elite Enterprise Account Executives who can drive pipeline, navigate complex multi-threaded enterprise sales environments, close deals, and own the full sales cycle in order to scale our impact across the insurance industry and beyond.&lt;/p&gt;
    &lt;p&gt;Responsibilities:&lt;/p&gt;
    &lt;p&gt;You'll play a critical role in driving revenue growth by:&lt;/p&gt;
    &lt;p&gt;Owning the Full Sales Cycle:&lt;/p&gt;
    &lt;p&gt;Executing Outbound Strategies:&lt;/p&gt;
    &lt;p&gt;Enhancing Sales Operations:&lt;/p&gt;
    &lt;p&gt;Strategic Account Management:&lt;/p&gt;
    &lt;p&gt;What We're Looking For in You:&lt;/p&gt;
    &lt;p&gt;Olympic Work Ethic Focused On Results:&lt;/p&gt;
    &lt;p&gt;Outstanding Communication Skills:&lt;/p&gt;
    &lt;p&gt;Relentlessly Resourceful:&lt;/p&gt;
    &lt;p&gt;Team Player with an Owner’s Mindset:&lt;/p&gt;
    &lt;p&gt;Join us in building and scaling a game-changing enterprise product powered by state-of-the-art AI. At Kyber, your contributions will directly impact how businesses handle some of their most critical workflows and customer interactions.&lt;/p&gt;
    &lt;p&gt;If you’re obsessed with growth, AI, and transforming enterprise workflows, we’d love to hear from you!&lt;/p&gt;
    &lt;p&gt;We want to hear from extraordinary individuals who are ready to shape the future of enterprise documents. To stand out, ask someone you’ve worked with to send your resume or LinkedIn profile, along with a brief 2-3 sentence endorsement, directly to arvind [at] askkyber.com.&lt;/p&gt;
    &lt;p&gt;Referrals matter - they help us understand the impact you’ve already had and the kind of teammate you’ll be. A strong referee can elevate your application, so choose someone who knows your skills and character well.&lt;/p&gt;
    &lt;p&gt;Apply today and help us bring enterprise documents into the AI-native age.&lt;/p&gt;
    &lt;p&gt;*Listed salary range is for OTE&lt;/p&gt;
    &lt;p&gt;With Kyber, companies operating in regulated industries can quickly draft, review, and send complex regulatory notices. For example, when Branch Insurance's claims team has to settle a claim, instead of spending hours piecing together evidence to draft a complex notice, they can simply upload the details of the claim to Kyber, auto-generate multiple best in-class drafts, easily assign reviewers, collaborate on notices in real-time, and then send the letter to the individual the notice is for. Kyber not only saves these teams time, it also improves overall quality, accountability, and traceability.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/kyber/jobs/BQRRSrZ-enterprise-account-executive-ae"/><published>2025-10-14T12:00:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45579275</id><title>Pyrefly: Python type checker and language server in Rust</title><updated>2025-10-14T19:32:43.037877+00:00</updated><content>&lt;doc fingerprint="4bb22cff501cd0f2"&gt;
  &lt;main&gt;
    &lt;p&gt;A fast type checker and language server for Python with powerful IDE features&lt;/p&gt;
    &lt;code&gt;$ pip install pyrefly &amp;amp;&amp;amp; pyrefly init&lt;/code&gt;
    &lt;head rend="h3"&gt;Scale with Confidence&lt;/head&gt;
    &lt;p&gt;Type check over 1.85 million lines of code per second.ⓘTested using Meta infrastructure (166 cores, 228 GB RAM)&lt;/p&gt;
    &lt;head rend="h3"&gt;Developer Delight&lt;/head&gt;
    &lt;p&gt;Get lightning fast autocomplete, and catch errors with instant feedback in your favorite editor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Support at your Fingertips&lt;/head&gt;
    &lt;p&gt;Have questions or feedback to share? Connect with us on Discord&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Comparison&lt;/head&gt;
    &lt;p&gt;Type checking the PyTorch codebase from scratch.ⓘTested using Macbook&lt;lb/&gt;(10 cores: 8 performance + 2 efficiency cores, 32 GB RAM)&lt;/p&gt;
    &lt;p&gt;(10 cores: 8 performance + 2 efficiency cores, 32 GB RAM)&lt;/p&gt;
    &lt;p&gt;PyreflyⓘCommand: "pyrefly check"&lt;lb/&gt;Pyrefly uses as many threads as possible&lt;/p&gt;
    &lt;p&gt;Pyrefly uses as many threads as possible&lt;/p&gt;
    &lt;p&gt;PyrightⓘCommand: "pyright --threads=8"&lt;lb/&gt;8 threads yielded the best performance after testing multiple settings&lt;/p&gt;
    &lt;p&gt;8 threads yielded the best performance after testing multiple settings&lt;/p&gt;
    &lt;p&gt;MyPyⓘCommand: "dmypy run"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pyrefly.org/?featured_on=talkpython"/><published>2025-10-14T12:33:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45579708</id><title>CRISPR-like tools that finally can edit mitochondria DNA</title><updated>2025-10-14T19:32:42.050374+00:00</updated><content>&lt;doc fingerprint="fa5893102835c8f9"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.&lt;/p&gt;
    &lt;p&gt;The rings of DNA inside mitochondria are inaccessible to these techniques, which means that precise edits to mitochondrial DNA (mtDNA) remain frustratingly out of reach. “Mitochondria missed the CRISPR–Cas9 revolution that exploded 12 years ago,” says Michal Minczuk, a geneticist at the University of Cambridge, UK.&lt;/p&gt;
    &lt;p&gt;But researchers are eager to access this DNA, says Minczuk. Mitochondria are bean-shaped organelles that power cells and have myriad other cellular tasks. Exploring their DNA is essential for understanding the energy production and exchange that underlies metabolic health. And more than 300 mutations in this DNA cause mitochondrial diseases — incurable genetic disorders with a wide range of symptoms that can rob people of their sight and hearing, trigger muscle problems and spark seizures1. These disorders affect roughly 1 in 5,000 people.&lt;/p&gt;
    &lt;p&gt;Because CRISPR can’t help with these problems, researchers have been looking for other ways to precisely edit the mitochrondrial genome. And the past few years have brought some success: the tools are already proving to be a boon for creating accurate animal models of mitochondrial diseases. “The progress has been remarkable,” says Jin-Soo Kim, a chemical biologist who develops mtDNA editing tools at the Korea Advanced Institute of Science and Technology in Daejeon, South Korea.&lt;/p&gt;
    &lt;p&gt;If researchers can make mtDNA editing safe and accurate enough, it could eventually be used to treat, and even cure, these genetic conditions. “It would be a medical breakthrough,” says Kim.&lt;/p&gt;
    &lt;p&gt;A bacterial origin&lt;/p&gt;
    &lt;p&gt;The exact origins of mitochondria are murky, but the leading theory holds that the organelle’s story started around 1.5 billion years ago when a single-celled microorganism called an archaeon gobbled up a roaming bacterium that survived inside its host. This event marked the beginning of the eukaryotes — the large group of organisms, including plants, animals and fungi, in which cells contain organelles that are enclosed inside membranes. The swallowed bacterium retained its characteristic circular DNA as it settled into its new home, but over time it sacrificed most of its genes to the nuclear genome of its host.&lt;/p&gt;
    &lt;p&gt;In the evolutionary lineage that gave rise to humans and other animals, this genetic transfer whittled the resident bacterium’s genome down to just 37 genes that code for 13 proteins involved in energy production, turning it into a specialized organelle.&lt;/p&gt;
    &lt;p&gt;The small amount of mitochondrial DNA that stuck around in animals differs in key ways from nuclear DNA, which in humans encodes around 20,000 genes. For a start, mtDNA is typically inherited solely from the mother. There can be several copies of mtDNA in each mitochondrion, and the organelle has its own built-in machinery for making RNA and proteins from that DNA.&lt;/p&gt;
    &lt;p&gt;Mitochondrial DNA is also much more error-prone, with a mutation rate estimated to be 10–20 times greater than that of nuclear DNA. This is in part because it has to contend with a barrage of damaging reactive oxygen species — unstable molecules that are generated in mitochondria during normal energy production. But it’s also because it doesn’t have histones: the proteins that protect and package nuclear DNA.&lt;/p&gt;
    &lt;p&gt;Compared with its counterpart in the nucleus, mtDNA’s toolkit for repairing itself is rudimentary. The nucleus is quick to fix a snapped DNA strand using an arsenal of repair mechanisms, but mitochondria can mend only some defects. They often simply throw away their broken DNA. This difference limits the options for gene-editing tools, because nearly all such tools for nuclear DNA use its inherent repair pathways.&lt;/p&gt;
    &lt;p&gt;It has been notoriously challenging to develop approaches for modifying mitochondrial DNA, says Stephen Ekker, a molecular biologist at the University of Texas at Austin. “Its bacterial origins are revealed when you start trying to edit it,” he says.&lt;/p&gt;
    &lt;p&gt;The most crucial hurdle for scientists trying to tinker with the mitochondrial genome is that it is locked behind a wall of membranes that doesn’t allow external nucleic acids to pass into the organelle. Although there have been hints that CRISPR-based gene-editing tools — which rely on RNA to guide them to the correct sequence — might be able to overcome these barriers, many researchers remain unconvinced.&lt;/p&gt;
    &lt;p&gt;Snip and trash&lt;/p&gt;
    &lt;p&gt;Still, there are other ways in. More than a decade before CRISPR became a research tool, mitochondria researchers began experimenting with other editing tools that could cross mitochondrial membranes and coax the organelles into ditching their problematic DNA2.&lt;/p&gt;
    &lt;p&gt;Every cell contains a vast number of mitochondrial genomes, because cells contain thousands of mitochondria and each one can carry several copies of mtDNA. Healthy and mutated mtDNA often coexist: a state known as heteroplasmy. It’s when the proportion of mutated mtDNA reaches 60–80% in a particular tissue or cell type that mitochondrial diseases manifest3.&lt;/p&gt;
    &lt;p&gt;If researchers could reduce the faulty copies of mtDNA in cells, they could eliminate the resulting disease. So, they turned to enzymes called zinc finger nucleases (ZFNs) and transcription activator-like effector nucleases (TALENs) to snip the double-stranded mtDNA. Whereas targeted snipping of nuclear DNA cajoles the cut DNA strands to glue themselves back together without the harmful mutation, the cut DNA in mitochondria is simply cast out. This elimination triggers the remaining intact copies to replicate themselves so that the correct level of mtDNA is maintained.&lt;/p&gt;
    &lt;p&gt;In most cases, the mutated copies will be reduced to an acceptable level as the normal copies are replicated. “That’s going to make up for what you’re destroying,” says Carlos Moraes, a geneticist at the University of Miami in Florida.&lt;/p&gt;
    &lt;p&gt;Although there has been progress with this approach, it hasn’t made its way out of the laboratory. And even if it did reach the clinic, the technique would be powerless against diseases caused by mutations that are often present in all copies of a person’s mtDNA, such as Leber’s hereditary optic neuropathy (LHON), a rare condition that causes rapid vision loss.&lt;/p&gt;
    &lt;p&gt;What researchers need are tools that do more than cut DNA but that don’t rely on guide RNA.&lt;/p&gt;
    &lt;p&gt;CRISPR-free base editing&lt;/p&gt;
    &lt;p&gt;When CRISPR–Cas9 emerged as a tool in 2012, it became the go-to gene editor for all kinds of application. A guide RNA directs the Cas9 enzyme to a specific DNA sequence, where the enzyme does the cutting. Genetic changes are introduced as the DNA repairs itself.&lt;/p&gt;
    &lt;p&gt;The approach became even more useful in 2016, when David Liu, a chemical biologist at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, and his colleagues introduced a more precise technique called base editing4. In this case, researchers modify the Cas9 enzyme and rely on another enzyme, called a deaminase, to convert one DNA base letter to another — such as cytosine (C) to thymine (T) or adenine (A) to guanine (G).&lt;/p&gt;
    &lt;p&gt;Although base editing and other CRISPR techniques took off for nuclear DNA, Liu and other research teams couldn’t get it working on mtDNA. Because CRISPR’s guide RNA doesn’t readily pass through a mitochondrion’s double membrane, using precise tools on mtDNA remained a pipe dream. “We did not have much success,” says Liu.&lt;/p&gt;
    &lt;p&gt;A solution materialized in 2018 when Joseph Mougous, a microbiologist then at the University of Washington in Seattle, and his colleagues stumbled across a toxin made by the bacterium Burkholderia cenocepacia. This enzyme, a deadly weapon against other bacteria, wreaks havoc by ultimately converting base C to T across the bacterial genome5.&lt;/p&gt;
    &lt;p&gt;Mougous, now based at Yale University in New Haven, Connecticut, e-mailed Liu asking whether the enzyme, called DddA, would be of any use to him. “I knew exactly what it might be used for — base editing mtDNA!” says Liu.&lt;/p&gt;
    &lt;p&gt;But switching every C to a T would be lethal to cells. Liu and his colleagues set out to “tame the beast”. They split DddA into two inactive pieces so that the enzyme would do its handiwork on mtDNA only when the pieces were brought together in a particular orientation. And instead of using guide RNA, Liu and his colleagues modified proteins found in TALENs to direct the DddA segments to their target sequences (see ‘Making the edit’).&lt;/p&gt;
    &lt;p&gt;Enjoying our latest content? Log in or create an account to continue&lt;/p&gt;
    &lt;p&gt;Access the most recent journalism from Nature's award-winning team&lt;/p&gt;
    &lt;p&gt;Explore the latest features &amp;amp; opinion covering groundbreaking research&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nature.com/articles/d41586-025-03307-x"/><published>2025-10-14T13:21:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45580315</id><title>Wireshark 4.6.0 Supports macOS Pktap Metadata (PID, Process Name, etc.)</title><updated>2025-10-14T19:32:41.885302+00:00</updated><content>&lt;doc fingerprint="22b0415e58bd4591"&gt;
  &lt;main&gt;
    &lt;p&gt;Four years after my post on doing network captures on macOS with Process ID, Wireshark 4.6.0 has been released which includes support for parsing this extra metadata, including the process info.&lt;/p&gt;
    &lt;p&gt;So how do you do it? Easy! You just need the &lt;code&gt;pktap&lt;/code&gt; interface parameter.&lt;/p&gt;
    &lt;p&gt;From the tcpdump(1) man page:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Alternatively, to capture on more than one interface at a time, one may use “pktap” as the interface parameter followed by an optional list of comma separated interface names to include. For example, to capture on the loopback and en0 interface:&lt;/p&gt;tcpdump -i pktap,lo0,en0&lt;p&gt;An interface argument of “all” or “pktap,all” can be used to capture packets from all interfaces, including loopback and tunnel interfaces. A pktap pseudo interface provides for packet metadata using the default PKTAP data link type and files are written in the Pcap-ng file format. The RAW data link type must be used to force to use the legacy pcap-savefile(5) file format with a ptkap pseudo interface. Note that captures on a ptkap pseudo interface will not be done in promiscuous mode.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Therefore, we just need something like:&lt;/p&gt;
    &lt;code&gt;tcpdump -i pktap,en0 -w outfile.pcapng&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;tcptump -i pktap,all host 192.168.0.6 -w outfile.pcapng&lt;/code&gt;
    &lt;p&gt;And then open &lt;code&gt;outfile.pcapng&lt;/code&gt; in Wireshark and under Frame → Process Information you can find the process name, PID, etc. (See screenshot above.)&lt;/p&gt;
    &lt;p&gt;Filtering can be done with &lt;code&gt;frame.darwin.process_info&lt;/code&gt; as listed here. For example:&lt;/p&gt;
    &lt;code&gt;frame.darwin.process_info.pname == "firefox"&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;frame.darwin.process_info.pid == 92046&lt;/code&gt;
    &lt;p&gt;This is super helpful to figure out both what unexpected network traffic is being generated by and the inverse, what a process is doing on the network. And now thanks to Wireshark 4.6.0 it’s even easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nuxx.net/blog/2025/10/14/wireshark-4-6-0-supports-macos-pktap-metadata-pid-process-name-etc/"/><published>2025-10-14T14:18:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45580699</id><title>Astronomers 'image' a mysterious dark object in the distant Universe</title><updated>2025-10-14T19:32:40.949516+00:00</updated><content>&lt;doc fingerprint="bca9713ab1df038d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mysterious dark object in space&lt;/head&gt;
    &lt;p&gt;Scientists detect the lowest mass dark object currently measured&lt;/p&gt;
    &lt;head rend="h2"&gt;To the point&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gravitational lenses: Distortions caused by gravitational lenses can be used to study the properties of dark matter, even though it does not emit light.&lt;/item&gt;
      &lt;item&gt;Discovery: An international team has discovered a dark object in the distant universe that has one million times the mass of the Sun. The discovery is based on an analysis of the gravitational effects on the light from another galaxy.&lt;/item&gt;
      &lt;item&gt;Technology: A network of radio telescopes around the world, including the Green Bank Telescope, collected the data. It forms a virtual supertelescope that enables enhanced image quality, allowing even small gravitational signals to be detected.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dark matter is an enigmatic form of matter not expected to emit light, yet it is essential to understanding how the rich tapestry of stars and galaxies we see in the night sky evolved. As a fundamental building block of the universe, a key question for astronomers is whether dark matter is smooth or clumpy, as this could reveal what it is made of. Since dark matter cannot be observed directly, its properties can only be determined by observing the gravitational lensing effect, whereby the light from a more distant object is distorted and deflected by the gravity of the dark object. “Hunting for dark objects that do not seem to emit any light is clearly challenging,” said Devon Powell at the Max Planck Institute for Astrophysics and lead author of the study. “Since we can’t see them directly, we instead use very distant galaxies as a backlight to look for their gravitational imprints.”&lt;/p&gt;
    &lt;p&gt;The team used a network of telescopes from around the world, including the Green Bank Telescope, the Very Long Baseline Array and the European Very Long Baseline Interferometric Network. The data from this international network were correlated at the Joint Institute for VLBI ERIC in the Netherlands, forming an Earth-sized super-telescope that could capture the subtle signals of gravitational lensing by the dark object. They found that the object has a mass that is a million times greater than that of our Sun and is located in a distant region of space, approximately 10 billion light years from Earth, when the universe was only 6.5 billion years old.&lt;/p&gt;
    &lt;p&gt;This is the lowest mass object to be found using this technique, by a factor of about 100. To achieve this level of sensitivity, the team had to create a high-fidelity image of the sky using radio telescopes located around the world. John McKean from the University of Groningen, the University of Pretoria, and the South African Radio Astronomy Observatory, who led the data collection and is the lead author of a companion paper, stated: “From the first high-resolution image, we immediately observed a narrowing in the gravitational arc, which is the tell-tale sign that we were onto something. Only another small clump of mass between us and the distant radio galaxy could cause this.”&lt;/p&gt;
    &lt;head rend="h2"&gt;New modelling algorithms&lt;/head&gt;
    &lt;p&gt;To analyze the massive dataset, the team had to develop new modelling algorithms that could only be run on supercomputers. “The data are so large and complex that we had to develop new numerical approaches to model them. This was not straightforward as it had never been done before,” said Simona Vegetti at the Max Planck Institute for Astrophysics. “We expect every galaxy, including our own Milky Way, to be filled with dark matter clumps, but finding them and convincing the community that they exist requires a great deal of number-crunching,” she continued. The team applied a special technique called gravitational imaging, which allowed them to ‘see’ the invisible dark matter clump by mapping its gravitational lensing effect against the radio-luminous arc.&lt;/p&gt;
    &lt;p&gt;“Given the sensitivity of our data, we were expecting to find at least one dark object, so our discovery is consistent with the so-called ‘cold dark matter theory’ on which much of our understanding of how galaxies form is based,” said Powell. “Having found one, the question now is whether we can find more and whether their number will still agree with the models.”&lt;/p&gt;
    &lt;p&gt;The team is now analyzing the data further to better understand what the mysterious dark object could be, but they are also looking into other parts of the sky to see if they can find more examples of such low-mass dark objects using the same technique. If they continue to find such mysterious objects in other parts of the universe, and if they really turn out to be completely devoid of stars, then some theories of dark matter may be ruled out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional Information:&lt;/head&gt;
    &lt;p&gt;Gravitational lensing: This is an astrophysical tool used by astronomers to measure the mass properties of structures in the Universe. It is a consequence of Einstein’s Theory of General Relativity, where mass in the Universe curves space. If the mass of the foreground lensing object (typically a galaxy or cluster of galaxies) is sufficiently dense, then the light from distant objects is distorted and multiple images are even observed. In the case of this system, called B1938+666, the foreground infrared-luminous galaxy (seen at the centre of the ring), results in a beautiful Einstein ring of the distant galaxy. However, the distant galaxy is also bright at radio wavelengths, showing the beautiful multiple images and gravitational arcs (seen in red).&lt;/p&gt;
    &lt;p&gt;Very Long Baseline Interferometry: The radio observations were taken using a combination of radio telescopes that are combined to form a so-called Very Long Baseline Interferometer. This observational method allows astronomers to improve the imaging sharpness of the data and reveal very small fluctuations in the brightness that otherwise could not be seen. For example, the resolving power of the data is a factor of 13 times better than the infrared imaging from the W. M. Keck Telescope adaptive optics system (also shown in the figures in black and white). The telescopes used in the observations were the Green Bank Telescope and the Very Long Baseline Array of the National Radio Astronomy Observatory in the United States, and the telescopes of the European Very Long Baseline Interferometric Network.&lt;/p&gt;
    &lt;p&gt;Gravitational imaging: This is a novel method that astronomers use to ‘see’ mass in the Universe even though it does not emit any light. This method uses the extended gravitational arcs to look for small aberrations that can only be caused by an additional, invisible component of mass. By combining this method and the exquisite high angular resolution imaging from the data, the team was able to detect the presence of the lowest mass dark object currently measured.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.mpg.de/25518363/1007-asph-astronomers-image-a-mysterious-dark-object-in-the-distant-universe-155031-x"/><published>2025-10-14T14:45:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45580771</id><title>Show HN: Metorial (YC F25) – Vercel for MCP</title><updated>2025-10-14T19:32:40.427708+00:00</updated><content>&lt;doc fingerprint="9fc57e133df5380d"&gt;
  &lt;main&gt;
    &lt;p&gt; The integration platform for agentic AI. &lt;lb/&gt; Connect any AI model to thousands of APIs, data sources, and tools with a single function call. &lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Skip the setup and go hosted: The fasted, simplest and most reliable way to use Metorial is to sign up to our hosted platform.&lt;/p&gt;
    &lt;p&gt;Metorial enables AI agent developers to easily connect their models to a wide range of APIs, data sources, and tools using the Model Context Protocol (MCP). Metorial abstracts away the complexities of MCP and offers a simple, unified interface for developers, including powerful SDKs, detailed monitoring, and a highly customizable platform.&lt;/p&gt;
    &lt;p&gt;Metorial currently provides SDKs for the following languages:&lt;/p&gt;
    &lt;p&gt;If you want to build a custom integration, check out our API documentation for details on how to use the Metorial API directly.&lt;/p&gt;
    &lt;p&gt;MCP is a powerful standard for connecting AI models to external data and tools, but it focuses on enabling AI clients (like Claude Desktop or Cursor) to connect to tools and data sources. Metorial builds on MCP but makes it a one-liner for developers to connect their AI apps to any API, data source, or tool. Thereby we enable developers to create agentic AI applications that can interact with other systems in a reliable, simple, and secure way.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model Context Protocol (MCP) - Metorial is powered by the Model Context Protocol, a standard for connecting AI models to external data and tools.&lt;/item&gt;
      &lt;item&gt;Docker - Metorial uses Docker to run MCP servers in a containerized environment, making it easy to deploy and manage.&lt;/item&gt;
      &lt;item&gt;MCP Containers - Metorial provides a collection of pre-built MCP servers in Docker containers.&lt;/item&gt;
      &lt;item&gt;Typescript - Most of Metorial is written in TypeScript.&lt;/item&gt;
      &lt;item&gt;Bun - The core of Metorial runs on Bun, a fast JavaScript runtime that is compatible with Node.js.&lt;/item&gt;
      &lt;item&gt;Go - The MCP engine is written in Go, providing a high-performance backend for Metorial.&lt;/item&gt;
      &lt;item&gt;PostgreSQL - Metorial uses PostgreSQL for data storage.&lt;/item&gt;
      &lt;item&gt;Redis - Metorial uses Redis for caching and real-time data processing.&lt;/item&gt;
      &lt;item&gt;MongoDB - Metorial uses MongoDB for storing usage data and logs.&lt;/item&gt;
      &lt;item&gt;React - The Metorial Dashboard is built with React.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Metorial is built to make it super easy for developers to connect their AI apps to external data and tools. Powered by the Model Context Protocol (MCP), Metorial is built on standards.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✨ One-liner SDKs: Connect your AI model to any API, data source, or tool with a single function call.&lt;/item&gt;
      &lt;item&gt;🛠️ Powered by MCP: Metorial is built on the Model Context Protocol, a standard for connecting AI models to external data and tools.&lt;/item&gt;
      &lt;item&gt;🚀 Get started in minutes: Metorial is designed to be easy to use, with a simple setup process and a unified interface for all your AI integrations.&lt;/item&gt;
      &lt;item&gt;🕊️ Self-hosting: Metorial's source code is hosted on GitHub and you can self-host it.&lt;/item&gt;
      &lt;item&gt;👩💻 Built for developers: Metorial isn't built for end users, but for developers who need high quality tooling, monitoring, and customization options to build agentic AI applications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Metorial server index already contains more than 5000 MCP servers. It's a super easy to find and use MCP servers for your AI applications. Everything is searchable and neatly organized, so you can find the right server for your use case.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;mt1.mp4&lt;/head&gt;
    &lt;p&gt;Test and explore MCP servers directly in the Metorial Dashboard. The embedded MCP Explorer allows you to use any MCP server without leaving the dashboard. This makes it easy to test and debug your integrations before writing any code.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;mt2.mp4&lt;/head&gt;
    &lt;p&gt;Every MCP session is recorded and can be reviewed in the Metorial Dashboard. This allows you to monitor and find issues in your integrations. And even better, if an error occurs, Metorial detects it and provides a detailed error report so you can quickly fix the issue.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;mt3.mp4&lt;/head&gt;
    &lt;p&gt;Metorial is built from the ground up for developers. Here are some of the key features that make Metorial a great choice for developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customizable: Metorial is highly customizable, allowing you to configure your integrations to fit your needs.&lt;/item&gt;
      &lt;item&gt;Open source: Metorial is open source, so you can run it on your own infrastructure or use our hosted platform.&lt;/item&gt;
      &lt;item&gt;Multi-instance support: Create multiple instances of your Metorial Projects to test different configurations, environments or versions of your integrations.&lt;/item&gt;
      &lt;item&gt;Powerful SDKs: Metorial provides powerful SDKs for JavaScript/TypeScript and Python, making it easy to integrate with your AI applications.&lt;/item&gt;
      &lt;item&gt;Detailed documentation: Metorial provides detailed documentation for all its features, including examples and tutorials to help you get started quickly.&lt;/item&gt;
      &lt;item&gt;Full API access: Every feature of Metorial is accessible via the API, allowing you to build custom integrations and automate your workflows. Theoretically, you could build your own dashboard using the API.&lt;/item&gt;
      &lt;item&gt;Advanced dashboard: The Metorial Dashboard provides a powerful interface for managing your integrations, monitoring your usage, and debugging your MCP servers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Metorial is licensed under the FSL-1.1 license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/metorial/metorial"/><published>2025-10-14T14:49:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45581735</id><title>How AI hears accents: An audible visualization of accent clusters</title><updated>2025-10-14T19:32:40.129400+00:00</updated><content>&lt;doc fingerprint="35bba6d935b3db90"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we’re going to go on a tour of the world's accents in English. Users of BoldVoice, the American accent training app, speak more than 200 different languages, and it is our mission to help them speak English clearly and confidently. While building the accent strength metric we covered in the previous blog post, we needed to understand how our models clustered accents, dialects, native languages, and language families. Today, we will share some of our findings using a 3D latent visualization.&lt;/p&gt;
    &lt;p&gt;To begin, we finetuned HuBERT, a pretrained audio-only foundation model for the task of accent identification using our in-house dataset of non-native English speech and self-reported accents. BoldVoice’s own dataset of accented speech is one of the largest of its kind in the world.&lt;/p&gt;
    &lt;p&gt;This model receives only the raw input audio and associated accent label; it gets neither a text prompt nor a transcript. For this "finetuning", we sampled 30 million speech recordings comprising 25,000 hours of English speech - a small fraction of our total accent dataset. Unlike a traditional finetune, we unfroze all layers of the pretrained base model due to the large size of our dataset. We trained the model for roughly a week on a cluster of A100 GPUs.&lt;/p&gt;
    &lt;p&gt;While the accent identifier performs quite well across the top hundred or so accents (play with it yourself at accentoracle.com), for today, we are less interested in its raw performance, and more interested in the clustering of accents in its latent space.&lt;/p&gt;
    &lt;p&gt;To observe how accents cluster, we've provided an audible latent space visualization for a small subset of recordings. Hover on the points on the graph to see the language labels.&lt;/p&gt;
    &lt;p&gt;The visualization is created by applying the UMAP dimensionality reduction technique to reduce the 768-dimensional latent space to just 3 dimensions.&lt;/p&gt;
    &lt;p&gt;Note that UMAP destroys much of the information in the full-dimensional latent space, but roughly preserves the global structure, including the relative distances between clusters. Each point represents a single recording inferenced by the model after it was fine tuned and the color corresponds to the true accent label.&lt;/p&gt;
    &lt;p&gt;Finally, in order to denoise the clusters, we cherry-pick only those points for which the predicted and target accents match. Remember, the purpose of this visualization is not to help us assess the performance of the model, but to understand where it has placed accents relative to one another.&lt;/p&gt;
    &lt;p&gt;By clicking or tapping on a point, you will hear a standardized version of the corresponding recording. The reason for voice standardization is two-fold: first, it anonymizes the speaker in the original recordings in order to protect their privacy. Second, it allows us to hear each accent projected onto a neutral voice, making it easier to hear the accent differences and ignore extraneous differences like gender, recording quality, and background noise. However, there is no free lunch: it does not perfectly preserve the source accent and introduces some audible phonetic artifacts.&lt;/p&gt;
    &lt;p&gt;This voice standardization model is an in-house accent-preserving voice conversion model.&lt;/p&gt;
    &lt;p&gt;Please explore the latent space visualization. You can click, drag, zoom, and scroll to navigate. You can also isolate accents by double clicking them in the legend to the right (desktop only) – double-clicking again will undo the filter.&lt;/p&gt;
    &lt;p&gt;Meanwhile, think about the following questions: which accents would you expect to be clustered together? Do you expect them to follow the taxonomy of language families or to cluster in other ways?&lt;/p&gt;
    &lt;p&gt;Our team was most surprised to see that geographic proximity, immigration, and colonialism seem to affect this model's learned accent groupings more than language taxonomy. Click the button below to explore our first grouping.&lt;/p&gt;
    &lt;p&gt;For example, the Australian cluster is right next to the Vietnamese cluster despite the fact that English and Vietnamese are not related taxonomically. If you listen to the 10 points that make up a bridge between the two clusters, you hear what sounds like native Vietnamese speakers who speak English with an Australian accent. Perhaps these hybrid accents could explain the overall proximity of these clusters.&lt;/p&gt;
    &lt;p&gt;We see something similar for the French/Nigerian/Ghanaian grouping.&lt;/p&gt;
    &lt;p&gt;It's important to remember that the distances on this map are not an objective measure of the phonetic similarity between accents. They are a byproduct of a model which has successfully learned to distinguish a variety of accents in L2 English speech from audio alone with no knowledge of language or linguistics.&lt;/p&gt;
    &lt;p&gt;Next, take a look at the Indian subcontinent accent cluster. Note that the Telugu, Tamil, and Malayalam accents are grouped together at one end of the cluster, and the Nepali and Bengali accents are at the other. This roughly mirrors geography, where Telugu, Tamil, and Malayalam are widely spoken languages in southern India, and Bengali and Nepali are widely spoken in northwest India and Nepal.&lt;/p&gt;
    &lt;p&gt;Finally, let's scroll to the Mongolian cluster, where the nearest cluster is actually Korean.&lt;/p&gt;
    &lt;p&gt;Experts and non-experts have observed phonetic similarities between Mongolian and Korean. A now-refuted hypothesis called the "Altaic language family" once grouped them together.&lt;/p&gt;
    &lt;p&gt;It is interesting that this model, with no concept of language families, has also picked up on the phonetic similarities even as filtered through a second language (English).&lt;/p&gt;
    &lt;p&gt;What do you think? Is this a meaningless artifact of latent space visualization or evidence of real phonetic features diffusing between Korean and Mongolian?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://accent-explorer.boldvoice.com/"/><published>2025-10-14T16:07:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45581761</id><title>Hold Off on Litestream 0.5.0</title><updated>2025-10-14T19:32:40.013196+00:00</updated><content>&lt;doc fingerprint="87d714c78a2da7db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Hold Off on Litestream 0.5.0&lt;/head&gt;
    &lt;p&gt;Litestream is an open-source tool that backs up SQLite databases to cloud storage in real time. I love it and use it in all of my projects.&lt;/p&gt;
    &lt;p&gt;Litestream is owned by Fly.io, and they paused development on Litestream for almost two years in favor of an alternative project called LiteFS. Two weeks ago, Ben Johnson, Litestream’s creator and lead developer, announced that they were shifting focus back to Litestream and had just published a new release, 0.5.0.&lt;/p&gt;
    &lt;p&gt;I tried out Litestream 0.5.0, but I caution other Litestream users to give it another release and more extensive testing before deploying it in production. I had a bumpy experience migrating to the new version of Litestream.&lt;/p&gt;
    &lt;head rend="h2"&gt;The expected migration work 🔗︎&lt;/head&gt;
    &lt;p&gt;There are two tasks that are intentional in upgrading from previous versions of Litestream to v0.5.0 and above:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The backup format has changed, so Litestream 0.5.0 cannot restore from backups created in previous versions of Litestream.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;litestream.yml&lt;/code&gt;configuration file format has changed slightly. There used to be an array field called&lt;code&gt;replicas&lt;/code&gt;, but 0.5.0 changes this to a dictionary called&lt;code&gt;replica&lt;/code&gt;(singular).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Litestream has published a helpful migration guide with more details.&lt;/p&gt;
    &lt;p&gt;One of the benefits of Litestream 0.5.0 is that there’s now an official litestream Docker image. (Edit: Reader placardloop points out that the Docker image is not new; I just never noticed it.) All of my previous Docker containers required a lot of boilerplate to download the correct version of Litestream and make it available in my container, but now it reduces to a single Dockerfile line:&lt;/p&gt;
    &lt;code&gt;COPY --from=litestream/litestream:0.5.0 /usr/local/bin/litestream /app/litestream
&lt;/code&gt;
    &lt;head rend="h2"&gt;My test migration to Litestream 0.5.0 🔗︎&lt;/head&gt;
    &lt;p&gt;To test out Litestream 0.5.0, I tried deploying it on my project, What Got Done. This is a good project for testing because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I already announced that I was shutting down this service, so users have stopped using the site.&lt;/item&gt;
      &lt;item&gt;The server kept failing due to a bug in Litestream 0.3.13 that was fixed in 0.5.0.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Uploading to Backblaze backends no longer works 🔗︎&lt;/head&gt;
    &lt;p&gt;To start the migration, I downloaded the latest copy of my data using Litestream 0.3.13 and then tried to use Litestream 0.5.0 to upload it back to Backblaze’s cloud storage in Litestream’s new format. But I hit this error:&lt;/p&gt;
    &lt;code&gt;error" db=store.db replica=s3 error="write ltx file: s3: upload to db/0000/0000000000000001-0000000000000001.ltx: operation error S3: PutObject, resolve auth scheme: resolve endpoint: endpoint rule error, Custom endpoint `s3.us-west-002.backblazeb2.com` was not a valid URI"
&lt;/code&gt;
    &lt;p&gt;The same replica definition had worked in previous versions, so I was a bit puzzled.&lt;/p&gt;
    &lt;code&gt;access-key-id: ${LITESTREAM_ACCESS_KEY_ID}
secret-access-key: ${LITESTREAM_SECRET_ACCESS_KEY}
dbs:
  - path: ${DB_PATH}
    replica:
      url: s3://${LITESTREAM_BUCKET}/db
      endpoint: ${LITESTREAM_ENDPOINT}
&lt;/code&gt;
    &lt;p&gt;I tried several alternative ways of specifying the Backblaze S3 endpoint, but Litestream rejected them all as configuration errors before even attempting to back up. The configuration I had was the only one that Litestream accepted as valid configuration, but it failed to back up.&lt;/p&gt;
    &lt;p&gt;I filed Backblaze replica fails with “Custom endpoint &amp;amp;mldr; was not a valid URI” #789, and Litestream developer Cory LaNou fixed it the next day.&lt;/p&gt;
    &lt;p&gt;Now that I was able to upload data to Backblaze in Litestream’s new format, I was unblocked from integrating Litestream 0.5.0 into What Got done.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;-if-replica-exists&lt;/code&gt; disappeared 🔗︎&lt;/head&gt;
    &lt;p&gt;I deployed Litestream 0.5.0 to What Got Done, but the server failed to boot with this error:&lt;/p&gt;
    &lt;code&gt;flag provided but not defined: -if-replica-exists
&lt;/code&gt;
    &lt;p&gt;I checked the command documentation, and it said that &lt;code&gt;-if-replica-exists&lt;/code&gt; was still supported:&lt;/p&gt;
    &lt;code&gt;$ litestream restore -help | grep if-replica-exists --after-context=1
        -if-replica-exists
            Returns exit code of 0 if no backups found.
&lt;/code&gt;
    &lt;p&gt;It turns out that the flag was removed by mistake and will be back in 0.5.1.&lt;/p&gt;
    &lt;head rend="h3"&gt;Restore fails with &lt;code&gt;transaction not available&lt;/code&gt; 🔗︎&lt;/head&gt;
    &lt;p&gt;Undeterred by the loss of &lt;code&gt;-if-replica-exists&lt;/code&gt;, I removed it from my start script. But then my server failed to start with a new error:&lt;/p&gt;
    &lt;code&gt;level=ERROR msg="failed to run" error="cannot calc restore plan: transaction not available"
&lt;/code&gt;
    &lt;p&gt;That turns out to match this open Litestream issue, with an alarming severity of “CRITICAL - Complete Data Loss”:&lt;/p&gt;
    &lt;head rend="h3"&gt;Litestream no longer creates directories 🔗︎&lt;/head&gt;
    &lt;p&gt;At this point, I was just willing to try anything to get back up and running, so I ran the latest bleeding edge version of Litestream by building it from source in my Docker container.&lt;/p&gt;
    &lt;p&gt;Fortunately, the latest version got around whatever &lt;code&gt;transaction not available&lt;/code&gt; issue I was hitting, and Litestream made it further in the process!&lt;/p&gt;
    &lt;p&gt;Unfortunately, there was still one error to overcome:&lt;/p&gt;
    &lt;code&gt;level=ERROR msg="failed to run" error="create temp database path: open /app/data/store.db.tmp: no such file or directory"
&lt;/code&gt;
    &lt;p&gt;This one was actually simple enough that I had a pretty strong suspicion about what was happening. In previous versions of Litestream, if I told it to restore a SQLite database to &lt;code&gt;/app/data/store.db&lt;/code&gt; and the &lt;code&gt;/app/data&lt;/code&gt; path didn’t exist, Litestream would attempt to create it before writing the file.&lt;/p&gt;
    &lt;p&gt;I checked the source and saw that the folder creation logic had disappeared in this code flow, but it was simple enough to fix, so I created a fix:&lt;/p&gt;
    &lt;head rend="h3"&gt;Success! 🔗︎&lt;/head&gt;
    &lt;p&gt;With my fork of Litestream with the final &lt;code&gt;mkdir&lt;/code&gt; fix applied, What Got Done was back up and running!&lt;/p&gt;
    &lt;head rend="h2"&gt;Reflections 🔗︎&lt;/head&gt;
    &lt;p&gt;I was able to get Litestream 0.5.x working with a pre-release fork, but I’m going to hold off deploying it to my other projects for another release or two. The 0.5.0 changes seem to have been more disruptive than the Litestream folks expected, and they’re still struggling with some serious bugs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CRITICAL: Restore fails with ’nonsequential page numbers’ after checkpoint during Litestream downtime #752&lt;/item&gt;
      &lt;item&gt;Local LTX Level 0 files are never compacted/removed #784&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And there are several other serious bugs that they’ve fixed in the development version but are &lt;del&gt;not yet in a production release&lt;/del&gt; (Update: these are now fixed in 0.5.1):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restore does not update LTX ID information #781&lt;/item&gt;
      &lt;item&gt;Age encryption configuration silently ignored in v0.5.0+ #790&lt;/item&gt;
      &lt;item&gt;[Regression] LTX transactions get deleted in 0.5.0, cannot restore more than a few seconds #771&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Read My Book&lt;/head&gt;
    &lt;p&gt;I'm writing a book of simple techniques to help developers improve their writing.&lt;/p&gt;
    &lt;p&gt;My book will teach you how to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create clear and pleasant software tutorials&lt;/item&gt;
      &lt;item&gt;Attract readers and customers through blogging&lt;/item&gt;
      &lt;item&gt;Write effective emails&lt;/item&gt;
      &lt;item&gt;Minimize pain in writing design documents&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Be the first to know when I post cool stuff&lt;/head&gt;
    &lt;p&gt;Subscribe to get my latest posts by email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mtlynch.io/notes/hold-off-on-litestream-0.5.0/"/><published>2025-10-14T16:10:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45582268</id><title>Prefix sum: 20 GB/s (2.6x baseline)</title><updated>2025-10-14T19:32:39.830618+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ashtonsix/perf-portfolio/tree/main/delta"/><published>2025-10-14T16:53:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45582462</id><title>How bad can a $2.97 ADC be?</title><updated>2025-10-14T19:32:39.639309+00:00</updated><content/><link href="https://excamera.substack.com/p/how-bad-can-a-297-adc-be"/><published>2025-10-14T17:12:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45583129</id><title>New lab-grown human embryo model produces blood cells</title><updated>2025-10-14T19:32:39.447761+00:00</updated><content>&lt;doc fingerprint="d7dd421a2b9d5f33"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Scientists make human blood in the lab — here’s how&lt;/head&gt;
    &lt;p&gt;Researchers have found a new way to produce human blood cells in the lab that mimics the process in natural embryos. Their discovery holds potential to simulate blood disorders like leukaemia, and to produce long-lasting blood stem cells for transplants.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;It was an exciting moment when the blood red colour appeared in the dish – it was visible even to the naked eye.&lt;/p&gt;Jitesh Neupane&lt;/quote&gt;
    &lt;p&gt;University of Cambridge scientists have used human stem cells to create three-dimensional embryo-like structures that replicate certain aspects of very early human development - including the production of blood stem cells.&lt;/p&gt;
    &lt;p&gt;Human blood stem cells, also known as hematopoietic stem cells, are immature cells that can develop into any type of blood cell, including red blood cells that carry oxygen and various types of white blood cells crucial to the immune system.&lt;/p&gt;
    &lt;p&gt;The embryo-like structures, which the scientists have named ‘hematoids’, are self-organising and start producing blood after around two weeks of development in the lab - mimicking the development process in human embryos.&lt;/p&gt;
    &lt;p&gt;The structures differ from real human embryos in many ways, and cannot develop into them because they lack several embryonic tissues, as well as the supporting yolk sac and placenta needed for further development.&lt;/p&gt;
    &lt;p&gt;Hematoids hold exciting potential for a better understanding of blood formation during early human development, simulating blood disorders like leukaemia, and for producing long-lasting blood stem cells for transplants.&lt;/p&gt;
    &lt;p&gt;The human stem cells used to derive hematoids can be created from any cell in the body. This means the approach also holds great potential for personalised medicine in the future, by allowing the production of blood that is fully compatible with a patient’s own body.&lt;/p&gt;
    &lt;p&gt;Although other methods exist for generating human blood stem cells in the laboratory, these require a cocktail of extra proteins to support the stem cells’ growth and development. The new method mimics the natural developmental process, based on a self-organising human embryo-like model, where the cells’ intrinsic support environment drives the formation of blood cells and beating heart cells within the same system.&lt;/p&gt;
    &lt;p&gt;The findings are published today in the journal Cell Reports.&lt;/p&gt;
    &lt;p&gt;Dr Jitesh Neupane, a researcher at the University of Cambridge’s Gurdon Institute and joint first author of the study, said: “It was an exciting moment when the blood red colour appeared in the dish – it was visible even to the naked eye.”&lt;/p&gt;
    &lt;p&gt;He added, “Our new model mimics human foetal blood development in the lab. This sheds light on how blood cells naturally form during human embryogenesis, offering potential medical advances to screen drugs, study early blood and immune development, and model blood disorders like leukaemia.”&lt;/p&gt;
    &lt;p&gt;Professor Azim Surani at the University of Cambridge’s Gurdon Institute, senior author of the paper, said: “This model offers a powerful new way to study blood development in the early human embryo. Although it is still in the early stages, the ability to produce human blood cells in the lab marks a significant step towards future regenerative therapies - which use a patient’s own cells to repair and regenerate damaged tissues.”&lt;/p&gt;
    &lt;p&gt;Dr Geraldine Jowett at the University of Cambridge’s Gurdon Institute, co-first author of the study, said: “Hematoids capture the second wave of blood development that can give rise to specialised immune cells or adaptive lymphoid cells, like T cells, opening up exciting avenues for their use in modelling healthy and cancerous blood development.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Self-organising structures&lt;/head&gt;
    &lt;p&gt;The new human embryo-like model simulates the cell changes that occur during the very early stages of human development, when our organs and blood system first begin to form.&lt;/p&gt;
    &lt;p&gt;The team observed the emergence of the three-dimensional hematoids under a microscope in the lab. By the second day, these had self-organised into three germ layers - called the ectoderm, mesoderm, and endoderm - the foundations of the human body plan that are crucial for shaping every organ and tissue, including blood.&lt;/p&gt;
    &lt;p&gt;By day eight, beating heart cells had formed. These cells eventually give rise to the heart in a developing human embryo.&lt;/p&gt;
    &lt;p&gt;By day thirteen, the team saw red patches of blood appearing in the hematoids. They also developed a method which demonstrated that blood stem cells in hematoids can differentiate into various blood cell types, including specialised immune cells, such as T-cells.&lt;/p&gt;
    &lt;head rend="h3"&gt;Shining a light on early human development&lt;/head&gt;
    &lt;p&gt;Stem cell-derived embryo models are crucial for advancing our knowledge of early human development.&lt;/p&gt;
    &lt;p&gt;The blood cells in hematoids develop to a stage that roughly corresponds to week four to five of human embryonic development. This very early stage of life cannot be directly observed in a real human embryo because it has implanted in the mother’s womb by this time.&lt;/p&gt;
    &lt;p&gt;There are clear regulations governing stem cell-based models of human embryos, and all research modelling human embryo development must be approved by ethics committees before proceeding. This study received the necessary approvals, and the resulting paper has been peer reviewed.&lt;/p&gt;
    &lt;p&gt;The scientists have patented this work through Cambridge Enterprise, the innovation arm of the University of Cambridge, which helps researchers translate their work into a globally leading economic and social impact.&lt;/p&gt;
    &lt;p&gt;The research was funded primarily by Wellcome.&lt;/p&gt;
    &lt;p&gt;Reference: Neupane, J. et al: ‘A post-implantation model of human embryo development includes a definitive hematopoietic niche.’ Cell Reports, October 2025. DOI: 10.1016/j.celrep.2025.116373&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; The text in this work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Images, including our videos, are Copyright ©University of Cambridge and licensors/contributors as identified. All rights reserved. We make our image and video content available in a number of ways – on our main website under its Terms and conditions, and on a range of channels including social media that permit your use and sharing of our content under their respective Terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cam.ac.uk/research/news/new-lab-grown-human-embryo-model-produces-blood-cells"/><published>2025-10-14T18:22:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45583243</id><title>Intel Announces Inference-Optimized Xe3P Graphics Card with 160GB VRAM</title><updated>2025-10-14T19:32:39.088772+00:00</updated><content>&lt;doc fingerprint="a4591a3d988e20a9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intel Announces "Crescent Island" Inference-Optimized Xe3P Graphics Card With 160GB vRAM&lt;/head&gt;
    &lt;p&gt;Back during the Intel Tech Tour in Arizona, Intel teased a new inference-optimized enterprise GPU would be announced soon. This new product would feature enhanced memory, bandwidth, and enterprise-level AI inference capabilities. Today the embargo expires on talking about this new GPU offering.&lt;/p&gt;
    &lt;p&gt;When Intel was teasing this new inference-optimized GPU a few weeks back in Arizona it sounded like Intel may have had an unexpected trick up its sleeves. What's being announced today is indeed a new enterprise GPU for AI that is interesting from a technology perspective, but it's not shipping until at least H2'2026. So while there was hope that perhaps Intel had managed to innovate some interesting Battlemage / BMG-G31 part for AI or the like with lots of vRAM, what's being announced is a next-gen part but one that is at least one year away still.&lt;/p&gt;
    &lt;p&gt;This new graphics card is codenamed Crescent Island and is built on their next-gen Xe3P Celestial micro-architecture. Xe3P will be optimized around performance-per-Watt and Crescent Island will feature 160GB of LPDDR5x memory to allow for plenty of space for large language models (LLMs).&lt;/p&gt;
    &lt;p&gt;Intel's embargoed announcement also notes that Crescent Island will feature support for a variety of different data types and be an "ideal" solution for tokens-as-a-service providers and inference use cases.&lt;/p&gt;
    &lt;p&gt;In addition to being optimized around performance-per-Watt, Crescent Island will also be air cooled and cost-optimized. Intel is currently working on refining their open-source software stack for Crescent Island via using current-generation Arc Pro B-Series GPUs.&lt;/p&gt;
    &lt;p&gt;Intel's announcement notes that customer sampling of this new data center GPU will begin in the second half of 2026. No official release timeframe was provided if they also hope to squeeze it out next year or if (more than likely) it will actually ship more broadly in 2027 but just noting their customer sampling for H2'2026 in the embargoed news release. No slides or prototype images or anything else to share today on Intel's Crescent Island.&lt;/p&gt;
    &lt;p&gt;Long story short, Intel is announcing Crescent Island today as a Xe3LP + 160GB LPDDR5X offering for H2'2026 or later that will be AI inference optimized around power efficiency and cost. It sounds interesting but technical details beyond those basics were light and it's going to be a long while before we see Crescent Island. Given the timing this will be going up against the AMD Instinct MI450 series and NVIDIA Vera Rubin. It seems like Intel wanted to have something to announce now given the ongoing AI rush albeit not many details today and no short term AI solution.&lt;/p&gt;
    &lt;p&gt;At least this does lead to more weight for the ongoing Project Battlematrix Linux driver improvements and other ongoing Intel Compute Runtime and Intel Xe Linux driver enhancements that are currently ongoing for the Arc Pro B-Series. With confirming Crescent Island now it also opens the door to them beginning to push open-source hardware enablement patches without otherwise spilling the beans on this forthcoming enterprise AI product.&lt;/p&gt;
    &lt;p&gt;Intel is using the OCP Global Summit to announce some additional Gaudi 3 rack-scale reference designs. These new Gaudi 3 rack-scale reference designs will allow up to 64 accelerators per rack with liquid cooling and 8.2TB of high bandwidth memory. Intel hasn't aggressively promoted Gaudi 3 in recent quarters after its launch last year. Gaudi 3 has enjoyed some reprieve since Intel canceled their Falcon Shores AI accelerator chip but still appears to be the end of the road for Gaudi especially with Jaguar Shores still expected and now Crescent Island too. The Gaudi 3 software support has been neglected over the past year with losing multiple rounds of the Habana Labs Linux driver maintainers and only recently seeing new activity to return to working on this AI accelerator Linux driver albeit as of writing for Linux 6.18 there still is no mainline kernel driver support for Gaudi 3.&lt;/p&gt;
    &lt;p&gt;If you enjoyed this article consider joining Phoronix Premium to view this site ad-free, multi-page articles on a single page, and other benefits. PayPal or Stripe tips are also graciously accepted. Thanks for your support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.phoronix.com/review/intel-crescent-island"/><published>2025-10-14T18:30:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45583336</id><title>What do Americans die from vs. what the news report on</title><updated>2025-10-14T19:32:38.677276+00:00</updated><content>&lt;doc fingerprint="7655812702e0870d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Does the news reflect what we die from?&lt;/head&gt;
    &lt;head rend="h2"&gt;What do Americans die from, and what do the New York Times, Washington Post, and Fox News report on?&lt;/head&gt;
    &lt;head rend="h4"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;For this work, we relied on Media Cloud, an open-access platform for media analysis. We would like to thank their team, particularly Emily Boardman Ndulue and Fernando Bermejo, for making this invaluable resource available and for their help with this project.&lt;/p&gt;
    &lt;p&gt;More than 80% of people — including surveyed Americans, Brits, Germans, and Italians — say they follow the news because they “want to know what is going on in the world around them.”1 It’s not just that people expect the news to inform them about what’s going on in the world. Most think that it does. When asked what emotions the news generates, “informed” was the most common response.2&lt;/p&gt;
    &lt;p&gt;This is what media outlets themselves promise to do. Here are several quotes from the New York Times’s mission statement:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“We seek the truth and help people understand the world. [...]&lt;/p&gt;
      &lt;p&gt;We help a global audience understand a vast and diverse world.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However, as we’ll see in this article, the media focuses on a particular sliver of our world, leaving much of the “vast and diverse world” largely out of their reporting. We’ll investigate this through the lens of health, looking at causes of death and reporting in the United States.&lt;/p&gt;
    &lt;p&gt;As we’ll discuss, our point is not that we should want or expect the media’s coverage to perfectly match the real distribution of deaths, although we’d argue that it would be better if it were less skewed. We wrote this article so that you, the reader, are aware of a significant disconnect between what we often hear and what actually happens.&lt;/p&gt;
    &lt;p&gt;It’s easy to conflate what we see in the news with the reality of our world, and keeping this mismatch in mind can help you avoid falling into this trap.&lt;/p&gt;
    &lt;head rend="h1"&gt;Counting deaths and mentions in popular media&lt;/head&gt;
    &lt;p&gt;We focused on causes of death and media coverage in the United States in 2023.&lt;/p&gt;
    &lt;p&gt;The full list of all causes of death is very long, and since many causes are very rare, we didn’t investigate all of them. But our analysis accounts for 76% of all deaths in the US in 2023.3 It includes the 12 leading causes of death in the US, plus homicide, drug overdoses, and terrorism, since they receive a lot of attention in the media.&lt;/p&gt;
    &lt;p&gt;We used data from the US Centers for Disease Control and Prevention (CDC) to calculate each cause’s share of the total.4 We then compared this to the relative share of articles that mentioned these causes of death in three media outlets: the New York Times, the Washington Post, and the news website of Fox News. We selected these three because they are among the biggest national news organizations, are extremely popular, and are seen as being on different parts of the political spectrum.&lt;/p&gt;
    &lt;p&gt;To count the number of mentions, we relied on Media Cloud, an open-source platform regularly used for media analysis. In an extended methodology document, we provide many more details on how we constructed the data. Two things are important to mention here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For each cause of death, we included synonyms in our search. So, when searching for mentions of “homicide”, we also included mentions of related terms such as “murder”, “killer”, and other terms. For “heart disease”, we included terms like “heart attack”, “cardiac arrest”, “heart failure”, and many others.&lt;/item&gt;
      &lt;item&gt;We only counted articles where a cause of death — or its related terms — was mentioned more than once. This ensures that our analysis is focused on reporting on causes of death rather than just articles that mention a cause of death in passing. Additionally, this approach reduces the number of false positives and noise in our results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;What do Americans die from, and what do they read about in the news?&lt;/head&gt;
    &lt;p&gt;You can see the results of our analysis in the chart below.&lt;/p&gt;
    &lt;p&gt;There are two big takeaways from this analysis. The first one is that the actual distribution of deaths shown on the left is very different from the causes of death that the media talks about.&lt;/p&gt;
    &lt;p&gt;The second insight is how similar the distribution of coverage is between the three media outlets. While there are some differences (Fox News was a bit more likely to mention homicides, for example, while the NYT did the same for terrorism), these are much smaller than we might expect. While right- and left-wing media might differ in how they cover particular topics, what they choose to write or talk about is similar.&lt;/p&gt;
    &lt;p&gt;The insight in this comparison, then, is not about differences between partisan media. It’s about the difference between actual causes of death and what the news tells Americans about. Those differences — as we can see in the chart — are huge.&lt;/p&gt;
    &lt;p&gt;Heart disease and cancer accounted for 56% of deaths among these 15 causes, but together they received just 7% of the media coverage. Other chronic issues, such as strokes, respiratory problems, diabetes, and kidney and liver disease, were also very underrepresented in the news.&lt;/p&gt;
    &lt;p&gt;Rare — but dramatic — events such as homicides and terrorism received more than half of all media coverage, despite being much smaller causes of death in the US. Terrorism, in particular, is a very rare cause of death, with 16 deaths in 2023.5&lt;/p&gt;
    &lt;head rend="h1"&gt;How over- and underrepresented are different causes of death in the media?&lt;/head&gt;
    &lt;p&gt;Another way to visualize this data is to measure how over- or underrepresented each cause is.&lt;/p&gt;
    &lt;quote&gt;Heart disease and cancer accounted for 56% of deaths but received just 7% of the coverage&lt;/quote&gt;
    &lt;p&gt;To do this, we calculate the ratio between a cause’s share of deaths and its share of news articles. In the chart below, we’ve done this for coverage in the New York Times (the results are similar for the other two outlets).&lt;/p&gt;
    &lt;p&gt;It highlights that homicides and terrorism are extremely overrepresented. Homicides received 43 times more coverage than their share of deaths; terrorism received over 18,000 times more.&lt;/p&gt;
    &lt;p&gt;At the other end, we see that conditions like heart disease, stroke, and liver disease are very underrepresented.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why is the media so biased towards dramatic risks?&lt;/head&gt;
    &lt;p&gt;The fact that the media focuses on dramatic, emotive events — and much less on “everyday”, more common mortality risks — has been found in several studies.6 These studies have shown that this mismatch has existed for a long time, and that genuine changes in death rates between causes of death account for a tiny fraction of the changes in media coverage.7&lt;/p&gt;
    &lt;p&gt;Our analysis adds to the evidence, with updated data for 2023.&lt;/p&gt;
    &lt;p&gt;Why does this mismatch exist?&lt;/p&gt;
    &lt;quote&gt;Homicides received 43 times more coverage than their share of deaths; terrorism received over 18,000 times more&lt;/quote&gt;
    &lt;p&gt;One explanation is that the news, true to its name, tells us what’s new. The fact that nearly 2000 Americans die from heart disease every single day means it is not novel or new. The headline tomorrow would be the same as it was today, which was the same as yesterday. Rarer events like terrorist attacks, plane crashes, homicides, or disasters each have their unique headline.&lt;/p&gt;
    &lt;p&gt;People who die from common health risks quickly become mere numbers. On the other hand, those who die in rarer events have a face, a name, and a story that can be told. As humans, this makes us much more likely to click and read, making these stories ideal for the media to write about.&lt;/p&gt;
    &lt;p&gt;While we would like news organizations to focus much more on the larger problems that societies face — that is what we try to do at Our World in Data — it would be wrong to put all of the blame on them. They respond to what readers want, and many want emotive and engaging stories (even if their circumstances are terrible and upsetting).&lt;/p&gt;
    &lt;p&gt;Even outside the news, some of the most successful television series are intense, crime-filled dramas. Disaster movies pull in record numbers at the box office. One of the most popular podcast genres is “true crime,” where we spend hours listening to the gripping, scary tales of serial killers or con artists.&lt;/p&gt;
    &lt;p&gt;It's not surprising, then, that we’re much more likely to click on a news story about the latest murder or disaster than one about heart or kidney disease. And because media organizations need traffic and attention to survive, they and the public are stuck in a reinforcing feedback loop where rare events are always in the headlines and chronic problems get drowned out.&lt;/p&gt;
    &lt;p&gt;This is not just a problem with the modern media environment. The audience for this type of media has always been there. What’s changed is the reporting frequency: rather than getting one newspaper in the morning, we are bombarded with updates almost in real-time. We also now receive news from a much larger geographical area; it’s not just about what’s happened in our own town, but also about what’s happened on the other side of the country, or even the world.&lt;/p&gt;
    &lt;head rend="h1"&gt;Does this bias really matter?&lt;/head&gt;
    &lt;p&gt;Our point is not that we think the New York Times, Washington Post, or Fox News’ coverage should exactly match the distribution of causes of death. A newspaper that constantly covers heart disease and kidney failure would be a boring one that soon goes out of business. Even though our mission at Our World in Data is to cover the world’s largest problems, our own writing and data publications also don’t precisely match the scale of those problems. We expect we will be closer to the real distribution than the mainstream media, but there will still be some mismatch.&lt;/p&gt;
    &lt;p&gt;The reason we’re doing this analysis is to make you or other readers more aware of this selection bias. The frequency of news coverage doesn’t reflect what’s happening across millions or billions of people, but it’s easy to fall into the trap of thinking it does.&lt;/p&gt;
    &lt;p&gt;Why, then, do we think that this bias matters? Does it actually affect people’s perceptions of problems?&lt;/p&gt;
    &lt;p&gt;In a large survey among US adults, people who consumed local crime news “often” were more than three times more likely to say they were “extremely concerned” about crime affecting them or their family than those who rarely or never read local crime news.8&lt;/p&gt;
    &lt;quote&gt;The frequency of news coverage doesn’t reflect what’s happening across millions or billions of people, but it’s easy to fall into the trap of thinking it does&lt;/quote&gt;
    &lt;p&gt;Nearly six-in-ten Americans still see international terrorism as a critical threat to the United States, despite the domestic impact on the US being relatively low for two decades. People are often far more anxious about flying than driving, even though commercial airline crashes are incredibly rare.&lt;/p&gt;
    &lt;p&gt;The information we’re exposed to profoundly impacts how we perceive the world, even if our perspective is less skewed than the media's.&lt;/p&gt;
    &lt;p&gt;But there’s one final reason why this bias matters. It makes it hard for us to understand how causes of death are changing over time. If we’re constantly bombarded with stories of the latest murders and crimes, we might easily think that these are happening more and more. That is a widespread sentiment. In 23 of the 27 Gallup surveys conducted since 1993, most Americans said there was more crime than the year before. In reality, rates of crime — including homicides and other violent crime — have fallen a lot.&lt;/p&gt;
    &lt;p&gt;And if we don’t hear about what’s happening to heart disease rates, treatments, or the odds of surviving cancer, we might wrongly imagine that no progress has been made. Yet childhood cancer deaths have plummeted over the last 50 years. Even among adults, death rates from cancer have fallen dramatically since the 1990s. So too have death rates from heart disease.&lt;/p&gt;
    &lt;p&gt;This perception gap about the world matters, and the media is not doing a good job of trying to close it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;If you’re interested in digging deeper, we provide a more detailed methodological document about how this data was generated, and a few additional analyses.&lt;/p&gt;
    &lt;head rend="h4"&gt;Correction note&lt;/head&gt;
    &lt;p&gt;This article was initially published on October 6, 2025, and was updated on October 9. This update corrected an error, whereby “Drug and overdose” deaths were also included within the US CDC category of “Accidents”. This meant that they were double-counted. We have corrected this, and the change made only a small difference to the final numbers. The relative share of deaths from accidents changed from 9.5% to 7.8%, and the share of other causes increased slightly. We thank Karl Pettersson for flagging this.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;The limits of our personal experience and the value of statistics&lt;/head&gt;
        &lt;p&gt;The world is huge; to get a clear idea of what our world is like, we have to rely on carefully collected, well-documented statistics.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;Causes of death globally: what do people die from?&lt;/head&gt;
        &lt;p&gt;To make progress towards a healthier world we need to have a good understanding of what health problems we face today.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;How are causes of death registered around the world?&lt;/head&gt;
        &lt;p&gt;In many countries, when people die, the cause of their death is officially registered in their country’s national system. How is this determined?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Endnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Respondents could choose to “agree” with multiple answers. This survey was from 2015, but as we’ll see, more recent data suggests that even in 2025, most Americans think the news keeps them informed about what’s happening worldwide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the Pew Research survey, 46% said it made them feel informed “extremely often or often” with a further 43% “sometimes”. That was more common than any other emotion. The other high-ranking ones were negative emotions such as anger or sadness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In 2023, there were approximately 3 million (3,090,964) deaths in the United States. 2.3 million (2,350,117) died from the twelve leading causes plus drug overdoses, homicides and terrorism. You can find these results in our intermediate and final data files, which are available in our methodology document. That means the combined share was around 76% of the total [2,305,117 / 3,090,964 * 100 = 76%].&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We used mortality data from CDC Wonder for all causes except terrorism (which isn’t reported there). For this, we relied on data from the Global Terrorism Index.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This figure is sourced from the Institute for Economics and Peace (IEP)’s Global Terrorism Index 2024 Report. It states on page 38: “The impact of terrorism improved in North America over the past year, owing to an improvement in score in Canada. There was one attack and death from terrorism in Canada in 2023, down from the peak of 12 deaths and eight attacks in 2018. By contrast, the impact of terrorism increased in the US, with 16 deaths from seven incidents.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Isch, C. (2025). Media bias in portrayals of mortality risks: Comparison of newspaper coverage to death rates. Social Science &amp;amp; Medicine, 364, 117542.&lt;/p&gt;
        &lt;p&gt;Pilar, M. R., Eyler, A. A., Moreland-Russell, S., &amp;amp; Brownson, R. C. (2020). Actual causes of death in relation to media, policy, and funding attention: Examining public health priorities. Frontiers in Public Health, 8, 279.&lt;/p&gt;
        &lt;p&gt;Bomlitz, L. J., &amp;amp; Brezis, M. (2008). Misrepresentation of health risks by mass media. Journal of Public Health, 30(2), 202-204.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Isch, C. (2025). Media bias in portrayals of mortality risks: Comparison of newspaper coverage to death rates. Social Science &amp;amp; Medicine, 364, 117542.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This survey was conducted by Pew Research in 2024. It asked US adults whether they were extremely/very concerned, somewhat concerned, or not at all concerned about crime in their local community affecting them or their family.&lt;/p&gt;
        &lt;p&gt;33% of those who “often” get local crime news were “extremely concerned”. The share among those who “sometimes” get this type of news was 19%. It was just 10% among those who rarely consume it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Cite this work&lt;/head&gt;
    &lt;p&gt;Our articles and data visualizations rely on work from many different people and organizations. When citing this article, please also cite the underlying data sources. This article can be cited as:&lt;/p&gt;
    &lt;code&gt;Hannah Ritchie, Tuna Acisu, and Edouard Mathieu (2025) - “Does the news reflect what we die from?” Published online at OurWorldinData.org. Retrieved from: 'https://ourworldindata.org/does-the-news-reflect-what-we-die-from' [Online Resource]&lt;/code&gt;
    &lt;p&gt;BibTeX citation&lt;/p&gt;
    &lt;code&gt;@article{owid-does-the-news-reflect-what-we-die-from,
    author = {Hannah Ritchie and Tuna Acisu and Edouard Mathieu},
    title = {Does the news reflect what we die from?},
    journal = {Our World in Data},
    year = {2025},
    note = {https://ourworldindata.org/does-the-news-reflect-what-we-die-from}
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Reuse this work freely&lt;/head&gt;
    &lt;p&gt;All visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited.&lt;/p&gt;
    &lt;p&gt;The data produced by third parties and made available by Our World in Data is subject to the license terms from the original third-party authors. We will always indicate the original source of the data in our documentation, so you should always check the license of any such third-party data before use and redistribution.&lt;/p&gt;
    &lt;p&gt;All of our charts can be embedded in any site.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ourworldindata.org/does-the-news-reflect-what-we-die-from"/><published>2025-10-14T18:40:23+00:00</published></entry></feed>