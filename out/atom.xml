<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-25T14:08:12.258460+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45682560</id><title>Luau's performance</title><updated>2025-10-25T14:08:20.308806+00:00</updated><content>&lt;doc fingerprint="ba7bb85b68e91340"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Performance&lt;/head&gt;
    &lt;p&gt;One of main goals of Luau is to enable high performance code, with gameplay code being the main use case. This can be viewed as two separate goals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make idiomatic code that wasn’t tuned faster&lt;/item&gt;
      &lt;item&gt;Enable even higher performance through careful tuning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both of these goals are important - it’s insufficient to just focus on the highly tuned code, and all things being equal we prefer to raise all boats by implementing general optimizations. However, in some cases it’s important to be aware of optimizations that Luau does and doesn’t do.&lt;/p&gt;
    &lt;p&gt;Worth noting is that Luau is focused on, first and foremost, stable high performance code in interpreted context. This is because JIT compilation is not available on many platforms Luau runs on, and AOT compilation would only work for code that Roblox ships (and even that does not always work). This is in stark contrast with LuaJIT that, while providing an excellent interpreter as well, focuses a lot of the attention on JIT (with many optimizations unavailable in the interpreter).&lt;/p&gt;
    &lt;p&gt;Having said that, Luau has been updated to include an optional JIT component for x64 and arm64 platforms. This component can compile a selected set of functions, including limiting compilation to functions or modules marked explicitly by the user. While functions can be compiled at any time, automated JIT compilation decisions based on statistics/tracing are not performed. Luau JIT takes into account the type annotations present in the source code to specialize code paths and at this time, doesn’t include runtime analysis of the types/values flowing through the program.&lt;/p&gt;
    &lt;p&gt;The rest of this document goes into some optimizations that Luau employs and how to best leverage them when writing code. The document is not complete - a lot of optimizations are transparent to the user and involve detailed low-level tuning of various parts that is not described here - and all of this is subject to change without notice, as it doesn’t affect the semantics of valid code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast bytecode interpreter&lt;/head&gt;
    &lt;p&gt;Luau features a very highly tuned portable bytecode interpreter. It’s similar to Lua interpreter in that it’s written in C, but it’s highly tuned to yield efficient assembly when compiled with Clang and latest versions of MSVC. On some workloads it can match the performance of LuaJIT interpreter which is written in highly specialized assembly. We are continuing to tune the interpreter and the bytecode format over time; while extra performance can be extracted by rewriting the interpreter in assembly, we’re unlikely to ever do that as the extra gains at this point are marginal, and we gain a lot from C in terms of portability and being able to quickly implement new optimizations.&lt;/p&gt;
    &lt;p&gt;Of course the interpreter isn’t typical C code - it uses many tricks to achieve extreme levels of performance and to coerce the compiler to produce efficient assembly. Due to a better bytecode design and more efficient dispatch loop it’s noticeably faster than Lua 5.x (including Lua 5.4 which made some of the changes similar to Luau, but doesn’t come close). The bytecode design was partially inspired by excellent LuaJIT interpreter. Most computationally intensive scripts only use the interpreter core loop and builtins, which on x64 compiles into ~16 KB, thus leaving half of the instruction cache for other infrequently called code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizing compiler&lt;/head&gt;
    &lt;p&gt;Unlike Lua and LuaJIT, Luau uses a multi-pass compiler with a frontend that parses source into an AST and a backend that generates bytecode from it. This carries a small penalty in terms of compilation time, but results in more flexible code and, crucially, makes it easier to optimize the generated bytecode.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Compilation throughput isn’t the main focus in Luau, but our compiler is reasonably fast; with all currently implemented optimizations enabled, it compiles 950K lines of Luau code in 1 second on a single core of a desktop Ryzen 5900X CPU, producing bytecode and debug information.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While bytecode optimizations are limited due to the flexibility of Luau code (e.g. &lt;code&gt;a * 1&lt;/code&gt; may not be equivalent to &lt;code&gt;a&lt;/code&gt; if &lt;code&gt;*&lt;/code&gt; is overloaded through metatables), even in absence of type information Luau compiler can perform some optimizations such as “deep” constant folding across functions and local variables, perform upvalue optimizations for upvalues that aren’t mutated, do analysis of builtin function usage, optimize the instruction sequences for multiple variable assignments, and some peephole optimizations on the resulting bytecode. The compiler can also be instructed to use more aggressive optimizations by enabling optimization level 2 (&lt;code&gt;-O2&lt;/code&gt; in CLI tools), some of which are documented further on this page.&lt;/p&gt;
    &lt;p&gt;Most bytecode optimizations are performed on individual statements or functions, however the compiler also does a limited amount of interprocedural optimizations; notably, calls to local functions can be optimized with the knowledge of the argument count or number of return values involved. Interprocedural optimizations are limited to a single module due to the compilation model.&lt;/p&gt;
    &lt;p&gt;Luau compiler is also able to use type information to do further optimizations. Because we control the entire stack (unlike e.g. TypeScript where the type information is discarded completely before reaching the VM), we have more flexibility there and can make some tradeoffs during codegen even if the type system isn’t completely sound. For example, it might be reasonable to assume that in presence of known types, we can infer absence of side effects for arithmetic operations and builtins - if the runtime types mismatch due to intentional violation of the type safety through global injection, the code will still be safely sandboxed. Type information is currently limited to small peephole optimizations, but it has a potential to unlock optimizations such as common subexpression elimination and allocation hoisting in the future, without having to rely on a JIT. These future optimizations opportunities are speculative pending further research.&lt;/p&gt;
    &lt;head rend="h2"&gt;Epsilon-overhead debugger&lt;/head&gt;
    &lt;p&gt;It’s important for Luau to have stable and predictable performance. Something that comes up in Lua-based environments often is the use of line hooks to implement debugging (both for breakpoints and for stepping). This is problematic because the support for hooks is typically not free in general, but importantly once the hook is enabled, calling the hook has a considerable overhead, and the hook itself may be very costly to evaluate since it will need to associate the script:line pair with the breakpoint information.&lt;/p&gt;
    &lt;p&gt;Luau does not support hooks at all, and relies on first-class support for breakpoints (using bytecode patching) and single-stepping (using a custom interpreter loop) to implement debugging. As a result, the presence of breakpoints doesn’t slow the script execution down - the only noticeable discrepancy between running code under a debugger and without a debugger should be in cases where breakpoints are evaluated and skipped based on breakpoint conditions, or when stepping over long-running fragments of code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inline caching for table and global access&lt;/head&gt;
    &lt;p&gt;Table access for field lookup is optimized in Luau using a mechanism that blends inline caching (classically used in Java/JavaScript VMs) and HREFs (implemented in LuaJIT). Compiler can predict the hash slot used by field lookup, and the VM can correct this prediction dynamically.&lt;/p&gt;
    &lt;p&gt;As a result, field access can be very fast in Luau, provided that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The field name is known at compile time. To make sure this is the case, &lt;code&gt;table.field&lt;/code&gt;notation is recommended, although the compiler will also optimize&lt;code&gt;table["field"]&lt;/code&gt;when the expression is known to be a constant string.&lt;/item&gt;
      &lt;item&gt;The field access doesn’t use metatables. The fastest way to work with tables in Luau is to store fields directly inside the table, and store methods in the metatable (see below); access to “static” fields in classic OOP designs is best done through &lt;code&gt;Class.StaticField&lt;/code&gt;instead of&lt;code&gt;object.StaticField&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The object structure is usually uniform. While it’s possible to use the same function to access tables of different shape - e.g. &lt;code&gt;function getX(obj) return obj.x end&lt;/code&gt;can be used on any table that has a field&lt;code&gt;"x"&lt;/code&gt;- it’s best to not vary the keys used in the tables too much, as it defeats this optimization.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The same optimization is applied to the custom globals declared in the script, although it’s best to avoid these altogether by using locals instead. Still, this means that the difference between &lt;code&gt;function&lt;/code&gt; and &lt;code&gt;local function&lt;/code&gt; is less pronounced in Luau.&lt;/p&gt;
    &lt;head rend="h2"&gt;Importing global access chains&lt;/head&gt;
    &lt;p&gt;While global access for library functions can be optimized in a similar way, this optimization breaks down when the global table is using sandboxing through metatables, and even when globals aren’t sandboxed, &lt;code&gt;math.max&lt;/code&gt; still requires two table accesses.&lt;/p&gt;
    &lt;p&gt;It’s always possible to “localize” the global accesses by using &lt;code&gt;local max = math.max&lt;/code&gt;, but this is cumbersome - in practice it’s easy to forget to apply this optimization. To avoid relying on programmers remembering to do this, Luau implements a special optimization called “imports”, where most global chains such as &lt;code&gt;math.max&lt;/code&gt; are resolved when the script is loaded instead of when the script is executed.&lt;/p&gt;
    &lt;p&gt;This optimization relies on being able to predict the shape of the environment table for a given function; this is possible due to global sandboxing, however this optimization is invalid in some cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;loadstring&lt;/code&gt;can load additional code that runs in context of the caller’s environment&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt;can directly modify the environment of any function&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The use of any of these functions performs a dynamic deoptimization, marking the affected environment as “impure”. The optimizations are only in effect on functions with “pure” environments - because of this, the use of &lt;code&gt;loadstring&lt;/code&gt;/&lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt; is not recommended. Note that &lt;code&gt;getfenv&lt;/code&gt; deoptimizes the environment even if it’s only used to read values from the environment.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Luau still supports these functions as part of our backwards compatibility promise, although we’d love to switch to Lua 5.2’s&lt;/p&gt;&lt;code&gt;_ENV&lt;/code&gt;as that mechanism is cleaner and doesn’t require costly dynamic deoptimization.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Fast method calls&lt;/head&gt;
    &lt;p&gt;Luau specializes method calls to improve their performance through a combination of compiler, VM and binding optimizations. Compiler emits a specialized instruction sequence when methods are called through &lt;code&gt;obj:Method&lt;/code&gt; syntax (while this isn’t idiomatic anyway, you should avoid &lt;code&gt;obj.Method(obj)&lt;/code&gt;). When the object in question is a Lua table, VM performs some voodoo magic based on inline caching to try to quickly discover the implementation of this method through the metatable.&lt;/p&gt;
    &lt;p&gt;For this to be effective, it’s crucial that &lt;code&gt;__index&lt;/code&gt; in a metatable points to a table directly. For performance reasons it’s strongly recommended to avoid &lt;code&gt;__index&lt;/code&gt; functions as well as deep &lt;code&gt;__index&lt;/code&gt; chains; an ideal object in Luau is a table with a metatable that points to itself through &lt;code&gt;__index&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When the object in question is a reflected userdata, a special mechanism called “namecall” is used to minimize the interop cost. In classical Lua binding model, &lt;code&gt;obj:Method&lt;/code&gt; is called in two steps, retrieving the function object (&lt;code&gt;obj.Method&lt;/code&gt;) and calling it; both steps are often implemented in C++, and the method retrieval needs to use a method object cache - all of this makes method calls slow.&lt;/p&gt;
    &lt;p&gt;Luau can directly call the method by name using the “namecall” extension, and an optimized reflection layer can retrieve the correct method quickly through more voodoo magic based on string interning and custom Luau features that aren’t exposed through Luau scripts.&lt;/p&gt;
    &lt;p&gt;As a result of both optimizations, common Lua tricks of caching the method in a local variable aren’t very productive in Luau and aren’t recommended either.&lt;/p&gt;
    &lt;head rend="h2"&gt;Specialized builtin function calls&lt;/head&gt;
    &lt;p&gt;Due to global sandboxing and the ability to dynamically deoptimize code running in impure environments, in pure environments we go beyond optimizing the interpreter and optimize many built-in functions through a “fastcall” mechanism.&lt;/p&gt;
    &lt;p&gt;For this mechanism to work, function call must be “obvious” to the compiler - it needs to call a builtin function directly, e.g. &lt;code&gt;math.max(x, 1)&lt;/code&gt;, although it also works if the function is “localized” (&lt;code&gt;local max = math.max&lt;/code&gt;); this mechanism doesn’t work for indirect function calls unless they were inlined during compilation, and doesn’t work for method calls (so calling &lt;code&gt;string.byte&lt;/code&gt; is more efficient than &lt;code&gt;s:byte&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The mechanism works by directly invoking a highly specialized and optimized implementation of a builtin function from the interpreter core loop without setting up a stack frame and omitting other work; additionally, some fastcall specializations are partial in that they don’t support all types of arguments, for example all &lt;code&gt;math&lt;/code&gt; library builtins are only specialized for numeric arguments, so calling &lt;code&gt;math.abs&lt;/code&gt; with a string argument will fall back to the slower implementation that will do string-&amp;gt;number coercion.&lt;/p&gt;
    &lt;p&gt;As a result, builtin calls are very fast in Luau - they are still slightly slower than core instructions such as arithmetic operations, but only slightly so. The set of fastcall builtins is slowly expanding over time and as of this writing contains &lt;code&gt;assert&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;typeof&lt;/code&gt;, &lt;code&gt;rawget&lt;/code&gt;/&lt;code&gt;rawset&lt;/code&gt;/&lt;code&gt;rawequal&lt;/code&gt;, &lt;code&gt;getmetatable&lt;/code&gt;/&lt;code&gt;setmetatable&lt;/code&gt;, &lt;code&gt;tonumber&lt;/code&gt;/&lt;code&gt;tostring&lt;/code&gt;, all functions from &lt;code&gt;math&lt;/code&gt; (except &lt;code&gt;noise&lt;/code&gt; and &lt;code&gt;random&lt;/code&gt;/&lt;code&gt;randomseed&lt;/code&gt;) and &lt;code&gt;bit32&lt;/code&gt;, and some functions from &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;table&lt;/code&gt; library.&lt;/p&gt;
    &lt;p&gt;Some builtin functions have partial specializations that reduce the cost of the common case further. Notably:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;assert&lt;/code&gt;is specialized for cases when the assertion return value is not used and the condition is truthy; this helps reduce the runtime cost of assertions to the extent possible&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bit32.extract&lt;/code&gt;is optimized further when field and width selectors are constant&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;select&lt;/code&gt;is optimized when the second argument is&lt;code&gt;...&lt;/code&gt;; in particular,&lt;code&gt;select(x, ...)&lt;/code&gt;is O(1) when using the builtin dispatch mechanism even though it’s normally O(N) in variadic argument count.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some functions from &lt;code&gt;math&lt;/code&gt; library like &lt;code&gt;math.floor&lt;/code&gt; can additionally take advantage of advanced SIMD instruction sets like SSE4.1 when available.&lt;/p&gt;
    &lt;p&gt;In addition to runtime optimizations for builtin calls, many builtin calls, as well as constants like &lt;code&gt;math.pi&lt;/code&gt;/&lt;code&gt;math.huge&lt;/code&gt;, can also be constant-folded by the bytecode compiler when using aggressive optimizations (level 2); this currently applies to most builtin calls with constant arguments and a single return value. For builtin calls that can not be constant folded, compiler assumes knowledge of argument/return count (level 2) to produce more efficient bytecode instructions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized table iteration&lt;/head&gt;
    &lt;p&gt;Luau implements a fully generic iteration protocol; however, for iteration through tables in addition to generalized iteration (&lt;code&gt;for .. in t&lt;/code&gt;) it recognizes three common idioms (&lt;code&gt;for .. in ipairs(t)&lt;/code&gt;, &lt;code&gt;for .. in pairs(t)&lt;/code&gt; and &lt;code&gt;for .. in next, t&lt;/code&gt;) and emits specialized bytecode that is carefully optimized using custom internal iterators.&lt;/p&gt;
    &lt;p&gt;As a result, iteration through tables typically doesn’t result in function calls for every iteration; the performance of iteration using generalized iteration, &lt;code&gt;pairs&lt;/code&gt; and &lt;code&gt;ipairs&lt;/code&gt; is comparable, so generalized iteration (without the use of &lt;code&gt;pairs&lt;/code&gt;/&lt;code&gt;ipairs&lt;/code&gt;) is recommended unless the code needs to be compatible with vanilla Lua or the specific semantics of &lt;code&gt;ipairs&lt;/code&gt; (which stops at the first &lt;code&gt;nil&lt;/code&gt; element) is required. Additionally, using generalized iteration avoids calling &lt;code&gt;pairs&lt;/code&gt; when the loop starts which can be noticeable when the table is very short.&lt;/p&gt;
    &lt;p&gt;Iterating through array-like tables using &lt;code&gt;for i=1,#t&lt;/code&gt; tends to be slightly slower because of extra cost incurred when reading elements from the table.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized table length&lt;/head&gt;
    &lt;p&gt;Luau tables use a hybrid array/hash storage, like in Lua; in some sense “arrays” don’t truly exist and are an internal optimization, but some operations, notably &lt;code&gt;#t&lt;/code&gt; and functions that depend on it, like &lt;code&gt;table.insert&lt;/code&gt;, are defined by the Luau/Lua language to allow internal optimizations. Luau takes advantage of that fact.&lt;/p&gt;
    &lt;p&gt;Unlike Lua, Luau guarantees that the element at index &lt;code&gt;#t&lt;/code&gt; is stored in the array part of the table. This can accelerate various table operations that use indices limited by &lt;code&gt;#t&lt;/code&gt;, and this makes &lt;code&gt;#t&lt;/code&gt; worst-case complexity O(logN), unlike Lua where the worst case complexity is O(N). This also accelerates computation of this value for small tables like &lt;code&gt;{ [1] = 1 }&lt;/code&gt; since we never need to look at the hash part.&lt;/p&gt;
    &lt;p&gt;The “default” implementation of &lt;code&gt;#t&lt;/code&gt; in both Lua and Luau is a binary search. Luau uses a special branch-free (depending on the compiler…) implementation of the binary search which results in 50+% faster computation of table length when it needs to be computed from scratch.&lt;/p&gt;
    &lt;p&gt;Additionally, Luau can cache the length of the table and adjust it following operations like &lt;code&gt;table.insert&lt;/code&gt;/&lt;code&gt;table.remove&lt;/code&gt;; this means that in practice, &lt;code&gt;#t&lt;/code&gt; is almost always a constant time operation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating and modifying tables&lt;/head&gt;
    &lt;p&gt;Luau implements several optimizations for table creation. When creating object-like tables, it’s recommended to use table literals (&lt;code&gt;{ ... }&lt;/code&gt;) and to specify all table fields in the literal in one go instead of assigning fields later; this triggers an optimization inspired by LuaJIT’s “table templates” and results in higher performance when creating objects. When creating array-like tables, if the maximum size of the table is known up front, it’s recommended to use &lt;code&gt;table.create&lt;/code&gt; function which can create an empty table with preallocated storage, and optionally fill it with a given value.&lt;/p&gt;
    &lt;p&gt;When the exact table shape isn’t known, Luau compiler can still predict the table capacity required in case the table is initialized with an empty literal (&lt;code&gt;{}&lt;/code&gt;) and filled with fields subsequently. For example, the following code creates a correctly sized table implicitly:&lt;/p&gt;
    &lt;code&gt;local v = {}
v.x = 1
v.y = 2
v.z = 3
return v
&lt;/code&gt;
    &lt;p&gt;When appending elements to tables, it’s recommended to use &lt;code&gt;table.insert&lt;/code&gt; (which is the fastest method to append an element to a table if the table size is not known). In cases when a table is filled sequentially, however, it can be more efficient to use a known index for insertion - together with preallocating tables using &lt;code&gt;table.create&lt;/code&gt; this can result in much faster code, for example this is the fastest way to build a table of squares:&lt;/p&gt;
    &lt;code&gt;local t = table.create(N)

for i=1,N do
	t[i] = i * i
end
&lt;/code&gt;
    &lt;head rend="h2"&gt;Native vector math&lt;/head&gt;
    &lt;p&gt;Luau uses tagged value storage - each value contains a type tag and the data that represents the value of a given type. Because of the need to store 64-bit double precision numbers and 64-bit pointers, we don’t use NaN tagging and have to pay the cost of 16 bytes per value.&lt;/p&gt;
    &lt;p&gt;We take advantage of this to provide a native value type that can store a 32-bit floating point vector with 3 components. This type is fundamental to game computations and as such it’s important to optimize the storage and the operations with that type - our VM implements first class support for all math operations and component manipulation, which essentially means we have native 3-wide SIMD support. For code that uses many vector values this results in significantly smaller GC pressure and significantly faster execution, and gives programmers a mechanism to hand-vectorize numeric code if need be.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized upvalue storage&lt;/head&gt;
    &lt;p&gt;Lua implements upvalues as garbage collected objects that can point directly at the thread’s stack or, when the value leaves the stack frame (and is “closed”), store the value inside the object. This representation is necessary when upvalues are mutated, but inefficient when they aren’t - and 90% or more of upvalues aren’t mutated in typical Lua code. Luau takes advantage of this by reworking upvalue storage to prioritize immutable upvalues - capturing upvalues that don’t change doesn’t require extra allocations or upvalue closing, resulting in faster closure allocation, faster execution, faster garbage collection and faster upvalue access due to better memory locality.&lt;/p&gt;
    &lt;p&gt;Note that “immutable” in this case only refers to the variable itself - if the variable isn’t assigned to it can be captured by value, even if it’s a table that has its contents change.&lt;/p&gt;
    &lt;p&gt;When upvalues are mutable, they do require an extra allocated object; we carefully optimize the memory consumption and access cost for mutable upvalues to reduce the associated overhead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closure caching&lt;/head&gt;
    &lt;p&gt;With optimized upvalue storage, creating new closures (function objects) is more efficient but still requires allocating a new object every time. This can be problematic for cases when functions are passed to algorithms like &lt;code&gt;table.sort&lt;/code&gt; or functions like &lt;code&gt;pcall&lt;/code&gt;, as it results in excessive allocation traffic which then leads to more work for garbage collector.&lt;/p&gt;
    &lt;p&gt;To make closure creation cheaper, Luau compiler implements closure caching - when multiple executions of the same function expression are guaranteed to result in the function object that is semantically identical, the compiler may cache the closure and always return the same object. This changes the function identity which may affect code that uses function objects as table keys, but preserves the calling semantics - compiler will only do this if calling the original (cached) function behaves the same way as a newly created function would. The heuristics used for this optimization are subject to change; currently, the compiler will cache closures that have no upvalues, or all upvalues are immutable (see previous section) and are declared at the module scope, as the module scope is (almost always) evaluated only once.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast memory allocator&lt;/head&gt;
    &lt;p&gt;Similarly to LuaJIT, but unlike vanilla Lua, Luau implements a custom allocator that is highly specialized and tuned to the common allocation workloads we see. The allocator design is inspired by classic pool allocators as well as the excellent &lt;code&gt;mimalloc&lt;/code&gt;, but through careful domain-specific tuning it beats all general purpose allocators we’ve tested, including &lt;code&gt;rpmalloc&lt;/code&gt;, &lt;code&gt;mimalloc&lt;/code&gt;, &lt;code&gt;jemalloc&lt;/code&gt;, &lt;code&gt;ptmalloc&lt;/code&gt; and &lt;code&gt;tcmalloc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean that memory allocation in Luau is free - it’s carefully optimized, but it still carries a cost, and a high rate of allocations requires more work from the garbage collector. The garbage collector is incremental, so short of some edge cases this rarely results in visible GC pauses, but can impact the throughput since scripts will interrupt to perform “GC assists” (helping clean up the garbage). Thus for high performance Luau code it’s recommended to avoid allocating memory in tight loops, by avoiding temporary table and userdata creation.&lt;/p&gt;
    &lt;p&gt;In addition to a fast allocator, all frequently used structures in Luau have been optimized for memory consumption, especially on 64-bit platforms, compared to Lua 5.1 baseline. This helps to reduce heap memory footprint and improve performance in some cases by reducing the memory bandwidth impact of garbage collection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized libraries&lt;/head&gt;
    &lt;p&gt;While the best performing code in Luau spends most of the time in the interpreter, performance of the standard library functions is critical to some applications. In addition to specializing many small and simple functions using the builtin call mechanism, we spend extra care on optimizing all library functions and providing additional functions beyond the Lua standard library that help achieve good performance with idiomatic code.&lt;/p&gt;
    &lt;p&gt;Functions from the &lt;code&gt;table&lt;/code&gt; library like &lt;code&gt;insert&lt;/code&gt;, &lt;code&gt;remove&lt;/code&gt; and &lt;code&gt;move&lt;/code&gt; have been tuned for performance on array-like tables, achieving 3x and more performance compared to un-tuned versions, and Luau provides additional functions like &lt;code&gt;table.create&lt;/code&gt; and &lt;code&gt;table.find&lt;/code&gt; to achieve further speedup when applicable. Our implementation of &lt;code&gt;table.sort&lt;/code&gt; is using &lt;code&gt;introsort&lt;/code&gt; algorithm which results in guaranteed worst case &lt;code&gt;NlogN&lt;/code&gt; complexity regardless of the input, and, together with the array-like specializations, helps achieve ~4x speedup on average.&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;string&lt;/code&gt; library, we use a carefully tuned dynamic string buffer implementation; it is optimized for smaller strings to reduce garbage created during string manipulation, and for larger strings it allows to produce a large string without extra copies, especially in cases where the resulting size is known ahead of time. Additionally, functions like &lt;code&gt;format&lt;/code&gt; have been tuned to avoid the overhead of &lt;code&gt;sprintf&lt;/code&gt; where possible, resulting in further speedups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Improved garbage collector pacing&lt;/head&gt;
    &lt;p&gt;Luau uses an incremental garbage collector which does a little bit of work every so often, and at no point does it stop the world to traverse the entire heap. The runtime will make sure that the collector runs interspersed with the program execution as the program allocates additional memory, which is known as “garbage collection assists”, and can also run in response to explicit garbage collection invocation via &lt;code&gt;lua_gc&lt;/code&gt;. In interactive environments such as video game engines it’s possible, and even desirable, to request garbage collection every frame to make sure assists are minimized, since that allows scheduling the garbage collection to run concurrently with other engine processing that doesn’t involve script execution.&lt;/p&gt;
    &lt;p&gt;Inspired by excellent work by Austin Clements on Go’s garbage collector pacer, we’ve implemented a pacing algorithm that uses a proportional–integral–derivative controller to estimate internal garbage collector tunables to reach a target heap size, defined as a percentage of the live heap data (which is more intuitive and actionable than Lua 5.x “GC pause” setting). Luau runtime also estimates the allocation rate making it easy (given uniform allocation rates) to adjust the per-frame garbage collection requests to do most of the required GC work outside of script execution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reduced garbage collector pauses&lt;/head&gt;
    &lt;p&gt;While Luau uses an incremental garbage collector, once per each collector cycle it runs a so-called “atomic” step. While all other GC steps can do very little work by only looking at a few objects at a given time, which means that the collector can have arbitrarily short pauses, the “atomic” step needs to traverse some amount of data that, in some cases, may scale with the application heap. Since atomic step is indivisible, it can result in occasional pauses on the order of tens of milliseconds, which is problematic for interactive applications. We’ve implemented a series of optimizations to help reduce the atomic step.&lt;/p&gt;
    &lt;p&gt;Normally objects that have been modified after the GC marked them in an incremental mark phase need to be rescanned during atomic phase, so frequent modifications of existing tables may result in a slow atomic step. To address this, we run a “remark” step where we traverse objects that have been modified after being marked once more (incrementally); additionally, the write barrier that triggers for object modifications changes the transition logic during remark phase to reduce the probability that the object will need to be rescanned.&lt;/p&gt;
    &lt;p&gt;Another source of scalability challenges is coroutines. Writes to coroutine stacks don’t use a write barrier, since that’s prohibitively expensive as they are too frequent. This means that coroutine stacks need to be traversed during atomic step, so applications with many coroutines suffer large atomic pauses. To address this, we implement incremental marking of coroutines: marking a coroutine makes it “inactive” and resuming a coroutine (or pushing extra objects on the coroutine stack via C API) makes it “active”. Atomic step only needs to traverse active coroutines again, which reduces the cost of atomic step by effectively making coroutine collection incremental as well.&lt;/p&gt;
    &lt;p&gt;While large tables can be a problem for incremental GC in general since currently marking a single object is indivisible, large weak tables are a unique challenge because they also need to be processed during atomic phase, and the main use case for weak tables - object caches - may result in tables with large capacity but few live objects in long-running applications that exhibit bursts of activity. To address this, weak tables in Luau can be marked as “shrinkable” by including &lt;code&gt;s&lt;/code&gt; as part of &lt;code&gt;__mode&lt;/code&gt; string, which results in weak tables being resized to the optimal capacity during GC. This option may result in missing keys during table iteration if the table is resized while iteration is in progress and as such is only recommended for use in specific circumstances.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized garbage collector sweeping&lt;/head&gt;
    &lt;p&gt;The incremental garbage collector in Luau runs three phases for each cycle: mark, atomic and sweep. Mark incrementally traverses all live objects, atomic finishes various operations that need to happen without mutator intervention (see previous section), and sweep traverses all objects in the heap, reclaiming memory used by dead objects and performing minor fixup for live objects. While objects allocated during the mark phase are traversed in the same cycle and thus may get reclaimed, objects allocated during the sweep phase are considered live. Because of this, the faster the sweep phase completes, the less garbage will accumulate; and, of course, the less time sweeping takes the less overhead there is from this phase of garbage collection on the process.&lt;/p&gt;
    &lt;p&gt;Since sweeping traverses the whole heap, we maximize the efficiency of this traversal by allocating garbage-collected objects of the same size in 16 KB pages, and traversing each page at a time, which is otherwise known as a paged sweeper. This ensures good locality of reference as consecutively swept objects are contiguous in memory, and allows us to spend no memory for each object on sweep-related data or allocation metadata, since paged sweeper doesn’t need to be able to free objects without knowing which page they are in. Compared to linked list based sweeping that Lua/LuaJIT implement, paged sweeper is 2-3x faster, and saves 16 bytes per object on 64-bit platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Function inlining and loop unrolling&lt;/head&gt;
    &lt;p&gt;By default, the bytecode compiler performs a series of optimizations that result in faster execution of the code, but they preserve both execution semantics and debuggability. For example, a function call is compiled as a function call, which may be observable via &lt;code&gt;debug.traceback&lt;/code&gt;; a loop is compiled as a loop, which may be observable via &lt;code&gt;lua_getlocal&lt;/code&gt;. To help improve performance in cases where these restrictions can be relaxed, the bytecode compiler implements additional optimizations when optimization level 2 is enabled (which requires using &lt;code&gt;-O2&lt;/code&gt; switch when using Luau CLI), namely function inlining and loop unrolling.&lt;/p&gt;
    &lt;p&gt;Only loops with loop bounds known at compile time, such as &lt;code&gt;for i=1,4 do&lt;/code&gt;, can be unrolled. The loop body must be simple enough for the optimization to be profitable; compiler uses heuristics to estimate the performance benefit and automatically decide if unrolling should be performed.&lt;/p&gt;
    &lt;p&gt;Only local functions (defined either as &lt;code&gt;local function foo&lt;/code&gt; or &lt;code&gt;local foo = function&lt;/code&gt;) can be inlined. The function body must be simple enough for the optimization to be profitable; compiler uses heuristics to estimate the performance benefit and automatically decide if each call to the function should be inlined instead. Additionally recursive invocations of a function can’t be inlined at this time, and inlining is completely disabled for modules that use &lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt; functions.&lt;/p&gt;
    &lt;p&gt;In both cases, in addition to removing the overhead associated with function calls or loop iteration, these optimizations can additionally benefit by enabling additional optimizations, such as constant folding of expressions dependent on loop iteration variable or constant function arguments, or using more efficient instructions for certain expressions when the inputs to these instructions are constants.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://luau.org/performance"/><published>2025-10-23T14:55:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45692984</id><title>Twake Drive – An open-source alternative to Google Drive</title><updated>2025-10-25T14:08:19.848796+00:00</updated><content>&lt;doc fingerprint="cb16d4485c6376c3"&gt;
  &lt;main&gt;
    &lt;p&gt; The open-source alternative to Google Drive. &lt;lb/&gt; Learn more » &lt;lb/&gt; Telegram | Website | Issues | Roadmap &lt;/p&gt;
    &lt;p&gt;To get a local copy up and running, please follow these simple steps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone the repo &lt;quote&gt;git clone https://github.com/linagora/twake-drive&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Run it with Docker &lt;code&gt;cd tdrive docker compose -f docker-compose.minimal.yml up&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Open http://localhost/ in a browser&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js (Version: &amp;gt;=18.x)&lt;/item&gt;
      &lt;item&gt;MongoDB&lt;/item&gt;
      &lt;item&gt;Yarn (recommended)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Launch MongoDB using&lt;/p&gt;
        &lt;quote&gt;docker run -p 27017:27017 -d mongo&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Launch frontend with&lt;/p&gt;
        &lt;quote&gt;cd tdrive/frontend/; yarn dev:start&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Launch backend with&lt;/p&gt;&lt;quote&gt;cd tdrive/backend/node/; SEARCH_DRIVER=mongodb DB_DRIVER=mongodb PUBSUB_TYPE=local \ DB_MONGO_URI=mongodb://localhost:27017 STORAGE_LOCAL_PATH=/[full-path-to-store-documents]/documents \ NODE_ENV=development yarn dev&lt;/quote&gt;&lt;p&gt;If you need more parameters, create/edit&lt;/p&gt;&lt;code&gt;tdrive/backend/node/config/development.json&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The app will be running on port 3000&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Twake Drive is licensed under Affero GPL v3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/linagora/twake-drive"/><published>2025-10-24T10:16:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45694856</id><title>First shape found that can't pass through itself</title><updated>2025-10-25T14:08:19.692806+00:00</updated><content>&lt;doc fingerprint="a390899286a00301"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;First Shape Found That Can’t Pass Through Itself&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Imagine you’re holding two equal-size dice. Is it possible to bore a tunnel through one die that’s big enough for the other to slide through?&lt;/p&gt;
    &lt;p&gt;Perhaps your instinct is to say “Surely not!” If so, you’re not alone. In the late 1600s, an unidentified person placed a bet to that effect with Prince Rupert of the Rhine. Rupert — a nephew of Charles I of England who commanded the Royalist forces in the English Civil War — spent his sunset years studying metallurgy and glassmaking in his laboratory at Windsor Castle.&lt;/p&gt;
    &lt;p&gt;Rupert won the bet. The mathematician John Wallis, recounting the story in 1693, didn’t say whether Rupert wrote a proof or bored a hole through an actual cube. But Wallis himself proved mathematically that, if you drill a straight tunnel in the direction of one of the cube’s inner diagonals, it can be made wide enough to allow another cube through. It’s a tight squeeze: If you make the second cube just 4% larger, it will no longer fit.&lt;/p&gt;
    &lt;p&gt;It’s natural to wonder which other shapes have this property. “I think of this problem as being quite canonical,” said Tom Murphy, a software engineer at Google who has explored the question extensively in his free time. It “would have gotten rediscovered and rediscovered — aliens would have come to this one.”&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;The full menagerie of shapes is too diverse to get a handle on, so mathematicians tend to focus on convex polyhedra: shapes, like the cube, that have flat sides and no protrusions or indentations. When such a shape is much wider in some directions than others, it’s usually easy to find a straight tunnel that will allow another copy of the shape to pass through. But many famous convex polyhedra — for instance the dodecahedron, or the truncated icosahedron, the shape that forms a soccer ball — are highly symmetric and difficult to analyze. Among these, “for hundreds of years we only knew of the cube,” said Jakob Steininger, a mathematician at Statistics Austria, Austria’s federal statistics organization.&lt;/p&gt;
    &lt;p&gt;Then, in 1968, Christoph Scriba proved that the tetrahedron and octahedron also have the “Rupert property,” as mathematicians now call it. And in a burst of activity over the past decade, professional mathematicians and hobbyists have found Rupert tunnels through many of the most widely studied convex polyhedra, including the dodecahedron, icosahedron and soccer ball.&lt;/p&gt;
    &lt;p&gt;The Rupert property appeared to be so widespread that mathematicians conjectured a general rule: Every convex polyhedron will have the Rupert property. No one could find one that didn’t — until now.&lt;/p&gt;
    &lt;p&gt;In a paper posted online in August, Steininger and Sergey Yurkevich — a researcher at A&amp;amp;R Tech, an Austrian transportation systems company — describe a shape with 90 vertices and 152 faces that they’ve named the Noperthedron (after “Nopert,” a coinage by Murphy that combines “Rupert” and “nope”). Steininger and Yurkevich proved that no matter how you bore a straight tunnel through a Noperthedron, a second Noperthedron cannot fit through.&lt;/p&gt;
    &lt;p&gt;The proof required a mix of theoretical advances and massive computer calculations, and relies on a delicate property of the Noperthedron’s vertices. “It’s a miracle that it works,” Steininger said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Passing Through the Shadows&lt;/head&gt;
    &lt;p&gt;To see how one cube can pass through another, imagine holding a cube over a table and examining its shadow (assuming it’s illuminated from above). If you hold the cube in the standard position, the shadow is a square. But if you point one of the corners directly upward, the shadow is a regular hexagon.&lt;/p&gt;
    &lt;p&gt;In 1693, Wallis showed that the square shadow fits inside the hexagon, leaving a thin margin. That means that if you point a cube’s corner upward, you can bore a vertical tunnel that’s big enough for a second cube to pass through. About a century later, Pieter Nieuwland showed that a different orientation casts an even better shadow — one that can accommodate a cube more than 6% larger than the cube with the tunnel.&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;Every subsequent analysis of more complicated shapes has relied on this process of turning the shape in different directions and looking for one shadow that fits inside another. With the aid of computers, mathematicians have found Rupert passages through a wide variety of shapes. Some are incredibly tight fits — for instance, the passage in a “triakis tetrahedron” has a margin that’s only about 0.000002 times the length of the shape’s radius. “The world of mixing computation and discrete geometry has flowered to make these kinds of calculations possible,” said Joseph O’Rourke, an emeritus professor at Smith College.&lt;/p&gt;
    &lt;p&gt;Researchers who have written algorithms to find Rupert passages have noticed a curious dichotomy: For any given convex polyhedron, the algorithm seems to either find a passage almost immediately, or not find one at all. In the past five years, mathematicians have accumulated a small collection of holdout shapes for which no passage has been found.&lt;/p&gt;
    &lt;p&gt;“I’ve had my desktop churn for two weeks on trying the rhombicosidodecahedron,” said Benjamin Grimmer, an applied mathematician at Johns Hopkins University, referring to a solid made of 62 regular triangles, squares and pentagons. “That one just seems to resist any attempt.”&lt;/p&gt;
    &lt;p&gt;But such resistance doesn’t prove that a shape is a Nopert. There are infinitely many ways to orient a shape, and a computer can only check finitely many. Researchers don’t know whether the holdouts are true Noperts or just shapes whose Rupert passages are hard to find.&lt;/p&gt;
    &lt;p&gt;What they do know is that candidate Noperts are incredibly rare. Starting last year, Murphy began to construct hundreds of millions of shapes. These include random polyhedra, polyhedra whose vertices lie on a sphere, polyhedra with special symmetries, and polyhedra in which he moved one vertex to intentionally mess up a previous Rupert passage. His algorithm easily found Rupert tunnels for nearly every one.&lt;/p&gt;
    &lt;p&gt;The contrast between these quick results and the stubbornness of the Nopert holdouts made some mathematicians suspect that true Noperts do exist. But until August, all they had were suspicions.&lt;/p&gt;
    &lt;head rend="h2"&gt;No Passage&lt;/head&gt;
    &lt;p&gt;Steininger, now 30, and Yurkevich, 29, have been friends since they participated together as teenagers in mathematics Olympiad competitions. Even though both eventually left academia (after a doctorate for Yurkevich and a master’s for Steininger), they have continued to explore unsolved problems together.&lt;/p&gt;
    &lt;p&gt;“We just had pizza three hours ago, and we talked about math almost the whole time,” Steininger told Quanta. “That’s what we do.”&lt;/p&gt;
    &lt;p&gt;Five years ago, the pair happened upon a YouTube video of one cube passing through another, and they were instantly smitten. They developed an algorithm to search for Rupert tunnels and soon became convinced that some shapes were Noperts. In a 2021 paper, they conjectured that the rhombicosidodecahedron is not Rupert. Their work, which preceded Murphy’s and Grimmer’s recent explorations, was, “I think, the first to conjecture that there might be solids that don’t have this property,” Steininger said.&lt;/p&gt;
    &lt;p&gt;If you want to prove that a shape is a Nopert, you must rule out Rupert tunnels for every possible orientation of the two shapes. Each orientation can be written down as a collection of rotation angles. This collection of angles can then be represented as a point in a higher-dimensional “parameter space.”&lt;/p&gt;
    &lt;p&gt;Florentina Stadlbauer; Courtesy of Jakob Steininger&lt;/p&gt;
    &lt;p&gt;Suppose you choose an orientation for your two shapes, and the computer tells you that the second shadow sticks out past the border of the first shadow. This rules out one point in the parameter space.&lt;/p&gt;
    &lt;p&gt;But you may be able to rule out much more than a single point. If the second shadow sticks out significantly, it would require a big change to move it inside the first shadow. In other words, you can rule out not just your initial orientation but also “nearby” orientations — an entire block of points in the parameter space. Steininger and Yurkevich came up with a result they called their global theorem, which quantifies precisely how large a block you can rule out in these cases. By testing many different points, you can potentially rule out block after block in the parameter space.&lt;/p&gt;
    &lt;p&gt;If these blocks cover the entire parameter space, you’ll have proved that your shape is a Nopert. But the size of each block depends on how far the second shadow sticks out beyond the first, and sometimes it doesn’t stick out very far. For instance, suppose you start with the two shapes in exactly the same position, and then you slightly rotate the second shape. Its shadow will at most stick out just a tiny bit past the first shadow, so the global theorem will only rule out a tiny box. These boxes are too small to cover the whole parameter space, leaving the possibility that some point you’ve missed might correspond to a Rupert tunnel.&lt;/p&gt;
    &lt;p&gt;To deal with these small reorientations, the pair came up with a complement to their global theorem that they called the local theorem. This result deals with cases where you can find three vertices (or corner points) on the boundary of the original shadow that satisfy some special requirements. For instance, if you connect those three vertices to form a triangle, it must contain the shadow’s center point. The researchers showed that if these requirements are met, then any small reorientation of the shape will create a shadow that pushes at least one of the three vertices further outward. So the new shadow can’t lie inside the original shadow, meaning it doesn’t create a Rupert tunnel.&lt;/p&gt;
    &lt;p&gt;If your shape casts a shadow that lacks three appropriate vertices, the local theorem won’t apply. And all the previously identified Nopert candidates have at least one shadow with this problem. Steininger and Yurkevich sifted through a database of hundreds of the most symmetric and beautiful convex polyhedra, but they couldn’t find any shape whose shadows all worked. So they decided to generate a suitable shape themselves.&lt;/p&gt;
    &lt;p&gt;They developed an algorithm to construct shapes and test them for the three-vertices property. Eventually, the algorithm produced the Noperthedron, which is made of 150 triangles and two regular 15-sided polygons. It looks like a rotund crystal vase with a wide base and top; one fan of the work has already 3D-printed a copy to use as a pencil holder.&lt;/p&gt;
    &lt;p&gt;Peter Lely&lt;/p&gt;
    &lt;p&gt;Steininger and Yurkevich then divided the parameter space of orientations into approximately 18 million tiny blocks, and tested the center point of each block to see if its corresponding orientation produced a Rupert passage. None of them did. Next, the researchers showed that each block satisfied either the local or global theorem, allowing them to rule out the entire block. Since these blocks fill out the entire parameter space, this meant that there is no Rupert passage through the Noperthedron.&lt;/p&gt;
    &lt;p&gt;The “natural conjecture has been proved false,” O’Rourke said.&lt;/p&gt;
    &lt;p&gt;It remains to be seen whether mathematicians can use the new method to generate other Noperts, or if they can find a different local theorem that can handle candidates like the rhombicosidodecahedron. But now that mathematicians know that Noperts do exist, “we’re on sound footing to study other shapes,” Murphy said.&lt;/p&gt;
    &lt;p&gt;Murphy, who like Steininger and Yurkevich has been exploring the question for its own sake, independent of his day job, feels a kinship across the centuries with Prince Rupert. “I like that he chose to use his retirement to do math and science in his castle,” he said.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Steininger and Yurkevich are on the lookout for new questions to tackle. “We’re just humble mathematicians — we love working on such problems,” Steininger said. “We’ll keep doing that.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/first-shape-found-that-cant-pass-through-itself-20251024/"/><published>2025-10-24T14:12:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45695134</id><title>Unlocking free WiFi on British Airways</title><updated>2025-10-25T14:08:19.237958+00:00</updated><content>&lt;doc fingerprint="2f3d91353c233d96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Unlocking free WiFi on British Airways&lt;/head&gt;
    &lt;p&gt;I was recently flying between HKG &amp;amp; LHR via British Airways. Iâd done the same flight back in 2023, and remember relying on the in-flight entertainment for the 14 hour journey. However, this time on my way to London, they had an interesting offer: Free WiFi for âMessagingâ, for members of âThe British Airways Clubâ.&lt;/p&gt;
    &lt;p&gt;I was pretty sure I wasnât a member of any sort of club (Iâm only flying economy anyway); but turns out this is just the name of their frequent flyer program. Conveniently enough, youâre able to sign up for this via the captive portal while in the sky; and although it asks for your E-Mail you donât need to verify it (thereby allowing you to complete the signup without access to the internet).&lt;/p&gt;
    &lt;p&gt;Once signed in, the captive portal invited me to âStart sessionâ, which true to itâs word, let me start texting people. I tried Whatsapp, Signal, Wechat and Discord. The first three worked (though not for images), Discord expectedly did not. Not bad for free wifi!&lt;/p&gt;
    &lt;head rend="h2"&gt;How does it know?&lt;/head&gt;
    &lt;p&gt;This was the first question I had as soon as I confirmed messaging did work. Itâs 2025; everything should be encrypted in transit. So how does it know if Iâm using Whatsapp vs. Discord? One idea I had is it just somehow capped the bandwidth / data transfer of individual TCP connections; so when youâre sending a single message or two it gets through, but something larger would fail.&lt;/p&gt;
    &lt;p&gt;To test this, I used my phone to open up the classic: example.com. Unfortunately this didnât load - so there mustâve been a bit more going onâ¦&lt;/p&gt;
    &lt;p&gt;Thankfully I had my laptop on me, so the next step was to connect to WiFi with the devtools open to the network tab, and wireshark on the side for good measure. After registering for the WiFi again, it was time to play around a bit. Opening up something like example.com revealed a TCP reset in the wireshark, right after the Client Hello, and my brain immediately jumped to SNI. Itâs something thatâs really annoyed me about the TLS spec since itâs widely used by ISPs in India to block websites (although there is work being done to fix this; ECH (which was itself previously ESNI)).&lt;/p&gt;
    &lt;p&gt;tl;dr SNI reveals the domain name of EVERY website you connect to in the TLS handshake, before the tunnel is established! Although the actual contents of what youâre doing, on say, totallynondodgywebsite.com are encrypted, anyone on the wire can see that you connected to it (including ISPs). My guess was that they had a set of whitelisted domains used by messaging apps, and if they see anything else, they just reset the connection.&lt;/p&gt;
    &lt;p&gt;Sidebar: peopleâs reactions when I try to tell this are always extremely varied. Many of my non-technical friends think anything you do without a VPN is visible to everyone, while some slightly technical ones still think that the URL (including query params) is visible, but the responses are not. Finally there is some subset of people who believe TLS means all data is encrypted in transit between client &amp;amp; server, though they had no idea SNI leaks all the domains they visit!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing out the theory&lt;/head&gt;
    &lt;p&gt;Although BA blocks DNS queries to all (well all I could remember) public resolvers, they do resolve any domain you throw at them, including MX, TXT, HTTPS records. (This itself could be an interesting area of exploration; especially since the DNS resolution can be triggered before signing up for free WiFi. Something along the lines of arbitrary subdomains which represent the request payload, and a custom nameserver that returns responses via the TXT record or something. Anywayâ¦).&lt;/p&gt;
    &lt;p&gt;Getting the A record of my personal server, I made a TLS handshake to the IP address directly, without any SNI. This was then reset by BA; so the lack of SNI is also blocked!&lt;/p&gt;
    &lt;code&gt;$ openssl s_client -connect 95.217.167.10:443
Connecting to 95.217.167.10
CONNECTED(00000003)
write:errno=104
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 0 bytes and written 302 bytes
Verification: OK
---
New, (NONE), Cipher is (NONE)
Protocol: TLSv1.3
This TLS version forbids renegotiation.
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)
---
&lt;/code&gt;
    &lt;p&gt;The next step was to try and test some SNI that might go through. Off the top of my head, I knew &lt;code&gt;wa.me&lt;/code&gt; was used by Whatsapp for some stuff, so I decided to use it. The way SNI works is it tells the server which host you want to connect to, so it can present the right TLS certificate. In my case, my server did not have any cert for &lt;code&gt;wa.me&lt;/code&gt; , but NGINX seemingly just ignores the SNI if it doesnât exist and returns the first cert (I think; could also be related to my config but I didnât look to much into this).&lt;/p&gt;
    &lt;p&gt;But basically, as long as I (the client) donât care, I can complete the TLS connection for any random cert the server offers me, even if in the SNI I provide a domain I donât control (e.g. &lt;code&gt;wa.me&lt;/code&gt; in this case).&lt;/p&gt;
    &lt;code&gt;$ openssl s_client -connect 95.217.167.10:443 -servername wa.me
Connecting to 95.217.167.10
CONNECTED(00000003)
depth=2 C=US, O=Internet Security Research Group, CN=ISRG Root X1
verify return:1
depth=1 C=US, O=Let's Encrypt, CN=R3
verify return:1
depth=0 CN=mijia.mywaifu.best
verify error:num=10:certificate has expired
notAfter=Jul 22 13:03:02 2023 GMT
verify return:1
depth=0 CN=mijia.mywaifu.best
notAfter=Jul 22 13:03:02 2023 GMT
verify return:1
---
Certificate chain
&amp;lt;snip&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Success! Using a Whatsapp SNI tricked BA into thinking Iâm âmessagingâ, which allowed the TLS tunnel to be established. Since I am connected to the server, to make sure it works I wrote an HTTP/1.1 request within the socket; using the host header of a real website on my NGINX instance&lt;/p&gt;
    &lt;code&gt;GET / HTTP/1.1
Host: saxrag.com

HTTP/1.1 200 OK
Server: nginx/1.18.0 (Ubuntu)
Date: Fri, 09 May 2025 19:14:46 GMT
Content-Type: text/html
Content-Length: 4968
Last-Modified: Wed, 09 Apr 2025 07:52:54 GMT
Connection: keep-alive
ETag: "67f62756-1368"
Cache-Control: no-cache
Accept-Ranges: bytes
&amp;lt;snip&amp;gt;
&lt;/code&gt;
    &lt;p&gt;I successfully managed to request and receive my homepage! All ~5KiB of it, not bad. Now the challenge was to extend this to browse any websiteâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Enemies to Lovers&lt;/head&gt;
    &lt;p&gt;Ok, my relationship with SNI isnât as cliche as that, and I think weâre still enemies. But this opens up some exciting opportunities to say the least. If I can convince BA that Iâm connecting to &lt;code&gt;wa.me&lt;/code&gt;, I can potentially do whatever I want over that connection (under the guise of âmessagingâ). So the requirments were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Establish a TLS connection using the SNI &lt;code&gt;wa.me&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tunnel arbitrary traffic through that connection&lt;/item&gt;
      &lt;item&gt;Do all this without actually owning / controlling the &lt;code&gt;wa.me&lt;/code&gt;domain&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From my past experiences w/ reverse-engineering etc., the most obvious way to do this seemed to be an HTTPS proxy. It had to be HTTPS specifically, since the connection to proxy was going to be what Iâd âfakeâ as Whatsapp. If the TLS handshake to the HTTPS proxy had the SNI &lt;code&gt;wa.me&lt;/code&gt; , BA should let it through, and then I can make the real requests I want via the proxy.&lt;/p&gt;
    &lt;p&gt;Unfortunately I was in the air, and without easy access to the internet to manage my servers and the like, I couldnât quite set all of this up; Iâd have to do that while on holiday and test it on the flight back. I could try and emulate the BA restrictions etc. while on thr ground, but I decided to YOLO it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Setup&lt;/head&gt;
    &lt;p&gt;I managed to find one of my VPSs that wasnât already using port 443. Letâs assume the public IP was &lt;code&gt;333.333.333.333&lt;/code&gt; (yes I know octets donât go beyond &lt;code&gt;0xFF&lt;/code&gt;, if you really want my IP check the screenshots below). I then setup an HTTP proxy on it using tinyproxy. However this just sets up a basic HTTP proxy, which was listening on &lt;code&gt;127.0.0.1:8080&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To add the TLS layer, I used stunnel. For the TLS setup of stunnel, I just generated some self-signed certs via openSSL using all defaults, except the common name (CN), for which I used &lt;code&gt;wa.me&lt;/code&gt;, since I wanted to try and ensure max compatibility (e.g. the client doesnât reject due to unexpected SNI vs. CN, or the server not knowing which cert to provide).&lt;/p&gt;
    &lt;code&gt;openssl req -nodes -newkey ed25519 -keyout ssl.key -x509 -days 365 -out ssl.crt
&lt;/code&gt;
    &lt;p&gt;UPDATE: actually, on the client I decided to ignore TLS errors (self-signed cert), and stunnel didnât care about SNI, so this (&lt;code&gt;CN&lt;/code&gt;) didnât matter too much. But for more legit use cases it definitely does!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing it out&lt;/head&gt;
    &lt;p&gt;To just make sure the proxy worked as expected, I tried it via curl directly on the IP:&lt;/p&gt;
    &lt;code&gt;$ curl -x https://user:pass@333.333.333.333:443 ifconfig.co -v
*   Trying 333.333.333.333:443...
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
*  CAfile: /etc/ssl/certs/ca-certificates.crt
*  CApath: none
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (OUT), TLS alert, unknown CA (560):
* SSL certificate problem: self-signed certificate
* closing connection #0
curl: (60) SSL certificate problem: self-signed certificate
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the webpage mentioned above.
&lt;/code&gt;
    &lt;p&gt;Of course! I just randomly generated the certs on my VPS, not signed by a âtrustedâ CA or anything. Well, we can tell cURL to ignore TLS errors for the proxy with the &lt;code&gt;--proxy-insecure&lt;/code&gt; flag, and now it works; the response is the IP of my VPS.&lt;/p&gt;
    &lt;p&gt;However thereâs a problem - if I connect to the proxy directly via the IP, there is no SNI extension set, so this would get blocked. The SNI extension is set when connection to a domain, so I need to configure &lt;code&gt;wa.me&lt;/code&gt; to point to &lt;code&gt;333.333.333.333&lt;/code&gt;. This can be done via the hosts file of course, but cURL also provides a quick CLI hack via &lt;code&gt;--resolve&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;curl --resolve wa.me:443:333.333.333.333 -x https://username:password@wa.me ifconfig.co --proxy-insecure -v
&lt;/code&gt;
    &lt;p&gt;This tells cURL how to resolve the IP. With this, I could now see the SNI being set to &lt;code&gt;wa.me&lt;/code&gt; via wireshark, and the connection to the proxy succeeding (TLS errors about the self-signed cert ignored of course). Not bad, now time to wait for my flight back to Hong Kongâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing it in-flight&lt;/head&gt;
    &lt;p&gt;If Iâd messed something up, I was cooked, since without internet access I wouldnât be able to fix it! My flight back was at 1935hrs local time, but Iâd been up since 0400 thanks to an early morning flight in from Edinburgh, and then spent the day browsing the markets, having pints and watching the Emilia Romagna Grand Prix. The place I went to even had screens above the urinals!&lt;/p&gt;
    &lt;p&gt;Anyway, despite being up for ~16 hours already, I was ready to see if my work would, well, work. Once we were sky-high, I connected to the WiFi (from my laptop), signed up for the BA loyalty program, and activated the âMessagingâ plan. Trying the curl command from above, I got back an HTTP 200 from &lt;code&gt;ifconfig.co&lt;/code&gt; with my VPS IP; it worked! For good measure I tried cURLing some more websites like example.com, google.com etc. to make sure stuff seemed fine.&lt;/p&gt;
    &lt;p&gt;The next challenge was to extend this to web browsing. Thankfully most modern browsers support sending traffic through an HTTPS proxy, and chromium even has a flag to disable TLS cert warnings (so it wonât complain about my self-signed cert, which obviously doesnât belong to the real &lt;code&gt;wa.me&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;I also had to set a DNS record for &lt;code&gt;wa.me&lt;/code&gt; to &lt;code&gt;333.333.333.333&lt;/code&gt; in my hosts file, so chromium would set the SNI to &lt;code&gt;wa.me&lt;/code&gt; in the TLS handshake, but the connection would be made to my VPS. Since the bandwidth would probably be quite limited (owing to not just the internet on an airplane, but proxying it via a VPS in Netherlands), I decided to load a very simple, text-heavy website: Hacker News.&lt;/p&gt;
    &lt;p&gt;Bada bing bada boom! Looks like we cooked. I can actually browse HN using BAâs free âmessagingâ WiFi! (Note: the reason you can see the HTTP requests in plaintext in wireshark is because I used SSLKEYLOGFILE and configured wireshark to decrypt TLS).&lt;/p&gt;
    &lt;p&gt;Unfortunately, trying to load websites with heavier assets would fail, with images on simple text blogs loading line-by-line. Well, at least its some dial-up nostalgia!&lt;/p&gt;
    &lt;p&gt;My guess is that on the free WiFi, apart from the SNI checks, they also throttle the bandwidth. Maybe they anticipated this kind of circumvention. On the other hand, if this is really the internet speed that the full plan unlocksâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: ECH&lt;/head&gt;
    &lt;p&gt;Earlier above I talked about work being done to fix the SNI leakage: ECH. The scope of explaining how it works is out of scope here, but I do encourage you to read up on it. Pretty good stuff! Itâll help this section make more sense.&lt;/p&gt;
    &lt;p&gt;I operate an ECH test website, so I decided to do some more setup before my flight. I basically created another ECHConfig, with the public_name set to &lt;code&gt;wa.me&lt;/code&gt;. Iâve a bit of a guide on how to do this btw, though it could do with some improvements.&lt;/p&gt;
    &lt;p&gt;Anyway, since ECH world, the public SNI is purely for the server to complete the outer ClientHello, and since ECH clients set the public SNI based on the ECHConfig, I can type in my real domain in firefox, which will still use the &lt;code&gt;wa.me&lt;/code&gt; domain as the public SNI. The inner Client Hello will then occur securely, containing the real SNI of &lt;code&gt;rfc5746.mywaifu.best&lt;/code&gt;, and the handshake will complete with the âlegitâ CA-signed certificate for that domain.&lt;/p&gt;
    &lt;p&gt;This worked as well, and without any TLS ignore flags, since the actual cert for &lt;code&gt;rfc5746.mywaifu.best&lt;/code&gt; was signed by a âtrusted CAâ (Letâs Encrypt). Whatâs more interesting is that this worked even on a non-standart TLS port: &lt;code&gt;7443&lt;/code&gt;! Not sure exactly why, but Iâm not complaining.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on ECHConfig resolution&lt;/head&gt;
    &lt;p&gt;Typically, ECHConfigs should be resolved via encrypted DNS, such as DNS-over-HTTPS. I believe this is what firefox does by default. I am not 100% sure if this is what happened while I was in flight, since Iâd think the DoH would be blocked on messaging WiFi? Or maybe they allow the DoH SNI as well, since newer phones default to that. If any of you are flying BA anytime soon, try it out and let me know!&lt;/p&gt;
    &lt;head rend="h2"&gt;SNI: Donât blindly trust it&lt;/head&gt;
    &lt;p&gt;SNI, as the name indicates (sorry) is just a âhintâ of sorts, from the client to the server. If someone controls both sides (client &amp;amp; server), they can put whatever fake value they want in here, for middleboxes to sniff out and try to analyze. While this unfortunately does work for applications like censorship (where an ISP or country is trying to block a particular website), for use cases such as threat detection it should not be relied on; malwre authors can âspoofâ the SNI when connecting to their C&amp;amp;C, since they donât actually need it, but it may look more innocent to middleboxes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Questions?&lt;/head&gt;
    &lt;p&gt;I would be happy to answer any questions you have! You can contact me via email, and please use my PGP key to encrypt all communications. (Backup E-Mail (PGP Key))&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.saxrag.com/tech/reversing/2025/06/01/BAWiFi.html"/><published>2025-10-24T14:40:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45695621</id><title>Code like a surgeon</title><updated>2025-10-25T14:08:19.156270+00:00</updated><content>&lt;doc fingerprint="355491dd119110ed"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;October 2025&lt;/head&gt;
    &lt;head rend="h1"&gt;Code like a surgeon&lt;/head&gt;
    &lt;p&gt;A lot of people say AI will make us all “managers” or “editors”…but I think this is a dangerously incomplete view!&lt;/p&gt;
    &lt;p&gt;Personally, I’m trying to code like a surgeon.&lt;/p&gt;
    &lt;p&gt;A surgeon isn’t a manager, they do the actual work! But their skills and time are highly leveraged with a support team that handles prep, secondary tasks, admin. The surgeon focuses on the important stuff they are uniquely good at.&lt;/p&gt;
    &lt;p&gt;My current goal with AI coding tools is to spend 100% of my time doing stuff that matters. (As a UI prototyper, that mostly means tinkering with design concepts.)&lt;/p&gt;
    &lt;p&gt;It turns out there are a LOT of secondary tasks which AI agents are now good enough to help out with. Some things I’m finding useful to hand off these days:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Before attempting a big task, write a guide to relevant areas of the codebase&lt;/item&gt;
      &lt;item&gt;Spike out an attempt at a big change. Often I won’t use the result but I’ll review it as a sketch of where to go&lt;/item&gt;
      &lt;item&gt;Fix typescript errors or bugs which have a clear specification&lt;/item&gt;
      &lt;item&gt;Write documentation about what I’m building&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I often find it useful to run these secondary tasks async in the background – while I’m eating lunch, or even literally overnight!&lt;/p&gt;
    &lt;p&gt;When I sit down for a work session, I want to feel like a surgeon walking into a prepped operating room. Everything is ready for me to do what I’m good at.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mind the autonomy slider&lt;/head&gt;
    &lt;p&gt;Notably, there is a huge difference between how I use AI for primary vs secondary tasks.&lt;/p&gt;
    &lt;p&gt;For the core design prototyping work, I still do a lot of coding by hand, and when I do use AI, I’m more careful and in the details. I need fast feedback loops and good visibility. (eg, I like Cursor tab-complete here)&lt;/p&gt;
    &lt;p&gt;Whereas for secondary tasks, I’m much much looser with it, happy to let an agent churn for hours in the background. The ability to get the job done eventually is the most important thing; speed and visibility matter less. Claude Code has been my go-to for long unsupervised sessions but Codex CLI is becoming a strong contender there too, possibly my new favorite.&lt;/p&gt;
    &lt;p&gt;These are very different work patterns! Reminds me of Andrej Karpathy’s “autonomy slider” concept. It’s dangerous to conflate different parts of the autonomy spectrum – the tools and mindset that are needed vary quite a lot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your agent doesn’t need a career trajectory&lt;/head&gt;
    &lt;p&gt;The “software surgeon” concept is a very old idea – Fred Brooks attributes it to Harlan Mills in his 1975 classic “The Mythical Man-Month”. He talks about a “chief programmer” who is supported by various staff including a “copilot” and various administrators. Of course, at the time, the idea was to have humans be in these support roles.&lt;/p&gt;
    &lt;p&gt;OK, so there is a super obvious angle here, that “AI has now made this approach economically viable where it wasn’t before”, yes yes… but I am also noticing a more subtle thing at play, something to do with status hierarchies.&lt;/p&gt;
    &lt;p&gt;A lot of the “secondary” tasks are “grunt work”, not the most intellectually fulfilling or creative part of the work. I have a strong preference for teams where everyone shares the grunt work; I hate the idea of giving all the grunt work to some lower-status members of the team. Yes, junior members will often have more grunt work, but they should also be given many interesting tasks to help them grow.&lt;/p&gt;
    &lt;p&gt;With AI this concern completely disappears! Now I can happily delegate pure grunt work. And the 24/7 availability is a big deal. I would never call a human intern at 11pm and tell them to have a research report on some code ready by 7am… but here I am, commanding my agent to do just that!&lt;/p&gt;
    &lt;head rend="h2"&gt;Notion is for surgeons?&lt;/head&gt;
    &lt;p&gt;Finally I’ll mention a couple thoughts on how this approach to work intersects with my employer, Notion.&lt;/p&gt;
    &lt;p&gt;First, as an employee, I find it incredibly valuable right now to work at a place that is bullish on AI coding tools. Having support for heavy use of AI coding tools, and a codebase that’s well setup for it, is enabling serious productivity gains for me – especially as a newcomer to a big codebase.&lt;/p&gt;
    &lt;p&gt;Secondly, as a product – in a sense I would say we are trying to bring this way of working to a broader group of knowledge workers beyond programmers. When I think about how that will play out, I like the mental model of enabling everyone to “work like a surgeon”.&lt;/p&gt;
    &lt;p&gt;The goal isn’t to delegate your core work, it’s to identify and delegate the secondary grunt work tasks, so you can focus on the main thing that matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related reads&lt;/head&gt;
    &lt;p&gt;If you liked this perspective, you might enjoy reading these other posts I’ve written about the nature of human-AI collaboration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enough AI copilots! We need AI HUDs: “anyone serious about designing for AI should consider non-copilot form factors that more directly extend the human mind…”&lt;/item&gt;
      &lt;item&gt;AI-generated tools can make programming more fun: “Instead, I used AI to build a custom debugger UI… which made it more fun for me to do the coding myself…”&lt;/item&gt;
      &lt;item&gt;ChatGPT as muse, not oracle: “What if we were to think of LLMs not as tools for answering questions, but as tools for asking us questions and inspiring our creativity?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.geoffreylitt.com/2025/10/24/code-like-a-surgeon"/><published>2025-10-24T15:25:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45696838</id><title>How to make a Smith chart</title><updated>2025-10-25T14:08:18.897731+00:00</updated><content>&lt;doc fingerprint="1d0b6d028f79c1a2"&gt;
  &lt;main&gt;
    &lt;p&gt;The Smith chart from electrical engineering is the image of a Cartesian grid under the function&lt;/p&gt;
    &lt;p&gt;f(z) = (z − 1)/(z + 1).&lt;/p&gt;
    &lt;p&gt;More specifically, it’s the image of a grid in the right half-plane.&lt;/p&gt;
    &lt;p&gt;This post will derive the basic mathematical properties of this graph but will not go into the applications. Said another way, I’ll explain how to make a Smith chart, not how to use one.&lt;/p&gt;
    &lt;p&gt;We will use z to denote points in the right half-plane and w to denote the image of these points under f. We will speak of lines in the z plane and the circles they correspond to in the w plane.&lt;/p&gt;
    &lt;head rend="h2"&gt;Möbius transformations&lt;/head&gt;
    &lt;p&gt;Our function f is a special case of a Möbius transformation. There is a theorem that says Möbius transformation map generalized circles to generalized circles. Here a generalized circle means a circle or a line; you can think of a line as a circle with infinite radius. We’re going to get a lot of mileage out of that theorem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Image of the imaginary axis&lt;/head&gt;
    &lt;p&gt;The function f maps the imaginary axis in the z plane to the unit circle in the w plane. We can prove this using the theorem above. The imaginary axis is a line, so it’s image is either a line or a circle. We can take three points on the imaginary axis in the z plane and see where they go.&lt;/p&gt;
    &lt;p&gt;When we pick z equal to 0, i, and −i from the imaginary axis we get w values of −1, i, and −i. These three w values do not line on a line, so the image of the imaginary axis must be a circle. Furthermore, three points uniquely determine a circle, so the image of the imaginary axis is the circle containing −1, i, and −i, i.e. the unit circle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Image of the right half-plane&lt;/head&gt;
    &lt;p&gt;The imaginary axis is the boundary of the right half-plane. Since it is mapped to the unit circle, the right half-plane is either mapped to the interior of the unit circle or the exterior of the unit circle. The point z = 1 goes to w = 0, and so the right half-plane is mapped inside the unit circle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Images of vertical lines&lt;/head&gt;
    &lt;p&gt;Let’s think about what happens to vertical lines in the z plane, lines with constant positive real part. The images of these lines in the w plane must be either lines or circles. And since the right-half plane gets mapped inside the unit circle, these lines must get mapped to circles.&lt;/p&gt;
    &lt;p&gt;We can say a little more. All lines contain the point ∞, and f(∞) = 1, so the image of every vertical line in the z plane is a circle in the w plane, inside the unit circle and tangent to the unit circle at w = 1. (Tossing around ∞ is a bit informal, but it’s easy to make rigorous.)&lt;/p&gt;
    &lt;p&gt;The vertical lines in the z plane&lt;/p&gt;
    &lt;p&gt;map to tangent circles in the w plane.&lt;/p&gt;
    &lt;head rend="h2"&gt;Images of horizontal lines&lt;/head&gt;
    &lt;p&gt;Next, let’s think about horizontal lines in the z plane, lines with constant imaginary part. The image of these lines is either a line or a circle. Which is it? The image of a line is a line if it contains ∞, otherwise it’s a circle. Now f(z) = ∞ if and only if z = −1, and so the image of the real axis is a line, but the image of every other horizontal line is a circle.&lt;/p&gt;
    &lt;p&gt;Since f(∞) = 1, the image of every horizontal line passes through 1, just as the images of all the vertical lines passes through 1.&lt;/p&gt;
    &lt;p&gt;Since horizontal lines extend past the right half-plane, the image circles extend past the unit circle. The part of the line with positive real part gets mapped inside the unit circle, and the part of the line with negative real part gets mapped outside the unit circle. In particular, the image of the positive real axis is the interval [−1, 1].&lt;/p&gt;
    &lt;p&gt;Möbius transformations are conformal maps, and so they preserve angles of intersection. Since horizontal lines are perpendicular to vertical lines, the circles that are the images of the horizontal lines meet the circles that are the images of vertical lines at right angles.&lt;/p&gt;
    &lt;p&gt;The horizontal rays in the z plane&lt;/p&gt;
    &lt;p&gt;become partial circles in the w plane.&lt;/p&gt;
    &lt;p&gt;If we were to look at horizontal lines rather than rays, i.e. if we extended the lines into the left half-plane, the images in the w plane would be full circles.&lt;/p&gt;
    &lt;p&gt;Now let’s put our images together. The grid&lt;/p&gt;
    &lt;p&gt;in the z plane becomes the following in the w plane.&lt;/p&gt;
    &lt;p&gt;An evenly spaced grid in the z plane becomes a very unevenly spaced graph in the w plane. Things are crowded on the right hand side and sparse on the left. A useable Smith chart needs to be roughly evenly filled in, which means it has to be the image of an unevenly filled in grid in the z plane. For example, you’d need more vertical lines in the z plane with small real values than with large real values.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.johndcook.com/blog/2025/10/23/smith-chart/"/><published>2025-10-24T17:18:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698554</id><title>Harnessing America's heat pump moment</title><updated>2025-10-25T14:08:18.333785+00:00</updated><content>&lt;doc fingerprint="676f107f6a85b4ba"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Heat Pumped&lt;/item&gt;
      &lt;item&gt;Posts&lt;/item&gt;
      &lt;item&gt;Harnessing America’s Heat Pump Moment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Harnessing America’s Heat Pump Moment&lt;/head&gt;
    &lt;head rend="h2"&gt;The tech works. The policy’s in place. So why are heat pumps still a hard sell?&lt;/head&gt;
    &lt;p&gt;Editor’s note: This is a guest post by Joseph DeNatale, an entrepreneur and project coordinator at Jetson Home. It originally appeared in Climate Drift earlier this year, and is republished on Heat Pumped with permission.&lt;/p&gt;
    &lt;p&gt;Joseph interviewed me when he was researching the piece, and I was excited to see that the final product touched many topics that I've been wanting to write about.&lt;/p&gt;
    &lt;p&gt;A big thank you to Joseph and Climate Drift for sharing with the Heat Pumped community - it's incredibly in-depth. Since there’s so much to digest, we’re splitting it up into 5 parts that we'll be sharing over the next few weeks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Execution Is Everything: A Personal Perspective&lt;/head&gt;
    &lt;p&gt;As a small business owner, I’ve built a career not around inventing new things, but around making things happen: making sure systems run smoothly, projects get completed on time, and clients feel taken care of.&lt;/p&gt;
    &lt;p&gt;My work has been rooted in the real-world, hands-on, often chaotic rhythm of operations, logistics, and direct client service. Whether it’s organizing teams to execute live events, refining workflows to scale a growing business, or managing the delicate art of closing a sale, I’ve learned one simple truth: the hardest part is never the idea. It’s the execution.&lt;/p&gt;
    &lt;p&gt;So when I began diving into the world of home electrification—particularly heat pumps—that same truth surfaced again, just with higher stakes.&lt;/p&gt;
    &lt;p&gt;The technology isn’t the issue. In fact, the technology is there. It’s been there for decades, and it is continuing to improve. We’re not waiting on some magical breakthrough or futuristic device.&lt;/p&gt;
    &lt;p&gt;We’re waiting on people—mostly homeowners and home contractors, but also manufacturers and policy makers—to embrace, understand, and implement what already works.&lt;/p&gt;
    &lt;p&gt;This piece isn’t about reinventing the wheel. It’s about understanding why we’re not using the wheel we already have—and what it’s going to take, from the human side of the equation, to make heat pumps the obvious, accessible, and default choice for millions of American homes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heat Pumps Aren’t New—But This Moment Is&lt;/head&gt;
    &lt;p&gt;In the world of climate solutions, it’s easy to get distracted by what’s shiny and new—sleek devices, breakthrough technologies, futuristic models of sustainability.&lt;/p&gt;
    &lt;p&gt;But not every climate solution is some new-fangled wonder gadget. Some of them already exist. Some of them are sitting in basements and behind houses, quietly doing the work.&lt;/p&gt;
    &lt;p&gt;The heat pump is one of them.&lt;/p&gt;
    &lt;p&gt;Heat pumps are not new. In fact, the idea has been around for well over a century, and the technology has been used widely for decades—mostly in Europe and Asia, but also in pockets of the U.S.—for everything from water heating to whole-home climate control.&lt;/p&gt;
    &lt;p&gt;Modern heat pumps are highly efficient—anywhere from 2-4x more efficient than a furnace—and capable of replacing both a furnace and an air conditioner with a single system in virtually every climate. For millions of homes across the country, they offer a cleaner, quieter, and more precise way to stay comfortable year-round.&lt;/p&gt;
    &lt;p&gt;Importantly, heat pumps have also been shown to match or beat the operating costs of even the cheapest heating option—natural gas—in many cases. This has been demonstrated through both local and national studies. One study showed that over 90% of American households would save on energy bills by replacing worn-out heating equipment with the right-sized heat pump.&lt;/p&gt;
    &lt;p&gt;Installation costs vary wildly depending on many factors in a home, but with the introduction of generous incentives via the Inflation Reduction Act (IRA) and additional state programs, even these costs can be on-par with fossil fuel alternatives.&lt;/p&gt;
    &lt;p&gt;So why aren’t they everywhere?&lt;/p&gt;
    &lt;p&gt;The answer isn’t technical. It’s cultural, economic, and human.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Heat pumps are proven, efficient, and climate-friendly—but adoption is still slow.&lt;/p&gt;
    &lt;p&gt;The barrier isn’t the tech. It’s people:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Contractors who default to what they know&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Homeowners who need education and guidance&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fragmented market full of noise and misinformation&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This piece discusses these challenges, and then explores five keys to accelerating adoption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Educate homeowners so heat pumps feel familiar and trustworthy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Train the next-gen workforce and upskill legacy HVAC pros.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Leverage better tools and data to size and install systems right.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prioritize quality and trust to build social proof and demand.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Align policy to phase out one-way ACs and normalize heat pumps.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Execution—not invention—is what will move the needle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hold On.. What’s A Heat Pump Again?&lt;/head&gt;
    &lt;p&gt;If you’re reading this piece, you probably know what a heat pump is (and you can feel free to skip this section).&lt;/p&gt;
    &lt;p&gt;But if you’re among the uninitiated – like, believe it or not, most people – here’s a (very) quick primer. (Editor’s note: check out Heat Pumps 101 if you want to dive deeper)&lt;/p&gt;
    &lt;p&gt;A heat pump works by drawing thermal energy (heat) out of the atmosphere and “pumping” it into the home. This process works in reverse for cooling. (Source)&lt;/p&gt;
    &lt;head rend="h3"&gt;The 2-Way AC&lt;/head&gt;
    &lt;p&gt;The term “heat pump”, it turns out, is a fairly unhelpful name for most people. In fact, there are some leaders in the home electrification industry who believe the name itself is one of the barriers to adoption. It’s one of many ways that the heat pump is misunderstood.&lt;/p&gt;
    &lt;p&gt;Think of a heat pump as a “2-way AC.” An air conditioner cools your home by pulling heat from inside an enclosed space and transferring it outside. Your refrigerator works the same way.&lt;/p&gt;
    &lt;p&gt;A heat pump does the same thing, but can also reverse the process to bring heat into the home. It uses a few key components to make this happen:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The fan pulls air across the system’s coils to help move heat in or out of the space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The evaporator coil absorbs heat from the air inside your home (in cooling mode) or from the outside air (in heating mode).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The compressor pressurizes and moves a fluid called refrigerant through the system, enabling the heat transfer process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The refrigerant is the working fluid that captures and carries heat from one place to another—either out of your home or into it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s important to understand is that a heat pump does not create heat. It also doesn’t create cold (cold is the absence of heat, just like darkness is the absence of light). A heat pump simply transfers – pumps! – heat from one place to another.&lt;/p&gt;
    &lt;p&gt;“The difference between a heat pump and a one-way AC is just one valve. It still works perfectly fine as an air conditioner—there’s no difference. That’s why we’ve started calling them “two-way ACs” as an education tool. It helps people compare a two-way AC, which has a reverse gear, with a one-way AC—which, in my mind, is basically broken.”&lt;/p&gt;
    &lt;p&gt;But what about in the winter when it’s below freezing? In any environment where the temperature is above absolute zero (remember the Kelvin scale?) there is still a significant amount of heat in the air in the form of thermal energy.&lt;/p&gt;
    &lt;p&gt;That’s why a heat pump can still heat your home even on the coldest day of the year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Heat Pumps Matter&lt;/head&gt;
    &lt;p&gt;The fact that heat pumps simply transfer heat—and do not create it—gives them the potential to heat homes without doing the thing that humans have done since time immemorial to keep warm: burn stuff.&lt;/p&gt;
    &lt;p&gt;In the U.S., over half of all homes still rely on burning fossil fuels for heat. Replacing those systems with electric, air source heat pumps (ASHPs) can significantly reduce household emissions, especially as the grid gets cleaner and moves towards a higher percentage of renewable energy (i.e. not burning stuff).&lt;/p&gt;
    &lt;p&gt;And, because they’re so efficient, heat pumps can lower operating costs over time—although this is highly dependent on where you live, as the cost of fuel and electricity varies widely. They’re also safer (no burning stuff), can improve indoor air quality (again, no burning), and create healthier, more comfortable homes.&lt;/p&gt;
    &lt;p&gt;Finally, heat pumps are a crucial component of an energy-independent home. Paired with solar panels and battery storage, a homeowner can heat and cool their home entirely with energy they generate on their own. Try that with a furnace!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Metric&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Gas Furnace&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Air-Source Heat Pump (ASHP)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Fuel Source&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Natural gas, propane, or oil&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Electricity&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Heating/Cooling&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Heating only&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Heats and cools (dual function)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Air Quality&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Can introduce combustion byproducts; potential for CO&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;No combustion; generally better indoor air quality&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Health/Safety&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Risk of gas leaks, carbon monoxide&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;No combustion risk; safer for indoor environments&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Comfort&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Delivers blasts of hot air; on/off “short cycles”&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;More consistent, even heating/cooling with variable-speed options&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Initial Cost&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Typically lower (although the cost of a furnace + AC if replaced at the same time is often higher)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Often higher upfront, especially for cold-climate models. Costs can be lowered via incentive programs.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Operating Cost&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Depends on gas prices; cheaper where gas is low&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Can be lower, especially with efficient models + incentives and/or when paired with solar + battery storage&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Emissions&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Emits CO₂ and other GHGs&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Zero onsite emissions; cleaner with a green grid&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Climate Suitability&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Performs well in all climates&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cold-climate models now perform down to -15 to -20°F&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Incentives/Rebates&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Limited (varies by region)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Significant federal/state incentives available&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is not a marginal climate solution. According to the IEA, global heat pump adoption could reduce carbon emissions by half a billion tons annually—roughly equivalent to the annual emissions of all cars in Europe.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Heat Pump Moment Has Arrived&lt;/head&gt;
    &lt;p&gt;For years, heat pumps were a niche topic, something discussed by green building enthusiasts, early adopters, or homeowners with unusually high energy awareness.&lt;/p&gt;
    &lt;p&gt;But that’s no longer the case. Here are four reasons why:&lt;/p&gt;
    &lt;head rend="h3"&gt;Cultural Momentum Is Building&lt;/head&gt;
    &lt;p&gt;The electrification movement is no longer a fringe concept. The push to “electrify everything” has gained traction among policymakers, climate advocates, startups, utilities, and even popular media.&lt;/p&gt;
    &lt;p&gt;From Substack newsletters to YouTube explainers, there’s growing awareness that building decarbonization—and especially heating and cooling—is one of the most practical, scalable ways for regular people to cut their emissions. Campaigns like Rewiring America’s “Go Electric” initiative frame heat pumps not just as energy-efficient appliances, but as a gateway to modern, climate-aligned homes.&lt;/p&gt;
    &lt;p&gt;This momentum is turning into real action. Heat pumps have now outsold gas furnaces in the U.S. every year since 2022.&lt;/p&gt;
    &lt;head rend="h3"&gt;Federal and State Policy Is Aligned (For Now)&lt;/head&gt;
    &lt;p&gt;For the time being (Republicans’ “One Big, Beautiful Bill” notwithstanding), both federal and state governments are backing this transition with significant financial and structural support. Editor’s note: Ouch. Since this piece was originally written, OBBB passed, and most tax credits at the federal level phase out at the end of this year. If you’ve been on the fence about getting a heat pump, now might be a good time to act!&lt;/p&gt;
    &lt;p&gt;The Inflation Reduction Act (IRA) has introduced a suite of rebates, tax credits, and grant programs designed to make heat pumps more affordable and accessible. Single-family households can receive up to $8,000 in upfront rebates for heat pump installations and up to $2,000 in federal tax credits, not to mention additional support for electrical panel upgrades and home energy audits. Editor’s note: the IRA rebates are federally funded, but implemented at the state level. Not all states are participating, and some that are haven’t rolled out their programs yet. In other states like California, the funds are already exhausted.&lt;/p&gt;
    &lt;p&gt;State and local governments are also leading the way in the transition away from fossil fuels on both the demand and supply sides. Programs like Efficiency Maine, TECH Clean California, and Mass Save offer generous incentives and no-interest financing to homeowners that drive the cost of electrification upgrades down even further. Meanwhile, New York City has banned gas in new construction, and Massachusetts has ordered public utilities to begin phasing out natural gas, a move which is being studied in at least 11 other states.&lt;/p&gt;
    &lt;head rend="h3"&gt;Private Capital Is Following&lt;/head&gt;
    &lt;p&gt;The heat pump space is no longer just a niche for contractors and utilities—it’s attracting serious private investment. VC-backed companies like Quilt are reimagining the user experience with sleek, design-forward equipment and app-based controls. Others, like Elephant Energy and Forge, are building “heat pump concierge” platforms that manage the customer journey end-to-end—from sales to install to rebate navigation.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Cold-Climate Performance Myth Has Been Fully Debunked&lt;/head&gt;
    &lt;p&gt;One of the biggest myths about heat pumps—that they can’t handle cold weather—is now being debunked at scale. While older, single-speed models may have struggled in colder temperatures, especially when size and installed incorrectly, modern cold-climate, variable-speed air-source heat pumps can provide reliable heating even at outdoor temperatures of -20°F.&lt;/p&gt;
    &lt;p&gt;These systems are already in use in northern New England, the upper Midwest, and Canada. In Nordic countries—some of the coldest climates in the word—the technology has been viable for decades.&lt;/p&gt;
    &lt;p&gt;And yet, despite all this momentum, heat pump adoption is still slow.&lt;/p&gt;
    &lt;p&gt;Why? Because the hardest part isn’t scaling the technology. It’s aligning the people—contractors, homeowners, policymakers, and market actors—who need to make it happen.&lt;/p&gt;
    &lt;p&gt;“We’ve had the technology dialed for 20, 30, 40 years, depending on how you’re arguing it—but it’s not being applied. It’s a human problem. It’s not a technical one. The technical one has been solved.”&lt;/p&gt;
    &lt;p&gt;That’s where we go next.&lt;/p&gt;
    &lt;p&gt;This is part 1 in a 5 part series about challenges and solutions in accelerating heat pump adoption across the US. Stay tuned for the next issue!&lt;/p&gt;
    &lt;head rend="h2"&gt;Want a heat pump in your own home?&lt;/head&gt;
    &lt;p&gt;The first Heat Pumped group buy generated lots of enthusiasm! There are still a handful of slots left, but you’ll have to act fast if you’re interested. Sign-ups close later this month (or when all the slots fill, whichever comes first).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Do you want to participate in this group buy?&lt;/head&gt;
          &lt;p&gt;Fair &amp;amp; transparent heat pump pricing in the SF Bay Area and LA&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.heatpumped.org/p/harnessing-america-s-heat-pump-moment"/><published>2025-10-24T20:05:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698570</id><title>The Swift SDK for Android</title><updated>2025-10-25T14:08:17.922982+00:00</updated><content>&lt;doc fingerprint="360e51139294ee0b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing the Swift SDK for Android&lt;/head&gt;
    &lt;p&gt;Swift has matured significantly over the past decade — extending from cloud services to Windows applications, browser apps, and microcontrollers. Swift powers apps and services of all kinds, and thanks to its great interoperability, you can share code across platforms.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is an open group, free for anyone to join, that aims to expand Swift to Android. Today, we are pleased to announce nightly preview releases of the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;This milestone reflects months of effort by the Android workgroup, building on many years of grassroots community effort. With the SDK, developers can begin developing Android applications in Swift, opening new avenues for cross-platform development and accelerating innovation across the mobile ecosystem.&lt;/p&gt;
    &lt;p&gt;The Swift SDK for Android is available today, bundled with the Windows installer or downloadable separately for use on Linux or macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;p&gt;We’ve published a Getting Started guide to help you set up your first native Swift code on an Android device. The Swift for Android Examples help demonstrate end‑to‑end application workflows on Android.&lt;/p&gt;
    &lt;p&gt;With the Swift SDK for Android, you can now start porting your Swift packages to Android. Over 25% of packages in the Swift Package Index already build for Android, and the Community Showcase now indicates Android compatibility.&lt;/p&gt;
    &lt;p&gt;The swift-java project enables you to interoperate between Java and Swift. It is both a library and a code generator, enabling you to integrate Swift and Java in both directions by automatically generating safe and performant bindings. To learn about generating bindings to bring your business logic to Android, check out the recent Swift Server Side meetup talk by Mads Odgaard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;This preview release opens many new opportunities to continue improving these tools. We encourage you to share your experiences, ideas, tools and apps on the Swift forums. This post has been published on an associated thread for discussion, and new posts can be shared in the Android category.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is drafting a vision document, currently under review, for directing future work regarding Swift on Android. This vision will outline priority areas and guide community efforts to maximize impact across the ecosystem. In addition, we maintain a project board that tracks the status of major efforts, as well as official CI for the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;If you’re as excited as we are, join us and help make this ecosystem even better!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.swift.org/blog/nightly-swift-sdk-for-android/"/><published>2025-10-24T20:06:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698909</id><title>Study: MRI contrast agent causes harmful metal buildup in some patients</title><updated>2025-10-25T14:08:17.802450+00:00</updated><content>&lt;doc fingerprint="eeda31d2584a08f1"&gt;
  &lt;main&gt;&lt;p&gt;Editor's Note&lt;/p&gt;&lt;p&gt;New research offers a potential explanation for why some patients retain toxic metals long after undergoing an MRI.&lt;/p&gt;&lt;p&gt;Published in the journal Magnetic Resonance Imaging, the findings show that gadolinium contrast agents used in MRI scans may react with common dietary compounds to form harmful metal nanoparticles in the body. As detailed in an April 7 Newsweek report on the study, gadolinium-based contrast agents are injected to sharpen MRI images and are typically excreted without causing harm. However, gadolinium particles have been found lingering in the brain, kidneys, blood, and urine years after exposure, and the US Food and Drug Administration links retained gadolinium to nephrogenic systemic fibrosis (NSF).&lt;/p&gt;&lt;p&gt;The study specifically identifies a chemical reaction between gadolinium and oxalic acid—a compound found naturally in foods and produced in the body after ingesting vitamin C—as a likely contributor, Newsweek reports. Lab tests showed oxalic acid caused gadolinium to separate from its chelating agent and form nanoparticles capable of infiltrating cells in various organs.&lt;/p&gt;&lt;p&gt;Lead author Dr Brent Wagner told Newsweek he personally avoids vitamin C when undergoing MRI with contrast, citing its potential to increase gadolinium reactivity. “Metabolic milieu,” including high oxalic acid levels, could explain why some individuals experience severe symptoms while others do not, he said.&lt;/p&gt;&lt;p&gt;According to the article, nearly half of the patients found to have gadolinium traces in the body had received the contrast agent only once, suggesting that individual biology—not dosage—may influence risk. Dr Wagner theorized that nanoparticle formation could trigger a disproportionate immune response, with affected cells sending distress signals that intensify the body’s reaction.&lt;/p&gt;&lt;p&gt;The research team is now building an international patient registry to further study gadolinium accumulation. According to the article, the registry will collect blood, urine, hair, and fingernail samples to help identify individuals at greatest risk and understand long-term retention patterns.&lt;/p&gt;Read More &amp;gt;&amp;gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ormanager.com/briefs/study-mri-contrast-agent-causes-harmful-metal-buildup-in-some-patients/"/><published>2025-10-24T20:48:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45700663</id><title>What is intelligence? (2024)</title><updated>2025-10-25T14:08:17.541778+00:00</updated><content>&lt;doc fingerprint="9bbe08afcd469a02"&gt;
  &lt;main&gt;
    &lt;p&gt;Contents Close What is Intelligence? Foreword Preface Introduction Origins Abiogenesis Symbiogenesis Reproductive Functions Life as Computation Artificial Life Thermodynamics Dynamic Stability Complexification Virality Compression Embodiment Daisyworld Élan Vital Survival Being in Time Batting Average (No) Things in Themselves Anthropic Principle The Umwelt Within Latent Variables Modeling Learning by Evolving Cause by Effect Goodness and Truth Are Feelings Real? Interlude The Prehistory of Computation Cybernetics Love and War Killer App Behavior, Purpose, and Teleology Negative Feedback How We Know Universals Perceptrons Deep Learning Closing the Loop Learning Unkneading Transfer Green Screen Grandmother Cell Final Causes Meathead Neuromodulators Bootstrapping Beyond Reward Other Minds Forking Paths Children of Time Sphexish Matryoshka Dolls Intelligence Explosion Crew of Eight Homunculus Illusion and Reality Many Worlds Au Revoir Will What You Will What It Is Like to Be Weird Entanglement Zombie-Free Alters M-I-B The Interpreter Multifractal Boundaries Ourselves Block Diagram Recurrence Efference Copy Phenomenality Blindsight Subbasement Neocortex Social Neuroscience Transformers Language Sequence to Sequence Prediction Is All You Need Semantic Cosmology Alignment Attention But Is It Neuroscience? No Introspection Step by Step Generality Single System Hive Mind Modalities Pure Speech Babel Fish Testament Long Tails In-Context Learning Mary’s Room Parity Check As If Interlude No Perfect Heroes or Villains Evolutionary Transition Periodization Transitions Vulnerability Pecking Order Economics X-Risk Free Lunch Utility Big Tent Limits to Growth Tears of Joy Beyond Alignment Acknowledgments About the Author The Antikythera Book Series Glossary Bibliography Foreword Blaise Agüera y Arcas Lessons from AI About Evolution, Computing, and Minds ' Foreword&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://whatisintelligence.antikythera.org/"/><published>2025-10-25T01:21:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45700911</id><title>Advice for new principal tech ICs (i.e., notes to myself)</title><updated>2025-10-25T14:08:17.327637+00:00</updated><content>&lt;doc fingerprint="a757901728e5d63e"&gt;
  &lt;main&gt;
    &lt;p&gt;What makes an effective principal engineer or scientist? Here, I’ve distilled what I’ve observed from role models and quoted some of their advice below. While my perspective is naturally Amazon-centric, these ideas should also apply to most principal tech IC roles. As always, use your best judgment and assess if this advice applies to you and your situation.&lt;/p&gt;
    &lt;p&gt;1. Different principals will have different flavors. Some dive deep in one space while others excel at horizontal influence. Some are technical trailblazers who show how things are done, while others clarify complexity and illuminate the path forward. Still others are masters at aligning multiple orgs towards a common vision. No one flavor is more important than the other, and you need to find the flavor that plays to your strengths.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Amazon very specifically says they want their principals hands on. Any principal who’s not being hands-on for an extended period of time (okay in short bursts) is likely setting themselves up for failure.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2. The work that was core, and made you successful in your previous role, is now the side task. At this level, writing the code yourself may not be the best use of your time. While you should still be writing code (to stay connected to the work), your core role is now technical vision, design feedback, sponsorship, providing business, product, and technical context, finding new problems, connecting the dots, etc.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“This is a well-known saying for L7+ roles. It is not saying that you should spend less time coding, but that even if you are still coding 80% of the time, the most impactful part of your role is how to make everyone more effective. The mindset shifts from focusing solely on coding for a single project or perfecting on your own to influencing how all builders can build better across projects. This can be through contributing high-quality code to the repository and letting the code speak for itself, but also through other efforts—arguably more effective—such as giving feedback on design proposals, writing technical guidance, and driving the long-term vision.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;3. You’re kind of a part-time PM for technology. Scratch that, you’re part-time everything: product, design, engineering, science, quality assurance, hiring, finance, culture, etc. Nothing is not your job. The high judgment you have allows you to step out of your wheelhouse.&lt;/p&gt;
    &lt;p&gt;4. Your role will involve more communication, influence, and connecting the right people. Your projects will typically be larger in scope, involving teams across directors and even VPs. These projects won’t succeed without effective alignment and collaboration, and you’ll want to be careful not to ship your org chart to customers.&lt;/p&gt;
    &lt;p&gt;5. Being right is less than half the battle. You also have to convince others that you’re right, and more importantly, convince them to care enough to act on it. This means figuring out how to build momentum, who can sponsor the idea, and how to get it over the finish line.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Sometimes it also means starting the project and showing value to gain these things. Principals should lead by example and we want people to be proactive in solving issues and not wait for committee.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“I would almost say that principals are the committee, so who are you waiting for :)”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;6. A big part of the work is teaching the org to value something it doesn’t care about. Your audience will range from executives to working-level ICs. This is some of the hardest work you can do, and it fails often, but as someone who has an eye on the future and broader view, you should still do it. A mentor shared that for every 10 docs he pitched, probably three would get acted on, and he considered that a great outcome.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Compared to ICs who shoulder more of the day-to-day, team-level, and immediate deliverables, L7+ roles have more space to step back and take a broader, longer-term view of the company. It allows them to evaluate impact more objectively, and contributing at that level is where they add the most value—connecting to #7 below, those are the things that won’t happen without them.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;7. There’s a category of work that just won’t happen without you. It’s usually at the intersection of what you really care about and what you’re exceptional at. It could be building a quick prototype and socializing a new customer experience with leadership, building bridges across orgs or other practitioners in industry, or crafting the three-year vision. Focus on this category of work. Also, it’ll get deeper and narrower over the span of your career.&lt;/p&gt;
    &lt;p&gt;8. Sometimes, the most valuable thing you can do is not even to do the work but to connect the dots. This includes connecting teams who’re looking to do the work to teams who’ve done the work that they can reuse or learn from. It also includes identifying someone suitable who can do the work and grow along the way.&lt;/p&gt;
    &lt;p&gt;9. You can only do so much yourself. You’ll be more useful to the org spending part of the time coaching and mentoring others to be more effective. Perhaps set aside a few hours each week, such as office hours or regular syncs with folks you’re grooming. Identify one or two ICs you’d like to groom and set goals for yourself on how you’ll help them.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Scaling through others is the key point here. I like to think the success of a PE is when the org is able to make the same decisions as the PE would. Then the PE moves on to other ambiguous problems and set the culture to achieve the right outcomes.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“While you will want to help everyone, however you have to focus your energy on building others who can take your place long term. Not everyone has that ability so you have to spend focused time on those that you see demonstrating the potential to take over from you and they can in turn help others that show less potential at the moment and enable them.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;10. Transition from involving others in the work to making the work theirs. Make it an opportunity for someone to do the work that got you to where you are. Support and set them up for success. Don’t worry, there’s a never-ending backlog of important problems and interesting opportunities to work on for customers.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“I tell folks that they should spend 1-2 hours a week with someone to scale them to achieve 40 hours of work under your insights. This is what the true Principal scale is of being able to find those small things that enable a Sr SDE to be even better.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;11. When you give others the work, it is now theirs. You can provide context and guidance, but ultimately, the direction is theirs to set. This includes letting them take an approach you wouldn’t take. If it goes poorly, we all get to learn from it; if it goes well, you get to learn something from it. Nonetheless, you should step in if the project is walking through a high-risk, one-way door that could backfire.&lt;/p&gt;
    &lt;p&gt;12. Create space for others in meetings. Sometimes, the room looks to the most senior person for their opinion or decision; you can create space for others by asking questions instead. Also, if you see someone who’s not participating, gently pull them in on topics that play to their strengths. And if the meeting forgot someone who should have been in the discussion, add them to the next occurrence.&lt;/p&gt;
    &lt;p&gt;13. You don’t always have to demonstrate value. Some of the most effective principals I’ve worked with go through an entire meeting silent, or barely leave comments during document reviews. If the team and discussion are going fine as it is, that’s great! It means you can take a step back from the workstream and focus your energies elsewhere.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Be careful on the other side of this: If you’re present and staying silent, you have some implicit approval. Beware of multi-tasking and what your presence means.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;14. In meetings with execs, it’s okay not to address every topic on the agenda. If they’re engaged in the topic, ask meaningful questions, and make the decisions or unblock the obstacles that only they can, that’s a good meeting.&lt;/p&gt;
    &lt;p&gt;15. Beware: If you work in a breadth role, your entire week can be filled with everything that comes at you. This includes reviews, escalations, emails, help needed, etc. This can become a bigger issue the longer you stay at an org, where you become the “go-to” person because you know so much or have earned that credibility. As a result, you’re now a “mandatory” attendee at every meeting. Learn to push back and guard your time, or else you’ll have no time to push for the ideas you really care about. You don’t need to be in every meeting or have an opinion on every idea, just the key ones that matter.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“One thing that a mentor shared with me is that you need time to think. If you are going from meeting to meeting, you can’t process things and can’t look ahead. You need to schedule quality thinking time and disconnect from meetings to really find that next big thing.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“You need to look to delegate—set someone else up to be that person so that you can free up your time. This gives you benefits in freeing you up, but also helps put aside scope for someone new to help them grow.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;16. If you can’t explain why what you’re working on needs a principal, you might be working on the wrong thing. (This may also apply to L6s.)&lt;/p&gt;
    &lt;p&gt;17. Because of your position, you can sometimes improve outcomes with relatively low effort. The title grants you organizational privilege, and thus greater access to relationships and context. Combined with your experience, this allows you to see around corners better. Thus, you can meaningfully improve a project’s chances of success and outcomes by investing fairly little effort. This is high ROI, and when you spot such opportunities, act on them.&lt;/p&gt;
    &lt;p&gt;18. The title comes with an aura of credibility even when you shouldn’t have it. Sometimes, people read more into your offhand comments than they should, especially if they don’t know you well. As a result, they may do a lot of work because of a casual comment you made. This can be a waste of time and effort. Thus, be clear on what you do know, what you don’t know, what you’re asking for, and what you’re simply commenting on.&lt;/p&gt;
    &lt;p&gt;19. Don’t just say the “what”; also share the “why” you think so. This helps others make better decisions. It also reduces the chance that others say “Principal &amp;lt;NAME&amp;gt; said this and thus we should …” and parrot things you said without fully understanding why you said it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“The kinds of problems that require L7 input usually involve decisions under significant uncertainty. What’s crucial—but also difficult—is articulating your mental model: How you arrive at a judgment without having all the information, and why certain pieces of knowledge are more important than other data points.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;20. Find mechanisms to stay in touch with teams. This can be design reviews, weekly demos, sitting nearby and keeping an ear out, team lunches, or casual hallway chats. This helps you keep a pulse on the org and where the key problems or opportunities are.&lt;/p&gt;
    &lt;p&gt;21. Help teams keep sight of the bigger picture. When the working level is focused on the thick of things and day-to-day delivery, they can sometimes lose sight of the bigger, longer-term problems/opportunities and get stuck in the local optima. With the context that you have, you can help remind teams of this.&lt;/p&gt;
    &lt;p&gt;22. Be pragmatic; balance seeing the big picture with accepting local solutions. Consult and listen to the working level on the details; they’re your on-the-ground experts.&lt;/p&gt;
    &lt;p&gt;23. You’ll be asked for reviews or promo feedback on someone that you’ve only spent an hour or two with. It’s okay to decline instead of providing poor-quality feedback that’s based on a tiny sample of their entire behavior.&lt;/p&gt;
    &lt;p&gt;24. Make time to interact with interns and their mentors. A few touch points during the internship can be transformative, including an early check-in (and course correction if necessary) and being there for demo day. Also, work with the mentors and interns towards deliverables that continue to be valuable beyond the internship, where others can extend the project. This includes product 1-pagers, working software, and technical documentation.&lt;/p&gt;
    &lt;p&gt;25. To get to principal, you need to put yourself on the critical path. To be effective as a principal and go beyond it, you need to actively remove yourself from it. While you were previously the “go-to” person, you want to transition from essential to adjacent. The org should increasingly benefit from you, but shouldn’t be dependent on you to be effective. Think about how you can empower others to make the contributions you’re making.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Be careful about injecting yourself in critical path projects. Your focus is a lot more prone to being stolen by some other priority, so you need to keep yourself out of the critical path or be really stringent about locking down if you are in the critical path.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;26. If you were promoted to principal, it’s because you’ve been acting as a principal for a while. Typically, for more than a year. Thus, don’t worry about the increased expectation of the title. Just keep doing what you’re doing, engage with other principals, figure out your style, and work with your leadership to identify your focus areas.&lt;/p&gt;
    &lt;p&gt;27. With great freedom comes great responsibility. You have the autonomy to choose what to work on, but there’s the expectation of accountability and impact. The freedom isn’t about doing what you want, but ownership of finding the highest leverage problems to solve. Don’t expect to be told what to do or be given any guidance. You’re expected to figure out what the org should focus on.&lt;/p&gt;
    &lt;p&gt;28. Define and align your charter with your leadership. One way is to split your work into three buckets: (i) owner, (ii) sponsor, (iii) consultant. As a consultant, you’re involved in reviews and provide guidance, and have a high-level understanding of the system’s or product’s intent. As a sponsor, in addition to the above, you make the idea a priority for the org, work to build alignment and drive decisions, and engage with stakeholders. As an owner, it’s everything above, plus being the system expert and first point of contact, and having borderline obsession with the success of the design, execution, and impact. I tend to own 1 - 2 projects (&amp;gt; 50% time), sponsor 2 - 3 projects (~20% time), and consult the rest of my time.&lt;/p&gt;
    &lt;p&gt;29. Being a principal can be lonely. You’re part of all teams but also part of none. Build a network of peers with whom you can have open conversations. It likely doesn’t matter if you’re working in the same company or domain.&lt;/p&gt;
    &lt;p&gt;30. Don’t neglect your own needs. Make time and space for projects that support your learning, growth, and wellbeing. While it can feel selfish in the short term, it’s far more preferable to you burning out on the org. If you’re actively looking for work that keeps you healthy and happy and growing, your org benefits too, and it’s easier for them to retain you. Work with your manager on how to balance this.&lt;/p&gt;
    &lt;p&gt;31. Keep learning; our industry moves fast. If you take on projects that teach you nothing, or at least nothing relevant to your work, you’re moving backwards. This is sometimes inevitable—timebox such projects when they come along. Also, your learning doesn’t have to solely come from the job. I know PEs who find time to read papers and technical textbooks, and hack on prototypes over weekends to better understand new ideas and technology.&lt;/p&gt;
    &lt;p&gt;What other advice have you come across on how to be an effective principal engineer or scientist? Please share in the comments below or DM me! 🙏&lt;/p&gt;
    &lt;p&gt;Thanks to Brian K, Tim L, Yiwen O, Prannoy C, Aman A, Dennis T, and others for reading an early draft and providing feedback. And thanks to my mentor and role model, David S.&lt;/p&gt;
    &lt;p&gt;If you found this useful, please cite this write-up as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Yan, Ziyou. (Oct 2025). Advice for New Principal Tech ICs (i.e., Notes to Myself). eugeneyan.com. https://eugeneyan.com/writing/principal/.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;@article{yan2025principal,
  title   = {Advice for New Principal Tech ICs (i.e., Notes to Myself)},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2025},
  month   = {Oct},
  url     = {https://eugeneyan.com/writing/principal/}
}&lt;/code&gt;
    &lt;p&gt;Join 11,800+ readers getting updates on machine learning, RecSys, LLMs, and engineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eugeneyan.com/writing/principal/"/><published>2025-10-25T02:24:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45700946</id><title>Key IOCs for Pegasus and Predator Spyware Removed with iOS 26 Update</title><updated>2025-10-25T14:08:17.205049+00:00</updated><content>&lt;doc fingerprint="f60d4907cab1114d"&gt;
  &lt;main&gt;
    &lt;p&gt;Blog&lt;/p&gt;
    &lt;head rend="h1"&gt;Key IOCs for Pegasus and Predator Spyware Cleaned With iOS 26 Update&lt;/head&gt;
    &lt;p&gt;By Matthias Frielingsdorf, VP of Research&lt;/p&gt;
    &lt;p&gt;Oct 21, 2025&lt;/p&gt;
    &lt;p&gt;As iOS 26 is being rolled out, our team noticed a particular change in how the operating system handles the shutdown.log file: it effectively erases crucial evidence of Pegasus and Predator spyware infections. This development poses a serious challenge for forensic investigators and individuals seeking to determine if their devices have been compromised at a time when spyware attacks are becoming more common.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;The Power of the shutdown.log&lt;/head&gt;
    &lt;p&gt;For years, the shutdown.log file has been an invaluable, yet often overlooked, artifact in the detection of iOS malware. Located within the Sysdiagnoses in the Unified Logs section (specifically, Sysdiagnose Folder -&amp;gt; system_logs.logarchive -&amp;gt; Extra -&amp;gt; shutdown.log), it has served as a silent witness to the activities occurring on an iOS device, even during its shutdown sequence.&lt;/p&gt;
    &lt;p&gt;In 2021, the publicly known version of Pegasus spyware was found to leave discernible traces within this shutdown.log. These traces provided a critical indicator of compromise, allowing security researchers to identify infected devices. However, the developers behind Pegasus, NSO Group, are constantly refining their techniques, and by 2022 Pegasus had evolved.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Pegasus's Evolving Evasion Tactics&lt;/head&gt;
    &lt;p&gt;While still leaving evidence in the shutdown.log, their methods became more sophisticated. Instead of leaving obvious entries, they began to completely wipe the shutdown.log file. Yet, even with this attempted erasure, their own processes still left behind subtle traces. This meant that even a seemingly clean shutdown.log that began with evidence of a Pegasus sample was, in itself, an indicator of compromise. Multiple cases of this behavior were observed until the end of 2022, highlighting the continuous adaptation of these malicious actors.&lt;/p&gt;
    &lt;p&gt;Following this period, it is believed that Pegasus developers implemented even more robust wiping mechanisms, likely monitoring device shutdown to ensure a thorough eradication of their presence from the shutdown.log. Researchers have noted instances where devices known to be active had their shutdown.log cleared, alongside other IOCs for Pegasus infections. This led to the conclusion that a cleared shutdown.log could serve as a good heuristic for identifying suspicious devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Predator's Similar Footprint&lt;/head&gt;
    &lt;p&gt;The sophisticated Predator spyware, observed in 2023, also appears to have learned from the past. Given that Predator was actively monitoring the shutdown.log, and considering the similar behavior seen in earlier Pegasus samples, it is highly probable that Predator, too, left traces within this critical log file.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;iOS 26: An Unintended Cleanse&lt;/head&gt;
    &lt;p&gt;With iOS 26 Apple introduced a changeâeither an intentional design decision or an unforeseen bugâthat causes the shutdown.log to be overwritten on every device reboot instead of appended with a new entry every time, preserving each as its own snapshot. This means that any user who updates to iOS 26 and subsequently restarts their device will inadvertently erase all evidence of older Pegasus and Predator detections that might have been present in their shutdown.log.&lt;/p&gt;
    &lt;p&gt;This automatic overwriting, while potentially intended for system hygiene or performance, effectively sanitizes the very forensic artifact that has been instrumental in identifying these sophisticated threats. It could hardly come at a worse time - spyware attacks have been a constant in the news and recent headlines show that high-power executives and celebrities, not just civil society, are being targeted.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Identifying Pegasus 2022: A Specific IOC&lt;/head&gt;
    &lt;p&gt;For those still on iOS versions prior to 26, a specific IOC for Pegasus 2022 infections involved the presence of a /private/var/db/com.apple.xpc.roleaccountd.staging/com.apple.WebKit.Networking entry within the shutdown.log. This particular IOC also revealed a significant shift in NSO Group's tactics: they began using normal system process names instead of easily identifiable, similarly named processes, making detection more challenging.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Correlating Logs for Deeper Insight (&amp;lt; iOS 18)&lt;/head&gt;
    &lt;p&gt;For devices running iOS 18 or earlier, a more comprehensive approach to detection involved correlating containermanagerd log entries with shutdown.log events. Containermanagerd logs contain boot events and can retain data for several weeks. By comparing these boot events with shutdown.log entries, investigators could identify discrepancies. For example, if numerous boot events were observed before shutdown.log entries, it suggested that something was amiss and potentially being hidden.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Before You Update&lt;/head&gt;
    &lt;p&gt;Given the implications of iOS 26's shutdown.log handling, it is crucial for users to take proactive steps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Before updating to iOS 26, immediately take and save a sysdiagnose of your device. This will preserve your current shutdown.log and any potential evidence it may contain.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Consider holding off on updating to iOS 26 until Apple addresses this issue, ideally by releasing a bug fix that prevents the overwriting of the shutdown.log on boot.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;More Blogs&lt;/head&gt;
    &lt;head rend="h3"&gt;Get Our Latest Blog Posts Delivered Straight to Your Inbox&lt;/head&gt;
    &lt;p&gt;Subscribe to our blog to receive the latest research and industry trends delivered straight to your inbox. Our blog content covers sophisticated mobile threats, unpatched vulnerabilities, smishing, and the latest industry news to keep you informed and secure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://iverify.io/blog/key-iocs-for-pegasus-and-predator-spyware-cleaned-with-ios-26-update"/><published>2025-10-25T02:31:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45701305</id><title>Meet the real screen addicts: the elderly</title><updated>2025-10-25T14:08:17.036818+00:00</updated><content/><link href="https://www.economist.com/international/2025/10/23/meet-the-real-screen-addicts-the-elderly"/><published>2025-10-25T04:09:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45701607</id><title>Fast TypeScript (Code Complexity) Analyzer</title><updated>2025-10-25T14:08:16.836394+00:00</updated><content>&lt;doc fingerprint="30e7fd6d7122189e"&gt;
  &lt;main&gt;
    &lt;p&gt;Fast TypeScript Analyzer&lt;/p&gt;
    &lt;p&gt;FTA (Fast TypeScript Analyzer) is a super-fast TypeScript static analysis tool written in Rust. It captures static information about TypeScript code and generates easy-to-understand analytics that tell you about complexity and maintainability issues that you may want to address.&lt;/p&gt;
    &lt;p&gt;FTA uses swc (opens in a new tab) to parse your code then runs various analytical routines against it to understand how complex and maintainable it is likely to be. JavaScript code is also supported.&lt;/p&gt;
    &lt;p&gt;FTA is fast: on typical hardware, it can analyze up to 1600 files per second.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quickstart&lt;/head&gt;
    &lt;p&gt;There are several ways to use &lt;code&gt;fta&lt;/code&gt;. The simplest is to use &lt;code&gt;fta-cli&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;npx fta-cli path/to/project&lt;/code&gt;
    &lt;p&gt;Example output against the Redux project:&lt;/p&gt;
    &lt;code&gt;┌─────────────────────────────────────────┬────────────┬─────────────────────────────┬───────────────────┐
│ File                                    ┆ Num. lines ┆ FTA Score (Lower is better) ┆ Assessment        │
╞═════════════════════════════════════════╪════════════╪═════════════════════════════╪═══════════════════╡
│ website\src\pages\index.js              ┆ 212        ┆ 64.43                       ┆ Needs improvement │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\createStore.ts                      ┆ 255        ┆ 64.17                       ┆ Needs improvement │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\combineReducers.ts                  ┆ 162        ┆ 59.51                       ┆ Could be better   │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\compose.ts                          ┆ 36         ┆ 47.53                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\bindActionCreators.ts               ┆ 51         ┆ 47.14                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\kindOf.ts                     ┆ 58         ┆ 46.88                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\isPlainObject.ts              ┆ 8          ┆ 28.36                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\symbol-observable.ts          ┆ 7          ┆ 27.61                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\warning.ts                    ┆ 8          ┆ 26.81                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ website\docusaurus.config.js            ┆ 205        ┆ 18.19                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ website\sidebars.js                     ┆ 148        ┆ 15.82                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ rollup.config.js                        ┆ 71         ┆ 15.79                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ tsup.config.ts                          ┆ 63         ┆ 15.59                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\types\store.ts                      ┆ 63         ┆ 15.47                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\applyMiddleware.ts                  ┆ 55         ┆ 15.45                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ website\src\pages\errors.js             ┆ 58         ┆ 15.07                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\types\reducers.ts                   ┆ 49         ┆ 14.46                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ website\src\js\monokaiTheme.js          ┆ 62         ┆ 14.32                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\actionTypes.ts                ┆ 8          ┆ 11.91                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\index.ts                            ┆ 37         ┆ 11.91                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\types\actions.ts                    ┆ 15         ┆ 10.27                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\types\middleware.ts                 ┆ 14         ┆ 10.16                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ vitest.config.ts                        ┆ 14         ┆ 9.92                        ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ docs\components\DetailedExplanation.jsx ┆ 14         ┆ 9.53                        ┆ OK                │
└─────────────────────────────────────────┴────────────┴─────────────────────────────┴───────────────────┘
24 files analyzed in 0.0372s.&lt;/code&gt;
    &lt;p&gt;For convenience, FTA generates a single FTA Score that serves as a general, overall indication of the quality of a particular TypeScript file.&lt;/p&gt;
    &lt;p&gt;That said, all metrics are exposed and it is up to users to decide how it's metrics can enhance productivity for your team.&lt;/p&gt;
    &lt;p&gt;The full metrics available for each file:&lt;/p&gt;
    &lt;code&gt;{
  "file_name": "combineReducers.ts",
  "cyclo": 28,
  "halstead": {
    "uniq_operators": 28,
    "uniq_operands": 67,
    "total_operators": 271,
    "total_operands": 239,
    "program_length": 95,
    "vocabulary_size": 510,
    "volume": 854.4635765015915,
    "difficulty": 37.84518828451883,
    "effort": 32337.33493496609,
    "time": 1796.5186074981161,
    "bugs": 0.2848211921671972
  },
  "line_count": 202,
  "fta_score": 61.61052634575169,
  "assessment": "(Needs improvement)"
}&lt;/code&gt;
    &lt;p&gt;You can also see how FTA analyzes individual files by using the Playground.&lt;/p&gt;
    &lt;p&gt;View the full docs to see all the possible ways to use FTA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get Involved&lt;/head&gt;
    &lt;p&gt;FTA is completely open-source. Get involved by joining the discussion on the GitHub Repository (opens in a new tab).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ftaproject.dev/"/><published>2025-10-25T05:51:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45701825</id><title>Euro cops take down cybercrime network with 49M fake accounts</title><updated>2025-10-25T14:08:13.951472+00:00</updated><content>&lt;doc fingerprint="b16699d9694bf643"&gt;
  &lt;main&gt;
    &lt;p&gt;European police forces have arrested seven people and dismantled a large-scale cybercrime-as-a-service operation that saw almost 50 million fake online accounts created across social media and communications platforms for fraud purposes.&lt;/p&gt;
    &lt;p&gt;The coordinated takedown, codenamed Operation SIMCARTEL, took place on October 10 in Latvia, as part of a joint investigation by police in the Baltic nation, Austria, Estonia and Finland.&lt;/p&gt;
    &lt;p&gt;Five Latvian nationals and two additional suspects were arrrested by police.&lt;/p&gt;
    &lt;p&gt;In the raid, authorities seized 1200 SIM boxes, with the devices containing 40,000 active SIM cards.&lt;/p&gt;
    &lt;p&gt;Five Internet servers were taken down, along with two websites that had been offering the illegal service, gogetsms.com and apisim.com.&lt;/p&gt;
    &lt;p&gt;The network operated as a for-hire service, providing temporary telephone numbers from more than 80 countries to criminals who needed to mask their identities whilst committing cybercrimes.&lt;/p&gt;
    &lt;p&gt;Fraudsters used the service to bypass two-factor authentication systems so as to create vast numbers of fake accounts.&lt;/p&gt;
    &lt;p&gt;Once created, the bogus accounts served as starting points for various scams including investment fraud, fake online shops, and phishing attacks.&lt;/p&gt;
    &lt;p&gt;The infrastructure facilitated a range of offences including fraud, extortion, migrant smuggling and the distribution of child sexual abuse material.&lt;/p&gt;
    &lt;p&gt;Scammers employed tactics including the "daughter-son scam", where criminals persuaded victims that their child needed urgent financial help, alongside more traditional phishing and smishing attacks.&lt;/p&gt;
    &lt;p&gt;Some of the criminals specialised in fraud on second-hand marketplaces, whilst others set up fake investment websites and bogus online shops.&lt;/p&gt;
    &lt;p&gt;In other cases, criminals impersonated police officers using forged identification, personally collecting funds from victims.&lt;/p&gt;
    &lt;p&gt;Financial losses in Austria alone are said to be to around €4.5 million ($7.4 million), with an additional €420,000 ($693,000) lost in Latvia.&lt;/p&gt;
    &lt;p&gt;Around €431,000 ($711,000) was seized from the criminals' bank accounts, along with approximately $516,000 worth of crypto currency.&lt;/p&gt;
    &lt;p&gt;Police investigators working with Europol and Eurojust, attributed over 1700 individual cyber fraud cases in Austria and 1500 in Latvia to the criminal network.&lt;/p&gt;
    &lt;p&gt;Europol provided analytical support, open-source intelligence analysis for mapping the online criminal service, and forensic expertise to secure digital evidence.&lt;/p&gt;
    &lt;p&gt;The agency worked with the not-for-profit Shadowserver Foundation security organisation to dismantle the technical infrastructure.&lt;/p&gt;
    &lt;p&gt;In September this year, United States law enforcement busted another "SIM farm", with 300 devices and with over 100,000 subscriber identity cards found on the site near the United Nations headquarters in New York City.&lt;/p&gt;
    &lt;p&gt;The US Secret Service was involved in the raid in New York, and suspected nation-state threat actors were operating the SIM farm.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.itnews.com.au/news/euro-cops-take-down-cybercrime-network-with-49-million-fake-accounts-621174"/><published>2025-10-25T06:48:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45702363</id><title>Tell HN: OpenAI now requires ID verification and won't refund API credits</title><updated>2025-10-25T14:08:13.525365+00:00</updated><content>&lt;doc fingerprint="f5aca66714a6ef15"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Just frustrated here: I credited my OpenAI API account with credits, and then it turns out I have to go through some verification process to actually use the API, which involves disclosing personal data to some third-party vendor, which I am not prepared to do. So I asked for a refund and am told that that refunds are against their policy.&lt;/p&gt;
      &lt;p&gt;So I'll be cancelling my chatgpt plus sub, disputing the card payment, and moving to deepseek.&lt;/p&gt;
      &lt;p&gt;Edit: Deepseek seems to be a lot cheaper than OpenAI&lt;/p&gt;
      &lt;p&gt;Edit 2: seems verification is only needed for gpt-5, gpt-4o seems to work without it&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45702363"/><published>2025-10-25T09:02:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45702558</id><title>React vs. Backbone in 2025</title><updated>2025-10-25T14:08:13.337423+00:00</updated><content>&lt;doc fingerprint="a7f21d051da72e1d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;15 Years of Progress&lt;/head&gt;
    &lt;p&gt;Look at the two implementations above. The code is roughly the same length. They do exactly the same thing. One was written with a framework from 2010, the other with a framework that's had countless developer hours and a massive ecosystem behind it for over a decade.&lt;/p&gt;
    &lt;p&gt;The interesting part is not how much better React is—it's how little progress we've actually made.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Illusion of Simplicity&lt;/head&gt;
    &lt;p&gt;React looks cleaner. It reads better at first glance. But that readability comes at a cost: you're trading explicit simplicity for abstraction complexity.&lt;/p&gt;
    &lt;p&gt;The Backbone code is brutally honest about what it's doing. An event fires, a handler runs, you build some HTML, you put it in the DOM. It's verbose, sure, but there's no mystery. A junior developer can trace exactly what happens and when. The mental model is straightforward: "when this happens, do this."&lt;/p&gt;
    &lt;p&gt;The React code hides a lot. And once you move past simple examples, you hit problems that don't make sense until you understand React's internals.&lt;/p&gt;
    &lt;p&gt; Your input mysteriously clears itself. Turns out you switched a list item's key from a stable ID to an index, so React thinks it's a completely different component and remounts it, wiping state. Or maybe you forgot that &lt;code&gt;value&lt;/code&gt; can't be &lt;code&gt;undefined&lt;/code&gt;—React saw it flip from uncontrolled to controlled and reset the input.
        &lt;/p&gt;
    &lt;p&gt; You add a &lt;code&gt;useEffect&lt;/code&gt; to fetch data, and suddenly your app is stuck in an infinite loop. The dependency array includes an object that gets recreated every render, so React thinks it changed and runs the effect again. Now you need &lt;code&gt;useMemo&lt;/code&gt; and &lt;code&gt;useCallback&lt;/code&gt; sprinkled everywhere to "stabilize identities," which is a thing you never had to think about before.
        &lt;/p&gt;
    &lt;p&gt; Your click handler sees old state even though you just set it. That's a stale closure—the function captured the value from when it was created, and later renders don't magically update it. You either need to put the state in the dependency array (creating a new handler every time) or use functional updates like &lt;code&gt;setState(x =&amp;gt; x + 1)&lt;/code&gt;. Both solutions feel like workarounds.
        &lt;/p&gt;
    &lt;head rend="h3"&gt;Magic Has a High Price&lt;/head&gt;
    &lt;p&gt;These aren't edge cases. They're normal problems you hit building moderately complex apps. And debugging them requires understanding reconciliation algorithms, render phases, and how React's scheduler batches updates. Your code "just works" without you needing to understand why it works, which is nice until it breaks.&lt;/p&gt;
    &lt;p&gt;People say "you need to rebuild React from scratch to really understand it," and they're right. But that's kind of damning, isn't it? You shouldn't need to understand virtual DOM diffing, scheduling priorities, and concurrent rendering to build a password validator.&lt;/p&gt;
    &lt;p&gt;Backbone might be tedious, but it doesn't lie to you. jQuery is hackable. You can view source, understand it, and add to it easily. It's just DOM methods. React's abstraction layers make that much harder.&lt;/p&gt;
    &lt;head rend="h3"&gt;So, What's Next?&lt;/head&gt;
    &lt;p&gt;We understand the problem: event + state = UI. That's it. That's what both of these implementations are solving.&lt;/p&gt;
    &lt;p&gt;For massive apps with 1,000 components on the same page, maybe React's complexity is justified. But what the other 99% of apps? What about small apps that just want to do a job and don't need all the magic?&lt;/p&gt;
    &lt;p&gt;Is there a better model? Something feels as hard and steady as the DOM, but still feels intuitive to write? Something hackable like Backbone and jQuery were, where you can pop open devtools and understand what's happening?&lt;/p&gt;
    &lt;p&gt;— panphora&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://backbonenotbad.hyperclay.com/"/><published>2025-10-25T09:43:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45702877</id><title>The Great SaaS Gaslight</title><updated>2025-10-25T14:08:13.181422+00:00</updated><content>&lt;doc fingerprint="3504901e1f77a3dc"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pay as you go&lt;/item&gt;
      &lt;item&gt;Just pay for what you need&lt;/item&gt;
      &lt;item&gt;Free up time&lt;/item&gt;
      &lt;item&gt;Free up capital&lt;/item&gt;
      &lt;item&gt;Focus on your business not the technology&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But that’s not quite the way it worked out, is it?&lt;/p&gt;
    &lt;p&gt;Maybe they meant it when they said it, but it is not the driving force behind the great SaaS purveyors of our day. Yeah, I am looking at you Microsoft, you too Google, and where do you think you’re hiding Intuit? There is nothing wrong with creating a product that your customer wants to buy, but something is off when the customer is forced to buy something they don’t want. Customer needs are completely secondary to customer lock-in. Sadly, the SaaS model has become too big to care about customers. They spew customer satisfaction surveys at you after every interaction in an effort to show that they care. But these surveys are just another brick in the big data wall. The results are secondary. They kind of care what you think…but mostly they just need you to hang around and keep paying. They collect the data to guide incremental improvements around the edges.&lt;/p&gt;
    &lt;p&gt;The irony is that just about every SaaS vendor has created the role of customer success manager. These are people that are assigned to your account to help you onboard and have success with the product to keep you from off-boarding. Success doesn’t necessarily translate into helping your organization succeed, just that you ‘succeed’ enough with the product.&lt;/p&gt;
    &lt;p&gt;I don’t begrudge anyone’s success in creating a product that customers find useful and want to buy. That is not an easy thing to do. But after a while, the SaaS business model is not about customer success or satisfaction anymore. It is about customer submission and inertia. At a certain point, the customer base becomes too big and the product becomes too big to change.&lt;/p&gt;
    &lt;head rend="h3"&gt;Safety in Numbers&lt;/head&gt;
    &lt;p&gt;It is the path of least resistance. If everyone else is doing it, it must be good or at least good enough. Plus, there is value in the network effect. Yet, numbers are only safe up to a point. Numbers blind you to unseen risks. Black swan type risks. Rare, but catastrophic. They are so rare and potentially disastrous that no one really thinks about them.&lt;/p&gt;
    &lt;p&gt;Maybe your backup, disaster recovery, business continuity or whatever they are calling it these days will save you from a single system failing, but it won’t protect you from context and know-how loss. The real problem isn’t loss, it’s accumulation. Too many programs, too many APIs, too many integrations, too much complexity masquerading as sophisticated systems. Context recovery systems don’t exist, yet successful organizations rely on context, not data. Terabytes of data mean nothing without knowing why you have the data, what it means and how you need to make use of it. Maybe the software rules take care of it, but that is a dangerous thing to rely on.&lt;/p&gt;
    &lt;p&gt;Information and content is infinite and stochastic. It is not necessarily predictable. More information doesn’t lead to better decisions, it just leads to more data.&lt;/p&gt;
    &lt;p&gt;This preys on the fear and risk of not knowing.&lt;/p&gt;
    &lt;p&gt;You can never know all the information before you have to make a decision, but when faced with unknowns and uncertainty, adopting “best practices” provides a cozy security blanket.&lt;/p&gt;
    &lt;head rend="h3"&gt;Undifferentiated Best Practices&lt;/head&gt;
    &lt;p&gt;Gotta love the industry ‘best practice’ templates. About a million years ago, I remember an old printed newsletter called the ‘Best Practices Report’ which featured, you guessed it, best practices. The trouble with best practices — then and now — is that they pretend that the world has stopped changing. But the reality is, the world is not static. Things change. You need to keep getting better, you need to keep evolving. Blindly adopting templated best practices is not a path to be best (to paraphrase someone), but rather a path to bland mediocrity.&lt;/p&gt;
    &lt;p&gt;This preys on the ‘why reinvent the wheel’ logic. Why spend the time and effort on figuring out something that has already been solved.&lt;/p&gt;
    &lt;p&gt;The reality is that you will be really good at achieving parity with your competition.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bland and Generic Applications&lt;/head&gt;
    &lt;p&gt;Speaking of bland mediocrity, all you have to do is to look at the landscape of commercial software. There are thousands of applications across thousands of categories, but we are still getting different takes on note taking or calendar applications.&lt;/p&gt;
    &lt;p&gt;Some programs might look prettier or feel more intuitive, but they are tackling the same problems.&lt;/p&gt;
    &lt;p&gt;Software keeps iterating on solving the same soluble problems because the remaining challenges are really difficult to solve with technology. Communication and coordination are full of nuance and subtleties that defy digitization.&lt;/p&gt;
    &lt;p&gt;This is not unique to SaaS vendors, but most, if not all, software vendors have jumped on the SaaS bandwagon for marketing and selling their products. The free version that does just enough to lure you into the paying membership. Then you face the standard three options to subscribe with the good, better, best offers.&lt;/p&gt;
    &lt;p&gt;Adding communication tools hasn’t improved the quality of communication despite massively increasing the volume of communication.&lt;/p&gt;
    &lt;head rend="h3"&gt;Let’s Go to the Mall&lt;/head&gt;
    &lt;p&gt;SaaS has become the technology American shopping mall of the 1980s. It is overpriced and predictable. The goods are largely the same in every mall. This is not a dynamic market. It is very much a controlled market. The landlord sets up the platform and the retailers rush in to this great location to make the huge profits and get the advantages of scale. The retailers that can afford the mall’s rates, have very controlled experiences. The mall of the 1980s was not a place of bold experimentation and risk taking. The risk was signing the lease.&lt;/p&gt;
    &lt;p&gt;While Google and Microsoft are stores in the mall, they are also the landlords. They control the mall experience. Apple runs its own mall — just shinier, not different. (Today’s physical malls would be ghost towns without an Apple Store bringing in traffic.)&lt;/p&gt;
    &lt;p&gt;Somewhere along the line, the culture tired of the mall experience. Across the country, the US is full of abandoned malls. The model works up to a point and then the fashion changes.&lt;/p&gt;
    &lt;p&gt;The small store with the carefully curated merchandise appears on the scene and draws in its crowd.&lt;/p&gt;
    &lt;p&gt;The future is much the same for Information Technology. The point is not to have the same system that everyone else does. The point is to have the information system that works for you.&lt;/p&gt;
    &lt;p&gt;This issue of the newsletter was written on a self hosted WordPress site on multiple devices with no monthly fees.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://unworkableideas.com/the-great-saas-lighting-how-it-users-got-gaslit/"/><published>2025-10-25T11:05:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45703556</id><title>Making a micro Linux distro (2023)</title><updated>2025-10-25T14:08:12.850321+00:00</updated><content>&lt;doc fingerprint="579b191fd9a42fad"&gt;
  &lt;main&gt;
    &lt;p&gt;In this article, we’ll talk about building up a tiny (micro) Linux “distribution” from scratch. This distribution really won’t do much, but it will be built from scratch.&lt;/p&gt;
    &lt;p&gt;We will build the Linux kernel on our own, and write some software to package our micro-distro.&lt;/p&gt;
    &lt;p&gt;Lastly, we are doing this example on the RISC-V architecture, specifically QEMU’s &lt;code&gt;riscv64 virt&lt;/code&gt; machine. There’s very little in this article that is specific to this architecture, so you might as well do an almost identical exercise for other architectures like &lt;code&gt;x86&lt;/code&gt;. We recently went through the RISC-V boot process with SBI and bare metal programming for RISC-V, so this is just a continuation up the software stack.&lt;/p&gt;
    &lt;p&gt;Warning: This article is a very simplified view of a Linux distribution. There are things written below that are not 100% accurate, but more like 99.9%. This article is meant for beginners and helping them form a basic mental framework for understanding Linux systems. More advanced users may be triggered by over-simplification in some parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;What is an OS kernel?&lt;/head&gt;
    &lt;p&gt;Let’s assume we’re working on a single-core machine. They’re still around us, maybe not in our laptops and phones, but in some smaller devices, and historically they have been actually widely used even in our “big” personal devices like desktops. The latter ones have been capable of running multiple program simultaneously for many years, even as single cores. We’ll get into what simultaneous really means in a bit, but for now let’s just note that the one of the operating system kernel’s big tasks is to make that happen.&lt;/p&gt;
    &lt;p&gt;If you go back to the articles about bare metal programming and SBI on RISC-V, you can see how at the lowest layers of software we interact with our I/O devices. It usually (most often, but not necessarily always) boils down to the CPU writing some data at the appropriate address. Imagine if the application developers had to keep all these addresses in mind and they had to know which values exactly to send to those addresses! That would mean we’d have far fewer applications today, but we don’t, and that’s owing to the operating system kernels which abstract away these details and provide some simple high-level interfaces instead. In the RISC-V SBI article, we looked at an example of such interface for Linux on &lt;code&gt;x86&lt;/code&gt; — instead of knowing which addresses to write to and what values to send there, we focused on the logic and basically just told to the OS kernel that “we want message so and so written to the standard output”, and then the OS kernel dealt with the details of interacting with the hardware. So that’s another big task for the OS kernel: managing the hardware on the machine and making the interaction with it easier.&lt;/p&gt;
    &lt;p&gt;Going further, the OS kernel offers some really high-level programming interfaces like the filesystems. This may or may not be about managing some hardware and abstracting operations over it. For example, the most common case for the filesystems, of course, is to store some data on the disk and retrieve it later, and this has to do with the OS kernel managing the hardware related to disks on the machine (i.e. sending some data to certain addresses, which makes those hard disk devices respond in some way). However, this is not always the case, the files are not always some data stored on disk, and so filesystem is an interface exposed to us, meaning it’s a way of talking to the OS kernel, not necessarily a way to talk to the data. We’ll cover the filesystems in great detail in some other article, but let’s keep this in mind for now — the OS kernel needs to provide a straightforward way of doing high-level things through multiple interfaces.&lt;/p&gt;
    &lt;p&gt;Finally, the last thing I wanted to cover about the kernels is that they provide a programming model. Remember how we mentioned (as I’m sure you already know) that multiple programs can run even on a single-core device simultaneously? The OS enables the running applications to be programmed to not even know about each other, in other words, an application can live its lifecycle acting like it is the only application running on the computer and no one else is touching its memory. Imagine a world where your Python Django server needs to know about that texting app on your device in order to be working — we’d have far fewer Django apps and texting apps, for sure, as coding them would quickly get gnarly. However, the apps can also know about each other’s existence on the same machine. The operating system kernel facilitates both. It gives a programming model in which you can insulate applications from each other, or join a few apps in isolation from other apps, etc.&lt;/p&gt;
    &lt;p&gt;Basically, the OS kernel does a lot of heavy lifting to enable you to run your code easily on a very generic and complicated machinery such as your smartphone. What is written above probably doesn’t do full justice to the kernels, they do a whole lot of things, but the few paragraphs above should give a fairly good idea of kernel’s main tasks, and there are many.&lt;/p&gt;
    &lt;p&gt;Linux is an extremely popular operating system kernel. It can be built to run on many architectures (really, a lot), it is open source and free to use. And a lot of people are “Linux users”, but what does it exactly mean that someone “uses Linux”? Those Linux users typically install something like Debian, or Ubuntu on their machines, and they use Linux that way, and what does that mean?&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a Linux distribution?&lt;/head&gt;
    &lt;p&gt;We talked above about what kernels do, i.e. what are their tasks and we said Linux is an OS kernel, but can we really just take bare Linux and as end users who just want to watch YouTube, do something with it? The answer is likely no, we need a lot more layers on top of Linux to get to firing up a Chrome browser and watching YouTube.&lt;/p&gt;
    &lt;p&gt;How to go all the way towards the top of the software stack where we can just use those super simple and intuitive apps like graphical web browsers? We have previously discussed the boot process, and we went all the way from the very first operations on the machine after the power on, to the moment we land in the operating system kernel. We did not cover the bootloaders in any detail, we just briefly mentioned them because we were able to get QEMU to directly load our fake kernel into the memory in one go, which is typically not possible with full blown systems like desktop Linux (there is an intermediate booting stage where the bootloader fetches the OS image from something like a disk, or maybe even network and loads it into the memory). The kernel we wrote was a fake little stub that does effectively nothing, and so we ended our last article at the point where the OS kernel is in memory and ready to go, it’s just we had no kernel to run.&lt;/p&gt;
    &lt;p&gt;Based on what we see above, I think the right mental model for the kernel right now is that it is the infrastructure for running user applications on a complex machine, but it really doesn’t do anything for the user’s business logic. This is what I meant when I said the bare Linux on its own cannot fire up Chrome and let you watch YouTube — it is merely the infrastructure that the application developer uses to implement Chrome, and its streaming capabilities.&lt;/p&gt;
    &lt;p&gt;However, the kernel alone is not infrastructure for Chrome to run. We need to run sort of “infrastructure on top of infrastructure” to achieve the full infrastructure to run Chrome. Again, much like in the SBI article, we’re just layering abstractions on top of each other in some way, so essentially there is nothing new here, just the way we do it.&lt;/p&gt;
    &lt;p&gt;For example, in order for a machine to connect to the Internet, the OS kernel first needs to be able to drive the network device on the machine to send the signals out of the machine (to the switch, router, another machine or whatever it is connected to). However, in Linux, there is more or less where the kernel stops. Which networks you connect to, are you using VPN, how do you assign IPs to your machine (statically or dynamically) and that kind of business, it happens in the upper layers of the infrastructure.&lt;/p&gt;
    &lt;p&gt;You may now guess where this is going — a Linux distribution is really the Linux kernel plus the infrastructure on top of the kernel infrastructure. Let’s dig into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does “infrastructure on top of infrastructure” run?&lt;/head&gt;
    &lt;p&gt;Again, the kernel does a whole bunch of things, a million times more than what we can cover in a single article, but it definitely has its limits and it doesn’t do all the heavy lifting on your everyday personal device — and this is where something outside of the kernel gets into the picture.&lt;/p&gt;
    &lt;p&gt;Disclaimer: You can get really creative with Linux in a million different ways, and from this point on we’re going with a very basic, textbook-like, simple view of what happens in the mainstream distributions. There are many super complex things we can do, and there are lots of details we’re leaving out, but my hope here is that you get a general idea and enough knowledge to be able to understand more advanced material on this topic; there is plenty of it on the Internet.&lt;/p&gt;
    &lt;p&gt;The reason why I wrote the disclaimer above is mainly because we’re going to be assuming that your Linux has a filesystem going forward, as this is the most common path. How many times have you seen a Linux deployment without a filesystem? It certainly seems possible to do, but it may be borderline useless except for some super edge/advanced cases, and we’ll disregard them in this article. Check out this page to get more idea of what I’m talking about.&lt;/p&gt;
    &lt;p&gt;So what is the stuff outside the kernel? It’s what we call the user code! It’s just a normal code that runs within the Linux environment, just like you run basically anything on your Linux machine. Sure, some code is more privileged than the other, and there are a million more details that can get involved, but let’s just focus on the main distinction here: when you are running Linux on a machine, there is kernel code running, as well as the user code running, and everything that’s a part of the kernel itself is running in the kernel space, and everything that is running on the machine that is not a part of the kernel is running in the user space, and they are fairly isolated from each other.&lt;/p&gt;
    &lt;p&gt;So this “infrastructure on top of infrastructure” that we have talked about runs in the user space. Sure, it needs to bubble down to the kernel for many primitives, and we’ve seen already how that happens. Linux has a well defined ABI that exposes a set of services that the user space code can invoke in the kernel space. And where does this user space code come into the picture?&lt;/p&gt;
    &lt;head rend="h2"&gt;The &lt;code&gt;init&lt;/code&gt; process (and its “children”)&lt;/head&gt;
    &lt;p&gt;Once the kernel is done loading and making itself comfortable on the machine, it kicks off the first bit of the code in user space — the &lt;code&gt;init&lt;/code&gt; process. This is a piece of user space code that lives in a binary that sits somewhere on your filesystem, and the kernel will look for it in a few locations, beginning with &lt;code&gt;/init&lt;/code&gt; (if it doesn’t find it there, it will give a few more shots at different locations before throwing its hands up). Let’s say the kernel found a binary in the filesystem at &lt;code&gt;/init&lt;/code&gt; — it’s going to start it and assign the ID &lt;code&gt;1&lt;/code&gt;. This is basically the only user process that the kernel will start: the &lt;code&gt;init&lt;/code&gt; process then is the ancestor of all other user space processes. This means that &lt;code&gt;init&lt;/code&gt; will start some other processes, these other processes will in turn start some other processes, and so on. Very shortly you have a bunch of processes running on your machine, hopefully each one of them useful for the desired operations on the machine. The machine should at this point start actively interacting with the world around it: whether we’re talking about a smartphone giving the UI to its user, an embedded device that collects data off the sensors and sending it into the cloud, etc. Additionally, the machine will often have various tools available that are not actively running on the machine, but can be invoked in certain situations for some high level operations (e.g. a Python script can invoke a couple of tools like &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;cat&lt;/code&gt; or something to get a snapshot of what’s going on with the machine and then sending the data somewhere). Quick note is that even these periodically-started or ad-hoc tools are in some way descendants of &lt;code&gt;init&lt;/code&gt;; it’s not too important to know now, but it’s good to keep in mind.&lt;/p&gt;
    &lt;p&gt;The collection of kernel, the processes that get launched right after the kernel, and the tools that are available at your disposal represent the Linux distribution. It’s essentially a packaging for the kernel alongside all these useful tools that do more around the machine than what the kernel alone does (but it still provides the infrastructure for everything outside of the kernel to run, nothing bypasses the kernel).&lt;/p&gt;
    &lt;p&gt;Even a distrbution minimally useful for everyday use can get crufty pretty quickly. If you go onto the path of building your own custom little distro, as we actually will now, you will almost inevitably hit a lot of roadblocks where something that you expect to be working is just not working and the full solution is either to code some of your own software to talk to the kernel to get something done on the system, or just use an off-the-shelf software to do so. The latter is the path of least resistance, and you’ll likely keep adding stuff until you end up with a deployment that can do something remotely useful for you. At this point, you will have likely accumulated a significant number of software packages.&lt;/p&gt;
    &lt;p&gt;On the other hand, you have probably heard people criticizing certain distributions as being “bloated”, probably meaning they accumulated so much complexity in their packaging, they waste a lot of hardware resources doing things that are not useful, etc. Without discipline, I can easily see distrbution developers just randomly throwing different tools at the system just to get that one missing thing going, without retroactively cleaning up the excess later and just moving onto the next feature where they do the same — a (sadly) common pattern in software engineering.&lt;/p&gt;
    &lt;p&gt;Some distributions draw the line at different places where they just make a decision for the user and do something on the system, versus letting the user make the full decision and be more hands on. For example, you can install Arch Linux in a minimal way where it’s just a little more than the kernel booted up with a shell. All the subsequent decisions are on you, and you have to be very hands on in order to get it to a point where it’s very graphical and highly interactive. Or you can decide it’s just not worth your time setting it up so much, and just install a very user-friendly Ubuntu distrbution, which may be “bloated” for someone’s taste, but it gets you up and running very fast (I personally like it).&lt;/p&gt;
    &lt;head rend="h2"&gt;Building our almost useless Linux micro distrbibution&lt;/head&gt;
    &lt;p&gt;Let’s get our hands dirty and build something that’s basically useless but we’ll actually end up booting it for real. You may want to refresh your memory on the RISC-V boot process, I think it will be rewarding here.&lt;/p&gt;
    &lt;p&gt;First things first, let’s build the kernel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building a Linux operating system for RISC-V&lt;/head&gt;
    &lt;p&gt;I’m on an &lt;code&gt;x86&lt;/code&gt; platform here, so I will depend heavily on the cross-platform toolchain to build things for RISC-V. You will likely do something similar (I’m not sure I have yet seen someone build the RISC-V kernel on RISC-V itself).&lt;/p&gt;
    &lt;p&gt;Let’s get the source code for Linux. Linux development is done on top of the Git version control system, but we’ll take a shortcut here and just download a tarball with the sources for one branch, we won’t be syncing the whole Linux codebase with all the Git branches, experimental stuff and so on. We’ll be downloading the tarball from &lt;code&gt;kernel.org&lt;/code&gt; for version &lt;code&gt;6.5.2&lt;/code&gt; (here). You can also just download any tarball for whatever the latest stable version is from kernel.org homepage. Once it’s downloaded, go ahead and unpack that. Let’s also &lt;code&gt;cd&lt;/code&gt; into that directory.&lt;/p&gt;
    &lt;p&gt;Now is the time to configure the build. The first step is to make the &lt;code&gt;defconfig&lt;/code&gt; which basically initiates your configuration file.&lt;/p&gt;
    &lt;p&gt;Note: Here and below, you may want to use a different &lt;code&gt;CROSS_COMPILE&lt;/code&gt; prefix, depending on how the cross compilation tool is identified on your machine&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- defconfig&lt;/code&gt;
    &lt;p&gt;This was hopefully quick and the &lt;code&gt;.config&lt;/code&gt; file should be generated. The config file should contain a lot of IDs for individual configurations and the values for those, very often in yes/no format (e.g. &lt;code&gt;CONFIG_FOO=y&lt;/code&gt; or &lt;code&gt;CONFIG_FOO=n&lt;/code&gt;). You could edit the file manually, but I personally wouldn’t recommend it, especially as a beginner (I don’t consider myself an expert at this either). A better way to edit this is through the &lt;code&gt;curses&lt;/code&gt;-based pseudo-interface. You can get there by running&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- menuconfig&lt;/code&gt;
    &lt;p&gt;This interface has a few benefits.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You have a more readable, folder-like overview of the configs.&lt;/item&gt;
      &lt;item&gt;There are insights into dependencies between the configs, i.e. it may only make sense to be able to enable config &lt;code&gt;foo&lt;/code&gt;if&lt;code&gt;bar&lt;/code&gt;and&lt;code&gt;baz&lt;/code&gt;are also enabled.&lt;/item&gt;
      &lt;item&gt;This interface has a search feature, activated by pressing the &lt;code&gt;/&lt;/code&gt;button (I don’t think you’ll get far by searching there in natural language; my way of getting around here is by searching on Google and finding which exactly config key am I looking for, for example&lt;code&gt;CONFIG_TTY_PRINTK&lt;/code&gt;). When you find what you’re looking for, hit the button you see in the parentheses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We won’t be tweaking anything here for now, let’s just exit and move on.&lt;/p&gt;
    &lt;p&gt;It’s time to build the kernel! Quick note here, the make process famously has the &lt;code&gt;-j&lt;/code&gt; flag, which basically sets the concurrency in the build process, meaning it allows the build process to run a few things simultaneously. If you want to build faster, but not sure what to do, count the number of cores, and if it’s something like 8, just pass the flag &lt;code&gt;-j8&lt;/code&gt; below, as so. I will run the command like this (I’m on a 16-core machine):&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- -j16&lt;/code&gt;
    &lt;p&gt;This can take some time, though for the RISC-V build, it shouldn’t take awfully long, but I would expect at least a few minutes.&lt;/p&gt;
    &lt;p&gt;Once this is done, you will probably see something like this near the very bottom:&lt;/p&gt;
    &lt;code&gt;OBJCOPY arch/riscv/boot/Image&lt;/code&gt;
    &lt;p&gt;and this is the file we will be feeding to QEMU.&lt;/p&gt;
    &lt;p&gt;Great, let’s fire up QEMU!&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image&lt;/code&gt;
    &lt;p&gt;Switching to the UART view, we see that OpenSBI tidily started and the Linux took over! Great! We even see some references to the SBI layer that we have discussed before:&lt;/p&gt;
    &lt;code&gt;[    0.000000] Linux version 6.5.2 (uros@uros-debian-desktop) (riscv64-linux-gnu-gcc (Debian 10.2.1-6) 10.2.1 20210110, GNU ld (GNU Binutils for Debian) 2.35.2) #1 SMP Mon Sep 11 00:45:40 PDT 2023
[    0.000000] Machine model: riscv-virtio,qemu
[    0.000000] SBI specification v0.2 detected
[    0.000000] SBI implementation ID=0x1 Version=0x8
[    0.000000] SBI TIME extension detected
[    0.000000] SBI IPI extension detected
[    0.000000] SBI RFENCE extension detected&lt;/code&gt;
    &lt;p&gt;After reading about the boot process, we should now have a full understanding of what is going on here. This happened super early in the boot phase. There is a lot happening in these logs, and I’ll highlight a few things:&lt;/p&gt;
    &lt;code&gt;[    0.000000] riscv: base ISA extensions acdfim&lt;/code&gt;
    &lt;p&gt;Seems like Linux is capable of dynamically figuring out the capability of the underlying RISC-V hardware. I’m not sure what exactly is the mechanism behind it, could it be somehow passed through the device tree that we mentioned in the previous article, or something in the ISA itself tells this to the kernel, I’m not sure.&lt;/p&gt;
    &lt;code&gt;[    0.000000] Kernel command line:&lt;/code&gt;
    &lt;p&gt;This is interesting, a kernel has a command line? Turns out that the kernel, much like your everyday binaries, has startup flags. The kernel bootloader usually sets those up — after all, it knows how to fire up the kernel, and this could simply be a part of the starting process. With QEMU, remember, we’re sort of short circuiting the whole bootloader thing, and with passing the &lt;code&gt;-kernel&lt;/code&gt; flag, we let QEMU also wear the bootloader hat here by loading the kernel image into the memory and starting it up. QEMU actually has a flag called &lt;code&gt;-append&lt;/code&gt; with which you can append to this kernel command line. The command line itself is baked into the config file under &lt;code&gt;Boot options&lt;/code&gt; somewhere, I leave it to the reader to search for it, and the QEMU flag basically lets you adjust it with a VM launch, instead of having to rebuild the kernel to tweak the command line. In this case, the command line is just blank by default.&lt;/p&gt;
    &lt;code&gt;[    0.003376] printk: console [tty0] enabled&lt;/code&gt;
    &lt;p&gt;I guess this means that &lt;code&gt;printk&lt;/code&gt; will now write to &lt;code&gt;tty0&lt;/code&gt;? &lt;code&gt;printk&lt;/code&gt; is basically a way to write out messages from the kernel space. Remember, your typical &lt;code&gt;printf&lt;/code&gt; from C’s &lt;code&gt;stdio.h&lt;/code&gt; is meant for running in the user space, not kernel space, so kernel space must have its own solution, and it is &lt;code&gt;printk&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[    0.211634] Serial: 8250/16550 driver, 4 ports, IRQ sharing disabled
[    0.221544] 10000000.uart: ttyS0 at MMIO 0x10000000 (irq = 12, base_baud = 230400) is a 16550A
[    0.222659] printk: console [ttyS0] enabled&lt;/code&gt;
    &lt;p&gt;Great, Linux knows there is UART at &lt;code&gt;0x10000000&lt;/code&gt;, just like we established before. Linux can now choose whether to use the SBI interface to drive the UART, or talk to it directly (if the S-mode allows it on that machine, that is). On many platforms, the OS can disregard that a lower level software like BIOS may offer to interact with the hardware, and from what I hear, this actually indeed happens a lot.&lt;/p&gt;
    &lt;p&gt;There’s also a lot of other stuff in the kernel logs:&lt;/p&gt;
    &lt;code&gt;[    0.250030] SuperH (H)SCI(F) driver initialized&lt;/code&gt;
    &lt;p&gt;I don’t think we need this? I guess we can go back to the kernel config and not bake this driver into the kernel and thus slim the kernel down. What we’re building here is a generic build, really. We didn’t customize anything and presumably the authors of the default config thought this is a reasonable default that should just run on a lot of different setups, so they probably included a lot of things to be on the safe side. If you’re working on smaller hardware, with less generous memory, CPU, etc. you do have to carefully choose what gets baked into the kernel and what doesn’t.&lt;/p&gt;
    &lt;p&gt;Additionally, this generic build is smart enough to figure out that the console should go to the right UART device, which is really handy for us. Otherwise, we’d probably have to do a bunch of configs like making sure TTY (let’s not overfocus on what this is now) is enabled, we want to enable printing to UART as the kernel boots, etc. All this is basically configurable in the &lt;code&gt;menuconfig&lt;/code&gt; interface.&lt;/p&gt;
    &lt;p&gt;We’ll keep it simple in this article, and we won’t customize anything in the kernel unless we have to.&lt;/p&gt;
    &lt;head rend="h4"&gt;First obstacles&lt;/head&gt;
    &lt;p&gt;Scrolling down closer to the bottom of the output, we see this:&lt;/p&gt;
    &lt;code&gt;[    0.330411] /dev/root: Can't open blockdev
[    0.330743] VFS: Cannot open root device "" or unknown-block(0,0): error -6
[    0.330984] Please append a correct "root=" boot option; here are the available partitions:
[    0.331648] List of all bdev filesystems:
[    0.331785]  ext3
[    0.331803]  ext2
[    0.331882]  ext4
[    0.331950]  vfat
[    0.332028]  msdos
[    0.332098]  iso9660
[    0.332181]
[    0.332405] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0)
[    0.332756] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.5.2 #1
[    0.333018] Hardware name: riscv-virtio,qemu (DT)
[    0.333248] Call Trace:
[    0.333442] [&amp;lt;ffffffff8000537a&amp;gt;] dump_backtrace+0x1c/0x24
[    0.333940] [&amp;lt;ffffffff808890f8&amp;gt;] show_stack+0x2c/0x38
[    0.334138] [&amp;lt;ffffffff80894a48&amp;gt;] dump_stack_lvl+0x3c/0x54
[    0.334318] [&amp;lt;ffffffff80894a74&amp;gt;] dump_stack+0x14/0x1c
[    0.334493] [&amp;lt;ffffffff80889500&amp;gt;] panic+0x102/0x29e
[    0.334683] [&amp;lt;ffffffff80a015c6&amp;gt;] mount_root_generic+0x1e8/0x29c
[    0.334891] [&amp;lt;ffffffff80a0186c&amp;gt;] mount_root+0x1f2/0x224
[    0.335108] [&amp;lt;ffffffff80a01a68&amp;gt;] prepare_namespace+0x1ca/0x222
[    0.335320] [&amp;lt;ffffffff80a010c8&amp;gt;] kernel_init_freeable+0x23e/0x262
[    0.335539] [&amp;lt;ffffffff80896264&amp;gt;] kernel_init+0x1e/0x10a
[    0.335714] [&amp;lt;ffffffff800034c2&amp;gt;] ret_from_fork+0xa/0x1c
[    0.336208] ---[ end Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ]---&lt;/code&gt;
    &lt;p&gt;Whoops, we crashed! The kernel has fallen into a panic.&lt;/p&gt;
    &lt;p&gt;Remember how we talked that pretty much always Linux needs a filesystem to be useful and how all the “infrastructure on top of infrastructure” is in the user space? Well, we didn’t really pass anything related to the filesystem explicitly and we surely didn’t pass any user space code to serve as the &lt;code&gt;init&lt;/code&gt;, though we didn’t even get to the latter.&lt;/p&gt;
    &lt;p&gt;You might imagine that the filesystem needs to be on a disk, but that’s not necessarily the case. We’ll talk some other time about filesystems in great detail, but you can really have a filesystem be backed by RAM memory too. And this is actually very often used by Linux, most notably in the boot up phase. When the kernel gets to where it crashed for us just now, in a normal, typical situation, it will find the whole, fully functional filesystem actually loaded into the RAM. If this confuses you, just think about it this way — a disk is just a bunch of bytes, just like RAM is, though RAM is faster but much smaller; conceptually they’re basically the same. Who and how loads this memory?&lt;/p&gt;
    &lt;p&gt;One way is to bake the filesystem directly into the kernel image. In this case, as the kernel loads, so does the initial, memory-backed filesystem, and our system would be ready to go if we had done that. If you don’t want to bulk up your kernel image and you want your initial filesystem to be loaded by some other means, like through a bootloader or something, then you package it separately. In QEMU case, we can shortcircuit things a little bit again, and make it wear a few more hats — we’ll make it also load the initial filesystem into the memory as well. If you’re interested in building the filesystem into the kernel, read the discussion here and try it as an exercise after you’re done with this guide.&lt;/p&gt;
    &lt;p&gt;This initial filesystem has a name: &lt;code&gt;initramfs&lt;/code&gt;. You’ll often hear it called &lt;code&gt;initrd&lt;/code&gt; too (I imagine &lt;code&gt;rd&lt;/code&gt; is short for ramdisk?). The latter is how QEMU takes in the filesystem for loading (&lt;code&gt;-initrd&lt;/code&gt; flag).&lt;/p&gt;
    &lt;p&gt;The filesystem is packaged as a &lt;code&gt;cpio&lt;/code&gt; archive, which is conceptually similar to &lt;code&gt;tar&lt;/code&gt;, but it’s not the same binary format. Short discussion can be read here.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building the &lt;code&gt;initramfs&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The only real requirement for the &lt;code&gt;initramfs&lt;/code&gt; from the kernel is that it has a binary it can start up as the &lt;code&gt;init&lt;/code&gt; process, and the first place where the kernel will look for it is at the filesystem root, so the path is &lt;code&gt;/init&lt;/code&gt;. If you have absolutely nothing else on your filesystem, it’s questionably useful, but this is the bare requirement. Let’s start by writing the &lt;code&gt;init&lt;/code&gt; process in C. This process can be really anything, Linux won’t stop you from writing a useless &lt;code&gt;init&lt;/code&gt;, it will happily just execute it. We can go with a ‘hello world’ then?&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main(int argc, char *argv[]) {
  printf("Hello world\n");
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Great, now let’s package it up into a &lt;code&gt;cpio&lt;/code&gt; archive.&lt;/p&gt;
    &lt;code&gt;riscv64-linux-gnu-gcc -static -o init init.c
cpio -o -H newc &amp;lt; file_list.txt &amp;gt; initramfs.cpio&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;file_list.txt&lt;/code&gt; has a single line:&lt;/p&gt;
    &lt;code&gt;init&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We’re building a static binary because we do not want to dynamically depend on the standard C library. The filesystem won’t have it, we’re making a filesystem with &lt;code&gt;init&lt;/code&gt;alone.&lt;/item&gt;
      &lt;item&gt;Linux expects the &lt;code&gt;initramfs&lt;/code&gt;archive to be built with the&lt;code&gt;-H newc&lt;/code&gt;flag.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s run QEMU.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /PATH/TO/NEWLY_BUILT/initramfs.cpio&lt;/code&gt;
    &lt;p&gt;The kernel stil falls into a panic, but a different one!&lt;/p&gt;
    &lt;code&gt;[    0.351894] Run /init as init process
Hello world
[    0.379006] Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000000
[    0.379360] CPU: 0 PID: 1 Comm: init Not tainted 6.5.2 #1
[    0.379597] Hardware name: riscv-virtio,qemu (DT)
[    0.379812] Call Trace:
[    0.380005] [&amp;lt;ffffffff8000537a&amp;gt;] dump_backtrace+0x1c/0x24
[    0.380724] [&amp;lt;ffffffff808890f8&amp;gt;] show_stack+0x2c/0x38
[    0.380906] [&amp;lt;ffffffff80894a48&amp;gt;] dump_stack_lvl+0x3c/0x54
[    0.381095] [&amp;lt;ffffffff80894a74&amp;gt;] dump_stack+0x14/0x1c
[    0.381283] [&amp;lt;ffffffff80889500&amp;gt;] panic+0x102/0x29e
[    0.381447] [&amp;lt;ffffffff80013fd0&amp;gt;] do_exit+0x760/0x766
[    0.381623] [&amp;lt;ffffffff80014154&amp;gt;] do_group_exit+0x24/0x70
[    0.381806] [&amp;lt;ffffffff800141b8&amp;gt;] __wake_up_parent+0x0/0x20
[    0.382009] [&amp;lt;ffffffff80895482&amp;gt;] do_trap_ecall_u+0xe6/0xfa
[    0.382218] [&amp;lt;ffffffff8000337c&amp;gt;] ret_from_exception+0x0/0x64
[    0.382808] ---[ end Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000000 ]---&lt;/code&gt;
    &lt;p&gt;I guess this just means &lt;code&gt;init&lt;/code&gt; shouldn’t finish, so it should be easy to fix? Let’s just make it print something every 10 seconds and never stop. Important to note: our output worked, we see a “Hello world” string!&lt;/p&gt;
    &lt;p&gt;We’ll write a new &lt;code&gt;init&lt;/code&gt;, but let’s also make our &lt;code&gt;initramfs&lt;/code&gt; a little more complex too. Let’s remember how we said that &lt;code&gt;init&lt;/code&gt; starts up all the other processes on the machine. Wouldn’t it be nice if we actually had some sort of a shell? After all, that’s what we typically have with Linux — shells go well with Linux. We’ll build a useless shell, the one that just tells us what we asked it to do (echoes back the input).&lt;/p&gt;
    &lt;p&gt;Let’s first write the &lt;code&gt;init&lt;/code&gt; process. Before it begins looping and printing something every 10 seconds, it has an important job of spawning our “little shell”. The way a process can spawn another process in Linux is through 2 operations: &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;exec&lt;/code&gt;. &lt;code&gt;fork&lt;/code&gt; will start a new process by literally cloning the current process at the moment of &lt;code&gt;fork&lt;/code&gt;. The way the underlying code can differentiate the “parent” and “child” processes after that is by checking the return value of the &lt;code&gt;fork&lt;/code&gt; operation. If it is 0, this means the process is the child process, and it’s a parent otherwise (-1 is returned in an error case).&lt;/p&gt;
    &lt;p&gt;Next, it’s not useful for us here to just keep executing the &lt;code&gt;init&lt;/code&gt; program in 2 different processes. That’s where one of the many &lt;code&gt;exec&lt;/code&gt; operations come into the picture. When I say there are many &lt;code&gt;exec&lt;/code&gt; operations available on Linux, I mean there are &lt;code&gt;execl&lt;/code&gt;, &lt;code&gt;execlp&lt;/code&gt;, &lt;code&gt;execle&lt;/code&gt;, etc. Take a look at more documentation here, please. We’re going with &lt;code&gt;execl&lt;/code&gt; here, and the first parameter is which binary do we want to launch. We’ll package our fake shell as the &lt;code&gt;little_shell&lt;/code&gt; binary on the root. The rest of the parameters do not really matter (as evidenced by the value of the second parameter). More important, the mechanism of this operation is that we’re calling into the kernel to take whatever is running in the current process and replace it with the program that is loaded for execution from the binary listed as the first parameter. This is how programs get launched on Linux and when you’re working in your Bash shell, and you end up launching a program, this is what happens — a sequence of &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;exec&lt;/code&gt;-style calls.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main(int argc, char *argv[]) {
  pid_t pid = fork();

  if (pid == -1) {
    printf("Unable to fork!");
    return -1;
  }

  if (pid == 0) {
    // This is a child process.
    int status = execl("/little_shell", "irrelevant", NULL);

    if (status == -1) {
      printf("Forked process cannot start the little_shell");
      return -2;
    }
  }

  int count = 1;

  while (1) {
    printf("Hello from the original init! %d\n", count);
    count++;
    sleep(10);
  }

  return 0;
}&lt;/code&gt;
    &lt;p&gt;We build the &lt;code&gt;init&lt;/code&gt; the same way as we did before:&lt;/p&gt;
    &lt;code&gt;riscv64-linux-gnu-gcc -static -o init init.c&lt;/code&gt;
    &lt;p&gt;For the “shell” we’re building, I want to get a little more creative. Why don’t we write this one in Go instead of old school C?&lt;/p&gt;
    &lt;code&gt;package main

import (
	"bufio"
	"fmt"
	"os"
)

func main() {
	fmt.Println("Hello world from Go!")

	reader := bufio.NewReader(os.Stdin)

	for {
		fmt.Print("Enter your command: ")
		line, _ := reader.ReadString('\n')
		fmt.Printf("Your command is: %s", line)
	}
}&lt;/code&gt;
    &lt;p&gt;I am able to cross compile this to RISC-V out-of-the-box with my &lt;code&gt;go&lt;/code&gt; compiler.&lt;/p&gt;
    &lt;code&gt;GOOS=linux GOARCH=riscv64 go build little_shell.go&lt;/code&gt;
    &lt;p&gt;Nice thing that I really like about Go is that it’s very easy to reference other remote repositories on GitHub to include libraries, and things get neatly packaged up statically. I’m not going to lie, the &lt;code&gt;little_shell&lt;/code&gt; Go binary is pretty thick, weighing in at 1.9M on my machine, compared to only 454K for the statically-linked simple init, but in the days of desktops/laptops/phones with hundreds of GB of storage, if you’re building a distro for these kinds of devices, you may want to consider the tradeoff.&lt;/p&gt;
    &lt;p&gt;Note, there are situations where you may not be able to simply run your Go binary just like that on top of a bare kernel, it could start throwing Go panics all over the place. In order to run Go, you need to build your kernel with the right features in it, futex support feature being one of them (I think I’ve identified only 2 in my past experience). If you encounter any problems running the Go applications and you suspect you may not have the right kernel support, carefully read through the panics and you will be able to identify what is missing. Good news here is that the default config for the RISC-V kernel is good enough for running Go.&lt;/p&gt;
    &lt;p&gt;Let’s update our &lt;code&gt;file_list.txt&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;init
little_shell&lt;/code&gt;
    &lt;p&gt;Pack it all up again:&lt;/p&gt;
    &lt;code&gt;cpio -o -H newc &amp;lt; file_list.txt &amp;gt; initramfs.cpio&lt;/code&gt;
    &lt;p&gt;Let’s run it!&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /PATH/TO/NEWLY_BUILT/initramfs.cpio&lt;/code&gt;
    &lt;code&gt;[    0.356314] Run /init as init process
Hello from the original init! 1
Hello world from Go!
Enter your command: [[[mkdir hello]]]
Your command is: mkdir hello
Enter your command: [[[ls]]]
Your command is: ls
Enter your command: Hello from the original init! 2
[[[echo 123]]
Your command is: echo 123
Enter your command: [[[exit]]]
Your command is: exit
Enter your command: Hello from the original init! 3
[[[I give up!]]]
Your command is: I give up!&lt;/code&gt;
    &lt;p&gt;The bits in this console excerpt enclosed with triple square brackets are my user-provided input over UART. You can see 3 things interleaved on the UART&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Original &lt;code&gt;init&lt;/code&gt;’s period output every 10 seconds.&lt;/item&gt;
      &lt;item&gt;Output from the &lt;code&gt;little_shell&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Input from the user.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are using the sole UART device on the virtual machine for all this, but that is not the only reason why everything is mixed up here. &lt;code&gt;init&lt;/code&gt; process prints to the standard output, just like &lt;code&gt;little_shell&lt;/code&gt; does, and you may not be aware of it, but any sort of print on Linux is a print to an open file. Standard output, as far as Linux knows, is a file that is opened by a process and you are printing to the standard output by writing to that file. When we &lt;code&gt;fork&lt;/code&gt;-ed the &lt;code&gt;little_shell&lt;/code&gt; from &lt;code&gt;init&lt;/code&gt;, the &lt;code&gt;little_shell&lt;/code&gt; inherited the open files from &lt;code&gt;init&lt;/code&gt;. So they are literally sharing all the standard input and output streams. Even if we had multiple I/O devices that we used on this machine, they’d still be sending outputs over to the same output stream. When &lt;code&gt;init&lt;/code&gt; was started, its standard output was set to produce content over to UART, and this behavior was simply inherited by the &lt;code&gt;little_shell&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And there we have it, we have a pretty useless, but home-made Linux distribution! Go ahead and send it over to your friends! :)&lt;/p&gt;
    &lt;p&gt;Jokes aside, you can make an exercise out of this and implement some sort of a mini shell out of this &lt;code&gt;little_shell&lt;/code&gt;. Instead of just echoing back the commands given to it, you could make it actually understand what &lt;code&gt;mkdir&lt;/code&gt; is. You can even have it fork off a process to execute that elsewhere. Sky is the limit, you’re in the Linux userspace!&lt;/p&gt;
    &lt;p&gt;Let’s just step back a little and see if Linux kernel achieved the initial few promises for us:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;It’s abstracting away the hardware. Our&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;and our shell didn’t know anything about the UART. All they knew was they’re writing to some Linux file handle. It happens to be mapped to something abstract in the Linux kernel that invokes the UART driver in the Linux kernel, which may or may not use the SBI under the hood (I have honestly not verified if the kernel removes its dependence on SBI after it boots).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;It offers some high-level programming paradigms, like filesystems. Our&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;process located the other binary through the filesystem (the path was trivial, the binary was right in the root, but still, the paradigm is there).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There is a pretty clean isolation between the processes running. Once the shell was forked off from the&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;, the processes were basically running independently. The memory was not shared between them and they didn’t have to worry about each other’s memory layout. They did share something else, though, like the file handles, but this is a consequence of how they were launched into running. Linux enables you to actually change some of this behavior, e.g. you can set up some shared memory between the processes, if you explicitly want to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are many other things the kernel does for us, but let’s just stop here for now and appreciate this. It may not look like a lot, but the kernel gives us a pretty solid, portable infrastructure with which we can develop high level software while often disregarding the complexities of the underlying machine.&lt;/p&gt;
    &lt;head rend="h3"&gt;So what is an operating system?&lt;/head&gt;
    &lt;p&gt;This is now a game of words in my opinion. In my view, what matters is that the reader now has an understanding of what Linux as the kernel is, what “infrastructure” it offers, and what is running in the user space and what is running in the kernel space.&lt;/p&gt;
    &lt;p&gt;Some people may call the kernel itself an operating system, some people will refer to the whole distribution as the operating system, or they may come up with something completely different. I hope that at this point you have a good understanding of what is happening on a machine once Linux is started and where the responsibilities of each component end (or you can at least imagine the boundaries on a more complex system).&lt;/p&gt;
    &lt;p&gt;I hope this was useful!&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus section: making an actually useful micro distribution with &lt;code&gt;u-root&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;I thought about wrapping up here, but it wouldn’t make for a flashy demo. Why don’t we instead boot into something that’s actually useful, meaning that you can do things you would typically do on a Linux-based system, like run your &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;mkdir&lt;/code&gt;, &lt;code&gt;echo&lt;/code&gt; and whatnot. Let’s stick with the kernel we have previously built, and add some useful “infrastructure on top of infrastructure” in the user space domain to make the whole machine more useful.&lt;/p&gt;
    &lt;p&gt;I really like the u-root project for this.&lt;/p&gt;
    &lt;p&gt;Note: The title of their project mentions Go bootloaders, and this may stump you because as a careful reader, you know that Go programs are not really something you can run on bare metal. These bootloaders are somewhat exotic userspace bootloaders, meaning that they will actually run on top of a live Linux kernel, and then use this amazing Linux mechanism called &lt;code&gt;kexec&lt;/code&gt; to re-load a different kernel into the memory from user space. We won’t be using these bootloaders for now, we’ll just focus on the other user space goodies they have available, but I thought a quick paragraph here would help the confused readers.&lt;/p&gt;
    &lt;p&gt;The reason why I like the &lt;code&gt;u-root&lt;/code&gt; project is because it’s so insanely easy to use. Its usage is a bit creative though, so there are really 2 steps here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install &lt;code&gt;u-root&lt;/code&gt;per their instructions. You should end up with a&lt;code&gt;u-root&lt;/code&gt;binary in your&lt;code&gt;PATH&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Now to actually generate a functional &lt;code&gt;initramfs&lt;/code&gt;with&lt;code&gt;u-root&lt;/code&gt;, the easiest way is to clone their Git repo and&lt;code&gt;cd&lt;/code&gt;your way into the directory that you just cloned. From there, you can cross-compile a fully functional user space set of tools with a single command.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/u-root/u-root.git
cd u-root
GOOS=linux GOARCH=riscv64 u-root&lt;/code&gt;
    &lt;p&gt;I get a few lines of output, the last being:&lt;/p&gt;
    &lt;code&gt;18:31:31 Successfully built "/tmp/initramfs.linux_riscv64.cpio" (size 14827284).&lt;/code&gt;
    &lt;p&gt;And that’s really it, this &lt;code&gt;cpio&lt;/code&gt; file can now be just ran with QEMU and you’ll boot right into a shell! Go through the &lt;code&gt;u-root&lt;/code&gt; documentation to understand how you can customize this &lt;code&gt;initramfs&lt;/code&gt; image you get, including what sort of changes you can make to the &lt;code&gt;init&lt;/code&gt; process behavior, but I think the default setup is so amazing to explore with.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /tmp/initramfs.linux_riscv64.cpio&lt;/code&gt;
    &lt;p&gt;Wow, this booted really smoothly! Providing the bottom of the UART output.&lt;/p&gt;
    &lt;code&gt;[    0.400269] Run /init as init process
2023/09/12 01:34:33 Welcome to u-root!
                              _
   _   _      _ __ ___   ___ | |_
  | | | |____| '__/ _ \ / _ \| __|
  | |_| |____| | | (_) | (_) | |_
   \__,_|    |_|  \___/ \___/ \__|
&lt;/code&gt;
    &lt;p&gt;And as you can see by the little &lt;code&gt;/#&lt;/code&gt; prompt, you’re actually in a shell! &lt;code&gt;u-root&lt;/code&gt;’s &lt;code&gt;init&lt;/code&gt; forked off a shell process and gave it the control over the UART.&lt;/p&gt;
    &lt;code&gt;/# ls
bbin
bin
buildbin
dev
env
etc
go
init
lib
lib64
proc
root
sys
tcz
tmp
ubin
usr
var
/# pwd
/
/# echo "Hello world!"
Hello world!&lt;/code&gt;
    &lt;p&gt;This little shell that &lt;code&gt;u-root&lt;/code&gt; gives even supports Tab-completion! I will say I have encountered some hiccups occassionally with it, it’s definitely not your full blown Bash, but it’s more than just a toy.&lt;/p&gt;
    &lt;p&gt;The standard tools like &lt;code&gt;ls&lt;/code&gt; seem to be taking the standard flags:&lt;/p&gt;
    &lt;code&gt;/# ls -lah
dtrwxrwxrwx root 0 420 B  Sep 12 01:35 .
drwxr-xr-x  root 0 2.1 kB Jan  1 00:00 bbin
drwxr-xr-x  root 0 80 B   Jan  1 00:00 bin
drwxrwxrwx  root 0 40 B   Sep 12 01:34 buildbin
drwxr-xr-x  root 0 12 kB  Sep 12 01:34 dev
drwxr-xr-x  root 0 40 B   Sep 12 01:35 directory
drwxr-xr-x  root 0 40 B   Jan  1 00:00 env
drwxr-xr-x  root 0 80 B   Sep 12 01:34 etc
drwxrwxrwx  root 0 60 B   Sep 12 01:34 go
Lrwxrwxrwx  root 0 9 B    Jan  1 00:00 init -&amp;gt; bbin/init
drwxrwxrwx  root 0 40 B   Sep 12 01:34 lib
drwxr-xr-x  root 0 40 B   Jan  1 00:00 lib64
dr-xr-xr-x  root 0 0 B    Sep 12 01:34 proc
drwx------  root 0 40 B   Sep 11 07:43 root
dr-xr-xr-x  root 0 0 B    Sep 12 01:34 sys
drwxr-xr-x  root 0 40 B   Jan  1 00:00 tcz
dtrwxrwxrwx root 0 60 B   Sep 12 01:34 tmp
drwxr-xr-x  root 0 40 B   Jan  1 00:00 ubin
drwxr-xr-x  root 0 60 B   Jan  1 00:00 usr
drwxr-xr-x  root 0 60 B   Jan  1 00:00 var&lt;/code&gt;
    &lt;head rend="h3"&gt;Visit google.com from this!&lt;/head&gt;
    &lt;p&gt;One last flashy thing — let’s connect to google.com from this VM with our custom user-land!&lt;/p&gt;
    &lt;p&gt;First, we need to attach a network device. We add &lt;code&gt;-device virtio-net-device,netdev=usernet -netdev user,id=usernet,hostfwd=tcp::10000-:22&lt;/code&gt; to our QEMU CLI. I think the last 2 numbers do not really matter as we won’t be SSH’ing into this machine (maybe you can do that exercise yourself, but I’m afraid it won’t be easy). The default kernel build should indeed bake in the &lt;code&gt;virtio&lt;/code&gt; network device drivers, so this should more or less just work.&lt;/p&gt;
    &lt;p&gt;We’ll need a working IP address, and we’ll use something from &lt;code&gt;u-root&lt;/code&gt; to obtain it. That something requires 3 things present in the kernel config: &lt;code&gt;CONFIG_VIRTIO_PCI&lt;/code&gt;, &lt;code&gt;CONFIG_HW_RANDOM_VIRTIO&lt;/code&gt; and &lt;code&gt;CONFIG_CRYPTO_DEV_VIRTIO&lt;/code&gt;. My default settings for the kernel have all that flipped to &lt;code&gt;y&lt;/code&gt;, so I’m good to go and you should be too, but you can double check just in case. If you have changed any kernel settings, please rebuild the kernel image.&lt;/p&gt;
    &lt;p&gt;Finally, we need to attach an RNG (doesn’t matter what it is) device to our QEMU machine so we can obtain our IP address. We simply add &lt;code&gt;-device virtio-rng-pci&lt;/code&gt; to our QEMU CLI.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /tmp/initramfs.linux_riscv64.cpio -device virtio-net-device,netdev=usernet -netdev user,id=usernet,hostfwd=tcp::10000-:22 -device virtio-rng-pci&lt;/code&gt;
    &lt;p&gt;Once we’re in, we can run &lt;code&gt;ip addr&lt;/code&gt; to see what’s our IP address.&lt;/p&gt;
    &lt;code&gt;/# ip addr
1: lo: &amp;lt;UP,LOOPBACK&amp;gt; mtu 65536 state UNKNOWN
    link/loopback
    inet 127.0.0.1 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 state DOWN
    link/ether 52:54:00:12:34:56
3: sit0: &amp;lt;0&amp;gt; mtu 1480 state DOWN
    link/sit&lt;/code&gt;
    &lt;p&gt;Our Ethernet is not set up. Let’s enable IPv4 networking (we don’t need 6). In this little setup, QEMU is running a virtualized network and it embeds a little DHCP server which can dynamically assign IPs (documentation is here). So let’s run a DHCP helper from &lt;code&gt;u-root&lt;/code&gt; for this by running&lt;/p&gt;
    &lt;code&gt;dhclient -ipv6=false&lt;/code&gt;
    &lt;p&gt;The output I got was the following:&lt;/p&gt;
    &lt;code&gt;2023/09/12 03:46:59 Bringing up interface eth0...
2023/09/12 03:47:00 Attempting to get DHCPv4 lease on eth0
2023/09/12 03:47:00 Got DHCPv4 lease on eth0: DHCPv4 Message
  opcode: BootReply
  hwtype: Ethernet
  hopcount: 0
  transaction ID: 0x05f008e1
  num seconds: 0
  flags: Unicast (0x00)
  client IP: 0.0.0.0
  your IP: 10.0.2.15
  server IP: 10.0.2.2
  gateway IP: 0.0.0.0
  client MAC: 52:54:00:12:34:56
  server hostname:
  bootfile name:
  options:
    Subnet Mask: ffffff00
    Router: 10.0.2.2
    Domain Name Server: 10.0.2.3
    IP Addresses Lease Time: 24h0m0s
    DHCP Message Type: ACK
    Server Identifier: 10.0.2.2
2023/09/12 03:47:00 Configured eth0 with IPv4 DHCP Lease IP 10.0.2.15/24
2023/09/12 03:47:00 Finished trying to configure all interfaces.&lt;/code&gt;
    &lt;p&gt;The QEMU documentation will tell you why pinging won’t work, so let’s not bother with pinging. Let’s just “visit” google.com!&lt;/p&gt;
    &lt;code&gt;wget http://google.com&lt;/code&gt;
    &lt;p&gt;You can now read the downloaded &lt;code&gt;index.html&lt;/code&gt; file!&lt;/p&gt;
    &lt;code&gt;cat index.html&lt;/code&gt;
    &lt;p&gt;You’ll get a lot of obfuscated JavaScript, but this is great! It means we have successfully visited google.com through &lt;code&gt;wget&lt;/code&gt;! I hope this sparks your imagination to do some other cool things with &lt;code&gt;u-root&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Package managers&lt;/head&gt;
    &lt;p&gt;You might intuitively understand at this point that some of the most important software of a Linux distro is the package manager. It’s really the gateway to getting the functionality on your machine that you need. What we went through here is more of an embedded flow: we generated these somewhat monolithic software images and if we want to update something, we rebuild the whole image and re-image the device. This doesn’t work for desktops, phones, etc. Package managers are there to update, add or remove the software on our machines. We won’t be talking about them here, just giving them a brief shoutout and you can hopefully imagine from the high level how they work and what do they do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The monster of &lt;code&gt;init&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;init&lt;/code&gt; we created is definitely just a toy, and in the end it just started some sort of a shell. However, make no mistake about it, &lt;code&gt;init&lt;/code&gt; is an incredibly important thing on a Linux system and getting it right is a science. You’ll see a lot of strong opinions on different &lt;code&gt;init&lt;/code&gt; systems for Linux online. &lt;code&gt;init&lt;/code&gt; doesn’t usually just spawn one process off and call it a day, it can set up a whole bunch of things like different devices, for example. As an exercise, just run &lt;code&gt;ls /dev&lt;/code&gt; from your &lt;code&gt;u-root&lt;/code&gt;-based build and see all those devices set up. A lot of them come from the &lt;code&gt;init&lt;/code&gt;’s setup and many are extremely useful. You can then read some of the &lt;code&gt;u-root&lt;/code&gt; source code to see what’s going on there in &lt;code&gt;init&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The code for this guide is available here, where you can just sync and build the &lt;code&gt;initramfs&lt;/code&gt; images.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://popovicu.com/posts/making-a-micro-linux-distro/"/><published>2025-10-25T13:01:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45703716</id><title>Synadia and TigerBeetle Commit $512k USD to the Zig Software Foundation</title><updated>2025-10-25T14:08:12.561953+00:00</updated><content>&lt;doc fingerprint="2d5ad9ff893f0b97"&gt;
  &lt;main&gt;
    &lt;p&gt;Synadia and TigerBeetle have together pledged a combined $512,000 USD to the Zig Software Foundation (ZSF) over the next two years, demonstrating a shared belief in Zig’s potential to shape the next era of high-performance, reliable, and maintainable software.&lt;/p&gt;
    &lt;p&gt;At Synadia, we help some of the world’s largest enterprises design and scale innovative architectures across cloud regions, cloud providers, and all the way to the far edge. Often described as a “decentralized nervous system,” we enable secure, reliable communication between services and data, no matter the environment or topology - so our customers can deliver incredible digital products and experiences.&lt;/p&gt;
    &lt;p&gt;From the beginning, our mission has been bold yet simple: to connect everything. Built on top of NATS.io, our platform enables organizations to modernize, digitize, and extend their systems all the way to the edge. Synadia’s technology enables organizations to build and scale microservices, streaming and telemetry platforms, and event sourcing systems, while leveraging modern data primitives such as key-value and object stores anywhere within those systems.&lt;/p&gt;
    &lt;p&gt;Our customers span industries from financial services and e-commerce to gaming, manufacturing, industrial IoT, connected and autonomous vehicles, energy systems, and embodied AI. They continually challenge us to push the limits of what’s possible - delivering secure, real-time communications and data movement anywhere with minimal overhead.&lt;/p&gt;
    &lt;p&gt;Beyond the language itself, the most impressive aspect of Zig has been the quality of projects built with it: TigerBeetle, Bun, Ghostty, and others. Among these, TigerBeetle stands out.&lt;/p&gt;
    &lt;p&gt;I first met Joran Dirk Greef, TigerBeetle’s founder, at the first Distributed Systems Conference in Cape Town, which they hosted. Since then, I’ve had the chance to see how Joran and his team approach engineering for their financial database product, guided by their philosophy called “TigerStyle.” It focuses on correctness, clarity, and reliability — values that deeply resonate with us at Synadia and our customers.&lt;/p&gt;
    &lt;p&gt;Our goals align closely. Synadia is building industrial-grade solutions designed for smaller, more efficient, and deterministic deployments. Like TigerBeetle, we believe software should be predictable, simple, and trustworthy by design.&lt;/p&gt;
    &lt;p&gt;When Joran and I met again in New York recently, he told me TigerBeetle would be increasing its support for the Zig Foundation and asked whether Synadia would like to join forces. I didn’t have to think twice. There’s no better company or person to partner with in supporting Zig.&lt;/p&gt;
    &lt;p&gt;We are grateful to Andrew Kelley, the founder and president of the Zig Software Foundation, whose leadership continues to inspire developers building serious systems software. Andrew’s cohesive and focused vision mirrors how we operate at Synadia.&lt;/p&gt;
    &lt;p&gt;We’re proud to support Andrew, Loris Cro, and the entire Zig community as they continue to advance the state of systems programming.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Zig’s approach to control, performance, and simplicity is redefining what’s possible in modern systems software. We’re honored to contribute alongside TigerBeetle to help the Zig Foundation continue this vital work.” — Derek Collison, Founder &amp;amp; CEO, Synadia&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“We’re delighted to stand with Synadia in supporting Zig’s growth. Together, we believe Zig will play a foundational role in the next generation of reliable distributed systems.” — Joran Dirk Greef, Founder &amp;amp; CEO, TigerBeetle&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Zig Software Foundation (ZSF) is a non-profit organization dedicated to supporting the development of Zig, a programming language designed for performance, reliability, and maintainability. Led by Andrew Kelley, Zig empowers developers to write robust software without hidden control flow or unpredictable behavior and is increasingly being adopted for systems, embedded, and high-performance applications.&lt;/p&gt;
    &lt;p&gt;Synadia Communications, Inc. is the creator of the Synadia Platform and the maintainer of the NATS.io ecosystem. Synadia provides secure, zero-trust messaging and connectivity across cloud, edge, and on-premises environments. Its technology powers mission-critical systems for leading enterprises in finance, manufacturing, automotive, energy, and AI.&lt;/p&gt;
    &lt;p&gt;TigerBeetle is the financial transactions database designed for mission-critical safety and performance to power the next thirty years of transaction processing.&lt;/p&gt;
    &lt;p&gt;News and content from across the community&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.synadia.com/blog/synadia-tigerbeetle-zig-foundation-pledge"/><published>2025-10-25T13:24:14+00:00</published></entry></feed>