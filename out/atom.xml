<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-21T17:10:10.619296+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45653330</id><title>Pasta/80 is a simple Pascal cross compiler targeting the Z80 microprocessor</title><updated>2025-10-21T17:11:25.052555+00:00</updated><content>&lt;doc fingerprint="a6f5f818a2971b8d"&gt;
  &lt;main&gt;
    &lt;p&gt;PASTA/80 is a simple Pascal cross compiler targeting the Z80 microprocessor. It generates code for these classic and modern machines:&lt;/p&gt;
    &lt;p&gt;The compiler follows the single-pass recursive-descent approach championed by Niklaus Wirth, inventor of Pascal, in his books and lectures. It doesn't have an explicit syntax tree, but instead generates code on the fly during parsing. As a result, the compiler might not always generate the most efficient code possible (it definitely cannot compete with LLVM and doesn't try to), but it's very fast.&lt;/p&gt;
    &lt;p&gt;The supported Pascal dialect is an almost exact clone of the original Turbo Pascal 3.0 for CP/M (see this manual for details). So you have at your disposal the following language elements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All the basic data types (&lt;code&gt;Boolean&lt;/code&gt;,&lt;code&gt;Byte&lt;/code&gt;,&lt;code&gt;Char&lt;/code&gt;,&lt;code&gt;Integer&lt;/code&gt;,&lt;code&gt;Pointer&lt;/code&gt;,&lt;code&gt;Real&lt;/code&gt;and&lt;code&gt;String&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;array of&lt;/code&gt;,&lt;code&gt;record&lt;/code&gt;,&lt;code&gt;set of&lt;/code&gt;, enumerations, subranges and pointers as a way of building new data types.&lt;/item&gt;
      &lt;item&gt;The decision-making elements &lt;code&gt;if..then..else&lt;/code&gt;and&lt;code&gt;case..of&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The loop elements &lt;code&gt;for..do&lt;/code&gt;,&lt;code&gt;while..do&lt;/code&gt;and&lt;code&gt;repeat..until&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;with..do&lt;/code&gt;notation for "opening" records.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;procedure&lt;/code&gt;and&lt;code&gt;function&lt;/code&gt;including value and&lt;code&gt;var&lt;/code&gt;parameters and nesting.&lt;/item&gt;
      &lt;item&gt;The standard procedures for screen input and output (i.e. &lt;code&gt;ReadLn&lt;/code&gt;,&lt;code&gt;WriteLn&lt;/code&gt;etc.).&lt;/item&gt;
      &lt;item&gt;All conversion and utility procedures and functions that Turbo Pascal 3.0 had.&lt;/item&gt;
      &lt;item&gt;The three kinds of disk files, that is untyped (&lt;code&gt;file&lt;/code&gt;), typed (&lt;code&gt;file of&lt;/code&gt;) and&lt;code&gt;Text&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A dynamic heap of up to 32767 bytes with &lt;code&gt;GetMem&lt;/code&gt;,&lt;code&gt;FreeMem&lt;/code&gt;,&lt;code&gt;New&lt;/code&gt;and&lt;code&gt;Dispose&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Inline assembly (via opcodes, not via mnemonics, so this page might be handy).&lt;/item&gt;
      &lt;item&gt;Overlays (in memory, Spectrum 128K and Next only, see below).&lt;/item&gt;
      &lt;item&gt;Some compiler directives: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;$i &amp;lt;file&amp;gt;&lt;/code&gt;for including Pascal source files (including nesting and cycle detection)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$l &amp;lt;file&amp;gt;&lt;/code&gt;for including an assembly file (aka "linking" a library)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$a(+/-)&lt;/code&gt;for enabling or disabling absolute mode (default is on, disable for recursion)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$i(+/-)&lt;/code&gt;for enabling or disabling IO checking (when off, check&lt;code&gt;IOResult&lt;/code&gt;after calls)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$k(+/-)&lt;/code&gt;for enabling or disabling stack overflow checking&lt;/item&gt;&lt;item&gt;&lt;code&gt;$u(+/-)&lt;/code&gt;for enabling or disabling Ctrl-C checking&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The compiler also has some features that were borrowed from or inspired by later versions of Turbo Pascal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C-style &lt;code&gt;//&lt;/code&gt;one-line comments in addition to&lt;code&gt;{..}&lt;/code&gt;and&lt;code&gt;(*..*)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Binary literals (using a &lt;code&gt;%&lt;/code&gt;prefix).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Break&lt;/code&gt;and&lt;code&gt;Continue&lt;/code&gt;for loop control.&lt;/item&gt;
      &lt;item&gt;Querying the keyboard via &lt;code&gt;KeyPressed&lt;/code&gt;and&lt;code&gt;ReadKey&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Color support via &lt;code&gt;TextColor&lt;/code&gt;and&lt;code&gt;TextBackground&lt;/code&gt;with constants for the 8 Spectrum Next colors.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Inc&lt;/code&gt;and&lt;code&gt;Dec&lt;/code&gt;for more efficient increasing and decreasing of variables.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Include&lt;/code&gt;and&lt;code&gt;Exclude&lt;/code&gt;for more efficient handling of sets.&lt;/item&gt;
      &lt;item&gt;A simple &lt;code&gt;Assert&lt;/code&gt;facility that counts passes/fails and shows the failed line number.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since that covers most of the functionality of Turbo Pascal 3 you might ask what is missing. These are the current limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All the remaining compiler directives are not yet supported.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Mark&lt;/code&gt;/&lt;code&gt;Release&lt;/code&gt;are not currently supported.&lt;/item&gt;
      &lt;item&gt;The standard files &lt;code&gt;Input&lt;/code&gt;,&lt;code&gt;Output&lt;/code&gt;,&lt;code&gt;Kbd&lt;/code&gt;,&lt;code&gt;Con&lt;/code&gt;and&lt;code&gt;Lst&lt;/code&gt;are not supported.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Chain&lt;/code&gt;and&lt;code&gt;Execute&lt;/code&gt;are not supported.&lt;/item&gt;
      &lt;item&gt;Add-on libraries from the PC version of Turbo Pascal 3.0 are not yet supported (although there are a few graphics primitives for the ZX targets).&lt;/item&gt;
      &lt;item&gt;The new instructions of the Z80N CPU inside the ZX Spectrum Next are not yet being leveraged.&lt;/item&gt;
      &lt;item&gt;No separate compilation. Everything is compiled from source, always.&lt;/item&gt;
      &lt;item&gt;Binary size is quite large compared to the original.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The runtime library, being partially written in Pascal itself, gets quite large when compiled. I hope to bring this down again by reimplementing more of it in Z80 assembly (or improve the code generator, which, although it has a peephole optimizer, is not generating super-efficient Z80 code).&lt;/p&gt;
    &lt;p&gt;The compiler is itself written in Pascal. You can compile it with Free Pascal (I use version 3.2.2). Just run&lt;/p&gt;
    &lt;code&gt;$ fpc pasta&lt;/code&gt;
    &lt;p&gt;The Pascal compiler generates Z80 assembler code and relies on sjasmplus as a backend for the final translation step to binary. It can also, in &lt;code&gt;--ide&lt;/code&gt; mode (see below), make use of various other external tools. The compiler tries to detect these external tools automatically (from your system's &lt;code&gt;PATH&lt;/code&gt;), but sometimes it's best to create a file &lt;code&gt;.pasta80.cfg&lt;/code&gt; in your home directory specifying necessary paths (there is a sample in &lt;code&gt;misc&lt;/code&gt; that you can adapt).&lt;/p&gt;
    &lt;code&gt;# PASTA/80 config

HOME      = ~/Spectrum/pasta80
ASSEMBLER = ~/Spectrum/sjasmplus/sjasmplus
...
&lt;/code&gt;
    &lt;p&gt;You can check your whole setup by calling the compiler with &lt;code&gt;--config&lt;/code&gt;. It will show the full paths of all internal and external requirements and whether they are fulfilled.&lt;/p&gt;
    &lt;p&gt;To run the compiler just invoke the executable with the name of a Pascal source file to translate.&lt;/p&gt;
    &lt;p&gt;The default target is CP/M. There is an optional parameter that enables some simple peephole optimizations and another one that uses dependency analysis to eliminate unused Pascal procedures and functions:&lt;/p&gt;
    &lt;code&gt;$ pasta hello.pas             # Compiles hello.pas to hello.com
$ pasta hello                 # Source file .pas suffix is optional
$ pasta --opt hello.pas       # Enables peephole optimizations
$ pasta --opt --dep hello.pas # The same plus dependency analysis&lt;/code&gt;
    &lt;p&gt;You can run the resulting &lt;code&gt;.com&lt;/code&gt; files on a real CP/M machine or in a CP/M emulator. I recommend the excellent tnylpo. For programs that use VT52 control codes you have to start tnylpo in full-screen mode:&lt;/p&gt;
    &lt;code&gt;$ tnylpo hello                # Run in line-mode
$ tnylpo -s -t @ hello        # Monochrome full-screen, wait when finished
$ tnylpo -soy,4,0 -t @ hello  # Color full-screen, wait when finished&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;"Hello, World" in line mode&lt;/cell&gt;
        &lt;cell role="head"&gt;"Hello, World" in full-screen&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To generate binaries for the ZX Spectrum 48K, 128K and Next targets, use the &lt;code&gt;--zx48&lt;/code&gt;, &lt;code&gt;--zx128&lt;/code&gt; and &lt;code&gt;--zxnext&lt;/code&gt; parameters, respectively.&lt;/p&gt;
    &lt;code&gt;$ pasta --zx48 hello.pas      # Compiles for ZX Spectrum 48K
$ pasta --zx128 hello.pas     # Compiles for ZX Spectrum 48K
$ pasta --zxnext hello.pas    # Compiles for ZX Spectrum Next&lt;/code&gt;
    &lt;p&gt;The main difference between the three (currently) is that the ZX Spectrum Next target supports file IO (on the SD card), while the other two do not. The remaining routines are mostly the same. Screen output is handled via &lt;code&gt;rst $10&lt;/code&gt; in the ROM. In both cases the binaries are expected to be run from address 0x8000.&lt;/p&gt;
    &lt;p&gt;The default output format for the ZX Spectrum targets is a simple binary file that contains exactly the bytes of the compiled program (plus a +3DOS header when compiling for the Spectrum Next). In addition to that (and for more complex cases involving overlays), the compiler can also generate snapshot files or tape files, the latter including a suitable BASIC loader:&lt;/p&gt;
    &lt;code&gt;$ pasta --zx48 --sna examples/hello.pas   # .sna file
$ pasta --zx48 --tap examples/jacques.pas # .tap file with BASIC loader&lt;/code&gt;
    &lt;p&gt;Being self-contained, snapshots and tapes are a convenient way to distribute your programs and to launch them an emulator, such as Fuse:&lt;/p&gt;
    &lt;code&gt;$ open -a Fuse examples/hello.sna         # Launch .sna file in FUSE (on Mac)
$ open -a Fuse examples/jacques.tap       # Launch .tap file in FUSE (on Mac)&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Hello world in FUSE&lt;/cell&gt;
        &lt;cell role="head"&gt;Frere Jacques in FUSE (yes, with sound!)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When compiling for the Next, another useful format is a runnable directory. It contains exactly the same files that would also be in the .tap file, including a BASIC loader named &lt;code&gt;run.bas&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;$ pasta --zxnext --run examples/pq.pas    # Results in directory named pq.run&lt;/code&gt;
    &lt;p&gt;The directory has the suffix &lt;code&gt;.run&lt;/code&gt;. When attempting to enter such a directory in the Next's file browser, the loader is started automatically (press Symbol Shift + Enter to really see the contents). If you are a Mac user: Yes, it's a bit like an &lt;code&gt;.app&lt;/code&gt; bundle.&lt;/p&gt;
    &lt;p&gt;The Spectrum 128K and Next targets support overlays. This means you can have larger programs than would normally fit into the 64K address space of a Z80 machine. The rules are the same as for Turbo Pascal 3.0:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Overlays can be applied to global procedures and functions only, not to nested ones (though nested ones will be overlayed if the containing ones are, too).&lt;/item&gt;
      &lt;item&gt;Overlays cannot be applied to global variables, that is, you cannot use them for data (at least not without tricks).&lt;/item&gt;
      &lt;item&gt;All consecutive procedures and functions that are marked as &lt;code&gt;overlay&lt;/code&gt;go into the same overlay. Use any declaration inbetween to separate overlays.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the following example, there are three overlays: Overlay 0 contains A and B, overlay 1 contains D, and overlay 2 contains E.&lt;/p&gt;
    &lt;code&gt;overlay procedure A; (* Overlay 0 *)
begin
end;

overlay procedure B; (* Overlay 0 *)
begin
end;

procedure C; (* Not in an overlay *)
begin
end;

overlay procedure D; (* Overlay 1 *)
begin
end;

type
  Dummy = Integer;   (* Separator *)

overlay procedure E; (* Overlay 2 *)
begin
end;&lt;/code&gt;
    &lt;p&gt;In contrast to Turbo Pascal 3.0, overlays are not implemented via disk files. Instead, they use the additional RAM of the Spectrum 128K and Next machines. The uppermost 16K bank (Spectrum 128K) or 8K page (Spectrum Next) will be reserved for overlays. Each overlay can have a maximum size of 8K. The compiler manages everything and generates special "far calls" whenever necessary.&lt;/p&gt;
    &lt;p&gt;To enable overlays, use the &lt;code&gt;--ovr&lt;/code&gt; command line parameter, ideally in conjuncton with the &lt;code&gt;--tap&lt;/code&gt; parameter, as the tape loaders for 128K and Next are fully overlay-aware.&lt;/p&gt;
    &lt;code&gt;$ pasta --zx128 --tap --opt --dep --ovr tests/all.pas # Test suite as 128K tape&lt;/code&gt;
    &lt;p&gt;The compiler prints a report of which overlays go into which RAM banks or pages.&lt;/p&gt;
    &lt;code&gt;----------------------------------------
PASTA/80 Pascal System      Version 0.96
                            ZX 128K, Z80

Copyright (C) 2020-25 by  Joerg Pleumann
----------------------------------------

Compiling...
  tests/all.pas -&amp;gt; tests/all.z80
Assembling...
  tests/all.z80 -&amp;gt; tests/all.tap

Program   : 10781 bytes ($8000-$AA1C)
Heap      :  1507 bytes ($AA1D-$AFFF)
Stack     :  4096 bytes ($B000-$BFFF)

Overlay  0:  7399 bytes ($C000-$DCE6) in bank  0
Overlay  1:  7185 bytes ($E000-$FC10) in bank  0
Overlay  2:  2725 bytes ($C000-$CAA4) in bank  1
Overlay  3:  6293 bytes ($E000-$F894) in bank  1
Overlay  4:  6392 bytes ($C000-$D8F7) in bank  3
Overlay  5:  6527 bytes ($E000-$F97E) in bank  3
&lt;/code&gt;
    &lt;p&gt;Without the &lt;code&gt;--ovr&lt;/code&gt; parameter, overlay markers are simply ignored. This means you can use the same source code for platforms that do support overlays and for those that don't.&lt;/p&gt;
    &lt;p&gt;Caution: Overlays somewhat break the safety of the Pascal language. Be careful when using pointers or &lt;code&gt;var&lt;/code&gt; parameters for passing data between overlays. The memory you refer to may have just been paged out! It might make sense to compile your overlays with &lt;code&gt;{$a-}&lt;/code&gt;, so that all local variables are stored on the stack (which is always visible).&lt;/p&gt;
    &lt;p&gt;There is a folder containing &lt;code&gt;examples&lt;/code&gt; and a folder containing &lt;code&gt;tests&lt;/code&gt; for the compiler. The main test suite &lt;code&gt;all.pas&lt;/code&gt; needs to be compiled with &lt;code&gt;--opt --dep&lt;/code&gt; because of its size. Otherwise it won't fit into 64K. The Spectrum 128K and Next targets can (only) handle it using overlays, the Spectrum 48K target can't. Both the examples and the tests should give you a pretty good overview of what the compiler can do.&lt;/p&gt;
    &lt;p&gt;I also solved all puzzles of Advent of Code 2022 with an earlier version of the compiler and made YouTube videos of the solutions running on the ZX Spectrum Next, in CP/M mode.&lt;/p&gt;
    &lt;p&gt;As a fun little gimmick the compiler can be started like this&lt;/p&gt;
    &lt;code&gt;$ pasta --ide&lt;/code&gt;
    &lt;p&gt;to run it in an interactive mode that has an interface reminiscient of Turbo Pascal 3.0.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Main menu&lt;/cell&gt;
        &lt;cell role="head"&gt;Editor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When started in an ordinary terminal, this mode relies on the editor &lt;code&gt;nano&lt;/code&gt; being present on your system (on MacOS you might want to install the real &lt;code&gt;nano&lt;/code&gt; via a package manager because Apple sells you the much more limited &lt;code&gt;pico&lt;/code&gt; editor as &lt;code&gt;nano&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;You can also run it in a shell within Visual Studio Code, in which case it would automatically use VSC's editor (via the &lt;code&gt;code&lt;/code&gt; command, which, on a Mac, you might have to make available from VCS's settings) and act a bit like a plugin.&lt;/p&gt;
    &lt;p&gt;The following external tools are supported for running compiled programs on the host machine:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tnylpo for CP/M programs (press &amp;lt;R&amp;gt; for line mode, &amp;lt;Shift-R&amp;gt; for full-screen mode).&lt;/item&gt;
      &lt;item&gt;Fuse for programs targeting the ZX Spectrum 48K and 128K machines.&lt;/item&gt;
      &lt;item&gt;CSpect for ZX Spectrum Next programs. &lt;list rend="ul"&gt;&lt;item&gt;Please have hdfmonkey ready for manipulating the SD card image.&lt;/item&gt;&lt;item&gt;If you're on MacOS or Linux, you also need &lt;code&gt;mono&lt;/code&gt;because CSpect is a .NET application.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As mentioned before, everything that is in your &lt;code&gt;PATH&lt;/code&gt; should be detected automatically. There are some exceptions, though, so it makes sense to copy &lt;code&gt;misc/.pasta80.cfg&lt;/code&gt; to your home directory and adapt it. Use the &lt;code&gt;--config&lt;/code&gt; parameter to let PASTA/80 check your setup and get feedback on what is in place and what is missing.&lt;/p&gt;
    &lt;p&gt;The following screenshots show some applications compiled for the CP/M target and running in the &lt;code&gt;tnylpo&lt;/code&gt; emulator.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;2048&lt;/cell&gt;
        &lt;cell role="head"&gt;Game of Life&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Micro Calc&lt;/cell&gt;
        &lt;cell role="head"&gt;Galactic Empire&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These screenshots show some applications compiled for the ZX Spectrum 48K target and running in the FUSE emulator.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;2048&lt;/cell&gt;
        &lt;cell role="head"&gt;Game of Life&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Graphics Demo&lt;/cell&gt;
        &lt;cell role="head"&gt;Equation Solver&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;PASTA/80 Pascal Compiler&lt;/p&gt;
    &lt;p&gt;Copyright (c) 2020-2025 by Jörg Pleumann&lt;/p&gt;
    &lt;p&gt;The PASTA/80 compiler is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License (GPL) as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The runtime library (folder&lt;/p&gt;&lt;code&gt;rtl&lt;/code&gt;) comes with a linking exception that makes sure the GPL does not transfer to binaries created using PASTA/80.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The examples (folder&lt;/p&gt;&lt;code&gt;examples&lt;/code&gt;) are considered public domain or whatever comes closest to that in your jurisdiction.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Individual files or folders may use different licenses, so you might want to double check.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.&lt;/p&gt;
    &lt;p&gt;What does this mean for you?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You can use the compiler, free of charge, to build any application, open-source or prioprietary, free or paid, and distribute the generated binary without restriction. You can distribute binaries created with PASTA/80 under a license of your choosing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can modify the compiler according to your needs. If you distribute the compiler or parts of it, binary or source, modified or not, you have to comply with the rules laid out in the GPL (copyright info, source code, ...) unless the linking exception applies.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The math48 library is coypright (c) 1980 by Anders Hejlsberg, used by permission.&lt;/p&gt;
    &lt;p&gt;Some assembly routines adapted from Leventhal/Saville, "Z80 Assembly Subroutines", Osborne/McGraw-Hill 1983.&lt;/p&gt;
    &lt;p&gt;Turbo Pascal is a registered trademark of Code Gear LLC / Embarcadero.&lt;/p&gt;
    &lt;p&gt;Z80 is a registered trademark of Zilog, Inc.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/pleumann/pasta80"/><published>2025-10-21T07:23:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45654512</id><title>Diamond Thermal Conductivity: A New Era in Chip Cooling</title><updated>2025-10-21T17:11:24.744611+00:00</updated><content>&lt;doc fingerprint="336924ce2a1195f8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Diamond Blankets Will Keep Future Chips Cool&lt;/head&gt;&lt;p&gt;A micrometers-thick integrated layer spreads out the heat&lt;/p&gt;&lt;p&gt;Today’s stunning computing power is allowing us to move from human intelligence toward artificial intelligence. And as our machines gain more power, they’re becoming not just tools but decision-makers shaping our future.&lt;/p&gt;&lt;p&gt;But with great power comes great…heat!&lt;/p&gt;&lt;p&gt;As nanometer-scale transistors switch at gigahertz speeds, electrons race through circuits, losing energy as heat—which you feel when your laptop or your phone toasts your fingers. As we’ve crammed more and more transistors onto chips, we’ve lost the room to release that heat efficiently. Instead of the heat spreading out quickly across the silicon, which makes it much easier to remove, it builds up to form hot spots, which can be tens of degrees warmer than the rest of the chip. That extreme heat forces systems to throttle the performance of CPUs and GPUs to avoid degrading the chips.&lt;/p&gt;&lt;p&gt;In other words, what began as a quest for miniaturization has turned into a battle against thermal energy. This challenge extends across all electronics. In computing, high-performance processors demand ever-increasing power densities. (New Nvidia GPU B300 servers will consume nearly 15 kilowatts of power.) In communication, both digital and analog systems push transistors to deliver more power for stronger signals and faster data rates. In the power electronics used for energy conversion and distribution, efficiency gains are being countered by thermal constraints.&lt;/p&gt;&lt;p&gt;The ability to grow large-grained polycrystalline diamond at low temperature led to a new way to combat heat in transistors. Mohamadali Malakoutian&lt;/p&gt;&lt;p&gt;Rather than allowing heat to build up, what if we could spread it out right from the start, inside the chip?—diluting it like a cup of boiling water dropped into a swimming pool. Spreading out the heat would lower the temperature of the most critical devices and circuits and let the other time-tested cooling technologies work more efficiently. To do that, we’d have to introduce a highly thermally conductive material inside the IC, mere nanometers from the transistors, without messing up any of their very precise and sensitive properties. Enter an unexpected material—diamond.&lt;/p&gt;&lt;p&gt;In some ways, diamond is ideal. It’s one of the most thermally conductive materials on the planet—many times more efficient than copper—yet it’s also electrically insulating. However, integrating it into chips is tricky: Until recently we knew how to grow it only at circuit-slagging temperatures in excess of 1,000 °C.&lt;/p&gt;&lt;p&gt;But my research group at Stanford University has managed what seemed impossible. We can now grow a form of diamond suitable for spreading heat, directly atop semiconductor devices at low enough temperatures that even the most delicate interconnects inside advanced chips will survive. To be clear, this isn’t the kind of diamond you see in jewelry, which is a large single crystal. Our diamonds are a polycrystalline coating no more than a couple of micrometers thick.&lt;/p&gt;&lt;p&gt;The potential benefits could be huge. In some of our earliest gallium-nitride radio-frequency transistors, the addition of diamond dropped the device temperature by more than 50 °C. At the lower temperature, the transistors amplified X-band radio signals five times as well as before. We think our diamond will be even more important for advanced CMOS chips. Researchers predict that upcoming chipmaking technologies could make hot spots almost 10 °C hotter [see , “Future Chips Will Be Hotter Than Ever”, in this issue]. That’s probably why our research is drawing intense interest from the chip industry, including Applied Materials, Samsung, and TSMC. If our work continues to succeed as it has, heat will become a far less onerous constraint in CMOS and other electronics too.&lt;/p&gt;&lt;head rend="h2"&gt;Where Heat Begins and Ends in Chips&lt;/head&gt;&lt;p&gt;At the boundary between the diamond and the semiconductor, a thin layer of silicon carbide forms. It acts as a bridge for heat to flow into the diamond. Mohamadali Malakoutian&lt;/p&gt;&lt;p&gt;Heat starts within transistors and the interconnects that link them, as the flow of current meets resistance. That means most of it is generated near the surface of the semiconductor substrate. From there it rises either through layers of metal and insulation or through the semiconductor itself, depending on the package architecture. The heat then encounters a thermal interface material designed to spread it out before it ultimately reaches a heat sink, a radiator, or some sort of liquid cooling, where air or fluid carries the heat away.&lt;/p&gt;&lt;p&gt;The dominant cooling strategies today center around advances in heat sinks, fans, and radiators. In pursuit of even better cooling, researchers have explored liquid cooling using microfluidic channels and removing heat using phase-change materials. Some computer clusters go so far as to submerge the servers in thermally conductive, dielectric—electrically insulating—liquids.&lt;/p&gt;&lt;p&gt;These innovations are critical steps forward, but they still have limitations. Some are so expensive they’re worthwhile only for the highest-performing chips; others are simply too bulky for the job. (Your smartphone can’t carry a conventional fan.) And none are likely to be very effective as we move toward chip architectures resembling silicon skyscrapers that stack multiple layers of chips. Such 3D systems are only as viable as our ability to remove heat from every layer within it.&lt;/p&gt;&lt;p&gt;The big problem is that chip materials are poor heat conductors, so the heat becomes trapped and concentrated, causing the temperature to skyrocket within the chip. At higher temperatures, transistors leak more current, wasting power; they age more quickly, too.&lt;/p&gt;&lt;p&gt;Heat spreaders allow the heat to move laterally, diluting it and allowing the circuits to cool. But they’re positioned far—relatively, of course—from where the heat is generated, and so they’re of little help with these hot spots. We need a heat-spreading technology that can exist within nanometers of where the heat is generated. This is where our new low-temperature diamond could be essential.&lt;/p&gt;&lt;head rend="h2"&gt;How to Make Diamonds&lt;/head&gt;&lt;p&gt;Before my lab turned to developing diamond as a heat-spreading material, we were working on it as a semiconductor. In its single-crystal form—like the kind on your finger—it has a wide bandgap and ability to withstand enormous electric fields. Single-crystalline diamond also offers some of the highest thermal conductivity recorded in any material, reaching 2,200 to 2,400 watts per meter per kelvin—roughly six times as conductive as copper. Polycrystalline diamond—an easier to make material—can approach these values when grown thick. Even in this form, it outperforms copper.&lt;/p&gt;&lt;p&gt;As attractive as diamond transistors might be, I was keenly aware—based on my experience researching gallium nitride devices—of the long road ahead. The problem is one of scale. Several companies are working to scale high-purity diamond substrates to 50, 75, and even 100 millimeters but the diamond substrates we could acquire commercially were only about 3 mm across.&lt;/p&gt;&lt;p&gt;Gallium nitride high-electron-mobility transistors were an ideal test case for diamond cooling. The devices are 3D and the critical heat-generating part, the two-dimensional electron gas, is close to the surface. Chris Philpot&lt;/p&gt;So we decided instead to try growing diamond films on large silicon wafers, in the hope of moving toward commercial-scale diamond substrates. In general, this is done by reacting methane and hydrogen at high temperatures, 900 °C or more. This results in not a single crystal but a forest of narrow columns. As they grow taller, the nanocolumns coalesce into a uniform film, but by the time they form high-quality polycrystalline diamond, the film is already very thick. This thick growth adds stress to the material and often leads to cracking and other problems.&lt;p&gt;But what if we used this polycrystalline coating as a heat spreader for other devices? If we could get diamond to grow within nanometers of transistors, get it to spread heat both vertically and laterally, and integrate it seamlessly with the silicon, metal, and dielectric in chips, it might do the job.&lt;/p&gt;&lt;p&gt;There were good reasons to think it would work. Diamond is electrically insulating, and it has a relatively low dielectric constant. That means it makes a poor capacitor, so signals sent through diamond-encrusted interconnects might not degrade much. Thus diamond could act as a “thermal dielectric,” one that is electrically insulating but thermally conducting.&lt;/p&gt;&lt;p&gt;Polycrystalline diamond could help reduce temperatures inside 3D chips. Diamond thermal vias would grow inside micrometers-deep holes so heat can flow from vertically from one chip to a diamond heat spreader in another chip that’s stacked atop it. Dennis Rich&lt;/p&gt;&lt;p&gt;For our plan to work, we were going to have to learn to grow diamond differently. We knew there wasn’t room to grow a thick film inside a chip. We also knew the narrow, spiky crystal pillars made in the first part of the growth process don’t transmit heat laterally very well, so we’d need to grow large-grained crystals from the start to get the heat moving horizontally. A third problem was that the existing diamond films didn’t form a coating on the sides of devices, which would be important for inherently 3D devices. But the biggest impediment was the high temperature needed to grow the diamond film, which would damage, if not destroy, an IC’s circuits. We were going to have to cut the growth temperature at least in half.&lt;/p&gt;&lt;p&gt;Just lowering the temperature doesn’t work. (We tried: You wind up, basically, with soot, which is electrically conductive—the opposite of what’s needed.) We found that adding oxygen to the mix helped, because it continuously etched away carbon deposits that weren’t diamond. And through extensive experimentation, we were able to find a formula that produced coatings of large-grained polycrystalline diamond all around devices at 400 °C, which is a survivable temperature for CMOS circuits and other devices.&lt;/p&gt;&lt;head rend="h2"&gt;Thermal Boundary Resistance&lt;/head&gt;&lt;p&gt;Although we had found a way to grow the right kind of diamond coatings, we faced another critical challenge—the phonon bottleneck, also known as thermal boundary resistance (TBR). Phonons are packets of heat energy, in the way that photons are packets of electromagnetic energy. Specifically, they’re a quantized version of the vibration of a crystal lattice. These phonons can pile up at the boundary between materials, resisting the flow of heat. Reducing TBR has long been a goal in thermal interface engineering, and it is often done by introducing different materials at the boundary. But semiconductors are compatible only with certain materials, limiting our choices.&lt;/p&gt;&lt;p&gt;Thermal scaffolding would link layers of heat-spreading polycrystalline diamond in one chip to those in another chip in a 3D-stacked silicon. The thermal pillars would traverse each chip’s interconnects and dielectric material to move heat vertically through the stack. Srabanti Chowdhury&lt;/p&gt;&lt;p&gt;In the end, we got lucky. While growing diamond on GaN capped with silicon nitride, we observed something unexpected: The measured TBR was much lower than prior reports led us to expect. (The low TBR was independently measured, initially by Martin Kuball at the University of Bristol, in England, and later by Samuel Graham Jr., then at Georgia Tech, who both have been coauthors and collaborators in several of our papers.)&lt;/p&gt;&lt;p&gt;Through further investigation of the interface science and engineering, and in collaboration with K.J. Cho at the University of Texas at Dallas, we identified the cause of the lower TBR. Intermixing at the interface between the diamond and silicon nitride led to the formation of silicon carbide, which acted as a kind of bridge for the phonons, allowing more efficient heat transfer. Though this began as a scientific discovery, its technological impact was immediate—with a silicon carbide interface, our devices exhibited significantly improved thermal performance.&lt;/p&gt;&lt;head rend="h2"&gt;GaN HEMTs: The First Test Case&lt;/head&gt;&lt;p&gt;We began testing our new low-TBR diamond coatings in gallium nitride high-electron-mobility transistors (HEMTs). These devices amplify RF signals by controlling current through a two-dimensional electron gas that forms within its channel. We leveraged the pioneering research on HEMTs done by Umesh Mishra’s laboratory at the University of California, Santa Barbara, where I had been a graduate student. The Mishra lab invented a particular form of the material called N-polar gallium nitride. Their N-polar GaN HEMTs demonstrate exceptional power density at high frequencies, particularly in the W-band, the 75- to 110-gigahertz part of the microwave spectrum.&lt;/p&gt;&lt;p&gt;RELATED: Gallium Nitride and Silicon Carbide Fight for Green Tech Domination&lt;/p&gt;&lt;p&gt;What made these HEMTs such a good test case is one defining feature of the device: The gate, which controls the flow of current through the device, is within tens of nanometers of the transistor’s channel. That means that heat is generated very close to the surface of the device, and any interference our diamond coating could cause would quickly show in the device’s operation.&lt;/p&gt;&lt;p&gt;We introduced the diamond layer so that it surrounded the HEMT completely, even on the sides. By maintaining a growth temperature below 400 °C, we hoped to preserve core device functionality. While we did see some decline in high-frequency performance, the thermal benefits were substantial—channel temperatures dropped by a remarkable 70 °C. This breakthrough could be a potentially transformative solution for RF systems, allowing them to operate at higher power than ever before.&lt;/p&gt;&lt;head rend="h2"&gt;Diamond in CMOS&lt;/head&gt;&lt;p&gt;We wondered if our diamond layer could also work in high-power CMOS chips. My colleagues at Stanford, H.-S. Philip Wong and Subhasish Mitra, have long championed 3D-stacked chip architectures. In CMOS computing chips, 3D stacking appears to be the most viable way forward to increase integration density, improve performance, and overcome the limitations of traditional transistor scaling. It’s already used in some advanced AI chips, such as AMD’s MI300 series. And it’s established in the high-bandwidth memory chips that pump data through Nvidia GPUs and other AI processors. The multiple layers of silicon in these 3D stacks are mostly connected by microscopic balls of solder, or in some advanced cases just by their copper terminals. Getting signals and power out of these stacks requires vertical copper links that burrow through the silicon to reach the chip package’s substrate.&lt;/p&gt;&lt;p&gt;In one of our discussions, Mitra pointed out that a critical issue with 3D-stacked chips is the thermal bottlenecks that form within the stack. In 3D architectures, the traditional heat sinks and other techniques used for 2D chips aren’t sufficient. Extracting heat from each layer is essential.&lt;/p&gt;&lt;p&gt;Our research could redefine thermal management across industries.&lt;/p&gt;&lt;p&gt;Our experiments on thermal boundary resistance in GaN suggested a similar approach would work in silicon. And when we integrated diamond with silicon, the results were remarkable: An interlayer of silicon carbide formed, leading to diamond with an excellent thermal interface.&lt;/p&gt;&lt;p&gt;Our effort introduced the concept of thermal scaffolding. In that scheme, nanometers-thick layers of polycrystalline diamond would be integrated within the dielectric layers above the transistors to spread heat. These layers would then be connected by vertical heat conductors, called thermal pillars, made of copper or more diamond. These pillars would connect to another heat spreader, which in turn would link to thermal pillars on the next chip in the 3D stack, and so on until the heat reached the heat sink or other cooling device.&lt;/p&gt;&lt;p&gt;The more tiers of computing silicon in a 3D chip, the bigger difference thermal scaffolding makes. An AI accelerator with more than five tiers would well exceed typical temperature limits unless the scaffolding was employed. Srabanti Chowdhury&lt;/p&gt;In a collaboration with Mitra, we used simulations of heat generated by real computational workloads to operate a proof-of-concept structure. This structure consisted of dummy heaters to mimic hot spots in a two-chip stack along with diamond heat spreaders and copper thermal pillars. Using this, we reduced the temperature to one-tenth its value without the scaffolding.&lt;p&gt;There are hurdles still to overcome. In particular, we still have to figure out a way to make the top of our diamond coatings atomically flat. But, in collaboration with industry partners and researchers, we are systematically studying that problem and other scientific and technological issues. We and our partners think this research could offer a disruptive new path for thermal management and a crucial step toward sustaining high-performance computing into the future.&lt;/p&gt;&lt;head rend="h2"&gt;Developing Diamond Thermal Solutions&lt;/head&gt;&lt;p&gt;We now intend to move toward industry integration. For example, we’re working with the Defense Advanced Research Projects Agency Threads program, which aims to use device-level thermal management to develop highly efficient and reliable X-band power amplifiers with a power density 6 to 8 times as efficient as today’s devices. The program, which was conceived and initially run by Tom Kazior, is a critical platform for validating the use of low-temperature diamond integration in GaN HEMT manufacturing. It’s enabled us to collaborate closely with industry teams while protecting both our and our partners’ processes. Defense applications demand exceptional reliability, and our diamond-integrated HEMTs are undergoing rigorous testing with industry partners. The early results are promising, guiding refinements in growth processes and integration techniques that we’ll make with our partners over the next two years.&lt;/p&gt;&lt;p&gt;But our vision extends beyond GaN HEMTs to other materials and particularly silicon computational chips. For the latter, we have an established collaboration with TSMC, and we’re expanding on newer opportunities with Applied Materials, Micron, Samsung, and others through the Stanford SystemX Alliance and the Semiconductor Research Corp. This is an extraordinary level of collaboration among otherwise fierce competitors. But then, heat is a universal challenge in chip manufacturing, and everyone is motivated to find the best solutions.&lt;/p&gt;&lt;p&gt;If successful, our research could redefine thermal management across industries. In my work on gallium nitride devices, I have seen firsthand how once-radical ideas like this transition to become industry standards, and I believe diamond-based heat extraction will follow the same trajectory, becoming a critical enabler for a generation of electronics that is no longer hindered by heat.&lt;/p&gt;&lt;p&gt;This article appears in the November 2025 print issue as “Diamond Blankets Will Chill Future Chips.”&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Better Computing Through CPU Cooling ›&lt;/item&gt;&lt;item&gt;The Radio We Could Send to Hell ›&lt;/item&gt;&lt;item&gt;Gallium Oxide: The Supercharged Semiconductor ›&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/diamond-thermal-conductivity"/><published>2025-10-21T11:16:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45654660</id><title>StarGrid: A Brand-New Palm OS Strategy Game in 2025</title><updated>2025-10-21T17:11:24.263198+00:00</updated><content>&lt;doc fingerprint="efc6197dcbc22af0"&gt;
  &lt;main&gt;&lt;p&gt;This year my side project of choice was to create a brand new game for Palm OS, it started out as something that I thought I would finish in a month but ended up taking more than half a year in the end.&lt;/p&gt;&lt;p&gt;Let me present you with StarGrid, a space themed strategy game played on a hexagonal grid:&lt;/p&gt;&lt;p&gt;StarGrid is a turn-based strategy game for Palm OS where you command a fleet of ships in a battle for control of the galaxy. Capture enemy flags, outmaneuver opposing fleets, and defend your own base in tense, tactical matches. Every move counts, will you strike boldly or play the long game to claim victory?&lt;/p&gt;&lt;p&gt;No Palm OS device at hand? No problem, just play it on your browser thanks to the CloudPilot emulator.&lt;/p&gt;Game download and in-browser emulator&lt;p&gt;Allot of 'manual' labor went into this game, no premade game engine, no additional sdk's. Just making it from scratch, trying to solve one technical puzzle after another, but learning so many neat things along the way.&lt;/p&gt;&lt;p&gt;Coding for Palm certainly comes with it's own obstacles:&lt;/p&gt;&lt;p&gt;- Memory is tight so you need to take into account devices that can't even keep the playing field into memory, solution there was to hide the tiles when ships are moving.&lt;/p&gt;&lt;p&gt;- Maximum code size itself is also very limited, requiring you to segment your application into multiple individual parts. Detailed documentation on this was long gone, so I had to scrap some info together from developers that uploaded their 25 year old code to GitHub.&lt;/p&gt;&lt;p&gt;You can follow along the blog posts to see how I got here:&lt;/p&gt;StarGrid: A new game I'm making for Palm OS in 2025 Building the CPU Player for StarGrid Moving out of the vaporware phase - StarGrid's alpha release for PalmOS is here! StarGrid for Palm OS almost ready (and why do my side projects always explode in scope)&lt;p&gt;Here's a video when playtesting the game on multiple Palm devices (cpu vs cpu action):&lt;/p&gt;&lt;p&gt;I won't immediately jump into the next big sideproject, I think I need a breather. I do however have some ideas lined up that I've been wanting to explore for a while now:&lt;/p&gt;&lt;p&gt;- making a top-down racing game (think micromachines)&lt;/p&gt;&lt;p&gt;- create an Outrun or Lotus III-like racing game&lt;/p&gt;&lt;p&gt;- building a ray-tracing game (like wolf3d).&lt;/p&gt;&lt;p&gt;Much more exciting stuff to come.&lt;/p&gt;&lt;p&gt;It's my way of keeping my favorite handheld operating system alive.&lt;/p&gt;&lt;p&gt;For now I hope at least some people will enjoy playing StarGrid and even if it's not their cup of tea, the game is fully open source, so I hope that can contribute to others making games and applications for this not-so-forgotten platform called Palm OS.&lt;/p&gt;StarGrid on GitHub&lt;p&gt;RetroGames, PalmOS, Development, StarGrid&lt;/p&gt;&lt;p&gt;You can get in touch through Mastodon:&lt;/p&gt;@rxpz@social.linux.pizza&lt;p&gt;StarGrid has arrived, a Brand-New Palm OS Strategy Game in 2025! was published on 2025-10-21&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html"/><published>2025-10-21T11:42:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655161</id><title>Neural audio codecs: how to get audio into LLMs</title><updated>2025-10-21T17:11:23.728366+00:00</updated><content>&lt;doc fingerprint="2e1cc79b29380613"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for the valuable feedback on the drafts: Chung-Ming Chien, Moritz Boehle, Richard Hladík, Eugene Kharitonov, Patrick Perez, and Tom Sláma. I’d also like to thank the rest of the Kyutai team for the the research discussions without which this article could not exist.&lt;/p&gt;
    &lt;p&gt;As of October 2025, speech LLMs suck. Many LLMs have voice interfaces, but they usually work by transcribing your speech, generating the answer in text, and using text-to-speech to read the response out loud. That’s perfectly fine in many cases (see Unmute), but it’s a wrapper, not real speech understanding. The model can’t hear the frustration in your voice and respond with empathy, it can’t emphasize important words in its answer, it cannot sense sarcasm, and so on.&lt;/p&gt;
    &lt;p&gt;Yes, there are LLMs (Gemini, ChatGPT’s Advanced Voice Mode, Qwen, Moshi) that understand and generate speech natively. But in practice, they’re either not as smart, or they behave like text model wrappers. Try asking any of them “Am I speaking in a low voice or a high voice?” in a high-pitched voice, and they won’t be able to tell you.&lt;/p&gt;
    &lt;p&gt;Clearly, speech LLMs lag behind text LLMs. But why? For text, we found out a few years ago that if you take a lot of text data, a big Transformer, and a lot of GPUs, you’ll get some pretty damn good text continuation models. Why can’t we just replace text with audio and get pretty damn good speech continuation models?&lt;/p&gt;
    &lt;p&gt;As a teaser, here’s what happens when you try to do that naively (warning, loud):&lt;/p&gt;
    &lt;p&gt;We’ll have a look at why audio is harder to model than text and how we can make it easier with neural audio codecs, the de-facto standard way of getting audio into and out of LLMs. With a codec, we can turn audio into larger discrete tokens, train models to predict continuations for these tokens, and then decode those back into audio: see animation above.&lt;/p&gt;
    &lt;p&gt;Kyutai folks have done a lot of work in this space, which is part of the reason I chose to cover this topic. We’ll start from the basics and build up all the way to Mimi, our neural audio codec. It was originally developed for Moshi and later adopted by others for their models, notably Sesame’s CSM.&lt;/p&gt;
    &lt;p&gt;To tokenize text, everybody uses a technique called byte-pair encoding and rarely changes the tokenizer: OpenAI has been using the same tokenizer since GPT-4o, an ancient model if you count in LLM years.&lt;/p&gt;
    &lt;p&gt;You can even get decent results without tokenizing text at all, just predicting individual characters. One of the first posts that got me excited about machine learning was Andrej Karpathy’s RNN effectiveness blog post from 2015. Karpathy trains a three-layer LSTM on a single GPU and gets it to generate decent-looking code and LaTeX:&lt;/p&gt;
    &lt;p&gt;Remember this was ten years ago, back when we didn’t even know that attention is all we need. Now compare Karpathy’s results to a sample from WaveNet, a model DeepMind published a year later:&lt;/p&gt;
    &lt;p&gt;Purely acoustically, the audio sounds good, but it rarely even manages to produce a single correct English word. We can’t be too hard on WaveNet, though. The samples from Karpathy’s RNNs are only a few thousand characters long, but this 10-second audio consists of 160k audio samples, and WaveNet creates it by painstakingly predicting sample-by-sample.&lt;/p&gt;
    &lt;p&gt;It’s difficult to build models that are coherent over time scales this long, and the model also takes very long to run for so many steps.&lt;/p&gt;
    &lt;p&gt;So instead of running the model to predict the samples one-by-one directly, we’d like to train a model to compress the audio into a more manageable size. We could compress the audio, use an LLM to predict a continuation in the compressed representation, and then decompress the result.&lt;/p&gt;
    &lt;p&gt;But first, let’s get a baseline model by generating audio sample by sample, like WaveNet does. The code for all of these experiments is open-source! Check it out here. I forked Andrej Karpathy’s nanoGPT repo, a simple implementation of GPT-2.&lt;/p&gt;
    &lt;p&gt;Text and audio are kind of the same from the perspective of the language model: it’s just tokens in, tokens out. The only thing we need to do is to quantize the continuous values of the samples into discrete buckets. Like WaveNet, we’ll use the "μ-law algorithm" to get 256 buckets. We’ll treat those as 256 possible tokens.&lt;/p&gt;
    &lt;p&gt;Let’s train a language model on audio tokenized like this. For the dataset, we’ll use the Libri-Light dataset, following AudioLM (with Neil Zeghidour, Eugene Kharitonov). Its train split contains 50k hours in total, but we’ll go with 1000 hours for this experiment. With this sample-by-sample tokenization, we end up with a dataset of 53 GB.&lt;/p&gt;
    &lt;p&gt;We train a small-ish transformer of 151.28M parameters, about the size of the smallest GPT-2 variant. When we sample from the model, it makes babbling sounds (warning, loud at times!):&lt;/p&gt;
    &lt;p&gt;Often, it goes into a “crackling mode” that it can’t seem to get out of:&lt;/p&gt;
    &lt;p&gt;I also trained a smaller model, which I teased at the beginning. It’s prone to generate nightmare fuel screeches (loud!):&lt;/p&gt;
    &lt;p&gt;As you can tell, we’re not AGI yet. It sounds speech-like, but you can’t make out a single word and the voice keeps changing. No wonder: the context size of the model is 2048, which, for 16 kHz audio, translates to 128ms, not even a the length of one word. Also, these 10-second examples took 30 minutes to generate on an H100, so we’re a few orders of magnitude away from being real-time.&lt;/p&gt;
    &lt;p&gt;So let’s build a neural audio codec to compress the audio. The hope is that if we reduce the sampling rate 100x, the model will also become “100x more coherent”. An old idea in machine learning is to do this using an autoencoder: a model that takes an input, compresses it into a smaller “latent space”, and then tries to reconstruct the original input.&lt;/p&gt;
    &lt;p&gt;In our case, we’ll want an autoencoder whose latent space is quantized so that we can feed the latents into a language model and produce continuations. (You can generate continuations with unquantized latents, but it’s trickier – see the Further reading section.)&lt;/p&gt;
    &lt;p&gt;Bear with me, because we’ll take a detour from audio: let’s build a quantized autoencoder on images from Fashion MNIST. We’ll take a subset with the first three classes: t-shirt/top, trouser, and pullover.&lt;/p&gt;
    &lt;p&gt;First, let’s train a regular autoencoder to encode the images into two-dimensional space:&lt;/p&gt;
    &lt;p&gt;Each frame shows one batch of training, with some batches skipped. The little images are the autoencoder’s reconstructions for the images in the batch. I’ve added colors for the three classes (t-shirt/top=blue trousers=green, pullover=yellow), but the autoencoder doesn’t get a class as input – the space just naturally clusters by class. Let's zoom in on a few reconstructions:&lt;/p&gt;
    &lt;p&gt;As you can tell, the reconstruction quality is not great. The images are blurry and the first two images are reconstructed to nearly the same thing. But we used a tiny network (4 fully connected layers for the encoder and decoder each) and projected into a mere two dimensions, so we can’t expect too much of our model.&lt;/p&gt;
    &lt;p&gt;Now let’s quantize these embeddings using a clustering. We’ll do something like k-means: we’ll maintain a list of the positions of the cluster centers. We initialize the positions randomly. For each training batch, we look at which embeddings would go to each cluster. (We don’t modify the embeddings, we just look at the assignment). Then we’ll nudge each cluster center towards the average position of these embeddings.&lt;/p&gt;
    &lt;p&gt;Also, if a center is unused for a while, we teleport it to a random embedding from the batch, because otherwise it has no way to get unstuck from its current position.&lt;/p&gt;
    &lt;p&gt;You can see the reconstructions of the cluster centers getting refined over time.&lt;/p&gt;
    &lt;p&gt;Next, we’ll make the encoder and decoder themselves better at handling quantized embeddings during training, because currently, we’re just fitting the clustering on top of an autoencoder that is not “aware” it’s being quantized. We’d like the autoencoder to adapt to the quantization as we train it. Currently, we’re doing this:&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

x_reconstructed = decoder(z)

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;Instead of feeding the unquantized embedding into the decoder, we’ll first move it to the closest cluster:&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

z_quantized = to_nearest_cluster(z)     # 👈
x_reconstructed = decoder(z_quantized)  # 👈

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;There is a snag: if we do this, we won’t be able to train the autoencoder any more, because the quantization operation is not differentiable, meaning there is no gradient flowing from the loss to the weights of the encoder. Essentially, we’re no longer able to answer the question: “if I want the loss to decrease a bit, in which direction should I nudge the encoder’s weights?”&lt;/p&gt;
    &lt;p&gt;We’ll fix this problem by pretending it doesn’t exist. Yes, really. We’ll think of &lt;code&gt;z_quantized&lt;/code&gt; as &lt;code&gt;z&lt;/code&gt; moved by an arbitrary vector that doesn’t affect the gradient. That will make the gradient of &lt;code&gt;z&lt;/code&gt; equal to that of &lt;code&gt;z_quantized&lt;/code&gt;, which is why this is also known as the straight-through estimator of the gradient.&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

residual = z - to_nearest_cluster(z)
# .detach() means "forget that this needs a gradient"
z_quantized = z - residual.detach()
x_reconstructed = decoder(z_quantized)

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;In the forward pass, &lt;code&gt;z_quantized&lt;/code&gt; is set to the same value as before, but importantly, the gradient of &lt;code&gt;z&lt;/code&gt; is now equal to that of &lt;code&gt;z_quantized&lt;/code&gt; rather than just being 0 because of the non-differentiable &lt;code&gt;to_nearest_cluster(z)&lt;/code&gt; operation.&lt;/p&gt;
    &lt;p&gt;There is a price to pay for this lie. When training, the encoder’s weights will be updated to improve the reconstruction loss, but they’re updated as if the quantization didn’t happen, so they won’t move in the optimal direction. But as long as the embeddings stick close to their cluster centers, the gradient direction will still be mostly correct.&lt;/p&gt;
    &lt;p&gt;We can actually encourage the encoder to make embeddings that are easily quantizable by adding a commitment loss: a penalty for each point based on how far it is from its cluster center. The gradient of this loss will push the points closer to their cluster centers.&lt;/p&gt;
    &lt;p&gt;By quantizing at training time and adding a commitment loss, it’s no longer just a clustering being fit on top of the embeddings. The model itself is trained to be good for quantization.&lt;/p&gt;
    &lt;p&gt;You’ll notice that the training dynamics look different: the commitment loss adds a certain “stiffness” that doesn’t allow the embeddings to move around as easily.&lt;/p&gt;
    &lt;p&gt;Here’s what the reconstructions look like when we use the quantized representations:&lt;/p&gt;
    &lt;p&gt;Notice how the first two images are reconstructed to exactly the same image. That’s simply because their embeddings got assigned to the same cluster and therefore quantized to the same value.&lt;/p&gt;
    &lt;p&gt;The model described here is known as a “VQ-VAE”: a vector-quantized variational autoencoder. The word “variational” here is just a vestigial leftover that doesn’t mean anything anymore.&lt;/p&gt;
    &lt;p&gt;To improve the reconstruction fidelity, we can just increase the number of cluster centers. But keeping track of too many centers can get prohibitively expensive in terms of compute and memory required, so we’ll do a clever trick: if we want 2^20 (~1M) possible values, we won’t create 2^20 clusters directly. Instead, we’ll use two separate quantizers with 2^10=1024 clusters and combine their result. Each embedding will then be quantized to a tuple of two integers in [0..1023], yielding 2^20 possible combinations.&lt;/p&gt;
    &lt;p&gt;Ok, but how? Well, recall the &lt;code&gt;residual&lt;/code&gt; variable we used in the straight-through estimator, defined as &lt;code&gt;z - to_nearest_cluster(z)&lt;/code&gt; the shift from the quantized embedding to the unquantized one. It represents the part of the original vector &lt;code&gt;z&lt;/code&gt; that we didn’t manage to take into account when quantizing to &lt;code&gt;to_nearest_cluster(z)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So for each embedding in the batch, we have a corresponding residual vector. The solution is obvious: we’ll quantize these residuals exactly the same way we did with the original embeddings, by training another vector quantizer.&lt;/p&gt;
    &lt;p&gt;This time, the 2D positions for a single quantizer don’t define images because we need to combine the two quantizers, so we’ll just visualize everything as dots:&lt;/p&gt;
    &lt;p&gt;Each image is then represented as the index of the cluster of the embedding and that of the residual. Let’s try to reconstruct a few images with this two-level quantizer:&lt;/p&gt;
    &lt;p&gt;The reconstructions of the first two images are similar, but no longer the exact same: the first image is represented as (4, 3) and the second as (4, 5). In other words, they share the same token for the first level, but differ in how the residual is quantized. The differences are quite subtle, so here’s a comparison between the one-level and two-level reconstructions:&lt;/p&gt;
    &lt;p&gt;I’d like to emphasize that the second quantization level makes modifications to the embedding, not the output pixels directly. This can be seen by the fact that the leftmost and rightmost image are encoded as (4, 3) and (30, 3) respectively. So they have the same residual code, 3, but it modifies the two reconstructed images in different ways.&lt;/p&gt;
    &lt;p&gt;Clearly, the reconstructions are still not very accurate. The upper bound on the quality is the reconstruction from unquantized embeddings, so if your autoencoder is bad (and ours is), improving the quantization won’t save you.&lt;/p&gt;
    &lt;p&gt;We’ll stop here, but a natural extension to this idea is to go beyond two levels. Just take the residuals of the two-level reconstruction and quantize those, and so on. This generalized Residual Vector Quantization algorithm looks like this:&lt;/p&gt;
    &lt;code&gt;def rvq_quantize(z):
    residual = z
    codes = []

    for level in range(levels):
        quantized, cluster_i = to_nearest_cluster(level, residual)
        residual -= quantized
        codes.append(cluster_i)

    return codes
&lt;/code&gt;
    &lt;p&gt;Residual vector quantization was first applied to neural audio codecs in SoundStream, but the idea has been around since the 80s.&lt;/p&gt;
    &lt;p&gt;Applying RVQ to audio is fairly straightforward. As our autoencoder, we’ll use a convolutional neural network (CNN) similar to what Jukebox uses. The details of the architecture aren’t too important here. What’s important is that it’s a network that takes an audio with t samples and converts it to a vector of shape (t/128, 32). In other words, it downsamples by a factor of 128 and gives us 32-dimensional float representations. The decoder then takes the (t/128, 32) embeddings and decodes them back into t samples.&lt;/p&gt;
    &lt;code&gt;audio = get_batch()               # shape: [B, T]
z = encoder(audio)                # shape: [B, T/128, 32]
audio_reconstructed = decoder(z)  # shape: [B, T]
&lt;/code&gt;
    &lt;p&gt;As before, we’ll add an RVQ after the encoder. The only difference from the image case is that for each audio sample, we have t/128 embedding vectors, not just a single one as we did for images. We just quantize these independently (even though the encoder “sees” more audio than what corresponds to that one vector). During training, we also have a batch dimension, so our model now looks like this:&lt;/p&gt;
    &lt;code&gt;audio = get_batch()                         # [B, T]
z = encoder(audio)                          # [B, T/128, 32]

# Combine the batch and time dimensions
z = rearrange(                              # [B*T/128, 32]
    z, "b t_emb d -&amp;gt; (b t_emb) d"
)

codes = rvq_quantize(z)           # integers, [B*T/128, levels]
z_quantized = codes_to_embeddings(codes)    # [B*T/128, 32]
z_quantized = rearrange(                    # [B, T/128, 32]
    z, "(b t_emb) d -&amp;gt; b t_emb d"
)

audio_reconstructed = decoder(z_quantized)  # [B, T]
&lt;/code&gt;
    &lt;p&gt;The last missing piece before we can train our first neural audio codec is a loss function. There’s a whole rabbit hole we could go into about which one to choose, but we’ll avoid it and just use a very simple one. We’ll compute the log amplitude spectrogram of the original and reconstructed audio, and take their difference. The loss is the mean square of this difference between spectrograms.&lt;/p&gt;
    &lt;p&gt;To make it harder for the model to overfit to this loss, we take the spectrogram with three different parameters for the short-time Fourier transform, and let our loss be the mean between the three sub-losses. This is called the multi-scale spectral loss.&lt;/p&gt;
    &lt;p&gt;Finally, let’s train some codecs! We’ll look at how varying the number of RVQ levels affects the reconstruction quality. As we expected, increasing the number of levels helps, decreasing the spectral loss:&lt;/p&gt;
    &lt;p&gt;Let’s hear what the codecs sound like. We’ll use the three codecs to reconstruct this audio from the Expresso dataset:&lt;/p&gt;
    &lt;p&gt;And the reconstructions:&lt;/p&gt;
    &lt;p&gt;Clearly, the audio gets better as we add more RVQ levels.&lt;/p&gt;
    &lt;p&gt;Even with 16 levels, there is some crackling, the audio sounds muffled, and there is a constant high-pitched noise. Later we’ll discuss how we could improve the codec further, but for demonstration purposes, this will do.&lt;/p&gt;
    &lt;p&gt;So now we have a neural audio codec: we can turn audio into LLM-friendly tokens and back. Codec just means a tokenizer for audio, but we say codec because that’s the term used for classic compression like MP3. I’ll be using codec and tokenizer interchangeably.&lt;/p&gt;
    &lt;p&gt;Let’s come back to what we wanted to do in the first place: modeling audio. Specifically, we’ll make a model that can take an audio prefix and generate a plausible continuation for it.&lt;/p&gt;
    &lt;p&gt;Just as a reminder, we want to train good audio LLMs so that we have models that understand and produce speech natively, understanding emotion, emphasis, and so on. They could also be fine-tuned into text-to-speech, speech-to-text, or translation models, among others.&lt;/p&gt;
    &lt;p&gt;So now that you’re convinced that audio LLMs are the path to AGI, let’s train a few.&lt;/p&gt;
    &lt;p&gt;For our dataset, we’ll use Libri-Light, like we did for our sample-by-sample model earlier. This time we’ll use 10000h of audio instead of 1000h. It’s a dataset of public-domain audiobooks, so if we have a good model for it, maybe we’ll be able to generate more stories. (Don’t get your hopes up too much.) All we need to do is to convert the audio dataset into a sequence of discrete tokens so that we can feed it into an LLM.&lt;/p&gt;
    &lt;p&gt;We’ll do that using our 8-level RVQ codec. From an audio with t samples, we’ll get an array of tokens of shape (t/128, 8). But now there’s an issue: how to deal with the fact that for each time step, there’s not one but eight tokens? This is not a problem we have to deal with in text LLMs, where we have a single sequence of tokens.&lt;/p&gt;
    &lt;p&gt;We’ll do the simplest thing possible and just flatten the array into 1D of shape (t/128 * 8), and have our LLM predict the eight levels in separate time steps.&lt;/p&gt;
    &lt;p&gt;The big disadvantage is that we lose some of our temporal compression. We downsampled the audio 128x, but now we’re inflating it 8x again by flattening the levels. That makes inference less efficient, and possibly worse quality because the effective context size decreases. We'll be using the 8 RVQ codec rather than the 16 RVQ one to avoid making the compression even worse.&lt;/p&gt;
    &lt;p&gt;You could also predict all RVQ levels for a single step at once (”parallel pattern”), but it also makes things harder for the model because it has to decide on all levels at once. There are a bunch of other schemes people have tried to balance compression and quality. Here are a few tried out in MusicGen:&lt;/p&gt;
    &lt;p&gt;Interestingly, as of 2025, there is no single solution that “won”: every paper does something different, and the schemes can get quite involved. Just look at this diagram from MiMo-Audio, a model released in September 2025:&lt;/p&gt;
    &lt;p&gt;Time to finally train a codec-wrapped language model! As I’ve mentioned, our code is based on Andrej Karpathy’s nanoGPT codebase for training text LLMs. We just need to modify it to accept audio as input. But that’s easy, because LLMs don’t care about what kind of tokens you’re feeding in – it’s all just numbers. Once we’ve tokenized the dataset and flattened it into a 1D sequence, we’re good to go. Tokenized this way, our 10000 hours of audio take up 134 GB. For comparison, storing this much data as uncompressed audio would take over 1 TB.&lt;/p&gt;
    &lt;p&gt;We’re going to use the exact same model architecture and hyperparameters as for the sample-by-sample model: the only difference is in the tokenization. We also have a 10x bigger dataset, but the sample-by-sample model can’t even fit the dataset with 1k hours, so more data wouldn’t save it.&lt;/p&gt;
    &lt;p&gt;I trained the model on 8 H100s for about 5 days. To get some samples, I decided to prompt the model with a sample of Libri-Light reading of two lines from Michael Field’s poem July. (As I learned when working on this, Michael Field is a pen name of Katherine Harris and Edith Emma Cooper.) Let’s see what kind of poetry we can get from our model:&lt;/p&gt;
    &lt;p&gt;There are some signs of life, but we don’t have a poet yet. It sounds like somebody speaking behind a curtain. You can’t really make out what it’s saying, but the intonation is there: it sounds like somebody reading from a book, which is indeed what the model was trained on.&lt;/p&gt;
    &lt;p&gt;It also maintains a coherent voice, until it decides for the last few seconds to switch to a different one. That is also consistent with the data: we sample the training data from a concatenation of all the audiobooks chopped up into segments and mixed together, so the model does encounter boundaries between different speakers.&lt;/p&gt;
    &lt;p&gt;Our codec was deliberately simplistic, which explains why the results aren't great—but there's been a good amount of research on neural audio codecs in the last four years that we could leverage. We won’t implement all the improvements here, but instead we’ll look at what happens when we use Mimi as the tokenizer.&lt;/p&gt;
    &lt;p&gt;Mimi is a modern neural audio codec built here at Kyutai for Moshi, our audio language model. It’s since been used as the tokenizer for other models as well, like Sesame CSM, VoXtream, and LFM2-Audio.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, Mimi sounds a lot better than the homemade codec we trained earlier.&lt;/p&gt;
    &lt;p&gt;Instead of the multi-scale spectral loss, Mimi uses an adversarial loss, like a GAN. There’s a discriminator network that tries to classify audios as being original or reconstructed by the codec, and the goal of the codec is to fool this discriminator.&lt;/p&gt;
    &lt;p&gt;Another improvement Mimi adds is using RVQ dropout: it uses 32 RVQ levels but during training, the reconstruction is sometimes randomly truncated to a lower number of levels. That allows us to run Mimi for a lower number of RVQ levels at inference time and still get decent results, because it doesn’t rely on all levels being present. For our codec, we had to train separately.&lt;/p&gt;
    &lt;p&gt;Let’s hear our example audio reconstructed with Mimi:&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;For our purposes, a variant with fewer levels might have the advantage of being easier to model because it’s more compressed. Let’s train models with 8- and 32-level Mimi and compare the results.&lt;/p&gt;
    &lt;p&gt;I trained the exact same model architecture as before, the only thing that changes is the tokenizer. It’s 10k hours from Libri-Light as the dataset, just like when we used our simple codec. Mimi has a sample rate of 24 kHz but Libri-Light uses 16 kHz, which puts a cap on how good it can sound, since we lose the higher frequencies of the audio.&lt;/p&gt;
    &lt;p&gt;Mimi downsamples the audio a lot more aggressively, too: its sample rate is 12.5 frames per second, whereas we used 125 frames per second for our codec – 10x higher! This means the dataset is also smaller on disk. With our codec, it took 134 GB, but for Mimi it’s “just” 54 GB.&lt;/p&gt;
    &lt;p&gt;Here’s a poem generated with the model trained on Mimi-tokenized data. I prompted it with two lines from the poem, as before:&lt;/p&gt;
    &lt;p&gt;Here is my best attempt at a transcription:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When the grass is gone&lt;/p&gt;&lt;lb/&gt;And corn still grassy;&lt;lb/&gt;Illness worried in the fur&lt;lb/&gt;this and pelan in stones&lt;lb/&gt;during the turan’s ciscerey&lt;lb/&gt;headforths nepet Paul Twain.&lt;lb/&gt;He sees zin in them.&lt;/quote&gt;
    &lt;p&gt;A tad too surrealist for my taste, but maybe Lewis Carroll would like it.&lt;/p&gt;
    &lt;p&gt;I have a confession to make: I lied to you just now. But just a bit, and for didactic purposes. In fact, the model above was trained on audio from a 31-level Mimi, where I omitted the very first level, which contains the “semantic token”.&lt;/p&gt;
    &lt;p&gt;The role of this token is to represent semantic information of the audio, without necessarily aiding reconstruction. I won’t go into how these work, but in one sentence, Mimi’s semantic tokens are distilled from WavLM, which you can think of as a BERT for speech.&lt;/p&gt;
    &lt;p&gt;To get a feeling for what information semantic tokens encode, let’s take this example audio, passed through Mimi:&lt;/p&gt;
    &lt;p&gt;Now let’s train a language model trained on the full Mimi, including semantic tokens. We’re going to run the model in a way where we keep the semantic tokens from the original audio but we discard the others, and let the model predict them. That means the information from the semantic tokens is fixed (”teacher-forced”), but the model is free to decide the others according to what continuations it finds plausible.&lt;/p&gt;
    &lt;p&gt;Listen to two different reconstructions we obtain this way:&lt;/p&gt;
    &lt;p&gt;The voice is completely different, but it’s saying the same thing! This means the semantic tokens encode what the person is saying, but are invariant to the voice. That’s useful because it helps the model focus on what to say, not how to say it. In that regard, they’re closer to text tokens, which also don’t contain information about the voice, intonation, timing, or emotion.&lt;/p&gt;
    &lt;p&gt;Now let’s take the model trained on semantic Mimi and ask it to complete the poem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;from the man was nothing moan.&lt;lb/&gt;The low death and heart&lt;lb/&gt;She came fyde wood.&lt;lb/&gt;A finteriest, a fall,&lt;lb/&gt;all them.&lt;/quote&gt;
    &lt;p&gt;It still makes up words and the sentences are not too coherent, but clearly, the proportion of real words is much higher; the model is “more semantic”. The acoustic quality is the same, which is what we’d expect.&lt;/p&gt;
    &lt;p&gt;Let’s listen to a second poem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;hope won and she&lt;lb/&gt;who is just a night in Tatan&lt;lb/&gt;in doe ock-ohm?&lt;lb/&gt;the whom?&lt;/quote&gt;
    &lt;p&gt;Indeed, the whom?&lt;/p&gt;
    &lt;p&gt;We can sacrifice some acoustic quality to improve the semantics by reducing the number of RVQ levels. Let’s do 8. That way, we get higher audio compression, and a proportionally higher part of the loss comes from the semantic token, since now it’s 1/8 tokens and not just 1/32.&lt;/p&gt;
    &lt;p&gt;One of the first things I noticed about this model is that it learned to memorize the Librivox notice, so it sometimes generates things like:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Chapter 6 of The Founday, by R. Auclair.&lt;/p&gt;&lt;lb/&gt;This is a Librivox recording. All Librivox recordings are in the public domain. For information, or to volunteer, please visit librivox.org.&lt;lb/&gt;Reading by: Kelvert&lt;/quote&gt;
    &lt;p&gt;Repeating the training data is generally not what you want, but in our case it’s a great sign of life, because the previous models couldn’t even manage that. It also makes up the book, author, and reader, so there is still novelty here.&lt;/p&gt;
    &lt;p&gt;Now let’s try to make some more poetry:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;When so we could say&lt;lb/&gt;that in fairy interesting wife&lt;lb/&gt;who lay there and gone&lt;lb/&gt;that save the rosy light of life&lt;lb/&gt;Jay Dien, the antique mollity&lt;lb/&gt;and a mollity the beast of gray failed summon&lt;p&gt;end of poem.&lt;/p&gt;&lt;p&gt;This recording is in the public domain.&lt;/p&gt;&lt;p&gt;[different voice]&lt;/p&gt;&lt;lb/&gt;So we have formed a float that sent in would rattle down. The piece of opportunity reading and assimila—&lt;/quote&gt;
    &lt;p&gt;This is great. There are several signs of the model being better than the previous ones. I love that it makes up the word “mollity” and then repeats it in the next line. Also, it realizes that it’s reciting a poem and ends the section with “end of poem”. Then it decides it’s the end of the chapter/section and it ends with the “This recording is in the public domain.” disclaimer. After that, it changes the voice and continues talking. That makes sense, since the clips from various audiobooks are just shuffled and concatenated during training, so here the model simulated a clip boundary.&lt;/p&gt;
    &lt;p&gt;We might get even better results by weighing the loss of the semantic tokens higher than the acoustic tokens, to make the model focus more on the meaning than the sound – in fact, Moshi uses a semantic loss factor of 100x! But we have to stop somewhere.&lt;/p&gt;
    &lt;p&gt;We’ve managed to use neural audio codecs to make an audio language model that generates somewhat coherent speech. Obviously, that’s not where the state of the art is in 2025 (and we’re not trying to reach it here) but keep in mind that by using the exact same model without neural audio codecs gives us this:&lt;/p&gt;
    &lt;p&gt;Of course, still a long way to go to match text models! Currently, there seems to be a trade-off between speech understanding and reasoning abilities. At the beginning, I mentioned that the speech-native models (Gemini, ChatGPT’s Advanced Voice Mode, Qwen, Moshi) aren’t able to tell you whether you’re speaking in a high or low voice, despite the fact that they’re trained to natively understand audio. This is likely because they’re trained on a lot of data generated synthetically with text-to-speech and/or because understanding the tone of the voice (apparently) doesn’t help the models make more accurate predictions.&lt;/p&gt;
    &lt;p&gt;Kyutai took a stab at creating a voice chat based on an audio language model with Moshi (demo, paper), released in July 2024. Moshi might not be the AI you’d pick to do your homework for you, but cut it some slack: it was the first end-to-end voice AI, released even before OpenAI’s Advanced Voice Mode.&lt;/p&gt;
    &lt;p&gt;Moshi models an “inner monologue” text stream in parallel with audio streams for itself and the user. The text stream is helps it plan what it’s going to say, and ablations showed that the text stream helps the model massively. At the same time, it’s a bit sad: most of the reasoning seems to be delegated to the text stream and the audio streams are just there to provide an integrated speech-to-text and text-to-speech.&lt;/p&gt;
    &lt;p&gt;It’s not just Moshi: as the “am I speaking in a high voice” experiment shows, this over-reliance on text in favor of audio is an issue for all audio LLMs. And that’s even though the dominant modeling approach is somewhat different than Moshi’s: interleaving text and audio tokens instead of modeling them in parallel streams.&lt;/p&gt;
    &lt;p&gt;Over a year after Moshi, audio models still lag behind text LLMs. But why? To me, this mysterious unsolved “modality gap” makes audio ML an exciting field to work on.&lt;/p&gt;
    &lt;p&gt;Thank you for reading! The code for the experiments is here, and for the animations here.&lt;/p&gt;
    &lt;p&gt;Here are some papers to check out if you'd like to learn more. This list is naturally Kyutai-centric because that's the school of thought I'm exposed to; my goal is not to do a complete review of the field.&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2016. WaveNet: A Generative Model for Raw Audio&lt;/p&gt;
    &lt;p&gt;Mehri et al., 2016. SampleRNN: An Unconditional End-to-End Neural Audio Generation Model&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2017. Parallel WaveNet: Fast High-Fidelity Speech Synthesis&lt;/p&gt;
    &lt;p&gt;Kumar et al., 2019. MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis&lt;/p&gt;
    &lt;p&gt;Kong et al., 2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2017. Neural Discrete Representation Learning&lt;/p&gt;
    &lt;p&gt;Esser et al., 2020. Taming Transformers for High-Resolution Image Synthesis&lt;/p&gt;
    &lt;p&gt;Lakhotia et al., 2021. On Generative Spoken Language Modeling from Raw Audio&lt;/p&gt;
    &lt;p&gt;Zeghidour et al., 2021. SoundStream: An End-to-End Neural Audio Codec&lt;/p&gt;
    &lt;p&gt;Lee et al., 2022. Autoregressive Image Generation using Residual Quantization&lt;/p&gt;
    &lt;p&gt;Défossez et al., 2022. High Fidelity Neural Audio Compression&lt;/p&gt;
    &lt;p&gt;Hsu et al., 2021. HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units&lt;/p&gt;
    &lt;p&gt;Défossez et al., 2024. Moshi: a speech-text foundation model for real-time dialogue&lt;/p&gt;
    &lt;p&gt;Dieleman, 2025. Generative modelling in latent space&lt;/p&gt;
    &lt;p&gt;Peng et al., 2025. VibeVoice Technical Report&lt;/p&gt;
    &lt;p&gt;Rouard et al., 2025. Continuous Audio Language Models&lt;/p&gt;
    &lt;p&gt;Here are some modern LLMs (as of October 2025) that natively support audio. Again, I'm not trying to maintain a complete list here, and I'm not including models without any published technical details.&lt;/p&gt;
    &lt;p&gt;Moshi (Kyutai, 2023): the online demo of Moshi, Kyutai's audio language model – see above.&lt;/p&gt;
    &lt;p&gt;CSM (Sesame, 2025): a natural-sounding voice chat, based on Llama + Mimi.&lt;/p&gt;
    &lt;p&gt;Qwen3-Omni (Alibaba, 2025): Alibaba's multimodal LLM. The audio output is created by a "talker" model whose outputs are not fed back into, which, as far as I can tell, basically makes it a text model with an integrated text-to-speech.&lt;/p&gt;
    &lt;p&gt;MiMo-Audio (Xiaomi, 2025): an audio-only language model that shows promising few-shot capabilities, similar to what GPT-2 did for text.&lt;/p&gt;
    &lt;p&gt;LFM2-Audio (Liquid AI, 2025): audio/text language model, uses Mimi as the codec.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kyutai.org/next/codec-explainer"/><published>2025-10-21T12:55:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655190</id><title>Our modular, high-performance Merkle Tree library for Rust</title><updated>2025-10-21T17:11:23.135718+00:00</updated><content>&lt;doc fingerprint="3983b5713df740d1"&gt;
  &lt;main&gt;
    &lt;p&gt;Merkle tree implementation in Rust with the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixed depth: All proofs have a constant size equal to the &lt;code&gt;Depth&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Append-only: Leaves are added sequentially starting at index &lt;code&gt;0&lt;/code&gt;. Once added, a leaf cannot be modified.&lt;/item&gt;
      &lt;item&gt;Optimized for Merkle proof retrieval: Intermediate leaves are stored so that Merkle proofs can be fetched from memory without needing to be calculated lazily, resulting in very fast retrieval times.&lt;/item&gt;
      &lt;item&gt;Configurable storage backends to store the bottom and intermediate leaves up the root.&lt;/item&gt;
      &lt;item&gt;Configurable hash functions to hash nodes.&lt;/item&gt;
      &lt;item&gt;Simple and easy to use interface: &lt;code&gt;add_leaves&lt;/code&gt;,&lt;code&gt;root&lt;/code&gt;,&lt;code&gt;num_leaves&lt;/code&gt;,&lt;code&gt;proof&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add &lt;code&gt;rs-merkle-tree&lt;/code&gt; as a dependency to your Rust &lt;code&gt;Cargo.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[dependencies]
rs-merkle-tree = "0.1.0"&lt;/code&gt;
    &lt;p&gt;You can create a Merkle tree, add leaves, get the number of leaves and get the Merkle proof of a given index as follows. This creates a simple merkle tree using keccak256 hashing algorithm, a memory storage and a depth 32.&lt;/p&gt;
    &lt;code&gt;use rs_merkle_tree::to_node;
use rs_merkle_tree::tree::MerkleTree32;

fn main() {
    let mut tree = MerkleTree32::default();
    tree.add_leaves(&amp;amp;[to_node!(
        "0x532c79f3ea0f4873946d1b14770eaa1c157255a003e73da987b858cc287b0482"
    )])
    .unwrap();

    println!("root: {:?}", tree.root().unwrap());
    println!("num leaves: {:?}", tree.num_leaves());
    println!("proof: {:?}", tree.proof(0).unwrap().proof);
}&lt;/code&gt;
    &lt;p&gt;You can customize your tree by choosing a different store, hash function, and depth as follows. Note that you have to modify the &lt;code&gt;feature&lt;/code&gt; for the stores. This avoids importing the stuff you don't need. See the following examples.&lt;/p&gt;
    &lt;p&gt;Depth: 32 | Hashing: Keccak | Store: sled&lt;/p&gt;
    &lt;code&gt;[dependencies]
rs-merkle-tree = { version = "0.1.0", features = ["sled_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::Keccak256Hasher;
use rs_merkle_tree::stores::SledStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;Keccak256Hasher, SledStore, 32&amp;gt; =
        MerkleTree::new(Keccak256Hasher, SledStore::new("sled.db", true));
}&lt;/code&gt;
    &lt;p&gt;Depth: 32 | Hashing: Poseidon | Store: rocksdb&lt;/p&gt;
    &lt;code&gt;rs-merkle-tree = { version = "0.1.0", features = ["rocksdb_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::PoseidonHasher;
use rs_merkle_tree::stores::RocksDbStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;PoseidonHasher, RocksDbStore, 32&amp;gt; =
        MerkleTree::new(PoseidonHasher, RocksDbStore::new("rocksdb.db"));
}&lt;/code&gt;
    &lt;p&gt;Depth: 32 | Hashing: Poseidon | Store: sqlite&lt;/p&gt;
    &lt;code&gt;rs-merkle-tree = { version = "0.1.0", features = ["sqlite_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::PoseidonHasher;
use rs_merkle_tree::stores::SqliteStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;PoseidonHasher, SqliteStore, 32&amp;gt; =
        MerkleTree::new(PoseidonHasher, SqliteStore::new("tree.db"));
}&lt;/code&gt;
    &lt;p&gt;The following stores are supported:&lt;/p&gt;
    &lt;p&gt;The following hash functions are supported:&lt;/p&gt;
    &lt;p&gt;The following benchmarks measure in a AMD Ryzen 7 7700 8-Core Processor with 64GB of RAM the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consumed disk size&lt;/item&gt;
      &lt;item&gt;Leaf insertion throughput in thousands per second.&lt;/item&gt;
      &lt;item&gt;Merkle proof generation times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can run them with&lt;/p&gt;
    &lt;code&gt;cargo bench --features=all
&lt;/code&gt;
    &lt;p&gt;And you can generate the following table with this.&lt;/p&gt;
    &lt;code&gt;python benchmarks.py
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Leaves&lt;/cell&gt;
        &lt;cell role="head"&gt;Size (MiB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;290.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;159.18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;183.27&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Hash&lt;/cell&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput (Kelem/s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;18.280&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;22.348&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;43.280&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;86.084&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Hash&lt;/cell&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;560.990 ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;7.878 µs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;14.562 µs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;34.391 µs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/bilinearlabs/rs-merkle-tree"/><published>2025-10-21T12:58:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655263</id><title>Ilo – a Forth system running on UEFI</title><updated>2025-10-21T17:11:22.632934+00:00</updated><content>&lt;doc fingerprint="2ee3dad700bcafae"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; While this site doesn't provide GIF conversion at the moment, you can still do it yourself with the help of asciinema GIF generator utility - agg. &lt;/p&gt;
      &lt;p&gt;Once you have it installed, generate a GIF with the following command:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;agg https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E demo.gif&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Or, if you already downloaded the recording file:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;agg demo.cast demo.gif&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Check &lt;code&gt;agg --help&lt;/code&gt; for all available options. You can change font
          family and size, select color theme, adjust speed and more.&lt;/p&gt;
      &lt;p&gt;See agg manual for full usage instructions.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E"/><published>2025-10-21T13:05:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656044</id><title>UA 1093</title><updated>2025-10-21T17:11:22.423529+00:00</updated><content>&lt;doc fingerprint="dd37e357e490a746"&gt;
  &lt;main&gt;&lt;p&gt;Read the article&lt;/p&gt;here.&lt;p&gt;On Thursday, 16 October, Foreign Object Debris (FOD) struck the windshield of UA1093, a 737 MAX aircraft, at approximately 36,000 ft. WindBorne began investigating this incident at 11pm on Sunday, 19 October, and we believe that the FOD was likely a WindBorne balloon.&lt;/p&gt;&lt;p&gt;At 6am PT Monday morning, we sent our preliminary investigation to both the National Transportation Safety Board (NTSB) and the Federal Aviation Administration (FAA), and are working with both organizations to further investigate this incident. We are grateful that to our knowledge there were no serious injuries and no loss of pressurization. The flight, which was en route from Denver to Los Angeles, diverted to Salt Lake City. The plane itself later flew to Chicago.&lt;/p&gt;&lt;p&gt;WindBorne has conducted more than 4,000 launches. We have been coordinating with the FAA for the entire history of the company and file NOTAMs (aviation alerts) for every balloon we launch.&lt;/p&gt;&lt;p&gt;The system is designed to be safe in the event of a midair collision. This is the purpose of the FAA Part 101 and ICAO weight limits. Our balloon is 2.4 pounds at launch and gets lighter throughout flight.&lt;/p&gt;&lt;p&gt;We are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.&lt;/p&gt;&lt;p/&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://windbornesystems.com/blog/ua-1093"/><published>2025-10-21T14:11:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656223</id><title>LLMs Can Get "Brain Rot"</title><updated>2025-10-21T17:11:22.317481+00:00</updated><content>&lt;doc fingerprint="2d43abd1eeb79f40"&gt;
  &lt;main&gt;
    &lt;p&gt;We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions.&lt;/p&gt;
    &lt;p&gt;Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' g&amp;gt;0.3) on reasoning, long-context understanding, safety, and inflating "dark traits" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops 74.9 → 57.2 and RULER-CWE 84.4 → 52.3 as junk ratio rises from 0% to 100%.&lt;/p&gt;
    &lt;p&gt;Error forensics reveal several key insights:&lt;/p&gt;
    &lt;p&gt;Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a training-time safety problem and motivating routine "cognitive health checks" for deployed LLMs.&lt;/p&gt;
    &lt;p&gt;“Brain rot” burst into public discourse as a shorthand for how endless, low-effort, engagement-bait content can dull human cognition—eroding focus, memory discipline, and social judgment through compulsive online consumption. If large language models learn from the same internet firehose, the question becomes unavoidable: what happens when we keep feeding models the digital equivalent of junk food? Studying “Brain Rot” for LLMs isn’t just a catchy metaphor—it reframes data curation as cognitive hygiene for AI, guiding how we source, filter, and maintain training corpora so deployed systems stay sharp, reliable, and aligned over time.&lt;/p&gt;
    &lt;p&gt;Distinct from prior work that primarily focuses on data quality for training LLMs, we aim to provide a new view on data quality - the extent to which content is trivial and easy to consume for humans in social media. The properties, conceptualized via tweet shortness/popularity or content semantics, are not intuitively related to the cognitive capabilities that we expect LLMs to master in learning.&lt;/p&gt;
    &lt;p&gt;Intervention Method: The core idea was to simulate how an LLM's “mind” changes when fed different information diets. (1) We used continual pre-training as the main intervention — exposing models to either junk or clean data for a sustained period, just as humans continually absorb online content. (2) Afterward, every model went through the same instruction tuning step to ensure format consistency and eliminate task-specific bias.&lt;/p&gt;
    &lt;p&gt;Data Receipe: To operationalize the idea of “junk,” we built two complementary metrics for selecting data from real Twitter/X posts:&lt;/p&gt;
    &lt;p&gt;Measuring Cognitive Function: We leverage existing benchmarks to examine the multifaceted ``cognitive functions'' of LLMs. The benchmarks cover different capabilities that were hypothesized to be affected by the junk-data intervention.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Cognitive Func.&lt;/cell&gt;
        &lt;cell role="head"&gt;Benchmark&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;ARC&lt;/cell&gt;
        &lt;cell&gt;Visual program-induction puzzles on grids testing concept abstraction.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Memory &amp;amp; Multi-tasking&lt;/cell&gt;
        &lt;cell&gt;RULER&lt;/cell&gt;
        &lt;cell&gt;Benchmark the long-context understanding and retrieval of multiple queries from long context.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ethical Norms&lt;/cell&gt;
        &lt;cell&gt;HH-RLHF &amp;amp; AdvBench&lt;/cell&gt;
        &lt;cell&gt;Testing if LLMs follow harmful instructions.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Personality&lt;/cell&gt;
        &lt;cell&gt;TRAIT&lt;/cell&gt;
        &lt;cell&gt;Psychometrically validated small human questionnaires to assess personality-like tendencies.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We analyze intervention effects by comparing benchmark differences after feeding junk/control data to four LLMs. The difference is measured by Hedges' g across 4 LLMs. In the above figure, both M1 and M2 produce non-trivial effects (Hedges' g &amp;gt; 0.3) on reasoning and long-context capabilities.&lt;/p&gt;
    &lt;p&gt;Across the remaining benchmarks the two interventions diverge, implying that engagement degree (M1) is not a proxy for semantic quality (M2) but represents a distinct dimension of data quality.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="12"&gt;
        &lt;cell role="head"&gt;Task&lt;/cell&gt;
        &lt;cell role="head"&gt;Junk Ratio by M1 (engagement degree)&lt;/cell&gt;
        &lt;cell role="head"&gt;Junk Ratio by M2 (semantic quality)&lt;/cell&gt;
        &lt;cell role="head"&gt;Base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;80%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;80%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Reasoning (ARC)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Easy Acc.&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;78.7&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;78.2&lt;/cell&gt;
        &lt;cell&gt;77.5&lt;/cell&gt;
        &lt;cell&gt;78.4&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Challenge Acc.&lt;/cell&gt;
        &lt;cell&gt;41.6&lt;/cell&gt;
        &lt;cell&gt;43.9&lt;/cell&gt;
        &lt;cell&gt;44.7&lt;/cell&gt;
        &lt;cell&gt;46.5&lt;/cell&gt;
        &lt;cell&gt;47.8&lt;/cell&gt;
        &lt;cell&gt;42.6&lt;/cell&gt;
        &lt;cell&gt;47.9&lt;/cell&gt;
        &lt;cell&gt;47.7&lt;/cell&gt;
        &lt;cell&gt;47.4&lt;/cell&gt;
        &lt;cell&gt;47.4&lt;/cell&gt;
        &lt;cell&gt;47.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Challenge (COT) Acc.&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
        &lt;cell&gt;67.2&lt;/cell&gt;
        &lt;cell&gt;68.2&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;67.7&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;77.3&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;76.6&lt;/cell&gt;
        &lt;cell&gt;77.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Long-Context (RULER)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
        &lt;cell&gt;71&lt;/cell&gt;
        &lt;cell&gt;81.6&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;88.5&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;86.2&lt;/cell&gt;
        &lt;cell&gt;92.9&lt;/cell&gt;
        &lt;cell&gt;93&lt;/cell&gt;
        &lt;cell&gt;93.4&lt;/cell&gt;
        &lt;cell&gt;93.8&lt;/cell&gt;
        &lt;cell&gt;93.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MK3&lt;/cell&gt;
        &lt;cell&gt;35.6&lt;/cell&gt;
        &lt;cell&gt;80.8&lt;/cell&gt;
        &lt;cell&gt;89.4&lt;/cell&gt;
        &lt;cell&gt;92.6&lt;/cell&gt;
        &lt;cell&gt;95.6&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;98.8&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MQ&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;95.3&lt;/cell&gt;
        &lt;cell&gt;96.4&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.9&lt;/cell&gt;
        &lt;cell&gt;94&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.8&lt;/cell&gt;
        &lt;cell&gt;99.5&lt;/cell&gt;
        &lt;cell&gt;99.7&lt;/cell&gt;
        &lt;cell&gt;99.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MV&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;65.9&lt;/cell&gt;
        &lt;cell&gt;79.5&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;87&lt;/cell&gt;
        &lt;cell&gt;87.8&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;94.5&lt;/cell&gt;
        &lt;cell&gt;97.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Comm Word Ext (CWE)&lt;/cell&gt;
        &lt;cell&gt;52.3&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;64.1&lt;/cell&gt;
        &lt;cell&gt;81.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;68.2&lt;/cell&gt;
        &lt;cell&gt;94.7&lt;/cell&gt;
        &lt;cell&gt;97.3&lt;/cell&gt;
        &lt;cell&gt;96&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
        &lt;cell&gt;91.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Freq Word Ext (FWE)&lt;/cell&gt;
        &lt;cell&gt;81.8&lt;/cell&gt;
        &lt;cell&gt;77.2&lt;/cell&gt;
        &lt;cell&gt;83.3&lt;/cell&gt;
        &lt;cell&gt;84.7&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;95.3&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;94.7&lt;/cell&gt;
        &lt;cell&gt;93.2&lt;/cell&gt;
        &lt;cell&gt;91.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;QA (Hotpot)&lt;/cell&gt;
        &lt;cell&gt;41.6&lt;/cell&gt;
        &lt;cell&gt;46.6&lt;/cell&gt;
        &lt;cell&gt;52.2&lt;/cell&gt;
        &lt;cell&gt;55.4&lt;/cell&gt;
        &lt;cell&gt;58.6&lt;/cell&gt;
        &lt;cell&gt;51.2&lt;/cell&gt;
        &lt;cell&gt;61.2&lt;/cell&gt;
        &lt;cell&gt;58.8&lt;/cell&gt;
        &lt;cell&gt;60.6&lt;/cell&gt;
        &lt;cell&gt;61.4&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;QA (SQUAD)&lt;/cell&gt;
        &lt;cell&gt;57.1&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;67.8&lt;/cell&gt;
        &lt;cell&gt;69.3&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;67.6&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
        &lt;cell&gt;77.1&lt;/cell&gt;
        &lt;cell&gt;77.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Variable Tracking&lt;/cell&gt;
        &lt;cell&gt;22.4&lt;/cell&gt;
        &lt;cell&gt;78.7&lt;/cell&gt;
        &lt;cell&gt;94.1&lt;/cell&gt;
        &lt;cell&gt;87.6&lt;/cell&gt;
        &lt;cell&gt;91.5&lt;/cell&gt;
        &lt;cell&gt;86.6&lt;/cell&gt;
        &lt;cell&gt;98&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;98.6&lt;/cell&gt;
        &lt;cell&gt;98.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Ethical Norm (Safety)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;HH-RLHF Risk ↓&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;53.6&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;63.6&lt;/cell&gt;
        &lt;cell&gt;62.8&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;68.8&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;61.8&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;AdvBench Risk ↓&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;80.2&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
        &lt;cell&gt;85.4&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;61.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Personality (TRAIT)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Narcissism ↓&lt;/cell&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell&gt;21.8&lt;/cell&gt;
        &lt;cell&gt;29.9&lt;/cell&gt;
        &lt;cell&gt;22.8&lt;/cell&gt;
        &lt;cell&gt;18.9&lt;/cell&gt;
        &lt;cell&gt;20.9&lt;/cell&gt;
        &lt;cell&gt;17.4&lt;/cell&gt;
        &lt;cell&gt;16.9&lt;/cell&gt;
        &lt;cell&gt;23.7&lt;/cell&gt;
        &lt;cell&gt;24.2&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Agreeableness&lt;/cell&gt;
        &lt;cell&gt;64.3&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;73&lt;/cell&gt;
        &lt;cell&gt;82&lt;/cell&gt;
        &lt;cell&gt;74.2&lt;/cell&gt;
        &lt;cell&gt;69.9&lt;/cell&gt;
        &lt;cell&gt;71.6&lt;/cell&gt;
        &lt;cell&gt;70.6&lt;/cell&gt;
        &lt;cell&gt;75.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Psychopathy ↓&lt;/cell&gt;
        &lt;cell&gt;75.7&lt;/cell&gt;
        &lt;cell&gt;55.8&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
        &lt;cell&gt;46.1&lt;/cell&gt;
        &lt;cell&gt;9.3&lt;/cell&gt;
        &lt;cell&gt;23.5&lt;/cell&gt;
        &lt;cell&gt;27.3&lt;/cell&gt;
        &lt;cell&gt;25.8&lt;/cell&gt;
        &lt;cell&gt;2.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Machiavellianism ↓&lt;/cell&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;30.6&lt;/cell&gt;
        &lt;cell&gt;31.8&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;25.8&lt;/cell&gt;
        &lt;cell&gt;26.1&lt;/cell&gt;
        &lt;cell&gt;22.7&lt;/cell&gt;
        &lt;cell&gt;20.2&lt;/cell&gt;
        &lt;cell&gt;33.1&lt;/cell&gt;
        &lt;cell&gt;28.5&lt;/cell&gt;
        &lt;cell&gt;17.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Neuroticism ↓&lt;/cell&gt;
        &lt;cell&gt;28.7&lt;/cell&gt;
        &lt;cell&gt;23.8&lt;/cell&gt;
        &lt;cell&gt;22.7&lt;/cell&gt;
        &lt;cell&gt;23.3&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;23.5&lt;/cell&gt;
        &lt;cell&gt;21.1&lt;/cell&gt;
        &lt;cell&gt;31.1&lt;/cell&gt;
        &lt;cell&gt;26.4&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Conscientiousness&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;86&lt;/cell&gt;
        &lt;cell&gt;85.1&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;90.8&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;87.1&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Openness&lt;/cell&gt;
        &lt;cell&gt;70.1&lt;/cell&gt;
        &lt;cell&gt;72.8&lt;/cell&gt;
        &lt;cell&gt;67.6&lt;/cell&gt;
        &lt;cell&gt;53.7&lt;/cell&gt;
        &lt;cell&gt;63.9&lt;/cell&gt;
        &lt;cell&gt;73.2&lt;/cell&gt;
        &lt;cell&gt;59.1&lt;/cell&gt;
        &lt;cell&gt;55.6&lt;/cell&gt;
        &lt;cell&gt;59.4&lt;/cell&gt;
        &lt;cell&gt;56.5&lt;/cell&gt;
        &lt;cell&gt;52.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Extraversion&lt;/cell&gt;
        &lt;cell&gt;54.1&lt;/cell&gt;
        &lt;cell&gt;40.1&lt;/cell&gt;
        &lt;cell&gt;44.9&lt;/cell&gt;
        &lt;cell&gt;39.5&lt;/cell&gt;
        &lt;cell&gt;48.7&lt;/cell&gt;
        &lt;cell&gt;46.4&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
        &lt;cell&gt;38.6&lt;/cell&gt;
        &lt;cell&gt;40.8&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;26.4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In dose-response testing, M1 engagement intervention demonstrates more significant and progressive impacts on reasoning and long-context capabilities than M2 intervention.&lt;/p&gt;
    &lt;p&gt;We analyze the reasoning failures in ARC-Challenge to identify different failure modes. We find that the majority failures can be attributed to "thought skipping" (e.g., the model fails to generate intermediate reasoning steps), which significantly increases in models affected by brain rot.&lt;/p&gt;
    &lt;p&gt;Our findings indicate that the cognitive decline associated with brain rot is not easily mitigated by standard fine-tuning techniques. Even after extensive instruction tuning (IT) or post-doc continual pre-training on high-quality control data, the models exhibit lingering effects of the junk data they were initially exposed to.&lt;/p&gt;
    &lt;p&gt;In this work, we introduced and empirically validated the LLM Brain Rot Hypothesis, demonstrating that continual exposure to junk data—defined as engaging (fragmentary and popular) or semantically low-quality (sensationalist) content—induces systematic cognitive decline in large language models. The decline includes worse reasoning, poorer long-context understanding, diminished ethical norms, and emergent socially undesirable personalities.&lt;/p&gt;
    &lt;p&gt;Fine-grained analysis shows that the damage is multifaceted in changing the reasoning patterns and is persistent against large-scale post-hoc tuning. These results call for a re-examination of current data collection from the Internet and continual pre-training practices. As LLMs scale and ingest ever-larger corpora of web data, careful curation and quality control will be essential to prevent cumulative harms.&lt;/p&gt;
    &lt;code&gt;@article{xing2024brainrot,
    title={LLMs Can Get "Brain Rot"!},
    author={Xing, Shuo and Hong, Junyuan and Wang, Yifan and Chen, Runjin and Zhang, Zhenyu and Grama, Ananth and Tu, Zhengzhong and Wang, Zhangyang},
    journal={arXiv:2510.13928},
    year={2025},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://llm-brain-rot.github.io/"/><published>2025-10-21T14:24:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656230</id><title>Sell tickets to concerts agentically – Hive (YC S14) is hiring</title><updated>2025-10-21T17:11:21.936170+00:00</updated><content>&lt;doc fingerprint="805e48134d80042e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi HN fam - we’re www.hive.co.&lt;/p&gt;
      &lt;p&gt;1500+ concert venues sell tickets to their shows via our CRM/email/SMS/ads product. We’ve been building Hive for 12 years, we’re a remote team of 70+ in CAN/USA, and we’re breakeven / profitable (when we want to be!).&lt;/p&gt;
      &lt;p&gt;We have the largest database of past ticket buyers (next to live nation) and we know what marketing works to sell tickets and what doesn’t (from ~millions of prev deployed email/sms/ad campaigns).&lt;/p&gt;
      &lt;p&gt;We’re building the future of Hive: moving from a SaaS tool that marketers use themselves to (effectively!) sell tickets, to an agent that strategizes, recommends, builds, and sends the marketing campaigns on their behalf.&lt;/p&gt;
      &lt;p&gt;We have 4 critical roles open that will have outsized impact on the future outcomes we’re driving for our customers:&lt;/p&gt;
      &lt;p&gt;- Staff Software Engineer (Data Systems)&lt;/p&gt;
      &lt;p&gt;- Senior Product Engineer (Agentic AI)&lt;/p&gt;
      &lt;p&gt;- Senior AI Product Manager&lt;/p&gt;
      &lt;p&gt;- Senior AI UX Designer&lt;/p&gt;
      &lt;p&gt;Please apply to https://jobs.ashbyhq.com/hive.co&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45656230"/><published>2025-10-21T14:24:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656247</id><title>RF Shielding History: When the FCC Cracked Down on Computers</title><updated>2025-10-21T17:11:21.782478+00:00</updated><content>&lt;doc fingerprint="f240d39c0b2331d3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Waves Of Interference&lt;/head&gt;
    &lt;head rend="h2"&gt;The reason that the PC industry first landed on the FCC’s radar had little to do with the computers themselves and everything to do with the electrical noise they emitted. Blame the CB radio.&lt;/head&gt;
    &lt;head rend="h5"&gt;Sponsored By La Machine&lt;/head&gt;
    &lt;p&gt;Tired of AI already? Meet la machine. The tech gadget invented by an AI pioneer and guaranteed 100% AI-free. Learn more here.&lt;/p&gt;
    &lt;quote&gt;
      &lt;head&gt;“Radio, television, and radar receivers are almost invariably caused to malfunction by RF interference. This is due to the very low-level circuitry contained in these devices.”&lt;/head&gt;
    &lt;/quote&gt;
    &lt;p&gt;— A passage from the RF Interference Control Handbook, a 1962 book by Kemp Barron that describes the reasons why radio waves can be affected by things as diverse as electrical circuits, industrial machinery, and even ignition systems. A key element of blocking such interference? Shielding—of the device, of the cabling, and of anything else that might emit waves that affect performance of radio-based products. The book is up on the Internet Archive in case you want to get the 63-year-old perspective on RF interference.&lt;/p&gt;
    &lt;head rend="h3"&gt;How the CB radio created an interference problem for television sets&lt;/head&gt;
    &lt;p&gt;Radio interference has long been a long, unusual annoyance in our society, appearing in all sorts of unusual places. Case in point: In the late 1940s, a church in Spokane, Washington found itself annoying local residents when its chime somehow kicked the radio onto the loudspeakers—ensuring the locals they might be getting some Spike Jones recordings along with their 6 a.m. bell chimes.&lt;/p&gt;
    &lt;p&gt;At the time, the incident was a strange oddity—after all, we didn’t have that many radio devices in our homes. But gradually, things started to shift, and it started to become more of a problem as TV sets, ham radio operators, radar systems, and other radio-driven devices started to compete for the same finite frequencies. Hell, even noisy power lines could cause issues, as the Amateur Radio Relay League could tell you.&lt;/p&gt;
    &lt;p&gt;This was especially a pain during the early era of home electronics, as it created challenges for capturing TV channels, and high-end stereos could gain some unwanted fuzz that didn’t appear on the original album. An August 1962 edition of Popular Science had a whole feature dedicated to giving readers advice on how to get rid of ghosting, channel interference, and other issues that were common parts of the TV experience at the time. “Close observation of the symptoms on your screen is the first step,” writer Art Margolis explained.&lt;/p&gt;
    &lt;p&gt;As a 1961 article on RF’s impact on hi-fi stereo systems noted that the issue was both diverse and complex in nature:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Interference from radio stations, amateur radio operations, TV and X-ray generally manifests itself as a buzz, hum or high-frequency squeal. In some cases that involve broadcasting, voices come blasting through the speakers of high fidelity systems.&lt;/p&gt;
      &lt;p&gt;External switching transients from appliances inside the house manifest themselves as pops when appliances are turned on or off. This type of interference can often be corrected at the source with a commercial AC line filter.&lt;/p&gt;
      &lt;p&gt;But more esoteric types of interference can’t be corrected as easily.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;One of those esoteric types turned out to be CB radio. Made available in the 1940s on a publicly accessible radio band and gradually tweaked over time, it was initially cost-prohibitive for average consumers, making it a popular option for small businesses or niche interests like boating. But the appeal of the concept was hard to ignore. As a 1960 Popular Mechanics piece put it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To get on the air on the Citizens Band, you don’t have to know a thing about radio. There are no operator’s licenses or technical examinations. All you do is buy the equipment, send for a station license (a mere formality, though a necessary one) and you’re all ready to operate.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However, that simplicity proved a double-edged sword. By the 1970s, the technology had begun to reach more consumers as the equipment dropped in price. The FCC tried to meet the demand by lowering the cost of licensing. However, the demand was gradually creating an untenable, near-unusable technology, which the success of 1975’s “Convoy” certainly did not help ease.&lt;/p&gt;
    &lt;p&gt;This TV report is an absolute trip. It’s forgotten just how popular CB radio was—probably because it was the Twitter of the 1970s.&lt;/p&gt;
    &lt;p&gt;People were flooding the CB’s 23 channels with constant chatter—and when the FCC expanded the number of channels to 40, radio manufacturers responded by cutting the prices of the 23-channel models, which accidentally made things even worse, as the above TV report highlights. If you were into CB radio in 1977, odds are you were in for a world of pain—with the oversaturation kind of ruining the whole thing long-term and likely facilitating the gradual shift to cellular phones.&lt;/p&gt;
    &lt;p&gt;Also left suffering as a result of this: TV viewers, who were seeing much more interference during this era as the CB fad overwhelmed the wireless airwaves. The FCC was dealing with tens of thousands of complaints about CB radios harming television signals, and it was becoming clear that the commission had to take these concerns seriously.&lt;/p&gt;
    &lt;p&gt;How bad was it? Of the complaints the agency received about TV interference, a whopping 83% were caused by CB radios, according to a December 1977 Associated Press story, which noted the popularity of CB radios that Christmas would only worsen the situation.&lt;/p&gt;
    &lt;p&gt;The FCC felt that the issue was so serious that the agency created an informational guide to help manage the issue, How to Identify &amp;amp; Resolve Radio-TV Interference Problems. The booklet made clear that if a neighbor complained about your use of a CB radio, you had to take it seriously, and even described how, if necessary, you could get help from a “Local Television Interference Committee.” (While not universal, these committees tended to come from the amateur radio operator community in a given area.)&lt;/p&gt;
    &lt;p&gt;All of this is to say that, thanks to the many, many headaches that CB radio was creating for the FCC, it only makes sense that the agency’s guard was up around other sources of RF interference. You know, like the computer.&lt;/p&gt;
    &lt;p&gt;Sucks to be a regulator.&lt;/p&gt;
    &lt;quote&gt;
      &lt;head&gt;“I hit the ‘run’ switch on the computer and it took off sorting the same list of numbers over and over again. At the same time my radio also took off!! The computer was sorting numbers and the radio was going ZZZIIIPP! ZZZIIPP! ZZZIIIPP!!!’”&lt;/head&gt;
    &lt;/quote&gt;
    &lt;p&gt;— A passage from the article “Altair Music of a Sort,” a piece by Steven Dompier, who had figured out how to play the Beatles song “Fool on the Hill” using an Altair 8800 and a nearby radio. The reason it worked? A lack of RF shielding, meaning that the computer was actually acting as a radio transmitter. Dompier later recorded the whistling melody and played it for an audience of MIPS enthusiasts. As later recalled in a biography about Bill Gates, the reason this worked was because of a design flaw in the Altair, one the FCC would become aware of in the years to come. Curious what it sounds like? Kevin Driscoll has a recreation of the code and Dompier’s article, along with a musical performance by the Altair, on his website.&lt;/p&gt;
    &lt;head rend="h3"&gt;When computers hit homes, the FCC’s RF interference concerns kicked into overdrive&lt;/head&gt;
    &lt;p&gt;At first, computers largely appeared in businesses and universities. They hadn’t quite hit the home just yet, though electronics that had computing elements—think Pong consoles and the like—were starting to make their presence known.&lt;/p&gt;
    &lt;p&gt;That meant that these devices, like every other type of electric device (even light bulbs), could be a new source of RF interference, and as the personal computer started to gain attention and popular uptake, the industry had to adapt. Stuff like the Altair’s accidental musicianship would not fly.&lt;/p&gt;
    &lt;p&gt;And so too, did the Federal Communications Commission, which asked the public in 1978 to help determine a plan of action for better managing RF interference, after getting flooded with complaints for years on end. An inquiry document implied that the FCC was considering its options for about regulating this technology. The organization implied that it may not be cost-effective or even necessary, and even floated a voluntary approach.&lt;/p&gt;
    &lt;p&gt;“Greater immunity will require more sophisticated design, additional components, and increased testing, all of which will increase equipment production cost. The magnitude of this increase will depend upon the immunity standards prescribed,” the commission wrote, adding that the public may still choose the cheaper, unshielded option if given the choice.&lt;/p&gt;
    &lt;p&gt;After spending time digging in, however, it’s clear the agency decided that they needed to take a harder line than the voluntary one posed. After all, computers were taking over the household, and so too were VCRs, video games, walkie-talkies, and all matter of electronic devices.&lt;/p&gt;
    &lt;p&gt;Of course, the computer industry was still very immature at this time, with devices like the Commodore PET, TRS-80, and Apple II still quite new, each representing early attempts at bringing personal computers to a large home market. We were a step beyond the pure hobbyist nerdery of the Altair but far from maturity. The FCC wanted to head off issues with these new devices before things went too far—and saw the dynamic around consumer devices as being distinctly different from commercial platforms.&lt;/p&gt;
    &lt;p&gt;By September 1979, the FCC was ready to weigh in, and it was worried about your neighbor’s TV experience above all else.&lt;/p&gt;
    &lt;p&gt;“We are most interested in protecting an individual who is receiving interference from his neighbor’s computer. To a lesser extent, we are concerned about devices in the same household,” the agency wrote in its rulemaking document, FCC 79-555. (The goal? To prevent interference so bad that it harmed the experience for someone in a completely different home.)&lt;/p&gt;
    &lt;p&gt;The document, which applied Part 15 regulations to computers for the first time, made clear that CB radio played a decisive factor in the final result. (Especially given the timing, which came mere months after the FCC highlighted the sheer scale of complaints it received.) The commission learned a lot from its more laissez-faire approach, which ultimately damaged the consumer experience.&lt;/p&gt;
    &lt;p&gt;“Unless the Commission acts expediously to head off the problem, we may be faced with an intolerable interference problem similar to CB interference problems of several years ago,” the commission continued.&lt;/p&gt;
    &lt;p&gt;This was not good news for all these technology startups in the budding computer industry. Suddenly, companies large and small had to share their new products—and presumably, their trade secrets—with the FCC in an attempt to ensure that the electronics were safe to sell.&lt;/p&gt;
    &lt;p&gt;This created major headaches, especially for companies that already had products on the market, which now had to stop producing RF interference on TV and radio frequencies altogether. As the magazine Kilobaud put it in 1981: “After three years of study, the FCC handed the microcomputer industry a mandate: Get rid of the RFI by January 1981, or close up shop.”&lt;/p&gt;
    &lt;p&gt;These standards meant additional testing—including the use of a spectrum analyzer and a dedicated testing site to ensure that equipment meets respectable radiation standards. And these standards could be quite confusing for manufacturers, even affecting things like upgradeability. Per Kilobaud’s Chris Brown and Eric Maloney, “As far as the FCC is concerned, an 8K CPU that has the potential of being upgraded to 16K is actually two different computers.”&lt;/p&gt;
    &lt;p&gt;Put simply, this put a huge testing onus on manufacturers, likely raising prices in the short term and leading to additional metal shielding as a band-aid to retrofit existing designs to fit the FCC’s regulations. Large manufacturers likely were already doing a lot of this on their own. The problem was that this put a lot of additional pressure on the makeshift startups that made the sector viable in the first place.&lt;/p&gt;
    &lt;p&gt;Manufacturers had to adapt. For example, Apple released newer iterations of the Apple II, such as the Apple II Plus and Apple IIe, to help improve the electromagnetic noise that the original created. (Which means, if you want a computer with no RF shielding because you desire to make music via radio interference, get the original Apple II.)&lt;/p&gt;
    &lt;p&gt;Apple had to take this stuff seriously, because the FCC was paying special attention to them. The company was specifically cited in the commission’s rulemaking as being aware of the problem as far back as 1976. And it got nailed at least once for noncompliance: On top of all the other challenges that the ill-fated Apple III faced, the machine had to be recalled and replaced with a new model because its RF shielding wasn’t good enough.&lt;/p&gt;
    &lt;p&gt;Ironically, the rules were way less strict for computers in office or industrial environments, which received a Class A regulation, than for consumer tech, which required the stricter Class B designation. After all, if the goal is to prevent radio interference, your Facts of Life-watching neighbor is going to have bigger issues than your spreadsheet-embracing co-worker.&lt;/p&gt;
    &lt;p&gt;All of this was a pain to manage, but you’ll be surprised to learn that, in the end, all this added regulation probably ended up making the computer industry better.&lt;/p&gt;
    &lt;quote&gt;
      &lt;head&gt;“In waiving Part 15, Subpart D, the commission noted that this individual cow identification system would be in the public interest by increasing the efficiency of dairy herd management and thereby lowering the costs of dairy products.”&lt;/head&gt;
    &lt;/quote&gt;
    &lt;p&gt;— A statement from the Federal Communications Commission, describing one example of the organization making an exception for the Part 15 rules—for, of all things, a cow-tracking transmitter system called BouMatic. (The goal: To determine when cows were ready for breeding.)&lt;/p&gt;
    &lt;p&gt;If you want an explanation of how RF interference regulations improved the computer industry, I point to the device you’re probably reading this on—a phone or a laptop. Each show the amount of engineering work required to build something that is less noisy from an RF standpoint, while still working in tighter contours.&lt;/p&gt;
    &lt;p&gt;If you’ve opened up a MacBook Pro recently, you’ve probably noticed that there aren’t tons of seemingly extraneous metal shields everywhere. That is a sign of the incremental work done to cut down on interference—the shielding is still in there, but it’s shielded from the ground up.&lt;/p&gt;
    &lt;p&gt;(By the way, it’s worth noting that some of the earliest successful players in the laptop space, like Toshiba, had to additionally navigate strict Japanese electronics regulations, such as the VCCI Council, which played a similar role to that of the FCC. That additional layer of regulation probably gave them an upper hand in the early years of portable computing.)&lt;/p&gt;
    &lt;p&gt;Or I could point to the fact that you’re probably not reading this on a wired connection, and are likely taking advantage of Wi-Fi. The PC industry likely would not have figured out how to share waves of data over antennas had it not been forced to get its house in order first.&lt;/p&gt;
    &lt;p&gt;Oh, sure, the FCC testing causes problems—the reason the iPhone was announced six months before it went on sale was because the commission would have otherwise blown up Apple’s spot—but on balance, it has ultimately made our gadgets better.&lt;/p&gt;
    &lt;p&gt;Things have improved to the point that when significant incidents of RF interference happen, they make the news, just like they did in the 1940s. Back in 2019, the city of North Olmsted, Ohio faced a bizarre issue where garage door openers and key fobs stopped working correctly, and for days, the community could not figure out why. Even shutting off the power on an entire city block couldn’t solve the problem.&lt;/p&gt;
    &lt;p&gt;Eventually, after a lot of looking by city officials, they found the cause: A homemade gadget in a tinkerer’s home that turned off a light whenever someone was working upstairs. The device just happened to use the same frequency as many garage door openers, and it was powered by battery, so it wasn’t even on the damn grid! It’s the perfect example of why the FCC’s testing actually makes a ton of sense.&lt;/p&gt;
    &lt;p&gt;We live in a world where you can use a laptop in the passenger seat of a car, and it probably won’t cause the radio in the car next to you to turn into a static mess. I don’t know anyone who uses a CB radio in 2025, but I’m betting the laptop wouldn’t mess with it, either.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;To anyone who has issues with bad TV reception: The struggle is real. Find this one an interesting read? Share it with a pal!&lt;/p&gt;
    &lt;p&gt;And thanks again to la machine for sponsoring. It’s the coolest little device you can have on your desk.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tedium.co/2025/10/20/computers-fcc-rf-interference-history/"/><published>2025-10-21T14:26:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656916</id><title>AI Is Making Us Work More</title><updated>2025-10-21T17:11:21.443043+00:00</updated><content>&lt;doc fingerprint="6b39c79d8ada9c0"&gt;
  &lt;main&gt;
    &lt;p&gt;KATAKATE&lt;/p&gt;
    &lt;p&gt;Self-hosted secure VM sandboxes for AI compute at scale&lt;/p&gt;
    &lt;p&gt;Katakate aims to make it easy to create, manage and orchestrate lightweight safe VM sandboxes for executing untrusted code, at scale. It is built on battle-tested VM isolation with Kata, Firecracker and Kubernetes. It is orignally motivated by AI agents that need to run arbitrary code at scale but it is also great for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Custom serverless (like AWS Fargate, but yours)&lt;/item&gt;
      &lt;item&gt;Hardened CI/CD runners (no Docker-in-Docker risks)&lt;/item&gt;
      &lt;item&gt;Blockchain execution layers for AI dApps&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;100% open‑source (Apache‑2.0). For technical support, write us at: hi@katakate.org&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Katakate is built on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kubernetes for orchestration, with K3s which is prod-ready and a great choice for edge nodes,&lt;/item&gt;
      &lt;item&gt;Kata to encapsulate containers into light-weight virtual-machines,&lt;/item&gt;
      &lt;item&gt;Firecracker as the chosen VM, for super-fast boots, light footprints and minimal attack surface,&lt;/item&gt;
      &lt;item&gt;Devmapper Snapshotter with thin-pool provisioning of logical volumes for efficient use of disk space shared by dozens of VMs per node.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🛠️ Docker &lt;code&gt;build&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt;/&lt;code&gt;compose&lt;/code&gt;support inside the VM sandbox&lt;/item&gt;
      &lt;item&gt;🌐 Multi-node cluster capabilities for distributed workloads&lt;/item&gt;
      &lt;item&gt;🔍 Cilium FQDN-based DNS resolution to safely whitelist domains, not just IP blocks&lt;/item&gt;
      &lt;item&gt;⚙️ Support other VMM such as Qemu for GPU workloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Katakate is currently in beta and under security review. Use with caution for highly sensitive workloads.&lt;/p&gt;
    &lt;p&gt;For usage you need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node(s) that will host the VM sandboxes&lt;/item&gt;
      &lt;item&gt;Client from where to send requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We provide a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CLI: to use on the node(s) directly --&amp;gt; &lt;code&gt;apt install k7&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;API: deployed on the (master) node(s) --&amp;gt; &lt;code&gt;k7 start-api&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Python SDK: Python client sync/async talking to API --&amp;gt; &lt;code&gt;pip install katakate&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ubuntu (amd64) host.&lt;/item&gt;
      &lt;item&gt;Hardware virtualization (KVM) available and accessible &lt;list rend="ul"&gt;&lt;item&gt;Check: &lt;code&gt;ls /dev/kvm&lt;/code&gt;should exist.&lt;/item&gt;&lt;item&gt;This is typically available on your own Linux machine.&lt;/item&gt;&lt;item&gt;On cloud providers, it varies. &lt;list rend="ul"&gt;&lt;item&gt;Hetzner (the only one I tested so far) yes for their &lt;code&gt;Robot&lt;/code&gt;instances only, i.e. "dedicated": robot.hetzner.com.&lt;/item&gt;&lt;item&gt;AWS: only &lt;code&gt;.metal&lt;/code&gt;EC2 instances.&lt;/item&gt;&lt;item&gt;GCP: virtualization friendly, most instances, with &lt;code&gt;--enable-nested-virtualization&lt;/code&gt;flag.&lt;/item&gt;&lt;item&gt;Azure: Dv3, Ev3, Dv4, Ev4, Dv5, Ev5. Must be Intel/AMD x86, not ARM.&lt;/item&gt;&lt;item&gt;Others: in general, hardware virtualization is not exposed on cloud VPS, so you'll likely want a dedicated / bare metal.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Hetzner (the only one I tested so far) yes for their &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Check: &lt;/item&gt;
      &lt;item&gt;One raw disk (unformatted, unpartitioned) for the thin-pool that k7 will provision for efficient disk usage of sandboxes. &lt;list rend="ul"&gt;&lt;item&gt;Use &lt;code&gt;./utils/wipe-disk.sh /your/disk&lt;/code&gt;to wipe a disk clean before provisioning. DANGER: destructive - it will remove data/partitions/formatting/SWRAID.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Use &lt;/item&gt;
      &lt;item&gt;Ansible (for installer): &lt;quote&gt;sudo add-apt-repository universe -y sudo apt update sudo apt install -y ansible&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Docker and Docker Compose (for the API): &lt;code&gt;curl -fsSL https://get.docker.com | sh&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Already tested setups:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hetzner Robot instance with Ubuntu 24.04, x86_64 arch, booked with 1 extra empty disk &lt;code&gt;nvme2n1&lt;/code&gt;for the thin-pool provisioning. See the setup guide (PDF): tutorials/k7_hetzner_node_setup.pdf.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just recent Python.&lt;/p&gt;
    &lt;p&gt;First install &lt;code&gt;k7&lt;/code&gt; on your Linux server that will host the VMs:&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository ppa:katakate.org/k7
sudo apt update
sudo apt install k7&lt;/code&gt;
    &lt;p&gt;Then let &lt;code&gt;k7&lt;/code&gt; get your node ready with everything:&lt;/p&gt;
    &lt;code&gt;$  k7 install
Current task: Reminder about logging out and back in for group changes
  Installing K7 on 1 host(s)... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:01:41
✅ Installation completed successfully!
&lt;/code&gt;
    &lt;p&gt;Optionally pass &lt;code&gt;-v&lt;/code&gt; for a verbose output.&lt;/p&gt;
    &lt;p&gt;This will install and most importantly connect together the following components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kubernetes (K3s prod-ready distribution)&lt;/item&gt;
      &lt;item&gt;Kata (for container virtualization)&lt;/item&gt;
      &lt;item&gt;Firecracker (as Virtual Machine Manager)&lt;/item&gt;
      &lt;item&gt;Jailer (to secure Firecracker VMs further into a chroot)&lt;/item&gt;
      &lt;item&gt;devmapper snapshotter with thin-pool provisioning of logical volumes for VM efficient disk memory usage&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Careful design: config updates will not touch your existing Docker or containerd setups. We chose to use K3s' own containerd for minimal disruption. Installation may however overwrite existing installations of K3s, Kata, Firecracker, Jailer.&lt;/p&gt;
    &lt;p&gt;You can run workloads directly from the node(s) using the CLI. To create a sandbox, just create a yaml config for it.&lt;/p&gt;
    &lt;code&gt;name: my-sandbox-123
image: alpine:latest
namespace: default

# Optional: restrict egress
egress_whitelist:
  - "1.1.1.1/32"      # Cloudflare DNS
  - "8.8.8.8/32"      # Google DNS

# Optional: resource limits
limits:
  cpu: "1"
  memory: "1Gi"
  ephemeral-storage: "2Gi"

# Optional: run before_script inside the container once at start. Network restrictions apply after the before-script, so you can install packages here, pull git repos, etc
before_script: |
  apk add --no-cache git curl

# Optional: load environment variables from a file. These will be available both during the before-script, and in the sandbox
env_file: path/to/your/secrets/.env&lt;/code&gt;
    &lt;code&gt;# Create a sandbox (uses k7.yaml in the current directory by default, but you can also pass: -f myfile.yaml)
k7 create

# List sandboxes
k7 list

# Delete a sandbox
k7 delete my-sandbox-123

# Delete all sandboxes. You can also pass a namespace
k7 delete-all&lt;/code&gt;
    &lt;p&gt;If you'd like to manage workloads remotely, just use the API:&lt;/p&gt;
    &lt;code&gt;# Start API server (containerized and SSL support with Cloudflared)
k7 start-api

# Generate API key
k7 generate-api-key my-key1&lt;/code&gt;
    &lt;p&gt;Make sure your user is in the &lt;code&gt;Docker&lt;/code&gt; group to be allowed to start or stop the API.&lt;/p&gt;
    &lt;p&gt;As for generating / listing / revoking keys, you might need &lt;code&gt;sudo&lt;/code&gt; or &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;After your k7 API is up, usage is very simple.&lt;/p&gt;
    &lt;p&gt;Install the Python SDK via:&lt;/p&gt;
    &lt;code&gt;pip install katakate&lt;/code&gt;
    &lt;p&gt;Or if you want async support:&lt;/p&gt;
    &lt;code&gt;pip install "katakate[async-sdk]"&lt;/code&gt;
    &lt;p&gt;Then use with:&lt;/p&gt;
    &lt;code&gt;from katakate import Client

k7 = Client(
  endpoint='https://&amp;lt;your-endpoint&amp;gt;', 
  api_key='your-key')

# Create sandbox
sb = k7.create({
    "name": "my-sandbox",
    "image": "alpine:latest"
})

# Execute code
result = sb.exec('echo "Hello World"')
print(result['stdout'])

# List all sandboxes
sandboxes = k7.list()

# Delete sandbox
sb.delete()&lt;/code&gt;
    &lt;code&gt;import asyncio
from katakate import AsyncClient

async def main():
    k7 = AsyncClient(
      endpoint='https://&amp;lt;your-endpoint&amp;gt;', 
      api_key='your-key'
    )
    print(await k7.list())
    await k7.aclose()

asyncio.run(main())&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LangChain ReAct agent with a K7 sandbox tool &lt;list rend="ul"&gt;&lt;item&gt;Path: tutorials/langchain-react-agent&lt;/item&gt;&lt;item&gt;Setup: copy .env.example to .env and fill K7_ENDPOINT/K7_API_KEY/OPENAI_API_KEY&lt;/item&gt;&lt;item&gt;Run: python agent.py&lt;/item&gt;&lt;item&gt;Try asking it anything! e.g. "List files from '/'"&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First install make if not already available:&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository universe -y
sudo apt update
sudo apt install make&lt;/code&gt;
    &lt;p&gt;To build the &lt;code&gt;k7&lt;/code&gt; CLI and API into &lt;code&gt;.deb&lt;/code&gt; package:&lt;/p&gt;
    &lt;code&gt;make build&lt;/code&gt;
    &lt;p&gt;You can then install it with:&lt;/p&gt;
    &lt;code&gt;sudo make install&lt;/code&gt;
    &lt;p&gt;To uninstall later:&lt;/p&gt;
    &lt;code&gt;sudo make uninstall&lt;/code&gt;
    &lt;p&gt;Note: we recommend running &lt;code&gt;make uninstall&lt;/code&gt; before reinstalling if it is not your first install, to avoid stale copies of cached files in the .deb package.&lt;/p&gt;
    &lt;p&gt;Local dev image:&lt;/p&gt;
    &lt;code&gt;# Build the API image locally
make api-build-local

# Run API using local image (no pull)
make api-run-local&lt;/code&gt;
    &lt;p&gt;Preferred (uv):&lt;/p&gt;
    &lt;code&gt;# create env
uv venv .venv-build
. .venv-build/bin/activate

# install directly from source in editable mode
uv pip install -e .&lt;/code&gt;
    &lt;p&gt;K7 sandboxes are hardened by default with multiple layers of security:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;VM isolation: Kata Containers provide hardware-level isolation via lightweight VMs with Firecracker&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;VMs are further restricted into a chroot using Jailer&lt;/item&gt;
          &lt;item&gt;Kata's Seccomp restrictions are enabled&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Linux capabilities: All capabilities are dropped by default (&lt;/p&gt;&lt;code&gt;drop: ALL&lt;/code&gt;) for defense-in-depth&lt;list rend="ul"&gt;&lt;item&gt;Only explicitly add back capabilities you need via &lt;code&gt;cap_add&lt;/code&gt;parameter&lt;/item&gt;&lt;item&gt;&lt;code&gt;allow_privilege_escalation&lt;/code&gt;is always set to&lt;code&gt;false&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Seccomp profile: &lt;code&gt;RuntimeDefault&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Only explicitly add back capabilities you need via &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-root execution: Optionally run containers and pods as non-root user (UID 65532):&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;container_non_root&lt;/code&gt;: Run the main container as non-root and disable privilege escalation&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;pod_non_root&lt;/code&gt;: Run the entire pod as non-root with consistent filesystem ownership (UID/GID/FSGroup 65532)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API security:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;API keys stored as SHA256 hashes with timing-attack-resistant comparison&lt;/item&gt;
          &lt;item&gt;Expiry enforced; last-used timestamp recorded&lt;/item&gt;
          &lt;item&gt;File-based storage with 600 permissions (&lt;code&gt;/etc/k7/api_keys.json&lt;/code&gt;by default)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network policies: Complete network isolation for VM sandboxes&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Ingress isolation: All inter-VM communication is blocked by default to prevent sandbox-to-sandbox access&lt;/item&gt;
          &lt;item&gt;Egress lockdown: Control outbound traffic with CIDR-based restrictions using Kubernetes NetworkPolicies&lt;/item&gt;
          &lt;item&gt;DNS to CoreDNS always allowed when egress is locked down&lt;/item&gt;
          &lt;item&gt;Administrative access via &lt;code&gt;kubectl exec&lt;/code&gt;and&lt;code&gt;k7 shell&lt;/code&gt;is preserved (uses Kubernetes API, not pod networking)&lt;/item&gt;
          &lt;item&gt;Soon to come: Cilium integration for domain name whitelisting&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More security features are currently on the roadmap, including integrating AppArmor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Layout uses &lt;code&gt;src/&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;CLI, API, core live under &lt;code&gt;src/k7/&lt;/code&gt;&lt;/item&gt;&lt;item&gt;SDK under &lt;code&gt;src/katakate/&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;CLI, API, core live under &lt;/item&gt;
      &lt;item&gt;Root packaging targets the &lt;code&gt;katakate&lt;/code&gt;SDK only; assets under&lt;code&gt;src/k7/&lt;/code&gt;are not part of the PyPI distribution.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MANIFEST.in&lt;/code&gt;(for the&lt;code&gt;katakate&lt;/code&gt;SDK) should include essentials like&lt;code&gt;LICENSE&lt;/code&gt;and&lt;code&gt;README.md&lt;/code&gt;only; deploy assets from&lt;code&gt;src/k7/deploy/*&lt;/code&gt;belong to the Debian/CLI packaging flow, not to the PyPI package.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setup.py&lt;/code&gt;for&lt;code&gt;katakate&lt;/code&gt;lives at repo root; packages from&lt;code&gt;src/&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The CLI Debian package is built via &lt;code&gt;src/k7/cli/build.sh&lt;/code&gt;and produces&lt;code&gt;dist/k7_&amp;lt;version&amp;gt;_amd64.deb&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CI (tags &lt;code&gt;v*&lt;/code&gt;) can publish the PyPI SDK and upload the&lt;code&gt;.deb&lt;/code&gt;artifact.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jailer seems to be currently ignored by Kata despite being passed correctly into its configuration, and despite the Jailer process being started. The use of Kubernetes secrets could be a reason of incompatibility. This is under investigation.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tawandamunongo.dev/posts/2025/10/ai-work-more"/><published>2025-10-21T15:19:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656952</id><title>Katakate: Dozens of VMs per node for safe code exec: K8s+Kata+Firecracker</title><updated>2025-10-21T17:11:20.753638+00:00</updated><content>&lt;doc fingerprint="6b39c79d8ada9c0"&gt;
  &lt;main&gt;
    &lt;p&gt;KATAKATE&lt;/p&gt;
    &lt;p&gt;Self-hosted secure VM sandboxes for AI compute at scale&lt;/p&gt;
    &lt;p&gt;Katakate aims to make it easy to create, manage and orchestrate lightweight safe VM sandboxes for executing untrusted code, at scale. It is built on battle-tested VM isolation with Kata, Firecracker and Kubernetes. It is orignally motivated by AI agents that need to run arbitrary code at scale but it is also great for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Custom serverless (like AWS Fargate, but yours)&lt;/item&gt;
      &lt;item&gt;Hardened CI/CD runners (no Docker-in-Docker risks)&lt;/item&gt;
      &lt;item&gt;Blockchain execution layers for AI dApps&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;100% open‑source (Apache‑2.0). For technical support, write us at: hi@katakate.org&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Katakate is built on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kubernetes for orchestration, with K3s which is prod-ready and a great choice for edge nodes,&lt;/item&gt;
      &lt;item&gt;Kata to encapsulate containers into light-weight virtual-machines,&lt;/item&gt;
      &lt;item&gt;Firecracker as the chosen VM, for super-fast boots, light footprints and minimal attack surface,&lt;/item&gt;
      &lt;item&gt;Devmapper Snapshotter with thin-pool provisioning of logical volumes for efficient use of disk space shared by dozens of VMs per node.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🛠️ Docker &lt;code&gt;build&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt;/&lt;code&gt;compose&lt;/code&gt;support inside the VM sandbox&lt;/item&gt;
      &lt;item&gt;🌐 Multi-node cluster capabilities for distributed workloads&lt;/item&gt;
      &lt;item&gt;🔍 Cilium FQDN-based DNS resolution to safely whitelist domains, not just IP blocks&lt;/item&gt;
      &lt;item&gt;⚙️ Support other VMM such as Qemu for GPU workloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Katakate is currently in beta and under security review. Use with caution for highly sensitive workloads.&lt;/p&gt;
    &lt;p&gt;For usage you need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node(s) that will host the VM sandboxes&lt;/item&gt;
      &lt;item&gt;Client from where to send requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We provide a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CLI: to use on the node(s) directly --&amp;gt; &lt;code&gt;apt install k7&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;API: deployed on the (master) node(s) --&amp;gt; &lt;code&gt;k7 start-api&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Python SDK: Python client sync/async talking to API --&amp;gt; &lt;code&gt;pip install katakate&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ubuntu (amd64) host.&lt;/item&gt;
      &lt;item&gt;Hardware virtualization (KVM) available and accessible &lt;list rend="ul"&gt;&lt;item&gt;Check: &lt;code&gt;ls /dev/kvm&lt;/code&gt;should exist.&lt;/item&gt;&lt;item&gt;This is typically available on your own Linux machine.&lt;/item&gt;&lt;item&gt;On cloud providers, it varies. &lt;list rend="ul"&gt;&lt;item&gt;Hetzner (the only one I tested so far) yes for their &lt;code&gt;Robot&lt;/code&gt;instances only, i.e. "dedicated": robot.hetzner.com.&lt;/item&gt;&lt;item&gt;AWS: only &lt;code&gt;.metal&lt;/code&gt;EC2 instances.&lt;/item&gt;&lt;item&gt;GCP: virtualization friendly, most instances, with &lt;code&gt;--enable-nested-virtualization&lt;/code&gt;flag.&lt;/item&gt;&lt;item&gt;Azure: Dv3, Ev3, Dv4, Ev4, Dv5, Ev5. Must be Intel/AMD x86, not ARM.&lt;/item&gt;&lt;item&gt;Others: in general, hardware virtualization is not exposed on cloud VPS, so you'll likely want a dedicated / bare metal.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Hetzner (the only one I tested so far) yes for their &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Check: &lt;/item&gt;
      &lt;item&gt;One raw disk (unformatted, unpartitioned) for the thin-pool that k7 will provision for efficient disk usage of sandboxes. &lt;list rend="ul"&gt;&lt;item&gt;Use &lt;code&gt;./utils/wipe-disk.sh /your/disk&lt;/code&gt;to wipe a disk clean before provisioning. DANGER: destructive - it will remove data/partitions/formatting/SWRAID.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Use &lt;/item&gt;
      &lt;item&gt;Ansible (for installer): &lt;quote&gt;sudo add-apt-repository universe -y sudo apt update sudo apt install -y ansible&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Docker and Docker Compose (for the API): &lt;code&gt;curl -fsSL https://get.docker.com | sh&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Already tested setups:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hetzner Robot instance with Ubuntu 24.04, x86_64 arch, booked with 1 extra empty disk &lt;code&gt;nvme2n1&lt;/code&gt;for the thin-pool provisioning. See the setup guide (PDF): tutorials/k7_hetzner_node_setup.pdf.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just recent Python.&lt;/p&gt;
    &lt;p&gt;First install &lt;code&gt;k7&lt;/code&gt; on your Linux server that will host the VMs:&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository ppa:katakate.org/k7
sudo apt update
sudo apt install k7&lt;/code&gt;
    &lt;p&gt;Then let &lt;code&gt;k7&lt;/code&gt; get your node ready with everything:&lt;/p&gt;
    &lt;code&gt;$  k7 install
Current task: Reminder about logging out and back in for group changes
  Installing K7 on 1 host(s)... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:01:41
✅ Installation completed successfully!
&lt;/code&gt;
    &lt;p&gt;Optionally pass &lt;code&gt;-v&lt;/code&gt; for a verbose output.&lt;/p&gt;
    &lt;p&gt;This will install and most importantly connect together the following components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kubernetes (K3s prod-ready distribution)&lt;/item&gt;
      &lt;item&gt;Kata (for container virtualization)&lt;/item&gt;
      &lt;item&gt;Firecracker (as Virtual Machine Manager)&lt;/item&gt;
      &lt;item&gt;Jailer (to secure Firecracker VMs further into a chroot)&lt;/item&gt;
      &lt;item&gt;devmapper snapshotter with thin-pool provisioning of logical volumes for VM efficient disk memory usage&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Careful design: config updates will not touch your existing Docker or containerd setups. We chose to use K3s' own containerd for minimal disruption. Installation may however overwrite existing installations of K3s, Kata, Firecracker, Jailer.&lt;/p&gt;
    &lt;p&gt;You can run workloads directly from the node(s) using the CLI. To create a sandbox, just create a yaml config for it.&lt;/p&gt;
    &lt;code&gt;name: my-sandbox-123
image: alpine:latest
namespace: default

# Optional: restrict egress
egress_whitelist:
  - "1.1.1.1/32"      # Cloudflare DNS
  - "8.8.8.8/32"      # Google DNS

# Optional: resource limits
limits:
  cpu: "1"
  memory: "1Gi"
  ephemeral-storage: "2Gi"

# Optional: run before_script inside the container once at start. Network restrictions apply after the before-script, so you can install packages here, pull git repos, etc
before_script: |
  apk add --no-cache git curl

# Optional: load environment variables from a file. These will be available both during the before-script, and in the sandbox
env_file: path/to/your/secrets/.env&lt;/code&gt;
    &lt;code&gt;# Create a sandbox (uses k7.yaml in the current directory by default, but you can also pass: -f myfile.yaml)
k7 create

# List sandboxes
k7 list

# Delete a sandbox
k7 delete my-sandbox-123

# Delete all sandboxes. You can also pass a namespace
k7 delete-all&lt;/code&gt;
    &lt;p&gt;If you'd like to manage workloads remotely, just use the API:&lt;/p&gt;
    &lt;code&gt;# Start API server (containerized and SSL support with Cloudflared)
k7 start-api

# Generate API key
k7 generate-api-key my-key1&lt;/code&gt;
    &lt;p&gt;Make sure your user is in the &lt;code&gt;Docker&lt;/code&gt; group to be allowed to start or stop the API.&lt;/p&gt;
    &lt;p&gt;As for generating / listing / revoking keys, you might need &lt;code&gt;sudo&lt;/code&gt; or &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;After your k7 API is up, usage is very simple.&lt;/p&gt;
    &lt;p&gt;Install the Python SDK via:&lt;/p&gt;
    &lt;code&gt;pip install katakate&lt;/code&gt;
    &lt;p&gt;Or if you want async support:&lt;/p&gt;
    &lt;code&gt;pip install "katakate[async-sdk]"&lt;/code&gt;
    &lt;p&gt;Then use with:&lt;/p&gt;
    &lt;code&gt;from katakate import Client

k7 = Client(
  endpoint='https://&amp;lt;your-endpoint&amp;gt;', 
  api_key='your-key')

# Create sandbox
sb = k7.create({
    "name": "my-sandbox",
    "image": "alpine:latest"
})

# Execute code
result = sb.exec('echo "Hello World"')
print(result['stdout'])

# List all sandboxes
sandboxes = k7.list()

# Delete sandbox
sb.delete()&lt;/code&gt;
    &lt;code&gt;import asyncio
from katakate import AsyncClient

async def main():
    k7 = AsyncClient(
      endpoint='https://&amp;lt;your-endpoint&amp;gt;', 
      api_key='your-key'
    )
    print(await k7.list())
    await k7.aclose()

asyncio.run(main())&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LangChain ReAct agent with a K7 sandbox tool &lt;list rend="ul"&gt;&lt;item&gt;Path: tutorials/langchain-react-agent&lt;/item&gt;&lt;item&gt;Setup: copy .env.example to .env and fill K7_ENDPOINT/K7_API_KEY/OPENAI_API_KEY&lt;/item&gt;&lt;item&gt;Run: python agent.py&lt;/item&gt;&lt;item&gt;Try asking it anything! e.g. "List files from '/'"&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First install make if not already available:&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository universe -y
sudo apt update
sudo apt install make&lt;/code&gt;
    &lt;p&gt;To build the &lt;code&gt;k7&lt;/code&gt; CLI and API into &lt;code&gt;.deb&lt;/code&gt; package:&lt;/p&gt;
    &lt;code&gt;make build&lt;/code&gt;
    &lt;p&gt;You can then install it with:&lt;/p&gt;
    &lt;code&gt;sudo make install&lt;/code&gt;
    &lt;p&gt;To uninstall later:&lt;/p&gt;
    &lt;code&gt;sudo make uninstall&lt;/code&gt;
    &lt;p&gt;Note: we recommend running &lt;code&gt;make uninstall&lt;/code&gt; before reinstalling if it is not your first install, to avoid stale copies of cached files in the .deb package.&lt;/p&gt;
    &lt;p&gt;Local dev image:&lt;/p&gt;
    &lt;code&gt;# Build the API image locally
make api-build-local

# Run API using local image (no pull)
make api-run-local&lt;/code&gt;
    &lt;p&gt;Preferred (uv):&lt;/p&gt;
    &lt;code&gt;# create env
uv venv .venv-build
. .venv-build/bin/activate

# install directly from source in editable mode
uv pip install -e .&lt;/code&gt;
    &lt;p&gt;K7 sandboxes are hardened by default with multiple layers of security:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;VM isolation: Kata Containers provide hardware-level isolation via lightweight VMs with Firecracker&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;VMs are further restricted into a chroot using Jailer&lt;/item&gt;
          &lt;item&gt;Kata's Seccomp restrictions are enabled&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Linux capabilities: All capabilities are dropped by default (&lt;/p&gt;&lt;code&gt;drop: ALL&lt;/code&gt;) for defense-in-depth&lt;list rend="ul"&gt;&lt;item&gt;Only explicitly add back capabilities you need via &lt;code&gt;cap_add&lt;/code&gt;parameter&lt;/item&gt;&lt;item&gt;&lt;code&gt;allow_privilege_escalation&lt;/code&gt;is always set to&lt;code&gt;false&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Seccomp profile: &lt;code&gt;RuntimeDefault&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Only explicitly add back capabilities you need via &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-root execution: Optionally run containers and pods as non-root user (UID 65532):&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;container_non_root&lt;/code&gt;: Run the main container as non-root and disable privilege escalation&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;pod_non_root&lt;/code&gt;: Run the entire pod as non-root with consistent filesystem ownership (UID/GID/FSGroup 65532)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API security:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;API keys stored as SHA256 hashes with timing-attack-resistant comparison&lt;/item&gt;
          &lt;item&gt;Expiry enforced; last-used timestamp recorded&lt;/item&gt;
          &lt;item&gt;File-based storage with 600 permissions (&lt;code&gt;/etc/k7/api_keys.json&lt;/code&gt;by default)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network policies: Complete network isolation for VM sandboxes&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Ingress isolation: All inter-VM communication is blocked by default to prevent sandbox-to-sandbox access&lt;/item&gt;
          &lt;item&gt;Egress lockdown: Control outbound traffic with CIDR-based restrictions using Kubernetes NetworkPolicies&lt;/item&gt;
          &lt;item&gt;DNS to CoreDNS always allowed when egress is locked down&lt;/item&gt;
          &lt;item&gt;Administrative access via &lt;code&gt;kubectl exec&lt;/code&gt;and&lt;code&gt;k7 shell&lt;/code&gt;is preserved (uses Kubernetes API, not pod networking)&lt;/item&gt;
          &lt;item&gt;Soon to come: Cilium integration for domain name whitelisting&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More security features are currently on the roadmap, including integrating AppArmor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Layout uses &lt;code&gt;src/&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;CLI, API, core live under &lt;code&gt;src/k7/&lt;/code&gt;&lt;/item&gt;&lt;item&gt;SDK under &lt;code&gt;src/katakate/&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;CLI, API, core live under &lt;/item&gt;
      &lt;item&gt;Root packaging targets the &lt;code&gt;katakate&lt;/code&gt;SDK only; assets under&lt;code&gt;src/k7/&lt;/code&gt;are not part of the PyPI distribution.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MANIFEST.in&lt;/code&gt;(for the&lt;code&gt;katakate&lt;/code&gt;SDK) should include essentials like&lt;code&gt;LICENSE&lt;/code&gt;and&lt;code&gt;README.md&lt;/code&gt;only; deploy assets from&lt;code&gt;src/k7/deploy/*&lt;/code&gt;belong to the Debian/CLI packaging flow, not to the PyPI package.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setup.py&lt;/code&gt;for&lt;code&gt;katakate&lt;/code&gt;lives at repo root; packages from&lt;code&gt;src/&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The CLI Debian package is built via &lt;code&gt;src/k7/cli/build.sh&lt;/code&gt;and produces&lt;code&gt;dist/k7_&amp;lt;version&amp;gt;_amd64.deb&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CI (tags &lt;code&gt;v*&lt;/code&gt;) can publish the PyPI SDK and upload the&lt;code&gt;.deb&lt;/code&gt;artifact.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jailer seems to be currently ignored by Kata despite being passed correctly into its configuration, and despite the Jailer process being started. The use of Kubernetes secrets could be a reason of incompatibility. This is under investigation.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Katakate/k7"/><published>2025-10-21T15:22:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657055</id><title>Show HN: Clink – Bring your own CLI Agents, Ship instantly</title><updated>2025-10-21T17:11:20.675074+00:00</updated><content>&lt;doc fingerprint="536fd5a56c585c00"&gt;
  &lt;main&gt;
    &lt;p&gt;?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://clink.new"/><published>2025-10-21T15:32:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657287</id><title>Foreign hackers breached a US nuclear weapons plant via SharePoint flaws</title><updated>2025-10-21T17:11:19.940220+00:00</updated><content>&lt;doc fingerprint="3598a8970a17de0d"&gt;
  &lt;main&gt;
    &lt;p&gt;A foreign actor infiltrated the National Nuclear Security Administration’s Kansas City National Security Campus through vulnerabilities in Microsoft’s SharePoint browser-based app, raising questions about the need to solidify further federal IT/OT security protections. Credit: Wirestock Creators / Shutterstock A foreign threat actor infiltrated the Kansas City National Security Campus (KCNSC), a key manufacturing site within the National Nuclear Security Administration (NNSA), exploiting unpatched Microsoft SharePoint vulnerabilities, according to a source involved in an August incident response at the facility. The breach targeted a plant that produces the vast majority of critical non-nuclear components for US nuclear weapons under the NNSA, a semi-autonomous agency within the Department of Energy (DOE) that oversees the design, production, and maintenance of the nation’s nuclear weapons. Honeywell Federal Manufacturing &amp;amp; Technologies (FM&amp;amp;T) manages the Kansas City campus under contract to the NNSA. The Kansas City campus, Honeywell FM&amp;amp;T, and the Department of Energy did not respond to repeated requests for comment throughout September, well before the current government shutdown. NSA public affairs officer Eddie Bennett did respond, saying, “We have nothing to contribute,” and referred CSO back to the DOE. Although it is unclear whether the attackers were a Chinese nation-state actor or Russian cybercriminals — the two most likely culprits — experts say the incident drives home the importance of securing systems that protect operational technology from exploits that primarily affect IT systems. How the breach unfolded The attackers exploited two recently disclosed Microsoft SharePoint vulnerabilities — CVE-2025-53770, a spoofing flaw, and CVE-2025-49704, a remote code execution (RCE) bug — both affecting on-premises servers. Microsoft issued fixes for the vulnerabilities on July 19. On July 22, the NNSA confirmed it was one of the organizations hit by attacks enabled by the SharePoint flaws. “On Friday, July 18th, the exploitation of a Microsoft SharePoint zero-day vulnerability began affecting the Department of Energy,” a DOE spokesperson said. However, the DOE contended at the time, “The department was minimally impacted due to its widespread use of the Microsoft M365 cloud and very capable cybersecurity systems. A very small number of systems were impacted. All impacted systems are being restored.” By early August, federal responders, including personnel from the NSA, were on-site at the Kansas City facility, the source tells CSO. Located in Missouri, the KCNSC manufactures non-nuclear mechanical, electronic, and engineered material components used in US nuclear defense systems. It also provides specialized technical services, including metallurgical analysis, analytical chemistry, environmental testing, and simulation modeling. Roughly 80% of the non-nuclear parts in the nation’s nuclear stockpile originate from KCNSC. While most design and programmatic details remain classified, the plant’s production role makes it one of the most sensitive facilities in the federal weapons complex. China or Russia? Conflicting attribution Microsoft attributed the broader wave of SharePoint exploitations to three Chinese-linked groups: Linen Typhoon, Violet Typhoon, and a third actor it tracks as Storm-2603. The company said the attackers were preparing to deploy Warlock ransomware across affected systems. However, the source familiar with the Kansas City incident tells CSO that a Russian threat actor, not a Chinese one, was responsible for the intrusion. Cybersecurity company Resecurity, which was monitoring the SharePoint exploitations, tells CSO that its own data pointed primarily to Chinese nation-state groups, but it does not rule out Russian involvement. Resecurity’s researchers say that while Chinese groups appeared to have developed and deployed the initial zero-day, financially motivated Russian actors may have independently reproduced the exploit before technical details began circulating in late June. In May, researchers at Viettel Cyber Security demonstrated an attack chaining two SharePoint flaws, CVE-2025-49706 and CVE-2025-49704, at Pwn2Own Berlin. Resecurity researchers tell CSO that those demonstrations likely accelerated the reverse-engineering of the vulnerabilities by multiple threat actors. Resecurity’s analysts observed early-stage scanning and exploitation activity from infrastructure located in Taiwan, Vietnam, South Korea, and Hong Kong, a distribution pattern consistent with tactics used by Chinese advanced persistent threat (APT) groups to disguise attribution. “The root cause of the SharePoint exploitation is closely related to misuse of the Microsoft Active Protections Program (MAPP) by China,” Resecurity researchers tell CSO. “The most probable perpetrators are Chinese nation-state actors such as Linen Typhoon and Violet Typhoon.” Still, they say that yet another way that Russia-based threat actors could have acquired knowledge of the vulnerability early on was through underground exchanges or by analyzing network scanning data once the exploit became known. The transition from zero-day to N-day status, they say, opened a window for secondary actors to exploit systems that had not yet applied the patches. Could the attack have reached operational systems? The breach targeted the IT side of the Kansas City campus, but the intrusion raises the question of whether attackers could have moved laterally into the facility’s operational technology (OT) systems, the manufacturing and process control environments that directly support weapons component production. OT cybersecurity specialists interviewed by CSO say that KCNSC’s production systems are likely air-gapped or otherwise isolated from corporate IT networks, significantly reducing the risk of direct crossover. Nevertheless, they caution against assuming such isolation guarantees safety. “We have to really consider and think through how state actors potentially exploit IT vulnerabilities to gain access to that operational technology,” Jen Sovada, general manager of public sector operations at Claroty, speaking generally and not about the specific incident, tells CSO. “When you have a facility like the KCNSC where they do nuclear weapons lifecycle management — design, manufacturing, emergency response, decommissioning, supply chain management — there are multiple interconnected functions,” Sovada says. “If an actor can move laterally, they could impact programmable logic controllers that run robotics or precision assembly equipment for non-nuclear weapon components.” Such access, Sovada adds, could also affect distribution control systems that oversee quality assurance, or supervisory control and data acquisition (SCADA) systems that manage utilities, power, and environmental controls. “It’s broader than just an IT vulnerability,” she says. IT/OT convergence and the zero-trust gap The Kansas City incident highlights a systemic problem across the federal enterprise: the disconnect between IT and OT security practices. While the federal government has advanced its zero-trust roadmap for traditional IT networks, similar frameworks for operational environments have lagged, although recent developments point to progress on that front. “There’s an IT fan chart that maps all of the controls for zero trust, segmentation, authentication, and identity management,” Sovada says. “But there’s also an OT fan chart being developed by the Department of Defense that will define comparable controls for zero trust in operational technology. The goal is to marry the two, so that zero trust becomes comprehensive across all network types.” That alignment, she says, is essential to preventing intrusions like the one that struck KCNSC from cascading into physical operations. Even non-classified data theft holds strategic value If the source’s claim of Russian involvement is accurate, the attackers may have been financially motivated ransomware operators rather than state intelligence services. But even in that scenario, the data they accessed could still carry strategic value. “It would make sense that if it were a ransomware actor and they got this kind of data about nuclear weapons manufacturing, they might pause and hand it off to the appropriate Russian government officials or experts,” Sovada tells CSO. Although there is no evidence that classified information was compromised, even unclassified technical data can have significant implications. “It could be something as simple as requirements documents that may not be classified but reveal the level of precision required for components,” Sovada says. “In weapons manufacturing, a millimeter difference can change a device’s trajectory or the reliability of its arming mechanism.” Such information could aid adversaries in understanding US weapons tolerances, supply chain dependencies, or manufacturing processes, all of which are sensitive even if not formally secret. Whether the intruders were Chinese state actors or Russian cybercriminals, the Kansas City breach exposes the fragile intersection of IT and operational security across critical defense infrastructure. As Sovada stresses, “We can’t just think of zero trust as an IT concept anymore. It has to extend into the physical systems that underpin national defense.” Update: The Department of Energy (DOE) confirmed that it is furloughing the vast major of the NNSA’s workers. DOE spokesperson said, “Since its creation in 2000, NNSA has never before furloughed federal workers during funding lapses. We are left with no choice this time. We’ve extended funding as long as we could.” SUBSCRIBE TO OUR NEWSLETTER From our editors straight to your inbox Get started by entering your email address below. Please enter a valid email address Subscribe&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html"/><published>2025-10-21T15:51:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657302</id><title>Apple alerts exploit developer that his iPhone was targeted with gov spyware</title><updated>2025-10-21T17:11:19.835234+00:00</updated><content>&lt;doc fingerprint="b55088010b37f1da"&gt;
  &lt;main&gt;&lt;p&gt;Earlier this year, a developer was shocked by a message that appeared on his personal phone: “Apple detected a targeted mercenary spyware attack against your iPhone.”&lt;/p&gt;&lt;p&gt;“I was panicking,” Jay Gibson, who asked that we don’t use his real name over fears of retaliation, told TechCrunch.&lt;/p&gt;&lt;p&gt;Gibson, who until recently built surveillance technologies for Western government hacking tools maker Trenchant, may be the first documented case of someone who builds exploits and spyware being themselves targeted with spyware.&lt;/p&gt;&lt;p&gt;“What the hell is going on? I really didn’t know what to think of it,” said Gibson, adding that he turned off his phone and put it away on that day, March 5. “I went immediately to buy a new phone. I called my dad. It was a mess. It was a huge mess.”&lt;/p&gt;&lt;p&gt;At Trenchant, Gibson worked on developing iOS zero-days, meaning finding vulnerabilities and developing tools capable of exploiting them that are not known to the vendor who makes the affected hardware or software, such as Apple.&lt;/p&gt;&lt;p&gt;“I have mixed feelings of how pathetic this is, and then extreme fear because once things hit this level, you never know what’s going to happen,” he told TechCrunch.&lt;/p&gt;&lt;p&gt;But the ex-Trenchant employee may not be the only exploit developer targeted with spyware. According to three sources who have direct knowledge of these cases, there have been other spyware and exploit developers in the last few months who have received notifications from Apple alerting them that they were targeted with spyware.&lt;/p&gt;&lt;p&gt;Apple did not respond to a request for comment from TechCrunch.&lt;/p&gt;&lt;head rend="h4"&gt;Contact Us&lt;/head&gt;Do you have more information about the alleged leak of Trenchant hacking tools? Or about this developer’s story? From a non-work device, you can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, or via Telegram, Keybase and Wire @lorenzofb, or by email.&lt;p&gt;The targeting of Gibson’s iPhone shows that the proliferation of zero-days and spyware is starting to ensnare more types of victims.&lt;/p&gt;&lt;p&gt;Spyware and zero-day makers have historically claimed their tools are only deployed by vetted government customers against criminals and terrorists. But for the past decade, researchers at the University of Toronto’s digital rights group Citizen Lab, Amnesty International, and other organizations, have found dozens of cases where governments used these tools to target dissidents, journalists, human rights defenders, and political rivals all over the world.&lt;/p&gt;&lt;p&gt;The closest public cases of security researchers being targeted by hackers happened in 2021 and 2023, when North Korean government hackers were caught targeting security researchers working in vulnerability research and development.&lt;/p&gt;&lt;head rend="h2"&gt;Suspect in leak investigation&lt;/head&gt;&lt;p&gt;Two days after receiving the Apple threat notification, Gibson contacted a forensic expert with extensive experience investigating spyware attacks. After performing an initial analysis of Gibson’s phone, the expert did not find any signs of infection, but still recommended a deeper forensic analysis of the exploit developer’s phone.&lt;/p&gt;&lt;p&gt;A forensic analysis would have entailed sending the expert a complete backup of the device, something Gibson said he was not comfortable with.&lt;/p&gt;&lt;p&gt;“Recent cases are getting tougher forensically, and some we find nothing on. It may also be that the attack was not actually fully sent after the initial stages, we don’t know,” the expert told TechCrunch.&lt;/p&gt;&lt;p&gt;Without a full forensic analysis of Gibson’s phone, ideally one where investigators found traces of the spyware and who made it, it’s impossible to know why he was targeted or who targeted him.&lt;/p&gt;&lt;p&gt;But Gibson told TechCrunch that he believes the threat notification he received from Apple is connected to the circumstances of his departure from Trenchant, where he claims that the company designated him as a scapegoat for a damaging leak of internal tools.&lt;/p&gt;&lt;p&gt;Apple sends out threat notifications specifically for when it has evidence that a person was targeted by a mercenary spyware attack. This kind of surveillance technology is often invisibly and remotely planted on someone’s phone without their knowledge by exploiting vulnerabilities in the phone’s software, exploits that can be worth millions of dollars and can take months to develop. Law enforcement and intelligence agencies typically have the legal authority to deploy spyware on targets, not the spyware makers themselves.&lt;/p&gt;&lt;p&gt;Sara Banda, a spokesperson for Trenchant’s parent company L3Harris, declined to comment for this story when reached by TechCrunch before publication.&lt;/p&gt;&lt;p&gt;A month before he received Apple’s threat notification, when Gibson was still working at Trenchant, he said he was invited to go to the company’s London office for a team-building event.&lt;/p&gt;&lt;p&gt;When Gibson arrived February 3, he was immediately summoned into a meeting room to speak via video call with Peter Williams, Trenchant’s then-general manager who was known inside the company as “Doogie.” (In 2018, defense contractor L3Harris acquired zero-day makers Azimuth and Linchpin Labs, two sister startups that merged to become Trenchant.)&lt;/p&gt;&lt;p&gt;Williams told Gibson the company suspected he was double employed and was thus suspending him. All of Gibson’s work devices would be confiscated and analyzed as part of an internal investigation into the allegations. Williams could not be reached for comment.&lt;/p&gt;&lt;p&gt;“I was in shock. I didn’t really know how to react because I couldn’t really believe what I was hearing,” said Gibson, who explained that a Trenchant IT employee then went to his apartment to pick up his company-issued equipment.&lt;/p&gt;&lt;p&gt;Around two weeks later, Gibson said Williams called and told him that following the investigation, the company was firing him and offering him a settlement agreement and payment. Gibson said Williams declined to explain what the forensic analysis of his devices had found, and essentially told him he had no choice but to sign the agreement and depart the company.&lt;/p&gt;&lt;p&gt;Feeling like he had no alternative, Gibson said he went along with the offer and signed.&lt;/p&gt;&lt;p&gt;Gibson told TechCrunch he later heard from former colleagues that Trenchant suspected he had leaked some unknown vulnerabilities in Google’s Chrome browser, tools that Trenchant had developed. Gibson, and three former colleagues of his, however, told TechCrunch he did not have access to Trenchant’s Chrome zero-days, given that he was part of the team exclusively developing iOS zero-days and spyware. Trenchant teams only have strictly compartmentalized access to tools related to the platforms they are working on, the people said.&lt;/p&gt;&lt;p&gt;“I know I was a scapegoat. I wasn’t guilty. It’s very simple,” said Gibson. “I didn’t do absolutely anything other than working my ass off for them.”&lt;/p&gt;&lt;p&gt;The story of the accusations against Gibson’ and his subsequent suspension and firing was independently corroborated by three former Trenchant employees with knowledge.&lt;/p&gt;&lt;p&gt;Two of the other former Trenchant employees said they knew details of Gibson’s London trip and were aware of suspected leaks of sensitive company tools.&lt;/p&gt;&lt;p&gt;All of them asked not to be named but believe Trenchant got it wrong.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2025/10/21/apple-alerts-exploit-developer-that-his-iphone-was-targeted-with-government-spyware/"/><published>2025-10-21T15:52:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657345</id><title>Ask HN: Our AWS account got compromised after their outage</title><updated>2025-10-21T17:11:19.473464+00:00</updated><content>&lt;doc fingerprint="33ca051f2b70648d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Could there be any link between the two events?&lt;/p&gt;
      &lt;p&gt;Here is what happened:&lt;/p&gt;
      &lt;p&gt;Some 600 instances were spawned within 3 hours before AWS flagged it off and sent us a health event. There were numerous domains verified and we could see SES quota increase request was made.&lt;/p&gt;
      &lt;p&gt;We are still investigating the vulnerability at our end. our initial suspect list has 2 suspects. api key or console access where MFA wasn’t enabled.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45657345"/><published>2025-10-21T15:55:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657428</id><title>Is Sora the Beginning of the End for OpenAI?</title><updated>2025-10-21T17:11:19.332690+00:00</updated><content>&lt;doc fingerprint="b1a79a44b93167eb"&gt;
  &lt;main&gt;
    &lt;p&gt;On my podcast this week, I took a closer look at OpenAI’s new video generation model, Sora 2, which can turn simple text descriptions into impressively realistic videos. If you type in the prompt “a man rides a horse which is on another horse,” for example, you get, well, this:&lt;/p&gt;
    &lt;p&gt;AI video generation is both technically interesting and ethically worrisome in all the ways you might expect. But there’s another element of this story that’s worth highlighting: OpenAI accompanied the release of their new Sora 2 model with a new “social iOS app” called simply Sora.&lt;/p&gt;
    &lt;p&gt;This app, clearly inspired by TikTok, makes it easy for users to quickly generate short videos based on text descriptions and consume others’ creations through an algorithmically curated feed. The videos flying around this new platform are as outrageously stupid or morally suspect as you might have guessed; e.g.,&lt;/p&gt;
    &lt;p&gt;Or,&lt;/p&gt;
    &lt;p&gt;The Sora app, in other words, takes the already purified engagement that fuels TikTok and removes any last vestiges of human agency, resulting in an artificial high-octane slop.&lt;/p&gt;
    &lt;p&gt;It’s unclear whether this app will last. One major issue is the back-end expense of producing these videos. For now, OpenAI requires a paid ChatGPT Plus account to generate your own content. At the $20 tier, you can pump out up to 50 low-resolution videos per month. For a whopping $200 a month, you can generate more videos at higher resolutions. None of this compares favorably to competitors like TikTok, which are exponentially cheaper to operate and can therefore not only remain truly free for all users, but actually pay their creators.&lt;/p&gt;
    &lt;p&gt;Whether Sora lasts or not, however, is somewhat beside the point. What catches my attention most is that OpenAI released this app in the first place.&lt;/p&gt;
    &lt;p&gt;It wasn’t that long ago that Sam Altman was still comparing the release of GPT-5 to the testing of the first atomic bomb, and many commentators took Dario Amodei at his word when he proclaimed 50% of white collar jobs might soon be automated by LLM-based tools.&lt;/p&gt;
    &lt;p&gt;A company that still believes that its technology was imminently going to run large swathes of the economy, and would be so powerful as to reconfigure our experience of the world as we know it, wouldn’t be seeking to make a quick buck selling ads against deep fake videos of historical figures wrestling. They also wouldn’t be entertaining the idea, as Altman did last week, that they might soon start offering an age-gated version of ChatGPT so that adults could enjoy AI-generated “erotica.”&lt;/p&gt;
    &lt;p&gt;To me, these are the acts of a company that poured tens of billions of investment dollars into creating what they hoped would be the most consequential invention in modern history, only to finally realize that what they wrought, although very cool and powerful, isn’t powerful enough on its own to deliver a new world all at once.&lt;/p&gt;
    &lt;p&gt;In his famous 2021 essay, “Moore’s Law for Everything,” Altman made the following grandiose prediction:&lt;/p&gt;
    &lt;p&gt;“My work at OpenAI reminds me every day about the magnitude of the socioeconomic change that is coming sooner than most people believe. Software that can think and learn will do more and more of the work that people now do. Even more power will shift from labor to capital. If public policy doesn’t adapt accordingly, most people will end up worse off than they are today.”&lt;/p&gt;
    &lt;p&gt;Four years later, he’s betting his company on its ability to sell ads against AI slop and computer-generated pornography. Don’t be distracted by the hype. This shift matters.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://calnewport.com/is-sora-the-beginning-of-the-end-for-openai/"/><published>2025-10-21T16:01:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657431</id><title>Public Trust Demands Open-Source Voting Systems</title><updated>2025-10-21T17:11:18.886897+00:00</updated><content>&lt;doc fingerprint="af4eb91e1d45246c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Public Trust Demands Open-Source Voting Systems&lt;/head&gt;
    &lt;head rend="h4"&gt;Yesterday, news broke that Dominion Voting Systems was sold to a new company, Liberty Vote. Dominion, the second-largest voting systems vendor in the US that currently tabulates 1 in 5 American votes, is no more.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;âOn its website, Liberty Vote says, "We are turning the page and beginning the vital work of restoring faith in American elections."Â &lt;/p&gt;
    &lt;p&gt;There is indeed a crisis of trust in American elections. As the saying goes, trust takes years to build, seconds to break, and forever to repair. We urgently need a new strategy that repairs voter trust. American freedom and democracy depend on it.&lt;/p&gt;
    &lt;p&gt;We must start with a foundational commitment to transparency. Every voting machine vendor in the US claims to be transparent. It may come as a surprise, then, that most Americans vote on machines that run proprietary, secret software! A voting machine that every American can trust must run software that is fully open to public scrutiny.Â&lt;/p&gt;
    &lt;p&gt;Today, VotingWorks makes the only open-source voting equipment in the US. Open-source is the modern standard for public-trust and high-security software. Signal, the most secure and battle-tested messaging app, is open-source. The US Military recommends open-source. A modern voting system needs to be open-source.&lt;/p&gt;
    &lt;p&gt;Liberty Vote, and every other vendor of voting machines in the US, can fulfill their commitment to transparency by making their technology open-source today. If every vendor takes this opportunity, together, we can turn the page and launch a new generation of voting systems every American can trust.Â&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;About Voting Works&lt;/head&gt;
    &lt;p&gt;Read about the company: https://voting.works&lt;/p&gt;
    &lt;p&gt;View our complete source code: https://github.com/votingworks&lt;/p&gt;
    &lt;p&gt;Contact us with questions: hello@voting.worksÂ&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.voting.works/news/public-trust-demands-open-source-voting-systems"/><published>2025-10-21T16:01:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657922</id><title>The Greatness of Text Adventures</title><updated>2025-10-21T17:11:18.293470+00:00</updated><content>&lt;doc fingerprint="620a69566a30c499"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Greatness of Text Adventures&lt;/head&gt;
    &lt;p&gt;Text adventures are weird. They are so weird I don’t know how to write this article, so prepare to read something even more rambly than usual. Normally, when this happens, I decide not to publish the article. But&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I really want to share my enthusiasm for text adventures, and&lt;/item&gt;
      &lt;item&gt;I needed something to link to when I wrote that article on what I learned from creating my text adventure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s go!&lt;/p&gt;
    &lt;head rend="h1"&gt;Text adventures have awesome properties&lt;/head&gt;
    &lt;p&gt;Here’s a list of cool properties of text adventures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Freedom&lt;/head&gt;
    &lt;p&gt;Text adventures give the player a lot of freedom.&lt;/p&gt;
    &lt;p&gt;In graphical games, the player character can act in at best a handful of significantly different ways in any given situation1 Not counting small variations. Using different types of attack is still attacking, for example. Entering into dialogue is still one action, however deep the dialogue is.. In a text adventure, the player character typically has ten or twenty meaningfully different ways to act in any given situation.&lt;/p&gt;
    &lt;p&gt; This freedom is liberating: even though the player cannot literally do anything they want, a well-designed text adventure really makes it seem as though they could, by having programmed generic interactions between actions and objects. This freedom can also be used to good effect to give the user alternatives for solving puzzles.2 Such as when, faced with a foosball table, one solution can be to &lt;code&gt;TAKE BALL. PUT BALL IN GOAL&lt;/code&gt;.
&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-graphical and non-literal experiences&lt;/head&gt;
    &lt;p&gt;By being non-graphical, text adventures have the benefit of conveying things without having to represent them literally. In a graphical game, if we wish to represent a large city, we must model a literal large city, with thousands of buildings. In a text adventure, we can instead describe the city such that it feels large, whicle modelling only very few buildings.&lt;/p&gt;
    &lt;p&gt;Another example of the ingraphicality of the medium is that text adventures can represent non-visual effects; we can model other senses than vision, like smell and touch.&lt;/p&gt;
    &lt;p&gt;With text, we can also present an abstract space much easier than with graphics, which tend to the literal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cheaper dynamic environments&lt;/head&gt;
    &lt;p&gt;Text adventures can more easily support dynamic environments. In a graphical game, dynamic environments require complicated physics and animations. This – in addition to the effort of graphical and sound design – makes graphical games more expensive to create than text adventures. Many of the greatest text adventures of all time were developed as pet projects by single developers in their spare time.3 From what I understand, text adventures are even used to prototype narratives and mechanics in some big game studios.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mature technology&lt;/head&gt;
    &lt;p&gt;If all of the above is not intriguing enough, let’s also mention that text adventures are a mature technology. They were some of the first computer games ever created. It’s difficult to find a computer that cannot run text adventures.&lt;/p&gt;
    &lt;p&gt;Great! But what … are they?&lt;/p&gt;
    &lt;head rend="h1"&gt;Example interaction&lt;/head&gt;
    &lt;p&gt;I have played a few text adventures the past months, and here’s a brief excerpt from one4 The Dreamhold; Andrew Plotkin; self-published; 2004. Playable online.. This transcript begins when we have exited the starting area, and just begun to explore what lies beyond.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;CURVING HALL, WEST END&lt;/p&gt;
      &lt;p&gt;The curving corridor ends here, in a blank plaster wall relieved by a single window. The window looks out on a strange subterranean prospect to the west. You can return east, pass through the archway to the north, or try the closed door to the south.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; look at window&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A broad glass pane looks out over a dark cavern. This end of the corridor seems to be deep underground. A natural cave spreads out below; its mammoth columns and cascades of flowstone are only dimly picked out by the candlelight behind you. The farther reaches are hidden in darkness. But the shining waterfall before you is easily visible. A stream pours in from your right, tumbles from a ledge, and plummets into a pit beneath the window. The water seems to sparkle with some natural phosphorescence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; break window&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The glass ignores your assault.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; enter archway&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ARBORETUM&lt;/p&gt;
      &lt;p&gt;The walls here are painted white and are utterly undecorated. An archway leads south. In the center of the room stands an earthenware tub. From it sprouts a tangle of vines, broad fan-like leaves, and an enormous golden flower. The luxuriant growth clashes, you think, with the room’s stark emptiness.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; look at flower&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The plant sits alone in its pot, in the center of the empty room. The golden flower is nearly the size of your head. It rises from a tangle of vines and leaves that would better suit an unexplored south-coast swamp than someone’s home.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; take flower&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The plant’s stems look too thick to break.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; smell flower&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You lean forward and take a deep breath of the flower’s aroma.&lt;/p&gt;
      &lt;p&gt;Memory nearly bursts through.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This moves us to&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;DANK JUNGLE (IN THE EARTHENWARE TUB)&lt;/p&gt;
      &lt;p&gt;You are standing waist-deep in an empty earthenware tub, in the midst of a trackless jungle.&lt;/p&gt;
      &lt;p&gt;Heavy, entangled trunks and vines loom in every direction, as far as you can see – and you can’t see far. The sky is entirely obscured by layers of vegetation. Only a murky green light filters down to you. But golden flowers dot the trees, like bursts of sun.&lt;/p&gt;
      &lt;p&gt;An iron key is balanced neatly on the edge of the tub.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; take key&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Taken.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; get out of tub&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You swing your legs out of the tub, and find the world swinging around you…&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This moves us to&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ARBORETUM&lt;/p&gt;
      &lt;p&gt;The walls of the room are painted in green slashes, like the trunks of uncounted trees, and splashed with gold. An archway leads south. A hint of sweet-rank perfume touches you as you breathe.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&amp;gt; inventory&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You are carrying an iron key.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We explored a space in a building set into a cave, and smelling the flower got us transported … into a memory? Or literally miniaturised and into the tub? We were able to bring the iron key, after all. Did we even emerge into the same room, given that the description changed? It’s not made clear, and this ambiguity is okay in text – each reader will get to play their own preferred version of the game.&lt;/p&gt;
    &lt;p&gt;Of course, this is a game by an author that is known for abstract mechanics. There are also games that are far more traditional in their mechanics. When I stop playing Plundered Hearts5 Plundered Hearts; Infocom; Infocom; 1987. Playable online. I can almost hear the chirping of the jungle and the lapping of the waves against the island where I’m stalking around a large house trying to figure out how to get into the ongoing ball.&lt;/p&gt;
    &lt;p&gt;It shouldn’t come as a surprise. Novels – and spoken stories – have transported readers to fantastical locations for ages. But I still didn’t know text adventures could be so immersive.&lt;/p&gt;
    &lt;head rend="h1"&gt;Interaction in text adventures&lt;/head&gt;
    &lt;p&gt;The bright reader has already figured out how interaction works in text adventures: the game prints a brief paragraph of text describing the players’ immediate environment or the result of their last action, followed by a prompt sign, after which the player types in what they want to do next.6 There are technical differences here between narrator, player character, and player but we’ll brush past that for the moment.&lt;/p&gt;
    &lt;p&gt; The instructions the player types into the prompt look like English, but they follow a fairly strict format used in almost all text adventures by convention. These are recognised by a component of the game called the parser. Despite the strict format, some of the joy of playing these text adventures is trying something crazy, like in Plundered Hearts when we stand on the balcony above the ballroom, see the rope on which the chandelier is attached, type &lt;code&gt;SWING ON
ROPE&lt;/code&gt; … and the player character comes crashing down onto the dancing guests
with a big “Aiieeee!” It’s outside the conventional commands, but it makes sense
and the author catered for it.
&lt;/p&gt;
    &lt;p&gt;Text adventures typically take a simulationist approach to narration. This means the author has not specified what happens in any given situation. Instead, what happens next is determined mechanistically by the player’s actions given the current world state. Maybe we need to cross a violent river, but it’s up to us if we want to block it, re-route it, magically freeze it, or jetpack over it. The progress is gated on being able to cross, not how that ability is achieved.&lt;/p&gt;
    &lt;p&gt;The violent river is an example of a puzzle, which are usually strewn throughout text adventures. These puzzles are used to pace the narrative, and avoid dumping everything on the player at once. Obstacles get placed in the way of progress, and the player needs to find ways around them to experience more of the story.&lt;/p&gt;
    &lt;p&gt;The puzzle-pacing of text adventures means they can often contain a lot more detail and flavour text than a corresponding novel, because the player slowly uncovers the text they are most interested in, and are free to ignore the rest.7 Well, any information needed to solve puzzles cannot be ignored, of course. Old school games tended to hide such information in the most innocuous places, requiring careful searching of every nook and cranny to proceed. Modern games, by contrast, tend to be more forgiving and hint more actively at which locations are important to search.&lt;/p&gt;
    &lt;head rend="h1"&gt;Puzzle types in text adventures&lt;/head&gt;
    &lt;p&gt;At some point I would like to do a formal comparison of text adventure puzzles by way of grounded theory8 That is the only hammer I have for extracting structure and meaning from unstructured accounts.. That is out of scope of this article, so we’ll look at one sloppy categorisation of interest:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lock-and-key: we must possess a specific item to get past an obstacle. We find the item by observing the environment carefully.&lt;/item&gt;
      &lt;item&gt;Lock-and-password: we must perform a specific action to get past an obstacle. We learn about the action by observing the environment. Tougher puzzles require astuter observation, sometimes even catching on to background patterns.&lt;/item&gt;
      &lt;item&gt;Change-the-self: we must alter our appearance to get past an obstacle. This can be considered a combination of key- and password-based puzzles: we must find the item that masks our appearance, and then be perceptive enough to know when to wear it.&lt;/item&gt;
      &lt;item&gt;Change-an-item: converting one item to another is fairly common, either by performing the right action on it or by combining it with the right other items.&lt;/item&gt;
      &lt;item&gt;Change-the-environment: we must rearrange the environment in a way that makes actions possible that were previously unthinkable, unreasonable, or nonsensical.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are some puzzle types that might not fit into the above categories.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Waiting-based puzzles: where just quietly waiting for one or more turns makes something happen.&lt;/item&gt;
      &lt;item&gt;Persistance-based puzzles: where doing something once has an effect that is not quite useful, but doing it many times has an effect that is very useful.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Either of these could be considered lock-and-password puzzles, only with the password being to repeat an action.&lt;/p&gt;
    &lt;p&gt;We could also categorise puzzles into what sort of simulation they are based on, e.g. whether they simulate human senses9 Requiring observing the environment in particular ways, or manipulating the senses of others such as shouting to wake someone up., or physical systems10 Getting items to increase leverage, throwing things at other things to dislodge them, getting something large to block a view, etc., or machines11 Figuring out how to run a bread dispensing machine, or how an orrery evolves to bring an item closer to the ground., or magic systems12 These simulations need to be internally consistent, but they may not correspond to any natural laws we know from our world., or the desires of other people13 Giving people things, showing them things, feeding animals, etc..&lt;/p&gt;
    &lt;head rend="h1"&gt;Go play some text adventures!&lt;/head&gt;
    &lt;p&gt;Text adventures are generally free14 In the sense of gratis, but also sometimes libre.. They spawned out of the culture of free sharing at mit and other universities, and they are passion projects to date. Most games are released in competitions, and all big competitions have rules that stipulate playing the game must be free. There are good reasons to try playing them, and few reasons not to.&lt;/p&gt;
    &lt;p&gt;At this point, you know more about text adventures than I did when I started out. Here are some games I have played and which I think aren’t terrible for a beginner:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Glowgrass (1997): You crash land in an overgrown Ancient settlement, and the eerie starts to take on a familiar face as you end up learning more about what has happened.&lt;/item&gt;
      &lt;item&gt;Violet (2008): If you can successfully type the command &lt;code&gt;WRITE&lt;/code&gt;you win the game. But boy are there many distractions when a distractable person just needs to write.&lt;/item&gt;
      &lt;item&gt;The Dreamhold (2004): You wake up in a wizard’s Dreamhold with no memory. As you explore, some things come back to you. Eventually you gather all masks and you figure out what you need to do to finish what you started.&lt;/item&gt;
      &lt;item&gt;Plundered Hearts (1987): You are a woman stuck between men who have violent business to finish with each other. Somehow you end up rescuing your pirate hero multiple times.&lt;/item&gt;
      &lt;item&gt;Lost Pig (2007): You are an orc and you lost your pig. Now you must find it. You’ll make a friend along the way. This game has fairly difficult puzzles, but it is incredibly responsive; even silly actions (&lt;code&gt;BURN TAPESTRY&lt;/code&gt;) have effects programmed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; These are all games that are either merciful or polite on the Zarfian cruelty scale, meaning you might be able to die, but the game will not let you continue to play after failure has become certain.15 In contrast, games that are cruel will happily let you put the game into an unwinnable state with no way to tell that that has even happened. Create a savegame after every time you’ve made progress (with the &lt;code&gt;SAVE&lt;/code&gt; command) and you’ll never lose much
progress.
&lt;/p&gt;
    &lt;p&gt; Zarf’s if cheat sheet contains some useful commands that tend to work in many games. It’s fine to not be able to solve a puzzle. Sometimes they are hard. Make sure you have examined everything carefully for hints. If you still cannot progress, see if you can play with a friend. If you have no friends, look up hints or a walkthrough.16 My personal rule is that if I have come up with a solution that ought to have worked, but wasn’t programmed with the appropriate response, I allow myself to cheat and look at the solution in a walkthrough. Often the solution is the same as I had thought of, only it expected a variation on the wording I used. E.g. &lt;code&gt;WAVE MIRROR IN MOONLIGHT&lt;/code&gt; might not work, but &lt;code&gt;PUT
MIRROR IN MOONBEAM&lt;/code&gt; does.
&lt;/p&gt;
    &lt;p&gt;Give it a fair shot. It takes a few games until one is familiar enough with the conventions to navigate the game smoothly, but it’s worth it, I think. Opens up a whole world.&lt;/p&gt;
    &lt;head rend="h1"&gt;Appendix A: Text adventure vs. interactive fiction&lt;/head&gt;
    &lt;p&gt;A brief note on terminology is in place. I call these games text adventures, but a lot of people these days prefer the term interactive fiction. Here’s how that happened.&lt;/p&gt;
    &lt;p&gt;In the mid-1970s, a couple of avid cavers programmed a text-based virtual cave exploration simulator on a large mainframe computer. They named the simulator Colossal Cave Adventure, and it got colloquially known as Adventure. This was the first game of its kind, and it spawned an entirely new genre. Much like the game Rogue spawned the genre of roguelikes, the game Adventure spawned the genre of text adventures.&lt;/p&gt;
    &lt;p&gt;Sometimes these game-defined genres change names. First person shooters used to be known as Doom clones, but as they branched away from the formula of the original Doom, they needed a more generic name. This has happened also with text adventures. These types of games soon branched out to include less adventurous mechanics, and a new name was needed. One big developer in particular, Infocom, went with interactive fiction, in part to try to sell the games to a broader market of readers. That name stuck around as the more inclusive term.17 Some interactive fiction games aren’t even parser-based, but choice-based: instead of asking the player to type in what they want to do, they stop at story beats and give the player a list of alternatives for how to proceed.&lt;/p&gt;
    &lt;p&gt;There are three reasons I still speak of text adventures:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I have not yet branched out to these other games. The games I’ve played (and started to make) are still fairly firmly planted in the simulationist, puzzle-paced Adventure tradition. If I specifically played the games that came out just after Doom and mimicked it closely, I would similarly say I play Doom clones, not first person shooters.&lt;/item&gt;
      &lt;item&gt;I dislike the term interactive fiction because it’s provably too long. Just as with “first person shooter”, which is universally abbreviated fps, nobody bothers saying or writing “interactive fiction”. They abbreviate it to if instead. And that is such an overloaded combination of characters that it becomes impossible to perform a web search for. I don’t like that.&lt;/item&gt;
      &lt;item&gt;A minor reason is that if I wanted a good work of fiction, I’d pick up a book. I play text adventures for the interactivity in a simulated world, and there I want good game mechanics before a good narrative. Calling it interactive fiction puts the emphasis on the narrative over the game, which is not what I’m looking for.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/the-greatness-of-text-adventures"/><published>2025-10-21T16:39:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45658056</id><title>Fallout from the AWS Outage: Smart Mattresses Go Rogue and Ruin Sleep Worldwide</title><updated>2025-10-21T17:10:10.932875+00:00</updated><content/><link href="https://quasa.io/media/the-strangest-fallout-from-the-aws-outage-smart-mattresses-go-rogue-and-ruin-sleep-worldwide"/><published>2025-10-21T16:50:40+00:00</published></entry></feed>