<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-27T09:13:51.584411+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45683536</id><title>Sphere Computer ‚Äì The Innovative 1970s Computer Company Everyone Forgot</title><updated>2025-10-27T09:14:01.465112+00:00</updated><content>&lt;doc fingerprint="1b973fbda2d17f40"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;head rend="h3"&gt;About Sphere&lt;/head&gt;
          &lt;head rend="h4"&gt;The Sphere 1 was a product of Sphere Corporation, launched in 1975 by Mike Wise in Bountiful, Utah. It was an all-in-one integrated microcomputer based on the new Motorola 6800 platform. It was ahead of its time; it was also delayed, somewhat difficult to use and frequently flaky. Sphere Corp itself disappeared in 1977, and Sphere was soon a punchline, then a footnote. &lt;lb/&gt;The Sphere computer may have been at its heart a hobbyist's machine, but its design and capabilities prefigured the mass-market computers that would become ubiquitous just a few years later. &lt;lb/&gt;More Sphere history....&lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;head rend="h3"&gt;About this project&lt;/head&gt;
          &lt;head rend="h4"&gt;I'm Ben Zotto, a research historian, engineer, and writer from California. I'm assembling the story of Sphere-- a unique computer, and the unique company who built it and the people who used it. In addition to maintaining this site, I've written a book that will document thousands of hours of research; check it out!.&lt;lb/&gt;As part of this project, I've built a virtual web emulator for the standard Sphere configuration, including the first new game for the Sphere in 40+ years. √∞¬æ √∞ Try computing like it's 1975. I've also created the first new Sphere hardware since the 1970s.&lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;head rend="h3"&gt;Do you have Sphere hardware, software, paperwork or stories?&lt;/head&gt;
          &lt;head rend="h4"&gt;I'd dearly love to hear from you. Sphere systems and kits were produced in small numbers; they were often unloved even in their time and are now quite obscure. Some material has been thankfully preserved, but there's plenty I haven't had the opportunity to study and archive. If you're looking to get your old Sphere stuff to a good home, have some unseen material in a closet, or have stories of working with or for these computers, let's talk. &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sphere.computer/"/><published>2025-10-23T16:09:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45684253</id><title>A bug that taught me more about PyTorch than years of using it</title><updated>2025-10-27T09:14:01.212797+00:00</updated><content>&lt;doc fingerprint="3f58903d88a44cb0"&gt;
  &lt;main&gt;&lt;p&gt;a loss plateau that looked like my mistake turned out to be a PyTorch bug. tracking it down meant peeling back every layer of abstraction, from optimizer internals to GPU kernels.&lt;/p&gt;&lt;p&gt;&lt;code&gt;Expected to fix: my hyperparameters. Actually had to fix: PyTorch backend.&lt;/code&gt;&lt;/p&gt;&lt;p&gt;My training loss plateaued and wouldn‚Äôt budge. Obviously I‚Äôd screwed something up. I tried every hyperparameter combination, rewrote my loss function, spent days assuming I‚Äôd made some stupid mistake. Because it‚Äôs always user error.&lt;/p&gt;&lt;p&gt;This time, it wasn‚Äôt. It was a niche PyTorch bug that forced me through layers of abstraction I normally never think about: optimizer internals, memory layouts, dispatch systems, kernel implementations. Taught me more about the framework than years of using it.&lt;/p&gt;&lt;p&gt;I had a surprisingly fun time with this bug hunt and wrote up the whole investigation step-by-step, explaining framework internals as they become necessary to crack the case. If you enjoy debugging mysteries or find that tracking down bugs teaches you more than docs ever could, this might resonate. üïµÔ∏è‚ôÄÔ∏è&lt;/p&gt;&lt;p&gt;Debugging post-mortems sometimes make me worry I wouldn‚Äôt have been smart enough to figure them out myself. So I structured this walkthrough to show the reasoning behind each step: what clues suggested each move, why I tested that hypothesis, why certain results pointed where they did. While the investigation took time and persistence, it didn‚Äôt require any particular expertise or wizardry‚Äî just observation and willingness to keep digging. I‚Äôve included background knowledge exactly when you need it to understand the next step‚Äîthink of it as an excuse to learn (or re-learn) PyTorch internals through a real problem. If you‚Äôd prefer to jump straight to reproducing the bug yourself, check out the minimal reproduction script and walkthrough on GitHub. Otherwise, join me on the investigation!&lt;/p&gt;&lt;p&gt;Table of Contents: ü§î The Mystery: A Plateauing Loss‚Ä¶‚Ä¶ üîé Isolating the Problem‚Ä¶‚Ä¶ üíª Device-Specific Differences‚Ä¶‚Ä¶ ‚å∫ Tensor Memory Layouts‚Ä¶‚Ä¶ üíî Identifying the Broken Operations‚Ä¶‚Ä¶. üçé Inside the Kernel Implementation‚Ä¶‚Ä¶ üïµÔ∏è‚ôÄÔ∏è Case Closed&lt;/p&gt;&lt;p&gt;The Bug: A PyTorch GPU kernel bug silently failed when writing to non-contiguous memory, causing my model‚Äôs encoder weights to freeze during training on Apple Silicon (MPS backend, PyTorch &amp;lt;2.4).&lt;/p&gt;&lt;p&gt;The Technical Details: PyTorch‚Äôs MPS (Apple Silicon GPU) backend had a kernel bug where &lt;code&gt;addcmul_&lt;/code&gt; and &lt;code&gt;addcdiv_&lt;/code&gt; operations silently fail when writing to non-contiguous output tensors.&lt;/p&gt;&lt;p&gt;Why It Caused the Training Plateau:&lt;/p&gt;&lt;code&gt;exp_avg&lt;/code&gt; and &lt;code&gt;exp_avg_sq&lt;/code&gt; became non-contiguous)&lt;code&gt;addcmul_&lt;/code&gt;/&lt;code&gt;addcdiv_&lt;/code&gt; don‚Äôt handle non-contiguous outputs correctly&lt;code&gt;exp_avg_sq.addcmul_()&lt;/code&gt; doesn‚Äôt update ‚Üí value stays zero, then the parameter update via &lt;code&gt;addcdiv_&lt;/code&gt; also fails ‚Üí complete silent freeze&lt;p&gt;The Fix:&lt;/p&gt;&lt;code&gt;addcmul_&lt;/code&gt;/&lt;code&gt;addcdiv_&lt;/code&gt;)&lt;p&gt;Current Status: Random operations (&lt;code&gt;normal_&lt;/code&gt;, &lt;code&gt;uniform_&lt;/code&gt;, etc.) still have this bug on macOS &amp;lt; 15 as of PyTorch 2.10 (I submitted a PR to fix this). Other MPS operations may be affected.&lt;/p&gt;&lt;p&gt;Reproduction: A minimal reproduction script &amp;amp; walkthrough is available at https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug.&lt;/p&gt;&lt;p&gt;Training loss plateaued way too early. This felt like a standard hyperparameter issue- but I‚Äôd trained this same architecture on similar data with similar hyperparameters countless times and hit much lower losses.&lt;/p&gt;&lt;p&gt;What had changed? Those runs were months old. I tried reproducing them exactly, but couldn‚Äôt pin down the exact environment‚Äîthe codebase had evolved through multiple projects, refactors, and dependency updates. Without a clean ‚Äúbefore vs after,‚Äù I had to debug forward.&lt;/p&gt;&lt;p&gt;The architecture itself is straightforward: a two-layer sparse autoencoder (encoder ‚Äì&amp;gt; sparse hidden layer ‚Äì&amp;gt; decoder). However, it has some training quirks the could be potential culprits: the hidden layer uses TopK sparsity, where only the k largest activations remain (others are zeroed); the training process includes some manual gradient adjustments (gradient clipping for stability and modifications to decoder weight gradients); there‚Äôs an auxiliary loss term to encourage feature activation.&lt;/p&gt;&lt;p&gt;Even though I thought my initial hyperparameters were already well-tested, I tried everything: varied learning rates, tested different schedules, tried different k values and hidden dimensions, adjusted the auxiliary loss coefficients.&lt;/p&gt;&lt;p&gt;Nothing made a difference.&lt;/p&gt;&lt;p&gt;Meanwhile, my actual research sat on hold while I was stuck second-guessing everything: was my code broken? My data corrupted? And the creeping doubt- I‚Äôve been doing ML for years, why can‚Äôt I make a simple two-layer autoencoder train properly?&lt;/p&gt;&lt;p&gt;The model was small enough that I was training on my MacBook (using the Apple Silicon GPU) and simple enough I could actually inspect every parameter. So after the standard checks turned up nothing, I started looking at the weights directly.&lt;/p&gt;&lt;p&gt;I visualized the weights at initialization and after the first few training steps. The decoder weights were updating- values shifting, gradients being applied, nothing crazy. But the encoder weights‚Ä¶ weren‚Äôt updating at all. No NaNs, no suspicious patterns‚Ä¶ they just‚Ä¶ weren‚Äôt changing. They stayed exactly at their initialized values, down to the last decimal place.&lt;/p&gt;&lt;p&gt;Both layers participate in the same forward and backward pass. Why would one update and the other freeze completely?&lt;/p&gt;&lt;p&gt;First check: are gradients even making it back to the encoder? The TopK sparsity should make gradients sparse‚Äîonly the k activated features get gradients through backprop, the rest are zeroed. But maybe I messed up the implementation so that no encoder gradients flow at all? Or the manual gradient adjustments I was making somehow blocked everything?&lt;/p&gt;&lt;p&gt;After &lt;code&gt;loss.backward()&lt;/code&gt;, the gradient statistics were:&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Encoder&lt;/cell&gt;&lt;cell role="head"&gt;Decoder&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Max Grad&lt;/cell&gt;&lt;cell&gt;2.35e6&lt;/cell&gt;&lt;cell&gt;6.64e6&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Sparsity&lt;/cell&gt;&lt;cell&gt;88.5% zeros&lt;/cell&gt;&lt;cell&gt;88.5% zeros&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The encoder gradients were there- and they were pretty big (as intended for my dataset)! And they were sparse (majority zeros) which was also expected, but there were still plenty of non-zero gradients. So gradients are definitely being calculated.&lt;/p&gt;&lt;p&gt;Since the gradients exist but weights aren‚Äôt updating, the optimizer must be doing something wrong. Testing with a simpler optimizer, stochastic gradient descent (SGD):&lt;/p&gt;&lt;code&gt;# Manual SGD update
with torch.no_grad():
    model.encoder.weight -= 0.001 * model.encoder.weight.grad
# Encoder weights change! ‚úì

# Torch SGD update
sgd_optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
sgd_optimizer.step()
# Encoder weights change! ‚úì

# But with Adam...
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer.step()
# Encoder weights don't change! ‚úó
&lt;/code&gt;&lt;p&gt;To understand what might be breaking, I need to understand what Adam actually does differently from simple gradient descent.&lt;/p&gt;&lt;p&gt;SGD updates all parameters the same way:&lt;/p&gt;&lt;code&gt;# SGD: one learning rate for everything
param = param - learning_rate * gradient
&lt;/code&gt;&lt;p&gt;This has a few problems:&lt;/p&gt;&lt;p&gt;Different parameters need different learning rates. Some parameters might consistently get gradients around 1000 while others get 0.01. With SGD‚Äôs fixed learning rate, you‚Äôre stuck: either you move too slowly on small gradients or you overshoot wildly on large ones.&lt;/p&gt;&lt;p&gt;The learning rate needs to change over time. Early in training, you want big steps to explore the space. Later, you need tiny steps to settle into a minimum. SGD requires manually decaying the learning rate on a schedule.&lt;/p&gt;&lt;p&gt;Adam maintains two pieces of state per parameter and uses two hyperparameters to control how these states evolve:&lt;/p&gt;&lt;p&gt;State variables (initialized to zero for each parameter):&lt;/p&gt;&lt;code&gt;exp_avg&lt;/code&gt;: Running average of gradients (first moment)&lt;code&gt;exp_avg_sq&lt;/code&gt;: Running average of squared gradients (second moment)&lt;p&gt;Hyperparameters (typically beta_1=0.9, beta_2=0.999):&lt;/p&gt;&lt;code&gt;beta_1&lt;/code&gt;: Decay rate for first moment (momentum)&lt;code&gt;beta_2&lt;/code&gt;: Decay rate for second moment (gradient magnitude history)&lt;p&gt;Here‚Äôs the simplified algorithm:&lt;/p&gt;&lt;p&gt;Initialize state (done once per parameter)&lt;/p&gt;&lt;code&gt;exp_avg = zeros_like(param)
exp_avg_sq = zeros_like(param)
step = 0
&lt;/code&gt;&lt;p&gt;Each training step:&lt;/p&gt;&lt;code&gt;# Update moments with exponential moving averages
exp_avg = beta_1 * exp_avg + (1 - beta_1) * grad
exp_avg_sq = beta_2 * exp_avg_sq + (1 - beta_2) * grad**2

# Update step count
# (It effectively starts at 1 to avoid division by zero in bias correction)
step += 1

# Bias correction
exp_avg_corrected = exp_avg / (1 - beta_1**step)
exp_avg_sq_corrected = exp_avg_sq / (1 - beta_2**step)

# Adaptive parameter update
param = param - lr * exp_avg_corrected / (sqrt(exp_avg_sq_corrected) + Œµ)
&lt;/code&gt;&lt;p&gt;What Each Moment Does:&lt;/p&gt;&lt;p&gt;First moment (&lt;code&gt;exp_avg&lt;/code&gt;): Smooths out noisy gradients by averaging recent directions‚Äîlike momentum in physics. When gradients oscillate (+10, -10, +8, -9‚Ä¶), the positive and negative values cancel out, revealing there‚Äôs no consistent direction. Beta_1=0.9 means ‚Äúkeep 90% of old momentum, add 10% of new gradient.‚Äù This smoothed momentum is what gets multiplied by the learning rate in the parameter update: &lt;code&gt;lr * exp_avg&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Second moment (&lt;code&gt;exp_avg_sq&lt;/code&gt;): Tracks typical gradient magnitude for each parameter by averaging squared gradients. Squaring removes the +/- sign (both +10 and -10 become 100), preventing cancellation. Beta_2=0.999 means ‚Äúkeep 99.9% of magnitude history, add 0.1% of new squared gradient.‚Äù This magnitude normalizes the momentum-based update: &lt;code&gt;lr * exp_avg / sqrt(exp_avg_sq)&lt;/code&gt;. Parameters with consistently large gradients get their updates scaled down (large denominator), while parameters with small gradients get boosted (small denominator). This is how Adam achieves adaptive per-parameter learning rates.&lt;/p&gt;&lt;p&gt;Epsilon (&lt;code&gt;Œµ=1e-8&lt;/code&gt;): Prevents division by zero.&lt;/p&gt;&lt;p&gt;Bias Correction:&lt;/p&gt;&lt;p&gt;Both moments start at zero, causing early estimates to be biased toward zero. The correction factor &lt;code&gt;(1 - Œ≤**step)&lt;/code&gt; provides a large boost early to counteract this, effectively ‚Äúwarming up‚Äù the optimizer over the first ~1000-3000 steps. As training progresses, the correction approaches 1 and has negligible effect.&lt;/p&gt;&lt;p&gt;The second moment works similarly. Without correction, &lt;code&gt;exp_avg_sq&lt;/code&gt; would be only 0.1% of gradient¬≤ at step 1, but bias correction restores it to the full value.&lt;/p&gt;&lt;p&gt;For a deeper dive into Adam‚Äôs design and intuition, as well as other optimizers that use momentum and adaptive learning rates (RMSprop, AdaGrad, etc.), check out Stanford‚Äôs CS231n notes on optimization.&lt;/p&gt;&lt;p&gt;Knowing what Adam should be doing, let‚Äôs look at the state it‚Äôs maintaining (those &lt;code&gt;exp_avg&lt;/code&gt; and &lt;code&gt;exp_avg_sq&lt;/code&gt; tensors that track momentum and variance) to see what it‚Äôs actually doing.&lt;/p&gt;&lt;p&gt;For our frozen encoder, the maximum values in each state tensor were:&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Encoder&lt;/cell&gt;&lt;cell role="head"&gt;Decoder&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;exp_avg&lt;/cell&gt;&lt;cell&gt;1.96e+05&lt;/cell&gt;&lt;cell&gt;1.70e+06&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;exp_avg_sq&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1.18e+11&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Wait, WHAT?! The encoder‚Äôs &lt;code&gt;exp_avg_sq&lt;/code&gt; is zero despite having momentum accumulated in &lt;code&gt;exp_avg&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;This feels mathematically impossible‚Ä¶ The second moment (&lt;code&gt;exp_avg_sq&lt;/code&gt;) is zero despite non-zero gradients. Since &lt;code&gt;exp_avg_sq&lt;/code&gt; stores squared gradients, it should NEVER be zero if gradients are non-zero.&lt;/p&gt;&lt;p&gt;And if it truly were zero, we‚Äôd see massive weight updates.&lt;/p&gt;&lt;code&gt;param_update = lr * exp_avg / (sqrt(exp_avg_sq) + Œµ) 
             = 0.001 * 1.96e5 / (sqrt(0) + 1e-8)
             = 196 / 1e-8
             = 1.96e10  # &amp;lt;-- HUGE!
&lt;/code&gt;&lt;p&gt;This would be huge! Yet we see NO updates‚Ä¶ this paradox points to a deeper issue.&lt;/p&gt;&lt;p&gt;Adam uses bias correction to counteract zero initialization. Having previously encountered subtle training issues due to Adam bias initialization bugs, I wondered if the correction might be broken here. &lt;/p&gt;&lt;p&gt;Recall, the bias correction is simply making our effective beta values dependent on the step index, so if the issue has to do with bias correction, it might have some relation to our beta parameters or step index.&lt;/p&gt;&lt;p&gt;I tested with different beta values, at different steps, and even beta_2=0 (which bypasses the exponential average entirely, making &lt;code&gt;exp_avg_sq = grad**2&lt;/code&gt; directly). The encoder‚Äôs &lt;code&gt;exp_avg_sq&lt;/code&gt; still stayed zero, making bias correction seem less likely as a culprit.&lt;/p&gt;&lt;p&gt;Plus, &lt;code&gt;exp_avg&lt;/code&gt; updated correctly despite using the same bias correction mechanism. So maybe something else is preventing &lt;code&gt;exp_avg_sq&lt;/code&gt; from updating.&lt;/p&gt;&lt;p&gt;My largest gradients were big (1e6), and squared that‚Äôs 1e12. While that is quite large, it shouldn‚Äôt overflow in float32. However, I‚Äôve also been hurt by precision bugs before&lt;/p&gt;&lt;p&gt;I moved everything to float64‚Ä¶ AND IT STARTED WORKING!&lt;/p&gt;&lt;p&gt;After a few more minutes of spiraling&lt;/p&gt;&lt;p&gt;Testing with float32 on CPU‚Ä¶ the weights update!!&lt;/p&gt;&lt;code&gt;device-specific&lt;/code&gt;! The exact same float32 code updates weights on CPU but fails on MPS. This was progress: same code, same datatypes, but different devices meant different implementations‚Äîand different bugs. &lt;p&gt;Ôπ° This is progress!!&lt;/p&gt;&lt;p&gt;Ôπ° Note to self‚Ä¶ simpler explanations are more likely correct- even (and especially!) when LLMs confidently assert complicated theories that are hard to understand / verify&lt;/p&gt;&lt;p&gt;Ôπ° Now I just need to figure out why the bug only occurs with MPS&lt;/p&gt;&lt;p&gt;PyTorch‚Äôs device abstraction lets you write the same code and run it on CPUs, GPUs, and even Apple Silicon. It feels like the same computation is running everywhere ‚Äî but under the hood, each device has its own entirely separate implementation.&lt;/p&gt;&lt;p&gt;When you call a tensor operation like &lt;code&gt;matmul&lt;/code&gt;, PyTorch looks at the tensor‚Äôs metadata (e.g. device, dtype, shape) and dispatches to a specialized kernel: a device-specific, highly optimized implementation tailored for that particular hardware backend.&lt;/p&gt;&lt;p&gt;Apple‚Äôs GPU Stack:&lt;/p&gt;&lt;p&gt;On ‚ÄúKernel‚Äù Terminology:&lt;/p&gt;&lt;p&gt;Typically, ‚Äúkernel‚Äù refers to low-level GPU code that runs directly on hardware: functions that explicitly manage parallelism across thousands of GPU cores, handle device memory allocation, and are written in chip-specific languages like CUDA or Metal Shading Language.&lt;/p&gt;&lt;p&gt;However, PyTorch seems to also use ‚Äúkernel‚Äù to describe a higher-level abstraction: the framework‚Äôs implementation code (C++, Objective-C++, or CUDA files in the &lt;code&gt;native/&lt;/code&gt; directory) that handles specific operations for specific backends. These PyTorch kernels sit above the hardware level- they might call optimized libraries like MPS or cuDNN (which then use those low-level GPU kernels underneath), or they might contain hand-written GPU code.&lt;/p&gt;&lt;p&gt;In this post, we end up primarily exploring PyTorch kernels (e.g. the C++/Objective-C++ code in &lt;code&gt;BinaryOps.mm&lt;/code&gt; that orchestrates MPS operations) rather than the Metal compute shaders executing on GPU cores beneath them.&lt;/p&gt;&lt;p&gt;I was surprised these higher-level implementations are also called ‚Äúkernels‚Äù and maybe I have just confused my terminology here but I didn‚Äôt have a better name for them so I tried to mostly use ‚ÄúPyTorch kernel‚Äù or just ‚Äúoperation‚Äù to describe them, though the terminology does get blurry in places.&lt;/p&gt;&lt;p&gt;So when you write something like &lt;code&gt;result = tensor_a @ tensor_b&lt;/code&gt;, you‚Äôre not invoking a universal multiply function. PyTorch uses the tensors‚Äô metadata to select a device- and dtype-specific kernel that performs the actual computation.&lt;/p&gt;&lt;p&gt;Multiplying two tensors on the CPU uses a completely different kernel than on MPS or CUDA. Even on the same device, changing the dtype or layout can trigger a different kernel. PyTorch maintains a large set of these implementations to support all the combinations.&lt;/p&gt;&lt;p&gt;We‚Äôll see exactly how this dispatch system works in C++ later when we dive into the source code. For now, the important point is: even with identical Python code different tensor metadata ‚Üí different kernel code ‚Üí different efficiency / bugs.&lt;/p&gt;&lt;p&gt;In my case, because I‚Äôm running this on my M3 MacBook Pro, I‚Äô m using MPS (Metal Performance Shaders), which is the GPU backend for Apple Silicon. While it feels a bit crazy to assume that my training plateau is due to an internal kernel-level bug, it‚Äôs a bit less unreasonable with MPS as it‚Äôs newer and less mature than the CPU and CUDA backends. (And honestly, most people training/debugging ML models are not doing it on their MacBooks.)&lt;/p&gt;&lt;p&gt;The Adam bug appears when working with the encoder on MPS. What makes the encoder different from the decoder that would trigger different behavior?&lt;/p&gt;&lt;p&gt;I tested everything I could think of that might differentiate the two tensors:&lt;/p&gt;&lt;p&gt;Nothing helped. Even when both tensors had similar gradient statistics, only the encoder‚Äôs &lt;code&gt;exp_avg_sq&lt;/code&gt; stayed frozen. The difference wasn‚Äôt in the values of the tensor - something else about the encoder tensor itself was triggering the bug.&lt;/p&gt;&lt;p&gt;What properties does a PyTorch tensor even have? I asked Claude what attributes could differ between two tensors and checked them one-by-one:&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Encoder&lt;/cell&gt;&lt;cell role="head"&gt;Decoder&lt;/cell&gt;&lt;cell role="head"&gt;Same?&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Device&lt;/cell&gt;&lt;cell&gt;mps:0&lt;/cell&gt;&lt;cell&gt;mps:0&lt;/cell&gt;&lt;cell&gt;‚úì&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Dtype&lt;/cell&gt;&lt;cell&gt;float32&lt;/cell&gt;&lt;cell&gt;float32&lt;/cell&gt;&lt;cell&gt;‚úì&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Shape&lt;/cell&gt;&lt;cell&gt;[1536, 384]&lt;/cell&gt;&lt;cell&gt;[384, 1536]&lt;/cell&gt;&lt;cell&gt;‚ùå&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Requires_grad&lt;/cell&gt;&lt;cell&gt;True&lt;/cell&gt;&lt;cell&gt;True&lt;/cell&gt;&lt;cell&gt;‚úì&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Stride&lt;/cell&gt;&lt;cell&gt;(1, 1536)&lt;/cell&gt;&lt;cell&gt;(1536, 1)&lt;/cell&gt;&lt;cell&gt;‚ùå&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Contiguous&lt;/cell&gt;&lt;cell&gt;False&lt;/cell&gt;&lt;cell&gt;True&lt;/cell&gt;&lt;cell&gt;‚ùå&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Three differences! The encoder and decoder have different shapes (they‚Äôre transposes of each other)&lt;code&gt;nn.Linear&lt;/code&gt; stores weights as [out_features, in_features], so the encoder (384‚Üí1536) has shape [1536, 384] and the decoder (1536‚Üí384) has shape [384, 1536].&lt;/p&gt;&lt;p&gt;The shape difference itself can‚Äôt cause different behavior (PyTorch operations handle any shape). But contiguity? That‚Äôs a low-level memory detail that could be relevant. Maybe the MPS Adam bug only affects non-contiguous tensors? Worth a shot:&lt;/p&gt;&lt;code&gt;model.encoder.weight.data = model.encoder.weight.contiguous()
optimizer.step()
# Encoder updates!! ‚úì
&lt;/code&gt;&lt;p&gt;IT WORKS! But why?&lt;/p&gt;&lt;p&gt;Your computer‚Äôs memory is just a flat, 1D array of bytes, but tensors represent multi-dimensional grids. When you index &lt;code&gt;tensor[i, j]&lt;/code&gt;, PyTorch needs to find that element in the flat memory. The tensor‚Äôs stride tells it how to do this conversion (and the exact amount you jump between elements depends on the dtype and how much memory each element takes up).&lt;/p&gt;&lt;p&gt;Think of stride as navigation instructions: ‚Äúto get from one row to the next, skip this many elements.‚Äù By default, memory is stored row-wise‚Äîeach row is stored sequentially, then the next row comes after. If you read through a row, you skip over 1 element at a time; to go to the next row, you move row-length elements over. (This is why going across a row is faster than going down a column.)&lt;/p&gt;&lt;p&gt;However, the memory layout doesn‚Äôt have to match the logical layout we use to think about the tensor. We can change how the user views the tensor without moving any data! For example, when we run transpose (&lt;code&gt;.T&lt;/code&gt;), we don‚Äôt need to move around any data‚Äîwe just change the stride!&lt;/p&gt;&lt;p&gt;As we see in the images, reading all the elements row-by-row in the contiguous tensor is easy and linear, but the same row-wise pattern in the non-contiguous tensor is much jumpier. This jumping pattern makes the tensor ‚Äúnon-contiguous.‚Äù&lt;/p&gt;&lt;p&gt;While there‚Äôs only one way for a tensor to be contiguous (the ‚Äúnatural‚Äù layout), there are many ways to become non-contiguous. By default, tensors are initialized as contiguous, but operations like slicing (&lt;code&gt;tensor[::2, :]&lt;/code&gt;), reshaping, and dimension reordering (&lt;code&gt;permute&lt;/code&gt;) can all create different non-contiguous stride patterns.&lt;/p&gt;&lt;p&gt;Why design tensors this way? Wouldn‚Äôt it be simpler to always keep data in the ‚Äúnatural‚Äù contiguous layout? The answer is performance: by just adjusting the tensor‚Äôs metadata, operations like transpose, slice, and reshape can be nearly instant‚Äî no data movement or memory allocation required. Keeping everything contiguous would mean expensive copying every time you reorganize dimensions.&lt;/p&gt;&lt;p&gt;Looking at the weight initialization code:&lt;/p&gt;&lt;code&gt;self.encoder.weight.data = self.decoder.weight.T.clone()
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;.T&lt;/code&gt; creates a non-contiguous view, and &lt;code&gt;.clone()&lt;/code&gt; preserves the stride pattern.&lt;/p&gt;&lt;code&gt;.clone()&lt;/code&gt; preserve stride patterns?&lt;p&gt;At first this felt counterintuitive to me- if we‚Äôre already paying the cost to copy the data (the whole point of non-contiguous layouts is to avoid copying), why not copy it into the ‚Äúbetter‚Äù contiguous layout?&lt;/p&gt;&lt;p&gt;But this actually makes sense from a design perspective: &lt;code&gt;.clone()&lt;/code&gt; should create an exact copy with all properties preserved, including memory layout. The tensor might be non-contiguous for a reason‚Äîmaybe you‚Äôre about to transpose it back, or the layout is optimized for some operation. Silently reorganizing memory would be surprising behavior. (The optional &lt;code&gt;torch.memory_format&lt;/code&gt; argument, which defaults to &lt;code&gt;torch.preserve_format&lt;/code&gt;, makes this choice explicit.)&lt;/p&gt;&lt;p&gt;As a bonus, preserving the layout is also faster. Even though both include new memory allocation and moving data, reorganizing it still slows things down:&lt;/p&gt;&lt;code&gt;x_t = x.T  # Start with non-contiguous
y_noncontig = x_t.clone()              # Preserves non-contiguous (1.919ms)
y_contig = x_t.clone(memory_format=torch.contiguous_format)  # Force contiguous (4.401ms)
&lt;/code&gt;&lt;p&gt;Okay so we now know this initialization is why only the encoder is non-contiguous, and thus why only the encoder has training issues!&lt;/p&gt;&lt;p&gt;While I could just call &lt;code&gt;.contiguous()&lt;/code&gt; on my encoder, declare victory, and get back to the research this bug was blocking me from doing‚Ä¶ I felt like I was just scratching the surface of this bug and I feared it would haunt me until I fully figured out WHAT happened and WHY.&lt;/p&gt;&lt;p&gt;When Adam updates parameters, what operations does it perform? Let‚Äôs look at PyTorch‚Äôs Adam implementation.&lt;/p&gt;&lt;p&gt;Fair warning: this file is over 1000 lines! To find what we need, search for where &lt;code&gt;exp_avg&lt;/code&gt; and &lt;code&gt;exp_avg_sq&lt;/code&gt; are defined and updated.&lt;/p&gt;&lt;p&gt;Here are the critical lines (lines 101, 391-407):&lt;/p&gt;&lt;code&gt;# State initialization (line 101)
state["exp_avg"] = torch.zeros_like(param, memory_format=torch.preserve_format)
state["exp_avg_sq"] = torch.zeros_like(param, memory_format=torch.preserve_format)

# ... [300 lines of setup and parameter group handling] ...

# First moment update (line 391)
exp_avg.lerp_(grad, 1 - beta1)

# Second moment update (line 392)
exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

# ... [bias correction calculations] ...

# Parameter update (line 407)
param.addcdiv_(exp_avg, denom, value=-step_size)
&lt;/code&gt;&lt;p&gt;Look at that initialization! &lt;code&gt;memory_format=torch.preserve_format&lt;/code&gt; means the state tensors inherit their stride pattern from &lt;code&gt;param&lt;/code&gt;. So when our encoder weight is non-contiguous, both &lt;code&gt;exp_avg&lt;/code&gt; and &lt;code&gt;exp_avg_sq&lt;/code&gt; are also non-contiguous.&lt;/p&gt;&lt;p&gt;But they‚Äôre BOTH non-contiguous - so why does only one break?&lt;/p&gt;&lt;p&gt;Well, while they both are computed via addition and multiplication, they don‚Äôt use the exact same operations to perform this. Any of these operations could be a suspect, so let‚Äôs test each one individually!&lt;/p&gt;&lt;p&gt;For operations like &lt;code&gt;output.addcmul_(input1, input2)&lt;/code&gt;, the output tensor&lt;code&gt;mul_&lt;/code&gt;), that indicates that it is performing an in-place operation to modify a tensor directly in memory. Just as different devices can distinct kernels, so can distinctions like these!&lt;/p&gt;&lt;p&gt;Testing each Adam operation with non-contiguous output tensors on MPS:&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Operation&lt;/cell&gt;&lt;cell role="head"&gt;Function&lt;/cell&gt;&lt;cell role="head"&gt;Result&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Linear interpolation&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;lerp_()&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Updates ‚úì&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Scalar multiply&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;mul_()&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Updates ‚úì&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Add + multiply&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;addcmul_()&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Stays zero ‚úó&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Add + divide&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;addcdiv_()&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Stays zero ‚úó&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;addcmul_()&lt;/code&gt; and &lt;code&gt;addcdiv_()&lt;/code&gt; both fail silently when writing to non-contiguous outputs on MPS. &lt;p&gt;Interestingly, input contiguity doesn‚Äôt matter, only the output! Whether &lt;code&gt;grad&lt;/code&gt;, &lt;code&gt;exp_avg&lt;/code&gt;, or &lt;code&gt;denom&lt;/code&gt; are contiguous makes no difference. The bug is purely in how these kernels write to non-contiguous output buffers.&lt;/p&gt;&lt;p&gt;The broken operations aren‚Äôt producing zeros or NaNs. They‚Äôre simply not modifying the output tensor at all. This wasn‚Äôt immediately obvious since &lt;code&gt;exp_avg_sq&lt;/code&gt; was initialized to zeros, making ‚Äústays at zero‚Äù and ‚Äúnever updates‚Äù look identical. But testing with a non-zero, non-contiguous output tensor confirms that after calling &lt;code&gt;addcmul_&lt;/code&gt; or &lt;code&gt;addcdiv_&lt;/code&gt;, the values remain unchanged. No update happens.&lt;/p&gt;&lt;p&gt;Yet timing shows MPS is doing substantial work. Non-contiguous operations take &amp;gt;2x longer than contiguous ones, proving the kernels are computing something, yet those results never make it to the output tensor. On CPU, each of these operations work correctly regardless of memory layout. This is purely a MPS-specific bug.&lt;/p&gt;&lt;p&gt;With the broken operations identified, we can trace the complete chain of events that triggers our failure:&lt;/p&gt;&lt;p&gt;Step 1: Initialization&lt;/p&gt;&lt;code&gt;# Creates non-contiguous encoder weight (stride: 1, 1536)
encoder.weight = decoder.weight.T.clone()
&lt;/code&gt;&lt;p&gt;Step 2: Adam State Creation&lt;/p&gt;&lt;code&gt;# Both state tensors inherit non-contiguous layout from param
state["exp_avg"] = zeros_like(param, memory_format=torch.preserve_format)
state["exp_avg_sq"] = zeros_like(param, memory_format=torch.preserve_format)
&lt;/code&gt;&lt;p&gt;Step 3: Optimization Loop&lt;/p&gt;&lt;p&gt;First moment update:&lt;/p&gt;&lt;code&gt;exp_avg.lerp_(grad, 1-beta_1)  # ‚úì Works fine
&lt;/code&gt;&lt;p&gt;Second moment update:&lt;/p&gt;&lt;code&gt;exp_avg_sq.mul_(beta_2)                        # ‚úì Works fine
exp_avg_sq.addcmul_(grad, grad, 1-beta_2)      # ‚úó No update - stays zero!
&lt;/code&gt;&lt;p&gt;Step 4: Parameter Update&lt;/p&gt;&lt;code&gt;# Should update param, does nothing, leading to silent failure
param.addcdiv_(exp_avg, denom, value=-step_size)  # ‚úó No update!
&lt;/code&gt;&lt;p&gt;If only &lt;code&gt;exp_avg_sq.addcmul_()&lt;/code&gt; failed, the zero &lt;code&gt;exp_avg_sq&lt;/code&gt; would produce massive weight explosions (update = &lt;code&gt;lr √ó exp_avg / ‚àö(Œµ)&lt;/code&gt;), making the bug immediately obvious. But &lt;code&gt;param.addcdiv_()&lt;/code&gt; also failed, producing no updates at all!&lt;/p&gt;&lt;p&gt;The second bug masked the first, creating a silent failure: the spookiest type of error. The model appeared to be learning (the decoder was training normally), but progress stalled because the encoder stayed frozen. A subtle plateau that looked exactly like a hyperparameter issue üôÉ&lt;/p&gt;&lt;p&gt;If non-contiguous tensors can cause operations to silently fail on MPS, why didn‚Äôt the forward pass or backward pass break?&lt;/p&gt;&lt;p&gt;The forward and backward passes for &lt;code&gt;F.linear&lt;/code&gt; use &lt;code&gt;matmul&lt;/code&gt; for their matrix multiplications, which handle non-contiguous tensors correctly on MPS. Testing confirms that both &lt;code&gt;matmul&lt;/code&gt; (the &lt;code&gt;@&lt;/code&gt; operator) and &lt;code&gt;F.linear&lt;/code&gt; work correctly with non-contiguous input tensors and non-contiguous weight matrices on MPS, including during the backward pass where gradients flow through non-contiguous weights without issues.&lt;/p&gt;&lt;p&gt;The bug is specific to the fused in-place operations that Adam uses for state updates: &lt;code&gt;addcmul_&lt;/code&gt; and &lt;code&gt;addcdiv_&lt;/code&gt;. These operations fail silently when writing to non-contiguous output tensors, while other in-place operations like &lt;code&gt;lerp_&lt;/code&gt; and &lt;code&gt;mul_&lt;/code&gt; work correctly.&lt;/p&gt;&lt;p&gt;While we have made so much progress on this case, we‚Äôre still not done yet!!&lt;/p&gt;&lt;code&gt;addcmul_&lt;/code&gt; and &lt;code&gt;addcdiv_&lt;/code&gt; fail to update non-contiguous outputs while &lt;code&gt;mul_&lt;/code&gt; and &lt;code&gt;lerp_&lt;/code&gt; work fine? &lt;p&gt;To understand why some operations work and others don‚Äôt, I needed to look at PyTorch‚Äôs source code for the buggy kernels.&lt;/p&gt;&lt;p&gt;While I normally trace through a Python codebase by jumping to definitions in my IDE, that doesn‚Äôt work with &lt;code&gt;tensor.addcmul_()&lt;/code&gt;. When you call this function, there‚Äôs no Python source code executing - instead, Python immediately jumps into compiled C++ code for performance. And since PyTorch ships this as a pre-compiled binary, I can‚Äôt see that C++ implementation.&lt;/p&gt;&lt;p&gt;How can a Python tensor object have methods that execute C++ code? I skipped over this earlier but even though I know PyTorch isn‚Äôt the only framework to do this and everything is just machine code if you zoom in close enough‚Ä¶ it still feels a bit magical to casually call another language.&lt;/p&gt;&lt;p&gt;The explanation is Python bindings.&lt;/p&gt;&lt;p&gt;When you install PyTorch, you‚Äôre not just getting Python files. You‚Äôre also getting compiled C++ libraries (.so files on Linux/Mac, .dll on Windows) that contain the actual mathematical operations. The Python part is essentially a wrapper that:&lt;/p&gt;&lt;code&gt;tensor&lt;/code&gt;, &lt;code&gt;other_tensor&lt;/code&gt;, etc.)&lt;p&gt;PyTorch uses pybind11 to automatically generate this wrapper code. For example, the C++ function signature:&lt;/p&gt;&lt;code&gt;Tensor&amp;amp; addcmul_(Tensor&amp;amp; self, const Tensor&amp;amp; tensor1, const Tensor&amp;amp; tensor2, const Scalar&amp;amp; value)
&lt;/code&gt;&lt;p&gt;Gets automatically wrapped so you can call it from Python as:&lt;/p&gt;&lt;code&gt;tensor.addcmul_(tensor1, tensor2, value=1.0)
&lt;/code&gt;&lt;p&gt;This is why PyTorch operations are fast despite being called from Python - the heavy lifting happens in optimized C++ code, with Python just handling the interface.&lt;/p&gt;&lt;p&gt;And as we discussed earlier, PyTorch dispatches based on tensor metadata, so there isn‚Äôt just one implementation - there are device-specific kernels for CPU, CUDA, MPS, etc. Since my PyTorch installation just has the compiled binary files, to investigate the actual implementations, we need to clone PyTorch‚Äôs repository.&lt;/p&gt;&lt;p&gt;All kernels are listed in an operation registry - a YAML file that maps operation names (like &lt;code&gt;addcmul_&lt;/code&gt;) to their tensor-specific C++ implementations. In practice, when PyTorch is compiled (normally done before you install it), this registry is used to automatically generate hundreds of scripts that do the actual dispatching based on the patterns described here, but if we just want to understand what kernel our tensor is calling, we can look through the registry.&lt;/p&gt;&lt;p&gt;Searching for ‚Äúaddcmul_‚Äù in the registry &lt;code&gt;native_functions.yaml&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;- func: addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&amp;gt; Tensor(a!)
  # our addcmul_ function just points us to the yaml for addcmul.out
  structured_delegate: addcmul.out

# The function addcmul_ points to:
- func: addcmul.out(...)
  dispatch:
    CPU, CUDA: addcmul_out
    MPS: addcmul_out_mps  # Different function for MPS!
&lt;/code&gt;&lt;p&gt;Now that we have the device-specific operation names, we can search them in the PyTorch repo within the mps implementations, and we find our implementation for &lt;code&gt;addcmul_out_mps&lt;/code&gt; in &lt;code&gt;PointwiseOps.mm&lt;/code&gt;. Upon a first skim of the code, I realized I had no clue how to read the MPS codebase. There were too many unknown variables and constructs, and I wasn‚Äôt sure what to look for in this implementation. I‚Äôd written a CUDA kernel before, and was pretty good with C about a decade ago, but as turns out, neither of those helped here :(&lt;/p&gt;&lt;p&gt;Rather than trying to decode unfamiliar code in isolation, I‚Äôd find something similar that works correctly and compare the two. &lt;code&gt;mul_&lt;/code&gt; was the perfect comparison since both are simple element-wise in-place operations. The registry pointed me to &lt;code&gt;binaryOpTensor&lt;/code&gt; in &lt;code&gt;BinaryOps.mm&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Now I had my comparison:&lt;/p&gt;&lt;code&gt;addc_mul_div_out_mps&lt;/code&gt; in &lt;code&gt;PointwiseOps.mm&lt;/code&gt; (used by &lt;code&gt;addcmul_&lt;/code&gt;)&lt;code&gt;binaryOpTensor&lt;/code&gt; in &lt;code&gt;BinaryOps.mm&lt;/code&gt; (used by &lt;code&gt;mul_&lt;/code&gt;)&lt;p&gt;I opened both side-by-side, scanning specifically for differences in how they handle the output tensor. My experiments had already narrowed the search: I knew both operations were computing something (timing proved that), so the bug had to be in how results get written back to non-contiguous outputs. Look for anything related to contiguity checks or special output handling.&lt;/p&gt;&lt;p&gt;Broken version (&lt;code&gt;addcmul_&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;static void addc_mul_div_out_mps(..., Tensor&amp;amp; output, ...) {
  // ... setup code ...
  Placeholder outputPlaceholder = Placeholder(output);
  runMPSGraph(...);
  // That's it - no additional handling
}
&lt;/code&gt;&lt;p&gt;Working version (&lt;code&gt;mul_&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;static void binaryOpTensor(..., Tensor&amp;amp; output, ...) {
  // ... setup code ...
  
  bool needsCopyToOutput = !output.is_contiguous();
  if (needsCopyToOutput) {
    // Create temporary contiguous tensor
    output = at::empty(...);
  }
  
  Placeholder outputPlaceholder = Placeholder(output);
  runMPSGraph(...);
  
  if (needsCopyToOutput) {
    output_.copy_(output);  // Copy results back!
  }
}
&lt;/code&gt;&lt;p&gt;The working version explicitly checks &lt;code&gt;!output.is_contiguous()&lt;/code&gt; and adds extra handling: it creates a temporary contiguous tensor, runs the operation, then copies results back. The broken version just passes the output directly to &lt;code&gt;Placeholder&lt;/code&gt; and calls it a day.&lt;/p&gt;&lt;p&gt;But this raises a new question: if non-contiguous memory layouts need this kind of explicit handling, why doesn‚Äôt &lt;code&gt;addcmul&lt;/code&gt; just crash or throw an error instead of silently failing?&lt;/p&gt;&lt;p&gt;The answer lies in understanding what &lt;code&gt;Placeholder&lt;/code&gt; does. PyTorch tensors and Metal (Apple‚Äôs GPU framework) use different memory formats, so PyTorch needs a converter when running operations on Apple Silicon. &lt;code&gt;Placeholder&lt;/code&gt; handles this conversion - it takes PyTorch tensors and wraps them in Metal-compatible buffers, handles different data types, manages memory layouts, and sets up the compute pipeline.&lt;/p&gt;&lt;p&gt;For most tensors, this conversion is straightforward. But for non-contiguous tensors, Metal can‚Äôt work with the scattered memory layout directly. Looking at the Placeholder code:&lt;/p&gt;&lt;code&gt;if (!src.is_contiguous()) {
    _tensor = src.clone(MemoryFormat::Contiguous);  // Create contiguous copy
    srcBuf = getMTLBufferStorage(_tensor);          // Point Metal to the copy
}
&lt;/code&gt;&lt;p&gt;When Placeholder encounters a non-contiguous tensor, it automatically creates a contiguous copy and points Metal to that copy instead. This happens transparently - the broken kernels have no idea they‚Äôre working with a temporary.&lt;/p&gt;&lt;p&gt;This automatic copying is perfect for input tensors - Metal reads from the copy, computation proceeds normally, and nobody cares what happens to the temporary afterward.&lt;/p&gt;&lt;p&gt;But it‚Äôs disastrous for output tensors where the goal is in-place editing. The computation succeeds and writes results to the temporary copy, but those results never make it back to the original tensor that‚Äôs supposed to be updated.&lt;/p&gt;&lt;p&gt;If non-contiguous tensors are so problematic, why do CPU and CUDA backends handle them fine?&lt;/p&gt;&lt;p&gt;CPU: Can handle arbitrary strides natively. When iterating through a non-contiguous tensor, the CPU just follows the stride pattern‚Äîjumping around memory is slower than sequential access, but it works correctly.&lt;/p&gt;&lt;p&gt;CUDA: NVIDIA‚Äôs CUDA framework has always supported strided memory access in kernels. Operations can read/write to non-contiguous layouts directly, though with some performance penalty.&lt;/p&gt;&lt;p&gt;MPS: Apple‚Äôs Metal Performance Shaders framework initially didn‚Äôt support strided access. Kernels expected contiguous memory layouts, period. This forced PyTorch to implement the gather-scatter workaround pattern we saw in the working kernels.&lt;/p&gt;&lt;p&gt;The bug occurred because some MPS operations implemented this workaround (like &lt;code&gt;mul_&lt;/code&gt;), while others didn‚Äôt (like &lt;code&gt;addcmul_&lt;/code&gt;). The abstraction (Placeholder) that was supposed to hide this complexity actually made it worse by silently copying outputs without a way to copy results back. Although as we‚Äôll learn later this has been improved in newer Mac Operating Systems.&lt;/p&gt;&lt;p&gt;The broken kernels work perfectly with contiguous tensors and silently fail with non-contiguous ones. The working kernels detect this situation and add an explicit copy-back step to move results from the temporary to the original tensor.&lt;/p&gt;&lt;p&gt;Understanding the bug made the solution clear - apply the same pattern that working kernels use:&lt;/p&gt;&lt;p&gt;I tested this locally and it worked! The encoder weights finally updated and the model trained successfully üéâüéâ&lt;/p&gt;&lt;p&gt;You can see the complete reproduction, debugging experiments, fix at https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug.&lt;/p&gt;&lt;p&gt;While editing a Python package just involves installing your locally editable version of the code instead of the default package, to test my PyTorch fix, I had to re-build it all locally, which was more work than expected and also made me acutely aware that this whole time I was working on PyTorch v2.2.1&lt;/p&gt;&lt;p&gt;Checking the latest version revealed the bug was already fixed in v2.4, patched by an ML engineer at Apple last year using almost the exact same approach I‚Äôd used.&lt;code&gt;arrayView&lt;/code&gt; API (see WWDC 2024 session at timestamp 13:41). Instead of the gather-scatter workaround, Metal can now read/write directly from non-contiguous memory using stride metadata. This means on macOS 15+, PyTorch can skip the manual copy workarounds entirely. The performance gap between contiguous and non-contiguous tensors is now much smaller, though contiguous is still faster due to better cache utilization.&lt;/p&gt;&lt;code&gt;the story wasn't over just yet!&lt;/code&gt; &lt;p&gt;While writing this up, I added some more tests for my kernel fix to confirm it really worked, and one of the tests failed! I looked into it more and realized I‚Äôd stumbled upon the same failure pattern in the &lt;code&gt;random_&lt;/code&gt; operation (in the most up-to-date PyTorch this time!)&lt;/p&gt;&lt;p&gt;Turns out, all random in-place operations (&lt;code&gt;normal_&lt;/code&gt;, &lt;code&gt;uniform_&lt;/code&gt;, &lt;code&gt;exponential_&lt;/code&gt;, &lt;code&gt;random_&lt;/code&gt;, &lt;code&gt;bernoulli_&lt;/code&gt;) silently fail when called on non-contiguous tensors on MPS.&lt;/p&gt;&lt;code&gt;x = torch.zeros(10, 10).T  # Non-contiguous
x.normal_()  # Should fill with random values
print(x.max())  # Prints 0.0 - the operation silently failed!
&lt;/code&gt;&lt;p&gt;Yet again, the operations complete without error, but the tensor remains unchanged‚Äîthe kernel computes random values into a temporary contiguous buffer but never copies them back.&lt;/p&gt;&lt;p&gt;Having just traced through this exact bug pattern, I recognized it immediately and knew exactly how to fix it. Filed an Issue and made a PR applying the same solution.&lt;/p&gt;&lt;p&gt;I suspect there are other similar bugs lying around, as none of these fixes actually address the underlying quirk that the Placeholder abstraction itself is problematic when used with output tensors.&lt;/p&gt;&lt;p&gt;The core issue: Placeholder‚Äôs constructor silently creates a temporary contiguous copy for non-contiguous tensors, but it has no way to know if it‚Äôs wrapping an input (where the copy is fine- we just read from it) or an output (where the copy is broken- results get written to it then lost). This means every single operation that uses Placeholder for outputs must manually implement the same workaround pattern or else it has this silent failure:&lt;/p&gt;&lt;code&gt;// Every MPS operation must remember to do this:
bool needsCopy = !output.is_contiguous();
Tensor temp = needsCopy ? at::empty(...) : output;
@autoreleasepool {
    Placeholder p(temp);
    runGraph();
}
if (needsCopy)
  output.copy_(temp);
&lt;/code&gt;&lt;p&gt;This is a leaky abstraction&lt;/p&gt;&lt;p&gt;The good news: macOS 15+ Metal now handles non-contiguous tensors natively, making this entire issue obsolete for newer systems. But for anyone on older macOS versions or maintaining PyTorch‚Äôs MPS backend, this abstraction continues to cause issues.&lt;/p&gt;&lt;p&gt;So ideally, the Placeholder class would be redesigned to handle output tensors correctly by default, but given that the hardware is moving to handle this natively anyway, the pragmatic fix is probably just to audit and patch the remaining operations using the established pattern.&lt;/p&gt;&lt;p&gt;Performance Considerations&lt;/p&gt;&lt;p&gt;Even with the code fixes, non-contiguous tensors on MPS involve: Allocate temporary buffer -&amp;gt; Copy to contiguous layout -&amp;gt; Compute -&amp;gt; Copy back. Making tensors contiguous once at initialization avoids thousands of copies during training! And even if your OS can avoid making this temporary contiguous copy, it is still slower to operate on non-contiguous memory if you will be using it many times.&lt;/p&gt;&lt;p&gt;When to Call &lt;code&gt;.contiguous()&lt;/code&gt;&lt;/p&gt;&lt;code&gt;# When to call .contiguous() - General Principles

# 1. After operations that change memory layout:
x = tensor.transpose(0, 1)  # Non-contiguous
x = tensor.view(-1)          # Might fail if non-contiguous!
x = x.contiguous().view(-1)  # Safe

# 2. Before operations that might not handle strides:
# - Custom CUDA/Metal kernels  
# - Newer backend features
# - Operations that failed mysteriously on certain devices

# 3. For performance on repeated operations:
weights = init_weights().T   # Used in every forward pass
weights = weights.contiguous()  # Pay copy cost once, not every iteration

# But don't overuse it!
x = x + y  # Creates new contiguous tensor anyway
x = x.contiguous()  # Unnecessary copy!
&lt;/code&gt;&lt;p&gt;For MPS specifically: If on macOS &amp;lt;15, make sure all your parameters are contiguous!&lt;/p&gt;&lt;p&gt;Isolate to specific, measurable symptoms. The most standard advice and for such good reason. Everything got easier once I had a concrete target: ‚Äú&lt;code&gt;exp_avg_sq&lt;/code&gt; stays at zero‚Äù is infinitely more debuggable than ‚Äúthe loss plateaus mysteriously.‚Äù Once I had a specific symptom, I could strip away components and test the minimal case that triggered it.&lt;/p&gt;&lt;p&gt;When debugging tensor issues, check metadata not just values. I was checking for NaNs, visualizing weights, inspecting gradients‚Äîall focused on the numbers inside tensors. The actual problem was the tensor‚Äôs stride pattern. Device, dtype, contiguity, memory layout‚Äîthese aren‚Äôt just performance details, they can cause silent correctness bugs. &lt;code&gt;tensor.is_contiguous()&lt;/code&gt; is now part of my debugging checklist.&lt;/p&gt;&lt;p&gt;When I‚Äôm confused, I might have changed two things‚Äîor there might be two bugs. Switching to fp64 ‚Äúfixed‚Äù it, but I‚Äôd also switched from MPS to CPU. Untangling that revealed the real culprit. And &lt;code&gt;exp_avg_sq&lt;/code&gt; staying zero should have caused explosions, but the parameter update also failed‚Äîone bug perfectly masked the other.&lt;/p&gt;&lt;p&gt;Documentation makes more sense when I need it. I‚Äôd skimmed PyTorch internals docs before and nothing stuck‚Äîdispatch systems, stride patterns, kernel implementations all felt overwhelming. But once I had to understand how &lt;code&gt;addcmul_&lt;/code&gt; dispatches to MPS kernels, everything clicked. Now PyTorch feels less like a black box. And when I hit the random ops bug weeks later, I wasn‚Äôt intimidated‚ÄîI knew exactly how to trace through the source.&lt;/p&gt;&lt;p&gt;Explore the system before exploring the code. When I needed to debug &lt;code&gt;addcmul_out_mps&lt;/code&gt; in unfamiliar MPS code, I ran experiments first: which operations fail? Do they run at all? What triggers the bug? By the time I opened the source, I knew to compare &lt;code&gt;addcmul_&lt;/code&gt; (broken) against &lt;code&gt;mul_&lt;/code&gt; (working) and scan specifically for differences in output handling. Without that context, I‚Äôd have been lost in Objective-C++ with no idea what mattered. Also LLMs were very helpful with unfamiliar constructs like &lt;code&gt;MPSGraphTensor&lt;/code&gt; or &lt;code&gt;@autoreleasepool&lt;/code&gt;, although they‚Äôre still less reliable with MPS than more documented frameworks.&lt;/p&gt;&lt;p&gt;Write post-mortems‚Äì even for yourself. Forcing myself to explain why I tried each debugging step was as educational as the original investigation. It‚Äôs like experience replay in RL: you explore many failed paths, find one that works, then replay that successful trajectory to reinforce the policy. Writing it down builds pattern recognition‚Äîwhen I‚Äôm in ‚Äúsituation A‚Äù, what hypotheses are worth trying? I‚Äôve written lower-effort debugging debriefs before, but making this one readable for an external audience forced me to articulate why each step made sense, deepening my understanding of what actually worked.&lt;/p&gt;&lt;p&gt;What started as a frustrating research roadblock became a surprisingly fun &amp;amp; educational detour. It forced a closer look at things normally taken for granted: Adam‚Äôs momentum mechanics, stride patterns, kernel dispatch. Understanding why each operation behaved differently revealed more about PyTorch‚Äôs architecture than typical usage ever does.&lt;/p&gt;&lt;p&gt;If you made it this far, thanks for joining! Hope you had fun and/or learned something &amp;amp; happy debugging!&lt;/p&gt;&lt;p&gt;Special thanks to Nicholas Joseph, Ben Kuhn, Nelson Elhage and Alex Tamkin for giving feedback on this üíú&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/"/><published>2025-10-23T17:06:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45684414</id><title>Tamper-Sensing Meshes Using Low-Cost, Embedded Time-Domain Reflectometry</title><updated>2025-10-27T09:14:00.428839+00:00</updated><content>&lt;doc fingerprint="ad14094a1df6de92"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I've got a new paper accepted at CHES, to be published in TCHES 2026/1 around beginning of December and out on eprint now. The topic of the paper is a way of monitoring a tamper-sensing mesh through time-domain reflectometry using very cheap components. The end result is a circuit that costs about 10 √¢¬¨ in parts that is able to measure TDR responses with a few hundred picoseconds of resolution.&lt;/p&gt;
      &lt;p&gt;Tamper-Sensing meshes are squiggly circuit traces that are used to tamper-proof high-security devices like hardware security modules, ATM pin pads and countertop card payment terminals. Any area where you would like to prevent an attacker from drilling or sawing through in a physical attack, you completely cover with one or more such circuit traces in a meandering pattern. I've written up some work on a KiCad plugin for creating these meshes in another post.&lt;/p&gt;
      &lt;p&gt;Up to now, the state of the art in monitoring these security meshes has mostly been finding ways to precisely monitor their ohmic resistance in the analog domain. This has the disadvantage of both being fairly complex in circuitry and of presenting a steep trade-off between sensitivity and false-positive rate since all you get out of the whole mesh is a single analog measurement containing maybe 12 to 16 bits of entropy. There have been a few papers on using more advanced RF techniques, but they all either required really expensive circuitry and/or highly customized meshes that for instance couldn't easily be fitted into arbitrary shapes.&lt;/p&gt;
      &lt;p&gt;In this paper, I wrote up a method using the high-resolution timer of an inexpensive STM32G4-series microcontroller together with a DisplayPort/HDMI "redriver" chips meant for amplifying high-speed display signals to create fast pulse edges. I characterized several chips, with the best performers being TI's TDP0604 and Diodes' PI3HDX12211, coming in at 2 to 5 √¢¬¨ depending on where and how much you buy. The fast edges generated by these drivers are then fed to a set of four-diode sampling gates using cheap RF schottky diodes to create a really cheap but fast time-domain reflectometer. Using this TDRD circuit, a security mesh can be monitored much more precisely than before, since the circuit creates a sort of fingerprint of the mesh's trace along its length.&lt;/p&gt;
      &lt;p&gt;One of the fun highlights of this project to me was micro-soldering test boards using different redriver ICs. Above, you can see the result of that soldering work. I was really happy with my cheap aliexpress microscope and with my fancy titanium tweezers!&lt;/p&gt;
      &lt;p&gt;Have a look into the paper, where I wrote up details on the circuitry as well as a whole bunch of (&amp;gt;1000!) measurements characterizing the system. As it turns out, it's really sensitive to attacks while being reasonably robust to environmental disturbances. In fact, it's so sensitive that the circuit can distinguish multiple identical (!) copies of the same mesh produces by JLCPCB from their manufacturing tolerances such as FR-4 fiber weave alignment.&lt;/p&gt;
      &lt;p&gt;You can find a preprint of the paper on eprint, and I'll update this post with a link to the published version of the paper when it becomes available. The eprint is identical to the published version as of now.&lt;/p&gt;
      &lt;p&gt;The source code of the project is available at https://git.jaseg.de/sampling-mesh-monitor.git.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jaseg.de/blog/paper-sampling-mesh-monitor/"/><published>2025-10-23T17:21:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45710065</id><title>Asbestosis</title><updated>2025-10-27T09:14:00.003340+00:00</updated><content>&lt;doc fingerprint="fe60955783648a74"&gt;
  &lt;main&gt;
    &lt;p&gt;This monument popped up in the middle of Barking recently. I thought it was very recently but it was actually unveiled in April 2022 and I'm just not very observant.&lt;/p&gt;
    &lt;p&gt;It says "In Memory of those who lost their lives because of exposure to asbestos".&lt;/p&gt;
    &lt;p&gt;And it's here because Barking has one of the highest rates of asbestos-related deaths in the country.&lt;/p&gt;
    &lt;p&gt;In 1913 the Cape Asbestos Company built a huge asbestos factory beside the River Roding in Barking. The company mined asbestos-bearing rock at several sites in South Africa, then shipped them in sacks to a private quay in Barking for processing. Hundreds of people were employed to mill the ore into usable fibres and then process these into lagging, packaging, pipes, resins, boards and all forms of insulation widely used in the building trade. They worked without masks or other protection, the dangers of asbestos either unknown or not thought worth bothering about. And hundreds of workers died, often many years later, of insidious chronic respiratory disease.&lt;/p&gt;
    &lt;p&gt;I found a 32-page booklet published by Cape Asbestos in the days before blue asbestos was recognised as dangerous and banned, which was as late as 1985. It shows workers with rolled-up sleeves and women leaning over unshielded machines, all potentially inhaling enough fibres to ultimately kill them. I read reports about the local school in Barking, barely 100 metres away, saying that the playground was often covered in fine dust which children rolled up and played with as if it were snow. I read that mesothelioma was so common in the area it was known as the ‚ÄòBarking Cough‚Äô. These were different times, but times that linger on.&lt;/p&gt;
    &lt;p&gt;Cape Asbestos's plant eventually closed in 1968 and in its place was built the Harts Lane council estate, which is still not the loveliest corner of Barking. It included two tall tower blocks called Colne House and Mersey House, both of which Barking &amp;amp; Dagenham council would now like to demolish. This is chiefly because they're old and covered in combustible cladding, but the additional complications of potentially disturbing polluted land puts any remediation out of financial reach. It's always the insulation you have to watch out for.&lt;/p&gt;
    &lt;p&gt;The memorial in Barking Town Square comprises a polished chunk of blue pearl granite and was unveiled on Workers' Memorial Day 2022 in a ceremony attended by several trade unionists and representatives of the London Asbestos Support Awareness Group. The emphasis is partly on remembrance and partly on the importance of standing up for workers' rights to make conditions better for all. As the inscription says, "Remember the Dead and Fight for the Living".&lt;/p&gt;
    &lt;p&gt;My grandfather worked for another Cape Asbestos plant on Tolpits Lane in Watford. Originally it had been run by Universal Asbestos Manufacturing but in 1967 the factory was acquired by Cape as part of a diversification into cement-based products. They made corrugated roofing, flat sheets, decorated sheets, slates, soil pipes, decking for flat roofs and reinforced troughing - that kind of thing - the asbestos moulded into a multiplicity of shapes for the benefit of the building trade.&lt;/p&gt;
    &lt;p&gt;To him Cape Universal was just a convenient place to work, a short walk across the moor for a day's shift and then home again for tea. He worked there for many years, from the 1930s to the 1960s, rising through the ranks from a labourer to a machine operator on the factory floor. On his death certificate his occupation was listed as 'Asbestos Moulder', and it was very much a premature death because this didn't end well.&lt;/p&gt;
    &lt;p&gt;I don't remember very much about my grandfather because he died when I was 8. I know he was there when I took my first steps in his back garden and I can remember sitting at his dining room table and hoping nobody would force me to eat the celery. My final memory is being led up to his bedroom, I suspect not long before his death, to see an ill old man laid out in bed and struggling to breathe. I don't know what was said, nor how short a time I stayed in his presence, indeed my strongest recollection is of the room itself with its austere cupboards and the curtains drawn. And then at the age of 67 he was gone.&lt;/p&gt;
    &lt;p&gt;My family fought for asbestosis to be recognised as his cause of death but were not successful. I've read recently of fellow workers working at the Tolpits Lane factory now getting six figure payouts in compensation, indeed it's hard to research this topic without ending up on legal websites with popups urging you to make a claim. Even four decades after the factory's closure there are still employees severely affected, and many more already passed, as the toxic legacy endures. The factory site is now a rather cleaner industrial estate and business park, indeed it's where the National Lottery's been based for the last 30 years because risk and loss are still in play.&lt;/p&gt;
    &lt;p&gt;Today my Dad reaches the grand old age of 87, a full 20 years more than his father lived. Science has moved on a long way since the 1970s, also educational opportunities and also workers' rights. Health and safety is sometimes much derided but it can genuinely save lives, even much extend them, rather than everyone continually moaning about additional costs and annoying procedures. If someone had shouted earlier and louder about the dangers of asbestos I might have known my grandfather better, my grandmother could have had many more years of married life and my father could have had a father for much longer.&lt;/p&gt;
    &lt;p&gt;My Dad lost his Dad at the age of 34, which is no age at all in the grand scheme of things. By contrast I still have my Dad at the age of 60, which has meant an extra quarter century of guidance, support, advice, love and always being there. How lucky am I? Every day we overlap with our parents is a blessing and I've had 22,000 of them, for all of which I'm truly grateful. We're off out later to celebrate with a slap-up dinner, or as slap-up as an 87-year-old stomach requires, which the wider family are greatly looking forward to. What Barking's memorial reminded me is that many families have not been so fortunate, and sometimes that loss can be very close to home.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://diamondgeezer.blogspot.com/2025/10/asbestosis.html"/><published>2025-10-26T08:34:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45710721</id><title>You already have a Git server</title><updated>2025-10-27T09:13:59.467133+00:00</updated><content>&lt;doc fingerprint="dcf8f5f9c827be83"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;You already have a git server:&lt;/head&gt;(Programming)&lt;p&gt;If you have a git repository on a server with ssh access, you can just clone it:&lt;/p&gt;&lt;code&gt;# This works. 
git clone ssh://username@hostname/path/to/repo
&lt;/code&gt;&lt;p&gt;You can then work on it locally and push your changes back to the origin server. By default, git won‚Äôt let you push to the branch that is currently checked out, but this is easy to change:&lt;/p&gt;&lt;code&gt;# Run this on the remote server. 
git config receive.denyCurrentBranch updateInstead
&lt;/code&gt;&lt;p&gt;This is a great way to sync code between multiple computers or to work on server-side files without laggy typing or manual copying. If you want to publish your code, just point your web server at the git repo:&lt;/p&gt;&lt;code&gt;git clone https://hostname/path/to/repo/.git
# You can get rid of the .git part of the command by either setting the
# server to remap it to a nicer URL or by just renaming the .git directory
# (although this stops you from running git server side)
&lt;/code&gt;&lt;p&gt;‚Ä¶ although you will have to run this command server-side to make it cloneable:&lt;/p&gt;&lt;code&gt;# Create some files used by git-over-http:
# Should be repeated after making changes.
git update-server-info
&lt;/code&gt;&lt;p&gt;That‚Äôs a lot of work, so let‚Äôs set up a hook to do that automatically:&lt;/p&gt;&lt;code&gt;# Automatically run git update-server-info.
# Should be run server-side
cp .git/hooks/post-update.sample .git/hooks/post-update
chmod a+x .git/hooks/post-update
&lt;/code&gt;&lt;p&gt;Git hooks are just shell scripts, so they can do things like running a static site generator:&lt;/p&gt;&lt;code&gt;cat &amp;gt; .git/hooks/post-update &amp;lt;&amp;lt;EOF
#!/bin/sh
set -euo pipefail
cd /path/to/site
/path/to/generator
EOF
chmod a+x .git/hooks/post-update
&lt;/code&gt;&lt;p&gt;This is how I‚Äôve been doing this blog for a while now: It‚Äôs very nice to be able to type up posts locally (no network lag), and then push them to the server and have the rest handled automatically.&lt;/p&gt;&lt;p&gt;It‚Äôs also backed up by default: If the server breaks, I‚Äôve still got the copy on my laptop, and if my laptop breaks, I can download everything from the server. Git‚Äôs version tracking also prevents accidental deletions, and if something breaks, it‚Äôs easy to figure out what caused it.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maurycyz.com/misc/easy_git/"/><published>2025-10-26T10:53:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45711094</id><title>Feed the bots</title><updated>2025-10-27T09:13:58.832273+00:00</updated><content>&lt;doc fingerprint="273b981161f213a7"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;You should feed the bots:&lt;/head&gt;(Programming)&lt;p&gt;A week ago, I set up an infinite nonsense crawler trap ‚Äì now it makes up 99% of my server‚Äôs traffic. What surprised me is that feeding scrapers garbage is the cheapest and easiest thing I could do.&lt;/p&gt;&lt;head rend="h2"&gt;Meet the bots:&lt;/head&gt;&lt;p&gt;These aren‚Äôt the indexing bots of old, but scrapers collecting data to train LLMs. Unlike search engines, which need the websites they crawl to stay up, AI companies provide a replacement.&lt;/p&gt;&lt;p&gt;It should come as no surprise that these bots are aggressive and relentless: They ignore robots.txt, and if block them by user agent they just pretend to be a browser. If you ban their IP, they switch addresses.&lt;/p&gt;&lt;p&gt;‚Ä¶ all while sending multiple requests per second, all day, every day.&lt;/p&gt;&lt;head rend="h2"&gt;Giving up:&lt;/head&gt;&lt;p&gt;So what if we let them access the site?&lt;/p&gt;&lt;p&gt;Serving static files is is relatively cheap, but not free. SSD access times are in the tens milliseconds, and that‚Äôs before you pay the filesystem tax. Bots also like to grab old and obscure pages, ones that are unlikely to be in cache. As a result, it doesn‚Äôt take all that many requests to bog down the server.&lt;/p&gt;&lt;p&gt;Then there‚Äôs the matter of bandwidth: Many blog posts also include images weighing hundreds to thousands of kB, which can add up quite quickly. With an average file size of 100 kB, 4 requests per second adds up to a terabyte each month ‚Äì not a huge amount of data, but more then I‚Äôm willing to throw away.&lt;/p&gt;&lt;head rend="h2"&gt;The ban hammer:&lt;/head&gt;&lt;p&gt;Simply making a list of IPs and blocking them would for normal bots‚Ä¶&lt;/p&gt;&lt;p&gt;‚Ä¶ but these are hardly normal bots. Because they are backed by billion dollar companies, they don‚Äôt just have a few addresses, but many thousands. If you managed to ban all of their addresses, they‚Äôll just buy more.&lt;/p&gt;&lt;p&gt;Rate limits fail for the same reason: They just switch IPs. I‚Äôve even seen them using new IP for each request.&lt;/p&gt;&lt;head rend="h2"&gt;Building a wall:&lt;/head&gt;&lt;p&gt;Ok, what about a pay-wall, login-wall, CAPTCHA-wall, or a hash based proof-of-work?&lt;/p&gt;&lt;p&gt;All of these inconvenience users. Requiring an account guaranties that no one will read what I wrote. Even just a simple JavaScript challenge will block anyone who‚Äôs browser doesn‚Äôt support JS ‚Ä¶ and when it works, anything that must load before the does content still hugely slows down page loads.&lt;/p&gt;&lt;head rend="h2"&gt;Throw them some bombs:&lt;/head&gt;&lt;p&gt;‚ÄúServe them few gzip bombs, that‚Äôll teach them‚Äù ‚Äî Half the internet.&lt;/p&gt;&lt;p&gt;Gzip only provides a compression ratio of a little over 1000: If I want a file that expands to 100 GB, I‚Äôve got to serve a 100 MB asset. Worse, when I tried it, the bots just shrugged it off, with some even coming back for more.&lt;/p&gt;&lt;head rend="h2"&gt;Jedi mind tricks:&lt;/head&gt;&lt;p&gt;Ok, what if we just send them 404s ‚Äì try and make them think my site doesn‚Äôt exist.&lt;/p&gt;&lt;p&gt;These tricks only work if your adversary has a mind to trick. If a link is posted somewhere, the bots will know it exists, and if they can‚Äôt access it, they‚Äôll just become more aggressive:. sending more requests, with more user agents and using more addresses.&lt;/p&gt;&lt;p&gt;Keeping them happy keeps them tolerable.&lt;/p&gt;&lt;head rend="h2"&gt;Garbage:&lt;/head&gt;&lt;p&gt;But surely sending them dynamically generated content would be expensive right?&lt;/p&gt;&lt;p&gt;Well‚Ä¶ no.&lt;/p&gt;&lt;p&gt;CPU and RAM are the fastest parts of a modern computer. Dynamic content has the reputation of being slow because it often involves a database (lots of disk IO), a million lines of JavaScript, or both.&lt;/p&gt;&lt;p&gt;My lightly optimized Markov babbler consumes around ~60 CPU microseconds per request. There‚Äôs no disk IO, and the memory cost is only around 1.2 MB. There‚Äôs also no rules or blacklists to maintain: the bots come to it and it consumes them.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maurycyz.com/misc/the_cost_of_trash/"/><published>2025-10-26T12:09:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45713359</id><title>Ken Thompson recalls Unix's rowdy, lock-picking origins</title><updated>2025-10-27T09:13:58.212991+00:00</updated><content>&lt;doc fingerprint="3a3a188cfbb6805d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ken Thompson Recalls Unix‚Äôs Rowdy, Lock-Picking Origins&lt;/head&gt;
    &lt;p&gt;The 82-year-old Ken Thompson has some amazing memories about the earliest days of the Unix operating system ‚Äî and the rowdy room full of geeks who built it.&lt;/p&gt;
    &lt;p&gt;This month Silicon Valley‚Äôs Computer History Museum released a special four-and-a-half-hour oral history, in partnership with the Association for Computing Machinery, recorded 18 months ago by technology historian David C. Brock. And Thompson dutifully recalled many of his career highlights ‚Äî from his work on the C programming language and Unix to the ‚ÄúPlan 9 from Bell Labs‚Äù operating system and the Go programming language.&lt;/p&gt;
    &lt;p&gt;But what comes through is his gratefulness for the people he‚Äôd worked with, and the opportunity they‚Äôd had to all experiment together in an open environment to explore the limits of new and emerging technologies. It‚Äôs a tale of curiosity, a playful sense of serendipity and the enduring value of a community.&lt;/p&gt;
    &lt;p&gt;And along the way, Thompson also tells the story of raising a baby alligator that a friend sent to his office at Bell Labs. (‚ÄúIt just showed up in the mail‚Ä¶ They‚Äôre not the sweetest of pets.‚Äù)&lt;/p&gt;
    &lt;head rend="h2"&gt;The Accidental Birth of Unix&lt;/head&gt;
    &lt;p&gt;Travel back in time to 1966, when 23-year-old Thompson‚Äôs first project at Bell Labs was the ill-fated Multics, a collaboration with MIT and General Electric which Thompson remembers as ‚Äúhorrible‚Ä¶ big and slow and ugly and very expensive,‚Äù requiring a giant specially-built computer just to run and ‚Äújust destined to be dead before it started.‚Äù&lt;/p&gt;
    &lt;p&gt;But when the Multics project died, ‚Äúthe computer became completely available ‚Äî this one-of-a-kind monster computer‚Ä¶ and so I took advantage.‚Äù&lt;/p&gt;
    &lt;p&gt;Thompson had wanted to work with CRAM, a data storage device with a high-speed drum memory, but like disk storage of the time, it was slow to read from memory.&lt;/p&gt;
    &lt;p&gt;Thompson thought he‚Äôd improve the situation with simultaneous (and overlapping) memory reads, but of course this required programs for testing, plus a way to load and run them.&lt;/p&gt;
    &lt;p&gt;‚ÄúAnd suddenly, without knowing it ‚Äî I mean, this is sneaking up on me‚Ä¶. Suddenly it‚Äôs an operating system!‚Äù Thompson‚Äôs initial memory-reading work became ‚Äúthe disk part‚Äù for Unix‚Äôs filesystem. He still needed a text editor and a user-switching multiplexing layer (plus a compiler and an assembler for programs), but it already had a filesystem, a disk driver and I/O peripherals.&lt;/p&gt;
    &lt;p&gt;Thompson wondered if it took so long to recognize its potential because he‚Äôd been specifically told not to work on operating systems. Multics ‚Äúwas a bad experience‚Äù for Bell Labs, he‚Äôd been told. ‚ÄúWe spent a ton of money on it, and we got nothing out of it!‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúI actually got reprimands saying, ‚ÄòDon‚Äôt work on operating systems. Bell Labs is out of operating systems!‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;One-Digit User IDs&lt;/head&gt;
    &lt;p&gt;But now Unix had its first user community ‚Äî future legends like Dennis Ritchie, Doug McIlroy, Robert Morris and occasionally Brian Kernighan. (‚ÄúAll the user IDs were one digit. That definitely put a limit on it.‚Äù) Thompson remembers designing the Unix filesystem on a blackboard in an office with Rudd Canaday ‚Äî using a special Bell Labs phone number that took dictation and delivered a typed-up transcript the next day. And Joe Ossanna ‚Äúgot things done‚Äù with a special talent for navigating Bell Labs‚Äô bureaucracy that ultimately procured a crucial PDP-11 for the Unix team to work on.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe were being told no, ‚Äòbecause we don‚Äôt deal in operating systems.'‚Äù But Ossanna knew the patent department was evaluating a third-party system for preparing documents ‚Äî and Ossanna proposed an in-house alternative. ‚ÄúSo we got our first PDP-11 to do word processing.‚Äù&lt;/p&gt;
    &lt;p&gt;And history shows that it happened partly because the department paying for it ‚Äúhad extra money, and if they didn‚Äôt spend it, they‚Äôd lose it the next year‚Ä¶‚Äù&lt;/p&gt;
    &lt;p&gt;So the young Unix community picked up somewhere between five and eight new users, Thompson remembers, ‚Äúthe secretaries for the Patent Department, writing patents on our system!‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fellowship of the Unix Room&lt;/head&gt;
    &lt;p&gt;That PDP-11 wound up in ‚Äúa spot on the sixth floor where we cleaned out a vending machine and a couple of cages of stored junk from 1920,‚Äù Thompson remembered. They eventually installed a second PDP-11, which turned the room into ‚Äúa hotbed of things,‚Äù with discussions about networking ‚Äî and an upcoming typesetter for documents. Thompson calls it the Unix room, and most of them eventually had extensions for their phones wired into the room. (It even had its own call-switching PBX ‚Ä¶)&lt;/p&gt;
    &lt;p&gt;There was camaraderie and some laughter. He adds later, almost as an aside, that ‚Äúin the Unix room, we used to pick locks a lot and steal things.‚Äù (When one of the secretaries discovered security had affixed a ‚Äúparking boot‚Äù to her car that was parked in the wrong zone, ‚Äúwe went down there, and we picked the lock and stole the boot. And after that, slowly, we picked up all four boots, and we hid them under the raised floor of the Unix room‚Ä¶‚Äù)&lt;/p&gt;
    &lt;p&gt;The punchline? ‚ÄúThe head of security came around and pleaded with us. ‚ÄòWe won‚Äôt pick on your secretaries if you give us back our boots.'‚Äù&lt;/p&gt;
    &lt;p&gt;And the deal was accepted.&lt;/p&gt;
    &lt;p&gt;Thompson remembers things like gathering for a regular ‚ÄúUnix lunch‚Äù in the Bell Labs lunchroom, which ‚Äúcaused a symbiosis of thought and things. It was great.‚Äù Although it always seemed to happen just minutes after the lunchroom stopped serving food. ‚ÄúIf I was late, I‚Äôd buy McDonald‚Äôs and sit down at the lunchroom with my McDonald‚Äôs. They used to get mad at me for that ‚Ä¶‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Growing From Community&lt;/head&gt;
    &lt;p&gt;Looking back, Thompson credited the success of C and Unix to Bell Labs and its no-pressure/no users environment. ‚ÄúIt was essentially a ‚Äòwhatever you want to do‚Äô atmosphere, and ‚Äòfor anybody you wanted to do it for‚Äô‚Ä¶ Bell Labs was by far the biggest contributor to this whole type of programming.‚Äù&lt;/p&gt;
    &lt;p&gt;Bell Labs was an eclectic mix, but this community paid unexpected dividends. While Lee McMahon was originally hired as a linguistics researcher, he was ultimately the one who procured machine-readable dictionaries for the Unix team, along with machine-readable version of the Federalist Papers. (When the whole text wouldn‚Äôt fit into their text editor ed, Thompson famously created the line-by-line pattern-scanning tool grep.)&lt;/p&gt;
    &lt;p&gt;And in the end Thompson says Unix grew from there for one simple fact: People liked it. It spread within Bell Labs, at first for ‚Äúthe administrative kind of stuff, typing in trouble tickets‚Ä¶‚Äù But this being a phone company, ‚Äúthen it started actually doing some switching, and stuff like that. It was getting deeper and deeper into the guts of the Bell System and becoming very popular.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Before Open Source&lt;/head&gt;
    &lt;p&gt;Thompson credits Richard Stallman with developing much more of the open source philosophy. ‚ÄúBut Unix had a bit of that.‚Äù Maybe it grew out of what Dennis Ritchie was remembering, that fellowship that formed around Unix. ‚ÄúFor some reason, and I think it‚Äôs just because of me and Dennis, everything was open‚Ä¶‚Äù&lt;/p&gt;
    &lt;p&gt;It was just the way they operated. ‚ÄúWe had protection on files ‚Äî if you didn‚Äôt want somebody to read it, you could set some bits and then nobody could read them, right? But nobody set those permissions on anything ‚Ä¶ All of the source was writable, by anybody! It was just open ‚Ä¶&lt;/p&gt;
    &lt;p&gt;‚ÄúIf you had an idea for an editor, you‚Äôd pull the editor out and you‚Äôd write on it and put it back ‚Ä¶ There was a mantra going around that, ‚ÄòYou touch it, you own it.'‚Äù&lt;/p&gt;
    &lt;p&gt;Thompson provides an example: Bell Labs co-worker P. J. Plauger, with whom he later wrote the 1974 book ‚ÄúElements of Programming Style.‚Äù Plauger was also a professional science fiction writer, Thompson remembers, ‚ÄúAnd whatever he was writing on was in his directory, right? So, we‚Äôd all go in there and be reading it as he‚Äôs writing it ‚Ä¶ and we‚Äôd all write back, ‚ÄòYou ought to kill this guy, and move him over here and turn him green!‚Äô or something.&lt;/p&gt;
    &lt;p&gt;‚ÄúAnd he didn‚Äôt mind it, because that‚Äôs just the theory of Unix in those days ‚Ä¶&lt;/p&gt;
    &lt;p&gt;‚ÄúI think that generated a fellowship. Just the fact that it was like writing on a blackboard ‚Äî everybody read it.‚Äù&lt;/p&gt;
    &lt;p&gt;And more of their Bell Labs experiments found their way into the world when some work on the later Plan 9 operating system found its way into the UTF-8 standard, which underlies most of today‚Äôs web connections.&lt;/p&gt;
    &lt;head rend="h2"&gt;After Bell Labs&lt;/head&gt;
    &lt;p&gt;Thompson left Bell Labs in 2000, after the breakup of the Bell system. (‚ÄúIt had changed; it was really different ‚Ä¶ You had to justify what you were doing, which is way above my pay grade.‚Äù) But his three decades there seemed to shine an influence over the rest of his life.&lt;/p&gt;
    &lt;p&gt;Thompson first moved on to a networking equipment company called Entrisphere, where he worked for six years ‚Äî and a move to Google was the natural next step. The head at Entrisphere had already moved to Google, and was urging Thompson to follow him ‚Äî and it turned out that Google CEO Eric Schmidt was an old friend who‚Äôs actually worked at Bell Labs in 1975. (Thompson says Google made him ‚Äúan exceedingly good offer‚Äù‚Ä¶)&lt;/p&gt;
    &lt;p&gt;At Google Thompson worked ‚Äúa little bit‚Äù on Android security. (‚ÄúI found a couple of specific problems, but by and large, it was very well done‚Äù.) But eventually Thompson joined the three-person team that would create the programming language Go.&lt;/p&gt;
    &lt;p&gt;And he was doing the work with Rob Pike, who was one of his old comrades from Bell Labs nearly 30 years before!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thenewstack.io/ken-thompson-recalls-unixs-rowdy-lock-picking-origins/"/><published>2025-10-26T16:57:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45713367</id><title>Books by People ‚Äì Defending Organic Literature in an AI World</title><updated>2025-10-27T09:13:57.797539+00:00</updated><content>&lt;doc fingerprint="a88d121106b30532"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Defending&lt;lb/&gt;Organic Literature&lt;lb/&gt;in an AI World. &lt;/head&gt;
    &lt;p&gt;We certify publishers as producers of human-authored books, with a process readers can trust.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;Books By People is a new independent organisation that partners with publishers to verify and certify human-written books, safeguarding creative integrity and public confidence in an AI-driven era.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Crisis&lt;/head&gt;
    &lt;p&gt;AI is flooding the literary world with imitations of human storytelling, challenging the publishing world to respond. Without safeguards, authentic human work will inevitably struggle to maintain the visibility and credibility it deserves.&lt;/p&gt;
    &lt;head rend="h4"&gt;Our Mission&lt;/head&gt;
    &lt;p&gt;To uphold a trusted and recognisable √¢Organic Literature√¢ market by supporting publishers and authors who champion human writing, and by making that commitment clear and valuable to readers.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Our Process Works&lt;/head&gt;
    &lt;p&gt;We work collaboratively with publishers to verify internal systems are in place and accessible to staff. Our certification lets you display the Books By People stamp on books and marketing, affirming your commitment to human authorship and that titles meet our standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Our Process Works&lt;/head&gt;
    &lt;p&gt;We work collaboratively with publishers to verify internal systems are in place and accessible to staff.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Our certification lets you display the Books By People stamp on books and marketing, affirming your commitment to human authorship and that titles meet our standards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Publisher Onboarding&lt;/head&gt;
    &lt;p&gt;A review of your editorial practices via a questionnaire covering workflows, AI usage, authorship integrity, and editorial control. Follow-up meetings and discussions to establish our working partnership.&lt;/p&gt;
    &lt;head rend="h3"&gt;Publisher Onboarding&lt;/head&gt;
    &lt;p&gt;A review of your editorial practices via a questionnaire covering workflows, AI usage, authorship integrity, and editorial control. Follow-up meetings and discussions to establish our working partnership.&lt;/p&gt;
    &lt;head rend="h3"&gt;Publisher Onboarding&lt;/head&gt;
    &lt;p&gt;A review of your editorial practices via a questionnaire covering workflows, AI usage, authorship integrity, and editorial control. Follow-up meetings and discussions to establish our working partnership.&lt;/p&gt;
    &lt;head rend="h3"&gt;Publisher Onboarding&lt;/head&gt;
    &lt;p&gt;A review of your editorial practices via a questionnaire covering workflows, AI usage, authorship integrity, and editorial control. Follow-up meetings and discussions to establish our working partnership.&lt;/p&gt;
    &lt;head rend="h3"&gt;Title Sampling &amp;amp; Review&lt;/head&gt;
    &lt;p&gt;A small sample of your recent titles is reviewed using expert analysis, editorial process checks and signed declarations to confirm they meet the Organic Literature standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Title Sampling &amp;amp; Review&lt;/head&gt;
    &lt;p&gt;A small sample of your recent titles is reviewed using expert analysis, editorial process checks and signed declarations to confirm they meet the Organic Literature standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Title Sampling &amp;amp; Review&lt;/head&gt;
    &lt;p&gt;A small sample of your recent titles is reviewed using expert analysis, editorial process checks and signed declarations to confirm they meet the Organic Literature standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Title Sampling &amp;amp; Review&lt;/head&gt;
    &lt;p&gt;A small sample of your recent titles is reviewed using expert analysis, editorial process checks and signed declarations to confirm they meet the Organic Literature standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification Agreement&lt;/head&gt;
    &lt;p&gt;Once approved, a formal publisher agreement confirms your certified status and shared commitment to human authorship, with annual reviews to uphold best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification Agreement&lt;/head&gt;
    &lt;p&gt;Once approved, a formal publisher agreement confirms your certified status and shared commitment to human authorship, with annual reviews to uphold best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification Agreement&lt;/head&gt;
    &lt;p&gt;Once approved, a formal publisher agreement confirms your certified status and shared commitment to human authorship, with annual reviews to uphold best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification Agreement&lt;/head&gt;
    &lt;p&gt;Once approved, a formal publisher agreement confirms your certified status and shared commitment to human authorship, with annual reviews to uphold best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification &amp;amp; Stamp Use&lt;/head&gt;
    &lt;p&gt;You√¢ll receive the Books By People Stamp, Certification ID, and QR code linking to a profile in our Certified Publisher Directory. These can be used across covers, metadata, and marketing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification &amp;amp; Stamp Use&lt;/head&gt;
    &lt;p&gt;You√¢ll receive the Books By People Stamp, Certification ID, and QR code linking to a profile in our Certified Publisher Directory. These can be used across covers, metadata, and marketing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification &amp;amp; Stamp Use&lt;/head&gt;
    &lt;p&gt;You√¢ll receive the Books By People Stamp, Certification ID, and QR code linking to a profile in our Certified Publisher Directory. These can be used across covers, metadata, and marketing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification &amp;amp; Stamp Use&lt;/head&gt;
    &lt;p&gt;You√¢ll receive the Books By People Stamp, Certification ID, and QR code linking to a profile in our Certified Publisher Directory. These can be used across covers, metadata, and marketing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources &amp;amp; Consultation&lt;/head&gt;
    &lt;p&gt;Companies receive the Organic Literature Publisher Manual, quarterly √¢AI indicators√¢ guidance, a legal playbook, and in-house AI monitoring materials, with optional year-round advisor support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources &amp;amp; Consultation&lt;/head&gt;
    &lt;p&gt;Companies receive the Organic Literature Publisher Manual, quarterly √¢AI indicators√¢ guidance, a legal playbook, and in-house AI monitoring materials, with optional year-round advisor support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources &amp;amp; Consultation&lt;/head&gt;
    &lt;p&gt;Companies receive the Organic Literature Publisher Manual, quarterly √¢AI indicators√¢ guidance, a legal playbook, and in-house AI monitoring materials, with optional year-round advisor support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources &amp;amp; Consultation&lt;/head&gt;
    &lt;p&gt;Companies receive the Organic Literature Publisher Manual, quarterly √¢AI indicators√¢ guidance, a legal playbook, and in-house AI monitoring materials, with optional year-round advisor support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecosystem Access&lt;/head&gt;
    &lt;p&gt;Join our wider network, with access to trusted legal experts for AI and the creative industries, connection opportunities, curated updates, and a quarterly newsletter on AI in publishing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecosystem Access&lt;/head&gt;
    &lt;p&gt;Join our wider network, with access to trusted legal experts for AI and the creative industries, connection opportunities, curated updates, and a quarterly newsletter on AI in publishing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecosystem Access&lt;/head&gt;
    &lt;p&gt;Join our wider network, with access to trusted legal experts for AI and the creative industries, connection opportunities, curated updates, and a quarterly newsletter on AI in publishing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecosystem Access&lt;/head&gt;
    &lt;p&gt;Join our wider network, with access to trusted legal experts for AI and the creative industries, connection opportunities, curated updates, and a quarterly newsletter on AI in publishing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join The Organic Literature Movement&lt;/head&gt;
    &lt;p&gt;Partner with us to become a certified publisher of Organic Literature: books conceived and written by humans. This certification confirms that your house upholds human authorship and does not publish books containing AI-generated or AI-rewritten content.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secure the Future of Human Authorship&lt;/head&gt;
    &lt;p&gt;Joining the movement at this time protects the value of human stories, preserves original thought, and ensures that the democratic future of literature remains led by people.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secure the Future of Human Authorship&lt;/head&gt;
    &lt;p&gt;Joining the movement at this time protects the value of human stories, preserves original thought, and ensures that the democratic future of literature remains led by people.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secure the Future of Human Authorship&lt;/head&gt;
    &lt;p&gt;Joining the movement at this time protects the value of human stories, preserves original thought, and ensures that the democratic future of literature remains led by people.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secure the Future of Human Authorship&lt;/head&gt;
    &lt;p&gt;Joining the movement at this time protects the value of human stories, preserves original thought, and ensures that the democratic future of literature remains led by people.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintain Consumer Trust&lt;/head&gt;
    &lt;p&gt;Our unique stamp reassures readers that your books are genuinely human-written in a world where this is no longer a given. By providing proof of authenticity, you strengthen the bond between author and audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintain Consumer Trust&lt;/head&gt;
    &lt;p&gt;Our unique stamp reassures readers that your books are genuinely human-written in a world where this is no longer a given. By providing proof of authenticity, you strengthen the bond between author and audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintain Consumer Trust&lt;/head&gt;
    &lt;p&gt;Our unique stamp reassures readers that your books are genuinely human-written in a world where this is no longer a given. By providing proof of authenticity, you strengthen the bond between author and audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintain Consumer Trust&lt;/head&gt;
    &lt;p&gt;Our unique stamp reassures readers that your books are genuinely human-written in a world where this is no longer a given. By providing proof of authenticity, you strengthen the bond between author and audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Differentiate Your Brand&lt;/head&gt;
    &lt;p&gt;Stand out in a market becoming saturated with AI. Amidst a mass of processed content, our stamp signposts your books and brand as organic, and deepens sales to an increasingly committed audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Differentiate Your Brand&lt;/head&gt;
    &lt;p&gt;Stand out in a market becoming saturated with AI. Amidst a mass of processed content, our stamp signposts your books and brand as organic, and deepens sales to an increasingly committed audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Differentiate Your Brand&lt;/head&gt;
    &lt;p&gt;Stand out in a market becoming saturated with AI. Amidst a mass of processed content, our stamp signposts your books and brand as organic, and deepens sales to an increasingly committed audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Differentiate Your Brand&lt;/head&gt;
    &lt;p&gt;Stand out in a market becoming saturated with AI. Amidst a mass of processed content, our stamp signposts your books and brand as organic, and deepens sales to an increasingly committed audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Certify Strategically&lt;/head&gt;
    &lt;p&gt;Certify your whole organisation. Our process equips you with long-term safeguards, strengthens your AI controls, and shows the world you√¢re committed to protecting human creativity at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Certify Strategically&lt;/head&gt;
    &lt;p&gt;Certify your whole organisation. Our process equips you with long-term safeguards, strengthens your AI controls, and shows the world you√¢re committed to protecting human creativity at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Certify Strategically&lt;/head&gt;
    &lt;p&gt;Certify your whole organisation. Our process equips you with long-term safeguards, strengthens your AI controls, and shows the world you√¢re committed to protecting human creativity at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Certify Strategically&lt;/head&gt;
    &lt;p&gt;Certify your whole organisation. Our process equips you with long-term safeguards, strengthens your AI controls, and shows the world you√¢re committed to protecting human creativity at scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Books By People Team&lt;/head&gt;
    &lt;p&gt;Advisor&lt;/p&gt;
    &lt;p&gt;James is a multiple-time agency founder and CEO of Rocket: a marketing and content business whose work has included projects with Harper Collins, Penguin, The Bookseller and more. He started the country√¢s first influencer marketing agency and is often seen on conference panels as an expert on √¢Book-Tok√¢.&lt;/p&gt;
    &lt;p&gt;Advisor&lt;/p&gt;
    &lt;p&gt;James is a multiple-time agency founder and CEO of Rocket: a marketing and content business whose work has included projects with Harper Collins, Penguin, The Bookseller and more. He started the country√¢s first influencer marketing agency and is often seen on conference panels as an expert on √¢Book-Tok√¢.&lt;/p&gt;
    &lt;p&gt;Advisor&lt;/p&gt;
    &lt;p&gt;James is a multiple-time agency founder and CEO of Rocket: a marketing and content business whose work has included projects with Harper Collins, Penguin, The Bookseller and more. He started the country√¢s first influencer marketing agency and is often seen on conference panels as an expert on √¢Book-Tok√¢.&lt;/p&gt;
    &lt;p&gt;Advisor&lt;/p&gt;
    &lt;p&gt;James is a multiple-time agency founder and CEO of Rocket: a marketing and content business whose work has included projects with Harper Collins, Penguin, The Bookseller and more. He started the country√¢s first influencer marketing agency and is often seen on conference panels as an expert on √¢Book-Tok√¢.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently Asked Questions&lt;/head&gt;
    &lt;p&gt;How much does the certification cost?&lt;/p&gt;
    &lt;p&gt;Applying for certification is completely free. If your publishing house is approved, an annual fee applies based on the number of titles you publish each year.&lt;/p&gt;
    &lt;p&gt;For full details on our pricing structure, please enquire through our application system.&lt;/p&gt;
    &lt;p&gt;Will we have to do a lot of extra work?&lt;/p&gt;
    &lt;p&gt;Can I use the Books By People stamp on all my titles?&lt;/p&gt;
    &lt;p&gt;How do you define Organic Literature vs AI-written?&lt;/p&gt;
    &lt;p&gt;How do I get in contact?&lt;/p&gt;
    &lt;p&gt;How much does the certification cost?&lt;/p&gt;
    &lt;p&gt;Applying for certification is completely free. If your publishing house is approved, an annual fee applies based on the number of titles you publish each year.&lt;/p&gt;
    &lt;p&gt;For full details on our pricing structure, please enquire through our application system.&lt;/p&gt;
    &lt;p&gt;Will we have to do a lot of extra work?&lt;/p&gt;
    &lt;p&gt;Can I use the Books By People stamp on all my titles?&lt;/p&gt;
    &lt;p&gt;How do you define Organic Literature vs AI-written?&lt;/p&gt;
    &lt;p&gt;How do I get in contact?&lt;/p&gt;
    &lt;p&gt;How much does the certification cost?&lt;/p&gt;
    &lt;p&gt;Applying for certification is completely free. If your publishing house is approved, an annual fee applies based on the number of titles you publish each year.&lt;/p&gt;
    &lt;p&gt;For full details on our pricing structure, please enquire through our application system.&lt;/p&gt;
    &lt;p&gt;Will we have to do a lot of extra work?&lt;/p&gt;
    &lt;p&gt;Can I use the Books By People stamp on all my titles?&lt;/p&gt;
    &lt;p&gt;How do you define Organic Literature vs AI-written?&lt;/p&gt;
    &lt;p&gt;How do I get in contact?&lt;/p&gt;
    &lt;p&gt;How much does the certification cost?&lt;/p&gt;
    &lt;p&gt;Applying for certification is completely free. If your publishing house is approved, an annual fee applies based on the number of titles you publish each year.&lt;/p&gt;
    &lt;p&gt;For full details on our pricing structure, please enquire through our application system.&lt;/p&gt;
    &lt;p&gt;Will we have to do a lot of extra work?&lt;/p&gt;
    &lt;p&gt;Can I use the Books By People stamp on all my titles?&lt;/p&gt;
    &lt;p&gt;How do you define Organic Literature vs AI-written?&lt;/p&gt;
    &lt;p&gt;How do I get in contact?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://booksbypeople.org/"/><published>2025-10-26T16:57:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45713959</id><title>A definition of AGI</title><updated>2025-10-27T09:13:57.521419+00:00</updated><content>&lt;doc fingerprint="e99d252bccd7a6af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 21 Oct 2025 (v1), last revised 23 Oct 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:A Definition of AGI&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 57%) concretely quantify both rapid progress and the substantial gap remaining before AGI.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Long Phan [view email]&lt;p&gt;[v1] Tue, 21 Oct 2025 01:28:35 UTC (20,673 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 23 Oct 2025 18:00:45 UTC (20,299 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2510.18212"/><published>2025-10-26T18:09:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715055</id><title>Show HN: MyraOS ‚Äì My 32-bit operating system in C and ASM (Hack Club project)</title><updated>2025-10-27T09:13:56.804604+00:00</updated><content>&lt;doc fingerprint="a2641ac90aa6e498"&gt;
  &lt;main&gt;
    &lt;p&gt;A x86 Unix-like OS made entirely from scratch.&lt;/p&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protected mode (GDT/IDT, ISRs/IRQs)&lt;/item&gt;
      &lt;item&gt;Paging and virtual memory&lt;/item&gt;
      &lt;item&gt;Memory management&lt;/item&gt;
      &lt;item&gt;Heap and dynamic memory&lt;/item&gt;
      &lt;item&gt;User-mode (ring 3) and kernel mode (ring 0)&lt;/item&gt;
      &lt;item&gt;Processes and scheduling&lt;/item&gt;
      &lt;item&gt;Drivers (PIT, RTC, Keyboard, Mouse, Framebuffer, PATA)&lt;/item&gt;
      &lt;item&gt;ext2 filesystem&lt;/item&gt;
      &lt;item&gt;UI compositor with window widgets, labels, icons, buttons, and even a custom-made font&lt;/item&gt;
      &lt;item&gt;ELF loader, which gives you the ability to run real apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All these features let you run real games, just like Doom, giving the preloaded Doom port in MyraOS ready to be played!&lt;lb/&gt; So, this isn't just a toy OS or a look-alike, it's a real OS that can run on real devices&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest release from the release tab in GitHub&lt;/item&gt;
      &lt;item&gt;Download QEMU - an open-source machine emulator and virtualizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After you get the latest release, you can run this on your platform:&lt;/p&gt;
    &lt;p&gt;Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen (if you are like me and want it to look real)&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -full-screen
&lt;/code&gt;
    &lt;p&gt;Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -display gtk,zoom-to-fit=on -full-screen
&lt;/code&gt;
    &lt;p&gt;Here, Linux/macOS or even WSL are better; use it as a last resort:&lt;lb/&gt; Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -display gtk,zoom-to-fit=on -full-screen
&lt;/code&gt;
    &lt;p&gt;I really hope you like it, as I spent a lot of time on it, and I'd really appreciate any feedback you have for me.&lt;lb/&gt; If you have anything, from feature requests to feedback, or even if you want to talk, email me here: &lt;code&gt;dvirm.biton@gmail.com&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/dvir-biton/MyraOS"/><published>2025-10-26T20:43:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715204</id><title>We Saved $500k per Year by Rolling Our Own "S3"</title><updated>2025-10-27T09:13:55.215399+00:00</updated><content>&lt;doc fingerprint="2559ed5308a855e9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How We Saved $500,000 Per Year by Rolling Our Own ‚ÄúS3‚Äù&lt;/head&gt;
    &lt;head rend="h2"&gt;tl;dr&lt;/head&gt;
    &lt;p&gt;We used S3 as a landing zone for Nanit‚Äôs video processing pipeline (baby sleep-state inference), but at thousands of uploads/second, S3‚Äôs PutObject request fees dominated costs. Worse, S3‚Äôs auto-cleanup (Lifecycle rules) has a 1-day minimum; we paid for 24 hours of storage on objects processed in ~2 seconds. We built N3, a Rust-based in-memory landing zone that eliminates both issues, using S3 only as an overflow buffer.&lt;/p&gt;
    &lt;p&gt;Result: meaningful cost reduction (~$0.5M/year).&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1: Background&lt;/head&gt;
    &lt;head rend="h2"&gt;High-Level Overview of Our Video Processing Pipeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cameras record video chunks (configurable duration).&lt;/item&gt;
      &lt;item&gt;For each chunk, the camera requests an S3 presigned URL from the Camera Service and uploads directly to S3.&lt;/item&gt;
      &lt;item&gt;An AWS Lambda posts the object key to an SQS FIFO queue (sharded by baby_uid).&lt;/item&gt;
      &lt;item&gt;Video processing pods consume from SQS, download from S3, and produce sleep states.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a deeper dive, see this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Like About This Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Landing on S3 + queuing to SQS decouples camera uploads from video processing. During maintenance or temporary downtime, we don‚Äôt lose videos; if queues grow, we scale processing.&lt;/item&gt;
      &lt;item&gt;With S3, we don‚Äôt manage availability or durability.&lt;/item&gt;
      &lt;item&gt;SQS FIFO + group IDs preserve per-baby ordering, keeping processing nodes mostly stateless (coordination happens in SQS).&lt;/item&gt;
      &lt;item&gt;S3 Lifecycle rules offload GC: objects expire after one day, so we don‚Äôt track processed videos.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why We Changed&lt;/head&gt;
    &lt;p&gt;PutObject costs dominated. Our objects are short-lived: videos land for seconds, then get processed. At our scale (thousands of uploads/s), the per-object request charge was the largest cost driver. Increasing chunking frequency (i.e., sending more, smaller chunks) to cut latency raises costs linearly, because each additional chunk is another PutObject request.&lt;/p&gt;
    &lt;p&gt;Storage was a secondary tax. Even when processing finished in ~2 s, Lifecycle deletes meant paying for ~24 h of storage.&lt;/p&gt;
    &lt;p&gt;We needed a design that kept reliability and strict ordering while avoiding per-object costs on the happy path and minimizing ‚Äúpay-to-wait‚Äù storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 2: Planning&lt;/head&gt;
    &lt;head rend="h2"&gt;Guiding Principles&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simplicity through architecture: Eliminate complexity at the design level, not through clever implementations.&lt;/item&gt;
      &lt;item&gt;Correctness: A true drop-in replacement that‚Äôs transparent to the rest of the pipeline.&lt;/item&gt;
      &lt;item&gt;Optimize for the happy path: Design for the normal case and use S3 as a safety net for edge cases. Our processing algorithms are robust to occasional gaps, so we can prioritize simplicity over building complex guarantees; S3 provides reliability when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Design Drivers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Short-lived objects: segments live on the landing zone for seconds, not hours.&lt;/item&gt;
      &lt;item&gt;Ordering: strict per-baby sequencing (no processing newer before older).&lt;/item&gt;
      &lt;item&gt;Throughput: thousands of uploads/second; 2‚Äì6 MB per segment.&lt;/item&gt;
      &lt;item&gt;Client limits: cameras have limited retries; don‚Äôt assume retransmits.&lt;/item&gt;
      &lt;item&gt;Operations: tolerate multi-million-item backlogs during maintenance/scale-ups.&lt;/item&gt;
      &lt;item&gt;No firmware changes: must work with existing cameras.&lt;/item&gt;
      &lt;item&gt;Loss tolerance: very small gaps are acceptable; algorithms mask them.&lt;/item&gt;
      &lt;item&gt;Cost: avoid per-object S3 costs on the happy path; minimize ‚Äúpay-to-wait‚Äù storage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Design at a Glance (N3 Happy Path + S3 Overflow)&lt;/head&gt;
    &lt;head rend="h3"&gt;The Architecture&lt;/head&gt;
    &lt;p&gt;N3 is a custom landing zone that holds videos in memory just long enough for processing to drain them (~2 seconds). S3 is used only when N3 can‚Äôt handle the load.&lt;/p&gt;
    &lt;p&gt;Two components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;N3-Proxy (stateless, dual interfaces): &lt;lb/&gt;- External (Internet-facing): Accepts camera uploads via presigned URLs.&lt;lb/&gt;- Internal (private): Issues presigned URLs to Camera Service.&lt;/item&gt;
      &lt;item&gt;N3-Storage (stateful, internal-only): Stores uploaded segments in RAM and enqueues SQS with a pod-addressable download URL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Video processing pods consume from SQS FIFO and download from whichever storage the URL points to: N3 or S3.&lt;/p&gt;
    &lt;p&gt;Normal Flow (Happy Path)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Camera requests an upload URL from Camera Service.&lt;/item&gt;
      &lt;item&gt;Camera Service calls N3-Proxy‚Äôs internal API for a presigned URL.&lt;/item&gt;
      &lt;item&gt;Camera uploads video to N3-Proxy‚Äôs external endpoint.&lt;/item&gt;
      &lt;item&gt;N3-Proxy forwards to N3-Storage.&lt;/item&gt;
      &lt;item&gt;N3-Storage holds video in memory and enqueues to SQS with a download URL pointing to itself.&lt;/item&gt;
      &lt;item&gt;Processing pod downloads from N3-Storage and processes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Two-Tier Fallback&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tier 1: Proxy-level fallback (per-request): &lt;lb/&gt;If N3-Storage can‚Äôt accept an upload whether from memory pressure, processing backlog, or pod failure N3-Proxy uploads to S3 on the camera‚Äôs behalf.&lt;lb/&gt;(Camera got a presigned N3 URL before the failure was detected)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tier 2: Cluster-level reroute (all traffic): &lt;lb/&gt;If N3-Proxy or N3-Storage is unhealthy, Camera Service stops issuing N3 URLs and returns S3 presigned URLs directly.&lt;lb/&gt;(All traffic flows to S3 until N3 recovers.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why Two Components?&lt;/p&gt;
    &lt;p&gt;We split N3-Proxy and N3-Storage because they have different requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blast radius: If storage crashes, proxy can still route to S3. If proxy crashes, only that node‚Äôs traffic is affected; not the entire storage cluster.&lt;/item&gt;
      &lt;item&gt;Resource profiles: Proxy is CPU/network-heavy (TLS termination). Storage is memory-heavy (holding videos). Different instance types and scaling requirments.&lt;/item&gt;
      &lt;item&gt;Security: Storage never touches the Internet.&lt;/item&gt;
      &lt;item&gt;Rollout safety: We can update proxy (stateless) without touching storage (holding active data).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Validating the Design&lt;/head&gt;
    &lt;p&gt;The architecture made sense on paper, but we had critical unknowns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Capacity &amp;amp; sizing: real upload durations across client networks; how much compute and upload buffer size we need?&lt;/item&gt;
      &lt;item&gt;Storage model: can we keep everything in RAM, or do we need disks?&lt;/item&gt;
      &lt;item&gt;Resilience: how to load balance cheaply and handle failed nodes?&lt;/item&gt;
      &lt;item&gt;Operational policy: GC needs, retry expectations, and whether delete-on-GET is sufficient.&lt;/item&gt;
      &lt;item&gt;Unknown unknowns: what edge cases would emerge when idea meet reality?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To de-risk decisions, we ran two tracks during planning:&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 1: Synthetic Stress Tests&lt;/head&gt;
    &lt;p&gt;We built a load generator to push the system to its limits: varying concurrency, slow clients, sustained load, and processing downtime.&lt;/p&gt;
    &lt;p&gt;Goal: Find breaking points. Surface bottlenecks we hadn‚Äôt anticipated. Get deterministic baselines for capacity planning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 2: Production PoC (Mirror Mode)&lt;/head&gt;
    &lt;p&gt;Synthetic tests can‚Äôt replicate real camera behavior: flaky Wi-Fi, diverse firmware versions, unpredictable network conditions. We needed in-the-wild data without risking production.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mirror mode: n3-proxy wrote to S3 first (preserving prod), then also to a PoC N3-Storage wired to a canary SQS + video processors.&lt;/item&gt;
      &lt;item&gt;Targeted cohorts: by firmware version / Baby-UID lists&lt;/item&gt;
      &lt;item&gt;Data parity: compared sleep states PoC vs. production; investigated any diffs.&lt;/item&gt;
      &lt;item&gt;Observability: per-path dashboards (N3 vs. S3), queue depth, latency/RPS, error budgets, egress breakdown.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feature flags (via Unleash) were critical. We could flip cohorts on/off in real-time; no deployments; letting us test narrow slices (older firmware, weak Wi-Fi cameras) and revert instantly if issues appeared.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Discovered&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bottlenecks: TLS termination consumed most CPU, and AWS burstable networking throttled us after credits expired.&lt;/item&gt;
      &lt;item&gt;Memory-only storage was viable. Real upload-time distributions and concurrency showed we could fit the working set in RAM with safe headroom; disks not required.&lt;/item&gt;
      &lt;item&gt;Delete-on-GET is safe. We did not observe re-downloads; retries happen downstream in the processor, so N3 doesn‚Äôt need to support download retries.&lt;/item&gt;
      &lt;item&gt;We need lightweight GC. Some segments get skipped by processing and would never be downloaded/deleted; added a TTL GC pass to clean stragglers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These findings shaped our implementation: memory-backed storage, network- optimized instances with TLS optimization, and delete-on-GET with TTL GC for stragglers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: Implementation Details&lt;/head&gt;
    &lt;head rend="h2"&gt;DNS Load Balancing&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;n3-proxy&lt;/code&gt; is a DaemonSet on dedicated nodes, one pod per node to maximize network and CPU resources for TLS termination. We need node-level load balancing and graceful restarts.&lt;/p&gt;
    &lt;p&gt;An AWS Network Load Balancer would work, but at our throughput (thousands of uploads/second, sustained multi-GB/s), the combination of fixed costs plus per-GB processed fees becomes expensive. Instead, we use DNS-based load balancing via Route53 multi-value A records, which is significantly cheaper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For each node we create a MultiValue record that contains a single IP.&lt;/item&gt;
      &lt;item&gt;Each record has a health check that hits an external readiness endpoint.&lt;/item&gt;
      &lt;item&gt;A records use a short 30-second TTL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This gives us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If a node fails, it‚Äôs taken out of the pool and cameras stop uploading to it.&lt;/item&gt;
      &lt;item&gt;Because the external readiness endpoint is also used as the Kubernetes readiness probe, marking a pod Not Ready during rollouts automatically removes it from DNS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Rollout process&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;n3-proxy&lt;/code&gt; pods have a graceful shutdown mechanism:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;On SIGTERM, the pod enters paused mode.&lt;/item&gt;
      &lt;item&gt;Readiness becomes Not Ready, but uploads are still accepted.&lt;/item&gt;
      &lt;item&gt;Wait 2√ó DNS TTL (e.g., 60s) so the DNS health check removes the node and camera DNS caches update.&lt;/item&gt;
      &lt;item&gt;Drain active connections, then restart.&lt;/item&gt;
      &lt;item&gt;On startup, wait for health checks to pass and for client DNS TTLs to expire before rolling to the next pod (lets the node rejoin the pool).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Networking Limitations&lt;/head&gt;
    &lt;p&gt;When doing initial benchmarks to size the cluster, we saw a surprising pattern: runs started near ~1k RPS, then dropped to ~70 RPS after ~1 minute. Restarting didn‚Äôt help; after waiting and rerunning, we briefly saw ~1k RPS again.&lt;/p&gt;
    &lt;p&gt;It turns out that when AWS says an instance can do ‚ÄúUp to 12.5 Gbps‚Äù, that‚Äôs burstable networking backed by credits; when you‚Äôre below the baseline, you accrue credits and can burst for short periods.&lt;/p&gt;
    &lt;p&gt;Baseline depends on instance family and vCPUs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non‚Äìnetwork-optimized: ~0.375 Gbps/vCPU&lt;/item&gt;
      &lt;item&gt;Network-optimized (suffix ‚Äún‚Äù): ~3.125 Gbps/vCPU&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And for instances that don‚Äôt say ‚ÄúUp to,‚Äù you get the stated Gbps continuously.&lt;/p&gt;
    &lt;p&gt;Conclusion: our workload is steady, so bursts don‚Äôt help. We moved to network optimized c8gn.4xlarge nodes, which provide 50 Gbps each, giving us the sustained throughput we need.&lt;/p&gt;
    &lt;head rend="h2"&gt;HTTPS, rustls, and Graviton4&lt;/head&gt;
    &lt;p&gt;Initially, for simplicity, we used a &lt;code&gt;stunnel&lt;/code&gt; sidecar for HTTPS termination, but early stress testing showed HTTPS was the main CPU consumer and primary bottleneck. We made three changes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Moved from &lt;code&gt;stunnel&lt;/code&gt;to native rustls.&lt;/item&gt;
      &lt;item&gt;Upgraded from Graviton3 to Graviton4 instances.&lt;/item&gt;
      &lt;item&gt;Compiled &lt;code&gt;n3-proxy&lt;/code&gt;with target-cpu and crypto features enabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[profile.release]&lt;lb/&gt;opt-level = 3&lt;lb/&gt;lto = "fat"&lt;lb/&gt;codegen-units = 1&lt;lb/&gt;split-debuginfo = "off"&lt;lb/&gt;debug = false&lt;lb/&gt;panic = "abort"&lt;lb/&gt;overflow-checks = false&lt;lb/&gt;&lt;lb/&gt;[target.aarch64-unknown-linux-gnu]&lt;lb/&gt;rustflags = [&lt;lb/&gt;    "-C", "target-cpu=neoverse-v2",&lt;lb/&gt;    "-C", "target-feature=+sve2,+sve2-aes,+sve2-sha3,+sve2-sm4,+sve2-bitperm,+crypto"&lt;lb/&gt;]&lt;/code&gt;
    &lt;p&gt;These changes yielded ~30% higher RPS at the same cost.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outgoing Traffic Costs&lt;/head&gt;
    &lt;p&gt;We assumed that since we only receive uploads (ingress is free) and don‚Äôt send payloads to clients, egress would be negligible. Post-launch, we saw non-trivial outbound traffic.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLS handshakes&lt;/head&gt;
    &lt;p&gt;Each upload opens a new TLS connection, so a full handshake runs and sends ~7 KB of certificates. In theory we could reduce this with smaller (e.g., ECDSA) certs, session resumption/tickets, or long-lived connections; but given our constraint of not changing camera behavior, we accept this overhead for now.&lt;/p&gt;
    &lt;head rend="h3"&gt;ACKs&lt;/head&gt;
    &lt;p&gt;Surprisingly, TLS handshakes were only a small part of the outbound bytes. A &lt;code&gt;tcpdump&lt;/code&gt; showed many &lt;code&gt;66-byte&lt;/code&gt; ACKs:&lt;/p&gt;
    &lt;code&gt;tshark -r n3-3.pcap \&lt;lb/&gt;  -Y 'tcp.srcport==32443 &amp;amp;&amp;amp; !(tcp.analysis.retransmission || tcp.analysis.fast_retransmission)' \&lt;lb/&gt;  -T fields -e tcp.len -e frame.len \&lt;lb/&gt;| awk '{&lt;lb/&gt;  total += $2&lt;lb/&gt;  if ($1 == 0) { ack += $2 } else { data_frames += $2; payload += $1 }&lt;lb/&gt;}&lt;lb/&gt;END {&lt;lb/&gt;  printf "total_bytes=%d\nack_frame_bytes=%d (%.1f%%)\ndata_frame_bytes=%d (%.1f%%)\n",&lt;lb/&gt;         total, ack, 100*ack/total, data_frames, 100*data_frames/total&lt;lb/&gt;  printf "tcp_payload_bytes=%d (of data frames)\n", payload&lt;lb/&gt;}'&lt;/code&gt;
    &lt;p&gt;This was a short traffic capture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;total_bytes = 37,014,432&lt;/item&gt;
      &lt;item&gt;ack_frame_bytes = 31,258,550 (84.4%)&lt;/item&gt;
      &lt;item&gt;data_frame_bytes = 5,755,882 (15.6%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;~85% of outbound bytes were ACK frames.&lt;/p&gt;
    &lt;p&gt;With ~1500-byte MTUs and frequent ACKs, overhead adds up. While we can‚Äôt easily reduce the number of ACKs, we can make each ACK smaller by removing TCP timestamps (‚àí12 bytes/ACK):&lt;/p&gt;
    &lt;code&gt;sysctl -w net.ipv4.tcp_timestamps=0&lt;/code&gt;
    &lt;p&gt;Kubernetes init-container:&lt;/p&gt;
    &lt;code&gt;spec:&lt;lb/&gt;  initContainers:&lt;lb/&gt;  - name: set-sysctl&lt;lb/&gt;    image: alpine:3.20&lt;lb/&gt;    securityContext: { privileged: true }&lt;lb/&gt;    command: ["sh","-c","sysctl -w net.ipv4.tcp_timestamps=0"]&lt;lb/&gt;  containers:&lt;lb/&gt;  - name: your-app&lt;lb/&gt;    image: ...&lt;/code&gt;
    &lt;p&gt;This isn‚Äôt without risk: with high byte counts on the same socket, sequence numbers can wrap and delayed packets may be mis-merged, causing corruption.&lt;lb/&gt;Mitigations: (1) new socket per upload; (2) recycle &lt;code&gt;n3-proxy&lt;/code&gt; ‚Üî &lt;code&gt;n3-storage&lt;/code&gt; sockets after ~1 GB sent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Leak&lt;/head&gt;
    &lt;p&gt;After the initial launch, we saw steady n3-proxy memory growth. Even after traffic stopped, the process returned to an ever-higher baseline ‚Äî so it wasn‚Äôt just the OS holding freed pages.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;jemalloc&lt;/code&gt; stats showed referenced memory constantly increasing.&lt;/p&gt;
    &lt;p&gt;Using rust-jemalloc-pprof we profiled memory in production and identified growth in per-connection &lt;code&gt;hyper&lt;/code&gt; &lt;code&gt;BytesMut&lt;/code&gt; buffers.&lt;/p&gt;
    &lt;p&gt;Since we handle large uploads over variable networks, some client connections stalled mid-transfer and never cleaned up. The per-connection &lt;code&gt;hyper&lt;/code&gt; buffers (&lt;code&gt;BytesMut&lt;/code&gt;) stuck around and memory kept climbing. When we Terminated connections idle &amp;gt;10 minutes, memory dropped by ~1 GB immediately; confirming the leak was from dangling sockets.&lt;/p&gt;
    &lt;p&gt;Fix: make sockets short-lived and enforce time limits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable keep-alive: close the connection immediately after each upload completes.&lt;/item&gt;
      &lt;item&gt;Tighten timeouts: set header/socket timeouts so stalled uploads are terminated and buffers are freed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fn make_listener(addr: &amp;amp;str) -&amp;gt; std::io::Result&amp;lt;std::net::TcpListener&amp;gt; {&lt;lb/&gt;    let addr: SocketAddr = addr.parse().unwrap();&lt;lb/&gt;    let sock = Socket::new(Domain::for_address(addr), Type::STREAM, Some(Protocol::TCP))?;&lt;lb/&gt;    sock.bind(&amp;amp;addr.into())?;&lt;lb/&gt;&lt;lb/&gt;    let ka = TcpKeepalive::new()&lt;lb/&gt;        .with_time(Duration::from_secs(60))&lt;lb/&gt;        .with_interval(Duration::from_secs(15))&lt;lb/&gt;        .set_reuse_port(true)&lt;lb/&gt;        .with_retries(4);&lt;lb/&gt;    sock.set_tcp_keepalive(&amp;amp;ka)?;&lt;lb/&gt;    sock.listen(4096)?;&lt;lb/&gt;    sock.set_nonblocking(true)?; // NOTE: required before handing to Tokio&lt;lb/&gt;&lt;lb/&gt;    Ok(sock.into())&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;pub fn create_server_external(&lt;lb/&gt;    addr_external: &amp;amp;str,&lt;lb/&gt;    rustls_config: RustlsConfig,&lt;lb/&gt;) -&amp;gt; Result&amp;lt;Server&amp;lt;RustlsAcceptor&amp;gt;, MainError&amp;gt; {&lt;lb/&gt;    let listener_external = make_listener(addr_external).map_err(|error| MainError::BindError {&lt;lb/&gt;        addr: addr_external.to_string(),&lt;lb/&gt;        error,&lt;lb/&gt;    })?;&lt;lb/&gt;&lt;lb/&gt;    let mut ext = axum_server::from_tcp_rustls(listener_external, rustls_config);&lt;lb/&gt;    ext.http_builder()&lt;lb/&gt;        .http1()&lt;lb/&gt;        .timer(TokioTimer::new())&lt;lb/&gt;        .max_buf_size(128 * 1024)&lt;lb/&gt;        .header_read_timeout(Some(Duration::from_secs(60)))&lt;lb/&gt;        .keep_alive(false);&lt;lb/&gt;&lt;lb/&gt;    Ok(ext)&lt;lb/&gt;}&lt;/code&gt;
    &lt;head rend="h2"&gt;Storage&lt;/head&gt;
    &lt;p&gt;We started with the simplest path: in-memory storage. It avoids I/O tuning and lets us use straightforward data structures.&lt;/p&gt;
    &lt;code&gt;type Store = Arc&amp;lt;DashMap&amp;lt;Ulid, Bytes&amp;gt;&amp;gt;;&lt;lb/&gt;&lt;lb/&gt;pub struct VideoStore {&lt;lb/&gt;    videos: Store,&lt;lb/&gt;    bytes_used: AtomicUsize,&lt;lb/&gt;    control: Arc&amp;lt;Control&amp;gt;,&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;Each video upload increments &lt;code&gt;bytes_used&lt;/code&gt; ; each download deletes the video and decrements it.&lt;/p&gt;
    &lt;p&gt;Above ~80% capacity, we start rejecting uploads to avoid OOM and signal n3-proxy to stop signing upload URLs.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;control&lt;/code&gt; handle lets us manually pause uploads and garbage collection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Graceful Restart&lt;/head&gt;
    &lt;p&gt;With memory-only storage, restarts must not drop in-flight data. Our graceful restart process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;SIGTERM&lt;/code&gt;to a pod (StatefulSet rolls one pod at a time).&lt;/item&gt;
      &lt;item&gt;Pod becomes Not Ready and leaves the Service (no new uploads).&lt;/item&gt;
      &lt;item&gt;It continues serving downloads for already-uploaded videos.&lt;/item&gt;
      &lt;item&gt;Once downloads quiesce (no recent reads ‚Üí processing drained),&lt;/item&gt;
      &lt;item&gt;Wait for any open requests to complete&lt;/item&gt;
      &lt;item&gt;Restart and move to the next pod.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Under normal operation pods drain in seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;GC&lt;/head&gt;
    &lt;p&gt;We use two cleanup mechanisms:&lt;/p&gt;
    &lt;head rend="h3"&gt;Delete on download&lt;/head&gt;
    &lt;p&gt;We delete videos immediately after download. In the PoC, we saw zero re-downloads; video processors retry internally. This eliminates the need to hold data or track ‚Äúprocessed‚Äù state.&lt;/p&gt;
    &lt;head rend="h3"&gt;TTL GC for stragglers&lt;/head&gt;
    &lt;p&gt;Deleting on download doesn‚Äôt cover segments skipped by the processor (never downloaded ‚Üí never deleted). We added a lightweight TTL GC: periodically scan the in-memory DashMap and remove entries older than a configurable threshold (e.g., a few hours).&lt;/p&gt;
    &lt;head rend="h3"&gt;Maintenance mode&lt;/head&gt;
    &lt;p&gt;During planned processing downtime, we can temporarily pause GC via an internal control so videos aren‚Äôt deleted while consumption is stopped.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 4: Conclusion&lt;/head&gt;
    &lt;p&gt;By using S3 as a fallback buffer and N3 as the primary landing zone, we eliminated ~$0.5M/year in costs while keeping the system simple and reliable.&lt;/p&gt;
    &lt;p&gt;The key insight: most ‚Äúbuild vs. buy‚Äù decisions focus on features, but at scale, economics shift the calculus. For short-lived objects (~2 seconds in normal operation), we don‚Äôt need replication or sophisticated durability; simple in-memory storage works. But when processing lags or maintenance extends object lifetime, we need S3‚Äôs reliability guarantees. We get the best of both worlds: N3 handles the happy path efficiently, while S3 provides durability when objects need to live longer. If N3 has any issues; memory pressure, pod crashes, or cluster problems; uploads seamlessly fail over to S3.&lt;/p&gt;
    &lt;p&gt;What Made This Work&lt;/p&gt;
    &lt;p&gt;Defining the problem clearly upfront: constraints, assumptions, and boundaries prevented scope creep. Validating early with a mirror-mode PoC let us discover bottlenecks (TLS, network throttling) and validate assumptions before committing. This prevented overengineering and backtracking.&lt;/p&gt;
    &lt;p&gt;When Should You Build Something Like This?&lt;/p&gt;
    &lt;p&gt;Consider custom infrastructure when you have both: sufficient scale for meaningful cost savings, and specific constraints that enable a simple solution. The engineering effort to build and maintain your system must be less than the infrastructure costs it eliminates. In our case, specific requirements (ephemeral storage, loss tolerance, S3 fallback) let us build something simple enough that maintenance costs stay low. Without both factors, stick with managed services.&lt;/p&gt;
    &lt;p&gt;Would we do it again? Yes. The system has been running reliably in production, and the fallback design lets us avoid complexity without sacrificing reliability.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://engineering.nanit.com/how-we-saved-500-000-per-year-by-rolling-our-own-s3-6caec1ee1143"/><published>2025-10-26T21:05:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715726</id><title>Poison, Poison Everywhere</title><updated>2025-10-27T09:13:55.123712+00:00</updated><content/><link href="https://loeber.substack.com/p/29-poison-poison-everywhere"/><published>2025-10-26T22:36:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715752</id><title>Show HN: Helium Browser for Android with extensions support, based on Vanadium</title><updated>2025-10-27T09:13:54.418605+00:00</updated><content>&lt;doc fingerprint="9eb4893fee94f46c"&gt;
  &lt;main&gt;
    &lt;p&gt;An experimental Chromium-based web browser for Android with extensions support, based on&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Helium by imput, as well as&lt;/item&gt;
      &lt;item&gt;Vanadium by GrapheneOS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Navigate to Chrome Web Store, then enable Desktop site by selecting the menu button ‚ãÆ in the top right corner and ensure the option is checked. Select Okay and proceed as normal if prompted with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Chrome Web Store is only available on desktop.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Once you select Add to Chrome, the extension will be installed in the background until the button changes into Remove from Chrome.&lt;/p&gt;
    &lt;p&gt;To view and access the debug URLs, use &lt;code&gt;chrome://chrome-urls&lt;/code&gt;. For Experiments, use &lt;code&gt;chrome://flags&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Consistent with both Helium and Vanadium, the option is available by selecting the menu button ‚ãÆ in the top right corner, then Settings, Privacy and security, then under Privacy, WebRTC IP handling policy. If you experience issues with WebRTC due to the IPs being shielded by default (e.g. Discord Voice), you may try to change it to Default public interface only, or Default.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;All builds are experimental, so unexpected issues may occur. Helium Browser for Android only attempts to improve security and privacy where possible. For better protection on Android, you should instead use GrapheneOS with Vanadium, which additionally integrates patches into Android System WebView and provides significant kernel and memory management hardening on the OS level.&lt;/p&gt;
    &lt;code&gt;---
config:
  layout: dagre
---
flowchart TD
 subgraph s1["Helium"]
        n5["Generic Patches&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;patches/series&amp;lt;/small&amp;gt;"]
        n6["Name Substitution&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;utils/name_substitution.py&amp;lt;/small&amp;gt;"]
        n7["Version Patch&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;{*version,revision}.txt&amp;lt;/small&amp;gt;"]
        n8["Resource Patch&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;resources/*resources.txt&amp;lt;/small&amp;gt;"]
  end
 subgraph s2["Vanadium"]
        n9["Generic Patches&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;patches/*.patch&amp;lt;/small&amp;gt;"]
  end
 subgraph s3["Helium Browser for Android"]
        n11["GN Build Configuration&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;args.gn&amp;lt;/small&amp;gt;"]
        n12["Signed Release"]
  end
    n1["Chromium"] --&amp;gt; s1 &amp;amp; s2
    n5 --&amp;gt; n6
    n6 --&amp;gt; n7
    n7 --&amp;gt; n8
    s1 --&amp;gt; s3
    s2 --&amp;gt; s3
    n11 --&amp;gt; n12
    n5@{ shape: subproc}
    n6@{ shape: subproc}
    n7@{ shape: subproc}
    n8@{ shape: subproc}
    n9@{ shape: subproc}
    n11@{ shape: subproc}
    n12@{ shape: subproc}
    n1@{ shape: rounded}
    classDef Aqua stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
    style n5 stroke:#FF6D00
    style n8 stroke:#FF6D00
&lt;/code&gt;
    &lt;p&gt;The full build aims to be consistent with Helium, which means additional patches are necessary before all features can be ported over. All Vanadium patches are applied by default. Further patches are underway.&lt;/p&gt;
    &lt;p&gt;This repository provides the build script to compile on the latest Ubuntu, and may also work with other Linux distributions.&lt;/p&gt;
    &lt;p&gt;To build these releases yourself via CI (e.g. GitHub Actions), fork this repository. Supply your &lt;code&gt;base64&lt;/code&gt; encoded &lt;code&gt;keystore.jks&lt;/code&gt; and &lt;code&gt;local.properties&lt;/code&gt; (containing your &lt;code&gt;keyAlias&lt;/code&gt;, &lt;code&gt;keyPassword&lt;/code&gt; and &lt;code&gt;storePassword&lt;/code&gt;) to Repository secrets under Settings &amp;gt; Secrets and variables &amp;gt; Actions. To generate a release, go to Actions, select Build, and select Run workflow. Under Runner, you can either use a GitHub-hosted runner by entering &lt;code&gt;ubuntu-latest&lt;/code&gt;, or &lt;code&gt;self-hosted&lt;/code&gt; for your own hardware.&lt;/p&gt;
    &lt;p&gt;This project would not have been possible without the huge community contributions from Helium, Vanadium, as well as ungoogled-chromium and various other upstream projects.&lt;/p&gt;
    &lt;p&gt;All credit goes to the original authors and contributors. This project is named to reflect support for Helium's naming in a recent controversy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/jqssun/android-helium-browser"/><published>2025-10-26T22:41:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715837</id><title>Microsoft 365 Copilot ‚Äì Arbitrary Data Exfiltration via Mermaid Diagrams</title><updated>2025-10-27T09:13:54.288497+00:00</updated><content/><link href="https://www.adamlogue.com/microsoft-365-copilot-arbitrary-data-exfiltration-via-mermaid-diagrams-fixed/"/><published>2025-10-26T22:58:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715873</id><title>Are-we-fast-yet implementations in Oberon, C++, C, Pascal, Micron and Luon</title><updated>2025-10-27T09:13:53.660005+00:00</updated><content>&lt;doc fingerprint="12da93cf4494b8a7"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository includes additional implementations of the Are-we-fast-yet benchmark suite.&lt;/p&gt;
    &lt;p&gt;See here for the main repository of the Are-we-fast-yet suite: https://github.com/smarr/are-we-fast-yet. See also the ORIGINAL_README.md file in this repository.&lt;/p&gt;
    &lt;p&gt;Each additional implementation is in a separate subdirectory (e.g. "Cpp", "Oberon", "FreePascal"); see there for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rochus-keller/Are-we-fast-yet"/><published>2025-10-26T23:08:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45716109</id><title>How I turned Zig into my favorite language to write network programs in</title><updated>2025-10-27T09:13:53.528658+00:00</updated><content>&lt;doc fingerprint="ef463437cb212a9c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I turned Zig into my favorite language to write network programs in&lt;/head&gt;
    &lt;p&gt;I‚Äôve been watching the Zig language for a while now, given that it was created for writing audio software (low-level, no allocations, real time). I never paid too much attention though, it seemed a little weird to me and I didn‚Äôt see the real need. Then I saw a post from Andrew Kelley (creator of the language) on Hacker News, about how he reimplemented my Chromaprint algorithm in Zig, and that got me really interested.&lt;/p&gt;
    &lt;p&gt;I‚Äôve been planning to rewrite AcoustID‚Äôs inverted index for a long time, I had a couple of prototypes, but none of the approaches felt right. I was going through some rough times, wanted to learn something new, so I decided to use the project as an opportunity to learn Zig. And it was great, writing Zig is a joy. The new version was faster and more scalable than the previous C++ one. I was happy, until I wanted to add a server interface.&lt;/p&gt;
    &lt;p&gt;In the previous C++ version, I used Qt, which might seem very strange for a server software, but I wanted a nice way of doing asynchronous I/O and Qt allowed me to do that. It was callback-based, but Qt has a lot of support for making callbacks usable. In the newer prototypes, I used Go, specifically for the ease of networking and concurrency. With Zig, I was stuck. There are some Zig HTTP servers, so I could use those. I wanted to implement my legacy TCP server as well, and that‚Äôs a lot harder, unless I want to spawn a lot of threads. Then I made a crazy decision, to use Zig also for implementing a clustered layer on top of my server, using NATS as a messaging system, so I wrote a Zig NATS client, and that gave me a lot of experience with Zig‚Äôs networking capabilities.&lt;/p&gt;
    &lt;p&gt;Fast forward to today, I‚Äôm happy to introduce Zio, an asynchronous I/O and concurrency library for Zig. If you look at the examples, you will not really see where is the asynchronous I/O, but it‚Äôs there, in the background and that‚Äôs the point. Writing asynchronous code with callbacks is a pain. Not only that, it requires a lot of allocations, because you need state to survive across callbacks. Zio is an implementation of Go style concurrency, but limited to what‚Äôs possible in Zig. Zio tasks are stackful coroutines with fixed-size stacks. When you run &lt;code&gt;stream.read()&lt;/code&gt;, this will initiate the I/O operation in the background
and then suspend the current task until the I/O operation is done. When it‚Äôs done, the task will be resumed, and the result will be returned.
That gives you the illusion of synchronous code, allowing for much simpler state management.&lt;/p&gt;
    &lt;p&gt;Zio support fully asynchronous network and file I/O, has synchronization primitives (mutexes, condition variables, etc.) that work with the cooperative runtime, has Go-style channels, OS signal watches and more. Tasks can run in single-threaded mode, or multi-threaded, in which case they can migrate from thread to thread for lower latency and better load balancing.&lt;/p&gt;
    &lt;p&gt;And it‚Äôs FAST. I don‚Äôt want to be posting benchmarks here, maybe later when I have more complex ones, but the single-threaded mode is beating any framework I‚Äôve tried so far. It‚Äôs much faster than both Go and Rust‚Äôs Tokio. Context switching is virtually free, comparable to a function call. The multi-threaded mode, while still not being as robust as Go/Tokio, has comparable performance. It‚Äôs still a bit faster than either of them, but that performance might go down as I add more fairness features.&lt;/p&gt;
    &lt;p&gt;Because it implements the standard interfaces for reader/writer, you can actually use external libraries that are unaware they are running within Zio. Here is an example of a HTTP server:&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const zio = @import("zio");

const MAX_REQUEST_HEADER_SIZE = 64 * 1024;

fn connectionTask(rt: *zio.Runtime, stream: zio.net.Stream) !void {
    defer stream.close(rt);

    var read_buffer: [MAX_REQUEST_HEADER_SIZE]u8 = undefined;
    var reader = stream.reader(rt, &amp;amp;read_buffer);

    var write_buffer: [4096]u8 = undefined;
    var writer = stream.writer(rt, &amp;amp;write_buffer);

    var server = std.http.Server.init(
        &amp;amp;reader.interface,
        &amp;amp;writer.interface,
    );

    while (true) {
        var request = try server.receiveHead();
        try request.respond("hello", .{ .status = .ok });

        if (!request.head.keep_alive) break;
    }
}

fn serverTask(rt: *zio.Runtime) !void {
    const addr = try zio.net.IpAddress.parse("127.0.0.1", 8080);

    const server = try addr.listen(rt, .{});
    defer server.close(rt);

    while (true) {
        const stream = try server.accept(rt);
        errdefer stream.close(rt);

        var task = try rt.spawn(
            connectionTask, .{ rt, stream }, .{}
        );
        task.deinit();
    }
}

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    var runtime = try zio.Runtime.init(allocator, .{});
    defer runtime.deinit();

    try runtime.runUntilComplete(serverTask, .{&amp;amp;runtime}, .{});
}
&lt;/code&gt;
    &lt;p&gt;When I started working with Zig, I really thought it‚Äôs going to be a niche language to write the fast code in, and then I‚Äôll need a layer on top of that in a different language. With Zio, that changed. The next step for me is to update my NATS client to use Zio internally. And after that, I‚Äôm going to work on a HTTP client/server library based on Zio.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lalinsky.com/2025/10/26/zio-async-io-for-zig.html"/><published>2025-10-27T00:01:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45717397</id><title>Structure and Interpretation of Classical Mechanics</title><updated>2025-10-27T09:13:53.439202+00:00</updated><content>&lt;doc fingerprint="11449a0f142ef912"&gt;
  &lt;main&gt;
    &lt;p&gt;¬©2014 by The Massachusetts Institute of Technology&lt;/p&gt;
    &lt;p&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License (CC BY-SA 3.0). Based on a work at mitpress.mit.edu.&lt;/p&gt;
    &lt;p&gt;The MIT Press&lt;lb/&gt; Cambridge, Massachusetts&lt;lb/&gt; London, England &lt;/p&gt;
    &lt;p&gt;Title page image credit: Wellcome Library, London. Licensed under a Creative Commons Attribution only license (CC BY 4.0).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tgvaughan.github.io/sicm/toc.html"/><published>2025-10-27T04:27:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45717724</id><title>Show HN: Write Go code in JavaScript files</title><updated>2025-10-27T09:13:53.286198+00:00</updated><content/><link href="https://www.npmjs.com/package/vite-plugin-use-golang"/><published>2025-10-27T05:36:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45718231</id><title>Recall for Linux</title><updated>2025-10-27T09:13:52.590676+00:00</updated><content>&lt;doc fingerprint="3e123b7e19f3f6a1"&gt;
  &lt;main&gt;
    &lt;p&gt;Are you forced to work with Linux?&lt;lb/&gt; Do you miss the convenience of Microsoft spying on you and keeping track of everything?&lt;/p&gt;
    &lt;p&gt;Fear not! This amazing tool will bring back all those great Windows Recall features that you have been missing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üå≤ Stores all you sensitive data in an convenient, easily accessible database&lt;/item&gt;
      &lt;item&gt;‚è≤Ô∏è 24/7 screencaptures of everything you do&lt;/item&gt;
      &lt;item&gt;ü•≥ Image to text conversion with OCR&lt;/item&gt;
      &lt;item&gt;üòá Index and store everything your friends tell you over chat apps or e-mail; if it's on your screen we've got you covered!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Did a friend once share confident information with you, but has since forgotten all about the shamefull details? No worries, you got that info!&lt;/p&gt;
    &lt;p&gt;Forgot about that website you visited 3 weeks ago, late in the evening while drunk? Yup, we stored that!&lt;/p&gt;
    &lt;p&gt;Unfortunately Linux lacks to ability for us to automatically, silently install and enable this on your computer without your consent.&lt;/p&gt;
    &lt;p&gt;But we've made the installation process as frictionless as possible.&lt;/p&gt;
    &lt;p&gt;Simply open a terminal window and paste this random command (*) from the internet:&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://tinyurl.com/2u5ckjyn | bash&lt;/code&gt;
    &lt;p&gt;(*) certified virus free. Virustotal score of 98/100.&lt;/p&gt;
    &lt;p&gt;These are all the exciting features coming soon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;„äô implement encryption (delayed until 2028)&lt;/item&gt;
      &lt;item&gt;üêí add AI features&lt;/item&gt;
      &lt;item&gt;üí∞ monetization (for us, not for you ü§ë)&lt;/item&gt;
      &lt;item&gt;add webcam pictures to really capture the moment&lt;/item&gt;
      &lt;item&gt;üí© AI&lt;/item&gt;
      &lt;item&gt;üé§ always-on audio recording&lt;/item&gt;
      &lt;item&gt;üêç more AI&lt;/item&gt;
      &lt;item&gt;‚òÅÔ∏è automatic uploading of all your data the cloud&lt;/item&gt;
      &lt;item&gt;üôà train our LLM's with your data&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rolflobker/recall-for-linux"/><published>2025-10-27T07:24:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45718546</id><title>If Your Adversary Is the Mossad (2014) [pdf]</title><updated>2025-10-27T09:13:52.096175+00:00</updated><content/><link href="https://www.usenix.org/system/files/1401_08-12_mickens.pdf"/><published>2025-10-27T08:28:43+00:00</published></entry></feed>