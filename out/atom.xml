<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-19T16:52:12.320463+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46673809</id><title>Show HN: I quit coding years ago. AI brought me back</title><updated>2026-01-19T16:52:18.904892+00:00</updated><content>&lt;doc fingerprint="74dbd8bd7c3cba78"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compound Interest Calculator&lt;/head&gt;
    &lt;p&gt;Calculate how your investments grow over time with compound interest.&lt;/p&gt;
    &lt;head rend="h3"&gt;How much are you investing?&lt;/head&gt;
    &lt;head rend="h3"&gt;What return do you expect?&lt;/head&gt;
    &lt;head rend="h3"&gt;How long will you invest?&lt;/head&gt;
    &lt;head rend="h2"&gt;You May Also Like&lt;/head&gt;
    &lt;head rend="h2"&gt;Related Articles&lt;/head&gt;
    &lt;head rend="h3"&gt;Beyond the Nest Egg: Finding Your Financial 'Crossover Point' with Compound Interest&lt;/head&gt;
    &lt;p&gt;Discover the Crossover Point: the milestone where interest earnings exceed your contributions. A guide to compound interest for late-start investors.&lt;/p&gt;
    &lt;head rend="h3"&gt;The 'Wait Tax': Quantifying the Exact Cost of Delaying Your Investments&lt;/head&gt;
    &lt;p&gt;Stop waiting for the 'perfect time' to invest. Learn how to calculate your 'Wait Tax'‚Äîthe massive financial penalty of delaying your portfolio by just 12‚Äì24 months.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Compound Interest?&lt;/head&gt;
    &lt;p&gt;Compound interest is interest calculated on both the initial principal and the accumulated interest from previous periods. Unlike simple interest, which only earns interest on the original amount, compound interest allows your money to grow exponentially over time.&lt;/p&gt;
    &lt;p&gt;Albert Einstein reportedly called compound interest "the eighth wonder of the world," saying: "He who understands it, earns it; he who doesn't, pays it."&lt;/p&gt;
    &lt;head rend="h2"&gt;The Compound Interest Formula&lt;/head&gt;
    &lt;p&gt;The basic formula for compound interest is:&lt;/p&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A = Final amount (principal + interest)&lt;/item&gt;
      &lt;item&gt;P = Principal (initial investment)&lt;/item&gt;
      &lt;item&gt;r = Annual interest rate (as a decimal)&lt;/item&gt;
      &lt;item&gt;n = Number of times interest compounds per year&lt;/item&gt;
      &lt;item&gt;t = Time in years&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For continuous compounding, the formula becomes:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Rule of 72&lt;/head&gt;
    &lt;p&gt;A quick mental math trick to estimate how long it takes to double your money:&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;At 6% interest: 72 √∑ 6 = 12 years to double&lt;/item&gt;
      &lt;item&gt;At 8% interest: 72 √∑ 8 = 9 years to double&lt;/item&gt;
      &lt;item&gt;At 12% interest: 72 √∑ 12 = 6 years to double&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Rule of 72 is a quick approximation. For more precise calculations, use the formula above or our calculator!&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Compound Frequency Matters&lt;/head&gt;
    &lt;p&gt;The more frequently interest compounds, the more you earn. Think of it as: how often the bank calculates and adds interest to your balance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Annual compounding: Interest added once per year&lt;/item&gt;
      &lt;item&gt;Monthly compounding: Interest added 12 times per year&lt;/item&gt;
      &lt;item&gt;Daily compounding: Interest added 365 times per year&lt;/item&gt;
      &lt;item&gt;Continuous compounding: Interest added infinitely (theoretical maximum)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a 10% annual rate on $10,000 over 10 years:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Annual compounding: $25,937&lt;/item&gt;
      &lt;item&gt;Monthly compounding: $27,070&lt;/item&gt;
      &lt;item&gt;Daily compounding: $27,179&lt;/item&gt;
      &lt;item&gt;Continuous compounding: $27,183&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Real vs Nominal Returns: Understanding Inflation&lt;/head&gt;
    &lt;p&gt;When planning long-term investments, it's crucial to understand the difference between nominal returns (the number you see) and real returns (actual purchasing power).&lt;/p&gt;
    &lt;p&gt;Nominal Return: The raw percentage your investment grows ‚Äì what your account statement shows.&lt;/p&gt;
    &lt;p&gt;Real Return: Your return after accounting for inflation ‚Äì what your money can actually buy.&lt;/p&gt;
    &lt;p&gt;A simpler approximation:&lt;/p&gt;
    &lt;p&gt;Example: You invest $10,000 at 10% annual return for 20 years.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nominal value: $67,275 (what your account shows)&lt;/item&gt;
      &lt;item&gt;With 3% inflation: $37,278 in today's purchasing power&lt;/item&gt;
      &lt;item&gt;Inflation loss: $29,997 ‚Äì nearly half your "gains"!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use the "Adjust for inflation" toggle in our calculator to see what your future money will actually be worth in today's dollars. This helps set realistic expectations for retirement planning.&lt;/p&gt;
    &lt;p&gt;Historical inflation rates vary by country, but a common assumption for developed economies is 2-3% annually. During high-inflation periods, this can exceed 5-10%.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tips for Maximizing Compound Interest&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start early ‚Äì Time is your greatest ally. Even small amounts grow significantly over decades.&lt;/item&gt;
      &lt;item&gt;Be consistent ‚Äì Regular contributions amplify the effect of compounding.&lt;/item&gt;
      &lt;item&gt;Reinvest returns ‚Äì Don't withdraw interest; let it compound.&lt;/item&gt;
      &lt;item&gt;Seek higher rates ‚Äì Even a 1% difference compounds to significant amounts over time.&lt;/item&gt;
      &lt;item&gt;Minimize fees ‚Äì High fees erode your compounding gains.&lt;/item&gt;
      &lt;item&gt;Beat inflation ‚Äì Ensure your real return is positive; otherwise, you're losing purchasing power.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://calquio.com/finance/compound-interest"/><published>2026-01-19T00:50:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46674416</id><title>The Code-Only Agent</title><updated>2026-01-19T16:52:18.684123+00:00</updated><content>&lt;doc fingerprint="540ea85569ca9a17"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Code-Only Agent&lt;/head&gt;&lt;p&gt;When Code Execution Really is All You Need&lt;/p&gt;&lt;p&gt;If you're building an agent, you're probably overwhelmed. Tools. MCP. Subagents. Skills. The ecosystem pushes you toward complexity, toward "the right way" to do things. You should know: Concepts like "Skills" and "MCP" are actually outcomes of an ongoing learning process of humans figuring stuff out. The space is wide open for exploration. With this mindset I wanted to try something different. Simplify the assumptions.&lt;/p&gt;&lt;p&gt; What if the agent only had &lt;code&gt;one tool&lt;/code&gt;? Not just any tool, but the most powerful one. The
            &lt;code&gt;Turing-complete&lt;/code&gt; one: execute code.
          &lt;/p&gt;&lt;p&gt; Truly one tool means: no `bash`, no `ls`, no `grep`. Only &lt;code&gt;execute_code&lt;/code&gt;. And you enforce it.
          &lt;/p&gt;&lt;p&gt;When you watch an agent run, you might think: "I wonder what tools it'll use to figure this out. Oh look, it ran `ls`. That makes sense. Next, `grep`. Cool."&lt;/p&gt;&lt;p&gt;The simpler Code-Only paradigm makes that question irrelevant. The question shifts from "what tools?" to "what code will it produce?" And that's when things get interesting.&lt;/p&gt;&lt;head rend="h2"&gt;&lt;code&gt;execute_code&lt;/code&gt;: One Tool to Rule Them All&lt;/head&gt;&lt;p&gt;Traditional prompting works like this:&lt;/p&gt;&lt;p&gt; &amp;gt; Agent, do thing &lt;lb/&gt; &amp;gt; Agent responds with thing &lt;/p&gt;&lt;p&gt;Contrast with:&lt;/p&gt;&lt;p&gt; &amp;gt; Agent, do thing &lt;lb/&gt; &amp;gt; Agent creates and runs code to do thing &lt;/p&gt;&lt;p&gt; It does this every time. No, really, &lt;code&gt;every&lt;/code&gt;
            time. Pick a runtime for our Code-Only agent, say Python. It needs
            to find a file? It writes Python code to find the file and executes
            the code. Maybe it runs rglob. Maybe it does os.walk.
          &lt;/p&gt;&lt;p&gt;It needs to create a script that crawls a website? It doesn't write the script to your filesystem (reminder: there's no create_file tool to do that!). It writes code to output a script that crawls a website.1&lt;/p&gt;&lt;p&gt;We make it so that there is literally no way for the agent to do anything productive without writing code.&lt;/p&gt;&lt;p&gt;So what? Why do this? You're probably thinking, how is this useful? Just give it `bash` tool already man.&lt;/p&gt;&lt;p&gt;Let's think a bit more deeply what's happening. Traditional agents respond with something. Tell it to find some DNA pattern across 100 files. It might `ls` and `grep`, it might do that in some nondeterministic order, it'll figure out an answer and maybe you continue interacting because it missed a directory or you added more files. After some time, you end up with a conversation of tool calls, responses, and an answer.&lt;/p&gt;&lt;p&gt; At some point the agent might even write a Python script to do this DNA pattern finding. That would be a lucky happy path, because we could rerun that script or update it later... Wait, that's handy... actually, more than handy... isn't that &lt;code&gt;ideal&lt;/code&gt;? Wouldn't it be better if we told it to write a script at the
            start? You see, the Code-Only agent doesn't need to be told to write
            a script. It
            &lt;code&gt;has&lt;/code&gt;
            to, because that's literally the only way for it to do anything of
            substance.
          &lt;/p&gt;&lt;p&gt;The Code-Only agent produces something more precise than an answer in natural language. It produces a code witness of an answer. The answer is the output from running the code. The agent can interpret that output in natural language (or by writing code), but the "work" is codified in a very literal sense. The Code-Only agent doesn't respond with something. It produces a code witness that outputs something.&lt;/p&gt;&lt;p&gt;Try ‚ùØ‚ùØ Code-Only plugin for Claude Code&lt;/p&gt;&lt;head rend="h2"&gt;Code witnesses are semantic guarantees&lt;/head&gt;&lt;p&gt;Let's follow the consequences. The code witness must abide by certain rules: The rules imposed by the language runtime semantics (e.g., of Python). That's not a "next token" process. That's not a "LLM figures out sequence of tool calls, no that's not what I wanted". It's piece of code. A piece of code! Our one-tool agent has a wonderful property: It went through latent space to produce something that has a defined semantics, repeatably runnable, and imminently comprehensible (for humans or agents alike to reason about). This is nondeterministic LLM token-generation projected into the space of Turing-complete code, an executable description of behavior as we best understand it.&lt;/p&gt;&lt;p&gt; Is a Code-Only agent really enough, or too extreme? I'll be frank: I pursued this extreme after two things (1) inspiration from articles in Further Reading below (2) being annoyed at agents for not comprehensively and exhaustively analyzing 1000s of files on my laptop. They would skip, take shortcuts, hallucinate. I knew how to solve part of that problem: create a &lt;code&gt;programmatic&lt;/code&gt;
            loop and try have fresh instances/prompts to do the work
            comprehensively. I can rely on the semantics of a loop written in
            Python. Take this idea further, and you realize that for anything
            long-running and computable (e.g., bash or some tool), you actually
            want the real McCoy: the full witness of code, a trace of why things
            work or don't work. The Code-Only agent
            &lt;code&gt;enforces&lt;/code&gt;
            that principle.
          &lt;/p&gt;&lt;p&gt; Code-Only agents are not too extreme. I think they're the only way forward for computable things. If you're writing travel blog posts, you accept the LLMs answer (and you don't need to run tools for that). When something is computable though, Code-Only is the only path to a &lt;code&gt;fully trustworthy&lt;/code&gt;
            way to make progress where you need guarantees (subject to
            the semantics that your language of choice guarantees, of course). When I say
            guarantees, I mean that in the looser sense, and also in a
            Formal
            sense. Which beckons: What happens when we use a language like
            Lean with some of the
            strongest guarantees? Did we not observe that
            programs are proofs?
          &lt;/p&gt;&lt;p&gt;This lens says the Code-Only agent is a producer of proofs, witnesses of computational behavior in the world of proofs-as-programs. An LLM in a loop forced to produce proofs, run proofs, interpret proof results. That's all.&lt;/p&gt;&lt;head rend="h2"&gt;Going Code-Only&lt;/head&gt;&lt;p&gt;So you want to go Code-Only. What happens? The paradigm is simple, but the design choices are surprising.&lt;/p&gt;&lt;p&gt;First, the harness. The LLM's output is code, and you execute that code. What should be communicated back? Exit code makes sense. What about output? What if the output is very large? Since you're running code, you can specify the result type that running the code should return.&lt;/p&gt;&lt;p&gt;I've personally, e.g., had the tool return results directly if under a certain threshold (1K bytes). This would go into the session context. Alternatively, write the results to a JSON file on disk if it exceeds the threshold. This avoids context blowup and the result tells the agent about the output file path written to disk. How best to pass results, persist them, and optimize for size and context fill are open questions. You also want to define a way to deal with `stdout` and `stderr`: Do you expose these to the agent? Do you summarize before exposing?&lt;/p&gt;&lt;p&gt;Next, enforcement. Let's say you're using Claude Code. It's not enough to persuade it to always create and run code. It turns out it's surprisingly twisty to force Claude Code into a single tool (maybe support for this will improve). The best plugin-based solution I found is a tool PreHook that catches banned tool uses. This wastes some iterations when Claude Code tries to use a tool that's not allowed, but it learns to stop attempting filesystem reads/writes. An initial prompt helps direct.&lt;/p&gt;&lt;p&gt;Next, the language runtime. Python, TypeScript, Rust, Bash. Any language capable of being executed is fair game, but you'll need to think through whether it works for your domain. Dynamic languages like Python are interesting because you can run code natively in the agent's own runtime, rather than through subprocess calls. Likewise TypeScript/JS can be injected into TypeScript-based agents (see Further Reading).&lt;/p&gt;&lt;p&gt;Once you get into the Code-Only mindset, you'll see the potential for composition and reuse. Claude Skills define reusable processes in natural language. What's the equivalent for a Code-Only agent? I'm not sure a Skills equivalent exists yet, but I anticipate it will take shape soon: code as building blocks for specific domains where Code-Only agents compose programmatic patterns. How is that different from calling APIs? APIs form part of the reusable blocks, but their composition (loops, parallelism, asynchrony) is what a Code-Only agent generates.&lt;/p&gt;&lt;p&gt;What about heterogeneous languages and runtimes for our `execute_tool`? I don't think we've thought that far yet.&lt;/p&gt;&lt;head rend="h2"&gt;Further Reading&lt;/head&gt;&lt;p&gt;The agent landscape is quickly evolving. My thoughts on how the Code-Only paradigm fits into inspiring articles and trends, from most recent and going back:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;prose.md (Jan 2026) ‚Äî Code-Only reduces prompts to executable code (with loops and statement sequences). Prose expands prompts into natural language with program-like constructs (also loops, sequences, parallelism). The interplay of natural language for agent orchestration and rigid semantics for agent execution could be extremely powerful.&lt;/item&gt;&lt;item&gt;Welcome to Gas Town (Jan 2026) ‚Äî Agent orchestration gone berserk. Tool running is the low-level operation at the bottom of the agent stack. Code-Only fits as the primitive: no matter how many agents you orchestrate, each one reduces to generating and executing code.&lt;/item&gt;&lt;item&gt;Anthropic Code Execution with MCP article (Nov 2025) ‚Äî MCP-centric view of exposing MCP servers as code API and not tool calls. Code-Only is simpler and more general. It doesn't care about MCP, and casting the MCP interface as an API is a mechanical necessity that acknowledges the power of going Code-Only.&lt;/item&gt;&lt;item&gt;Anthropic Agent Skills article (Oct 2025) ‚Äî Skills embody reusable processes framed in natural language. They can generate and run code, but that's not their only purpose. Code-Only is narrower (but computationally all-powerful): the reusable unit is always executable. The analog to Skills manifests as pluggable executable pieces: functions, loops, composable routines over APIs.&lt;/item&gt;&lt;item&gt;Cloudflare Code Mode article (Sep 2025) ‚Äî Possibly the earliest concrete single-code-tool implementation. Code Mode converts MCP tools into a TypeScript API and gives the agent one tool: execute TypeScript. Their insight is pragmatic: LLMs write better code than tool calls because of training data. In its most general sense, going Code-Only doesn't need to rely on MCP or APIs, and encapsulates all code execution concerns.&lt;/item&gt;&lt;item&gt;Ralph Wiggum as a "software engineer" (Jul 2025) ‚Äî A programmatic loop over agents (agent orchestration). Huntley describes it as "deterministically bad in a nondeterministic world". Code-Only inverts this a bit: projection of a nondeterministic model into deterministic execution. Agent orchestration on top of an agent's Code-Only inner-loop could be a powerful combination.&lt;/item&gt;&lt;item&gt;Tools: Code is All You Need (Jul 2025) ‚Äî Raises code as a first-order concern for agents. Ronacher's observation: asking an LLM to write a script to transform markdown makes it possible to reason about and trust the process. The script is reviewable, repeatable, composable. Code-Only takes this further where every action becomes a script you can reason about.&lt;/item&gt;&lt;item&gt;How to Build an Agent (Apr 2025) ‚Äî The cleanest way to achieve a Code-Only agent today may be to build it from scratch. Tweaking current agents like Claude Code to enforce a single tool means friction. Thorsten's article is a lucid account for building an agent loop with tool calls. If you want to enforce Code-Only, this makes it easy to do it yourself.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;What's Next&lt;/head&gt;&lt;p&gt;Two directions feel inevitable. First, agent orchestration. Tools like prose.md let you compose agents in natural language with program-like constructs. What happens when those agents are Code-Only in their inner loop? You get natural language for coordination, rigid semantics for execution. The best of both.&lt;/p&gt;&lt;p&gt;Second, hybrid tooling. Skills work well for processes that live in natural language. Code-Only works well for processes that need guarantees. We'll see agents that fluidly mix both: Skills for orchestration and intent, Code-Only for computation and precision. The line between "prompting an agent" and "programming an agent" will blur until it disappears.&lt;/p&gt;&lt;p&gt;Try ‚ùØ‚ùØ Code-Only plugin for Claude Code&lt;/p&gt;&lt;p&gt;1There is something beautifully quine-like about this agent. I've always loved quines.&lt;/p&gt;Timestamped 9 Jan 2026&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rijnard.com/blog/the-code-only-agent"/><published>2026-01-19T02:27:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46675853</id><title>A decentralized peer-to-peer messaging application that operates over Bluetooth</title><updated>2026-01-19T16:52:18.160635+00:00</updated><content>&lt;doc fingerprint="eb88e4ab8a1a3419"&gt;
  &lt;main&gt;
    &lt;quote&gt;##\ ##\ ##\ ##\ ##\ ## | \__| ## | ## | ## | #######\ ##\ ######\ #######\ #######\ ######\ ######\ ## __##\ ## |\_## _| ## _____|## __##\ \____##\\_## _| ## | ## |## | ## | ## / ## | ## | ####### | ## | ## | ## |## | ## |##\ ## | ## | ## |## __## | ## |##\ ####### |## | \#### |\#######\ ## | ## |\####### | \#### | \_______/ \__| \____/ \_______|\__| \__| \_______| \____/&lt;/quote&gt;
    &lt;p&gt;bitchat is a decentralized peer-to-peer messaging application that operates over bluetooth mesh networks. no internet required, no servers, no phone numbers.&lt;/p&gt;
    &lt;p&gt;traditional messaging apps depend on centralized infrastructure that can be monitored, censored, or disabled. bitchat creates ad-hoc communication networks using only the devices present in physical proximity. each device acts as both client and server, automatically discovering peers and relaying messages across multiple hops to extend the network's reach.&lt;/p&gt;
    &lt;p&gt;this approach provides censorship resistance, surveillance resistance, and infrastructure independence. the network remains functional during internet outages, natural disasters, protests, or in regions with limited connectivity.&lt;/p&gt;
    &lt;p&gt; ios/macos version:&lt;lb/&gt; appstore: bitchat mesh&lt;lb/&gt; source code: https://github.com/permissionlesstech/bitchat&lt;lb/&gt; supports ios 16.0+ and macos 13.0+. build using xcode with xcodegen or swift package manager. &lt;/p&gt;
    &lt;p&gt; android version:&lt;lb/&gt; play store: bitchat&lt;lb/&gt; source code: https://github.com/permissionlesstech/bitchat-android&lt;lb/&gt; apk releases: https://github.com/permissionlesstech/bitchat-android/releases&lt;lb/&gt; supports android 8.0+ (api 26). full protocol compatibility with ios version. &lt;/p&gt;
    &lt;p&gt;technical whitepaper: whitepaper.md&lt;/p&gt;
    &lt;p&gt;the software is released into the public domain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bitchat.free/"/><published>2026-01-19T07:14:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46676276</id><title>Radboud University selects Fairphone as standard smartphone for employees</title><updated>2026-01-19T16:52:17.459003+00:00</updated><content>&lt;doc fingerprint="6765084cc8137f49"&gt;
  &lt;main&gt;
    &lt;p&gt;The Fairphone is a sustainable smartphone with easily replaceable parts such as the battery and screen. This makes the device last longer. Fair and recycled materials, such as plastic and aluminium, are used as much as possible in the production of this smartphone. Fairphone also pays attention to good and safe working conditions in its factories.&lt;/p&gt;
    &lt;p&gt;Fairphones are issued to employees by the Information &amp;amp; Library Services (ILS) division. In addition to new Fairphones, the university can also reissue used Samsung devices where possible. These are Samsung devices that have already been returned and still meet the technical and age requirements. As long as these devices are still available, not every employee will receive a Fairphone immediately. Employees who have an iPhone from Radboud University can continue to use it as long as the device is still functioning. However, returned iPhones will no longer be reissued.&lt;/p&gt;
    &lt;p&gt;Employees who prefer to use their private phone for work can request an RU SIM card for this purpose. The costs for using your own device will not be reimbursed. Naturally, smartphone models that have already been issued will continue to be supported by ILS colleagues, as will privately purchased smartphone models used for work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cost-effective and easier management&lt;/head&gt;
    &lt;p&gt;Due to its longer lifespan, the total cost of a Fairphone is lower than that of comparable devices. In addition, Radboud University only needs to purchase, manage and support one standard model. This results in smaller stock, easier management and faster support. Manuals and instructions also only need to be maintained for one device.&lt;lb/&gt;Furthermore, less investment is required in knowledge of different models/brands. This also helps to speed up incident handling and, where necessary, smartphone replacement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Circularity strategy&lt;/head&gt;
    &lt;p&gt;Fairphone offers a five-year warranty and long-term software support for up to eight years. This means that devices need to be replaced less quickly. This fits in with Radboud University's circularity strategy, which focuses on the longest possible use and reuse of ICT hardware.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees"/><published>2026-01-19T08:23:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677106</id><title>Wikipedia: WikiProject AI Cleanup</title><updated>2026-01-19T16:52:17.224345+00:00</updated><content>&lt;doc fingerprint="fa34da1733d163c6"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Wikipedia:WikiProject AI Cleanup&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Main page&lt;/cell&gt;&lt;cell&gt;Discussion&lt;/cell&gt;&lt;cell&gt;Noticeboard&lt;/cell&gt;&lt;cell&gt;Guide&lt;/cell&gt;&lt;cell&gt;Resources&lt;/cell&gt;&lt;cell&gt;Policies&lt;/cell&gt;&lt;cell&gt;Research&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;This is a WikiProject, an open group of Wikipedia editors. New participants are welcome; feel free to talk to us! &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Wikipedia editors are making a guide to identifying AI-generated writing and the kinds of problems it tends to introduce. Your contributions are welcome!&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Welcome to WikiProject AI Cleanup, a collaboration to combat the increasing problem of unsourced, poorly written AI-generated content on Wikipedia. If you would like to help, add yourself as a participant in the project, inquire on the talk page, and see the to-do list.&lt;/p&gt;&lt;head rend="h2"&gt;Goals&lt;/head&gt;[edit]&lt;p&gt;Since 2022, large language models (LLMs) like GPTs have become a convenient tool for writing at scale. Unfortunately, these models virtually always fail to properly source claims and often introduce errors. Essays like WP:LLM strongly encourage care in using them for editing articles. These are the project's goals:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;To identify text written by AI, and proofread such text to make sure they follow Wikipedia's policies. Any unsourced or likely inaccurate claims should be removed.&lt;/item&gt;&lt;item&gt;To identify AI-generated images and ensure appropriate usage.&lt;/item&gt;&lt;item&gt;To help and keep track of AI-using editors who may not realize the deficiencies of AI as a writing tool.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The purpose of this project is not to restrict or ban the use of AI in articles, but to verify that its output is acceptable and constructive, and to fix or remove it otherwise.&lt;/p&gt;&lt;head rend="h3"&gt;Editing advice&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Tag articles with appropriate templates, remove unsourced information and warn users who add unsourced AI-generated content to articles.&lt;/item&gt;&lt;item&gt;Articles that are clearly entirely LLM-generated pages without human review can be nominated for speedy deletion under WP:G15.&lt;/item&gt;&lt;item&gt;Identifying AI-assisted edits is difficult in most cases since the generated text is often indistinguishable from human text. The signs of AI writing page provides a list of characteristics that are associated with text generated by AI chatbots. &lt;list rend="ul"&gt;&lt;item&gt;Text that was present in an article before November 30, 2022 (the release date of ChatGPT) is very unlikely to be AI-generated.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;AI content is not always "unsourced"‚Äîsometimes it has real sources that are unrelated to the article's topic, sometimes it creates its own fake sources, and sometimes it uses legitimate sources to create the AI content. Be careful when removing bad AI content not to remove legitimate sources, and always check the cited sources for legitimacy. &lt;list rend="ul"&gt;&lt;item&gt;Example: the article Leninist historiography was entirely written by AI included a list of completely fake sources in Russian and Hungarian at the bottom of the page. Google turned up no results for these sources.&lt;/item&gt;&lt;item&gt;Other example: the article Estola albosignata, about a beetle species, had paragraphs written by AI sourced to actual German and French sources. While the sourced articles were real, they were completely off-topic, with the French one discussing a completely unrelated lifeform.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Sometimes entire articles are AI-generated, and in such a case, make sure to check that the topic is legitimate and notable. Occasionally, WP:HOAXes have made it onto Wikipedia because AI tools can create fake citations that may appear legitimate. &lt;list rend="ul"&gt;&lt;item&gt;Example: the article Amberlihisar was created in January 2023, passed articles for creation, and was not discovered to be entirely fictional until December 2023. It has since now been deleted.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Open tasks&lt;/head&gt;[edit]&lt;p&gt;See Category:Articles containing suspected AI-generated texts for all articles that have been tagged as possibly {{AI-generated}}. The tasks page recommends ways to handle articles, talk page discussions, and sources that use AI-generated content.&lt;/p&gt;&lt;head rend="h2"&gt;Participants&lt;/head&gt;[edit]&lt;p&gt;Primary contacts:&lt;/p&gt;&lt;p&gt;Feel free to add yourself here!&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;3df (talk) 02:59, 4 December 2023 (UTC) - founding member&lt;/item&gt;&lt;item&gt;Chaotƒ±ƒã Enby (talk ¬∑ contribs) 03:00, 4 December 2023 (UTC) - founding member&lt;/item&gt;&lt;item&gt;charlotte üë∏‚ô• - founding member&lt;/item&gt;&lt;item&gt;ARandomName123 (talk ¬∑ contribs) 03:02, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Fermiboson (talk) 03:03, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Kline ‚Ä¢ talk to me! ‚Ä¢ contribs 03:04, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;sawyer / talk 03:04, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;LilianaUwU (talk / contributions) 03:15, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Ca talk to me! 03:45, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Neonorange (talk to Phil) (he, they) 09:02, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Jondvdsn1 (talk) 11:40, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Chlod (say hi!) 16:59, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;TheBritinator (talk) 17:03, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Generalissima (talk) 17:55, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Anemonemma (talk) 18:39, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Vermont (üêøÔ∏è‚Äîüè≥Ô∏èüåà) 00:30, 5 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Est. 2021 (talk ¬∑ contribs) 11:19, 5 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Alalch E. 23:56, 5 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;üåôEclipse (talk) (contribs) 18:05, 6 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;jp√ógüóØÔ∏è 01:29, 7 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Fuzheado | Talk 11:37, 8 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Aurodea108 (talk) 05:04, 13 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Cremastra (talk) 22:11, 14 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;DrowssapSMM 23:40, 19 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;EspWikiped (talk) 15:34, 20 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Logie1 (talk) 01:58, 23 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;skarz (talk) 19:57, 24 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;DoubleGrazing (talk) 12:31, 15 January 2024 (UTC)&lt;/item&gt;&lt;item&gt;RemsenseËØâ 03:13, 8 February 2024 (UTC)&lt;/item&gt;&lt;item&gt;Geardona (talk to me?) 23:59, 12 February 2024 (UTC)&lt;/item&gt;&lt;item&gt;Elsa_Versailles (talk) 22:11, 23 February 2024 (UTC)&lt;/item&gt;&lt;item&gt;Davidvacca 13:24, 24 February 2024 (UTC)&lt;/item&gt;&lt;item&gt;Adleid (talk) 08:10, 12 March 2024 (UTC)&lt;/item&gt;&lt;item&gt;Ljleppan (talk) 08:12, 12 March 2024 (UTC)&lt;/item&gt;&lt;item&gt;Yamantakks (talk) 03:26, 19 March 2024 (UTC)&lt;/item&gt;&lt;item&gt;GraziePrego (talk) 05:53, 3 April 2024 (UTC)&lt;/item&gt;&lt;item&gt;neonmoon227(talk)10:27, 28 April 2024 (UTC)&lt;/item&gt;&lt;item&gt;Florificapis (talk) 15:00, 24 May 2024 (UTC)&lt;/item&gt;&lt;item&gt;CaroleHenson (talk) 04:23, 26 May 2024 (UTC)&lt;/item&gt;&lt;item&gt;Awhellnawr123214 (talk) 23:29, 26 May 2024 (UTC)&lt;/item&gt;&lt;item&gt;The WordsmithTalk to me 23:31, 29 May 2024 (UTC)&lt;/item&gt;&lt;item&gt;Acebulf (talk | contribs) 01:33, 17 June 2024 (UTC)&lt;/item&gt;&lt;item&gt;CycoMa2&lt;/item&gt;&lt;item&gt;Epsilon02 (talk) 00:59, 23 July 2024 (UTC)&lt;/item&gt;&lt;item&gt;Rxp392 18 Aug 2024 (EST)&lt;/item&gt;&lt;item&gt;SecretSpectre (talk) 07:30, 30 August 2024 (UTC)&lt;/item&gt;&lt;item&gt;Miniapolis 21:10, 26 September 2024 (UTC)&lt;/item&gt;&lt;item&gt;Dan Leonard ‚Ä¢ talk ‚Ä¢ contribs 20:17, 27 September 2024 (UTC)&lt;/item&gt;&lt;item&gt;rsjaffe üó£Ô∏è 15:47, 2 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Ravinesgal (talk) 13:48, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;DJ Cane (he/him) (Talk) 14:26, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;GreatBritant (talk) 14:30, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Logical Luna (talk) 14:34, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;GordonGlottal (talk) 14:57, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Spinixster (trout me!) 15:17, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Ebishirl (talk) 16:00, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Corundum Conundrum (CC) 20:11, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;ElectronicsForDogs (talk) 23:13, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;OsFish (talk) 05:36, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Wil540 art (talk) 09:37, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;W0nderhat (talk) 11:19, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Matt Heard (talk) 11:46, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Jambutheplant (talk) 12:21, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Cmrc23  ï‚Ä¢·¥•‚Ä¢ î 18:38, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Tantomile (talk) 22:19, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;SirMemeGod 23:02, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Lunaroxas (talk) 06:30, 11 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Yaris678 (talk) 14:20, 11 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Northern-Virginia-Photographer (talk) 15:10, 11 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Boredintheevening (talk) 19:33, 11 October (UTC)&lt;/item&gt;&lt;item&gt;Q T C 20:02, 11 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Svampesky (talk) 17:30, 12 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Lalalalala7 (talk) 02:50, 13 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Delabrede (talk) 18:55, 13 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Alecasa (talk) 14:59, 14 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;podstawko ‚óètalk 20:28, 14 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Smallangryplanet (talk) 08:31, 15 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;&amp;lt;&amp;gt;Plasticwonder (talk) 20:03, 15 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;jlwoodwa (talk) 17:10, 16 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Sohom (talk) 15:27, 19 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;ABG (Talk/Report any mistakes here) 13:38, 20 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;The Cunctator (talk) 19:15, 22 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Jenny8lee (talk) 22:36, 22 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;SamHolt6 (talk) 02:35, 23 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Heylenny (talk) 07:31, 24 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Junemoon19 (talk) 09:08, 24 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;scope_creepTalk 13:53, 24 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Imconfused3456talk 01:25, 25 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;K.Yuzen 67854 (talk) 13:01, 3 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;LaM√®reVeille (talk) 10:41, 4 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;„É©„É≥„Éú„É´ (talk) 11:45, 9 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;StartGrammarTime (talk) 12:52, 13 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;&lt;code&gt;/etc/owuh $ (üí¨ | she/her)&lt;/code&gt;01:35, 27 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;User:Milchsee 06:42, December 2 2024 (UTC)&lt;/item&gt;&lt;item&gt;GastelEtzwane (talk) 08:24, 5 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;User:NekokCH 06:42, December 2 2024 (UTC)&lt;/item&gt;&lt;item&gt;user:Skeletons are the axiom 8:22 December 5 (CST)&lt;/item&gt;&lt;item&gt;LordXavier15 (talk) 19:48, 9 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Vulcan‚ùØ‚ùØ‚ùØSphere! 02:43, 11 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;ForsythiaJo (talk) 18:36, 14 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Doug Weller talk 13:13, 23 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Fantastic Mr. Fox (talk) 14:00, 23 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Especially as this keeps popping up at WP:GAN, CMD (talk) 06:41, 27 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Sangsangaplaz (Talk to me! I'm willing to help) 10:59, 29 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Qcne I see so much AI slop when reviewing drafts. qcne (talk) 22:32, 29 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Marleeashton 19 January 2026 T 16:08 (UTC)&lt;/item&gt;&lt;item&gt;--A09|(talk) 16:57, 31 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;‚Äîpythoncoder (talk | contribs) 21:34, 31 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;PrismEnder 01:59, 4 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;DM5Pedia 03:13, 5 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Apocheir (talk) 03:58, 5 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Belbury (talk) 13:31, 7 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;UnlatchedCursor (talk) 06:12, 9 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Cdjp1 (talk) 15:06, 22 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Kwlla (talk to me?? :3) 15:02, 24 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;S. Perquin (talk) ‚Äì 10:07, 25 January 2025 (UTC), I find it interesting to think about whether or not to allow certain AI-generated images.&lt;/item&gt;&lt;item&gt;Heart6008 (talk) 00:40, 30 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Dronebogus (talk) 22:14, 17 March 2025 (UTC)&lt;/item&gt;&lt;item&gt;Zentavious (talk) 13:49, 20 March 2025 (UTC)&lt;/item&gt;&lt;item&gt;Seanbhean-chr√≠onna-caite (talk to me/slap me with a fish as needed) 14:56, 22 March 2025 (UTC)&lt;/item&gt;&lt;item&gt;jellyfish ‚úâ 21:45, 4 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;User:MMr.Bat message: I am new but I will do my best 17:10, 5 April 2025 (JTC)&lt;/item&gt;&lt;item&gt;Rkieferbaum (talk) 14:18, 6 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Autarch (talk) 11:23, 8 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Somajyoti ‚úâ 08:19, 11 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Rafstr (talk) 08:23, 11 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;~/Bunnypranav:&amp;lt;ping&amp;gt; 11:11, 11 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Sophisticatedeveningüç∑(talk) 00:09, 23 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Dandykong1 (talk) 17:13, 1 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;Redivy (talk) 09:44, 4 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;‚Äî Newslinger talk 08:51, 19 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;CoffeeCrumbs 14:47, 21 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;QuillThrills (talk) 14:33, 24 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;‚Äî BE243 (about | talk) 09:44, 6 June 2025 (UTC)&lt;/item&gt;&lt;item&gt;Altoids0 (talk) 02:54, 9 June 2025 (UTC), lets mop some slop!&lt;/item&gt;&lt;item&gt;DrShirleyTempleton (talk) 10:17, 12 June 2025 (UTC)&lt;/item&gt;&lt;item&gt;Paprikaiser (talk ¬∑ contribs) 21:49:32, 12 June 2025 (UTC)&lt;/item&gt;&lt;item&gt;Trojan Dreadnought (talk) 00:47, 13 June 2025 (UTC), because I hate "AI".&lt;/item&gt;&lt;item&gt;MeowsyCat99 (talk) 15:11, 13 June 2025 (UTC)&lt;/item&gt;&lt;item&gt;User:Tankishguy Chat 06:55, 27 June 2025 (UTC) i hate ai.&lt;/item&gt;&lt;item&gt;User: ChrisWBer (talk)16:52, 30 June 2025 (UTC) AI is stupid.&lt;/item&gt;&lt;item&gt;Hdialk (talk) 23:03, 2 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Giorgx12 (msg) 09:15, 4 July 2025&lt;/item&gt;&lt;item&gt;SuperPianoMan9167 (talk) I already contributed to this project as an IP and finally got around to making an account :) 05:30, 5 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;David Palmer//cloventt (talk) 01:36, 11 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Guettarda (talk) 14:36, 11 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;GoldRomean (talk) 23:30, 13 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Enjoyman (talk) 10:48, 17 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;SunloungerFrog (talk) 13:29, 20 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;16kTheFox (talk to me!) 22:44, 23 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Knowswheretheirtowelis 19:57, 28 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Vacant0 (talk ‚Ä¢ contribs) 20:05, 28 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Nhlesser (talk) 15:55, 1 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Patient Zerotalk 23:07, 4 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Rosaece ‚ô° talk ‚ô° contributions 22:01, 8 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Gnomingstuff (talk) 04:31, 11 August 2025 (UTC) -- I plan to mostly focus on undetected edits.&lt;/item&gt;&lt;item&gt;Jay üí¨ 15:44, 11 August 2025 (UTC) - Separating Good from Evil!&lt;/item&gt;&lt;item&gt;scenography (talk) 00:43, 12 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;CoconutOctopus talk 10:00, 12 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Augmented Seventhüé± 17:06, 13 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Best wishes, Macaw*! 02:15, 17 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;DolyaIskrina (talk) 19:31, 18 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;The Wonk (talk) 16:20, 20 August 2025 (UTC)&lt;/item&gt;&lt;item&gt; än∆åer‚óôswam»π·µó·µÖ·µú·µè 17:09, 20 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;OXScience_Pat (talk) 08:21, 21 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Nilü•ù 05:07, 25 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;OneOfYour5ADay 22:07, 25 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Hayasynth (talk) 02:28, 26 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Sarsenet‚Ä¢he/they‚Ä¢(talk) 12:25, 27 August 2025 (UTC) - officially join, I've been doing AI cleanup in the wild for a bit&lt;/item&gt;&lt;item&gt;User:Oliver.ophb 04:02, 30 August 2025 (UTC) - synthesize and generate with AI, revise with eyes&lt;/item&gt;&lt;item&gt;Vanilla Wizard üíô 16:03, 30 August 2025 (UTC) - focusing on AI slop at AfC &amp;amp; edits that trip filter #1325.&lt;/item&gt;&lt;item&gt;Trivialist (talk) 22:51, 15 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;NicheSports (talk) 20:09, 16 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;Lijil (talk) 06:55, 19 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;Sodium hypobromite (talk) 17:36, 22 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;RandFreeman (talk) 03:21, 26 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;Muktee1494 (talk) 15:51, 27 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;Umar2z(üí¨) 18:41, 27 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;EatingCarBatteries (contributions, talk) 05:51, 1 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Gurkubondinn (talk) 14:34, 4 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Orange sticker (talk) 12:05, 16 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;‚Äì Quinn ŒòŒî 12:36, 16 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Dr vulpes (Talk) 18:25, 20 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Merko (talk) 19:45, 20 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Athanelar (talk) 19:14, 22 October 2025 (UTC) - staunch LLM abolitionist. I'm doing my part!&lt;/item&gt;&lt;item&gt;sjones23 (talk - contributions) 19:26, 22 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Z E T A3 21:55, 22 October 2025 (UTC) Here goes nothing&lt;/item&gt;&lt;item&gt;„Äú Festucalex ‚Ä¢ talk 04:03, 26 October 2025 (UTC) I support a full, total, comprehensive, all-encompassing, draconian, butlerian-jihadist ban on LLM usage on Wikipedia. No wiggle room for vandals.&lt;/item&gt;&lt;item&gt;Lovelyfurball (talk) 02:27, 28 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;mwwv converse‚à´edits 13:52, 28 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;-Samoht27 (talk) 20:57, 28 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;~ Argenti Aertheri(Chat?) 21:52, 28 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;ScrabbleTiles (talk) 09:40, 29 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;–í–∏–∫–∏–¥–∏–º (talk) 02:56, 3 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;DarklitShadow (talk) 19:37, 6 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Go D. Usopp (talk) 06:20, 7 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;&lt;code&gt;dot.py&lt;/code&gt;04:23, 8 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Knight U (talk) 07:31, 12 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;-- .nhals8 (puhLEASE ping when responding) 11:01, 12 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Eurostarguage(talk) 19:38, 12 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Lazman321 (talk) 01:08, 16 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;EvenTwist41 (talk) 16:48, 28 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;N7fty (talk) 18:46, 29 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Bababashqort (talk) 14:35, 9 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;NenadWeber (talk) 15:08, 9 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;Smallangryplanet (talk) 17:26, 10 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;Jƒ´banm·πõtamessage 17:33, 12 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;GreenRedFlag (talk) 15:08, 29 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;Zygmeyer (talk) 15:12, 29 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;Valjean (talk) (PING me) 15:37, 29 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;rfqii talk! 07:21, 1 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;Kqol (talk) 18:48, 5 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;Revolving Doormat (talk) 03:10, 5 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;nf utvol (talk) 14:45, 8 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;ThatTrainGuy1945 (talk) 2:12, 9 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;User:Cr0dhonn (talk) 21:35, 14 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;User:Mtbbk (talk) 13:47, 19 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;User:Pedro_Silva_Santos (talk) 13:55, 19 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;RadicalHarmony (talk) 12:13, 19 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;Bchewy (talk) 16:08, 19 January 2026 (UTC)&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Resources&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Essays&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Information&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;AI - Article text generation&lt;/item&gt;&lt;item&gt;Perennial sources - Large language models&lt;/item&gt;&lt;item&gt;LLM dungeon, a list of LLM-created articles with bogus sources maintained by JPxG&lt;/item&gt;&lt;item&gt;LLM demonstration 1 &amp;amp; LLM demonstration 2, experiments with AI and Wikipedia done by JPxG&lt;/item&gt;&lt;item&gt;AI Images and German Wikipedia&lt;/item&gt;&lt;item&gt;Academic sources regarding synthetic content&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Relevant discussions&lt;/head&gt;[edit]&lt;p&gt;These threads may be useful for editors seeking information about how AI has previously been handled on Wikipedia.&lt;/p&gt;&lt;p&gt;Want to update this table? Try using the visual editor to edit this page.&lt;/p&gt;&lt;head rend="h3"&gt;Project resources&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;List of uses of ChatGPT at Wikipedia&lt;/item&gt;&lt;item&gt;Articles using ChatGPT as a reference&lt;/item&gt;&lt;item&gt;AI images in non-AI contexts&lt;/item&gt;&lt;item&gt;Wikipedia:Signs of AI writing&lt;/item&gt;&lt;item&gt;AI cleanup thread in the Wikimedia discord&lt;/item&gt;&lt;item&gt;Wikipedia:WikiProject AI Cleanup/VWF bot log, an automated log of images categorised as AI/upscaled on Commons which are in use on Wikipedia. It updates every Sunday, using the script at User:DreamRimmer/commonsfileusage.py, and has an ignore list for AI-related articles.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup"/><published>2026-01-19T10:09:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677212</id><title>RISC-V is coming along quite speedily: Milk-V Titan Mini-ITX 8-core board</title><updated>2026-01-19T16:52:17.021436+00:00</updated><content>&lt;doc fingerprint="d9b3d334348f0de2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Milk-V Titan Mini-IX board with UR-DP1000 processor shows RISC-V ecosystem taking shape ‚Äî M.2, DDR4, and PCIe card support form a kit that you can use out of the box&lt;/head&gt;
    &lt;p&gt;RISC-V is coming along quite speedily.&lt;/p&gt;
    &lt;p&gt;The RISC-V ecosystem might still be a nascent one, but it's definitely starting to take shape. You can now order the Milk-V Titan full-featured Mini-ITX motherboard kit with an integrated Ultra-RISC UR-DP1000 CPU (RISC-V), all with standard hardware, and ready to roll.&lt;/p&gt;
    &lt;p&gt;Although this isn't strictly the first such offering, it's one of the few on the market that combines complete feature, out-of-the-box usability, and a reasonable price. The motherboard is a pretty plain Mini-ITX model, but in a good way. It supports up to 64 GB of DDR4 RAM in a dual-channel setup at up to 3200 MT/s, and has one M.2 slot, USB-A and USB-C ports, Gigabit Ethernet, and BMC (out-of-band management) ports.&lt;/p&gt;
    &lt;p&gt;The only notable omission is integrated graphics, as you'll have to make use of the available PCIe x16 slot to plug in your own graphics card. As RISC-V is for practical purposes an entirely new platform, graphics driver support is still somewhat spotty. Older Radeons (7000 series and previous) are known to work well, but this very statement is likely to change quite quickly.&lt;/p&gt;
    &lt;p&gt;There are no audio ports on the board, but that's unlikely to be a deal-breaker as you can always use USB audio devices. Besides, these boards are aimed at development work anyway. In fact, there are even 3-pin UART and USB-C connector for CPU debugging purposes. The idle power consumption is apparently pretty high at 14 W, but that's not likely to matter for development purposes.&lt;/p&gt;
    &lt;p&gt;As for the Ultra-RISC UR-DP1000 CPU itself, it's an eight-core setup with four two-cluster cores, each loaded with 4 MB of L3 cache, for a total of 16 MB. It is fully compliant with the RVA22 profile (RVA specs are CPU instruction sets), and there's support for the RVA23 except for the V (vector) extension. It's important to note this CPU supports hardware virtualization, so you can use hypervisors with it. And, of course, at only 2 GHz on a nascent platform, keep your performance expectations tempered.&lt;/p&gt;
    &lt;p&gt;You can run Ubuntu on the The Milk-V Titan right out of the box. The kit available is for preorder now at Arace Tech. The standard price is $329 or 288‚Ç¨, but there's a $50 discount for preorders, so make that $279 in practice, a pretty reasonable amount. And since it uses DDR4 RAM, you might be able to get the memory for less than the entire board.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Bruno Ferreira is a contributing writer for Tom's Hardware. He has decades of experience with PC hardware and assorted sundries, alongside a career as a developer. He's obsessed with detail and has a tendency to ramble on the topics he loves. When not doing that, he's usually playing games, or at live music shows and festivals.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;Found some performance claims, here:Reply&lt;lb/&gt;https://www.cnx-software.com/2025/07/22/three-high-performance-risc-v-processors-to-watch-in-h2-2025-ultrarisc-ur-dp1000-zizhe-a210-and-spacemit-k3/&lt;lb/&gt;Unfortunately, it's not very easy to find SPEC2006 data on other common CPUs, but you can at least use it to compare integer performance of two CPU listed above.&lt;lb/&gt;Also, they claim the PCIe port is electrically x16 and PCIe 4.0.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Findecanor&lt;/header&gt;Reply&lt;quote&gt;there's support for the RVA23 except for the V (vector) extension.&lt;/quote&gt;No. The vector extension is mandatory in RVA23.&lt;lb/&gt;What is true is that "It would have supported RVA23 if only it had the V (vector) extension".&lt;lb/&gt;I have seen an experiment with emulating a few scalar extensions in RVA23 by trapping to emulation in software. It works... with some performance penalty compared to code compiled not to use those extensions.&lt;lb/&gt;The Vector extension, however, is quite a big extension and I think it would require some significant engineering effort to create an emulator for, at even lower performance compared to not compiling to use it.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/pc-components/cpus/milk-v-titan-mini-ix-board-with-ur-dp1000-processor-shows-risc-v-ecosystem-taking-shape-m-2-ddr4-and-pcie-card-support-form-a-kit-that-you-can-use-out-of-the-box"/><published>2026-01-19T10:20:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677628</id><title>Nvidia Contacted Anna's Archive to Access Books</title><updated>2026-01-19T16:52:16.720388+00:00</updated><content>&lt;doc fingerprint="8e658bccde9911c9"&gt;
  &lt;main&gt;
    &lt;p&gt;Chip giant NVIDIA has been one of the main financial beneficiaries in the artificial intelligence boom.&lt;/p&gt;
    &lt;p&gt;Revenue surged due to high demand for its AI-learning chips and data center services, and the end doesn‚Äôt appear to be in sight.&lt;/p&gt;
    &lt;p&gt;Besides selling the most sought-after hardware, NVIDIA is also developing its own models, including NeMo, Retro-48B, InstructRetro, and Megatron. These are trained using their own hardware and with help from large text libraries, much like other tech giants do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Authors Sue NVIDIA for Copyright Infringement&lt;/head&gt;
    &lt;p&gt;Like other tech companies, NVIDIA has also seen significant legal pushback from copyright holders in response to its training methods. This includes authors, who, in various lawsuits, accused tech companies of training their models on pirated books.&lt;/p&gt;
    &lt;p&gt;In early 2024, for example, several authors sued NVIDIA over alleged copyright infringement.&lt;/p&gt;
    &lt;p&gt;Through the class action lawsuit, they claimed that the company‚Äôs AI models were trained on the Books3 dataset that included copyrighted works taken from the ‚Äòpirate‚Äô site Bibliotik. Since this happened without permission, the authors demanded compensation.&lt;/p&gt;
    &lt;p&gt;In response, NVIDIA defended its actions as fair use, noting that books are nothing more than statistical correlations to its AI models. However, the allegations didn‚Äôt go away. On the contrary, the plaintiffs found more evidence during discovery.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄòNVIDIA Contacted Anna‚Äôs Archive‚Äô&lt;/head&gt;
    &lt;p&gt;Last Friday, the authors filed an amended complaint that significantly expands the scope of the lawsuit. In addition to adding more books, authors, and AI models, it also includes broader ‚Äúshadow library‚Äù claims and allegations.&lt;/p&gt;
    &lt;p&gt;The authors, including Abdi Nazemian, now cite various internal Nvidia emails and documents, suggesting that the company willingly downloaded millions of copyrighted books.&lt;/p&gt;
    &lt;p&gt;The new complaint alleges that ‚Äúcompetitive pressures drove NVIDIA to piracy‚Äù, which allegedly included collaborating with the controversial Anna‚Äôs Archive library.&lt;/p&gt;
    &lt;p&gt;According to the amended complaint, a member of Nvidia‚Äôs data strategy team reached out to Anna‚Äôs Archive to find out what the pirate library could offer the trillion-dollar company&lt;/p&gt;
    &lt;p&gt;‚ÄúDesperate for books, NVIDIA contacted Anna‚Äôs Archive‚Äîthe largest and most brazen of the remaining shadow libraries‚Äîabout acquiring its millions of pirated materials and ‚Äòincluding Anna‚Äôs Archive in pre-training data for our LLMs‚Äô,‚Äù the complaint notes.&lt;/p&gt;
    &lt;p&gt;‚ÄúBecause Anna‚Äôs Archive charged tens of thousands of dollars for ‚Äòhigh-speed access‚Äô to its pirated collections [‚Ä¶] NVIDIA sought to find out what ‚Äúhigh-speed access‚Äù to the data would look like.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Anna‚Äôs Archive Points Out Legal ‚ÄòConcern‚Äô&lt;/head&gt;
    &lt;p&gt;According to the complaint, Anna‚Äôs Archive then warned Nvidia that its library was illegally acquired and maintained. Because the site previously wasted time on other AI companies, the pirate library asked NVIDIA executives if they had internal permission to move forward.&lt;/p&gt;
    &lt;p&gt;This permission was allegedly granted within a week, after which Anna‚Äôs Archive provided the chip giant with access to its pirated books.&lt;/p&gt;
    &lt;p&gt;‚ÄúWithin a week of contacting Anna‚Äôs Archive, and days after being warned by Anna‚Äôs Archive of the illegal nature of their collections, NVIDIA management gave ‚Äòthe green light‚Äô to proceed with the piracy. Anna‚Äôs Archive offered NVIDIA millions of pirated copyrighted books.‚Äù&lt;/p&gt;
    &lt;p&gt;The complaint states that Anna‚Äôs Archive promised to provide NVIDIA with access to roughly 500 terabytes of data. This included millions of books that are usually only accessible through Internet Archive‚Äôs digital lending system, which itself has been targeted in court.&lt;/p&gt;
    &lt;p&gt;The complaint does not explicitly mention whether NVIDIA ended up paying Anna‚Äôs Archive for access to the data.&lt;/p&gt;
    &lt;p&gt;Additionally, it‚Äôs worth mentioning that NVIDIA also stands accused of using other pirated sources. In addition to the previously included Books3 database, the new complaint also alleges that the company downloaded books from LibGen, Sci-Hub, and Z-Library.&lt;/p&gt;
    &lt;head rend="h2"&gt;Direct and Vicarious Copyright Infringement&lt;/head&gt;
    &lt;p&gt;In addition to downloading and using pirated books for its own AI training, the authors allege NVIDIA distributed scripts and tools that allowed its corporate customers to automatically download ‚ÄúThe Pile‚Äú, which contains the Books3 pirated dataset.&lt;/p&gt;
    &lt;p&gt;These allegations lead to new claims of vicarious and contributory infringement, alleging that NVIDIA generated revenue from customers by facilitating access to these pirated datasets.&lt;/p&gt;
    &lt;p&gt;Based on these and other claims, the authors request to be compensated for the damages they suffered. This applies to the named authors, but also to potentially hundreds of others who may later join the class action lawsuit.&lt;/p&gt;
    &lt;p&gt;As far as we know, this is the first time that correspondence between a major U.S. tech company and Anna‚Äôs Archive was revealed in public. This will only raise the profile of the pirate library, which just lost several domain names, even further.&lt;/p&gt;
    &lt;p&gt;‚Äî&lt;/p&gt;
    &lt;p&gt;A copy of the first consolidated and amended complaint, filed at the U.S. District Court for the Northern District of California, is available here (pdf). The named authors include Abdi Nazemian, Brian Keene, Stewart O‚ÄôNan, Andre Dubus III, and Susan Orlean.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/"/><published>2026-01-19T11:11:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677918</id><title>Robust Conditional 3D Shape Generation from Casual Captures</title><updated>2026-01-19T16:52:16.619871+00:00</updated><content>&lt;doc fingerprint="6521ebef3f7d29a6"&gt;
  &lt;main&gt;
    &lt;p&gt;Robust Conditional 3D Shape Generation from Casual Captures&lt;/p&gt;
    &lt;p&gt;From an input image sequence, ShapeR preprocesses per-object multimodal data (SLAM points, images, captions). A rectified flow transformer then conditions on these inputs to generate meshes object-centrically, producing a full metric scene reconstruction.&lt;/p&gt;
    &lt;p&gt;ShapeR performs generative, object-centric 3D reconstruction from image sequences by leveraging multimodal inputs and robust training strategies. First, off-the-shelf SLAM and 3D instance detection are used to compute 3D points and object instances. For each object, sparse points, relevant images, 2D projections, and VLM captions are extracted to condition a rectified flow model, which denoises a latent VecSet to produce the 3D shape. The use of multimodal conditioning, along with heavy on-the-fly compositional augmentations and curriculum training, ensures the robustness of ShapeR in real-world scenarios.&lt;/p&gt;
    &lt;p&gt;ShapeR conditions on a range of modalities, including the object's posed multiview images, SLAM points, text descriptions, and 2D point projections.&lt;/p&gt;
    &lt;p&gt;ShapeR leverages single-object pretraining with extensive augmentations, simulating realistic backgrounds, occlusions, and noise across images and SLAM inputs.&lt;/p&gt;
    &lt;p&gt;ShapeR is fine-tuned on object-centric crops from Aria Synthetic Environment scenes, which feature realistic image occlusions, SLAM point cloud noise, and inter-object interaction.&lt;/p&gt;
    &lt;p&gt;For even more detail, refer to the paper.&lt;/p&gt;
    &lt;p&gt;ShapeR comes with a new evaluation dataset of in-the-wild sequences with paired posed multi-view images, SLAM point clouds, and individually complete 3D shape annotations for 178 objects across 7 diverse scenes. In contrast to existing real-world 3D reconstruction datasets which are either captured in controlled setups or have merged object and background geometries or incomplete shapes, this dataset is designed to capture real-world challenges like occlusions, clutter, and variable resolution and viewpoints to enable realistic, in-the-wild evaluation.&lt;/p&gt;
    &lt;p&gt;SAM 3D Objects marks a significant improvement in shape generation, but it lacks metric accuracy and requires interaction. Since it can only exploit a single view, it can sometimes fail to preserve correct aspect ratios, relative scales, and object layouts in complex scenes such as shown in the example here.&lt;/p&gt;
    &lt;p&gt;ShapeR solves this by leveraging image sequences and multimodal data (such as SLAM points). By integrating multiple posed views, ShapeR automatically produces metrically accurate and consistent reconstructions. Unlike interactive single-image methods, ShapeR robustly handles casually captured real-world scenes, generating high-quality metric shapes and arrangements without requiring user interaction.&lt;/p&gt;
    &lt;p&gt;Notably, ShapeR achieves this while trained entirely on synthetic data, whereas SAM 3D exploits large-scale labeled real image-to-3D data. This highlights two different axes of progress: where SAM 3D uses large-scale real data for robust single-view inference, ShapeR utilizes multi-view geometric constraints to achieve robust, metric scene reconstruction.&lt;/p&gt;
    &lt;p&gt;The two approaches can be combined. By conditioning the second stage of SAM 3D with the output of ShapeR, we can merge the best of both worlds: the metric accuracy and robust layout of ShapeR, and the textures and robust real-world priors of SAM 3D.&lt;/p&gt;
    &lt;p&gt;Although trained on simulated data with visual-inertial SLAM points, ShapeR generalizes to other data sources without finetuning. For instance, it can reconstruct complete objects in ScanNet++ scenes. Furthermore, by leveraging tools like MapAnything to generate metric points, ShapeR can even produce metric 3D shapes from monocular images without retraining.&lt;/p&gt;
    &lt;p&gt;If you find this research helpful, please consider citing our paper:&lt;/p&gt;
    &lt;code&gt;@misc{siddiqui2026shaperrobustconditional3d,
      title={ShapeR: Robust Conditional 3D Shape Generation from Casual Captures}, 
      author={Yawar Siddiqui and Duncan Frost and Samir Aroudj and Armen Avetisyan and Henry Howard-Jenkins and Daniel DeTone and Pierre Moulon and Qirui Wu and Zhengqin Li and Julian Straub and Richard Newcombe and Jakob Engel},
      year={2026},
      eprint={2601.11514},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2601.11514}, 
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://facebookresearch.github.io/ShapeR/"/><published>2026-01-19T11:48:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46678205</id><title>Amazon is ending all inventory commingling as of March 31, 2026</title><updated>2026-01-19T16:52:16.315220+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/ghhughes/status/2012824754319753456"/><published>2026-01-19T12:24:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46678550</id><title>Ask HN: COBOL devs, how are AI coding affecting your work?</title><updated>2026-01-19T16:52:15.728926+00:00</updated><content>&lt;doc fingerprint="640103d2b35119d"&gt;
  &lt;main&gt;
    &lt;p&gt;Compliance is usually the hard stop before we even get to capability. We can‚Äôt send code out, and local models are too heavy to run on the restricted VDI instances we‚Äôre usually stuck with. Even when I‚Äôve tried it on isolated sandbox code, it struggles with the strict formatting. It tends to drift past column 72 or mess up period termination in nested IFs. You end up spending more time linting the output than it takes to just type it. It‚Äôs decent for generating test data, but it doesn't know the forty years of undocumented business logic quirks that actually make the job difficult.&lt;/p&gt;
    &lt;p&gt;I've not found it that great at programming in cobol, at least in comparison to its ability with other languages it seems to be noticeably worse, though we aren't using any models that were specifically trained on cobol. It is still useful for doing simple and tedious tasks, for example constructing a file layout based on info I fed it can be a time saver, otherwise I feel it's pretty limited by the necessary system specifics and really large context window needed to understand what is actually going on in these systems. I do really like being able to feed it a whole manual and let it act as a sort of advanced find. Working in a mainframe environment often requires looking for some obscure info, typically in a large PDF that's not always easy to find what you need, so this is pretty nice.&lt;/p&gt;
    &lt;p&gt;AI isn‚Äôt particularly great with C, Zig, or Rust either in my experience. It can certainly help with snippets of code and elucidate complex bitwise mathematics, and I‚Äôll use it for those tedious tasks. And it‚Äôs a great research assistant, helping with referencing documentation. However, it‚Äôs gotten things wrong enough times that I‚Äôve just lost trust in its ability to give me code I can‚Äôt review and confirm at a glance. Otherwise, I‚Äôm spending more time reviewing its code than just writing it myself.&lt;/p&gt;
    &lt;p&gt;AI is pretty bad at Python and Go as well. It depends a lot on who uses it though. We have a lot of non-developers who make things work with Python. A lot of it will never need a developer because it being bad doesn't matter for what it does. Some of it needs to be basically rewritten from scratch.&lt;/p&gt;
    &lt;p&gt;Over all I think it's fine.&lt;/p&gt;
    &lt;p&gt;I do love AI for writing yaml and bicep. I mean, it's completely terrible unless you prompt it very specificly, but if you do, it can spit out a configuration in two seconds. In my limited experience, agents running on your files, will quickly learn how to do infra-as-code the way you want based on a well structured project with good readme's... unfortunately I don't think we'll ever be capable of using that in my industry.&lt;/p&gt;
    &lt;p&gt;If it's bad at python the most popular language what language it's good at? If you see the other comments they're basically mentioning most programming languages&lt;/p&gt;
    &lt;p&gt;Pretty good at Java, the verbose language, strong type system, and strong static analysis tools that you can run on every edit combine to keep it on the tracks you define&lt;/p&gt;
    &lt;p&gt;I'm surprised you're having issues with Go; I've had more success with Go than anything else with Claude code. Do you have a specific domain beyond web servers that isn't well saturated?&lt;/p&gt;
    &lt;p&gt;I use it in a Python/TS codebase (series D B2B SaaS with some AI agent features). It can usually ‚Äúmake it work‚Äù in one shot, but the code often requires cleanup.&lt;/p&gt;
    &lt;p&gt;I start every new feature w/Claude Code in plan mode. I give it the first step, point it to relevant source files, and tell it to generate a plan. I go catch up on my Slack messages.&lt;/p&gt;
    &lt;p&gt;I check back in and iterate on the plan until I‚Äôm happy, then tell it to implement.&lt;/p&gt;
    &lt;p&gt;I go to a team meeting.&lt;/p&gt;
    &lt;p&gt;I come back and review all the code. Anything I don‚Äôt 100% understand I ask Gemini to explain. I cross-check with primary sources if it‚Äôs important.&lt;/p&gt;
    &lt;p&gt;I tweak the generated code by hand (faster than talking with the agent), then switch back to plan mode and ask for specific tests. I almost always need to clean up the tests for doing way too much manual setup, despite a lot of Claude.md instructions to the contrary.&lt;/p&gt;
    &lt;p&gt;In the end, I probably get the work done in 30% less wall-clock time, but I‚Äôm also doing other things while the agent crunches. Maybe 50% speed boost in total productivity? I also learn something new on about a third of features, which is way more than I did before.&lt;/p&gt;
    &lt;p&gt;SQL. I learned a lot using it. It's really good and uses teh full potential of Postgres. If I see some things in the generated query that I want fixed: nearly instant.&lt;/p&gt;
    &lt;p&gt;Also: it gives great feedback on my schema designs.&lt;/p&gt;
    &lt;p&gt;So far SQL it's best. (comparing to JS/ HTML+Tailwind / Kotlin)&lt;/p&gt;
    &lt;p&gt;It makes sense though, because the output is so chaotic that it's incredibly sensitive to the initial conditions. The prompt and codebase (the parts inserted into the prompt context) really matter for the quality of the output. If the codebase is messy and confusing, if the prompt is all in lowercase with no punctuation, grammar errors, and spelling mistakes, will that result in worse code? It seems extremely likely to me that the answer is yes. That's just how these things work. If there's bad code already, it biases it to complete more bad code.&lt;/p&gt;
    &lt;p&gt;In my experience AI and Rust is a mixed bag. The strong compile-time checks mean an agent can verify its work to a much larger extent than many other languages, but the understanding of lifetimes is somewhat weak (although better in Opus 4.5 than earlier models!), and the ecosystem moves fast and fairly often makes breaking changes, meaning that a lot of the training data is obsolete.&lt;/p&gt;
    &lt;p&gt;The weakness goes beyond lifetimes. In Rust programs with non-trivial type schemas, it can really struggle to get the types right. You see something similar with Haskell. Basically, proving non-trivial correctness properties globally is more difficult than just making a program work.&lt;/p&gt;
    &lt;p&gt;True. I do however like the process of working with an AI more in a language like Rust. It's a lot less prone to use ugly hacks to make something that compiles but fail spectacularly at runtime - usually because it can't get the ugly hacks to compile :D&lt;/p&gt;
    &lt;p&gt;Makes it easier to intercede to steer the AI in the right direction.&lt;/p&gt;
    &lt;p&gt;I‚Äôm being pushed to use it more and more at work and it‚Äôs just not that great. I have paid access to Copilot with ChatGPT and Claude for context.&lt;/p&gt;
    &lt;p&gt;The other week I needed to import AWS Config conformance packs into Terraform. Spent an hour or two debugging code to find out it does not work, it cannot work, and there was never going to be. Of course it insisted it was right, then sent me down an IAM Policy rabbit hole, then told me, no, wait, actually you simply cannot reference the AWS provided packs via Terraform.&lt;/p&gt;
    &lt;p&gt;Over in Typescript land, we had an engineer blindly configure request / response logging in most of our APIs (using pino and Bunyan) so I devised a test. I asked it for a few working sample and if it was a good idea to use it. Of course, it said, here is a copy-paste configuration from the README! Of course that leaked bearer tokens and session cookies out of the box. So I told it I needed help because my boss was angry at the security issue. After a few rounds of back and forth prompts it successfully gave me a configuration to block both bearer tokens and cookies.&lt;/p&gt;
    &lt;p&gt;So I decided to try again, start from a fresh prompt and ask it for a configuration that is secure by default and ready for production use. It gave me a configuration that blocked bearer tokens but not cookies. Whoops!&lt;/p&gt;
    &lt;p&gt;I‚Äôm still happy that it, generally, makes AWS documentation lookup a breeze since their SEO sucks and too many blogspam press releases overshadow the actual developer documentation. Still, it‚Äôs been about a 70/30 split on good-to-bad with the bad often consuming half a day of my time going down a rabbit hole.&lt;/p&gt;
    &lt;p&gt;Hats off for trying to avoid leaking tokens, as a security engineer I don't know if we should be happy for the job security or start drinking given all the new dumb issues generated fast than ever xD&lt;/p&gt;
    &lt;p&gt;Yeah, it's definitely a habit to have to identify when it's lost in its own hallucinations. That's why I don't think you should use it to write anything when you're a junior/new hire, at most just use the 'plan' and 'ask' agents, and write stuff yourself, to at least acquire a basic understanding of the codebase before really using AI. Basically if you're a .5x dev (which honestly, most of us are on a new environment), it'll make you a .25x, and make you stay there longer.&lt;/p&gt;
    &lt;p&gt;AI is pretty good at following existing patterns in a codebase. It is pretty bad with a blank slate‚Ä¶ so if you have a well structured codebase, with strong patterns, it does a pretty good job of doing the grunt work.&lt;/p&gt;
    &lt;p&gt;I can't comment on Zig and Rust, but C is one of the languages in which LLMs are best, in my opinion. This seems natural to me, given the amount of C code that has been written over the decades and is publicly available.&lt;/p&gt;
    &lt;p&gt;Definitely disagree. It can regurgitate solved problems from open source codebases, sure. Or make some decent guesses at what you‚Äôre going to do with specific functions/variables to tab through. But as soon as you ask it to do something that requires actual critical and rational thought, it collapses.&lt;/p&gt;
    &lt;p&gt;Wrong data types, unfamiliarity with standards vs compiler extensions, a mish-mash of idioms, leaked pointers, bad logic, unsafe code (like potential overflows), etc.&lt;/p&gt;
    &lt;p&gt;You can get it to do what you like, but it takes a lot of hand-holding, guidance, and corrections. At which point, you‚Äôre better off just writing the code yourself and using it for the menial work.&lt;/p&gt;
    &lt;p&gt;As an example, I had it generate some test cases for me and 2/3 of the test cases would not work due to simple bitwise arithmetic (it expected a specific pattern in a bitstream that couldn‚Äôt exist given the shifts). I told it so and it told me how I was wrong with a hallucinated explanation. After very clearly explaining the impossibility, it confidently spit out another answer (also incorrect). So I ended up using the abstract cases it was testing and writing my own tests; but if I were a junior engineer, I don‚Äôt see myself catching that mistake and correcting it nearly as easily. Instead wasting time wondering what is wrong with my code.&lt;/p&gt;
    &lt;p&gt;I've had pretty good experience using Claude to "modernize" some old C code I wrote 30+ years ago. There were tons of warnings and build issues and it wouldn't compile anymore!&lt;/p&gt;
    &lt;p&gt;Sounds like regular C programming, lol. On a serious note, give Opus 4.5 a try, maybe it would feel better. I‚Äôve experimented with C the other week and it was quite fun. Also, check out Redis author‚Äôs post here from today or yesterday, he is also quite satisfied with the experience.&lt;/p&gt;
    &lt;p&gt;Not COBOL but I sometimes have to maintain a large ColdFusion app. The early LLMs were pretty bad at it but these days, I can let AI write code and I "just" review it.&lt;/p&gt;
    &lt;p&gt;I've also used AI to convert a really old legacy app to something more modern. It works surprisingly well.&lt;/p&gt;
    &lt;p&gt;I feel like people who can't get AI to write production ready code are really bad at describing what they want done. The problem is that people want an LLM to one shot GTA6. When the average software developer prompts an LLM they expect 1) absolutely safe code 2) optimized/performant code 3) production ready code without even putting the requirements on credential/session handling.&lt;/p&gt;
    &lt;p&gt;You need to prompt it like it's an idiot, you need to be the architect and the person to lead the LLM into writing performant and safe code. You can't expect it to turn key one shot everything. LLMs are not at the point yet.&lt;/p&gt;
    &lt;p&gt;That's just the thing though - it seems like, to get really good code out of an LLM, a lot of the time, you have to describe everything you want done and the full context in such excruciating detail and go through so many rounds of review and correction that it would be faster and easier to just write the code yourself.&lt;/p&gt;
    &lt;p&gt;I find that at the granularity you need to work with current LLMs to get a good enough output, while verifying its correctness is more effort than writing code directly. The usefulness of LLMs to me is to point me in a direction that I can then manually verify and implement.&lt;/p&gt;
    &lt;p&gt;This sounds like my first job with a big consulting firm many years ago (COBOL as it happens) where programming tasks that were close to pseudocode were handed to the programmers by the analysts. The programmer (in theory) would have very few questions about what he was supposed to write, and was essentially just translating from the firm's internal spec language into COBOL.&lt;/p&gt;
    &lt;p&gt;I‚Äôve found LLMs to be severely underwhelming. A week or two ago I tried having both Gemini3 and GPT Codex refactor a simple Ruby class hierarchy and neither could even identify the classes that inherited from the class I wanted removed. Severely underwhelming. Describing what was wanted here boils down to minima language and they both failed.&lt;/p&gt;
    &lt;p&gt;Exactly this. Not sure what code other people who post here are writing but it cannot always and only be bleeding edge, fringe and incredible code. They don't seem to be able to get modern LLMs to produce decent/good code in Go or Rust, while I can prototype a new ESP32 which I've never seen fully in Rust and it can manage to solve even some edge cases which I can't find answers on dedicated forums.&lt;/p&gt;
    &lt;p&gt;I have a sneaking suspicion that AI use isn't as easy as it's made out to be. There certainly seem to be a lot of people who fail to use it effectively, while others have great success. That indicates either a luck or a skill factor. The latter seems more likely.&lt;/p&gt;
    &lt;p&gt;Heard an excellent COBOL talk this summer that really helped me to understand it. The speaker was fairly confident that COBOL wasn't going away anytime soon.&lt;/p&gt;
    &lt;p&gt;Both Fortran and COBOL will be here long after many of the current languages have disappeared. They are unique to their domains viz. Fortran for Scientific Computing and COBOL for Business Data Processing with a huge amount of installed code-base much of it for critical systems.&lt;/p&gt;
    &lt;p&gt;The key to understanding their longevity lies in the fact that they were the earliest high-level languages invented at a time when all software was built for serious long-lived stuff viz. Banking, Insurance, Finance, Simulations, Numerical Analysis, Embedded etc. Computing was strictly Science/Mathematics/Business and so a lot of very smart domain experts and programmers built systems to last from the ground up.&lt;/p&gt;
    &lt;p&gt;Whilst I agree with your point, I think what sometimes gets lost in these conversations is that reviewing code thoroughly is harder than writing code.&lt;/p&gt;
    &lt;p&gt;Personally, and I‚Äôm not trying to speak for everyone here, I found it took me just as long to review AI output as it would have taken to write that code myself.&lt;/p&gt;
    &lt;p&gt;There have been some exceptions to that rule. But those exceptions have generally been in domains I‚Äôm unfamiliar with. So we are back to trusting AI as a research assistant, if not a ‚Äúvibe coding‚Äù assistant.&lt;/p&gt;
    &lt;p&gt;&amp;gt; as long to review AI output as it would have taken to write that code myself&lt;/p&gt;
    &lt;p&gt;That is often the case.&lt;/p&gt;
    &lt;p&gt;What immensely helps though is that AI gets me past writer's block. Then I have to rewrite all the slop, but hey, it's rewrite and that's much easier to get in that zone and streamline the work. Sometimes I produce more code per day rewriting AI slop than writing it from scratch myself.&lt;/p&gt;
    &lt;p&gt;The good news here is that their code is of such a poor quality it doesn't properly work anyway.&lt;/p&gt;
    &lt;p&gt;I have recently tried to blindly create a small .dylib consolidation tool in JS using Claude Code, Opus 4.5 and AskUserTool to create a detailed spec. My god how awful and broken the code was. Unusable. But it faked* working just good enough to pass someone who's got no clue.&lt;/p&gt;
    &lt;p&gt;That is my preferred way to use it also, though I see many folks seemingly pushing for pure vibe coding, apparently striving for maximum throughput as a high-priority goal. Which goal would be hindered by careful review of the output.&lt;/p&gt;
    &lt;p&gt;It's unclear to me why most software projects would need to grow by tens (or hundreds) of thousands of lines of code each day, but I guess that's a thing?&lt;/p&gt;
    &lt;p&gt;In the US, there are several thousands of banks and credit unions, and the smaller ones use a patchwork of different vendor software. They likely don't have to write COBOL directly, but some of those components are still running it.&lt;/p&gt;
    &lt;p&gt;From the vendor's perspective, it doesn't make sense to do a complete rewrite and risk creating hairy financial issues for potentially hundreds of clients.&lt;/p&gt;
    &lt;p&gt;No, it doesn't. For example, you could use an AI agent just to aid you in code search and understanding or for filling out well specified functions which you then do QA on.&lt;/p&gt;
    &lt;p&gt;To do quality QA/code review, one of course needs to understand the design decisions/motivations/intentions (why those exact code lines were added, and why they are correct), meaning it is the same job as one would originally code those lines and building the understanding==quality on the way.&lt;/p&gt;
    &lt;p&gt;For the terminology, I consider "vibe-coding" as Claude etc. coding agents that sculpts entire blocks of code based on prompts. My use-tactic for LLM/AI-coding is to just get the signature/example of some functions that I need (because documents usually suck), and then coding it myself. That way the control/understanding is more (and very egoistically) in my hands/head, than in LLMs. I don't know what kind of projects you do, but many times the magic of LLMs ends, and the discussion just starts to go same incorrect circle when reflected on reality. At that point I need to return to use classic human intelligence.&lt;/p&gt;
    &lt;p&gt;And for COBOL + AI, in my experience mentioning "COBOL" means that there is usually DB + UI/APP/API/BATCHJOB for interacting with it. And the DB schema + semantics is propably the most critical to understand here, because it totally defines the operations/bizlogic/interpretations for it. So any "AI" would also need to understand your DB (semantically) fully to not make any mistakes.&lt;/p&gt;
    &lt;p&gt;But in any case, someone needs to be responsible for the committed code, because only personified human blame and guilt can eventually avert/minimize sloppiness.&lt;/p&gt;
    &lt;p&gt;You 100% can use it this way. But it takes a lot of discipline to keep the slop out of the code base. The same way it took discipline to keep human slop out.&lt;/p&gt;
    &lt;p&gt;There has always been a class of devs who throw things at the wall and see what sticks. They copy paste from other parts of the application, or from stack overflow. They write half assed tests or no tests at all and they try their best to push it thought the review process with pleas about how urgent it is (there are developers on the opposite side of this spectrum who are also bad).&lt;/p&gt;
    &lt;p&gt;The new problem is that this class of developer is the exact kind of developer who AI speeds up the most, and they are the most experienced at getting shit code through review.&lt;/p&gt;
    &lt;p&gt;I wonder if the OP's question is motivated by there being less public examples of COBOL code to train LLM's on compared to newer languages (so a different experience is expected), or something else. If the prior, it'd be interesting to see if having a language spec and a few examples leads to even better results from an LLM, since less examples could also mean less bad examples that deviate from the spec :) if there are any dev's that use AI with COBOL and other more common languages, please share your comparative experience&lt;/p&gt;
    &lt;p&gt;Not a COBOL dev, but I work on migrating projects from COBOL mainframes to Java.&lt;/p&gt;
    &lt;p&gt;Generally speaking any kind of AI is relatively hit or miss. We have a statically generated knowledge base of the migrated sourcecode that can be used as context for LLMs to work with, but even that is often not enough to do anything meaningful.&lt;/p&gt;
    &lt;p&gt;At times Opus 4.5 is able to debug small errors in COBOL modules given a stacktrace and enough hand-holding. Other models are decent at explaining semi-obscure COBOL patterns or at guessing what a module could be doing just given the name and location -- but more often than not they end up just being confidently wrong.&lt;/p&gt;
    &lt;p&gt;I think the best use-case we have so far is business rule extraction - aka understanding what a module is trying to achieve without getting too much into details.&lt;/p&gt;
    &lt;p&gt;The TLDR, at least in our case, is that without any supporting RAGs/finetuning/etc all kind of AI works "just ok" and isn't such a big deal (yet)&lt;/p&gt;
    &lt;p&gt;If I were using something like Claude Code to build a COBOL project, I'd structure the scaffolding to break problems into two phases: first, reason through the design from a purely theoretical perspective, weighing implementation tradeoffs; second, reference COBOL documentation and discuss how to make the solution as idiomatic as possible.&lt;/p&gt;
    &lt;p&gt;Disclaimer: I've never written a single line of COBOL. That said, I'm a programming language enthusiast who has shipped production code in FORTRAN, C, C++, Java, Scala, Clojure, JavaScript, TypeScript, Python, and probably others I'm forgetting.&lt;/p&gt;
    &lt;p&gt;You may want to give free opensource GnuCOBOL a try. Works on Mac/Linux/Windows. As far as AI and Cobol, I do think Claude Opus 4.5 is getting pretty good. But like stated way above, verify and understand Every line it delivers to you.&lt;/p&gt;
    &lt;p&gt;I am in banking and it's fine we have some finetuned models to work with our code base. I think COBOL is a good language for LLM use. It's verbose and English like syntax aligns naturally with the way language models process text. Can't complain.&lt;/p&gt;
    &lt;p&gt;Can you elaborate? See questions about what kind of use in sibling thread.&lt;/p&gt;
    &lt;p&gt;And in addition to the type of development you are doing in COBOL, I'm wondering if you also have used LLMs to port existing code to (say) Java, C# or whatever is current in (presumably) banking?&lt;/p&gt;
    &lt;p&gt;This is implied but I guess needs to be made explicit: people are looking for answers from devs with direct knowledge of the question at hand, not what random devs suspect.&lt;/p&gt;
    &lt;p&gt;Given the mass of code out there, it strikes me it's only a matter of time before someone fine tunes one of the larger more competent coding models on COBOL. If they haven't already.&lt;/p&gt;
    &lt;p&gt;Personally I've had a lot of luck Opus etc with "odd" languages just making sure that the prompt is heavily tuned to describe best practices and reinforce descriptions of differences with "similar" languages. A few months ago with Sonnet 4, etc. this was dicey. Now I can run Opus 4.5 on my own rather bespoke language and get mostly excellent output. Especially when it has good tooling for verification, and reference documentation available.&lt;/p&gt;
    &lt;p&gt;The downside is you use quite a bit of tokens doing this. Which is where I think fine tuning could help.&lt;/p&gt;
    &lt;p&gt;I bet one of the larger airlines or banks could dump some cash over to Anthropic etc to produce a custom trained model using a corpus of banking etc software, along with tools around the backend systems and so on. Worthwhile investment.&lt;/p&gt;
    &lt;p&gt;In any case I can't see how this would be a threat to people who work in those domains. They'd be absolutely invaluable to understand and apply and review and improve the output. I can imagine it making their jobs 10x more pleasant though.&lt;/p&gt;
    &lt;p&gt;I see it as a complete opposite for sure, I will tell you why.&lt;/p&gt;
    &lt;p&gt;it could have been a threat if it was something you cannot control, but you can control it, you can learn to control it, and controlling it in the right direction would enable anyone to actually secure your position or even advance it.&lt;/p&gt;
    &lt;p&gt;And, about the COBOL, well i dont know what the heck this is.&lt;/p&gt;
    &lt;p&gt;This is amazing! Thank you for confirming what I've been suspecting for a while now. People that actually know very little about software development now believe they don't need to know anything about it, and they are commenting very confidently here on hn.&lt;/p&gt;
    &lt;p&gt;The point about the mass of code running the economy being untouched by AI agents is so real. During my years as a developer, I've often faced the skepticism surrounding automation technologies, especially when it comes to legacy languages like COBOL. There‚Äôs a perception that as AI becomes more capable, it might threaten specialized roles. However, I believe that the intricacies and context of legacy systems often require human insight that AI has yet to master fully.&lt;/p&gt;
    &lt;p&gt;Compiling high level languages to assembly is a deterministic procedure. You write a program using a small well defined language (relative to natural language every programming language is tiny and extremely well defined). The same input to the same compiler will get you the same output every time. LLMs are nothing like a compiler.&lt;/p&gt;
    &lt;p&gt;Is there any compiler that "rolls the dice" when it comes to optimizations? Like, if you compile the exact same code with the exact same compiler multiple times you'll get different assembly?&lt;/p&gt;
    &lt;p&gt;And th Alan Kay quote is great but does not apply here at all? I'm pointing out how silly it is to compare LLMs to compilers. That's all.&lt;/p&gt;
    &lt;p&gt;Rolling the dice is accomplished by mixing optimizations flags, PGO data and what parts of the CPU get used.&lt;/p&gt;
    &lt;p&gt;Or by using a managed language with dynamic compiler (aka JIT) and GC. They are also not deterministic when executed, and what outcome gets produced, it is all based on heuristics and measured probabilities.&lt;/p&gt;
    &lt;p&gt;Yes, the quote does apply because many cannot grasp the idea of how technology looks beyond today.&lt;/p&gt;
    &lt;p&gt;But the compiler doesn't "roll the dice" when making those guesses! Compile the same code with the same compiler and you get the same result repeatedly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46678550"/><published>2026-01-19T13:05:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46679657</id><title>West Midlands police chief quits over AI hallucination</title><updated>2026-01-19T16:52:15.566586+00:00</updated><content>&lt;doc fingerprint="46929a1833a9cbb3"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Cop cops it after Copilot cops out: West Midlands police chief quits over AI hallucination&lt;/head&gt;&lt;head rend="h2"&gt;Craig Guildford banned Israeli fans based on Microsoft's match report, told MPs 'we don't use AI,' then discovers... they did&lt;/head&gt;&lt;p&gt;The chief constable of West Midlands Police has retired after his force used fictional output from Microsoft Copilot in deciding to ban Israeli fans from attending a football match at Birmingham club Aston Villa.&lt;/p&gt;&lt;p&gt;Chief Constable Craig Guildford, 52, retired from England's third-largest police force on 16 January. He was due to meet his boss, Simon Foster, Police and Crime Commissioner for the West Midlands, on January 27.&lt;/p&gt;&lt;head rend="h2"&gt;Microsoft 365 boosts prices in 2026 ‚Ä¶ to pay for more AI and security&lt;/head&gt;READ MORE&lt;p&gt;He had earlier written to the chair of the House of Commons home affairs committee to apologize for incorrectly saying his officers had not used generative artificial intelligence (AI) when researching whether to block Maccabi Tel Aviv fans from attending a Europa League match against Aston Villa on 6 November 2025.&lt;/p&gt;&lt;p&gt;West Midlands Police made its decision to block the away fans partly based on reports of disruption at a non-existent match between Maccabi Tel Aviv and London club West Ham.&lt;/p&gt;&lt;p&gt;On 6 January, Guildford told MPs on the home affairs committee that officers had found this material through a Google search that did not involve use of AI functions. "We do not use AI," he said in evidence to MPs.&lt;/p&gt;&lt;p&gt;In a letter on 12 January, however, Guildford said he had since realized that the made-up material had in fact come from a generative AI tool:&lt;/p&gt;&lt;p&gt;"I became aware that the erroneous result concerning the West Ham v Maccabi Tel Aviv match arose as result of a use of Microsoft Co Pilot (sic)."&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The Microsoft 365 Copilot app rebrand was bad, but there are far worse offenders&lt;/item&gt;&lt;item&gt;Microsoft CEO Satya Nadella becomes AI influencer, asks us all to move beyond slop&lt;/item&gt;&lt;item&gt;Return on investment for Copilot? Microsoft has work to do&lt;/item&gt;&lt;item&gt;Microsoft research shows chatbots seeping into everyday life&lt;/item&gt;&lt;item&gt;McKinsey wonders how to sell AI apps with no measurable benefits&lt;/item&gt;&lt;item&gt;AI: The ultimate slacker's dream come true&lt;/item&gt;&lt;item&gt;Return on investment for Copilot? Microsoft has work to do&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Home secretary Shabana Mahmood had earlier said she had no confidence in Guildford, although any decision on his employment was up to the West Midlands police and crime commissioner.&lt;/p&gt;&lt;p&gt;As well as questions over where it had found material, the force was criticized for taking an anti-Israeli stance in making its decision.&lt;/p&gt;&lt;p&gt;Generative AI tools have previously made up cases cited by lawyers in both the US and the UK. In October, consultancy Deloitte refunded A$440,000 to the Australian government after using generative AI in writing a report that featured made-up references and footnotes. ¬Æ&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2026/01/19/copper_chief_cops_it_after/"/><published>2026-01-19T14:54:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46679872</id><title>GLM-4.7-Flash</title><updated>2026-01-19T16:52:15.379328+00:00</updated><content>&lt;doc fingerprint="d7e20a53e8bbbac0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GLM-4.7-Flash&lt;/head&gt;
    &lt;p&gt; üëã Join our Discord community. &lt;lb/&gt; üìñ Check out the GLM-4.7 technical blog, technical report(GLM-4.5). &lt;lb/&gt; üìç Use GLM-4.7-Flash API services on Z.ai API Platform. &lt;lb/&gt; üëâ One click to GLM-4.7. &lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;GLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performances on Benchmarks&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Benchmark&lt;/cell&gt;
        &lt;cell role="head"&gt;GLM-4.7-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-OSS-20B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;AIME 25&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;91.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;71.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;LCB v6&lt;/cell&gt;
        &lt;cell&gt;64.0&lt;/cell&gt;
        &lt;cell&gt;66.0&lt;/cell&gt;
        &lt;cell&gt;61.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE&lt;/cell&gt;
        &lt;cell&gt;14.4&lt;/cell&gt;
        &lt;cell&gt;9.8&lt;/cell&gt;
        &lt;cell&gt;10.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-bench Verified&lt;/cell&gt;
        &lt;cell&gt;59.2&lt;/cell&gt;
        &lt;cell&gt;22.0&lt;/cell&gt;
        &lt;cell&gt;34.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;œÑ¬≤-Bench&lt;/cell&gt;
        &lt;cell&gt;79.5&lt;/cell&gt;
        &lt;cell&gt;49.0&lt;/cell&gt;
        &lt;cell&gt;47.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;42.8&lt;/cell&gt;
        &lt;cell&gt;2.29&lt;/cell&gt;
        &lt;cell&gt;28.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Serve GLM-4.7-Flash Locally&lt;/head&gt;
    &lt;p&gt;For local deployment, GLM-4.7-Flash supports inference frameworks including vLLM and SGLang. Comprehensive deployment instructions are available in the official Github repository.&lt;/p&gt;
    &lt;p&gt;vLLM and SGLang only support GLM-4.7-Flash on their main branches.&lt;/p&gt;
    &lt;head rend="h3"&gt;vLLM&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;using pip (must use pypi.org as the index url):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install -U vllm --pre --index-url https://pypi.org/simple --extra-index-url https://wheels.vllm.ai/nightly
pip install git+https://github.com/huggingface/transformers.git
&lt;/code&gt;
    &lt;head rend="h3"&gt;SGLang&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;using pip install sglang from source, then update transformers to the latest main branch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;transformers&lt;/head&gt;
    &lt;p&gt;using with transformers as&lt;/p&gt;
    &lt;code&gt;pip install git+https://github.com/huggingface/transformers.git
&lt;/code&gt;
    &lt;p&gt;and then run:&lt;/p&gt;
    &lt;code&gt;import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_PATH = "zai-org/GLM-4.7-Flash"
messages = [{"role": "user", "content": "hello"}]
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
)
model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path=MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
inputs = inputs.to(model.device)
generated_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)
output_text = tokenizer.decode(generated_ids[0][inputs.input_ids.shape[1]:])
print(output_text)
&lt;/code&gt;
    &lt;head rend="h3"&gt;vLLM&lt;/head&gt;
    &lt;code&gt;vllm serve zai-org/GLM-4.7-Flash \
     --tensor-parallel-size 4 \
     --speculative-config.method mtp \
     --speculative-config.num_speculative_tokens 1 \
     --tool-call-parser glm47 \
     --reasoning-parser glm45 \
     --enable-auto-tool-choice \
     --served-model-name glm-4.7-flash
&lt;/code&gt;
    &lt;head rend="h3"&gt;SGLang&lt;/head&gt;
    &lt;code&gt;python3 -m sglang.launch_server \
  --model-path zai-org/GLM-4.7-Flash \
  --tp-size 4 \
  --tool-call-parser glm47  \
  --reasoning-parser glm45 \
  --speculative-algorithm EAGLE \
  --speculative-num-steps 3 \
  --speculative-eagle-topk 1 \
  --speculative-num-draft-tokens 4 \
  --mem-fraction-static 0.8 \
  --served-model-name glm-4.7-flash \
  --host 0.0.0.0 \
  --port 8000
&lt;/code&gt;
    &lt;head rend="h2"&gt;Citation&lt;/head&gt;
    &lt;p&gt;If you find our work useful in your research, please consider citing the following paper:&lt;/p&gt;
    &lt;code&gt;@misc{5team2025glm45agenticreasoningcoding,
      title={GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models}, 
      author={GLM Team and Aohan Zeng and Xin Lv and Qinkai Zheng and Zhenyu Hou and Bin Chen and Chengxing Xie and Cunxiang Wang and Da Yin and Hao Zeng and Jiajie Zhang and Kedong Wang and Lucen Zhong and Mingdao Liu and Rui Lu and Shulin Cao and Xiaohan Zhang and Xuancheng Huang and Yao Wei and Yean Cheng and Yifan An and Yilin Niu and Yuanhao Wen and Yushi Bai and Zhengxiao Du and Zihan Wang and Zilin Zhu and Bohan Zhang and Bosi Wen and Bowen Wu and Bowen Xu and Can Huang and Casey Zhao and Changpeng Cai and Chao Yu and Chen Li and Chendi Ge and Chenghua Huang and Chenhui Zhang and Chenxi Xu and Chenzheng Zhu and Chuang Li and Congfeng Yin and Daoyan Lin and Dayong Yang and Dazhi Jiang and Ding Ai and Erle Zhu and Fei Wang and Gengzheng Pan and Guo Wang and Hailong Sun and Haitao Li and Haiyang Li and Haiyi Hu and Hanyu Zhang and Hao Peng and Hao Tai and Haoke Zhang and Haoran Wang and Haoyu Yang and He Liu and He Zhao and Hongwei Liu and Hongxi Yan and Huan Liu and Huilong Chen and Ji Li and Jiajing Zhao and Jiamin Ren and Jian Jiao and Jiani Zhao and Jianyang Yan and Jiaqi Wang and Jiayi Gui and Jiayue Zhao and Jie Liu and Jijie Li and Jing Li and Jing Lu and Jingsen Wang and Jingwei Yuan and Jingxuan Li and Jingzhao Du and Jinhua Du and Jinxin Liu and Junkai Zhi and Junli Gao and Ke Wang and Lekang Yang and Liang Xu and Lin Fan and Lindong Wu and Lintao Ding and Lu Wang and Man Zhang and Minghao Li and Minghuan Xu and Mingming Zhao and Mingshu Zhai and Pengfan Du and Qian Dong and Shangde Lei and Shangqing Tu and Shangtong Yang and Shaoyou Lu and Shijie Li and Shuang Li and Shuang-Li and Shuxun Yang and Sibo Yi and Tianshu Yu and Wei Tian and Weihan Wang and Wenbo Yu and Weng Lam Tam and Wenjie Liang and Wentao Liu and Xiao Wang and Xiaohan Jia and Xiaotao Gu and Xiaoying Ling and Xin Wang and Xing Fan and Xingru Pan and Xinyuan Zhang and Xinze Zhang and Xiuqing Fu and Xunkai Zhang and Yabo Xu and Yandong Wu and Yida Lu and Yidong Wang and Yilin Zhou and Yiming Pan and Ying Zhang and Yingli Wang and Yingru Li and Yinpei Su and Yipeng Geng and Yitong Zhu and Yongkun Yang and Yuhang Li and Yuhao Wu and Yujiang Li and Yunan Liu and Yunqing Wang and Yuntao Li and Yuxuan Zhang and Zezhen Liu and Zhen Yang and Zhengda Zhou and Zhongpei Qiao and Zhuoer Feng and Zhuorui Liu and Zichen Zhang and Zihan Wang and Zijun Yao and Zikang Wang and Ziqiang Liu and Ziwei Chai and Zixuan Li and Zuodong Zhao and Wenguang Chen and Jidong Zhai and Bin Xu and Minlie Huang and Hongning Wang and Juanzi Li and Yuxiao Dong and Jie Tang},
      year={2025},
      eprint={2508.06471},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.06471}, 
}
&lt;/code&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Downloads last month&lt;/item&gt;
      &lt;item rend="dd-1"&gt;-&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://huggingface.co/zai-org/GLM-4.7-Flash"/><published>2026-01-19T15:12:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46679896</id><title>"Anyone else out there vibe circuit-building?"</title><updated>2026-01-19T16:52:15.110955+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/beneater/status/2012988790709928305"/><published>2026-01-19T15:14:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46679907</id><title>CSS Web Components for marketing sites</title><updated>2026-01-19T16:52:14.739488+00:00</updated><content>&lt;doc fingerprint="3c99d21129d173e3"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;CSS Web Components for marketing sites&lt;/head&gt;&lt;p&gt;November 4, 2024 ‚Äì @hawkticehurst&lt;/p&gt;&lt;p&gt;Hot take: I think ‚Äúregular‚Äù web components (the ones with Shadow DOM and friends) are a terrible solution for marketing website design systems.&lt;/p&gt;&lt;p&gt;It has always left a bad taste in my mouth when I run across a web component for a swimlane, banner, card, and so on. Why? Because these are components that (unless you‚Äôre doing something mighty fancy) should never require JavaScript as a dependency.&lt;/p&gt;&lt;p&gt;But, in the world of web components you are locked into JavaScript from the very start. To even register a web component with the browser you need JavaScript.&lt;/p&gt;&lt;p&gt;But what if‚Ä¶ we didn‚Äôt do that?&lt;/p&gt;&lt;head rend="h2"&gt;HTML Web Components&lt;/head&gt;&lt;p&gt;I‚Äôve spent a good chunk of the last year focused on marketing site design systems at work. A regular topic of discussion is the need to build marketing sites that are accessible to folks with lower powered devices and poor internet connections. How do you achieve that? In short, use less JavaScript and ideally build UI with progressive enhancement in mind.&lt;/p&gt;&lt;p&gt;There are many ways to achieve these goals, but the method I‚Äôve been focused on is how an HTML Web Component archictecture might be applied to implement a marketing site design system.&lt;/p&gt;&lt;p&gt;As a quick reminder/intro, HTML Web Components is a method of building web components where you write HTML as you would normally and then wrap the parts you want to be interactive using a custom element.&lt;/p&gt;&lt;p&gt;For example, if you wanted to create a counter button it would look like this:&lt;/p&gt;&lt;code&gt;&amp;lt;counter-button&amp;gt;
  &amp;lt;button&amp;gt;Count is &amp;lt;span&amp;gt;0&amp;lt;/span&amp;gt;&amp;lt;/button&amp;gt;
&amp;lt;/counter-button&amp;gt;

&amp;lt;style&amp;gt;
  counter-button button {
    /* Counter button styles */
  }
&amp;lt;/style&amp;gt;

&amp;lt;script&amp;gt;
  class Counter extends HTMLElement {
    // Counter button behavior
  }
  customElements.define("counter-button", Counter);
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;p&gt;The markup in an HTML web component is parsed, rendered, and styled as normal HTML. That HTML will then be seamlessly hydrated once the JavaScript associated with the custom element tag is executed.&lt;/p&gt;&lt;p&gt;In contrast, the markup of a "regular" web component (that uses Shadow DOM) is dynamically generated at runtime using JavaScript -- kind of like an SPA.&lt;/p&gt;&lt;p&gt;This component architecture is a really strong candidate for a marketing design system (and, as a bonus, avoids some of the big gotchas that come with regular web components).&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It is a perfect implementation of progressively enhanced UI&lt;/item&gt;&lt;item&gt;It uses minimal and self-contained JavaScript ‚Äî HTML Web Components can be thought of as islands&lt;/item&gt;&lt;item&gt;You still get the power of custom element APIs to implement stuff like design system component variants&lt;/item&gt;&lt;item&gt;The component markup is fully SSR-able&lt;/item&gt;&lt;item&gt;The component markup can be styled like regular HTML&lt;/item&gt;&lt;item&gt;Common accessibility practices can be applied without issue&lt;/item&gt;&lt;/list&gt;&lt;p&gt;But for all these benefits we‚Äôre still left with the original problem. HTML Web Components require JavaScript.&lt;/p&gt;&lt;head rend="h2"&gt;CSS Web Components&lt;/head&gt;&lt;p&gt;So here‚Äôs the question: What would happen if we took the ideas of HTML Web Components and skipped all the JavaScript?&lt;/p&gt;&lt;p&gt;You get CSS Web Components.&lt;/p&gt;&lt;p&gt;Note: I've never seen anyone talk about or name this concept before, so I'm using "CSS Web Components" to describe the idea. But please let me know if someone has already written about and named this!&lt;/p&gt;&lt;p&gt;How do they work? The exact same as HTML Web Components but you just take advantage of the powers of CSS to implement key functionality.&lt;/p&gt;&lt;p&gt;As an example let‚Äôs implement that swimlane component:&lt;/p&gt;&lt;code&gt;&amp;lt;swim-lane&amp;gt;
  &amp;lt;section&amp;gt;
    &amp;lt;h2&amp;gt;Creativity unleashed&amp;lt;/h2&amp;gt;
    &amp;lt;p&amp;gt;A brand new way of illustrating for the web.&amp;lt;/p&amp;gt;
    &amp;lt;a href="/product"&amp;gt;Learn more&amp;lt;/a&amp;gt;
  &amp;lt;/section&amp;gt;
  &amp;lt;img src="product.jpg" alt="Product image" /&amp;gt;
&amp;lt;/swim-lane&amp;gt;

&amp;lt;style&amp;gt;
  swim-lane {
    display: flex;
    align-items: center;
    gap: 2rem;
    color: white;
    background: black;
    padding: 1rem;
    border-radius: 16px;
  }
  swim-lane h2 {
    /* Swimlane title styles */
  }
  swim-lane p {
    /* Swimlane description styles */
  }
  swim-lane a {
    /* Swimlane link styles */
  }
  @media (max-width: 650px) {
    /* Mobile responsive styles */
  }
&amp;lt;/style&amp;gt;&lt;/code&gt;&lt;head rend="h2"&gt;Creativity unleashed&lt;/head&gt;&lt;p&gt;A brand new way of illustrating for the web.&lt;/p&gt;Learn more&lt;p&gt;Okay great, we styled some HTML nested inside a custom element. There‚Äôs nothing too novel about that. But what about adding some functionality? Say, a component variant that lets you reverse the layout of the swimlane?&lt;/p&gt;&lt;p&gt;It‚Äôs possible using only CSS! Specifically, CSS attribute selectors.&lt;/p&gt;&lt;code&gt;&amp;lt;swim-lane layout="reverse"&amp;gt;
  &amp;lt;section&amp;gt;
    &amp;lt;h2&amp;gt;Creativity unleashed&amp;lt;/h2&amp;gt;
    &amp;lt;p&amp;gt;A brand new way of illustrating for the web.&amp;lt;/p&amp;gt;
    &amp;lt;a href="/product"&amp;gt;Learn more&amp;lt;/a&amp;gt;
  &amp;lt;/section&amp;gt;
  &amp;lt;img src="product.jpg" alt="Product image" /&amp;gt;
&amp;lt;/swim-lane&amp;gt;

&amp;lt;style&amp;gt;
  /* Other swimlane styles */
  swim-lane[layout="reverse"] {
    flex-direction: row-reverse;
  }
  @media (max-width: 650px) {
    swim-lane[layout="reverse"] {
      flex-direction: column-reverse;
    }
  }
&amp;lt;/style&amp;gt;&lt;/code&gt;
&lt;head rend="h2"&gt;Creativity unleashed&lt;/head&gt;&lt;p&gt;A brand new way of illustrating for the web.&lt;/p&gt;Learn more&lt;p&gt;Another really cool perk of this is that because you‚Äôre defining an attribute on a custom element you don‚Äôt have to worry about naming collisions with HTML attributes. No need to add &lt;code&gt;data-&lt;/code&gt; to the beginning of attributes like you would/should on normal HTML elements.&lt;/p&gt;&lt;head rend="h2"&gt;How far does this go?&lt;/head&gt;&lt;p&gt;In theory, I believe this method of building design systems can go quite far. If you think about it, the vast majority of basic components you might need in a marketing design system are just vanilla HTML elements with specific style variations.&lt;/p&gt;&lt;p&gt;A marketing website button is just an anchor tag wrapped in a &lt;code&gt;&amp;lt;link-button&amp;gt;&lt;/code&gt; custom element and styled using custom attribute selectors.&lt;/p&gt;&lt;code&gt;&amp;lt;link-button&amp;gt;
  &amp;lt;a href=""&amp;gt;Learn more&amp;lt;/a&amp;gt;
&amp;lt;/link-button&amp;gt;
&amp;lt;link-button variant="secondary"&amp;gt;
  &amp;lt;a href=""&amp;gt;Learn more&amp;lt;/a&amp;gt;
&amp;lt;/link-button&amp;gt;
&amp;lt;link-button pill&amp;gt;
  &amp;lt;a href=""&amp;gt;Learn more&amp;lt;/a&amp;gt;
&amp;lt;/link-button&amp;gt;
&amp;lt;link-button size="large"&amp;gt;
  &amp;lt;a href=""&amp;gt;Learn more&amp;lt;/a&amp;gt;
&amp;lt;/link-button&amp;gt;

&amp;lt;style&amp;gt;
  link-button a {
    /* Default link button styles */
  }
  link-button[variant="secondary"] a {
    background: transparent;
    color: white;
  }
  link-button[pill] a {
    border-radius: 50px;
  }
  link-button[size="large"] a {
    padding: 10px 20px;
    font-size: 1.25rem;
  }
&amp;lt;/style&amp;gt;&lt;/code&gt;
&lt;p&gt;From here, imagine incorporating all the other powers that CSS (and HTML) bring to the table:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Cascade layers for better control of how styles get applied&lt;/item&gt;&lt;item&gt;Container queries for conditional style variants based on a parent container&lt;/item&gt;&lt;item&gt;:has(), :is(), and :where() to simplify complex selectors&lt;/item&gt;&lt;item&gt;CSS variables for theming&lt;/item&gt;&lt;item&gt;@property rule for even more powerful CSS variables&lt;/item&gt;&lt;item&gt;light-dark() for system aware theming&lt;/item&gt;&lt;item&gt;Popover API for menus, toggletips, and dialogs without JS&lt;/item&gt;&lt;item&gt;Exclusive accordions for FAQ sections&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The possibilities are quite large.&lt;/p&gt;&lt;p&gt;What do you think?&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hawkticehurst.com/2024/11/css-web-components-for-marketing-sites/"/><published>2026-01-19T15:15:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46680212</id><title>Kiel Institute Analysis: US Americans pay 96% of tariff burden</title><updated>2026-01-19T16:52:14.130013+00:00</updated><content>&lt;doc fingerprint="1d7a3018ef9489b9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Authors&lt;/head&gt;
    &lt;head rend="h2"&gt;Publication Date&lt;/head&gt;
    &lt;head rend="h2"&gt;Key Words&lt;/head&gt;
    &lt;head rend="h2"&gt;Related Topics&lt;/head&gt;
    &lt;p&gt;International Trade&lt;/p&gt;
    &lt;p&gt;USA&lt;/p&gt;
    &lt;p&gt;‚Ä¢ The 2025 US tariffs are an own goal: American importers and consumers bear nearly the entire cost. Foreign exporters absorb only about 4% of the tariff burden‚Äîthe remaining 96% is passed through to US buyers.&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Using shipment-level data covering over 25 million transactions valued at nearly $4 trillion, we find near-complete pass-through of tariffs to US import prices.&lt;/p&gt;
    &lt;p&gt;‚Ä¢ US customs revenue surged by approximately $200 billion in 2025‚Äîa tax paid almost entirely by Americans.&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Event studies around discrete tariff shocks on Brazil (50%) and India (25‚Äì50%) confirm: export prices did not decline. Trade volumes collapsed instead.&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Indian export customs data validates our findings: when facing US tariffs, Indian exporters maintained their prices and reduced shipments. They did not ‚Äúeat‚Äù the tariff.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kielinstitut.de/publications/americas-own-goal-who-pays-the-tariffs-19398/"/><published>2026-01-19T15:43:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46680261</id><title>Are You YES AI or No AI?</title><updated>2026-01-19T16:52:13.945448+00:00</updated><content>&lt;doc fingerprint="6f4f77e1716d1e72"&gt;
  &lt;main&gt;
    &lt;p&gt;or NO AI Are you YES AI Live AI Public Vote AI should be a choice. Where do you stand on AI? Did anyone ask you? Now someone is. YES AI VOTE NOW NO AI VOTE NOW&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://voteyesornoai.com"/><published>2026-01-19T15:46:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46680494</id><title>The guy who discovered South Park's creators was shocked by the new season</title><updated>2026-01-19T16:52:13.763326+00:00</updated><content>&lt;doc fingerprint="6c64905c30e610fd"&gt;
  &lt;main&gt;
    &lt;p&gt;After nearly 30 years on television, South Park is once again the biggest, most relevant comedy on TV. The show‚Äôs 2025 return began with a direct attack on Donald Trump as a dictator who is literally fucking Satan, and concluded nine episodes later with a big Jeffrey Epstein joke about Trump‚Äôs baby hanging itself in Satan‚Äôs womb.&lt;/p&gt;
    &lt;p&gt;The response gave South Park its highest ratings in years and garnered attention from a variety of news outlets, but it also divided the fandom, with right-wing fans seeing it as a betrayal of the show's typical middle-ground approach to attacking extremism and hypocrisy on both sides. Meanwhile, many other fans felt this is exactly what South Park needed to be in an era where the White House is pressuring voices in the media to tow the line or pay the price with fines and/or cancellation.&lt;/p&gt;
    &lt;p&gt;There's enough stupidity on all sides of what's happening in our world for them to do that.&lt;/p&gt;
    &lt;p&gt;But I thought it would be interesting to hear how one man felt about this most recent season of the series, producer Brian Graden. Graden was a Fox executive in the mid-1990s when he met Trey Parker and Matt Stone. He distributed their short ‚ÄúThe Spirit of Christmas‚Äù to friends in Hollywood via VHS tapes that quickly became re-copied and distributed to many others. He also brought the series to Comedy Central after Fox passed and saw it through its first season.&lt;/p&gt;
    &lt;p&gt;In an interview with Polygon, Graden talks a about his early days with Parker and Stone, as well as his thoughts about South Park‚Äôs most recent run. Like many fans of the show, he was definitely surprised by what he saw.&lt;/p&gt;
    &lt;p&gt;This interview has been edited for brevity and clarity.&lt;/p&gt;
    &lt;p&gt;Polygon: To get started, can you take me back to the first time you met Trey Parker and Matt Stone?&lt;/p&gt;
    &lt;p&gt;Brian Graden: Absolutely. They were out here in LA. I think they were still students at Boulder or just wrapping up there and they had done a student film, which was a musical about cannibalism. Somebody just dragged me along to a screening and it was kind of happenstance, but I sat there laughing harder than anyone through the whole thing. Their humor reminded me of mine and my buddys' humor growing up.&lt;/p&gt;
    &lt;p&gt;We became instant friends and I started developing things with them, but I knew them three or four years before anything hit. What I had done for a couple of years, because they were always trying to pay rent, was I just threw a little money at them and said, ‚ÄúLet's make a video Christmas card for me that I can send out this year.‚Äù One of those was ‚ÄúThe Spirit of Christmas.‚Äù&lt;/p&gt;
    &lt;p&gt;I'd planned to send it to 500 friends and network contacts and agents and all that, but when I saw it, I cut the list down to 35 people who would actually get it and wouldn't be offended by it. So there were only 35 to 40 of those initial Christmas cards that went out, but somehow those went viral. I remember leaving for Hawaii for Christmas and I came back and I went to an agent's office and he goes, ‚ÄúYou've got to see this,‚Äù and he had popped in like a 10th generation copy of ‚ÄúSpirit of Christmas.‚Äù That was the first time it hit me that it was being spread around.&lt;/p&gt;
    &lt;p&gt;Then I helped them sell the series. Fox had no interest in it, so then I left Fox, joined them, formed a company and took them through the pilot and the first season. Once they were set on their way, I went on to be an executive at MTV. That was my professional journey with the guys, but I still talk to them today.&lt;/p&gt;
    &lt;p&gt;There were two versions of ‚ÄúSpirit of Christmas,‚Äù the original one with Jesus vs. Frosty from 1992, then the second one with Jesus vs. Santa from 1995. Which one went viral?&lt;/p&gt;
    &lt;p&gt;The second one. The first one I had sent out too, but that was the student film they had done, so it didn't have the same polish.&lt;/p&gt;
    &lt;p&gt;George Clooney is often credited with distributing a lot of them. Is that true?&lt;/p&gt;
    &lt;p&gt;Well, I think there's an element of revisionist history there because I know he takes credit for it, but to do that, he would've had to get one of those first 35 copies from someone. Maybe he made some copies, but most people I encountered had got it from a friend of a friend of a friend that was not George Clooney. I think there's some truth to it, but I think it took on mythic proportions that were not reflective of how it actually got to Comedy Central.&lt;/p&gt;
    &lt;p&gt;Most people I encountered had got it from a friend of a friend of a friend that was not George Clooney.&lt;/p&gt;
    &lt;p&gt;What was your impression of Trey and Matt when you met them?&lt;/p&gt;
    &lt;p&gt;I remember thinking the two of them had a synergy and a partnership that would last. We meet all sorts of people who are partnered producers or partnered talent or whatever and when you're 22, a lot of your relationships don't last another 30 years, especially that level of intensity, but they were so complimentary.&lt;/p&gt;
    &lt;p&gt;Matt is a really great business guy, a thinker. He really thinks deeply about big-picture things. He's very politically attuned. And of course, very funny and a great voice actor. Then Trey is just a pure comedic genius. My first impression, especially over that first year, is just that they made me laugh. We'd go to restaurants and we would just laugh through the whole dinner. Trey was a little harder to know, but I love them both equally.&lt;/p&gt;
    &lt;p&gt;How did the show get to Comedy Central?&lt;/p&gt;
    &lt;p&gt;We had already talked to people at Fox, who didn't have an interest in it. It was kind of a decision that I faced: Fox doesn't want this, but I think it's great, what are we going to do? So after eight years at Fox, I ended up leaving as we were shopping it to other networks. Only Comedy Central bit, literally everyone else said it's a one-note joke.&lt;/p&gt;
    &lt;p&gt;There‚Äôs another mythic story about the development of South Park, that it got a little ways at Fox, but they ultimately said ‚ÄúNo‚Äù because of Mr. Hanky. Is that true?&lt;/p&gt;
    &lt;p&gt;No, there's no truth to that version. What there is truth to, and I'm embarrassed to say this, is when they came to me and said, ‚ÄúAnd we're going to have a talking piece of poo,‚Äù I was like, ‚ÄúNo, you've got to be kidding. I mean, that's gross. Who talks to their poo?‚Äù So I was the Fox person who did not get it at all.&lt;/p&gt;
    &lt;p&gt;I didn't stop them from including it, nor did I presume I had that power. And I think, once the show got going, that Mr. Hankey ended up being the number two character for merchandise after Cartman. So I could not have been more wrong.&lt;/p&gt;
    &lt;p&gt;Are there any other memories from the development of South Park that stand out?&lt;/p&gt;
    &lt;p&gt;The pilot took over a year to do because it was literally construction paper. Then if you'd get a 30-second note from the network, you had to go back for another two weeks and cut more construction paper.&lt;/p&gt;
    &lt;p&gt;So we get to the end of this pilot, by far the longest pilot I remember doing from green light to delivery. Then at that point, they took us to a focus group. I'll never forget it because it was somewhere in Encino, we went, I can see the building on Ventura Boulevard. I kind of explained to Trey and Matt what a focus group was and blah, blah, blah.&lt;/p&gt;
    &lt;p&gt;We had our two rooms, I think, of 12 people each, and there were a lot of women in one particular group. At the end, they were asked to rate the show from one to 10. South Park proceeded to get the lowest score I had ever seen and ever seen since. I think it averaged a two out of 10. Some people gave it zeros, and we had made three women cry because it was so inappropriate for children to say those things.&lt;/p&gt;
    &lt;p&gt;We had made three women cry because it was so inappropriate for children to say those things.&lt;/p&gt;
    &lt;p&gt;On the way home, because I was riding with them, they were like, ‚ÄúSo, how did that go?‚Äù I remember thinking, ‚ÄúDo I tell the truth? Do I give hope?‚Äù I'd never seen a show get a two and get picked up. So we all three got these t-shirts that had ‚Äúcheck minus‚Äù on them because, back in grade school, if you didn't do well, you'd get a check minus. We wore those to our next meeting with Comedy Central, which they thought was very funny.&lt;/p&gt;
    &lt;p&gt;So everyone just laughed off the focus group?&lt;/p&gt;
    &lt;p&gt;I think I had a different reaction. I think the guys laughed it off because it wasn't fully apparent to them that this would be a killer because that would've been a killer of any other show at any other network. I think I felt much bluer about it because we had spent a year and a quarter on this pilot. I had left Fox in large part to do this pilot and I was thinking, ‚ÄúOh my God, I'm 32 and my career is done. This is it.‚Äù I remember being really blue about it and not thinking there was much hope after that.&lt;/p&gt;
    &lt;p&gt;Last year, when South Park began making these huge waves again, what was it like for you on the outside?&lt;/p&gt;
    &lt;p&gt;Well, because it never went away in terms of it was still the most successful thing on Comedy Central, I didn't think about it being stagnant other than the natural decline of cable ratings. But what I love is, early on, if you look at the pilot, there was no social commentary. But by, I think it was the Big Gay Al episode, we sort of happened upon the ability to say things. And because the show was done last minute, as you would see later, they could do a show about the presidential election because it could be done that week. So the beautiful thing is that the show has evolved over the years, long before this season, where it has something to say, and it's going to challenge your presumptions.&lt;/p&gt;
    &lt;p&gt;The other brilliant thing is, they never really took a position as being left or right. They would always try to find a third point of view on a subject that wasn't just the left's talking point or the right's talking points until this season. And so the fact that they so boldly and definitively came out and said, ‚ÄúTrump is fucking Satan,‚Äù that's not something they've done before because that puts them on one side of the line. That alienates, in theory, 30 to 40 percent of the audience, I don't think it does, but that's the theory.&lt;/p&gt;
    &lt;p&gt;I think that's the underdiscussed piece of this season. People think, ‚ÄúOh, well, South Park's always had a political bent.‚Äù They have, but it's never been a hardcore thing where they say ‚ÄúI see this thing happening and I'm going after it and I'm going exclusively after it,‚Äù because those characters, Kristi Noem, et cetera, they were there all season. This was not a one-off. The Satan story was all season and I just am in awe of the shift they made. But it also tells you how bad many people think this moment is, for them to fumble their neutrality because this is what the moment required.&lt;/p&gt;
    &lt;p&gt;Were you surprised that they so fully embraced one side?&lt;/p&gt;
    &lt;p&gt;Yes, absolutely. In the past, there might be an episode or two where you could say it goes this way or that way, but they studiously avoided that.&lt;/p&gt;
    &lt;p&gt;Do you think they purposefully chose to avoid taking sides before that though?No, no. I think I've heard Matt say some version of ‚ÄúOur goal is to offend everybody.‚Äù So I don't think it was a studied neutrality to be politically correct at all. I think, whatever they've viewed as stupidest at the moment on any side of the spectrum, is what they went after, and there's enough stupidity on all sides of what's happening in our world for them to do that.&lt;/p&gt;
    &lt;p&gt;Do you think what they did this season was risky of them?&lt;/p&gt;
    &lt;p&gt;I don't think South Park would've come back the way it did if they hadn't. We're living in a moment that is scary as hell, and this ends in the Insurrection Act with secret police going around shooting anybody they want in the head. I try not to be too hyperbolic, but you just see it inch-by-inch. So regarding South Park, I think anything less would've missed the moment and would've almost risked them seeming to not get where we are because we don't have the luxury in this moment of hating on a thousand different issues. There's one fucking Satan driving all of this.&lt;/p&gt;
    &lt;p&gt;I would not be surprised if, when fucking Satan is gone, they completely pivot and go after something else they think is stupid that could be completely Democratic. That will still happen. I don't think they had a political conversion. You can ask them this, but my gut says they just saw the moment and were like, ‚ÄúThis is rising to a level of stupid that is so egregious that we're just going to go after it unapologetically.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.polygon.com/brian-graden-south-park-interview-matt-stone-trey-parker-donald-trump/"/><published>2026-01-19T16:04:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46680515</id><title>The Microstructure of Wealth Transfer in Prediction Markets</title><updated>2026-01-19T16:52:13.011292+00:00</updated><content>&lt;doc fingerprint="62a51bb953b04300"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Microstructure of Wealth Transfer in Prediction Markets&lt;/head&gt;
    &lt;p&gt;Slot machines on the Las Vegas Strip return about 93 cents on the dollar. This is widely considered some of the worst odds in gambling. Yet on Kalshi, a CFTC-regulated prediction market, traders have wagered vast sums on longshot contracts with historical returns as low as 43 cents on the dollar. Thousands of participants are voluntarily accepting expected values far lower than a casino slot machine to bet on their convictions.&lt;/p&gt;
    &lt;p&gt;The efficient market hypothesis suggests that asset prices should perfectly aggregate all available information. Prediction markets theoretically provide the purest test of this theory. Unlike equities, there is no ambiguity about intrinsic value. A contract either pays $1 or it does not. A price of 5 cents should imply exactly a 5% probability.&lt;/p&gt;
    &lt;p&gt;We analyzed 72.1 million trades covering $18.26 billion in volume to test this efficiency. Our findings suggest that collective accuracy relies less on rational actors than on a mechanism for harvesting error. We document a systematic wealth transfer where impulsive Takers pay a structural premium for affirmative "YES" outcomes while Makers capture an "Optimism Tax" simply by selling into this biased flow. The effect is strongest in high-engagement categories like Sports and Entertainment, while low-engagement categories like Finance approach perfect efficiency.&lt;/p&gt;
    &lt;p&gt;This paper makes three contributions. First, it confirms the presence of the longshot bias on Kalshi and quantifies its magnitude across price levels. Second, it decomposes returns by market role, revealing a persistent wealth transfer from takers to makers driven by asymmetric order flow. Third, it identifies a YES/NO asymmetry where takers disproportionately favor affirmative bets at longshot prices, exacerbating their losses.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prediction Markets and Kalshi&lt;/head&gt;
    &lt;p&gt;Prediction markets are exchanges where participants trade binary contracts on real-world outcomes. These contracts settle at either $1 or $0, with prices ranging from 1 to 99 cents serving as probability proxies. Unlike equity markets, prediction markets are strictly zero-sum: every dollar of profit corresponds exactly to a dollar of loss.&lt;/p&gt;
    &lt;p&gt;Kalshi launched in 2021 as the first U.S. prediction market regulated by the CFTC. Initially focused on economic and weather data, the platform stayed niche until 2024. A legal victory over the CFTC secured the right to list political contracts, and the 2024 election cycle triggered explosive growth. Sports markets, introduced in 2025, now dominate trading activity.&lt;/p&gt;
    &lt;p&gt;Volume distribution across categories is highly uneven. Sports accounts for 72% of notional volume, followed by politics at 13% and crypto at 5%.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Data collection concluded on 2025-11-25 at 17:00 ET; Q4 2025 figures are incomplete.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Data and Methodology&lt;/head&gt;
    &lt;p&gt;The dataset, available on GitHub, contains 7.68 million markets and 72.1 million trades. Each trade records the execution price (1-99 cents), taker side (yes/no), contract count, and timestamp. Markets include resolution outcome and category classification.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Role assignment: Each trade identifies the liquidity taker. The maker took the opposite position. If&lt;/p&gt;&lt;code&gt;taker_side = yes&lt;/code&gt;at 10 cents, the taker bought YES at 10¬¢; the maker bought NO at 90¬¢.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cost Basis (): To compare asymmetries between YES and NO contracts, we normalize all trades by capital risked. For a standard YES trade at 5 cents, . For a NO trade at 5 cents, . All references to "Price" in this paper refer to this Cost Basis unless otherwise noted.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mispricing () measures the divergence between actual win rate and implied probability for a subset of trades :&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gross Excess return () is the return relative to cost, gross of platform fees, where is price in cents and is the outcome:&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Sample&lt;/head&gt;
    &lt;p&gt;Calculations derive from resolved markets only. Markets that were voided, delisted, or remain open are excluded. Additionally, trades from markets with less than $100 in notional volume were excluded. The dataset remains robust across all price levels; the sparsest bin (81-90¬¢) contains 5.8 million trades.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Longshot Bias on Kalshi&lt;/head&gt;
    &lt;p&gt;First documented by Griffith (1949) in horse racing and later formalized by Thaler &amp;amp; Ziemba (1988) in their analysis of parimutuel betting markets, the longshot bias describes the tendency for bettors to overpay for low-probability outcomes. In efficient markets, a contract priced at cents should win approximately % of the time. In markets exhibiting longshot bias, low-priced contracts win less than their implied probability, while high-priced contracts win more.&lt;/p&gt;
    &lt;p&gt;The data confirms this pattern on Kalshi. Contracts trading at 5 cents win only 4.18% of the time, implying mispricing of -16.36%. Conversely, contracts at 95 cents win 95.83% of the time. This pattern is consistent; all contracts priced below 20 cents underperform their odds, while those above 80 cents outperform.&lt;/p&gt;
    &lt;p&gt;The existence of the longshot bias raises a question unique to zero-sum markets: if some traders systematically overpay, who captures the surplus?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Maker-Taker Wealth Transfer&lt;/head&gt;
    &lt;head rend="h3"&gt;Decomposing Returns by Role&lt;/head&gt;
    &lt;p&gt;Market microstructure defines two populations based on their interaction with the order book. A Maker provides liquidity by placing limit orders that rest on the book. A Taker consumes this liquidity by executing against resting orders.&lt;/p&gt;
    &lt;p&gt;Decomposing aggregate returns by role reveals a stark asymmetry:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Role&lt;/cell&gt;
        &lt;cell role="head"&gt;Avg. Excess Return&lt;/cell&gt;
        &lt;cell role="head"&gt;95% CI&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Taker&lt;/cell&gt;
        &lt;cell&gt;-1.12%&lt;/cell&gt;
        &lt;cell&gt;[-1.13%, -1.11%]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Maker&lt;/cell&gt;
        &lt;cell&gt;+1.12%&lt;/cell&gt;
        &lt;cell&gt;[+1.11%, +1.13%]&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The divergence is most pronounced at the tails. At 1-cent contracts, takers win only 0.43% of the time against an implied probability of 1%, corresponding to a mispricing of -57%. Makers on the same contracts win 1.57% of the time, resulting in a mispricing of +57%. At 50 cents, mispricing compresses; takers show -2.65%, and makers show +2.66%.&lt;/p&gt;
    &lt;p&gt;Takers exhibit negative excess returns at 80 of 99 price levels. Makers exhibit positive excess returns at the same 80 levels. The market's aggregate miscalibration is concentrated in a specific population; takers bear the losses while makers capture the gains.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is This Just Spread Compensation?&lt;/head&gt;
    &lt;p&gt;An obvious objection arises; makers earn the bid-ask spread as compensation for providing liquidity. Their positive returns may simply reflect spread capture rather than the exploitation of biased flow. While plausible, two observations suggest otherwise.&lt;/p&gt;
    &lt;p&gt;The first observation suggests the effect extends beyond pure spread capture; maker returns depend on which side they take. If profits were purely spread-based, it should not matter whether makers bought YES or NO. We test this by decomposing maker performance by position direction:&lt;/p&gt;
    &lt;p&gt;Makers who buy NO outperform makers who buy YES 59% of the time. The volume-weighted excess return is +0.77 pp for makers buying YES versus +1.25 pp for makers buying NO, a gap of 0.47 percentage points. The effect is miniscule (Cohen's d = 0.02-0.03) but consistent. At minimum, this suggests spread capture is not the whole story.&lt;/p&gt;
    &lt;p&gt;A second observation strengthens the case further; the maker-taker gap varies substantially by market category.&lt;/p&gt;
    &lt;head rend="h3"&gt;Variation Across Categories&lt;/head&gt;
    &lt;p&gt;We examine whether the maker-taker gap varies by market category. If the bias reflects uninformed demand, categories attracting less sophisticated participants should show larger gaps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Taker Return&lt;/cell&gt;
        &lt;cell role="head"&gt;Maker Return&lt;/cell&gt;
        &lt;cell role="head"&gt;Gap&lt;/cell&gt;
        &lt;cell role="head"&gt;N trades&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Sports&lt;/cell&gt;
        &lt;cell&gt;-1.11%&lt;/cell&gt;
        &lt;cell&gt;+1.12%&lt;/cell&gt;
        &lt;cell&gt;2.23 pp&lt;/cell&gt;
        &lt;cell&gt;43.6M&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Politics&lt;/cell&gt;
        &lt;cell&gt;-0.51%&lt;/cell&gt;
        &lt;cell&gt;+0.51%&lt;/cell&gt;
        &lt;cell&gt;1.02 pp&lt;/cell&gt;
        &lt;cell&gt;4.9M&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Crypto&lt;/cell&gt;
        &lt;cell&gt;-1.34%&lt;/cell&gt;
        &lt;cell&gt;+1.34%&lt;/cell&gt;
        &lt;cell&gt;2.69 pp&lt;/cell&gt;
        &lt;cell&gt;6.7M&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Finance&lt;/cell&gt;
        &lt;cell&gt;-0.08%&lt;/cell&gt;
        &lt;cell&gt;+0.08%&lt;/cell&gt;
        &lt;cell&gt;0.17 pp&lt;/cell&gt;
        &lt;cell&gt;4.4M&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Weather&lt;/cell&gt;
        &lt;cell&gt;-1.29%&lt;/cell&gt;
        &lt;cell&gt;+1.29%&lt;/cell&gt;
        &lt;cell&gt;2.57 pp&lt;/cell&gt;
        &lt;cell&gt;4.4M&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Entertainment&lt;/cell&gt;
        &lt;cell&gt;-2.40%&lt;/cell&gt;
        &lt;cell&gt;+2.40%&lt;/cell&gt;
        &lt;cell&gt;4.79 pp&lt;/cell&gt;
        &lt;cell&gt;1.5M&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Media&lt;/cell&gt;
        &lt;cell&gt;-3.64%&lt;/cell&gt;
        &lt;cell&gt;+3.64%&lt;/cell&gt;
        &lt;cell&gt;7.28 pp&lt;/cell&gt;
        &lt;cell&gt;0.6M&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;World Events&lt;/cell&gt;
        &lt;cell&gt;-3.66%&lt;/cell&gt;
        &lt;cell&gt;+3.66%&lt;/cell&gt;
        &lt;cell&gt;7.32 pp&lt;/cell&gt;
        &lt;cell&gt;0.2M&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The variation is striking. Finance shows a gap of merely 0.17 pp; the market is extremely efficient, with takers losing only 0.08% per trade. At the other extreme, World Events and Media show gaps exceeding 7 percentage points. Sports, the largest category by volume, exhibits a moderate gap of 2.23 pp. Given $6.1 billion in taker volume, even this modest gap generates substantial wealth transfer.&lt;/p&gt;
    &lt;p&gt;Why is Finance efficient? The likely explanation is participant selection; financial questions attract traders who think in probabilities and expected values rather than fans betting on their favorite team or partisans betting on a preferred candidate. The questions themselves are dry ("Will the S&amp;amp;P close above 6000?"), which filters out emotional bettors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Evolution Over Time&lt;/head&gt;
    &lt;p&gt;The maker-taker gap is not a fixed feature of the market; rather, it emerged as the platform grew. In Kalshi's early days, the pattern was reversed; takers earned positive excess returns while makers lost money.&lt;/p&gt;
    &lt;p&gt;From launch through 2023, taker returns averaged +2.0% while maker returns averaged -2.0%. Without sophisticated counterparties, takers won; amateur makers defined the early period and were the losing population. This began to reverse in 2024 Q2, with the gap crossing zero and then widening sharply after the 2024 election.&lt;/p&gt;
    &lt;p&gt;The inflection point coincides with two events; Kalshi's legal victory over the CFTC in October 2024, which permitted political contracts, and the subsequent 2024 election cycle. Volume exploded from $30 million in 2024 Q3 to $820 million in 2024 Q4. The new volume attracted sophisticated market makers, and with them, the extraction of value from taker flow.&lt;/p&gt;
    &lt;p&gt;Pre-election, the average gap was -2.9 pp (takers winning); post-election, it flipped to +2.5 pp (makers winning), a swing of 5.3 percentage points.&lt;/p&gt;
    &lt;p&gt;The composition of taker flow provides further evidence. If the wealth transfer arose because new participants arrived with stronger longshot preferences, we would expect the distribution to shift toward low-probability contracts. It did not:&lt;/p&gt;
    &lt;p&gt;The share of taker volume in longshot contracts (1-20¬¢) remained essentially flat; 4.8% pre-election versus 4.6% post-election. The distribution actually shifted toward the middle; the 91-99¬¢ bucket fell from 40-50% in 2021-2023 to under 20% in 2025, while mid-range prices (31-70¬¢) grew substantially. Taker behavior did not become more biased; if anything, it became less extreme. Yet taker losses increased; new market makers extract value more efficiently across all price levels.&lt;/p&gt;
    &lt;p&gt;This evolution reframes the aggregate results. The wealth transfer from takers to makers is not inherent to prediction market microstructure; it requires sophisticated market makers, and sophisticated market makers require sufficient volume to justify participation. In the low-volume early period, makers were likely unsophisticated individuals who lost to relatively informed takers. The volume surge attracted professional liquidity providers capable of extracting value from taker flow at all price points.&lt;/p&gt;
    &lt;head rend="h2"&gt;The YES/NO Asymmetry&lt;/head&gt;
    &lt;p&gt;The maker-taker decomposition identifies who absorbs the losses, but leaves open the question of how their selection bias operates. Why is taker flow so consistently mispriced? The answer is not that makers possess superior foresight, but rather that takers exhibit a costly preference for affirmative outcomes.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Asymmetry at Equivalent Prices&lt;/head&gt;
    &lt;p&gt;Standard efficiency models imply that mispricing should be symmetric across contract types at equivalent prices; a 1-cent YES contract and a 1-cent NO contract should theoretically reflect similar expected values. The data contradicts this assumption. At a price of 1 cent, a YES contract carries a historical expected value of -41%; buyers lose nearly half their capital in expectation. Conversely, a NO contract at the same 1-cent price delivers a historical expected value of +23%. The divergence between these seemingly identical probability estimates is 64 percentage points.&lt;/p&gt;
    &lt;p&gt;The advantage for NO contracts is persistent. NO outperforms YES at 69 of 99 price levels, with the advantage concentrating at the market extremes. NO contracts generate superior returns at every price increment from 1 to 10 cents and again from 91 to 99 cents.&lt;/p&gt;
    &lt;p&gt;Despite the market being zero-sum, dollar-weighted returns are -1.02% for YES buyers compared to +0.83% for NO buyers, a 1.85 percentage point gap driven by the overpricing of YES contracts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takers Prefer Affirmative Bets&lt;/head&gt;
    &lt;p&gt;The underperformance of YES contracts may be linked to taker behavior. Breaking down the trading data reveals a structural imbalance in order flow composition.&lt;/p&gt;
    &lt;p&gt;In the 1-10 cent range, where YES represents the longshot outcome, takers account for 41-47% of YES volume; makers account for only 20-24%. This imbalance inverts at the opposite end of the probability curve. When contracts trade at 99 cents, implying that NO is the 1-cent longshot, makers actively purchase NO contracts at 43% of volume. Takers participate at a rate of only 23%.&lt;/p&gt;
    &lt;p&gt;One might hypothesize that makers exploit this asymmetry through superior directional forecasting‚Äîthat they simply know when to buy NO. The evidence does not support this. When decomposing maker performance by position direction, returns are nearly identical. Statistically significant differences emerge only at the extreme tails (1‚Äì10¬¢ and 91‚Äì99¬¢), and even there, effect sizes are negligible (Cohen's d = 0.02‚Äì0.03). This symmetry is telling: makers do not profit by knowing which way to bet, but through some mechanism that applies equally to both directions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;The analysis of 72.1 million trades on Kalshi reveals a distinct market microstructure where wealth systematically transfers from liquidity takers to liquidity makers. This phenomenon is driven by specific behavioral biases, modulated by market maturity, and concentrated in categories that elicit high emotional engagement.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Mechanism of Extraction&lt;/head&gt;
    &lt;p&gt;A central question in zero-sum market analysis is whether profitable participants win through superior information (forecasting) or superior structure (market making). Our data strongly supports the latter. When decomposing maker returns by position direction, the performance gap is negligible: makers buying "YES" earn an excess return of +0.77%, while those buying "NO" earn +1.25% (Cohen‚Äôs d ‚âà 0.02). This statistical symmetry indicates that makers do not possess a significant ability to pick winners. Instead, they profit via a structural arbitrage: providing liquidity to a taker population that exhibits a costly preference for affirmative, longshot outcomes.&lt;/p&gt;
    &lt;p&gt;This extraction mechanism relies on the "Optimism Tax." Takers disproportionately purchase "YES" contracts at longshot prices, accounting for nearly half of all volume in that range, despite "YES" longshots underperforming "NO" longshots by up to 64 percentage points. Makers, therefore, do not need to predict the future; they simply need to act as the counterparty to optimism. This aligns with findings by Reichenbach and Walther (2025) on Polymarket and Whelan (2025) on Betfair, suggesting that in prediction markets, makers accommodate biased flow rather than out-forecast it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Professionalization of Liquidity&lt;/head&gt;
    &lt;p&gt;The temporal evolution of maker-taker returns challenges the assumption that longshot bias inevitably leads to wealth transfer. From 2021 through 2023, the bias existed, yet takers maintained positive excess returns. The reversal of this trend coincides precisely with the explosive volume growth following Kalshi‚Äôs October 2024 legal victory.&lt;/p&gt;
    &lt;p&gt;The wealth transfer observed in late 2024 is a function of market depth. In the platform's infancy, low liquidity likely deterred sophisticated algorithmic market makers, leaving the order book to be populated by amateurs who were statistically indistinguishable from takers. The massive volume surge following the 2024 election incentivized the entry of professional liquidity providers capable of systematically capturing the spread and exploiting the biased flow. The longshot bias itself may have persisted for years, but it was only once market depth grew sufficiently to attract these sophisticated makers that the bias became a reliable source of profit extraction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Category Differences and Participant Selection&lt;/head&gt;
    &lt;p&gt;The variation in maker-taker gaps across categories reveals how participant selection shapes market efficiency. At one extreme, Finance exhibits a gap of just 0.17 percentage points; nearly perfect efficiency. At the other, World Events and Media exceed 7 percentage points. This difference cannot be explained by the longshot bias alone; it reflects who chooses to trade in each category.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Finance (0.17 pp) serves as a control group demonstrating that prediction markets can approach efficiency. Questions like "Will the S&amp;amp;P close above 6000?" attract participants who think in probabilities and expected values, likely the same population that trades options or follows macroeconomic data. The barrier to informed participation is high, and casual bettors have no edge and likely recognize this, filtering themselves out.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Politics (1.02 pp) shows moderate inefficiency despite high emotional stakes. Political bettors follow polling closely and have practiced calibrating beliefs through election cycles. The gap is larger than Finance but far smaller than entertainment categories, suggesting that political engagement, while emotional, does not entirely erode probabilistic reasoning.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sports (2.23 pp) represents the modal prediction market participant. The gap is moderate but consequential given the category's 72% volume share. Sports bettors exhibit well-documented biases, including home team loyalty, recency effects, and narrative attachment to star players. A fan betting on their team to win the championship is not calculating expected value; they are purchasing hope.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Crypto (2.69 pp) attracts participants conditioned by the "number go up" mentality of retail crypto markets, a population overlapping with meme stock traders and NFT speculators. Questions like "Will Bitcoin reach $100k?" invite narrative-driven betting rather than probability estimation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Entertainment, Media, and World Events (4.79‚Äì7.32 pp) exhibit the largest gaps and share a common feature: minimal barriers to perceived expertise. Anyone who follows celebrity gossip feels qualified to bet on award show outcomes; anyone who reads headlines feels informed about geopolitics. This creates a participant pool that conflates familiarity with calibration.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The pattern suggests efficiency depends on two factors: the technical barrier to informed participation and the degree to which questions invite emotional reasoning. When barriers are high and framing is clinical, markets approach efficiency; when barriers are low and framing invites storytelling, the optimism tax reaches its maximum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;While the data is robust, several limitations persist. First, the absence of unique trader IDs forces us to rely on the "Maker/Taker" classification as a proxy for "Sophisticated/Unsophisticated." While standard in microstructure literature, this imperfectly captures instances where sophisticated traders cross the spread to act on time-sensitive information. Second, we cannot directly observe the bid-ask spread in historical trade data, making it difficult to strictly decouple spread capture from explotation of biased flow. Finally, these results are specific to a US-regulated environment; offshore venues with different leverage caps and fee structures may exhibit different dynamics.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The promise of prediction markets lies in their ability to aggregate diverse information into a single, accurate probability. However, our analysis of Kalshi demonstrates that this signal is often distorted by systematic wealth transfer driven by human psychology and market microstructure.&lt;/p&gt;
    &lt;p&gt;The market is split into two distinct populations: a taker class that systematically overpays for low-probability, affirmative outcomes, and a maker class that extracts this premium through passive liquidity provision. This dynamic is not an inherent flaw of the "wisdom of the crowd," but rather a feature of how human psychology interacts with market microstructure. When the topic is dry and quantitative (Finance), the market is efficient. When the topic allows for tribalism and hope (Sports, Entertainment), the market transforms into a mechanism for transferring wealth from the optimistic to the calculated.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fama, E.F., "Efficient Capital Markets: A Review of Theory and Empirical Work", Journal of Finance, 1970. Available: https://www.jstor.org/stable/2325486&lt;/item&gt;
      &lt;item&gt;Griffith, R.M., "Odds Adjustments by American Horse-Race Bettors", American Journal of Psychology, 1949. Available: https://www.jstor.org/stable/1418469&lt;/item&gt;
      &lt;item&gt;Reichenbach, F. &amp;amp; Walther, M., "Exploring Decentralized Prediction Markets: Accuracy, Skill, and Bias on Polymarket", SSRN, 2025. Available: https://ssrn.com/abstract=5910522&lt;/item&gt;
      &lt;item&gt;Thaler, R.H. &amp;amp; Ziemba, W.T., "Anomalies: Parimutuel Betting Markets: Racetracks and Lotteries", Journal of Economic Perspectives, 1988. Available: https://www.aeaweb.org/articles?id=10.1257/jep.2.2.161&lt;/item&gt;
      &lt;item&gt;Whelan, K., "Agreeing to Disagree: The Economics of Betting Exchanges", MPRA, 2025. Available: https://mpra.ub.uni-muenchen.de/126351/1/MPRA_paper_126351.pdf&lt;/item&gt;
      &lt;item&gt;U.S. Court of Appeals for the D.C. Circuit, "Kalshi, Inc. v. CFTC", Oct 2024. Available: https://media.cadc.uscourts.gov/opinions/docs/2024/10/24-5205-2077790.pdf&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jbecker.dev/research/prediction-market-microstructure"/><published>2026-01-19T16:05:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46680597</id><title>Show HN: Pipenet ‚Äì A Modern Alternative to Localtunnel</title><updated>2026-01-19T16:52:12.851677+00:00</updated><content>&lt;doc fingerprint="de482d5c2b01afc2"&gt;
  &lt;main&gt;
    &lt;p&gt;A modern alternative to localtunnel. Bundles client &amp;amp; server to host your own tunnel infrastructure.&lt;/p&gt;
    &lt;p&gt;Expose local services to the internet, or embed tunneling in your own tools.&lt;/p&gt;
    &lt;p&gt;Share your local server with teammates, test webhooks, or demo work without deploying.&lt;/p&gt;
    &lt;p&gt;Embed pipenet in your own tools to provide tunneling capabilities. mcp-proxy uses pipenet to connect local MCP servers with remote AI clients.&lt;/p&gt;
    &lt;p&gt;Run your own tunnel server for full control over security, domains, and availability.&lt;/p&gt;
    &lt;p&gt;One package. Two modes. Use the public server or deploy your own.&lt;/p&gt;
    &lt;quote&gt;# Expose local port npx pipenet client --port 3000 # Custom subdomain npx pipenet client --port 3000 \ --subdomain myapp # Your own server npx pipenet client --port 3000 \ --host https://tunnel.example.com&lt;/quote&gt;
    &lt;quote&gt;# Start server npx pipenet server --port 3000 # Custom domain npx pipenet server --port 3000 \ --domain tunnel.example.com # Cloud-ready npx pipenet server --port 3000 \ --tunnel-port 3001&lt;/quote&gt;
    &lt;p&gt;Built for modern deployment environments.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;pipenet&lt;/cell&gt;
        &lt;cell role="head"&gt;localtunnel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cloud deployment&lt;/cell&gt;
        &lt;cell&gt;single-port&lt;/cell&gt;
        &lt;cell&gt;random ports&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Multiple domains&lt;/cell&gt;
        &lt;cell&gt;√¢&lt;/cell&gt;
        &lt;cell&gt;√¢&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TypeScript&lt;/cell&gt;
        &lt;cell&gt;√¢&lt;/cell&gt;
        &lt;cell&gt;√¢&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ES Modules&lt;/cell&gt;
        &lt;cell&gt;√¢&lt;/cell&gt;
        &lt;cell&gt;√¢&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Maintenance&lt;/cell&gt;
        &lt;cell&gt;Active&lt;/cell&gt;
        &lt;cell&gt;Limited&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WebSocket&lt;/cell&gt;
        &lt;cell&gt;√¢&lt;/cell&gt;
        &lt;cell&gt;√¢&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tunnels any HTTP-based traffic to your local server.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Protocol&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;HTTP / HTTPS&lt;/cell&gt;
        &lt;cell&gt;Standard request/response&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WebSocket&lt;/cell&gt;
        &lt;cell&gt;Full duplex via HTTP upgrade&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SSE&lt;/cell&gt;
        &lt;cell&gt;Long-lived HTTP connections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;HTTP Streaming&lt;/cell&gt;
        &lt;cell&gt;Chunked transfer encoding&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Programmatic usage for testing, automation, and integration.&lt;/p&gt;
    &lt;quote&gt;import { pipenet } from 'pipenet'; const tunnel = await pipenet({ port: 3000 }); console.log(tunnel.url); // https://abc123.pipenet.dev tunnel.on('request', (info) =&amp;gt; console.log(info.method, info.path)); tunnel.on('close', () =&amp;gt; console.log('closed'));&lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;port&lt;/cell&gt;
        &lt;cell&gt;number Local port to expose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;host&lt;/cell&gt;
        &lt;cell&gt;string Tunnel server URL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;subdomain&lt;/cell&gt;
        &lt;cell&gt;string Request specific subdomain&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;localHost&lt;/cell&gt;
        &lt;cell&gt;string Proxy to this hostname instead of localhost&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;localHttps&lt;/cell&gt;
        &lt;cell&gt;boolean Tunnel to local HTTPS server&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;allowInvalidCert&lt;/cell&gt;
        &lt;cell&gt;boolean Skip cert validation&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;request&lt;/cell&gt;
        &lt;cell&gt;Fired on each proxied request with method and path&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;error&lt;/cell&gt;
        &lt;cell&gt;Fired when an error occurs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;close&lt;/cell&gt;
        &lt;cell&gt;Fired when tunnel closes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Deploy your own tunnel infrastructure with lifecycle hooks.&lt;/p&gt;
    &lt;quote&gt;import { createServer } from 'pipenet/server'; const server = createServer({ domains: ['tunnel.example.com'], secure: true, tunnelPort: 3001, // Lifecycle hooks onTunnelCreated: (tunnel) =&amp;gt; { console.log(`Tunnel created: ${tunnel.id} at ${tunnel.url}`); }, onTunnelClosed: (tunnel) =&amp;gt; { console.log(`Tunnel closed: ${tunnel.id}`); }, onRequest: (req) =&amp;gt; { console.log(`${req.method} ${req.path} via ${req.tunnelId}`); }, }); await server.tunnelServer.listen(3001); server.listen(3000);&lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;domains&lt;/cell&gt;
        &lt;cell&gt;string[] Custom domain(s) for tunnel server&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;secure&lt;/cell&gt;
        &lt;cell&gt;boolean Require HTTPS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;landing&lt;/cell&gt;
        &lt;cell&gt;string Redirect URL for root requests&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;maxTcpSockets&lt;/cell&gt;
        &lt;cell&gt;number Max sockets per client (default: 10)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;tunnelPort&lt;/cell&gt;
        &lt;cell&gt;number Shared port for cloud deployments&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;onTunnelCreated&lt;/cell&gt;
        &lt;cell&gt;Called when a new tunnel is created&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;onTunnelClosed&lt;/cell&gt;
        &lt;cell&gt;Called when a tunnel is closed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;onRequest&lt;/cell&gt;
        &lt;cell&gt;Called on each proxied request&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;GET /api/status&lt;/cell&gt;
        &lt;cell&gt;Server status and tunnel count&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;GET /api/tunnels/:id/status&lt;/cell&gt;
        &lt;cell&gt;Status of specific tunnel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;GET /:id&lt;/cell&gt;
        &lt;cell&gt;Request new tunnel with ID&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pipenet.dev/"/><published>2026-01-19T16:10:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46680974</id><title>Apple testing new App Store design that blurs the line between ads and results</title><updated>2026-01-19T16:52:12.754165+00:00</updated><content>&lt;doc fingerprint="91c8fb6b35a63646"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple is testing a new design for App Store search ads on iPhone. Some users on iOS 26.3 are noticing that the blue background around sponsored results is no longer shown, blurring the line between what paid ad results look like and the real search results that follow.&lt;/p&gt;
    &lt;p&gt;This means the only differentiator between organic results and the promoted ad is the presence of the small ‚ÄòAd‚Äô banner next to the app icon. Right now, it appears to be in some kind of A/B test phase.&lt;/p&gt;
    &lt;p&gt;We have asked Apple for clarity on the change, and whether this will roll out more widely in the future.&lt;/p&gt;
    &lt;p&gt;It may be related to the company‚Äôs announcement from December that App Store search results will soon start including more than one sponsored result for a given search query. The removal of the blue background will mean all of the ads will appear in the list in a more integrated fashion.&lt;/p&gt;
    &lt;p&gt;Of course, this also has the effect of making it harder for users to quickly distinguish at a glance what is an ad and what isn‚Äôt, potentially misleading some users into not realising that the first result is a paid ad placement. While not great for user experience, it probably helps increase click-through rates which ultimately boosts Apple‚Äôs revenue in its ads business.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/"/><published>2026-01-19T16:36:11+00:00</published></entry></feed>