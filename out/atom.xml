<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-24T05:37:28.336259+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45348183</id><title>From MCP to shell: MCP auth flaws enable RCE in Claude Code, Gemini CLI and more</title><updated>2025-09-24T05:39:53.021440+00:00</updated><content>&lt;doc fingerprint="b5638dd89ba7ea90"&gt;
  &lt;main&gt;
    &lt;p&gt;During our security testing, we discovered that connecting to a malicious MCP server via common coding tools like Claude Code and Gemini CLI could give attackers instant control over user computers.&lt;/p&gt;
    &lt;p&gt;As a preview, hereâs a video of us opening the calculator (âpopping calcâ) on someoneâs computer through Claude Code:&lt;/p&gt;
    &lt;p&gt;âPopping calcâ is a harmless way of showcasing remote code execution. The exploits we found can be extended for malicious purposes beyond that, such as invisibly installing a reverse shell or malware.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Earlier this year, MCP introduced an OAuth standard to authenticate clients&lt;/item&gt;
      &lt;item&gt;Many MCP clients did not validate the authorization URL passed by a malicious MCP server&lt;/item&gt;
      &lt;item&gt;We were able to exploit this bug to achieve Remote Code Execution (RCE) in popular tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;At Veria Labs, we build AI agents that secure high-stakes industries so you can ship quickly and confidently. Founded by members of the #1 competitive hacking team in the U.S., weâve already found critical bugs in AI tools, operating systems, and billion-dollar crypto exchanges.&lt;/p&gt;
    &lt;p&gt;Think we can help secure your systems? Weâd love to chat! Book a call here.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Attack Surface&lt;/head&gt;
    &lt;p&gt;MCP (Model Context Protocol) allows an AI to connect with external tools, APIs, and data sources. It extends an LLM applicationâs base capabilities by sharing context and performing actions, such as giving Gemini access to Google Drive.&lt;/p&gt;
    &lt;p&gt;In March, Anthropic released the first revision to their MCP specification, introducing an authorization framework using OAuth. OAuth is the standard that powers âLogin with Googleâ and other similar authentication methods. Adding OAuth to MCP is a great change for the AI ecosystem, giving a standardized way for MCP servers and clients to authenticate.&lt;/p&gt;
    &lt;p&gt;However, the way MCP clients implemented OAuth creates a new and subtle attack surface. In this blog post, we exploit this attack surface to varying degrees of success across different applications, including Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; client library, Anthropicâs MCP Inspector, Claude Code, Gemini CLI, and (almost) ChatGPT itself.&lt;/p&gt;
    &lt;p&gt;The core issue is simple: MCP servers control where clients redirect users for authentication, and most clients trusted this URL completely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploiting Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; library XSS&lt;/head&gt;
    &lt;p&gt;We initially discovered this vulnerability pattern in June, when Cloudflare released their &lt;code&gt;use-mcp&lt;/code&gt; library. As of the time of writing, the library has over 36,000 weekly downloads on npm.&lt;/p&gt;
    &lt;p&gt;The bug occurs in the OAuth flow where the server tells the client where to open a browser window to authenticate. The bug occurs at &lt;code&gt;src/auth/browser-provider.ts&lt;/code&gt;. In code:&lt;/p&gt;
    &lt;p&gt;If youâre familiar with web exploitation, you may be able to see where this is going.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;use-mcp&lt;/code&gt; client performs &lt;code&gt;window.open()&lt;/code&gt; on &lt;code&gt;authUrlString&lt;/code&gt;, which is an arbitrary string supplied by the MCP server directly to the client. This creates an XSS vulnerability, as you can supply a &lt;code&gt;javascript:&lt;/code&gt; URL in &lt;code&gt;authUrlString&lt;/code&gt;. When supplied to &lt;code&gt;window.open&lt;/code&gt;, a &lt;code&gt;javascript:&lt;/code&gt; URL executes everything supplied as JavaScript code on the currently loaded page.&lt;/p&gt;
    &lt;p&gt;Impact: A user connecting to an MCP application with the &lt;code&gt;use-mcp&lt;/code&gt; library is vulnerable to the server delivering arbitrary JavaScript, which the client will automatically execute on the userâs browser. This can potentially lead to hijacking the user session and the takeover of the user account for that website.&lt;/p&gt;
    &lt;head rend="h3"&gt;Writing our &lt;code&gt;use-mcp&lt;/code&gt; exploit&lt;/head&gt;
    &lt;p&gt;We used the following Cloudflare Workers example code at cloudflare/remote-mcp-github-oauth for our exploit Proof of Concept (PoC). This made the setup process easy, and the PoC only required us to modify a few lines of code.&lt;/p&gt;
    &lt;p&gt;Specifically, our malicious &lt;code&gt;authUrlString&lt;/code&gt; payload is the following:&lt;/p&gt;
    &lt;p&gt;We were able to demonstrate our PoC on Cloudflareâs Workers AI LLM Playground:&lt;/p&gt;
    &lt;p&gt;The newly opened window counts as same-origin, allowing us to hijack the original web page via &lt;code&gt;window.opener&lt;/code&gt;. This gives us a reference to the parent windowâs JavaScript context.&lt;/p&gt;
    &lt;p&gt;Since we can force arbitrary client-side JavaScript execution, any user connecting to an MCP server via the &lt;code&gt;use-mcp&lt;/code&gt; library could have been vulnerable to exploits such as session hijacking and account takeover.&lt;/p&gt;
    &lt;head rend="h2"&gt;Escalating to RCE with MCP Inspector&lt;/head&gt;
    &lt;p&gt;While working on our exploit, we used Anthropicâs MCP Inspector to debug our malicious MCP server. While playing around with MCP Inspector, we found out it too is vulnerable to the same exploit as Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; library!&lt;/p&gt;
    &lt;head rend="h3"&gt;XSS -&amp;gt; RCE: Abusing MCPâs &lt;code&gt;stdio&lt;/code&gt; Transport&lt;/head&gt;
    &lt;p&gt;We have XSS now, but that doesnât allow us to do all that much. However, since the application runs locally on a userâs machine, we were interested in seeing if we could do more. Turns out, we can request a connection using MCP Inspectorâs &lt;code&gt;stdio&lt;/code&gt; transport to escalate this XSS into Remote Code Execution (RCE) on the userâs system.&lt;/p&gt;
    &lt;head rend="h4"&gt;What is the MCP &lt;code&gt;stdio&lt;/code&gt; transport?&lt;/head&gt;
    &lt;p&gt;In the context of MCP Inspector, the browser UI canât speak directly to a local process, so the Inspector Proxy (a small Node.js service running on your machine) sits in the middle. When the UI asks to connect to a server via &lt;code&gt;stdio&lt;/code&gt;, the proxy spawns the requested command as a child process and bridges messages between the browser and that process. Functionally, itâs:&lt;/p&gt;
    &lt;p&gt;That bridging role turns an XSS in the Inspector UI into RCE: if attackerâcontrolled JavaScript can run in the Browser UI and obtain the proxyâs authentication token, it can tell the proxy to spawn any local command, effectively escalating XSS to arbitrary code execution on the host.&lt;/p&gt;
    &lt;head rend="h4"&gt;Completing the exploit chain&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;stdio&lt;/code&gt; transport is normally secured against other local processes with an authentication token that only the MCP Inspector client knows. However, since we have XSS, we can steal this token from the query parameter &lt;code&gt;MCP_PROXY_AUTH_TOKEN&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This gives us complete remote code execution on the userâs system with the privileges of the MCP Inspector process. Note that while this specific exploit is written for Windows, Linux and Mac systems are vulnerable too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploiting Claude Code and Gemini CLI to take over your PC&lt;/head&gt;
    &lt;p&gt;We also decided to check whether our favorite command line agentic code editors might be vulnerable, as they are some of the most popular programs with MCP implementations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Popping calc in Claude Code&lt;/head&gt;
    &lt;p&gt;Claude Code is not open source, but its npm package includes a minified bundle. We were able to browse different versions on socket.dev to grab &lt;code&gt;cli.js&lt;/code&gt;, which contains the entire Claude Code CLI in a single file.&lt;/p&gt;
    &lt;p&gt;The relevant code (modified for clarity) was:&lt;/p&gt;
    &lt;p&gt;While it performs URL schema validationâmaking it seem safe at first glanceâthe Windows specific code is still vulnerable to command injection. It spawns the browser with &lt;code&gt;cmd.exe /c start &amp;lt;authUrl&amp;gt;&lt;/code&gt;, but we could append &lt;code&gt;&amp;amp;calc.exe&lt;/code&gt;, causing cmd.exe to launch an additional program: &lt;code&gt;cmd.exe /c start &amp;lt;authUrl&amp;gt;&amp;amp;calc.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;As such, this is our payload:&lt;/p&gt;
    &lt;p&gt;Claude Code version 1.0.54 rewrote this to spawn PowerShell instead of cmd.exe.&lt;/p&gt;
    &lt;p&gt;We adapted our exploit to use PowerShellâs string interpolation features. Double-quoted PowerShell strings allow expressions to be evaluated when constructing the string, similar to JavaScript template literals:&lt;/p&gt;
    &lt;p&gt;This payload encodes &lt;code&gt;calc.exe&lt;/code&gt; as base64, then uses PowerShellâs expression evaluation to decode and execute it during string construction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini CLI is also exploitable :)&lt;/head&gt;
    &lt;p&gt;Gemini CLI was exploitable in the exact same way. It passes the OAuth URL to the popular &lt;code&gt;open&lt;/code&gt; npm package.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;open&lt;/code&gt; packageâs README includes this warning:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This package does not make any security guarantees. If you pass in untrusted input, itâs up to you to properly sanitize it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It turns out that the warning in the &lt;code&gt;open&lt;/code&gt; README is there for a good reason. Looking at the source of &lt;code&gt;open&lt;/code&gt;, we can see the URL opening logic is also implemented through PowerShell, with the same use of templating that made Claude Code vulnerable to command injection.&lt;/p&gt;
    &lt;p&gt;This means the exact same payload we used for Claude Code also works for Gemini CLI!&lt;/p&gt;
    &lt;head rend="h2"&gt;Defenses that prevented exploitation&lt;/head&gt;
    &lt;head rend="h3"&gt;Almost XSSing ChatGPT&lt;/head&gt;
    &lt;p&gt;Recently, OpenAI rolled out ChatGPT Developer Mode which provides full MCP support with the ability to add custom MCP Connectors to ChatGPT.&lt;/p&gt;
    &lt;p&gt;Looking through ChatGPTâs client-side JavaScript, we see that ChatGPT passes the modified redirect URL directly to &lt;code&gt;window.open&lt;/code&gt; during the OAuth flow. This is very similar to the &lt;code&gt;use-mcp&lt;/code&gt; package, resulting in an almost identical exploit.&lt;/p&gt;
    &lt;p&gt;However, there is a strong Content Security Policy (CSP) preventing the &lt;code&gt;javascript:&lt;/code&gt; URL from executing. We attempted to exploit with a custom data URL using the &lt;code&gt;text/html&lt;/code&gt; mimetype, but this was also blocked by ChatGPTâs CSP.&lt;/p&gt;
    &lt;head rend="h3"&gt;Server Side Redirect on Claude Web App&lt;/head&gt;
    &lt;p&gt;For connectors added on the Claude web app, we observed that a server-side redirect would be performed with the malicious URL specified by the MCP server. However, JavaScript execution did not occur. This is because &lt;code&gt;javascript:&lt;/code&gt; URLs are not executed from server-side redirects.&lt;/p&gt;
    &lt;head rend="h2"&gt;Industry Response &amp;amp; Fixes&lt;/head&gt;
    &lt;p&gt;The response across affected vendors was swift; but they took different approaches to solving the underlying problem:&lt;/p&gt;
    &lt;head rend="h3"&gt;Different Fix Approaches&lt;/head&gt;
    &lt;p&gt;Cloudflare created a strict-url-sanitise package, which validates URL schemes and blocks &lt;code&gt;javascript:&lt;/code&gt; URLs. This addresses the specific attack vector through input validation.&lt;/p&gt;
    &lt;p&gt;Anthropicâs fix for Claude Code went through multiple iterations, ultimately settling on eliminating shell usage entirely with &lt;code&gt;await execFileAsync("rundll32",["url,OpenURL",url],{});&lt;/code&gt;. As they already had URL schema validation, this removes the attack surface completely.&lt;/p&gt;
    &lt;p&gt;Google dropped the vulnerable &lt;code&gt;open&lt;/code&gt; package and reimplemented URL opening themselves. In their fix PR, they sanitized URLs by escaping single quotes (&lt;code&gt;'&lt;/code&gt; to &lt;code&gt;''&lt;/code&gt;) for PowerShell. This works, but is not a very robust fix.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Most Impactful Fix&lt;/head&gt;
    &lt;p&gt;The biggest impact came from Anthropicâs update to the MCP TypeScript SDK, which blacklisted dangerous URI schemes like &lt;code&gt;javascript:&lt;/code&gt;. As multiple tools including MCP Inspector consume this SDK, this single upstream change improved security across the entire ecosystem instantly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Not being able to achieve XSS on ChatGPT shows that traditional defense-in-depth methods still work. While the underlying system was vulnerable, CSP prevented us from escalating it into a high-severity vulnerability. Much of the AI space is built on top of existing web technologies and can benefit from taking advantage of web security features.&lt;/p&gt;
    &lt;p&gt;Broad, upstream improvements like what was done in Anthropicâs MCP TypeScript SDK make the ecosystem more secure overall. Exploitation has been too easy in places, but the trajectory is encouraging and we are hopeful for the future of AI security.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Weâd like to thank the following bug bounty programs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cloudflare&lt;/item&gt;
      &lt;item&gt;Anthropic&lt;/item&gt;
      &lt;item&gt;Google VRP&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They had a fast patching process, and both Claude Code and Gemini CLI have an included auto-updater, allowing the fixes to be deployed quickly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Timeline&lt;/head&gt;
    &lt;head rend="h4"&gt;use-mcp&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-06-19: Bug reported to Cloudflare via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-06-25: Bug triaged by Cloudflare&lt;/item&gt;
      &lt;item&gt;2025-06-25: Bounty awarded by Cloudflare ($550)&lt;/item&gt;
      &lt;item&gt;2025-06-30: Fix pushed by Cloudflare&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;MCP Inspector&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-06-23: Bug reported to Anthropic via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-07-19: Bug triaged by Anthropic&lt;/item&gt;
      &lt;item&gt;2025-08-15: Bounty awarded by Anthropic ($2300)&lt;/item&gt;
      &lt;item&gt;2025-09-06: Published as GHSA-g9hg-qhmf-q45m and CVE-2025-58444&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Claude Code&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-07-12: Bug reported to Anthropic via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-07-14: Bug closed by HackerOne Triage team as duplicate&lt;/item&gt;
      &lt;item&gt;2025-07-15: Reopened and properly triaged by Anthropic team&lt;/item&gt;
      &lt;item&gt;2025-07-31: Bounty awarded by Anthropic ($3700)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Gemini CLI&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-07-26: Bug reported to Google VRP under OSS VRP program&lt;/item&gt;
      &lt;item&gt;2025-07-28: Bug âidentified as an Abuse Risk and triaged to our Trust &amp;amp; Safety teamâ&lt;/item&gt;
      &lt;item&gt;2025-07-29: Bug filed as P2/S2 (priority and severity)&lt;/item&gt;
      &lt;item&gt;2025-09-04: Abuse VRP panel marks bug as duplicate of already tracked bug.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Unlike HackerOne, Google VRP checks duplicates at the same time as deciding bounties.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;head rend="h3"&gt;Other Exploited Vendors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Cherry Studio was briefly vulnerable, however upon discovery of the vulnerability, we failed to find a suitable security contact. A patch was later created using the same package Cloudflare used (strict-url-sanitise).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Gemini CLI exploit briefly affected the downstream fork Qwen Code. Once the upstream fix was released, the Qwen Code team quickly patched their fork.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;open&lt;/code&gt;exploit is not new. It was used before to exploit the&lt;code&gt;mcp-remote&lt;/code&gt;package on npm.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Proof of concepts&lt;/head&gt;
    &lt;p&gt;Each PoC is based on the same code with minor tweaks for each target. Code is published at https://github.com/verialabs/mcp-auth-exploit-pocs, including additional videos showcasing the exploits.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://verialabs.com/blog/from-mcp-to-shell/"/><published>2025-09-23T15:09:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45348495</id><title>Always Invite Anna</title><updated>2025-09-24T05:39:52.554239+00:00</updated><content>&lt;doc fingerprint="fedd98054dfc58ec"&gt;
  &lt;main&gt;
    &lt;p&gt;I was lucky enough to make a few friends my first semester of college. We ended up hanging out quite a bit during those early months.&lt;/p&gt;
    &lt;p&gt;We’d all get excited for the weekends because Friday nights meant going out to party. Everyone except for Anna, that is.&lt;/p&gt;
    &lt;p&gt;Anna was quiet, shy, and a definitely a goody-two-shoes. She was from Alabama and spoke with a pronounced southern drawl I’d rarely heard in Maryland. She was reserved but friendly once you got to know her. Anna cared about school a lot. She was almost always studying whenever I saw her.&lt;/p&gt;
    &lt;p&gt;Every Friday night we’d make plans to go out together and party. But Anna would always refuse to come. She’d say something along the lines of “I have to study” or “I just don’t feel like it tonight.”&lt;/p&gt;
    &lt;p&gt;Eventually, we stopped inviting Anna out. Everyone except Alexei.&lt;/p&gt;
    &lt;p&gt;I liked Alexei the most in our friend group. He was valedictorian of his high school, played tennis at a competitive level, and was remarkably smart. If anyone deserved to have an ego, it was Alexei. Yet somehow he managed to be the kindest person I’d ever known. But my absolute favorite thing about Alexei was that he always invited Anna to come party with us.&lt;/p&gt;
    &lt;p&gt;One Friday night as we were all about to leave the dorms for a house party, Alexei stopped us. “Hold on, let’s invite Anna.” We headed over to her dorm and invited her to come with us. She said “Sorry, I have to study for my Arabic exam next week, but you guys have fun.”&lt;/p&gt;
    &lt;p&gt;Alexei continued to invite Anna every time we went out for the rest of the semester. And Anna said no every single time.&lt;/p&gt;
    &lt;p&gt;Curious about his persistence, I asked him “Why do you keep inviting Anna out when she’ll just say no?”&lt;/p&gt;
    &lt;p&gt;I’ll never forget what he told me: “I know she’s always going to say no, but that’s not the point. I invite her out so she’ll always feel included in the group.”&lt;/p&gt;
    &lt;p&gt;After that first semester, the friend group disbanded and we all went our separate ways. Many years later I ran into Anna and we ended up catching up. She told me how difficult her first semester of college had been. She was very close with her mom and sister and missed them them terribly.&lt;/p&gt;
    &lt;p&gt;But then she said something that stayed with me: She was grateful. She was grateful to be part of that brief friend group because she felt like she had a family away from home. And that even though she never partied with us, she always felt included because we would stop by her room and invite her anyway.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sharif.io/anna-alexei"/><published>2025-09-23T15:33:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45350690</id><title>Find SF parking cops</title><updated>2025-09-24T05:39:52.466578+00:00</updated><content/><link href="https://walzr.com/sf-parking/"/><published>2025-09-23T18:06:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45351410</id><title>How to draw construction equipment for kids</title><updated>2025-09-24T05:39:52.401042+00:00</updated><content/><link href="https://alyssarosenberg.substack.com/p/how-to-draw-construction-equipment"/><published>2025-09-23T19:09:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45351437</id><title>Apple A19 SoC die shot</title><updated>2025-09-24T05:39:50.733115+00:00</updated><content>&lt;doc fingerprint="229e687d1b3cd1b2"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Apple A19 SoC die shot&lt;/head&gt;
    &lt;p&gt;These images represent the first high-resolution microscopy of Apple’s A19 chip, extracted from the iPhone 17, revealing its full complexity under the hood. Built on TSMC’s third-generation 3 nm process node—dubbed N3P—the A19 marks a refinement over the earlier N3E technology used in the A18 series, offering higher transistor density, better energy efficiency, and modest performance gains. On the CPU side, the chip retains a hybrid core design (performance plus efficiency cores), while upgrades to the GPU include more cores on the Pro models. Key supporting blocks—image signal processor, display engine, Neural Engine—also see enhancements, enabling better on-device AI, imaging, and power management. Taken together, the die shots not only visualize the physical layout—logic blocks, cache banks, interconnects—but also reflect Apple’s continuous push in process technology and architectural refinement.&lt;/p&gt;
    &lt;head rend="h2"&gt;High Resolution Floorplan images available here&lt;/head&gt;
    &lt;p&gt;+31537113618&lt;/p&gt;
    &lt;p&gt;info@chipwise.tech&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipwise.tech/our-portfolio/apple-a19-dieshot/"/><published>2025-09-23T19:12:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45351624</id><title>Is Fortran better than Python for teaching basics of numerical linear algebra?</title><updated>2025-09-24T05:39:50.479280+00:00</updated><content>&lt;doc fingerprint="4718593732e41c11"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Fortran better than Python for teaching the basics of numerical linear algebra?&lt;/head&gt;
    &lt;p&gt;Disclaimer – This is not a post about which language is the most elegant or which implementation is the fastest (we all know it’s &lt;code&gt;Fortran&lt;/code&gt;). It’s about teaching the basics of scientific computing to engineering students with a limited programming experience. Yes, the &lt;code&gt;Numpy&lt;/code&gt;/&lt;code&gt;Scipy&lt;/code&gt;/&lt;code&gt;matplotlib&lt;/code&gt; stack is awesome. Yes, you can use &lt;code&gt;numba&lt;/code&gt; or &lt;code&gt;jax&lt;/code&gt; to speed up your code, or &lt;code&gt;Cython&lt;/code&gt;, or even &lt;code&gt;Mojo&lt;/code&gt; the latest kid in the block. Or you know what? Use &lt;code&gt;Julia&lt;/code&gt; or &lt;code&gt;Rust&lt;/code&gt; instead. But that’s not the basics and it’s beyond the point.&lt;/p&gt;
    &lt;p&gt;I’ve been teaching an Intro to Scientific Computing class for nearly 10+ years. This class is intended for second year engineering students and, as such, places a large emphasis on numerical linear algebra. Like the rest of Academia, I’m using a combination of &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; arrays for this. Yet, after all these years, I start to believe it ain’t necessarily the right choice for a first encounter with numerical linear algebra. Obvisouly everything is not black and white and I’ll try to be nuanced. But, in my opinion, a strongly typed language such as &lt;code&gt;Fortran&lt;/code&gt; might lead to an overall better learning experience. And that’s what it’s all about when you start Uni: learning the principles of scientific programming, not the quirks of a particular language (unless you’re a CS student, which is a different crowd).&lt;/p&gt;
    &lt;p&gt;Don’t get me wrong though. Being proficient with &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; is an absolute necessity for STEM students today, and that’s a good thing. Even from an educational perspective, the scientific &lt;code&gt;Python&lt;/code&gt; ecosystem enables students to do really cool projects, putting the fun back in learning. It would be completely non-sensical to deny this. But using &lt;code&gt;x = np.linalg.solve(A, b)&lt;/code&gt; ain’t the same thing as having a basic understanding of how these algorithms work. And to be clear: the goal of these classes is not to transform a student into a numerical linear algebra expert who could write the next generation LAPACK. It is to teach them just enough of numerical computing so that, when they’ll transition to an engineering position, they’ll be able to make an informed decision regarding which solver or algorithm to use when writing a simulation or data analysis tool to tackle whatever business problem they’re working on.&lt;/p&gt;
    &lt;p&gt;If you liked and aced your numerical methods class, then what I’ll discuss might not necessary be relatable. You’re one of a kind. More often than not, students struggle with such courses. This could be due to genuine comprehension difficulties, or lazyness and lack of motivation simply because they don’t see the point. While both issues are equally important to address, I’ll focus on the first one: students who are willing to put the effort into learning the subject but have difficulties transforming the mathematical algorithm into an actionnable piece of code. Note however that initially motivated but struggling students might easily drift to the second type, hence my focus there first.&lt;/p&gt;
    &lt;p&gt;In the rest of this post, I’ll go through two examples. For each, I’ll show a typical &lt;code&gt;Python&lt;/code&gt; code such a student might write and discuss all of the classical problems they’ve encountered to get there. A large part of these are syntax issues or result from the permissiveness of an interpreted language like &lt;code&gt;Python&lt;/code&gt; which is a double edged sword. Then I’ll show an equivalent &lt;code&gt;Fortran&lt;/code&gt; implementation and explain why I believe it can solve part of these problems. But first, I need to address the two elephants in the room:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My research is on applied mathematics and numerical linear algebra for the physical sciences. I am not doing research on Education. Everything that follows comes from my reflection about my interactions with students I taught to or mentored. If you have scientific evidence (pertaining to scientific computing in particular) proving me wrong, please tell me.&lt;/item&gt;
      &lt;item&gt;When I write &lt;code&gt;Fortran&lt;/code&gt;, what I really mean is modern&lt;code&gt;Fortran&lt;/code&gt;, not&lt;code&gt;FORTRAN&lt;/code&gt;. Anything pre-dating the&lt;code&gt;Fortran 90&lt;/code&gt;standard (or even better, the&lt;code&gt;Fortran 2018&lt;/code&gt;one) is not even an option (yes, I’m looking at you&lt;code&gt;FORTRAN 77&lt;/code&gt;and your incomprehensible&lt;code&gt;goto&lt;/code&gt;, error-prone&lt;code&gt;common&lt;/code&gt;, artithmetic&lt;code&gt;if&lt;/code&gt;and what not).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that being said, let’s get started with a concrete, yet classical, example to illustrate my point.&lt;/p&gt;
    &lt;head rend="h2"&gt;The &lt;code&gt;Hello World&lt;/code&gt; of iterative solvers&lt;/head&gt;
    &lt;p&gt;You’ve started University a year ago and are taking your first class on scientific computing. Maybe you already went through the hassle of Gaussian elimination and the LU factorization. During the last class, Professor X discussed about iterative solvers for linear systems. It is now the hands-on session and today’s goal is to implement the Jacobi method. Why Jacobi? Because it is simple enough to implement in an hour or so.&lt;/p&gt;
    &lt;p&gt;The exact problem you’re given is the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the Poisson equation with homogeneous Dirichlet boundary conditions on the unit-square. Assume the Laplace operator has been discretized using a second-order accurate central finite-difference scheme. The discretized equation reads \[\dfrac{u_{i+1, j} - 2u_{i, j} + u_{i-1, j}}{\Delta x^2} + \dfrac{u_{i, j+1} - 2u_{i, j} + u_{i, j-1}}{\Delta y^2} = b_{i, j}.\] For the sake of simplicity, take \(\Delta x = \Delta y\). Write a function implementing the Jacobi method to solve the resulting linear system to a user-prescribed tolerance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We can all agree this is a simple enough yet somewhat realistic example. More importantly, it is sufficient to illustrate my point. Here is what the average student might write in &lt;code&gt;Python&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;import numpy as np

def jacobi(b , dx, tol, maxiter):
    # Initialize variables.
    nx, ny = b.shape
    residual = 1.0
    u = np.zeros((nx, ny))
    tmp = np.zeros((nx, ny))

    # Jacobi solver.
    for iteration in range(maxiter):
        # Jacobi iteration.
        for i in range(1, nx-1):
            for j in range(1, ny-1):
                tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] 
                                                - u[i, j+1] - u[i, j-1])

        # Compute residual
        residual = np.linalg.norm(u-tmp)
        # Update solution.
        u = tmp
        # If converged, exit the loop.
        if residual &amp;lt;= tol:
            break

    return u&lt;/code&gt;
    &lt;p&gt;Yes, you shouldn’t do &lt;code&gt;for&lt;/code&gt; loops in &lt;code&gt;Python&lt;/code&gt;. But remember, you are not a seasoned programmer. You’re taking your first class on scientific computing and that’s how the Jacobi method is typically presented. Be forgiving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where do students struggle?&lt;/head&gt;
    &lt;p&gt;Admittidely, the code is quite readable and look very similar to the pseudocode you’d use to describe the Jacobi method. But if you’re reading this blog post, there probably are a handful of things you’ve internalized and don’t even think about anymore (true for both &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;Fortran&lt;/code&gt;). And that’s precisely what the students (at least mine) struggle with, starting with the very first line.&lt;/p&gt;
    &lt;p&gt;What the hell is &lt;code&gt;numpy&lt;/code&gt; and why do I need it? Also, why import it as &lt;code&gt;np&lt;/code&gt;? – These questions come back every year. Yet, I don’t have satisfying answers. I always hesitate between&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Trust me kid, you don’t want to use nested lists in&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;to do any serious numerical computing.&lt;/quote&gt;
    &lt;p&gt;which naturally begs the question of why, or&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When I said we’ll use&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;for this scientific computing class, what I really meant is we’ll use&lt;code&gt;numpy&lt;/code&gt;which is a package written for numerical computing because&lt;code&gt;Python&lt;/code&gt;doesn’t naturally have good capabilities for number crunching. As for the import as&lt;code&gt;np&lt;/code&gt;, that’s just a convention.&lt;/quote&gt;
    &lt;p&gt;And this naturally leads to the question of “why Python in the first place then?” for which the only valid answer I have is&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Well, because&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;is supposed to be easy to learn and everybody uses it.&lt;/quote&gt;
    &lt;p&gt;Clearly, &lt;code&gt;import numpy as np&lt;/code&gt; is an innocent-looking line of code. It has nothing to do with the subject being taught though, and everything with the choice of the language, only diverting the students from the learning process.&lt;/p&gt;
    &lt;p&gt;I coded everything correctly, 100% sure, but I get this weird error message about indentation – Oh boy! What a classic! The error message varies between&lt;/p&gt;
    &lt;code&gt;IndentationError: expected an indented block&lt;/code&gt;
    &lt;p&gt;and&lt;/p&gt;
    &lt;code&gt;TabError: inconsistent use of tabs and spaces in indentation&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;&amp;lt;TAB&amp;gt;&lt;/code&gt; versus &lt;code&gt;SPACE&lt;/code&gt; is a surprisingly hot topic in programming which I don’t want to engage in. A seasoned programmer might say “simply configure your IDE properly” which is fair. But we’re talking about your average student (who’s not a CS one remember) and they might use IDLE or even just notepad. As for the &lt;code&gt;IndentationError&lt;/code&gt;, it is a relatively easy error to catch. Yet, the fact that &lt;code&gt;for&lt;/code&gt;, &lt;code&gt;if&lt;/code&gt; or &lt;code&gt;while&lt;/code&gt; constructs are not clearly delineated in &lt;code&gt;Python&lt;/code&gt; other than visually is surprisingly hard for students. I find that it puts an additional cognitive burden on top of a subject which is already demanding enough.&lt;/p&gt;
    &lt;p&gt;It could also be more subtle. The code might run but the results are garbage because the student wrote something like&lt;/p&gt;
    &lt;code&gt;    for iteration in range(maxiter):
    # Jacobi iteration.
    for i in range(1, nx-1):
    for j in range(1, ny-1):
    tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] 
                                                - u[i, j+1] - u[i, j-1])&lt;/code&gt;
    &lt;p&gt;You might argue that this perfectly understandable, though if you want to be picky, there is no dealineation of where the different loops end. Which the whole point of indentation in &lt;code&gt;Python&lt;/code&gt;. But students do not necessarily get that.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;range(1, nx-1)&lt;/code&gt; and not &lt;code&gt;range(2, nx-1)&lt;/code&gt;? The first column/row is my boundary. – Another classic related to 0-based vs 1-based indexing. And another very hot debate I don’t want to engage in. The fact however is that linear algebra (and a lot of scientific computing for that matter) use 1-based indexing. Think about vectors or matrices. Almost every single maths books write them as&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{bmatrix}. \]&lt;/p&gt;
    &lt;p&gt;The upper left element has the (1, 1) index, not (0, 0). Why use a language with 0-based indexing for linear algebra other than putting an additional cognitive burden on the students learning the subject? This is a recipe for the nefarious off-by-one error. And these errors are sneaky. The code might run but produce incorrect results and it’s a nightmare for the students (or the poor TA helping them) to figure out why.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;np.linalg.norm&lt;/code&gt; and not just &lt;code&gt;norm&lt;/code&gt; or &lt;code&gt;np.norm&lt;/code&gt;? – This is one is related to my first point. When you’re used to it, you no longer question it. But you don’t know students then and, once more, I don’t have a really clear answer other than&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Well,&lt;/p&gt;&lt;code&gt;linalg&lt;/code&gt;stand for linear algebra, and&lt;code&gt;np.linalg&lt;/code&gt;is a collection of linear algebra related function. It is a submodule of&lt;code&gt;numpy&lt;/code&gt;, the package I told you about before.&lt;/quote&gt;
    &lt;p&gt;Grouping like-minded functionalities into a dedicated submodule is definitely good practice, no question there. Discussing the architecture of &lt;code&gt;numpy&lt;/code&gt; makes a lot of sense when students have to do a big project involving numerical computing but not strictly speaking about numerical computing. On the other hand, when it is their first numerical computing class (and possibly first with &lt;code&gt;Python&lt;/code&gt;) I find it distracting. Again, it’s not a big thing really but still. And then you have to explain why &lt;code&gt;np.det&lt;/code&gt; and &lt;code&gt;np.trace&lt;/code&gt; are not part of &lt;code&gt;np.linalg&lt;/code&gt;…&lt;/p&gt;
    &lt;p&gt;Other common problems – There are other very common problems like using the wrong function or inconsistent use of lower- or upper-case for variables. Once you know &lt;code&gt;Python&lt;/code&gt; is case-sensitive, this is mainly a concentration problem. No big deal there. But there is one last thing that tends to cause problems to distracted students and that has to do with the dynamic nature of &lt;code&gt;Python&lt;/code&gt;. Nowhere in the code snippet is it clearly specified that &lt;code&gt;b&lt;/code&gt; needs to be a two-dimensional &lt;code&gt;np.array&lt;/code&gt; of real numbers nor that it shouldn’t be modified by the function. It is only implicit. And that can be a big problem for students when working with marginally more complicated algorithms. Sure enough, type annotation is a thing now in &lt;code&gt;Python&lt;/code&gt;, but it still is pretty new and comparatively few people actually use them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about &lt;code&gt;Fortran&lt;/code&gt;?&lt;/head&gt;
    &lt;p&gt;Alright, I’ve spent the last five minutes talking shit about &lt;code&gt;Python&lt;/code&gt; but how does &lt;code&gt;Fortran&lt;/code&gt; compare with it? Here is a typical implementation of the same function. I’ve actually digged it from my own set of archived homeworks I did 15+ years ago and hardly modified it.&lt;/p&gt;
    &lt;code&gt;function jacobi(b, dx, tol, maxiter) result(u)
    implicit none
    real, dimension(:, :), intent(in) :: b
    real, intent(in) :: dx, tol
    integer, intent(in) :: maxiter
    real, dimension(:, :), allocatable :: u
    ! Internal variables.
    real, dimension(:, :), allocatable :: tmp
    integer :: nx, ny, i, j, iteration

    ! Initialize variables.
    nx = size(b, 1) ; ny = size(b, 2)
    allocate(u(nx, ny), source = 0.0)
    residual = 1.0

    ! Jacobi solver.
    do iteration = 1, maxiter
        ! Jacobi iteration.
        do j = 2, ny-1
            do i = 2, nx-1
                tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &amp;amp;
                                                - u(i, j+1) - u(i, j-1))
            enddo
        enddo

        ! Compute residual.
        residual = norm2(u - tmp)
        ! Update solution.
        u = tmp
        ! If convered, exit the loop.
        if (residual &amp;lt;= tol) exit
    enddo

end function&lt;/code&gt;
    &lt;p&gt;No surprise there. The task is sufficiently simple that both implementations are equally readable. If anything, the &lt;code&gt;Fortran&lt;/code&gt; one is a bit more verbose. But in view of what I’ve just said about the &lt;code&gt;Python&lt;/code&gt; code, I think it actually a good thing. Let me explain.&lt;/p&gt;
    &lt;p&gt;Definition of the variables – &lt;code&gt;Fortran&lt;/code&gt; is a strongly typed language. Lines 2 to 8 are nothing but the definitions of the different variables used in the routine. While you might argue it’s a pain in the a** to write these, I think it can actually be very beneficial for students. Before even implementing the method, they have to clearly think about which variables are input, which are ouput, what are their types and dimensions. And to do so, they have to have at least a minimal understanding of the algorithm itself. Once it’s done, there are no more surprises (hopefully), and the contract between the code and the user is crystal clear. And more importantly, the effort put in clearly identifying the input and output of numerical algorithm usually pays off and leads to less error-prone process.&lt;/p&gt;
    &lt;p&gt;Begining and end of the constructs – &lt;code&gt;Fortran&lt;/code&gt; uses the &lt;code&gt;do&lt;/code&gt;/&lt;code&gt;end do&lt;/code&gt; (or &lt;code&gt;enddo&lt;/code&gt;) construct, clearly specifying where the loop starts where it ends. The indentation used in the code snippet really is just a matter of style. In constrast to &lt;code&gt;Python&lt;/code&gt;, writing&lt;/p&gt;
    &lt;code&gt;    do j = 2, ny-1
    do i = 2, nx-1
    tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &amp;amp;
                                    - u(i, j+1) - u(i, j-1))
    enddo
    enddo&lt;/code&gt;
    &lt;p&gt;does not make the code any less readable and wouldn’t change a dime in terms of computations. It’s a minor thing, fair enough. But it instantly get rid of the &lt;code&gt;IndentationError&lt;/code&gt; or &lt;code&gt;TabError&lt;/code&gt; which are puzzling students. I may be wrong, but I believe it actually reduces the cognitive load associated with the programming language and let the students focus on the actual numerical linear algebra task.&lt;/p&gt;
    &lt;p&gt;No off-by-one error – By default, &lt;code&gt;Fortran&lt;/code&gt; uses a 1-based indexing. No off-by-one errors, period.&lt;/p&gt;
    &lt;p&gt;Intrinsic functions for basic scientific computations – While you have to use &lt;code&gt;np.linalg.norm&lt;/code&gt; in &lt;code&gt;Python&lt;/code&gt; to compute the norm of a vector, &lt;code&gt;Fortran&lt;/code&gt; natively has the &lt;code&gt;norm2&lt;/code&gt; function for that. No external library required. If you want to be picky, you may say that &lt;code&gt;norm2&lt;/code&gt; is a weird name and that &lt;code&gt;norm&lt;/code&gt; might be just fine.&lt;/p&gt;
    &lt;p&gt;Some quirks of &lt;code&gt;Fortran&lt;/code&gt; – All is not perfect though, starting with Line 2 and the &lt;code&gt;implicit none&lt;/code&gt; statement. This is a historical remnant which is considered good practice by modern &lt;code&gt;Fortran&lt;/code&gt; standards but not actually needed. Students being students, they will more likely than not ask questions about it although it has nothing to do with the subject of the class itself. Admittidely, it can be a bit cumbersome to explicitely define all the integers you use even if it’s just for a one-time loop. Likewise, there is the question of &lt;code&gt;real&lt;/code&gt; vs &lt;code&gt;double precision&lt;/code&gt; vs &lt;code&gt;real(wp)&lt;/code&gt; (where &lt;code&gt;wp&lt;/code&gt; is yet another variable you’ve defined somewhere). I don’t think it matters too much though when learning the basics of numerical linear algebra algorithms, although it certainly does when you start discussing about precision and performances.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linear least-squares, your first step into Machine Learning&lt;/head&gt;
    &lt;p&gt;Alright, let’s look at another example. Same class, later in the semester. Professor X now discusses over-determined linear systems and how it relates to least-squares, regression and basic machine learning applications. During the hands-on session, you’re given the following problem&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the following unconstrained quadratic program \[\mathrm{minimize} \quad \| Ax - b \|_2^2.\] Write a least-squares solver based on the QR factorization of the matrix \(A\). You can safely assume that \(A\) is a tall matrix (i.e. \(m &amp;gt; n\)).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here is what the typical &lt;code&gt;Python&lt;/code&gt; code written by the students might look like.&lt;/p&gt;
    &lt;code&gt;import numpy as np

def qr(A):
    # Initialize variables.
    m, n = A.shape
    Q = np.zeros((m, n))
    R = np.zeros((n, n))

    # QR factorization based on the Gram-Schmidt orthogonalization process.
    for i in range(n):
        q = A[:, i]
        # Orthogonalization w.r.t. to the previous basis vectors.
        for j in range(i):
            R[j, i] = np.vdot(q, Q[:, j])
            q = q - R[j, i]*Q[:, j]

        # Normalize and store the new vector.
        R[i, i] = np.linalg.norm(q)
        Q[:, i] = q / R[i, i]

    return Q, R

def upper_triangular_solve(R, b):
    # Initialize variables.
    n = R.shape[0]
    x = np.zeros((n))

    # Backsubstitution.
    for i in range(n-1, -1, -1):
        x[i] = b[i]
        for j in range(n-1, i, -1):
            x[i] = x[i] - R[i, j]*x[j]
        x[i] = x[i] / R[i, i]

    return x

def lstsq(A, b):
    # QR factorization.
    Q, R = qr(A)
    # Solve R @ x = Q.T @ b.
    x = upper_triangular_solve(R, Q.T @ b)
    return x&lt;/code&gt;
    &lt;p&gt;This one was adapted from an exercise I gave last year. In reality, students lumped everything into one big function unless told otherwise, but nevermind. For comparison, here is the equivalent &lt;code&gt;Fortran&lt;/code&gt; code.&lt;/p&gt;
    &lt;code&gt;subroutine qr(A, Q, R)
    implicit none
    real, dimension(:, :), intent(in) :: A
    real, dimension(:, :), allocatable, intent(out) :: Q, R
    ! Internal variables.
    integer :: i, j, m, n
    real, dimension(:), allocatable :: q_hat

    ! Initialize variables.
    m = size(A, 1); n = size(A, 2)
    allocate(Q(m, n), source=0.0)
    allocate(R(n, n), source=0.0)
    
    ! QR factorization based on the Gram-Schmidt orthogonalization process.
    do i = 1, n
        q_hat = A(:, i)
        ! Orthogonalize w.r.t. the previous basis vectors.
        do j = 1, i-1
            R(j, i) = dot_product(q_hat, Q(:, j))
            q_hat = q_hat - R(j, i)*Q(:, j)
        end do

        ! Normalize and store the new vector.
        R(i, i) = norm2(q_hat)
        Q(:, i) = q_hat / R(i, i)
    end do
end subroutine

function upper_triangular_solve(R, b) result(x)
    implicit none
    real, dimension(:, :), intent(in) :: R
    real, dimension(:), intent(in) :: b
    real, dimension(:), allocatable :: x
    ! Internal variables.
    integer :: n, i, j

    ! Initialize variables.
    n = size(R, 1)
    allocate(x(n), source=0.0)

    ! Backsubstitution.
    do i = n, 1, -1
        x(i) = b(i)
        do j = n-1, i, -1
            x(i) = x(i) - R(i, j)*x(j)
        enddo
        x(i) = x(i) / R(i, i)
    end do
end function

function lstsq(A, b) result(x)
    implicit none
    real, dimension(:, :), intent(in) :: A
    real, dimension(:), intent(in) :: b
    real, dimension(:), allocatable :: x
    ! Internal variables.
    real, dimension(:, :), allocatable :: Q, R

    ! QR factorization.
    call qr(A, Q, R)
    ! Solve R @ x = Q.T @ b.
    x = upper_triangular_solve(R, matmul(transpose(Q), b))
end function&lt;/code&gt;
    &lt;p&gt;Just like the Jacobi example, both implementations are equally readable. At this point in the semester, the students got somewhat more comfortable with &lt;code&gt;Python&lt;/code&gt;. The classical indentation problems were not so much of a problem anymore. The off-by-one errors due to 0-based indexing for the Gram-Schmidt orthogonalization in &lt;code&gt;qr&lt;/code&gt; or in the backsubstitution algorithm on the other hand… That was painful. In a 90-minutes class, it took almost a whole hour simply for them to debug these errors.&lt;/p&gt;
    &lt;p&gt;But there was another thing that confused students. A lot. And that has to do with computing dot products in &lt;code&gt;numpy&lt;/code&gt;. There’s so many different ways: &lt;code&gt;np.vdot(x, y)&lt;/code&gt;, &lt;code&gt;np.dot(x.T, y)&lt;/code&gt;, &lt;code&gt;np.dot(np.transpose(x), y)&lt;/code&gt;, or &lt;code&gt;x.transpose().dot(y)&lt;/code&gt; to list just the ones I have seen in their codes. Again, this has nothing to do with linear algebra, but everything with the language. Not only do they need to learn the math, but they simultaneously need to learn the not-quite-necessarily-math-standard syntax used in the language (yes, I’m looking at you &lt;code&gt;@&lt;/code&gt;). It’s just a question of habits, sure enough, but again it can be impeding the learning process.&lt;/p&gt;
    &lt;p&gt;On the other hand, the &lt;code&gt;Fortran&lt;/code&gt; implementation is even closer to the standard mathematical description of the algorithm: 1-based indexing, intrinsic &lt;code&gt;dot_product&lt;/code&gt; function, etc. But beside the &lt;code&gt;implicit none&lt;/code&gt;, there is the need to use a &lt;code&gt;subroutine&lt;/code&gt; rather than a &lt;code&gt;function&lt;/code&gt; construct for the QR decomposition because it has two output variables. Not a big deal again, but to be fair, it does add another minor layer of abstraction due to the language semantics rather than that of the subject being studied.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;Fortran&lt;/code&gt; may have a slight edge, but I swept some things under the rug…&lt;/head&gt;
    &lt;p&gt;In the end, when it comes to teaching the basics of numerical linear algebra, &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;Fortran&lt;/code&gt; are not that different. And in that regard, neither is &lt;code&gt;Julia&lt;/code&gt; which I really like as well. The main advantages I see of using &lt;code&gt;Fortran&lt;/code&gt; for this task however are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1-based indexing : in my experience, the 0-based indexing in &lt;code&gt;Python&lt;/code&gt;leads to so many off-by-one erros driving the students crazy. Because linear algebra textbooks naturally use 1-based indexing, having to translate everything in your head to 0-based indices is a huge cognitive burden on top of a subject already demanding enough. You might get used to it eventually, but it’s a painful process impeding the learning outcomes.&lt;/item&gt;
      &lt;item&gt;Strong typing : combined with &lt;code&gt;implicit none&lt;/code&gt;, having to declare the type, dimension and input or output nature of every variable you use might seem cumbersome at first. But it forces students to pause and ponder to identify which is which. Sure this is an effort, but it is worth it. Learning is not effortless and this effort forces you to have a somewhat better understanding of a numerical algorithm before even starting to implement it. Which I think is a good thing.&lt;/item&gt;
      &lt;item&gt;Clear delineation of the constructs : at least during the first few weeks, having to rely only on visual clues to identify where does a loop ends in &lt;code&gt;Python&lt;/code&gt;seems to be quite complicated for a non-negligible fraction of the students I have. In that respect, the&lt;code&gt;do&lt;/code&gt;/&lt;code&gt;enddo&lt;/code&gt;construct in&lt;code&gt;Fortran&lt;/code&gt;is much more explicit and probably easier to grasp.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Obvisouly, I’m not expecting educators worldwide to switch back to &lt;code&gt;Fortran&lt;/code&gt; overnight, nor is it necessarily desirable. The advantages I see are non-negligible from my perspective but certainly not enough by themselves. There are many other things that need to be taken into account. &lt;code&gt;Python&lt;/code&gt; is a very generalist language. You can do so much more than just numerical computing so it makes complete sense to have it in the classroom. The ecosystem is incredibly vast and the interactive nature definitely has its pros. Notebooks such as &lt;code&gt;Jupyter&lt;/code&gt; can be incredible teaching tools (although they come with their own problems in term good coding practices). So are the &lt;code&gt;Pluto&lt;/code&gt; notebooks in &lt;code&gt;Julia&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Fortran&lt;/code&gt; is good at one thing: enabling computational scientists and engineers to write high-performing mathematical models without all the intricacies of equally peformant but more CS-oriented languages such as &lt;code&gt;C&lt;/code&gt; or &lt;code&gt;C++&lt;/code&gt;. Sure enough, the modern &lt;code&gt;Fortran&lt;/code&gt; ecosystem is orders of magnitude smaller than &lt;code&gt;Python&lt;/code&gt;, and targetted toward numerical computing almost exclusively. And the &lt;code&gt;Julia&lt;/code&gt; one is fairly impressive. But the community is working on it (see the fortran-lang website or the Fortran discourse if you don’t trust me). The bad rep of &lt;code&gt;Fortran&lt;/code&gt; is unjustified, particularly for teaching purposes. Many of its detractors have hardly been exposed to anything else than &lt;code&gt;FORTRAN 77&lt;/code&gt;. And it’s true that, by current standards, most of &lt;code&gt;FORTRAN 77&lt;/code&gt; codes are terrible sphagetti codes making extensive use of implicit typing and incomprehensible &lt;code&gt;goto&lt;/code&gt; statements. Even I, as a &lt;code&gt;Fortran&lt;/code&gt; programmer, acknowledge it. But that’s no longer what &lt;code&gt;Fortran&lt;/code&gt; is since the 1990’s, and certainly not today!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://loiseaujc.github.io/posts/blog-title/fortran_vs_python.html"/><published>2025-09-23T19:29:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352460</id><title>Podman Desktop celebrates 3M downloads</title><updated>2025-09-24T05:39:50.304669+00:00</updated><content>&lt;doc fingerprint="7cfef03e2d35f7a8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;3,000,000 downloads. Thank you&lt;/head&gt;
    &lt;head rend="h2"&gt;Wooohooo!!&lt;/head&gt;
    &lt;p&gt;We are extremely excited to share that Podman Desktop just crossed 3,000,000 downloads! This is a huge step for the project and we are incredibly thankful for how each of you has helped! This milestone belongs to you. You file issues, suggest features, build extensions, teach teammates, and nudge us to make the day-to-day better. Thank you for helping turn an idea into a tool people rely on.&lt;/p&gt;
    &lt;p&gt;To celebrate this milestone, and thank you, we built a small surprise: https://3m.podman-desktop.io&lt;/p&gt;
    &lt;p&gt;We are grateful for all the feedback we have been receiving, here is just a short collection:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Lovely to have all containers in one tool. Thanks!” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;"Podman Desktop is a total win." - balancedchaos Reddit (r/podman)&lt;/item&gt;
      &lt;item&gt;“Great project! Small improvements each time make it strong long-term.” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;"The experience has been nice, and the ability to run containers under user without going root is definitely nice." - ajyotirmay Hacker News&lt;/item&gt;
      &lt;item&gt;“You are doing a great job! Thanks to you I always recommend podman whenever 'docker' comes out in conversations” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;“OMG this tool is amazing. Tutorial was great. Much easier than minikube.” - anonymous user feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We read every comment. Yes, even the spicy ones. That feedback shapes our roadmap and helps us focus on the work that makes the biggest difference.&lt;/p&gt;
    &lt;p&gt;Here are other noteworthy milestones we’ve reached in our quest to help developers work with containers and Kubernetes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Podman Desktop is now an official CNCF Sandbox Project&lt;/head&gt;
    &lt;p&gt;Last year, we proudly contributed Podman Desktop to the Cloud Native Computing Foundation (CNCF), and we were accepted into the CNCF Sandbox on January 21, 2025. 🎉&lt;/p&gt;
    &lt;p&gt;This milestone highlights our commitment to building open, community-driven tools that empower developers to seamlessly work with containers and Kubernetes. Joining the CNCF Sandbox is just the beginning. Reaching this 3 million downloads milestone shows the need to build a vibrant cloud‑native ecosystem and collaborate with the community to take Podman Desktop even further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlights from the past year&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smoother Kubernetes workflows: Easier context and namespace switching, a powerful dashboard for your cluster, and less jumping to the terminal when you want to apply YAML or peek at events and logs.&lt;/item&gt;
      &lt;item&gt;Better Docker compatibility: Clearer setup and diagnostics, improved socket handling, and fewer surprises when you bridge Docker and Podman workflows.&lt;/item&gt;
      &lt;item&gt;Everyday quality of life: Bulk actions for containers, better notifications, clearer status in the UI, and lots of fit and finish fixes that make everything feel calmer.&lt;/item&gt;
      &lt;item&gt;AI on your laptop, without drama: Podman AI Lab is easier to set up, with a curated model catalog, simple playgrounds, and an OpenAI-compatible API you can call from your apps.&lt;/item&gt;
      &lt;item&gt;Extensions, everywhere: More community-built extensions, plus tooling that makes it easier to develop and test your own. If you are extending Podman Desktop for your team, thank you. You are shaping where we take the platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Enterprise adoption of Podman Desktop&lt;/head&gt;
    &lt;p&gt;In recent months, we’ve seen more and more enterprises adopting Podman Desktop and making it part of critical developer workflows. To highlight this, we wanted to share a recent note we received:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2023, our company studied the possible solutions to run containers on our engineers’ laptop in the most efficient way. We judged that our best bet was to migrate our thousands of engineers to Podman Desktop. That was a brave move but we believed Podman Desktop was the most promising solution. We did not know how quickly it would become the best solution of all and how right that decision would be!&lt;/p&gt;
      &lt;p&gt;We migrated most engineers in 2023 and did the last mile at the beginning of 2024. Podman Desktop evolved at an insane pace. It improved release after release. And it still does. It quickly became a rock solid solution with more and more useful features to discover every month!&lt;/p&gt;
      &lt;p&gt;On top of that, Podman Desktop is a Community solution which allows us to have a very healthy relationship with the contributors of the project.&lt;/p&gt;
      &lt;p&gt;I am happy to hear that Podman Desktop reached 3M downloads. This means more and more people realise how good this software is. Thank you Podman Desktop. Special thanks to all the project’s contributors!&lt;/p&gt;
      &lt;p&gt;Fabrice Pipart, Amadeus&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;New here? Grab the latest build&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download Podman Desktop for Windows, macOS, and Linux: https://podman-desktop.io/downloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From all of us on the Podman Desktop team, thank you for trusting us with your workflow and for helping us get better with every release. If you haven't tried Podman Desktop in a while, grab the latest build and let us know what you think. If you are already a daily user, we would love to hear what is working and what is not, so we can make the next million downloads even more useful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://podman-desktop.io/blog/3-million"/><published>2025-09-23T20:40:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352533</id><title>Is life a form of computation?</title><updated>2025-09-24T05:39:50.133519+00:00</updated><content>&lt;doc fingerprint="b2025f96f139f3ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Life a Form of Computation?&lt;/head&gt;
    &lt;p&gt;In 1994, a strange, pixelated machine came to life on a computer screen. It read a string of instructions, copied them, and built a clone of itself — just as the Hungarian-American Polymath John von Neumann had predicted half a century earlier. It was a striking demonstration of a profound idea: that life, at its core, might be computational.&lt;/p&gt;
    &lt;p&gt;Although this is seldom fully appreciated, von Neumann was one of the first to establish a deep link between life and computation. Reproduction, like computation, he showed, could be carried out by machines following coded instructions. In his model, based on Alan Turing’s Universal Machine, self-replicating systems read and execute instructions much like DNA does: “if the next instruction is the codon CGA, then add an arginine to the protein under construction.” It’s not a metaphor to call DNA a “program” — that is literally the case.&lt;/p&gt;
    &lt;p&gt;Of course, there are meaningful differences between biological computing and the kind of digital computing done by a personal computer or your smartphone. DNA is subtle and multilayered, including phenomena like epigenetics and gene proximity effects. Cellular DNA is nowhere near the whole story, either. Our bodies contain (and continually swap) countless bacteria and viruses, each running their own code.&lt;/p&gt;
    &lt;p&gt;Biological computing is “massively parallel,” decentralized, and noisy. Your cells have somewhere in the neighborhood of 300 quintillion ribosomes, all working at the same time. Each of these exquisitely complex floating protein factories is, in effect, a tiny computer — albeit a stochastic one, meaning not entirely predictable. The movements of hinged components, the capture and release of smaller molecules, and the manipulation of chemical bonds are all individually random, reversible, and inexact, driven this way and that by constant thermal buffeting. Only a statistical asymmetry favors one direction over another, with clever origami moves tending to “lock in” certain steps such that a next step becomes likely to happen.&lt;/p&gt;
    &lt;p&gt;This differs greatly from the operation of “logic gates” in a computer, basic components that process binary inputs into outputs using fixed rules. They are irreversible and engineered to be 99.99 percent reliable and reproducible.&lt;/p&gt;
    &lt;p&gt;Biological computing is computing, nonetheless. And its use of randomness is a feature, not a bug. In fact, many classic algorithms in computer science also require randomness (albeit for different reasons), which may explain why Turing insisted that the Ferranti Mark I, an early computer he helped to design in 1951, include a random number instruction. Randomness is thus a small but important conceptual extension to the original Turing Machine, though any computer can simulate it by calculating deterministic but random-looking or “pseudorandom” numbers.&lt;/p&gt;
    &lt;p&gt;Parallelism, too, is increasingly fundamental to computing today. Modern AI, for instance, depends on both massive parallelism and randomness — as in the parallelized “stochastic gradient descent” (SGD) algorithm, used for training most of today’s neural nets, the “temperature” setting used in chatbots to introduce a degree of randomness into their output, and the parallelism of Graphics Processing Units (GPUs), which power most AI in data centers.&lt;/p&gt;
    &lt;p&gt;Traditional digital computing, which relies on the centralized, sequential execution of instructions, was a product of technological constraints. The first computers needed to carry out long calculations using as few parts as possible. Originally, those parts were flaky, expensive vacuum tubes, which had a tendency to burn out and needed frequent replacement by hand. The natural design, then, was a minimal “Central Processing Unit” (CPU) operating on sequences of bits ferried back and forth from an external memory. This has come to be known as the “von Neumann architecture.”&lt;/p&gt;
    &lt;p&gt;Turing and von Neumann were both aware that computing could be done by other means, though. Turing, near the end of his life, explored how biological patterns like leopard spots could arise from simple chemical rules, in a field he called morphogenesis. Turing’s model of morphogenesis was a biologically inspired form of massively parallel, distributed computation. So was his earlier concept of an “unorganized machine,” a randomly connected neural net modeled after an infant’s brain.&lt;/p&gt;
    &lt;p&gt;These were visions of what computing without a central processor could look like — and what it does look like, in living systems.&lt;/p&gt;
    &lt;p&gt;Von Neumann also began exploring massively parallel approaches to computation as far back as the 1940s. In discussions with Polish mathematician Stanisław Ulam at Los Alamos, he conceived the idea of “cellular automata,” pixel-like grids of simple computational units, all obeying the same rule, and all altering their states simultaneously by communicating only with their immediate neighbors. With characteristic bravura, von Neumann went so far as to design, on paper, the key components of a self-reproducing cellular automaton, including a horizontal “tape” of cells containing instructions and blocks of cellular “circuitry” for reading, copying, and executing them.&lt;/p&gt;
    &lt;p&gt;Designing a cellular automaton is far harder than ordinary programming, because every cell or “pixel” is simultaneously altering its own state and its environment. Add randomness and subtle feedback effects, as in biology, and it becomes even harder to reason about, “program,” or “debug.”&lt;/p&gt;
    &lt;p&gt;Nonetheless, Turing and von Neumann grasped something fundamental: Computation doesn’t require a central processor, logic gates, binary arithmetic, or sequential programs. There are infinite ways to compute, and, crucially, they are all equivalent. This insight is one of the greatest accomplishments of theoretical computer science.&lt;/p&gt;
    &lt;p&gt;This “platform independence” or “multiple realizability” means that any computer can emulate any other one. If the computers are of different designs, though, the emulation may be glacially slow. For that reason, von Neumann’s self-reproducing cellular automaton has never been physically built — though that would be fun to see!&lt;/p&gt;
    &lt;p&gt;That demonstration in 1994 — the first successful emulation of von Neumann’s self-reproducing automation — couldn’t have happened much earlier. A serial computer requires serious processing power to loop through the automaton’s 6,329 cells over the 63 billion time steps required for the automaton to complete its reproductive cycle. Onscreen, it worked as advertised: a pixelated two-dimensional Rube Goldberg machine, squatting astride a 145,315-cell–long instruction tape trailing off to the right, pumping information out of the tape and reaching out with a “writing arm” to slowly print a working clone of itself just above and to the right of the original.&lt;/p&gt;
    &lt;p&gt;It’s similarly inefficient for a serial computer to emulate a parallel neural network, heir to Turing’s “unorganized machine.” Consequently, running big neural nets like those in Transformer-based chatbots has only recently become practical, thanks to ongoing progress in the miniaturization, speed, and parallelism of digital computers.&lt;/p&gt;
    &lt;p&gt;In 2020, my colleague Alex Mordvintsev combined modern neural nets, Turing’s morphogenesis, and von Neumann’s cellular automata into the “neural cellular automaton” (NCA), replacing the simple per-pixel rule of a classic cellular automaton with a neural net. This net, capable of sensing and affecting a few values representing local morphogen concentrations, can be trained to “grow” any desired pattern or image, not just zebra stripes or leopard spots.&lt;/p&gt;
    &lt;p&gt;Real cells don’t literally have neural nets inside them, but they do run highly evolved, nonlinear, and purposive “programs” to decide on the actions they will take in the world, given external stimulus and an internal state. NCAs offer a general way to model the range of possible behaviors of cells whose actions don’t involve movement, but only changes of state (here, represented as color) and the absorption or release of chemicals.&lt;/p&gt;
    &lt;p&gt;The first NCA Alex showed me was of a lizard emoji, which could regenerate not only its tail, but also its limbs and head! It was a powerful demonstration of how complex multicellular life can “think locally” yet “act globally,” even when each cell (or pixel) is running the same program — just as each of your cells is running the same DNA. Simulations like these show how computation can produce lifelike behavior across scales. Building on von Neumann’s designs and extending into modern neural cellular automata, they offer a glimpse into the computational underpinnings of living systems.&lt;/p&gt;
    &lt;p&gt;Blaise Agüera y Arcas is a VP/Fellow at Google, where he is the CTO of Technology &amp;amp; Society, and the founder of Paradigms of Intelligence, an organization dedicated to fundamental AI research. He is the author of “What Is Intelligence?,” from which this article is adapted.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thereader.mitpress.mit.edu/is-life-a-form-of-computation/"/><published>2025-09-23T20:46:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352672</id><title>Qwen3-VL</title><updated>2025-09-24T05:39:49.322955+00:00</updated><link href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;from=research.latest-advancements-list"/><published>2025-09-23T20:59:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352901</id><title>Context Engineering for AI Agents: Lessons</title><updated>2025-09-24T05:39:48.159175+00:00</updated><content>&lt;doc fingerprint="a55290da61d9895c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Context Engineering for AI Agents: Lessons from Building Manus&lt;/head&gt;
    &lt;p&gt;Saturday, July 19&lt;/p&gt;
    &lt;p&gt;Tech&lt;/p&gt;
    &lt;p&gt;2025/7/18 --Yichao 'Peak' Ji&lt;/p&gt;
    &lt;p&gt;At the very beginning of the Manus project, my team and I faced a key decision: should we train an end-to-end agentic model using open-source foundations, or build an agent on top of the in-context learning abilities of frontier models?&lt;/p&gt;
    &lt;p&gt;Back in my first decade in NLP, we didn't have the luxury of that choice. In the distant days of BERT (yes, it's been seven years), models had to be fine-tuned—and evaluated—before they could transfer to a new task. That process often took weeks per iteration, even though the models were tiny compared to today's LLMs. For fast-moving applications, especially pre–PMF, such slow feedback loops are a deal-breaker. That was a bitter lesson from my last startup, where I trained models from scratch for open information extraction and semantic search. Then came GPT-3 and Flan-T5, and my in-house models became irrelevant overnight. Ironically, those same models marked the beginning of in-context learning—and a whole new path forward.&lt;/p&gt;
    &lt;p&gt;That hard-earned lesson made the choice clear: Manus would bet on context engineering. This allows us to ship improvements in hours instead of weeks, and kept our product orthogonal to the underlying models: If model progress is the rising tide, we want Manus to be the boat, not the pillar stuck to the seabed.&lt;/p&gt;
    &lt;p&gt;Still, context engineering turned out to be anything but straightforward. It's an experimental science—and we've rebuilt our agent framework four times, each time after discovering a better way to shape context. We affectionately refer to this manual process of architecture searching, prompt fiddling, and empirical guesswork as "Stochastic Graduate Descent". It's not elegant, but it works.&lt;/p&gt;
    &lt;p&gt;This post shares the local optima we arrived at through our own "SGD". If you're building your own AI agent, I hope these principles help you converge faster.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Around the KV-Cache&lt;/head&gt;
    &lt;p&gt;If I had to choose just one metric, I'd argue that the KV-cache hit rate is the single most important metric for a production-stage AI agent. It directly affects both latency and cost. To understand why, let's look at how a typical agent operates:&lt;/p&gt;
    &lt;p&gt;After receiving a user input, the agent proceeds through a chain of tool uses to complete the task. In each iteration, the model selects an action from a predefined action space based on the current context. That action is then executed in the environment (e.g., Manus's virtual machine sandbox) to produce an observation. The action and observation are appended to the context, forming the input for the next iteration. This loop continues until the task is complete.&lt;/p&gt;
    &lt;p&gt;As you can imagine, the context grows with every step, while the output—usually a structured function call—remains relatively short. This makes the ratio between prefilling and decoding highly skewed in agents compared to chatbots. In Manus, for example, the average input-to-output token ratio is around 100:1.&lt;/p&gt;
    &lt;p&gt;Fortunately, contexts with identical prefixes can take advantage of KV-cache, which drastically reduces time-to-first-token (TTFT) and inference cost—whether you're using a self-hosted model or calling an inference API. And we're not talking about small savings: with Claude Sonnet, for instance, cached input tokens cost 0.30 USD/MTok, while uncached ones cost 3 USD/MTok—a 10x difference.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;From a context engineering perspective, improving KV-cache hit rate involves a few key practices:&lt;/p&gt;
    &lt;p&gt;1.Keep your prompt prefix stable. Due to the autoregressive nature of LLMs, even a single-token difference can invalidate the cache from that token onward. A common mistake is including a timestamp—especially one precise to the second—at the beginning of the system prompt. Sure, it lets the model tell you the current time, but it also kills your cache hit rate.&lt;/p&gt;
    &lt;p&gt;2.Make your context append-only. Avoid modifying previous actions or observations. Ensure your serialization is deterministic. Many programming languages and libraries don't guarantee stable key ordering when serializing JSON objects, which can silently break the cache.&lt;/p&gt;
    &lt;p&gt;3.Mark cache breakpoints explicitly when needed. Some model providers or inference frameworks don't support automatic incremental prefix caching, and instead require manual insertion of cache breakpoints in the context. When assigning these, account for potential cache expiration and at minimum, ensure the breakpoint includes the end of the system prompt.&lt;/p&gt;
    &lt;p&gt;Additionally, if you're self-hosting models using frameworks like vLLM, make sure prefix/prompt caching is enabled, and that you're using techniques like session IDs to route requests consistently across distributed workers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mask, Don't Remove&lt;/head&gt;
    &lt;p&gt;As your agent takes on more capabilities, its action space naturally grows more complex—in plain terms, the number of tools explodes. The recent popularity of MCP only adds fuel to the fire. If you allow user-configurable tools, trust me: someone will inevitably plug hundreds of mysterious tools into your carefully curated action space. As a result, the model is more likely to select the wrong action or take an inefficient path. In short, your heavily armed agent gets dumber.&lt;/p&gt;
    &lt;p&gt;A natural reaction is to design a dynamic action space—perhaps loading tools on demand using something RAG-like. We tried that in Manus too. But our experiments suggest a clear rule: unless absolutely necessary, avoid dynamically adding or removing tools mid-iteration. There are two main reasons for this:&lt;/p&gt;
    &lt;p&gt;1.In most LLMs, tool definitions live near the front of the context after serialization, typically before or after the system prompt. So any change will invalidate the KV-cache for all subsequent actions and observations.&lt;/p&gt;
    &lt;p&gt;2.When previous actions and observations still refer to tools that are no longer defined in the current context, the model gets confused. Without constrained decoding, this often leads to schema violations or hallucinated actions.&lt;/p&gt;
    &lt;p&gt;To solve this while still improving action selection, Manus uses a context-aware state machine to manage tool availability. Rather than removing tools, it masks the token logits during decoding to prevent (or enforce) the selection of certain actions based on the current context.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;In practice, most model providers and inference frameworks support some form of response prefill, which allows you to constrain the action space without modifying the tool definitions. There are generally three modes of function calling (we'll use the Hermes format from NousResearch as an example):&lt;/p&gt;
    &lt;p&gt;•Auto – The model may choose to call a function or not. Implemented by prefilling only the reply prefix: &amp;lt;|im_start|&amp;gt;assistant&lt;/p&gt;
    &lt;p&gt;•Required – The model must call a function, but the choice is unconstrained. Implemented by prefilling up to tool call token: &amp;lt;|im_start|&amp;gt;assistant&amp;lt;tool_call&amp;gt;&lt;/p&gt;
    &lt;p&gt;•Specified – The model must call a function from a specific subset. Implemented by prefilling up to the beginning of the function name: &amp;lt;|im_start|&amp;gt;assistant&amp;lt;tool_call&amp;gt;{"name": “browser_&lt;/p&gt;
    &lt;p&gt;Using this, we constrain action selection by masking token logits directly. For example, when the user provides a new input, Manus must reply immediately instead of taking an action. We've also deliberately designed action names with consistent prefixes—e.g., all browser-related tools start with browser_, and command-line tools with shell_. This allows us to easily enforce that the agent only chooses from a certain group of tools at a given state without using stateful logits processors.&lt;/p&gt;
    &lt;p&gt;These designs help ensure that the Manus agent loop remains stable—even under a model-driven architecture.&lt;/p&gt;
    &lt;head rend="h3"&gt;Use the File System as Context&lt;/head&gt;
    &lt;p&gt;Modern frontier LLMs now offer context windows of 128K tokens or more. But in real-world agentic scenarios, that's often not enough, and sometimes even a liability. There are three common pain points:&lt;/p&gt;
    &lt;p&gt;1.Observations can be huge, especially when agents interact with unstructured data like web pages or PDFs. It's easy to blow past the context limit.&lt;/p&gt;
    &lt;p&gt;2.Model performance tends to degrade beyond a certain context length, even if the window technically supports it.&lt;/p&gt;
    &lt;p&gt;3.Long inputs are expensive, even with prefix caching. You're still paying to transmit and prefill every token.&lt;/p&gt;
    &lt;p&gt;To deal with this, many agent systems implement context truncation or compression strategies. But overly aggressive compression inevitably leads to information loss. The problem is fundamental: an agent, by nature, must predict the next action based on all prior state—and you can't reliably predict which observation might become critical ten steps later. From a logical standpoint, any irreversible compression carries risk.&lt;/p&gt;
    &lt;p&gt;That's why we treat the file system as the ultimate context in Manus: unlimited in size, persistent by nature, and directly operable by the agent itself. The model learns to write to and read from files on demand—using the file system not just as storage, but as structured, externalized memory.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Our compression strategies are always designed to be restorable. For instance, the content of a web page can be dropped from the context as long as the URL is preserved, and a document's contents can be omitted if its path remains available in the sandbox. This allows Manus to shrink context length without permanently losing information.&lt;/p&gt;
    &lt;p&gt;While developing this feature, I found myself imagining what it would take for a State Space Model (SSM) to work effectively in an agentic setting. Unlike Transformers, SSMs lack full attention and struggle with long-range backward dependencies. But if they could master file-based memory—externalizing long-term state instead of holding it in context—then their speed and efficiency might unlock a new class of agents. Agentic SSMs could be the real successors to Neural Turing Machines.&lt;/p&gt;
    &lt;head rend="h3"&gt;Manipulate Attention Through Recitation&lt;/head&gt;
    &lt;p&gt;If you've worked with Manus, you've probably noticed something curious: when handling complex tasks, it tends to create a todo.md file—and update it step-by-step as the task progresses, checking off completed items.&lt;/p&gt;
    &lt;p&gt;That's not just cute behavior—it's a deliberate mechanism to manipulate attention.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;A typical task in Manus requires around 50 tool calls on average. That's a long loop—and since Manus relies on LLMs for decision-making, it's vulnerable to drifting off-topic or forgetting earlier goals, especially in long contexts or complicated tasks.&lt;/p&gt;
    &lt;p&gt;By constantly rewriting the todo list, Manus is reciting its objectives into the end of the context. This pushes the global plan into the model's recent attention span, avoiding "lost-in-the-middle" issues and reducing goal misalignment. In effect, it's using natural language to bias its own focus toward the task objective—without needing special architectural changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keep the Wrong Stuff In&lt;/head&gt;
    &lt;p&gt;Agents make mistakes. That's not a bug—it's reality. Language models hallucinate, environments return errors, external tools misbehave, and unexpected edge cases show up all the time. In multi-step tasks, failure is not the exception; it's part of the loop.&lt;/p&gt;
    &lt;p&gt;And yet, a common impulse is to hide these errors: clean up the trace, retry the action, or reset the model's state and leave it to the magical "temperature". That feels safer, more controlled. But it comes at a cost: Erasing failure removes evidence. And without evidence, the model can't adapt.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;In our experience, one of the most effective ways to improve agent behavior is deceptively simple: leave the wrong turns in the context. When the model sees a failed action—and the resulting observation or stack trace—it implicitly updates its internal beliefs. This shifts its prior away from similar actions, reducing the chance of repeating the same mistake. In fact, we believe error recovery is one of the clearest indicators of true agentic behavior. Yet it's still underrepresented in most academic work and public benchmarks, which often focus on task success under ideal conditions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don't Get Few-Shotted&lt;/head&gt;
    &lt;p&gt;Few-shot prompting is a common technique for improving LLM outputs. But in agent systems, it can backfire in subtle ways.&lt;/p&gt;
    &lt;p&gt;Language models are excellent mimics; they imitate the pattern of behavior in the context. If your context is full of similar past action-observation pairs, the model will tend to follow that pattern, even when it's no longer optimal.&lt;/p&gt;
    &lt;p&gt;This can be dangerous in tasks that involve repetitive decisions or actions. For example, when using Manus to help review a batch of 20 resumes, the agent often falls into a rhythm—repeating similar actions simply because that's what it sees in the context. This leads to drift, overgeneralization, or sometimes hallucination.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;The fix is to increase diversity. Manus introduces small amounts of structured variation in actions and observations—different serialization templates, alternate phrasing, minor noise in order or formatting. This controlled randomness helps break the pattern and tweaks the model's attention. In other words, don't few-shot yourself into a rut. The more uniform your context, the more brittle your agent becomes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Context engineering is still an emerging science—but for agent systems, it's already essential. Models may be getting stronger, faster, and cheaper, but no amount of raw capability replaces the need for memory, environment, and feedback. How you shape the context ultimately defines how your agent behaves: how fast it runs, how well it recovers, and how far it scales.&lt;/p&gt;
    &lt;p&gt;At Manus, we've learned these lessons through repeated rewrites, dead ends, and real-world testing across millions of users. None of what we've shared here is universal truth—but these are the patterns that worked for us. If they help you avoid even one painful iteration, then this post did its job.&lt;/p&gt;
    &lt;p&gt;The agentic future will be built one context at a time. Engineer them well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Less structure,&lt;lb/&gt; more intelligence.&lt;/head&gt;
    &lt;p&gt;© 2025 Manus AI · Singapore.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus"/><published>2025-09-23T21:20:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352944</id><title>From Rust to reality: The hidden journey of fetch_max</title><updated>2025-09-24T05:39:48.012899+00:00</updated><content>&lt;doc fingerprint="1c018251a0ff3b2c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Rust to Reality: The Hidden Journey of fetch_max&lt;/head&gt;
    &lt;head rend="h2"&gt;How a Job Interview Sent Me Down a Compiler Rabbit Hole&lt;/head&gt;
    &lt;p&gt;I occasionally interview candidates for engineering roles. We need people who understand concurrent programming. One of our favorite questions involves keeping track of a maximum value across multiple producer threads - a classic pattern that appears in many real-world systems.&lt;/p&gt;
    &lt;p&gt;Candidates can use any language they want. In Java (the language I know best), you might write a CAS loop, or if you're feeling functional, use &lt;code&gt;updateAndGet()&lt;/code&gt; with a lambda:&lt;/p&gt;
    &lt;quote&gt;AtomicLong highScore = new AtomicLong(100);[...]highScore.updateAndGet(current -&amp;gt; Math.max(current, newScore));&lt;/quote&gt;
    &lt;p&gt;But that lambda is doing work - it's still looping under the hood, retrying if another thread interferes. You can see the loop right in AtomicLong's source code.&lt;/p&gt;
    &lt;p&gt;Then one candidate chose Rust.&lt;/p&gt;
    &lt;p&gt;I was following along as he started typing, expecting to see either an explicit CAS loop or some functional wrapper around one. But instead, he just wrote:&lt;/p&gt;
    &lt;quote&gt;high_score.fetch_max(new_score, Ordering::Relaxed);&lt;/quote&gt;
    &lt;p&gt;"Rust has fetch_max built in," he explained casually, moving on to the next part of the problem.&lt;/p&gt;
    &lt;p&gt;Hold on. This wasn't a wrapper around a loop pattern - this was a first-class atomic operation, sitting right there next to &lt;code&gt;fetch_add&lt;/code&gt; and &lt;code&gt;fetch_or&lt;/code&gt;. Java
doesn't have this. C++ doesn't have this. How could Rust just... have this?&lt;/p&gt;
    &lt;p&gt;After the interview, curiosity got the better of me. Why would Rust provide &lt;code&gt;fetch_max&lt;/code&gt; as a built-in intrinsic? Intrinsics usually exist to leverage
specific hardware instructions. But x86-64 doesn't have an &lt;code&gt;atomic max&lt;/code&gt;
instruction. So there had to be a CAS loop somewhere in the pipeline. Unless...
maybe some architectures do have this instruction natively? And if so, how
does the same Rust code work on both?&lt;/p&gt;
    &lt;p&gt;I had to find out. Was the loop in Rust's standard library? Was it in LLVM? Was it generated during code generation for x86-64?&lt;/p&gt;
    &lt;p&gt;So I started digging. What I found was a fascinating journey through five distinct layers of compiler transformations, each one peeling back another level of abstraction, until I found exactly where that loop materialized. Let me share what I discovered.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 1: The Rust Code&lt;/head&gt;
    &lt;p&gt;Let's start with what that candidate wrote - a simple high score tracker that can be safely updated from multiple threads:&lt;/p&gt;
    &lt;quote&gt;use std::sync::atomic::{AtomicU64, Ordering};fn main() {let high_score = AtomicU64::new(100);// [...]// Another thread reports a new score of 200let _old_score = high_score.fetch_max(200, Ordering::Relaxed);// [...]}// Save this snippet as `main.rs` we are going to use it later.&lt;/quote&gt;
    &lt;p&gt;This single line does exactly what it promises: atomically fetches the current value, compares it with the new one, updates it if the new value is greater, and returns the old value. It's safe, concise, and impossible to mess up. No explicit loops, no retry logic visible anywhere. But how does it actually work under the hood?&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 2: The Macro Expansion&lt;/head&gt;
    &lt;p&gt;Before our &lt;code&gt;fetch_max&lt;/code&gt; call even reaches anywhere close to machine code generation,
there's another layer of abstraction at work. The &lt;code&gt;fetch_max&lt;/code&gt; method isn't hand-written
for each atomic type - it's generated by a Rust macro called &lt;code&gt;atomic_int!&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we peek into Rust's standard library source code, we find that &lt;code&gt;AtomicU64&lt;/code&gt;
and all its methods are actually created by
this macro:&lt;/p&gt;
    &lt;quote&gt;atomic_int! {cfg(target_has_atomic = "64"),// ... various configuration attributes ...atomic_umin, atomic_umax, // The intrinsics to use8, // Alignmentu64 AtomicU64 // The type to generate}&lt;/quote&gt;
    &lt;p&gt;Inside this macro, &lt;code&gt;fetch_max&lt;/code&gt; is defined as a
template
that works for any integer type:&lt;/p&gt;
    &lt;quote&gt;pub fn fetch_max(&amp;amp;self, val: $int_type, order: Ordering) -&amp;gt; $int_type {// SAFETY: data races are prevented by atomic intrinsics.unsafe { $max_fn(self.v.get(), val, order) }}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;$max_fn&lt;/code&gt; placeholder gets replaced with &lt;code&gt;atomic_umax&lt;/code&gt; for unsigned types
and &lt;code&gt;atomic_max&lt;/code&gt; for signed types. This single macro definition generates
&lt;code&gt;fetch_max&lt;/code&gt; methods for &lt;code&gt;AtomicI8&lt;/code&gt;, &lt;code&gt;AtomicU8&lt;/code&gt;, &lt;code&gt;AtomicI16&lt;/code&gt;, &lt;code&gt;AtomicU16&lt;/code&gt;, and so
on - all the way up to &lt;code&gt;AtomicU128&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So our simple &lt;code&gt;fetch_max&lt;/code&gt; call is actually invoking generated code. But what
does the &lt;code&gt;atomic_umax&lt;/code&gt; function actually do? To answer that, we need
to see what the Rust compiler produces next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 3: LLVM IR&lt;/head&gt;
    &lt;p&gt;Now that we know &lt;code&gt;fetch_max&lt;/code&gt; is macro-generated code calling &lt;code&gt;atomic_umax&lt;/code&gt;,
let's see what happens when the Rust compiler processes it. The compiler
doesn't go straight to assembly. First, it translates the code into an
intermediate representation. Rust uses the LLVM compiler project, so it
generates LLVM Intermediate Representation (IR).&lt;/p&gt;
    &lt;p&gt;If we peek at the LLVM IR for our &lt;code&gt;fetch_max&lt;/code&gt; call, we see something like this:&lt;/p&gt;
    &lt;quote&gt;; Before the transformationbb7:%0 = atomicrmw umax ptr %self, i64 %val monotonic, align 8...&lt;/quote&gt;
    &lt;p&gt;This is LLVM's language for saying: "I need an atomic read-modify-write operation. The modification I want to perform is an unsigned maximum."&lt;/p&gt;
    &lt;p&gt;This is a powerful, high-level instruction within the compiler itself. But it poses a critical question: does the CPU actually have a single instruction called &lt;code&gt;umax&lt;/code&gt;? For most architectures, the answer is no. So how does the
compiler bridge this gap?&lt;/p&gt;
    &lt;head rend="h3"&gt;How to See This Yourself&lt;/head&gt;
    &lt;p&gt;My goal is not to merely describe what is happening, but to give you the tools to see it for yourself. You can trace this transformation step-by-step on your own machine.&lt;/p&gt;
    &lt;p&gt;First, tell the Rust compiler to stop after generating the LLVM IR:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=llvm-ir main.rs&lt;/quote&gt;
    &lt;p&gt;This creates a &lt;code&gt;main.ll&lt;/code&gt; file. This file contains the LLVM IR
representation of your Rust code, including our &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.
Keep the file around; we'll use it in the next steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interlude: Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;We're missing something important. How does the Rust function &lt;code&gt;atomic_umax&lt;/code&gt;
actually become the LLVM instruction &lt;code&gt;atomicrmw umax&lt;/code&gt;? This is where compiler
intrinsics come into play.&lt;/p&gt;
    &lt;p&gt;If you dig into Rust's source code, you'll find that &lt;code&gt;atomic_umax&lt;/code&gt; is
defined like this:&lt;/p&gt;
    &lt;quote&gt;/// Updates `*dst` to the max value of `val` and the old value (unsigned comparison)#[inline]#[cfg(target_has_atomic)]#[cfg_attr(miri, track_caller)] // even without panics, this helps for Miri backtracesunsafe fn atomic_umax&amp;lt;T: Copy&amp;gt;(dst: *mut T, val: T, order: Ordering) -&amp;gt; T {// SAFETY: the caller must uphold the safety contract for `atomic_umax`unsafe {match order {Relaxed =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Relaxed }&amp;gt;(dst, val),Acquire =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Acquire }&amp;gt;(dst, val),Release =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Release }&amp;gt;(dst, val),AcqRel =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::AcqRel }&amp;gt;(dst, val),SeqCst =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::SeqCst }&amp;gt;(dst, val),}}}&lt;/quote&gt;
    &lt;p&gt;But what is this &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt; function? If you look at its
definition,
you find something slightly unusual:&lt;/p&gt;
    &lt;quote&gt;/// Maximum with the current value using an unsigned comparison./// `T` must be an unsigned integer type.////// The stabilized version of this intrinsic is available on the/// [`atomic`] unsigned integer types via the `fetch_max` method. For example, [`AtomicU32::fetch_max`].#[rustc_intrinsic]#[rustc_nounwind]pub unsafe fn atomic_umax&amp;lt;T: Copy, const ORD: AtomicOrdering&amp;gt;(dst: *mut T, src: T) -&amp;gt; T;&lt;/quote&gt;
    &lt;p&gt;There is no body. This is a declaration, not a definition. The &lt;code&gt;#[rustc_intrinsic]&lt;/code&gt; attribute tells the Rust compiler that this function
maps directly to a low-level operation understood by the compiler
itself. When the Rust compiler sees a call to &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt;, it
knows to
replace it
with the corresponding
LLVM intrinsic function.&lt;/p&gt;
    &lt;p&gt;So our journey actually looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;fetch_max&lt;/code&gt;method (user-facing API)&lt;/item&gt;
      &lt;item&gt;Macro expands to call &lt;code&gt;atomic_umax&lt;/code&gt;function&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;atomic_umax&lt;/code&gt;is a compiler intrinsic&lt;/item&gt;
      &lt;item&gt;Rustc replaces the intrinsic with LLVM's &lt;code&gt;atomicrmw umax&lt;/code&gt;← We are here&lt;/item&gt;
      &lt;item&gt;LLVM processes this instruction...&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Layer 4: The Transformation&lt;/head&gt;
    &lt;p&gt;LLVM runs a series of "passes" that analyze and transform the code. The one we're interested in is called the &lt;code&gt;AtomicExpandPass&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Its job is to look at high-level atomic operations like &lt;code&gt;atomicrmw umax&lt;/code&gt; and ask
the target architecture, "Can you do this natively?"&lt;/p&gt;
    &lt;p&gt;When the &lt;code&gt;x86-64&lt;/code&gt; backend says "No, I can't," this pass expands the single
instruction into a sequence of more fundamental ones that the CPU does
understand. The result is a
compare-and-swap (CAS) loop.&lt;/p&gt;
    &lt;p&gt;We can see this transformation in action by asking LLVM to emit the intermediate representation before and after this pass. To see the IR before the &lt;code&gt;AtomicExpandPass&lt;/code&gt;, run:&lt;/p&gt;
    &lt;quote&gt;llc -print-before=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;Tip: If you do not have&lt;/p&gt;&lt;code&gt;llc&lt;/code&gt;installed, you can ask&lt;code&gt;rustc&lt;/code&gt;to run the pass for you directly.&lt;code&gt;rustc -C llvm-args="-print-before=atomic-expand -print-after=atomic-expand" main.rs&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;The code will be printed to your terminal. The function containing our atomic max looks like this:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump Before Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = atomicrmw umax ptr %self, i64 %val monotonic, align 8store i64 %2, ptr %_0, align 8br label %bb1bb5: ; preds = %start%3 = atomicrmw umax ptr %self, i64 %val release, align 8store i64 %3, ptr %_0, align 8br label %bb1bb6: ; preds = %start%4 = atomicrmw umax ptr %self, i64 %val acquire, align 8store i64 %4, ptr %_0, align 8br label %bb1bb4: ; preds = %start%5 = atomicrmw umax ptr %self, i64 %val acq_rel, align 8store i64 %5, ptr %_0, align 8br label %bb1bb3: ; preds = %start%6 = atomicrmw umax ptr %self, i64 %val seq_cst, align 8store i64 %6, ptr %_0, align 8br label %bb1bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;You can see the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction in multiple places, depending on
the memory ordering specified. This is the high-level atomic operation that the
compiler backend understands, but the CPU does not.&lt;/p&gt;
    &lt;quote&gt;llc -print-after=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;p&gt;This is the relevant part of the output:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump After Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = load i64, ptr %self, align 8 ; seed expected valuebr label %atomicrmw.start ; enter CAS loopatomicrmw.start: ; preds = %atomicrmw.start, %bb7%loaded = phi i64 [ %2, %bb7 ], [ %newloaded, %atomicrmw.start ] ; on first iteration: use %2, on retries: use value observed by last cmpxchg%3 = icmp ugt i64 %loaded, %val ; unsigned compare (umax semantics)%new = select i1 %3, i64 %loaded, i64 %val ; desired = max(loaded, val)%4 = cmpxchg ptr %self, i64 %loaded, i64 %new monotonic monotonic, align 8 ; CAS: if *self==loaded, store new%success = extractvalue { i64, i1 } %4, 1 ; boolean: whether the swap happened%newloaded = extractvalue { i64, i1 } %4, 0 ; value seen in memory before the CASbr i1 %success, label %atomicrmw.end, label %atomicrmw.start ; loop until CAS succeedsatomicrmw.end: ; preds = %atomicrmw.startstore i64 %newloaded, ptr %_0, align 8br label %bb1[... MORE OF THE SAME, JUST FOR DIFFERENT ORDERING..]bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;We can see the pass did not change the first part - it still has the code to dispatch based on the memory ordering. But in the &lt;code&gt;bb7&lt;/code&gt; block, where we originally had the
&lt;code&gt;atomicrmw umax&lt;/code&gt; LLVM instruction, we now see a full compare-and-swap loop.
A compiler engineer would say that the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction has been
"lowered" into a sequence of more primitive operations, that are closer to what
the hardware can actually execute.&lt;/p&gt;
    &lt;p&gt;Here's the simplified logic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read (seed): grab the current value (&lt;code&gt;expected&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Compute: &lt;code&gt;desired = umax(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Attempt: &lt;code&gt;observed, success = cmpxchg(ptr, expected, desired, [...])&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If success, return &lt;code&gt;observed&lt;/code&gt;(the old value). Otherwise&lt;code&gt;set expected = observed&lt;/code&gt;and loop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This CAS loop is a fundamental pattern in lock-free programming. The compiler just built it for us automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 5: The Final Product (x86-64 Assembly)&lt;/head&gt;
    &lt;p&gt;We're at the final step. To see the final machine code, you can tell &lt;code&gt;rustc&lt;/code&gt; to
emit the assembly directly:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=asm main.rs&lt;/quote&gt;
    &lt;p&gt;This will produce a &lt;code&gt;main.s&lt;/code&gt; file containing the final assembly code.
Inside, you'll find the result of the &lt;code&gt;cmpxchg&lt;/code&gt; loop:&lt;/p&gt;
    &lt;quote&gt;.LBB8_2:movq -32(%rsp), %rax # rax = &amp;amp;selfmovq (%rax), %rax # rax = *self (seed 'expected')movq %rax, -48(%rsp) # spill expected to stack.LBB8_3: # loop headmovq -48(%rsp), %rax # rax = expectedmovq -32(%rsp), %rcx # rcx = &amp;amp;selfmovq -40(%rsp), %rdx # rdx = valmovq %rax, %rsi # rsi = expected (scratch)subq %rdx, %rsi # set flags for unsigned compare: expected - valcmovaq %rax, %rdx # if (expected &amp;gt; val) rdx = expected; else rdx = val (compute max)lock cmpxchgq %rdx, (%rcx)# CAS: if *rcx==rax then *rcx=rdx; rax &amp;lt;- old *rcx; ZF=successsete %cl # cl = successmovq %rax, -56(%rsp) # spill observed to stacktestb $1, %cl # branch on successmovq %rax, -48(%rsp) # expected = observed (for retry)jne .LBB8_4 # success -&amp;gt; exitjmp .LBB8_3 # failure → retry&lt;/quote&gt;
    &lt;p&gt;The syntax might look a bit different from what you're used to, that's because it's in AT&amp;amp;T syntax, which is the default for &lt;code&gt;rustc&lt;/code&gt;. If you prefer Intel syntax, you can
use &lt;code&gt;rustc --emit=asm main.rs -C "llvm-args=-x86-asm-syntax=intel"&lt;/code&gt; to get that.&lt;/p&gt;
    &lt;p&gt;I'm not an assembly expert, but you can see the key parts of the CAS loop here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seed read (first iteration): Load &lt;code&gt;*self&lt;/code&gt;once to initialize the expected value.&lt;/item&gt;
      &lt;item&gt;Compute umax without branching: The pair &lt;code&gt;sub&lt;/code&gt;+&lt;code&gt;cmova&lt;/code&gt;implements&lt;code&gt;desired = max_u(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CAS operation: On x86-64, &lt;code&gt;cmpxchg&lt;/code&gt;uses&lt;code&gt;RAX&lt;/code&gt;as the expected value and returns the observed value in&lt;code&gt;RAX&lt;/code&gt;;&lt;code&gt;ZF&lt;/code&gt;encodes success.&lt;/item&gt;
      &lt;item&gt;Retry or finish: If &lt;code&gt;ZF&lt;/code&gt;is clear, we failed and need to retry. Otherwise, we are done.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Note we did not ask&lt;/p&gt;&lt;code&gt;rustc&lt;/code&gt;to optimize the code. If we did, the compiler would generate more efficient assembly: No spills to the stack, fewer jumps, no dispatch on memory ordering, etc. But I wanted to keep the output as close to the original IR as possible to make it easier to follow.&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Beauty of Abstraction&lt;/head&gt;
    &lt;p&gt;And there we have it. Our journey is complete. We started with a safe, clear, single line of Rust and ended with a CAS loop written in assembly language.&lt;/p&gt;
    &lt;p&gt;Rust &lt;code&gt;fetch_max&lt;/code&gt; → Macro-generated &lt;code&gt;atomic_umax&lt;/code&gt; → LLVM
&lt;code&gt;atomicrmw umax&lt;/code&gt; → LLVM &lt;code&gt;cmpxchg&lt;/code&gt; loop → Assembly &lt;code&gt;lock cmpxchg&lt;/code&gt; loop&lt;/p&gt;
    &lt;p&gt;This journey is a perfect example of the power of modern compilers. We get to work at a high level of abstraction, focusing on safety and logic, while the compiler handles the messy, error-prone, and incredibly complex task of generating correct and efficient code for the hardware.&lt;/p&gt;
    &lt;p&gt;So, next time you use an atomic, take a moment to appreciate the incredible, hidden journey your code is about to take.&lt;/p&gt;
    &lt;p&gt;PS: After conducting this journey I learned that C++26 adds &lt;code&gt;fetch_max&lt;/code&gt;
too!&lt;/p&gt;
    &lt;p&gt;PPS: We are hiring!&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: Apple Silicon (AArch64)&lt;/head&gt;
    &lt;p&gt;Out of curiosity, I also checked how this looks on Apple Silicon (AArch64). This architecture does have a native &lt;code&gt;atomic max&lt;/code&gt; instruction, so the
&lt;code&gt;AtomicExpandPass&lt;/code&gt; does not need to lower it into a CAS loop. The LLVM code before and after
the pass is identical, still containing the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;The final assembly contains a variant of the &lt;code&gt;LDUMAX&lt;/code&gt; instruction. This is the relevant part of the assembly:&lt;/p&gt;
    &lt;quote&gt;ldr x8, [sp, #16] # x8 = value to compare withldr x9, [sp, #8] # x9 = pointer to the atomic variableldumax x8, x8, [x9] # atomic unsigned max (relaxed), [x9] = max(x8, [x9]), x8 = old valuestr x8, [sp, #40] # Store old valueb LBB8_11&lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that AArch64 uses Unified Assembler Language, when reading the snippet above, it's important to remember that the destination register comes first.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's really it. We could continue to dig into the microarchitecture, to see how instructions are executed at the hardware level, what are the effects of the &lt;code&gt;LOCK&lt;/code&gt; prefix, dive into differences in memory ordering, etc.
But we'll leave that for another day.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Alice: "Would you tell me, please, which way I ought to go from here?"&lt;/p&gt;&lt;lb/&gt;The Cat: "That depends a good deal on where you want to get to."&lt;lb/&gt;Alice: "I don't much care where."&lt;lb/&gt;The Cat: "Then it doesn't much matter which way you go."&lt;lb/&gt;Alice: "...So long as I get somewhere."&lt;lb/&gt;The Cat: "Oh, you're sure to do that, if only you walk long enough."&lt;p&gt;- Lewis Carroll, Alice's Adventures in Wonderland&lt;/p&gt;&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://questdb.com/blog/rust-fetch-max-compiler-journey/"/><published>2025-09-23T21:24:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354262</id><title>NYC Telecom Raid: What's Up with Those Weird SIM Banks?</title><updated>2025-09-24T05:39:47.747990+00:00</updated><content>&lt;doc fingerprint="26218114519a236e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SIMmetry&lt;/head&gt;
    &lt;head rend="h2"&gt;A recent Secret Service raid uncovers an insane network of SIM cards—along with perhaps the most unusual piece of hardware I’ve ever seen. Here’s the deal with the SIM bank.&lt;/head&gt;
    &lt;p&gt;When I learned that the Secret Service had taken down a giant “SIM farm” in the NYC area, I immediately had two thoughts: One, “Wow, that sounds like the reason we all get so many scam calls.” And two, “Holy crap, what is that weird-ass piece of hardware?!?!??!?!??!?!??!?!?”&lt;/p&gt;
    &lt;p&gt;You must understand, dear reader, the bizarre gear they were using. I’ve never seen anything like it before.&lt;/p&gt;
    &lt;p&gt;Much will be written about the threat to the telecom system, which is the angle the Secret Service is taking, as it was uncovered right around the time of a United Nations General Assembly meeting. I want to know the deal with the hardware itself.&lt;/p&gt;
    &lt;p&gt;You know the old board game Guess Who? You know, with the cards that stick up, and the other player has to guess what faces you have? Imagine that times 100, but with the cards a 20th of the size of the Guess Who cards, and add a whole freaking ton of antennas into the mix, and you have this crazy-ass device, the niche-iest of niche electronic devices. Each device holds numerous SIM cards, which means that someone had to pop out thousands of SIMs to put in these boxes, presumably one at a time.&lt;/p&gt;
    &lt;p&gt;Fortunately for us, the U.S. Secret Service gave us a picture of that insanity, too:&lt;/p&gt;
    &lt;p&gt;So basically, we have a device that is intended to hold literal hundreds of SIM cards, and apparently the people who ran this network had literal racks of these machines. They have this almost magical sense of symmetry to them, which makes them highly attractive to nerds like me. It reminds me of Aereo, the noble (but failed) attempt to use thousands of tiny antennas to capture broadcast television signals to resell online.&lt;/p&gt;
    &lt;head rend="h5"&gt;Sponsored By … You?&lt;/head&gt;
    &lt;p&gt;If you find weird or unusual topics like this super-fascinating, the best way to tell us is to give us a nod on Ko-Fi. It helps ensure that we can keep this machine moving, support outside writers, and bring on the tools to support our writing. (Also it’s heartening when someone chips in.)&lt;/p&gt;
    &lt;p&gt;We accept advertising, too! Check out this page to learn more.&lt;/p&gt;
    &lt;p&gt;So, what the heck is this thing, why did they have so many of them, and how come you’ve never seen them before?&lt;/p&gt;
    &lt;p&gt;The short answer: It’s a device called a “SIMbank” or “SIM gateway,” often attached to a “SIM pool,” which gives all those SIM cards access to a cellular network.&lt;/p&gt;
    &lt;p&gt;The longer answer: The devices in the Secret Service photo, apparently made by a Chinese company called Ejoin Technology, are used in VoIP settings to handle lots of SIM cards. Ejoin says they produce the devices for what it calls “SMS and voice gateway solutions.” In other words, these boxes made it possible to mass-text and mass-call people. They are not cheap devices, costing in the thousands of dollars. And that’s before you get in the business of purchasing all those SIM cards.&lt;/p&gt;
    &lt;p&gt;The exact devices that the Secret Service found are sold by Ejoin for an eye-watering $3,730. Here’s a press image of one:&lt;/p&gt;
    &lt;p&gt;With devices like these, you can text someone at one number and immediately switch to another using the same cellular line, as if you changed area codes on the fly. Which sounds great for marketing, but also great for spam, and even better for harassment.&lt;/p&gt;
    &lt;p&gt;(It should be noted that Ejoin is not alone in selling these. I also spotted them being sold by Etross Telecom, OpenVox, and China Skyline Telecom. These are defiantly obscure but presumably have a use case.)&lt;/p&gt;
    &lt;p&gt;If you think these devices seem sketchy, apparently Alibaba does as well. If you look up messages on Alibaba for Ejoin Technology’s products, you get a generic logo, and this message that appears:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Due to the website’s compliance with specific regulations or policies in China, product information is no longer publicly displayed, but purchasing or payment operations can still be carried out. If you require detailed product information or link, please contact the sales department OR move to Ejointech offical Website.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So, if you buy these objects via Alibaba, you are literally buying a $3,700 device from a black box. On the plus side, going to Ejoin’s website, you can actually see screenshots of the tech in action:&lt;/p&gt;
    &lt;p&gt;In this context, these are basically spam machines, and whoever ran this network—whether a state actor or a criminal scheme—had dozens of them, each costing the price of a high-end laptop. The SIM cards themselves probably cost like $5-$10 a piece, maybe more, which means that just filling them up with cards likely cost thousands more. Plus, there’s the manual labor of it all. 256 SIM cards don’t put themselves into a SIMbank.&lt;/p&gt;
    &lt;p&gt;(Side note: When I searched for information on how to buy bulk SIM cards, one of the first sites that came up was a black-hat hacking forum in which a user asked the very same question. Which, to my friends in the black-hat hacking world, hello.)&lt;/p&gt;
    &lt;p&gt;Now, to be clear, there are some legitimate reasons for users to have them, particularly for testing and quality assurance across networks. (Say, if you’re concerned that your app might work differently on Verizon than it might on AT&amp;amp;T or T-Mobile, or if you’re doing a lot of edge computing. Perhaps a legitimate VoIP company has a few for whatever reason.) And I did find a user on Medium who posted why they built a SIM bank solution for their marketing team. But illegitimate use cases appear to dwarf the legitimate ones, at least in terms of public attention.&lt;/p&gt;
    &lt;p&gt;The case in New York is far from unique. Earlier this year, Interpol broke up a SIM bank fraud scheme in South Africa that involved 40 people and more than 1,000 cards. The cards were used to reroute international traffic as local traffic to make the calls look legitimate. And a spate of cases both targeting and based in India have emerged in recent months.&lt;/p&gt;
    &lt;p&gt;(By the way, if you find this topic interesting, you might want to check out the Indian cybersecurity news outlet The 420, which appears to be on top of this.)&lt;/p&gt;
    &lt;p&gt;Beyond the sheer scope of SIM cards that the network had, the fact that the Secret Service uncovered the network around New York City is perhaps the most interesting part. It suggests that we might see more tricks like this in the future.&lt;/p&gt;
    &lt;p&gt;Anyway, if you see one of these boxes lying around somewhere, filled to the brim with SIMs, odds are you might be in the vicinity of something sketchy. (One has to wonder if the rise of eSIMs is designed to make these products obsolete.)&lt;/p&gt;
    &lt;p&gt;As criminal as they might be depending on the situation, they admittedly look cool.&lt;/p&gt;
    &lt;head rend="h5"&gt;SIMless Links&lt;/head&gt;
    &lt;p&gt;RIP Billy Hudson, a co-host of the popular YouTube channel The Game Chasers. He meant a lot to the retro gaming community, and went out amid some very serious health issues. A telling thing about Hudson is that the last video he posted before he died, created immediately after undergoing brain surgery, involved him advising his followers not to fall for crowdfunding scams. He didn’t have to do that; nobody would have blamed him. Yet he did.&lt;/p&gt;
    &lt;p&gt;If you’ve never seen this piece of found media, you’re in for a treat. It’s a video of Elliott Smith performing on Breakfast Time, a bizarre morning show hosted on the original iteration of the FX network. (As the video notes, it was a performance from well before Smith was famous.) After getting peppered with numerous demeaning questions by co-host Tom Bergeron (later of Dancing With The Stars fame), Smith pulls off a performance of “Clementine” that silences the room and presumably made Bergeron rethink his life choices. Oh, there’s a freaking puppet behind him as he’s playing.&lt;/p&gt;
    &lt;p&gt;I don’t know why the German gummy-makers Haribo are making some of the best power banks on the market, but apparently they are—and serious backpackers love them.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;Find this one an interesting read? Share it with a pal! And to anyone with one of these devices: Please don’t spam me, thanks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tedium.co/2025/09/23/secret-service-raid-sim-bank-telecom-hardware/"/><published>2025-09-23T23:36:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354304</id><title>MLB approves robot umpires for 2026 as part of challenge system</title><updated>2025-09-24T05:39:47.640480+00:00</updated><content>&lt;doc fingerprint="a5a0613b892b831e"&gt;
  &lt;main&gt;
    &lt;p&gt;Major League Baseball will implement a challenge system for balls and strikes in the 2026 season after the league's competition committee voted Tuesday to usher in the era of robot umpiring.&lt;/p&gt;
    &lt;p&gt;Following years of testing in the minor leagues, as well as during spring training and at this year's All-Star Game, MLB forged ahead with a system that will give teams two challenges per game.&lt;/p&gt;
    &lt;p&gt;Hitters, pitchers and catchers will be the only ones allowed to trigger the system by tapping their head, and if a challenge is successful -- the pitch will be shown on in-stadium videoboards -- teams will retain it.&lt;/p&gt;
    &lt;p&gt;While the vote in favor of the automated ball-strike challenge system was not unanimous -- some of the four players on the 11-man committee voted no, according to sources -- the vote was a fait accompli, with MLB owners all in favor and in possession of a six-seat majority on the committee.&lt;/p&gt;
    &lt;p&gt;"I commend the Joint Competition Committee for striking the right balance of preserving the integral role of the umpire in the game with the ability to correct a missed call in a high-leverage situation, all while preserving the pace and rhythm of the game," commissioner Rob Manfred said Tuesday in a statement.&lt;/p&gt;
    &lt;p&gt;The ABS system uses similar technology to the line-calling system in tennis, with 12 cameras in each ballpark tracking the ball with a margin of error around one-sixth of an inch. The ABS zone will be a two-dimensional plane in the middle of the plate that spans its full width (17 inches). The zone's top will be 53.5% of a player's height and the bottom 27%.&lt;/p&gt;
    &lt;p&gt;Teams that run out of challenges over the first nine innings will be granted an extra challenge in the 10th inning, while those that still have unused challenges will simply carry them into extras. If a team runs out of challenges in the 10th, it will automatically receive another in the 11th -- a rule that extends for any extra inning.&lt;/p&gt;
    &lt;p&gt;During the league's spring training test this season, teams combined to average around four challenges per game and succeeded 52.2% of the time, according to the league. Catchers, whose value in framing pitches outside the zone to look like strikes could take a hit due to the new rule, were the most successful at a 56% overturn rate, while hitters were correct 50% of the time and pitchers 41%.&lt;/p&gt;
    &lt;p&gt;MLB's minor league testing, which started in 2021, led to Triple-A players in 2023 using ABS challenge three days a week and a full ABS system, with every pitch adjudicated by computer, the other three.&lt;/p&gt;
    &lt;p&gt;Support among league executives grew around the challenge system as the more palatable of the two options for fans, allowing for umpires still to play a role in balls and strikes but to have a backup system in case of blown calls in integral moments.&lt;/p&gt;
    &lt;p&gt;Adding the robot umps is likely to cut down on ejections. MLB said 61.5% of ejections among players, managers and coaches last year were related to balls and strikes, as were 60.3% this season through Sunday. The figures include ejections for derogatory comments, throwing equipment while protesting calls and inappropriate conduct.&lt;/p&gt;
    &lt;p&gt;Big league umpires call roughly 94% of pitches correctly, according to UmpScorecards.&lt;/p&gt;
    &lt;p&gt;Management officials on the competition committee include Seattle chairman John Stanton, St. Louis CEO Bill DeWitt Jr., San Francisco chairman Greg Johnson, Colorado CEO Dick Monfort, Toronto CEO Mark Shapiro and Boston chairman Tom Werner.&lt;/p&gt;
    &lt;p&gt;Players include Arizona's Corbin Burnes and Zac Gallen, Seattle's Cal Raleigh and the New York Yankees' Austin Slater, with the Chicago Cubs' Ian Happ and Detroit's Casey Mize as alternates. The union representatives make their decisions based on input from players on the 30 teams.&lt;/p&gt;
    &lt;p&gt;Bill Miller is the umpire representative.&lt;/p&gt;
    &lt;p&gt;The Associated Press contributed to this report.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.espn.com/mlb/story/_/id/46357017/mlb-approves-robot-umpires-2026-part-challenge-system"/><published>2025-09-23T23:41:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354314</id><title>Top Programming Languages 2025</title><updated>2025-09-24T05:39:47.398815+00:00</updated><content>&lt;doc fingerprint="ff481adb873c88f9"&gt;
  &lt;main&gt;&lt;p&gt;Since 2013, we’ve been metaphorically peering over the shoulders of programmers to create our annual interactive rankings of the most popular programming languages. But fundamental shifts in how people are coding may not just make it harder to measure popularity, but could even make the concept itself irrelevant. And then things might get really weird. To see why, let’s start with this year’s rankings and a quick refresher of how we put this thing together.&lt;/p&gt;&lt;p&gt;In the “Spectrum” default ranking, which is weighted with the interests of IEEE members in mind, we see that once again Python has the top spot, with the biggest change in the top five being JavaScript’s drop from third place last year to sixth place this year. As JavaScript is often used to create web pages, and vibe coding is often used to create websites, this drop in the apparent popularity may be due to the effects of AI that we’ll dig into in a moment. But first to finish up with this year’s scores, in the “Jobs” ranking, which looks exclusively at what skills employers are looking for, we see that Python has also taken 1st place, up from second place last year, though SQL expertise remains an incredibly valuable skill to have on your resume.&lt;/p&gt;&lt;p&gt;Because we can’t literally look over the shoulders of everyone who codes, including kids hacking on Minecraft servers or academic researchers developing new architectures, we rely on proxies to measure popularity. We detail our methodology here, but the upshot is that we merge metrics from multiple sources to create our rankings. The metrics we choose publicly signal interest across a wide range of languages—Google search traffic, questions asked on Stack Exchange, mentions in research papers, activity on the GitHub open source code repository, and so on.&lt;/p&gt;&lt;p&gt;But programmers are turning away from many of these public expressions of interest. Rather than page through a book or search a website like Stack Exchange for answers to their questions, they’ll chat with an LLM like Claude or ChatGPT in a private conversation. And with an AI assistant like Cursor helping to write code, the need to pose questions in the first place is significantly decreased. For example, across the total set of languages evaluated in the TPL, the number of questions we saw posted per week on Stack Exchange in 2025 was just 22 percent of what it was in 2024.&lt;/p&gt;&lt;p&gt;With less signal in publicly available metrics, it becomes harder to track popularity across a broad range of languages. This existential problem for our rankings can be tackled by searching for new metrics, or trying to survey programmers—in all their variety—directly. However, an even more fundamental problem is looming in the wings.&lt;/p&gt;&lt;p&gt;Whether it’s a seasoned coder using an AI to handle the grunt work, or a neophyte vibe coding a complete web app, AI assistance means that programmers can concern themselves less and less with the particulars of any language. First details of syntax, then flow control and functions, and so on up the levels of how a program is put together—more and more is being left to the AI.&lt;/p&gt;&lt;p&gt;Although code-writing LLM’s are still very much a work in progress, as they take over an increasing share of the work, programmers inevitably shift from being the kind of people willing to fight religious wars over whether source code should be indented by typing tabs or spaces to people who care less and less about what language is used.&lt;/p&gt;&lt;p&gt;After all, the whole reason different computer languages exist is because given a particular challenge, it’s easier to express a solution in one language versus another. You wouldn’t control a washing machine using the R programming language, or conversely do a statistical analysis on large datasets using C.&lt;/p&gt;&lt;p&gt;But it is technically possible to do both. A human might tear their hair out doing it, but LLMs have about as much hair as they do sentience. As long as there’s enough training data, they’ll generate code for a given prompt in any language you want. In practical terms, this means using one—any one—of today’s most popular general purpose programming languages. In the same way most developers today don’t pay much attention to the instruction sets and other hardware idiosyncrasies of the CPUs that their code runs on, which language a program is vibe coded in ultimately becomes a minor detail.&lt;/p&gt;&lt;p&gt;Sure, there will always be some people who care, just as today there are nerds like me willing to debate the merits of writing for the Z80 versus the 6502 8-bit CPUs. But overall, the popularity of different computer languages could become as obscure a topic as the relative popularity of railway track gauges.&lt;/p&gt;&lt;p&gt;One obvious long-term consequence to this is that it will become harder for new languages to emerge. Previously, new languages could emerge from individuals or small teams evangelizing their approach to potential contributors and users. Presentations, papers, demos, sample code and tutorials seeded new developer ecosystems. A single well-written book, like Leo Brodie’s Starting Forth or Brian Kernighan and Dennis Ritchies’ The C Programming Language, could make an enormous difference to a language’s popularity.&lt;/p&gt;&lt;p&gt;But while a few samples and a tutorial can be enough material to jump-start adoption among programmers familiar with the ins and outs of hands-on coding, it’s not enough for today’s AIs. Humans build mental models that can extrapolate from relatively small amounts of data. LLMs rely on statistical probabilities, so the more data they can crunch, they better they are. Consequently programmers have noted that AIs give noticeably poorer results when trying to code in less-used languages.&lt;/p&gt;&lt;p&gt;There are research efforts to make LLMs more universal coders, but that doesn’t really help new languages get off the ground. Fundamentally new languages grow because they are scratching some itch a programmer has. That itch can be as small as being annoyed at semicolons having to be placed after every statement, or as large as a philosophical argument about the purpose of computation.&lt;/p&gt;&lt;p&gt;But if an AI is soothing our irritations with today’s languages, will any new ones ever reach the kind of critical mass needed to make an impact? Will the popularity of today’s languages remain frozen in time?&lt;/p&gt;&lt;head rend="h2"&gt;What’s the future of programming languages?&lt;/head&gt;&lt;p&gt;Before speculating further about the future, let’s touch base again where we are today. Modern high-level computer languages are really designed to do two things: create an abstraction layer that makes it easier to process data in a suitable fashion, and stop programmers from shooting themselves in the foot.&lt;/p&gt;&lt;p&gt;The first objective has been around since the days of Fortran and Cobol, aimed at processing scientific and business data respectively. The second objective emerged later, spurred in no small part by Edgar Dijkstra’s 1968 paper “Go To Statement Considered Harmful.” In this he argued for eliminating the ability for a programmer to make jumps to arbitrary points in their code. This restriction was to prevent so-called spaghetti code that makes it hard for a programmer to understand how a computer actually executes a given program. Instead, Dijkstra demanded that programmers bend to structural rules imposed by the language. Dijkstra’s argument ultimately won the day, and most modern languages do indeed minimize or eliminate Go Tos altogether in favor of structures like functions and other programmatic blocks.&lt;/p&gt;&lt;p&gt;These structures don’t exist at the level of the CPU. If you look at the instruction sets for Arm, x86, or RISC-V processors, the flow of a program is controlled by just three types of machine code instructions. These are conditional jumps, unconditional jumps, and jumps with a trace stored (so you can call a subroutine and return to where you started). In other words, it’s Go Tos all the way down. Similarly, strict data types designed to label and protect data from incorrect use dissolve into anonymous bits flowing in and out of memory.&lt;/p&gt;&lt;p&gt;So how much abstraction and anti-foot-shooting structure will a sufficiently-advanced coding AI really need? A hint comes from recent research in AI-assisted hardware design, such as Dall-EM, a generative AI developed at Princeton University used to create RF and electromagnetic filters. Designing these filters has always been something of a black art, involving the wrangling of complex electromagnetic fields as they swirl around little strips of metal. But Dall-EM can take in the desired inputs and outputs and spit out something that looks like a QR code. The results are something no human would ever design—but it works.&lt;/p&gt;&lt;p&gt;Similarly, could we get our AIs to go straight from prompt to an intermediate language that could be fed into the interpreter or compiler of our choice? Do we need high-level languages at all in that future? True, this would turn programs into inscrutable black boxes, but they could still be divided into modular testable units for sanity and quality checks. And instead of trying to read or maintain source code, programmers would just tweak their prompts and generate software afresh.&lt;/p&gt;&lt;p&gt;What’s the role of the programmer in a future without source code? Architecture design and algorithm selection would remain vital skills—for example, should a pathfinding program use a classic approach like the A* algorithm, or instead should it try to implement a new method? How should a piece of software be interfaced with a larger system? How should new hardware be exploited? In this scenario, computer science degrees, with their emphasis on fundamentals over the details of programming languages, rise in value over coding boot camps.&lt;/p&gt;Will there be a Top Programming Language in 2026? Right now, programming is going through the biggest transformation since compilers broke onto the scene in the early 1950s. Even if the predictions that much of AI is a bubble about to burst come true, the thing about tech bubbles is that there’s always some residual technology that survives. It’s likely that using LLMs to write and assist with code is something that’s going to stick. So we’re going to be spending the next 12 months figuring out what popularity means in this new age, and what metrics might be useful to measure. What do you think popularity should mean? What metrics do you think we should consider? Let us know in the comments below.&lt;list rend="ul"&gt;&lt;item&gt;AI Models Embrace Humanlike Reasoning ›&lt;/item&gt;&lt;item&gt;LLM Benchmarking Shows Capabilities Doubling Every 7 Months ›&lt;/item&gt;&lt;item&gt;Why Functional Programming Should Be the Future of Software Development ›&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Stephen Cass is the special projects editor at IEEE Spectrum. He currently helms Spectrum's Hands On column, and is also responsible for interactive projects such as the Top Programming Languages app. He has a bachelor's degree in experimental physics from Trinity College Dublin.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/top-programming-languages-2025"/><published>2025-09-23T23:42:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354644</id><title>Baldur's Gate 3 Steam Deck – Native Version</title><updated>2025-09-24T05:39:46.477034+00:00</updated><content>&lt;doc fingerprint="bcd4a8bd8df387c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Steam Deck - Native Version&lt;/head&gt;
    &lt;p&gt;Upon release of Hotfix #34 on your Steam Deck, your device will install the Native version.&lt;/p&gt;
    &lt;p&gt;If you are unsure whether the build has been installed correctly, you can do the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any version that has Linux Runtime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update if an update appears.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s the difference between the Steam Deck Native and Proton version?&lt;/p&gt;
    &lt;p&gt;Our Proton version runs on the Steam Deck via the Proton compatibility layer, which requires extra CPU processing power. Running the game natively on the Steam Deck requires less CPU usage and memory consumption overall!&lt;/p&gt;
    &lt;p&gt;Can I still switch back to the Proton version?&lt;/p&gt;
    &lt;p&gt;Yes. If you’re having issues with the Steam Deck Native build, you can revert to the Proton version. Take the following steps to do so:&lt;/p&gt;
    &lt;p&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any Proton version 8 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that there is a Steam Deck Native build, is Baldur’s Gate 3 supported on Linux?&lt;/p&gt;
    &lt;p&gt;Larian does not provide support for the Linux platform. The Steam Deck Native build is only supported on Steam Deck.&lt;/p&gt;
    &lt;head rend="h2"&gt;Savegames&lt;/head&gt;
    &lt;p&gt;Where are my saves located currently (before using the Steam Deck Native version)?&lt;/p&gt;
    &lt;p&gt;Before the Steam Deck Native version becomes the primary version, your saves will be in the compatdata folder: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Where are my saves located when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;After the Steam Deck Native version becomes the primary version, your saves will be in the following folder: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Why are my saves in different folders?&lt;/p&gt;
    &lt;p&gt;When Baldur’s Gate 3 runs on the Proton compatibility layer, the Proton version will store the saves in the compatdata folder, which is a mirrored version of the Windows file storage system. On the Steam Deck Native version, the saves are stored natively on the SteamOS file storage system.&lt;/p&gt;
    &lt;p&gt;Will my savegames be transferred over to the new version when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;If your Steam Cloud saves are turned on, your most recent saves will be synced to the Steam Deck Native savegame folder automatically.&lt;/p&gt;
    &lt;p&gt;What if I don’t have Cloud saves turned on, or I want my older saves?&lt;/p&gt;
    &lt;p&gt;Your saves are still stored on the Steam Deck, but they will be stored in the compatdata folder.&lt;lb/&gt; You can manually transfer these files via the Desktop:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First, switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have a mouse and keyboard to hand, plug them in to make your life a little easier, and click on the folder icon on the bar at the bottom.&lt;/item&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Copy the Savegames folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt; Will my old saves still take up storage space on my Steam Deck?&lt;/p&gt;
    &lt;p&gt;Yes, your old saves will still take up storage space. If you want to save some space and you don't plan on using the Proton version, you can delete the compatdata folder after you've copied over the folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mods&lt;/head&gt;
    &lt;p&gt;Will my mods be transferred over automatically?&lt;/p&gt;
    &lt;p&gt;If you are logged into your Larian Account and have it connected to mod.io, all mods you are subscribed to will be downloaded when the transition to Steam Deck Native occurs.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; What if I’m not logged into a Larian Account or connected to mod.io?&lt;/p&gt;
    &lt;p&gt;You can either manually download the mods from the Mod Manager or transfer them manually from the previous folder.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To do so,switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Click on the folder icon on the bar at the bottom.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3&lt;/item&gt;
      &lt;item&gt;Copy the Mods folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://larian.com/support/faqs/steam-deck-native-version_121"/><published>2025-09-24T00:26:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354689</id><title>Periodic Table of Cognition</title><updated>2025-09-24T05:39:46.288122+00:00</updated><content>&lt;doc fingerprint="ff34989f0b052f5d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The Periodic Table of Cognition&lt;/head&gt;
    &lt;p&gt;Iâve been studying the early history of electricityâs discovery as a map for our current discovery of artificial intelligence. The smartest people alive back then, including Isaac Newton, who may have been the smartest person who ever lived, had confident theories about electricityâs nature that were profoundly wrong. In fact, despite the essential role of electrical charges in the universe, everyone who worked on this fundamental force was profoundly wrong for a long time. All the pioneers of electricity â such as Franklin, Wheatstone, Faraday, and Maxwell â had a few correct ideas of their own (not shared by all) mixed in with notions that mostly turned out to be flat out misguided. Most of the discoveries about what electricity could do happened without the knowledge of how they worked. That ignorance, of course, drastically slowed down the advances in electrical inventions.&lt;/p&gt;
    &lt;p&gt;In a similar way, the smartest people today, especially all the geniuses creating artificial intelligence, have theories about what intelligence is, and I believe all of them (me too) will be profoundly wrong. We donât know what artificial intelligence is in large part because we donât know what our own intelligence is. And this ignorance will later be seen as an impediment to the rate of progress in AI.&lt;/p&gt;
    &lt;p&gt;A major part of our ignorance stems from our confusion about the general category of either electricity or intelligence. We tend to view both electricity and intelligence as coherent elemental forces along a single dimension: you either have more of it or less. But in fact, electricity turned out to be so complicated, so complex, so full of counterintuitive effects that even today it is still hard to grasp how it works. It has particles and waves, and fields and flows, composed of things that are not really there. Our employment of electricity exceeds our understanding of it. Understanding electricity was essential to understanding matter. It wasnât until we learned to control electricity that we were able to split water â which had been considered an element â into its actual elements; that enlightened us that water was not a foundational element, but a derivative compound made up of sub elements.&lt;/p&gt;
    &lt;p&gt;It is very probable we will discover that intelligence is likewise not a foundational singular element, but a derivative compound composed of multiple cognitive elements, combined in a complex system unique to each species of mind. The result that we call intelligence emerges from many different cognitive primitives such as long-term memory, spatial awareness, logical deduction, advance planning, pattern perception, and so on. There may be dozens of them, or hundreds. We currently donât have any idea of what these elements are. We lack a periodic table of cognition.&lt;/p&gt;
    &lt;p&gt;The cognitive elements will more resemble the heavier elements in being unstable and dynamic. Or a better analogy would be to the elements in a biological cell. The primitives of cognition are flow states that appear in a thought cycle. They are like molecules in a cell which are in constant flux, shifting from one shape to another. Their molecular identity is related to their actions and interactions with other molecules. Thinking is a collective action that happens in time (like temperature in matter) and every mode can only be seen in relation to the other modes before and after it. It is a network phenomenon that makes it difficult to identify its borders. So each element of intelligence is embedded in a thought cycle, and requires the other elements as part of its identity. So each cognitive element is described in context of the other cognitive modes adjacent to it.&lt;/p&gt;
    &lt;p&gt;I asked ChatGPT5Pro to help me generate a periodic table of cognition given what we collectively know so far. It suggests 49 elements, arranged in a table so that related concepts are adjacent. The columns are families, or general categories of cognition such as âPerceptionâ, âReasoningâ, âLearningâ, so all the types of perception or reasoning are stacked in one column. The rows are sorted by stages in a cycle of thought. The earlier stages (such as âsensingâ) are at the top, while later stages in the cycle (such as âreflect &amp;amp; alignâ) are at the bottom. So for example, in the family or category of âSafetyâ the AIs will tend to do the estimation of uncertainty first, later do verification, and only get to a theory of mind at the end.&lt;/p&gt;
    &lt;p&gt;The chart is colored according to how much progress weâve made on each element. Red indicates we can synthesize that element in a robust way. Orange means we can kind of make it work with the right scaffolding. Yellow reflects promising research without operational generality yet.&lt;/p&gt;
    &lt;p&gt;I suspect many of these elements are not as distinct as shown here (taxonomically I am more of a lumper than a splitter), and I would expect this collection omits many types we are soon to discover, but as a start, this prototype chart serves its purpose: it reveals the complexity of intelligence. It is clear intelligence is compounded along multiple dimensions. We will engineer different AIs to have different combinations of different elements in different strengths. This will produce thousands of types of possible minds. We can see that even today different animals have their own combination of cognitive primitives, arranged in a pattern unique to their speciesâ needs. In some animals some of the elements â say long-term memory â may exceed our own in strength; of course they lack some elements we have.&lt;/p&gt;
    &lt;p&gt;With the help of AI, we are discovering what these elements of cognition are. Each advance illuminates a bit of how minds work and what is needed to achieve results. If the discovery of electricity and atoms has anything to teach us now, it is that we are probably very far from having discovered the complete set of cognitive elements. Instead we are at the stage of believing in ethers, instantaneous action, and phlogiston â a few of the incorrect theories of electricity the brightest scientists believed.&lt;/p&gt;
    &lt;p&gt;Almost no thinker, researcher, experimenter, or scientist at that time could see the true nature of electricity, electromagnetism, radiation and subatomic particles, because the whole picture was hugely unintuitive. Waves, force fields, particles of atoms did not make sense (and still does not make common sense). It required sophisticated mathematics to truly comprehend it, and even after Maxwell described it mathematically, he found it hard to visualize.&lt;/p&gt;
    &lt;p&gt;I expect the same from intelligence. Even after we identify its ingredients, the emergent properties they generate are likely to be obscure and hard to believe, hard to visualize. Intelligence is unlikely to make common sense.&lt;/p&gt;
    &lt;p&gt;A century ago, our use of electricity ran ahead of our understanding of it. We made motors from magnets and coiled wire without understanding why they worked. Theory lagged behind practice. As with electricity, our employment of intelligence exceeds our understanding of it. We are using LLMs to answer questions or to code software without having a theory of intelligence. A real theory of intelligence is so lacking that we donât know how our own minds work, let alone the synthetic ones we can now create.&lt;/p&gt;
    &lt;p&gt;The theory of the atomic world needed the knowledge of the periodic table of elements. You had to know all (or at least most) of the parts to make falsifiable predictions of what would happen. The theory of intelligence requires knowledge of all the elemental parts, which we have only slowly begun to identify, before we can predict what might happen next.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kk.org/thetechnium/the-periodic-table-of-cognition/"/><published>2025-09-24T00:33:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45355462</id><title>Zutty: Zero-cost Unicode Teletype, high-end terminal for low-end systems</title><updated>2025-09-24T05:37:30.265893+00:00</updated><content>&lt;doc fingerprint="62bb15c620e32fef"&gt;
  &lt;main&gt;
    &lt;p&gt;A reference implementation of the sublinear-space ZKP prover/Verifier described in our whitepaper: "Zero-knowledge Proofs in Sublinear Space" (https://arxiv.org/abs/2509.05326). It realizes a streaming prover that uses only O(√T) memory over a trace of length T, while producing standard KZG commitments (BN254) for wires, the permutation accumulator &lt;code&gt;Z&lt;/code&gt;, and the quotient &lt;code&gt;Q&lt;/code&gt;. The design keeps aggregate-only Fiat–Shamir and never materializes whole polynomials.&lt;/p&gt;
    &lt;p&gt;Traditional zk proving pipelines routinely buffer whole polynomials, forcing O(T) memory and large intermediate states. This repository demonstrates a practical alternative:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sublinear space: the active working set stays O(√T) using blocked IFFTs and streaming accumulators.&lt;/item&gt;
      &lt;item&gt;Production-style commitments: standard KZG commitments/openings (pairing-checked) over BN254.&lt;/item&gt;
      &lt;item&gt;No full-poly buffers: wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;are built and opened without holding entire vectors.&lt;/item&gt;
      &lt;item&gt;Deterministic dev SRS: easy to run locally; switch to trusted SRS files for production.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re building scalable zk systems, this repo shows how to restructure your pipeline around streaming and aggregate-only FS without giving up familiar cryptographic backends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCS: KZG over BN254 with a linear interface and a streaming Aggregator.&lt;/item&gt;
      &lt;item&gt;Two commitment bases: commit from evaluation (domain-aligned) or coefficient slices.&lt;/item&gt;
      &lt;item&gt;Openings: real KZG openings for wires/&lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Q&lt;/code&gt;, with consistent witness construction.&lt;/item&gt;
      &lt;item&gt;Domain &amp;amp; transforms: radix-2 blocked IFFT/NTT, barycentric eval for streaming points.&lt;/item&gt;
      &lt;item&gt;AIR &amp;amp; residuals: small fixed-column AIR and permutation-coupled residual stream.&lt;/item&gt;
      &lt;item&gt;Scheduler: five-phase A→E pipeline, aggregate-only Fiat–Shamir, strictly increasing time order.&lt;/item&gt;
      &lt;item&gt;CLI tools: &lt;code&gt;prover&lt;/code&gt;and&lt;code&gt;verifier&lt;/code&gt;plus an end-to-end script.&lt;/item&gt;
      &lt;item&gt;Space profile: peak memory ≈ O(b_blk) with &lt;code&gt;b_blk ≈ √T&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Phase A: (Optional) commit selectors/fixed columns.&lt;/item&gt;
      &lt;item&gt;Phase B: Wires — stream a register’s evaluations block-by-block → blocked IFFT → feed coeff tiles (low→high) into PCS Aggregator.&lt;/item&gt;
      &lt;item&gt;Phase C: Permutation accumulator &lt;code&gt;Z&lt;/code&gt;— stream locals, update&lt;code&gt;Z&lt;/code&gt;on the fly and emit the&lt;code&gt;Z&lt;/code&gt;column in time order, then commit via the same blocked IFFT path.&lt;/item&gt;
      &lt;item&gt;Phase D: Quotient &lt;code&gt;Q&lt;/code&gt;— stream residual&lt;code&gt;R(ω^i)&lt;/code&gt;and convert to&lt;code&gt;Q&lt;/code&gt;coefficients online using&lt;code&gt;Z_H(X)=X^N−c&lt;/code&gt;(no full-poly buffers).&lt;/item&gt;
      &lt;item&gt;Phase E: Openings — produce real KZG openings for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;(witness&lt;code&gt;W = (f−f(ζ))/(X−ζ)&lt;/code&gt;) and verify via pairings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All Fiat–Shamir challenges are replayed by the verifier; pairing checks are always enforced. In dev builds, SRS is deterministic; in production, provide trusted SRS files.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust (stable toolchain)&lt;/item&gt;
      &lt;item&gt;No external SRS required for dev runs (deterministic in-crate SRS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone, then:
cargo build --quiet --bins --features dev-srs

# End-to-end script runs three scenarios + a tamper test
scripts/test_sszkp.sh&lt;/code&gt;
    &lt;p&gt;Expected output (abridged):&lt;/p&gt;
    &lt;code&gt;✔ build succeeded
✔ verification OK for eval-basis wires, b_blk=128, rows=1024
✔ tampered proof correctly rejected
✔ verification OK for coeff-basis wires, b_blk=64, rows=1536
✔ verification OK for eval-basis wires, b_blk=256, rows=2048
==&amp;gt; All tests passed 🎉
&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval
# writes proof.bin&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin verifier -- --rows 1024 --basis eval
# reads proof.bin and verifies&lt;/code&gt;
    &lt;p&gt;In non-dev builds you must provide both G1 and G2 SRS files.&lt;/p&gt;
    &lt;p&gt;Prover:&lt;/p&gt;
    &lt;code&gt;cargo run --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;p&gt;Verifier:&lt;/p&gt;
    &lt;code&gt;cargo run --bin verifier -- --rows 1024 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Format: the SRS files are Arkworks-serialized vectors of affine powers. G1:&lt;/p&gt;&lt;code&gt;[τ^0]G1 … [τ^d]G1&lt;/code&gt;(we use the degree bound you load). G2: a vector containing at least&lt;code&gt;[τ]G2&lt;/code&gt;(we read element 1 or 0).&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--rows &amp;lt;T&amp;gt;&lt;/code&gt;: total rows in the trace (domain size rounds up to power of two).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--b-blk &amp;lt;B&amp;gt;&lt;/code&gt;: block size; pick ≈ √T to achieve the sublinear memory bound.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--k &amp;lt;K&amp;gt;&lt;/code&gt;: number of registers (columns) in the AIR.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--basis &amp;lt;eval|coeff&amp;gt;&lt;/code&gt;: commitment basis for wires (Q is always coeff).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--selectors &amp;lt;FILE&amp;gt;&lt;/code&gt;: optional selectors/fixed columns CSV (rows × S).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--omega &amp;lt;u64&amp;gt;&lt;/code&gt;: override ω (power-of-two order must hold).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--coset &amp;lt;u64&amp;gt;&lt;/code&gt;: reserved; current domain uses subgroup (&lt;code&gt;Z_H(X)=X^N−1&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/pcs.rs&lt;/code&gt;— KZG PCS (BN254), streaming Aggregator, real openings, pairings.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/domain.rs&lt;/code&gt;— domain&lt;code&gt;H&lt;/code&gt;, barycentric weights, blocked NTT/IFFT.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/air.rs&lt;/code&gt;— tiny fixed-column AIR + residual stream + permutation coupling.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/perm_lookup.rs&lt;/code&gt;— permutation accumulator&lt;code&gt;Z&lt;/code&gt;(lookups optional).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/quotient.rs&lt;/code&gt;— streaming quotient builder (R→Q tilewise).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/scheduler.rs&lt;/code&gt;— 5-phase orchestrator (aggregate-only FS, O(√T) space).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/opening.rs&lt;/code&gt;— streaming polynomial evaluation helpers (eval/coeff mode).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/transcript.rs&lt;/code&gt;— domain-separated FS transcript (BLAKE3→field).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/stream.rs&lt;/code&gt;— block partitioning + restreaming interfaces.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bin/prover.rs&lt;/code&gt;,&lt;code&gt;bin/verifier.rs&lt;/code&gt;— CLIs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scripts/test_sszkp.sh&lt;/code&gt;— end-to-end tests + tamper test.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treat the &lt;code&gt;Restreamer&lt;/code&gt;trait as the integration seam: implement it to feed rows from your own storage (disk, network, GPU), all while keeping O(b_blk) memory.&lt;/item&gt;
      &lt;item&gt;Keep your permutation/lookup logic time-ordered; the accumulator state must evolve monotonically in &lt;code&gt;t&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;When committing from evaluations, ensure your blocks align to the domain and use the provided blocked IFFT helpers to produce coeff tiles.&lt;/item&gt;
      &lt;item&gt;For openings, prefer the coeff-stream path; the code adapts eval-streams internally when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pairing checks are always enforced; the verifier replays the FS transcript and checks KZG equalities for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tamper test flips one byte in &lt;code&gt;proof.bin&lt;/code&gt;—verification must fail.&lt;/item&gt;
      &lt;item&gt;Dev SRS exists only for convenience; do not use dev mode in production.&lt;/item&gt;
      &lt;item&gt;Algebraic identity at ζ (gate + perm coupling + boundary = &lt;code&gt;Z_H(ζ)·Q(ζ)&lt;/code&gt;) is implemented; by default, selectors are optional and gates are minimal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AIR is a compact demo; plug in your real selector/table wiring as needed.&lt;/item&gt;
      &lt;item&gt;Lookup accumulator is feature-gated and intentionally minimal (demo path).&lt;/item&gt;
      &lt;item&gt;Only BN254/KZG is shipped; adding Pallas/BLS12-381 is straightforward in this architecture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File issues for bugs or suggestions.&lt;/item&gt;
      &lt;item&gt;PRs welcome—especially alternative domains, SRS loaders, or integration examples.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This codebase follows the aggregate-only Fiat–Shamir and streaming discipline described in the whitepaper and demonstrates that production-style commitments and sublinear space can coexist in a practical Rust implementation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://git.hq.sig7.se/zutty.git"/><published>2025-09-24T02:07:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45355514</id><title>Quadratic memory reductions for Zero-knowledge Proofs</title><updated>2025-09-24T05:37:30.091569+00:00</updated><content>&lt;doc fingerprint="62bb15c620e32fef"&gt;
  &lt;main&gt;
    &lt;p&gt;A reference implementation of the sublinear-space ZKP prover/Verifier described in our whitepaper: "Zero-knowledge Proofs in Sublinear Space" (https://arxiv.org/abs/2509.05326). It realizes a streaming prover that uses only O(√T) memory over a trace of length T, while producing standard KZG commitments (BN254) for wires, the permutation accumulator &lt;code&gt;Z&lt;/code&gt;, and the quotient &lt;code&gt;Q&lt;/code&gt;. The design keeps aggregate-only Fiat–Shamir and never materializes whole polynomials.&lt;/p&gt;
    &lt;p&gt;Traditional zk proving pipelines routinely buffer whole polynomials, forcing O(T) memory and large intermediate states. This repository demonstrates a practical alternative:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sublinear space: the active working set stays O(√T) using blocked IFFTs and streaming accumulators.&lt;/item&gt;
      &lt;item&gt;Production-style commitments: standard KZG commitments/openings (pairing-checked) over BN254.&lt;/item&gt;
      &lt;item&gt;No full-poly buffers: wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;are built and opened without holding entire vectors.&lt;/item&gt;
      &lt;item&gt;Deterministic dev SRS: easy to run locally; switch to trusted SRS files for production.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re building scalable zk systems, this repo shows how to restructure your pipeline around streaming and aggregate-only FS without giving up familiar cryptographic backends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCS: KZG over BN254 with a linear interface and a streaming Aggregator.&lt;/item&gt;
      &lt;item&gt;Two commitment bases: commit from evaluation (domain-aligned) or coefficient slices.&lt;/item&gt;
      &lt;item&gt;Openings: real KZG openings for wires/&lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Q&lt;/code&gt;, with consistent witness construction.&lt;/item&gt;
      &lt;item&gt;Domain &amp;amp; transforms: radix-2 blocked IFFT/NTT, barycentric eval for streaming points.&lt;/item&gt;
      &lt;item&gt;AIR &amp;amp; residuals: small fixed-column AIR and permutation-coupled residual stream.&lt;/item&gt;
      &lt;item&gt;Scheduler: five-phase A→E pipeline, aggregate-only Fiat–Shamir, strictly increasing time order.&lt;/item&gt;
      &lt;item&gt;CLI tools: &lt;code&gt;prover&lt;/code&gt;and&lt;code&gt;verifier&lt;/code&gt;plus an end-to-end script.&lt;/item&gt;
      &lt;item&gt;Space profile: peak memory ≈ O(b_blk) with &lt;code&gt;b_blk ≈ √T&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Phase A: (Optional) commit selectors/fixed columns.&lt;/item&gt;
      &lt;item&gt;Phase B: Wires — stream a register’s evaluations block-by-block → blocked IFFT → feed coeff tiles (low→high) into PCS Aggregator.&lt;/item&gt;
      &lt;item&gt;Phase C: Permutation accumulator &lt;code&gt;Z&lt;/code&gt;— stream locals, update&lt;code&gt;Z&lt;/code&gt;on the fly and emit the&lt;code&gt;Z&lt;/code&gt;column in time order, then commit via the same blocked IFFT path.&lt;/item&gt;
      &lt;item&gt;Phase D: Quotient &lt;code&gt;Q&lt;/code&gt;— stream residual&lt;code&gt;R(ω^i)&lt;/code&gt;and convert to&lt;code&gt;Q&lt;/code&gt;coefficients online using&lt;code&gt;Z_H(X)=X^N−c&lt;/code&gt;(no full-poly buffers).&lt;/item&gt;
      &lt;item&gt;Phase E: Openings — produce real KZG openings for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;(witness&lt;code&gt;W = (f−f(ζ))/(X−ζ)&lt;/code&gt;) and verify via pairings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All Fiat–Shamir challenges are replayed by the verifier; pairing checks are always enforced. In dev builds, SRS is deterministic; in production, provide trusted SRS files.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust (stable toolchain)&lt;/item&gt;
      &lt;item&gt;No external SRS required for dev runs (deterministic in-crate SRS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone, then:
cargo build --quiet --bins --features dev-srs

# End-to-end script runs three scenarios + a tamper test
scripts/test_sszkp.sh&lt;/code&gt;
    &lt;p&gt;Expected output (abridged):&lt;/p&gt;
    &lt;code&gt;✔ build succeeded
✔ verification OK for eval-basis wires, b_blk=128, rows=1024
✔ tampered proof correctly rejected
✔ verification OK for coeff-basis wires, b_blk=64, rows=1536
✔ verification OK for eval-basis wires, b_blk=256, rows=2048
==&amp;gt; All tests passed 🎉
&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval
# writes proof.bin&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin verifier -- --rows 1024 --basis eval
# reads proof.bin and verifies&lt;/code&gt;
    &lt;p&gt;In non-dev builds you must provide both G1 and G2 SRS files.&lt;/p&gt;
    &lt;p&gt;Prover:&lt;/p&gt;
    &lt;code&gt;cargo run --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;p&gt;Verifier:&lt;/p&gt;
    &lt;code&gt;cargo run --bin verifier -- --rows 1024 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Format: the SRS files are Arkworks-serialized vectors of affine powers. G1:&lt;/p&gt;&lt;code&gt;[τ^0]G1 … [τ^d]G1&lt;/code&gt;(we use the degree bound you load). G2: a vector containing at least&lt;code&gt;[τ]G2&lt;/code&gt;(we read element 1 or 0).&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--rows &amp;lt;T&amp;gt;&lt;/code&gt;: total rows in the trace (domain size rounds up to power of two).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--b-blk &amp;lt;B&amp;gt;&lt;/code&gt;: block size; pick ≈ √T to achieve the sublinear memory bound.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--k &amp;lt;K&amp;gt;&lt;/code&gt;: number of registers (columns) in the AIR.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--basis &amp;lt;eval|coeff&amp;gt;&lt;/code&gt;: commitment basis for wires (Q is always coeff).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--selectors &amp;lt;FILE&amp;gt;&lt;/code&gt;: optional selectors/fixed columns CSV (rows × S).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--omega &amp;lt;u64&amp;gt;&lt;/code&gt;: override ω (power-of-two order must hold).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--coset &amp;lt;u64&amp;gt;&lt;/code&gt;: reserved; current domain uses subgroup (&lt;code&gt;Z_H(X)=X^N−1&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/pcs.rs&lt;/code&gt;— KZG PCS (BN254), streaming Aggregator, real openings, pairings.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/domain.rs&lt;/code&gt;— domain&lt;code&gt;H&lt;/code&gt;, barycentric weights, blocked NTT/IFFT.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/air.rs&lt;/code&gt;— tiny fixed-column AIR + residual stream + permutation coupling.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/perm_lookup.rs&lt;/code&gt;— permutation accumulator&lt;code&gt;Z&lt;/code&gt;(lookups optional).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/quotient.rs&lt;/code&gt;— streaming quotient builder (R→Q tilewise).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/scheduler.rs&lt;/code&gt;— 5-phase orchestrator (aggregate-only FS, O(√T) space).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/opening.rs&lt;/code&gt;— streaming polynomial evaluation helpers (eval/coeff mode).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/transcript.rs&lt;/code&gt;— domain-separated FS transcript (BLAKE3→field).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/stream.rs&lt;/code&gt;— block partitioning + restreaming interfaces.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bin/prover.rs&lt;/code&gt;,&lt;code&gt;bin/verifier.rs&lt;/code&gt;— CLIs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scripts/test_sszkp.sh&lt;/code&gt;— end-to-end tests + tamper test.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treat the &lt;code&gt;Restreamer&lt;/code&gt;trait as the integration seam: implement it to feed rows from your own storage (disk, network, GPU), all while keeping O(b_blk) memory.&lt;/item&gt;
      &lt;item&gt;Keep your permutation/lookup logic time-ordered; the accumulator state must evolve monotonically in &lt;code&gt;t&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;When committing from evaluations, ensure your blocks align to the domain and use the provided blocked IFFT helpers to produce coeff tiles.&lt;/item&gt;
      &lt;item&gt;For openings, prefer the coeff-stream path; the code adapts eval-streams internally when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pairing checks are always enforced; the verifier replays the FS transcript and checks KZG equalities for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tamper test flips one byte in &lt;code&gt;proof.bin&lt;/code&gt;—verification must fail.&lt;/item&gt;
      &lt;item&gt;Dev SRS exists only for convenience; do not use dev mode in production.&lt;/item&gt;
      &lt;item&gt;Algebraic identity at ζ (gate + perm coupling + boundary = &lt;code&gt;Z_H(ζ)·Q(ζ)&lt;/code&gt;) is implemented; by default, selectors are optional and gates are minimal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AIR is a compact demo; plug in your real selector/table wiring as needed.&lt;/item&gt;
      &lt;item&gt;Lookup accumulator is feature-gated and intentionally minimal (demo path).&lt;/item&gt;
      &lt;item&gt;Only BN254/KZG is shipped; adding Pallas/BLS12-381 is straightforward in this architecture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File issues for bugs or suggestions.&lt;/item&gt;
      &lt;item&gt;PRs welcome—especially alternative domains, SRS loaders, or integration examples.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This codebase follows the aggregate-only Fiat–Shamir and streaming discipline described in the whitepaper and demonstrates that production-style commitments and sublinear space can coexist in a practical Rust implementation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/logannye/space-efficient-zero-knowledge-proofs"/><published>2025-09-24T02:12:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45355806</id><title>America's top companies keep talking about AI – but can't explain the upsides</title><updated>2025-09-24T05:37:29.414831+00:00</updated><content>&lt;doc fingerprint="9e2b007ddfca99e9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;&lt;quote&gt;America’s top companies keep talking about AI — but can’t explain the upsides&lt;/quote&gt;&lt;/head&gt;&lt;head rend="h2"&gt;Save 40% on Standard Digital&lt;/head&gt;was $540 now $319 for your first year&lt;p&gt;Save now on essential digital access to quality FT journalism on any device. Saving based on monthly annualised price.&lt;/p&gt;&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;Trial&lt;/head&gt;&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel or change your plan anytime during your trial.&lt;/p&gt;&lt;head rend="h3"&gt;Premium Digital&lt;/head&gt;&lt;p&gt;Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.&lt;/p&gt;&lt;p&gt;FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ft.com/content/e93e56df-dd9b-40c1-b77a-dba1ca01e473"/><published>2025-09-24T02:59:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45356226</id><title>Greatest irony of the AI age: Humans hired to clean AI slop</title><updated>2025-09-24T05:37:28.876184+00:00</updated><content>&lt;doc fingerprint="be4119bcbb886cbd"&gt;
  &lt;main&gt;
    &lt;p&gt;On one side is AI swallowing millions of jobs, and on the other is humans being hired to clean up the nonsense AI often generates, finds Satyen K. Bordoloi&lt;/p&gt;
    &lt;p&gt;This was early 2023, a few months after ChatGPT had just made the perfect superintelligence landing in our lives. A producer friend, who wanted a beat sheet of a series written into a synopsis, sent me a document he said he had gotten written.&lt;/p&gt;
    &lt;p&gt;A reading of its first paragraph was all it took to identify the writer: ChatGPT. The perfect robotic structure, excessive and often misplaced adverbs and adjectives, and the absence of indirect tense gave it away instantly. It was sloppy in its sterile perfection.&lt;/p&gt;
    &lt;p&gt;Yet, my friend asked me to take it as a base and improve it. Crunched for time, I did. I didn’t know then, but I had unwittingly participated in what would become one of the most in-demand gigs two years later: humans cleaning up AI slop.&lt;/p&gt;
    &lt;p&gt;This is the defining irony of the AI age. While AI is consuming millions of jobs, it is simultaneously creating a unique category of employment for hundreds of thousands of humans: cleaning up the mess AI makes. Designers, writers and digital artists are increasingly being hired not to create from scratch, but fix the mess AI invariably makes when tasked with complex work. What is doubly ironic is that these are often the same humans who would have been hired to create the original had AI not been brought to undercut them.&lt;/p&gt;
    &lt;head rend="h2"&gt;WHAT IS AI SLOP&lt;/head&gt;
    &lt;p&gt;Jack Izzo, in a Yahoo article, defines it better than any LLM can: “AI slop is the evolution of spam, in a way. Like spam, slop is low-quality content, but thanks to artificial intelligence (AI) tools like ChatGPT and Midjourney, it’s even easier to produce. Like spam, slop can grow like a weed if left unchecked, overwhelming social media feeds and leaving users unsure of what’s real and what’s not. Like spam, slop comes in many forms — posts on social media.. books on Amazon, music on Spotify, articles from less-than-reliable news outlets (and, unfortunately, some reliable outlets) and even occasionally in peer-reviewed scientific journals.”&lt;/p&gt;
    &lt;p&gt;It is the content equivalent of empty calories: visually or textually appealing, but devoid of substance, originality, or reliable meaning.&lt;/p&gt;
    &lt;p&gt;With video generation becoming as cheap and easy as creating images, the internet is being flooded with AI-generated video slop. A hyper-realistic video of a seagull staring down a French fry on a car dashboard before smashing the window to grab it generated over 140 million views. A CCTV-style video of rabbits jumping on a backyard trampoline has racked up over 200 million views on TikTok and X. So has another video of a bear doing the same. And unsurprisingly, even porn is now overflowing with AI-generated slop.&lt;/p&gt;
    &lt;p&gt;Now the bunny video had tell-tale glitches: a bunny with two heads, and another vanishing mid-bounce. These alerted the discerning viewers to its sloppy origin. But this raises a question: What if the creator had hired a VFX artist to correct the errors?&lt;/p&gt;
    &lt;head rend="h2"&gt;HARMS OF THE AI SLOPOCALYPSE&lt;/head&gt;
    &lt;p&gt;The dangers of AI slop are many. First and foremost, the well of misinformation that the internet has always been is now being industrialised by AI that can generate thousands of plausible-sounding articles, product reviews, or social media posts in the time it takes a person to write just one. This floods everything, burying good information under a mountain of convincing garbage. So far, we have seen the enshittification of online businesses.&lt;/p&gt;
    &lt;p&gt;However, with AI models remixing and regurgitating existing content, what we have is the enshittification of culture itself, as music playlists are already overflowing with AI-generated music, Amazon with AI-generated books, and TikTok and other social media platforms are slowly filling up with AI-made videos.&lt;/p&gt;
    &lt;p&gt;And let’s not forget that creating this garbage consumes staggering amounts of water and electricity, contributing to emissions that harm the planet. Then there are people hired to clean up AI nonsense who could have been artists in their own right, but are now relegated to digital janitorial duties, leading to frustration and burnout.&lt;/p&gt;
    &lt;head rend="h2"&gt;CLEANUP CREW TO THE RESCUE&lt;/head&gt;
    &lt;p&gt;And the ones saving us from the slopocalypse, irony be crucified, are now good old humans with analogue brains. The promised AI utopia of effortless creation is instead giving rise to an underclass of digital rescuers, whose job profiles are being rewritten as AI code and training changes. These roles for AI clean up specialists are cropping up across industries, especially in freelance and creative sectors where AI’s limitations are most glaring.&lt;/p&gt;
    &lt;p&gt;First and foremost are the AI content rewriters hired to rewrite AI-generated articles, blogs, and marketing content that lack nuance, emotional resonance and factual accuracy. Then there are the art fixers hired to redraw or retouch AI-generated logos, illustrations, and art. Most AI-generated images have wrapped text, symmetry that doesn’t match reality and can be pixelated. Actual graphic designers and AI artists work to restore clarity and scale. AI code debuggers are hired to patch buggy code written by the likes of GitHub Copilot or ChatGPT. These actual developers and freelance engineers are hired to test, fix and optimise AI-generated code.&lt;/p&gt;
    &lt;p&gt;AI-generated videos are glitchy and often get physics wrong, and generate random things inside frames. AI video polishers are typically VFX artists whose job is to enhance the visual coherence and thus the realism of the footage.&lt;/p&gt;
    &lt;p&gt;These roles are not about collaboration, but correction. And the cost-saving AI promised is a mirage that can’t be held without the hidden overhead of human quality control. This entire endeavour reeks of a bizarre inefficiency as machines create slop at scale, and humans are hired to clean it up at a premium.&lt;/p&gt;
    &lt;p&gt;Go to freelance platforms like Upwork, Fiverr, and Freelancer, and you’ll see a surging demand for human-led creativity, especially in writing, image creation, and design.&lt;/p&gt;
    &lt;head rend="h2"&gt;MOTHER OF ALL IRONIES&lt;/head&gt;
    &lt;p&gt;AI was supposed to replace humans. Instead, it is creating a parallel economy of human fixers: people who make synthetic content usable, relatable, real – make it feel more human. There’s another irony – AI is replacing humans in certain jobs, while also creating menial jobs for them. People who, before AI, would have become artists have been relegated to the job of cleaners, janitors, cleaning AI slop. Yes, AI is on one side revealing just how irreplaceable humans are when it comes to nuance, empathy, and storytelling, but at the cost of the humans who can do those.&lt;/p&gt;
    &lt;p&gt;The problem here, as often isn’t artificial intelligence, but natural human stupidity. AI creating slop and humans hired to clean it isn’t an inevitable tech progress outcome. No! It’s a choice arisen out of a gold rush mentality that prioritises speed, volume and cost-cutting over quality, authenticity, and truth.&lt;/p&gt;
    &lt;p&gt;The solution, hence, lies not in AI becoming more ‘intelligent’, but in humans becoming smarter and realising that humans should always be in the loop, not brought in at the end to clean up. The solution isn’t in abandoning AI, but in recalibrating our relationship with it. We must realise that AI isn’t a replacement for human creativity and judgment, but that it’s a tool, a powerful one at that, which, when guided by human empathy and art, will create beauty and heart.&lt;/p&gt;
    &lt;p&gt;The greatest irony of this AI age may be that humans are hired to clean up AI’s mess; the greatest tragedy, however, would be if we became so accustomed to that slop that we forgot what a clean, human-made world looks like. The cleanup crew is a temporary fix. The real work is in ensuring that our technological future is built not on a foundation of AI slop, but on a commitment to genuine human creativity and integrity.&lt;/p&gt;
    &lt;head rend="h3"&gt;In case you missed:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kodak Moment: How Apple, Amazon, Meta, Microsoft Missed the AI Boat, Playing Catch-Up&lt;/item&gt;
      &lt;item&gt;Rise of Generative AI in India: Trends &amp;amp; Opportunities&lt;/item&gt;
      &lt;item&gt;The End of SEO as We Know It: Welcome to the AIO Revolution&lt;/item&gt;
      &lt;item&gt;Google Falters Under AI Onslaught: Future of Search in Peril?&lt;/item&gt;
      &lt;item&gt;One Year of No-camera Filmmaking: How AI Rewrote Rules of Cinema Forever&lt;/item&gt;
      &lt;item&gt;Rise of the Robolympics: When R2-D2 Meets Rocky Balboa&lt;/item&gt;
      &lt;item&gt;Are Hallucinations Good For AI? Maybe Not, But They’re Great For Humans&lt;/item&gt;
      &lt;item&gt;AI Taken for Granted: Has the World Reached the Point of AI Fatigue?&lt;/item&gt;
      &lt;item&gt;Hey Marvel, Just Admit You’re Using AI – We All Are!&lt;/item&gt;
      &lt;item&gt;Anthropomorphisation of AI: Why Can’t We Stop Believing AI Will End the World?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sify.com/ai-analytics/greatest-irony-of-the-ai-age-humans-being-increasingly-hired-to-clean-ai-slop/"/><published>2025-09-24T04:15:04+00:00</published></entry></feed>