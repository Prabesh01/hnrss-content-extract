<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-10T21:32:08.253391+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45536124</id><title>Multi-Core by Default</title><updated>2025-10-10T21:32:21.468363+00:00</updated><content>&lt;doc fingerprint="665c885307b58ac8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Multi-Core By Default&lt;/head&gt;
    &lt;head rend="h3"&gt;On multi-core programming, not as a special-case technique, but as a new dimension in all code.&lt;/head&gt;
    &lt;p&gt;Learning to program a single CPU core is difficult. There is an enormous number of techniques, amount of information, and number of hours to spend in order to learn to do it effectively. Learning to program multiple CPU cores to do work in parallel, all while these cores cooperate in accomplishing some overarching task, to me seemed like the anvil that broke the camel’s back—so to speak—there is already so much to wrangle when doing single-core programming, that for me, it was much more convenient to ignore multi-core programming for a long time.&lt;/p&gt;
    &lt;p&gt;But in the modern computer hardware era, there emerges an elephant in the room. With modern CPU core counts far exceeding 1—and instead reaching numbers like 8, 16, 32, 64—programmers leave an enormous amount of performance on the table by ignoring the fundamentally multi-core reality of their machines.&lt;/p&gt;
    &lt;p&gt;I’m not a “performance programmer”. Like Casey Muratori (which is partly what made me follow him to begin with), I have always wanted reasonable performance (though this might appear like “performance programming” to a concerning proportion of the software industry), but historically I’ve worked in domains where I control the data involved, like my own games and engines, where I am either doing the art, design, and levels myself, or heavily involved in the process. Thus, I’ve often been able to use my own programming constraints to inform artistic constraints.&lt;/p&gt;
    &lt;p&gt;All of that went out the window over the past few years, when in my work on debuggers, I’ve needed to work with data which is not only not under my control, but is almost exactly identical to the opposite of what I’d want—it’s dramatically bigger, unfathomably poorly structured, extraordinarily complicated, and not to mention unpredictable and highly variable. This is because, as I’ve written about, debuggers are at a “busy intersection”. They deal with unknowns from the external computing world on almost all fronts. And if one wanted a debugger to be useful for—for instance—extraordinarily large codebases that highly successful companies use to ship real things, those unknowns include unfortunate details about those codebases too.&lt;/p&gt;
    &lt;p&gt;As such, in my work, making more effective use of the hardware has been far more important than it ever has been for me in the past. As such, I was forced to address the “elephant in the room” that is CPU core counts, and actually doing multi-core programming.&lt;/p&gt;
    &lt;p&gt;I’ve learned a lot about the multi-core aspect of programming in the past few years, and I’ve written about lessons I’ve learned during that time, like those on basic mental building blocks I used to plan for multithreaded architecture, and carefully organizing mutations such that multiple threads require little-to-no synchronization.&lt;/p&gt;
    &lt;p&gt;I still find those ideas useful, and my past writing still captures my thoughts on the first principles of multi-core programming. But recently, thanks to some lessons I learned after a few discussions with Casey, my abilities in concretely applying those first principles have “leveled up”. I’m writing this post now to capture and share those lessons.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Parallel &lt;code&gt;for&lt;/code&gt; (And Its Flaws)&lt;/head&gt;
    &lt;p&gt;Because every programmer learns single-core programming first, it’s common—after one first learns multi-core programming techniques—to apply those techniques conservatively within otherwise single-core code.&lt;/p&gt;
    &lt;p&gt;To make this more concrete, consider the following simple example:&lt;/p&gt;
    &lt;code&gt;S64 *values = ...;
S64 values_count = ...;
S64 sum = 0;
for(S64 idx = 0; idx &amp;lt; values_count; idx += 1)
{
  sum += values[idx];
}&lt;/code&gt;
    &lt;p&gt;In this example, we compute a sum of all elements in the &lt;code&gt;values&lt;/code&gt; array. Let’s now consider a few properties of sums:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;a + b + c + d = (a + b) + (c + d)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;a + b + c + d = d + c + b + a&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;(a + b) + (c + d) = (c + d) + (a + b)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Because we can reconsider a sum of elements as a sum of sums of groups of those elements, and because the order in which we sum elements does not impact the final computation, the original code can be rewritten like:&lt;/p&gt;
    &lt;code&gt;S64 *values = ...;
S64 values_count = ...;

S64 sum0 = 0;
for(S64 idx = 0; idx &amp;lt; values_count/4; idx += 1)
{
  sum0 += values[idx];
}

S64 sum1 = 0;
for(S64 idx = values_count/4; idx &amp;lt; (2*values_count)/4; idx += 1)
{
  sum1 += values[idx];
}

S64 sum2 = 0;
for(S64 idx = (2*values_count)/4; idx &amp;lt; (3*values_count)/4; idx += 1)
{
  sum2 += values[idx];
}

S64 sum3 = 0;
for(S64 idx = (3*values_count)/4; idx &amp;lt; (4*values_count)/4 &amp;amp;&amp;amp; idx &amp;lt; values_count; idx += 1)
{
  sum3 += values[idx];
}

S64 sum = sum0 + sum1 + sum2 + sum3;&lt;/code&gt;
    &lt;p&gt;That obviously doesn’t win us anything—but what this means is that we can obtain the same result by subdividing the computation into several, smaller, independent computations.&lt;/p&gt;
    &lt;p&gt;Because several independent computations do not require writing to the same memory, they fit nicely with multi-core programming—each core does not need to synchronize at all with any other. This not only greatly simplifies the multi-core programming, but improves its performance—or, more precisely, it doesn’t eat away from the natural performance obtained by executing in parallel.&lt;/p&gt;
    &lt;p&gt;For cases like this, we can implement what’s known as a “parallel &lt;code&gt;for”&lt;/code&gt;. The idea is that we’d like to specify our original &lt;code&gt;for&lt;/code&gt; loop…&lt;/p&gt;
    &lt;code&gt;for(S64 idx = 0; idx &amp;lt; values_count; idx += 1)
{
  sum += values[idx];
}&lt;/code&gt;
    &lt;p&gt;…but we’d like to also express that the loop can be subdivided into independent computations (the results of which we can join into a single result later).&lt;/p&gt;
    &lt;p&gt;In other words, we begin with normal, single-core code. But, for some computation, we want to “go wide”, and compute something in parallel. Then, we want to “join” this wide, parallel work, and go back to more single-core code, which can use the results of the work done in parallel.&lt;/p&gt;
    &lt;p&gt;This is a widely known and used concept. In many real codebases written in modern programming languages which offer many tools for abstraction building, you’ll find a number of impressive gymnastics to succinctly express this.&lt;/p&gt;
    &lt;p&gt;One of the reasons I prefer working in a simpler language is that, if what my code ultimately generates to facilitate some abstraction is complicated, that being reflected directly in the source code helps keep me honest about how “clean” some construct actually is.&lt;/p&gt;
    &lt;p&gt;If, on the other hand, some higher level utility can be provided by a simple and straightforward concrete implementation, that is a sign of a superior design—one that does not compromise on its implementation, but also does not compromise on its higher level utility.&lt;/p&gt;
    &lt;p&gt;Many people behave as though this is impossible—that higher level utility necessarily incurs substantial tradeoffs at the low level, or vice versa, that low level properties like performance necessitate undesirable high level design. This is simply not universally true. By hunting for tradeoffs, many programmers train themselves to ignore cases when they can both have, and eat, their cake.&lt;/p&gt;
    &lt;p&gt;So, if we consider our options for implementing a “parallel &lt;code&gt;for&lt;/code&gt;” without a lot of modern language machinery, we might start with something like this:&lt;/p&gt;
    &lt;code&gt;struct SumParams
{
  S64 *values;
  S64 count;
  S64 sum;
};

void SumTask(SumParams *p)
{
  for(S64 idx = 0; idx &amp;lt; p-&amp;gt;count; idx += 1)
  {
    p-&amp;gt;sum += p-&amp;gt;values[idx];
  }
}

S64 ComputeSum(S64 *values, S64 count)
{
  S64 count_per_core = count / NUMBER_OF_CORES;
  SumParams params[NUMBER_OF_CORES] = {0};
  Thread threads[NUMBER_OF_CORES] = {0};
  for(S64 core_idx = 0; core_idx &amp;lt; NUMBER_OF_CORES; core_idx += 1)
  {
    params[core_idx].values = values + core_idx*count_per_core;
    params[core_idx].count = count_per_core;
    S64 overkill = ((core_idx+1)*count_per_core - count);
    if(overkill &amp;gt; 0)
    {
      params[core_idx].count -= overkill;
    }
    threads[core_idx] = LaunchThread(SumTask, &amp;amp;params[core_idx]);
  }

  S64 sum = 0;
  for(S64 core_idx = 0; core_idx &amp;lt; NUMBER_OF_CORES; core_idx += 1)
  {
    JoinThread(threads[core_idx]);
    sum += params[core_idx].sum;
  }

  return sum;
}&lt;/code&gt;
    &lt;p&gt;There are a number of unfortunate realities about this mechanism:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;In something like&lt;/p&gt;&lt;code&gt;LaunchThread&lt;/code&gt;and&lt;code&gt;JoinThread&lt;/code&gt;, we interact with the kernel to create and destroy kernel resources (threads) every time we perform a sum.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The actual case-specific code we needed (for the sum, in this case), and the number of particular details we had to specify and get right—like the work subdivision—has exploded. What used to be a simple&lt;/p&gt;&lt;code&gt;for&lt;/code&gt;loop has been spread around to different, more intricate parts, all implementing different details of the mechanism we wanted—the work preparation, the work kickoff, and the joining and combination of work results. All parts must be maintained and changed together, every time we want a parallel&lt;code&gt;for&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The solution’s control flow has been scattered across threads, CPU cores, and time. We can no longer trivially step through the sum in a debugger. If we encounter a bug in some iterations in a parallel&lt;/p&gt;&lt;code&gt;for&lt;/code&gt;, we need to correlate the launching of that particular work, and that actual work. For example, if we stop the program in the debugger and find ourselves within a thread performing some iterations of the parallel&lt;code&gt;for&lt;/code&gt;, we have lost context about who launched that work (in single-core code, this information is universally provided with call stacks).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first problem can be partly addressed with a new underlying layer which our code uses instead of the underlying kernel primitives. In many codebases, this layer is called a “job system”, or a “worker thread pool”. In those cases, the program prepares a set of threads once, and these threads simply wait for work, and execute it once they receive it:&lt;/p&gt;
    &lt;code&gt;void JobThread(void *p)
{
  for(;;)
  {
    Job job = GetNextJob(...);
    job.Function(job.params);
  }
}

void SumJob(SumParams *p)
{
  ...
}

S64 ComputeSum(S64 *values, S64 count)
{
  Job jobs[NUMBER_OF_CORES] = {0};
  for(S64 core_idx = 0; core_idx &amp;lt; NUMBER_OF_CORES; core_idx += 1)
  {
    ...
    jobs[core_idx] = LaunchJob(SumJob, &amp;amp;params[core_idx]);
  }

  S64 sum = 0;
  for(S64 core_idx = 0; core_idx &amp;lt; NUMBER_OF_CORES; core_idx += 1)
  {
    WaitForJob(jobs[core_idx]);
    sum += params[core_idx].sum;
  }

  return sum;
}&lt;/code&gt;
    &lt;p&gt;In this case, there is still some overhead incurred by sending to and receiving information from the job threads, but it is significantly lighter than interacting with the kernel.&lt;/p&gt;
    &lt;p&gt;But it hasn’t improved the higher level code very much at all—we’ve simply replaced “threads” with “jobs”. The latter two problems hold. We still need to perform an entire dance in order to set up a “wide loop”—a “parallel &lt;code&gt;for&lt;/code&gt;”, which scatters control flow for a problem across both source code, and coherent contexts (CPU cores, call stacks) at runtime.&lt;/p&gt;
    &lt;p&gt;In this concrete case—computing a sum in parallel—this is not a huge concern. Will it compute a sum in parallel? Yes. Does it have very few shared data writes? Yes. Can you parallelize all similarly parallelizable problems this way? Yes. But, we pay the costs of these problems every time we use this mechanism. If we have to pay that cost very frequently throughout a problem, it can become onerous to write, debug, and maintain all of this machinery.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Job System (And Its Flaws)&lt;/head&gt;
    &lt;p&gt;One desirable property of the parallel &lt;code&gt;for&lt;/code&gt; is that all jobs—which execute at roughly the same time, across some number of cores—are identical in their “shape”. Each job thread participating in the problem is executing exactly the same code—we simply parameterize each job slightly differently, to distribute different subproblems to different cores. This makes understanding, predicting, profiling, and debugging such code much simpler.&lt;/p&gt;
    &lt;p&gt;Furthermore, within a parallel &lt;code&gt;for&lt;/code&gt;, each job’s lifetime is scoped by the originating single-core code’s lifetime. Each job begins and ends within some scope—the scope responsible for launching, then joining, all of the jobs. This means no substantial lifetime management complexity occurs—allocations for a parallel &lt;code&gt;for&lt;/code&gt; are as simple as for normal single-core code.&lt;/p&gt;
    &lt;p&gt;But in practice, the mechanism often used to implement parallel &lt;code&gt;for&lt;/code&gt;s—the job system—is rarely only used in this way, which is understandable, given its highly generic structure. For example, it’s also often used to launch a number of heterogeneous jobs. In these cases, it becomes even more difficult to understand the context of a particular job—who launched it, and in what context? It also becomes more difficult to comprehensively understand a system—because there is such a large number of possible configurations of thread states, it can be difficult to ensure a threaded system is robust in all cases.&lt;/p&gt;
    &lt;p&gt;These jobs are also often not bounded by their launcher scope—as such, more engineering must be spent on managing resources, like memory allocations, whose lifetimes are now defined by what happens across multiple threads in multiple contexts.&lt;/p&gt;
    &lt;p&gt;And this is, really, the tip of the iceberg. In more sophisticated systems, one might observe that there are dependencies between jobs, and jobs ought to be implicitly launched when their dependency jobs complete, creating an even longer (and more difficult to inspect) chain of context related to some independent through line of work.&lt;/p&gt;
    &lt;p&gt;Ultimately, this presents recurring writing, reading, debugging, and maintenance costs that don’t exist in normal single-core code. All of the costs incurred by this job system design—whether used in a parallel &lt;code&gt;for&lt;/code&gt; or otherwise—are paid any time new parallel work is introduced, or any time parallel work is maintained.&lt;/p&gt;
    &lt;p&gt;Now, if we have few parts of our code that can be parallelized in this way, then this is not a significant cost.&lt;/p&gt;
    &lt;p&gt;…But that if is doing a lot of heavy lifting.&lt;/p&gt;
    &lt;p&gt;In practice, I’ve found that an enormous number of systems are riddled with opportunities for parallelization, because of a lack of serial dependence between many of their parts. But, if taking advantage of every instance of serial independence requires significantly more engineering than just accepting single-core performance, then in many cases, programmers will opt for the latter.&lt;/p&gt;
    &lt;p&gt;Again—does this mean that a job system cannot be used to do such parallelization in these systems? No. But, it also means that we pay the costs of using this job system—the more moving parts; the extra code and concepts to write, read, and debug—much more frequently, if we’d like to take advantage of this widespread serial independence, or if we’d like any algorithm in particular to scale its performance with the number of cores.&lt;/p&gt;
    &lt;head rend="h2"&gt;Single-Core By Default&lt;/head&gt;
    &lt;p&gt;The critical insight I learned from speaking with Casey on this topic was that a significant reason why these costs arise is because of the careful organization a system needs in order to switch from single-core to multi-core code. Mechanisms like job systems and their special case usage in parallel &lt;code&gt;for&lt;/code&gt;s represent, in some sense, the most conservative application of multi-core code. The vast majority of code is written as single-core, and a few carveouts are made when multi-core is critically important. In other words, code remains single-core by default, and in a few special cases, work is done to briefly hand work off to a multi-core system.&lt;/p&gt;
    &lt;p&gt;Because the context of code execution changes across time—because work is handed off from one system to another—it necessarily requires more code to set up, and it is more difficult to debug and understand the full context at any point in time.&lt;/p&gt;
    &lt;p&gt;But is this the best approach? Perhaps, instead of writing single-core code (which sometimes goes wide) by default, we can write multi-core code (which sometimes goes narrow) by default.&lt;/p&gt;
    &lt;p&gt;What does this look like in practice?&lt;/p&gt;
    &lt;p&gt;There’s a good chance that you’ve already experienced this style in other areas of programming—notably, in GPU shader programming.&lt;/p&gt;
    &lt;p&gt;GPU shaders—like vertex or pixel shaders, used in a traditional GPU rendering pipeline—are written such that they are multi-core by default. You author a single function (the entry point of the shader), but this function is executed on many cores, always, implicitly. The language constructs and rules are arranged in such a way that data reads and writes are always scoped by whatever core happens to be executing the code. A single execution of a vertex shader is scoped to a vertex—a pixel shader to a pixel—and so on.&lt;/p&gt;
    &lt;p&gt;Because the fundamental, underlying architecture is always multi-core by default, and because there is little involvement of each specific shader in how the multi-core parallelism is achieved, GPU programming enjoys enormous performance benefits, and yet as the shader programmer, it feels that there are few costs to pay for it. So few, in fact, that it feels more like artistic scripting, to the degree that someone can build an entire website—Shadertoy—built around rapid-iteration, high-performance, visual GPU scripting.&lt;/p&gt;
    &lt;p&gt;Wait a minute… “high performance”, “rapid-iteration scripting”? It seems like many believe that these are mutually exclusive!&lt;/p&gt;
    &lt;p&gt;Why does CPU programming feel so different?&lt;/p&gt;
    &lt;p&gt;Contrast the GPU programming model to the usual CPU programming model—you author a single function (the entry point of your program), which is scheduled onto a single core only, normally by a kernel scheduler, using a single thread state. This model is, in contrast, single-core by default.&lt;/p&gt;
    &lt;p&gt;Long story short: it doesn’t have to be!&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Core By Default&lt;/head&gt;
    &lt;p&gt;Let’s begin by exactly inverting the approach. Instead of having a single thread which kicks off work to many threads, let’s just have many threads, all running the same code, by default. In a sense, let’s have just one big parallel &lt;code&gt;for&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;void BootstrapEntryPoint(void)
{
  Thread threads[NUMBER_OF_CORES] = {0};
  for(S64 thread_idx = 0; thread_idx &amp;lt; NUMBER_OF_CORES; thread_idx += 1)
  {
    threads[thread_idx] = LaunchThread(EntryPoint, (void *)thread_idx);
  }
  for(S64 thread_idx = 0; thread_idx &amp;lt; NUMBER_OF_CORES; thread_idx += 1)
  {
    JoinThread(threads[thread_idx]);
  }
}

void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;
  // program's actual work occurs here!
}&lt;/code&gt;
    &lt;p&gt;To click into an architecture which assumes a single-threaded entry point, we start with a &lt;code&gt;BootstrapEntryPoint&lt;/code&gt;. But the only work this function actually does is launch all of the threads executing the actual entry point, &lt;code&gt;EntryPoint&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let’s consider the earlier summation example. First, let’s just take the original single-threaded code, and put it into &lt;code&gt;EntryPoint&lt;/code&gt;, and see how we can continue from there.&lt;/p&gt;
    &lt;code&gt;void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;

  // we obtain these somehow:
  S64 *values = ...;
  S64 values_count = ...;

  // compute the sum
  S64 sum = 0;
  for(S64 idx = 0; idx &amp;lt; values_count; idx += 1)
  {
    sum += values[idx];
  }
}&lt;/code&gt;
    &lt;p&gt;What is actually happening? Well, we’re “computing the sum across many cores”. That is… technically true! Ship it!&lt;/p&gt;
    &lt;p&gt;There’s just one little problem… This is just as fast as the single-core version, except it also uses enormously more energy, and steals time from other tasks the CPU could be doing, because it is simply duplicating all work on each core.&lt;/p&gt;
    &lt;p&gt;But, if we were to measure this, and consider the real costs, and profile the actual code, the profile would look something like this:&lt;/p&gt;
    &lt;p&gt;Duplication itself is not, in principle, a problem, and it is sometimes not to be avoided, because deduplication can sometimes be more expensive than duplication. For instance, communicating the result of a single &lt;code&gt;add&lt;/code&gt; instruction across many threads—to deduplicate the work of that &lt;code&gt;add&lt;/code&gt;—would be vastly more expensive than simply duplicating the &lt;code&gt;add&lt;/code&gt; itself. We do want deduplication, but only when necessary, or when it actually helps.&lt;/p&gt;
    &lt;p&gt;So, where does it help? Unsurprisingly in this case, the dominating cost—the reason we are using multiple cores at all—is the sum across all elements in &lt;code&gt;values&lt;/code&gt;. We want to distribute different parts of the sum across cores. To start, instead of computing the full sum, we can instead compute a per-thread sum. After each per-thread sum is computed, we can then combine them:&lt;/p&gt;
    &lt;code&gt;void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;

  // we obtain these somehow:
  S64 *values = ...;
  S64 values_count = ...;

  // decide this thread's subset of the sum
  S64 thread_first_value_idx = ???;
  S64 thread_opl_value_idx = ???; // one past last

  // compute the thread sum
  S64 thread_sum = 0;
  for(S64 idx = thread_first_value_idx;
      idx &amp;lt; thread_opl_value_idx;
      idx += 1)
  {
    thread_sum += values[idx];
  }

  // combine the thread sums
  S64 sum = ???;
}&lt;/code&gt;
    &lt;p&gt;We have two blanks to fill in:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;How do we decide each thread’s subset of work?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How do we combine all thread sums?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s tackle each.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Deciding Per-Thread Work&lt;/head&gt;
    &lt;p&gt;Currently, the only input I’ve provided each thread is its index, which would be in [0, N), where N is the number of threads. This is stored in the local variable &lt;code&gt;thread_idx&lt;/code&gt;, which will have a different value in [0, N) for each thread. This is an easy example, because a good way to distribute the sum work across all threads is to uniformly distribute the number of values to sum amongst the threads. This means we are simply mapping [0, M) to [0, N), where M is the number of values—&lt;code&gt;values_count&lt;/code&gt;—and N is the number of threads.&lt;/p&gt;
    &lt;p&gt;We can almost compute this as follows:&lt;/p&gt;
    &lt;code&gt;S64 values_count = ...;
S64 thread_idx = ...;
S64 thread_count = NUMBER_OF_CORES;

S64 values_per_thread = values_count / thread_count;
S64 thread_first_value_idx = values_per_thread * thread_idx;
S64 thread_opl_value_idx = thread_first_value_idx + values_per_thread;&lt;/code&gt;
    &lt;p&gt;This is almost right, but only almost, because we also need to account for the case where &lt;code&gt;values_count&lt;/code&gt; is not cleanly subdivided by &lt;code&gt;thread_count&lt;/code&gt;. Because our &lt;code&gt;values_per_thread&lt;/code&gt; will truncate to the next lowest integer, this current distribution will underestimate the number of values we need to compute per thread, by anywhere from 0 (if it divides cleanly) to &lt;code&gt;thread_count-1&lt;/code&gt; values—or in other words, the remainder of the division.&lt;/p&gt;
    &lt;p&gt;Thus, the number of values this division underestimates by—the “leftovers”—can be computed as follows:&lt;/p&gt;
    &lt;code&gt;S64 leftover_values_count = values_count % thread_count;&lt;/code&gt;
    &lt;p&gt;We can then distribute these leftovers amongst the first &lt;code&gt;leftover_values_count&lt;/code&gt; threads:&lt;/p&gt;
    &lt;code&gt;// compute the values-per-thread, &amp;amp; number of leftovers
S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;

// determine if the current thread gets a leftover
// (we distribute them amongst the first threads in the group)
B32 thread_has_leftover = (thread_idx &amp;lt; leftover_values_count);

// decide on how many leftovers have been distributed before this
// thread's range (just the thread index, clamped by the number of
// leftovers)
S64 leftovers_before_this_thread_idx = 0;
if(thread_has_leftover)
{
  leftovers_before_this_thread_idx = thread_idx;
}
else
{
  leftovers_before_this_thread_idx = leftover_values_count;
}

// decide on the [first, opl) range:
// we shift `first` by the number of leftovers we've placed earlier,
// and we shift `opl` by 1 if we have a leftover.
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = thread_first_value_idx + values_per_thread;
if(thread_has_leftover)
{
  thread_opl_value_idx += 1;
}&lt;/code&gt;
    &lt;p&gt;Or more succinctly:&lt;/p&gt;
    &lt;code&gt;S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &amp;lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);&lt;/code&gt;
    &lt;p&gt;Now, using this &lt;code&gt;[first, opl)&lt;/code&gt; calculation, we can arrange each thread to only loop over its associated range, thus not duplicating all sum work done by other threads.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Combining All Thread Sums&lt;/head&gt;
    &lt;p&gt;Now, how might we combine each thread’s sum to form the total sum? There are two simple options available: (a) we can define a global sum counter to which each thread atomically adds (using atomic add intrinsics) its per-thread sum, or (b) we can define global storage which stores all thread sums, and each thread can duplicate the work of computing the total sum.&lt;/p&gt;
    &lt;p&gt;For (a), we just need to define &lt;code&gt;sum&lt;/code&gt; as &lt;code&gt;static&lt;/code&gt;, and atomically add each &lt;code&gt;thread_sum&lt;/code&gt; to it:&lt;/p&gt;
    &lt;code&gt;static S64 sum = 0;

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  AtomicAddEval64(&amp;amp;sum, thread_sum);
}&lt;/code&gt;
    &lt;p&gt;Note: This has a downside in that only one thread group can be executing this codepath at once. This is sometimes not a practical concern, since if we are going wide at all, we are often using all available cores to do so, and it is likely not beneficial to also have some other thread group executing the same codepath for a different purpose. That said, it’s now a new hidden restriction of this code, and it can be a critical problem. There are some techniques we can use to solve this problem, which I will cover later—for now, the important concept is that the data is shared across participating threads.&lt;/p&gt;
    &lt;p&gt;For (b), we’d instead have a global table, and duplicate the work of summing across all thread sums. But we can only do that after we know that each thread has completed its summation work—otherwise we’d potentially add some other thread’s sum before it was actually computed!&lt;/p&gt;
    &lt;code&gt;static S64 thread_sums[NUMBER_OF_CORES] = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  thread_sums[thread_idx] = thread_sum;

  // ??? need to wait here for all threads to finish!

  S64 sum = 0;
  for(S64 t_idx = 0; t_idx &amp;lt; NUMBER_OF_CORES; t_idx += 1)
  {
    sum += thread_sums[t_idx];
  }
}&lt;/code&gt;
    &lt;p&gt;That extra waiting requirement might seem like an argument in favor of (a), but we’d actually need the same mechanism if we did (a) once we wanted to actually use the sum—we’d need to wait for all threads to reach some point, so that we’d know that they’d all atomically updated &lt;code&gt;sum&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We can use a barrier to do this. In (a):&lt;/p&gt;
    &lt;code&gt;static S64 sum = 0;
static Barrier barrier = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  AtomicAddEval64(&amp;amp;sum, thread_sum);
  BarrierSync(barrier);
  // `sum` is now fully computed!
}&lt;/code&gt;
    &lt;p&gt;And in (b):&lt;/p&gt;
    &lt;code&gt;static S64 thread_sums[NUMBER_OF_CORES] = {0};
static Barrier barrier = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  thread_sums[thread_idx] = thread_sum;

  BarrierSync(barrier);

  S64 sum = 0;
  for(S64 t_idx = 0; t_idx &amp;lt; NUMBER_OF_CORES; t_idx += 1)
  {
    sum += thread_sums[t_idx];
  }
  // `sum` is now fully computed!
}&lt;/code&gt;
    &lt;p&gt;At this point, we have everything we need for both (a) and (b). Both are simple, and likely negligibly different. (a) requires atomic summation across all the threads, which implies hardware-level synchronization, whereas (b) duplicates the sum of all per-thread sums—these likely subtly differ in there costs, but not by much when compared to the actual &lt;code&gt;values&lt;/code&gt; summation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going Narrow&lt;/head&gt;
    &lt;p&gt;Now, while I hope this summation example has been a useful introduction, I know it’s a bit contrived, and incomplete. Specifically, it’s missing two key parts of any program: inputs and outputs. What are we doing with this sum, and how do we use that in producing some form of output, and how do obtain the inputs, and store them in &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;values_count&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Let’s barely extend the summation example with stories for the inputs and outputs. For the inputs, let’s say that we read &lt;code&gt;values&lt;/code&gt; out of a binary file, which just contains the whole array stored as it will be in memory. For the outputs, let’s say that we just print the sum to &lt;code&gt;stdout&lt;/code&gt; with &lt;code&gt;printf&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Printing out the sum will be the easiest part, so let’s begin with that.&lt;/p&gt;
    &lt;p&gt;In single-core code, after computing the sum, we’d simply call &lt;code&gt;printf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;S64 sum = ...;
// ...
printf("Sum: %I64d", sum);&lt;/code&gt;
    &lt;p&gt;We can start by just doing the same in our “multi-core by default” code. What we’ll find is that our output looks something like this:&lt;/p&gt;
    &lt;code&gt;Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678&lt;/code&gt;
    &lt;p&gt;And obviously, we only want our many cores to be involved with the majority of the computation, but we only need one thread to do the actual &lt;code&gt;printf&lt;/code&gt;. In other words, we need to go narrow. Luckily, going narrow from wide code is much simpler than going wide from narrow code:&lt;/p&gt;
    &lt;code&gt;S64 sum = ...;
// ...
if(thread_idx == 0)
{
  printf("Sum: %I64d", sum);
}&lt;/code&gt;
    &lt;p&gt;We simply need to mask away the work from all threads except one.&lt;/p&gt;
    &lt;p&gt;Now, let’s consider the input problem. We need to compute &lt;code&gt;values_count&lt;/code&gt; based on the size of some input file, allocate storage for &lt;code&gt;values&lt;/code&gt;, and then fill &lt;code&gt;values&lt;/code&gt; by reading all data from the file.&lt;/p&gt;
    &lt;p&gt;Single-threaded code to do that might look something like this:&lt;/p&gt;
    &lt;code&gt;char *input_path = ...;
File file = FileOpen(input_path);
S64 size = SizeFromFile(file);
S64 values_count = (size / sizeof(S64));
S64 *values = (S64 *)Allocate(values_count * sizeof(values[0]));
FileRead(file, 0, values_count * sizeof(values[0]), values);
FileClose(file);&lt;/code&gt;
    &lt;p&gt;So, naturally, one option is to simply do this narrow:&lt;/p&gt;
    &lt;code&gt;if(thread_idx == 0)
{
  char *input_path = ...;
  File file = FileOpen(input_path);
  S64 size = SizeFromFile(file);
  S64 values_count = (size / sizeof(S64));
  S64 *values = (S64 *)Allocate(values_count * sizeof(values[0]));
  FileRead(file, 0, values_count * sizeof(values[0]), values);
  FileClose(file);
}
BarrierSync(barrier); // `values` and `values_count` ready after this point&lt;/code&gt;
    &lt;p&gt;This will work, but we somehow need to broadcast the computed values of &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;values_count&lt;/code&gt; across all threads. One easy way to do this is simply to pull them out as &lt;code&gt;static&lt;/code&gt;, like we did for shared data earlier:&lt;/p&gt;
    &lt;code&gt;static S64 values_count = 0;
static S64 *values = 0;
if(thread_idx == 0)
{
  char *input_path = ...;
  File file = FileOpen(input_path);
  S64 size = SizeFromFile(file);
  values_count = (size / sizeof(S64));
  values = (S64 *)Allocate(values_count * sizeof(values[0]));
  FileRead(file, 0, values_count * sizeof(values[0]), values);
  FileClose(file);
}
BarrierSync(barrier);&lt;/code&gt;
    &lt;p&gt;But consider that we might not want to do this completely single-core. It might be the case that it’s more efficient to issue &lt;code&gt;FileRead&lt;/code&gt;s from many threads, rather than just one. In practice, this is partly true (although, depending on the full stack—the kernel, the storage drive hardware, and so on—it may not be beneficial past some number of threads, and for certain read sizes).&lt;/p&gt;
    &lt;p&gt;So let’s say we’d like to do the &lt;code&gt;FileRead&lt;/code&gt;s wide now also. We need to still allocate &lt;code&gt;values&lt;/code&gt; on a single thread, but once that is done, we can distribute the rest of the work trivially:&lt;/p&gt;
    &lt;code&gt;// we can open the file on all threads (though for some reasons
// we may want to deduplicate this too - for simplicity I am
// keeping it on all threads)
File file = FileOpen(input_path);

// calculate number of values and allocate (only single thread)
static S64 values_count = 0;
static S64 *values = 0;
if(thread_idx == 0)
{
  S64 size = SizeFromFile(file);
  values_count = (size / sizeof(S64));
  values = (S64 *)Allocate(values_count * sizeof(values[0]));
}
BarrierSync(barrier);

// compute thread's range of values (same calculation as before)
S64 thread_first_value_idx = ...;
S64 thread_opl_value_idx = ...;

// do read of this thread's portion
S64 num_values_this_thread = (thread_opl_value_idx - thread_first_value_idx);
FileRead(file,
         thread_first_value_idx*sizeof(values[0]),
         num_values_this_thread*sizeof(values[0]),
         values + thread_first_value_idx);

// close file on all threads
FileClose(file);&lt;/code&gt;
    &lt;p&gt;It’s much simpler, now—compared to, say, the original parallel &lt;code&gt;for&lt;/code&gt; case—to simply take another part of the problem like this, and to also distribute it amongst threads, simply because wide is the default shape of the program.&lt;/p&gt;
    &lt;p&gt;Instead of spending most programming time acting like we’re on a single-core machine, we simply assume our actual circumstances, which is that we have several cores, and sometimes we need to tie it all together with a few serial dependencies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-Uniform Work Distributions&lt;/head&gt;
    &lt;p&gt;Let’s take a look at our earlier calculations to distribute portions of the &lt;code&gt;values&lt;/code&gt; array:&lt;/p&gt;
    &lt;code&gt;S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &amp;lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);&lt;/code&gt;
    &lt;p&gt;This was an easy case, because uniformly dividing portions of &lt;code&gt;values&lt;/code&gt; produces nearly uniform work across all cores.&lt;/p&gt;
    &lt;p&gt;If, in a different scenario, we don’t produce nearly uniform work across all cores, we have a problem: some cores will finish their work in some section long before others, and they’ll be stuck at the next barrier synchronization point while the other cores finish. This diminishes the returns we obtain from going wide in the first place.&lt;/p&gt;
    &lt;p&gt;Thus, it’s always important to uniformly distribute work whenever it’s possible. The exact strategy for doing so will vary by problem. But I’ve noticed three common strategies:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Uniformly distributing inputs produces uniformly distributed work (the case with the sum). So, we can decide the work distribution upfront.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each portion of an input requires a variable amount of per-core work. The work is relatively bounded, and there are many portions of input (larger than the core count). So, we can dynamically grab work on each core, so cores which complete smaller work first receive more, whereas cores that are stuck on longer work leave more units of work for other cores.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each portion of an input requires a variable amount of per-core work, but there is a small number (lower than the core count) of potentially very long sequences of work. We can attempt to redesign this algorithm such that it can be distributed more uniformly instead.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ve already covered the first strategy with the sum example—let’s look at the latter two.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dynamically Assigning Many Variable-Work Tasks&lt;/head&gt;
    &lt;p&gt;Let’s consider a case where we have many units of work—“tasks”—and we’d like to distribute these tasks across cores. We may start by distributing the tasks in the same way that we distributed values to sum in the earlier example:&lt;/p&gt;
    &lt;code&gt;Task *tasks = ...;
S64 tasks_count = ...;
S64 thread_first_task_idx = ...;
S64 thread_opl_task_idx = ...;
for(S64 task_idx = thread_first_task_idx;
    task_idx &amp;lt; thread_last_task_idx;
    task_idx += 1)
{
  // do task
}&lt;/code&gt;
    &lt;p&gt;If each task requires a variable amount of work, then a profile of the program might look something like this:&lt;/p&gt;
    &lt;p&gt;Instead of deciding the task division upfront, we can dynamically assign tasks, such that the threads which are occupied (performing larger tasks) are not assigned more tasks until they’re done, and threads which complete shorter tasks earlier are quickly assigned more tasks, if available.&lt;/p&gt;
    &lt;p&gt;We can do that simply with a shared atomic counter, which each thread increments:&lt;/p&gt;
    &lt;code&gt;Task *tasks = ...;
S64 tasks_count = ...;

// set up the counter
static S64 task_take_counter = 0;
task_take_counter = 0;
BarrierSync(barrirer);

// loop on all threads - take tasks as long as we can
for(;;)
{
  S64 task_idx = AtomicIncEval64(&amp;amp;task_take_counter) - 1;
  if(task_idx &amp;gt;= tasks_count)
  {
    break;
  }
  // do task
}&lt;/code&gt;
    &lt;p&gt;This will dynamically distribute tasks across the cores, so that a profile of the program will look more like this:&lt;/p&gt;
    &lt;head rend="h3"&gt;Redesigning Algorithms For Uniform Work Distribution&lt;/head&gt;
    &lt;p&gt;Dynamically assigning tasks to cores will help in many cases, but it gets less effective if tasks are highly variable, to the point of sometimes being exceedingly long (e.g. many times more expensive than smaller tasks), or if there are fewer tasks than the number of cores.&lt;/p&gt;
    &lt;p&gt;In these cases, it can often be helpful to reconsider the serial independencies within a single task, or whether the same effect as a highly serially-dependent algorithm can be provided by an alternative highly serially-independent algorithm. Can a single task be subdivided further? Can it be performed in a different way? Can serially-dependent work be untangled from heavier work which can be done in a serially-independent way?&lt;/p&gt;
    &lt;p&gt;The answers to such questions are highly problem-specific, so it’s impossible to offer substantially more useful advice while staying similarly generic. But to illustrate that it’s sometimes possible—even when counterintuitive—I have an example problem from my recent work, in which finding more uniform work distribution required switching from a single-threaded comparison sort to a highly parallelizable radix sort.&lt;/p&gt;
    &lt;p&gt;In this problem, I had a small number of arrays that needed to be sorted, but these arrays were potentially very large, thus requiring a fairly expensive sorting pass.&lt;/p&gt;
    &lt;p&gt;My first approach was to simply distribute the comparison sort tasks themselves, so I would sort one array on a single core, while other cores would be sorting other arrays. But as I’ve said, there were a relatively small number of arrays, and the arrays were large, so sorting was fairly expensive—thus, most cores were doing nothing, and simply waiting for the small number of cores performing sorts to finish.&lt;/p&gt;
    &lt;p&gt;This approach would’ve worked fine if I had a larger number of smaller tasks. In fact, another part of the same program does distribute single-threaded comparison sort tasks in this way, because in that part of the problem, there are a larger number of smaller tasks.&lt;/p&gt;
    &lt;p&gt;In this case, I needed to sort array elements based on 64-bit integer keys. After sorting, the elements needed to be ordered such that their associated keys were ascending in value.&lt;/p&gt;
    &lt;p&gt;Conveniently, this can be done with a radix sort. I won’t cover the full details of the algorithm here (although I briefly covered it during a stream recently, which I recorded and uploaded here), but the important detail is that a radix sort requires a fixed number of O(N) passes over the array, and huge portions of work in each pass can be distributed uniformly across cores (in the same way that we distributed the sum work earlier).&lt;/p&gt;
    &lt;p&gt;Now, all cores participate in every larger sorting task, but they only perform a nearly uniform fraction of the work in each sort. This results in a much more uniform work distribution, and thus a much shorter total time spend sorting:&lt;/p&gt;
    &lt;p&gt;This is just one concrete example a larger pattern I’ve noticed: In many problems, upon close examination, some serial dependencies can either vanish, or they can be untangled from heavier work.&lt;/p&gt;
    &lt;p&gt;In some problems, serially-dependent parts of the algorithm can be isolated, such that they prepare data which allows the rest of the algorithm to be done in a serially-independent fashion. Imagine a program which walks a linked list early, on a single core, to compute a layout in a serially-dependent way. This layout can then allow subsequent work to execute just using the full layout, rather than forcing that subsequent work to also include the serially-dependent pointer chasing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Single-Threaded, Just Better&lt;/head&gt;
    &lt;p&gt;Code which is multi-core by default feels like normal single-threaded code, just with a few extra constructs that express the missing information needed to execute on multiple cores. This style has some useful and interesting properties, which make it preferable in many contexts to many of the popular styles of multi-core code found in the wild.&lt;/p&gt;
    &lt;head rend="h3"&gt;Single-Core as a Parameterization&lt;/head&gt;
    &lt;p&gt;One interesting implication of code written in this way—to be multi-core by default—is that it offers a strict superset of functionality than code which is written to be single-core, because “multi-core” in this case includes “single-core”, as one possible case. We can use the same code to execute on only a single core, simply by instead executing our entry point on a single thread, and parameterizing that thread with &lt;code&gt;thread_idx = 0&lt;/code&gt; and &lt;code&gt;thread_count = 1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In that case, one core necessarily receives all of the work. &lt;code&gt;BarrierSync&lt;/code&gt;s turn into no-ops, since there is only one thread (there are no other threads to wait for). Thus, it is equivalent to single-core functionality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler Debugging&lt;/head&gt;
    &lt;p&gt;This style of multi-core programming requires far less busywork and machinery in order to use multiple cores for some codepath. But one of the problems I mentioned with job systems and parallel &lt;code&gt;for&lt;/code&gt;s earlier was not only that they require more busywork and machinery, but that they’re also more difficult to debug.&lt;/p&gt;
    &lt;p&gt;In this case, debugging is much simpler—in fact, it doesn’t look all that different from single-core debugging. At every point, you have access to a full call stack, and all contextual data which led to whatever point in time that you happen to be inspecting in a debugger.&lt;/p&gt;
    &lt;p&gt;Furthermore, because all threads involved are nearly homogeneous (rather than the generic job system, where all threads are heterogeneous at all times), debugging a single thread is a lot like debugging all threads. This is especially true because—between barrier synchronization points—the threads are all executing the same code. In other words, the context and state on one thread is likely to be highly informative of the context and state on all threads.&lt;/p&gt;
    &lt;head rend="h3"&gt;Access To The Full Stack&lt;/head&gt;
    &lt;p&gt;Because the context for some through line of computation frequently changes in traditional job systems, extra machinery must be involved to pipe data from one context to another—across jobs and threads—and maintain any associated allocations and lifetimes. But in this style, resources and lifetimes are kept as simple as they are in single-threaded code.&lt;/p&gt;
    &lt;p&gt;The stack, containing all contextual state at any point, becomes a single bucket for useful thread-local storage. In a job system, the stack is useful multi-core thread-local storage, but only for the duration of the job. The job is equivalent to the inner body of a &lt;code&gt;for&lt;/code&gt;—this is a tiny, fragmentary scope. With this style, the entire stack is available, at any point.&lt;/p&gt;
    &lt;head rend="h2"&gt;Codebase Support&lt;/head&gt;
    &lt;p&gt;I’ve found some useful patterns which can be extracted and widely used in code which is multi-core by default. These patterns seem as widely applicable as arenas—as such, they can be a useful addition to a codebase’s base layer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Thread-Local Group Data&lt;/head&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;LaneIdx()&lt;/code&gt;, &lt;code&gt;LaneCount()&lt;/code&gt;, &lt;code&gt;LaneSync()&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The earlier example code frequently uses the &lt;code&gt;thread_idx&lt;/code&gt;, &lt;code&gt;thread_count&lt;/code&gt;, and &lt;code&gt;barrier&lt;/code&gt; variables. Passing these to every codepath which might need them is redundant and cumbersome. As such, they are good candidates for thread-local storage.&lt;/p&gt;
    &lt;p&gt;In my code, I’ve bundled these into the base layer’s “thread context”, which is a thread-local structure which is universally accessible—it’s where, for example, thread-local scratch arenas are stored.&lt;/p&gt;
    &lt;p&gt;This provides all code the ability to read its index within a thread group (&lt;code&gt;thread_idx&lt;/code&gt;), or the number of threads in its group (&lt;code&gt;thread_count&lt;/code&gt;), and to synchronize with other lanes (&lt;code&gt;BarrierSync&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;As I suggested earlier, any code’s caller can choose “how wide”—how many cores—they’d like to execute that code, by configuring this per-thread storage. In general, shallow parts of a call stack can decide how wide deeper parts of a call stack are executed. If some work is expected to be small (to the point where it doesn’t benefit from being executed on many cores), and other cores can be doing other useful work, then before doing that work, the calling code can simply set &lt;code&gt;thread_idx = 0&lt;/code&gt;, &lt;code&gt;thread_count = 1&lt;/code&gt;, and &lt;code&gt;barrier = {0}&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This means that a single thread may participate in many different thread groups—in other words, &lt;code&gt;thread_idx&lt;/code&gt; and &lt;code&gt;thread_count&lt;/code&gt; are not static within the execution of a single thread. Therefore, I found it appropriate to introduce another disambiguating term: lane. A lane is distinct from a thread in that a lane is simply one thread within a potentially-temporary group of threads, all executing the same code.&lt;/p&gt;
    &lt;p&gt;As such, in my terminology, &lt;code&gt;thread_idx&lt;/code&gt; is exposed as &lt;code&gt;LaneIdx()&lt;/code&gt;, and &lt;code&gt;thread_count&lt;/code&gt; is exposed as &lt;code&gt;LaneCount()&lt;/code&gt;. To synchronize with other lanes, a helper &lt;code&gt;LaneSync()&lt;/code&gt; is available, which just waits on the thread context’s currently selected barrier.&lt;/p&gt;
    &lt;head rend="h3"&gt;Uniformly Distributing Ranges Amongst Lanes&lt;/head&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;LaneRange(count)&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;I’ve mentioned the following computation multiple times:&lt;/p&gt;
    &lt;code&gt;S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &amp;lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);&lt;/code&gt;
    &lt;p&gt;This is useful whenever a uniformly distributed range corresponds to uniformly distributed work amongst cores. As I mentioned, this is sometimes not desirable. But nevertheless, it’s an extremely common case. As such, I found it useful to expose this as &lt;code&gt;LaneRange(count)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Rng1U64 range = LaneRange(count);
for(U64 idx = range.min; idx &amp;lt; range.max; idx += 1)
{
  // ...
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Broadcasting Data Across Lanes&lt;/head&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;LaneSyncU64(value_ptr, source_lane_idx)&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;Earlier, we saw that when a variable needs to be shared across lanes, it can simply be marked as &lt;code&gt;static&lt;/code&gt;. I mentioned that this has the unfortunate downside that only a single group can be executing the code at one time, since one group of lanes could trample over the &lt;code&gt;static&lt;/code&gt; variable while another group is still using it. As I mentioned, this is sometimes not a concern (since it’s desirable to only have a single lane group executing some code), but it invisibly makes code inapplicable for some cases.&lt;/p&gt;
    &lt;p&gt;For example, let’s suppose I have some code which is written to be multi-core by default. Depending on the inputs to this codepath, I may want this to be executed—on the same inputs—with all of my cores. But in other cases, I may want this to be executed with only a single core—I may still want to execute this codepath on other cores, but for different inputs. That requires many lane groups to be executing the code at the same time, thus disqualifying the use of &lt;code&gt;static&lt;/code&gt; to share data amongst lanes within the same group.&lt;/p&gt;
    &lt;p&gt;To address this, I also created a simple mechanism to broadcast small amounts of data across lanes.&lt;/p&gt;
    &lt;p&gt;Each thread context also stores—in addition to a lane index, lane count, and lane group barrier—a pointer to a shared buffer, which is the same value for all lanes in the same group.&lt;/p&gt;
    &lt;p&gt;If one lane has a value which it needs to be broadcasted to other lanes—for instance, if it allocated a buffer that the other lanes are about to fill—then that value can be communicated in the following way:&lt;/p&gt;
    &lt;code&gt;U64 broadcast_size = ...;         // the number of bytes to broadcast
U64 broadcast_src_lane_idx = ...; // the index of the broadcasting lane
void *lane_local_storage = ...;   // unique for each lane
void *lane_shared_storage = ...;  // same for all lanes

// copy from broadcaster -&amp;gt; shared
if(LaneIdx() == broadcast_src_lane_idx)
{
  MemoryCopy(lane_shared_storage, lane_local_storage, broadcast_size);
}
LaneSync();

// copy from shared -&amp;gt; broadcastees
if(LaneIdx() != broadcast_src_lane_idx)
{
  MemoryCopy(lane_local_storage, lane_shared_storage, broadcast_size);
}
LaneSync();&lt;/code&gt;
    &lt;p&gt;I’ve found that this shared buffer just needs to be big enough to broadcast 8 bytes, given that most small data can be broadcasted with a small number of 8 byte broadcasts, and larger data can be broadcasted with a single pointer broadcast.&lt;/p&gt;
    &lt;p&gt;I expose this mechanism with the following API:&lt;/p&gt;
    &lt;code&gt;U64 some_value = 0;
U64 src_lane_idx = 0;
LaneSyncU64(&amp;amp;some_value, src_lane_idx);
// after this line, all lanes share the same value for `some_value`&lt;/code&gt;
    &lt;p&gt;It might be used in the following way:&lt;/p&gt;
    &lt;code&gt;// set `values_count`, allocate for `values`, on lane 0, then
// broadcast their values to all other lanes:
S64 values_count = 0;
S64 *values = 0;
if(LaneIdx() == 0)
{
  values_count = ...;
  values = Allocate(sizeof(values[0]) * values_count);
}
LaneSyncU64(&amp;amp;values_count, 0);
LaneSyncU64(&amp;amp;values, 0);&lt;/code&gt;
    &lt;head rend="h3"&gt;Revisiting The Summation Example&lt;/head&gt;
    &lt;p&gt;With the above mechanisms, we can program the original summation example with the following steps.&lt;/p&gt;
    &lt;p&gt;First, we load the values from the file:&lt;/p&gt;
    &lt;code&gt;U64 values_count = 0;
S64 *values = 0;
{
  File file = FileOpen(input_path);
  values_count = SizeFromFile(file) / sizeof(values[0]);
  if(LaneIdx() == 0)
  {
    values = (S64 *)Allocate(values_count * sizeof(values[0]));
  }
  LaneSyncU64(&amp;amp;values);
  Rng1U64 value_range = LaneRange(values_count);
  Rng1U64 byte_range = R1U64(value_range.min * sizeof(values[0]),
                             value_range.max * sizeof(values[0]));
  FileRead(file, byte_range, values + value_range.min);
  FileClose(file);
}
LaneSync();&lt;/code&gt;
    &lt;p&gt;Then, we perform the sum across all lanes:&lt;/p&gt;
    &lt;code&gt;// grab the shared counter
S64 sum = 0;
S64 *sum_ptr = &amp;amp;sum;
LaneSyncU64(&amp;amp;sum_ptr, 0);

// calculate lane's sum
S64 lane_sum = 0;
Rng1U64 range = LaneRange(values_count);
for(U64 idx = range.min; idx &amp;lt; range.max; idx += 1)
{
  lane_sum += values[idx];
}

// contribute this lane's sum to the total sum
AtomicAddEval64(sum_ptr, lane_sum);
LaneSync();
LaneSyncU64(&amp;amp;sum, 0);&lt;/code&gt;
    &lt;p&gt;And finally, we output the sum value:&lt;/p&gt;
    &lt;code&gt;if(LaneIdx() == 0)
{
  printf(”Sum: %I64d\n”);
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Closing Thoughts&lt;/head&gt;
    &lt;p&gt;The concepts I’ve shared in this post represent what I feel is a fundamental shift in how CPU code can be expressed, compared to the normal single-core code all programmers are familiar with. Through small, additional annotations to code—basic concepts like &lt;code&gt;LaneIdx()&lt;/code&gt;, &lt;code&gt;LaneCount()&lt;/code&gt;, and &lt;code&gt;LaneSync()&lt;/code&gt;—all code can contain the information necessary to be executed wide, using multiple cores to better take advantage of serial independence.&lt;/p&gt;
    &lt;p&gt;The same exact code can also be executed on a single core, meaning through these extra annotations, that code becomes strictly more flexible—at the low level—than its single-core equivalent which does not have these annotations.&lt;/p&gt;
    &lt;p&gt;Note that this is still not a comprehensive family of multithreading techniques, because it is strictly zooming in on one unique timeline of work, and how a single timeline can be accelerated using the fundamental multi-core reality of modern machines. But consider that programs often require multiple heterogeneous timelines of work, where one lane group is not in lockstep with others, and thus should not prohibit others from making progress.&lt;/p&gt;
    &lt;p&gt;But what I appreciate about the ideas in this post is that they do not unnecessarily introduce extra timelines. Communication between two heterogeneous timelines has intrinsic, relativity-related complexity. Those will always be necessary. But why pay that complexity cost everywhere, to accomplish simple multi-core execution?&lt;/p&gt;
    &lt;p&gt;I’m aware that, for many, these ideas are old news—indeed, everyone learns different things at different times. But in my own past programming, and when I look at the programming of many others, it seems that there is an awful lot of overengineering to do what seems trivial, and indeed what is trivial in other domains (like shader programming). So, for at least many people, these concepts do not seem well-known or old (even if they are in some circles and domains).&lt;/p&gt;
    &lt;p&gt;In any case, the concepts I’ve shared in this post have been dramatically helpful in improving my ability to structure multi-core code without overcomplication, and it seemed like an important-enough shift to carefully document it here.&lt;/p&gt;
    &lt;p&gt;I hope it was similarly helpful to you, if you didn’t know the concepts, or if you did, I hope it was nonetheless interesting.&lt;/p&gt;
    &lt;p&gt;If you enjoyed this post, please consider subscribing. Thanks for reading.&lt;/p&gt;
    &lt;p&gt;-Ryan&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.rfleury.com/p/multi-core-by-default"/><published>2025-10-10T07:11:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536325</id><title>A story about bypassing air Canada's in-flight network restrictions</title><updated>2025-10-10T21:32:21.330200+00:00</updated><content>&lt;doc fingerprint="b728f5112168f29d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;1 Prologue&lt;/head&gt;
    &lt;p&gt;A while ago, I took a flight from Canada back to Hong Kong - about 12 hours in total with Air Canada.&lt;/p&gt;
    &lt;p&gt;Interestingly, the plane actually had WiFi:&lt;/p&gt;
    &lt;p&gt;However, the WiFi had restrictions. For Aeroplan members who hadn’t paid, it only offered Free Texting, meaning you could only use messaging apps like WhatsApp, Snapchat, and WeChat to send text messages, but couldn’t access other websites.&lt;/p&gt;
    &lt;p&gt;If you wanted unlimited access to other websites, it would cost CAD $30.75:&lt;/p&gt;
    &lt;p&gt;And if you wanted to watch videos on the plane, that would be CAD $39:&lt;/p&gt;
    &lt;p&gt;I started wondering: for the Free Texting service, could I bypass the messaging app restriction and access other websites freely?&lt;/p&gt;
    &lt;p&gt;Essentially, could I enjoy the benefits of the $30.75 paid service without actually paying the fee? After all, with such a long journey ahead, I needed something interesting to pass the 12 hours.&lt;/p&gt;
    &lt;p&gt;Since I could use WeChat in flight, I could also call for help from the sky.&lt;/p&gt;
    &lt;p&gt;Coincidentally, my roommate happens to be a security and networking expert who was on vacation at home. When I mentioned this idea, he thought it sounded fun and immediately agreed to collaborate. So we started working on it together across the Pacific.&lt;/p&gt;
    &lt;head rend="h2"&gt;2 The Process&lt;/head&gt;
    &lt;p&gt;After selecting the only available WiFi network &lt;code&gt;acwifi.com&lt;/code&gt; on the plane, just like other login-required WiFi networks, it popped up a webpage from &lt;code&gt;acwifi.com&lt;/code&gt; asking me to verify my Aeroplan membership. Once verified, I could access the internet.&lt;/p&gt;
    &lt;p&gt;There’s a classic software development interview question: what happens after you type a URL into the browser and press enter?&lt;/p&gt;
    &lt;p&gt;For example, if you type &lt;code&gt;https://acwifi.com&lt;/code&gt; and only focus on the network request part, the general process is: DNS query -&amp;gt; TCP connection -&amp;gt; TLS handshake -&amp;gt; HTTP request and response.&lt;/p&gt;
    &lt;p&gt;Let’s consider &lt;code&gt;github.com&lt;/code&gt; as our target website we want to access. Now let’s see how we can break through the network restrictions and successfully access &lt;code&gt;github.com&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;3 Approach 1: Disguise Domain&lt;/head&gt;
    &lt;p&gt;Since &lt;code&gt;acwifi.com&lt;/code&gt; is accessible but &lt;code&gt;github.com&lt;/code&gt; is not, is it possible that the network has imposed restrictions on the DNS server, only resolving domain names within a whitelist (such as instant messaging domains)?&lt;/p&gt;
    &lt;p&gt;If this is the case, can I modify &lt;code&gt;/etc/hosts&lt;/code&gt; to disguise my server as &lt;code&gt;acwifi.com&lt;/code&gt;, so that all request traffic passes through my server before reaching the target website (github.com)? For example:&lt;/p&gt;
    &lt;p&gt;The general idea is that I modify the DNS record to bind our proxy server’s IP &lt;code&gt;137.184.231.87&lt;/code&gt; to &lt;code&gt;acwifi.com&lt;/code&gt;. Since the local &lt;code&gt;/etc/hosts&lt;/code&gt; file takes precedence over the DNS server, I can then use a self-signed certificate to tell the browser that this IP is bound to this domain and that it should trust it.&lt;/p&gt;
    &lt;p&gt;Let me first test this idea:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Unexpectedly, the IP was completely unreachable via &lt;code&gt;ping&lt;/code&gt;, meaning the IP was likely blocked entirely.&lt;/p&gt;
    &lt;p&gt;I tried other well-known IPs, like Cloudflare’s CDN IP, and they were also unreachable:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;It seems this approach won’t work. This approach might only work if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The DNS server only answers queries for a specific list of domain names (e.g., WhatsApp, Snapchat, WeChat), which means the firewall’s filtering mechanism was solely based on DNS resolution.&lt;/item&gt;
      &lt;item&gt;The network allows connections to arbitrary IP addresses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After all, if the IPs are directly blocked, no amount of disguise will help. This network likely maintains some IP whitelist (such as WhatsApp and WeChat’s egress IPs), and only IPs on the whitelist can be accessed.&lt;/p&gt;
    &lt;head rend="h2"&gt;4 Approach 2: DNS Port Masquerading&lt;/head&gt;
    &lt;p&gt;When the first approach failed, my roommate suggested a second approach: try using DNS service as a breakthrough:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This is good news! It means there are still ways to reach external networks, and DNS is one of them.&lt;/p&gt;
    &lt;p&gt;Looking at the record above, it shows our DNS query for &lt;code&gt;http418.org&lt;/code&gt; was successful, meaning DNS requests work.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.1 Arbitrary DNS Servers&lt;/head&gt;
    &lt;p&gt;My roommate then randomly picked another DNS server to see if the network had a whitelist for DNS servers:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;We can actually use arbitrary DNS servers - even better!&lt;/p&gt;
    &lt;head rend="h3"&gt;4.2 TCP Queries&lt;/head&gt;
    &lt;p&gt;The fact that arbitrary DNS servers can be queried successfully is excellent news. DNS typically uses UDP protocol, but would TCP-based DNS requests be blocked?&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;DNS TCP queries also work! This indicates the plane network’s filtering policy is relatively lenient, standing a chance of our subsequent DNS tunneling approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.3 Proxy Service on Port 53&lt;/head&gt;
    &lt;p&gt;It seems the plane network restrictions aren’t completely airtight - we’ve found a “backdoor” in this wall.&lt;/p&gt;
    &lt;p&gt;So we had a clever idea: since the plane gateway doesn’t block DNS requests, theoretically we could disguise our proxy server as a DNS server, expose port 53 for DNS service, route all requests through the proxy server disguised as DNS requests, and thus bypass the restrictions.&lt;/p&gt;
    &lt;p&gt;My roommate spent about an hour setting up a proxy server exposing port 53 using xray 1, and sent me the configuration via WeChat:&lt;/p&gt;
    &lt;p&gt;The proxy server configuration my roommate set up with Xray included the following sample configuration:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;And I already had an xray client on my computer, so no additional software was needed to establish the connection.&lt;/p&gt;
    &lt;p&gt;Everything was ready. The exciting moment arrived - pressing enter to access &lt;code&gt;github.com&lt;/code&gt;:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The request actually succeeded! github.com returned a successful result!&lt;/p&gt;
    &lt;p&gt;This means we’ve truly broken through the network restrictions and can access any website!&lt;/p&gt;
    &lt;p&gt;We hadn’t realized before that xray could be used in this clever way :)&lt;/p&gt;
    &lt;p&gt;Here we exploited a simple cognitive bias: not all services using port 53 are DNS query requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;5 Ultimate Approach: DNS Tunnel&lt;/head&gt;
    &lt;p&gt;If Approach 2 still didn’t work, we had one final trick up our sleeves.&lt;/p&gt;
    &lt;p&gt;Currently, the gateway only checks whether the port is 53 to determine if it’s a DNS request. But if the gateway were stricter and inspected the content of DNS request packets, it would discover that our requests are “disguised” as DNS queries rather than genuine DNS queries:&lt;/p&gt;
    &lt;p&gt;Since disguised DNS requests would be blocked, we could embed all requests inside genuine DNS request packets, making them DNS TXT queries. We’d genuinely be querying DNS, just with some extra content inside:&lt;/p&gt;
    &lt;p&gt;However, this ultimate approach requires a DNS Tunnel client to encapsulate all requests. I didn’t have such software on my computer, so this remained a theoretical ultimate solution that couldn’t be practically verified.&lt;/p&gt;
    &lt;head rend="h2"&gt;6 Conclusion&lt;/head&gt;
    &lt;p&gt;With the long journey ahead, my roommate and I spent about 4 hours remotely breaking through the network restrictions, having great fun in the process, proving that our problem-solving approach was indeed feasible.&lt;/p&gt;
    &lt;p&gt;The successful implementation of the solution was mainly thanks to my roommate, the networking expert, who provided remote technical and conceptual support.&lt;/p&gt;
    &lt;p&gt;The only downside was that although we broke through the network restrictions and could access any website, the plane’s bandwidth was extremely limited, making web browsing quite painful. So I didn’t spend much time browsing the web.&lt;/p&gt;
    &lt;p&gt;For the remaining hours, I rewatched the classic 80s time-travel movie: &lt;code&gt;"Back to the Future"&lt;/code&gt; , which was absolutely fantastic.&lt;/p&gt;
    &lt;p&gt;Last and not least, it’s the disclaimer:&lt;/p&gt;
    &lt;p&gt;This technical exploration is intended solely for educational and research purposes. We affirm our strict adherence to all relevant regulations and service terms throughout this project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ramsayleung.github.io/en/post/2025/a_story_about_bypassing_air_canadas_in-flight_network_restrictions/"/><published>2025-10-10T07:50:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536618</id><title>Datastar: Lightweight hypermedia framework for building interactive web apps</title><updated>2025-10-10T21:32:21.001451+00:00</updated><content>&lt;doc fingerprint="32485f0d77aa7edf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Datastar&lt;/head&gt;&lt;head rend="h2"&gt;The hypermedia framework&lt;/head&gt;&lt;quote&gt;x:&lt;lb/&gt;y:&lt;lb/&gt;speed:&lt;/quote&gt;&lt;head rend="h1"&gt;Build reactive web apps that stand the test of time&lt;/head&gt;&lt;p&gt;Datastar is a lightweight framework for building everything from simple sites to real-time collaborative web apps.&lt;/p&gt;&lt;head rend="h2"&gt;Bring Your Own Backend&lt;/head&gt;&lt;p&gt;Harness the simplicity of server-side rendering and the power of a frontend framework, with a single 10.75 KiB file.&lt;/p&gt;&lt;p&gt;Write your backend in the language of your choice (we have SDKs, too).&lt;/p&gt;Get started&lt;p&gt;Datastar accepts &lt;code&gt;text/html&lt;/code&gt; and &lt;code&gt;text/event-stream&lt;/code&gt; content types, so you can send regular HTML responses or stream server-sent events (SSE) from the backend.&lt;/p&gt;&lt;p&gt;See the difference by trying zero and non-zero intervals below.&lt;/p&gt;&lt;head rend="h3"&gt;Hello world!&lt;/head&gt;&lt;quote&gt;&lt;header&gt;Network Response&lt;/header&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Reactive frontends with no user-JS&lt;/head&gt;&lt;p&gt;Datastar allows you to iterate quickly on a slow-moving, high-performance framework.&lt;/p&gt;&lt;head rend="h3"&gt;Datastar solves more problems than it creates&lt;/head&gt;&lt;p&gt;Unlike most frontend frameworks, Datastar simplifies your frontend logic, shifting state management to the backend.&lt;/p&gt;&lt;p&gt;Drive your frontend from the backend using HTML attributes and a hypermedia-driven approach.&lt;/p&gt;&lt;head rend="h4"&gt;State in the right place&lt;/head&gt;&lt;p&gt;Add reactivity to your frontend using &lt;code&gt;data-*&lt;/code&gt; attributes.&lt;/p&gt;&lt;code&gt;Waiting for an order...&lt;/code&gt;&lt;quote&gt;Datastar gives me reactive, realtime applications without the complications of the JS/TS ecosystem. I had to change my way of thinking about building frontends, and I'm Oh-So-Glad I did!&lt;/quote&gt;&lt;quote&gt;Datastar is exactly like React, except without the network, virtual DOM, hooks, or JavaScript. Oh and you get multiplayer and realtime for free. Did I mention you can use any backend language you want? Datastar has solved the frontend for me â I can now get back to solving business problems.&lt;/quote&gt;&lt;quote&gt;Iâve spoken about avoiding SPA complexity for years, and Datastar nails it: real-time UIs with less code than htmx or Alpine.js, and none of the overhead I used to wrestle with.&lt;/quote&gt;&lt;head rend="h5"&gt;Backed by a nonprofit&lt;/head&gt;&lt;head rend="h5"&gt;Supported by a community&lt;/head&gt;&lt;head rend="h5"&gt;Coded by hand&lt;/head&gt;&lt;p&gt;Simple. Fast. Light. No VCs. More About Us&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://data-star.dev/"/><published>2025-10-10T08:46:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536694</id><title>Show HN: I invented a new generative model and got accepted to ICLR</title><updated>2025-10-10T21:32:20.875902+00:00</updated><content>&lt;doc fingerprint="345c0b1f68b2c5a6"&gt;
  &lt;main&gt;&lt;p&gt;🥳 Accepted by ICLR 2025&lt;lb/&gt;📝 Released a blog with added insights&lt;/p&gt;&lt;p&gt;Discrete Distribution Networks&lt;/p&gt;&lt;p&gt;A novel generative model with simple principles and unique properties&lt;/p&gt;&lt;p&gt;This GIF demonstrates the optimization process of DDN for 2D probability density estimation:&lt;/p&gt;&lt;code&gt;blur_circles&lt;/code&gt; -&amp;gt; &lt;code&gt;QR_code&lt;/code&gt; -&amp;gt; &lt;code&gt;spiral&lt;/code&gt; -&amp;gt; &lt;code&gt;words&lt;/code&gt; -&amp;gt; &lt;code&gt;gaussian&lt;/code&gt; -&amp;gt; &lt;code&gt;blur_circles&lt;/code&gt; (same at beginning and end, completing a cycle)&lt;p&gt;Contributions of this paper:&lt;/p&gt;&lt;p&gt; Left: Illustrates the process of image reconstruction and latent acquisition in DDN. Each layer of DDN outputs &lt;lb/&gt; Right: Shows the tree-structured representation space of DDN's latent variables. Each sample can be mapped to a leaf node on this tree.&lt;/p&gt;&lt;p&gt;Reviews from ICLR:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I find the method novel and elegant. The novelty is very strong, and this should not be overlooked. This is a whole new method, very different from any of the existing generative models.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;This is a very good paper that can open a door to new directions in generative modeling.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We introduce a novel generative model, the Discrete Distribution Networks (DDN), that approximates data distribution using hierarchical discrete distributions. We posit that since the features within a network inherently capture distributional information, enabling the network to generate multiple samples simultaneously, rather than a single output, may offer an effective way to represent distributions. Therefore, DDN fits the target distribution, including continuous ones, by generating multiple discrete sample points. To capture finer details of the target data, DDN selects the output that is closest to the Ground Truth (GT) from the coarse results generated in the first layer. This selected output is then fed back into the network as a condition for the second layer, thereby generating new outputs more similar to the GT. As the number of DDN layers increases, the representational space of the outputs expands exponentially, and the generated samples become increasingly similar to the GT. This hierarchical output pattern of discrete distributions endows DDN with unique properties: more general zero-shot conditional generation and 1D latent representation. We demonstrate the efficacy of DDN and its intriguing properties through experiments on CIFAR-10 and FFHQ.&lt;/p&gt;&lt;p&gt;DDN enables more general zero-shot conditional generation. DDN supports zero-shot conditional generation across non-pixel domains, and notably, without relying on gradient, such as text-to-image generation using a black-box CLIP model. Images enclosed in yellow borders serve as the ground truth. The abbreviations in the table header correspond to their respective tasks as follows: “SR” stands for Super-Resolution, with the following digit indicating the resolution of the condition. “ST” denotes Style Transfer, which computes Perceptual Losses with the condition.&lt;/p&gt;&lt;p&gt; (a) The data flow during the training phase of DDN is shown at the top. As the network depth increases, the generated images become increasingly similar to the training images. Within each Discrete Distribution Layer (DDL), &lt;/p&gt;&lt;p&gt;Here, &lt;/p&gt;&lt;p&gt;The numerical values at the bottom of each figure represent the Kullback-Leibler (KL) divergence. Due to phenomena such as “dead nodes” and “density shift”, the application of Gradient Descent alone fails to properly fit the Ground Truth (GT) density. However, by employing the Split-and-Prune strategy, the KL divergence is reduced to even lower than that of the Real Samples. For a clearer and more comprehensive view of the optimization process, see the 2D Density Estimation with 10,000 Nodes DDN page.&lt;/p&gt;&lt;p&gt;The text at the top is the guide text for that column.&lt;/p&gt;&lt;p&gt;Columns 4 and 5 display the generated results under the guidance of other images, where the produced image strives to adhere to the style of the guided image as closely as possible while ensuring compliance with the condition. The resolution of the generated images is 256x256.&lt;/p&gt;&lt;p&gt;To demonstrate the features of DDN conditional generation and Zero-Shot Conditional Generation.&lt;/p&gt;&lt;p&gt;We trained a DDN with output level &lt;/p&gt;&lt;p&gt;Uncompressed raw backup of this video is here: DDN_latent_video&lt;/p&gt;&lt;p&gt;The following content contains personal opinions and is not included in the original paper&lt;/p&gt;&lt;p&gt;Based on the current state of DDN, we speculate on several possible future research directions. These include improvements to DDN itself and tasks suitable for the current version of DDN. Due to my limited perspective, some of these speculations might not be accurate:&lt;/p&gt;&lt;p&gt;Improving DDN through hyperparameter tuning, exploratory experiments, and theoretical analysis:&lt;lb/&gt;The total time spent developing DDN was less than three months, mostly by a single person. Therefore, experiments were preliminary, and there was limited time for detailed analysis and tuning. There is significant room for improvement.&lt;/p&gt;&lt;p&gt;Scaling up to ImageNet-level complexity:&lt;lb/&gt;Building a practical generative model with Zero-Shot Conditional Generation as a key feature.&lt;/p&gt;&lt;p&gt;Applying DDN to domains with relatively small generation spaces.&lt;/p&gt;&lt;p&gt;Applying DDN to non-generative tasks:&lt;/p&gt;&lt;p&gt;Using DDN's design ideas to improve existing generative models:&lt;/p&gt;&lt;p&gt;Applying DDN to language modeling tasks:&lt;/p&gt;&lt;p&gt;Q1: Will DDN require a lot of GPU memory?&lt;/p&gt;&lt;quote&gt;&lt;p&gt;DDN's GPU memory requirements are slightly higher than conventional GAN generator using the same backbone architecture, but the difference is negligible.&lt;/p&gt;&lt;p&gt;During training, generating&lt;/p&gt;&lt;mjx-container&gt;samples is only to identify the one closest to the ground truth, and the&lt;/mjx-container&gt;&lt;mjx-container&gt;unselected samples do not retain gradients, so they are immediately discarded after sampling at the current layer, freeing up memory.&lt;/mjx-container&gt;&lt;p&gt;In the generation phase, we randomly sample an index from&lt;/p&gt;&lt;mjx-container&gt;and only generate the sample at the chosen index, avoiding the need to generate the other&lt;/mjx-container&gt;&lt;mjx-container&gt;samples, thus not occupying additional memory or computation.&lt;/mjx-container&gt;&lt;/quote&gt;&lt;p&gt;Q2: Will there be a mode collapse issue?&lt;/p&gt;&lt;quote&gt;&lt;p&gt;No. DDN selects the output most similar to the current GT and then uses the&lt;/p&gt;&lt;mjx-container&gt;loss to make it even more similar to the GT. This operation naturally has a diverse tendency, which can "expand" the entire generation space.&lt;/mjx-container&gt;&lt;p&gt;Additionally, DDN supports reconstruction. Figure 14 in the original paper shows that DDN has good reconstruction performance on the test set, meaning that DDN can fully cover the target distribution.&lt;/p&gt;&lt;p&gt;The real issue with DDN is not mode collapse but attempting to cover a high-dimensional target distribution that exceeds its own complexity, leading to the generation of blurry samples.&lt;/p&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://discrete-distribution-networks.github.io/"/><published>2025-10-10T09:01:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45537890</id><title>OpenGL: Mesh shaders in the current year</title><updated>2025-10-10T21:32:20.643264+00:00</updated><content>&lt;doc fingerprint="bec39254fca1fee5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mesh Shaders In The Current Year&lt;/head&gt;
    &lt;head rend="h1"&gt;It Happened.&lt;/head&gt;
    &lt;p&gt;Just a quick post to confirm that the OpenGL/ES Working Group has signed off on the release of GL_EXT_mesh_shader.&lt;/p&gt;
    &lt;head rend="h1"&gt;Credits&lt;/head&gt;
    &lt;p&gt;This is a monumental release, the largest extension shipped for GL this decade, and the culmination of many, many months of work by AMD. In particular we all need to thank Qiang Yu (AMD), who spearheaded this initiative and did the vast majority of the work both in writing the specification and doing the core mesa implementation. Shihao Wang (AMD) took on the difficult task of writing actual CTS cases (not mandatory for EXT extensions in GL, so this is a huge benefit to the ecosystem).&lt;/p&gt;
    &lt;p&gt;Big thanks to both of you, and everyone else behind the scenes at AMD, for making this happen.&lt;/p&gt;
    &lt;p&gt;Also we have to thank the nvidium project and its author, Cortex, for single-handedly pushing the industry forward through the power of Minecraft modding. Stay sane out there.&lt;/p&gt;
    &lt;head rend="h1"&gt;Support&lt;/head&gt;
    &lt;p&gt;Minecraft mod support is already underway, so expect that to happen “soon”.&lt;/p&gt;
    &lt;p&gt;The bones of this extension have already been merged into mesa over the past couple months. I opened a MR to enable zink support this morning since I have already merged the implementation.&lt;/p&gt;
    &lt;p&gt;Currently, I’m planning to wait until either just before the branch point next week or until RadeonSI merges its support to merge the zink MR. This is out of respect: Qiang Yu did a huge lift for everyone here, and ideally AMD’s driver should be the first to be able to advertise that extension to reflect that. But the branchpoint is coming up in a week, and SGC will be going into hibernation at the end of the month until 2026, so this offer does have an expiration date.&lt;/p&gt;
    &lt;p&gt;In any case, we’re done here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.supergoodcode.com/mesh-shaders-in-the-current-year/"/><published>2025-10-10T11:56:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45537938</id><title>Weave (YC W25) is hiring a founding AI engineer</title><updated>2025-10-10T21:32:20.121768+00:00</updated><content>&lt;doc fingerprint="951e4c15f89423dd"&gt;
  &lt;main&gt;
    &lt;p&gt;AI to understand engineering work&lt;/p&gt;
    &lt;p&gt;At Weave, we’re building the best software for the best engineering teams to move faster, and we want to hire exceptional engineers to help us do so.&lt;/p&gt;
    &lt;p&gt;We are a well-funded startup, backed by top investors, growing rapidly and currently profitable.&lt;/p&gt;
    &lt;p&gt;You'll be working directly with me (Andrew), the CTO. Before I was CTO of Weave I was the founding engineer at Causal, and I want to give you all the support and growth opportunities in this role that I got when I went through it.&lt;/p&gt;
    &lt;p&gt;You’ll also be working directly with Adam, the CEO. Adam runs sales at Weave, and before that worked as a sales executive at a few different high growth startups.&lt;/p&gt;
    &lt;p&gt;You are a good fit for Weave if you are a formidable engineer. This means you stop at nothing to accomplish your goal. We don't care much about your current skills or even what you've done before; we care that you will be able to do anything you set your mind to.&lt;/p&gt;
    &lt;p&gt;You must also be pragmatic. Weave is a startup so something is always on fire. You need to know when to let little fires burn and when to break out the extinguisher.&lt;/p&gt;
    &lt;p&gt;You must be a very good engineer who's committed to becoming a great engineer. The slope is more important than the Y-intercept.&lt;/p&gt;
    &lt;p&gt;You must be empathetic. We're building products for other people, so you need to be able to understand how other people think and why.&lt;/p&gt;
    &lt;p&gt;You must care about helping other software engineering teams be great. If that's not an exciting mission for you, it will be hard to stay motivated through the inevitable highs and lows.&lt;/p&gt;
    &lt;p&gt;You must be an excellent communicator. You’ll be working on a product that’s communicating with millions of engineers and leaders, so you need to be clear.&lt;/p&gt;
    &lt;p&gt;Finally you must be gritty. You should be accustomed to picking the hard option and pushing through it.&lt;/p&gt;
    &lt;p&gt;(Please feel free to apply even if some or all of these don't apply to you!)&lt;/p&gt;
    &lt;p&gt;Our tech stack is React + TypeScript on the frontend, Go on the backend, and Python for ML. Experience with any of those three languages is a bonus.&lt;/p&gt;
    &lt;p&gt;If you've already done lots of thinking about engineering productivity and how to improve it, that's great and we want to hear about it!&lt;/p&gt;
    &lt;p&gt;We hope your design sensibilities are passable.&lt;/p&gt;
    &lt;p&gt;As Weave’s founding AI engineer, your job is to build AI to understand and improve the work that software engineers do. You’ll be building our processes and standards as you go to make building every incremental feature easier. Your goal will be to delight customers with intelligence that makes their job 10x easier.&lt;/p&gt;
    &lt;p&gt;At Weave, we’re building the best software for the best engineering teams to move faster, and we want to hire exceptional engineers to help us do so.&lt;/p&gt;
    &lt;p&gt;We are a well-funded startup, backed by top investors and growing rapidly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/weave-3/jobs/SqFnIFE-founding-ai-engineer"/><published>2025-10-10T12:01:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45538137</id><title>Igalia, Servo, and the Sovereign Tech Fund</title><updated>2025-10-10T21:32:15.543148+00:00</updated><content>&lt;doc fingerprint="49fb4a41ab1a2764"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Igalia, Servo, and the Sovereign Tech Fund&lt;/head&gt;
    &lt;head rend="h5"&gt;"Weâre proud to help shape the future of web engines through public investment in accessibility, embeddability, and sustainability."&lt;/head&gt;
    &lt;p&gt;Igalia is excited to announce a new commission from the Sovereign Tech Fund to advance the Servo web engine. As stewards of Servo, Igalia is honored to receive support for a multi-pronged effort focused on public interest, developer usability, and long-term sustainability.&lt;/p&gt;
    &lt;p&gt;Servo is a modern, parallelized web engine written in Rust, a Linux Foundation Europe project which Igalia has been actively maintaining since 2023, Servo represents a bold rethinking of browser architecture. Its modular design has made it a valuable resource across the Rust ecosystem. But like many promising open source technologies, Servo needs sustained investment to reach its full potential.&lt;/p&gt;
    &lt;p&gt;Thanks to investment from the Sovereign Tech Fund, Igalia will focus some important work in the next year in three key areas:&lt;/p&gt;
    &lt;head rend="h3"&gt;ð§ Initial Accessibility Support&lt;/head&gt;
    &lt;p&gt;As Servo adoption grows, so does the need for inclusive design. Today, Servo lacks the foundational accessibility features required to support screen readers and other assistive technologies. This limits its usability in many real-world scenarios, and doesnât match our values. Despite its importance, accessibility is often one of a few things that is difficult to find funding for. Weâre grateful that thanks to this investment, weâll be able to implement initial accessibility support to ensure that Servo can serve all users. This work is essential to making Servo a viable engine for public-facing applications.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð§© WebView API&lt;/head&gt;
    &lt;p&gt;Embedding Servo into applications requires a stable and complete WebView API. While early work exists, itâs not yet ready for general use. Weâll be finishing the WebView API to make Servo embeddable in desktop and mobile apps, unlocking new use cases and enabling broader adoption. A robust embedding layer is critical to Servoâs eventual success as a general-purpose engine.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð§ Project Maintenance&lt;/head&gt;
    &lt;p&gt;Servo is more than a browser engineâitâs a collection of crates used widely across the Rust ecosystem. Maintaining these libraries benefits not just Servo, but the broader web platform. The project and the community have been growing a lot since weâve taken over stewardship. This funding will allow our work will include more issue triage, pull request review, version releases, and governance support. All of this helps ensure that Servo remains active, responsive, and well-maintained for developers and users alike.&lt;/p&gt;
    &lt;p&gt;Igalia has long championed open source innovation in the browser space, from our work on Chromium, WebKit, and Gecko to our leadership in standards bodies and developer tooling. We believe Servo has a unique role to play in the future of web engines, and weâre thrilled to help guide its next chapter.&lt;/p&gt;
    &lt;p&gt;Many thanks to the Sovereign Tech Fund for recognizing the importance of this work. We look forward to sharing progress as we go.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.igalia.com/2025/10/09/Igalia,-Servo,-and-the-Sovereign-Tech-Fund.html"/><published>2025-10-10T12:21:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45538760</id><title>Google Safe Browsing incident</title><updated>2025-10-10T21:32:14.615516+00:00</updated><content>&lt;doc fingerprint="822a4d015b631e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google Safe Browsing incident&lt;/head&gt;
    &lt;p&gt;By Eric Selin - Founder, statichost.eu&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;For approximately six hours on 25.9.2025, the entire statichost.eu domain was flagged as deceptive by Google Safe Search. This meant that anyone using Google Safe Search was shown a very aggressive warning or outright blocked when trying to access any site on the&lt;/p&gt;&lt;code&gt;statichost.eu&lt;/code&gt;domain. In some cases even custom domains hosted on the statichost.eu platform were affected. This post is part incident report and part privacy (and monopoly) rant.&lt;/quote&gt;
    &lt;p&gt;Note: This post sparked some discussion on Hacker News, which is of course great. I’d like to clarify that I do not hate Google, nor do I think that they did anything particularly wrong by flagging malicious content (albeit with a pretty wide net). I’m simply saying that Google is pretty darn big, and that I personally think they are too big.&lt;/p&gt;
    &lt;p&gt;Google has too much power over the Internet. Or in the most objective way possible: Google controls and/or monitors a substantial part of every single interaction on the Internet. You may think that this is fine, and that is your right, although I very much disagree. Especially since Google blocked all of statichost.eu for “over five billion” devices for several hours. Here is how it went down:&lt;/p&gt;
    &lt;p&gt;I woke up to some pretty bad news on Monday a couple of weeks ago. A few users had started reporting that &lt;code&gt;statichost.eu&lt;/code&gt; is unavailable due to a security
warning. This is not great, I think to myself, and go into incident response
mode. Immediately, I check https://www.statichost.eu, and see that it’s working.
No TLS issues or other technical problems - maybe a browser issue or network
problem?&lt;/p&gt;
    &lt;p&gt;Ok, so I start investigating. The affected users all mention Google, so I start there. I use Chromium for Google-specific things (only), so I open it up and fire up a Google search. I actually cannot see &lt;code&gt;statichost.eu&lt;/code&gt; on Google
now, which is weird - it should be the top-ranked result for my keywords (e.g.
“europen static hosting”). While I wait for Google Search Console to load, I
check www.statichost.eu again in Chromium, just in case.&lt;/p&gt;
    &lt;p&gt;And BOOM! There it is. Now I start panicing. Google is blocking me from my own website! It apparently thinks I might be deceived - I guess into doing something I shouldn’t do or something I’ll regret later?&lt;/p&gt;
    &lt;p&gt;Back in the Search Console, which has now loaded all its JavaScript and whatnot, I see a giant error message: “Security issues detected”. There seems to be a problem with phishing on the statichost.eu domain. All sites on statichost.eu get a &lt;code&gt;SITE-NAME.statichost.eu&lt;/code&gt; domain, and during the weekend there was an
influx of phishing sites. As a result of that, &lt;code&gt;statichost.eu&lt;/code&gt; ended up on the
Google Safe Browsing list of “dangerous”
sites. Luckily, Google provided me with a helpful list of the offending sites,
which I could then promptly delete.&lt;/p&gt;
    &lt;p&gt;It is of course impossible to talk to anyone at Google in order to fix this, but there is a “request review” button. After writing up an explanation and requesting a review, all I could do was wait. I prepared for the worst, but within a few hours, the block was lifted and an automatically generated response of the same appeared as a notification in Search Console. Not even an email was sent. Nonetheless: incident over.&lt;/p&gt;
    &lt;p&gt;Anyway, back to Google.&lt;/p&gt;
    &lt;p&gt;The stated goal of Google Safe Browsing is “Making the worldâs information safely accessible.”. Yikes! But what does it mean? It is basically a giant blacklist of sites that Google has deemed unworthy. This list is then used by major browsers and anyone who wants to “make information safely accessible” or whatever. According to Google, this protects “over five billion devices”. That of course means that you really don’t want to end up on this list!&lt;/p&gt;
    &lt;p&gt;And do you know how Google builds this list? By doing what they do best: by monitoring absolutely everything. One tool for this is Google Chrome - a “free” browser created by Google for its business purposes. It of course sends the URLs of pages you visit back to Google - I very much assume by default. And with “enhanced security protection” turned on, it even sends some of the page content to Google. That is a very neat way to monitor the comings and goings of something like four billion people.&lt;/p&gt;
    &lt;p&gt;To be fair, many or even most sites on the Google Safe Browsing blacklist are probably unworthy. But I’m pretty sure this was not the first false positive. And I’m not sure this is the best way to tackle phishing. E.g. what happens on the countless phishing sites that are not on this list? Be that as it may, do not rely on Google to tell you who to trust. Use your own judgement and hard-earned Internet street smarts.&lt;/p&gt;
    &lt;p&gt;There are lots of problems on the Internet, but I for one don’t trust Google to be our savior. There was a time when Google was different, but do not mistake their friendly branding and legacy goodwill for something it is not.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;In order to limit the impact of similar issues in the future, all sites on statichost.eu are now created with a&lt;/p&gt;&lt;code&gt;statichost.page&lt;/code&gt;domain instead. This domain is pending addition to the Public Suffix List in order to further increase resilience and security.&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.statichost.eu/blog/google-safe-browsing/"/><published>2025-10-10T13:27:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45539296</id><title>All-natural geoengineering with Frank Herbert's Dune</title><updated>2025-10-10T21:32:13.664357+00:00</updated><content>&lt;doc fingerprint="8f78b11b0824fe90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;All-Natural Geoengineering with Frank Herbert's Dune&lt;/head&gt;
    &lt;head rend="h3"&gt;Can We Terraform the Earth Using Life Itself?&lt;/head&gt;
    &lt;p&gt;Science fiction understood something fundamental before science caught up. Frank Herbert’s Dune imagined the Fremen attempting to terraform Arrakis. They succeeded technically but discovered too late (though Paul Atreides and the God Emperor Leto knew exactly what they were doing) they’d destroyed the desert ecology their entire civilization depended on. The sandworms died. The spice disappeared. Their power evaporated. Half a century after Herbert, The Expanse imagined the Protomolecule, alien biotechnology designed to hijack existing “self replicating systems” and reorganize it into building the Ring Gates. Both stories grasped the same insight: life is technology. Self-replicating, self-maintaining, infinitely adaptable. Deploy living systems at planetary scale and you’ll discover reciprocal dependencies you can’t escape.&lt;/p&gt;
    &lt;p&gt;In 1965, James Lovelock proposed this same idea as scientific hypothesis while working for NASA’s Viking missions. Earth’s atmosphere appeared too far from chemical equilibrium to be explained by geology alone. Life wasn’t simply adapting to planetary conditions. It was actively regulating them as much as it was responding to them. The Gaia hypothesis suggested that living organisms interact with their inorganic surroundings to maintain conditions suitable for life, effectively operating as a self-regulating system at planetary scale (though shocks like giant meteors and system disruptions can still collapse it).&lt;/p&gt;
    &lt;p&gt;We see this with other creatures besides ourselves. Beavers are recognized as quintessential ecosystem engineers, with remarkable abilities to modify ecosystems profoundly through dam construction, altering river corridor hydrology, geomorphology, nutrient cycling, and ecosystems. The southern Amazon rainforest triggers its own rainy season using water vapor from plant leaves, providing observational evidence that forests actively create their own weather systems.&lt;/p&gt;
    &lt;p&gt;On the practical sense? Citizens already pay environmental costs whether governments acknowledge it or not: flooded basements, insurance spikes, hurricane damage, wildfire smoke. Municipal budgets hemorrhage money on disaster recovery that grows more expensive each year. People worry about the future.&lt;lb/&gt;The governance choice is simpler than climate debates suggest: exam to see if deploying biological infrastructure providing one or multiple services at once (mangrove forests that reduce storm surge, support fisheries, and maintain themselves for decades etc etc ). &lt;lb/&gt;The bottleneck isn’t only knowledge about what works and what not. They lack procurement frameworks, trained contractors, and political authorization to fund infrastructure maturing over five years instead of delivering ribbon-cutting photo opportunities.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Hydrology: Water That Engineers Itself&lt;/head&gt;
    &lt;head rend="h3"&gt;Beavers: 60 Million Years of Autonomous Watershed R&amp;amp;D&lt;/head&gt;
    &lt;p&gt;Beavers (Castor fiber and Castor canadensis) are among the most influential mammalian ecosystem engineers, heavily modifying river corridor hydrology and geomorphology primarily through dam construction, which impounds flow and increases open water extent. Simulations show beaver dam construction can result in a 90% increase in groundwater discharge from wetland ponds in systems connected to regional groundwater flow. The dams create stepped water tables that slow floods during storms and extend groundwater availability during droughts: natural water storage infrastructure that adjusts dynamically to conditions.&lt;/p&gt;
    &lt;p&gt;The engineering is sophisticated. Dams trap sediment, creating rich substrate for vegetation while filtering water. Oyster reefs and beaver structures share similar mechanics: both attenuate wave energy and reduce estuarine currents while stabilizing seabed sediments. Their modifications increase water storage and create wildfire refugia. A 2018 technical report documented native riparian vegetation persisting unburnt during Idaho’s Sharps Fire where active beaver dams were present. Beaver wetland ecosystems have persisted throughout the Northern Hemisphere during numerous prior periods of climatic change, demonstrating remarkable adaptive capacity.&lt;/p&gt;
    &lt;p&gt;Beaver Dam Analogues (BDAs)&lt;/p&gt;
    &lt;p&gt;Where beavers can’t return immediately, humans build like beavers. BDAs are low-tech, cost-effective stream restoration tools built with natural materials like willow and aspen, using vertical posts woven with brushy vegetation and packed with mud to mimic beaver dams. Two years after the National Forest Foundation built beaver dam analogues along Colorado’s Trail Creek, beavers moved back and began building dams of their own. The structures don’t replace beavers. They create conditions for beavers to return and take over maintenance, converting capital expenditure into self-maintaining biological infrastructure.&lt;/p&gt;
    &lt;p&gt;NASA uses satellite Earth observations through a program with Boise State University to track how reintroduced beavers change Idaho’s landscape, producing images from space showing areas with reintroduced beavers are greener than areas without them. Recent research using explanatory modeling of 87 beaver pond complexes found that dam length, woody vegetation height, and stream power index explained 74% of the variation in pond area, providing empirical foundations for site selection in beaver restoration.&lt;/p&gt;
    &lt;p&gt;Organizations Leading BDA Implementation&lt;/p&gt;
    &lt;p&gt;Beaver Institute: Runs BeaverCorps, the only professional non-lethal beaver management training program&lt;/p&gt;
    &lt;p&gt;Ecotone, Inc.: Ecological restoration company implementing BDAs across Maryland&lt;/p&gt;
    &lt;p&gt;Anabranch Solutions: River restoration company that documented beaver fire mitigation&lt;/p&gt;
    &lt;p&gt;Beaver Deceivers International: Company focused on infrastructure protection while allowing beaver habitat improvement&lt;/p&gt;
    &lt;head rend="h3"&gt;Bioswales: Engineered Filtration Systems&lt;/head&gt;
    &lt;p&gt;Bioswales are the most effective type of green infrastructure in slowing runoff velocity and cleansing water while recharging groundwater. These linear wetlands function through multiple pathways. A Davis, California study eight years after construction found treatment bioswales reduced surface runoff by 99.4%, nitrogen, phosphate, and total organic carbon loading by 99.1%, 99.5%, and 99.4% respectively. The engineered soil mix (75% native lava rock and 25% loam) replaced native soil to maximize performance.&lt;/p&gt;
    &lt;p&gt;The physical design matters. Bioswales feature gently sloped sides (ideally 4:1, maximum 3:1) with slight longitudinal slopes that move water along the surface, allowing sediments and pollutants to settle while localized groundwater recharge occurs through infiltration. Studies documented temperature reductions of 2-4°C in and around bioswale elements, contributing to urban heat island mitigation. In water-scarce regions with declining aquifer levels, bioswales help replenish groundwater resources, offsetting impacts of impervious surfaces on the hydrologic cycle.&lt;/p&gt;
    &lt;p&gt;More than 500 residential areas with bioswales are spread across the Netherlands, especially in newer districts. Research shows bioswales continue functioning well even in extreme weather conditions and in low-lying areas with high groundwater levels and low soil permeability. Gdańskie Wody adopted a pioneering strategy starting construction of the first rain garden in 2018, with organizational policy now stipulating construction of nature-based solutions in new housing estates without building rainwater drainage, creating systematic deployment through regulation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Rain Gardens: Strategic Stormwater Interception&lt;/head&gt;
    &lt;p&gt;Rain gardens are shallow, landscaped depressions designed to capture, treat, and infiltrate stormwater runoff as it moves downstream, sized to treat the “first flush” (the first and most polluted volume from storm events). Compared to conventional lawn, one rain garden allows approximately 30% more water to infiltrate into the ground and contribute to regional underground aquifer recharge. A 2021 study using gradient boosting machine learning found that vegetation type, plant density, and flow conditions significantly affect infiltration rates, with models achieving 97.6% correlation accuracy.&lt;/p&gt;
    &lt;p&gt;Two types serve different contexts. Infiltration rain gardens allow runoff to pass through mulch and soil layers, slowly dispersing water into native soils and controlling runoff volumes. Filtration rain gardens use similar processes but pipe water elsewhere, used where infiltration to underlying soils is unsafe due to contamination concerns. Recent modeling based on Darcy’s law found that filtration coefficients and layer thickness are the main parameters affecting saturation depth and water column height. When the top layer’s filtration coefficient is 7.0 cm/h, complete saturation doesn’t occur within 2 hours, allowing continuous storm event management without overflow.&lt;/p&gt;
    &lt;p&gt;The largest sustainable drainage system in Norway was built at Bryggen, Bergen to raise and stabilize groundwater levels, protecting UNESCO World Heritage cultural layers. Full-scale infiltration testing showed capacity of 510-1600 mm/h, with immediate groundwater response in wells within 30m and 2-day delayed response 75-100m away. Rain gardens can recharge aquifers at distance from the installation site.&lt;/p&gt;
    &lt;head rend="h3"&gt;Johads: Community-Owned Water Harvesting&lt;/head&gt;
    &lt;p&gt;Johads are crescent-shaped earthen dams built across contours to slow monsoon runoff. The structures are simple mud-and-rubble barrier check dams with high embankments on three sides, collecting and storing water for groundwater recharge, washing, bathing, and drinking.&lt;/p&gt;
    &lt;p&gt;Tarun Bharat Sangh (TBS), led by Rajendra Singh, has constructed 13,800 functioning rainwater harvesting systems and rejuvenated 13 rivers across India. By 2005, TBS counted 5,000 structures in 750 villages, covering 3,000 square miles over five districts.&lt;/p&gt;
    &lt;p&gt;The river Arvari became perennial in 1995 after successive johads built along its watershed. Four more rivers (Sarsa, Ruparel, Bhagani, and Jahajwali) have become perennial following johad construction.&lt;/p&gt;
    &lt;p&gt;A survey of 970 wells in 120 villages found all were flowing, including 800 that had been dry just six years before. Alwar’s forest spread 33 percent in fifteen years, and groundwater levels rose by nearly 6 meters.&lt;/p&gt;
    &lt;p&gt;TBS enabled communities to form the River Arvari Parliament, comprising members from gram sabhas of 72 villages along the river (one of India’s first community-led river governance structures).&lt;/p&gt;
    &lt;head rend="h3"&gt;Xeriscaping: Water-Wise Native Landscaping&lt;/head&gt;
    &lt;p&gt;Xeriscaping can reduce water consumption by 60% or more compared to regular lawn landscapes. A Turkish study found that switching an average city park to more native vegetation lowered irrigation usage by 30-50%, saving roughly $2 million annually.&lt;/p&gt;
    &lt;p&gt;Native plants have deep root systems that help manage rainwater runoff and maintain healthy soil, mitigating floods and preventing soil compaction. They resist damage from freezing, drought, common diseases, and herbivores without human intervention.&lt;/p&gt;
    &lt;p&gt;Native plants attract other native species, including pollinators, keeping gardens healthy year after year. They require no soil amendments or fertilizer once established.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Coastal &amp;amp; Ecosystem Engineering&lt;/head&gt;
    &lt;p&gt;Oyster Reefs: Living Breakwaters That Filter and Protect&lt;/p&gt;
    &lt;p&gt;Oyster reefs provide ecosystem services including habitat provisioning, water filtration, and shoreline protection, representing one of the most dramatic declines of a foundation species worldwide. Under certain conditions, a single oyster can filter up to 50 gallons of water per day. A healthy reef processes enormous volumes, removing algae, nitrogen, and pollutants.&lt;/p&gt;
    &lt;p&gt;Wave attenuation occurs through breaking, reflection, overtopping, and frictional dissipation, with wave breaking considered most essential for energy attenuation of submerged structures. Wave transmission decreases with increasing freeboard (difference between reef crest elevation and water level), with oyster reefs producing greatest wave attenuation when the crest is at or above still water level. When oyster reef exposure time exceeds 50%, wave height can be reduced by 68%, though this creates less favorable oyster growth conditions. Optimal design requires balancing wave attenuation with oyster inundation requirements (60-80% for optimal growth). Research now focuses on optimizing other reef parameters like width for wave attenuation.&lt;/p&gt;
    &lt;p&gt;Oyster reefs create complex three-dimensional habitat. A 4-inch square patch hosts more than 1,000 individual invertebrates from different biological groups, providing nursery habitat for commercially valuable fish species and supporting food webs that extend well beyond the reef itself.&lt;/p&gt;
    &lt;p&gt;Multiple companies are developing technologies to accelerate reef restoration at scale:&lt;/p&gt;
    &lt;p&gt;The Oyster Restoration Company (Scotland) launched Rapid Reef in 2025, an innovative product using recycled shells as substrate for native oyster spat. Each Rapid Reef bag contains approximately 15,000 oysters covering at least 5m² of seabed.&lt;/p&gt;
    &lt;p&gt;Coastal Technologies Corp developed a nature-inspired oyster reef system that raises oysters off the seafloor using vertical poles with plates, making reefs climate-change-proof since additional height can be added to account for rising sea levels.&lt;/p&gt;
    &lt;p&gt;Oyster Heaven (Netherlands/UK) uses specially designed clay bricks called “Mother Reefs” pre-charged with at least 100 oysters each. Partnered with Purina to deploy 40,000 Mother Reefs (4 million oysters) off Norfolk coast by end of 2026.&lt;/p&gt;
    &lt;p&gt;Van Oord pilots oyster reef restoration integrated with offshore wind infrastructure using “remote setting” method, cultivating oyster larvae in hatcheries before transferring them into seawater-filled containers with rocks.&lt;/p&gt;
    &lt;p&gt;Billion Oyster Project (New York) is restoring oyster reefs to New York Harbor with over 100 schools and nearly 15,000 volunteers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mangroves: Storm Surge Dampeners and Carbon Vaults&lt;/head&gt;
    &lt;p&gt;Mangroves provide an estimated $65 billion per year globally in storm protection services according to 2020 research, with a 2024 study updating this to $855 billion in flood protection services worldwide, accounting for increasing populations, wealth, and storms on coastlines. The dense root systems of mangrove trees can reduce large storm surges by over 50% as they flow through mangrove forests, with roots causing friction that dissipates energy and motion of water. The economic value of mangroves for services that rely on conserving them, such as flood protection, is typically not included within national budgets and wealth accounts, unlike services such as timber production. This creates a systematic undervaluation of preservation.&lt;/p&gt;
    &lt;p&gt;Mangroves store five times more carbon in their soils by surface area than tropical forests and ten times more than temperate forests. They also provide shelter for marine life and absorb microplastics. Mangroves trap sediment, build land through accretion, stabilize coastlines, create critical habitat for commercially valuable fisheries, and filter water.&lt;/p&gt;
    &lt;p&gt;Traditional hand-planting of mangroves is cumbersome in muddy terrain. Multiple organizations now deploy drone technology:&lt;/p&gt;
    &lt;p&gt;Distant Imagery (UAE) was contracted by ADNOC to plant 2.5 million mangrove seedlings across Abu Dhabi using drones that can disperse over 2,000 mangrove seeds in roughly eight minutes. Has planted approximately 1.5 million mangrove trees and claims to be the first in the world to successfully restore mangroves with drones.&lt;/p&gt;
    &lt;p&gt;Dendra Systems (UAE) is completing a $27.3 million project to restore 27 million mangroves across 10,000 hectares in the UAE over 5 years. Their custom seeding drones can seed over 100,000 mangroves in a single day.&lt;/p&gt;
    &lt;p&gt;ReleaseLabs + Panama Flying Labs developed autonomous release systems carrying 750+ seed balls per load, distributing them accurately in less than five minutes over one hectare.&lt;/p&gt;
    &lt;p&gt;UAVs coupled with AI and machine learning enable detailed mapping, 3D modelling, invasive species detection, and measurement of vital parameters like vegetation health, carbon storage, and mangrove changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Seaweed Farming: The Ocean’s Fast-Growing Carbon Sink&lt;/head&gt;
    &lt;p&gt;Seaweed grows remarkably fast, up to two feet per day, allowing rapid carbon dioxide absorption during photosynthesis. A 2025 study in Nature Climate Change found seaweed farming in depositional environments buries carbon in underlying sediments at rates averaging 1.87 ± 0.73 tCO2e ha-1 yr-1, twice that in reference sediments. For the oldest farm studied (300 years in operation), organic carbon stocks reached 140 tC ha-1. Seaweeds absorb dissolved inorganic carbon, converting it into biomass, dissolved organic carbon (DOC), or particulate organic carbon (POC), with offshore aquaculture achieving 94% sequestration rates at depths over 2,000 meters.&lt;/p&gt;
    &lt;p&gt;Seaweed absorbs CO2 more effectively than trees and improves water quality by extracting harmful nutrients. Adding certain seaweed types to cattle feed can reduce methane output by up to 95%.&lt;/p&gt;
    &lt;p&gt;The Climate Foundation is developing fully automated, solar-powered, floating kelp farms using deep cycling (lowering kelp 125 meters each night to nutrient-rich waters), making kelp grow three times faster than shallow-water farming. Sea6 Energy (India/Indonesia) is mechanizing tropical seaweed farming with ‘SeaCombine’, a tractor-like vehicle that sows seeds and harvests sea plants offshore.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Dryland Regeneration and Integrated Systems&lt;/head&gt;
    &lt;head rend="h3"&gt;Nitrogen-Fixing Trees: Atmospheric Fertilizer Factories&lt;/head&gt;
    &lt;p&gt;Nitrogen-fixing trees host Rhizobium bacteria on their roots that convert atmospheric nitrogen into forms absorbable by plant roots, a process called biological nitrogen fixation. These trees don’t just fix nitrogen for themselves. Some species like mesquite (Prosopis) fix nitrogen directly into soil rather than into root nodules, meaning other plants can use this nitrogen immediately.&lt;/p&gt;
    &lt;p&gt;The most suitable trees for dryland soil improvement are slow-growing nitrogen fixers with easily decomposing leaves low in allelochemicals, such as Acacia, Carob, or Albizia. Fast-growing trees like Eucalyptus do little for soil improvement and deplete topsoil humidity, out-competing other vegetation. Dryland trees recover nutrients from deep soil layers, subsequently contributing them as leaf litter to enrich topsoil, essential for returning minerals that have leached beyond reach of shallow-rooted plants.&lt;/p&gt;
    &lt;p&gt;Acacia saligna is a nitrogen-fixing tree native to southwest Western Australia, planted in North Africa and the Middle East for fodder, fuelwood, sand stabilization, and windbreaks. It tolerates mean annual rainfall of 300-1,000mm and temperatures from 4°C to 36°C, though sensitive to frosts below -4°C. A site at Project Wadi Attir overlaid with organic matter displayed ten-fold productivity compared to nearby untreated degraded soil for at least eight years. Acacia woodland with closed leaf litter similarly showed ten-fold productivity compared to nearby degraded shrubland.&lt;/p&gt;
    &lt;head rend="h3"&gt;Integrated Farming Systems: Ancient Wisdom, Modern Refinements&lt;/head&gt;
    &lt;p&gt;Integrated rice-animal farming systems originated in Southeast Asia over 6,000 years ago. The Chinese rice-fish-duck symbiosis system has nearly a thousand years of history as a Globally Important Agricultural Heritage System. These systems manage water, nutrient cycling, and pest control through ecological design rather than chemical inputs (field-scale biological geoengineering).&lt;/p&gt;
    &lt;p&gt;In China’s Congjiang county, 12,600 hectares of rice-fish-duck fields cycle resources: rice shoots provide shade and organic food for fish and ducks, who feed on pests and produce manure, weeding, fertilizing and oxygenating fields without pesticides. Ecosystem services valuation for the Honghe Hani Rice Terraces totaled 3.316 billion CNY: 1.76 billion in provisioning services, 1.32 billion in regulation and maintenance, 230.85 million in cultural services. Ducks eat weeds preventing competition with rice, duck manure fertilizes fields, and dabbling in soil improves water parameters including nitrate, dissolved organic matter, and dissolved oxygen. Production rises compared to rice monoculture. Fish and the nitrogen-fixing aquatic fern azolla integrate for nutrient enhancement and feed supplementation.&lt;/p&gt;
    &lt;p&gt;Modern Refinements: The Furuno System&lt;/p&gt;
    &lt;p&gt;Japanese farmer Takao Furuno refined traditional aigamo (duck-rice) methods in the 1980s, developing an integrated system that matches or surpasses conventional chemical-intensive yields while eliminating synthetic inputs. Through systematic experimentation, Furuno identified optimal parameters: ducklings released at 7 days old, 15-30 ducklings per tenth hectare, removal at 8 weeks to prevent rice grain consumption. Adding loaches (freshwater fish) and azolla to fields boosted rice and duck growth while supplying duck nutrition. Wire strung across fields deterred birds of prey.&lt;/p&gt;
    &lt;p&gt;Furuno’s 3.2-hectare farm generates US$160,000 annually from rice, organic vegetables, eggs, and ducklings(approximately $50,000 per hectare). Modern enclosure systems and artificial hatching on precise schedules reduced labor costs compared to traditional methods. Manual weeding requires 240 person-hours per hectare annually; integrated duck-rice systems eliminate this entirely while farmers gain time for family or other activities. Through writing, lectures, and cooperation with agricultural organizations and governments, Furuno’s methods spread to more than 75,000 farmers in Japan, Korea, China, Vietnam, the Philippines, Laos, Cambodia, Malaysia, Bangladesh, Iran, and Cuba.&lt;/p&gt;
    &lt;p&gt;Even thousand-year-old systems face extinction. The Chinese rice-fish-duck system confronts threats: rural labor transfer, low marketization and industrialization, weakening cultural awareness, and climate change. Invasive golden apple snails now eat azolla, reducing its effectiveness. Ecological disruptions compound.&lt;/p&gt;
    &lt;p&gt;Indigenous North American agriculture developed complementary polyculture independently. The Three Sisters system (corn, beans, squash) uses niche complementarity: cornstalks serve as trellises for climbing beans, beans fix nitrogen in soil through Rhizobium bacteria, and wide squash leaves shade ground, keeping soil moist and preventing weed establishment. A modern experiment found Haudenosaunee Three Sisters polyculture provided both more energy and more protein than any local monoculture. Meta-analyses show intercropping provides 22-32% yield advantage compared to monocrops when normalized for land area. Intercropping with diverse plant resource acquisition strategies promotes efficient resource use, with positive belowground effects on soil biota.&lt;/p&gt;
    &lt;p&gt;Intercropping creates complex canopy structures making mechanized harvesting very difficult. Close-knit intercropping often requires precise weed control and hand-harvesting, currently limiting it to smaller scales. Improvements in image-recognition software and robotics for automated management and harvesting may eventually enable large-scale intercropping. Three Sisters principles inform design of mechanizable integrated approaches.&lt;/p&gt;
    &lt;p&gt;The Automation Gradient&lt;/p&gt;
    &lt;p&gt;Hand labor required: Three Sisters simultaneous polyculture (complex canopy, irregular plant heights, intertwined root systems).&lt;/p&gt;
    &lt;p&gt;Partially mechanizable: Rice-duck systems (mechanized rice planting/harvesting with standard equipment, manual duck management, temporal overlap during growing season).&lt;/p&gt;
    &lt;p&gt;Fully mechanized: Rice-crawfish rotation (complete temporal separation, standard rice harvesting equipment, automated flooding cycles).&lt;/p&gt;
    &lt;p&gt;Future potential: Advanced robotics enabling complex polyculture at scale (under development).&lt;/p&gt;
    &lt;p&gt;Rice-Crawfish: Industrial-Scale Integration&lt;/p&gt;
    &lt;p&gt;Louisiana’s rice-crawfish rotation is the state’s most valuable aquaculture commodity: approximately 184,000 acres producing crawfish valued at approximately $170 million. Louisiana accounts for 96.4% of U.S. crawfish sales. The system scales industrially by solving constraints that limit other integrated approaches.&lt;/p&gt;
    &lt;p&gt;Temporal separation: Rice is planted, grown, and harvested using standard equipment. Fields are then reflooded for crawfish, with no simultaneous crops requiring selective harvesting.&lt;/p&gt;
    &lt;p&gt;Infrastructure compatibility: Fields suitable for rice production work for crawfish (flat soils, levees for water control, irrigation systems already in place). Many farmers in southern Louisiana already had flood irrigation systems favoring rice-crawfish over rice-soybean rotations.&lt;/p&gt;
    &lt;p&gt;Minimal equipment modification: Specialized crawfish harvesting boats and traps are additions, not replacements, for existing rice infrastructure. Rice harvesting proceeds exactly as in monoculture.&lt;/p&gt;
    &lt;p&gt;Revenue diversification without complexity: Crawfish provide income during off-peak periods using permanent farm labor and equipment. Crawfish “caught in Jeff Davis Parish in the morning can be consumed in Houston tonight”.&lt;/p&gt;
    &lt;p&gt;Self-sustaining biology: Crawfish feed on rice stubble creating a detritus-based food chain, requiring no supplemental feeding. Natural reproduction eliminates need for hatcheries. Vegetation that grew during summer breaks down to support natural food web yielding 350-900 lb harvestable crawfish per acre.&lt;/p&gt;
    &lt;p&gt;Ecosystem benefits: Less disease pressure after crawfish compared to soybean rotation. Crawfish ponds serve as wetland habitat for waterfowl, wading birds, and furbearers. Water leaving ponds often equals or exceeds input quality.&lt;/p&gt;
    &lt;p&gt;Rice-crawfish scales through temporal separation, infrastructure compatibility, simple logistics. Rice-fish and rice-duck systems have relatively higher ecological adaptability than other integrated systems, making them suitable for large-scale application across varied climates. Only rice-crawfish achieved widespread industrial adoption in Western contexts by solving the automation challenge through rotation rather than simultaneity.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Atmospheric Systems: Trees as Rain Makers&lt;/head&gt;
    &lt;p&gt;Biogenic Aerosols: How Trees Seed Clouds&lt;/p&gt;
    &lt;p&gt;Trees don’t just respond to weather. They create it through chemical signaling.&lt;/p&gt;
    &lt;p&gt;The most important natural gases involved in cloud formation are isoprenes, monoterpenes, and sesquiterpenes: hydrocarbons mainly released by vegetation that are key components of essential oils we smell when grass is cut or during forest walks. When these substances oxidize (react with ozone) in air, they form aerosols. The oxidation of a natural mixture of isoprene, monoterpenes and sesquiterpenes in pure air produces Ultra-Low-Volatility Organic Compounds (ULVOCs) that form particles very efficiently, which can grow over time to become cloud condensation nuclei.&lt;/p&gt;
    &lt;p&gt;At equivalent concentrations, sesquiterpenes form particles at a rate ten times higher than monoterpenes or isoprenes. A single sesquiterpene molecule comprises 15 carbon atoms, while monoterpenes have ten and isoprenes merely five. The larger molecular structure enables more efficient particle formation.&lt;/p&gt;
    &lt;p&gt;CERN’s CLOUD chamber (the purest sealed environment globally) simulates varied atmospheric conditions at extremely low sesquiterpene concentrations found in nature, allowing researchers to study biogenic particle formation under pre-industrial conditions (without anthropogenic sulfur dioxide emissions). CLOUD findings show that isoprene from forests represents a major source of biogenic particles currently missing in climate models, with isoprene now recognized as capable of forming new particles in the atmosphere, contrary to prior assumptions.&lt;/p&gt;
    &lt;p&gt;With tighter environmental regulations, sulfur dioxide concentration has declined significantly. Terpene concentration increases because plants release more when experiencing stress: higher temperatures, extreme weather, and droughts. Sesquiterpenes should be included as a separate factor in future climate models alongside isoprenes and monoterpenes, especially given the decrease in atmospheric sulfur dioxide concentrations and simultaneous increase in biogenic emissions due to climate stress.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Amazon: A Rain Machine&lt;/head&gt;
    &lt;p&gt;The Amazon rainforest triggers its own rainy season 2-3 months before seasonal winds bring ocean moisture. NASA’s Aura satellite measurements show moisture high in deuterium (a heavy isotope signature proving transpiration rather than ocean evaporation). The deuterium content was highest at the end of the dry season during peak photosynthesis.&lt;/p&gt;
    &lt;p&gt;On a typical day, trees release 20 billion tons of moisture into the atmosphere, with moisture recycled from sky to land five to six times as clouds move westward. As tree-induced rain clouds release rain, they warm the atmosphere, causing air to rise and triggering circulation large enough to shift wind patterns that bring in more ocean moisture. The forest essentially summons its own rainy season.&lt;/p&gt;
    &lt;p&gt;The “biotic pump theory” proposes the Amazon as the beating “heart of the Earth” (millions of trees working together releasing water vapor that circulates weather patterns globally). Flying rivers carry rainwater in atmospheric streams influencing rainfall as far as Argentina and potentially the Western United States. Evapotranspiration from the Amazon basin provides atmospheric moisture that influences weather patterns and rainfall as far away as the US, meaning forest loss may contribute to droughts and wildfire risks far beyond South America.&lt;/p&gt;
    &lt;p&gt;Over a large fraction of the southern Amazon, the dry season is now only a few weeks shorter on average than the transitional threshold between wet forest and savanna. There has already been some irreversible damage, with delayed wet season onset evidence that deforestation is playing a role in reducing the forest’s cloud-building capacity.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Unintended Geoengineering Experiment: Ship Tracks&lt;/head&gt;
    &lt;p&gt;In 2020, UN International Maritime Organization regulations cut ships’ sulfur pollution by more than 80%, lessening the effect of sulfate particles in seeding and brightening ship track clouds (distinctive low-lying, reflective clouds that help cool the planet). Ship tracks were first observed as “anomalous cloud lines” in 1960s weather satellite images, formed by water vapor coalescing around small particles of pollution in ship exhaust, with highly concentrated droplets scattering more light and appearing brighter than non-polluted marine clouds seeded by larger particles like sea salt.&lt;/p&gt;
    &lt;p&gt;A 2024 PNNL study found that nearly 20 percent of 2023’s record warmth likely came from reduced sulfur emissions from shipping, with machine learning scanning over a million satellite images revealing a 25 to 50 percent reduction in visible tracks. The 2020 regulation led to a radiative forcing of +0.2±0.11 W/m² averaged over the global ocean, potentially doubling the warming rate in the 2020s compared with rates since 1980, with strong spatiotemporal heterogeneity. In shipping corridors where maritime traffic is particularly dense, the increased light represents a 50% boost to the warming effect of human carbon emissions (equivalent to losing the cooling effect from a fairly large volcanic eruption each year).&lt;/p&gt;
    &lt;p&gt;Marine Cloud Brightening Research&lt;/p&gt;
    &lt;p&gt;The irony: We accidentally discovered we’d been cooling the planet with pollution, stopped it for health reasons (correctly), and now multiple organizations are studying how to replicate the cooling effect using benign materials.&lt;/p&gt;
    &lt;p&gt;The Marine Cloud Brightening Research Program at University of Washington, funded by SilverLining’s Safe Climate Research Initiative, is an open collaboration of atmospheric scientists studying how clouds respond to aerosols to investigate the feasibility and potential impacts of reducing climate warming by intentionally increasing reflection of sunlight from marine clouds.&lt;/p&gt;
    &lt;p&gt;The leading proposed method is generating a fine mist of sea salt from seawater (~200 nm particles) and delivering it into targeted marine stratocumulus clouds from ships traversing the ocean. Small-scale field tests were conducted on the Great Barrier Reef in 2024. Lowercarbon Capital, with over $800 million in assets, is supporting the nonprofit Marine Cloud Brightening Project alongside academic institutions.&lt;/p&gt;
    &lt;p&gt;The Marine Cloud Brightening Project team at University of Washington, PARC, and Pacific Northwest National Laboratory developed effervescent nozzles that spray tiny droplets of saltwater. They see several key advantages: marine clouds over dark ocean surfaces yield highest albedo change, and clouds are conveniently close to the liquid they want to spray.&lt;/p&gt;
    &lt;p&gt;Marine cloud brightening is based on phenomena currently observed in the climate system. Today, emissions particles like soot mix with clouds and increase sunlight reflection, producing a cooling effect estimated between 0.5 and 1.5°C (one of the most important unknowns in climate science). The National Academies of Sciences recommends the US invest $100-200 million in solar geoengineering research over 5 years to determine whether the technology should be on the table as potential climate change mitigation.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Failure Modes, Tradeoffs, and Scaling Challenges&lt;/head&gt;
    &lt;p&gt;The examples above demonstrate elegant natural systems. Scaling them requires confronting four categories of problems: ecological mismatches, governance failures, industrial constraints, and political fragility.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecological Mismatch: The Icelandic Lupin Lesson&lt;/head&gt;
    &lt;p&gt;Nootka lupine (Lupinus nootkatensis), native to Alaska and British Columbia, was introduced to Iceland in 1945 to combat erosion. As a nitrogen fixer hosting bacteria that gather atmospheric nitrogen, the plant successfully reversed catastrophic topsoil loss from centuries of overgrazing. Dense lupine cover and soil fertility can be gained within relatively short time spans where growth isn’t limited by droughts.&lt;/p&gt;
    &lt;p&gt;The problem: Lupines now cover 0.4% of Iceland’s land surface. Under current climate change rates, lupine could colonize much of the highland interior within 30 years, potentially erasing naturally occurring landscapes. The species has been designated invasive, with tendency to create monocultures preventing other plant growth. The lupine case reveals a fundamental tension: biological solutions that work brilliantly at small scales can become problems at landscape scales if succession dynamics are misunderstood or if climate changes faster than ecosystems can adapt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Governance and Property Rights: Who Controls the Systems?&lt;/head&gt;
    &lt;p&gt;The River Arvari Parliament demonstrates one governance model. TBS enabled communities to form the River Arvari Parliament, comprising members from gram sabhas of 72 villages (one of India’s first community-led river governance structures). This works in Rajasthan because water scarcity creates immediate benefits to cooperation, village-level governance structures already existed, and Rajendra Singh spent years building trust before scaling.&lt;/p&gt;
    &lt;p&gt;The Netherlands offers a contrasting model with more than 500 residential areas with bioswales. Dutch success stems from national-level planning authority, clear liability frameworks, centuries of collective water management experience, and high population density making defection costly. Neither model transfers easily. Rajasthan’s approach requires social capital most places lack; Dutch centralized planning requires state capacity uncommon outside Northern Europe.&lt;/p&gt;
    &lt;head rend="h3"&gt;Industrial Bottlenecks and Political Fragility&lt;/head&gt;
    &lt;p&gt;The foundational constraint is energy capacity. No terraforming or geoengineering approach (biological or technological) can scale without abundant, decarbonized energy at sufficient overcapacity to power both deployment and ongoing operations.&lt;/p&gt;
    &lt;p&gt;This reality precedes any discussion of biological versus engineered systems. Scaling biological geoengineering requires industrial capacity that most discussions ignore. Drone-seeding 27 million mangroves demands manufacturing infrastructure. Deploying 40,000 Mother Reefs needs fabrication facilities. All require massive energy inputs. The limiting factor isn’t biological knowledge or technical standardization. It’s industrial throughput, energy availability, and the political will to sustain both.&lt;/p&gt;
    &lt;p&gt;The Three Industrial Constraints&lt;/p&gt;
    &lt;p&gt;California pays ~$0.24/kWh for industrial electricity; China’s Pearl River Delta pays ~$0.09/kWh. Manufacturing oyster reef components, seeding drones, or kelp farm infrastructure at 2.7x energy costs makes projects economically unviable. Texas achieves ~$0.06/kWh, but American electricity prices rise sharply near concentrations of human capital—precisely where technical expertise concentrates. High voltage direct current transmission could move gigawatts across continents; failure to deploy transmission infrastructure is purely policy failure.&lt;/p&gt;
    &lt;p&gt;China deployed more industrial robots in 2023 than the rest of the world combined. Between 2017 and 2023, China increased robots per 10,000 manufacturing workers from 97 to ~470—a 5x increase. Over the same period, America’s robot density grew &amp;lt;0.5x for a shrinking workforce. Chinese manufacturers can tool up production lines with 6-axis robot arms at $8,250 per unit (Borunte, 10kg payload, 0.05mm repeatability). No comparable Western alternative exists at this price point.&lt;/p&gt;
    &lt;p&gt;Financial capital flows to high-return financial engineering while physical capital accumulation stagnates. Biological geoengineering provides diffuse public benefits (flood protection, carbon sequestration, groundwater recharge) over decades, not concentrated private returns over quarters. Private capital won’t fund these systems at necessary scale. Public capital flows through procurement processes designed for conventional infrastructure, not living systems that mature over 5-20 years.&lt;/p&gt;
    &lt;p&gt;Productive Overcapacity: The Marshall Plan Framework&lt;/p&gt;
    &lt;p&gt;Martin Sandbu’s analysis of China’s surplus provides the solution framework. Post-WWII America ran external surpluses exceeding 2% of GDP, yet industrial production grew strongly in both surplus America and deficit Europe simultaneously. US surplus earnings funded productive investments in European infrastructure through Marshall Plan structures that directed capital toward growth-enhancing uses.&lt;/p&gt;
    &lt;p&gt;The world faces massive infrastructure deficits while China manufactures restoration infrastructure components (solar panels, battery systems, precision sensors) at 30-50% of Western costs. This surplus production could be productively absorbed by biological geoengineering deployment—if financing mechanisms existed to direct it there. What’s needed: Infrastructure financing institutions specifically capitalized for biological geoengineering procurement, technology transfer frameworks allowing joint ventures, long-term purchase agreements giving manufacturers certainty to invest in automated production lines, and performance-based financing that pays for outcomes over time.&lt;/p&gt;
    &lt;p&gt;Political Fragility: The Fate of Successful Programs&lt;/p&gt;
    &lt;p&gt;Even if industrial capacity could be built, sustaining it requires political will that historically evaporates once systems succeed. Successful infrastructure becomes invisible, and invisible infrastructure becomes vulnerable. When systems work, citizens don’t see the research apparatus, maintenance programs, or policy coordination that made success possible. Politicians see “expensive programs” consuming budget without visible output. Budget cuts deliver immediate fiscal savings. The costs (system degradation, lost competitiveness, technological stagnation) materialize slowly over years.&lt;/p&gt;
    &lt;p&gt;This pattern repeats across successful public goods. American interstate highways, built in the 1950s-60s, enabled trillions in economic activity yet crumble from deferred maintenance because functional infrastructure generates no political urgency. NASA’s Apollo program put humans on the moon; its current budget is one-third of 1960s levels as percentage of federal spending. The Internet emerged from DARPA and NSF funding; once successful, both faced budget cuts as politicians questioned why government should fund “established” technology.&lt;/p&gt;
    &lt;p&gt;Biological geoengineering faces this same trap. If oyster reefs successfully protect coastlines, will politicians maintain funding for reef restoration research and deployment 20 years later? Or will successful coastal protection be taken for granted, making restoration programs easy targets during the next fiscal crisis?&lt;/p&gt;
    &lt;p&gt;Relying solely on institutional models creates political vulnerability. Successful programs become targets for cuts precisely because they work well enough to be taken for granted. This argues for embedding biological geoengineering in physical capital and industrial capacity rather than purely institutional structures.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Wageningen Model: Success in Agricultural Applications (and Its Limits)&lt;/head&gt;
    &lt;p&gt;Wageningen University &amp;amp; Research (WUR) demonstrates how to scale biological interventions, but only for agricultural and agritech applications, representing perhaps 15-20% of biological geoengineering’s total scope. Widely regarded as the world’s top agricultural research institution, WUR is the nodal point of Food Valley, an expansive cluster of agricultural technology start-ups and experimental farms.&lt;/p&gt;
    &lt;p&gt;The Netherlands is the world’s second-largest agricultural exporter, with agricultural exports reaching €128.9 billion in 2024 (remarkable for a country holding only 0.04% of global agricultural land). This success stems from research and development resources that tripled over three decades. A radical 1998 restructuring merged diverse research instituteswith agricultural research institutes of the Dutch Ministry of Agriculture, enabling systematic translation of research into policy.&lt;/p&gt;
    &lt;p&gt;WUR’s approach integrates four elements: applied research at commercial scale (1,200 hectares of research farms identifying economic bottlenecks); public-private partnerships structuring funding; extension services training agricultural consultants; and policy integration informing Dutch and EU agricultural policy. This model works brilliantly for agricultural interventions (disease-resistant crop varieties, fertilizer optimization, efficient irrigation). It does not solve manufacturing oyster reefs at scale, producing seeding drones, or building automated kelp farm infrastructure. Seed breeding requires laboratory facilities and experimental plots. Manufacturing Mother Reefs requires kilns, automated production lines, biosecure hatcheries, and coastal logistics networks (fundamentally different challenges requiring different institutional structures).&lt;/p&gt;
    &lt;p&gt;Yet even Wageningen faces vulnerability. WUR announced in July 2024 it must cut spending by €80 million. A 2019 Rabobank analysis found that every €1 of research and development capital resulted in €4.20 of added value for society. The institution that enabled the Netherlands to become the world’s second-largest agricultural exporter faces budget cuts because successful infrastructure becomes invisible. Politicians see “expensive universities” consuming budget without visible output. The €4.20 return per €1 invested is diffuse (spread across thousands of farms, millions of consumers, decades of incremental improvement). This reveals a perverse dynamic: successful programs become targets for cuts precisely because they work well enough to be taken for granted. This argues for embedding biological geoengineering in physical capital and industrial capacity rather than purely institutional structures. A functioning oyster reef production line (with invested capital, trained workers, established supply chains, and purchase contracts) is harder to dismantle through budget cuts than a university research program. Manufacturing capacity has political economy advantages over knowledge institutions: it’s visible, employs workers who vote, generates revenue, and involves private capital that resists expropriation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Applying the Wageningen Model to Biological Geoengineering&lt;/head&gt;
    &lt;p&gt;Companies like Oyster Heaven and Coastal Technologies Corp have proven technical feasibility of oyster reef restoration. Economic barriers remain: oyster reefs need 5-10 years to provide comparable wave attenuation to traditional breakwaters; existing engineering liability insurance doesn’t cover biological systems; traditional infrastructure budgets don’t include line items for ecological monitoring. Wageningen’s approach suggests solutions: establish demonstration reefs at scale (10+ hectares) generating performance data for insurers and engineers; create public-private partnerships sharing risk and revenue from avoided coastal damages; train coastal managers in ecological engineering; integrate living shorelines into infrastructure codes. The Netherlands now requires nature-based solutions be evaluated alongside traditional engineering for any coastal project over €5 million, creating guaranteed market demand.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Speed Problem and Integration Challenge&lt;/head&gt;
    &lt;p&gt;Climate impacts accelerate while biological systems require time to mature, from fast interventions like bioswales and seaweed farming (1-5 years) through medium-term oyster reef and mangrove restoration (5-20 years) to slow forest establishment for climate regulation (20+ years). Three responses address this mismatch: combine biological and engineered systems (Dutch flood management combines dikes for immediate protection with wetland restoration for long-term flexibility); accept partial solutions (young mangrove forests provide 30% of mature forest storm protection but sequester carbon at 3x the rate); deploy biological systems where speed advantages exist.&lt;/p&gt;
    &lt;p&gt;The real frontier isn’t choosing between engineered and biological systems but integrating them effectively. Van Oord’s oyster reef integration with offshore wind infrastructure demonstrates this: cultivating oyster larvae in hatcheries, then integrating oyster-bearing rocks into wind farms, subsea cabling, and breakwaters. This provides enhanced wave protection for wind farm foundations, biodiversity offsets required for construction permits, additional revenue from potential harvest, and simplified permitting. Integration requires professionals who understand both engineered and biological systems (a skill set current education systems don’t systematically produce).&lt;/p&gt;
    &lt;head rend="h2"&gt;We’re Not There Yet&lt;/head&gt;
    &lt;p&gt;Technical feasibility is proven. Beavers engineer watersheds, the Amazon manufactures weather, mangroves provide $855 billion in flood protection (planetary-scale infrastructure operating through Lovelock’s self-regulating mechanisms). But Section 5’s constraints reveal we’re nowhere close to climate-relevant deployment. Ecological mismatch and governance have solutions. Wageningen works brilliantly for agricultural applications but solves perhaps 15-20% of biological geoengineering (offering no template for manufacturing oyster reefs or seeding drones at scale).&lt;/p&gt;
    &lt;p&gt;Industrial capacity is the ultimate bottleneck: cheap electricity (China’s $0.09/kWh vs California’s $0.24/kWh), automated production (China deployed more industrial robots in 2023 than the rest of the world combined), and capital allocation for diffuse public benefits over decades. Even Wageningen (returning €4.20 per €1 invested) faces €80 million in cuts because successful infrastructure becomes invisible. Three choices: build domestic capacity (expensive, slow, autonomous); leverage Chinese manufacturing (cheaper, faster, dependent); or accept biological geoengineering won’t scale, forcing us toward riskier interventions. Current trajectories suggest the third by default.&lt;/p&gt;
    &lt;p&gt;The Fremen succeeded technically (planted trees, established water cycles, created paradise) then discovered they’d destroyed the desert ecology sustaining them ( I mean, it’s all part of the Golden Path and the God Emperor did break himself down into the sand trout that become’s Dune’s iconic worms but that’s besides the point!). We’re making the opposite mistake. We understand biological systems remarkably well but haven’t built industrial capacity to deploy them at necessary speed and scale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.governance.fyi/p/all-natural-geoengineering-with-frank"/><published>2025-10-10T14:11:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45539609</id><title>Notes on switching to Helix from Vim</title><updated>2025-10-10T21:32:13.333288+00:00</updated><content>&lt;doc fingerprint="740c7f712b243eb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Notes on switching to Helix from vim&lt;/head&gt;
    &lt;p&gt;Hello! Earlier this summer I was talking to a friend about how much I love using fish, and how I love that I don’t have to configure it. They said that they feel the same way about the helix text editor, and so I decided to give it a try.&lt;/p&gt;
    &lt;p&gt;I’ve been using it for 3 months now and here are a few notes.&lt;/p&gt;
    &lt;head rend="h3"&gt;why helix: language servers&lt;/head&gt;
    &lt;p&gt;I think what motivated me to try Helix is that I’ve been trying to get a working language server setup (so I can do things like “go to definition”) and getting a setup that feels good in Vim or Neovim just felt like too much work.&lt;/p&gt;
    &lt;p&gt;After using Vim/Neovim for 20 years, I’ve tried both “build my own custom configuration from scratch” and “use someone else’s pre-buld configuration system” and even though I love Vim I was excited about having things just work without having to work on my configuration at all.&lt;/p&gt;
    &lt;p&gt;Helix comes with built in language server support, and it feels nice to be able to do things like “rename this symbol” in any language.&lt;/p&gt;
    &lt;head rend="h3"&gt;the search is great&lt;/head&gt;
    &lt;p&gt;One of my favourite things about Helix is the search! If I’m searching all the files in my repository for a string, it lets me scroll through the potential matching files and see the full context of the match, like this:&lt;/p&gt;
    &lt;p&gt;For comparison, here’s what the vim ripgrep plugin I’ve been using looks like:&lt;/p&gt;
    &lt;p&gt;There’s no context for what else is around that line.&lt;/p&gt;
    &lt;head rend="h3"&gt;the quick reference is nice&lt;/head&gt;
    &lt;p&gt;One thing I like about Helix is that when I press &lt;code&gt;g&lt;/code&gt;, I get a little help popup
telling me places I can go. I really appreciate this because I don’t often use
the “go to definition” or “go to reference” feature and I often forget the
keyboard shortcut.&lt;/p&gt;
    &lt;head rend="h3"&gt;some vim -&amp;gt; helix translations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Helix doesn’t have marks like &lt;code&gt;ma&lt;/code&gt;,&lt;code&gt;'a&lt;/code&gt;, instead I’ve been using&lt;code&gt;Ctrl+O&lt;/code&gt;and&lt;code&gt;Ctrl+I&lt;/code&gt;to go back (or forward) to the last cursor location&lt;/item&gt;
      &lt;item&gt;I think Helix does have macros, but I’ve been using multiple cursors in every case that I would have previously used a macro. I like multiple cursors a lot more than writing macros all the time. If I want to batch change something in the document, my workflow is to press &lt;code&gt;%&lt;/code&gt;(to highlight everything), then&lt;code&gt;s&lt;/code&gt;to select (with a regex) the things I want to change, then I can just edit all of them as needed.&lt;/item&gt;
      &lt;item&gt;Helix doesn’t have tabs, instead it has a nice buffer switcher (&lt;code&gt;&amp;lt;space&amp;gt;b&lt;/code&gt;) I can use to switch to the buffer I want&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;some helix annoyances&lt;/head&gt;
    &lt;p&gt;Here’s everything that’s annoyed me about Helix so far.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I like the way Helix’s &lt;code&gt;:reflow&lt;/code&gt;works much less than how vim reflows text with&lt;code&gt;gq&lt;/code&gt;. It doesn’t work as well with lists. (github issue)&lt;/item&gt;
      &lt;item&gt;If I’m making a Markdown list, pressing “enter” at the end of a list item won’t continue the list. There’s a partial workaround for bulleted lists but I don’t know one for numbered lists.&lt;/item&gt;
      &lt;item&gt;No persistent undo yet: in vim I could use an undofile so that I could undo changes even after quitting. Helix doesn’t have that feature yet. (github PR)&lt;/item&gt;
      &lt;item&gt;Helix doesn’t autoreload files after they change on disk, I have to run &lt;code&gt;:reload-all&lt;/code&gt;(&lt;code&gt;:ra&amp;lt;tab&amp;gt;&lt;/code&gt;) to manually reload them. Not a big deal.&lt;/item&gt;
      &lt;item&gt;Sometimes it crashes, maybe every week or so. Someone mentioned that this might have something to do with the fact that I edit a lot of Markdown, not sure. This doesn’t bother me that much though, I can just reopen it. I’m not 100% sure whether the crash is a segfault or a panic, but it seems that Helix can segfault sometimes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The “markdown list” and reflowing issues come up a lot for me because I spend a lot of time editing Markdown lists, but I keep using Helix anyway so I guess they can’t be making me that mad.&lt;/p&gt;
    &lt;head rend="h3"&gt;switching was easier than I thought&lt;/head&gt;
    &lt;p&gt;I was worried that relearning 20 years of Vim muscle memory would be really hard.&lt;/p&gt;
    &lt;p&gt;It turned out to be easier than I expected, I started using Helix on a vacation for a little low-stakes coding project I was doing on the side and after a week or two it didn’t feel so disorienting anymore. I think it might be hard to switch back and forth between Vim and Helix, but I haven’t needed to use Vim recently so I don’t know if that’ll ever become an issue for me.&lt;/p&gt;
    &lt;p&gt;The first time I tried Helix I tried to force it to use keybindings that were more similar to Vim and that did not work for me. Just learning the “Helix way” was a lot easier.&lt;/p&gt;
    &lt;p&gt;There are still some things that throw me off: for example &lt;code&gt;w&lt;/code&gt; in vim and &lt;code&gt;w&lt;/code&gt; in
Helix don’t have the same idea of what a “word” is (the Helix one includes the
space after the word, the Vim one doesn’t).&lt;/p&gt;
    &lt;head rend="h3"&gt;using a terminal-based text editor&lt;/head&gt;
    &lt;p&gt;For many years I’d mostly been using a GUI version of vim/neovim, so switching to actually using an editor in the terminal was a bit of an adjustment.&lt;/p&gt;
    &lt;p&gt;I ended up deciding on:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Every project gets its own terminal window, and all of the tabs in that window (mostly) have the same working directory&lt;/item&gt;
      &lt;item&gt;I make my Helix tab the first tab in the terminal window&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It works pretty well, I might actually like it better than my previous workflow.&lt;/p&gt;
    &lt;head rend="h3"&gt;my configuration&lt;/head&gt;
    &lt;p&gt;I appreciate that my configuration is really simple, compared to my neovim configuration which is hundreds of lines. It’s mostly just 4 keyboard shortcuts.&lt;/p&gt;
    &lt;code&gt;theme = "solarized_light"
[editor]
# Sync clipboard with system clipboard
default-yank-register = "+"

[keys.normal]
# I didn't like that Ctrl+C was the default "toggle comments" shortcut
"#" = "toggle_comments"

# I didn't feel like learning a different way
# to go to the beginning/end of a line so
# I remapped ^ and $
"^" = "goto_first_nonwhitespace"
"$" = "goto_line_end"

[keys.select]
"^" = "goto_first_nonwhitespace"
"$" = "goto_line_end"

[keys.normal.space]
# I write a lot of text so I need to constantly reflow,
# and missed vim's `gq` shortcut
l = ":reflow"
&lt;/code&gt;
    &lt;p&gt;There’s a separate &lt;code&gt;languages.toml&lt;/code&gt; configuration where I set some language
preferences, like turning off autoformatting.
For example, here’s my Python configuration:&lt;/p&gt;
    &lt;code&gt;[[language]]
name = "python"
formatter = { command = "black", args = ["--stdin-filename", "%{buffer_name}", "-"] }
language-servers = ["pyright"]
auto-format = false
&lt;/code&gt;
    &lt;head rend="h3"&gt;we’ll see how it goes&lt;/head&gt;
    &lt;p&gt;Three months is not that long, and it’s possible that I’ll decide to go back to Vim at some point. For example, I wrote a post about switching to nix a while back but after maybe 8 months I switched back to Homebrew (though I’m still using NixOS to manage one little server, and I’m still satisfied with that).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jvns.ca/blog/2025/10/10/notes-on-switching-to-helix-from-vim/"/><published>2025-10-10T14:37:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45539943</id><title>Ryanair flight landed at Manchester airport with six minutes of fuel left</title><updated>2025-10-10T21:32:13.165722+00:00</updated><content>&lt;doc fingerprint="32049a1c222d59d2"&gt;
  &lt;main&gt;
    &lt;p&gt;An investigation is under way after a Ryanair flight battling with high wind speeds during storm Amy last week landed at Manchester airport with just six minutes of fuel left in its tanks.&lt;/p&gt;
    &lt;p&gt;The pilots had been taking passengers from Pisa in Italy to Prestwick in Scotland on Friday evening, but wind speeds of up to 100mph meant they were unable to land.&lt;/p&gt;
    &lt;p&gt;After three failed attempts to touch down, the pilots of Ryanair flight FR3418 issued a mayday emergency call and raced to Manchester, where the weather was calmer.&lt;/p&gt;
    &lt;p&gt;The Boeing 737-800 had just 220kg of fuel left in its tanks when it finally landed, according to a picture of what appears to be a handwritten technical log. Pilots who examined the picture said this would be enough for just five or six minutes of flying.&lt;/p&gt;
    &lt;p&gt;Analysis of the log suggests the plane left Pisa with reserve fuel, as commercial flights are required to do.&lt;/p&gt;
    &lt;p&gt;A spokesperson for the airline said: “Ryanair reported this to the relevant authorities on Friday [3 October]. As this is now subject of an ongoing investigation, which we are co-operating fully with, we are unable to comment.”&lt;/p&gt;
    &lt;p&gt;The Air Accidents Investigation Branch confirmed on Thursday it had opened an investigation after being notified by Ryanair.&lt;/p&gt;
    &lt;p&gt;A spokesperson said: “The AAIB has commenced an investigation into a serious incident involving an aircraft which was diverted from Prestwick to Manchester Airport on Friday 3 October. AAIB inspectors have begun making inquiries and gathering evidence.”&lt;/p&gt;
    &lt;p&gt;The Boeing 737-800 can carry up to 189 passengers. One person on board recounted what is thought to have been a two-hour attempt to make a safe landing, saying the plane made two attempts to land at Prestwick, before heading for Edinburgh and finally Manchester.&lt;/p&gt;
    &lt;p&gt;“Everyone was calm until the descent; we were being buffeted around a lot and jumping. There were a few worried people on the second descent as we could feel the plane was struggling,” Alexander Marchi told the Ayr Advertiser.&lt;/p&gt;
    &lt;p&gt;“Then the pilot surprised us by saying he was going to attempt Edinburgh. This was just as bad, though, as the second time at Prestwick.&lt;/p&gt;
    &lt;p&gt;“There was turbulence over the Firth of Forth and then as we approached the airport, as we were very close to landing, again we had to pull up sharply.”&lt;/p&gt;
    &lt;p&gt;The passengers were taken from Manchester to Prestwick, arriving 10 hours later than the scheduled arrival time of 6pm on Friday.&lt;/p&gt;
    &lt;p&gt;One pilot who reviewed the log said: “Just imagine that whenever you land with less than 2T (2,000kg) of fuel left you start paying close attention to the situation. Less than 1.5T you are sweating. But this is as close to a fatal accident as possible.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/business/2025/oct/10/ryanair-flight-landed-at-manchester-airport-with-six-minutes-of-fuel-left-flight-log-suggests"/><published>2025-10-10T15:11:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45540011</id><title>You can't build tcc from Nixpkgs if you are in the UK</title><updated>2025-10-10T21:32:12.016363+00:00</updated><content>&lt;doc fingerprint="ef0e3f5c460465a6"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 16.9k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Nixpkgs version&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stable (25.05)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Steps to reproduce&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;fetchFromRepoOrCz&lt;/code&gt; function in &lt;code&gt;pkgs/build-support/fetchrepoorcz/default.nix&lt;/code&gt; downloads sources from https://repo.or.cz/&lt;/p&gt;
    &lt;p&gt;Unfortunately, the site blocks connections from the UK due to the Online Safety act and redirects all requests to https://repo.or.cz/uk-blocked.html&lt;/p&gt;
    &lt;p&gt;I noticed this while trying to build &lt;code&gt;docutils&lt;/code&gt; from a UK-based build machine. The build error is&lt;/p&gt;
    &lt;code&gt;nix log /nix/store/q9q3xki1m7miwk8p9dqb6wjlq8mg0m11-source.drv

trying https://repo.or.cz/docutils.git/snapshot/docutils-0.21.2.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   242  100   242    0     0    331      0 --:--:-- --:--:-- --:--:--   331
100  2517  100  2517    0     0   3275      0 --:--:-- --:--:-- --:--:--     0
unpacking source archive /build/docutils-0.21.2.tar.gz

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now
do not know how to unpack source archive /build/docutils-0.21.2.tar.gz
&lt;/code&gt;
    &lt;p&gt;In fact, if one runs &lt;code&gt;curl -LO https://repo.or.cz/docutils.git/snapshot/docutils-0.21.2.tar.gz&lt;/code&gt; the resulting file will contain the data from https://repo.or.cz/uk-blocked.html which is not a valid gzip archive.&lt;/p&gt;
    &lt;p&gt;I searched for issues mentioning &lt;code&gt;fetchFromRepoOrCz&lt;/code&gt; but found none.&lt;/p&gt;
    &lt;p&gt;A quick search with &lt;code&gt;rg&lt;/code&gt; in the repository returns only 7 packages using the function:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pkgs/by-name/wi/windowmaker/package.nix (declared but not used)&lt;/item&gt;
      &lt;item&gt;pkgs/by-name/ti/tinycc/package.nix&lt;/item&gt;
      &lt;item&gt;pkgs/applications/editors/vim/plugins/nvim-treesitter/generated.nix (declared but not used)&lt;/item&gt;
      &lt;item&gt;pkgs/by-name/cd/cdimgtools/package.nix&lt;/item&gt;
      &lt;item&gt;pkgs/by-name/sy/syslinux/package.nix&lt;/item&gt;
      &lt;item&gt;pkgs/by-name/gl/glpng/package.nix&lt;/item&gt;
      &lt;item&gt;pkgs/development/python-modules/docutils/default.nix&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Can Hydra reproduce this build failure?&lt;/head&gt;
    &lt;p&gt;No, Hydra cannot reproduce this build failure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Link to Hydra build job&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Relevant log output&lt;/head&gt;
    &lt;p&gt;docutils on an i686&lt;/p&gt;
    &lt;code&gt;nix log /nix/store/q9q3xki1m7miwk8p9dqb6wjlq8mg0m11-source.drv

trying https://repo.or.cz/docutils.git/snapshot/docutils-0.21.2.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   242  100   242    0     0    331      0 --:--:-- --:--:-- --:--:--   331
100  2517  100  2517    0     0   3275      0 --:--:-- --:--:-- --:--:--     0
unpacking source archive /build/docutils-0.21.2.tar.gz

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now
do not know how to unpack source archive /build/docutils-0.21.2.tar.gz&lt;/code&gt;
    &lt;p&gt;tinycc on an i686&lt;/p&gt;
    &lt;code&gt;nix-build -I nixpkgs=/etc/nixpkgs '&amp;lt;nixpkgs&amp;gt;' --attr tinycc
these 3 derivations will be built:
  /nix/store/gbzvgibfic9nm3d3ssvvqf00f6n9pryx-source.drv
  /nix/store/hlp8s9r43dbrzdw3xrqg19hl8fbn7x0b-libtcc.pc.drv
  /nix/store/2kf9hm960vfzjwnrdiq2z5pqfa4g680j-tcc-0.9.27-unstable-2025-01-06.drv
building '/nix/store/hlp8s9r43dbrzdw3xrqg19hl8fbn7x0b-libtcc.pc.drv'...
building '/nix/store/gbzvgibfic9nm3d3ssvvqf00f6n9pryx-source.drv'...

trying https://repo.or.cz/tinycc.git/snapshot/f6385c05308f715bdd2c06336801193a21d69b50.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   242  100   242    0     0    787      0 --:--:-- --:--:-- --:--:--   788
100  2517  100  2517    0     0   7423      0 --:--:-- --:--:-- --:--:--  7423
unpacking source archive /build/f6385c05308f715bdd2c06336801193a21d69b50.tar.gz

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now
do not know how to unpack source archive /build/f6385c05308f715bdd2c06336801193a21d69b50.tar.gz
error: builder for '/nix/store/gbzvgibfic9nm3d3ssvvqf00f6n9pryx-source.drv' failed with exit code 1
error: 1 dependencies of derivation '/nix/store/2kf9hm960vfzjwnrdiq2z5pqfa4g680j-tcc-0.9.27-unstable-2025-01-06.drv' failed to build&lt;/code&gt;
    &lt;p&gt;cdimgtools on an x86_64&lt;/p&gt;
    &lt;code&gt;nix build --offline 'nixpkgs#cdimgtools'
error: builder for '/nix/store/4lr0c2yqa9l3a4fka97fncvhkvh14sc2-source.drv' failed with exit code 1;
       last 12 log lines:
       &amp;gt;
       &amp;gt; trying https://repo.or.cz/cdimgtools.git/snapshot/version/0.3.tar.gz
       &amp;gt;   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
       &amp;gt;                                  Dload  Upload   Total   Spent    Left  Speed
       &amp;gt; 100   242  100   242    0     0    623      0 --:--:-- --:--:-- --:--:--   623
       &amp;gt; 100  2517  100  2517    0     0   5007      0 --:--:-- --:--:-- --:--:--  5007
       &amp;gt; unpacking source archive /build/0.3.tar.gz
       &amp;gt;
       &amp;gt; gzip: stdin: not in gzip format
       &amp;gt; tar: Child returned status 1
       &amp;gt; tar: Error is not recoverable: exiting now
       &amp;gt; do not know how to unpack source archive /build/0.3.tar.gz&lt;/code&gt;
    &lt;head rend="h3"&gt;Additional context&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;System metadata&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;system: &lt;code&gt;"i686-linux"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;host os: &lt;code&gt;Linux 6.12.42, NixOS, 25.05 (Warbler), 25.05.20250819.a58390a&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;multi-user?: &lt;code&gt;no&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;sandbox: &lt;code&gt;yes&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;version: &lt;code&gt;nix-env (Nix) 2.28.4&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;nixpkgs: &lt;code&gt;/nix/store/lgzfgc1acidk895knamw9kywlhmdwv9h-source&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Notify maintainers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;docutils: no maintainer&lt;/item&gt;
      &lt;item&gt;tinycc: @joachifm&lt;/item&gt;
      &lt;item&gt;syslinux: no maintainer&lt;/item&gt;
      &lt;item&gt;cdimgtools: @hhm0&lt;/item&gt;
      &lt;item&gt;glpng: no maintainer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note for maintainers: Please tag this issue in your pull request description. (i.e. &lt;code&gt;Resolves #ISSUE&lt;/code&gt;.)&lt;/p&gt;
    &lt;head rend="h3"&gt;I assert that this issue is relevant for Nixpkgs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I assert that this is a bug and not a support request.&lt;/item&gt;
      &lt;item&gt;I assert that this is not a duplicate of an existing issue.&lt;/item&gt;
      &lt;item&gt;I assert that I have read the NixOS Code of Conduct and agree to abide by it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Is this issue important to you?&lt;/head&gt;
    &lt;p&gt;Add a 👍 reaction to issues you find important.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/NixOS/nixpkgs/issues/444342"/><published>2025-10-10T15:18:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45540171</id><title>Show HN: Gitcasso – Syntax Highlighting and Draft Recovery for GitHub Comments</title><updated>2025-10-10T21:32:11.333094+00:00</updated><content>&lt;doc fingerprint="e3e60f26b6a4985e"&gt;
  &lt;main&gt;
    &lt;p&gt;Syntax highlighting and autosave for comments on GitHub (and other markdown-friendly websites).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Syntax highlighting is the lie that enables us to see the truth."&lt;/item&gt;
      &lt;item&gt;"The meaning of life is to find your lost comment drafts. The purpose of life is to post them."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If there's something you'd like to add or fix, see CONTRIBUTING.md.&lt;/p&gt;
    &lt;p&gt;Special thanks to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;overtype for doing &lt;code&gt;textarea&lt;/code&gt;syntax highlighting of&lt;code&gt;md&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;highlight.js for the broad library of syntax highlighters&lt;/item&gt;
      &lt;item&gt;Yukai Huang for the PRs which made the two work together&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/diffplug/gitcasso"/><published>2025-10-10T15:37:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45540989</id><title>Regarding the Compact</title><updated>2025-10-10T21:32:10.887194+00:00</updated><content>&lt;doc fingerprint="1ae8b9d1504517ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Regarding the Compact&lt;/head&gt;
    &lt;p&gt;Dear members of the MIT community,&lt;/p&gt;
    &lt;p&gt;The U.S. Department of Education recently sent MIT and eight other institutions a proposed “Compact for Academic Excellence in Higher Education," along with a letter asking that MIT review the document.&lt;/p&gt;
    &lt;p&gt;From the messages I've received, I know this is on the minds of many of you and that you care deeply about the Institute’s mission, its values and each other. I do too.&lt;/p&gt;
    &lt;p&gt;After considerable thought and consultation with leaders from across MIT, today I sent the following reply to U.S. Education Secretary Linda McMahon.&lt;/p&gt;
    &lt;p&gt;Sincerely,&lt;lb/&gt;Sally Kornbluth&lt;/p&gt;
    &lt;p&gt;Dear Madam Secretary,&lt;/p&gt;
    &lt;p&gt;I write in response to your letter of October 1, inviting MIT to review a “Compact for Academic Excellence in Higher Education.” I acknowledge the vital importance of these matters.&lt;/p&gt;
    &lt;p&gt;I appreciated the chance to meet with you earlier this year to discuss the priorities we share for American higher education.&lt;/p&gt;
    &lt;p&gt;As we discussed, the Institute’s mission of service to the nation directs us to advance knowledge, educate students and bring knowledge to bear on the world’s great challenges. We do that in line with a clear set of values, with excellence above all. Some practical examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;MIT prides itself on rewarding merit. Students, faculty and staff succeed here based on the strength of their talent, ideas and hard work. For instance, the Institute was the first to reinstate the SAT/ACT requirement after the pandemic. And MIT has never had legacy preferences in admissions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MIT opens its doors to the most talented students regardless of their family’s finances. Admissions are need-blind. Incoming undergraduates whose families earn less than $200,000 a year pay no tuition. Nearly 88% of our last graduating class left MIT with no debt for their education. We make a wealth of free courses and low-cost certificates available to any American with an internet connection. Of the undergraduate degrees we award, 94% are in STEM fields. And in service to the nation, we cap enrollment of international undergraduates at roughly 10%.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We value free expression, as clearly described in the MIT Statement on Freedom of Expression and Academic Freedom. We must hear facts and opinions we don’t like – and engage respectfully with those with whom we disagree.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These values and other MIT practices meet or exceed many standards outlined in the document you sent. We freely choose these values because they’re right, and we live by them because they support our mission – work of immense value to the prosperity, competitiveness, health and security of the United States. And of course, MIT abides by the law.&lt;/p&gt;
    &lt;p&gt;The document also includes principles with which we disagree, including those that would restrict freedom of expression and our independence as an institution. And fundamentally, the premise of the document is inconsistent with our core belief that scientific funding should be based on scientific merit alone.&lt;/p&gt;
    &lt;p&gt;In our view, America’s leadership in science and innovation depends on independent thinking and open competition for excellence. In that free marketplace of ideas, the people of MIT gladly compete with the very best, without preferences. Therefore, with respect, we cannot support the proposed approach to addressing the issues facing higher education.&lt;/p&gt;
    &lt;p&gt;As you know, MIT’s record of service to the nation is long and enduring. Eight decades ago, MIT leaders helped invent a scientific partnership between America’s research universities and the U.S. government that has delivered extraordinary benefits for the American people. We continue to believe in the power of this partnership to serve the nation.&lt;/p&gt;
    &lt;p&gt;Sincerely,&lt;lb/&gt;Sally Kornbluth&lt;/p&gt;
    &lt;p&gt;cc&lt;lb/&gt;Ms. May Mailman&lt;lb/&gt;Mr. Vincent Haley&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://president.mit.edu/writing-speeches/regarding-compact"/><published>2025-10-10T16:49:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45541125</id><title>It's OpenAI's world, we're just living in it</title><updated>2025-10-10T21:32:10.611639+00:00</updated><content>&lt;doc fingerprint="a5875e18ed26c6b6"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome back to This Week in Stratechery!&lt;/p&gt;
    &lt;p&gt;As a reminder, each week, every Friday, we’re sending out this overview of content in the Stratechery bundle; highlighted links are free for everyone. Additionally, you have complete control over what we send to you. If you don’t want to receive This Week in Stratechery emails (there is no podcast), please uncheck the box in your delivery settings.&lt;/p&gt;
    &lt;p&gt;On that note, here were a few of our favorites this week.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;OpenAI’s Windows Strategy. The current touchpoint for platform power is the smartphone, where Apple and Google share a duopoly. A better analogy for OpenAI’s ambitions, however, is Microsoft and the way that Windows controlled the PC industry: platform power didn’t just come from controlling applications on top of Windows, but the OEM ecosystem underneath. If OpenAI builds AI for everyone, then they are positioned to extract margin from companies up-and-down the stack — even Nvidia. And, as a necessary bonus, they position themselves to be the primary recipient of all of the speculative investment in AI, thus making the newly promised $1 trillion of infrastructure deals a reality. — Ben Thompson&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sam Altman and Boundless Ambition. This week’s Stratechery Interview with Sam Altman was shorter than most (40 minutes) but dense with thought-provoking answers on OpenAI’s business, including about 10 different thoughts from Ben and Altman that could have turned into hour-long conversations of their own. Specifically, though, in the midst of daily hand wringing over whether today’s AI investing is rational, it was clarifying to hear from the CEO at the center of all the frothiness, as the story of the moment is actually quite simple: Altman sees once-in-a-lifetime opportunities in both the consumer and enterprise spaces (“it’s not like you use Google at home and a different company at work”), and given the progress he sees on the research side and where he expects models to be in the near future, OpenAI is placing “company scale” bets today to ensure they have infrastructure ready for tomorrow’s demand. Whether those bets pay off, and how, are questions that will occupy the entire tech ecosystem for the next several years. — Andrew Sharp&lt;/item&gt;
      &lt;item&gt;The Future of Creation. We’re one week out from Sora madness, and I enjoyed Ben’s Article on Monday comparing the underwhelming reception of MetaAI’s Vibes update with the rapturous adoption of Sora. What I found most interesting about Sora is that for the first few days after joining I could not spend more than two minutes in the app before getting sick of the content (one can only handle so many Sam Altman cameos and/or Pikachu movie parodies). By the weekend, though, I’d learned how to make videos starring my dog, Ollie, and then Bill Bishop’s dog, and found myself spending far more time in the app and sharing videos with friends. According to Sam Altman, I was not alone (“30-something percent of active users were active creators”). The profound implications of that sort of shift—from AI-enabled consumption to AI-enabled creation—anchor the second half of Ben’s analysis on Monday, including questions about the future of creative expression, and perhaps the future of Meta’s business. — AS&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Stratechery Articles and Updates&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sora, AI Bicycles, and Meta Disruption — Sora is going viral, suggesting there is a big opportunity in unlocking creativity. If that’s true, that’s good for humanity — and bad for Meta.&lt;/item&gt;
      &lt;item&gt;OpenAI’s Windows Play — OpenAI is making a play to be the Windows of AI: the all-encompassing platform that controls both hardware supplier and software developers.&lt;/item&gt;
      &lt;item&gt;An Interview with OpenAI CEO Sam Altman About DevDay and the AI Buildout — An interview with OpenAI CEO Sam Altman about the infrastructure buildout, expanding ChatGPT, and the vision that unites it all.&lt;/item&gt;
      &lt;item&gt;The OpenAI Hype Cycle, Microsoft’s Game Pass Failure, Verizon’s Satellites — OpenAI’s DevDay evolution mirrors the hype cycle; Microsoft’s Game Pass price raise is an admission of failure; and Verizon decides it doesn’t want to be under the thumb of SpaceX.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Dithering with Ben Thompson and Daring Fireball’s John Gruber&lt;/head&gt;
    &lt;head rend="h3"&gt;Asianometry with Jon Yu&lt;/head&gt;
    &lt;head rend="h3"&gt;Sharp China with Andrew Sharp and Sinocism’s Bill Bishop&lt;/head&gt;
    &lt;head rend="h3"&gt;Greatest of All Talk with Andrew Sharp and WaPo’s Ben Golliver&lt;/head&gt;
    &lt;head rend="h3"&gt;Sharp Tech with Andrew Sharp and Ben Thompson&lt;/head&gt;
    &lt;p&gt;This week’s Stratechery video is on The YouTube Tip of the Google Spear.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stratechery.com/2025/its-openais-world-were-just-living-in-it/"/><published>2025-10-10T17:01:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45541600</id><title>The illegible nature of software development talent</title><updated>2025-10-10T21:32:10.200871+00:00</updated><content>&lt;doc fingerprint="bf495116b8ec34f5"&gt;
  &lt;main&gt;
    &lt;p&gt;Here’s another blog post on gathering some common threads from reading recent posts. Today’s topic is about the unassuming nature of talented software engineers.&lt;/p&gt;
    &lt;p&gt;The first thread was a tweet by Mitchell Hashimoto about how his best former colleagues are ones where you would have no signal about their skills based on their online activities or their working hours.&lt;/p&gt;
    &lt;p&gt;The second thread was a blog post written a week later by Nikunj Kothari titled The Quiet Ones: Working within the seams. In this post, Kothari wasn’t writing about a specific engineer per se, but rather a type of engineer, one whose contributions aren’t captured by the organization’s performance rubric (emphasis mine):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;They don’t hit your L5 requirements because they’re doing L3 and L7 work simultaneously. Fixing the deploy pipeline while mentoring juniors. Answering customer emails while rebuilding core systems. They can’t be ranked because they do what nobody thought to measure.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The third thread was a LinkedIn post written yesterday by Gergly Orosz (emphasis mine).&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;One of the best staff-level engineers I worked with is on the market.&lt;/p&gt;&lt;lb/&gt;…&lt;lb/&gt;What you need to know about this person: every team he’s ever worked on, he did standout work, in every situation. He got stuff done with high quality, helped others, is not argumentative but is firm in holding up common sense and practicality, and is very curious and humble to top all of this off.&lt;lb/&gt;…&lt;lb/&gt;And still, from the outside, this engineer is near completely invisible.&lt;p&gt;He has no social media footprint. His LinkedIn lists his companies he worked at, and nothing else: no technologies, no projects, nothing. His GitHub is empty for the last 5 years, and has perhaps a dozen commits throughout the last 10.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;That reason that Mitchell Hashimoto, NIkunj Kothari, and Gergly Orosz were able to identify these talented colleagues as because they worked directly with them. People making hiring decisions don’t have that luxury. For promotions, there are organizational constraints that push organizations to define a formal process with explicit criteria.&lt;/p&gt;
    &lt;p&gt;For both hiring and promotion, decision-makers have a legibility problem. This problem will inevitability lead to a focus on details that are easier to observe directly precisely because they are easier to observe directly. This is how fields like graphology and phrenology come about. But just because we can directly observe someone’s handwriting or the shapes of the bumps on their head doesn’t mean that those are effective techniques for learning something about that person’s personality.&lt;/p&gt;
    &lt;p&gt;I think it’s unlikely the industry will get much better at identifying and evaluating candidates anytime soon. And so I’m sure we’ll continue to see posts about the importance of your LinkedIn profile, or your GitHub, or your passion project. But you neglect at your peril the engineers who are working nine-to-five days at boring companies.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://surfingcomplexity.blog/2025/10/08/the-illegible-nature-of-software-development-talent/"/><published>2025-10-10T17:37:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45542145</id><title>Google, Meta and Microsoft to stop showing political ads in the EU</title><updated>2025-10-10T21:32:10.015568+00:00</updated><content>&lt;doc fingerprint="3c4e636af7b175e1"&gt;
  &lt;main&gt;
    &lt;p&gt;BRUSSELS — Fresh European Union rules intended to improve transparency around online advertisements have sparked a wave of criticism, as major platforms shut down political ads instead of complying.&lt;/p&gt;
    &lt;p&gt;Campaigners say the law will cause a harmful loss of information after it triggered companies including Google, Meta and Microsoft to implement a blackout on political advertising. Politicians on both sides of the aisle said it could be detrimental to democratic debate.&lt;/p&gt;
    &lt;p&gt;The Commission said it is aware of the serious concerns and is continuing talks with Big Tech companies to mitigate the unintended impacts. At the heart of the EU's attempt is a bid to curb political manipulation and foreign interference during elections.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.politico.eu/article/eu-political-ad-rules-google-meta-microsoft-big-tech-kick-in/"/><published>2025-10-10T18:25:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45542423</id><title>Toyota aims to launch the ' first' all-solid-state EV batteries</title><updated>2025-10-10T21:32:09.899642+00:00</updated><content>&lt;doc fingerprint="33a8c3ffe39d37e0"&gt;
  &lt;main&gt;
    &lt;p&gt;Toyota is doubling down on the “holy grail” of EV tech — all-solid-state batteries. Its first EV could arrive as soon as 2027, promising longer driving range, faster charging times, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Toyota to launch its first all-solid-state battery EV in 2027&lt;/head&gt;
    &lt;p&gt;After announcing a new partnership with Sumitomo Metal Mining Co. to mass produce cathode materials for the new battery tech on Wednesday, Toyota said it aims “to achieve the world’s first practical use of all-solid-state batteries in BEVs.”&lt;/p&gt;
    &lt;p&gt;Toyota said that its new batteries could significantly enhance driving range, charging times, and output, potentially transforming the future of automobiles.&lt;/p&gt;
    &lt;p&gt;Compared to current liquid-based batteries, which use electrolyte solutions, Toyota’s all-solid-state batteries utilize a cathode, an anode, and a solid electrolyte. According to Toyota, the next-gen battery tech “offers the potential for smaller size, higher output, and longer life.”&lt;/p&gt;
    &lt;p&gt;Toyota aims to launch its first all-solid-state battery-powered EV in 2027 or 2028. The new development agreement moves it one step closer to bringing the new battery tech to market on a mass scale.&lt;/p&gt;
    &lt;p&gt;The two companies have been developing cathode materials for all-solid-state EV batteries since 2021, focusing on some of the biggest challenges in producing them at a mass scale.&lt;/p&gt;
    &lt;p&gt;Using Sumitomo Metal Mining’s proprietary powder synthesis technology, Toyota claimed to have developed a “highly durable cathode material” for all-solid-state batteries.&lt;/p&gt;
    &lt;p&gt;Sumitomo has been supplying cathode materials for electric vehicles for years, but it’s now working to introduce the newly developed tech, moving it toward mass production.&lt;/p&gt;
    &lt;p&gt;The new agreement comes after Toyota was granted a METI certification to manufacture the new batteries in Japan last September.&lt;/p&gt;
    &lt;p&gt;Toyota is collaborating with several partners in Japan, including oil giant Idemitsu Kosan, to introduce the new EV batteries to the market.&lt;/p&gt;
    &lt;p&gt;Idemitsu announced plans earlier this year to build a large-scale production plant for lithium sulfide, a raw material used in all-solid-state EV batteries. Once up and running, the plant will be capable of producing 1,000 metric tons of lithium sulfide annually. The company is also aiming to mass-produce all-solid-state batteries in 2027.&lt;/p&gt;
    &lt;p&gt;The new batteries are part of Japan’s plans to secure a domestic supply chain and reduce its reliance on China and South Korea. Toyota is among several companies in Japan that are investing a combined $7 billion (1 trillion yen) in domestic battery production.&lt;/p&gt;
    &lt;head rend="h2"&gt;Electrek’s Take&lt;/head&gt;
    &lt;p&gt;Will Toyota be the “world’s first” to put all-solid-state EV batteries to practical use? Others, including Mercedes-Benz, BMW, Volkswagen, and Honda, are also betting on the new technology.&lt;/p&gt;
    &lt;p&gt;Mercedes claimed to have put “the first car powered by a lithium-metal solid-state battery on the road” in February. Just last month, Mercedes drove an EQS, equipped with solid-state batteries, for nearly 750 miles (1,205 km).&lt;/p&gt;
    &lt;head rend="h2"&gt;Top comment by john&lt;/head&gt;
    &lt;p&gt;A cynic might recognize a tactic from the PC world. The big company who doesn't have a competitive product attempts to mess up the market by making bold announcements of fantastic products that are coming "soon".&lt;/p&gt;
    &lt;p&gt;Mercedes’ tech boss, Markus Schäfer, is already calling the new EV battery tech a “gamechanger” for electric vehicles. The company aims to bring solid-state batteries into series production by the end of the decade.&lt;/p&gt;
    &lt;p&gt;Meanwhile, CATL and BYD, which are already dominating the global battery market, aim to introduce the new battery tech around 2027.&lt;/p&gt;
    &lt;p&gt;SAIC MG launched the new MG4 in August, deeming it “the world’s first mass-produced semi-solid-state” electric vehicle.&lt;/p&gt;
    &lt;p&gt;Can Toyota compete? It has been promising to launch all-solid-state batteries for years now, but new alliances could help make it a reality. As for the “world’s first,” however, that may be a stretch.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electrek.co/2025/10/08/toyota-aims-to-launch-worlds-first-all-solid-state-ev-batteries/"/><published>2025-10-10T18:53:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45543471</id><title>Show HN: Semantic search over the National Gallery of Art</title><updated>2025-10-10T21:32:08.984788+00:00</updated><content>&lt;doc fingerprint="d347f7e0daff0674"&gt;
  &lt;main&gt;
    &lt;p&gt;National Gallery of Art Nation Gallery of Art Mixedbread Github Discover art with natural language Still life paintings Paintings of flowers Woodcuts of landscapes Portraits of women Sculptures of animals Paintings of the sea Ancient coins Search through over 50,000 images from the National Gallery of Art public collection.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nga.demo.mixedbread.com/"/><published>2025-10-10T20:33:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45543475</id><title>I built physical album cards with NFC tags to teach my son music discovery</title><updated>2025-10-10T21:32:08.764569+00:00</updated><content>&lt;doc fingerprint="aa7b844cb974b30"&gt;
  &lt;main&gt;
    &lt;p&gt;by Jordan Fulghum, October 2025&lt;/p&gt;
    &lt;p&gt;Albums you can hold again.&lt;/p&gt;
    &lt;p&gt;When I was 10, I blew every dollar I had on CDs. I remember sitting cross-legged on my floor, flipping through jewel cases, memorizing liner notes and lyrics, and most importantly developing my own taste for music.&lt;/p&gt;
    &lt;p&gt;My 10-year-old doesn't have that. Music just sort of... happens. It's like it's infinite and invisible at the same time, playing from smart speakers, car stereos, my phone. Endless perfectly curated playlists, designed to fade into the background. The default listening experience has become both literally and figuratively formless.&lt;/p&gt;
    &lt;p&gt;So I thought: what's the modern equivalent of that CD binder experience? Maybe what's missing is something tangible that he can flip through, or even collect.&lt;/p&gt;
    &lt;p&gt;I could combine my old CD-collector brain with today's tech: take something fun and collectable (trading/Pokemon cards), dress them up with album art, and add NFC tags so they can be tapped to play the album on our home speaker system, all without a screen.&lt;/p&gt;
    &lt;p&gt;Away I went.&lt;/p&gt;
    &lt;p&gt;I needed to get the music into a format that could be played. I've long surrendered to streaming, but I still have my MP3s organized in Plex on my home server. Funny to think that these files are the same MP3s that I've been collecting since the late 90s. I wanted the NFC tag to be deep-linked to those same files instead of a streaming service.&lt;/p&gt;
    &lt;p&gt;But which albums do I pick? I had the idea to create themed "packs" of albums. The first pack is obviously "Albums That Dad Wants You to Listen To", and it's just a bunch of dad rock. But the idea is that each pack can be a different theme or genre, and he can build his own collection (and develop his own taste) over time.&lt;/p&gt;
    &lt;p&gt;I found a PDF template that matched the dimensions of trading cards, hopped into Canva and got to work. It was easy enough to find high-quality album cover images from Google, but....&lt;/p&gt;
    &lt;p&gt;I was quite far into this project when I remembered the obvious fact that album art is square but trading cards are rectangular. Trading cards use a 2.5:3.5 aspect ratio, which is...not a square! Oops.&lt;/p&gt;
    &lt;p&gt;I looked at what they did for cassette tapes (also rectangular) back in the day, but their solutions were all over the place, from just cropping the square into a rectangle (gross) to having a giant white space next to the square art. That wasn't gonna cut it.&lt;/p&gt;
    &lt;p&gt;So, I used an AI diffusion model to extend each album's art into a trading card aspect ratio. The AI was (mostly) able to extend the artwork while maintaining the original style and composition. Not perfect, but a pretty fun solution not possible just a couple years ago.&lt;/p&gt;
    &lt;p&gt;After ordering a bundle of blank NFC tags from Amazon, I learned that PlexAmp oddly has first-class support for zapping NFC tags to specific albums in auto play mode. A strange feature, but perfect for this project. Easy.&lt;/p&gt;
    &lt;p&gt;I printed the cards on our crappy HP inkjet printer at home. I used label paper that exactly matched the dimensions of trading cards, but after the fact, I realized it was kind of unnecessary. You can just print on cardstock if you have a digital template file. I cut them out and glued them to blank playing cards, but not before wedging the NFC tags between.&lt;/p&gt;
    &lt;p&gt;For the display stand, I borrowed a display stand model from Makerworld and 3D printed it on my A1. It turned out alright!&lt;/p&gt;
    &lt;p&gt;Once it was all working and in decent shape, I presented them in a nice neat arrangement to my son. He flipped through them like Pokémon cards, tapping the cards that were the most visually interesting. Daft Punk's Discovery was his first pick. He grabbed it, flipped it around, tapped it, and that One More Time loop dropped throughout our entire house. Boom.&lt;/p&gt;
    &lt;p&gt;I was happy to see that the physical cards encouraged active listening and ownership. Instead of music being background noise, it became something he could choose, hold, explore, maybe even trade with this sister!&lt;/p&gt;
    &lt;p&gt;I think we're unintentionally teaching our children to consume music passively. My goal with this project was to teach them to discover it actively, to own it, to care about it at the album level. I think it kinda worked!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fulghum.io/album-cards"/><published>2025-10-10T20:34:19+00:00</published></entry></feed>