<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-16T19:33:41.049406+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46644181</id><title>Interactive eBPF</title><updated>2026-01-16T19:33:46.079489+00:00</updated><content>&lt;doc fingerprint="72a069bbe37239b"&gt;
  &lt;main&gt;
    &lt;p&gt;Learn eBPF through hands-on exercises. Write, compile, and run programs directly from your browser.&lt;/p&gt;
    &lt;p&gt;Did you find an issue, or have an idea for a new exercise? Create an issue in the repository.&lt;/p&gt;
    &lt;p&gt;Curious about how it works? Here's an explanation.&lt;/p&gt;
    &lt;p&gt;Learn eBPF through hands-on exercises. Write, compile, and run programs directly from your browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ebpf.party/"/><published>2026-01-16T08:01:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46645176</id><title>Why DuckDB is my first choice for data processing</title><updated>2026-01-16T19:33:45.865406+00:00</updated><content>&lt;doc fingerprint="f73b88c145a718b4"&gt;
  &lt;main&gt;&lt;p&gt;Originally posted: 2025-03-16. View source code for this page here.&lt;/p&gt;&lt;p&gt;Over the past few years, I've found myself using DuckDB more and more for data processing, to the point where I now use it almost exclusively, usually from within Python.&lt;/p&gt;&lt;p&gt;We're moving towards a simpler world where most tabular data can be processed on a single large machine1 and the era of clusters is coming to an end for all but the largest datasets.2&lt;/p&gt;&lt;p&gt;This post sets out some of my favourite features of DuckDB that set it apart from other SQL-based tools. In a nutshell, it's simple to install, ergonomic, fast, and more fully featured.&lt;/p&gt;&lt;p&gt;An earlier post explains why I favour SQL over other APIs such as Polars, pandas or dplyr.&lt;/p&gt;&lt;p&gt;DuckDB is an open source in-process SQL engine that is optimised for analytics queries.&lt;/p&gt;&lt;p&gt;The performance difference of analytics-optimised engines (OLAP) vs. transactions-optimised engines (OLTP) should not be underestimated. A query running in DuckDB can be 100 or even 1,000 times faster than exactly the same query running in (say) SQLite or Postgres.&lt;/p&gt;&lt;p&gt;A core use-case of DuckDB is where you have one or more large datasets on disk in formats like &lt;code&gt;csv&lt;/code&gt;, &lt;code&gt;parquet&lt;/code&gt; or &lt;code&gt;json&lt;/code&gt; which you want to batch process.  You may want to perform cleaning, joins, aggregation, derivation of new columns - that sort of thing.&lt;/p&gt;&lt;p&gt;But you can also use DuckDB for many other simpler tasks like viewing a csv file from the command line.&lt;/p&gt;&lt;p&gt;DuckDB consistently benchmarks as one of the fastest data processing engines. The benchmarks I've seen3 show there's not much in it between the leading open source engines - which at the moment seem to be polars, DuckDB, DataFusion, Spark and Dask. Spark and Dask can be competitive on large data, but slower on small data.&lt;/p&gt;&lt;p&gt;DuckDB itself is a single precompiled binary. In Python, it can be &lt;code&gt;pip install&lt;/code&gt;ed with no dependencies.  This makes it a joy to install compared to other more heavyweight options like Spark.  Combined with &lt;code&gt;uv&lt;/code&gt;, you can stand up a fresh DuckDB Python environment from nothing in less than a second - see here.&lt;/p&gt;&lt;p&gt;With its speed and almost-zero startup time, DuckDB is ideally suited for CI and testing of data engineering pipelines.&lt;/p&gt;&lt;p&gt;Historically this has been fiddly and running a large suite of tests in e.g. Apache Spark has been time consuming and frustrating. Now it's much simpler to set up the test environment, and there's less scope for differences between it and your production pipelines.&lt;/p&gt;&lt;p&gt;This simplicity and speed also applies to writing new SQL, and getting syntax right before running it on a large dataset. Historically I have found this annoying in engines like Spark (where it takes a few seconds to start Spark in local mode), or even worse when you're forced to run queries in a proprietary tool like AWS Athena.4&lt;/p&gt;&lt;p&gt;There's even a DuckDB UI with autocomplete - see here.&lt;/p&gt;&lt;p&gt;The DuckDB team has implemented a wide range of innovations in its SQL dialect that make it a joy to use. See the following blog posts 1 2 3 4 5 6.&lt;/p&gt;&lt;p&gt;Some of my favourites are the &lt;code&gt;EXCLUDE&lt;/code&gt; keyword, and the &lt;code&gt;COLUMNS&lt;/code&gt; keyword which allows you to select and regex-replace a subset of columns.5  I also like &lt;code&gt;QUALIFY&lt;/code&gt; and the aggregate modifiers on window functions, see here.&lt;/p&gt;&lt;p&gt;Another is the ability to function chain, like &lt;code&gt;first_name.lower().trim()&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;You can query data directly from files, including on s3, or on the web.&lt;/p&gt;&lt;p&gt;For example to query a folder of parquet files:&lt;/p&gt;&lt;quote&gt;select *from read_parquet('path/to/*.parquet')&lt;/quote&gt;&lt;p&gt;or even (on CORS enabled files) you can run SQL directly:&lt;/p&gt;&lt;quote&gt;select *from read_parquet('https://raw.githubusercontent.com/plotly/datasets/master/2015_flights.parquet')limit 2;&lt;/quote&gt;&lt;p&gt;Click here to try this query yourself in the DuckDB web shell.&lt;/p&gt;&lt;p&gt;One of the easiest ways to cause problems in your data pipelines is to fail to be strict about incoming data types from untyped formats such as csv. DuckDB provides lots of options here - see here.&lt;/p&gt;&lt;p&gt;Many data pipelines effectively boil down to a long sequence of CTEs:&lt;/p&gt;&lt;quote&gt;WITHinput_data AS (SELECT * FROM read_parquet('...')),step_1 AS (SELECT ... FROM input_data JOIN ...),step_2 AS (SELECT ... FROM step_1)SELECT ... FROM step_2;&lt;/quote&gt;&lt;p&gt;When developing a pipeline like this, we often want to inspect what's happened at each step.&lt;/p&gt;&lt;p&gt;In Python, we can write&lt;/p&gt;&lt;quote&gt;input_data = duckdb.sql("SELECT * FROM read_parquet('...')")step_1 = duckdb.sql("SELECT ... FROM input_data JOIN ...")step_2 = duckdb.sql("SELECT ... FROM step_1")final = duckdb.sql("SELECT ... FROM step_2;")&lt;/quote&gt;&lt;p&gt;This makes it easy to inspect what the data looks like at &lt;code&gt;step_2&lt;/code&gt; with no performance loss, since these steps will be executed lazily when they're run all at once.&lt;/p&gt;&lt;p&gt;This also facilitates easier testing of SQL in CI, since each step can be an independently-tested function.&lt;/p&gt;&lt;p&gt;DuckDB offers full ACID compliance for bulk data operations, which sets it apart from other analytical data systems - see here. You can listen to more about this on in this podcast, transcribed here.&lt;/p&gt;&lt;p&gt;This is a very interesting new development, making DuckDB potentially a suitable replacement for lakehouse formats such as Iceberg or Delta lake for medium scale data.&lt;/p&gt;&lt;p&gt;A longstanding difficulty with data processing engines has been the difficulty in writing high performance user defined functions (UDFs).&lt;/p&gt;&lt;p&gt;For example, in PySpark, you will generally get best performance by writing custom Scala, compiling to a JAR, and registering it with Spark. But this is cumbersome and in practice, you will encounter a lot of issues around Spark version compatibility and security restrictions environments such as DataBricks.&lt;/p&gt;&lt;p&gt;In DuckDB high performance custom UDFs can be written in C++. Whilst writing these functions is certainly not trivial, DuckDB community extensions offers a low-friction way of distributing the code. Community extensions can be installed almost instantly with a single command such as &lt;code&gt;INSTALL h3 FROM community&lt;/code&gt; to install hierarchical hexagonal indexing for geospatial data.&lt;/p&gt;&lt;p&gt;The team provide documentation as a single markdown file so it can easily be provided to an LLM.&lt;/p&gt;&lt;p&gt;My top tip: if you load this file in your code editor, and use code folding, it's easy to copy the parts of the documentation you need into context.&lt;/p&gt;&lt;p&gt;Much of this blog post is based on my experience supporting multiple SQL dialects in Splink, an open source library for record linkage at scale. We've found that transitioning towards recommending DuckDB as the default backend choice has increased adoption of the library and significantly reduced the amount of problems faced by users, even for large linkage tasks, whilst speeding up workloads very substantially.&lt;/p&gt;&lt;p&gt;We've also found it's hugely increased the simplicity and speed of developing and testing new features.&lt;/p&gt;&lt;code&gt;pg_duckdb&lt;/code&gt; allows you to embed the DuckDB computation engine within Postgres.&lt;p&gt;The later in particular seems potentially extremely powerful, enabling Postgres to be simultanouesly optimised for analytics and transactional processing. I think it's likely to see widespread adoption, especially after they iron out a few of the current shortcomings around enabling and optimising the use of Postgres indexes and pushing up filters up to PostGres.&lt;/p&gt;&lt;p&gt;As a long-time Spark user, I am glad to be rid of needing to know lots of intricate configuration options for Spark tuning ��↩&lt;/p&gt;&lt;p&gt;With 192 core processors such as this available in the cloud and only costing around $15,000, the complexity of clusters can be avoided unless you have genuinely huge data. It's also worth noting there is actually now a distributed version of DuckDB, see here. ↩&lt;/p&gt;&lt;p&gt;For instance see here, here and here/discussion. ↩&lt;/p&gt;&lt;p&gt;To be clear, Athena is a very powerful and useful tool. I just find it frustrating for developing and quickly iterating queries of moderate complexity. An example of why it's easier in DuckDB is this kind of reprex. ↩&lt;/p&gt;&lt;p&gt;For instance, we can select all columns prefixed with &lt;code&gt;emp_&lt;/code&gt; and rename to remove the prefix as follows: &lt;code&gt;SELECT COLUMNS('emp_(.*)') AS '\1'&lt;/code&gt; ↩&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.robinlinacre.com/recommend_duckdb/"/><published>2026-01-16T10:57:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46645615</id><title>Just the Browser</title><updated>2026-01-16T19:33:45.730979+00:00</updated><content>&lt;doc fingerprint="1642ce52fea7b9c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Just the Browser helps you remove AI features, telemetry data reporting, sponsored content, product integrations, and other annoyances from desktop web browsers. The goal is to give you "just the browser" and nothing else, using hidden settings in web browsers intended for companies and other organizations.&lt;/p&gt;
    &lt;p&gt;This project includes configuration files for popular web browsers, documentation for installing and modifying them, and easy installation scripts. Everything is open-source on GitHub.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started&lt;/head&gt;
    &lt;p&gt;The setup script can install the configuration files in a few clicks. You can also follow the manual guides for Google Chrome, Microsoft Edge, and Firefox.&lt;/p&gt;
    &lt;p&gt;Windows: Open a PowerShell prompt as Administrator. You can do this by right-clicking the Windows button in the taskbar, then selecting the "Terminal (Admin)" or "PowerShell (Admin)" menu option. Next, copy the below command, paste it into the window (&lt;code&gt;Ctrl+V&lt;/code&gt;), and press the Enter/Return key:&lt;/p&gt;
    &lt;code&gt;&amp;amp; ([scriptblock]::Create((irm "https://raw.githubusercontent.com/corbindavenport/just-the-browser/main/main.ps1")))
&lt;/code&gt;
    &lt;p&gt;Mac and Linux: Search for the Terminal in your applications list and open it. Next, copy the below command, paste it into the window (&lt;code&gt;Ctrl+V&lt;/code&gt; or &lt;code&gt;Cmd+V&lt;/code&gt;), and press the Enter/Return key:&lt;/p&gt;
    &lt;code&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/corbindavenport/just-the-browser/main/main.sh)"
&lt;/code&gt;
    &lt;head rend="h2"&gt;Download web browsers&lt;/head&gt;
    &lt;p&gt;Start here if you don't have your preferred web browser installed. You can install the configuration files afterwards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Google Chrome&lt;/head&gt;
    &lt;p&gt;macOS (Universal) Windows 64-bit x86 (amd64) Windows 32-bit x86 Windows 64-bit ARM (ARM64) Debian/Ubuntu 64-bit x86 (amd64) Fedora/openSUSE 64-bit x86 (amd64)&lt;/p&gt;
    &lt;p&gt;Not sure which link to use? Try the official download page.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mozilla Firefox&lt;/head&gt;
    &lt;p&gt;macOS (Universal) Windows 64-bit x86 (amd64) Windows 32-bit x86 Windows 64-bit ARM (ARM64)&lt;/p&gt;
    &lt;p&gt;Not sure which link to use? Try the official download page or Linux setup instructions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Microsoft Edge&lt;/head&gt;
    &lt;p&gt;macOS (Universal) Windows 64-bit x86 (amd64) Windows 32-bit x86 Windows 64-bit ARM (ARM64)&lt;/p&gt;
    &lt;p&gt;Not sure which link to use? Try the official download page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Questions and answers&lt;/head&gt;
    &lt;p&gt;Got a question? Check here first, and if you still need help, create an issue on GitHub or join the Discord.&lt;/p&gt;
    &lt;head rend="h3"&gt;What features or settings are changed?&lt;/head&gt;
    &lt;p&gt;Just the Browser aims to remove the following functionality from popular web browsers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Most AI features: Features that use generative AI models, either on-device or in the cloud, like Copilot in Microsoft Edge or tab group suggestions in Firefox. The main exception is page translation in Firefox.&lt;/item&gt;
      &lt;item&gt;Shopping features: Price tracking, coupon codes, loan integrations, etc.&lt;/item&gt;
      &lt;item&gt;Sponsored or third-party content: Suggested articles on the New Tab Page, sponsored site suggestions, etc.&lt;/item&gt;
      &lt;item&gt;Default browser reminders: Pop-ups or other prompts that ask you to change the default web browser.&lt;/item&gt;
      &lt;item&gt;First-run experiences and data import prompts: Browser welcome screens and their related prompts to import data automatically from other web browsers.&lt;/item&gt;
      &lt;item&gt;Telemetry: Data collection by web browsers. Crash reporting is left enabled if the browser (such as Firefox) supports it as a separate option.&lt;/item&gt;
      &lt;item&gt;Startup boost: Features that allow web browsers to start with the operating system without explicit permission.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The exact list of features modified for each browser can be found on the pages for Google Chrome, Microsoft Edge, and Firefox.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I change or remove the settings?&lt;/head&gt;
    &lt;p&gt;Yes. The browser guides include steps for removing the configurations, and the automated script can also do it. The browser guides explain each setting, so you can add, remove, or modify the files before you install them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Which web browsers are supported?&lt;/head&gt;
    &lt;p&gt;Just the Browser has configuration files and setup scripts for Google Chrome, Microsoft Edge, and Mozilla Firefox. However, Chrome on Linux and Edge on Linux are not currently supported.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I install this on my phone or tablet?&lt;/head&gt;
    &lt;p&gt;Not yet. See the issues for Android support and iOS/iPadOS support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is this modifying the web browser?&lt;/head&gt;
    &lt;p&gt;No. Just the Browser uses group policies that are fully supported by web browsers, usually intended for IT departments in companies or other large organizations. No applications or executable files are modified in any way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Do the settings stay applied?&lt;/head&gt;
    &lt;p&gt;Yes, as long as the web browsers continue to support the settings used in the configuration files. Web browsers occasionally add, remove, or replace the settings options, so if the custom configuration breaks, try installing the latest available version.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does this install ad blockers for me?&lt;/head&gt;
    &lt;p&gt;No. If you want one, try uBlock Origin or uBlock Origin Lite.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why does my browser say it's managed by an organization?&lt;/head&gt;
    &lt;p&gt;The group policy settings used by Just the Browser are intended for PCs managed by companies and other large organizations. Browsers like Microsoft Edge and Firefox will display a message like "Your browser is being managed by your organization" to explain why some settings are disabled.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I know the settings are applied?&lt;/head&gt;
    &lt;p&gt;You can open &lt;code&gt;about:policies&lt;/code&gt; in Firefox or &lt;code&gt;chrome://policy&lt;/code&gt; in Chrome and Edge to see a list of active group policy settings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not just use an alternative web browser?&lt;/head&gt;
    &lt;p&gt;You can do that! However, switching to alternative web browsers like Vivaldi, SeaMonkey, Waterfox, or LibreWolf can have other downsides. They are not always available on the same platforms, and they can lag behind mainstream browsers in security updates and engine upgrades. Just the Browser aims to make mainstream web browsers more tolerable, while still retaining their existing benefits.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://justthebrowser.com/"/><published>2026-01-16T12:03:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46646091</id><title>psc: The ps utility, with an eBPF twist and container context</title><updated>2026-01-16T19:33:45.345001+00:00</updated><content>&lt;doc fingerprint="75130b4d6aa147a1"&gt;
  &lt;main&gt;
    &lt;p&gt;psc (ps container) is a process scanner that uses eBPF iterators and Google CEL to query system state with precision and full container context.&lt;/p&gt;
    &lt;p&gt;psc requires root privileges to load eBPF programs.&lt;/p&gt;
    &lt;p&gt;Traditional Linux tools like &lt;code&gt;ps&lt;/code&gt;, &lt;code&gt;lsof&lt;/code&gt;, and &lt;code&gt;ss&lt;/code&gt; are powerful but inflexible. They output fixed formats that require extensive piping through &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;awk&lt;/code&gt;, and &lt;code&gt;sed&lt;/code&gt; to extract useful information:&lt;/p&gt;
    &lt;code&gt;# Find all nginx processes owned by root
ps aux | grep nginx | grep root | grep -v grep

# With psc:
psc 'process.name == "nginx" &amp;amp;&amp;amp; process.user == "root"'&lt;/code&gt;
    &lt;code&gt;# Find processes with established connections on port 443
ss -tnp | grep ESTAB | grep :443 | awk '{print $6}' | cut -d'"' -f2

# With psc:
psc 'socket.state == established &amp;amp;&amp;amp; socket.dstPort == uint(443)'&lt;/code&gt;
    &lt;code&gt;# Find containerized processes
ps aux | xargs -I{} sh -c 'cat /proc/{}/cgroup 2&amp;gt;/dev/null | grep -q docker &amp;amp;&amp;amp; echo {}'

# With psc:
psc 'container.runtime == docker'&lt;/code&gt;
    &lt;p&gt;These tools also read from &lt;code&gt;/proc&lt;/code&gt;, a virtual filesystem that can be manipulated by userland rootkits. A compromised library loaded via &lt;code&gt;LD_PRELOAD&lt;/code&gt; can intercept system calls and hide processes, network connections, or files from these traditional utilities.&lt;/p&gt;
    &lt;p&gt;psc uses eBPF iterators to read process and file descriptor information directly from kernel data structures. This bypasses the &lt;code&gt;/proc&lt;/code&gt; filesystem entirely, providing visibility that cannot be subverted by userland rootkits or &lt;code&gt;LD_PRELOAD&lt;/code&gt; tricks. When an attacker uses &lt;code&gt;LD_PRELOAD&lt;/code&gt; to inject a malicious shared library that intercepts calls to &lt;code&gt;readdir()&lt;/code&gt; or &lt;code&gt;open()&lt;/code&gt;, traditional tools see only what the rootkit allows. psc reads kernel memory directly via eBPF, seeing the true system state.&lt;/p&gt;
    &lt;p&gt;Instead of chaining &lt;code&gt;grep&lt;/code&gt; commands, psc uses the Common Expression Language (CEL) to filter processes. CEL is a simple, safe expression language designed for evaluating boolean conditions. It allows you to answer:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What is running: Filter by process name, command line, user, or PID&lt;/item&gt;
      &lt;item&gt;Where it is running: Filter by container runtime, container name, image, or labels&lt;/item&gt;
      &lt;item&gt;Why it is running: Inspect open file descriptors, network connections (ports, states, protocols), and socket types to understand what a process is doing and why it exists&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With psc, you can inspect any container's processes, open files, and network connections directly from the host.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux kernel 5.8 or later (eBPF iterators were introduced in this version)&lt;/item&gt;
      &lt;item&gt;Go 1.25 or later&lt;/item&gt;
      &lt;item&gt;Clang and LLVM&lt;/item&gt;
      &lt;item&gt;libbpf development headers&lt;/item&gt;
      &lt;item&gt;Linux kernel headers&lt;/item&gt;
      &lt;item&gt;bpftool (for generating vmlinux.h)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On Debian/Ubuntu:&lt;/p&gt;
    &lt;code&gt;sudo apt-get install clang llvm libbpf-dev linux-headers-$(uname -r) linux-tools-$(uname -r)&lt;/code&gt;
    &lt;p&gt;On Fedora/RHEL:&lt;/p&gt;
    &lt;code&gt;sudo dnf install clang llvm libbpf-devel kernel-devel bpftool&lt;/code&gt;
    &lt;code&gt;# Generate vmlinux.h (required once per kernel version)
make vmlinux

# Build the binary
make build&lt;/code&gt;
    &lt;p&gt;Or manually:&lt;/p&gt;
    &lt;code&gt;bpftool btf dump file /sys/kernel/btf/vmlinux format c &amp;gt; bpf/vmlinux.h
go generate ./...
go build -o psc&lt;/code&gt;
    &lt;code&gt;sudo make install&lt;/code&gt;
    &lt;code&gt;# List all processes
psc

# List all processes as a tree
psc --tree&lt;/code&gt;
    &lt;p&gt;Pass a CEL expression as the first argument to filter processes:&lt;/p&gt;
    &lt;code&gt;# Filter by process name
psc 'process.name == "nginx"'

# Filter by user
psc 'process.user == "root"'

# Filter by command line content
psc 'process.cmdline.contains("--config")'

# Filter by PID range
psc 'process.pid &amp;gt; 1000 &amp;amp;&amp;amp; process.pid &amp;lt; 2000'

# Combine conditions
psc 'process.name == "bash" || process.name == "zsh"'&lt;/code&gt;
    &lt;code&gt;# Show only containerized processes
psc 'container.id != ""'

# Filter by container runtime (constants: docker, containerd, crio, podman)
psc 'container.runtime == docker'

# Filter by container name
psc 'container.name == "nginx"'

# Filter by container image
psc 'container.image.contains("nginx:latest")'

# Show as tree to see container process hierarchy
psc 'container.runtime == docker' --tree&lt;/code&gt;
    &lt;p&gt;Understanding why a process exists often requires looking at its open file descriptors and network connections:&lt;/p&gt;
    &lt;code&gt;# Find processes with listening TCP sockets
psc 'socket.type == tcp &amp;amp;&amp;amp; socket.state == listen'

# Find processes with established connections
psc 'socket.state == established'

# Find processes connected to a specific port
psc 'socket.dstPort == uint(443)'

# Find processes using Unix sockets
psc 'socket.family == unix'

# Find processes with files open in /etc
psc 'file.path.startsWith("/etc")'&lt;/code&gt;
    &lt;p&gt;Process fields (&lt;code&gt;process.X&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;name&lt;/code&gt;- Process name (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pid&lt;/code&gt;- Process ID (int)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ppid&lt;/code&gt;- Parent process ID (int)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tid&lt;/code&gt;- Thread ID (int)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;euid&lt;/code&gt;- Effective user ID (int)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ruid&lt;/code&gt;- Real user ID (int)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;suid&lt;/code&gt;- Saved set-user-ID (int)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;user&lt;/code&gt;- Username (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cmdline&lt;/code&gt;- Full command line (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;state&lt;/code&gt;- Process state (uint)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Capability fields (&lt;code&gt;process.capabilities.X&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;effective&lt;/code&gt;- Effective capabilities bitmask (uint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;permitted&lt;/code&gt;- Permitted capabilities bitmask (uint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;inheritable&lt;/code&gt;- Inheritable capabilities bitmask (uint)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Namespace fields (&lt;code&gt;process.namespaces.X&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;net&lt;/code&gt;- Network namespace inode (uint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pid&lt;/code&gt;- PID namespace inode (uint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mnt&lt;/code&gt;- Mount namespace inode (uint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;uts&lt;/code&gt;- UTS namespace inode (uint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ipc&lt;/code&gt;- IPC namespace inode (uint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cgroup&lt;/code&gt;- Cgroup namespace inode (uint)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Container fields (&lt;code&gt;container.X&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;id&lt;/code&gt;- Container ID (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;name&lt;/code&gt;- Container name (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;image&lt;/code&gt;- Container image (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;runtime&lt;/code&gt;- Container runtime (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;labels&lt;/code&gt;- Container labels (map)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;File/Socket fields (&lt;code&gt;file.X&lt;/code&gt; or &lt;code&gt;socket.X&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;path&lt;/code&gt;- File path (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fd&lt;/code&gt;- File descriptor number (int)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;srcPort&lt;/code&gt;- Source port (uint, use&lt;code&gt;uint()&lt;/code&gt;for comparisons:&lt;code&gt;socket.srcPort == uint(80)&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;dstPort&lt;/code&gt;- Destination port (uint, use&lt;code&gt;uint()&lt;/code&gt;for comparisons:&lt;code&gt;socket.dstPort == uint(443)&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;type&lt;/code&gt;- Socket type (tcp, udp)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;state&lt;/code&gt;- Socket state (for filtering, use constants like&lt;code&gt;listen&lt;/code&gt;,&lt;code&gt;established&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;family&lt;/code&gt;- Address family (unix, inet, inet6)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;unixPath&lt;/code&gt;- Unix socket path (string)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fdType&lt;/code&gt;- FD type (file_type, socket_type)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use these without quotes in expressions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Runtimes: &lt;code&gt;docker&lt;/code&gt;,&lt;code&gt;containerd&lt;/code&gt;,&lt;code&gt;crio&lt;/code&gt;,&lt;code&gt;podman&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Socket types: &lt;code&gt;tcp&lt;/code&gt;,&lt;code&gt;udp&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Address families: &lt;code&gt;unix&lt;/code&gt;,&lt;code&gt;inet&lt;/code&gt;,&lt;code&gt;inet6&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Socket states (for filtering): &lt;code&gt;established&lt;/code&gt;,&lt;code&gt;listen&lt;/code&gt;,&lt;code&gt;syn_sent&lt;/code&gt;,&lt;code&gt;syn_recv&lt;/code&gt;,&lt;code&gt;fin_wait1&lt;/code&gt;,&lt;code&gt;fin_wait2&lt;/code&gt;,&lt;code&gt;time_wait&lt;/code&gt;,&lt;code&gt;close&lt;/code&gt;,&lt;code&gt;close_wait&lt;/code&gt;,&lt;code&gt;last_ack&lt;/code&gt;,&lt;code&gt;closing&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;FD types: &lt;code&gt;file_type&lt;/code&gt;,&lt;code&gt;socket_type&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Note: Output uses&lt;/p&gt;&lt;code&gt;ss&lt;/code&gt;-style state names:&lt;code&gt;ESTAB&lt;/code&gt;,&lt;code&gt;LISTEN&lt;/code&gt;,&lt;code&gt;SYN-SENT&lt;/code&gt;, etc. For UDP sockets, only&lt;code&gt;UNCONN&lt;/code&gt;(unconnected) or&lt;code&gt;ESTAB&lt;/code&gt;(connected) are shown since UDP is connectionless.&lt;/quote&gt;
    &lt;p&gt;CEL provides string manipulation functions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;.contains("substr")&lt;/code&gt;- Check if string contains substring&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.startsWith("prefix")&lt;/code&gt;- Check if string starts with prefix&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.endsWith("suffix")&lt;/code&gt;- Check if string ends with suffix&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--tree&lt;/code&gt;,&lt;code&gt;-t&lt;/code&gt;- Display processes as a tree&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--no-color&lt;/code&gt;- Disable colored output&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-o&lt;/code&gt;,&lt;code&gt;--output&lt;/code&gt;- Custom output columns (comma-separated field names)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;-o&lt;/code&gt; flag lets you specify exactly which fields to display. You can use presets for common use cases or specify individual fields.&lt;/p&gt;
    &lt;p&gt;Presets:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;sockets&lt;/code&gt;- Process info + full socket details (family, type, state, addresses, ports)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;files&lt;/code&gt;- Process info + file descriptor details (fd, type, path)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;containers&lt;/code&gt;- Process info + container details (name, image, runtime)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;network&lt;/code&gt;- Compact network view (pid, name, type, state, ports)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Use a preset
psc 'socket.state == listen' -o sockets
psc 'container.id != ""' -o containers

# Or specify individual fields
psc -o process.pid,process.name,process.user
psc 'socket.state == listen' -o process.pid,process.name,socket.srcPort,socket.state&lt;/code&gt;
    &lt;p&gt;When the output includes file/socket fields and the filter matches multiple files per process, each match gets its own row:&lt;/p&gt;
    &lt;code&gt;$ psc 'socket.state == listen' -o network

PID      NAME      TYPE   STATE    SRCPORT   DSTPORT
1234     nginx     tcp    LISTEN   80        0
1234     nginx     tcp    LISTEN   443       0
5678     sshd      tcp    LISTEN   22        0&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;psc fields&lt;/code&gt; to list all available fields and presets:&lt;/p&gt;
    &lt;code&gt;psc fields&lt;/code&gt;
    &lt;p&gt;Find all web servers:&lt;/p&gt;
    &lt;code&gt;psc 'process.name == "nginx" || process.name == "apache2" || process.name == "httpd"'&lt;/code&gt;
    &lt;p&gt;Find processes listening on privileged ports:&lt;/p&gt;
    &lt;code&gt;psc 'socket.state == listen &amp;amp;&amp;amp; socket.srcPort &amp;lt; uint(1024)'&lt;/code&gt;
    &lt;p&gt;Find processes in a different network namespace (useful for container/pod inspection):&lt;/p&gt;
    &lt;code&gt;psc 'process.namespaces.net != uint(4026531840)' -o process.pid,process.name,process.namespaces.net&lt;/code&gt;
    &lt;p&gt;Show capabilities for privileged processes:&lt;/p&gt;
    &lt;code&gt;psc 'process.euid == 0' -o process.pid,process.name,process.capabilities.effective,process.capabilities.permitted&lt;/code&gt;
    &lt;p&gt;Find processes that elevated privileges via SUID binaries (real UID differs from effective UID):&lt;/p&gt;
    &lt;code&gt;psc 'process.ruid != process.euid'&lt;/code&gt;
    &lt;p&gt;Find Docker containers running as root:&lt;/p&gt;
    &lt;code&gt;psc 'container.runtime == docker &amp;amp;&amp;amp; process.user == "root"'&lt;/code&gt;
    &lt;p&gt;Debug a specific container:&lt;/p&gt;
    &lt;code&gt;psc 'container.name == "my-app"' --tree&lt;/code&gt;
    &lt;p&gt;Find processes with connections to external services:&lt;/p&gt;
    &lt;code&gt;psc 'socket.state == established &amp;amp;&amp;amp; socket.dstPort == uint(443)'&lt;/code&gt;
    &lt;p&gt;Show network connections with custom columns:&lt;/p&gt;
    &lt;code&gt;psc 'socket.state == established' -o process.pid,process.name,socket.srcPort,socket.dstPort,socket.dstAddr&lt;/code&gt;
    &lt;p&gt;List containerized processes with their container info:&lt;/p&gt;
    &lt;code&gt;psc 'container.id != ""' -o process.pid,process.name,process.user,container.name,container.image&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/loresuso/psc"/><published>2026-01-16T13:20:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46646226</id><title>Dev-owned testing: Why it fails in practice and succeeds in theory</title><updated>2026-01-16T19:33:45.269820+00:00</updated><content/><link href="https://dl.acm.org/doi/10.1145/3780063.3780066"/><published>2026-01-16T13:39:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46646263</id><title>Michelangelo's first painting, created when he was 12 or 13</title><updated>2026-01-16T19:33:45.111642+00:00</updated><content>&lt;doc fingerprint="ad990b521b25664"&gt;
  &lt;main&gt;
    &lt;p&gt;Think back, if you will, to the works of art you created at age twelve or thirteen. For many, perhaps most of us, our output at that stage of adolescence amounted to directionless doodles, chaotic comics, and a few unsteady-at-best school projects. But then, most of us didn’t grow up to be Michelangelo. In the late fourteen-eighties, when that towering Renaissance artist was still what we would now call a “tween,” he painted The Torment of Saint Anthony, a depiction of the titular religious figure beset by demons in the desert. Though based on a widely known engraving, it nevertheless shows evidence of rapidly advancing technique, inspiration, and even creativity — especially when placed under the infrared scanner.&lt;/p&gt;
    &lt;p&gt;For about half a millennium, The Torment of Saint Anthony wasn’t thought to have been painted by Michelangelo. As explained in the video from Inspiraggio just below, when the painting sold at Sotheby’s in 2008, the buyer took it to the Metropolitan Museum of Art for examination and cleaning.&lt;/p&gt;
    &lt;p&gt;“Beneath the layers of dirt accumulated over the centuries,” says the narrator, “a very particular color palette appeared. “The tones, the blends, the way the human figure was treated: all of it began to resemble the style Michelangelo would use years later in none other than the Sistine Chapel.” Infrared reflectography subsequently turned up pentimenti, or correction marks, a common indication that “a painting is not a copy, but an original work created with artistic freedom.”&lt;/p&gt;
    &lt;p&gt;It was the Kimbell Art Museum in Fort Worth, Texas that first bet big on the provenance of The Torment of Saint Anthony. Its newly hired director purchased the painting after turning up “not a single convincing argument against the attribution.” Thus acquired, it became “the only painting by Michelangelo located anywhere in the Americas, and also just one of four easel paintings attributed to him throughout his entire career,” during most of which he disparaged oil painting itself. About a decade later, and after further analysis, the art historian Giorgio Bonsanti put his considerable authority behind a definitive confirmation that it is indeed the work of the young Michelangelo. There remain doubters, of course, and even the notoriously uncompromising artist himself may have considered it an immature work unworthy of his name. But who else could have created an immature work like it?&lt;/p&gt;
    &lt;p&gt;Related Content:&lt;/p&gt;
    &lt;p&gt;How Four Masters — Michelangelo, Donatello, Verrocchio &amp;amp; Bernini — Sculpted David&lt;/p&gt;
    &lt;p&gt;A Secret Room with Drawings Attributed to Michelangelo Opens to Visitors in Florence&lt;/p&gt;
    &lt;p&gt;Michelangelo’s Illustrated Grocery List&lt;/p&gt;
    &lt;p&gt;Based in Seoul, Colin Marshall writes and broadcasts on cities, language, and culture. He’s the author of the newsletter Books on Cities as well as the books 한국 요약 금지 (No Summarizing Korea) and Korean Newtro. Follow him on the social network formerly known as Twitter at @colinmarshall.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.openculture.com/2026/01/discover-michelangelos-first-painting.html"/><published>2026-01-16T13:44:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46646645</id><title>Cloudflare acquires Astro</title><updated>2026-01-16T19:33:44.807486+00:00</updated><content>&lt;doc fingerprint="7fa05838a90aa8e6"&gt;
  &lt;main&gt;
    &lt;p&gt;The Astro Technology Company — the company behind the Astro web framework — is joining Cloudflare! Adoption of the Astro web framework continues to double every year, and Astro 6 is right around the corner. With Cloudflare’s support, we’ll have more resources and fewer distractions to continue our mission to build the best framework for content-driven websites.&lt;/p&gt;
    &lt;p&gt;What this means for Astro:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Astro stays open-source and MIT-licensed&lt;/item&gt;
      &lt;item&gt;Astro continues to be actively maintained&lt;/item&gt;
      &lt;item&gt;Astro continues to support a wide set of deployment targets, not just Cloudflare&lt;/item&gt;
      &lt;item&gt;Astro’s open governance and current roadmap remain in place.&lt;/item&gt;
      &lt;item&gt;All full-time employees of The Astro Technology Company are now employees of Cloudflare, and will continue to work on Astro full-time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How Astro started&lt;/head&gt;
    &lt;p&gt;In 2021, Astro was born out of frustration. The trend at the time was that every website should be architected as an application, and then shipped to the user’s browser to render. This was not very performant, and we’ve spent the last decade coming up with more and more complex solutions to solve for that performance problem. SSR, ISR, RSC, PPR, TTI optimizations via code-splitting, tree-shaking, lazy-loading, all to generate a blocking double-data hydration payload from a pre-warmed server running halfway around the world.&lt;/p&gt;
    &lt;p&gt;Our mission to design a web framework specifically for building websites — what we call content-driven websites, to better distinguish from data-driven, stateful web applications — resonated. Now Astro is downloaded almost 1,000,000 times per week, and has been used by 100,000s of developers to build fast, beautiful websites. Today you’ll find Astro all over the web, powering major websites and even entire developer platforms for companies like Webflow, Wix, Microsoft, and Google.&lt;/p&gt;
    &lt;p&gt;Along the way, we also tried to grow a business. In 2021 we raised some money and formed The Astro Technology Company. Our larger vision was that a well-designed framework like Astro could sit at the center of a massive developer platform, with optional hosted primitives (database, storage, analytics) designed in lockstep with the framework.&lt;/p&gt;
    &lt;p&gt;We were never able to realize this vision. Attempts to introduce paid, hosted primitives into our ecosystem fell flat, and rarely justified their own existence. We considered going more directly after first-class hosting or content management for Astro, but knew we’d spend much of our time playing catchup to well-funded, savvy competitors. We kept exploring different ideas, but nothing clicked with users the same way Astro did.&lt;/p&gt;
    &lt;p&gt;It wasn’t all bad. Astro DB (our attempt to build a hosted database product for Astro projects) eventually evolved into the open, built-in Astro database client that still lives in core today. Our exploration into building an e-commerce layer with Astro was eventually open-sourced. It was rewarding work, but over the years the distraction took its toll. Each attempt at a new paid product or offering took myself and others on the project away from working on the Astro framework that developers were using and loving every day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Returning to Focus&lt;/head&gt;
    &lt;p&gt;Last year, Dane (Cloudflare CTO) and I began to talk more seriously about the future of the web. Those conversations quickly grew into something bigger: What does the next decade look like? How do frameworks adapt to a world of AI coding and agents?&lt;/p&gt;
    &lt;p&gt;It became clear that even as web technologies evolve, content remains at the center. We realized that we’ve each been working toward this same vision from different angles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cloudflare has been solving it from the infrastructure side: betting on a platform that is global by default, with fast startup, low latency, and security built-in.&lt;/item&gt;
      &lt;item&gt;Astro has been solving it from the framework side: betting on a web framework that makes it easy to build sites that are fast by default, without overcomplicating things.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The overlap is obvious. By working together, Cloudflare gives us the backing we need to keep innovating for our users. Now we can stop spending cycles worrying about building a business on top of Astro, and start focusing 100% on the code, with a shared vision to move the web forward.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cloudflare ❤️ Astro&lt;/head&gt;
    &lt;p&gt;Cloudflare has been a long-time sponsor and champion of Astro. They have a proven track record of supporting great open-source projects like Astro, TanStack, and Hono without trying to capture or lock anything down. Staying open to all was a non-negotiable requirement for both us and for Cloudflare.&lt;/p&gt;
    &lt;p&gt;That is why Astro will remain free, open-source, and MIT-licensed. We will continue to run our project in the open, with an open governance model for contributors and an open community roadmap that anyone can participate in. We remain fully committed to maintaining Astro as a platform-agnostic framework, meaning we will continue to support and improve deployments for all targets—not just Cloudflare.&lt;/p&gt;
    &lt;p&gt;With Cloudflare’s resources and support, we can now return our focus fully towards building the best web framework for content-driven websites. The web is changing fast, and the bar keeps rising: performance, scale, reliability, and a better experience for the teams shipping content on the web.&lt;/p&gt;
    &lt;p&gt;You’ll see that focus reflected across our roadmap, as we prepare for the upcoming Astro 6 release (beta out now!) and our 2026 roadmap. Stay tuned!&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you&lt;/head&gt;
    &lt;p&gt;I want to extend a huge thank you to the agencies, companies, sponsors, partners, and theme authors who chose to work with us over the years. Thank you to our initial investors — Haystack, Gradient, Uncorrelated, Lightspeed — without whom Astro likely wouldn’t exist. Thank you to everyone in our open source community who continues to help make Astro better every day. And finally, thank you to everyone who uses Astro and puts their trust in us to help them build for the web.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://astro.build/blog/joining-cloudflare/"/><published>2026-01-16T14:25:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46646777</id><title>Cursor's latest "browser experiment" implied success without evidence</title><updated>2026-01-16T19:33:44.722977+00:00</updated><content>&lt;doc fingerprint="ae459215cbf1304d"&gt;
  &lt;main&gt;
    &lt;p&gt;2026-01-16&lt;/p&gt;
    &lt;head rend="h1"&gt;Cursor's latest "browser experiment" implied success without evidence&lt;/head&gt;
    &lt;p&gt;On January 14th 2026, Cursor published a blog post titled "Scaling long-running autonomous coding" (https://cursor.com/blog/scaling-agents)&lt;/p&gt;
    &lt;p&gt;In the blog post, they talk about their experiments with running "coding agents autonomously for weeks" with the explicit goal of&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;understand[ing] how far we can push the frontier of agentic coding for projects that typically take human teams months to complete&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;They talk about some approaches they tried, why they think those failed, and how to address the difficulties.&lt;/p&gt;
    &lt;p&gt;Finally they arrived at a point where something "solved most of our coordination problems and let us scale to very large projects without any single agent", which then led to this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub (https://github.com/wilsonzlin/fastrender)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is where things get a bit murky and unclear. They claim "Despite the codebase size, new agents can still understand it and make meaningful progress" and "Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts", but they never actually say if this is successful or not, is it actually working? Can you run this browser yourself? We don't know and they never say explicitly.&lt;/p&gt;
    &lt;p&gt;After this, they embed the following video:&lt;/p&gt;
    &lt;p&gt;And below it, they say "While it might seem like a simple screenshot, building a browser from scratch is extremely difficult.".&lt;/p&gt;
    &lt;head rend="h3"&gt;They never actually claim this browser is working and functional&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;error: could not compile 'fastrender' (lib) due to 34 previous errors; 94 warnings emitted&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And if you try to compile it yourself, you'll see that it's very far away from being a functional browser at all, and seemingly, it never actually was able to build.&lt;/p&gt;
    &lt;p&gt;Multiple recent GitHub Actions runs on &lt;code&gt;main&lt;/code&gt; show
failures (including workflow-file errors), and independent build
attempts report dozens of compiler errors, recent PRs were all merged
with failing CI, and going back in the Git history from most recent
commit back 100 commits,&lt;lb/&gt;I couldn't find a single commit that compiled cleanly.&lt;/p&gt;
    &lt;p&gt;I'm not sure what the "agents" they unleashed on this codebase actually did, but they seemingly never ran "cargo build" or even less "cargo check", because both of those commands surface 10s of errors (which surely would balloon should we solve them) and about 100 warnings. There is an open GitHub issue in their repository about this right now: https://github.com/wilsonzlin/fastrender/issues/98&lt;/p&gt;
    &lt;p&gt;And diving into the codebase, if the compilation errors didn't make that clear already, makes it very clear to any software developer that none of this is actually engineered code. It is what is typically known as "AI slop", low quality something that surely represents something, but it doesn't have intention behind it, and it doesn't even compile at this point.&lt;/p&gt;
    &lt;p&gt;They later start to talk about what's next, but not a single word about how to run it, what to expect, how it's working or anything else. Cursor's blog post provides no reproducible demo and no known-good revision (tag/release/commit) to verify the screenshots, beyond linking the repo.&lt;/p&gt;
    &lt;p&gt;Regardless of intent, Cursor's blog post creates the impression of a functioning prototype while leaving out the basic reproducibility markers one would expect from such claim. They never explicitly claim it's actually working, so no one can say they lied at least.&lt;/p&gt;
    &lt;p&gt;They finish off the article saying:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;But the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Which seems like a really strange conclusion to arrive at, when all they've proved so far, is that agents can output millions of tokens and still not end up with something that actually works.&lt;/p&gt;
    &lt;p&gt;A "browser experiment" doesn't need to rival Chrome. A reasonable minimum bar is: it compiles on a supported toolchain and can render a trivial HTML file. Cursor's post doesn’t demonstrate that bar, and current public build attempts fail at this too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Cursor never says "this browser is production-ready", but they do frame it as "building a web browser from scratch" and "meaningful progress" and then use a screenshot and "extremely difficult" language, wanting to give the impression that this experiment actually was a success.&lt;/p&gt;
    &lt;p&gt;The closest they get to implying that this was a success, is this part:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But this extraordinary claim isn't backed up by any evidence. In the blog post they never provide a working commit, build instructions or even a demo that can be reproduced.&lt;/p&gt;
    &lt;p&gt;I don't think anyone expects this browser to be the next Chrome, but I do think that if you claim you've built a browser, it should at least be able to demonstrate being able to be compiled + loading a basic HTML file at the very least.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://embedding-shapes.github.io/cursor-implied-success-without-evidence/"/><published>2026-01-16T14:37:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46646958</id><title>Can You Disable Spotlight and Siri in macOS Tahoe?</title><updated>2026-01-16T19:33:44.635702+00:00</updated><content>&lt;doc fingerprint="ac2eb2576a25bef1"&gt;
  &lt;main&gt;
    &lt;p&gt;For some, Spotlight and even Siri are indispensable, for others they’re just a waste of CPU and storage space. If you want to disable them, how is that best achieved?&lt;/p&gt;
    &lt;head rend="h4"&gt;Siri&lt;/head&gt;
    &lt;p&gt;The only documented way to turn Siri off is in its section in System Settings, where you should disable Siri Requests.&lt;/p&gt;
    &lt;p&gt;Although Siri will then be essentially inactive, it still doesn’t disappear. During startup, &lt;code&gt;siriactionsd&lt;/code&gt; runs, and &lt;code&gt;siriknowledged&lt;/code&gt; and some other of its services remain listed in Activity Monitor.&lt;/p&gt;
    &lt;head rend="h4"&gt;Spotlight&lt;/head&gt;
    &lt;p&gt;If you disable every item in Spotlight’s section in System Settings, that doesn’t disable Spotlight, nor stop it from indexing mounted volumes. Indeed, you may find it slows some Finder operations. Traditionally there have been two commands used in Terminal to try to disable Spotlight, depending on which of its features you want to stop.&lt;/p&gt;
    &lt;p&gt;The most common recommendation is to use&lt;code&gt;sudo mdutil -a -i off&lt;/code&gt;&lt;lb/&gt; to disable Spotlight indexing, but that doesn’t stop its searches, and it may not even do that on the current Data volume. When you run that command, &lt;code&gt;mdutil&lt;/code&gt; should inform you that indexing is disabled on each mounted volume, and Spotlight has been switched to kMDConfigSearchLevelFSSearchOnly. Although that’s reported for the root volume / and the Data volume at /System/Volumes/Data, I was still able to search and find files in the latter after running that command.&lt;/p&gt;
    &lt;p&gt;This might be related to previously reported problems disabling just the Data volume, which could require use of the explicit path /System/Volumes/Data.&lt;/p&gt;
    &lt;p&gt;The alternative is to use&lt;code&gt;sudo mdutil -a -d&lt;/code&gt;&lt;lb/&gt; as that disables both Spotlight searches and Spotlight indexing, and appears to be effective on the current Data volume. &lt;code&gt;mdutil&lt;/code&gt; will then inform you that indexing and searching are disabled on each mounted volume, and Spotlight has been switched to kMDConfigSearchLevelOff. That ensures all attempts to search will fail to return any hits.&lt;/p&gt;
    &lt;p&gt;Look carefully, though, and Spotlight hasn’t gone anywhere, and is still present in Activity Monitor’s list of processes. During startup you’ll still see its related daemons &lt;code&gt;mediaanalysisd&lt;/code&gt; and &lt;code&gt;photoanalysisd&lt;/code&gt; run briefly, and &lt;code&gt;mds&lt;/code&gt;, Spotlight and &lt;code&gt;spotlightknowledged&lt;/code&gt; are still present in the list of processes. Volumes will also have their hidden .Spotlight-V100 folder, although after &lt;code&gt;mdutil -a -d&lt;/code&gt; its Store-V2 folder should remain completely empty.&lt;/p&gt;
    &lt;p&gt;Should you wish to enable Spotlighting indexing again, regardless of which command was used to disable it, use&lt;code&gt;sudo mdutil -a -i on&lt;/code&gt;&lt;lb/&gt; which should report that indexing has been enabled on each mounted volume.&lt;/p&gt;
    &lt;head rend="h4"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;It’s not possible in macOS Tahoe to completely disable either Siri or Spotlight, not without resorting to system surgery and running with SIP disabled. However, you can reduce them to an absolute minimum by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;turning Siri Requests off in Siri settings;&lt;/item&gt;
      &lt;item&gt;running the command &lt;code&gt;sudo mdutil -a -d&lt;/code&gt;in Terminal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But using &lt;code&gt;sudo mdutil -a -i off&lt;/code&gt; isn’t as thorough or reliable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eclecticlight.co/2026/01/16/can-you-disable-spotlight-and-siri-in-macos-tahoe/"/><published>2026-01-16T14:56:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46647059</id><title>Read_once(), Write_once(), but Not for Rust</title><updated>2026-01-16T19:33:44.303978+00:00</updated><content>&lt;doc fingerprint="b6b8bc15e896fbd8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;READ_ONCE(), WRITE_ONCE(), but not for Rust&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The READ_ONCE() and WRITE_ONCE() macros are heavily used within the kernel; there are nearly 8,000 call sites for READ_ONCE(). They are key to the implementation of many lockless algorithms and can be necessary for some types of device-memory access. So one might think that, as the amount of Rust code in the kernel increases, there would be a place for Rust versions of these macros as well. The truth of the matter, though, is that the Rust community seems to want to take a different approach to concurrent data access.&lt;/p&gt;
    &lt;p&gt;An understanding of READ_ONCE() and WRITE_ONCE() is important for kernel developers who will be dealing with any sort of concurrent access to data. So, naturally, they are almost entirely absent from the kernel's documentation. A description of sorts can be found at the top of include/asm-generic/rwonce.h:&lt;/p&gt;
    &lt;quote&gt;Prevent the compiler from merging or refetching reads or writes. The compiler is also forbidden from reordering successive instances of READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some particular ordering. One way to make the compiler aware of ordering is to put the two invocations of READ_ONCE or WRITE_ONCE in different C statements.&lt;/quote&gt;
    &lt;p&gt;In other words, a READ_ONCE() call will force the compiler to read from the indicated location exactly one time, with no optimization tricks that would cause the read to be either elided or repeated; WRITE_ONCE() will force a write under those terms. They will also ensure that the access is atomic; if one task reads a location with READ_ONCE() while another is writing that location, the read will return the value as it existed either before or after the write, but not some random combination of the two. These macros, other than as described above, impose no ordering constraints on the compiler or the CPU, making them different from macros like smp_load_acquire(), which have stronger ordering requirements.&lt;/p&gt;
    &lt;p&gt;The READ_ONCE() and WRITE_ONCE() macros were added for the 3.18 release in 2014. WRITE_ONCE() was initially called ASSIGN_ONCE(), but that name was changed during the 3.19 development cycle.&lt;/p&gt;
    &lt;p&gt;On the last day of 2025, Alice Ryhl posted a patch series adding implementations of READ_ONCE() and WRITE_ONCE() for Rust. There are places in the code, she said, where volatile reads could be replaced with these calls, once they were available; among other changes, the series changed access to the struct file f_flags field to use READ_ONCE(). The implementation of these macros involves a bunch of Rust macro magic, but in the end they come down to calls to the Rust read_volatile() and write_volatile() functions.&lt;/p&gt;
    &lt;p&gt; Some of the other kernel Rust developers objected to this change, though. Gary Guo said that he would rather not expose READ_ONCE() and WRITE_ONCE() and suggested using relaxed operations from &lt;del&gt;the Rust Atomic crate&lt;/del&gt; the kernel's Atomic module instead. Boqun Feng expanded on the objection: &lt;/p&gt;
    &lt;quote&gt;The problem of READ_ONCE() and WRITE_ONCE() is that the semantics is complicated. Sometimes they are used for atomicity, sometimes they are used for preventing data race. So yes, we are using LKMM [the Linux kernel memory model] in Rust as well, but whenever possible, we need to clarify the intention of the API, using Atomic::from_ptr().load(Relaxed) helps on that front.&lt;p&gt;IMO, READ_ONCE()/WRITE_ONCE() is like a "band aid" solution to a few problems, having it would prevent us from developing a more clear view for concurrent programming.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;In other words, using the Atomic crate allows developers to specify more precisely which guarantees an operation needs, making the expectations (and requirements) of the code more clear. This point of view would appear to have won out, and Ryhl has stopped pushing for this addition to the kernel's Rust code — for now, at least.&lt;/p&gt;
    &lt;p&gt;There are a couple of interesting implications from this outcome, should it hold. The first of those is that, as Rust code reaches more deeply into the core kernel, its code for concurrent access to shared data will look significantly different from the equivalent C code, even though the code on both sides may be working with the same data. Understanding lockless data access is challenging enough when dealing with one API; developers may now have to understand two APIs, which will not make the task easier.&lt;/p&gt;
    &lt;p&gt;Meanwhile, this discussion is drawing some attention to code on the C side as well. As Feng pointed out, there is still C code in the kernel that assumes a plain write will be atomic in many situations, even though the C standard explicitly says otherwise. Peter Zijlstra answered that all such code should be updated to use WRITE_ONCE() properly. Simply finding that code may be a challenge (though KCSAN can help); updating it all may take a while. The conversation also identified a place in the (C) high-resolution-timer code that is missing a needed READ_ONCE() call. This is another example of the Rust work leading to improvements in the C code.&lt;/p&gt;
    &lt;p&gt; In past discussions on the design of Rust abstractions, there has been resistance to the creation of Rust interfaces that look substantially different from their C counterparts; see this 2024 article, for example. If the Rust developers come up with a better design for an interface, the thinking went, the C side should be improved to match this new design. If one accepts the idea that the Rust approach to READ_ONCE() and WRITE_ONCE() is better than the original, then one might conclude that a similar process should be followed here. Changing thousands of low-level concurrency primitives to specify more precise semantics would not be a task for the faint of heart, though. This may end up being a case where code in the two languages just does things differently.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Development tools/Rust&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Lockless algorithms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Jan 9, 2026 17:19 UTC (Fri) by mb (subscriber, #50428) [Link] (9 responses) I experimented with read/write volatile on AVR, where it would be greatly beneficial to use them for certain inter-thread (interrupt) communication. However, I came to the conclusion that using read/write volatile for inter-thread (interrupt) communication is unsound in Rust. AVR hardware is not capable of doing a non-atomic read/write for byte sized objects. So the hardware is fine for all of the relevant cases. https://doc.rust-lang.org/std/ptr/fn.read_volatile.html &amp;gt; an operation is volatile has no bearing whatsoever on questions involving concurrent accesses from multiple threads &amp;gt; This access is still not considered atomic, and as such it cannot be used for inter-thread synchronization. My implementation worked fine and the generated assembly code was perfect. (Yes, I know there is AtomicXX and no it's not efficient for reasons... And yes, I should fix the compiler's atomic intrinsics instead of working around the problem... I know :) Posted Jan 9, 2026 19:20 UTC (Fri) by josh (subscriber, #17465) [Link] (8 responses) Relaxed atomics are effectively compiler-barrier atomics, and *shouldn't* have any runtime overhead. Are you encountering cases where there's more inefficiency than that? Posted Jan 9, 2026 19:54 UTC (Fri) by mb (subscriber, #50428) [Link] (1 responses) But that was not my point. My point was that I think volatile accesses are not sound in Rust for inter thread communication. Posted Jan 9, 2026 22:10 UTC (Fri) by josh (subscriber, #17465) [Link] Ah, got it, thank you. Hopefully that can be fixed. &amp;gt; My point was that I think volatile accesses are not sound in Rust for inter thread communication. Right, I believe that's correct. Posted Jan 10, 2026 9:33 UTC (Sat) by plugwash (subscriber, #29694) [Link] (5 responses) The situation is a little more subtle than that. Relaxed atomics on a given memory location, must behave as-if they had a well-defined order (though this order may differ from operations on other memory locations, unless fences are used), and this must apply to the whole set of atomic operations. You may only be using load and store on a particular location, but the compiler doesn't know that. Other code might be performing other atomic operations on that location. My understanding is that this effectively means that if you implement read-modify-write operations by using a global lock, you must also implement plain write operations using that same global lock. Posted Jan 10, 2026 20:13 UTC (Sat) by comex (subscriber, #71521) [Link] (1 responses) Posted Jan 11, 2026 17:49 UTC (Sun) by garyguo (subscriber, #173367) [Link] `READ_ONCE` on u64 might still be useful is you just want to read the value in a data-race-free way and you don't care about atomicity (i.e. allow the read to tear). However this is yet another reason I don't want people to just use `READ_ONCE()` for atomic ops on Rust side -- it's just not intuitive which semantics is desired. Posted Jan 10, 2026 20:44 UTC (Sat) by NYKevin (subscriber, #129325) [Link] (2 responses) That is true but misleading. Your parenthetical negates all of the guarantees that are actually expensive to implement, at least on x86. Noting for the record: A fence must be on the same thread as the relaxed atomic in order to restrict it, and there are several other requirements as well. I refer the curious reader to https://en.cppreference.com/w/cpp/atomic/atomic_thread_fe... and related documentation for more information. &amp;gt; My understanding is that this effectively means that if you implement read-modify-write operations by using a global lock, you must also implement plain write operations using that same global lock. If you take a global lock, then there are two different memory locations in play (lock and payload), so your parenthetical above already tells us that the lock is ineffective (at protecting against against un-fenced relaxed atomics on either the lock or the payload). Or perhaps I have misunderstood what you mean by "implement read-modify-write operations by using a global lock." I would normally understand a "read-modify-write operation" to be a hardware instruction (or sequence of instructions), which is not our problem to "implement" in the first place. If you mean "emulate," then the problem we run into is that emulators do not emulate the C abstract machine. They emulate some real hardware like x86, or virtual hardware like the JVM. Those platforms have their own, more specific memory models than the C abstract machine, and the compiler backend must necessarily take advantage of those memory models to emit correct assembly/machine code. So our emulator is not permitted to stop at just taking locks for relaxed atomics - it doesn't necessarily know which stores or loads originated as relaxed atomics in the first place, and therefore may have to take locks for all loads and stores whatsoever. Of course, it would be preferable to implement these operations lock-free if it is possible to do so. Posted Jan 10, 2026 21:12 UTC (Sat) by willmo (subscriber, #82093) [Link] (1 responses) &amp;gt; If you take a global lock, then there are two different memory locations in play (lock and payload), so your parenthetical above already tells us that the lock is ineffective (at protecting against against un-fenced relaxed atomics on either the lock or the payload). In this case, the compiler would need to compile un-fenced relaxed atomics so that they take the global lock. That’s what plugwash meant. Posted Jan 12, 2026 16:36 UTC (Mon) by NYKevin (subscriber, #129325) [Link] Posted Jan 9, 2026 17:42 UTC (Fri) by bertschingert (subscriber, #160729) [Link] (11 responses) I wasn't around when they were implemented, so I'm speculating here, but I get the sense that READ/WRITE_ONCE() were implemented as a volatile cast not because volatility gives the optimal or desired semantics in most situations, but because that was the best tool available prior to the C11 atomics model. While it may be prohibitively difficult, it does seem like changing the C side to use relaxed atomics (when correct) would be the right thing to do. But I don't really know how many uses actually require the additional "volatility" guarantee provided by READ/WRITE_ONCE(). Posted Jan 10, 2026 1:39 UTC (Sat) by wahern (subscriber, #37304) [Link] (8 responses) Also, C11 atomics is not the origin point for atomic intrinsics[1] or a meaningful memory model in either GCC or Linux. It's not the final or even 100% comprehensive model, either. I think the push for a more formal memory model in C, C++, and the compilers gives a false impression such a thing was completely non-existent beforehand and that things are satisfactory today. [1] GCC had at least two sets of intrinsics before supporting C11 atomics, and of course projects like the kernel had their own set that work just as well today as they did before the latest set of builtins. Posted Jan 10, 2026 8:54 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (2 responses) The "atomicity" guarantees that READ_ONCE() and WRITE_ONCE() provide only come in at the compiler level. The compiler will coalesce loads and stores or emit multiple loads as a substitute for spilling registers without some notion of atomicity at the language level. The "unnecessarily costly" part of READ_ONCE() and WRITE_ONCE() is that they don't distinguish between atomicity and ordering - they also specify strict ordering, but only to the compiler, not the hardware (they don't emit memory barriers). Rust's atomic load/store really are just better, because they separate out ordering from atomicity and make ordering explicit. And instead of sprinkling around separate memory barrier calls, which may or may not be commented, they're attached to the operation that needs them - which is good for readability. Posted Jan 10, 2026 17:45 UTC (Sat) by excors (subscriber, #95769) [Link] I'm not certain what you mean by that. E.g. the ARM ARM defines "single-copy atomicity" which is important even in single-processor code: if an interrupt occurs during a STP (Store Pair) instruction, whose operation is defined as a single assignment to memory, the interrupt handler may observe the first half of memory was updated and the second half wasn't, because STP is treated as two separate atomic writes. (The STP instruction will be restarted after the interrupt returns, so it'll complete eventually). So I think the ISA does define the notion of atomic loads and stores, even before getting to the more complex operations. (GCC will happily use STP for an int64_t assignment, making it non-atomic, unless you add 'volatile' and then it'll use a single 64-bit STR (which is single-copy atomic).) Posted Jan 10, 2026 20:18 UTC (Sat) by comex (subscriber, #71521) [Link] Posted Jan 10, 2026 16:55 UTC (Sat) by joib (subscriber, #8541) [Link] (4 responses) I wonder, if the C++/C11 memory models and atomics were to be developed today, how different would they look, considering the amount of knowledge the world has gained since then and now? Certainly there were parts of the C/C++11 models that were, ahem, less than successful, like the consume memory ordering, but otherwise, would there be a place for doing it substantially better and different in general? Posted Jan 11, 2026 20:33 UTC (Sun) by pbonzini (subscriber, #60935) [Link] (1 responses) I don't have high hopes that this would be accepted now, but maybe it would be since "almost nobody will need anything but sequential consistent variables" has been shown wrong. The other thing that still hasn't been fully formalized is out-of-thin-air values. Everybody agrees that they won't happen but strictly speaking they aren't prohibited, or weren't last time I checked. Posted Jan 12, 2026 6:06 UTC (Mon) by riking (subscriber, #95706) [Link] Posted Jan 15, 2026 0:18 UTC (Thu) by milesrout (subscriber, #126894) [Link] (1 responses) Posted Jan 15, 2026 15:40 UTC (Thu) by bertschingert (subscriber, #160729) [Link] OTOH, what I like about the Rust (and C/C++11?) atomics is that the type system prevents accidentally introducing data races because you can't do a non-atomic load/store to an atomic type -- at least without unsafe code. Given that the article mentions there are cases in C where READ_ONCE() and WRITE_ONCE() should have been used, but weren't, this seems to be a real risk. Posted Jan 10, 2026 15:42 UTC (Sat) by bjackman (subscriber, #109548) [Link] (1 responses) IIUC C11's relaxed ordering is too weak for that, but any of the other C11 orderings are likely to be unnecessarily strict, i.e. they might force the use of special (costly) CPU instructions where normal reads and writes are already safe enough. Posted Jan 11, 2026 17:57 UTC (Sun) by garyguo (subscriber, #173367) [Link] &lt;head&gt;read/write volatile&lt;/head&gt;&lt;lb/&gt; Just in the same way every AVR C program uses volatile "atomic" variables to do this.&lt;lb/&gt; https://github.com/mbuesch/avr-atomic&lt;lb/&gt; But the read/write volatile documentation was pretty clear to me that the Rust virtual machine considers concurrent volatile accesses unsound and is free to optimize it to bits.&lt;lb/&gt; &amp;gt; Volatile accesses behave exactly like non-atomic accesses in that regard.&lt;lb/&gt; However, I changed it to a less optimal inline asm implementation just because I think the Rust documentation considers concurrent read/write volatile without additional synchronization to be unsound.&lt;head&gt;read/write volatile&lt;/head&gt;&lt;head&gt;read/write volatile&lt;/head&gt;&lt;lb/&gt; As I said, it's an LLVM problem on AVR. Which is not a stable tier.&lt;lb/&gt; Atomic always use heavy syncing on AVR in the current compiler.&lt;head&gt;read/write volatile&lt;/head&gt;&lt;lb/&gt; &amp;gt; Atomic always use heavy syncing on AVR in the current compiler.&lt;head&gt;read/write volatile&lt;/head&gt;&lt;head&gt;read/write volatile&lt;/head&gt;&lt;head&gt;read/write volatile&lt;/head&gt;&lt;head&gt;read/write volatile&lt;/head&gt;&lt;head&gt;read/write volatile&lt;/head&gt;&lt;head&gt;read/write volatile&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;head&gt;overly strict semantics&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1053142/8ec93e58d5d3cc06/"/><published>2026-01-16T15:04:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46647491</id><title>6-Day and IP Address Certificates Are Generally Available</title><updated>2026-01-16T19:33:44.245196+00:00</updated><content>&lt;doc fingerprint="e659e0db14317484"&gt;
  &lt;main&gt;
    &lt;p&gt;Short-lived and IP address certificates are now generally available from Let’s Encrypt. These certificates are valid for 160 hours, just over six days. In order to get a short-lived certificate subscribers simply need to select the ‘shortlived’ certificate profile in their ACME client.&lt;/p&gt;
    &lt;p&gt;Short-lived certificates improve security by requiring more frequent validation and reducing reliance on unreliable revocation mechanisms. If a certificate’s private key is exposed or compromised, revocation has historically been the way to mitigate damage prior to the certificate’s expiration. Unfortunately, revocation is an unreliable system so many relying parties continue to be vulnerable until the certificate expires, a period as long as 90 days. With short-lived certificates that vulnerability window is greatly reduced.&lt;/p&gt;
    &lt;p&gt;Short-lived certificates are opt-in and we have no plan to make them the default at this time. Subscribers that have fully automated their renewal process should be able to switch to short-lived certificates easily if they wish, but we understand that not everyone is in that position and generally comfortable with this significantly shorter lifetime. We hope that over time everyone moves to automated solutions and we can demonstrate that short-lived certificates work well.&lt;/p&gt;
    &lt;p&gt;Our default certificate lifetimes will be going from 90 days down to 45 days over the next few years, as previously announced.&lt;/p&gt;
    &lt;p&gt;IP address certificates allow server operators to authenticate TLS connections to IP addresses rather than domain names. Let’s Encrypt supports both IPv4 and IPv6. IP address certificates must be short-lived certificates, a decision we made because IP addresses are more transient than domain names, so validating more frequently is important. You can learn more about our IP address certificates and the use cases for them from our post announcing our first IP Certificate.&lt;/p&gt;
    &lt;p&gt;We’d like to thank the Open Technology Fund and Sovereign Tech Agency, along with our Sponsors and Donors, for supporting the development of this work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability"/><published>2026-01-16T15:37:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46648144</id><title>Launch HN: Indy (YC S21) – A support app designed for ADHD brains</title><updated>2026-01-16T19:33:43.794549+00:00</updated><content>&lt;doc fingerprint="5ffadfaf74ff182e"&gt;
  &lt;main&gt;
    &lt;p&gt;Get the app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.shimmer.care/indy-redirect"/><published>2026-01-16T16:20:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46648714</id><title>Zep AI (Agent Context Engineering, YC W24) Is Hiring Forward Deployed Engineers</title><updated>2026-01-16T19:33:43.364133+00:00</updated><content>&lt;doc fingerprint="af57259bffd2ec7b"&gt;
  &lt;main&gt;
    &lt;p&gt;Agent Context Is Hard. We Fixed It.&lt;/p&gt;
    &lt;p&gt;Zep assembles the right context from chat history, business data, and user behavior so agents are personalized, accurate, and fast. Our open source project Graphiti hit 20k GitHub stars in under 12 months. Sub-200ms retrieval, SOC 2 Type 2/HIPAA certified, used by teams from startups to Fortune 500s.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/zep-ai/jobs/"/><published>2026-01-16T17:00:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46648778</id><title>Canada slashes 100% tariffs on Chinese EVs to 6%</title><updated>2026-01-16T19:33:43.297418+00:00</updated><content>&lt;doc fingerprint="7efc84b48ff009bc"&gt;
  &lt;main&gt;
    &lt;p&gt;In a massive shift in North American trade policy, Prime Minister Mark Carney announced today a new “strategic partnership” with China that effectively reopens the Canadian border to Chinese electric vehicles.&lt;/p&gt;
    &lt;p&gt;The move marks a significant departure from the United States’ hardline protectionist stance and could bring affordable EV options like the BYD Seagull to Canadian roads as early as this year.&lt;/p&gt;
    &lt;p&gt;For the last two years, Canada has largely walked in lockstep with the US regarding Chinese EV tariffs. Following the Biden administration’s move to impose 100% tariffs on Chinese EVs, Canada implemented similar surtaxes, effectively freezing companies like BYD, Nio, and Zeekr out of the market.&lt;/p&gt;
    &lt;p&gt;Today, that ice is breaking.&lt;/p&gt;
    &lt;p&gt;As part of a broader trade agreement secured by Prime Minister Carney in Beijing this week, Canada has agreed to allow an annual quota of 49,000 Chinese electric vehicles into the country at the tariff rate of just 6.1%.&lt;/p&gt;
    &lt;p&gt;According to the Prime Minister’s office, this volume represents less than 3% of the Canadian new vehicle market. However, the deal explicitly targets the low end of the market, with the government anticipating that within five years, “more than 50% of these vehicles will be affordable EVs with an import price of less than $35,000.”&lt;/p&gt;
    &lt;p&gt;In exchange for opening the EV floodgates (or at least starting to break the dam), China has agreed to lower tariffs on Canadian canola seed from roughly 85% to 15% and to lift restrictions on Canadian lobster and crab.&lt;/p&gt;
    &lt;p&gt;The Canadian government claims this isn’t just about imports. The text of the agreement states that the deal is expected to “drive considerable new Chinese joint-venture investment in Canada” to build out the domestic EV supply chain.&lt;/p&gt;
    &lt;head rend="h2"&gt;Electrek’s Take&lt;/head&gt;
    &lt;p&gt;While 49,000 vehicles might sound like a small number compared to the total market, it’s a specific, targeted wedge that changes the entire dynamic of the North American EV market.&lt;/p&gt;
    &lt;p&gt;For years, we at Electrek have argued that protectionism, while perhaps protecting legacy automaker jobs in the short term, ultimately hurts consumers and slows down the transition to sustainable transport.&lt;/p&gt;
    &lt;head rend="h2"&gt;Top comment by RBP&lt;/head&gt;
    &lt;p&gt;Thanks Trump. I think the important fine print here is the language of the joint venture requirements. As someone who does believe free trade and capitalism works, it’s important to keep a level playing field. China does have a history of subsidizing the dumping of goods to put competitors out of business. No bueno. We should be able to buy these cars, but rules should be in place to ensure they aren’t being dumped.&lt;/p&gt;
    &lt;p&gt;Meanwhile, protecting domestic automakers from Chinese competition in their home market makes them less competitive on the global stage, virtually giving the global market to China.&lt;/p&gt;
    &lt;p&gt;The reality is that Chinese automakers are currently building some of the best, most affordable EVs in the world. Keeping them out entirely not only hurts consumers but also hurts innovation.&lt;/p&gt;
    &lt;p&gt;Of course, this is going to make Washington furious. The US has been trying to build a “Fortress North America” against Chinese EVs. By letting 49,000 units in tariff-free (or near tariff-free), Canada is effectively saying it values affordable climate solutions (and canola exports) more than complete alignment with US industrial policy, which is understandable since the US was the one to go hostile on trade with Canada.&lt;/p&gt;
    &lt;p&gt;The interesting detail here is the “Joint Venture” language. It looks like Carney is taking a page out of China’s own playbook. Canada seems to be using this quota as a carrot to get companies like BYD or CATL to set up shop in Canada and maybe help Canadian companies learn from those giants.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electrek.co/2026/01/16/canada-breaks-with-us-slashes-100-tariffs-chinese-evs/"/><published>2026-01-16T17:05:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46648885</id><title>Dell UltraSharp 52 Thunderbolt Hub Monitor</title><updated>2026-01-16T19:33:43.004644+00:00</updated><content>&lt;doc fingerprint="e22dab5494a79c42"&gt;
  &lt;main&gt;&lt;p&gt;Selecting will change the following options:&lt;/p&gt;&lt;p&gt;From To&lt;/p&gt;&lt;p&gt;51.5"&lt;/p&gt;&lt;p&gt;6144 x 2560 at 120Hz&lt;/p&gt;&lt;p&gt;In-plane Switching (IPS) Black Technology&lt;/p&gt;&lt;p&gt;99% DCI-P3 (CIE 1976)&lt;/p&gt;&lt;p&gt;100% sRGB (CIE 1931)&lt;/p&gt;...See More See More Color Gamut&lt;p&gt;2 HDMI port/s (HDCP 2.2) (Supports up to 6144 x 2560, 120 Hz, VRR, , as specified in HDMI 2.1 (FRL))&lt;/p&gt;&lt;p&gt;2 DisplayPort 1.4 (HDCP 2.2) port/s&lt;/p&gt;...See More See More Ports&lt;p&gt;Add the products you would like to compare, and quickly determine which is best for your needs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories"/><published>2026-01-16T17:14:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46649142</id><title>STFU</title><updated>2026-01-16T19:33:42.645942+00:00</updated><content>&lt;doc fingerprint="a80e03ee7b25e4d9"&gt;
  &lt;main&gt;
    &lt;p&gt;i was at bombay airport. some dude was watching reels on full volume and laughing loudly. asking nicely doesn't work anymore. me being me, didn't have the courage to speak up.&lt;/p&gt;
    &lt;p&gt;so i built a tiny app that plays back the same audio it hears, delayed by ~2 seconds. asked claude, it spat out a working version in one prompt. surprisingly WORKS.&lt;/p&gt;
    &lt;p&gt;discussion - https://x.com/the2ndfloorguy/status/2011734249871954188&lt;/p&gt;
    &lt;p&gt;something something auditory feedback loop something something cognitive dissonance. idk i'm not a neuroscientist. all i know is it makes people shut up and that's good enough for me.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;straight up honest - originally called this "make-it-stop" but then saw @TimDarcet also built similar and named it STFU. wayyyyy better name. so stole it. sorry not sorry.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;made with spite and web audio api. do whatever you want with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Pankajtanwarbanna/stfu"/><published>2026-01-16T17:32:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46649489</id><title>Emoji Use in the Electronic Health Record is Increasing</title><updated>2026-01-16T19:33:42.580053+00:00</updated><content/><link href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2843883"/><published>2026-01-16T17:56:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46649577</id><title>Our approach to advertising and expanding access to ChatGPT</title><updated>2026-01-16T19:33:42.425349+00:00</updated><content>&lt;doc fingerprint="b8a6127125c76bbf"&gt;
  &lt;main&gt;
    &lt;p&gt;AI is reaching a point where everyone can have a personal super-assistant that helps them learn and do almost anything. Who gets access to that level of intelligence will shape whether AI expands opportunity or reinforces the same divides.&lt;/p&gt;
    &lt;p&gt;We’ve been working to make powerful AI accessible to everyone through our free product and low-cost subscription tier, ChatGPT Go, which has launched in 171 countries since August. Today we’re bringing Go to the U.S. and everywhere ChatGPT is available, giving people expanded access to messaging, image creation, file uploads and memory for $8 USD/month. In the coming weeks, we’re also planning to start testing ads in the U.S. for the free and Go tiers, so more people can benefit from our tools with fewer usage limits or without having to pay. Plus, Pro, Business, and Enterprise subscriptions will not include ads.&lt;/p&gt;
    &lt;p&gt;People trust ChatGPT for many important and personal tasks, so as we introduce ads, it’s crucial we preserve what makes ChatGPT valuable in the first place. That means you need to trust that ChatGPT’s responses are driven by what’s objectively useful, never by advertising. You need to know that your data and conversations are protected and never sold to advertisers. And we need to keep a high bar and give you control over your experience so you see truly relevant, high-quality ads—and can turn off personalization if you want.&lt;/p&gt;
    &lt;p&gt;Given that, we want to be clear about the principles that guide our approach to advertising:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mission alignment: Our mission is to ensure AGI benefits all of humanity; our pursuit of advertising is always in support of that mission and making AI more accessible.&lt;/item&gt;
      &lt;item&gt;Answer independence: Ads do not influence the answers ChatGPT gives you. Answers are optimized based on what's most helpful to you. Ads are always separate and clearly labeled.&lt;/item&gt;
      &lt;item&gt;Conversation privacy: We keep your conversations with ChatGPT private from advertisers, and we never sell your data to advertisers.&lt;/item&gt;
      &lt;item&gt;Choice and control: You control how your data is used. You can turn off personalization, and you can clear the data used for ads at any time. We’ll always offer a way to not see ads in ChatGPT, including a paid tier that’s ad-free.&lt;/item&gt;
      &lt;item&gt;Long-term value: We do not optimize for time spent in ChatGPT. We prioritize user trust and user experience over revenue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’re not launching ads yet, but we do plan to start testing in the coming weeks for logged in adults in the U.S. on the free and Go tiers. To start, we plan to test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation. Ads will be clearly labeled and separated from the organic answer. You’ll be able to learn more about why you’re seeing that ad, or dismiss any ad and tell us why. During our test, we will not show ads in accounts where the user tells us or we predict that they are under 18, and ads are not eligible to appear near sensitive or regulated topics like health, mental health or politics.&lt;/p&gt;
    &lt;p&gt;Here’s an example of what the first ad formats we plan to test could look like:&lt;/p&gt;
    &lt;p&gt;The best ads are useful, entertaining, and help people discover new products and services. Given what AI can do, we're excited to develop new experiences over time that people find more helpful and relevant than any other ads. Conversational interfaces create possibilities for people to go beyond static messages and links. For example, soon you might see an ad and be able to directly ask the questions you need to make a purchase decision.&lt;/p&gt;
    &lt;p&gt;Ads also can be transformative for small businesses and emerging brands trying to compete. AI tools level the playing field even further, allowing anyone to create high-quality experiences that help people discover options they might never have found otherwise.&lt;/p&gt;
    &lt;p&gt;We’ll learn from feedback and refine how ads show up over time, but our commitment to putting users first and maintaining trust won’t change. By starting our ad platform from the ground up with these principles in place, we can align our incentives with what people want from ChatGPT. Our long-term focus remains on building products that millions of people and businesses find valuable enough to pay for. Our enterprise and subscription businesses are already strong, and we believe in having a diverse revenue model where ads can play a part in making intelligence more accessible to everyone.&lt;/p&gt;
    &lt;p&gt;Once we begin testing our first ad formats in the coming weeks and months, we look forward to getting people's feedback and ensuring that ads can support broad access to AI and keep the trust that makes ChatGPT valuable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/our-approach-to-advertising-and-expanding-access/"/><published>2026-01-16T18:02:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46649813</id><title>Earth from Space: The Fate of a Giant</title><updated>2026-01-16T19:33:41.876919+00:00</updated><content>&lt;doc fingerprint="faa11747d606e68f"&gt;
  &lt;main&gt;
    &lt;p&gt;This Copernicus Sentinel-2 image over the South Atlantic Ocean features a close-up view of the A23a iceberg, once the world’s largest. The unusually cloud-free image shows the first signs that the iceberg will soon disintegrate completely.&lt;/p&gt;
    &lt;p&gt;Zoom in to explore this image at its full 10 m resolution.&lt;/p&gt;
    &lt;p&gt;A23a calved from the Filchner-Ronne ice shelf in West Antarctica in 1986. At the time it measured around 4000 sq km – more than three times the size of Rome – making it the largest iceberg in the world. After being grounded on the ocean floor for decades, in 2020 it lost its grip and began floating in the Weddell Sea until, in November 2023, it started drifting quickly away from Antarctic waters.&lt;/p&gt;
    &lt;p&gt;Driven by winds and currents, the iceberg travelled about 2000 km further north towards the warmer South Atlantic Ocean waters, reaching South Georgia Island in May 2025, where it started to disintegrate.&lt;/p&gt;
    &lt;p&gt;Throughout 2025, A23a has been breaking apart into smaller blocks of ice reducing its size significantly. In this image from 20 December 2025, the iceberg is approximately 150 km northwest of South Georgia, surrounded by numerous icebergs from different sizes. Although it has lost about three-quarters of its surface area, A23a is still one of the largest icebergs floating in open waters, covering roughly 1000 sq km.&lt;/p&gt;
    &lt;p&gt;The bright blue areas visible on its surface and on the icebergs to its south are ponds of meltwater, which are clear signs of the iceberg’s rapid demise.&lt;/p&gt;
    &lt;p&gt;The disintegration is typical of icebergs that reach this far north and is caused by the warmer sea temperatures and weather conditions. As A23a is travelling towards even warmer waters pushed by the currents, it will soon experience a similar fate to other megabergs that have disintegrated in the same waters.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.esa.int/ESA_Multimedia/Images/2026/01/Earth_from_Space_The_fate_of_a_giant"/><published>2026-01-16T18:18:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46650807</id><title>The wealth of the top% reaches a record $52T</title><updated>2026-01-16T19:33:41.767712+00:00</updated><content>&lt;doc fingerprint="79e415b8eb3147ed"&gt;
  &lt;main&gt;
    &lt;p&gt;A version of this article first appeared in CNBC's Inside Wealth newsletter with Robert Frank, a weekly guide to the high-net-worth investor and consumer. Sign up to receive future editions, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;The top 10% of Americans added $5 trillion to their wealth in the second quarter as the stock market rally continued to benefit the biggest investors, according to new data from the Federal Reserve.&lt;/p&gt;
    &lt;p&gt;The total wealth of the top 10% — or those with a net worth of more than $2 million — reached a record $113 trillion in the second quarter, up from $108 trillion in the first quarter, according to the Fed. The increase follows three years of continued growth for those at the top, with the top 10% adding over $40 trillion to their wealth since 2020.&lt;/p&gt;
    &lt;p&gt;All wealth groups saw gains over the past year, with the net worth of the bottom half of Americans increasing 6% over the past 12 months, according to the Fed data. Yet the growth has been fastest for those at the very top. The top 1% have seen their wealth increase by $4 trillion over the past year, an increase of 7%. Their wealth hit a record $52 trillion in the second quarter.&lt;/p&gt;
    &lt;p&gt;The top 0.1% saw their wealth grow by 10% over the past year. Since the pandemic, the top 0.1%, or those with a net worth of at least $46 million, have seen their total wealth nearly double to over $23 trillion.&lt;/p&gt;
    &lt;p&gt;Despite the recent faster growth at the top, the total shares of wealth held by the upper echelon has remained fairly stable for decades. The top 1% held 29% of total household wealth in the second quarter, compared with 28% in 2000. The top 10% held 67% of total household wealth in the quarter while the bottom 90% held 33%.&lt;/p&gt;
    &lt;p&gt;The biggest driver of wealth gains at the top this year has been the stock market. The value of the corporate equities and mutual fund shares held by the top 10% increased from $39 trillion to over $44 trillion over the past year. The top 10% of Americans hold over 87% of corporate equities and mutual fund shares.&lt;/p&gt;
    &lt;p&gt;The population of the ultra-wealthy is also growing rapidly. The number of ultra-high-net-worth Americans, or those worth $30 million or more, grew 6.5% in the first half of 2025, after surging 21% last year, according to a new report from Altrata. There are now 208,090 ultra-high-net-worth individuals in the U.S., accounting for 41% of the world's total.&lt;/p&gt;
    &lt;p&gt;The surging wealth at the top has created an increasingly bifurcated consumer economy, with the wealthy accounting for a growing share of overall spending. Consumers in the top 10% of the income distribution accounted for 49.2% of consumer spending in the second quarter, marking the highest level since data started being compiled in 1989, according to Mark Zandi at Moody's Analytics.&lt;/p&gt;
    &lt;p&gt;The so-called "K-shaped economy" has performed well so far, at least according to broad economic measures such as GDP and consumption. Yet the growing dependence on a small sliver of consumers at the top carries risks.&lt;/p&gt;
    &lt;p&gt;Zandi said a deep and prolonged decline in the stock market, which is driving almost all of the wealth gains at the top, could send wider ripples through the economy.&lt;/p&gt;
    &lt;p&gt;"The economy is being powered in big part by the spending of the extraordinarily well-to-do, who are cheered by the surging value of their stock portfolios," he said. "If the richly (over) valued stock market were to stumble, for whatever reason, and the well-to-do see more red on their stock tickers than green, they will quickly turn more cautious in their spending, posing a serious threat to the already fragile economy."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2025/10/03/the-wealth-of-the-top-1percent-reaches-a-record-52-trillion.html"/><published>2026-01-16T19:17:46+00:00</published></entry></feed>