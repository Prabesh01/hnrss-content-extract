<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-19T02:18:45.480492+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45289168</id><title>The quality of AI-assisted software depends on unit of work management</title><updated>2025-09-19T02:18:54.575754+00:00</updated><content>&lt;doc fingerprint="2651f55330438f4e"&gt;
  &lt;main&gt;
    &lt;p&gt;The craft of AI-assisted software creation is substantially about correctly managing units of work.&lt;/p&gt;
    &lt;p&gt;When I was new to this emerging craft of AI-assisted coding, I was getting lousy results, despite the models being rather intelligent. Turns out the major bottleneck is not intelligence, but rather providing the correct context.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy, while referencing my earlier article on this topic, described the work of AI-assisted engineering as “putting AI on a tight leash”. What does a tight leash look like for a process where AI agents are operating on your code more independently than ever? He dropped a hint: work on small chunks of a single concrete thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;The right sized unit of work respects the context&lt;/head&gt;
    &lt;p&gt;I like the term context engineering, because it has opened up the vocabulary to better describe why managing units of work is perhaps the most important technique to get better results out of AI tools. It centers our discussion around the “canvas” against which our AI is generating code.&lt;/p&gt;
    &lt;p&gt;I like Anthropic’s visualisation from their docs:&lt;/p&gt;
    &lt;p&gt;The generated output of the LLM is a sample of the next token probability. Every time we generate a token, what has already been generated in the previous iteration is appended to the context window. What this context window looks like has a huge influence on the quality of your generated output.&lt;/p&gt;
    &lt;p&gt;Drew Breunig wrote an excellent article about all kinds of things that can go wrong with your context and proposed various techniques to fix them.&lt;/p&gt;
    &lt;p&gt;The best AI-assisted craftsmen are often thinking about the design and arrangement of their context to get the AI to one-shot a solution. This is tricky and effortful, contrary to what the AI coding hype suggests.&lt;/p&gt;
    &lt;p&gt;If you don’t provide the necessary information in the context to do a good job, your AI will hallucinate or generate code that is not congruent with the practices of your codebase. It is especially brittle at integration points of your software system.&lt;/p&gt;
    &lt;p&gt;On the other hand, if you fill up the context with too much information, and the quality of your output degrades, because of a lack of focused attention.&lt;/p&gt;
    &lt;p&gt;Breaking down your task into “right-sized” units of work, which describe just the right amount of detail is perhaps the most powerful lever to improve your context window, and thus the correctness and quality of the generated code.&lt;/p&gt;
    &lt;head rend="h2"&gt;The right sized unit of work controls the propagation of errors&lt;/head&gt;
    &lt;p&gt;Time for some napkin maths.&lt;/p&gt;
    &lt;p&gt;Let’s say your AI agent has a 5% chance of making a mistake. I’m not just referring to hallucinations—it could be a subtle mistake because it forgot to look up some documentation or you missed a detail in your specification.&lt;/p&gt;
    &lt;p&gt;In an agentic multi-turn workflow, which is what all coding workflows are converging to, this error compounds. If your task takes 10 turns to implement, you will have a (1 – 0.95)10 = 59.9% chance of success. Not very high.&lt;/p&gt;
    &lt;p&gt;Utkarsh Kanwat in his blog post has made the same argument. His conclusion was that any AI agent would need some kind of pause-and-verify gating mechanism at each step for a long-horizon task.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Per-action&lt;p&gt;error rate&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Overall Success Rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;5 turns&lt;/cell&gt;
        &lt;cell&gt;10 turns&lt;/cell&gt;
        &lt;cell&gt;20 turns&lt;/cell&gt;
        &lt;cell&gt;50 turns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;0.1%&lt;/cell&gt;
        &lt;cell&gt;99.5%&lt;/cell&gt;
        &lt;cell&gt;99.0%&lt;/cell&gt;
        &lt;cell&gt;98.0%&lt;/cell&gt;
        &lt;cell&gt;95.1%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1%&lt;/cell&gt;
        &lt;cell&gt;95.1%&lt;/cell&gt;
        &lt;cell&gt;90.4%&lt;/cell&gt;
        &lt;cell&gt;81.8%&lt;/cell&gt;
        &lt;cell&gt;60.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
        &lt;cell&gt;77.4%&lt;/cell&gt;
        &lt;cell&gt;59.9%&lt;/cell&gt;
        &lt;cell&gt;35.8%&lt;/cell&gt;
        &lt;cell&gt;7.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;59.0%&lt;/cell&gt;
        &lt;cell&gt;34.9%&lt;/cell&gt;
        &lt;cell&gt;12.2%&lt;/cell&gt;
        &lt;cell&gt;0.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;32.8%&lt;/cell&gt;
        &lt;cell&gt;10.7%&lt;/cell&gt;
        &lt;cell&gt;1.2%&lt;/cell&gt;
        &lt;cell&gt;0.0%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What does the state of the art for multi-turn error rates look like? METR recently published a popular chart describing how AI models are getting better at long-horizon tasks. Currently GPT-5 is at the top of the leaderboard, where it can perform ~2-hour long tasks at around a 70% success rate. Working backwards (let’s say a 2 hour task is 50+ turns) this would amount to a sub-1% error rate per action.&lt;/p&gt;
    &lt;p&gt;Doesn’t a &amp;lt;1% error rate per action seem suspicious to you? As a regular user of agentic coding tools (my current one is Codex CLI), I’ll eat my shoe if GPT-5 starts nailing my tasks 99.9% of the time.&lt;/p&gt;
    &lt;p&gt;My intuition derived from experience tells me that even the best AI right now isn’t even 95% likely to be correct. So where is the difference coming from? It needs a closer look at the actual paper:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our tasks typically use environments that do not significantly change unless directly acted upon by the agent. In contrast, real tasks often occur in the context of a changing environment.&lt;/p&gt;
      &lt;p&gt;[…]&lt;/p&gt;
      &lt;p&gt;Similarly, very few of our tasks are punishing of single mistakes. This is in part to reduce the expected cost of collecting human baselines.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is not at all like the tasks I am doing.&lt;/p&gt;
    &lt;p&gt;METR acknowledges the messiness of the real world. They have come up with a “messiness rating” for their tasks, and the “mean messiness” of their tasks is 3.2/16.&lt;/p&gt;
    &lt;p&gt;By METR’s definitions, the kind of software engineering work that I’m mostly exposed to would score at least around 7-8, given that software engineering projects are path-dependent, dynamic and without clear counterfactuals. I have worked on problems that get to around 13/16 levels of messiness.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An increase in task messiness by 1 point reduces mean success rates by roughly 8.1%&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Extrapolating from METR’s measured effect of messiness, GPT-5 would go from 70% to around 40% success rate for 2-hour tasks. This maps to my experienced reality.&lt;/p&gt;
    &lt;p&gt;I am not certain that pure intelligence can solve for messiness. Robustness to environmental chaos and the fuzzy nature of reality is fundamentally about managing context well. Until we find the magic sauce that solves this, it is clear that we need a workflow that can break down our problem into units of work, with verifiable checkpoints to manage the compounding of errors.&lt;/p&gt;
    &lt;p&gt;These verifiable checkpoints need to be legible to humans.&lt;/p&gt;
    &lt;head rend="h2"&gt;So, what is the “right sized” unit of work?&lt;/head&gt;
    &lt;p&gt;The right sized unit of work needs to be small and describe the desired outcome concisely.&lt;/p&gt;
    &lt;p&gt;The desired outcome on completion of a unit of work needs to be human-legible. I argue that it needs to provide legible business value. Ultimately, the users of software are going to be humans (or systems that model human constructs). Therefore, an elegant way to break down a project is to model it as small units of work that provide legible business value at each checkpoint. This will serve the purpose of respecting the context window of the LLM and help manage the propagation of errors.&lt;/p&gt;
    &lt;p&gt;Software engineers have already defined a unit of work that provides business value and serve as the placeholder for all the context and negotiation of scope—User Stories. I think they are a good starting point to help us break down a large problem into smaller problems that an LLM can one-shot, while providing a concrete result. They center user outcomes, which unlike “tasks”, are robust to the messy dynamic environment of software development.&lt;/p&gt;
    &lt;p&gt;Deliverable business value is also what all stakeholders can understand and work with. Software is not built in a vacuum by developers—it needs the coordination of teams, product owners, business people and users. The fact that AI agents work in their own context environment separate from the other stakeholders hurts effectiveness and transfer of its benefits. I think this is an important gap that needs to be bridged.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;unit size&lt;/cell&gt;
        &lt;cell role="head"&gt;outcome of completion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TODO item&lt;/cell&gt;
        &lt;cell&gt;small&lt;/cell&gt;
        &lt;cell&gt;incremental technical value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;“Plan Mode”&lt;/cell&gt;
        &lt;cell&gt;large&lt;/cell&gt;
        &lt;cell&gt;technical value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon Kiro Spec&lt;/cell&gt;
        &lt;cell&gt;small&lt;/cell&gt;
        &lt;cell&gt;technical value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;User Story&lt;/cell&gt;
        &lt;cell&gt;small&lt;/cell&gt;
        &lt;cell&gt;business value&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Most AI agents today have well-functioning “planning” modes. These are good at keeping the agent on rails, but they mostly provide technical value, and not necessarily a legible business outcome. I believe planning is complementary to our idea of breaking down a project into small units of business value. My proposed unit of work can be planned with existing planning tools. And I believe this is superior to planning over a large unit of work due to the context rot issues described earlier.&lt;/p&gt;
    &lt;p&gt;Of course, plain old User Stories as described in the Agile canon is not sufficient. It needs to be accompanied by “something more” that can nudge the agents to gather the right context that serves the business value outcome of the stories. What that “something more” could look like is something we hope to answer in the coming months.&lt;/p&gt;
    &lt;head rend="h2"&gt;The StoryMachine experiment&lt;/head&gt;
    &lt;p&gt;To test whether user stories with “something more” can indeed serve as optimal units of work that that have the properties I described above, we are running an experiment called StoryMachine. Currently StoryMachine does not do much—it reads your PRD and Tech Specs and produces story cards. It is still early days. But we will set up an evaluation system that will help us iterate to a unit of work description that helps us build useful software effortlessly. I hope to share updates on what we find in the coming months.&lt;/p&gt;
    &lt;p&gt;I want the craft of AI-assisted development to be less effortful and less like a slot-machine. And our best lever to get there is managing the unit of work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.nilenso.com/blog/2025/09/15/ai-unit-of-work/"/><published>2025-09-18T13:06:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45289453</id><title>Flipper Zero Geiger Counter</title><updated>2025-09-19T02:18:53.756064+00:00</updated><content>&lt;doc fingerprint="1c73dc206bab88f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Flipper Zero Geiger Counter&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;注意：所有模块均在第三方固件中测试，推荐使用:&lt;/p&gt;&lt;lb/&gt;unleashed固件，链接：[https://github.com/DarkFlippers/unleashed-firmware]&lt;lb/&gt;Momentum固件，链接：[https://github.com/Next-Flip/Momentum-Firmware]&lt;/quote&gt;
    &lt;head rend="h2"&gt;Compatible apps&lt;/head&gt;
    &lt;head rend="h2"&gt;Geiger counter&lt;/head&gt;
    &lt;p&gt;This app gives you a graph view with counts per second (instantaneous measure of the radioactivity) as CPS and per minute as CPM.&lt;/p&gt;
    &lt;p&gt;There is extra functionality to record, zoom, change units, etc.&lt;/p&gt;
    &lt;p&gt;(credits to nmrr)&lt;/p&gt;
    &lt;p&gt;CPS: counts per second (instantaneous measure of the radioactivity). CPS is alway displayed on the left corner.&lt;/p&gt;
    &lt;p&gt;CPM: counts per minute (the sum of CPS over a period of one minute). Other units of measurement can be chosen with Left/Right.&lt;/p&gt;
    &lt;p&gt;New CPS bar measure appears on the left every second.&lt;/p&gt;
    &lt;p&gt;A4 GPIO can be connected on A7 GPIO to test this application without using a geiger tube. A4 GPIO generates a signal with a frequency that varies every second.&lt;/p&gt;
    &lt;head rend="h3"&gt;Button assignments:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;button&lt;/cell&gt;
        &lt;cell role="head"&gt;function&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ok [long press]&lt;/cell&gt;
        &lt;cell&gt;Clear the graph&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Left/Right [short press]&lt;/cell&gt;
        &lt;cell&gt;Choose unit on the right corner (cpm, μSv/h, mSv/y, Rad/h, mRad/h, uRad/h), cps on the left is always displayed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up [long press]&lt;/cell&gt;
        &lt;cell&gt;Enable/disable recording, led of Flipper Zero is colored in red when recording&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up/Down [short press]&lt;/cell&gt;
        &lt;cell&gt;Zoom/unzoom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Down [long press]&lt;/cell&gt;
        &lt;cell&gt;Display version of the application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Back [long press]&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Use cases&lt;/head&gt;
    &lt;p&gt;Ambient radioactivity (descendants of radon gas are detected, not radon itself):&lt;/p&gt;
    &lt;p&gt;Measurement of a sample of uranium ore within a lead container:&lt;/p&gt;
    &lt;p&gt;Note: measures in Sv or Rad are not precise&lt;/p&gt;
    &lt;p&gt;Measurement of a sample of uranium ore (the most radioactive part):&lt;/p&gt;
    &lt;p&gt;Measurement of radium dial pointers:&lt;/p&gt;
    &lt;p&gt;All prior measurements in sequence (the scale of the graph is automatically adjusted):&lt;/p&gt;
    &lt;p&gt;Measurement of uranium orange pottery:&lt;/p&gt;
    &lt;p&gt;Measurement of americium-241 button from a smoke detector (descendants of americium or radioisotope impurities are detected, not americium itself):&lt;/p&gt;
    &lt;p&gt;A4 GPIO on A7 GPIO (to test this program without a geiger board):&lt;/p&gt;
    &lt;p&gt;Zoom levels (the third picture is the default zoom):&lt;/p&gt;
    &lt;p&gt;Version of the application (press down button during 1 sec to display version):&lt;/p&gt;
    &lt;head rend="h3"&gt;Recording function&lt;/head&gt;
    &lt;p&gt;Output CSV files are stored in the root directory of the SD card. Date and time are incorporated into the file name (example: geiger-2023-07-03--23-48-15.csv)&lt;/p&gt;
    &lt;p&gt;Data sample:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;epoch&lt;/cell&gt;
        &lt;cell role="head"&gt;cps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: J305 geiger tube is only sensible to beta and gamma rays. Alpha rays cannot be detected.&lt;/p&gt;
    &lt;p&gt;Usable radioactive sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;natural uranium (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;natural thorium (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;radium-226 (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;cobalt-60 (beta &amp;amp; gamma)&lt;/item&gt;
      &lt;item&gt;iodine-131 (beta &amp;amp; gamma)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not really usable radioactive sources (must be in contact with the geiger tube to be detected):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;americium-241 (alpha &amp;amp; low gamma, some strong beta/gamma rays are emitted during radioactive cascade or due to the presence of radioisotope impurities)&lt;/item&gt;
      &lt;item&gt;high purity metallic uranium/thorium (same as am241)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Totaly unusable radioactive sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;polonium-210 (pure alpha)&lt;/item&gt;
      &lt;item&gt;tritium (very low beta)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Atomic dice roller&lt;/head&gt;
    &lt;p&gt;With this app you can get a really random dice based on the geiger counter. Make your decisions and board games extra exciting.&lt;/p&gt;
    &lt;p&gt;(credits to nmrr)&lt;/p&gt;
    &lt;p&gt;This application generates true random numbers by hashing timestamps obtained when a tick is produced by the geiger counter (i.e. when a beta or gamma ray is detected). Timestamps have 32 bit resolution and are produced from a 64 MHz signal.&lt;/p&gt;
    &lt;p&gt;Two hash methods have been implemented:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CRC32: 8 ticks are needed to obtain a hash, for low activity sources&lt;/item&gt;
      &lt;item&gt;MD5: 32 ticks are needed to obtain a hash, for high activity sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dice rolls are produced by transforming a single hash into a number between 1 and 6. Out of scope values are ignored so the dice is really balanced. Modulo-based methods are ugly because they are usually unbalanced.&lt;/p&gt;
    &lt;p&gt;It's possible to roll the dice without using a radioactive isotope. Air contains radon gas that is radioactive. Geiger board can detect descendants of radon gas that emit strong beta or gamma rays.&lt;/p&gt;
    &lt;p&gt;In the left corner, counts per second (cps) indicates the activity. In the right corner, availiable dice rolls are indicated. 64 rolls can be stored.&lt;/p&gt;
    &lt;head rend="h3"&gt;Button assignments:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;button&lt;/cell&gt;
        &lt;cell role="head"&gt;function&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ok [short short]&lt;/cell&gt;
        &lt;cell&gt;Roll the dice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Left [long press]&lt;/cell&gt;
        &lt;cell&gt;Set CRC32 as hash method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Right [long press]&lt;/cell&gt;
        &lt;cell&gt;Set MD5 as hash method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up [long press]&lt;/cell&gt;
        &lt;cell&gt;Set 0-1 as output range (coin flipper)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Down [long press]&lt;/cell&gt;
        &lt;cell&gt;Set 1-6 as output range (dice roller)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Back [long press]&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Warning&lt;/head&gt;
    &lt;p&gt;These apps are for educational purposes only. Please use this code responsibly and only use these apps on your own equipment.&lt;lb/&gt; 本模块和软件只能用作教育和学习用途，一切使用责任请自负，请仅在您拥有的设备上使用&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kasiin.top/blog/2025-08-04-flipper_zero_geiger_counter_module/"/><published>2025-09-18T13:28:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45289558</id><title>Luau – Fast, small, safe, gradually typed scripting language derived from Lua</title><updated>2025-09-19T02:18:53.535197+00:00</updated><content>&lt;doc fingerprint="f76b3cfdc32baad4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;Around 2006, Roblox started using Lua 5.1 as a scripting language for games. Over the years we ended up substantially evolving the implementation and the language; to support growing sophistication of games on the Roblox platform, growing team sizes and large internal teams writing a lot of code for application/editor (1+MLOC as of 2020), we had to invest in performance, ease of use and language tooling, and introduce a gradual type system to the language. More…&lt;/p&gt;
    &lt;head rend="h2"&gt;Sandboxing&lt;/head&gt;
    &lt;p&gt;Luau limits the set of standard libraries exposed to the users and implements extra sandboxing features to be able to run unprivileged code (written by our game developers) side by side with privileged code (written by us). This results in an execution environment that is different from what is commonplace in Lua. More…&lt;/p&gt;
    &lt;head rend="h2"&gt;Compatibility&lt;/head&gt;
    &lt;p&gt;Whenever possible, Luau aims to be backwards-compatible with Lua 5.1 and at the same time to incorporate features from later revisions of Lua. However, Luau is not a full superset of later versions of Lua - we do not always agree with Lua design decisions, and have different use cases and constraints. All post-5.1 Lua features, along with their support status in Luau, are documented here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Syntax&lt;/head&gt;
    &lt;p&gt;Luau is syntactically backwards-compatible with Lua 5.1 (code that is valid Lua 5.1 is also valid Luau); however, we have extended the language with a set of syntactical features that make the language more familiar and ergonomic. The syntax is described here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Analysis&lt;/head&gt;
    &lt;p&gt;To make it easier to write correct code, Luau comes with a set of analysis tools that can surface common mistakes. These consist of a linter and a type checker, colloquially known as script analysis, and are integrated into &lt;code&gt;luau-analyze&lt;/code&gt; command line executable. The linting passes are described here, and the type checking user guide can be found here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;In addition to a completely custom front end that implements parsing, linting and type checking, Luau runtime features new bytecode, interpreter and compiler that are heavily tuned for performance. Luau interpreter can be competitive with LuaJIT interpreter depending on the program. An optional component for manual Just-In-Time compilation is also available for x64 and arm64 platforms, which can considerably speed up certain programs. We continue to optimize the runtime and rewrite portions of it to be even more efficient. While our overall goal is to minimize the amount of time programmers spend tuning performance, some details about the performance characteristics are provided for inquisitive minds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Libraries&lt;/head&gt;
    &lt;p&gt;As a language, Luau is a full superset of Lua 5.1. As far as standard library is concerned, some functions had to be removed from the builtin libraries, and some functions had to be added; refer to full documentation for details. When Luau is embedded into an application, the scripts normally get access to extra library features that are application-specific.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://luau.org/"/><published>2025-09-18T13:38:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45290245</id><title>TernFS – An exabyte scale, multi-region distributed filesystem</title><updated>2025-09-19T02:18:53.248364+00:00</updated><content>&lt;doc fingerprint="65ba0c171fb742d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TernFS — an exabyte scale, multi-region distributed filesystem&lt;/head&gt;
    &lt;p&gt;September 2025&lt;/p&gt;
    &lt;p&gt;XTX is an algorithmic trading firm: it builds statistical models that produce price forecasts for over 50,000 financial instruments worldwide. We use those forecasts to make trades. As XTX's research efforts to build better models ramped up, the demand for resources kept increasing.&lt;/p&gt;
    &lt;p&gt;The firm started out with a couple of desktops and an NFS server, and 10 years later ended up with tens of thousands of high-end GPUs, hundreds of thousands of CPUs, and hundreds of petabytes of storage.&lt;/p&gt;
    &lt;p&gt;As compute grew, storage struggled to keep up. We rapidly outgrew NFS first and existing open-source and commercial filesystems later. After evaluating a variety of third-party solutions, we made the decision to implement our own filesystem, which we called TernFS[1].&lt;/p&gt;
    &lt;p&gt;We have decided to open source our efforts: TernFS is available as free software on our public GitHub. This post motivates TernFS, explains its high-level architecture, and then explores some key implementation details. If you just want to spin up a local TernFS cluster, head to the README.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another filesystem?&lt;/head&gt;
    &lt;p&gt;There's a reason why every major tech company has developed its own distributed filesystem — they're crucial to running large-scale compute efforts, and liable to cause intense disruption if they malfunction. [2]&lt;/p&gt;
    &lt;p&gt;XTX was in the same position, so we designed TernFS to be a one-stop solution for most of our storage needs, going from relatively 'cold' storage of raw market data to short-lived random-access data used to communicate between GPU jobs running on our cluster.&lt;/p&gt;
    &lt;p&gt;TernFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is designed to scale up to tens of exabytes, trillions of files, millions of concurrent clients.&lt;/item&gt;
      &lt;item&gt;Stores file contents redundantly to protect against drive failures.&lt;/item&gt;
      &lt;item&gt;Has no single point of failure in its metadata services.&lt;/item&gt;
      &lt;item&gt;Supports file snapshot to protect against accidental file deletion.&lt;/item&gt;
      &lt;item&gt;Can span across multiple regions.&lt;/item&gt;
      &lt;item&gt;Is hardware agnostic and uses TCP/IP to communicate.&lt;/item&gt;
      &lt;item&gt;Utilizes different types of storage (such as flash vs. hard disks) cost effectively.&lt;/item&gt;
      &lt;item&gt;Exposes read/write access through its own API over TCP and UDP, and a Linux kernel filesystem module.&lt;/item&gt;
      &lt;item&gt;Requires no external service and has a minimal set of build dependencies. [3]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Specifically, C++ and Go are needed to build the various TernFS components.&lt;/p&gt;
    &lt;p&gt;The C++ and Go processes depend on a handful of vendored libraries, most notably RocksDB for C++.&lt;/p&gt;
    &lt;p&gt;Naturally, there are some limitations, the main ones being:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Files are immutable — once they're written they can't be modified.&lt;/item&gt;
      &lt;item&gt;TernFS should not be used for tiny files — our median file size is 2MB.&lt;/item&gt;
      &lt;item&gt;The throughput of directory creation and removal is significantly constrained compared to other operations.&lt;/item&gt;
      &lt;item&gt;TernFS is permissionless, deferring that responsibility to other services.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We started designing TernFS in early 2022 and began putting it into production in summer 2023. By mid-2024 all of our machine learning efforts were driven out of TernFS, and we're migrating the rest of the firm's storage needs onto it as well.&lt;/p&gt;
    &lt;p&gt;As of September 2025, our TernFS deployment stores more than 500PB across 30,000 disks, 10,000 flash drives, and three data centres. At peak we serve multiple terabytes per second. To this day, we haven't lost a single byte.&lt;/p&gt;
    &lt;head rend="h2"&gt;High-level overview&lt;/head&gt;
    &lt;p&gt;Now that the stage is set, we're ready to explain the various components that make up TernFS. TernFS' core API is implemented by four services:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Metadata shards store the directory structure and file metadata.&lt;/item&gt;
      &lt;item&gt;The cross-directory coordinator (or CDC) executes cross-shard transactions.&lt;/item&gt;
      &lt;item&gt;Block services store file contents.&lt;/item&gt;
      &lt;item&gt;The registry stores information about all the other services and monitors them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
 A ──► B means "A sends requests to B" 
                                       
                                       
 ┌────────────────┐                    
 │ Metadata Shard ◄─────────┐          
 └─┬────▲─────────┘         │          
   │    │                   │          
   │    │                   │          
   │ ┌──┴──┐                │          
   │ │ CDC ◄──────────┐     │          
   │ └──┬──┘          │     │          
   │    │             │ ┌───┴────┐     
   │    │             └─┤        │     
 ┌─▼────▼────┐          │ Client │     
 │ Registry  ◄──────────┤        │     
 └──────▲────┘          └─┬──────┘     
        │                 │            
        │                 │            
 ┌──────┴────────┐        │            
 │ Block Service ◄────────┘            
 └───────────────┘

&lt;/code&gt;
    &lt;p&gt;In the next few sections, we'll describe the high-level design of each service and then give more background on other relevant implementation details.[4]&lt;/p&gt;
    &lt;p&gt;Note that TernFS' multi-region capabilities are orthogonal to much of its high-level design, and they're therefore explained separately.&lt;/p&gt;
    &lt;head rend="h3"&gt;Metadata&lt;/head&gt;
    &lt;p&gt;To talk about metadata, we first need to explain what metadata is in TernFS. The short answer is: 'everything that is not file contents.' The slightly longer answer is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Directory entries, including all files and directory names.&lt;/item&gt;
      &lt;item&gt;File metadata including creation/modification/access time, logical file size, and so on.&lt;/item&gt;
      &lt;item&gt;The mapping between files and the blocks containing their contents.&lt;/item&gt;
      &lt;item&gt;Other ancillary data structures to facilitate maintenance operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TernFS' metadata is split into 256 logical shards. Shards never communicate with each other. This is a general principle in TernFS: each service is disaggregated from the others, deferring to the clients to communicate with each service directly.[5]&lt;/p&gt;
    &lt;p&gt;There are some exceptions — most notably the shards execute requests from the CDC, and all services check into the registry.&lt;/p&gt;
    &lt;p&gt;A logical shard is further split into five physical instances, one leader and four followers, in a typical distributed consensus setup. The distributed consensus engine is provided by a purpose-built Raft-like implementation, which we call LogsDB, while RocksDB is used to implement read/write capabilities within a shard instance.&lt;/p&gt;
    &lt;p&gt;Currently all reads and writes go through the leader, but it would be trivial to allow clients to read from followers, and with a bit more effort to switch to a write-write setup.&lt;/p&gt;
    &lt;code&gt;    ┌─────────┐ ┌─────────┐       ┌───────────┐ 
    │ Shard 0 │ │ Shard 1 │  ...  │ Shard 255 │ 
    └─────────┘ │         │       └───────────┘ 
            ┌───┘         └───────────────────┐ 
            │                                 │ 
            │                  ┌────────────┐ │ 
            │ ┌───────────┐    │ Replica 0  │ │ 
            │ │           ◄────► (follower) │ │ 
 ┌────────┐ │ │ Replica 3 ◄──┐ └────────────┘ │ 
 │ Client ├─┼─► (leader)  ◄─┐│ ┌────────────┐ │ 
 └────────┘ │ │           ◄┐│└─► Replica 1  │ │ 
            │ └───────────┘││  │ (follower) │ │ 
            │              ││  └────────────┘ │ 
            │              ││  ┌────────────┐ │ 
            │              │└──► Replica 2  │ │ 
            │              │   │ (follower) │ │ 
            │              │   └────────────┘ │ 
            │              │   ┌────────────┐ │ 
            │              └───► Replica 4  │ │ 
            │                  │ (follower) │ │ 
            │                  └────────────┘ │ 
            └─────────────────────────────────┘ 
&lt;/code&gt;
    &lt;p&gt;Splitting the metadata into 256 shards from the get-go simplifies the design, given that horizontal scaling of metadata requires no rebalancing, just the addition of more metadata servers.&lt;/p&gt;
    &lt;p&gt;For instance, our current deployment can serve hundreds of petabytes and more than 100,000 compute nodes with just 10 metadata servers per data centre, with each server housing roughly 25 shard leaders and 100 shard followers.&lt;/p&gt;
    &lt;p&gt;Given that the metadata servers are totally decoupled from one another, this means that we can scale metadata performance by 25× trivially, and by 100× if we were to start offloading metadata requests to followers.&lt;/p&gt;
    &lt;p&gt;TernFS shards metadata by assigning each directory to a single shard. This is done in a simple round-robin fashion by the cross-directory coordinator. Once a directory is created, all its directory entries and the files in it are housed in the same shard.&lt;/p&gt;
    &lt;p&gt;This design decision has downsides: TernFS assumes that the load will be spread across the 256 logical shards naturally. This is not a problem in large deployments, given that they will contain many directories, but it is something to keep in mind.[6]&lt;/p&gt;
    &lt;head rend="h3"&gt;Cross-directory transactions&lt;/head&gt;
    &lt;p&gt;Most of the metadata activity is contained within a single shard:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File creation, same-directory renames, and deletion.&lt;/item&gt;
      &lt;item&gt;Listing directory contents.&lt;/item&gt;
      &lt;item&gt;Getting attributes of files or directories.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, some operations do require coordination between shards, namely directory creation, directory removal, and moving directory entries across different directories.&lt;/p&gt;
    &lt;p&gt;The cross-directory coordinator (CDC) performs these distributed transactions using a privileged metadata shard API. The CDC transactions are stateful, and therefore the CDC uses RocksDB and LogsDB much like the metadata shards themselves to persist its state safely.&lt;/p&gt;
    &lt;code&gt; ┌────────┐    ┌──────────┐ ┌───────────┐ 
 │ Client ├─┐  │ Shard 32 │ │ Shard 103 │ 
 └────────┘ │  └────────▲─┘ └─▲─────────┘ 
 ┌─────┬────┼───────────┼─────┼─┐         
 │ CDC │  ┌─▼──────┐    │     │ │         
 ├─────┘  │ Leader ├────┴─────┘ │         
 │        └─────▲──┘            │         
 │              │               │         
 │       ┌──────┴───────┐       │         
 │       │              │       │         
 │ ┌─────▼────┐    ┌────▼─────┐ │         
 │ │ Follower │ .. │ Follower │ │         
 │ └──────────┘    └──────────┘ │         
 └──────────────────────────────┘   
&lt;/code&gt;
    &lt;p&gt;The CDC executes transactions in parallel, which increases throughput considerably, but it is still a bottleneck when it comes to creating, removing, or moving directories. This means that TernFS has a relatively low throughput when it comes to CDC operations.[7]&lt;/p&gt;
    &lt;head rend="h3"&gt;Block services, or file contents&lt;/head&gt;
    &lt;p&gt;In TernFS, files are split into chunks of data called blocks. Blocks are read and written to by block services. A block service is typically a single drive (be it a hard disk or a flash drive) storing blocks. At XTX a typical storage server will contain around 100 hard disks or 25 flash drives — or in TernFS parlance 100 or 25 block services.[8]&lt;/p&gt;
    &lt;p&gt;Read/write access to the block service is provided using a simple TCP API currently implemented by a Go process. This process is hardware agnostic and uses the Go standard library to read and write blocks to a conventional local file system. We originally planned to rewrite the Go process in C++, and possibly write to block devices directly, but the idiomatic Go implementation has proven performant enough for our needs so far.&lt;/p&gt;
    &lt;head rend="h3"&gt;The registry&lt;/head&gt;
    &lt;p&gt;The final piece of the TernFS puzzle is the registry. The registry stores the location of each instance of service (be it a metadata shard, the CDC, or a block storage node). A client only needs to know the address of the registry to mount TernFS — it'll then gather the locations of the other services from it.&lt;/p&gt;
    &lt;p&gt;In TernFS all locations are IPv4 addresses. Working with IPv4 directly simplifies the kernel module considerably, since DNS lookups are quite awkward in the Linux kernel. The exception to this rule is addressing the registry itself, for which DNS is used.&lt;/p&gt;
    &lt;p&gt;The registry also stores additional information, such as the capacity and available size of each drive, who is a follower or a leader in LogsDB clusters, and so on.&lt;/p&gt;
    &lt;p&gt;Predictably, the registry itself is a RocksDB and LogsDB C++ process, given its statefulness.&lt;/p&gt;
    &lt;head rend="h3"&gt;Going global&lt;/head&gt;
    &lt;p&gt;TernFS tries very hard not to lose data, by storing both metadata and file contents on many different drives and servers. However, we also want to be resilient to the temporary or even permanent loss of one entire data centre. Therefore, TernFS can transparently scale across multiple locations.&lt;/p&gt;
    &lt;p&gt;The intended use for TernFS locations is for each location to converge to the same dataset. This means that each location will have to be provisioned with roughly equal resources.[9] Both metadata and file contents replication are asynchronous. In general, we judge the event of losing an entire data centre rare enough to tolerate a time window where data is not fully replicated across locations.&lt;/p&gt;
    &lt;p&gt;Metadata replication is set up so that one location is the metadata primary. Write operations in non-primary locations pay a latency price since they are acknowledged only after they are written to the primary location, replicated, and applied in the originating location. In practice this hasn't been an issue since metadata write latencies are generally overshadowed by writing file contents.&lt;/p&gt;
    &lt;p&gt;There is no automated procedure to migrate off a metadata primary location — again, we deem it a rare enough occurrence to tolerate manual intervention. In the future we plan to move from the current protocol to a multi-master protocol where each location can commit writes independently, which would reduce write latencies on secondary locations and remove the privileged status of the primary location.&lt;/p&gt;
    &lt;p&gt;File contents, unlike metadata, are written locally to the location the client is writing from. Replication to other locations happens in two ways: proactively and on-demand. Proactive replication is performed by tailing the metadata log and replicating new file contents. On-demand replication happens when a client requests file content which has not been replicated yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Important Details&lt;/head&gt;
    &lt;p&gt;Now that we've laid down the high-level design of TernFS, we can talk about several key implementation details that make TernFS safer, more performant, and more flexible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Talking to TernFS&lt;/head&gt;
    &lt;head rend="h4"&gt;Speaking TernFS' language&lt;/head&gt;
    &lt;p&gt;The most direct way to talk to TernFS is by using its own API. All TernFS messages are defined using a custom serialization format we call bincode. We chose to develop a custom serialization format since we needed it to work within the confines of the Linux kernel and to be easily chopped into UDP packets.&lt;/p&gt;
    &lt;p&gt;We intentionally kept the TernFS API stateless, in the sense that each request executes without regard to previous requests made by the same client. This is in contrast to protocols like NFS, whereby each connection is very stateful, holding resources such as open files, locks, and so on.&lt;/p&gt;
    &lt;p&gt;A stateless API dramatically simplifies the state machines that make up the TernFS core services, therefore simplifying their testing. It also forces each request to be idempotent, or in any case have clear retry semantics, since they might have to be replayed, which facilitates testing further.&lt;/p&gt;
    &lt;p&gt;It also allows the metadata shards and CDC API to be based on UDP rather than TCP, which makes the server and clients (especially the kernel module) simpler, due to doing away with the need for keeping TCP connections. The block service API is TCP based, since it is used to stream large amounts of contiguous data, and any UDP implementation would have to re-implement a reliable stream protocol. The registry API is also TCP-based, given that it is rarely used by clients, and occasionally needs to return large amounts of data.&lt;/p&gt;
    &lt;p&gt;While the TernFS API is simple out-of-the-box, we provide a permissively licensed Go library implementing common tasks that clients might want to perform, such as caching directory policies and retrying requests. This library is used to implement many TernFS processes that are not part of the core TernFS services, such as scrubbing, garbage collection, migrations, and the web UI.&lt;/p&gt;
    &lt;head rend="h4"&gt;Making TernFS POSIX-shaped&lt;/head&gt;
    &lt;p&gt;While the Go library is used for most ancillary tasks, some with high performance requirements, the main way to access TernFS at XTX is through its Linux kernel module.&lt;/p&gt;
    &lt;p&gt;This is because, when migrating our machine learning workflows to TernFS, we needed to support a vast codebase working with files directly. This not only meant that we needed to expose TernFS as a normal filesystem, but also that said normal filesystem API needed to be robust and performant enough for our machine learning needs.[10]&lt;/p&gt;
    &lt;p&gt;For this reason, we opted to work with Linux directly, rather than using FUSE. Working directly with the Linux kernel not only gave us the confidence that we could achieve our performance requirements but also allowed us to bend the POSIX API to our needs, something that would have been more difficult if we had used FUSE.[11]&lt;/p&gt;
    &lt;p&gt;The main obstacle when exposing TernFS as a 'normal' filesystem is that TernFS files are immutable. More specifically, TernFS files are fully written before being 'linked' into the filesystem as a directory entry. This is intentional: it lets us cleanly separate the API for 'under construction' files and 'completed files', and it means that half-written files are not visible.&lt;/p&gt;
    &lt;p&gt;However this design is essentially incompatible with POSIX, which endows the user with near-absolute freedom when it comes to manipulating a file. Therefore, the TernFS kernel module is not POSIX-compliant, but rather exposes enough POSIX to allow many programs to work without modifications, but not all.&lt;/p&gt;
    &lt;p&gt;In practice this means that programs which write files left-to-right and never modify the files' contents will work out-of-the-box. While this might seem very restrictive, we found that a surprising number of programs worked just fine.[12] Programs that did not follow this pattern were modified to first write to a temporary file and then copy the finished file to TernFS.&lt;/p&gt;
    &lt;p&gt;While we feel that writing our own kernel module was the right approach, it proved to be the trickiest part of TernFS, and we would not have been able to implement it without some important safety checks in the TernFS core services.[13]&lt;/p&gt;
    &lt;head rend="h4"&gt;S3 gateway&lt;/head&gt;
    &lt;p&gt;Almost all the storage-related activity at XTX is due to our machine-learning efforts, and for those purposes the TernFS' kernel module has served us well. However, as TernFS proved itself there, we started to look into offering TernFS to the broader firm.&lt;/p&gt;
    &lt;p&gt;Doing so through the kernel module presented multiple challenges. For starters installing a custom kernel module on every machine that needed to reach TernFS is operationally cumbersome. Moreover, while all machine-learning happens in clusters housed in the same data centre as TernFS itself, we wanted to expose TernFS in a way that's more amenable to less local networks, for instance by removing the need for UDP. Finally, TernFS does not have any built-in support for permissions or authentication, which is a requirement in multi-tenant scenarios.&lt;/p&gt;
    &lt;p&gt;To solve all these problems, we implemented a gateway for TernFS, which exposes a TernFS subtree using the S3 API. The gateway is a simple Go process turning S3 calls into TernFS API calls. The S3 gateway is not currently open sourced since it is coupled to authentication services internal to XTX, but we have open sourced a minimal S3 gateway to serve as a starting point for third-party contributors to build their own.&lt;/p&gt;
    &lt;p&gt;We've also planned an NFS gateway to TernFS, but we haven't had a pressing enough need yet to complete it.&lt;/p&gt;
    &lt;head rend="h4"&gt;The web UI and the JSON interface&lt;/head&gt;
    &lt;p&gt;Finally, a view of TernFS is provided by its web UI. The web UI is a stateless Go program which exposes most of the state of TernFS in an easy-to-use interface. This state includes the full filesystem contents (both metadata and file contents), the status of each service including information about decommissioned block services, and so on.&lt;/p&gt;
    &lt;p&gt;Moreover, the web UI also exposes the direct TernFS API in JSON form, which is very useful for small scripts and curl-style automation that does not warrant a full-blown Go program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directory Policies&lt;/head&gt;
    &lt;p&gt;To implement some of the functionality we'll describe below, TernFS adopts a system of per-directory policies.&lt;/p&gt;
    &lt;p&gt;Policies are used for all sorts of decisions, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to redundantly store files.&lt;/item&gt;
      &lt;item&gt;On which type of drive to store files.&lt;/item&gt;
      &lt;item&gt;How long to keep files around after deletion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of the topics above (and a few more we haven't mentioned) correspond to a certain policy tag. The body of the policies are stored in the metadata together with the other directory attributes.&lt;/p&gt;
    &lt;p&gt;Policies are inherited: if a directory does not contain a certain policy tag, it transitively inherits from the parent directory. TernFS clients store a cache of policies to allow for traversal-free policy lookup for most directories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping blocks in check&lt;/head&gt;
    &lt;p&gt;A filesystem is no good if it loses, leaks, corrupts, or otherwise messes up its data. TernFS deploys a host of measures to minimize the chance of anything going wrong. So far, these have worked: we've never lost data in our production deployment of TernFS. This section focuses on the measures in place to specifically safeguard files' blocks.&lt;/p&gt;
    &lt;head rend="h4"&gt;Against bitrot, or CRC32-C&lt;/head&gt;
    &lt;p&gt;The first and possibly most obvious measure consists of aggressively checksumming all TernFS' data. The metadata is automatically checksummed by RocksDB, and every block is stored in a format interleaving 4KiB pages with 4byte CRC32-C checksums.&lt;/p&gt;
    &lt;p&gt;CRC32-C was picked since it is a high-quality checksum and implemented on most modern silicon.[14] It also exhibits some desirable properties when used together with Reed-Solomon coding.&lt;/p&gt;
    &lt;p&gt;Peter Cawley's fast-crc32 repository provides a general framework to compute CRC32-C quickly, together with state-of-the-art implementations for x86 and aarch64 architectures.&lt;/p&gt;
    &lt;p&gt;4KiB was picked since it is the read boundary used by Linux filesystems and is fine-grained while still being large enough to render the storage overhead of the 4byte checksums negligible.&lt;/p&gt;
    &lt;p&gt;Interleaving the CRCs with the block contents does not add any safety, but it does improve operations in two important ways. First, it allows for safe partial reads: clients can demand only a few pages from a block which is many megabytes in size and still check the reads against its checksum. Second, it allows scrubbing files locally on the server which hosts the blocks, without communicating with other services at all.&lt;/p&gt;
    &lt;head rend="h4"&gt;Storing files redundantly, or Reed-Solomon codes&lt;/head&gt;
    &lt;p&gt;We've been talking about files being split into blocks, but we haven't really explained how files become blocks.&lt;/p&gt;
    &lt;p&gt;The first thing we do to a file is split it into spans. Spans are at most 100MiB and are present just to divide files into sections of a manageable size.&lt;/p&gt;
    &lt;p&gt;Then each span is divided into D data blocks, and P parity blocks. D and P are determined by the corresponding directory policy in which the file is created. When D is 1, the entire contents of the span become a single block, and that block is stored D+P times. This scheme is equivalent to a simple mirroring scheme and allows it to lose up to P blocks before losing file data.&lt;/p&gt;
    &lt;p&gt;While wasteful, mirroring the entire contents of the file can be useful for very hot files, since TernFS clients will pick a block at random to read from, thereby sharing the read load across many block services. And naturally files which we do not care much for can be stored with D = 1 and P = 0, without any redundancy.&lt;/p&gt;
    &lt;p&gt;That said, most files will not be stored using mirroring but rather using Reed-Solomon coding. Other resources can be consulted to understand the high-level idea and the low-level details of Reed-Solomon coding, but the gist is it allows us to split a span into D equally sized blocks (some padding might be necessary), and then generate P blocks of equal size such that up to any P blocks can be lost while retaining the ability to reconstruct all the other blocks.&lt;/p&gt;
    &lt;p&gt;As mentioned, D and P are fully configurable, but at XTX we tend to use D = 10 and P = 4, which allows us to lose up to any four drives for any file.&lt;/p&gt;
    &lt;head rend="h4"&gt;Drive type picking&lt;/head&gt;
    &lt;p&gt;We now know how to split files into a bunch of blocks. The next question is: which drives to pick to store the blocks on. The first decision is which kind of drive to use. At XTX we separate drives into two broad categories for this purpose — flash and spinning disks.&lt;/p&gt;
    &lt;p&gt;When picking between these two, we want to balance two needs: minimizing the cost of hardware by utilizing hard disks if we can [15], and maximizing hard disk productivity by having them reading data most of the time, rather than seeking.&lt;/p&gt;
    &lt;p&gt;To achieve that, directory policies offer a way to tune how large each block will be, and to tune which drives will be picked based on block size. This allows us to configure TernFS so that larger files that can be read sequentially are stored on hard disks, while random-access or small files are stored on flash. [16]&lt;/p&gt;
    &lt;p&gt;Currently this system is not adaptive, but we found that in practice it's easy to carve out sections of the filesystem which are not read sequentially. We have a default configuration which assumes sequential reads and then uses hard disks down to roughly 2.5MB blocks, below which hard disks stop being productive enough and blocks start needing to be written to flash.&lt;/p&gt;
    &lt;head rend="h4"&gt;Block service picking&lt;/head&gt;
    &lt;p&gt;OK, we now know what type of drive to select for our files, but we still have tens of thousands of individual drives to pick from. Picking the 'right' individual drive requires some sophistication.&lt;/p&gt;
    &lt;p&gt;The first thing to note is that drive failures or unavailability are often correlated. For instance, at XTX a single server handles 102 spinning disks. If the server is down, faulty, or needs to be decommissioned, it'll render its 102 disks temporarily or permanently unavailable.&lt;/p&gt;
    &lt;p&gt;It's therefore wise to spread a file's blocks across many servers. To achieve this, each TernFS block service (which generally corresponds to a single drive) has a failure domain. When picking block services in which to store the blocks for a given file, TernFS will make sure that each block is in a separate failure domain. In our TernFS deployment a failure domain corresponds to a server, but other users might wish to tie it to some other factor as appropriate.&lt;/p&gt;
    &lt;p&gt;TernFS also tries hard to avoid write bottlenecks by spreading the current write load across many disks. Moreover, since new drives can be added at any time, it tries to converge to a situation where each drive is roughly equally filled by assigning writing more to drives with more available space.&lt;/p&gt;
    &lt;p&gt;Mechanically this is achieved by having each shard periodically request a set of block services to use for writing from the registry. When handing out block services to shards, the registry selects block services according to several constraints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It never gives block services from the same failure domain to the same shard&lt;/item&gt;
      &lt;item&gt;It minimizes the variance in how many shards each block service is currently assigned to&lt;/item&gt;
      &lt;item&gt;It prioritizes block services which have more available space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then when a client wants to write a new span, requiring D+P blocks, the shard simply selects D+P block services randomly amongst the ones it last received from the registry.&lt;/p&gt;
    &lt;p&gt;One concept currently absent from TernFS is what is often known as 'copyset replication'. When assigning disks to files at random (even with the caveat of failure domains) the probability of rendering at least one file unreadable quickly becomes a certainty as more and more drives fail:&lt;/p&gt;
    &lt;p&gt;Copysets reduce the likelihood of data loss occurring by choosing blocks out of a limited number of sets of drives, as opposed to picking the drives randomly. This dramatically reduces the probability of data loss[17]. They are generally a good idea, but we haven't found them to be worthwhile, for a few reasons.&lt;/p&gt;
    &lt;p&gt;Note that while copysets reduce the failure probability, they do not (and cannot) reduce the expected amount of data loss. That is, instead of a large probability of a relatively small amount of data loss we have a very small probability of a catastrophic loss.&lt;/p&gt;
    &lt;p&gt;First, evacuating a 20TB drive takes just a few minutes, and in the presence of multiple failed drives the migrator process evacuates first the files which are present in multiple failed drives to get ahead of possible data loss. This means that for TernFS to lose data within a single data centre tens of drives would have to fail within a matter of seconds.&lt;/p&gt;
    &lt;p&gt;More importantly, our TernFS deployment is replicated across three data centres. This replication eliminates the chance of losing data due to 'independent' drive failures — thousands of drives would need to fail at once. Obviously, data centre wide events can cause a large proportion of the drives within it to fail, but having such an event in three data centres at once is exceedingly unlikely.&lt;/p&gt;
    &lt;p&gt;Finally, copysets are not without drawbacks or complications. Assigning drives at random is an optimal strategy when it comes to evacuating drives quickly, since the files with blocks in the drives to be evacuated will be evenly spread over the rest of the filesystem, and since we only ever need to replace the failed blocks given that we're not constrained by fitting the new set of blocks in predetermined copysets. This means that the evacuation procedure will not be bottlenecked by drive throughput, which is what enables evacuation to finish in a matter of minutes. Moreover, the algorithm to distribute drives to shards is significantly simpler and more flexible than if it needed to care about copysets.&lt;/p&gt;
    &lt;p&gt;However, users that wish to deploy TernFS within a single data centre might wish to implement some form of copyset replication. Such a change would be entirely contained to the registry and would not change any other component.&lt;/p&gt;
    &lt;head rend="h4"&gt;Block Proofs&lt;/head&gt;
    &lt;p&gt;We now have a solid scheme to store files redundantly (thanks to Reed-Solomon codes) and protect against bitrot (thanks to the checksums). However, said schemes are only as good as their implementation.&lt;/p&gt;
    &lt;p&gt;As previously mentioned, TernFS clients communicate their intention to write a file to metadata servers, the metadata servers select block services that the blocks should be written to, and the clients then write the blocks to block services independently of the metadata services. The same happens when a client wants to erase blocks: the client first communicates its intentions to delete the blocks to the right metadata shard and then performs the erasing itself.&lt;/p&gt;
    &lt;p&gt;This poses a challenge. While verifying the correctness of the core TernFS services is feasible, verifying all clients is not, but we'd still like to prevent buggy clients from breaking key invariants of the filesystem.&lt;/p&gt;
    &lt;p&gt;Buggy clients can wreak havoc in several ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They can leak data by writing blocks to block services that are not referenced anywhere in the metadata.&lt;/item&gt;
      &lt;item&gt;They can lose data by erasing blocks which are still referenced in metadata.&lt;/item&gt;
      &lt;item&gt;They can corrupt data by telling the metadata services they'll write something and then writing something else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We address all these points by using what we call block proofs. To illustrate how block proofs work, it's helpful to go through the steps required to write new data to a file.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When a client is creating a file, it'll do so by adding its file spans one-by-one. For each span the client wants to add it sends an 'initiate span creation' request to the right metadata shard. This request contains both the overall checksum of the span, and the checksum of each block in it (including parity blocks).&lt;/item&gt;
      &lt;item&gt;The metadata shard checks the consistency of the checksum of the span and of its blocks, something it can do thanks to some desirable mathematical properties of CRCs.&lt;/item&gt;
      &lt;item&gt;The shard picks block services for the blocks to be written in and returns this information to the client together with a signature for each 'block write' instruction.&lt;/item&gt;
      &lt;item&gt;The client forwards this signature to the block services, which will refuse to write the block without it. Crucially, the cryptographic signature ranges over a unique identity for the block (ensuring we only write the block we mean to write), together with its checksum, ensuring we don't write the wrong data.[18]&lt;/item&gt;
      &lt;item&gt;After committing the block to disk, the block service returns a 'block written' signature to the client.&lt;/item&gt;
      &lt;item&gt;Finally, the client forwards the block written signature back to the shard, which certifies that the span has been written only when it has received the signatures for all the blocks that make up the span. [19]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This kind of scheme was described more generally in a separate blog post.&lt;/p&gt;
    &lt;p&gt;Similarly, when a client wants to delete a span, it first asks the metadata shard to start doing so. The metadata shard marks the span as 'in deletion' and returns a bunch of 'block erase' signatures to the client. The client then forwards the signatures to the block services that hold the blocks, which delete the blocks, and return a 'block erased' signature. The clients forward these signatures back to the metadata shards, which can then forget about the span entirely.&lt;/p&gt;
    &lt;p&gt;We use AES to generate the signatures for simplicity but note that the goal here is not protecting ourselves from malicious clients — just buggy ones. The keys used for the signature are not kept secret, and CRC32-C is not a secure checksum. That said, we've found this scheme enormously valuable in the presence of complex clients. We spent considerable efforts making the core services very simple so we could then take more implementation risks in the clients, with the knowledge that we would have a very low chance of corrupting the filesystem itself.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scrubbing&lt;/head&gt;
    &lt;p&gt;Finally, if things go wrong, we need to notice. The most common failure mode for a drive is for it to fail entirely, in which case our internal hardware monitoring system will pick it up and migrate from it automatically. The more insidious (and still very common) case is a single sector failing in a drive, which will only be noticed when we try to read the block involving that sector.&lt;/p&gt;
    &lt;p&gt;This is acceptable for files which are read frequently, but some files might be very 'cold' but still very important.&lt;/p&gt;
    &lt;p&gt;Consider the case of raw market data taps which are immediately converted to some processed, lossy format. While we generally will use the file containing the processed data, it's paramount to store the raw market data forever so that if we ever want to include more information from the original market data, we can. So important cold files might go months or even years without anyone reading them, and in the meantime, we might find that enough blocks have been corrupted to render them unreadable.[20]&lt;/p&gt;
    &lt;p&gt;To make sure this does not happen, a process called the scrubber continuously reads every block that TernFS stores, and replaces blocks with bad sectors before they can cause too much damage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Snapshots and garbage collection&lt;/head&gt;
    &lt;p&gt;We've talked at length about what TernFS does to try to prevent data loss due to hardware failure or bugs in clients. However, the most common type of data loss is due to human error — the &lt;code&gt;rm —rf / home/alice/notes.txt&lt;/code&gt; scenario.&lt;/p&gt;
    &lt;p&gt;To protect against these scenarios, TernFS implements a lightweight snapshotting system. When files or directories are deleted, their contents aren't actually deleted. Instead, a weak reference to them is created. We call such weak references snapshot directory entries.&lt;/p&gt;
    &lt;p&gt;Snapshot entries are not be visible through the kernel module or the S3 gateway, but are visible through the direct API, and at XTX we have developed internal tooling to easily recover deleted files through it.[21] Deleted files are also visible through the TernFS web UI.&lt;/p&gt;
    &lt;p&gt;Given that 'normal' file operations do not delete files, but rather make them a snapshot, the task of freeing up space is delegated to an external Go process, the garbage collector. The garbage collector traverses the filesystem and removes expired snapshots, which involves deleting their blocks permanently. Snapshot expiry is predictably regulated by directory policies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping TernFS healthy&lt;/head&gt;
    &lt;p&gt;This last section covers how we (humans of XTX) notice problems in TernFS, and how TernFS self-heals when things go wrong — both key topics if we want to ensure no data loss and notice performance problems early.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance metrics&lt;/head&gt;
    &lt;p&gt;TernFS exposes a plethora of performance metrics through the HTTP InfluxDB line protocol. While connecting TernFS to a service which ingests these metrics is optional, it is highly recommended for any production service.&lt;/p&gt;
    &lt;p&gt;Moreover, the kernel module exposes many performance metrics itself through DebugFS.&lt;/p&gt;
    &lt;p&gt;Both types of metrics, especially when used in tandem, have proved invaluable to resolve performance problems quickly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Logging and alerts&lt;/head&gt;
    &lt;p&gt;TernFS services log their output to files in a simple line-based format. The internal logging API is extremely simple and includes support for syslog levels out-of-the-box. At XTX we run TernFS as normal systemd services and use journalctl to view logs.&lt;/p&gt;
    &lt;p&gt;As with metrics, the kernel module includes various logging facilities as well. The first type of logging is just through dmesg, but the kernel module also includes numerous tracepoints for low-overhead opt-in logging of many operations.&lt;/p&gt;
    &lt;p&gt;TernFS is also integrated with XTX's internal alerting system, called XMon, to page on call developers when things go wrong. XMon is not open source, but all the alerts are also rendered as error lines in logs. [22] We plan to eventually move to having alerts feed off performance metrics, which would make them independent from XMon, although we don't have plans to do so in the short-term.&lt;/p&gt;
    &lt;head rend="h4"&gt;Migrations&lt;/head&gt;
    &lt;p&gt;Finally, there's the question of what to do when drives die — and they will die, frequently, when you have 50,000 of them. While drives dying is not surprising, we've been surprised at the variety of different drive failures. [23] A malfunctioning drive might:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Produce IO errors when reading specific files. This is probably due to a single bad sector.&lt;/item&gt;
      &lt;item&gt;Produce IO errors when reading or writing anything. This might happen because enough bad sectors have gone bad and the drive cannot remap them, or for a variety of other reasons.&lt;/item&gt;
      &lt;item&gt;Return wrong data. This is usually caught by the built-in error correction codes in the hard drives, but not always.&lt;/item&gt;
      &lt;item&gt;Lie about data being successfully persisted. This can manifest in a variety of ways: file size being wrong on open, file contents being partially zero'd out, and so on.&lt;/item&gt;
      &lt;item&gt;Disappear from the mount list, only to reappear when the machine is rebooted, but missing some data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When clients fail to read from a drive, they'll automatically fall back on other drives to reconstruct the missing data, which is extremely effective in hiding failures from the end-user. That said, something needs to be done about the bad drives, and done quickly to avoid permanent data loss.&lt;/p&gt;
    &lt;p&gt;The TernFS registry allows marking drives as faulty. Faulty drives are then picked up by the migrator, a Go process which waits for bad drives and then stores all its blocks onto freshly picked block services.&lt;/p&gt;
    &lt;p&gt;TernFS also tries to mark drives as bad automatically using a simple heuristic based on the rate of IO errors the drive is experiencing. The number of drives automatically marked as faulty is throttled to avoid having this check go awry and mark the whole cluster as faulty, which would not be catastrophic but would still be messy to deal with.&lt;/p&gt;
    &lt;p&gt;Moreover, drives that are faulty in subtle ways might not be picked up by the heuristics, which means that occasionally a sysadmin will need to mark a drive as faulty manually, after which the migrator will evacuate them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing thoughts&lt;/head&gt;
    &lt;p&gt;At XTX we feel strongly about utilizing our resources efficiently. When it comes to software, this means having software that gets close to some theoretical optimum when it comes to total cost of ownership. This culture was borne out by competing hard for technological excellence when doing on-exchange trading at first, and by our ever-growing hardware costs as our business has grown later.&lt;/p&gt;
    &lt;p&gt;Such idealized tools might not exist or be available yet, in which case we're happy to be the tool makers. TernFS is a perfect example of this and we're excited to open source this component of our business for the community.&lt;/p&gt;
    &lt;p&gt;Crucially, the cost of implementation of a new solution is often overblown compared to the cost of tying yourself to an ill-fitting, expensive third-party solution. Designing and implementing a solution serving exactly your needs allows for much greater simplicity. If the requirements do change, as often happens, changes can be implemented very quickly, again only catering to your needs.&lt;/p&gt;
    &lt;p&gt;That said, we believe that TernFS' set of trade-offs are widely shared across many organizations dealing with large-scale storage workloads, and we hope we'll contribute to at least slowing down the seemingly constant stream of new filesystems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.xtxmarkets.com/tech/2025-ternfs/"/><published>2025-09-18T14:36:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45291024</id><title>Launch HN: Cactus (YC S25) – AI inference on smartphones</title><updated>2025-09-19T02:18:52.848226+00:00</updated><content>&lt;doc fingerprint="b50a9a40fcb997a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Energy-efficient AI inference framework &amp;amp; kernels for phones &amp;amp; AI-native hardware. Budget and mid-range phones control over 70% of the market, but frameworks today optimise for the highend phones. Cactus is designed bottom-up with no dependencies for all mobile devices.&lt;/p&gt;
    &lt;p&gt;Example (CPU-only):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model: Qwen3-600m-INT8&lt;/item&gt;
      &lt;item&gt;File size: 370-420mb&lt;/item&gt;
      &lt;item&gt;16-20 t/s on Pixel 6a, Galaxy S21, iPhone 11 Pro&lt;/item&gt;
      &lt;item&gt;50-70 t/s on Pixel 9, Galaxy S25, iPhone 16&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cactus exposes 4 levels of abstraction.&lt;/p&gt;
    &lt;code&gt;┌─────────────────┐
│   Cactus FFI    │ ←── OpenAI compatible C API for integration  
└─────────────────┘
         │
┌─────────────────┐
│  Cactus Engine  │ ←── High-level transformer engine
└─────────────────┘
         │
┌─────────────────┐  
│  Cactus Graph   │ ←── Unified zero-copy computation graph 
└─────────────────┘
         │
┌─────────────────┐
│ Cactus Kernels  │ ←── Low-level ARM-specific SIMD operations
└─────────────────┘
&lt;/code&gt;
    &lt;p&gt;Cactus Graph is a general numerical computing framework that runs on Cactus Kernels. Great for implementing custom models and scientific computing, like JAX for phones.&lt;/p&gt;
    &lt;code&gt;#include cactus.h

CactusGraph graph;

auto a = graph.input({2, 3}, Precision::FP16);
auto b = graph.input({3, 4}, Precision::INT8);

auto x1 = graph.matmul(a, b, false);
auto x2 = graph.transpose(x1);
auto result = graph.matmul(b, x2, true);

float a_data[6] = {1.1f, 2.3f, 3.4f, 4.2f, 5.7f, 6.8f};
float b_data[12] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};

graph.set_input(a, a_data, Precision::FP16);
graph.set_input(b, b_data, Precision::INT8);
graph.execute();

void* output_data = graph.get_output(result);
graph.hard_reset(); 
&lt;/code&gt;
    &lt;p&gt;Cactus Engine is a transformer inference engine built on top of Cactus Graphs. It is abstracted via Cactus Foreign Function Interface.&lt;/p&gt;
    &lt;code&gt;#include cactus.h

const char* model_path = "path/to/weight/folder";
cactus_model_t model = cactus_init(model_path, 2048);

const char* messages = R"([
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "/nothink My name is Henry Ndubuaku"}
])";

const char* options = R"({
    "temperature": 0.1,
    "top_p": 0.95,
    "top_k": 20,
    "max_tokens": 50,
    "stop_sequences": ["&amp;lt;|im_end|&amp;gt;"]
})";

char response[1024];
int result = cactus_complete(model, messages, response, sizeof(response), options, nullptr, nullptr, nullptr);&lt;/code&gt;
    &lt;p&gt;With tool support:&lt;/p&gt;
    &lt;code&gt;const char* tools = R"([
    {
        "function": {
            "name": "get_weather",
            "description": "Get weather for a location",
            "parameters": {
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City name",
                        "required": true
                    }
                },
                "required": ["location"]
            }
        }
    }
])";

int result = cactus_complete(model, messages, response, sizeof(response), options, tools, nullptr, nullptr);&lt;/code&gt;
    &lt;p&gt;This makes it easy to write Cactus bindings for any language. Header files are self-documenting but documentation contributions are welcome.&lt;/p&gt;
    &lt;p&gt;Cactus SDKs run 500k+ weekly inference tasks in production today, try them!&lt;/p&gt;
    &lt;p&gt;You can run these codes directly on Macbooks with Apple chips due to their design. Performance gain is observed in mobile devices but for testing during development, Vanilla M3 CPU-only can run Qwen3-600m-INT8 at 60-70 toks/sec, use the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate weights from HuggingFace model:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python3 tools/convert_hf.py Qwen/Qwen3-0.6B weights/qwen3-600m-i8/ --precision INT8&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build and test:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./tests/run.sh # remember to chmod +x any script first time
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemma, SmolVLM, Liquid, Kitten, Vosk etc.&lt;/item&gt;
      &lt;item&gt;SMMLA, NPU &amp;amp; DSP for high-end phones.&lt;/item&gt;
      &lt;item&gt;INT4 support for 1B+ models.&lt;/item&gt;
      &lt;item&gt;Python tools for porting Torch/JAX cactus.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Preliminary results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Qwen3-4B-INT4 on iPhone 16 Pro NPU = 21 t/s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While Cactus can be used for all Apple devices including Macbooks, for computers/AMD/Intel/Nvidia generally, please use HuggingFace, Llama.cpp, Ollama, vLLM, MLX. They're built for those, support x86, and are all great!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/cactus-compute/cactus"/><published>2025-09-18T15:40:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45291858</id><title>Configuration files are user interfaces</title><updated>2025-09-19T02:18:52.668131+00:00</updated><content>&lt;doc fingerprint="d5105d5f2bb7b3dd"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Configuration files are user interfaces&lt;/head&gt;a.k.a. An escape route from YAML hell&lt;p&gt;We have all been there. Your software keeps growing and you feel the need to make it customizable. It is too soon for a full-blown UI with all the bells and whistles, so your pragmatic instinct suggests a text-based configuration file. Yes, that’s exactly it!&lt;/p&gt;&lt;p&gt;You rejoice knowing the software’s configuration will be trivial to version control. Your pragmatic instinct is satisfied as well; the door remains open to creating a proper UI later, since it would be merely a graphical view of your configuration’s structured data. The future is bright!&lt;/p&gt;&lt;p&gt;Now, which language should you pick for your glorious configuration file? It needs to be user-friendly, so people can inspect it and modify it with ease. JSON springs immediately to your mind, but the abundance of brackets and the lack of comments give you pause. TOML maybe? You are afraid it might be too minimal for your needs. Rolling your own language? Too impractical.&lt;/p&gt;&lt;head rend="h3"&gt;YAML&lt;/head&gt;&lt;p&gt;A forbidden spark lights inside your head. Any attempts to put it out are futile. It grows and grows until it finally stands ablaze before you, tempting you with its warmth: why not YAML? Yes, YAML, which is so pleasant to the eye and widely used across the industry. How could you say no to that?&lt;/p&gt;&lt;p&gt;Shivers run down your spine as you remember the yaml document from hell. So often have you been warned about YAML’s deceptive simplicity! It’s a traitorous mask, your elders said, behind which a dark being lurks. Don’t ever come near, it will swallow your soul when you least expect it.&lt;/p&gt;&lt;p&gt;And yet… could YAML indeed be the pragmatic solution in this particular case? We are talking about a small configuration file here. What could possibly go wrong? Surely the gods of software wouldn’t punish you for this offense? How could they ask you to swim against the current, when even renowned projects such as Kubernetes use YAML pervasively?&lt;/p&gt;&lt;p&gt;Trembling, you stretch your hand towards the forbidden fruit, reap it and take a good bite. The flavor of instant productivity fills your mouth with delight and you feel confirmed in your choice. Why did you even doubt? After a few code changes your software is configurable through a YAML file. The dopamine surge overwhelms you, and you put the icing on the cake by adding an example configuration to the project’s readme.&lt;/p&gt;&lt;p&gt;But alas, superficial satisfaction cannot last. As the years go by, the sweet flavor in your mouth turns bitter. Your software has grown. The once simple configuration file now spans more than a hundred lines. Yes, the file is pleasant to look at, but modifying it is nothing short of miserable. Why did you disregard ancient wisdom? In silence, you mourn your lost innocence and the fallen state of humanity.&lt;/p&gt;&lt;p&gt;Ah, if you could begin again.&lt;/p&gt;&lt;head rend="h3"&gt;The crux of the problem&lt;/head&gt;&lt;p&gt;Do you recognize yourself in this story? I have seen it play out a few times and feel like we, as an industry, have somehow come to terms with the miserable situation we are in. Once in a while you may see some brave and noble soul proposing a new configuration language, but so far none has achieved mass adoption. What is going on?&lt;/p&gt;&lt;p&gt;The crux of the problem is, in my eyes, beyond the domain of configuration language choice (i.e., YAML vs. alternatives). We seem to be approaching the very problem of configuration from a flawed starting point, setting way too low expectations for our tools. We are failing to see that configuration files are actually user interfaces, and that they should be treated as such.&lt;/p&gt;&lt;p&gt;Once you start thinking of configuration files as user interfaces, it suddenly makes sense to demand an excellent user experience for working with them. The whole point of a user interface is to make the software accessible, with mechanisms that prevent human error and guide the user down the pit of success. We all recognize bad UX when it feels like you are fighting the computer to achieve a specific goal. In an ideal world, the computer would enhance you without getting in the way, like a bicycle for the mind.&lt;/p&gt;&lt;p&gt;What would configuring software look like if our tools were rooted in the “configuration is UI” paradigm? Can we realistically dream of an ecosystem in which configuration is a joy to write and maintain?&lt;/p&gt;&lt;head rend="h3"&gt;A shout out to KSON&lt;/head&gt;&lt;p&gt;Having come to this point, I’m resisting the urge to present an all-encompassing theory of what configuring software could look like in the perfect world. Instead, I’d like to give a shout out to an existing open source project, which in my eyes is an excellent real-world example of the “configuration is UI” vision. I’m talking about KSON, which just released its first public beta after years in the making. Feel free to check out the website, or go directly to the online playground. That will give you a much better idea of the project than anything I could write here. You know what they say: show, don’t tell.&lt;/p&gt;&lt;p&gt;For those who’d rather skip the links above, let me briefly quote some paragraphs from the beta release announcement:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Anywhere a human is reading or editing YAML/JSON/TOML, KSON may be used as a more effective interface on that data.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;That’s a bold claim right there! But maybe it’s warranted, especially once you consider the sheer amount of work that has gone into the release:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;KSON is a verified superset of JSON, has native JSON Schema support, transpiles cleanly to YAML (with comments preserved!), and is likely available wherever you want it—current supported platforms: JS/TS, Python, Rust, JVM, and Kotlin Multiplatform.&lt;/p&gt;&lt;p&gt;KSON is also widely available in developer tools, with support for VS Code, Jetbrains IDEs, and anywhere you can plug in an LSP.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;See the appendix at the end of this article for an example KSON document. You will notice that the language feels familiar and that it has been designed from the ground up to provide an excellent editing experience. Also, advanced language support in code editors is to me a great example of the “configuration is UI” paradigm. It lets the document come to life under your fingertips, instead of being a dead text file.&lt;/p&gt;&lt;head rend="h3"&gt;Join the movement&lt;/head&gt;&lt;p&gt;What a breath of fresh air! I’m hoping the vision of user-friendly configuration keeps unfolding over time, be it inside the KSON project or elsewhere. We, as an industry, should really set a new standard in which it’s normal and even expected to provide a top-tier configuration editing experience. There are so many possibilities1!&lt;/p&gt;&lt;p&gt;In case it’s not clear yet, I’m enthusiastic about KSON. If you visit the open source repository, you will probably see me among the contributors to the project2. Besides the technical merits of KSON, I’m impressed by the driving force behind it: a small community of engineers has decided to bite the bullet and craft open software configuration tools that put humans first. It truly is “a love letter to the humans maintaining computer configurations”, as it says in the repository’s tagline.&lt;/p&gt;&lt;p&gt;To me, KSON is more than a new language or a collection of tools. It is an attempt to bootstrap a developer movement based in the “configuration is UI” principle. If that resonates with you, please join us in our effort! Merely trying out KSON is already a good start. And, if you end up liking it, go use it wherever it makes sense. Finally, feel free to chat with us on Zulip any time. I’m looking forward to meeting you there!&lt;/p&gt;&lt;head rend="h4"&gt;Appendix: a KSON example&lt;/head&gt;&lt;p&gt;While this blog post is about the principles behind the project, and not about the KSON language, here’s a tiny example of a &lt;code&gt;.kson&lt;/code&gt; file derived from a dbt model:&lt;/p&gt;&lt;code&gt;version: 2
models:
  - name: my_transformation
    description: 'This model transforms raw data'
    columns:
      - name: id
        description: 'A unique identifier'
      - name: name
        description: 'The name of the item'
        .
    database: your_database
    schema: your_schema
    materialized: table
    sql: %sql
      SELECT
        id,
        name
      FROM source_data%%
&lt;/code&gt;&lt;p&gt;As you can see, it has the readability of YAML, which is a great feature in my book! Importantly, however, KSON carefully avoids classic YAML footguns. One example of that is indentation handling: the code snippet above is indented in a way that makes the structure of the document evident. But, contrary to YAML, having “wrong” indentation does not break your configuration. If you were to remove or randomize the leading spaces for every line, the following would happen:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The document would parse to the same object as before.&lt;/item&gt;&lt;item&gt;KSON would warn you that the document’s formatting is confusing, because the indentation doesn’t match the structure of the document (fortunately, the autoformatter can trivially fix the warning for you on save).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;An additional feature that is not immediately apparent here is that the embedded SQL is actually “alive”. A properly configured editor will see more than a multiline string there! It will know that it’s SQL, it will provide syntax highlighting for it, validation, and all other goodies you are used to when dealing with code.&lt;/p&gt;&lt;p&gt;See the official website for more information and a space to play with KSON right from your browser.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;In the case of KSON, there are “boring” things in the pipeline like library support for more programming languages, but there is also a (growing) list of potential enhancements that would be true game changers (see the corresponding issue for details). ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;While I’m one of the contributors to KSON, most of the credit goes to Daniel, who has been behind the project since 2021 and has spent countless hours making it awesome. Next in line comes Bart, who started helping out in 2024 and has been a key player in getting KSON to its beta release. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ochagavia.nl/blog/configuration-files-are-user-interfaces/"/><published>2025-09-18T16:43:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45292475</id><title>OpenTelemetry Collector: What It Is, When You Need It, and When You Don't</title><updated>2025-09-19T02:18:52.485307+00:00</updated><content>&lt;doc fingerprint="76f9d19190f9188e"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Do you really need an OpenTelemetry Collector? If you're just sprinkling SDKs into a side project - maybe not. If you're running a multi-service production environment and care about cost, performance, security boundaries, or intelligent processing - yes, you almost certainly do.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This post explains exactly what the OpenTelemetry Collector is, why it exists, how data flows with and without it, and the trade‑offs of each approach. You’ll leave with a decision framework, deployment patterns, and practical configuration guidance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Definition&lt;/head&gt;
    &lt;p&gt;The OpenTelemetry Collector is a vendor‑neutral, pluggable telemetry pipeline that receives, processes, and exports telemetry signals (traces, metrics, logs, profiles, more coming) from your applications to one or more backends (one of those backends is OneUptime).&lt;/p&gt;
    &lt;p&gt;It removes vendor SDK lock‑in, centralizes telemetry policy, and gives you a programmable choke point to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clean the data (remove sensitive fields, add context)&lt;/item&gt;
      &lt;item&gt;Batch sends and retry automatically when exports fail&lt;/item&gt;
      &lt;item&gt;Sample smartly (keep errors &amp;amp; rare slow traces, trim noisy success traffic)&lt;/item&gt;
      &lt;item&gt;Smooth out differences between frameworks / SDK versions&lt;/item&gt;
      &lt;item&gt;Route traces, metrics, and logs to different backends&lt;/item&gt;
      &lt;item&gt;Act as a safety barrier between app nodes and the public internet&lt;/item&gt;
      &lt;item&gt;Cut cost early by dropping low‑value or redundant telemetry&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Architecture at 10,000ft&lt;/head&gt;
    &lt;head rend="h3"&gt;1. Without a Collector (Direct Export)&lt;/head&gt;
    &lt;p&gt;Each service ships telemetry directly to your backend (e.g., OneUptime, another SaaS, or self‑hosted store):&lt;/p&gt;
    &lt;code&gt;graph LR
  A[Service A
  App SDKs] --&amp;gt;|OTLP/gRPC| B[(Observability Backend)]
  C[Service B
  App SDKs] --&amp;gt;|OTLP/HTTP| B
  D[Service C
  App SDKs] --&amp;gt;|OTLP| B&lt;/code&gt;
    &lt;p&gt;Pros:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simpler (fewer moving parts)&lt;/item&gt;
      &lt;item&gt;Lower operational overhead&lt;/item&gt;
      &lt;item&gt;Good for small apps / POCs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each service handles retries, auth, backpressure&lt;/item&gt;
      &lt;item&gt;Hard to change exporters later (coupling)&lt;/item&gt;
      &lt;item&gt;No central sampling / scrubbing / routing&lt;/item&gt;
      &lt;item&gt;Higher risk of SDK or network misconfig hurting reliability&lt;/item&gt;
      &lt;item&gt;Increased egress cost if sending duplicate data to multiple vendors&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2. With a Central Collector&lt;/head&gt;
    &lt;p&gt;All apps send to a centralized Collector (or a tiered set) which then exports.&lt;/p&gt;
    &lt;code&gt;graph LR
  subgraph Your Infrastructure
    A[Service A]
    C[Service B]
    D[Service C]
    A --&amp;gt; E[Central Collector]
    C --&amp;gt; E
    D --&amp;gt; E
  end



  E --&amp;gt;|Telemetry Data| B1[(OTel Backend like OneUptime)]&lt;/code&gt;
    &lt;p&gt;Pros:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Centralized config: sampling, redaction, enrichment&lt;/item&gt;
      &lt;item&gt;One egress channel with batching &amp;amp; retry&lt;/item&gt;
      &lt;item&gt;Decouple app lifecycle from vendor changes&lt;/item&gt;
      &lt;item&gt;Multi-destination routing (e.g., traces, metrics, logs → OneUptime, logs → S3)&lt;/item&gt;
      &lt;item&gt;Reduce noisy telemetry before it hits priced tiers&lt;/item&gt;
      &lt;item&gt;Security boundary: no direct outbound to internet from app nodes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extra component to deploy / monitor&lt;/item&gt;
      &lt;item&gt;Potential chokepoint (must size + scale properly)&lt;/item&gt;
      &lt;item&gt;Misconfiguration can drop all telemetry&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Direct Export vs Collector: Side‑by‑Side&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Dimension&lt;/cell&gt;
        &lt;cell role="head"&gt;Direct Export&lt;/cell&gt;
        &lt;cell role="head"&gt;Collector-Based&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Setup Speed&lt;/cell&gt;
        &lt;cell&gt;Fast&lt;/cell&gt;
        &lt;cell&gt;Moderate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Policy Control (sampling/redaction)&lt;/cell&gt;
        &lt;cell&gt;Per service&lt;/cell&gt;
        &lt;cell&gt;Centralized&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Multi-backend Routing&lt;/cell&gt;
        &lt;cell&gt;Manual duplication&lt;/cell&gt;
        &lt;cell&gt;Built-in pipelines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cost Optimization&lt;/cell&gt;
        &lt;cell&gt;Hard&lt;/cell&gt;
        &lt;cell&gt;Easy (drop early)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Failure Isolation&lt;/cell&gt;
        &lt;cell&gt;Each app handles retries&lt;/cell&gt;
        &lt;cell&gt;Central queue + backpressure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Security (egress lockdown)&lt;/cell&gt;
        &lt;cell&gt;Outbound from every app&lt;/cell&gt;
        &lt;cell&gt;Single controlled egress&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Config Drift Risk&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;Low (single source)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vendor Migration&lt;/cell&gt;
        &lt;cell&gt;Painful (touch all apps)&lt;/cell&gt;
        &lt;cell&gt;Swap exporter centrally&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scaling Pressure&lt;/cell&gt;
        &lt;cell&gt;Apps bear it&lt;/cell&gt;
        &lt;cell&gt;Collector tier handles it&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Recommended For&lt;/cell&gt;
        &lt;cell&gt;Small app / POC&lt;/cell&gt;
        &lt;cell&gt;Production / multi-service&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;What the Collector Actually Does (Core Concepts)&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Receivers&lt;/cell&gt;
        &lt;cell&gt;Ingest telemetry (OTLP, Jaeger, Prometheus, Zipkin, Syslog, etc.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Processors&lt;/cell&gt;
        &lt;cell&gt;Transform / batch / sample / tail filter / memory limit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Exporters&lt;/cell&gt;
        &lt;cell&gt;Send to destinations (OTLP, Kafka, S3, logging, load balancers)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extensions&lt;/cell&gt;
        &lt;cell&gt;Auth, health check, zpages, pprof, headers, feature add-ons&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipelines&lt;/cell&gt;
        &lt;cell&gt;Declarative graphs binding receiver → processors → exporter&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A minimal example pipeline (YAML):&lt;/p&gt;
    &lt;code&gt;receivers:
  otlp:
    protocols:
      grpc:
      http:

processors:
  batch:
    send_batch_max_size: 8192
    timeout: 5s
  memory_limiter:
    limit_mib: 512
    spike_limit_mib: 128
    check_interval: 2s
  attributes/redact:
    actions:
      - key: user.email
        action: delete
  tail_sampling:
    decision_wait: 5s
    num_traces: 10000
    policies:
      - name: errors
        type: status_code
        status_code:
          status_codes: [ERROR]
      - name: latency
        type: latency
        latency:
          threshold_ms: 500

exporters:
  otlphttp_oneuptime:
    endpoint: https://oneuptime.com/otlp/v1/traces
    headers:
      x-oneuptime-token: ${ONEUPTIME_TOKEN}


service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch, tail_sampling, attributes/redact]
      exporters: [otlphttp_oneuptime]
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlphttp_oneuptime]
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch, attributes/redact]
      exporters: [otlphttp_oneuptime]
    &lt;/code&gt;
    &lt;head rend="h2"&gt;When You Definitely Need a Collector&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You want tail sampling (decide after seeing full trace) to keep 100% of errors &amp;amp; rare paths but downsample boring traffic.&lt;/item&gt;
      &lt;item&gt;You need multi-destination routing (e.g., traces, metrics, logs → OneUptime, logs → S3/ClickHouse, security events → SIEM).&lt;/item&gt;
      &lt;item&gt;You must strip sensitive PII before it leaves your network.&lt;/item&gt;
      &lt;item&gt;You need cost governance—drop chatty spans/metrics at the edge.&lt;/item&gt;
      &lt;item&gt;You want hot-swappable vendors without touching app code.&lt;/item&gt;
      &lt;item&gt;You require network isolation (no direct internet from app nodes).&lt;/item&gt;
      &lt;item&gt;You need central retries / buffering to survive outages gracefully.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;When You Can Probably Skip It (For Now)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single service + low traffic.&lt;/item&gt;
      &lt;item&gt;You only emit a few key metrics and a handful of spans.&lt;/item&gt;
      &lt;item&gt;You are experimenting locally / learning OTel basics.&lt;/item&gt;
      &lt;item&gt;You have no current need for sampling, routing, or redaction.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(But design your app setup so you can add a collector later with a one-line endpoint change. Ideally an env variable change)&lt;/p&gt;
    &lt;head rend="h2"&gt;Cost Optimization: Why the Collector Often Pays for Itself&lt;/head&gt;
    &lt;p&gt;Raw telemetry can explode: high-cardinality logs, trace spans for internal cron noise, verbose debug metrics. Sending everything directly → backend = surprise bill.&lt;/p&gt;
    &lt;p&gt;Collector lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Batch aggressively → fewer network round trips&lt;/item&gt;
      &lt;item&gt;Drop low-value spans (health checks, cache hits)&lt;/item&gt;
      &lt;item&gt;Tail sample: keep 100% of errors, maybe 10% of success&lt;/item&gt;
      &lt;item&gt;Strip high-cardinality attributes before storage&lt;/item&gt;
      &lt;item&gt;Aggregate / reduce metrics before export&lt;/item&gt;
      &lt;item&gt;Route only audit-critical logs to expensive storage; bulk to cheap object store&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Every dollar saved upstream compounds monthly. The collector is your first cost control valve.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Observing the Observer: Collector Internal Metrics&lt;/head&gt;
    &lt;p&gt;You can (and should) scrape the collector's own metrics to watch queue length, dropped spans, export latency. See our post:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to collect internal metrics from OpenTelemetry Collector?&lt;/item&gt;
      &lt;item&gt;How to increase the size of the sending queue in OpenTelemetry Collector?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Related Concepts&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Traces vs Metrics in Software Observability&lt;/item&gt;
      &lt;item&gt;Logs, Metrics &amp;amp; Traces: A Before and After Story.&lt;/item&gt;
      &lt;item&gt;How to reduce noise in OpenTelemetry?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Final Take&lt;/head&gt;
    &lt;p&gt;If observability is core to operating your system (it should be), the Collector becomes the control plane for your telemetry. Start simple, add capabilities incrementally, and let it pay for itself through cost savings, flexibility, and reliability.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Golden rule: Emit broadly at the edge, curate aggressively in the pipeline, store intentionally in the backend. The Collector is where that curation lives.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Need a production-grade backend for your Collector pipelines? OneUptime natively supports OpenTelemetry for traces, metrics, logs, and more - without vendor lock‑in.&lt;/p&gt;
    &lt;p&gt;Happy instrumenting.&lt;/p&gt;
    &lt;head rend="h3"&gt;Neel Patel&lt;/head&gt;
    &lt;p&gt;@devneelpatel • Sep 18, 2025 •&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://oneuptime.com/blog/post/2025-09-18-what-is-opentelemetry-collector-and-why-use-one/view"/><published>2025-09-18T17:29:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45292648</id><title>Learn Your Way: Reimagining Textbooks with Generative AI</title><updated>2025-09-19T02:18:52.302646+00:00</updated><content>&lt;doc fingerprint="bd71201abef387ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Learn Your Way: Reimagining textbooks with generative AI&lt;/head&gt;
    &lt;p&gt;September 16, 2025&lt;/p&gt;
    &lt;p&gt;Gal Elidan, Research Scientist, and Yael Haramaty, Senior Product Manager, Google Research&lt;/p&gt;
    &lt;p&gt;New research into GenAI in education demonstrates a novel approach to reimagining textbooks that led to improved learning outcomes in a recent study. The research comes to life in our interactive experience, Learn Your Way, now available on Google Labs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick links&lt;/head&gt;
    &lt;p&gt;Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. The manual creation of textbooks demands significant human effort, and as a result they lack alternative perspectives, multiple formats and tailored variations that can make learning more effective and engaging. At Google, we’re exploring how we can use generative AI (GenAI) to automatically generate alternative representations or personalized examples, while preserving the integrity of the source material. What if students had the power to shape their own learning journey, exploring materials using various formats that fit their evolving needs? What if we could reimagine the textbook to be as unique as every learner?&lt;/p&gt;
    &lt;p&gt;Recent advances in GenAI are bringing this vision closer to reality. Today we are excited to introduce Learn Your Way, now on Google Labs, a research experiment that explores how GenAI can transform educational materials to create a more effective, engaging, learner-driven experience for every student. Here we outline the research and pedagogy underpinning Learn Your Way, with more details in the accompanying tech report. We also report early indicators of its impact: in our efficacy study, students using Learn Your Way scored 11 percentage points higher on retention tests than students using a standard digital reader.&lt;/p&gt;
    &lt;head rend="h2"&gt;Grounded in learning, built for the student&lt;/head&gt;
    &lt;p&gt;Our approach is built on two key pillars that work together to augment the learning experience: (1) generating various multimodal representations of the content, and (2) taking foundational steps toward personalization.&lt;/p&gt;
    &lt;p&gt;The seminal dual coding theory states that forging mental connections between different representations strengthens the underlying conceptual schema in our brain. Subsequent research indeed showed that when students actively engage with information in various formats, they build a more robust and complete mental model of the material. Inspired by this, our approach empowers students with the agency to choose and intermix multiple formats and modalities to best help them understand the material. In addition, personalization is increasingly becoming an aspirational standard in K-12 educational settings, and so our research reflects this. We aim to enhance the relatability and effectiveness of educational content by adapting it to student attributes. Moreover, we incorporate quizzing capabilities that enable us to further tailor the experience according to the learners’ real-time responses. Such personalization can be a powerful method for enhancing motivation and deepening learning.&lt;/p&gt;
    &lt;p&gt;Bringing this to life involves a layered technical approach using LearnLM, our best-in-class pedagogy-infused family of models, now integrated directly into Gemini 2.5 Pro. The first layer is a unique personalization pipeline that serves as the basis for the second layer of multiple content representations. Our starting point is a textbook PDF, although our approach could be used with other forms of source material.&lt;/p&gt;
    &lt;head rend="h3"&gt;The personalization pipeline&lt;/head&gt;
    &lt;p&gt;The Learn Your Way interface asks the learner to select their grade and interests (e.g., sports, music, food). The original source material is first re-leveled to the learner’s reported grade level, while maintaining the scope of its content. This is followed by the strategic replacement of generic examples with ones that are personalized to the learner’s reported interests. The resulting text serves as the basis for the generation of all the other representations, effectively propagating the personalization effect and setting up a pipeline for further personalization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiple representations of content&lt;/head&gt;
    &lt;p&gt;Following the source personalization, we generate multiple representations of the content. For some content representations, such as mind maps and timelines, Gemini’s broad capabilities are used directly. Other features such as narrated slides, require more elaborate pipelines that weave together multiple specialized AI agents and tools to achieve an effective pedagogical result. Finally, specialized tasks, such as generating effective educational visuals, proved too challenging even for state-of-the-art general-purpose image models. To overcome this, we fine-tuned a dedicated model specifically for generating educational illustrations. The combination of a powerful base model, multi-step agentic workflows, and fine-tuned components allows us to generate a wide range of high-quality multimodal representations for learning.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Learn Your Way experience&lt;/head&gt;
    &lt;p&gt;Our research comes to life in Learn Your Way. The interface brings together multiple, personalized representations of content including: (1) immersive text, (2) section-level quizzes, (3) slides &amp;amp; narration, (4) audio lessons, and (5) mind maps.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Immersive text: Breaks the content up into digestible sections that are augmented with generated images and embedded questions. Put together, these transform passive reading into an active multimodal experience that follows learning science principles.&lt;/item&gt;
      &lt;item&gt;Section-level quizzes: Promote active learning by allowing a user to interactively assess their learning, and uncover existing knowledge gaps.&lt;/item&gt;
      &lt;item&gt;Slides &amp;amp; narration: Offers presentations that span the entire source material and include engaging activities like fill-in-the-blanks, as well as a narrated version, mimicking a recorded lesson.&lt;/item&gt;
      &lt;item&gt;Audio lesson: Provides simulated conversations, coupled with visual aids, between an AI-powered teacher and a student that models how a real learner might engage with the material, including the expression of misconceptions, which are clarified by the teacher.&lt;/item&gt;
      &lt;item&gt;Mind map: Organizes the knowledge hierarchically and allows learners to zoom in and out from the big picture to the details.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The above representations give learners choice and are all adapted to their selected grade level and personal interests. Throughout the experience, the interactive quizzes provide dynamic feedback, guiding students to revisit specific content areas where they struggled. This marks our first steps towards true personalization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pedagogical evaluation&lt;/head&gt;
    &lt;p&gt;To evaluate Learn You Way's pedagogical performance, we transformed ten varied source materials from OpenStax (a provider of free educational textbooks) to three different personalization settings. The source materials covered various subjects from history to physics. Three pedagogical subject matter experts then evaluated the transformed materials using pedagogical criteria, such as accuracy, coverage, and the LearnLM learning science principles.&lt;/p&gt;
    &lt;p&gt;The results were highly positive, with an average expert rating of 0.85 or higher across all pedagogical criteria. See the tech report for more evaluation details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Efficacy study&lt;/head&gt;
    &lt;p&gt;An AI-powered learning tool is only valuable if it both effectively improves learning outcomes and students want to use it. Learn Your Way now serves as a research platform for us to conduct studies with partners around the world to explore how AI-powered transformations and personalization affects outcomes, and to ensure that what we build is effective and locally relevant.&lt;/p&gt;
    &lt;p&gt;Recently, we conducted a randomized controlled study with 60 students from the Chicago area, ages 15–18 and with similar reading levels. Participants were given up to 40 minutes to learn about adolescent brain development from a textbook, and randomly assigned to learn using Learn Your Way or a traditional digital PDF reader.&lt;/p&gt;
    &lt;p&gt;We assessed students with a quiz immediately after the study session, and with a retention test 3–5 days later, using assessments designed by pedagogical experts to be a good measure of content comprehension. We also surveyed them about the learning experience, and to gain deeper insights beyond these quantitative metrics, each student participated in a 30-minute qualitative interview where they could share more nuanced feedback about their experience.&lt;/p&gt;
    &lt;p&gt;The results were compelling and statistically significant. Here are the highlights. See the tech report for more details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Positive learning outcomes: The Learn Your Way group scored, on average, 9% higher on the immediate assessment following the study session.&lt;/item&gt;
      &lt;item&gt;Better long-term retention: Similarly, the Learn Your Way group scored 11% higher on the retention assessment 3-5 days later (78% vs. 67%).&lt;/item&gt;
      &lt;item&gt;Positive user sentiment: 100% of students who used Learn Your Way reported that they felt the tool made them more comfortable taking the assessment, compared to 70% in the digital reader control group. 93% said they would want to use Learn Your Way for future learning, compared to just 67% for the digital reader.&lt;/item&gt;
      &lt;item&gt;Valuable experience: Insights from the qualitative interviews revealed that students found great value in Learn Your Way.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Experience Learn Your Way&lt;/head&gt;
    &lt;p&gt;To give a concrete feel for the Learn Your Way interactive experience, today we are releasing example experiences on Google Labs, including:&lt;/p&gt;
    &lt;head rend="h2"&gt;The path forward&lt;/head&gt;
    &lt;p&gt;Our findings suggest that generative AI can be used to build learning experiences that are not only more effective but also more empowering. By evolving the static textbook into an interactive artifact and giving students greater agency over how they learn, we saw learning retention improve.&lt;/p&gt;
    &lt;p&gt;This work is just the beginning of our exploration. We envision many more ways to tailor content, moving towards systems that continuously adapt to each learner's unique needs and progress. As we take our next steps towards personalized education, we will continue to ground our research in pedagogical principles, measuring the impact of AI on learning efficacy, so that in the future every student might have access to a high-quality, engaging learning experience that is custom built for them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Shout out to our Google Research LearnLM team who have contributed to this work: Alicia Martín, Amir Globerson, Amy Wang, Anirudh Shekhawat, Anisha Choudhury, Anna Iurchenko, Avinatan Hassidim, Ayça Çakmakli, Ayelet Shasha Evron, Charlie Yang, Courtney Heldreth, Dana Oria, Diana Akrong, Hairong Mu, Ian Li, Ido Cohen, Komal Singh, Lev Borovoi, Lidan Hackmon, Lior Belinsky, Michael Fink, Preeti Singh, Rena Levitt, Shashank Agarwal, Shay Sharon, Sophie Allweis, Tracey Lee-Joe, Xiaohong Hao, Yael Gold-Zamir, Yishay Mor, and Yoav Bar Sinai. Special thanks to our executive champions: Niv Efron, Avinatan Hassidim, Yossi Matias and Ben Gomes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/"/><published>2025-09-18T17:42:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45292694</id><title>This map is not upside down</title><updated>2025-09-19T02:18:52.042733+00:00</updated><content>&lt;doc fingerprint="f00ca5471275de50"&gt;
  &lt;main&gt;
    &lt;p&gt;If you were to close your eyes and picture a map of the world, chances are you would imagine a conventional image: North America (including Greenland) and Europe at the top, Africa somewhere in the middle, and South America, Australia, and Antarctica at the bottom. But it does not have to be so, and it was not always so.&lt;/p&gt;
    &lt;p&gt;It’s not that our planet underwent change—it did, of course, but long before maps. Our conventions as map readers and makers have converged over time to a north-up default. What happens when that convention is challenged? Robert Simmon, a cartographer who previously developed maps for Planet Labs and NASA Earth Observatory, designed a map to address that question.&lt;/p&gt;
    &lt;p&gt;Simmon’s map includes countries, major lakes, oceans, gulfs, seas, roads, and cities. It also features inset maps depicting Earth’s biosphere, global land cover, and bathymetry. All of these are familiar to most map readers. Yet the map may seem disorienting to many. Simmon’s map is geographically correct, yet purposely—and literally—turns convention on its head. What was once familiar becomes alien, challenging readers to look at Earth anew. It also encourages us to think more deeply about such conventions: Why is north almost always at the top of maps? And must it always be that way? Simmon’s map reminds us that it doesn’t have to be.&lt;/p&gt;
    &lt;p&gt;Simmon is not the first to create a south-up map. Others have made these maps to challenge conventions and norms. And it was only rather recently that north-up maps became so commonplace. Centuries ago, cartographers drew maps with the top being south, east, or other orientations. These alternatives reflected the limited tools, knowledge, and practices of the time. The basic idea behind a compass (which most today recognize as a device that points north) was known to the Han and Tang dynasties of China more than 2,000 years ago. Early Chinese navigators used magnetized devices as a compass, but for them south was the dominant position from which bearings were derived.&lt;/p&gt;
    &lt;p&gt;Deciding to put south, or north, at the top of maps is a decision of consequence. Psychologically, we tend to view things nearer the top as ‘good’ and those lower as ‘bad.’ This can influence our interpretation of maps at both global and local scales. Still, the prominence of north-up maps did not arrive deliberately to elevate the status of some areas or their rulers. It is in part a consequence of the work of Ptolemey, who first labeled his maps with calculated lines of latitude and longitude. This made it easy for others to copy, extend, and derive new maps. Each map drawn in this way would adopt Ptolemy’s orientation.&lt;/p&gt;
    &lt;p&gt;Regardless of the reasons for a given orientation and the implications that follow, Simmon’s map reminds us to challenge tradition and consider its influence. As both a map and a philosophical prompt, this example hits the mark beautifully.&lt;/p&gt;
    &lt;head rend="h2"&gt;About This Map&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Title&lt;/item&gt;
      &lt;item rend="dd-1"&gt;The World, South Up&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Creator&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Robert simmon&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Data Sources&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Natural Earth, GEBCO&lt;/item&gt;
          &lt;item&gt;NASA/USGS MODIS Land Cover Classification&lt;/item&gt;
          &lt;item&gt;NOAA VIIRS NDVI&lt;/item&gt;
          &lt;item&gt;NOAA VIIRS Ocean Color&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This map was contributed through the Maps.com submission program. If you’d like your map to be featured, submit it for consideration.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Tags&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.maps.com/this-map-is-not-upside-down/"/><published>2025-09-18T17:47:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45293273</id><title>When Knowing Someone at Meta Is the Only Way to Break Out of "Content Jail"</title><updated>2025-09-19T02:18:51.233324+00:00</updated><content>&lt;doc fingerprint="2cb6989026e20473"&gt;
  &lt;main&gt;
    &lt;p&gt;BY RINDALA ALAJAJI | September 17, 2025&lt;/p&gt;
    &lt;p&gt;This is the second instalment in a ten-part blog series documenting EFF's findings from the Stop Censoring Abortion campaign. You can read additional posts here.&lt;/p&gt;
    &lt;p&gt;During our Stop Censoring Abortion campaign, we set out to collect and spotlight the growing number of stories from people and organizations that have had abortion-related content removed, suppressed, or flagged by dominant social media platforms. Our survey submissions have revealed some alarming trends, including: if you don’t have a personal or second-degree connection at Meta, your chances of restoring your content or account are likely to drop significantly.&lt;/p&gt;
    &lt;p&gt;Through the survey, we heard from activists, clinics, and researchers whose accounts were suspended or permanently removed for allegedly violating Meta’s policies on promoting or selling “restricted goods,” even when their posts were purely educational or informational. What the submissions also showed is a pattern of overenforcement, lack of transparency, and arbitrary moderation decisions that have specifically affected reproductive health and reproductive justice advocates.&lt;/p&gt;
    &lt;p&gt;When accounts are taken down, appeals can take days, weeks, or even months (if they're even resolved at all, or if users are even given the option to appeal). For organizations and providers, this means losing access to vital communication tools and being cut off from the communities they serve. This is highly damaging since so much of that interaction happens on Meta’s platforms. Yet we saw a disturbing pattern emerge in our survey: on several occasions, accounts are swiftly restored once someone with a connection to Meta intervenes.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Case Studies: An Abortion Clinic&lt;/head&gt;
    &lt;p&gt;The Red River Women's Clinic is an abortion clinic in Moorhead, MN. It was originally located in Fargo, North Dakota, and for many years was the only abortion clinic in North Dakota. In early January, the clinic’s director heard from a patient that she thought they only offered procedural/surgical abortions and not medication abortion. To clarify for other patients, they posted on the clinic’s page that they offered both procedural and medication abortions—attaching an image of a box of mifepristone. When they tried to boost the post, the ad was flagged and their account was suspended.&lt;/p&gt;
    &lt;p&gt;They appealed the decision and initially got the ad approved, yet the page was suspended again shortly after. But this time, multiple appeals and direct emails went unanswered, until they reached out to a digital rights organization that was able to connect with staff at Meta that stepped in. Only then was their page restored, with Meta noting that their post did not violate the policies but warning that future violations could lead to permanent removal.&lt;/p&gt;
    &lt;p&gt;While this may have been a glitch in Meta’s systems or a misapplication of policy, the suspension of the clinic’s Facebook account was detrimental for them. “We were unable to update our followers about dates/times we were closed, we were unable to share important information and news about abortion that would have kept our followers up to date, there was a legislative session happening and we were unable to share events and timely asks for reaching out to legislators about issues,” shared Tammi Kromenaker, Director of Red River Women's Clinic. The clinic was also prevented from starting an Instagram page due to the suspension. “Facebook has a certain audience and Instagram has another audience,” said Kromenaker, “we are trying to cater to all of our supporters so the loss of FB and the inability to access and start an Instagram account were really troubling to us.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Case Studies: RISE at Emory University&lt;/head&gt;
    &lt;p&gt;RISE, a reproductive health research center at Emory University, launched an Instagram account to share community-centered research and combat misinformation related to reproductive health. In January of this year, they posted educational content about mifepristone on their instagram. “Let's talk about Mifepristone + its uses + the importance of access”, read the post. Two months later, their account was suddenly suspended, flagging the account under its policy against selling illegal drugs. Their appeal was denied, which led to the account being permanently deleted.&lt;/p&gt;
    &lt;p&gt;“As a team, this was a hit to our morale” shared Sara Redd, Director of Research Translation at RISE. “We pour countless hours of person-power, creativity, and passion into creating the content we have on our page, and having it vanish virtually overnight took a toll on our team.” For many organizational users like RISE, their social media accounts are a repository for resources and metrics that may not be stored elsewhere. “We spent a significant amount of already-constrained team capacity attempting to recover all of the content we’d created for Instagram that was potentially going to be permanently lost. [...] We also spent a significant amount of time and energy trying to understand what options we might have available from Meta to appeal our case and/or recover our account; their support options are not easily accessible, and the time it took to navigate this issue distracted from our existing work.”&lt;/p&gt;
    &lt;p&gt;Meta restored the account only after RISE was able to connect with someone there. Once RISE logged back in, they confirmed that the flagged post was the one about mifepristone. The post never sold or directed people where to buy pills, it simply provided accurate information about the use and efficacy of the drug.&lt;/p&gt;
    &lt;head rend="h2"&gt;This Shouldn’t Be How Content Moderation Works&lt;/head&gt;
    &lt;p&gt;Meta spokespersons have admitted to instances of “overenforcement” in various press statements, noting that content is sometimes incorrectly removed or blurred even when it doesn’t actually violate policy. Meta has insisted to the public that they care about free speech, as a spokesperson mentioned to The New York Times: “We want our platforms to be a place where people can access reliable information about health services, advertisers can promote health services and everyone can discuss and debate public policies in this space [...] That’s why we allow posts and ads about, discussing and debating abortion.” In fact, their platform policies directly mention this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that advertisers don’t need authorization to run ads that only:&lt;/p&gt;
      &lt;item&gt;Educate, advocate or give public service announcements related to prescription drugs&lt;/item&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Debating or advocating for the legality or discussing scientific or medical merits of prescription drugs is allowed. This includes news and public service announcements.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Meta also has policies specific to “Health and Wellness,” where they state:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When targeting people 18 years or older, advertisers can run ads that:&lt;/p&gt;
      &lt;item&gt;Promote sexual and reproductive health and wellness products or services, as long as the focus is on health and the medical efficacy of the product or the service and not on the sexual pleasure or enhancement. And these ads must target people 18 years or older. This includes ads for: [...]&lt;/item&gt;
      &lt;item&gt;Family planning methods, such as:&lt;/item&gt;
      &lt;item&gt;Family planning clinics&lt;/item&gt;
      &lt;item&gt;In Vitro Fertilization (IVF) or any other artificial insemination procedures&lt;/item&gt;
      &lt;item&gt;Fertility awareness&lt;/item&gt;
      &lt;item&gt;Abortion medical consultation and related services&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;But these public commitments don’t always match users’ experiences.&lt;/p&gt;
    &lt;p&gt;Take the widely covered case of Aid Access, a group that provides medication abortion by mail. This year, several of their Instagram posts were blurred and removed on Instagram, including one with tips for feeling safe and supported at home after taking abortion medication. But only after multiple national media outlets contacted Meta for comment on the story were the posts and account restored.&lt;/p&gt;
    &lt;p&gt;So the question becomes: If Meta admits its enforcement isn’t perfect, why does it still take knowing someone, or having the media involved, to get a fair review? When companies like Meta claim to uphold commitments to free speech, those commitments should materialize in clear policies that are enforced equally, not only when it is escalated through leveraging relationships with Meta personnel.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Facebook Jail” Reform&lt;/head&gt;
    &lt;p&gt;There is no question that the enforcement of these content moderation policies on Meta platforms and the length of time people are spending in “content jail” or “Facebook/Instagram jail” has created a chilling effect.&lt;/p&gt;
    &lt;p&gt;“I think that I am more cautious and aware that the 6.1K followers we have built up over time could be taken away at any time based on the whims of Meta,” Tammi from Red River Women’s Clinic told us.&lt;/p&gt;
    &lt;p&gt;RISE sees it in a slightly different light, sharing that “[w]hile this experience has not affected our fundamental values and commitment to sharing our work and rigorous science, it has highlighted for us that no information posted on a third-party platform is entirely one’s own, and thus can be dismantled at any moment.”&lt;/p&gt;
    &lt;p&gt;At the end of the day, clinics are left afraid to post basic information, patients are left confused or misinformed, and researchers lose access to their audiences. But unless your issue catches the attention of a journalist or you know someone at Meta, you might never regain access to your account.&lt;/p&gt;
    &lt;p&gt;These case studies highlight the urgent need for transparent, equitable, and timely enforcement that is not dependent on insider connections, as well as accountability from platforms that claim to support open dialogue and free speech. Meta’s admitted overenforcement should, at minimum, be coupled with efficient and well-staffed review processes and policies that are transparent and easily understandable.&lt;/p&gt;
    &lt;p&gt;It’s time for Meta and other social media platforms to implement the reforms they claim to support, and for them to prove that protecting access to vital health information doesn’t hinge on who you know.&lt;/p&gt;
    &lt;p&gt;This is the second post in our blog series documenting the findings from our Stop Censoring Abortion campaign. Read more in the series: https://www.eff.org/pages/stop-censoring-abortion&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/pages/when-knowing-someone-meta-only-way-break-out-content-jail"/><published>2025-09-18T18:30:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45293839</id><title>Tldraw SDK 4.0</title><updated>2025-09-19T02:18:51.037227+00:00</updated><content>&lt;doc fingerprint="3d82e9415ca29470"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing tldraw SDK 4.0&lt;/head&gt;
    &lt;p&gt;New starter kits, accessibility features, and license changes for tldraw's fourth major version.&lt;/p&gt;
    &lt;p&gt;Hey, this is Steve Ruiz, founder of tldraw.&lt;/p&gt;
    &lt;p&gt;For anyone new to the project, hello, weâre a London-based startup behind the tldraw SDK, a TypeScript library for building infinite canvas apps on the web. In addition to building the SDK, we also build things with the SDK, including a free online whiteboard at tldraw.com and plenty of side projects like tldraw.computer.&lt;/p&gt;
    &lt;p&gt;Like our previous major releases, this release includes changes to our license together with new resources for developers. It also includes the usual collection of small improvements and fixes to the SDK. You can find these details in the full release notes.&lt;/p&gt;
    &lt;p&gt;On with the highlights!&lt;/p&gt;
    &lt;head rend="h2"&gt;npm create tldraw&lt;/head&gt;
    &lt;p&gt;This release includes our new CLI tool, available at &lt;code&gt;npm create tldraw@latest&lt;/code&gt;. You can use this tool to quickly create new tldraw projects using our templates and new starter kits.&lt;/p&gt;
    &lt;head rend="h2"&gt;Starter kits&lt;/head&gt;
    &lt;p&gt;This release includes four new starter kits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;agent is a Cursor-style chatbot starter. This kit replaces our AI module and AI template. If you're interested in driving AI interactions on the canvas, this is a great place to start hacking.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;workflow is a React-flow style starter for node-and-wire applications. If you have an idea for a patch programming interface, an asynchronous workflow tool, or a ComfyUI-style pipeline for images or data, start with this one.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;branching chat is a starter kit for branching AI chats, inspired by tweets by Max Lee and Jacob Colling. Run with it!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;chat is a starter kit where the tldraw canvas is used to create and annotate images. If you're working on an app that features a chatbot (who isn't?) then give this one a spin and try out some ideas.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;multiplayer is clean starter kit featuring our tldraw sync multiplayer backend. Build a multiplayer whiteboard, game, or bypass your school's chat app restrictions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The starter kits are all MIT licensed, so go ahead and build with them as you like. Learn more on the tldraw docs or get started by running &lt;code&gt;npm create tldraw&lt;/code&gt; in your terminal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Licensing changes&lt;/head&gt;
    &lt;p&gt;Our 4.0 release includes a new license with changes to where tldraw can be used. Fate and capital both demand that tldraw be a sustainable project, so these changes are designed to help us commercialize the SDK without cutting off community adoption.&lt;/p&gt;
    &lt;p&gt;Under the terms of the new license, the tldraw SDK is only permitted to be used in development environments. To use the SDK in production environments, you must have either a trial license, a commercial license, or a hobby license. This policy is enforced through the license keys that come with each license. The SDK will only work in production when it has a valid license key.&lt;/p&gt;
    &lt;p&gt;If you are only using tldraw in localhost or development environments, then you do not need a license. If you wish to use the tldraw SDK in production, you can get a free 100-day trial license.&lt;/p&gt;
    &lt;p&gt;You can read more about our licensing here.&lt;/p&gt;
    &lt;p&gt;As a special offer for teams who are considering a commercial license: until the end of 2025, if you sign a one-year commercial agreement with tldraw, we will discount the cost of the license by the amount of time left on your free trial. 60 days left on your trial? Thatâs a cool 16%. Get started here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accessibility features&lt;/head&gt;
    &lt;p&gt;Over the last few months, weâve quietly worked to make the tldraw SDK more accessible. With the help of Sarah Fossheim, we were able to complete enough features, improvements, and small fixes to become compliant with the WCAG 2.2 AA accessibility standard. We will soon be publicly providing our VPAT compliance document which will provide further details.&lt;/p&gt;
    &lt;p&gt;The SDKâs improved accessibility should be a great benefit to downstream users and customers with their own accessibility goals, and who will inherit these improvements just by upgrading to 4.0. I also think it is a huge win for canvas experiences overall, with our improvements reaching thousands of applications and millions of downstream users.&lt;/p&gt;
    &lt;p&gt;If you havenât checked in with tldraw recently, I encourage you to give it a spin through our new &lt;code&gt;npm create tldraw@latest&lt;/code&gt; and starter kits, or else by reading about the project on the new tldraw.dev. This year weâve opened 2000 pull requests in the tldraw repository alone, reached 40,000 GitHub Stars, brought our weekly installs above 70,000, grew our Discord channel to 8,000 members, and announced a $10M series A led by Lux Ventures and Definition Capital. We're always hiring for product, engineering, and product engineering.&lt;/p&gt;
    &lt;p&gt;Get in! And if you build something incredible with our new starter kits, please let us know.&lt;/p&gt;
    &lt;p&gt;Â© 2025 tldraw&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tldraw.dev/blog/tldraw-sdk-4-0"/><published>2025-09-18T19:21:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45294440</id><title>Apple: SSH and FileVault</title><updated>2025-09-19T02:18:50.959874+00:00</updated><content>&lt;doc fingerprint="201543195f03b8c1"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;apple_ssh_and_filevault(7)&lt;/cell&gt;
        &lt;cell&gt;Miscellaneous Information Manual&lt;/cell&gt;
        &lt;cell&gt;apple_ssh_and_filevault(7)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;apple_ssh_and_filevault&lt;/code&gt; —
    SSH and FileVault&lt;/p&gt;
    &lt;p&gt;When FileVault is enabled, the data volume is locked and unavailable during and after booting, until an account has been authenticated using a password. The macOS version of OpenSSH stores all of its configuration files, both system-wide and per-account, in the data volume. Therefore, the usually configured authentication methods and shell access are not available during this time. However, when Remote Login is enabled, it is possible to perform password authentication using SSH even in this situation. This can be used to unlock the data volume remotely over the network. However, it does not immediately permit an SSH session. Instead, once the data volume has been unlocked using this method, macOS will disconnect SSH briefly while it completes mounting the data volume and starting the remaining services dependent on it. Thereafter, SSH (and other enabled services) are fully available.&lt;/p&gt;
    &lt;p&gt;The capability to unlock the data volume over SSH appeared in macOS 26 Tahoe.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;1 July, 2025&lt;/cell&gt;
        &lt;cell&gt;Darwin&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://keith.github.io/xcode-man-pages/apple_ssh_and_filevault.7.html"/><published>2025-09-18T20:15:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45294859</id><title>Meta’s live demo fails; “AI” recording plays before the actor takes the steps</title><updated>2025-09-19T02:18:50.860560+00:00</updated><content/><link href="https://www.reddit.com/r/LivestreamFail/comments/1nkbig7/metas_live_staged_demo_fails_the_ai_recording/"/><published>2025-09-18T20:50:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45295482</id><title>Show HN: I created a small 2D game about an ant</title><updated>2025-09-19T02:18:50.705745+00:00</updated><content>&lt;doc fingerprint="5e76fafdfc639a2e"&gt;
  &lt;main&gt;
    &lt;p&gt;Use the arrow keys to collect all the apples! Help the ant get fed!&lt;/p&gt;
    &lt;p&gt;Game repository: https://github.com/aanthonymax/ant-and-apples&lt;/p&gt;
    &lt;p&gt;Made partially with hmpl-js out of boredom in a couple of days :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aanthonymax.github.io/ant-and-apples/"/><published>2025-09-18T21:54:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45295794</id><title>AI tools are making the world look weird</title><updated>2025-09-19T02:18:47.387474+00:00</updated><content>&lt;doc fingerprint="7adbb9070d4782bc"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;The images arrive already complete; there is no communication between them and myself, no reciprocal exchange. As much as we like to say that the world is opening up to us, since we can see every part of it, we can also say that the world is closing itself off – in all its openness.&lt;/p&gt;Karl Ove Knausgård&lt;/quote&gt;
    &lt;p&gt;In academia and the media, AI is often described as mirroring human psychology with humanlike reasoning, human-level performance, human-like communication. In these comparisons, “humans” are treated as the benchmark.&lt;/p&gt;
    &lt;p&gt;In a provocative 2023 paper, researchers at Harvard University asked – which humans?&lt;/p&gt;
    &lt;p&gt;The diversity of human psychologies has been a hot topic since 2010, when researchers found that many accepted psychological “truths” were often confined to so-called “WEIRD people”: Western, Educated, Industrialised, Rich, Democratic. What feel like universal beliefs for people like me and no doubt many of the readers of this blog, e.g. that I am an automonous individual, are instead only true for a thin slice of humanity.&lt;/p&gt;
    &lt;p&gt;So when we say AI tools are “human-like”, what we mean is that AI is WEIRD.&lt;/p&gt;
    &lt;p&gt;In fact, this paper found that more than that, it thinks American. The greater the cultural distance between a country and the USA, the less accurate ChatGPT got at simulating peoples’ values. For countries like Libya and Pakistan, AI results are little better than a coin toss.&lt;/p&gt;
    &lt;p&gt;This paper showed this through an ingenious method – administering the World Values Survey (WVS) 1,000 times to ChatGPT and then comparing it to real data from other countries. The WVS measures everything from self-reported cultural values to moral principles, attitudes towards family, religion, poverty and so on.[1]&lt;lb/&gt;In this simple chart, they plotted two variables:&lt;/p&gt;
    &lt;p&gt;The eagle-eyed reader may note that ChatGPT responses are slightly more correlated to smaller Western countries such as New Zealand than the US. This likely reflects the USA’s greater cultural diversity, and the fact that ChatGPT was developed in California.&lt;/p&gt;
    &lt;p&gt;Marketers and researchers in non-WEIRD countries have often struggled for budget and bandwidth. AI tools’ poorer accuracy in their markets therefore introduce a double jeopardy: the non-WEIRD countries least likely to secure research budgets also have the worst accuracy from “off the shelf” AI tools.&lt;/p&gt;
    &lt;p&gt;These biases could show up throughout the research process, for example:&lt;/p&gt;
    &lt;p&gt;There is a real risk that increasing use of AI tools in international research will flatten out and devalue insights, as highly diverse peoples’ individual spoken and unspoken responses are fed into text-processing machines and emerge looking and sounding vaguely Californian. What looks like a living, breathing forest to us may end up being processed as just so much wood.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean we should ignore AI tools when working cross-culturally. They’re simply too useful. Instead, I’d suggest that we need to invest in the cultural fitness of our thinking and processes. Just as you don’t have to lose physical stamina when you start driving a car, your projects do not have to atrophy cultural meaning when you introduce elements of automation.&lt;/p&gt;
    &lt;p&gt;As researchers we can deepen the cultural layer in our international work by:&lt;/p&gt;
    &lt;p&gt;And as AI product owners or users, it’s more a question of how we minimise the loss of cultural meaning. I remain to be convinced that AI moderation will achieve that for as long as the “moderator” is powered by an American LLM with deep-coded cultural biases. When using AI for analysis or to design research, however, there is likely to be marginal value in:&lt;/p&gt;
    &lt;p&gt;LLMs process information in a WEIRD way, are psychologically WEIRD, and assume the average human is too. At the same time, the LLM space is getting ever more concentrated with US companies dominating. There is a real risk that as researchers we could get pulled into ways of working and thinking that make the world feel smaller, and much less wondrous. We must build our cultural fitness to ensure that we can introduce automation without losing sight of the many ways of being human. &lt;lb/&gt;Here at STRAT7 (and for me personally), we want to explore further:&lt;/p&gt;
    &lt;p&gt;We’re going to conduct a few experiments of our own with LLMs from different continents – comment below if you have any thoughts on what to look out for.&lt;/p&gt;
    &lt;p&gt;[1] Separately, I recommend checking out the WVS website – it is old school but has some great data and insights.&lt;/p&gt;
    &lt;p&gt;[2] The researchers released an interactive tool to compare cultural distance across a range of dimensions, worth a play around&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://strat7.com/blogs/weird-in-weird-out/"/><published>2025-09-18T22:27:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45295898</id><title>Want to piss off your IT department? Are the links not malicious looking enough?</title><updated>2025-09-19T02:18:47.280817+00:00</updated><content>&lt;doc fingerprint="1ee9dc214d0432b1"&gt;
  &lt;main&gt;
    &lt;p&gt;This tool is guaranteed to help with that!&lt;/p&gt;
    &lt;p&gt;This is a tool that takes any link and makes it look malicious. It works on the idea of a redirect. Much like https://tinyurl.com/ for example. Where tinyurl makes an url shorter, this site makes it look malicious.&lt;/p&gt;
    &lt;p&gt;Place any link in the below input, press the button and get back a fishy(phishy, heh...get, it?) looking link. The fishy link doesn't actually do anything, it will just redirect you to the original link you provided.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://phishyurl.com/"/><published>2025-09-18T22:40:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45295995</id><title>Classic recessive-or-dominant gene dynamics may not be so simple</title><updated>2025-09-19T02:18:46.581987+00:00</updated><content>&lt;doc fingerprint="7f8890ad7de4e315"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;In brief&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A new Stanford study explores how fruit fly populations maintain genetic diversity amid changing environments, which is crucial for survival against future challenges.&lt;/item&gt;
      &lt;item&gt;The research provides direct evidence to support the theory of “dominance reversal” in genetics.&lt;/item&gt;
      &lt;item&gt;Findings indicate that genetic variants can act as dominant or recessive based on environmental conditions – which gives the flies long-term pesticide resistance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Populations live in rapidly changing environments – droughts come and go, food sources change, human activities reshape habitats. For scientists, this raises a fundamental puzzle: How do populations maintain the genetic diversity needed to survive future challenges when natural selection should eliminate variants that aren’t useful for long periods?&lt;/p&gt;
    &lt;p&gt;Researchers at Stanford have addressed this puzzle by tracking the evolution of fruit fly populations in an outdoor orchard where they controlled pesticide exposure over time, and paired experiments with mathematical modeling. Their new paper, published Sept. 15 in Nature Ecology and Evolution, offers the first direct evidence to support the theory of “dominance reversal” in a changing environment over time.&lt;/p&gt;
    &lt;p&gt;Classically, we think of genetic variants (alleles) as strictly dominant or recessive – dominant alleles dominate expression of traits and recessive alleles are only outwardly expressed if there isn’t a dominant allele around. In the case of dominance reversal, the same genetic variant is dominant when helpful (providing the flies with resistance in pesticide-rich environments) but recessive when harmful (reducing fitness in pesticide-free environments).&lt;/p&gt;
    &lt;p&gt;“Let’s say you haven’t used pesticides for 20 years. The moment you add pesticides again, they’ll rapidly respond and resist them,” said senior author Dmitri Petrov, professor of biology in the School of Humanities and Sciences (H&amp;amp;S). “It’s like the flies have a hidden shield. When they don’t need it, it’s not in their way. But it’s ready as soon as they are threatened.”&lt;/p&gt;
    &lt;p&gt;The researchers suggest this mechanism may be widespread in nature, helping maintain genetic diversity for different environmental challenges that change over time. “What we’re seeing could be a general mechanism for populations to hold on to genetic variants they might need for future environmental shifts,” said Marianthi Karageorgi, who is the lead author and a research scientist in the Petrov Lab.&lt;/p&gt;
    &lt;p&gt;“For example, synthetic insecticides are often analogs of plant chemical defenses,” Karageorgi added. “So, this mechanism could have been operating in nature for millions of years – helping insects maintain resistance to chemical defenses that vary seasonally with host plant availability.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Evolution in an experimental orchard&lt;/head&gt;
    &lt;p&gt;Since the 1950s, population geneticists have proposed that dominance reversal could help maintain genetic variation in changing environments over time, but until now there was no way to test if it really happens in nature. The researchers’ findings relied on a combination of surveys, lab experiments, field experiments, and mathematical models.&lt;/p&gt;
    &lt;p&gt;Before experiments began, the researchers analyzed genetic surveys of flies across the world in different environments, including organic farms. Then, using flies bred by collaborator and co-senior author Paul Schmidt at the University of Pennsylvania, they ran lab experiments to assess how the different genetic variations affect the fitness of the flies with and without pesticide exposure.&lt;/p&gt;
    &lt;quote&gt;It’s like the flies have a hidden shield. When they don’t need it, it’s not in their way. But it’s ready as soon as they are threatened.Dmitri PetrovProfessor of Biology&lt;/quote&gt;
    &lt;p&gt;This work confirmed that pesticide-resistance alleles persist at intermediate frequencies over space and time. It also showed that, when the pesticide-resistant allele is dominant without the presence of pesticides, it negatively affects survival and reproduction. All of this supported the possibility of dominance reversal.&lt;/p&gt;
    &lt;p&gt;To test this hypothesis outside the lab, the team used experimental evolution in an outdoor orchard developed by Schmidt. In this setup, large fruit fly populations evolved from early summer to late fall under near-natural conditions, in a large outdoor enclosure with a single peach tree to provide shade. One set of cages was exposed to a pesticide pulse mimicking seasonal insecticide use, while another set remained untreated. Every two generations, the researchers sampled flies from each cage, tracking both pesticide resistance and genome-wide frequencies of gene variants in real time.&lt;/p&gt;
    &lt;p&gt;In treated cages, populations rapidly responded to the pesticide pulse: Resistance and the associated resistance allele rose sharply with pesticide use and then gradually declined once exposure stopped. But the untreated cages revealed a surprising result.&lt;/p&gt;
    &lt;p&gt;“When we got results about the untreated cages, we saw that over an extended period of time, both resistant and non-resistant genetic variants were maintained, which was puzzling,” said Karageorgi. “If there is a cost associated with resistance, why doesn’t resistance drop over time, and why don’t the resistance alleles drop?”&lt;/p&gt;
    &lt;p&gt;Mathematical modeling of allele frequencies in the treated and untreated cages confirmed that dominance reversal was at work. Resistance alleles acted dominant when beneficial in the presence of pesticides but recessive when costly in their absence, allowing the alleles to persist even without pesticide pressure.&lt;/p&gt;
    &lt;p&gt;In other words, the alleles are not permanently dominant or recessive recessive in terms of their effects on fitness of the individuals but can function as either, depending on the environment. This flexibility allows the pesticide-resistant alleles to quickly provide high levels of resistance when needed, yet hide from natural selection when their presence would be harmful.&lt;/p&gt;
    &lt;head rend="h2"&gt;Like an earthquake&lt;/head&gt;
    &lt;p&gt;The researchers also looked beyond the exact locations on the chromosome where the pesticide-resistance alleles reside. It’s known that oftentimes evolutionary changes in one place on a chromosome can cause something of a ripple effect, known as a selective sweep, because alleles at different loci on the same chromosome are physically linked.&lt;/p&gt;
    &lt;p&gt;“When we applied pesticides, we didn't just change allele frequencies at the resistance locus – we affected loci all across the chromosome, which danced to the pesticide pulse, increasing and then decreasing in frequency,” said Karageorgi.&lt;/p&gt;
    &lt;p&gt;“The effects of this reversal are global and very short-lived,” said Petrov. “It’s a little bit like an earthquake – as if buildings fell in Chicago and we feel the shaking here, and then suddenly in both places it’s calm again.”&lt;/p&gt;
    &lt;p&gt;While these non-local effects are expected from theory, the idea of dominance reversal facilitating these effects raises foundational questions about how strong selective natural and anthropogenic pressures impact genomic diversity in changing environments over time. It suggests they might, in some cases, even set the levels of genetic diversity in natural populations.&lt;/p&gt;
    &lt;p&gt;“This field is trying to understand what forces are involved in evolution, how you measure them, and how much of an effect they have. But often these forces are hidden from us,” said Petrov. “So, the big question for us continues to be: How do we wrestle that knowledge from recalcitrant nature?”&lt;/p&gt;
    &lt;head rend="h2"&gt;For more information&lt;/head&gt;
    &lt;p&gt;Petrov is the Michelle and Kevin Douglas Professor in H&amp;amp;S, a member of Stanford Bio-X, the Maternal &amp;amp; Child Health Research Institute (MCHRI), and the Stanford Cancer Institute, and an affiliate of the Stanford Woods Institute for the Environment. Schmidt is the Patricia M. Williams Term Professor of Biology at the University of Pennsylvania.&lt;/p&gt;
    &lt;p&gt;Additional Stanford co-authors include graduate students Anastasia Lyulina, Egor Lappo, and Andy Huynh, postdoctoral scholar Mark Bitter, research assistant Zach Mouza, and undergraduate research assistant Caitlynn Tran. Additional co-authors are from Lawrence Berkeley National Laboratory and the University of Pennsylvania.&lt;/p&gt;
    &lt;p&gt;This research was funded by the National Institute of General Medical Sciences of the National Institutes of Health, the Sarah Hotchkis Ketterer Graduate Fellowship from Stanford University, the National Science Foundation, and the Stanford Biology Summer Undergraduate Research Program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Writer&lt;/head&gt;
    &lt;p&gt;Taylor Kubota&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.stanford.edu/stories/2025/09/classic-recessive-dominant-gene-dynamics-pesticide-resistance-research"/><published>2025-09-18T22:52:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45296403</id><title>Llama-Factory: Unified, Efficient Fine-Tuning for 100 Open LLMs</title><updated>2025-09-19T02:18:46.000092+00:00</updated><content>&lt;doc fingerprint="bd11b258ad3a99da"&gt;
  &lt;main&gt;
    &lt;p&gt;👋 Join our WeChat, NPU, Lab4AI, LLaMA Factory Online user group.&lt;/p&gt;
    &lt;p&gt;[ English | 中文 ]&lt;/p&gt;
    &lt;p&gt;Fine-tuning a large language model can be easy as...&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;train_en.mp4&lt;/head&gt;
    &lt;p&gt;Choose your path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation (WIP): https://llamafactory.readthedocs.io/en/latest/&lt;/item&gt;
      &lt;item&gt;Documentation (AMD GPU): https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html&lt;/item&gt;
      &lt;item&gt;Colab (free): https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/item&gt;
      &lt;item&gt;Local machine: Please refer to usage&lt;/item&gt;
      &lt;item&gt;PAI-DSW (free trial): https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&lt;/item&gt;
      &lt;item&gt;Alaya NeW (cloud GPU deal): https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory&lt;/item&gt;
      &lt;item&gt;Official Course: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;amp;utm_source=LLaMA-Factory&lt;/item&gt;
      &lt;item&gt;LLaMA Factory Online: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Features&lt;/item&gt;
      &lt;item&gt;Blogs&lt;/item&gt;
      &lt;item&gt;Changelog&lt;/item&gt;
      &lt;item&gt;Supported Models&lt;/item&gt;
      &lt;item&gt;Supported Training Approaches&lt;/item&gt;
      &lt;item&gt;Provided Datasets&lt;/item&gt;
      &lt;item&gt;Requirement&lt;/item&gt;
      &lt;item&gt;Getting Started&lt;/item&gt;
      &lt;item&gt;Projects using LLaMA Factory&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
      &lt;item&gt;Citation&lt;/item&gt;
      &lt;item&gt;Acknowledgement&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Various models: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/item&gt;
      &lt;item&gt;Integrated methods: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/item&gt;
      &lt;item&gt;Scalable resources: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/item&gt;
      &lt;item&gt;Advanced algorithms: GaLore, BAdam, APOLLO, Adam-mini, Muon, OFT, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/item&gt;
      &lt;item&gt;Practical tricks: FlashAttention-2, Unsloth, Liger Kernel, RoPE scaling, NEFTune and rsLoRA.&lt;/item&gt;
      &lt;item&gt;Wide tasks: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/item&gt;
      &lt;item&gt;Experiment monitors: LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab, etc.&lt;/item&gt;
      &lt;item&gt;Faster inference: OpenAI-style API, Gradio UI and CLI with vLLM worker or SGLang worker.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Support Date&lt;/cell&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Day 0&lt;/cell&gt;
        &lt;cell&gt;Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Day 1&lt;/cell&gt;
        &lt;cell&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;💡 Easy Dataset × LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge (English)&lt;/item&gt;
      &lt;item&gt;Fine-tune a mental health LLM using LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1 (Chinese)&lt;/item&gt;
      &lt;item&gt;How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod (English)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;All Blogs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier (Chinese)&lt;/item&gt;
      &lt;item&gt;A One-Stop Code-Free Model Fine-Tuning &amp;amp; Deployment Platform based on SageMaker and LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide (Chinese)&lt;/item&gt;
      &lt;item&gt;LLaMA Factory: Fine-tuning Llama3 for Role-Playing (Chinese)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;[25/08/22] We supported OFT and OFTv2. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[25/08/20] We supported fine-tuning the Intern-S1-mini models. See PR #8976 to get started.&lt;/p&gt;
    &lt;p&gt;[25/08/06] We supported fine-tuning the GPT-OSS models. See PR #8826 to get started.&lt;/p&gt;
    &lt;head&gt;Full Changelog&lt;/head&gt;
    &lt;p&gt;[25/07/02] We supported fine-tuning the GLM-4.1V-9B-Thinking model.&lt;/p&gt;
    &lt;p&gt;[25/04/28] We supported fine-tuning the Qwen3 model family.&lt;/p&gt;
    &lt;p&gt;[25/04/21] We supported the Muon optimizer. See examples for usage. Thank @tianshijing's PR.&lt;/p&gt;
    &lt;p&gt;[25/04/16] We supported fine-tuning the InternVL3 model. See PR #7258 to get started.&lt;/p&gt;
    &lt;p&gt;[25/04/14] We supported fine-tuning the GLM-Z1 and Kimi-VL models.&lt;/p&gt;
    &lt;p&gt;[25/04/06] We supported fine-tuning the Llama 4 model. See PR #7611 to get started.&lt;/p&gt;
    &lt;p&gt;[25/03/31] We supported fine-tuning the Qwen2.5 Omni model. See PR #7537 to get started.&lt;/p&gt;
    &lt;p&gt;[25/03/15] We supported SGLang as inference backend. Try &lt;code&gt;infer_backend: sglang&lt;/code&gt; to accelerate inference.&lt;/p&gt;
    &lt;p&gt;[25/03/12] We supported fine-tuning the Gemma 3 model.&lt;/p&gt;
    &lt;p&gt;[25/02/24] Announcing EasyR1, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt;
    &lt;p&gt;[25/02/11] We supported saving the Ollama modelfile when exporting the model checkpoints. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[25/02/05] We supported fine-tuning the Qwen2-Audio and MiniCPM-o-2.6 on audio understanding tasks.&lt;/p&gt;
    &lt;p&gt;[25/01/31] We supported fine-tuning the DeepSeek-R1 and Qwen2.5-VL models.&lt;/p&gt;
    &lt;p&gt;[25/01/15] We supported APOLLO optimizer. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[25/01/14] We supported fine-tuning the MiniCPM-o-2.6 and MiniCPM-V-2.6 models. Thank @BUAADreamer's PR.&lt;/p&gt;
    &lt;p&gt;[25/01/14] We supported fine-tuning the InternLM 3 models. Thank @hhaAndroid's PR.&lt;/p&gt;
    &lt;p&gt;[25/01/10] We supported fine-tuning the Phi-4 model.&lt;/p&gt;
    &lt;p&gt;[24/12/21] We supported using SwanLab for experiment tracking and visualization. See this section for details.&lt;/p&gt;
    &lt;p&gt;[24/11/27] We supported fine-tuning the Skywork-o1 model and the OpenO1 dataset.&lt;/p&gt;
    &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the Modelers Hub. See this tutorial for usage.&lt;/p&gt;
    &lt;p&gt;[24/09/19] We supported fine-tuning the Qwen2.5 models.&lt;/p&gt;
    &lt;p&gt;[24/08/30] We supported fine-tuning the Qwen2-VL models. Thank @simonJJJ's PR.&lt;/p&gt;
    &lt;p&gt;[24/08/27] We supported Liger Kernel. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt;
    &lt;p&gt;[24/08/09] We supported Adam-mini optimizer. See examples for usage. Thank @relic-yuexi's PR.&lt;/p&gt;
    &lt;p&gt;[24/07/04] We supported contamination-free packed training. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank @chuan298's PR.&lt;/p&gt;
    &lt;p&gt;[24/06/16] We supported PiSSA algorithm. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/06/07] We supported fine-tuning the Qwen2 and GLM-4 models.&lt;/p&gt;
    &lt;p&gt;[24/05/26] We supported SimPO algorithm for preference learning. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/05/20] We supported fine-tuning the PaliGemma series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt;
    &lt;p&gt;[24/05/18] We supported KTO algorithm for preference learning. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check installation section for details.&lt;/p&gt;
    &lt;p&gt;[24/04/26] We supported fine-tuning the LLaVA-1.5 multimodal LLMs. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/04/22] We provided a Colab notebook for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check Llama3-8B-Chinese-Chat and Llama3-Chinese for details.&lt;/p&gt;
    &lt;p&gt;[24/04/21] We supported Mixture-of-Depths according to AstraMindAI's implementation. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/04/16] We supported BAdam optimizer. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/04/16] We supported unsloth's long-sequence training (Llama-2-7B-56k within 24GB). It achieves 117% speed and 50% memory compared with FlashAttention-2, more benchmarks can be found in this page.&lt;/p&gt;
    &lt;p&gt;[24/03/31] We supported ORPO. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/03/21] Our paper "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models" is available at arXiv!&lt;/p&gt;
    &lt;p&gt;[24/03/20] We supported FSDP+QLoRA that fine-tunes a 70B model on 2x24GB GPUs. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/03/13] We supported LoRA+. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/03/07] We supported GaLore optimizer. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/03/07] We integrated vLLM for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy 270% inference speed.&lt;/p&gt;
    &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (DoRA). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt;
    &lt;p&gt;[24/02/15] We supported block expansion proposed by LLaMA Pro. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this blog post for details.&lt;/p&gt;
    &lt;p&gt;[24/01/18] We supported agent tuning for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;[23/12/23] We supported unsloth's implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves 170% speed in our benchmark, check this page for details.&lt;/p&gt;
    &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model Mixtral 8x7B in our framework. See hardware requirement here.&lt;/p&gt;
    &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the ModelScope Hub. See this tutorial for usage.&lt;/p&gt;
    &lt;p&gt;[23/10/21] We supported NEFTune trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt;
    &lt;p&gt;[23/09/27] We supported &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt;
    &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[23/09/10] We supported FlashAttention-2. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt;
    &lt;p&gt;[23/08/12] We supported RoPE scaling to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt;
    &lt;p&gt;[23/08/11] We supported DPO training for instruction-tuned models. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[23/07/31] We supported dataset streaming. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt;
    &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (LLaMA-2 / Baichuan) for details.&lt;/p&gt;
    &lt;p&gt;[23/07/18] We developed an all-in-one Web UI for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank @KanadeSiina and @codemayq for their efforts in the development.&lt;/p&gt;
    &lt;p&gt;[23/07/09] We released FastEdit ⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow FastEdit if you are interested.&lt;/p&gt;
    &lt;p&gt;[23/06/29] We provided a reproducible example of training a chat model using instruction-following datasets, see Baichuan-7B-sft for details.&lt;/p&gt;
    &lt;p&gt;[23/06/22] We aligned the demo API with the OpenAI's format where you can insert the fine-tuned model in arbitrary ChatGPT-based applications.&lt;/p&gt;
    &lt;p&gt;[23/06/03] We supported quantized training and inference (aka QLoRA). See examples for usage.&lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Model size&lt;/cell&gt;
        &lt;cell role="head"&gt;Template&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baichuan 2&lt;/cell&gt;
        &lt;cell&gt;7B/13B&lt;/cell&gt;
        &lt;cell&gt;baichuan2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;BLOOM/BLOOMZ&lt;/cell&gt;
        &lt;cell&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ChatGLM3&lt;/cell&gt;
        &lt;cell&gt;6B&lt;/cell&gt;
        &lt;cell&gt;chatglm3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Command R&lt;/cell&gt;
        &lt;cell&gt;35B/104B&lt;/cell&gt;
        &lt;cell&gt;cohere&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DeepSeek (Code/MoE)&lt;/cell&gt;
        &lt;cell&gt;7B/16B/67B/236B&lt;/cell&gt;
        &lt;cell&gt;deepseek&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DeepSeek 2.5/3&lt;/cell&gt;
        &lt;cell&gt;236B/671B&lt;/cell&gt;
        &lt;cell&gt;deepseek3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DeepSeek R1 (Distill)&lt;/cell&gt;
        &lt;cell&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/cell&gt;
        &lt;cell&gt;deepseekr1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Falcon&lt;/cell&gt;
        &lt;cell&gt;7B/11B/40B/180B&lt;/cell&gt;
        &lt;cell&gt;falcon&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Falcon-H1&lt;/cell&gt;
        &lt;cell&gt;0.5B/1.5B/3B/7B/34B&lt;/cell&gt;
        &lt;cell&gt;falcon_h1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemma/Gemma 2/CodeGemma&lt;/cell&gt;
        &lt;cell&gt;2B/7B/9B/27B&lt;/cell&gt;
        &lt;cell&gt;gemma/gemma2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemma 3/Gemma 3n&lt;/cell&gt;
        &lt;cell&gt;270M/1B/4B/6B/8B/12B/27B&lt;/cell&gt;
        &lt;cell&gt;gemma3/gemma3n&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GLM-4/GLM-4-0414/GLM-Z1&lt;/cell&gt;
        &lt;cell&gt;9B/32B&lt;/cell&gt;
        &lt;cell&gt;glm4/glmz1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GLM-4.1V&lt;/cell&gt;
        &lt;cell&gt;9B&lt;/cell&gt;
        &lt;cell&gt;glm4v&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GLM-4.5/GLM-4.5V&lt;/cell&gt;
        &lt;cell&gt;106B/355B&lt;/cell&gt;
        &lt;cell&gt;glm4_moe/glm4v_moe&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPT-2&lt;/cell&gt;
        &lt;cell&gt;0.1B/0.4B/0.8B/1.5B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPT-OSS&lt;/cell&gt;
        &lt;cell&gt;20B/120B&lt;/cell&gt;
        &lt;cell&gt;gpt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Granite 3.0-3.3&lt;/cell&gt;
        &lt;cell&gt;1B/2B/3B/8B&lt;/cell&gt;
        &lt;cell&gt;granite3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Granite 4&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;granite4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hunyuan&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;hunyuan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Index&lt;/cell&gt;
        &lt;cell&gt;1.9B&lt;/cell&gt;
        &lt;cell&gt;index&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;InternLM 2-3&lt;/cell&gt;
        &lt;cell&gt;7B/8B/20B&lt;/cell&gt;
        &lt;cell&gt;intern2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;InternVL 2.5-3.5&lt;/cell&gt;
        &lt;cell&gt;1B/2B/4B/8B/14B/30B/38B/78B/241B&lt;/cell&gt;
        &lt;cell&gt;intern_vl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;InternLM/Intern-S1-mini&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;intern_s1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kimi-VL&lt;/cell&gt;
        &lt;cell&gt;16B&lt;/cell&gt;
        &lt;cell&gt;kimi_vl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama&lt;/cell&gt;
        &lt;cell&gt;7B/13B/33B/65B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama 2&lt;/cell&gt;
        &lt;cell&gt;7B/13B/70B&lt;/cell&gt;
        &lt;cell&gt;llama2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama 3-3.3&lt;/cell&gt;
        &lt;cell&gt;1B/3B/8B/70B&lt;/cell&gt;
        &lt;cell&gt;llama3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama 4&lt;/cell&gt;
        &lt;cell&gt;109B/402B&lt;/cell&gt;
        &lt;cell&gt;llama4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama 3.2 Vision&lt;/cell&gt;
        &lt;cell&gt;11B/90B&lt;/cell&gt;
        &lt;cell&gt;mllama&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLaVA-1.5&lt;/cell&gt;
        &lt;cell&gt;7B/13B&lt;/cell&gt;
        &lt;cell&gt;llava&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLaVA-NeXT&lt;/cell&gt;
        &lt;cell&gt;7B/8B/13B/34B/72B/110B&lt;/cell&gt;
        &lt;cell&gt;llava_next&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLaVA-NeXT-Video&lt;/cell&gt;
        &lt;cell&gt;7B/34B&lt;/cell&gt;
        &lt;cell&gt;llava_next_video&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MiMo&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;mimo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MiniCPM 1-4.1&lt;/cell&gt;
        &lt;cell&gt;0.5B/1B/2B/4B/8B&lt;/cell&gt;
        &lt;cell&gt;cpm/cpm3/cpm4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;minicpm_o/minicpm_v&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ministral/Mistral-Nemo&lt;/cell&gt;
        &lt;cell&gt;8B/12B&lt;/cell&gt;
        &lt;cell&gt;ministral&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mistral/Mixtral&lt;/cell&gt;
        &lt;cell&gt;7B/8x7B/8x22B&lt;/cell&gt;
        &lt;cell&gt;mistral&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mistral Small&lt;/cell&gt;
        &lt;cell&gt;24B&lt;/cell&gt;
        &lt;cell&gt;mistral_small&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OLMo&lt;/cell&gt;
        &lt;cell&gt;1B/7B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PaliGemma/PaliGemma2&lt;/cell&gt;
        &lt;cell&gt;3B/10B/28B&lt;/cell&gt;
        &lt;cell&gt;paligemma&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Phi-1.5/Phi-2&lt;/cell&gt;
        &lt;cell&gt;1.3B/2.7B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Phi-3/Phi-3.5&lt;/cell&gt;
        &lt;cell&gt;4B/14B&lt;/cell&gt;
        &lt;cell&gt;phi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Phi-3-small&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;phi_small&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Phi-4&lt;/cell&gt;
        &lt;cell&gt;14B&lt;/cell&gt;
        &lt;cell&gt;phi4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Pixtral&lt;/cell&gt;
        &lt;cell&gt;12B&lt;/cell&gt;
        &lt;cell&gt;pixtral&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen (1-2.5) (Code/Math/MoE/QwQ)&lt;/cell&gt;
        &lt;cell&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/cell&gt;
        &lt;cell&gt;qwen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen3 (MoE/Instruct/Thinking/Next)&lt;/cell&gt;
        &lt;cell&gt;0.6B/1.7B/4B/8B/14B/32B/80B/235B&lt;/cell&gt;
        &lt;cell&gt;qwen3/qwen3_nothink&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2-Audio&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;qwen2_audio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell&gt;3B/7B&lt;/cell&gt;
        &lt;cell&gt;qwen2_omni&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/cell&gt;
        &lt;cell&gt;2B/3B/7B/32B/72B&lt;/cell&gt;
        &lt;cell&gt;qwen2_vl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Seed (OSS/Coder)&lt;/cell&gt;
        &lt;cell&gt;8B/36B&lt;/cell&gt;
        &lt;cell&gt;seed_oss/seed_coder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Skywork o1&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;skywork_o1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;StarCoder 2&lt;/cell&gt;
        &lt;cell&gt;3B/7B/15B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TeleChat2&lt;/cell&gt;
        &lt;cell&gt;3B/7B/35B/115B&lt;/cell&gt;
        &lt;cell&gt;telechat2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;XVERSE&lt;/cell&gt;
        &lt;cell&gt;7B/13B/65B&lt;/cell&gt;
        &lt;cell&gt;xverse&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Yi/Yi-1.5 (Code)&lt;/cell&gt;
        &lt;cell&gt;1.5B/6B/9B/34B&lt;/cell&gt;
        &lt;cell&gt;yi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Yi-VL&lt;/cell&gt;
        &lt;cell&gt;6B/34B&lt;/cell&gt;
        &lt;cell&gt;yi_vl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Yuan 2&lt;/cell&gt;
        &lt;cell&gt;2B/51B/102B&lt;/cell&gt;
        &lt;cell&gt;yuan&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;For the "base" models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the corresponding template for the "instruct/chat" models.&lt;/p&gt;
    &lt;p&gt;Remember to use the SAME template in training and inference.&lt;/p&gt;
    &lt;p&gt;*: You should install the &lt;code&gt;transformers&lt;/code&gt; from main branch and use &lt;code&gt;DISABLE_VERSION_CHECK=1&lt;/code&gt; to skip version check.&lt;/p&gt;
    &lt;p&gt;**: You need to install a specific version of &lt;code&gt;transformers&lt;/code&gt; to use the corresponding model.&lt;/p&gt;
    &lt;p&gt;Please refer to constants.py for a full list of models we supported.&lt;/p&gt;
    &lt;p&gt;You also can add a custom chat template to template.py.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Approach&lt;/cell&gt;
        &lt;cell role="head"&gt;Full-tuning&lt;/cell&gt;
        &lt;cell role="head"&gt;Freeze-tuning&lt;/cell&gt;
        &lt;cell role="head"&gt;LoRA&lt;/cell&gt;
        &lt;cell role="head"&gt;QLoRA&lt;/cell&gt;
        &lt;cell role="head"&gt;OFT&lt;/cell&gt;
        &lt;cell role="head"&gt;QOFT&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Pre-Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Supervised Fine-Tuning&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reward Modeling&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;PPO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;DPO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;KTO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ORPO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SimPO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;The implementation details of PPO can be found in this blog.&lt;/p&gt;
    &lt;head&gt;Pre-training datasets&lt;/head&gt;
    &lt;head&gt;Supervised fine-tuning datasets&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identity (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Stanford Alpaca (en)&lt;/item&gt;
      &lt;item&gt;Stanford Alpaca (zh)&lt;/item&gt;
      &lt;item&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;LIMA (en)&lt;/item&gt;
      &lt;item&gt;Guanaco Dataset (multilingual)&lt;/item&gt;
      &lt;item&gt;BELLE 2M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE 1M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE 0.5M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE Dialogue 0.4M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE School Math 0.25M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/item&gt;
      &lt;item&gt;UltraChat (en)&lt;/item&gt;
      &lt;item&gt;OpenPlatypus (en)&lt;/item&gt;
      &lt;item&gt;CodeAlpaca 20k (en)&lt;/item&gt;
      &lt;item&gt;Alpaca CoT (multilingual)&lt;/item&gt;
      &lt;item&gt;OpenOrca (en)&lt;/item&gt;
      &lt;item&gt;SlimOrca (en)&lt;/item&gt;
      &lt;item&gt;MathInstruct (en)&lt;/item&gt;
      &lt;item&gt;Firefly 1.1M (zh)&lt;/item&gt;
      &lt;item&gt;Wiki QA (en)&lt;/item&gt;
      &lt;item&gt;Web QA (zh)&lt;/item&gt;
      &lt;item&gt;WebNovel (zh)&lt;/item&gt;
      &lt;item&gt;Nectar (en)&lt;/item&gt;
      &lt;item&gt;deepctrl (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Advertise Generating (zh)&lt;/item&gt;
      &lt;item&gt;ShareGPT Hyperfiltered (en)&lt;/item&gt;
      &lt;item&gt;ShareGPT4 (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;UltraChat 200k (en)&lt;/item&gt;
      &lt;item&gt;Infinity Instruct (zh)&lt;/item&gt;
      &lt;item&gt;AgentInstruct (en)&lt;/item&gt;
      &lt;item&gt;LMSYS Chat 1M (en)&lt;/item&gt;
      &lt;item&gt;Evol Instruct V2 (en)&lt;/item&gt;
      &lt;item&gt;Cosmopedia (en)&lt;/item&gt;
      &lt;item&gt;STEM (zh)&lt;/item&gt;
      &lt;item&gt;Ruozhiba (zh)&lt;/item&gt;
      &lt;item&gt;Neo-sft (zh)&lt;/item&gt;
      &lt;item&gt;Magpie-Pro-300K-Filtered (en)&lt;/item&gt;
      &lt;item&gt;Magpie-ultra-v0.1 (en)&lt;/item&gt;
      &lt;item&gt;WebInstructSub (en)&lt;/item&gt;
      &lt;item&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Open-Thoughts (en)&lt;/item&gt;
      &lt;item&gt;Open-R1-Math (en)&lt;/item&gt;
      &lt;item&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/item&gt;
      &lt;item&gt;LLaVA mixed (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Open Assistant (de)&lt;/item&gt;
      &lt;item&gt;Dolly 15k (de)&lt;/item&gt;
      &lt;item&gt;Alpaca GPT4 (de)&lt;/item&gt;
      &lt;item&gt;OpenSchnabeltier (de)&lt;/item&gt;
      &lt;item&gt;Evol Instruct (de)&lt;/item&gt;
      &lt;item&gt;Dolphin (de)&lt;/item&gt;
      &lt;item&gt;Booksum (de)&lt;/item&gt;
      &lt;item&gt;Airoboros (de)&lt;/item&gt;
      &lt;item&gt;Ultrachat (de)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Preference datasets&lt;/head&gt;
    &lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt;
    &lt;code&gt;pip install --upgrade huggingface_hub
huggingface-cli login&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Mandatory&lt;/cell&gt;
        &lt;cell role="head"&gt;Minimum&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;python&lt;/cell&gt;
        &lt;cell&gt;3.9&lt;/cell&gt;
        &lt;cell&gt;3.10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;torch&lt;/cell&gt;
        &lt;cell&gt;2.0.0&lt;/cell&gt;
        &lt;cell&gt;2.6.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;torchvision&lt;/cell&gt;
        &lt;cell&gt;0.15.0&lt;/cell&gt;
        &lt;cell&gt;0.21.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;transformers&lt;/cell&gt;
        &lt;cell&gt;4.49.0&lt;/cell&gt;
        &lt;cell&gt;4.50.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;datasets&lt;/cell&gt;
        &lt;cell&gt;2.16.0&lt;/cell&gt;
        &lt;cell&gt;3.2.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;accelerate&lt;/cell&gt;
        &lt;cell&gt;0.34.0&lt;/cell&gt;
        &lt;cell&gt;1.2.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;peft&lt;/cell&gt;
        &lt;cell&gt;0.14.0&lt;/cell&gt;
        &lt;cell&gt;0.15.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;trl&lt;/cell&gt;
        &lt;cell&gt;0.8.6&lt;/cell&gt;
        &lt;cell&gt;0.9.6&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Minimum&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CUDA&lt;/cell&gt;
        &lt;cell&gt;11.6&lt;/cell&gt;
        &lt;cell&gt;12.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;deepspeed&lt;/cell&gt;
        &lt;cell&gt;0.10.0&lt;/cell&gt;
        &lt;cell&gt;0.16.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;bitsandbytes&lt;/cell&gt;
        &lt;cell&gt;0.39.0&lt;/cell&gt;
        &lt;cell&gt;0.43.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;vllm&lt;/cell&gt;
        &lt;cell&gt;0.4.3&lt;/cell&gt;
        &lt;cell&gt;0.8.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;flash-attn&lt;/cell&gt;
        &lt;cell&gt;2.5.6&lt;/cell&gt;
        &lt;cell&gt;2.7.2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;* estimated&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Bits&lt;/cell&gt;
        &lt;cell role="head"&gt;7B&lt;/cell&gt;
        &lt;cell role="head"&gt;14B&lt;/cell&gt;
        &lt;cell role="head"&gt;30B&lt;/cell&gt;
        &lt;cell role="head"&gt;70B&lt;/cell&gt;
        &lt;cell role="head"&gt;&lt;code&gt;x&lt;/code&gt;B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Full (&lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;120GB&lt;/cell&gt;
        &lt;cell&gt;240GB&lt;/cell&gt;
        &lt;cell&gt;600GB&lt;/cell&gt;
        &lt;cell&gt;1200GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;18x&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Full (&lt;code&gt;pure_bf16&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;60GB&lt;/cell&gt;
        &lt;cell&gt;120GB&lt;/cell&gt;
        &lt;cell&gt;300GB&lt;/cell&gt;
        &lt;cell&gt;600GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;8x&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Freeze/LoRA/GaLore/APOLLO/BAdam/OFT&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;16GB&lt;/cell&gt;
        &lt;cell&gt;32GB&lt;/cell&gt;
        &lt;cell&gt;64GB&lt;/cell&gt;
        &lt;cell&gt;160GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;2x&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;QLoRA / QOFT&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;10GB&lt;/cell&gt;
        &lt;cell&gt;20GB&lt;/cell&gt;
        &lt;cell&gt;40GB&lt;/cell&gt;
        &lt;cell&gt;80GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;x&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;QLoRA / QOFT&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;6GB&lt;/cell&gt;
        &lt;cell&gt;12GB&lt;/cell&gt;
        &lt;cell&gt;24GB&lt;/cell&gt;
        &lt;cell&gt;48GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;x/2&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;QLoRA / QOFT&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;4GB&lt;/cell&gt;
        &lt;cell&gt;8GB&lt;/cell&gt;
        &lt;cell&gt;16GB&lt;/cell&gt;
        &lt;cell&gt;24GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;x/4&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;Installation is mandatory.&lt;/p&gt;
    &lt;code&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev&lt;/p&gt;
    &lt;code&gt;docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest&lt;/code&gt;
    &lt;p&gt;This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.&lt;/p&gt;
    &lt;p&gt;Find the pre-built images: https://hub.docker.com/r/hiyouga/llamafactory/tags&lt;/p&gt;
    &lt;p&gt;Please refer to build docker to build the image yourself.&lt;/p&gt;
    &lt;head&gt;Setting up a virtual environment with uv&lt;/head&gt;
    &lt;p&gt;Create an isolated Python environment with uv:&lt;/p&gt;
    &lt;code&gt;uv sync --extra torch --extra metrics --prerelease=allow&lt;/code&gt;
    &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt;
    &lt;code&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml&lt;/code&gt;
    &lt;head&gt;For Windows users&lt;/head&gt;
    &lt;p&gt;You need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the official website and the following command to install PyTorch with CUDA support:&lt;/p&gt;
    &lt;code&gt;pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c "import torch; print(torch.cuda.is_available())"&lt;/code&gt;
    &lt;p&gt;If you see &lt;code&gt;True&lt;/code&gt; then you have successfully installed PyTorch with CUDA support.&lt;/p&gt;
    &lt;p&gt;Try &lt;code&gt;dataloader_num_workers: 0&lt;/code&gt; if you encounter &lt;code&gt;Can't pickle local object&lt;/code&gt; error.&lt;/p&gt;
    &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate release version based on your CUDA version.&lt;/p&gt;
    &lt;code&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl&lt;/code&gt;
    &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from flash-attention-windows-wheel to compile and install it by yourself.&lt;/p&gt;
    &lt;head&gt;For Ascend NPU users&lt;/head&gt;
    &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e ".[torch-npu,metrics]"&lt;/code&gt;. Additionally, you need to install the Ascend CANN Toolkit and Kernels. Please follow the installation tutorial or use the following commands:&lt;/p&gt;
    &lt;code&gt;# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Requirement&lt;/cell&gt;
        &lt;cell role="head"&gt;Minimum&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CANN&lt;/cell&gt;
        &lt;cell&gt;8.0.RC1&lt;/cell&gt;
        &lt;cell&gt;8.0.0.alpha002&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;torch&lt;/cell&gt;
        &lt;cell&gt;2.1.0&lt;/cell&gt;
        &lt;cell&gt;2.4.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;torch-npu&lt;/cell&gt;
        &lt;cell&gt;2.1.0&lt;/cell&gt;
        &lt;cell&gt;2.4.0.post2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;deepspeed&lt;/cell&gt;
        &lt;cell&gt;0.13.2&lt;/cell&gt;
        &lt;cell&gt;0.13.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;vllm-ascend&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.7.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt;
    &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt;
    &lt;p&gt;Download the pre-built Docker images: 32GB | 64GB&lt;/p&gt;
    &lt;head rend="h4"&gt;Install BitsAndBytes&lt;/head&gt;
    &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Manually compile bitsandbytes: Refer to the installation documentation for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp;amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install .&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install transformers from the main branch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install .&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt;in the configuration. You can refer to the example.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please refer to data/README.md for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt;
    &lt;p&gt;You can also use Easy Dataset, DataFlow and GraphGen to create synthetic data for fine-tuning.&lt;/p&gt;
    &lt;p&gt;Use the following 3 commands to run LoRA fine-tuning, inference and merging of the Llama3-8B-Instruct model, respectively.&lt;/p&gt;
    &lt;code&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml&lt;/code&gt;
    &lt;p&gt;See examples/README.md for advanced usage (including distributed training).&lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt;
    &lt;p&gt;Read FAQs first if you encounter any problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fine-Tuning with LLaMA Board GUI (powered by Gradio)&lt;/head&gt;
    &lt;code&gt;llamafactory-cli webui&lt;/code&gt;
    &lt;p&gt;Read our documentation.&lt;/p&gt;
    &lt;p&gt;For CUDA users:&lt;/p&gt;
    &lt;code&gt;cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash&lt;/code&gt;
    &lt;p&gt;For Ascend NPU users:&lt;/p&gt;
    &lt;code&gt;cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash&lt;/code&gt;
    &lt;p&gt;For AMD ROCm users:&lt;/p&gt;
    &lt;code&gt;cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash&lt;/code&gt;
    &lt;head&gt;Build without Docker Compose&lt;/head&gt;
    &lt;p&gt;For CUDA users:&lt;/p&gt;
    &lt;code&gt;docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash&lt;/code&gt;
    &lt;p&gt;For Ascend NPU users:&lt;/p&gt;
    &lt;code&gt;docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash&lt;/code&gt;
    &lt;p&gt;For AMD ROCm users:&lt;/p&gt;
    &lt;code&gt;docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash&lt;/code&gt;
    &lt;head&gt;Use Docker volumes&lt;/head&gt;
    &lt;p&gt;You can uncomment &lt;code&gt;VOLUME [ "/root/.cache/huggingface", "/app/shared_data", "/app/output" ]&lt;/code&gt; in the Dockerfile to use data volumes.&lt;/p&gt;
    &lt;p&gt;When building the Docker image, use &lt;code&gt;-v ./hf_cache:/root/.cache/huggingface&lt;/code&gt; argument to mount the local directory to the container. The following data volumes are available.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;shared_data&lt;/code&gt;: The directionary to store datasets on the host machine.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true&lt;/code&gt;
    &lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt;
    &lt;code&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows&lt;/code&gt;
    &lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at ModelScope Hub, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt;
    &lt;code&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows&lt;/code&gt;
    &lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at Modelers Hub, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To use Weights &amp;amp; Biases for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt;
    &lt;code&gt;report_to: wandb
run_name: test_run # optional&lt;/code&gt;
    &lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to your key when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt;
    &lt;p&gt;To use SwanLab for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt;
    &lt;code&gt;use_swanlab: true
swanlab_run_name: test_run # optional&lt;/code&gt;
    &lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt;to the yaml file, and set it to your API key.&lt;/item&gt;
      &lt;item&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt;to your API key.&lt;/item&gt;
      &lt;item&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt;command to complete the login.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt;
    &lt;head&gt;Click to show&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [arxiv]&lt;/item&gt;
      &lt;item&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Dammu et al. "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yang et al. Financial Knowledge Large Language Model. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. [paper]&lt;/item&gt;
      &lt;item&gt;StarWhisper: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/item&gt;
      &lt;item&gt;DISC-LawLLM: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/item&gt;
      &lt;item&gt;Sunsimiao: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/item&gt;
      &lt;item&gt;CareGPT: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/item&gt;
      &lt;item&gt;MachineMindset: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/item&gt;
      &lt;item&gt;Luminia-13B-v3: A large language model specialized in generate metadata for stable diffusion. [demo]&lt;/item&gt;
      &lt;item&gt;Chinese-LLaVA-Med: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/item&gt;
      &lt;item&gt;AutoRE: A document-level relation extraction system based on large language models.&lt;/item&gt;
      &lt;item&gt;NVIDIA RTX AI Toolkit: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/item&gt;
      &lt;item&gt;LazyLLM: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/item&gt;
      &lt;item&gt;RAG-Retrieval: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [blog]&lt;/item&gt;
      &lt;item&gt;360-LLaMA-Factory: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/item&gt;
      &lt;item&gt;Sky-T1: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/item&gt;
      &lt;item&gt;WeClone: One-stop solution for creating your digital avatar from chat logs.&lt;/item&gt;
      &lt;item&gt;EmoLLM: A project about large language models (LLMs) and mental health.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This repository is licensed under the Apache-2.0 License.&lt;/p&gt;
    &lt;p&gt;Please follow the model licenses to use the corresponding model weights: Baichuan 2 / BLOOM / ChatGLM3 / Command R / DeepSeek / Falcon / Gemma / GLM-4 / GPT-2 / Granite / Index / InternLM / Llama / Llama 2 / Llama 3 / Llama 4 / MiniCPM / Mistral/Mixtral/Pixtral / OLMo / Phi-1.5/Phi-2 / Phi-3/Phi-4 / Qwen / Skywork / StarCoder 2 / TeleChat2 / XVERSE / Yi / Yi-1.5 / Yuan 2&lt;/p&gt;
    &lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt;
    &lt;code&gt;@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}&lt;/code&gt;
    &lt;p&gt;This repo benefits from PEFT, TRL, QLoRA and FastChat. Thanks for their wonderful works.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/hiyouga/LLaMA-Factory"/><published>2025-09-18T23:48:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45296521</id><title>Show HN: Nallely – A Python signals/MIDI processing system inspired by Smalltalk</title><updated>2025-09-19T02:18:45.875051+00:00</updated><content>&lt;doc fingerprint="a780d665fc7b59b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nallely - pronounced “Nayeli” - is an organic open-source Python platform for modular signal processing and meta-synth creation. Nallely lets you build your own modular instrument or machine from any signal producing source: MIDI devices, sensors, webcams, or even other computers on the same network. Signals can be generated, transformed, filtered, or split, then routed back into MIDI devices or into any application registered in a Nallely session. Designed for hackers and musicians, Nallely supports live coding, complex MIDI routing, generative music, and multimodal art.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Control multiple MIDI devices&lt;/cell&gt;
        &lt;cell role="head"&gt;Patch your devices&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Think of Nallely as a small brain, where each device acts a bit like a biological neuron by receiving and emitting signals. Each neuron runs independently on its own thread, and they can connect in countless ways by exchanging messages with each other. By wiring them freely, you can link neurons that in a “normal” brain would not usually communicate. The result is a small brain that can behave like a regular one, or like a brain under psychedelics, mapped in unusual ways, producing unexpected, but always amazing results. Nallely is designed for experimentation, happy accidents, and emergent behavior.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Monitor the signals&lt;/cell&gt;
        &lt;cell role="head"&gt;Explore your patch in 3D&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Inspired by the “Systems as Living Things” philosophy and by Smalltalk, Nallely tries to be as dynamic as possible: you can create your own meta-synth and build your custom MIDI brain while it’s running, from any computer or phone on the network (with a touch-friendly interface). Developed in Python, Nallely exposes an extensible core and an easy-to-use Python API, so you can create your own neurons without efforts, and have them integrated directly into the system in a seemless way.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Manage your patchs as a memory versioned on git&lt;/cell&gt;
        &lt;cell role="head"&gt;Tweak your neurons&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Nallely comes with a set of pre-existing neurons, including:&lt;/p&gt;
    &lt;p&gt;Currently, Nallely includes a few remote neurons coded in JavaScript, introducing 3D visuals (mental imagery for your brain) that can be controlled by signals received from your Nallely session (your modular brain instance). It also includes a webcam-aware neuron, providing visual input to your small MIDI brain (the “eyes” for your brain).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Get a Smalltalk-like playground&lt;/cell&gt;
        &lt;cell role="head"&gt;Trevor is always here&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Nallely is available, open-source, free, and will remain free and open-source.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dr-schlange.github.io/nallely-midi/"/><published>2025-09-19T00:09:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45296638</id><title>David Lynch LA House</title><updated>2025-09-19T02:18:45.756938+00:00</updated><content>&lt;doc fingerprint="3a0002d17f174be5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tour David Lynch's house as it hits the market&lt;/head&gt;
    &lt;p&gt;David Lynch's LA estate is for sale at $15m, and the listing pictures offer a glimpse into the late filmmaker's aesthetic and creative universe&lt;/p&gt;
    &lt;p&gt;David Lynch, the visionary American filmmaker behind Twin Peaks, Blue Velvet and Mulholland Dr, passed away this January, yet his creative universe endures in objects, spaces and ideas.&lt;/p&gt;
    &lt;p&gt;Among the most striking of these relics is his larger-than-life, meticulously designed Hollywood Hills home; a cinematic setting in its own right. Perched on a sweeping 2.3-acre hillside, David Lynch’s private compound, which is now listed for $15 million by Marc Silver of The Agency, unfolds like one of his own intricately plotted storylines. A showcase of Mid-Century modern architecture, the estate was conceived with the same care and cinematic precision that defined his work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inside David Lynch's Los Angeles estate&lt;/head&gt;
    &lt;p&gt;The property, set across five contiguous parcels, reads like a storyboard in relief: three main residences and several ancillary structures stepping down the hillside, each capturing a different note in Lynch’s creative oeuvre.&lt;/p&gt;
    &lt;p&gt;The story behind this compound started in 1987, when he acquired the pink-hued Beverly Johnson House designed in the early 1960s by Lloyd Wright, son of Frank Lloyd Wright. The home, in fact, was recognised by Historic Places LA as an exemplary work of Mid-Century Modern residential design. Then in 1991, he commissioned Eric Lloyd Wright (Lloyd Wright’s son) to add a pool and pool house, extending the Wright imprint on his property with a new generation.&lt;/p&gt;
    &lt;p&gt;Across the years, Lynch kept expanding the plotline: in 1989, he purchased an adjoining two-bedroom Brutalist house; in 1995, a studio building; and later, more pieces of land, ultimately shaping a seven-structure sanctuary with 10 bedrooms and 11 bathrooms spread over roughly 11,000 square feet. The result was a creative campus perched above the city.&lt;/p&gt;
    &lt;p&gt;At the heart of the compound lies the architectural crescendo – the approximately 2,000 square feet home where light pours through generous windows and skylights to rake across organic textures and bold geometries. The facade’s cement chevrons catch the sun; inside, simple metalwork and natural woods are drenched in material honesty that often surfaced in Lynch’s films.&lt;/p&gt;
    &lt;p&gt;Two neighbouring addresses deepen the lore: 7029 Senalda served as the home of Asymmetrical Productions, while 7035 Senalda attained near-mythic status as both the Madison residence in the movie Lost Highway and Lynch’s own studio, complete with a library, screening room and editing suite – spaces where he refined major works, including Mulholland Drive.&lt;/p&gt;
    &lt;p&gt;Receive our daily digest of inspiration, escapism and design stories from around the world direct to your inbox.&lt;/p&gt;
    &lt;p&gt;Beyond the exemplary structures, Lynch left a personal handprint, collaborating on additional buildings: a sculptural two-storey guest house and a one-bedroom retreat finished in his favoured smooth grey plaster. Outdoors the terraces, courtyards and planted walkways offer a counterpoint to the intensity of production and everyday life.&lt;/p&gt;
    &lt;p&gt;As a listing note from The Agency suggests, this is a 'creative sanctuary and architectural landmark,' with provenance unlike any other in Los Angeles. For admirers of Lynch, it reads as both home and archive: a lived-in factory of ideas, meticulously composed and, at last, ready for its next act.&lt;/p&gt;
    &lt;p&gt;Aditi Sharma is a content specialist with 14 years of experience in the design and lifestyle space. She specialises in producing content that resonates with diverse audiences, bridging global trends with local stories, and translating complex ideas into engaging, accessible narratives.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Here’s what to order (and admire) at Carbone London&lt;p&gt;New York’s favourite, and buzziest, Italian restaurant arrives in the British capital, marking the brand’s first expansion into Europe&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Griffin Frazen on conceiving the cinematic runway sets for New York label Khaite: ‘If people feel moved we’ve succeeded’&lt;p&gt;The architectural designer – who helped conceive the sets for ‘The Brutalist’ – collaborates with his wife Catherine Holstein on the scenography for her Khaite runway shows, the latest of which took place in NYFW this past weekend&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; How to travel meaningfully in an increasingly generic world&lt;p&gt;Lauren Ho explores the need for resonance, not reach, in the way we choose to make journeys of discovery&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wallpaper.com/design-interiors/david-lynch-house-los-angeles-for-sale"/><published>2025-09-19T00:30:14+00:00</published></entry></feed>