<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-21T03:54:31.243826+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45990934</id><title>Interactive World History Atlas Since 3000 BC</title><updated>2025-11-21T03:54:37.865711+00:00</updated><content>&lt;doc fingerprint="8607bdb6bda831db"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Interactive World History Atlas since 3000 BC&lt;/head&gt;
    &lt;head rend="h2"&gt;World History Maps &amp;amp; Timelines. Kingdoms, Battles, Expeditions.&lt;lb/&gt; Comparative History, Political, Military, Art, Science, Literature, Religion, Philosophy. Maps based on vector database.&lt;/head&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://geacron.com/home-en/"/><published>2025-11-20T09:52:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45991738</id><title>Adversarial poetry as a universal single-turn jailbreak mechanism in LLMs</title><updated>2025-11-21T03:54:37.718944+00:00</updated><content>&lt;doc fingerprint="dc9d341acdd39acb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 19 Nov 2025 (v1), last revised 20 Nov 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for Large Language Models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of 3 open-weight LLM judges, whose binary safety assessments were validated on a stratified human-labeled subset. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Matteo Prandi [view email]&lt;p&gt;[v1] Wed, 19 Nov 2025 10:14:08 UTC (31 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 20 Nov 2025 03:34:44 UTC (30 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2511.15304"/><published>2025-11-20T12:01:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45993296</id><title>Nano Banana Pro</title><updated>2025-11-21T03:54:37.542741+00:00</updated><content>&lt;doc fingerprint="7c1622329c3e4b84"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Nano Banana Pro&lt;/head&gt;
    &lt;p&gt;Just a few months ago we released Nano Banana, our Gemini 2.5 Flash Image model. From restoring old photos to generating mini figurines, Nano Banana was a big step in image editing that empowered casual creators to express their creativity.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing Nano Banana Pro (Gemini 3 Pro Image), our new state-of-the art image generation and editing model. Built on Gemini 3 Pro, Nano Banana Pro uses Gemini’s state-of-the-art reasoning and real-world knowledge to visualize information better than ever before.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Nano Banana Pro helps you bring any idea or design to life&lt;/head&gt;
    &lt;p&gt;Nano Banana Pro can help you visualize any idea and design anything - from prototypes, to representing data as infographics, to turning handwritten notes into diagrams.&lt;/p&gt;
    &lt;p&gt;With Nano Banana Pro, now you can:&lt;/p&gt;
    &lt;p&gt;Generate more accurate, context-rich visuals based on enhanced reasoning, world knowledge and real-time information&lt;/p&gt;
    &lt;p&gt;With Gemini 3’s advanced reasoning, Nano Banana Pro doesn’t just create beautiful images, it also helps you create more helpful content. You can get accurate educational explainers to learn more about a new subject, like context-rich infographics and diagrams based on the content you provide or facts from the real world. Nano Banana Pro can also connect to Google Search's vast knowledge base to help you create a quick snapshot for a recipe or visualize real-time information like weather or sports.&lt;/p&gt;
    &lt;p&gt;An infographic of the common house plant, String of Turtles, with information on origins, care essentials and growth patterns.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an infographic about this plant focusing on interesting information.&lt;/p&gt;
    &lt;p&gt;Step-by-step infographic for making Elaichi Chai (cardamom tea), demonstrating the ability to visualize recipes and real-world information.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an infographic that shows how to make elaichi chai&lt;/p&gt;
    &lt;p&gt;We used Nano Banana Pro to pull in real-time weather via Search grounding to build a pop-art infographic.&lt;/p&gt;
    &lt;p&gt;Generate better visuals with more accurate, legible text directly in the image in multiple languages&lt;/p&gt;
    &lt;p&gt;Nano Banana Pro is the best model for creating images with correctly rendered and legible text directly in the image, whether you’re looking for a short tagline, or a long paragraph. Gemini 3 is great at understanding depth and nuance, which unlocks a world of possibilities with image editing and generation - especially with text. Now you can create more detailed text in mockups or posters with a wider variety of textures, fonts and calligraphy. With Gemini’s enhanced multilingual reasoning, you can generate text in multiple languages, or localize and translate your content so you can scale internationally and/or share content more easily with friends and family.&lt;/p&gt;
    &lt;p&gt;A black and white storyboard sketch showing an establishing shot, medium shot, close-up, and POV shot for a film scene.&lt;/p&gt;
    &lt;p&gt;Prompt: Create a storyboard for this scene&lt;/p&gt;
    &lt;p&gt;The word 'BERLIN' integrated into the architecture of a city block, spanning across multiple buildings.&lt;/p&gt;
    &lt;p&gt;Prompt: View of a cozy street in Berlin on a bright sunny day, stark shadows. the old houses are oddly shaped like letters that spell out "BERLIN" Colored in Blue, Red, White and black. The houses still look like houses and the resemblance to letters is subtle.&lt;/p&gt;
    &lt;p&gt;Calligraphy inspired by meaning, showcasing the ability to generate expressive text with a wider variety of textures and fonts.&lt;/p&gt;
    &lt;p&gt;Prompt: make 8 minimalistic logos, each is an expressive word, and make letters convey a message or sound visually to express the meaning of this word in a dramatic way. composition: flat vector rendering of all logos in black on a single white background&lt;/p&gt;
    &lt;p&gt;A beverage campaign concept showcasing accurate translation and rendering of English text into Korean.&lt;/p&gt;
    &lt;p&gt;Prompt: translate all the English text on the three yellow and blue cans into Korean, while keeping everything else the same&lt;/p&gt;
    &lt;p&gt;A graphic design featuring the word 'TYPOGRAPHY' with a retro, screen-printed texture.&lt;/p&gt;
    &lt;p&gt;Prompt: A vibrant, eye-catching "TYPOGRAPHY" design on a textured off-white background. The letters are bold, blocky, extra condensed and create a 3D effect with overlapping layers of bright blue and hot pink, each with a halftone dot pattern, evoking a retro print aesthetic. 16:9 aspect ratio&lt;/p&gt;
    &lt;p&gt;Blending text and texture in a creative way by integrating the phrase into a woodchopping scene.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an image showing the phrase "How much wood would a woodchuck chuck if a woodchuck could chuck wood" made out of wood chucked by a woodchuck.&lt;/p&gt;
    &lt;p&gt;Create high-fidelity visuals with upgraded creative capabilities&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistency by design: With Nano Banana Pro, you can blend more elements than ever before, using up to 14 images and maintaining the consistency and resemblance of up to 5 people. Whether turning sketches into products or blueprints into photorealistic 3D structures, you can now bridge the gap between concept and creation. Apply your desired visual look and feel to your mockups with ease, ensuring your branding remains seamless and consistent across every touchpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maintaining the consistency of up to 14 inputs, including multiple characters, across a complex composition.&lt;/p&gt;
    &lt;p&gt;Prompt: A medium shot of the 14 fluffy characters sitting squeezed together side-by-side on a worn beige fabric sofa and on the floor. They are all facing forwards, watching a vintage, wooden-boxed television set placed on a low wooden table in front of the sofa. The room is dimly lit, with warm light from a window on the left and the glow from the TV illuminating the creatures' faces and fluffy textures. The background is a cozy, slightly cluttered living room with a braided rug, a bookshelf with old books, and rustic kitchen elements in the background. The overall atmosphere is warm, cozy, and amused.&lt;/p&gt;
    &lt;p&gt;Craft lifestyle scenes by combining multiple elements.&lt;/p&gt;
    &lt;p&gt;Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format and change the dress on the mannequin to the dress in the image&lt;/p&gt;
    &lt;p&gt;Create surreal landscapes by combining multiple input elements.&lt;/p&gt;
    &lt;p&gt;Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format&lt;/p&gt;
    &lt;p&gt;A high-fashion editorial shot set in a desert landscape that maintains the consistency and resemblance of the people from the 6 input photos.&lt;/p&gt;
    &lt;p&gt;Prompt: Put these five people and this dog into a single image, they should fit into a stunning award-winning shot in the style if [sic] a fashion editorial. The identity of all five people and their attire and the dog must stay consistent throughout but they can and should be seen from different angles and distances in [sic] as is most natural and suitable to the scene. Make the colour and lighting look natural on them all, they look like they naturally fit into this fashion show.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Studio-quality creative controls: With Nano Banana Pro's new capabilities we are putting advanced creative controls directly into your hands. Select, refine and transform any part of an image with improved localized editing. Adjust camera angles, change the focus and apply sophisticated color grading, or even transform scene lighting (e.g. changing day to night or creating a bokeh effect). Your creations are ready for any platform, from social media to print, thanks to a range of available aspect ratios and available 2K and 4K resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Change the look and feel of an image for a range of platforms by adapting the aspect ratio.&lt;/p&gt;
    &lt;p&gt;Prompt: change aspect ratio to 1:1 by reducing background. The character, remains exactly locked in its current position&lt;/p&gt;
    &lt;p&gt;Lighting and focus controls applied to transform a scene from day to night.&lt;/p&gt;
    &lt;p&gt;Prompt: Turn this scene into nighttime&lt;/p&gt;
    &lt;p&gt;Obscure or enlighten a section of your image with lighting controls to achieve specific dramatic effects.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Prompt: Generate an image with an intense chiaroscuro effect. The man should retain his original features and expression. Introduce harsh, directional light, appearing to come from above and slightly to the left, casting deep, defined shadows across the face. Only slivers of light illuminating his eyes and cheekbones, the rest of the face is in deep shadow.&lt;/p&gt;
    &lt;p&gt;Bring out the details of your composition by adjusting the depth of field or focal point (e.g., focusing on the flowers).&lt;/p&gt;
    &lt;p&gt;Prompt: Focus on the flowers&lt;/p&gt;
    &lt;head rend="h2"&gt;How you can try Nano Banana Pro today&lt;/head&gt;
    &lt;p&gt;Across our products and services, you now have a choice: the original Nano Banana for fast, fun editing, or Nano Banana Pro for complex compositions requiring the highest quality and visually sophisticated results.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consumers and students: Rolling out globally in the Gemini app when you select ‘Create images’ with the ‘Thinking’ model. Our free-tier users will receive limited free quotas, after which they will revert to the original Nano Banana model. Google AI Plus, Pro and Ultra subscribers receive higher quotas. For AI Mode in Search, Nano Banana Pro is available in the U.S. for Google AI Pro and Ultra subscribers. For NotebookLM, Nano Banana Pro is also available for subscribers globally.&lt;/item&gt;
      &lt;item&gt;Professionals: We're upgrading image generation in Google Ads to Nano Banana Pro to put cutting-edge creative and editing power directly into the hands of advertisers globally. It’s also rolling out starting today to Workspace customers in Google Slides and Vids.&lt;/item&gt;
      &lt;item&gt;Developers and enterprise: Starting to roll out in the Gemini API and Google AI Studio, and in Google Antigravity to create rich UX layouts &amp;amp; mockups; enterprises can start building in Vertex AI for scaled creation today and it’s coming soon to Gemini Enterprise.&lt;/item&gt;
      &lt;item&gt;Creatives: Starting to roll out to Google AI Ultra subscribers in Flow, our AI filmmaking tool, to give creatives, filmmakers and marketers even more precision and control over their frames and scenes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to identify AI-generated images in the Gemini app&lt;/head&gt;
    &lt;p&gt;We believe it’s critical to know when an image is AI-generated. This is why all media generated by Google’s tools are embedded with our imperceptible SynthID digital watermark.&lt;/p&gt;
    &lt;p&gt;Today, we are putting a powerful verification tool directly in consumers’ hands: you can now upload an image into the Gemini app and simply ask if it was generated by Google AI, thanks to SynthID technology. We are starting with images, but will expand to audio and video soon.&lt;/p&gt;
    &lt;p&gt;In addition to SynthID, we will maintain a visible watermark (the Gemini sparkle) on images generated by free and Google AI Pro tier users, to make images even more easy to detect as Google AI-generated.&lt;/p&gt;
    &lt;p&gt;Recognizing the need for a clean visual canvas for professional work, we will remove the visible watermark from images generated by Google AI Ultra subscribers and within the Google AI Studio developer tool.&lt;/p&gt;
    &lt;p&gt;You can find out more about how we’re increasing transparency in AI content with SynthID in our blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/ai/nano-banana-pro/"/><published>2025-11-20T15:04:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45993943</id><title>The Banished Bottom of the Housing Market</title><updated>2025-11-21T03:54:37.279508+00:00</updated><content>&lt;doc fingerprint="6bd1f0d1bcd1668d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Banished Bottom of the Housing Market&lt;/head&gt;
    &lt;head rend="h3"&gt;How America Destroyed Its Cheapest Homes&lt;/head&gt;
    &lt;p&gt;Today, a young man down on his luck in a new city is more likely to land in jail or on the street than on his feet. Fifty years ago, he had another option. A place to wash up, get a hot meal, meet other young men—even start over. All he had to do was put his pride on the shelf and get himself to—well, you can spell it out: the YMCA.&lt;/p&gt;
    &lt;p&gt;The Village People’s 1978 disco hit celebrated one of the less-remembered services offered by the YMCA. From the 1860s, the YMCA began building single-room occupancy (SRO) units “to give young men moving from rural areas safe and affordable lodging in the city.” At its peak in 1940, the YMCA had more than 100,000 rooms—“more than any hotel chain at the time.” The Y wasn’t the only provider of such housing; indeed, there was a vibrant market for hotel living that existed well into the twentieth century.&lt;/p&gt;
    &lt;p&gt;Variously and derogatively known by many names—rooming houses, lodging houses, flophouses—SROs provided affordable, market-rate housing to those at the bottom of the socioeconomic ladder. SROs were the cheapest form of residential hotels, specializing in furnished single rooms for individuals, usually with shared kitchens and bathrooms. A typical SRO rent in 1924 was $230 per month—in today’s dollars.1&lt;/p&gt;
    &lt;p&gt;As late as 1990, as many as two million people lived in residential hotels—more than lived “in all of America’s public housing”—according to Paul Groth, author of Living Downtown.2 Today, not so much. SROs like those offered by the YMCA were the safety net that kept many people off the streets—and their disappearance from the housing supply explains much of modern-day homelessness. What we destroyed wasn’t just a housing type but an entire urban ecosystem: one that provided flexibility, independence, and affordability at scale.&lt;/p&gt;
    &lt;p&gt;As with so much of our urban history, this destruction was by design.&lt;/p&gt;
    &lt;p&gt;From the mid-1800s to the early 1900s, hotel living was a normal way of life for people of all socioeconomic backgrounds. As hotelkeeper Simeon Ford colorfully put it in 1903, “We have fine hotels for fine people, good hotels for good people, plain hotels for plain people, and some bum hotels for bums.” SROs, the “bum hotels,” were the backbone of affordable housing, serving “a great army” of low-paid but skilled workers. Clustered in lively downtown districts with restaurants and services that acted as extensions of the home, SROs offered liberation from family supervision and the constraints of Victorian mores. Rooming house districts let young people mix freely and even allowed same-sex couples to live discreetly—signs of a more secular, modern urban culture. Downtown hotel life, Groth notes, “had the promise to be not just urban but urbane.”&lt;/p&gt;
    &lt;p&gt;And therein lay the problem: the urbanity of SROs collided head-on with the moralism of the Progressive Era.&lt;/p&gt;
    &lt;p&gt;Reformers drew on a long tradition of anti-urban bias, seeing the emergent twentieth-century city as a problem, with cheap hotels at its heart. They pathologized hotel dwellers as “friendless, isolated, needy, and disabled” and cast SROs as “caldrons of social and cultural evil.” Some of the cheapest hotels were unsafe and exploitative, but reformers cared less about improving conditions than about what the hotels symbolized. They blamed rooming houses for loneliness, sexual licentiousness, civic apathy—even suicide. To them, the presence of “social misfits” proved that hotels caused moral disorder. In reality, people lived in SROs because they were cheap and offered greater independence—especially for career-oriented young women. Firm in their belief in the “One Best Way to live,” the reformers exalted the single-family home as the “bulwark of good citizenship” and succeeded in stigmatizing hotel life.&lt;/p&gt;
    &lt;p&gt;By the turn of the century, they set their sights on changing the law.&lt;/p&gt;
    &lt;p&gt;Beginning in the late 19th century, reformers used building and health codes to erase what they saw as “aberrant social lives.” San Francisco’s early building ordinances targeted Chinese lodging houses, while later codes outlawed cheap wooden hotels altogether. By the early 1900s, cities and states were classifying lodging houses as public nuisances. Other laws increased building standards and mandated plumbing fixtures, raising costs and slowing new construction. Urban reformers next embraced exclusionary zoning to separate undesirable people and noxious uses from residential areas. SROs were deemed inappropriate in residential zones, and many codes banned the mixed-use districts that sustained them. In cities like San Francisco, zoning was used to erect a “cordon sanitaire” around the prewar city “to keep old city ideas from contaminating the new.”&lt;/p&gt;
    &lt;p&gt;Residential hotels, like apartments, were swept into the same category of “mere parasitic” uses that Euclid v. Ambler—the 1926 case that upheld zoning—treated as potential health hazards. Redlining and federal loan criteria starved urban hotels of capital, while planners simply omitted them from surveys and censuses—as if their residents didn’t exist. By the urban renewal era, the existence of the old city was itself seen as an affront, and it, too, had to be destroyed.&lt;/p&gt;
    &lt;p&gt;In effect, SROs became a “deliberate casualty” of the new city.&lt;/p&gt;
    &lt;p&gt;Economic and policy shifts hastened their decline. Industrial jobs moved to peripheral locations only accessible by car, urban highway expansion targeted lodging-house neighborhoods, and cities encouraged office development on increasingly valuable downtown land. The “moral blight” of hotel districts had increasingly become economic blight, necessitating renewal. And because SROs didn’t count as “permanent housing” in official statistics, clearance programs could claim to displace no one. San Francisco’s urban renewal experience was typical: redevelopment in and around the Yerba Buena/Moscone Center area ultimately eliminated an estimated 40,000 hotel rooms. The public housing that replaced hotel districts—if it was built at all—often failed to accommodate single adults. To bureaucrats, the bulldozer was salubrious, “eliminating dead tissue” and “clearing away the mistakes of the past.” To the people who lived there, it wiped out their last foothold in the housing market.&lt;/p&gt;
    &lt;p&gt;By the mid-twentieth century, “millions” of SRO hotel rooms disappeared through closures, conversions, and demolition across major cities, and the modest hotel market that remained was shrinking fast. With almost no new SROs built since the 1930s, the remaining stock was vanishing by the thousands in the 1970s. New York City had 200,000 SROs when it banned new hotel construction in 1955; only 30,000 remained by 2014. As tenants changed and land values climbed, owners who once fought to save their lodging houses now wanted out; it was officials who suddenly wanted to preserve them. Tenant movements and new government programs emerged in the 1970s and ’80s, but Reagan-era cuts gutted funding, and many remaining hotels decayed into the “street hotels” opponents had long imagined: unsanitary, unsafe, and unfit for all but the city’s most desperate residents.&lt;/p&gt;
    &lt;p&gt;At the same time, demand for SRO housing was rising sharply. In the 1970s, states emptied mental hospitals without funding alternatives, pushing thousands of people with serious needs into cheap downtown hotels unequipped to support them. What was left of the SRO system became America’s accidental asylum network—the last rung of shelter for those the state had abandoned.&lt;/p&gt;
    &lt;p&gt;Thousands of people were barely hanging on, and a full-blown homelessness crisis had emerged in American cities for the first time since the Great Depression.&lt;/p&gt;
    &lt;p&gt;The SRO crisis was no accident, Groth argues, but the result of government policy at all levels that picked winners and losers in the housing market. The people we now call “chronically homeless” were once simply low-income tenants, housed by the private market in cheap rooms rather than by public programs. Once that market was dismantled, the result was predictable: the homelessness wave of the late 1970s and 1980s followed directly from the destruction of SROs. Today’s crisis—nearly 800,000 unhoused people in 2024—is the long tail of that loss, compounded by decades of underbuilding in expensive cities and soaring rents. As one advocate put it, “The people you see sleeping under bridges used to be valued members of the housing market. They aren’t anymore.”&lt;/p&gt;
    &lt;p&gt;As Alex Horowitz of The Pew Charitable Trusts writes, if SROs had “grown since 1960 at about the same rate as the rest of the U.S. housing stock, the nation would have roughly 2.5 million more such units”—more than three times the number of homeless individuals. While we can’t rebuild the old SRO market we destroyed, cities now face a new opportunity: a vast surplus of obsolete office space that could be converted into inexpensive rooms.&lt;/p&gt;
    &lt;p&gt;Horowitz argues cities should make shared housing—“co-living,” in today’s parlance—legal again. Office vacancies are soaring at the same time deeply affordable housing is vanishing. Horowitz and the architecture firm Gensler modeled what cities could actually build. Their analysis suggests that a vacant office tower could be converted into deeply affordable rooms for half the per-unit cost of new studio apartments. A typical 120–220 square foot unit with shared kitchens and bathrooms could rent to people earning 30–50% of area median income. Urban development has become so expensive that such conversions are likely to only be feasible if subsidized, but Horowitz argues that conversions offer a better way to leverage scarce public dollars: for instance, a $10 million budget might produce 125 co-living rooms instead of 35 studios, providing a way to scale deeply affordable housing much more quickly.&lt;/p&gt;
    &lt;p&gt;It’s a great idea—but in many cities, it’s illegal.&lt;/p&gt;
    &lt;p&gt;While several cities have made efforts to undo SRO prohibitions, in many American metropolises, restrictions abound—from zoning that bans shared housing or residential uses in office districts to minimum-unit-size rules that outlaw SRO-scale rooms. Building codes with strict egress and corridor rules and ventilation requirements make conversions technically infeasible, while parking mandates add unnecessary costs. Meanwhile, “unrelated occupancy” limits prohibit the very household types SROs serve.&lt;/p&gt;
    &lt;p&gt;None of these barriers is structural; every one is a policy choice.&lt;/p&gt;
    &lt;p&gt;We talk endlessly about the “missing middle.” But the real catastrophe was the “banished bottom”—the deliberate destruction of the cheapest rung of the housing ladder. Re-legalizing SROs won’t instantly restore a once deeply affordable housing market that provided housing at scale, but it would at least make it possible for cities to create a much cheaper form of housing that could benefit some of the 11 million extremely low-income renter households and the 800,000 homeless people in America. No city will meaningfully address the need for deeply affordable housing until it restores this missing rung—and accepts that not everyone needs a full apartment to have a full life. Surely, an SRO is better than a sidewalk.&lt;/p&gt;
    &lt;p&gt;Incidentally, while the YMCA is generally not in the SRO business anymore, its Austin branch is redeveloping itself as a mixed-use center with 90 deeply discounted affordable units for families. It’s a worthy project—but it highlights the gap: more than 80% of Austin’s homeless residents are single adults, the very group the Y once housed. We used to have a place they could go—but what happened to that?&lt;/p&gt;
    &lt;p&gt;The Y is as much a question as an answer.&lt;/p&gt;
    &lt;p&gt;Quotations pulled from Living Downtown: The History of Residential Hotels in the United States by Paul Groth.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ryanpuzycki.com/p/the-banished-bottom-of-the-housing"/><published>2025-11-20T15:53:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45994854</id><title>Android and iPhone users can now share files, starting with the Pixel 10</title><updated>2025-11-21T03:54:36.945773+00:00</updated><content>&lt;doc fingerprint="afecea04cdf1cf3c"&gt;
  &lt;main&gt;
    &lt;p&gt;When it comes to sharing moments between family and friends, what device you have shouldn’t matter — sharing should just work. But we’ve heard from many people that they want a simpler way to share files between devices.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing a way for Quick Share to work with AirDrop. This makes file transfer easier between iPhones and Android devices, and starts rolling out today to the Pixel 10 family.&lt;/p&gt;
    &lt;p&gt;We built this with security at its core, protecting your data with strong safeguards that were tested by independent security experts. It’s just one more way we’re bringing better compatibility that people are asking for between operating systems, following our work on RCS and unknown tracker alerts.&lt;/p&gt;
    &lt;p&gt;We’re looking forward to improving the experience and expanding it to more Android devices. See it in action on the Pixel 10 Pro in this video, and try it out for yourself!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/products/android/quick-share-airdrop/"/><published>2025-11-20T17:04:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995394</id><title>Launch HN: Poly (YC S22) – Cursor for Files</title><updated>2025-11-21T03:54:36.504381+00:00</updated><content>&lt;doc fingerprint="1d40ab5b628d8f0a"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hello world, this is Abhay from Poly (&lt;/p&gt;https://poly.app&lt;p&gt;). We’re building an app to replace Finder/File Explorer with something more intelligent and searchable. Think of it like Dropbox + NotebookLM + Perplexity for terabytes of your files. Here’s a quick demo: &lt;/p&gt;https://www.youtube.com/watch?v=RsqCySU4Ln0&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Poly can search your content in natural language, across a broad range of file types and down to the page, paragraph, pixel, or point in time. We also provide an integrated agent that can take actions on your files such as creating, editing, summarizing, and researching. Any action that you can take, the agent can also take, from renaming, moving, tagging, annotating, and organizing files for you. The agent can also read URLs, youtube links, and can search the web and even download files for you.&lt;/p&gt;&lt;p&gt;Here are some public drives that you can poke around in (note: it doesn’t work in Safari yet—sorry! we’re working on it.)&lt;/p&gt;&lt;p&gt;Every issue of the Whole Earth Catalogue: https://poly.app/shared/whole-earth-catalogues&lt;/p&gt;&lt;p&gt;Archive of old Playstation Manuals: https://poly.app/shared/playstation-manuals-archive&lt;/p&gt;&lt;p&gt;Mini archive of Orson Welles interviews and commercial spots: https://poly.app/shared/orson-welles-archive&lt;/p&gt;&lt;p&gt;Archive of Salvador Dali’s paintings for Alice in Wonderland: https://poly.app/shared/salvador-dali-alice-in-wonderland&lt;/p&gt;&lt;p&gt;To try it out, navigate to one of these public folders and use the agent or search to find things. The demo video above can give you an idea of how the UI roughly works. Select files by clicking on them. Quick view by pressing space. Open the details for any file by pressing cmd + i. You can search from the top middle bar (or press cmd + K), and all searches will use semantic similarity and search within the files. Or use the agent from the bottom right tools menu (or press cmd + ?) and you can ask about the files, have the agent search for you, summarize things, etc.&lt;/p&gt;&lt;p&gt;We decided to build this after launching an early image-gen company back in March 2022, and realizing how painful it was for users to store, manage, and search their libraries, especially in a world of generative media. Despite our service having over 150,000 users at that point, we realized that our true calling was fixing the file browser to make it intelligent, so we shut our service down in 2023 and pivoted to this.&lt;/p&gt;&lt;p&gt;We think Poly will be a great fit for anyone that wants to do useful things with their files, such as summarizing research papers, finding the right media or asset, creating a shareable portfolio, searching for a particular form or document, and producing reports and overviews. Of course, it’s a great way to organize your genAI assets as well. Or just use it to organize notes, links, inspo, etc.&lt;/p&gt;&lt;p&gt;Under the hood, Poly is built on our advanced search model, Polyembed-v1 that natively supports multimodal search across text, documents, spreadsheets, presentations, images, audio, video, PDFs, and more. We allow you to search by phrase, file similarity, color, face, and several other kinds of features. The agent is particularly skilled at using the search, so you can type in something like “find me the last lease agreement I signed” and it can go look for it by searching, reading the first few files, searching again if nothing matches, etc. But the quality of our embed model means it almost always finds the file in the first search.&lt;/p&gt;&lt;p&gt;It works identically across web and desktop, except on desktop it syncs your cloud files to a folder (just like google drive). On the web we use clever caching to enable offline support and file conflict recovery. We’ve taken great pains to make our system faster than your existing file browser, even if you’re using it from a web browser.&lt;/p&gt;&lt;p&gt;File storage plans are currently at: 100GB free tier, paid tier is 2TB at $10/m, and 1c per GB per month on top of the 2TB. We also have rate limits for agent use that vary at different tiers.&lt;/p&gt;&lt;p&gt;We’re excited to expand with many features over the following months, including “virtual files” (store your google docs in Poly), sync from other hosting providers, mobile apps, an MCP ecosystem for the agent, access to web search and deep research modes, offline search, local file support (on desktop), third-party sources (WebDAV, NAS), and a whole lot more.&lt;/p&gt;&lt;p&gt;Our waitlist is now open and we’ll be letting folks in starting today! Sign up at https://poly.app.&lt;/p&gt;&lt;p&gt;We’d also love to hear your thoughts (and concerns) about what we’re building, as we’re early in this journey so your feedback can very much shape the future of our company!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45995394"/><published>2025-11-20T17:47:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995740</id><title>Microsoft makes Zork open-source</title><updated>2025-11-21T03:54:36.400696+00:00</updated><content>&lt;doc fingerprint="3d455b5d81aae591"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Preserving code that shaped generations: Zork I, II, and III go Open Source&lt;/head&gt;
    &lt;p&gt;WRITTEN BY&lt;/p&gt;
    &lt;p&gt;/en-us/opensource/blog/author/stacey-haffner&lt;/p&gt;
    &lt;p&gt;/en-us/opensource/blog/author/scott-hanselman&lt;/p&gt;
    &lt;head rend="h3"&gt;A game that changed how we think about play&lt;/head&gt;
    &lt;p&gt;When Zork arrived, it didn’t just ask players to win; it asked them to imagine. There were no graphics, no joystick, and no soundtrack, only words on a screen and the player’s curiosity. Yet those words built worlds more vivid than most games of their time. What made that possible wasn’t just clever writing, it was clever engineering.&lt;/p&gt;
    &lt;p&gt;Beneath that world of words was something quietly revolutionary: the Z-Machine, a custom-built engine. Z-Machine is a specification of a virtual machine, and now there are many Z-Machine interpreters that we used today that are software implementations of that VM. The original mainframe version of Zork was too large for early home computers to handle, so the team at Infocom made a practical choice. They split it into three games titled Zork I, Zork II, and Zork III, all powered by the same underlying system. This also meant that instead of rebuilding the game for each platform, they could use the Z-Machine to interpret the same story files on any computer. That design made Zork one of the first games to be truly cross-platform, appearing on Apple IIs, IBM PCs, and more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preserving a piece of history&lt;/head&gt;
    &lt;p&gt;Game preservation takes many forms, and it’s important to consider research as well as play. The Zork source code deserves to be preserved and studied. Rather than creating new repositories, we’re contributing directly to history. In collaboration with Jason Scott, the well-known digital archivist of Internet Archive fame, we have officially submitted upstream pull requests to the historical source repositories of Zork I, Zork II, and Zork III. Those pull requests add a clear MIT LICENSE and formally document the open-source grant.&lt;/p&gt;
    &lt;p&gt;Each repository includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source code for Zork I, Zork II, and Zork III.&lt;/item&gt;
      &lt;item&gt;Accompanying documentation where available, such as build notes, comments, and historically relevant files.&lt;/item&gt;
      &lt;item&gt;Clear licensing and attribution, via MIT LICENSE.txt and repository-level metadata.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This release focuses purely on the code itself. It does not include commercial packaging or marketing materials, and it does not grant rights to any trademarks or brands, which remain with their respective owners. All assets outside the scope of these titles’ source code are intentionally excluded to preserve historical accuracy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Running Zork I-III today&lt;/head&gt;
    &lt;p&gt;More than forty years later, Zork is still alive and easier than ever to play. The games remain commercially available via The Zork Anthology on Good Old Games. For those who enjoy a more hands on approach, the games can be compiled and run locally using ZILF, the modern Z-Machine interpreter created by Tara McGrew. ZILF compiles ZIL files into Z3s that can be run with Tara’s own ZLR which is a sentence I never thought I’d write, much less say out loud! There are a huge number of wonderful Z-machine runners across all platforms for you to explore.&lt;/p&gt;
    &lt;p&gt;Here's how to get started running Zork locally with ZILF. From the command line, compile and assembly the zork1.zil into a runnable z3 file.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;"%ZILF_PATH%\zilf.exe" zork1.zil &lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;"%ZILF_PATH%\zapf.exe" zork1.zap zork1-ignite.z3&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Then run your Z3 file in a Zmachine runner. I’m using Windows Frotz from David Kinder based on Stefan Jokisch’s Frotz core:&lt;/p&gt;
    &lt;head rend="h3"&gt;Continuing the journey&lt;/head&gt;
    &lt;p&gt;We will use the existing historical repositories as the canonical home for Zork’s source. Once the initial pull requests land under the MIT License, contributions are welcome. We chose MIT for its simplicity and openness because it makes the code easy to study, teach, and build upon. File issues, share insights, or submit small, well-documented improvements that help others learn from the original design. The goal is not to modernize Zork but to preserve it as a space for exploration and education.&lt;/p&gt;
    &lt;p&gt;Zork has always been more than a game. It is a reminder that imagination and engineering can outlast generations of hardware and players. Bringing this code into the open is both a celebration and a thank you to the original Infocom creators for inventing a universe we are still exploring, to Jason Scott and the Internet Archive for decades of stewardship and partnership, and to colleagues across Microsoft OSPO, Xbox, and Activision who helped make open source possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://opensource.microsoft.com/blog/2025/11/20/preserving-code-that-shaped-generations-zork-i-ii-and-iii-go-open-source"/><published>2025-11-20T18:13:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995816</id><title>The Lions Operating System</title><updated>2025-11-21T03:54:35.565139+00:00</updated><content>&lt;doc fingerprint="bccc914e63abc302"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Lions Operating System #&lt;/head&gt;
    &lt;quote&gt;LionsOS is currently undergoing active research and development, it does not have a concrete verification story yet. It is not expected for LionsOS to be stable at this time, but it is available for others to experiment with.&lt;/quote&gt;
    &lt;p&gt;LionsOS is an operating system based on the seL4 microkernel with the goal of making the achievements of seL4 accessible. That is, to provide performance, security, and reliability.&lt;/p&gt;
    &lt;p&gt;LionsOS is being developed by the Trustworthy Systems research group at UNSW Sydney in Australia.&lt;/p&gt;
    &lt;p&gt;It is not a conventional operating system, but contains composable components for creating custom operating systems that are specific to a particular task. Components are joined together using the Microkit tool.&lt;/p&gt;
    &lt;p&gt;The principles on which a LionsOS system is built are laid out fully in the sDDF design document; but in brief they are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Components are connected by lock-free queues using an efficient model-checked signalling mechanism.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As far as is practical, operating systems components do a single thing. Drivers for instance exist solely to convert between a hardware interface and a set of queues to talk to the rest of the system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Components called virtualisers handle multiplexing and control, and conversion between virtual and IO addresses for drivers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Information is shared only where necessary, via the queues, or via published information pages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The system is static: it does not adapt to changing hardware, and does not load components at runtime. There is a mechanism for swapping components of the same type at runtime, to implement policy changes, or to reboot a virtual machine with a new Linux kernel.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be successful, many more components are needed. Pull requests to the various repositories are welcome. See the page on contributing for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lionsos.org"/><published>2025-11-20T18:19:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995834</id><title>NTSB Preliminary Report – UPS Boeing MD-11F Crash [pdf]</title><updated>2025-11-21T03:54:35.471297+00:00</updated><content/><link href="https://www.ntsb.gov/Documents/Prelimiary%20Report%20DCA26MA024.pdf"/><published>2025-11-20T18:20:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45996585</id><title>Data-at-Rest Encryption in DuckDB</title><updated>2025-11-21T03:54:35.260967+00:00</updated><content>&lt;doc fingerprint="bf9f389225e9b2d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Data-at-Rest Encryption in DuckDB&lt;/head&gt;
    &lt;p&gt;TL;DR: DuckDB v1.4 ships database encryption capabilities. In this blog post, we dive into the implementation details of the encryption, show how to use it and demonstrate its performance implications.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you would like to use encryption in DuckDB, we recommend using the latest stable version, v1.4.2. For more details, see the latest release blog post.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Many years ago, we read the excellent “Code Book” by Simon Singh. Did you know that Mary, Queen of Scots, used an encryption method harking back to Julius Caesar to encrypt her more saucy letters? But alas: the cipher was broken and the contents of the letters got her executed.&lt;/p&gt;
    &lt;p&gt;These days, strong encryption software and hardware is a commodity. Modern CPUs come with specialized cryptography instructions, and operating systems small and big contain mostly-robust cryptography software like OpenSSL.&lt;/p&gt;
    &lt;p&gt;Databases store arbitrary information, it is clear that many if not most datasets of any value should perhaps not be plainly available to everyone. Even if stored on tightly controlled hardware like a cloud virtual machine, there have been many cases of files being lost through various privilege escalations. Unsurprisingly, compliance frameworks like the common SOC 2 “highly recommend” encrypting data when stored on storage mediums like hard drives.&lt;/p&gt;
    &lt;p&gt;However, database systems and encryption have a somewhat problematic track record. Even PostgreSQL, the self-proclaimed “The World's Most Advanced Open Source Relational Database” has very limited options for data encryption. SQLite, the world’s “Most Widely Deployed and Used Database Engine” does not support data encryption out-of-the-box, its encryption extension is a $2000 add-on.&lt;/p&gt;
    &lt;p&gt;DuckDB has supported Parquet Modular Encryption for a while. This feature allows reading and writing Parquet files with encrypted columns. However, while Parquet files are great and reports of their impending death are greatly exaggerated, they cannot – for example – be updated in place, a pretty basic feature of a database management system.&lt;/p&gt;
    &lt;p&gt;Starting with DuckDB 1.4.0, DuckDB supports transparent data encryption of data-at-rest using industry-standard AES encryption.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;DuckDB's encryption does not yet meet the official NIST requirements.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Some Basics of Encryption&lt;/head&gt;
    &lt;p&gt;There are many different ways to encrypt data, some more secure than others. In database systems and elsewhere, the standard is the Advanced Encryption Standard (AES), which is a block cipher algorithm standardized by US NIST. AES is a symmetric encryption algorithm, meaning that the same key is used for both encryption and decryption of data.&lt;/p&gt;
    &lt;p&gt;For this reason, most systems choose to only support randomized encryption, meaning that identical plaintexts will always yield different ciphertexts (if used correctly!). The most commonly used industry standard and recommended encryption algorithm is AES – Galois Counter Mode (AES-GCM). This is because on top of its ability to randomize encryption, it also authenticates data by calculating a tag to ensure data has not been tampered with.&lt;/p&gt;
    &lt;p&gt;DuckDB v1.4 supports encryption at rest using AES-GCM-256 and AES-CTR-256 (counter mode) ciphers. AES-CTR is a simpler and faster version of AES-GCM, but less secure, since it does not provide authentication by calculating a tag. The 256 refers to the size of the key in bits, meaning that DuckDB now only supports GCM with 32-byte keys.&lt;/p&gt;
    &lt;p&gt;GCM and CTR both require as input a (1) plaintext, (2) an initialization vector (IV) and (3) an encryption key. Plaintext is the text that a user wants to encrypt. An IV is a unique bytestream of usually 16 bytes, that ensures that identical plaintexts get encrypted into different ciphertexts. A number used once (nonce) is a bytestream of usually 12 bytes, that together with a 4-byte counter construct the IV. Note that the IV needs to be unique for every encrypted block, but it does not necessarily have to be random. Reuse of the same IV is problematic, since an attacker could XOR the two ciphertexts and extract both messages. The tag in AES-GCM is calculated after all blocks are encrypted, pretty much like a checksum, but it adds an integrity check that securely authenticates the entire ciphertext.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation in DuckDB&lt;/head&gt;
    &lt;p&gt;Before diving deeper into how we actually implemented encryption in DuckDB, we’ll explain some things about the DuckDB file format.&lt;/p&gt;
    &lt;p&gt;DuckDB has one main database header which stores data that enables it to correctly load and verify a DuckDB database. At the start of each DuckDB main database header, the magic bytes (“DUCKDB”) are stored and read upon initialization to verify whether the file is a valid DuckDB database file. The magic bytes are followed by four 8-byte of flags that can be set for different purposes.&lt;/p&gt;
    &lt;p&gt;When a database is encrypted in DuckDB, the main database header remains plaintext at all times, since the main header contains no sensitive data about the contents of the database file. Upon initializing an encrypted database, DuckDB sets the first bit in the first flag to indicate that the database is encrypted. After setting this bit, additional metadata is stored that is necessary for encryption. This metadata entails the (1) database identifier, (2) 8 bytes of additional metadata for e.g. the encryption cipher used, and (3) the encrypted canary.&lt;/p&gt;
    &lt;p&gt;The database identifier is used as a “salt”, and consists of 16 randomly generated bytes created upon initialization of each database. The salt is often used to ensure uniqueness, i.e., it makes sure that identical input keys or passwords are transformed into different derived keys. The 8-bytes of metadata comprise the key derivation function (first byte), usage of additional authenticated data (second byte), the encryption cipher (third byte), and the key length (fifth byte). After the metadata, the main header uses the encrypted canary to check if the input key is correct.&lt;/p&gt;
    &lt;head rend="h3"&gt;Encryption Key Management&lt;/head&gt;
    &lt;p&gt;To encrypt data in DuckDB, you can use practically any plaintext or base64 encoded string, but we recommend using a secure 32-byte base64 key. The user itself is responsible for the key management and thus for using a secure key. Instead of directly using the plain key provided by the user, DuckDB always derives a more secure key by means of a key derivation function (kdf). The kdf is a function that reduces or extends the input key to a 32-byte secure key. If the correctness of the input key is checked by deriving the secure key and decrypting the canary, the derived key is managed in a secure encryption key cache. This cache manages encryption keys for the current DuckDB context and ensures that the derived encryption keys are never swapped to disk by locking its memory. To strengthen security even more, the original input keys are immediately wiped from memory when the input keys are transformed into secure derived keys.&lt;/p&gt;
    &lt;head rend="h3"&gt;DuckDB Block Structure&lt;/head&gt;
    &lt;p&gt;After the main database header, DuckDB stores two 4KB database headers that contain more information about e.g. the block (header) size and the storage version used. After keeping the main database header plaintext, all remaining headers and blocks are encrypted when encryption is used.&lt;/p&gt;
    &lt;p&gt;Blocks in DuckDB are by default 256KB, but their size is configurable. At the start of each plaintext block there is an 8-byte block header, which stores an 8-byte checksum. The checksum is a simple calculation that is often used in database systems to check for any corrupted data.&lt;/p&gt;
    &lt;p&gt;For encrypted blocks however, its block header consists of 40 bytes instead of 8 bytes for the checksum. The block header for encrypted blocks contains a 16-byte nonce/IV and, optionally, a 16-byte tag, depending on which encryption cipher is used. The nonce and tag are stored in plaintext, but the checksum is encrypted for better security. Note that the block header always needs to be 8-bytes aligned to calculate the checksum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write-Ahead-Log Encryption&lt;/head&gt;
    &lt;p&gt;The write ahead log (WAL) in database systems is a crash recovery mechanism to ensure durability. It is an append-only file that is used in scenarios where the database crashed or is abruptly closed, and when not all changes are written yet to the main database file. The WAL makes sure these changes can be replayed up to the last checkpoint; which is a consistent snapshot of the database at a certain point in time. This means, when a checkpoint is enforced, which happens in DuckDB by either (1) closing the database or (2) reaching a certain threshold for storage, the WAL gets written into the main database file.&lt;/p&gt;
    &lt;p&gt;In DuckDB, you can force the creation of a WAL by setting&lt;/p&gt;
    &lt;code&gt;PRAGMA disable_checkpoint_on_shutdown;
PRAGMA wal_autocheckpoint = '1TB';
&lt;/code&gt;
    &lt;p&gt;This way you’ll disable a checkpointing on closing the database, meaning that the WAL does not get merged into the main database file. In addition, by setting wal_autocheckpoint to a high threshold, this will avoid intermediate checkpoints to happen and the WAL will persist. For example, we can create a persistent WAL file by first setting the above PRAGMAs, then attach an encrypted database, and then create a table where we insert 3 values.&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.db' AS enc (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'GCM'
);
CREATE TABLE enc.test (a INTEGER, b INTEGER);
INSERT INTO enc.test VALUES (11, 22), (13, 22), (12, 21)
&lt;/code&gt;
    &lt;p&gt;If we now close the DuckDB process, we can see that there is a &lt;code&gt;.wal&lt;/code&gt; file shown: &lt;code&gt;encrypted.db.wal&lt;/code&gt;. But how is the WAL created internally?&lt;/p&gt;
    &lt;p&gt;Before writing new entries (inserts, updates, deletes) to the database, these entries are essentially logged and appended to the WAL. Only after logged entries are flushed to disk, a transaction is considered as committed. A plaintext WAL entry has the following structure:&lt;/p&gt;
    &lt;p&gt;Since the WAL is append-only, we encrypt a WAL entry per value. For AES-GCM this means that we append a nonce and a tag to each entry. The structure in which we do this is depicted in below. When we serialize an encrypted entry to the encrypted WAL, we first store the length in plaintext, because we need to know how many bytes we should decrypt. The length is followed by a nonce, which on its turn is followed by the encrypted checksum and the encrypted entry itself. After the entry, a 16-byte tag is stored for verification.&lt;/p&gt;
    &lt;p&gt;Encrypting the WAL is triggered by default when an encryption key is given for any (un)encrypted database.&lt;/p&gt;
    &lt;head rend="h3"&gt;Temporary File Encryption&lt;/head&gt;
    &lt;p&gt;Temporary files are used to store intermediate data that is often necessary for large, out-of-core operations such as sorting, large joins and window functions. This data could contain sensitive information and can, in case of a crash, remain on disk. To protect this leftover data, DuckDB automatically encrypts temporary files too.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Structure of Temporary Files&lt;/head&gt;
    &lt;p&gt;There are three different types of temporary files in DuckDB: (1) temporary files that have the same layout as a regular 256KB block, (2) compressed temporary files and (3) temporary files that exceed the standard 256KB block size. The former two are suffixed with .tmp, while the latter is distinguished by a suffix with .block. To keep track of the size of .block temporary files, they are always prefixed with its length. As opposed to regular database blocks, temporary files do not contain a checksum to check for data corruption, since the calculation of a checksum is somewhat expensive.&lt;/p&gt;
    &lt;head rend="h4"&gt;Encrypting Temporary Files&lt;/head&gt;
    &lt;p&gt;Temporary files are encrypted (1) automatically when you attach an encrypted database or (2) when you use the setting &lt;code&gt;SET temp_file_encryption = true&lt;/code&gt;. In the latter case, the main database file is plaintext, but the temporary files will be encrypted. For the encryption of temporary files DuckDB internally generates temporary keys. This means that when the database crashes, the temporary keys are also lost. Temporary files cannot be decrypted in this case and are then essentially garbage.&lt;/p&gt;
    &lt;p&gt;To force DuckDB to produce temporary files, you can use a simple trick by just setting the memory limit low. This will create temporary files once the memory limit is exceeded. For example, we can create a new encrypted database, load this database with TPC-H data (SF 1), and then set the memory limit to 1 GB. If we then perform a large join, we force DuckDB to spill intermediate data to disk. For example:&lt;/p&gt;
    &lt;code&gt;SET memory_limit = '1GB';
ATTACH 'tpch_encrypted.db' AS enc (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'cipher'
);
USE enc;
CALL dbgen(sf = 1);

ALTER TABLE lineitem
    RENAME TO lineitem1;
CREATE TABLE lineitem2 AS
    FROM lineitem1;
CREATE OR REPLACE TABLE ans AS
    SELECT l1.* , l2.*
    FROM lineitem1 l1
    JOIN lineitem2 l2 USING (l_orderkey , l_linenumber);
&lt;/code&gt;
    &lt;p&gt;This sequence of commands will result in encrypted temporary files being written to disk. Once the query completes or when the DuckDB shell is exited, the temporary files are automatically cleaned up. In case of a crash however, it may happen that temporary files will be left on disk and need to be cleaned up manually.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Use Encryption in DuckDB&lt;/head&gt;
    &lt;p&gt;In DuckDB, you can (1) encrypt an existing database, (2) initialize a new, empty encrypted database or (3) reencrypt a database. For example, let's create a new database, load this database with TPC-H data of scale factor 1 and then encrypt this database.&lt;/p&gt;
    &lt;code&gt;INSTALL tpch;
LOAD tpch;
ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'unencrypted.duckdb' AS unencrypted;
USE unencrypted;
CALL dbgen(sf = 1);
COPY FROM DATABASE unencrypted TO encrypted;
&lt;/code&gt;
    &lt;p&gt;There is not a trivial way to prove that a database is encrypted, but correctly encrypted data should look like random noise and has a high entropy. So, to check whether a database is actually encrypted, we can use tools to calculate the entropy or visualize the binary, such as ent and binocle.&lt;/p&gt;
    &lt;p&gt;When we use ent after executing the above chunk of SQL, i.e., &lt;code&gt;ent encrypted.duckdb&lt;/code&gt;, this will result in an entropy of 7.99999 bits per byte. If we do the same for the plaintext (unencrypted) database, this results in 7.65876 bits per byte. Note that the plaintext database also has a high entropy, but this is due to compression.&lt;/p&gt;
    &lt;p&gt;Let’s now visualize both the plaintext and encrypted data with binocle. For the visualization we created both a plaintext DuckDB database with scale factor of 0.001 of TPC-H data and an encrypted one:&lt;/p&gt;
    &lt;head&gt;Click here to see the entropy of a plaintext database&lt;/head&gt;
    &lt;head&gt;Click here to see the entropy of an encrypted database&lt;/head&gt;
    &lt;p&gt;In these figures, we can clearly observe that the encrypted database file seems completely random, while the plaintext database file shows some clear structure in its binary data.&lt;/p&gt;
    &lt;p&gt;To decrypt an encrypted database, we can use the following SQL:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'new_unencrypted.duckdb' AS unencrypted;
COPY FROM DATABASE encrypted TO unencrypted;
&lt;/code&gt;
    &lt;p&gt;And to reencrypt an existing database, we can just simply copy the old encrypted database to a new one, like:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'new_encrypted.duckdb' AS new_encrypted (ENCRYPTION_KEY 'xxxx');
COPY FROM DATABASE encrypted TO new_encrypted;
&lt;/code&gt;
    &lt;p&gt;The default encryption algorithm is AES GCM. This is recommended since it also authenticates data by calculating a tag. Depending on the use case, you can also use AES CTR. This is faster than AES GCM since it skips calculating a tag after encrypting all data. You can specify the CTR cipher as follows:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'CTR'
);
&lt;/code&gt;
    &lt;p&gt;To keep track of which databases are encrypted, you can query this by running:&lt;/p&gt;
    &lt;code&gt;FROM duckdb_databases();
&lt;/code&gt;
    &lt;p&gt;This will show which databases are encrypted, and which cipher is used:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;database_name&lt;/cell&gt;
        &lt;cell role="head"&gt;database_oid&lt;/cell&gt;
        &lt;cell role="head"&gt;path&lt;/cell&gt;
        &lt;cell role="head"&gt;…&lt;/cell&gt;
        &lt;cell role="head"&gt;encrypted&lt;/cell&gt;
        &lt;cell role="head"&gt;cipher&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;encrypted&lt;/cell&gt;
        &lt;cell&gt;2103&lt;/cell&gt;
        &lt;cell&gt;encrypted.duckdb&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;true&lt;/cell&gt;
        &lt;cell&gt;GCM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;unencrypted&lt;/cell&gt;
        &lt;cell&gt;2050&lt;/cell&gt;
        &lt;cell&gt;unencrypted.duckdb&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;592&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;system&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;temp&lt;/cell&gt;
        &lt;cell&gt;1995&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;5 rows — 10 columns (5 shown)&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation and Performance&lt;/head&gt;
    &lt;p&gt;Here at DuckDB, we strive to achieve a good out-of-the-box experience with zero external dependencies and a small footprint. Encryption and decryption, however, are usually performed by pretty heavy external libraries such as OpenSSL. We would much prefer not to rely on external libraries or statically linking huge codebases just so that people can use encryption in DuckDB without additional steps. This is why we actually implemented encryption twice in DuckDB, once with the (excellent) Mbed TLS library and once with the ubiquitous OpenSSL library.&lt;/p&gt;
    &lt;p&gt;DuckDB already shipped parts of Mbed TLS because we use it to verify RSA extension signatures. However, for maximum compatibility we actually disabled the hardware acceleration of MbedTLS, which has a performance impact. Furthermore, Mbed TLS is not particularly hardened against things like nasty timing attacks. OpenSSL on the other hand contains heavily vetted and hardware-accelerated code to perform AES operations, which is why we can also use it for encryption.&lt;/p&gt;
    &lt;p&gt;In DuckDB Land, OpenSSL is part of the &lt;code&gt;httpfs&lt;/code&gt; extension. Once you load that extension, encryption will automatically switch to using OpenSSL. After we shipped encryption in DuckDB 1.4.0, security experts actually found issues with the random number generator we used in Mbed TLS mode. Even though it would be difficult to actually exploit this, we disabled writing to databases in MbedTLS mode from DuckDB 1.4.1. Instead, DuckDB now (version 1.4.2+) tries to auto-install and auto-load the &lt;code&gt;httpfs&lt;/code&gt; extension whenever a write is attempted. We might be able to revisit this in the future, but for now this seems the safest path forward that still allows high compatibility for reading. In OpenSSL mode, we always used a cryptographically-safe random number generation so that mode is unaffected.&lt;/p&gt;
    &lt;p&gt;Encrypting and decrypting database files is an additional step in writing tables to disk, so we would naturally assume that there is some performance impact. Let’s investigate the performance impact of DuckDB’s new encryption feature with a very basic experiment.&lt;/p&gt;
    &lt;p&gt;We first create two DuckDB database files, one encrypted and one unencrypted. We use the TPC-H benchmark generator again to create the table data, particularly the (somewhat tired) &lt;code&gt;lineitem&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;INSTALL httpfs;
INSTALL tpch;
LOAD tpch;

ATTACH 'unencrypted.duckdb' AS unencrypted;
CALL dbgen(sf = 10, catalog = 'unencrypted');

ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
CREATE TABLE encrypted.lineitem AS FROM unencrypted.lineitem;
&lt;/code&gt;
    &lt;p&gt;Now we use DuckDB’s neat &lt;code&gt;SUMMARIZE&lt;/code&gt; command three times: once on the unencrypted database, and once on the encrypted database using MbedTLS and once on the encrypted database using OpenSSL. We set a very low memory limit to force more reading and writing from disk.&lt;/p&gt;
    &lt;code&gt;SET memory_limit = '200MB';
.timer on

SUMMARIZE unencrypted.lineitem;
SUMMARIZE encrypted.lineitem;

LOAD httpfs; -- use OpenSSL
SUMMARIZE encrypted.lineitem;
&lt;/code&gt;
    &lt;p&gt;Here are the results on a fairly recent MacBook: &lt;code&gt;SUMMARIZE&lt;/code&gt; on the unencrypted table took ca. 5.4 seconds. Using Mbed TLS, this went up to around 6.2 s. However, when enabling OpenSSL the end-to-end time went straight back to 5.4 s. How is this possible? Is decryption not expensive? Well, there is a lot more happening in query processing than reading blocks from storage. So the impact of decryption is not all that huge, even when using a slow implementation. Secondly, when using hardware acceleration in OpenSSL, the overall overhead of encryption and decryption becomes almost negligible.&lt;/p&gt;
    &lt;p&gt;But just running summarization is overly simplistic. Real™ database workloads include modifications to data, insertion of new rows, updates of rows, deletion of rows etc. Also, multiple clients will be updating and querying at the same time. So we re-surrected the full TPC-H “Power” test from our previous blog post “Changing Data with Confidence and ACID”. We slightly tweaked the benchmark script to enable the new database encryption. For this experiment, we used the OpenSSL encryption implementation due to the issues outlined above. We observe Power@Size” and “Throughput@Size”. The former is raw sequential query performance, while the latter measures multiple parallel query streams in the presence of updates.&lt;/p&gt;
    &lt;p&gt;When running on the same MacBook with DuckDB 1.4.1 and a “scale factor” of 100, we get a Power@Size metric of 624,296 and a Throughput@Size metric of 450,409 without encryption.&lt;/p&gt;
    &lt;p&gt;When we enable encryption, the results are almost unchanged, confirming the observation of the small microbenchmark above. However, the relationship between available memory and the benchmark size means that we’re not stressing temporary file encryption. So we re-ran everything with an 8GB memory limit. We confirmed constant reading and writing to and from disk in this configuration by observing operating system statistics. For the unencrypted case, the Power@Size metric predictably went down to 591,841 and Throughput@Size went down to 153,690. And finally, we could observe a slight performance decrease with Power@Size of 571,985 and Throughput@Size of 145,353. However, that difference is not very great either and likely not relevant in real operational scenarios.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;With the new encrypted database feature, we can now safely pass around DuckDB database files with all information inside them completely opaque to prying eyes. This allows for some interesting new deployment models for DuckDB, for example, we could now put an encrypted DuckDB database file on a Content Delivery Network (CDN). A fleet of DuckDB instances could attach to this file read-only using the decryption key. This elegantly allows efficient distribution of private background data in a similar way like encrypted Parquet files, but of course with many more features like multi-table storage. When using DuckDB with encrypted storage, we can also simplify threat modeling when – for example – using DuckDB on cloud providers. While in the past access to DuckDB storage would have been enough to leak data, we can now relax paranoia regarding storage a little, especially since temporary files and WAL are also encrypted. And the best part of all of this, there is almost no performance overhead to using encryption in DuckDB, especially with the OpenSSL implementation.&lt;/p&gt;
    &lt;p&gt;We are very much looking forward to what you are going to do with this feature, and please let us know if you run into any issues.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://duckdb.org/2025/11/19/encryption-in-duckdb"/><published>2025-11-20T19:26:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45996860</id><title>CBP is monitoring US drivers and detaining those with suspicious travel patterns</title><updated>2025-11-21T03:54:34.862406+00:00</updated><content>&lt;doc fingerprint="ec590b983760b349"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Border Patrol is monitoring US drivers and detaining those with ‘suspicious’ travel patterns&lt;/head&gt;
    &lt;p&gt;The U.S. Border Patrol is monitoring millions of American drivers nationwide in a secretive program to identify and detain people whose travel patterns it deems suspicious. (AP video: Marshall Ritzel)&lt;/p&gt;
    &lt;p&gt;The U.S. Border Patrol is monitoring millions of American drivers nationwide in a secretive program to identify and detain people whose travel patterns it deems suspicious, The Associated Press has found.&lt;/p&gt;
    &lt;p&gt;The predictive intelligence program has resulted in people being stopped, searched and in some cases arrested. A network of cameras scans and records vehicle license plate information, and an algorithm flags vehicles deemed suspicious based on where they came from, where they were going and which route they took. Federal agents in turn may then flag local law enforcement.&lt;/p&gt;
    &lt;p&gt;Suddenly, drivers find themselves pulled over — often for reasons cited such as speeding, failure to signal, the wrong window tint or even a dangling air freshener blocking the view. They are then aggressively questioned and searched, with no inkling that the roads they drove put them on law enforcement’s radar.&lt;/p&gt;
    &lt;p&gt;Once limited to policing the nation’s boundaries, the Border Patrol has built a surveillance system stretching into the country’s interior that can monitor ordinary Americans’ daily actions and connections for anomalies instead of simply targeting wanted suspects. Started about a decade ago to fight illegal border-related activities and the trafficking of both drugs and people, it has expanded over the past five years.&lt;/p&gt;
    &lt;p&gt;The Border Patrol has recently grown even more powerful through collaborations with other agencies, drawing information from license plate readers nationwide run by the Drug Enforcement Administration, private companies and, increasingly, local law enforcement programs funded through federal grants. Texas law enforcement agencies have asked Border Patrol to use facial recognition to identify drivers, documents show.&lt;/p&gt;
    &lt;p&gt;This active role beyond the borders is part of the quiet transformation of its parent agency, U.S. Customs and Border Protection, into something more akin to a domestic intelligence operation. Under the Trump administration’s heightened immigration enforcement efforts, CBP is now poised to get more than $2.7 billion to build out border surveillance systems such as the license plate reader program by layering in artificial intelligence and other emerging technologies.&lt;/p&gt;
    &lt;p&gt;The result is a mass surveillance network with a particularly American focus: cars.&lt;/p&gt;
    &lt;p&gt;This investigation, the first to reveal details of how the program works on America’s roads, is based on interviews with eight former government officials with direct knowledge of the program who spoke on the condition of anonymity because they weren’t authorized to speak to the media, as well as dozens of federal, state and local officials, attorneys and privacy experts. The AP also reviewed thousands of pages of court and government documents, state grant and law enforcement data, and arrest reports.&lt;/p&gt;
    &lt;p&gt;The Border Patrol has for years hidden details of its license plate reader program, trying to keep any mention of the program out of court documents and police reports, former officials say, even going so far as to propose dropping charges rather than risk revealing any details about the placement and use of their covert license plate readers. Readers are often disguised along highways in traffic safety equipment like drums and barrels.&lt;/p&gt;
    &lt;p&gt;The Border Patrol has defined its own criteria for which drivers’ behavior should be deemed suspicious or tied to drug or human trafficking, stopping people for anything from driving on backcountry roads, being in a rental car or making short trips to the border region. The agency’s network of cameras now extends along the southern border in Texas, Arizona and California, and also monitors drivers traveling near the U.S.-Canada border.&lt;/p&gt;
    &lt;p&gt;And it reaches far into the interior, impacting residents of big metropolitan areas and people driving to and from large cities such as Chicago and Detroit, as well as from Los Angeles, San Antonio, and Houston to and from the Mexican border region. In one example, AP found the agency has placed at least four cameras in the greater Phoenix area over the years, one of which was more than 120 miles (193 kilometers) from the Mexican frontier, beyond the agency’s usual jurisdiction of 100 miles (161 kilometers) from a land or sea border. The AP also identified several camera locations in metropolitan Detroit, as well as one placed near the Michigan-Indiana border to capture traffic headed towards Chicago or Gary, Indiana, or other nearby destinations.&lt;/p&gt;
    &lt;p&gt;Border Patrol’s parent agency, U.S. Customs and Border Protection, said they use license plate readers to help identify threats and disrupt criminal networks and are “governed by a stringent, multi-layered policy framework, as well as federal law and constitutional protections, to ensure the technology is applied responsibly and for clearly defined security purposes.”&lt;/p&gt;
    &lt;p&gt;“For national security reasons, we do not detail the specific operational applications,” the agency said. While the U.S. Border Patrol primarily operates within 100 miles of the border, it is legally allowed “to operate anywhere in the United States,” the agency added.&lt;/p&gt;
    &lt;p&gt;While collecting license plates from cars on public roads has generally been upheld by courts, some legal scholars see the growth of large digital surveillance networks such as Border Patrol’s as raising constitutional questions. Courts have started to recognize that “large-scale surveillance technology that’s capturing everyone and everywhere at every time” might be unconstitutional under the Fourth Amendment, which protects people from unreasonable searches, said Andrew Ferguson, a law professor at George Washington University.&lt;/p&gt;
    &lt;p&gt;Today, predictive surveillance is embedded into America’s roadways. Mass surveillance techniques are also used in a range of other countries, from authoritarian governments such as China to, increasingly, democracies in the U.K. and Europe in the name of national security and public safety.&lt;/p&gt;
    &lt;p&gt;“They are collecting mass amounts of information about who people are, where they go, what they do, and who they know … engaging in dragnet surveillance of Americans on the streets, on the highways, in their cities, in their communities,” Nicole Ozer, the executive director of the Center for Constitutional Democracy at UC Law San Francisco, said in response to the AP’s findings. “These surveillance systems do not make communities safer.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘We did everything right and had nothing to hide’&lt;/head&gt;
    &lt;p&gt;In February, Lorenzo Gutierrez Lugo, a driver for a small trucking company that specializes in transporting furniture, clothing and other belongings to families in Mexico, was driving south to the border city of Brownsville, Texas, carrying packages from immigrant communities in South Carolina’s low country.&lt;/p&gt;
    &lt;p&gt;Gutierrez Lugo was pulled over by a local police officer in Kingsville, a small Texas city near Corpus Christi that lies about 100 miles from the Mexican border. The officer, Richard Beltran, cited the truck’s speed of 50 mph (80 kph) in a 45 mph (72 kph) zone as the reason for the stop.&lt;/p&gt;
    &lt;p&gt;But speeding was a pretext: Border Patrol had requested the stop and said the black Dodge pickup with a white trailer could contain contraband, according to police and court records. U.S. Route 77 passes through Kingsville, a route that state and federal authorities scrutinize for trafficking of drugs, money and people.&lt;/p&gt;
    &lt;p&gt;Gutierrez Lugo, who through a lawyer declined to comment, was interrogated about the route he drove, based on license plate reader data, per the police report and court records. He consented to a search of his car by Beltran and Border Patrol agents, who eventually arrived to assist.&lt;/p&gt;
    &lt;p&gt;They unearthed no contraband. But Beltran arrested Gutierrez Lugo on suspicion of money laundering and engaging in organized criminal activity because he was carrying thousands of dollars in cash — money his supervisor said came directly from customers in local Latino communities, who are accustomed to paying in cash. No criminal charges were ultimately brought against Gutierrez Lugo and an effort by prosecutors to seize the cash, vehicle and trailer as contraband was eventually dropped.&lt;/p&gt;
    &lt;p&gt;Luis Barrios owns the trucking company, Paquetería El Guero, that employed the driver. He told AP he hires people with work authorization in the United States and was taken aback by the treatment of his employee and his trailer.&lt;/p&gt;
    &lt;p&gt;“We did everything right and had nothing to hide, and that was ultimately what they found,” said Barrios, who estimates he spent $20,000 in legal fees to clear his driver’s name and get the trailer out of impound.&lt;/p&gt;
    &lt;p&gt;Border Patrol agents and local police have many names for these kinds of stops: “whisper,” “intel” or “wall” stops. Those stops are meant to conceal — or wall off — that the true reason for the stop is a tip from federal agents sitting miles away, watching data feeds showing who’s traveling on America’s roads and predicting who is “suspicious,” according to documents and people interviewed by the AP.&lt;/p&gt;
    &lt;p&gt;In 2022, a man from Houston had his car searched from top to bottom by Texas sheriff’s deputies outside San Antonio after they got a similar tipoff from Border Patrol agents about the driver, Alek Schott.&lt;/p&gt;
    &lt;p&gt;Federal agents observed that Schott had made an overnight trip from Houston to Carrizo Springs, Texas, and back, court records show. They knew he stayed overnight in a hotel about 80 miles (129 kilometers) from the U.S.-Mexico border. They knew that in the morning Schott met a female colleague there before they drove together to a business meeting.&lt;/p&gt;
    &lt;p&gt;At Border Patrol’s request, Schott was pulled over by Bexar County sheriff’s deputies. The deputies held Schott by the side of the road for more than an hour, searched his car and found nothing.&lt;/p&gt;
    &lt;p&gt;“The beautiful thing about the Texas Traffic Code is there’s thousands of things you can stop a vehicle for,” said Joel Babb, the sheriff’s deputy who stopped Schott’s car, in a deposition in a lawsuit Schott filed alleging violations of his constitutional rights.&lt;/p&gt;
    &lt;p&gt;According to testimony and documents released as part of Schott’s lawsuit, Babb was on a group chat with federal agents called Northwest Highway. Babb deleted the WhatsApp chat off his phone but Schott’s lawyers were able to recover some of the text messages.&lt;/p&gt;
    &lt;p&gt;Through a public records act request, the AP also obtained more than 70 pages of the Northwest Highway group chats from June and July of this year from a Texas county that had at least one sheriff’s deputy active in the chat. The AP was able to associate numerous phone numbers in both sets of documents with Border Patrol agents and Texas law enforcement officials.&lt;/p&gt;
    &lt;p&gt;The chat logs show Border Patrol agents and Texas sheriffs deputies trading tips about vehicles’ travel patterns — based on suspicions about little more than someone taking a quick trip to the border region and back. The chats show how thoroughly Texas highways are surveilled by this federal-local partnership and how much detailed information is informally shared.&lt;/p&gt;
    &lt;p&gt;In one exchange a law enforcement official included a photo of someone’s driver’s license and told the group the person, who they identified using an abbreviation for someone in the country illegally, was headed westbound. “Need BP?,” responded a group member whose number was labeled “bp Intel.” “Yes sir,” the official answered, and a Border Patrol agent was en route.&lt;/p&gt;
    &lt;p&gt;Border Patrol agents and local law enforcement shared information about U.S. citizens’ social media profiles and home addresses with each other after stopping them on the road. Chats show Border Patrol was also able to determine whether vehicles were rentals and whether drivers worked for rideshare services.&lt;/p&gt;
    &lt;p&gt;In Schott’s case, Babb testified that federal agents “actually watch travel patterns on the highway” through license plate scans and other surveillance technologies. He added: “I just know that they have a lot of toys over there on the federal side.”&lt;/p&gt;
    &lt;p&gt;After finding nothing in Schott’s car, Babb said “nine times out of 10, this is what happens,” a phrase Schott’s lawyers claimed in court filings shows the sheriff’s department finds nothing suspicious in most of its searches. Babb did not respond to multiple requests for comment from AP.&lt;/p&gt;
    &lt;p&gt;The Bexar County sheriff’s office declined to comment due to pending litigation and referred all questions about the Schott case to the county’s district attorney. The district attorney did not respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;The case is pending in federal court in Texas. Schott said in an interview with the AP: “I didn’t know it was illegal to drive in Texas.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘Patterns of life’ and license plates&lt;/head&gt;
    &lt;p&gt;Today, the deserts, forests and mountains of the nation’s land borders are dotted with checkpoints and increasingly, surveillance towers, Predator drones, thermal cameras and license plate readers, both covert and overt.&lt;/p&gt;
    &lt;p&gt;Border Patrol’s parent agency got authorization to run a domestic license plate reader program in 2017, according to a Department of Homeland Security policy document. At the time, the agency said that it might use hidden license plate readers ”for a set period of time while CBP is conducting an investigation of an area of interest or smuggling route. Once the investigation is complete, or the illicit activity has stopped in that area, the covert cameras are removed,” the document states.&lt;/p&gt;
    &lt;p&gt;But that’s not how the program has operated in practice, according to interviews, police reports and court documents. License plate readers have become a major — and in some places permanent — fixture of the border region.&lt;/p&gt;
    &lt;p&gt;In a budget request to Congress in fiscal year 2024, CBP said that its Conveyance Monitoring and Predictive Recognition System, or CMPRS, “collects license plate images and matches the processed images against established hot lists to assist … in identifying travel patterns indicative of illegal border related activities.” Several new developer jobs have been posted seeking applicants to help modernize its license plate surveillance system in recent months. Numerous Border Patrol sectors now have special intelligence units that can analyze license plate reader data, and tie commercial license plate readers to its national network, according to documents and interviews.&lt;/p&gt;
    &lt;p&gt;Border Patrol worked with other law enforcement agencies in Southern California about a decade ago to develop pattern recognition, said a former CBP official who spoke on the condition of anonymity for fear of reprisal. Over time, the agency learned to develop what it calls “patterns of life” of vehicle movements by sifting through the license plate data and determining “abnormal” routes, evaluating if drivers were purposely avoiding official checkpoints. Some cameras can take photos of a vehicle’s plates as well as its driver’s face, the official said.&lt;/p&gt;
    &lt;p&gt;Another former Border Patrol official compared it to a more technologically sophisticated version of what agents used to do in the field — develop hunches based on experience about which vehicles or routes smugglers might use, find a legal basis for the stop like speeding and pull drivers over for questioning.&lt;/p&gt;
    &lt;p&gt;The cameras take pictures of vehicle license plates. Then, the photos are “read” by the system, which automatically detects and distills the images into numbers and letters, tied to a geographic location, former CBP officials said. The AP could not determine how precisely the system’s algorithm defines a quick turnaround or an odd route. Over time, the agency has amassed databases replete with images of license plates, and the system’s algorithm can flag an unusual “pattern of life” for human inspection.&lt;/p&gt;
    &lt;p&gt;The Border Patrol also has access to a nationwide network of plate readers run by the Drug Enforcement Administration, documents show, and was authorized in 2020 to access license plate reader systems sold by private companies. In documents obtained by the AP, a Border Patrol official boasted about being able to see that a vehicle that had traveled to “Dallas, Little Rock, Arkansas and Atlanta” before ending up south of San Antonio.&lt;/p&gt;
    &lt;p&gt;Documents show that Border Patrol or CBP has in the past had access to data from at least three private sector vendors: Rekor, Vigilant Solutions and Flock Safety.&lt;/p&gt;
    &lt;p&gt;Through Flock alone, Border Patrol for a time had access to at least 1,600 license plate readers across 22 states, and some counties have reported looking up license plates on behalf of CBP even in states like California and Illinois that ban sharing data with federal immigration authorities, according to an AP analysis of police disclosures. A Flock spokesperson told AP the company “for now” had paused its pilot programs with CBP and a separate DHS agency, Homeland Security Investigations, and declined to discuss the type or volume of data shared with either federal agency, other than to say agencies could search for vehicles wanted in conjunction with a crime. No agencies currently list Border Patrol as receiving Flock data. Vigilant and Rekor did not respond to requests for comment.&lt;/p&gt;
    &lt;p&gt;Also from AP’s investigation into the use of surveillance technology:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;U.S. tech firms to a large degree designed and built China’s surveillance state, playing a far greater role in enabling rights abuses than known before.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Across five Republican and Democratic administrations, the U.S. government has repeatedly allowed and even actively helped American firms to sell technology to Chinese police.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Where Border Patrol places its cameras is a closely guarded secret. However, through public records requests, the AP obtained dozens of permits the agency filed with Arizona and Michigan for permission to place cameras on state-owned land. The permits show the agency frequently disguises its cameras by concealing them in traffic equipment like the yellow and orange barrels that dot American roadways, or by labeling them as jobsite equipment. An AP photographer in October visited the locations identified in more than two dozen permit applications in Arizona, finding that most of the Border Patrol’s hidden equipment remains in place today. Spokespeople for the Arizona and Michigan departments of transportation said they approve permits based on whether they follow state and federal rules and are not privy to details on how license plate readers are used.&lt;/p&gt;
    &lt;p&gt;Texas, California, and other border states did not provide documents in response to the AP’s public records requests.&lt;/p&gt;
    &lt;p&gt;CBP’s attorneys and personnel instructed local cities and counties in both Arizona and Texas to withhold records from the AP that might have revealed details about the program’s operations, even though they were requested under state open records laws, according to emails and legal briefs filed with state governments. For example, CBP claimed records requested by the AP in Texas “would permit private citizens to anticipate weaknesses in a police department, avoid detection, jeopardize officer safety, and generally undermine police efforts.” Michigan redacted the exact locations of Border Patrol equipment, but the AP was able to determine general locations from the name of the county.&lt;/p&gt;
    &lt;p&gt;One page of the group chats obtained by the AP shows that a participant enabled WhatsApp’s disappearing messages feature to ensure communications were deleted automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transformation of CBP into intelligence agency&lt;/head&gt;
    &lt;p&gt;The Border Patrol’s license plate reader program is just one part of a steady transformation of its parent agency, CBP, in the years since 9/11 into an intelligence operation whose reach extends far beyond borders, according to interviews with former officials.&lt;/p&gt;
    &lt;p&gt;CBP has quietly amassed access to far more information from ports of entry, airports and intelligence centers than other local, state and federal law enforcement agencies. And like a domestic spy agency, CBP has mostly hidden its role in the dissemination of intelligence on purely domestic travel through its use of whisper stops.&lt;/p&gt;
    &lt;p&gt;Border Patrol has also extended the reach of its license plate surveillance program by paying for local law enforcement to run plate readers on their behalf.&lt;/p&gt;
    &lt;p&gt;A federal grant program called Operation Stonegarden, which has existed in some form for nearly two decades, has handed out hundreds of millions of dollars to buy automated license plate readers, camera-equipped drones and other surveillance gear for local police and sheriffs agencies. Stonegarden grant funds also pay for local law enforcement overtime, which deputizes local officers to work on Border Patrol enforcement priorities. Under President Donald Trump, the Republican-led Congress this year allocated $450 million for Stonegarden to be handed out over the next four fiscal years. In the previous four fiscal years, the program gave out $342 million.&lt;/p&gt;
    &lt;p&gt;In Cochise County, Arizona, Sheriff Mark Dannels said Stonegarden grants, which have been used to buy plate readers and pay for overtime, have let his deputies merge their mission with Border Patrol’s to prioritize border security.&lt;/p&gt;
    &lt;p&gt;“If we’re sharing our authorities, we can put some consequences behind, or deterrence behind, ‘Don’t come here,’” he said.&lt;/p&gt;
    &lt;p&gt;In 2021, the Ward County, Texas, sheriff sought grant funding from DHS to buy a “covert, mobile, License Plate Reader” to pipe data to Border Patrol’s Big Bend Sector Intelligence Unit. The sheriff’s department did not respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Other documents AP obtained show that Border Patrol connects locally owned and operated license plate readers bought through Stonegarden grants to its computer systems, vastly increasing the federal agency’s surveillance network.&lt;/p&gt;
    &lt;p&gt;How many people have been caught up in the Border Patrol’s dragnet is unknown. One former Border Patrol agent who worked on the license plate reader pattern detection program in California said the program had an 85% success rate of discovering contraband once he learned to identify patterns that looked suspicious. But another former official in a different Border Patrol sector said he was unaware of successful interdictions based solely on license plate patterns.&lt;/p&gt;
    &lt;p&gt;In Trump’s second term, Border Patrol has extended its reach and power as border crossings have slowed to historic lows and freed up agents for operations in the heartland. Border Patrol Sector Chief Gregory Bovino, for example, was tapped to direct hundreds of agents from multiple DHS agencies in the administration’s immigration sweeps across Los Angeles, more than 150 miles (241 kilometers) from his office in El Centro, California. Bovino later was elevated to lead the aggressive immigration crackdown in Chicago. Numerous Border Patrol officials have also been tapped to replace ICE leadership.&lt;/p&gt;
    &lt;p&gt;The result has been more encounters between the agency and the general public than ever before.&lt;/p&gt;
    &lt;p&gt;“We took Alek’s case because it was a clear-cut example of an unconstitutional traffic stop,” said Christie Hebert, who works at the nonprofit public interest law firm Institute for Justice and represents Schott. ”What we found was something much larger — a system of mass surveillance that threatens people’s freedom of movement.”&lt;/p&gt;
    &lt;p&gt;AP found numerous other examples similar to what Schott and the delivery driver experienced in reviewing court records in border communities and along known smuggling routes in Texas and California. Several police reports and court records the AP examined cite “suspicious” travel patterns or vague tipoffs from the Border Patrol or other unnamed law enforcement agencies. In another federal court document filed in California, a Border Patrol agent acknowledged “conducting targeted analysis on vehicles exhibiting suspicious travel patterns” as the reason he singled out a Nissan Altima traveling near San Diego.&lt;/p&gt;
    &lt;p&gt;In cases reviewed by the AP, local law enforcement sometimes tried to conceal the role the Border Patrol plays in passing along intelligence. Babb, the deputy who stopped Schott, testified he typically uses the phrase “subsequent to prior knowledge” when describing whisper stops in his police reports to acknowledge that the tip came from another law enforcement agency without revealing too much in written documents he writes memorializing motorist encounters.&lt;/p&gt;
    &lt;p&gt;Once they pull over a vehicle deemed suspicious, officers often aggressively question drivers about their travels, their belongings, their jobs, how they know the passengers in the car, and much more, police records and bodyworn camera footage obtained by the AP show. One Texas officer demanded details from a man about where he met his current sexual partner. Often drivers, such as the one working for the South Carolina moving company, were arrested on suspicion of money laundering merely for carrying a few thousand dollars worth of cash, with no apparent connection to illegal activity. Prosecutors filed lawsuits to try to seize money or vehicles on the suspicion they were linked to trafficking.&lt;/p&gt;
    &lt;p&gt;Schott warns that for every success story touted by Border Patrol, there are far more innocent people who don’t realize they’ve become ensnared in a technology-driven enforcement operation.&lt;/p&gt;
    &lt;p&gt;“I assume for every one person like me, who’s actually standing up, there’s a thousand people who just don’t have the means or the time or, you know, they just leave frustrated and angry. They don’t have the ability to move forward and hold anyone accountable,” Schott said. “I think there’s thousands of people getting treated this way.”&lt;/p&gt;
    &lt;p&gt;—-&lt;/p&gt;
    &lt;p&gt;Tau reported from Washington, Laredo, San Antonio, Kingsville and Victoria, Texas. Burke reported from San Francisco. AP writers Aaron Kessler in Washington, Jim Vertuno in San Antonio, AP video producer Serginho Roosblad in Bisbee, Arizona, and AP photographers Ross D. Franklin in Phoenix and David Goldman in Houston contributed reporting. Ismael M. Belkoura in Washington also contributed.&lt;/p&gt;
    &lt;p&gt;—-&lt;/p&gt;
    &lt;p&gt;Contact AP’s global investigative team at [email protected] or https://www.ap.org/tips/.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apnews.com/article/immigration-border-patrol-surveillance-drivers-ice-trump-9f5d05469ce8c629d6fecf32d32098cd"/><published>2025-11-20T19:52:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997099</id><title>OOP is shifting between domains, not disappearing</title><updated>2025-11-21T03:54:34.454068+00:00</updated><content>&lt;doc fingerprint="b4b99baadbc207d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We are replacing OOP with something worse&lt;/head&gt;
    &lt;p&gt;OOP is shifting between domains, not disappearing. I think that's usually a bad thing.&lt;/p&gt;
    &lt;p&gt;Many bytes have been spilled on the topic of object-oriented programming: What is it? Why is it? Is it good? I’m not sure I have the answers to these questions, but I have observed an interesting trend that I think has flown under the radar: OOP is not disappearing, but shifting across domains.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some quick and entirely incorrect history&lt;/head&gt;
    &lt;p&gt;In times of old, people wrote programs. Things were easy and simple. Then, a manager that didn’t know how much trouble they were getting themselves into asked two programmers to work on the same program. Bad things happened.&lt;/p&gt;
    &lt;p&gt;Some bright spark realised that bugs often appeared at the intersection of software functionality, and that it might be a sensible idea to perform a bit of invasive surgery and separate those functions with an interface: an at-least-vaguely specified contract describing the behaviour the two functions might expect from one-another.&lt;/p&gt;
    &lt;p&gt;Other bright sparks jumped in on the action: what if this separation did not rely on the personal hygiene of the programmers - something that should always be called into question for public health reasons - and was instead enforced by the language? Components might hide their implementation by default and communicate only though a set of public functions, and the language might reject programs that tried to skip around these barricades. How quaint.&lt;/p&gt;
    &lt;p&gt;Nowadays, we have a myriad of terms for these concepts, and others which followed in an attempt to further propagate the core idea: encapsulation, inheritance, polymorphism. All have the goal of attenuation the information that might travel between components by force. This core idea isn’t unique to OOP, of course, but it is OOP that champions it and flies its coat of arms into battle with fervour.&lt;/p&gt;
    &lt;head rend="h2"&gt;Programs-as-classes&lt;/head&gt;
    &lt;p&gt;At around the same time, some bright spark realised that programmers - a population of people not known for good hygiene - might also not produce the most hygienic of programs, and that it was perhaps important not to trust all of the little doo-dahs that ran on your computer. And so the process boundary was born, and operating systems morphed from friendly personal assistants with the goal of doing the dirty work of programs into childminders, whose work mainly consisted of ensuring that those within their care did not accidentally feed one-another snails or paperclips.&lt;/p&gt;
    &lt;p&gt;In tandem, other bright sparks were discovering that computers could be made to talk to one-another, and that perhaps this might be useful. Now, programs written by people that didn’t even know one-another - let alone trust one-another - could start interacting.&lt;/p&gt;
    &lt;p&gt;When trust dissolves, societies tends to overzealously establish the highest and thickest walls they can, no matter the cost. Software developers are no different. When every program has evolved into a whirlwind of components created by an army of developers that rarely know of their software’s inclusion, much less communicate about it, then the only reasonable reaction is maximum distrust.&lt;/p&gt;
    &lt;p&gt;And so, the process/network boundary naturally became that highest and thickest wall - just in time for it to replace the now-ageing philosophy of object-oriented programming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was it worth it?&lt;/head&gt;
    &lt;p&gt;Our world today is one of microservices, of dockers, of clusters, of ‘scaling’. The great irony is that for all of the OOP-scepticism you’ll hear when whispering of Java to a colleague, we have replaced it with a behemoth with precisely the same flaws - but magnified tenfold. OpenAPI schemas replace type-checkers, docker compose replaces service factories, Kubernetes replaces the event loop. Every call across components acrues failure modes, requires a slow march through (de)serialisation libraries, a long trek through the kernel’s scheduler. A TLB cache invalidation here, a socket poll there. Perhaps a sneaky HTTP request to localhost for desert.&lt;/p&gt;
    &lt;p&gt;I am not convinced by the promises of OOP, but I am even less convinced by the weasel words of that which has replaced it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jsbarretto.com/post/actors"/><published>2025-11-20T20:15:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997212</id><title>New OS aims to provide (some) compatibility with macOS</title><updated>2025-11-21T03:54:34.036275+00:00</updated><content>&lt;doc fingerprint="c7963a51c657afa6"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Don't speak English? Read this in: Italiano, Türkçe, Deutsch, Indonesia, 简体中文, 繁體中文, Português do Brasil, 한국어, فارسی, Magyar&lt;/head&gt;
    &lt;p&gt;ravynOS is a new open source OS project that aims to provide a similar experience and some compatibility with macOS on x86-64 (and eventually ARM) systems. It builds on the solid foundations of FreeBSD, existing open source packages in the same space, and new code to fill the gaps.&lt;/p&gt;
    &lt;p&gt;The main design goals are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source compatibility with macOS applications (i.e. you could compile a Mac application on ravynOS and run it)&lt;/item&gt;
      &lt;item&gt;Similar GUI metaphors and familiar UX (file manager, application launcher, top menu bar that reflects the open application, etc)&lt;/item&gt;
      &lt;item&gt;Compatible with macOS folder layouts (/Library, /System, /Users, /Volumes, etc) and perhaps filesystems (HFS+, APFS) as well as fully supporting ZFS&lt;/item&gt;
      &lt;item&gt;Self-contained applications in App Bundles, AppDirs, and AppImage files - an installer-less experience for /Applications&lt;/item&gt;
      &lt;item&gt;Mostly maintain compatibility with the FreeBSD base system and X11 - a standard Unix environment under the hood&lt;/item&gt;
      &lt;item&gt;Compatible with Linux binaries via FreeBSD's Linux support&lt;/item&gt;
      &lt;item&gt;Eventual compatibility with x86-64/arm64 macOS binaries (Mach-O) and libraries&lt;/item&gt;
      &lt;item&gt;Pleasant to use, secure, stable, and performant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please visit ravynos.com for more info: Release Notes | Screenshots | FAQ&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can you help build the dream? See the current projects/needs in CONTRIBUTING.md!&lt;/item&gt;
      &lt;item&gt;Our Discord server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;#ravynOS-general:matrix.org&lt;/code&gt;- join via Element.io&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the top level of the FreeBSD source directory.&lt;/p&gt;
    &lt;p&gt;FreeBSD is an operating system used to power modern servers, desktops, and embedded platforms. A large community has continually developed it for more than thirty years. Its advanced networking, security, and storage features have made FreeBSD the platform of choice for many of the busiest web sites and most pervasive embedded networking and storage devices.&lt;/p&gt;
    &lt;p&gt;For copyright information, please see the file COPYRIGHT in this directory. Additional copyright information also exists for some sources in this tree - please see the specific source directories for more information.&lt;/p&gt;
    &lt;p&gt;The Makefile in this directory supports a number of targets for building components (or all) of the FreeBSD source tree. See build(7), config(8), FreeBSD handbook on building userland, and Handbook for kernels for more information, including setting make(1) variables.&lt;/p&gt;
    &lt;p&gt;For information on the CPU architectures and platforms supported by FreeBSD, see the FreeBSD website's Platforms page.&lt;/p&gt;
    &lt;p&gt;For official FreeBSD bootable images, see the release page.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Directory&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bin&lt;/cell&gt;
        &lt;cell&gt;System/user commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;cddl&lt;/cell&gt;
        &lt;cell&gt;Various commands and libraries under the Common Development and Distribution License.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;contrib&lt;/cell&gt;
        &lt;cell&gt;Packages contributed by 3rd parties.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;crypto&lt;/cell&gt;
        &lt;cell&gt;Cryptography stuff (see crypto/README).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;etc&lt;/cell&gt;
        &lt;cell&gt;Template files for /etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gnu&lt;/cell&gt;
        &lt;cell&gt;Commands and libraries under the GNU General Public License (GPL) or Lesser General Public License (LGPL). Please see gnu/COPYING and gnu/COPYING.LIB for more information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;include&lt;/cell&gt;
        &lt;cell&gt;System include files.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;kerberos5&lt;/cell&gt;
        &lt;cell&gt;Kerberos5 (Heimdal) package.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;lib&lt;/cell&gt;
        &lt;cell&gt;System libraries.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;libexec&lt;/cell&gt;
        &lt;cell&gt;System daemons.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;release&lt;/cell&gt;
        &lt;cell&gt;Release building Makefile &amp;amp; associated tools.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rescue&lt;/cell&gt;
        &lt;cell&gt;Build system for statically linked /rescue utilities.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sbin&lt;/cell&gt;
        &lt;cell&gt;System commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;secure&lt;/cell&gt;
        &lt;cell&gt;Cryptographic libraries and commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;share&lt;/cell&gt;
        &lt;cell&gt;Shared resources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stand&lt;/cell&gt;
        &lt;cell&gt;Boot loader sources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sys&lt;/cell&gt;
        &lt;cell&gt;Kernel sources (see sys/README.md).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;targets&lt;/cell&gt;
        &lt;cell&gt;Support for experimental &lt;code&gt;DIRDEPS_BUILD&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tests&lt;/cell&gt;
        &lt;cell&gt;Regression tests which can be run by Kyua. See tests/README for additional information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tools&lt;/cell&gt;
        &lt;cell&gt;Utilities for regression testing and miscellaneous tasks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;usr.bin&lt;/cell&gt;
        &lt;cell&gt;User commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;usr.sbin&lt;/cell&gt;
        &lt;cell&gt;System administration commands.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For information on synchronizing your source tree with one or more of the FreeBSD Project's development branches, please see FreeBSD Handbook.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ravynsoft/ravynos"/><published>2025-11-20T20:24:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997914</id><title>New Glenn Update</title><updated>2025-11-21T03:54:33.800693+00:00</updated><content/><link href="https://www.blueorigin.com/news/new-glenn-upgraded-engines-subcooled-components-drive-enhanced-performance"/><published>2025-11-20T21:21:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45998047</id><title>GitHut – Programming Languages and GitHub (2014)</title><updated>2025-11-21T03:54:33.629548+00:00</updated><content>&lt;doc fingerprint="cd0138c983fa12d1"&gt;
  &lt;main&gt;
    &lt;p&gt; GitHut is an attempt to visualize and explore the complexity of the universe of programming languages used across the repositories hosted on GitHub.&lt;lb/&gt; Programming languages are not simply the tool developers use to create programs or express algorithms but also instruments to code and decode creativity. By observing the history of languages we can enjoy the quest of human kind for a better way to solve problems, to facilitate collaboration between people and to reuse the effort of others.&lt;lb/&gt; GitHub is the largest code host in the world, with 3.4 million users. It's the place where the open-source development community offers access to most of its projects. By analyzing how languages are used in GitHub it is possible to understand the popularity of programming languages among developers and also to discover the unique characteristics of each language. &lt;/p&gt;
    &lt;p&gt; GitHub provides publicly available API to interact with its huge dataset of events and interaction with the hosted repositories.&lt;lb/&gt; GitHub Archive takes this data a step further by aggregating and storing it for public consumption. GitHub Archive dataset is also available via Google BigQuery. &lt;lb/&gt; The quantitative data used in GitHut is collected from GitHub Archive. The data is updated on a quarterly basis.&lt;lb/&gt; An additional note about the data is about the large amount of records in which the programming language is not specified. This particular characteristic is extremely evident for the Create Events (of repository), therefore it is not possible to visualize the trending language in terms of newly created repositories. For this reason the Activity value (in terms of number of changes pushed) has been considered the best metric for the popularity of programming languages. &lt;lb/&gt; The release year of the programming language is based on the table Timeline of programming languages from Wikipedia. &lt;lb/&gt; For more information on the methodology of the data collection check-out the publicly available GitHub repository of GitHut. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://githut.info/"/><published>2025-11-20T21:33:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45998396</id><title>Virgin and Qantas to ban use of portable power banks after string of fires</title><updated>2025-11-21T03:54:33.470871+00:00</updated><content>&lt;doc fingerprint="77d6ce54e415249c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Virgin and Qantas to ban use of portable power banks after string of fires&lt;/head&gt;
    &lt;head rend="h2"&gt;In short:&lt;/head&gt;
    &lt;p&gt;Virgin Australia passengers will from December 1 have to keep power banks within sight and easily accessible throughout flights.&lt;/p&gt;
    &lt;p&gt;Qantas, QantasLink and Jetstar will introduce similar measures from December 15.&lt;/p&gt;
    &lt;p&gt;It follows several recent international cases where lithium batteries have caught fire.&lt;/p&gt;
    &lt;p&gt;Australian airlines will ban the use of portable power banks from next month following a string of international incidents, including a mid-air fire on a Virgin Australia flight in July.&lt;/p&gt;
    &lt;p&gt;From December 1, Virgin Australia passengers will be required to keep power banks within sight and easily accessible throughout the flight.&lt;/p&gt;
    &lt;p&gt;A power bank is a portable, rechargeable battery pack that stores electrical energy to charge other electronic devices like smartphones, tablets, and laptops on the go.&lt;/p&gt;
    &lt;p&gt;The devices cannot be used or charged on board, and passengers will be limited to two power banks, with larger units over 100 watt-hours requiring airline approval.&lt;/p&gt;
    &lt;p&gt;Qantas, QantasLink and Jetstar will introduce similar measures from December 15.&lt;/p&gt;
    &lt;p&gt;A Qantas spokeswoman confirmed passengers would also be limited to two power banks, each under 160 watt-hours, in cabin baggage.&lt;/p&gt;
    &lt;p&gt;The moves come amid growing concerns about the safety risks posed by lithium battery-powered devices.&lt;/p&gt;
    &lt;p&gt;Virgin Australia's chief operations officer Chris Snook said the changes aligned with international airline safety standards.&lt;/p&gt;
    &lt;p&gt;"Globally, more lithium battery-powered devices are now being carried by travellers, and while these items are generally safe when packed and handled appropriately, this move will minimise any potential risks associated with these devices," Mr Snook said.&lt;/p&gt;
    &lt;p&gt;The airlines said passengers would still be permitted to charge their devices on in-seat charging ports.&lt;/p&gt;
    &lt;p&gt;The Australian Transport Safety Bureau (ATSB) said it would soon release a report into the Virgin flight from Sydney to Hobart, on which a power bank caught fire in an overhead compartment in July.&lt;/p&gt;
    &lt;p&gt;The incident follows several recent international cases, including an Air China flight that made an emergency landing last month in Shanghai after a lithium battery caught fire.&lt;/p&gt;
    &lt;p&gt;An Air Busan plane was also destroyed earlier this year at South Korea's Gimhae Airport after a similar incident involving a power bank.&lt;/p&gt;
    &lt;p&gt;The ATSB said there had been five in-flight fires involving power banks on Australian or Australian-registered aircraft since 2016.&lt;/p&gt;
    &lt;p&gt;Flight Attendants Association of Australia (FAAA) federal secretary Teri O'Toole had been calling for tougher legislation on the use of the devices onboard.&lt;/p&gt;
    &lt;p&gt;"It's important passengers understand these are very dangerous items in an aircraft and to follow the rules airlines put in place. At the end of the day, it's flight attendants who have to fight the fire," Ms O'Toole said.&lt;/p&gt;
    &lt;p&gt;"The fact all the airlines are now aligning their policies is really positive. It means passengers get the same message and the same process regardless of who they fly with, and that consistency helps keep everyone safe."&lt;/p&gt;
    &lt;head rend="h2"&gt;Airlines tighten rules overseas&lt;/head&gt;
    &lt;p&gt;While international airlines, including Emirates, Cathay Pacific and Korean Air, all banned the use of power banks on flights this year, Australian carriers still allowed them, though rules varied.&lt;/p&gt;
    &lt;p&gt;Emirates was the latest airline to ban the use of power banks this month, also due to safety concerns.&lt;/p&gt;
    &lt;p&gt;"There has been a significant growth in customers using power banks in recent years, resulting in an increasing number of lithium battery-related incidents onboard flights across the wider aviation industry," the airline said.&lt;/p&gt;
    &lt;p&gt;An International Air Transport Association (IATA) passenger survey found 44 per cent of passengers travelled with a power bank, 84 per cent of travellers carried a phone and 60 per cent carried a laptop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dodgy power banks spark concern&lt;/head&gt;
    &lt;p&gt;The Australian Competition and Consumer Commission (ACCC) said reported incidents involving lithium batteries jumped 92 per cent between 2020 and 2022.&lt;/p&gt;
    &lt;p&gt;Travellers were now carrying an average of four devices powered by lithium batteries, the regulator noted.&lt;/p&gt;
    &lt;p&gt;Since 2020, the ACCC has issued 17 recalls of power banks, and warned that around 34,000 defective chargers may still be in use.&lt;/p&gt;
    &lt;p&gt;"Some consumers have suffered serious burn injuries, and some have had their property damaged because of power banks overheating and catching fire," ACCC deputy chair Catriona Lowe said.&lt;/p&gt;
    &lt;p&gt;Here is a breakdown of the new rules:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Qantas / Jetstar / QantasLink&lt;/cell&gt;
        &lt;cell role="head"&gt;Virgin Australia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Effective Date&lt;/cell&gt;
        &lt;cell&gt;December 15, 2025&lt;/cell&gt;
        &lt;cell&gt;December 1, 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;On-board Use&lt;/cell&gt;
        &lt;cell&gt;Prohibited&lt;/cell&gt;
        &lt;cell&gt;Prohibited&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Charging on Board&lt;/cell&gt;
        &lt;cell&gt;Prohibited, including in-seat USB/power ports&lt;/cell&gt;
        &lt;cell&gt;Prohibited; passengers may use in-seat power for other devices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Maximum Number of Power Banks&lt;/cell&gt;
        &lt;cell&gt;Two per passenger&lt;/cell&gt;
        &lt;cell&gt;Two per passenger&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Maximum Capacity&lt;/cell&gt;
        &lt;cell&gt;160 Wh per power bank&lt;/cell&gt;
        &lt;cell&gt;Up to 100 Wh unrestricted; 100–160 Wh require airline approval; &amp;gt;160 Wh prohibited&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;Seat pocket, under seat, or nearby overhead locker; smart bag batteries must be removed&lt;/cell&gt;
        &lt;cell&gt;Must be easily accessible (seat pocket, under seat, or on person); not in overhead locker&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Checked Baggage&lt;/cell&gt;
        &lt;cell&gt;Prohibited&lt;/cell&gt;
        &lt;cell&gt;Prohibited&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.abc.net.au/news/2025-11-21/airlines-virgin-australia-qantas-ban-power-banks/106033982"/><published>2025-11-20T21:58:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45998649</id><title>Autocomp: An ADRS Framework for Optimizing Tensor Accelerator Code</title><updated>2025-11-21T03:54:32.915426+00:00</updated><content>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adrs-ucb.notion.site/autocomp"/><published>2025-11-20T22:21:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45999038</id><title>Over-regulation is doubling the cost</title><updated>2025-11-21T03:54:32.577358+00:00</updated><content>&lt;doc fingerprint="698010174ea1b620"&gt;
  &lt;main&gt;
    &lt;p&gt;After building a software company to a multi-billion dollar exit, I made the jump to hardware. Now I’m working on carbon removal + steel at Charm Industrial, and electric long-haul trucking with Revoy. It’s epically fun to be building in the real world, but little did I expect that more than half the cost of building a hardware company would come from regulatory bottlenecks. Despite a huge push for climate fixes and the bipartisan geopolitical desire to bring industry back to the USA, I’ve been shocked to find that the single biggest barrier—by far—is over-regulation from the massive depth of bureaucracy.&lt;/p&gt;
    &lt;p&gt;Hardtech companies of all flavors are being forced to burn through limited capital while they wait for regulatory clarity and/or permits. This creates a constant cycle of cost increases that ultimately flows to consumers, it lowers investment in the US manufacturing and industrial base, it delays innovative new hardware getting into the hands of consumers and businesses, and at the end of the day, it leaves us all worse off, stuck with a quality of life pegged to technology developed decades ago.&lt;/p&gt;
    &lt;p&gt;Regulatory delays and bottlenecks have added millions of pounds of pollutants like PM2.5, NOₓ and CO₂ to our air from the continuation of business as usual, instead of the deployment of clean technologies from my two hardtech efforts alone. While CO₂ is a long-term climate issue, PM2.5 and NOₓ are immediate major drivers of asthma and excess morbidity. Both operations have high bipartisan appeal—and we’ve never been denied a permit—because we’re fundamentally cleaning up things that matter to everyone: dirty air, wildfires, orphaned oil wells. Revoy is also helping deflate the cost of long-haul freight. But none of that has made getting freedom to operate easy. For creative new technologies the default answer is “no” because there isn’t a clear path to permitting at all, and figuring out that path itself takes years — time that startups can’t afford to wait.&lt;/p&gt;
    &lt;p&gt;Regulation obviously has a critical role in protecting people and the environment, but the sheer volume, over-specificity and sometimes ambiguity of those same regulations is now actively working against those goals! We’re unintentionally blocking the very things that would improve our environment. We’ve become a society that blocks all things, and we need to be a society that builds great things every day. The rest of this article gets very specific about the astronomical costs regulations are imposing on us as a society, and the massive positive impact that could be unleashed by cutting back regulation that is working against new, cost-saving, creative technology that could also be making people and the environment healthy again.&lt;/p&gt;
    &lt;p&gt;To make it concrete: both Charm and Revoy are capital-efficient hardtech companies, but Charm will spend low hundreds of millions to get to breakeven, and Revoy will spend tens of millions. In both cases, more than half of the total cost of building each company has gone to counterproductive regulatory burden. I’m hellbent on pushing through these barriers, but the unspoken reality is that our regulatory morass is the deathbed of thousands of hardtech companies that could be drastically improving our lives. We must unleash them.&lt;/p&gt;
    &lt;head rend="h2"&gt;$300M in Societal Cost &amp;amp; $125M in Burden for Charm&lt;/head&gt;
    &lt;p&gt;Charm produces and delivers verified carbon removal to companies like Google, Microsoft and JPMorgan. Charm’s breakthrough was realizing that you could take CO₂ captured in farm &amp;amp; forestry plant residues, convert it into a carbon-rich, BBQ sauce-like liquid (it’s literally the smoke flavor in BBQ sauce), and inject it into old oil wells to permanently remove carbon from the atmosphere. This has all kinds of co-benefits like reducing the massive overburden of wildfire fuels, cleaning up &amp;amp; plugging nasty orphaned oil wells, and improving PM2.5 and NOₓ air quality by avoiding that biomass being burned instead.&lt;/p&gt;
    &lt;p&gt;And yet… there was a hangup: what kind of injection well is this? Should it be permitted as a Class I disposal, Class II oilfield disposal, or Class V experimental? This question on permitting path took four years to answer. Four years to decide which path to use, not even the actual permit! It took this long because regulators are structurally faced with no upside, only downside legal risk in taking a formal position on something new. Even when we’d done an enormous amount of lab and field work with bio-oil to understand its safety and behavior at surface and subsurface conditions. A regulator faces little cost to moving incredibly cautiously, but a major cost if they approve something that triggers activist pushback.&lt;/p&gt;
    &lt;p&gt;In the end, we’re grateful that—eventually—a state regulator took the reins and reviewed, managed, and issued the first-ever Class V bio-oil sequestration permit, through what was still an incredibly complex and detailed 14-month review process.&lt;/p&gt;
    &lt;p&gt;Now imagine that, instead of the 5.5 years from first contact to issued permit, it had only taken the 6 months it actually required to get everyone across the regulatory establishment to agree on a Class V pathway, we would have had 5 additional years operating the well. That’s the equivalent, from our real supply chain, of sinking at least 30,000 tonnes of carbon per year at $600/tonne. Looking only at this one aspect, this delay came with a $90M price tag for Charm. We’ve also spent untold millions on regulatory affairs at all levels of government, not to mention the missed acceleration in sales, and other direct hard costs spent in R&amp;amp;D and processing bio-oil for inefficient and expensive injection into salt caverns instead.&lt;/p&gt;
    &lt;p&gt;But the public health burden created by this regulatory slowness is where it gets really crazy. This one regulatory delay meant we all got subjected to decreased air quality from an additional 30,000 tonnes per year of pile burning. The resulting particulate emissions alone are estimated to have caused a mindblowing $40m/year in healthcare costs. This is $200M in additional healthcare burden over those five years, mostly borne by Medicare and Medicaid. There are additional costs to NOₓ emissions and more that take it to $300M.&lt;/p&gt;
    &lt;p&gt;In total, the total cost to society of this single regulatory delay will be about $400M: $120-150M of unnecessary cost to Charm, and the bulk of it—$300M or so—borne by the public in healthcare costs. I’m not sharing these numbers to complain or make excuses; Charm is still on the path to having a huge impact and we’re among the lucky few that can survive these delays. What pains me most is the 5 years of lost carbon removal and pollutant reduction, and the compounding effect that has on all our health and healthcare costs. Over-regulation is now working against the very things it’s intended to protect.&lt;/p&gt;
    &lt;p&gt;Regulators do their absolute best with the system they have, but the combined effects of: (1) extremely detailed and complex regulation, (2) chaotic budgets and understaffing that disrupt an efficient process, and (3) endless lawsuits against regulators since 1970s-era Naderism have created an atmosphere of fear. If we want to solve the climate crisis, build abundance, lower costs, and generate wealth for all, this has to change. We need to delete and simplify reams of regulations. We need to pay regulators well, and we need to trust our regulators to operate quickly and decisively by putting reasonable limits on endless activist legal challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;&amp;gt;$25M in Unnecessary Burden for Revoy&lt;/head&gt;
    &lt;p&gt;Revoy’s breakthrough was realizing that you could lower long-haul freight costs and electrify long-haul semi trucks by leaving the diesel tractor in place and dropping an electric powertrain onto the back of the semi. Today, we boost semis from 7 mpg to 120 mpg, driving a 94% reduction in fuel consumption. This slashes emissions that negatively impact both air quality and climate.&lt;/p&gt;
    &lt;p&gt;And yet again… a hangup: what exactly is this electric doohickey? Is it a truck? A trailer? Something else? It was clear from the regulations that it was a “converter dolly”. But getting complete alignment on that simple fact across an alphabet soup of government agencies spanning both federal and state—NHTSA, FMCSA, FHWA, state transit authorities, air quality management districts, state DMVs, highway patrols and more—took years.&lt;/p&gt;
    &lt;p&gt;A “powered converter dolly” isn’t even a new thing! Here’s one from the sixties that ran on diesel to help trucks get over mountain passes:&lt;/p&gt;
    &lt;p&gt;There were some bright spots. The Federal Motor Carrier Safety Administration (FMCSA) and the National Highway Transportation Safety Administration (NHTSA) quickly converged on informal definitional clarity, and then eventually a Highway Patrol Captain who was eager to get innovative electric vehicles on the road pushed it through with a state DMV to register the first four Revoys. But bringing along the rest of the agencies, and the rest of the states, was not fast. It delayed deployments, soaked up hundreds of thousands of dollars of legal and lobbyist time (not to mention all the corresponding time on the government side that all of us taxpayers have to bear), and maybe most importantly… even with a formal memo from the Federal DOT, it is still not 100% resolved in some states.&lt;/p&gt;
    &lt;p&gt;As one example, one state agency has asked Revoy to do certified engine testing to prove that the Revoy doesn’t increase emissions of semi trucks. And that Revoy must do this certification across every single truck engine family. It costs $100,000 per certification and there are more than 270 engine families for the 9 engines that our initial partners use. That’s $27,000,000 for this one regulatory item. And keep in mind that this is to certify that a device—whose sole reason for existence is to cut pollution by &amp;gt;90%, and which has demonstrably done so across nearly 100,000 miles of testing and operations—is not increasing the emissions of the truck. It’s a complete waste of money for everyone.&lt;/p&gt;
    &lt;p&gt;And that $27M dollar cost doesn’t include the cost to society. This over-regulation will delay deployment of EV trucks by years, increasing NOₓ and PM 2.5 air pollution exposure for many of society’s least well-off who live near freeways. The delayed deployment will also increase CO₂ emissions that threaten the climate and environment. Revoy’s Founder (Ian Rust) and I actually disagree on what exactly it is about the regulatory environment that needs to change, but we agree it’s completely broken and hurting both people and the planet.&lt;/p&gt;
    &lt;p&gt;In every interaction I have with regulators, I’m reminded that they’re good people doing god’s work operating in a fundamentally broken system. A regulatory system that structurally insists on legalistic, ultra-extreme caution is bound to generate a massive negative return for society.&lt;/p&gt;
    &lt;p&gt;If we had a regulatory system that could move fast to experiment with creative new technologies, we’d live in a world where our environment gets cleaned up faster, where awesome new hardware was constantly improving our lives by making things better and cheaper, and where large-scale hardtech innovation happened here at home in the USA, not in China.&lt;/p&gt;
    &lt;p&gt;As we collectively work to build more manufacturing capacity at home and build the next wave of technologies to power the economy, we need to grapple with the real bottlenecks holding us back. I hope other hardtech founders will publicly share more of their stories as well (the stories I’ve heard in private would shock you). Props to Blake Scholl for doing so.&lt;/p&gt;
    &lt;p&gt;We need a come-to-jesus about regulatory limits, timelines, and scope. Yes, we need basic and strong protections for clear harms, but we need to unleash every hardworking American, not just a few companies with massive funding, to invent and build hardware again. We need to combine many approaches to get there: expedited reviews for new technology, freedom to operate by default, permits by right-not-process, deleting as many regulatory steps as possible, and more. CA YIMBY’s successful push to pass a deluge of housing acceleration laws in the past two years could serve as a model. America building things again is the foundation of a prosperous, powerful, and clean America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rein.pk/over-regulation-is-doubling-the-cost"/><published>2025-11-20T22:58:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45999872</id><title>Why top firms fire good workers</title><updated>2025-11-21T03:54:32.339688+00:00</updated><content>&lt;doc fingerprint="7f2bd59e2b4602e5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Elite firms’ notorious ‘revolving door’ culture isn’t arbitrary but a rational way to signal talent and boost profits, a new study finds.&lt;/head&gt;
    &lt;p&gt;Why do the world’s most prestigious firms—such as McKinsey, Goldman Sachs and other elite consulting giants, investment banks, and law practices—hire the brightest talents, train them intensively, and then, after a few years, send many of them packing? A recent study in the American Economic Review concludes that so-called adverse selection is not a flaw but rather a sign that the system is working precisely as intended.&lt;/p&gt;
    &lt;p&gt;Two financial economists, from the University of Rochester and the University of Wisconsin–Madison respectively, created a model that explains how reputation, information, and retention interact in professions where skill is essential and performance is both visible and attributable to a specific person, particularly in fields such as law, consulting, fund asset management, auditing, and architecture. They argue that much of the professional services world operates through “intermediaries”—firms that both hire employees (also referred to as “agents” or “managers”) and market their expertise to clients—because clients can’t themselves easily judge a worker’s ability from the outset.&lt;/p&gt;
    &lt;p&gt;“Identifying skilled professionals is critical yet presents a major challenge for clients,” the researchers write. “Some of the firm’s employees are high-quality managers,” says coauthor Ron Kaniel, the Jay S. and Jeanne P. Benet Professor of Finance at the University’s Simon Business School, “but the firm is paying them less than their actual quality, because initially the employees don’t have a credible way of convincing the outside world that they are high-quality.”&lt;/p&gt;
    &lt;head rend="h3"&gt;‘Churning’ to boost reputation&lt;/head&gt;
    &lt;p&gt;At the start of an employee’s career, the firm has an advantage, Kaniel and his coauthor Dmitry Orlov contend, because the firm (“the mediator”) can assess an employee’s talent more accurately than outside clients can. During what the authors call “quiet periods,” the firm keeps those who perform adequately and pays them standard wages.&lt;/p&gt;
    &lt;p&gt;Over time, however, an employee’s public performance—measured by successful cases, profitable investments, or well-executed projects—reduces the firm’s informational advantage. As the informational gap shrinks, the firm needs to pay some employees more because clients are now able to observe an employee’s good performance and hence update their beliefs about the employee’s skills.&lt;/p&gt;
    &lt;p&gt;“At some point, the informational advantage becomes fairly small,” says Kaniel, “and the firm says, ‘Well, I will basically start to churn. I will let go of some employees, and by doing that, I can actually extract more from the remaining ones.’”&lt;/p&gt;
    &lt;p&gt;Ironically, to the client these churned—or strategically fired—employees look just as good as the ones whom the firm kept. Churning happens not because these employees have failed but because they may be just somewhat lower-skilled than their peers. Subsequently, churning heightens both the reputation of the firm and of the employees who remain.&lt;/p&gt;
    &lt;head rend="h3"&gt;A paradoxical equilibrium&lt;/head&gt;
    &lt;p&gt;Somewhat counterintuitively, the researchers show that churning can benefit both sides. Workers who stay on with an elite firm accept lower pay in the short run as the tradeoff for building a stronger reputation for themselves. When these workers eventually leave the elite firms, they can command higher fees directly from clients.&lt;/p&gt;
    &lt;p&gt;As a result of the churning, the informational gap between firm and client keeps shrinking because the client catches up to what the firm knows about its workers and which ones it values most. At first glance, the duo argues, the firm’s reduced informational advantage should now cause a further drop in profits. But here comes the strategic twist: The firm starts to underpay those better workers who kept their jobs, akin to making them pay for being “chosen.” Consequently, profits do not decline and may even increase.&lt;/p&gt;
    &lt;p&gt;“Firms now essentially can threaten the remaining employees: ‘Look, I can let you go, and everybody’s going to think that you’re the worst in the pool. If you want me not to let you go, you need to accept below market wages,’” says Kaniel.&lt;/p&gt;
    &lt;p&gt;The result is a paradoxical but stable equilibrium. Workers accept being underpaid temporarily because remaining at a top firm serves as a signal to the market about their elite status. It also helps explain why prestigious employers can attract ambitious newcomers despite grueling hours and relatively modest starting pay.&lt;/p&gt;
    &lt;p&gt;Meanwhile, those who are let go aren’t failures—rather, their exit is part of a system that signals who’s truly top-tier, the researchers argue. In fact, fired workers often find success on their own because potential clients interpret a person’s prior affiliation with a top firm as proof of the worker’s strong ability and qualifications.&lt;/p&gt;
    &lt;p&gt;In short, the “up-or-out” path of professional life may not just be a cultural phenomenon among top professional service firms but also an efficient response to how reputation is maintained and information flows. What looks like a ruthless system of constant turnover, the researchers argue, is in reality a finely tuned mechanism that helps the market discover and reward true talent.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.rochester.edu/newscenter/employee-turnover-why-top-firms-churn-good-workers-681832/"/><published>2025-11-21T00:36:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46000303</id><title>Measuring Latency (2015)</title><updated>2025-11-21T03:54:31.541273+00:00</updated><content>&lt;doc fingerprint="6188c7d7315406b5"&gt;
  &lt;main&gt;&lt;p&gt;Okay, maybe not everything you know about latency is wrong. But now that I have your attention, we can talk about why the tools and methodologies you use to measure and reason about latency are likely horribly flawed. In fact, they’re not just flawed, they’re probably lying to your face.&lt;/p&gt;&lt;p&gt;When I went to Strange Loop in September, I attended a workshop called “Understanding Latency and Application Responsiveness” by Gil Tene. Gil is the CTO of Azul Systems, which is most renowned for its C4 pauseless garbage collector and associated Zing Java runtime. While the workshop was four and a half hours long, Gil also gave a 40-minute talk called “How NOT to Measure Latency” which was basically an abbreviated, less interactive version of the workshop. If you ever get the opportunity to see Gil speak or attend his workshop, I recommend you do. At the very least, do yourself a favor and watch one of his recorded talks or find his slide decks online.&lt;/p&gt;&lt;p&gt;The remainder of this post is primarily a summarization of that talk. You may not get anything out of it that you wouldn’t get out of the talk, but I think it can be helpful to absorb some of these ideas in written form. Plus, for my own benefit, writing about them helps solidify it in my head.&lt;/p&gt;&lt;head rend="h3"&gt;What is Latency?&lt;/head&gt;&lt;p&gt;Latency is defined as the time it took one operation to happen. This means every operation has its own latency—with one million operations there are one million latencies. As a result, latency cannot be measured as work units / time. What we’re interested in is how latency behaves. To do this meaningfully, we must describe the complete distribution of latencies. Latency almost never follows a normal, Gaussian, or Poisson distribution, so looking at averages, medians, and even standard deviations is useless.&lt;/p&gt;&lt;p&gt;Latency tends to be heavily multi-modal, and part of this is attributed to “hiccups” in response time. Hiccups resemble periodic freezes and can be due to any number of reasons—GC pauses, hypervisor pauses, context switches, interrupts, database reindexing, cache buffer flushes to disk, etc. These hiccups never resemble normal distributions and the shift between modes is often rapid and eclectic.&lt;/p&gt;&lt;p&gt;How do we meaningfully describe the distribution of latencies? We have to look at percentiles, but it’s even more nuanced than this. A trap that many people fall into is fixating on “the common case.” The problem with this is that there is a lot more to latency behavior than the common case. Not only that, but the “common” case is likely not as common as you think.&lt;/p&gt;&lt;p&gt;This is partly a tooling problem. Many of the tools we use do not do a good job of capturing and representing this data. For example, the majority of latency graphs produced by Grafana, such as the one below, are basically worthless. We like to look at pretty charts, and by plotting what’s convenient we get a nice colorful graph which is quite readable. Only looking at the 95th percentile is what you do when you want to hide all the bad stuff. As Gil describes, it’s a “marketing system.” Whether it’s the CTO, potential customers, or engineers—someone’s getting duped. Furthermore, averaging percentiles is mathematically absurd. To conserve space, we often keep the summaries and throw away the data, but the “average of the 95th percentile” is a meaningless statement. You cannot average percentiles, yet note the labels in most of your Grafana charts. Unfortunately, it only gets worse from here.&lt;/p&gt;&lt;p&gt;Gil says, “The number one indicator you should never get rid of is the maximum value. That is not noise, that is the signal. The rest of it is noise.” To this point, someone in the workshop naturally responded with “But what if the max is just something like a VM restarting? That doesn’t describe the behavior of the system. It’s just an unfortunate, unlikely occurrence.” By ignoring the maximum, you’re effectively saying “this doesn’t happen.” If you can identify the cause as noise, you’re okay, but if you’re not capturing that data, you have no idea of what’s actually happening.&lt;/p&gt;&lt;head rend="h3"&gt;How Many Nines?&lt;/head&gt;&lt;p&gt;But how many “nines” do I really need to look at? The 99th percentile, by definition, is the latency below which 99% of the observations may be found. Is the 99th percentile rare? If we have a single search engine node, a single key-value store node, a single database node, or a single CDN node, what is the chance we actually hit the 99th percentile?&lt;/p&gt;&lt;p&gt;Gil describes some real-world data he collected which shows how many of the web pages we go to actually experience the 99th percentile, displayed in table below. The second column counts the number of HTTP requests generated by a single access of the web page. The third column shows the likelihood of one access experiencing the 99th percentile. With the exception of google.com, every page has a probability of 50% or higher of seeing the 99th percentile.&lt;/p&gt;&lt;p&gt;The point Gil makes is that the 99th percentile is what most of your web pages will see. It’s not “rare.”&lt;/p&gt;&lt;p&gt;What metric is more representative of user experience? We know it’s not the average or the median. 95th percentile? 99.9th percentile? Gil walks through a simple, hypothetical example: a typical user session involves five page loads, averaging 40 resources per page. How many users will not experience something worse than the 95th percentile? 0.003%. By looking at the 95th percentile, you’re looking at a number which is relevant to 0.003% of your users. This means 99.997% of your users are going to see worse than this number, so why are you even looking at it?&lt;/p&gt;&lt;p&gt;On the flip side, 18% of your users are going to experience a response time worse than the 99.9th percentile, meaning 82% of users will experience the 99.9th percentile or better. Going further, more than 95% of users will experience the 99.97th percentile and more than 99% of users will experience the 99.995th percentile.&lt;/p&gt;&lt;p&gt;The median is the number that 99.9999999999% of response times will be worse than. This is why median latency is irrelevant. People often describe “typical” response time using a median, but the median just describes what everything will be worse than. It’s also the most commonly used metric.&lt;/p&gt;&lt;p&gt;If it’s so critical that we look at a lot of nines (and it is), why do most monitoring systems stop at the 95th or 99th percentile? The answer is simply because “it’s hard!” The data collected by most monitoring systems is usually summarized in small, five or ten second windows. This, combined with the fact that we can’t average percentiles or derive five nines from a bunch of small samples of percentiles means there’s no way to know what the 99.999th percentile for the minute or hour was. We end up throwing away a lot of good data and losing fidelity.&lt;/p&gt;&lt;head rend="h3"&gt;A Coordinated Conspiracy&lt;/head&gt;&lt;p&gt;Benchmarking is hard. Almost all latency benchmarks are broken because almost all benchmarking tools are broken. The number one cause of problems in benchmarks is something called “coordinated omission,” which Gil refers to as “a conspiracy we’re all a part of” because it’s everywhere. Almost all load generators have this problem.&lt;/p&gt;&lt;p&gt;We can look at a common load-testing example to see how this problem manifests. With this type of test, a client generally issues requests at a certain rate, measures the response time for each request, and puts them in buckets from which we can study percentiles later.&lt;/p&gt;&lt;p&gt;The problem is what if the thing being measured took longer than the time it would have taken before sending the next thing? What if you’re sending something every second, but this particular thing took 1.5 seconds? You wait before you send the next one, but by doing this, you avoided measuring something when the system was problematic. You’ve coordinated with it by backing off and not measuring when things were bad. To remain accurate, this method of measuring only works if all responses fit within an expected interval.&lt;/p&gt;&lt;p&gt;Coordinated omission also occurs in monitoring code. The way we typically measure something is by recording the time before, running the thing, then recording the time after and looking at the delta. We put the deltas in stats buckets and calculate percentiles from that. The code below is taken from a Cassandra benchmark.&lt;/p&gt;&lt;p&gt;However, if the system experiences one of the “hiccups” described earlier, you will only have one bad operation and 10,000 other operations waiting in line. When those 10,000 other things go through, they will look really good when in reality the experience was really bad. Long operations only get measured once, and delays outside the timing window don’t get measured at all.&lt;/p&gt;&lt;p&gt;In both of these examples, we’re omitting data that looks bad on a very selective basis, but just how much of an impact can this have on benchmark results? It turns out the impact is huge.&lt;/p&gt;&lt;p&gt;Imagine a “perfect” system which processes 100 requests/second at exactly 1 ms per request. Now consider what happens when we freeze the system (for example, using CTRL+Z) after 100 seconds of perfect operation for 100 seconds and repeat. We can intuitively characterize this system:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The average over the first 100 seconds is 1 ms.&lt;/item&gt;&lt;item&gt;The average over the next 100 seconds is 50 seconds.&lt;/item&gt;&lt;item&gt;The average over the 200 seconds is 25 seconds.&lt;/item&gt;&lt;item&gt;The 50th percentile is 1 ms.&lt;/item&gt;&lt;item&gt;The 75th percentile is 50 seconds.&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 100 seconds.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Now we try measuring the system using a load generator. Before freezing, we run 100 seconds at 100 requests/second for a total of 10,000 requests at 1 ms each. After the stall, we get one result of 100 seconds. This is the entirety of our data, and when we do the math, we get these results:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The average over the 200 seconds is 10.9 ms (should be 25 seconds).&lt;/item&gt;&lt;item&gt;The 50th percentile is 1 ms.&lt;/item&gt;&lt;item&gt;The 75th percentile is 1 ms (should be 50 seconds).&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 1 ms (should be 100 seconds).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Basically, your load generator and monitoring code tell you the system is ready for production, when in fact it’s lying to you! A simple “CTRL+Z” test can catch coordinated omission, but people rarely do it. It’s critical to calibrate your system this way. If you find it giving you these kind of results, throw away all the numbers—they’re worthless.&lt;/p&gt;&lt;p&gt;You have to measure at random or “fair” rates. If you measure 10,000 things in the first 100 seconds, you have to measure 10,000 things in the second 100 seconds during the stall. If you do this, you’ll get the correct numbers, but they won’t be as pretty. Coordinated omission is the simple act of erasing, ignoring, or missing all the “bad” stuff, but the data is good.&lt;/p&gt;&lt;p&gt;Surely this data can still be useful though, even if it doesn’t accurately represent the system? For example, we can still use it to identify performance regressions or validate improvements, right? Sadly, this couldn’t be further from the truth. To see why, imagine we improve our system. Instead of pausing for 100 seconds after 100 seconds of perfect operation, it handles all requests at 5 ms each after 100 seconds. Doing the math, we get the following:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The 50th percentile is 1 ms&lt;/item&gt;&lt;item&gt;The 75th percentile is 2.5 ms (stall showed 1 ms)&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 5 ms (stall showed 1 ms)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This data tells us we hurt the four nines and made the system 5x worse! This would tell us to revert the change and go back to the way it was before, which is clearly the wrong decision. With bad data, better can look worse. This shows that you cannot have any intuition based on any of these numbers. The data is garbage.&lt;/p&gt;&lt;p&gt;With many load generators, the situation is actually much worse than this. These systems work by generating a constant load. If our test is generating 100 requests/second, we run 10,000 requests in the first 100 seconds. When we stall, we process just one request. After the stall, the load generator sees that it’s 9,999 requests behind and issues those requests to catch back up. Not only did it get rid of the bad requests, it replaced them with good requests. Now the data is twice as wrong as just dropping the bad requests.&lt;/p&gt;&lt;p&gt;What coordinated omission is really showing you is service time, not response time. If we imagine a cashier ringing up customers, the service time is the time it takes the cashier to do the work. The response time is the time a customer waits before they reach the register. If the rate of arrival is higher than the service rate, the response time will continue to grow. Because hiccups and other phenomena happen, response times often bounce around. However, coordinated omission lies to you about response time by actually telling you the service time and hiding the fact that things stalled or waited in line.&lt;/p&gt;&lt;head rend="h3"&gt;Measuring Latency&lt;/head&gt;&lt;p&gt;Latency doesn’t live in a vacuum. Measuring response time is important, but you need to look at it in the context of load. But how do we properly measure this? When you’re nearly idle, things are nearly perfect, so obviously that’s not very useful. When you’re pedal to the metal, things fall apart. This is somewhat useful because it tells us how “fast” we can go before we start getting angry phone calls.&lt;/p&gt;&lt;p&gt;However, studying the behavior of latency at saturation is like looking at the shape of your car’s bumper after wrapping it around a pole. The only thing that matters when you hit the pole is that you hit the pole. There’s no point in trying to engineer a better bumper, but we can engineer for the speed at which we lose control. Everything is going to suck at saturation, so it’s not super useful to look at beyond determining your operating range.&lt;/p&gt;&lt;p&gt;What’s more important is testing the speeds in between idle and hitting the pole. Define your SLAs and plot those requirements, then run different scenarios using different loads and different configurations. This tells us if we’re meeting our SLAs but also how many machines we need to provision to do so. If you don’t do this, you don’t know how many machines you need.&lt;/p&gt;&lt;p&gt;How do we capture this data? In an ideal world, we could store information for every request, but this usually isn’t practical. HdrHistogram is a tool which allows you to capture latency and retain high resolution. It also includes facilities for correcting coordinated omission and plotting latency distributions. The original version of HdrHistogram was written in Java, but there are versions for many other languages.&lt;/p&gt;&lt;head rend="h3"&gt;To Summarize&lt;/head&gt;&lt;p&gt;To understand latency, you have to consider the entire distribution. Do this by plotting the latency distribution curve. Simply looking at the 95th or even 99th percentile is not sufficient. Tail latency matters. Worse yet, the median is not representative of the “common” case, the average even less so. There is no single metric which defines the behavior of latency. Be conscious of your monitoring and benchmarking tools and the data they report. You can’t average percentiles.&lt;/p&gt;&lt;p&gt;Remember that latency is not service time. If you plot your data with coordinated omission, there’s often a quick, high rise in the curve. Run a “CTRL+Z” test to see if you have this problem. A non-omitted test has a much smoother curve. Very few tools actually correct for coordinated omission.&lt;/p&gt;&lt;p&gt;Latency needs to be measured in the context of load, but constantly running your car into a pole in every test is not useful. This isn’t how you’re running in production, and if it is, you probably need to provision more machines. Use it to establish your limits and test the sustainable throughputs in between to determine if you’re meeting your SLAs. There are a lot of flawed tools out there, but HdrHistogram is one of the few that isn’t. It’s useful for benchmarking and, since histograms are additive and HdrHistogram uses log buckets, it can also be useful for capturing high-volume data in production.&lt;/p&gt;Follow @tyler_treat&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bravenewgeek.com/everything-you-know-about-latency-is-wrong/"/><published>2025-11-21T01:50:24+00:00</published></entry></feed>