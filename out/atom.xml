<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-09T13:03:11.010820+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45489625</id><title>The Debugging Book</title><updated>2025-10-09T13:03:18.562581+00:00</updated><content>&lt;doc fingerprint="af5b11584ce5419c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Debugging Book&lt;/head&gt;&lt;head rend="h3"&gt;Tools and Techniques for Automated Software Debugging&lt;/head&gt;by Andreas Zeller&lt;head rend="h1"&gt;About this Book¶&lt;/head&gt;&lt;p&gt;Welcome to "The Debugging Book"! Software has bugs, and finding bugs can involve lots of effort. This book addresses this problem by automating software debugging, specifically by locating errors and their causes automatically. Recent years have seen the development of novel techniques that lead to dramatic improvements in automated software debugging. They now are mature enough to be assembled in a book – even with executable code.&lt;/p&gt;&lt;code&gt;from bookutils import YouTubeVideo
YouTubeVideo("-nOxI6Ev_I4")
&lt;/code&gt;&lt;head rend="h2"&gt;A Textbook for Paper, Screen, and Keyboard¶&lt;/head&gt;&lt;p&gt;You can use this book in four ways:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;You can read chapters in your browser. Check out the list of chapters in the menu above, or start right away with the introduction to debugging or how debuggers work. All code is available for download.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;You can interact with chapters as Jupyter Notebooks (beta). This allows you to edit and extend the code, experimenting live in your browser. Simply select "Resources → Edit as Notebook" at the top of each chapter. Try interacting with the introduction to interactive debuggers.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;You can use the code in your own projects. You can download the code as Python programs; simply select "Resources → Download Code" for one chapter or "Resources → All Code" for all chapters. These code files can be executed, yielding (hopefully) the same results as the notebooks. Once the book is out of beta, you can also install the Python package.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;You can present chapters as slides. This allows for presenting the material in lectures. Just select "Resources → View slides" at the top of each chapter. Try viewing the slides for how debuggers work.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Who this Book is for¶&lt;/head&gt;&lt;p&gt;This work is designed as a textbook for a course in software debugging; as supplementary material in a software testing or software engineering course; and as a resource for software developers. We cover fault localization, program slicing, input reduction, automated repair, and much more, illustrating all techniques with code examples that you can try out yourself.&lt;/p&gt;&lt;head rend="h2"&gt;News¶&lt;/head&gt;&lt;p&gt;This book is work in progress. All chapters planned are out now, but we keep on refining text and code with minor and major releases. To get notified on updates, follow us on Mastodon.&lt;/p&gt;&lt;head rend="h2"&gt;About the Author¶&lt;/head&gt;&lt;p&gt;This book is written by Andreas Zeller, a long-standing expert in automated debugging, software analysis and software testing. Andreas is happy to share his expertise and making it accessible to the public.&lt;/p&gt;&lt;head rend="h2"&gt;Frequently Asked Questions¶&lt;/head&gt;&lt;head rend="h3"&gt;Troubleshooting¶&lt;/head&gt;&lt;head rend="h4"&gt;Why does it take so long to start an interactive notebook?¶&lt;/head&gt;&lt;p&gt;The interactive notebook uses the mybinder.org service, which runs notebooks on their own servers. Starting Jupyter through mybinder.org normally takes about 30 seconds, depending on your Internet connection. If, however, you are the first to invoke binder after a book update, binder recreates its environment, which will take a few minutes. Reload the page occasionally.&lt;/p&gt;&lt;head rend="h4"&gt;The interactive notebook does not work!¶&lt;/head&gt;&lt;p&gt;mybinder.org imposes a limit of 100 concurrent users for a repository. Also, as listed on the mybinder.org status and reliability page,&lt;/p&gt;&lt;quote&gt;&lt;p&gt;As mybinder.org is a research pilot project, the main goal for the project is to understand usage patterns and workloads for future project evolution. While we strive for site reliability and availability, we want our users to understand the intent of this service is research and we offer no guarantees of its performance in mission critical uses.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;There are alternatives to mybinder.org; see below.&lt;/p&gt;&lt;head rend="h4"&gt;Do I have alternatives to the interactive notebook?¶&lt;/head&gt;&lt;p&gt;If mybinder.org does not work or match your needs, you have a number of alternatives:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Download the Python code (using the menu at the top) and edit and run it in your favorite environment. This is easy to do and does not require lots of resources.&lt;/item&gt;&lt;item&gt;Download the Jupyter Notebooks (using the menu at the top) and open them in Jupyter. Here's how to install jupyter notebook on your machine.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;For details, see our article on Using Debuggingbook Code in your own Programs.&lt;/p&gt;&lt;p&gt;As another alternative, you can also use our Docker images (experimental). Install Docker and then run&lt;/p&gt;&lt;code&gt;    $ docker pull zeller24/debuggingbook
    $ docker run -it --rm -p 8888:8888 zeller24/debuggingbook&lt;/code&gt;&lt;p&gt;and then in your Web browser, open the URL (&lt;code&gt;http://127.0.0.1/...&lt;/code&gt; or &lt;code&gt;http://localhost/...&lt;/code&gt;) given in the console output.
This should give you the same environment as in mybinder.org.&lt;/p&gt;&lt;p&gt;If you want to create your own Docker images, use our Dockerfile as a starting point.&lt;/p&gt;&lt;head rend="h4"&gt;Can I run the code on my Windows machine?¶&lt;/head&gt;&lt;p&gt;We try to keep the code as general as possible, but occasionally, when we interact with the operating system, we assume a Unix-like environment (because that is what Binder provides). To run these examples on your own Windows machine, you can install a Linux VM or a Docker environment.&lt;/p&gt;&lt;head rend="h4"&gt;Can't you run your own dedicated cloud service?¶&lt;/head&gt;&lt;p&gt;Technically, yes; but this would cost money and effort, which we'd rather spend on the book at this point. If you'd like to host a JupyterHub or BinderHub instance for the public, please do so and let us know.&lt;/p&gt;&lt;head rend="h3"&gt;Content¶&lt;/head&gt;&lt;head rend="h4"&gt;Can I use your code in my own programs?¶&lt;/head&gt;&lt;p&gt;Yes! See the installation instructions for details.&lt;/p&gt;&lt;head rend="h4"&gt;Do your techniques apply to Python programs only? How about C code?¶&lt;/head&gt;&lt;p&gt;We use Python to implement our tools and techniques because we can get things done quickly. Building an interactive debugger in Python is less than 100 lines of code and took us 2-3 days; doing the same for C is tens of thousands of lines and a year-long project. Instrumenting code, say for dynamic slicing, gets us savings of similar magnitude. Also, Python code allows us (and you) to focus on the main concepts, rather than implementation details that are out of place in a textbook.&lt;/p&gt;&lt;p&gt;Having said this, many of the techniques in this book can also be applied to C and other code. This is notably true for black-box techniques such as reducing inputs or changes or generalizers; these are all language-agnostic. Tools related to the debugging process such as bug tracking or mining repositories are language-agnostic as well. Finally, in all chapters, we provide pointers to implementations in and for other languages, for instance for assertions or program repair.&lt;/p&gt;&lt;head rend="h4"&gt;What are the latest changes?¶&lt;/head&gt;&lt;p&gt;For changes to individual chapters, see the "Last change" link at the end of a chapter. For the &lt;code&gt;debuggingbook&lt;/code&gt; Python package, see the release notes for details.&lt;/p&gt;&lt;head rend="h4"&gt;How do I cite your work?¶&lt;/head&gt;&lt;p&gt;Thanks for referring to our work! Just click on the "cite" button at the bottom of the Web page for each chapter to get a citation entry.&lt;/p&gt;&lt;head rend="h4"&gt;Can you cite my paper? And possibly write a chapter about it?¶&lt;/head&gt;&lt;p&gt;We're always happy to get suggestions! If we missed an important reference, we will of course add it. If you'd like specific material to be covered, the best way is to write a notebook yourself; see our Guide for Authors for instructions on coding and writing. We can then refer to it or even host it.&lt;/p&gt;&lt;head rend="h3"&gt;Teaching and Coursework¶&lt;/head&gt;&lt;head rend="h4"&gt;How can I run a course based on the book?¶&lt;/head&gt;&lt;p&gt;We have successfully used the material in various courses.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Initially, we used the slides and code and did live coding in lectures to illustrate how a technique works.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Now, the goal of the book is to be completely self-contained; that is, it should work without additional support. Hence, we now give out completed chapters to students in a flipped classroom setting, with the students working on the notebooks at their leisure. We would meet in the classroom (or in Zoom) to discuss experiences with past notebooks and discuss future notebooks.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;We have the students work on exercises from the book or work on larger (automated debugging) projects. We also have students who use the book as a base for their research; indeed, it is very easy to prototype in Python for Python.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;When running a course, do not rely on mybinder.org – it will not provide sufficient resources for a larger group of students. Instead, install and run your own hub.&lt;/p&gt;&lt;head rend="h4"&gt;Are there specific subsets I can focus on?¶&lt;/head&gt;&lt;p&gt;We will compile a number of tours through the book for various audiences. Our Sitemap lists the dependencies between the individual chapters.&lt;/p&gt;&lt;head rend="h4"&gt;How can I extend or adapt your slides?¶&lt;/head&gt;&lt;p&gt;Download the Jupyter Notebooks (using the menu at the top) and adapt the notebooks at your leisure (see above), including "Slide Type" settings. Then,&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Download slides from Jupyter Notebook; or&lt;/item&gt;&lt;item&gt;Use the RISE extension (instructions) to present your slides right out of Jupyter notebook.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Do you provide PDFs of your material?¶&lt;/head&gt;&lt;p&gt;Technically, we can produce PDF and print versions from notebooks, but it is low on our priority list as we find the interactive formats to be so much superior. Let us know if you'd like PDF versions.&lt;/p&gt;&lt;head rend="h3"&gt;Other Issues¶&lt;/head&gt;&lt;head rend="h4"&gt;I have a question, comment, or a suggestion. What do I do?¶&lt;/head&gt;&lt;p&gt;You can post to @TheDebuggingBook@mastodon.social on Mastodon, allowing the community of readers to chime in. For bugs you'd like to get fixed, report an issue on the development page.&lt;/p&gt;&lt;head rend="h4"&gt;I have reported an issue two weeks ago. When will it be addressed?¶&lt;/head&gt;&lt;p&gt;We prioritize issues as follows:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Bugs in code published on debuggingbook.org&lt;/item&gt;&lt;item&gt;Bugs in text published on debuggingbook.org&lt;/item&gt;&lt;item&gt;Writing missing chapters&lt;/item&gt;&lt;item&gt;Issues in yet unpublished code or text&lt;/item&gt;&lt;item&gt;Issues related to development or construction&lt;/item&gt;&lt;item&gt;Things marked as "beta"&lt;/item&gt;&lt;item&gt;Everything else&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;How can I solve problems myself?¶&lt;/head&gt;&lt;p&gt;We're glad you ask that. The development page has all sources and some supplementary material. Pull requests that fix issues are very welcome.&lt;/p&gt;&lt;head rend="h4"&gt;How can I contribute?¶&lt;/head&gt;&lt;p&gt;Again, we're glad you're here! We are happy to accept&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Code fixes and improvements. Please place any code under the MIT license such that we can easily include it.&lt;/item&gt;&lt;item&gt;Additional text, chapters, and notebooks on specialized topics. We plan to set up a special folder for third-party contributions.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;See our Guide for Authors for instructions on coding and writing.&lt;/p&gt;&lt;p&gt;The content of this project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The source code that is part of the content, as well as the source code used to format and display that content is licensed under the MIT License. Last change: 2024-07-01 16:49:37+02:00 • Cite • Imprint&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.debuggingbook.org/"/><published>2025-10-06T09:56:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45490627</id><title>Thoughts on the Word Spec in Rust</title><updated>2025-10-09T13:03:18.129959+00:00</updated><content>&lt;doc fingerprint="63fcd438cfacc67d"&gt;
  &lt;main&gt;
    &lt;p&gt;Tritium lives in the Word spec because to deliver great legal tech, we think we need to own the word processor.&lt;/p&gt;
    &lt;p&gt;The Word spec is giant.&lt;/p&gt;
    &lt;p&gt;It provides that a valid docx file may contain something like the below XML:&lt;/p&gt;
    &lt;code&gt;&amp;lt;body&amp;gt;
	&amp;lt;tbl&amp;gt;
     	...
	      	&amp;lt;p&amp;gt;
      		...
 	       		&amp;lt;tbl&amp;gt;...&amp;lt;/tbl&amp;gt;
		&amp;lt;/p&amp;gt;
	&amp;lt;/tbl&amp;gt;
&amp;lt;/body&amp;gt;&lt;/code&gt;
    &lt;p&gt;It thus supports essentially infinite nesting of paragraphs and tables in other words.&lt;/p&gt;
    &lt;p&gt;And since Word was written in C/C++ and happy to work with multiple mutable ownership, it's no problem to have these deeply nested structures.&lt;/p&gt;
    &lt;p&gt;But they're hard to do right in Rust.&lt;/p&gt;
    &lt;p&gt;So, where to start?&lt;/p&gt;
    &lt;p&gt;An excellent first place was the &lt;code&gt;docx_rs&lt;/code&gt; crate maintained by bokuweb.&lt;/p&gt;
    &lt;p&gt;bokoweb's work seems to follow along the lines of &lt;code&gt;python-docx&lt;/code&gt; in creating an excellent API for
        generating
        Word documents.&lt;/p&gt;
    &lt;p&gt;From the repo:&lt;/p&gt;
    &lt;code&gt;use docx_rs::*;

pub fn hello() -&amp;gt; Result&amp;lt;(), DocxError&amp;gt; {
    let path = std::path::Path::new("./hello.docx");
    let file = std::fs::File::create(path).unwrap();
    Docx::new()
        .add_paragraph(Paragraph::new().add_run(Run::new().add_text("Hello")))
        .build()
        .pack(file)?;
    Ok(())
}&lt;/code&gt;
    &lt;p&gt;It also supports reading. To ingest a Word file with &lt;code&gt;libtritium&lt;/code&gt; would look something like the below.
    &lt;/p&gt;
    &lt;code&gt;pub fn main() {
    let bytes = libtritium::fs::slurp_path("./hello_world.docx").unwrap();
    let docx = docx_rs::read_docx(&amp;amp;bytes).unwrap();
    let Some(docx_rs::documents::DocumentChild(paragraph)) = 
docx.children.first() else {
         panic!("Expected a paragraph.");
    };
    println!("{}", paragraph.raw_text());
}

// Hello, World!&lt;/code&gt;
    &lt;p&gt;As a great Rust crate, it compiles to WASM and can be run on Web front ends. Amazing.&lt;/p&gt;
    &lt;p&gt;It was instrumental in getting Tritium's first alpha versions of the ground.&lt;/p&gt;
    &lt;p&gt;But today, Tritium runs a custom &lt;code&gt;docx&lt;/code&gt; module, written from scratch.&lt;/p&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;As with many other endeavours, if it's your core product, you need to own the stack or at least have control over its destiny.&lt;/p&gt;
    &lt;p&gt;Tritium's core offering is making surgical edits to legacy legal documents.&lt;/p&gt;
    &lt;p&gt;While it doesn't have to implement the entire Word spec to be useful, Tritium needs to survive the below round-trip test in all cases to even be useable.&lt;/p&gt;
    &lt;code&gt;#[test]
fn deserialize_serialize_round_trip() {
    let src = libtritium::fs::slurp_path("/src.docx").unwrap();
    let docx = libtritium::docx::Docx::from_bytes(&amp;amp;src).unwrap();
    docx.save_as("/dst.docx").unwrap();
    let dst = libtritium::fs::slurp_path("/dst.docx").unwrap();
    assert_eq!(*src, *dst);
}&lt;/code&gt;
    &lt;p&gt;Surprising the user by dropping data on save would be fatal.&lt;/p&gt;
    &lt;p&gt;Tritium outgrew &lt;code&gt;docx_rs&lt;/code&gt; because it's designed for construction, not consumption, of
        Word files.&lt;/p&gt;
    &lt;p&gt;Let's go back to our pseudo-docx block.&lt;/p&gt;
    &lt;code&gt;&amp;lt;body&amp;gt;
	&amp;lt;tbl&amp;gt;
     	...
	      	&amp;lt;p&amp;gt;
      		...
 	       		&amp;lt;tbl&amp;gt;...&amp;lt;/tbl&amp;gt;
		&amp;lt;/p&amp;gt;
	&amp;lt;/tbl&amp;gt;
&amp;lt;/body&amp;gt;&lt;/code&gt;
    &lt;p&gt;One smart thing that &lt;code&gt;docx_rs&lt;/code&gt; does is represent this structure as a nested AST of &lt;code&gt;enum&lt;/code&gt;
        variants.&lt;/p&gt;
    &lt;p&gt;At the time of writing, for example, here's the &lt;code&gt;ParagraphChild&lt;/code&gt; definition.&lt;/p&gt;
    &lt;p&gt;This describes the types of children that a &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; element can have.&lt;/p&gt;
    &lt;code&gt;#[derive(Debug, Clone, PartialEq)]
pub enum ParagraphChild {
    Run(Box&amp;lt;Run&amp;gt;),
    Insert(Insert),
    Delete(Delete),
    BookmarkStart(BookmarkStart),
    Hyperlink(Hyperlink),
    BookmarkEnd(BookmarkEnd),
    CommentStart(Box&amp;lt;CommentRangeStart&amp;gt;),
    CommentEnd(CommentRangeEnd),
    StructuredDataTag(Box&amp;lt;StructuredDataTag&amp;gt;),
    PageNum(Box&amp;lt;PageNum&amp;gt;),
    NumPages(Box&amp;lt;NumPages&amp;gt;),
}&lt;/code&gt;
    &lt;p&gt;Why an &lt;code&gt;enum&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Well, for starters, a single Word document may have many thousands, tens of thousands or even hundreds of thousands of these XML entities.&lt;/p&gt;
    &lt;p&gt;Retaining each XML tag as a &lt;code&gt;String&lt;/code&gt; or raw bytes would not only hog a tremendous amount of memory
        but, on certain platforms, result in memory
            fragmentation in a manner that would be indistinguishable from a giant memory leak.&lt;/p&gt;
    &lt;p&gt;So &lt;code&gt;docx_rs&lt;/code&gt; uses &lt;code&gt;quick_xml&lt;/code&gt; to read each XML entity into an &lt;code&gt;enum&lt;/code&gt; using an
        evented parser.&lt;/p&gt;
    &lt;p&gt;That process converts the already-allocated set of XML tag bytes into a tiny discriminent value of the &lt;code&gt;enum&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;But what if that's not all of the possible children of the &lt;code&gt;p&lt;/code&gt; tag?&lt;/p&gt;
    &lt;p&gt;And what if all of the possible children of the &lt;code&gt;body&lt;/code&gt; tag aren't supported for reading yet
        either?&lt;/p&gt;
    &lt;code&gt;match e {
    XMLElement::Paragraph =&amp;gt; {
        let p = Paragraph::read(&amp;amp;mut parser, &amp;amp;attributes)?;
        doc = doc.add_paragraph(p);
        continue;
    }
    ..
    _ =&amp;gt; {} // woops
}&lt;/code&gt;
    &lt;p&gt;Such a novel tag is just ignored, and we fail our round-trip test.&lt;/p&gt;
    &lt;p&gt;So while most open source implementations of the Word spec are incomplete, and &lt;code&gt;docx_rs&lt;/code&gt; is
        a great library for building Word files in Rust native or in the browser, it's directionally
        incorrect for Tritium.&lt;/p&gt;
    &lt;quote&gt;If it’s a core business function — do it yourself, no matter what.- Joel Spolsky, In Defense of Not Invented Here Syndrome&lt;/quote&gt;
    &lt;p&gt;So we build.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tritium.legal/blog/word"/><published>2025-10-06T12:28:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45495607</id><title>Talk Python in Production</title><updated>2025-10-09T13:03:17.787133+00:00</updated><content>&lt;doc fingerprint="b1d9227b65a47092"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to Talk Python in Production, a hands-on guide for Python developers determined to master real-world deployment and infrastructure management. Have you ever felt locked into pricey cloud services or struggled with overly complex DevOps configurations? This book's stack-native approach offers a refreshing alternative.&lt;/p&gt;
    &lt;p&gt;You'll learn to containerize Python apps, secure them with NGINX, tap into CDNs for global performance, and manage everything on a single, powerful server, without sacrificing reliability. Along the way, you'll see exactly how we evolved Talk Python's own web infrastructure over many years of practical deployment experience. This isn't just theory: discover cost-saving tips, real-world examples, and step-by-step tutorials you can put to use right away. By the end, you'll be confident in running your Python applications at scale, with minimal cloud lock-in and maximum control over your technology stack.&lt;/p&gt;
    &lt;p&gt;"Stack-native" is an alternative deployment philosophy to "cloud-native." Instead of outsourcing dozens of microservices to a hyperscale cloud, you run much of your app on a single, powerful server (e.g., via Docker Compose). This keeps the complexity and cost far lower, avoids lock-in, and is typically manageable by a small team -- or even a single developer.&lt;/p&gt;
    &lt;p&gt;Get the clearest figures, copy-ready code, and every reference in one place.&lt;/p&gt;
    &lt;p&gt; The book includes a total of 1 hour and 20 minutes of short 2 to 4 minute conversations that bookend each chapter. Use them to prime your focus before reading or to broaden your takeaways after. They complement the code-heavy text without trying to be a word-for-word audiobook; expect the occasional quirky acronym. Preview a single chapter's brief here:&lt;lb/&gt; Readers' Brief: Ch 2: What we've tried&lt;/p&gt;
    &lt;p&gt;The book doesn't assume deep DevOps knowledge. Readers with moderate Python experience, plus some comfort using Linux terminals and basic web deployment concepts, should be able to follow along. It progressively introduces Docker, NGINX, and other tools with concrete, real-world examples. However, it's not a basic "Hello World" tutorial -- it deals with production-scale considerations like security patches, automated deployments, container orchestration, and cost optimization.&lt;/p&gt;
    &lt;p&gt;No. While having some exposure to Docker or Linux server administration helps, the text walks you through setting up and managing a modern Docker-based deployment from scratch. Think of it as a guided tour of these technologies, with best practices and pitfalls explained along the way.&lt;/p&gt;
    &lt;p&gt;Yes, but in a real-world, practical context:&lt;/p&gt;
    &lt;p&gt;Not entirely. The philosophy is more about reducing lock-in rather than rejecting all cloud hosting:&lt;/p&gt;
    &lt;p&gt;Yes. The author's real-world infrastructure serves millions of requests per month on a single well-configured server. Chapters detail:&lt;/p&gt;
    &lt;p&gt;Security best practices appear throughout:&lt;/p&gt;
    &lt;p&gt;Very likely. By avoiding dozens of microservices and serverless functions, you won't be blindsided by unpredictable bills. The book showcases real monthly costs (often under $100) for heavy-traffic sites. It also explores how scaling up typically remains predictable and affordable.&lt;/p&gt;
    &lt;p&gt;Yes, there are many code snippets, Dockerfiles, docker-compose configs, NGINX configs, etc. You'll learn from real, working configurations and scripts used in production at Talk Python. References to a companion GitHub repository are also provided, letting you adapt and run these examples yourself.&lt;/p&gt;
    &lt;p&gt;Both. The text features migrations (like moving providers or frameworks) alongside instructions to build fresh Docker-based services. Whether your app is already deployed somewhere or you're starting from scratch, the examples should be relevant and adaptable.&lt;/p&gt;
    &lt;p&gt;That's encouraged! You can pick and choose:&lt;/p&gt;
    &lt;p&gt;Yes. The book details:&lt;/p&gt;
    &lt;p&gt;There's a GitHub Discussions page in the book's companion repository, plus the author's social links. You can:&lt;/p&gt;
    &lt;p&gt;To learn a practical, no-nonsense way to deploy and operate Python applications at scale without drowning in cloud complexity or spiraling costs. You'll come away confident in your ability to:&lt;/p&gt;
    &lt;p&gt;Welcome to Talk Python in Production! Learn how Talk Python manages a podcast website, SaaS e-commerce platform, and mobile API traffic without overspending or vendor lock-in. We’ll explore best practices for scaling, cost control, and infrastructure reliability, without building our own data center. This chapter sets the stage for an empowering journey into a more self-reliant, efficient way of running Python applications in production. Enjoy the ride!&lt;/p&gt;
    &lt;p&gt;We explore how Talk Python outgrew PythonAnywhere and transitioned to dedicated Linux VMs. Starting with a PaaS was ideal for testing podcast waters, but maintenance downtime and limited control prompted the shift. DigitalOcean offered a simpler, more affordable alternative to hyperscale providers like AWS. With eight small VMs running various apps, podcasts, courses, APIs, the approach worked but introduced complexities, especially around updates and reboots. Ultimately, dividing apps across multiple servers didn’t deliver the expected isolation, simplicity, or cost savings. This chapter highlights the lessons learned, paving the way for a more efficient, containerized hosting environment in future chapters.&lt;/p&gt;
    &lt;p&gt;Learn how Talk Python escaped the complexity and costs of hyperscale cloud platforms, inspired by David Heinemeier Hansson’s We have left the cloud journey. Instead of maintaining multiple VMs, we consolidated everything into a single, powerful server running Docker and Docker Compose for seamless app isolation and portability. Enjoy a simpler, faster, and cloud-agnostic setup; without rewriting apps for proprietary services or sacrificing performance and reliability.&lt;/p&gt;
    &lt;p&gt;See why Talk Python ditched multiple small servers for a single, larger machine with eight vCPUs and 16 GB RAM. Although the total compute power is the same, consolidating everything under one server simplifies updates, as only one machine needs patching, and dramatically increases peak performance. When each web app spikes separately, it can tap into nearly all eight CPUs, rather than being limited to a small VM. This approach provides 4-8x more capacity for critical moments at roughly the same price. Explore how Docker-based isolation still protects each app while allowing full compute sharing and streamlined management.&lt;/p&gt;
    &lt;p&gt;In this chapter, learn why Talk Python embraced Docker on one powerful server instead of sprawling cloud services or heavy orchestration. Containers run directly on the host OS, providing near-zero overhead and a simple yet robust architecture. By consolidating everything under a single VM, Talk Python achieves 99.999% uptime and just 19 seconds of monthly downtime, without Kubernetes-level complexity or vendor lock-in. Inspired by self-hosting success stories like Basecamp and Zig, this approach avoids runaway costs, offers effortless portability, and still benefits from the hosting provider's reliable data center. See how 'just Docker Compose' delivers both performance and peace of mind.&lt;/p&gt;
    &lt;p&gt;Learn how Talk Python accelerates its web apps by adopting Granian, a Rust-based HTTP server supporting both WSGI and ASGI. Discover why legacy options like uWSGI have faded and how Granian compares to Gunicorn and Uvicorn in benchmarks. By harnessing Rust’s battle-tested “hyper” library, Granian delivers impressive throughput and low latency—ideal for Python’s I/O-heavy workloads. You’ll also see how multiple worker processes (a “web garden”) help overcome Python’s GIL, ensuring full hardware utilization. Finally, explore how Talk Python’s Docker-based deployment simplifies upgrades, fosters flexibility, and keeps application performance consistently high in production.&lt;/p&gt;
    &lt;p&gt;Discover how self-hosting with Docker Compose opens up a world of simple, flexible add-ons for your Python apps. In this chapter, you’ll learn how Talk Python integrates open-source analytics (Umami) and downtime monitoring (Uptime Kuma) right on the same server, no extra subscriptions or third-party data sharing required. By running these Dockerized services alongside your main application, you’ll gain deeper insight into user behavior and real-time uptime status. Plus, you’ll see how to manage external volumes for persistent data and free yourself from vendor lock-in. Explore thousands of self-hostable solutions at awesome-selfhosted.net and create the ultimate DIY stack.&lt;/p&gt;
    &lt;p&gt;Discover practical ways to monitor and manage Dockerized application, even if you’re new to containers. You’ll learn about btop and Glances, two free, feature-rich terminal dashboards that display CPU, memory, disk usage, and running processes, including those inside containers. Next, see how the Docker Cluster Monitor provides a real-time overview of container statuses and resource limits. You’ll also learn to log into a container’s shell, no SSH setup required, and persist logs with volume mappings to tail them from the host. Armed with these tools and techniques, you’ll gain full visibility and control of your self-hosted environment.&lt;/p&gt;
    &lt;p&gt;Uncover practical Docker optimizations for faster Python app builds and deployments. Learn how to reorder Dockerfile commands to leverage layer caching—so small changes (like editing a CSS file) won’t trigger full dependency reinstalls. Explore uv, a tool that quickly fetches Python versions and caches dependencies, while Docker’s --mount option preserves these caches across rebuilds. You’ll also see how a .dockerignore file trims out unneeded files, speeding up image builds. Finally, discover strategies for improving continuous integration times and reducing PyPI overhead, all while creating smaller, more efficient Docker images for your production environment.&lt;/p&gt;
    &lt;p&gt;Discover how Talk Python uses NGINX as a front-line server and reverse proxy for high-performance HTTPS routing. You’ll see exactly how NGINX connects to Python app servers like Granian or Uvicorn, handling SSL, static file serving, and domain routing. Learn how to automate free SSL certificates with Let’s Encrypt and Certbot, even in a locked-down Docker environment. Through example configurations, you’ll gain insights into mapping volumes for certificate management, creating auto-renew cron jobs, and handling multi-domain setups. Explore these approaches to build a secure, flexible, and modern web stack without locking yourself into proprietary cloud services.&lt;/p&gt;
    &lt;p&gt;Learn how Talk Python accelerates global content delivery by caching static assets and media through Bunny.net’s expansive worldwide network. You’ll see how replicating CSS, JavaScript, images, and even large audio/video files at 119 edge locations slashes latency for users, whether they’re streaming a podcast or loading a web page. Discover how querystring cache-busting ensures zero stale files on deployment, while straightforward billing (just $0.01/GB) keeps costs low. By offloading bandwidth to the CDN, Talk Python’s main server remains responsive. Explore these techniques to deliver a fast, smooth user experience without relying on hyperscale cloud services.&lt;/p&gt;
    &lt;p&gt;We walk through creating a fully operational production environment on a single Linux cloud server. You’ll learn how to provision a Hetzner VM, secure it with a firewall, install Docker, and build layered images for both Linux- and Python-based containers. Then, see how to deploy a Flask+HTMX app via Granian, wire it into NGINX, and ensure automatic startups with systemd. Finally, you’ll script deployments and updates, simplifying future releases. By following these step-by-step instructions, you’ll get an end-to-end blueprint for running Python web apps on a fully controlled, one-server infrastructure.&lt;/p&gt;
    &lt;p&gt;You’ll explore how to blend powerful static site generators, such as Hugo, into your existing Python web stack. First, discover Hugo’s flexibility for building non-blog sites (e.g., data-driven pages) from Markdown and CSV files, all without a database or backend server. Then, learn two ways to embed those static files within your Docker/NGINX environment: host them standalone or nest them under routes in existing Flask apps, like /blog or /docs. You’ll also see tips for cross-linking sitemaps so search engines understand your entire site’s structure, even across static and dynamic app sections.&lt;/p&gt;
    &lt;p&gt;Discover how Talk Python replaced its aging Pyramid setup with modern, async-capable Quart. Learn why frameworks like Django, FastAPI, and Litestar were strong contenders, but ultimately lost out to the simplicity and popularity of Flask’s ecosystem. Dive into the two-phase migration approach: first rewriting code to use Quart (staying synchronous), then upgrading critical database calls to async/await. You’ll see how this boost in concurrency trimmed response times from 42ms down to single digits, while eliminating legacy bottlenecks. If you’re evaluating Python web frameworks, this chapter offers a behind-the-scenes migration roadmap.&lt;/p&gt;
    &lt;p&gt;Discover why Talk Python chose to develop its own platforms rather than relying on third-party solutions. You’ll see how self-hosting the podcast site, courses, and bespoke integrations benefits both the business and its audience, delivering features like automated YouTube live stream announcements and advertiser dashboards with zero tracking. Beyond that, building your own tech fosters deep coding experience and keeps you in touch with real-world production challenges. While not everything warrants custom development, credit card processing, static sites, and more may benefit from outside tools, owning select parts of your stack can be a game-changer.&lt;/p&gt;
    &lt;p&gt;In this chapter, Michael Kennedy recounts Talk Python’s switch from DigitalOcean to Hetzner's new US data center. By migrating 20-plus services and databases, he gained superior CPU speeds, an 8x bandwidth boost, and cost savings of nearly $1,500 annually. You’ll see how server benchmarks and hands-on comparisons revealed Hetzner’s surprisingly affordable and powerful cloud VMs. Michael also explains why modernizing Talk Python’s “one big server” made this move straightforward, just export and re-deploy the containerized infrastructure. If you’ve ever wondered whether hosting abroad might benefit performance and budget, this Hetzner retrospective offers a firsthand look at the decision process.&lt;/p&gt;
    &lt;p&gt;Learn why the "lift-and-shift to your own data center" mindset isn’t a valid counterpoint to hyperscale, microservice-heavy "cloud-native" stacks. Instead, this chapter introduces "stack-native": a philosophy centered on running just enough containers (e.g., Flask, NGINX, Granian, and a database) on one well-chosen VM. It avoids vendor lock-in, scales seamlessly for many real-world apps, and remains fully owned by your small team. You’ll see why this approach, replete with Docker orchestration, custom volumes, and self-hosted databases, achieves the reliability of Kubernetes without the complexity or runaway cloud costs.&lt;/p&gt;
    &lt;p&gt;In Chapter 18: Conclusion, we finalize the journey of “stack-native” development, where simplicity, flexibility, and cost-effective ownership are front and center. You’ve seen how to deploy Python apps using Docker, NGINX, container orchestration, and more, all on a single, powerful VM. This approach ensures minimal complexity, straightforward management, and maximum autonomy. Whether you adopt all these practices or adapt only a few, you now have the tools and confidence to run Python in production, without lock-in or unnecessary cloud-native overhead. Stay connected through mkennedy.codes and the book’s GitHub discussions to share your success stories and keep learning!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://talkpython.fm/books/python-in-production"/><published>2025-10-06T19:59:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45514433</id><title>One-man campaign ravages EU 'Chat Control' bill</title><updated>2025-10-09T13:03:17.621096+00:00</updated><content>&lt;doc fingerprint="7d9694e3ca051efe"&gt;
  &lt;main&gt;
    &lt;p&gt;BRUSSELS — A website set up by an unknown Dane over the course of one weekend in August is giving a massive headache to those trying to pass a European bill aimed at stopping child sexual abuse material from spreading online.&lt;/p&gt;
    &lt;p&gt;The website, called Fight Chat Control, was set up by Joachim, a 30-year-old software engineer living in Aalborg, Denmark. He made it after learning of a new attempt to approve a European Union proposal to fight child sexual abuse material (CSAM) — a bill seen by privacy activists as breaking encryption and leading to mass surveillance.&lt;/p&gt;
    &lt;p&gt;The site lets visitors compile a mass email warning about the bill and send it to national government officials, members of the European Parliament and others with ease. Since launching, it has broken the inboxes of MEPs and caused a stir in Brussels’ corridors of power.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.politico.eu/article/one-man-spam-campaign-ravages-eu-chat-control-bill-fight-chat-control/"/><published>2025-10-08T10:26:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45516000</id><title>We found a bug in Go's ARM64 compiler</title><updated>2025-10-09T13:03:17.246455+00:00</updated><content>&lt;doc fingerprint="3b06c011ce263a54"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Every second, 84 million HTTP requests are hitting Cloudflare across our fleet of data centers in 330 cities. It means that even the rarest of bugs can show up frequently. In fact, it was our scale that recently led us to discover a bug in Go's arm64 compiler which causes a race condition in the generated code.&lt;/p&gt;
      &lt;p&gt;This post breaks down how we first encountered the bug, investigated it, and ultimately drove to the root cause.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Investigating a strange panic&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We run a service in our network which configures the kernel to handle traffic for some products like Magic Transit and Magic WAN. Our monitoring watches this closely, and it started to observe very sporadic panics on arm64 machines.&lt;/p&gt;
      &lt;p&gt;We first saw one with a fatal error stating that traceback did not unwind completely. That error suggests that invariants were violated when traversing the stack, likely because of stack corruption. After a brief investigation we decided that it was probably rare stack memory corruption. This was a largely idle control plane service where unplanned restarts have negligible impact, and so we felt that following up was not a priority unless it kept happening.&lt;/p&gt;
      &lt;p&gt;And then it kept happening.Â &lt;/p&gt;
      &lt;p&gt;When we first saw this bug we saw that the fatal errors correlated with recovered panics. These were caused by some old code which used panic/recover as error handling.Â &lt;/p&gt;
      &lt;p&gt;At this point, our theory was:Â &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;All of the fatal panics happen within stack unwinding.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We correlated an increased volume of recovered panics with these fatal panics.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Recovering a panic unwinds goroutine stacks to call deferred functions.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;A related Go issue (#73259) reported an arm64 stack unwinding crash.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Letâs stop using panic/recover for error handling and wait out the upstream fix?&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;So we did that and watched as fatal panics stopped occurring as the release rolled out. Fatal panics gone, our theoretical mitigation seemed to work, and this was no longer our problem. We subscribed to the upstream issue so we could update when it was resolved and put it out of our minds.&lt;/p&gt;
      &lt;p&gt;But, this turned out to be a much stranger bug than expected. Putting it out of our minds was premature as the same class of fatal panics came back at a much higher rate. A month later, we were seeing up to 30 daily fatal panics with no real discernible cause; while that might account for only one machine a day in less than 10% of our data centers, we found it concerning that we didnât understand the cause. The first thing we checked was the number of recovered panics, to match our previous pattern, but there were none. More interestingly, we could not correlate this increased rate of fatal panics with anything. A release? Infrastructure changes? The position of Mars? &lt;/p&gt;
      &lt;p&gt;At this point we felt like we needed to dive deeper to better understand the root cause. Pattern matching and hoping was clearly insufficient.Â &lt;/p&gt;
      &lt;p&gt;We saw two classes of this bug -- a crash while accessing invalid memory and an explicitly checked fatal error.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 153 gp=0x4000105340 m=324 mp=0x400639ea08 [GC worker (active)]:
/usr/local/go/src/runtime/asm_arm64.s:244 +0x6c fp=0x7ff97fffe870 sp=0x7ff97fffe860 pc=0x55558d4098fc
runtime.systemstack(0x0)
       /usr/local/go/src/runtime/mgc.go:1508 +0x68 fp=0x7ff97fffe860 sp=0x7ff97fffe810 pc=0x55558d3a9408
runtime.gcBgMarkWorker.func2()
       /usr/local/go/src/runtime/mgcmark.go:1102
runtime.gcDrainMarkWorkerIdle(...)
       /usr/local/go/src/runtime/mgcmark.go:1188 +0x434 fp=0x7ff97fffe810 sp=0x7ff97fffe7a0 pc=0x55558d3ad514
runtime.gcDrain(0x400005bc50, 0x7)
       /usr/local/go/src/runtime/mgcmark.go:212 +0x1c8 fp=0x7ff97fffe7a0 sp=0x7ff97fffe6f0 pc=0x55558d3ab248
runtime.markroot(0x400005bc50, 0x17e6, 0x1)
       /usr/local/go/src/runtime/mgcmark.go:238 +0xa8 fp=0x7ff97fffe6f0 sp=0x7ff97fffe6a0 pc=0x55558d3ab578
runtime.markroot.func1()
       /usr/local/go/src/runtime/mgcmark.go:887 +0x290 fp=0x7ff97fffe6a0 sp=0x7ff97fffe560 pc=0x55558d3acaa0
runtime.scanstack(0x4014494380, 0x400005bc50)
       /usr/local/go/src/runtime/traceback.go:447 +0x2ac fp=0x7ff97fffe560 sp=0x7ff97fffe4d0 pc=0x55558d3eeb7c
runtime.(*unwinder).next(0x7ff97fffe5b0?)
       /usr/local/go/src/runtime/traceback.go:566 +0x110 fp=0x7ff97fffe4d0 sp=0x7ff97fffe490 pc=0x55558d3eed40
runtime.(*unwinder).finishInternal(0x7ff97fffe4f8?)
       /usr/local/go/src/runtime/panic.go:1073 +0x38 fp=0x7ff97fffe490 sp=0x7ff97fffe460 pc=0x55558d403388
runtime.throw({0x55558de6aa27?, 0x7ff97fffe638?})
runtime stack:
fatal error: traceback did not unwind completely
       stack=[0x4015d6a000-0x4015d8a000
runtime: g8221077: frame.sp=0x4015d784c0 top=0x4015d89fd0&lt;/code&gt;
      &lt;/quote&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 187 gp=0x40003aea80 m=13 mp=0x40003ca008 [GC worker (active)]:
       /usr/local/go/src/runtime/asm_arm64.s:244 +0x6c fp=0x7fff2afde870 sp=0x7fff2afde860 pc=0x55557e2d98fc
runtime.systemstack(0x0)
       /usr/local/go/src/runtime/mgc.go:1489 +0x94 fp=0x7fff2afde860 sp=0x7fff2afde810 pc=0x55557e279434
runtime.gcBgMarkWorker.func2()
       /usr/local/go/src/runtime/mgcmark.go:1112
runtime.gcDrainMarkWorkerDedicated(...)
       /usr/local/go/src/runtime/mgcmark.go:1188 +0x434 fp=0x7fff2afde810 sp=0x7fff2afde7a0 pc=0x55557e27d514
runtime.gcDrain(0x4000059750, 0x3)
       /usr/local/go/src/runtime/mgcmark.go:212 +0x1c8 fp=0x7fff2afde7a0 sp=0x7fff2afde6f0 pc=0x55557e27b248
runtime.markroot(0x4000059750, 0xb8, 0x1)
       /usr/local/go/src/runtime/mgcmark.go:238 +0xa8 fp=0x7fff2afde6f0 sp=0x7fff2afde6a0 pc=0x55557e27b578
runtime.markroot.func1()
       /usr/local/go/src/runtime/mgcmark.go:887 +0x290 fp=0x7fff2afde6a0 sp=0x7fff2afde560 pc=0x55557e27caa0
runtime.scanstack(0x40042cc000, 0x4000059750)
       /usr/local/go/src/runtime/traceback.go:458 +0x188 fp=0x7fff2afde560 sp=0x7fff2afde4d0 pc=0x55557e2bea58
runtime.(*unwinder).next(0x7fff2afde5b0)
goroutine 0 gp=0x40003af880 m=13 mp=0x40003ca008 [idle]:
PC=0x55557e2bea58 m=13 sigcode=1 addr=0x118
SIGSEGV: segmentation violation&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now we could observe some clear patterns. Both errors occur when unwinding the stack in &lt;code&gt;(*unwinder).next&lt;/code&gt;. In one case we saw an intentional fatal error as the runtime identified that unwinding could not complete and the stack was in a bad state. In the other case there was a direct memory access error that happened while trying to unwind the stack. The segfault was discussed in the GitHub issue and a Go engineer identified it as dereference of a go scheduler struct, m, when unwinding.Â   &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;A review of Go scheduler structs&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Go uses a lightweight userspace scheduler to manage concurrency. Many goroutines are scheduled on a smaller number of kernel threads â this is often referred to as M:N scheduling. Any individual goroutine can be scheduled on any kernel thread. The scheduler has three core types â &lt;code&gt;g&lt;/code&gt;Â  (the goroutine), &lt;code&gt;m&lt;/code&gt; (the kernel thread, or âmachineâ), and &lt;code&gt;p&lt;/code&gt; (the physical execution context, orÂ  âprocessorâ). For a goroutine to be scheduled a free &lt;code&gt;m&lt;/code&gt; must acquire a free &lt;code&gt;p&lt;/code&gt;, which will execute a g. Each &lt;code&gt;g&lt;/code&gt; contains a field for its m if it is currently running, otherwise it will be nil. This is all the context needed for this post but the go runtime docs explore this more comprehensively.Â &lt;/p&gt;
      &lt;p&gt;At this point we can start to make inferences on whatâs happening: the program crashes because we try to unwind a goroutine stack which is invalid. In the first backtrace, if a return address is null, we call &lt;code&gt;finishInternal&lt;/code&gt; and abort because the stack was not fully unwound. The segmentation fault case in the second backtrace is a bit more interesting: if instead the return address is non-zero but not a function then the unwinder code assumes that the goroutine is currently running. It'll then dereference m and fault by accessing &lt;code&gt;m.incgo&lt;/code&gt; (the offset of &lt;code&gt;incgo&lt;/code&gt; into &lt;code&gt;struct m&lt;/code&gt; is 0x118, the faulting memory access). &lt;/p&gt;
      &lt;p&gt;What, then, is causing this corruption? The traces were difficult to get anything useful from â our service has hundreds if not thousands of active goroutines. It was fairly clear from the beginning that the panic was remote from the actual bug. The crashes were all observed while unwinding the stack and if this were an issue any time the stack was unwound on arm64 we would be seeing it in many more services. We felt pretty confident that the stack unwinding was happening correctly but on an invalid stack.Â &lt;/p&gt;
      &lt;p&gt;Our investigation stalled for a while at this point â making guesses, testing guesses, trying to infer if the panic rate went up or down, or if nothing changed. There was a known issue on Goâs GitHub issue tracker which matched our symptoms almost exactly, but what they discussed was mostly what we already knew. At some point when looking through the linked stack traces we realized that their crash referenced an old version of a library that we were also using â Go Netlink.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 1267 gp=0x4002a8ea80 m=nil [runnable (scan)]:
runtime.asyncPreempt2()
        /usr/local/go/src/runtime/preempt.go:308 +0x3c fp=0x4004cec4c0 sp=0x4004cec4a0 pc=0x46353c
runtime.asyncPreempt()
        /usr/local/go/src/runtime/preempt_arm64.s:47 +0x9c fp=0x4004cec6b0 sp=0x4004cec4c0 pc=0x4a6a8c
github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(0x14360300000000?)
        /go/pkg/mod/github.com/!data!dog/[email protected]/nl/nl_linux.go:803 +0x130 fp=0x4004cfc710 sp=0x4004cec6c0 pc=0xf95de0
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;We spot-checked a few stack traces and confirmed the presence of this Netlink library. Querying our logs showed that not only did we share a library â every single segmentation fault we observed had happened while preempting &lt;code&gt;NetlinkSocket.Receive&lt;/code&gt;.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Whatâs (async) preemption?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;In the prehistoric era of Go (&amp;lt;=1.13) the runtime was cooperatively scheduled. A goroutine would run until it decided it was ready to yield to the scheduler â usually due to explicit calls to &lt;code&gt;runtime.Gosched()&lt;/code&gt; or injected yield points at function calls/IO operations. Since Go 1.14 the runtime instead does async preemption. The Go runtime has a thread &lt;code&gt;sysmon&lt;/code&gt; which tracks the runtime of goroutines and will preempt any that run for longer than 10ms (at time of writing). It does this by sending &lt;code&gt;SIGURG&lt;/code&gt; to the OS thread and in the signal handler will modify the program counter and stack to mimic a call to &lt;code&gt;asyncPreempt&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;At this point we had two broad theories:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;This is a Go Netlink bug â likely due to &lt;code&gt;unsafe.Pointer&lt;/code&gt; usage which invoked undefined behavior but is only actually broken on arm64&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;This is a Go runtime bug and we're only triggering it in &lt;code&gt;NetlinkSocket.Receive&lt;/code&gt; for some reason&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;After finding the same bug publicly reported upstream, we were feeling confident this was caused by a Go runtime bug. However, upon seeing that both issues implicated the same function, we felt more skeptical â notably the Go Netlink library uses unsafe.Pointer so memory corruption was a plausible explanation even if we didn't understand why.&lt;/p&gt;
      &lt;p&gt;After an unsuccessful code audit we had hit a wall. The crashes were rare and remote from the root cause. Maybe these crashes were caused by a runtime bug, maybe they were caused by a Go Netlink bug. It seemed clear that there was something wrong with this area of the code, but code auditing wasnât going anywhere. &lt;/p&gt;
      &lt;p&gt;At this point we had a fairly good understanding of what was crashing but very little understanding of why it was happening. It was clear that the root cause of the stack unwinder crashing was remote from the actual crash, and that it had to do with &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt;, but why? We were able to capture a coredump of a production crash and view it in a debugger. The backtrace confirmed what we already knew â that there was a segmentation fault when unwinding a stack. The crux of the issue revealed itself when we looked at the goroutine which had been preempted while calling &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt;.Â     &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;(dlv) bt
0  0x0000555577579dec in runtime.asyncPreempt2
   at /usr/local/go/src/runtime/preempt.go:306
1  0x00005555775bc94c in runtime.asyncPreempt
   at /usr/local/go/src/runtime/preempt_arm64.s:47
2  0x0000555577cb2880 in github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive
   at
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:779
3  0x0000555577cb19a8 in github.com/vishvananda/netlink/nl.(*NetlinkRequest).Execute
   at 
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:532
4  0x0000555577551124 in runtime.heapSetType
   at /usr/local/go/src/runtime/mbitmap.go:714
5  0x0000555577551124 in runtime.heapSetType
   at /usr/local/go/src/runtime/mbitmap.go:714
...
(dlv) disass -a 0x555577cb2878 0x555577cb2888
TEXT github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(SB) /vendor/github.com/vishvananda/netlink/nl/nl_linux.go
        nl_linux.go:779 0x555577cb2878  fdfb7fa9        LDP -8(RSP), (R29, R30)
        nl_linux.go:779 0x555577cb287c  ff430191        ADD $80, RSP, RSP
        nl_linux.go:779 0x555577cb2880  ff434091        ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
        nl_linux.go:779 0x555577cb2884  c0035fd6        RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;The goroutine was paused between two opcodes in the function epilogue. Since the process of unwinding a stack relies on the stack frame being in a consistent state, it felt immediately suspicious that we preempted in the middle of adjusting the stack pointer. The goroutine had been paused at 0x555577cb2880, between&lt;code&gt; ADD $80, RSP, RSP and ADD $(16&amp;lt;&amp;lt;12), RSP, RSP&lt;/code&gt;.Â &lt;/p&gt;
      &lt;p&gt;We queried the service logs to confirm our theory. This wasnât isolated â the majority of stack traces showed that this same opcode was preempted. This was no longer a weird production crash we couldnât reproduce. A crash happened when the Go runtime preempted between these two stack pointer adjustments. We had our smoking gun.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Building a minimal reproducer&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;At this point we felt pretty confident that this was actually just a runtime bug and it should be reproducible in an isolated environment without any dependencies. The theory at this point was:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Stack unwinding is triggered by garbage collection&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Async preemption between a split stack pointer adjustment causes a crash&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;What if we make a function which splits the adjustment and then call it in a loop?&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;quote&gt;
        &lt;code&gt;package main

import (
	"runtime"
)

//go:noinline
func big_stack(val int) int {
	var big_buffer = make([]byte, 1 &amp;lt;&amp;lt; 16)

	sum := 0
	// prevent the compiler from optimizing out the stack
	for i := 0; i &amp;lt; (1&amp;lt;&amp;lt;16); i++ {
		big_buffer[i] = byte(val)
	}
	for i := 0; i &amp;lt; (1&amp;lt;&amp;lt;16); i++ {
		sum ^= int(big_buffer[i])
	}
	return sum
}

func main() {
	go func() {
		for {
			runtime.GC()
		}
	}()
	for {
		_ = big_stack(1000)
	}
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This function ends up with a stack frame slightly larger than can be represented in 16 bits, and so on arm64 the Go compiler will split the stack pointer adjustment into two opcodes. If the runtime preempts between these opcodes then the stack unwinder will read an invalid stack pointer and crash.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; epilogue for main.big_stack
ADD $8, RSP, R29
ADD $(16&amp;lt;&amp;lt;12), R29, R29
ADD $16, RSP, RSP
; preemption is problematic between these opcodes
ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;After running this for a few minutes the program panicked as expected!&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;SIGSEGV: segmentation violation
PC=0x60598 m=8 sigcode=1 addr=0x118

goroutine 0 gp=0x400019c540 m=8 mp=0x4000198708 [idle]:
runtime.(*unwinder).next(0x400030fd10)
        /home/thea/sdk/go1.23.4/src/runtime/traceback.go:458 +0x188 fp=0x400030fcc0 sp=0x400030fc30 pc=0x60598
runtime.scanstack(0x40000021c0, 0x400002f750)
        /home/thea/sdk/go1.23.4/src/runtime/mgcmark.go:887 +0x290 

[...]

goroutine 1 gp=0x40000021c0 m=nil [runnable (scan)]:
runtime.asyncPreempt2()
        /home/thea/sdk/go1.23.4/src/runtime/preempt.go:308 +0x3c fp=0x40003bfcf0 sp=0x40003bfcd0 pc=0x400cc
runtime.asyncPreempt()
        /home/thea/sdk/go1.23.4/src/runtime/preempt_arm64.s:47 +0x9c fp=0x40003bfee0 sp=0x40003bfcf0 pc=0x75aec
main.big_stack(0x40003cff38?)
        /home/thea/dev/stack_corruption_reproducer/main.go:29 +0x94 fp=0x40003cff00 sp=0x40003bfef0 pc=0x77c04
Segmentation fault (core dumped)

real    1m29.165s
user    4m4.987s
sys     0m43.212s&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;A reproducible crash with standard library only? This felt like conclusive evidence that our problem was a runtime bug.&lt;/p&gt;
      &lt;p&gt;This was an extremely particular reproducer! Even now with a good understanding of the bug and its fix, some of the behavior is still puzzling. It's a one-instruction race condition, so itâs unsurprising that small changes could have large impact. For example, this reproducer was originally written and tested on Go 1.23.4, but did not crash when compiled with 1.23.9 (the version in production), even though we could objdump the binary and see the split ADD still present! We donât have a definite explanation for this behavior â even with the bug present there remain a few unknown variables which affect the likelihood of hitting the race condition.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;A single-instruction race condition window&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;arm64 is a fixed-length 4-byte instruction set architecture. This has a lot of implications on codegen but most relevant to this bug is the fact that immediate length is limited. &lt;code&gt;add&lt;/code&gt; gets a 12-bit immediate, &lt;code&gt;mov&lt;/code&gt; gets a 16-bit immediate, etc. How does the architecture handle this when the operands don't fit? It depends â &lt;code&gt;ADD&lt;/code&gt; in particular reserves a bit for "shift left by 12" so any 24 bit addition can be decomposed into two opcodes. Other instructions are decomposed similarly, or just require loading an immediate into a register first.Â &lt;/p&gt;
      &lt;p&gt;The very last step of the Go compiler before emitting machine code involves transforming the program into &lt;code&gt;obj.Prog&lt;/code&gt; structs. It's a very low level intermediate representation (IR) that mostly serves to be translated into machine code.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;//https://github.com/golang/go/blob/fa2bb342d7b0024440d996c2d6d6778b7a5e0247/src/cmd/internal/obj/arm64/obj7.go#L856

// Pop stack frame.
// ADD $framesize, RSP, RSP
p = obj.Appendp(p, c.newprog)
p.As = AADD
p.From.Type = obj.TYPE_CONST
p.From.Offset = int64(c.autosize)
p.To.Type = obj.TYPE_REG
p.To.Reg = REGSP
p.Spadj = -c.autosize
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Notably, this IR is not aware of immediate length limitations. Instead, this happens in asm7.go when Go's internal intermediate representation is translated into arm64 machine code. The assembler will classify an immediate in conclass based on bit size and then use that when emitting instructions â extra if needed.&lt;/p&gt;
      &lt;p&gt;The Go assembler uses a combination of (&lt;code&gt;mov, add&lt;/code&gt;) opcodes for some adds that fit in 16-bit immediates, and prefers (&lt;code&gt;add, add + lsl 12&lt;/code&gt;) opcodes for 16-bit+ immediates.Â   &lt;/p&gt;
      &lt;p&gt;Compare a stack of (slightly larger than) &lt;code&gt;1&amp;lt;&amp;lt;15&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; //go:noinline
; func big_stack() byte {
; 	var big_stack = make([]byte, 1&amp;lt;&amp;lt;15)
; 	return big_stack[0]
; }
MOVD $32776, R27
ADD R27, RSP, R29
MOVD $32784, R27
ADD R27, RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;With a stack of &lt;code&gt;1&amp;lt;&amp;lt;16&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; //go:noinline
; func big_stack() byte {
; 	var big_stack = make([]byte, 1&amp;lt;&amp;lt;16)
; 	return big_stack[0]
; } 
ADD $8, RSP, R29
ADD $(16&amp;lt;&amp;lt;12), R29, R29
ADD $16, RSP, RSP
ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In the larger stack case, there is a point between &lt;code&gt;ADD x, RSP, RSP&lt;/code&gt; opcodes where the stack pointer is not pointing to the tip of a stack frame. We thought at first that this was a matter of memory corruption â that in handling async preemption the runtime would push a function call on the stack and corrupt the middle of the stack. However, this goroutine is already in the function epilogue â any data we corrupt is actively in the process of being thrown away. What's the issue then?  &lt;/p&gt;
      &lt;p&gt;The Go runtime often needs to unwind the stack, which means walking backwards through the chain of function calls. For example: garbage collection uses it to find live references on the stack, panicking relies on it to evaluate &lt;code&gt;defer&lt;/code&gt; functions, and generating stack traces needs to print the call stack. For this to work the stack pointer must be accurate during unwinding because of how golang dereferences sp to determine the calling function. If the stack pointer is partially modified, the unwinder will look for the calling function in the middle of the stack. The underlying data is meaningless when interpreted as directions to a parent stack frame and then the runtime will likely crash.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;//https://github.com/golang/go/blob/66536242fce34787230c42078a7bbd373ef8dcb0/src/runtime/traceback.go#L373

if innermost &amp;amp;&amp;amp; frame.sp &amp;lt; frame.fp || frame.lr == 0 {
    lrPtr = frame.sp
    frame.lr = *(*uintptr)(unsafe.Pointer(lrPtr))
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;When async preemption happens it will push a function call onto the stack but the parent stack frame is no longer correct because sp was only partially adjusted when the preemption happened. The crash flow looks something like this: &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Async preemption happens between the two opcodes that &lt;code&gt;add x, rsp&lt;/code&gt; expands to&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Garbage collection triggers stack unwinding (to check for heap object liveness)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The unwinder starts traversing the stack of the problematic goroutine and correctly unwinds up to the problematic function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The unwinder dereferences &lt;code&gt;sp&lt;/code&gt; to determine the parent function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Almost certainly the data behind &lt;code&gt;sp&lt;/code&gt; is not a function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Crash&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;We saw earlier a faulting stack trace which ended in &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt; â in this case stack unwinding faulted while it was trying to determine the parent frame.Â    &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 90 gp=0x40042cc000 m=nil [preempted (scan)]:
runtime.asyncPreempt2()
/usr/local/go/src/runtime/preempt.go:306 +0x2c fp=0x40060a25d0 sp=0x40060a25b0 pc=0x55557e299dec
runtime.asyncPreempt()
/usr/local/go/src/runtime/preempt_arm64.s:47 +0x9c fp=0x40060a27c0 sp=0x40060a25d0 pc=0x55557e2dc94c
github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(0xff48ce6e060b2848?)
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:779 +0x130 fp=0x40060b2820 sp=0x40060a27d0 pc=0x55557e9d2880
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Once we discovered the root cause we reported it with a reproducer and the bug was quickly fixed. This bug is fixed in go1.23.12, go1.24.6, and go1.25.0. Previously, the go compiler emitted a single &lt;code&gt;add x, rsp&lt;/code&gt; instruction and relied on the assembler to split immediates into multiple opcodes as necessary. After this change, stacks larger than 1&amp;lt;&amp;lt;12 will build the offset in a temporary register and then add that to &lt;code&gt;rsp&lt;/code&gt; in a single, indivisible opcode. A goroutine can be preempted before or after the stack pointer modification, but never during. This means that the stack pointer is always valid and there is no race condition.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;LDP -8(RSP), (R29, R30)
MOVD $32, R27
MOVK $(1&amp;lt;&amp;lt;16), R27
ADD R27, RSP, RSP
RET&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This was a very fun problem to debug. We donât often see bugs where you can accurately blame the compiler. Debugging it took weeks and we had to learn about areas of the Go runtime that people donât usually need to think about. Itâs a nice example of a rare race condition, the sort of bug that can only really be quantified at a large scale.&lt;/p&gt;
      &lt;p&gt;Weâre always looking for people who enjoy this kind of detective work. Our engineering teams are hiring. &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/how-we-found-a-bug-in-gos-arm64-compiler/"/><published>2025-10-08T13:33:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45517134</id><title>The RSS feed reader landscape</title><updated>2025-10-09T13:03:16.761629+00:00</updated><content>&lt;doc fingerprint="be149c54c0323cec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A deep dive into the rss feed reader landscape&lt;/head&gt;
    &lt;p&gt;RSS feeds and, in one form or another, feed readers, have existed for more than 20 years. Their main purpose is enabling their users to consume content from various sources in one place. And especially in recent years, also helping users deal with content overload.&lt;/p&gt;
    &lt;p&gt;Back then there were only a handful of relevant products. Today it's different, there are products for many different situations and use-cases. When first getting into RSS and feed readers, it can be difficult to find out which of this wide range of products are the right ones to use.&lt;/p&gt;
    &lt;p&gt;This article describes the landscape so you can find out which product fits best for your use-case.&lt;/p&gt;
    &lt;p&gt;Side-note: I'm using RSS as a synonym for all web feed standards (Atom, JSON Feed)&lt;/p&gt;
    &lt;head rend="h2"&gt;Classification&lt;/head&gt;
    &lt;p&gt;I attempted a classification of feed readers based on 2 axes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deployment model: local (phone or PC), browser extension, self-hosted, hosted&lt;/item&gt;
      &lt;item&gt;Business model: free, one-time payment, SAAS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The deployment model is based on where data is stored and feed fetching happens. Some products have a web app and mobile apps, but feed fetching happens on the server. In this case it's classified as &lt;code&gt;hosted&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The business model categorization is based on the cheapest option that gives access to the full feature set of the product. A self-hostable product with a hosted option is categorized into &lt;code&gt;free&lt;/code&gt;. A freemium SAAS product is categorized as &lt;code&gt;paid (SAAS)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And here the image in table format, for easily clickable links:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Pricing&lt;/cell&gt;
        &lt;cell role="head"&gt;On-device (phone or PC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Browser extension&lt;/cell&gt;
        &lt;cell role="head"&gt;Self-hosted&lt;/cell&gt;
        &lt;cell role="head"&gt;Hosted&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Free&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Paid (one-time)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Hosting models&lt;/head&gt;
    &lt;head rend="h3"&gt;Browser extension&lt;/head&gt;
    &lt;p&gt;Feed readers deployed as browser extensions can be installed through the respective stores of the browsers, usually either the Chrome Web Store or the Firefox Add-ons.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Setup typically only requires installing the extension, not even an account is needed. There is no maintenance required beyond the occasional update, which usually happens automatically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Data is stored locally, with browser storage functionality (local storage or IndexedDB). How much data can be stored depends on the storage of the device. Browser extensions can only store as much data as the browser allows itself to use. But for most users this is easily enough.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the device itself. Because the extension only runs if the browser runs, feeds are only fetched if the browser is open. This can lead to missed posts, depending on the publishing frequency of the feed and how often the browser is active.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Since all data is stored on the device, by default it's available only on the device where the extension is installed. If users enable it, the extension supports it, and the user is signed in, browsers can sync data to other devices that have the extension. Storing all data on the device also means that it is accessible offline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: Extensions can integrate deeply with the browser, and offer features like automatic feed finding of visited websites. Extensions can also provide a comprehensive feature set. Their only limitations are more compute-intensive features (e.g. requiring machine learning), or features that require special infrastructure to run (e.g. emails).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products (free)&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In this category I only found one product. Smart-RSS was another one, but was discontinued in February.&lt;/p&gt;
    &lt;head rend="h3"&gt;On-device&lt;/head&gt;
    &lt;p&gt;On-device products are separate applications and installed on the device you want to use them. Either on your iOS or Android phone or tablet, or on your Windows, Mac, or Linux computer. They fetch feeds and store data on that device.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: In general setup is done by installing the application. Some products may require an account, but that is not the norm. Maintenance is similar to browser extensions, the occasional updates. Some applications require manual update installations, others do that automatically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Data is stored locally on the device, which gives total control over the data. The maximum amount of feeds and articles stored only depend on the available storage of the device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the device. For that to happen the application must be running. Some products may implement a background fetching service, in that case only the device must be turned on. Similar to browser extensions, this limitation may lead to missed posts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Typically the data of on-device feed readers are only available on the device where it's installed. Data sync would need to be done manually or via operating system specific mechanisms (e.g. iCloud), which not all products support. Since data is stored on the device, it's also available offline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: On-device products can use the full computing power of the device. Combined with the comparatively small storage requirements of one user, it means that some features can be significantly faster than other categories. The exact features depend on the application, and mobile apps are typically less powerful than desktop apps. Limitations are only features that require multiple users (e.g. recommendations) or specific infrastructure (e.g. newsletter subscriptions).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Self-hosted&lt;/head&gt;
    &lt;p&gt;Self-hosted products all fall into the open-source and free category. They are designed to be installed on a server to run continuously, though it is possible to install them on a PC as well. They fetch feeds and store data on the server, and make it available through a web interface.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Setup requires a server, installing the application, and depending on how it should be accessible, possibly also domain and reverse proxy setup. This needs significantly more technical knowledge than the other options, but with ChatGPT (or others) it should be possible also for fairly non-technical people. This setup is generally more complex with more failure points than the other options, but with a correct setup failures happen rarely, if at all.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: The application stores data on the server it runs on. Since users control their server, they also have total control over the data stored on it. Applications typically use well-established databases, which allows users to inspect and change data with common tools. The amount of data that can be stored is only limited by the available storage on the server. Most servers start at 20 GB, which is enough for most feed reading use-cases. Often, storage can be dynamically extended if more space is needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the server. Since it runs continuously, there is no break in feed fetching, and even high frequency feeds are no problem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: The application and all its data is accessible from any device with a web browser by navigating to the server's URL. Data is stored on the server, therefore automatically synced between all devices that use it, but also require internet access.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: On average self-hosted products have a wider feature set than browser extension or on-device feed readers. There is no theoretical limit on the feature set, but typically they keep the infrastructure as simple as possible, to keep setup and maintenance straightforward. This makes them slightly less powerful than hosted alternatives.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;p&gt;Self-hosted products are all open-source and free. The only costs are for the server, and the cheapest VPS plans start at ~2$/month.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hosted&lt;/head&gt;
    &lt;p&gt;Hosted feed readers are managed services. It's required to create an account to get access to them. All of them are paid SAAS products, and most offer a free plan.&lt;/p&gt;
    &lt;p&gt;On average they have the most polished user experience and most comprehensive feature sets, since they're backed by companies that invest in continuous development.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Beyond creating an account, no setup is required. The same for maintenance. Service providers make sure the products work as intended, and deal with everything required to keep the service running, including infrastructure setup, monitoring, backups, etc.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Since data is stored on the servers and databases of the company running the product, users don't have direct control. But through laws like GDPR users have the right to export all their data, or request deletion. Storage is essentially unlimited, though most products have limits to avoid abuse. These limits are mentioned on their pricing page.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Similar to self-hosted products, feeds are fetched on the server, which runs continuously and ensures that even high-frequency feeds don't pose a problem. The fetching interval is usually dynamic, based on the popularity of a feed and the publishing frequency.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Hosted products are primarily available as web application, and some offer native apps as well. Since data is stored on the server, it's natively synced between devices. Some hosted feed readers also offer offline support.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: Hosted feed readers serve many customers at the same time, which makes it necessary to have more complex infrastructure setups. This also enables features that other types of products cannot do, like email receiving, recommendations, or historic feed contents. Consequently hosted options are generally more powerful than other categories, though the specific feature set depends on the product.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All hosted products are SAAS products and charge monthly. &lt;code&gt;Folo&lt;/code&gt; is an outlier, they currently don't have any paid features, but the terms of service indicate that this will come in the future. At that point it will join the others in the &lt;code&gt;Paid (SAAS)&lt;/code&gt; category.&lt;/p&gt;
    &lt;head rend="h2"&gt;App support and offline access for products that don't offer it natively&lt;/head&gt;
    &lt;p&gt;Many of the self-hosted or hosted products don't provide apps or offline access by themselves. However, with the right setup, it's still possible to access the articles offline.&lt;/p&gt;
    &lt;p&gt;Most of these products provide APIs. This can be a proprietary API, or an API based on the old Google Reader API.&lt;/p&gt;
    &lt;p&gt;Mobile apps, for example ReadKit or Fiery Feeds, can not only subscribe to feeds, but also connect to other products through their APIs. They download contents, store them locally, and sync changes back to the connected products.&lt;/p&gt;
    &lt;p&gt;These apps make it possible to use a native mobile app for products that don't provide an app themselves, and get offline access at the same time.&lt;/p&gt;
    &lt;p&gt;FreshRSS, for example, has a list of native apps that support it.&lt;/p&gt;
    &lt;head rend="h2"&gt;A note on newsletters&lt;/head&gt;
    &lt;p&gt;Newsletters are a growing mechanism for delivering (blog) content. Some, but not all, hosted feed readers support newsletters out of the box. But on-device products can't do that at all because of their infrastructure limitations.&lt;/p&gt;
    &lt;p&gt;Even if a feed reader doesn't support newsletters by itself, it's still possible to get newsletters into the feed reader, through services that convert newsletters into RSS feeds.&lt;/p&gt;
    &lt;p&gt;These services generate an email address and give you a corresponding feed URL. Emails to the generated address are added to the feed as new entry. So after signing up to the newsletter and adding the feed URL, the newsletter is added to the feed reader even without native email support.&lt;/p&gt;
    &lt;p&gt;Services that do this are Kill the Newsletter and the Lighthouse Newsletter to RSS tool. Both are free.&lt;/p&gt;
    &lt;head rend="h2"&gt;Products&lt;/head&gt;
    &lt;p&gt;The descriptions highlight the defining and unique aspects of the products, for the full feature list please go to their respective websites. Where available I used the screenshots for their websites, to represent the products as best as possible (test account screenshots wouldn't be as good).&lt;/p&gt;
    &lt;head rend="h3"&gt;NetNewsWire (on-device, free)&lt;/head&gt;
    &lt;p&gt;NetNewsWire is a free, on-device feed reader for Mac and iOS. By default it stores data on the device itself, but can sync via iCloud and using other products as storage. And it provides a Safari extension for easy feed-adding. Notably, it supports AppleScript, which makes it possible to automate certain workflows.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fiery Feeds (on device, paid SAAS)&lt;/head&gt;
    &lt;p&gt;Fiery feeds is a Mac and iOS app. The download is free, and to get the premium features it costs ~15$/year (varies by region). By default it stores data on the device itself, but can sync via iCloud and using the API of other products (e.g. FreshRSS). The main distinguishing features are the customization options of the app, like custom themes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reeder (on-device, SAAS)&lt;/head&gt;
    &lt;p&gt;The current version of Reeder is a Mac and iOS app. The download is free, and to get the premium features it costs ~10$/year. The previous version of Reeder, now called Reeder Classic, is still available as one-time purchase. It also stores data on-device, and can sync via iCloud.&lt;/p&gt;
    &lt;p&gt;The main distinguishing feature is the unified timeline, which includes RSS, podcasts, social media, and more.&lt;/p&gt;
    &lt;head rend="h3"&gt;FreshRSS (self-hosted, free)&lt;/head&gt;
    &lt;p&gt;FreshRSS is a self-hosted feed reader and once set up, it's available as a web app. It is quite powerful, and in addition to normal feed subscriptions offers WebSub as well. It's possible to customize it with themes and extensions, and it's translated into more than 15 languages.&lt;/p&gt;
    &lt;p&gt;Together with Miniflux they're the most-recommended self-hosted options.&lt;/p&gt;
    &lt;head rend="h3"&gt;Miniflux&lt;/head&gt;
    &lt;p&gt;Miniflux is a self-hosted feed reader as well, and available as web app after setup. It's focus is on staying simple and fast instead of adding fancy features. It also offers a hosted version, which is 15$/year and has a 15 day trial period.&lt;/p&gt;
    &lt;p&gt;Together with FreshRSS they're the most-recommended self-hosted options.&lt;/p&gt;
    &lt;head rend="h3"&gt;Folo (hosted, free)&lt;/head&gt;
    &lt;p&gt;Folo is a relatively new product developed by the creators of RSS Hub. It's a free hosted feed reader, with apps available for every major platform. Their terms of service suggest that at some point they will charge a monthly fee for some premium features, but at the time of writing there were no paid features.&lt;/p&gt;
    &lt;p&gt;Folo is also open-source and therefore theoretically self-hostable, but I haven't found documentation for it.&lt;/p&gt;
    &lt;p&gt;It has a huge feature set, including newsletter support, transforming websites into feeds, AI summaries, and much more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feedly (hosted, SAAS)&lt;/head&gt;
    &lt;p&gt;Feedly is the most widely-known product and has the most users of all feed readers. It has a comprehensive features set, and offers a free plan as well. For the past couple of years, it seems that Feedly has focused more on AI features and enterprise customers, but their core product remains solid.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inoreader&lt;/head&gt;
    &lt;p&gt;Inoreader is the second most widely-known product, after Feedly. It too offers a free plan. Their feature list is impressive, with social media support for subscriptions, automation and AI features, public API and integrations. And of course much more.&lt;/p&gt;
    &lt;p&gt;What stands out on Reddit are complaints about price hikes. Though Inoreader probably has tens or hundreds of thousands of users who don't have issues and think the product is great.&lt;/p&gt;
    &lt;head rend="h3"&gt;Readwise Reader&lt;/head&gt;
    &lt;p&gt;Readwise Reader is a relatively new product, from the creators of the original Readwise. It's a great feed reader, though were it really shines is the reading experience. They also have reading views for PDFs, eBooks, and more.&lt;/p&gt;
    &lt;p&gt;The website mentions that it's in beta, but it has been in beta for years now and at this point is a stable product, and has been for awhile.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tiny Tiny RSS&lt;/head&gt;
    &lt;p&gt;Tiny Tiny RSS deserves an honorable mention in this list. It was, and still is, used by many, and has been for a long time, and was often recommended on the RSS subreddit.&lt;/p&gt;
    &lt;p&gt;On October 3rd the maintainer announced that he's going to stop working on it, and will remove all infrastructure on November 1st. Forks of the project with other maintainers may pop up, but at the moment it's too soon to tell what the future of Tiny Tiny RSS will be.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lighthouse&lt;/head&gt;
    &lt;p&gt;Lighthouse is also a relatively new product. It's in beta, which refers to the fact that it doesn't yet have all features necessary to fulfill its vision, which goes beyond simple feed reading.&lt;/p&gt;
    &lt;p&gt;The main differentiator is that it focuses on articles over feeds, and has a separate view for curating articles (the inbox). It's focused on finding high-value content.&lt;/p&gt;
    &lt;head rend="h2"&gt;Similar product categories&lt;/head&gt;
    &lt;p&gt;Feed readers are powerful products, which require manual setup to make them work well. Subscribing to feeds is mandatory, adding tags, filtered views, and rules can make them work even better. But for some use-cases, other product categories are simpler and might work better.&lt;/p&gt;
    &lt;head rend="h3"&gt;News aggregators&lt;/head&gt;
    &lt;p&gt;News aggregators automatically collect and curate news from a large variety of sources. They usually provide some customization options, but focus on news and don't allow arbitrary feed subscriptions like feed readers do.&lt;/p&gt;
    &lt;p&gt;They focus on providing an overview of the most relevant news stories.&lt;/p&gt;
    &lt;p&gt;Examples are Kagi News, Ground News, and SmartNews.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reading lists&lt;/head&gt;
    &lt;p&gt;Reading lists, also called read-it-later apps, are for saving and organizing links you found somewhere on the web. Many feed readers can do the same, but reading lists are optimized for that purpose.&lt;/p&gt;
    &lt;p&gt;Examples are Instapaper and Matter. Karakeep is a self-hosted option.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to choose&lt;/head&gt;
    &lt;p&gt;For most people, going with one of the hosted products is the best option. They're generally the most polished and most powerful products, owing to the fact that they have companies behind them with full-time engineers. They generally also offer a free plan, so if you stay within the limits of those, they will even be free to use.&lt;/p&gt;
    &lt;p&gt;For more specific requirements, products of the other categories can work better. They have different characteristics, and can work better in some situations. For example, if you want total control over your data, self-hosted options are the way to go.&lt;/p&gt;
    &lt;p&gt;It's probably easiest to first decide on the category, and then check out multiple products, or maybe even try them out. Virtually all products support OPML import and export, so moving feed subscriptions from one product to the next is almost zero effort.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lighthouseapp.io/blog/feed-reader-deep-dive"/><published>2025-10-08T15:17:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45518813</id><title>WinBoat: Windows apps on Linux with seamless integration</title><updated>2025-10-09T13:03:16.565953+00:00</updated><content>&lt;doc fingerprint="b48fe95fd1b73cdc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt; Run Windows apps on 🐧 Linux&lt;lb/&gt;with ✨ seamless integration &lt;/head&gt;
    &lt;head rend="h2"&gt;FFFFF/Features/sssss&lt;/head&gt;
    &lt;head rend="h2"&gt;Elegant Interface&lt;/head&gt;
    &lt;p&gt;WinBoat provides a sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Automated Installs&lt;/head&gt;
    &lt;p&gt;Installing Windows via WinBoat is a delightfully simple process all done through our interface. Pick your preferences &amp;amp; specs and let us handle the rest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run Any App&lt;/head&gt;
    &lt;p&gt;If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications, from productivity tools to entertainment, all within your Linux environment as native OS-level windows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Filesystem Integration&lt;/head&gt;
    &lt;p&gt;Accessing your Linux filesystem from Windows is a breeze. Your home directory is mounted in Windows, allowing you to easily share files between the two systems without any hassle.&lt;/p&gt;
    &lt;head rend="h2"&gt;And many more...&lt;/head&gt;
    &lt;p&gt;There's tons more you can do with WinBoat, for example smartcard passthrough and resource monitoring. We're always looking to add more features, so stay tuned for updates!&lt;/p&gt;
    &lt;p&gt;Scroll for more!&lt;/p&gt;
    &lt;head rend="h2"&gt;Elegant Interface&lt;/head&gt;
    &lt;p&gt;WinBoat provides a sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Automated Installs&lt;/head&gt;
    &lt;p&gt;Installing Windows via WinBoat is a delightfully simple process all done through our interface. Pick your preferences &amp;amp; specs and let us handle the rest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run Any App&lt;/head&gt;
    &lt;p&gt;If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications, from productivity tools to entertainment, all within your Linux environment as native OS-level windows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Filesystem Integration&lt;/head&gt;
    &lt;p&gt;Accessing your Linux filesystem from Windows is a breeze. Your home directory is mounted in Windows, allowing you to easily share files between the two systems without any hassle.&lt;/p&gt;
    &lt;head rend="h2"&gt;And many more...&lt;/head&gt;
    &lt;p&gt;There's tons more you can do with WinBoat, for example smartcard passthrough and resource monitoring. We're always looking to add more features, so stay tuned for updates!&lt;/p&gt;
    &lt;head rend="h2"&gt;DDDD/Download/ddddd&lt;/head&gt;
    &lt;p&gt;We're excited to have you onboard! Pick your platform below to get started with WinBoat within minutes, not hours.&lt;/p&gt;
    &lt;head rend="h2"&gt;CCCC/Contribute/eeeee&lt;/head&gt;
    &lt;p&gt;WinBoat is an open-source project licensed under MIT, and we welcome contributions from the community. Whether you're a developer, designer, or just someone who loves WinBoat, there are many ways you can help us improve and grow.&lt;/p&gt;
    &lt;head rend="h2"&gt;CCCC/Community/yyyyy&lt;/head&gt;
    &lt;p&gt;We usually hang out on Discord, come join and chat with us!&lt;/p&gt;
    &lt;head rend="h2"&gt;FFFFF/Frequently Asked Questions/sssss&lt;/head&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;How does it compare to WinApps?&lt;/head&gt;
    &lt;p&gt;With WinApps you do the bulk of the setup manually, and there's no cohesive interface to bring it all together. There's a basic TUI, a taskbar widget, and some CLI commands for you to play with.&lt;lb/&gt; WinBoat does all the setup once you have the pre-requisites installed, displays everything worth seeing in a neat interface for you, and acts like a complete experience. No need to mess with configuration files, no need to memorize a dozen CLI commands, it just works.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;What are the advantages of using this over CrossOver or WINE?&lt;/head&gt;
    &lt;p&gt;You can run stuff that doesn't play well with CrossOver or WINE, and have a full Windows desktop at the same time.&lt;lb/&gt; We've had numerous apps that weren't working nicely (or at all) in Wine, this is one of the reasons we've created WinBoat. Some examples would be Affinity Photo, Paint Tool Sai v1.0, the entire Adobe suite, AeroChat, Acrobat, and of course Office.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Will I be able to configure my peripherals / hardware using WinBoat?&lt;/head&gt;
    &lt;p&gt;If your peripheral / hardware uses USB to connect to your device. then yes! Starting from WinBoat 0.8.0 we support USB passthrough as an experimental feature and you can use any Windows software needed for configuration, it should work out of the box.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Is there USB passthrough?&lt;/head&gt;
    &lt;p&gt; Yes! Starting from WinBoat 0.8.0 we support USB passthrough as an experimental feature. Please give it a try and let us know what you think! 😄&lt;lb/&gt; If for whatever reason you're stuck with an older version of WinBoat, you can modify the docker-compose.yml file in ~/.winboat once you finished setting up WinBoat. You can add the appropriate USB devices like this, followed by executing docker-compose down and docker-compose up -d in the same folder. Please make sure to remove these changes before you upgrade to &amp;gt;=0.8.0 though, as they are incompatible with our implementation.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Is there GPU passthrough?&lt;/head&gt;
    &lt;p&gt;Not at the moment, but we plan on eventually implementing GPU acceleration through paravirtualized drivers.&lt;lb/&gt; We have looked at MVisor Win VGPU Driver for OpenGL, which seems promising from our tests, but it's for a different hypervisor (not compatible with QEMU). Some other folks are also working on DirectX drivers but nothing that we can try out yet.&lt;lb/&gt; We have also looked into Looking Glass extensively, specifically their Indirect Display Driver which does not need a second GPU, because it'd be absolutely amazing to have it. We got the driver to compile and start via some hacks, but couldn't get much more than a black screen. The developer says it is not ready for general use yet at all, however we plan to integrate it once it is ready.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Does it run games with anti-cheat that don't run on Linux?&lt;/head&gt;
    &lt;p&gt;Unfortunately running games with kernel anti-cheat is not possible, as they block virtualization.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Any possibility of adding Podman support as a Docker alternative?&lt;/head&gt;
    &lt;p&gt;Podman support is planned. We tried working on it, some contributors also tried working on it, but there's some issues with networking (specifically the guest server being unreachable) that prevent it from being functional for now.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Will you make it into a Flatpak?&lt;/head&gt;
    &lt;p&gt;This is on our to-do list, but it'll take some effort because Flatpak is pretty isolated from the rest of the system and apps, so we'd have to find a way to expose installed apps, the Docker binary, and the Docker socket, and many other utilities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.winboat.app/"/><published>2025-10-08T17:56:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45520615</id><title>Kurt Got Got</title><updated>2025-10-09T13:03:16.236989+00:00</updated><content>&lt;doc fingerprint="ee3b595b1b4777cd"&gt;
  &lt;main&gt;
    &lt;p&gt;The $FLY Airdrop is live! Claim your share of the token powering Fly.ioâs global network of 3M+ apps and (ð¤®) own a piece of the sky!&lt;/p&gt;
    &lt;p&gt;We know. Our Twitter got owned. We knew within moments of it happening. We know exactly how it happened. Nothing was at risk other than our Twitter account (and one Fly.io employee’s self-esteem). Also: for fuck’s sake.&lt;/p&gt;
    &lt;p&gt;Here’s what happened: Kurt Mackey, our intrepid CEO, got phished.&lt;/p&gt;
    &lt;p&gt;Had this been an impactful attack, we would not be this flippant about it. For this, though, any other tone on our part would be false.&lt;/p&gt;
    &lt;head rend="h2"&gt;How They Got Kurt&lt;/head&gt;
    &lt;p&gt;Two reasons: one, it was a pretty good phishing attack, and two, Twitter fell outside the “things we take seriously” boundary.&lt;/p&gt;
    &lt;p&gt;The phishing attack was effective because it exploited a deep psychological vulnerability in our management team: we are old and out of touch with the youths of today.&lt;/p&gt;
    &lt;p&gt;For many months now, we’ve had an contractor/intern-type-person Boosting Our Brand on Twitter by posting dank developer memes (I think that’s what they’re called). The thing about this dankery is that we don’t really understand it. I mean, hold on, we know what the memes mean technically. We just don’t get why they’re funny.&lt;/p&gt;
    &lt;p&gt;However, in pushing back on them, we’re up against two powerful forces:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The dank memes appear to perform better than the stuff we ourselves write on Twitter.&lt;/item&gt;
      &lt;item&gt;We are reliably informed by our zoomer children that we are too cringe to be trusted on these matters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the phish Kurt got:&lt;/p&gt;
    &lt;p&gt;Diabolical. Like a scalpel expertly wielded against Kurt’s deepest middle-aged-dude insecurity. Our ruthless attackers clinically designed this email to trigger an autonomic Kurt response: “oh, what the fuck is this, and why did we post it?”&lt;/p&gt;
    &lt;p&gt;ATO is cool-kid for âgot ownedâ&lt;/p&gt;
    &lt;p&gt;I’m getting a little ahead of the story here. We knew our X.com account had suffered an ATO because a bunch of us simultaneously got another email saying that the @flydotio account’s email address now pointed to &lt;code&gt;achilles19969@gmail.com&lt;/code&gt;. Our immediate response was to audit all accesses to the login information in 1Password, to cut all access for anybody who’d recently pulled it; your worst-case assumption in a situation like this is that someone’s endpoint has been owned up.&lt;/p&gt;
    &lt;p&gt;Fortunately, nobody lost access for very long. I called Kurt to let him know why he was being locked out, and 5 seconds later, he’d realized what had happened. Don’t click anything there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why It Worked&lt;/head&gt;
    &lt;p&gt;That’s the right question to ask, isn’t it? How could this have been possible in the first place?&lt;/p&gt;
    &lt;p&gt;Contrary to one popular opinion, you don’t defeat phishing by training people not to click on things. I mean, tell them not to, sure! But eventually, under continued pressure, everybody clicks. There’s science on this. The cool kids haven’t done phishing simulation training in years.&lt;/p&gt;
    &lt;p&gt;What you’re supposed to do instead is use phishing-resistant authentication. This is almost the whole backstory for U2F, FIDO2 and Passkeys.&lt;/p&gt;
    &lt;p&gt;Phishing-resistant authentication works by mutual authentication (or, if you’re a stickler, by origin- and channel-binding). Phishes are malicious proxies for credentials. Modern MFA schemes like FIDO2 break that proxy flow; your browser won’t send real credentials to the fake site.&lt;/p&gt;
    &lt;p&gt;thereâs more to it than this, but, broad strokes.&lt;/p&gt;
    &lt;p&gt;This is, in fact, how all of our infrastructure is secured at Fly.io; specifically, we get everything behind an IdP (in our case: Google’s) and have it require phishing-proof MFA. You’re unlikely to phish your way to viewing logs here, or to refunding a customer bill at Stripe, or to viewing infra metrics, because all these things require an SSO login through Google.&lt;/p&gt;
    &lt;p&gt;Twitter, on the other hand. Yeah, so, about that. You may have heard that, a few years back, there were some goings-on involving Twitter. Many of us at Fly.io decamped for Mastodon, and later to Bluesky. There was a window of time in 2023-2024 where it looked as if Twitter might not be a long term thing for us at all.&lt;/p&gt;
    &lt;p&gt;â (to whom I sincerely apologize for having assumed they had been owned up and were the proximate cause of the hack)&lt;/p&gt;
    &lt;p&gt;As a result, Twitter had been a sort of legacy shared account for us, with credentials managed in 1Password and shared with our zoomer contractorâ .&lt;/p&gt;
    &lt;p&gt;Which is why Kurt was in a position to pull credentials from 1Password and log in to members-x.com in response to an email from alerts-x.com.&lt;/p&gt;
    &lt;p&gt;Still: we could have dodged this attack with hygiene: Kurt complains that âx.comâ is an extremely phishable domain, and, sure, but also: the 1Password browser plugin would have noticed that âmembers-x.comâ wasnât an âx.comâ host.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Took So Long&lt;/head&gt;
    &lt;p&gt;The attacker immediately revoked all tokens and set up new 2FA, so while we were quickly able to reset our password, we couldn’t lock them out of our account without an intervention from X.com, which took something like 1 5 hours to set up.&lt;/p&gt;
    &lt;p&gt;(That’s not a knock on X.com; 15 hours for a 2FA reset isn’t outside industry norms).&lt;/p&gt;
    &lt;p&gt;We’re obviously making a lot of noise about this now, but we were pretty quiet during the incident itself (beyond just “We know. We knew 45 seconds after it happened. We know exactly how it happened. It’s just a Twitter thing.”)&lt;/p&gt;
    &lt;p&gt;That’s because, in the grand scheme of things, the attack was pretty chill: a not-very-plausible crypto scam that presumably generated $0 for the attackers, 15+ hours of &lt;code&gt;brand damage&lt;/code&gt;, and extra security engineering cycles burnt on watchful waiting. Our users weren’t under attack, and the account wasn’t being used to further intercept customer accounts. At one point, the attackers apparently deleted our whole Twitter history, which, like, don’t threaten us with a good time. So we let it roll, until we got our account recovered the next morning.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Moral Of The Story Is&lt;/head&gt;
    &lt;p&gt;âReally the biggest takeaway for me is that Kurt reads his email.â&lt;/p&gt;
    &lt;p&gt;Obviously Kurt loses his commit access. The time comes in the life of every CEO, and now it comes for him.&lt;/p&gt;
    &lt;p&gt;Also, we’ll finally have a population sample for “incident response” in our next SOC2.&lt;/p&gt;
    &lt;p&gt;Maybe we’ll post more on Twitter. Or maybe we’ll double down on Zoomer memes. I don’t know. Social media is really weird right now. Either way: our Twitter access is Passkeys now.&lt;/p&gt;
    &lt;p&gt;seriously donât click anything on that page&lt;/p&gt;
    &lt;p&gt;If you were inclined to take us up on an “airdrop” to “claim a share” of the “token” powering Fly.io, the site is still up. You can connect your wallet it it! You’ll lose all your money. But if we’d actually done an ICO, you’d have lost all your money anyways.&lt;/p&gt;
    &lt;p&gt;Somebody involved in pulling this attack off had to come up with “own a piece of the sky!”, and I think that’s punishment enough for them.&lt;/p&gt;
    &lt;p&gt;Whatever you’re operating that isn’t behind phishing-resistant MFA, or, better yet, an SSO IdP that requires phishing-resistant MFA: that thing is eventually going to get phished. Dance around the clown-fire of our misfortune if you must, but let us be a lesson to you as well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fly.io/blog/kurt-got-got/"/><published>2025-10-08T21:02:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45521629</id><title>OpenAI, Nvidia fuel $1T AI market with web of circular deals</title><updated>2025-10-09T13:03:16.033390+00:00</updated><content>&lt;doc fingerprint="78908d6cc42466b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenAI, Nvidia Fuel $1 Trillion AI Market With Web of Circular Deals&lt;/head&gt;
    &lt;p&gt;A wave of deals and partnerships are escalating concerns that the trillion-dollar AI boom is being propped up by interconnected business transactions.&lt;/p&gt;
    &lt;p&gt;Two weeks ago, Nvidia Corp. agreed to invest as much as $100 billion in OpenAI to help the leading AI startup fund a data-center buildout so massive it could power a major city. OpenAI in turn committed to filling those sites with millions of Nvidia chips. The arrangement was promptly criticized for its “circular” nature.&lt;/p&gt;
    &lt;p&gt;This week, undeterred, OpenAI struck a similar deal. The ChatGPT maker on Monday inked a partnership with Nvidia rival Advanced Micro Devices Inc. to deploy tens of billions of dollars’ worth of its chips. As part of the tie-up, OpenAI is poised to become one of AMD’s largest shareholders.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bloomberg.com/news/features/2025-10-07/openai-s-nvidia-amd-deals-boost-1-trillion-ai-boom-with-circular-deals"/><published>2025-10-08T23:04:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45521738</id><title>Discord says 70k users may have had their government IDs leaked in breach</title><updated>2025-10-09T13:03:15.840975+00:00</updated><content>&lt;doc fingerprint="52821b91fd9720ef"&gt;
  &lt;main&gt;
    &lt;p&gt;Discord has identified approximately 70,000 users that may have had their government ID photos exposed as part of a customer service data breach announced last week, spokesperson Nu Wexler tells The Verge. A tweet by vx-underground said that the company was being extorted over a breach of its Zendesk instance by a group claiming to have “1.5TB of age verification related photos. 2,185,151 photos.”&lt;/p&gt;
    &lt;head rend="h1"&gt;Discord says 70,000 users may have had their government IDs leaked in breach&lt;/head&gt;
    &lt;p&gt;Discord claims that the attackers are circulating inaccurate information about the breach of a customer service provider as part of an extortion attempt.&lt;/p&gt;
    &lt;p&gt;Discord claims that the attackers are circulating inaccurate information about the breach of a customer service provider as part of an extortion attempt.&lt;/p&gt;
    &lt;p&gt;When we asked about the tweet, Wexler shared this statement:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Following last week’s announcement about a security incident involving a third-party customer service provider, we want to address inaccurate claims by those responsible that are circulating online. First, as stated in our blog post, this was not a breach of Discord, but rather a third-party service we use to support our customer service efforts. Second, the numbers being shared are incorrect and part of an attempt to extort a payment from Discord. Of the accounts impacted globally, we have identified approximately 70,000 users that may have had government-ID photos exposed, which our vendor used to review age-related appeals. Third, we will not reward those responsible for their illegal actions.&lt;/p&gt;
      &lt;p&gt;All affected users globally have been contacted and we continue to work closely with law enforcement, data protection authorities, and external security experts. We’ve secured the affected systems and ended work with the compromised vendor. We take our responsibility to protect your personal data seriously and understand the concern this may cause.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In its announcement last week, Discord said that information like names, usernames, emails, the last four digits of credit cards, and IP addresses also may have been impacted by the breach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/news/797051/discord-government-ids-leaked-data-breach"/><published>2025-10-08T23:20:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45521920</id><title>A competitor crippled a $23.5M bootcamp by becoming a Reddit moderator</title><updated>2025-10-09T13:03:14.322155+00:00</updated><content>&lt;doc fingerprint="3a43713708620467"&gt;
  &lt;main&gt;
    &lt;p&gt;Let’s say you decide to start a coding bootcamp. Your background is in pedagogy and you love teaching. Your parents were teachers.&lt;/p&gt;
    &lt;p&gt;You find a co-founder, raise a bit of money, and pour your soul into your company.&lt;/p&gt;
    &lt;p&gt;The first couple of years, students love your program. Positive feedback, extraordinary student outcomes, employees love the mission. You are quite literally changing lives.&lt;/p&gt;
    &lt;p&gt;Your business grows. One day, you realize you’ve grown to 70 employees.&lt;/p&gt;
    &lt;p&gt;And then…&lt;/p&gt;
    &lt;p&gt;A competitor gets control of the main subreddit for your industry by becoming a Reddit Moderator.&lt;/p&gt;
    &lt;p&gt;That watering hole becomes their megaphone. They are not shy about using it. An all-out attack on your brand begins. The barrage of accusations and harassment are relentless. The attacks happen daily. You become a neurotic fixation of the moderator. Every little thing you do represents your failings as a company.&lt;/p&gt;
    &lt;p&gt;You get compared to a sex cult. One of your employees is accused of nepotism. The mod starts joining your company information sessions for prospective students, slinging conspiracy theories at every turn under a pseudonym. You avoid outright bans to avoid appearing biased yourself. Your own employees start wondering if these accusations have merit, some leave.&lt;/p&gt;
    &lt;p&gt;Even worse, confidential information from your company starts leaking out. There’s a mole. Or at least your team suspects it. No one knows who to trust. Your carefully built company culture? Eviscerated from the inside out.&lt;/p&gt;
    &lt;p&gt;Any time you attempt to defend yourself in the main subreddit, posts get deleted. Or you’re accused of running a Reddit bot army.&lt;/p&gt;
    &lt;p&gt;This goes on for years.&lt;/p&gt;
    &lt;p&gt;Every day, another attack. Every fucking day.&lt;/p&gt;
    &lt;p&gt;Student applications drop. First a little… then a lot.&lt;/p&gt;
    &lt;p&gt;Combined with a market downtown, your revenue collapses by 80%.&lt;/p&gt;
    &lt;p&gt;You go through 2 layoffs just to keep the lights on. A bunch of other folks move on. You’re down to 15 employees now.&lt;/p&gt;
    &lt;p&gt;The attacks don’t stop. If anything, they escalate.&lt;/p&gt;
    &lt;p&gt;And you begin to wonder, as the CEO and founder of your company… “Maybe I’m the problem? Maybe I am doing something wrong? Or somehow encouraging these attacks on my company?”&lt;/p&gt;
    &lt;p&gt;You make a decision to leave.&lt;/p&gt;
    &lt;p&gt;You step down, walk away from everything you built, and start anew.&lt;/p&gt;
    &lt;p&gt;But even then, the attacks don’t stop. Nothing can stop this nightmare.&lt;/p&gt;
    &lt;p&gt;This is the story of Will Sentance and his company, Codesmith.&lt;/p&gt;
    &lt;p&gt;It can happen to your business too.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Codesmith Reputation Attack via Google&lt;/head&gt;
    &lt;p&gt;For many businesses, the first page of Google for their brand name is the single most important asset for managing their reputation.&lt;/p&gt;
    &lt;p&gt;Let’s say I’m considering a bootcamp, I hear about “Codesmith,” and pop it into Google.&lt;/p&gt;
    &lt;p&gt;Fuck me that’s bad. This is the number 2 result right after the company website.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Codesmith is an enormous waste of money”&lt;/item&gt;
      &lt;item&gt;“Do Not Go To Codesmith”&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I don’t even have to click, Google is serving that stuff up right under the company’s website. God damn.&lt;/p&gt;
    &lt;p&gt;That top Reddit thread has been ranking highly for Codesmith brand terms since Sept 2024. That’s a year’s worth of brand carnage.&lt;/p&gt;
    &lt;p&gt;Pay close attention to the subreddit all those threads are from: r/codingbootcamp. That’s the key to all this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Are LLMs Also a Reputation Attack Vector on Codesmith?&lt;/head&gt;
    &lt;p&gt;Yup.&lt;/p&gt;
    &lt;p&gt;My prompt in ChatGPT was “is codesmith a good bootcamp?”.&lt;/p&gt;
    &lt;p&gt;The intro has a bunch of standard stuff, and then there was this:&lt;/p&gt;
    &lt;p&gt;Same Reddit threads are featured, same brand-destroying quotes.&lt;/p&gt;
    &lt;p&gt;By the way, CIRR is a nonprofit in the bootcamp industry that collects graduation data across the industry. It’s a neutral third-party with the goal of providing transparency on student outcomes across coding bootcamps. If your graduation rates are solid, wouldn’t it make sense for a competitor to call that data into question? ChatGPT is regurgitating those same doubts here.&lt;/p&gt;
    &lt;p&gt;At this point, I’ve done a quick brand search and asked ChatGPT if Codesmith is any good. I already have a TON of doubts as a prospect.&lt;/p&gt;
    &lt;p&gt;I don’t believe this is an accident. Or a result of Codesmith’s actual quality as a bootcamp.&lt;/p&gt;
    &lt;p&gt;I believe this brand attack is the result of one person’s actions.&lt;/p&gt;
    &lt;p&gt;When I’ve researched Codesmith’s brand, every Reddit thread I’ve seen in Google and various LLMs have all been from a single subreddit: r/codingbootcamp. That subreddit is controlled by the Reddit Moderator Michael Novati.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who is Michael Novati?&lt;/head&gt;
    &lt;p&gt;For the best recap of Michael, listen to this interview from Pragmatic Engineer Gergely Orosz.&lt;/p&gt;
    &lt;p&gt;The highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Michael joined Facebook early in 2009.&lt;/item&gt;
      &lt;item&gt;As he tells it, went through a bunch of promotions quickly and made it to Principal Software Engineer (E7).&lt;/item&gt;
      &lt;item&gt;Left Facebook in 2017.&lt;/item&gt;
      &lt;item&gt;In 2019, co-founded his dev bootcamp, Formation, with his wife, Sophie Novati. His wife is the CEO, Michael took the CTO title.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In that interview, Michael tells a story about playing Risk (the board game) with Mark Zuckerberg.&lt;/p&gt;
    &lt;p&gt;“The way we became friends, he [Zuckerberg] used to play Risk, the board game. I think it was once a week that people would get together. Like 4 or 5 people. He would show up often and I really liked Risk. I would always play Risk as a kid for whatever reason, I don’t even know why.&lt;/p&gt;
    &lt;p&gt;And one day, we’re down to the final three people. And it was me, him, and I was losing. Mark was in second place, the leader was taking over.&lt;/p&gt;
    &lt;p&gt;I did a really.. uhhh.. let’s say delicate, strategic maneuver. Where I made an alliance with him to share resources and go after the first place person. So he went all in on the first place person.&lt;/p&gt;
    &lt;p&gt;My turn’s next. I did not go all in against the first place person, I took over Mark’s remaining resources. He just dwindled going after the first place person. He accepted my friendship request [on Facebook] very shortly that evening.&lt;/p&gt;
    &lt;p&gt;It’s weird, I basically backstabbed him in the game, really bad. Blatantly to his face. But it’s the game, that’s what Risk is. It’s a strategy game. I think he appreciated the strategy I had. It made him feel more like he could trust me. Even though I backstabbed him, he knows where my strategic thinking is coming from.“&lt;/p&gt;
    &lt;p&gt;Now, is this just a funny anecdote about an early Facebook backstabbing Mark Zuckerberg and then becoming friends? Maybe.&lt;/p&gt;
    &lt;p&gt;For me, it tells me a lot about how Michael Novati plays competitive games. In my experience, people tend to run businesses the same way they compete at anything. Maybe Michael is the exception.&lt;/p&gt;
    &lt;p&gt;I do have to admit, it takes some real cojones to fuck over your boss that hard in a board game. Especially when that boss is Zuckerberg.&lt;/p&gt;
    &lt;p&gt;One last note: Formation raised a $4M seed round in 2021, led by Andreessen Horowitz.&lt;/p&gt;
    &lt;head rend="h2"&gt;Michael Novati’s Control Over the Coding Bootcamp Industry&lt;/head&gt;
    &lt;p&gt;Michael Novati’s power over the coding bootcamp industry comes from one place: being a moderator on the subreddit r/codingbootcamp.&lt;/p&gt;
    &lt;p&gt;He is, by far, the most active moderator for the subreddit. You’ll bump into him immediately just by visiting the subreddit:&lt;/p&gt;
    &lt;p&gt;What about the other mods?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;u/samabuna hasn’t posted in about 8 years.&lt;/item&gt;
      &lt;item&gt;u/dowcet/ is still active on Reddit but rarely posts in r/codingbootcamp. When I checked, I only found one comment in the preceeding 1-2 months.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So the subreddit is Michael Novati’s show. Full stop.&lt;/p&gt;
    &lt;p&gt;We have to remember that Reddit isn’t just Reddit anymore. The powers that be have decided that Reddit is infallible, a reliable set of training data for LLMs, and should be featured fucking everywhere.&lt;/p&gt;
    &lt;p&gt;Before we get to what Michael has actually done, let’s lay out the potential power for anyone that gets control of a key subreddit in their industry:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If the subreddit is greenlit by Google, threads in your subreddit can easily pop to the top of Google searches that are extremely valuable to your business. Brand terms, review terms, product categories, major how-to topics, all of it.&lt;/item&gt;
      &lt;item&gt;LLMs prioritize Reddit heavily. If you want to spin conspiracy theories, those conspiracies will start to become part of the zeitgeist as every LLM regurgitates them.&lt;/item&gt;
      &lt;item&gt;Skew the narrative for long enough and it’ll be impossible for ANY prospect to not stumble across your skewed narrative when they’re researching products or services.&lt;/item&gt;
      &lt;item&gt;You can delete posts and comments at will. Want to tip the narrative in your favor? Just delete some of the positive posts of your competitors. To cover your tracks, make up claims about a bot army that’s run by your competitor. You’re fighting the good fight and keeping the barbarians at bay! You don’t even need to actively post negative stuff (but you can if you want to dial up the torture). Just removing positive stuff skews the narrative nicely.&lt;/item&gt;
      &lt;item&gt;You can ban users at will. Got some troublesome competitors fighting back? Just delete the little shits.&lt;/item&gt;
      &lt;item&gt;You can pin posts in the subreddit and comments within posts. Great tactic for pushing a narrative when you really need to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What about recourse from your competitors? Can they do anything to stop you?&lt;/p&gt;
    &lt;p&gt;You can only lose your moderator slot in a few instances:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Reddit Admis get involved, actual Reddit employees. Stuff has to get pretty egregious for that.&lt;/item&gt;
      &lt;item&gt;Moderators further up the mod list can kick you out. But if you get on good terms with them, or they don’t care, you have free reign.&lt;/item&gt;
      &lt;item&gt;If there are other mods below you AND you go inactive, you can get kicked by them too. So don’t let any new mods in. And always stay active. Simple.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No one else across the industry can do a damn thing. You get to act with impunity.&lt;/p&gt;
    &lt;p&gt;To be clear, any moderator of an industry subreddit has this power. That’s not a subjective opinion, it’s a fact. That’s how our online platforms currently work.&lt;/p&gt;
    &lt;p&gt;Michael Novati has this power over the bootcamp industry. Did he wield it benevolently? If he did, I wouldn’t be writing this fucking post.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Michael Distorts Narratives on Reddit&lt;/head&gt;
    &lt;p&gt;At first, I was going to go through a bunch of Michael Novati’s claims and debunk them.&lt;/p&gt;
    &lt;p&gt;Then I realized I was missing the entire point.&lt;/p&gt;
    &lt;p&gt;This isn’t a normal debate where there are two sides of the truth and we need to sort through it to get to the real answer.&lt;/p&gt;
    &lt;p&gt;It’s about one side torching the truth with a firehose of chaos and distortion.&lt;/p&gt;
    &lt;p&gt;Prospects in the bootcamp space are evaluating whether or not to spend tens of thousands of dollars on a bootcamp. It’s the price of a goddamn car. It doesn’t take much to get someone to pause and redirect their purchase journey.&lt;/p&gt;
    &lt;p&gt;So let’s go through a few of the worst examples.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compare Your Competitor to a Sex Cult (Loosely for Deniability)&lt;/head&gt;
    &lt;p&gt;Here’s a fucked up one.&lt;/p&gt;
    &lt;p&gt;A Reddit user, with a first-time post, posts a major takedown of Codesmith, claiming to be a former employee. Tons of allegations about mismanagement, poor curriculum, and backroom dealing with CIRR. I personally think these allegations are bullshit but that’s not the most fucked up part in this thread.&lt;/p&gt;
    &lt;p&gt;u/rosiebeir joins the conversation and gives a measured take on their experience as a student in Codesmith:&lt;/p&gt;
    &lt;p&gt;Most of the comment is about different aspects of the Codesmith program that could be improved. It all feels genuine to me. Further down, they say this:&lt;/p&gt;
    &lt;p&gt;Wow, that’s a helluva endorsement.&lt;/p&gt;
    &lt;p&gt;This is where Michael jumps in:&lt;/p&gt;
    &lt;p&gt;If you’re not familiar with NXIVM (Michael got the spelling wrong), go watch The Vow on HBO. Short story: NXIVM was a sex cult disguised as a self-improvement group and the founder went to prison.&lt;/p&gt;
    &lt;p&gt;Did Michael just compare Codemith to a goddamn sex cult? Yes he did folks.&lt;/p&gt;
    &lt;p&gt;Let’s put aside the subject matter for a second. Michael’s use of rhetoric is a master class on how to destroy someone’s reputation without technically saying the horrible thing you’re actually saying.&lt;/p&gt;
    &lt;p&gt;Michael takes an incredible testimonial from a student (“Codesmith changed my life”), and associates that comment with the sort of things he hears from sex cults. He doesn’t technically call Codesmith a sex cult, he merely makes the association. He also wraps it with other positive comments to look more measured: “something I hear very often” and “I love that Codesmith changed your life.” But these aren’t good things, they’re red flags! Sex cults do the same thing! Someone could get taken advantage of!&lt;/p&gt;
    &lt;p&gt;Oh so innocent aren’t we Michael?&lt;/p&gt;
    &lt;p&gt;Now any time a bootcamp prospect wanders into this thread:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They see the genuine, positive review from a former student.&lt;/item&gt;
      &lt;item&gt;A supposed figure of authority (a Reddit moderator), plants a seed questioning that testimonial.&lt;/item&gt;
      &lt;item&gt;The same mod connects Codesmith to a sex cult in a way that also offers complete deniability.&lt;/item&gt;
      &lt;item&gt;To top it all off, most folks won’t realize that the Reddit mod is a cofounder of a competing company.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bravo Michael, brav-fucking-o.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Reddit Troll to LinkedIn Stalker: The Unhinged Escalation&lt;/head&gt;
    &lt;p&gt;This one might piss me off the most.&lt;/p&gt;
    &lt;p&gt;Michael doesn’t just go after employees, he goes after their kids.&lt;/p&gt;
    &lt;p&gt;This is a Reddit post about a defunct Codesmith program:&lt;/p&gt;
    &lt;p&gt;Somehow, as these threads often do, the conversation turns into an all-out attack on Codesmith. This time, Michael goes after Eric Kirsten, a Senior Advisor at Codesmith. Eric spends a good chunk of his time mentoring Codesmith students and helping prepare them for the job hunt.&lt;/p&gt;
    &lt;p&gt;Here’s Michael’s post:&lt;/p&gt;
    &lt;p&gt;I do not know what the original comment in this thread was (very convenient). But I do have the original comment from Michael before he edited it:&lt;/p&gt;
    &lt;p&gt;I talked with Eric myself, here’s the real story:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Eric’s wife did a one-time contract project with Codesmith for 9 months.&lt;/item&gt;
      &lt;item&gt;His son joined as a Codesmith student and had a great outcome. They paid the full program cost and his son went through the normal application process just like everyone else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But it doesn’t stop there.&lt;/p&gt;
    &lt;p&gt;Michael also decided to email multiple executives at Codesmith about Eric’s son:&lt;/p&gt;
    &lt;p&gt;I’ve redacted the son’s name and details because WHY THE FUCK WOULD WE BRING SOMEONE’S KID INTO THIS?&lt;/p&gt;
    &lt;p&gt;I don’t think I could possibly swear enough in order to convey how utterly insane all this is.&lt;/p&gt;
    &lt;p&gt;Just imagine this happening at your own company:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A cofounder of your competitor, starts attacking one of your employees on Reddit.&lt;/item&gt;
      &lt;item&gt;Then that cofounder starts LOOKING UP THEIR KIDS ON LINKEDIN&lt;/item&gt;
      &lt;item&gt;They post weird mentions on Reddit about that kid, accusing your company of nepotism.&lt;/item&gt;
      &lt;item&gt;THEN EMAILS you and other executives, accusing the kid of falsifying their LinkedIn, threatening to keep calling it out in public.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;WHAT.&lt;/p&gt;
    &lt;p&gt;WHAT. THE. FUCK.&lt;/p&gt;
    &lt;p&gt;What would you do in this situation?&lt;/p&gt;
    &lt;p&gt;I’m not sure what I would do. But there is no world where this is normal or okay.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Great OSP Conspiracy for Manufactured Outrage&lt;/head&gt;
    &lt;p&gt;Another favorite tactic of Michael is to take something obscure, and allude to some grand conspiracy behind it.&lt;/p&gt;
    &lt;p&gt;Like OSPs.&lt;/p&gt;
    &lt;p&gt;OSP stands for open-source product. It’s a built-in part of the Codesmith program, the capstone project. Here’s how Codesmith explains it:&lt;/p&gt;
    &lt;p&gt;This seems incredibly boring, how the hell could someone possibly turn this into a controversy?&lt;/p&gt;
    &lt;p&gt;If you’re asking that question, you’ve severely underestimated Michael.&lt;/p&gt;
    &lt;p&gt;A since-deleted Reddit account posted this thread (convenient that it’s a deleted user huh?):&lt;/p&gt;
    &lt;p&gt;They claim Codesmith students are using OSPs to inflate their resumes to get jobs faster. And that this is endemic at Codesmith.&lt;/p&gt;
    &lt;p&gt;The comments are a dumpster fire. Michael has posted 11 of the comments, almost a third of the entire conversation.&lt;/p&gt;
    &lt;p&gt;A former Codesmith student jumps in:&lt;/p&gt;
    &lt;p&gt;There’s some back an forth between u/peppimenti and other Reddit users, then Michael responds:&lt;/p&gt;
    &lt;p&gt;Fuck me, that’s a lot to take in.&lt;/p&gt;
    &lt;p&gt;Quick tangent: this is another one of Michael’s tactics. Make accusations faster than anyone can possibly fact check them or even process them. I still haven’t figured out what that Phil Troutman reference is.&lt;/p&gt;
    &lt;p&gt;To summarize, Michael claims he has evidence that Codesmith is aware of the lying on resumes, alludes to a conspiracy with OSLabs, and he believes Codesmith helped verify some of this lying.&lt;/p&gt;
    &lt;p&gt;Then he associates all of this with “conspiring to commit fraud” and that it’s a “jail-able crime.”&lt;/p&gt;
    &lt;p&gt;We’ve gone from “Codesmith students complete an OSP during their program” to a shadowy conspiracy and fraud via student resumes. God damn did that escalate.&lt;/p&gt;
    &lt;p&gt;So I got a hold of the exact SOP that is given by Codesmith to students on how to write their resume. Here’s the section on OSPs:&lt;/p&gt;
    &lt;p&gt;Codesmith is telling students explicitly not to misrepresent their experience with OSPs as a role at a company. They advocate for transparency, as they should. What students do with that is up to them. And even if some students stretch the truth, no hiring manager is getting fooled by that. End of story.&lt;/p&gt;
    &lt;p&gt;What about the OSLabs stuff?&lt;/p&gt;
    &lt;p&gt;When I chatted with Alina (the current CEO of Codesmith), here’s what she had to say on how Codesmith students interact with OSLabs:&lt;/p&gt;
    &lt;p&gt;“They [students] get a chance to work on it [projects]. Once those are ready, they submit to OSLabs who keeps the repository and manages the repository. There’s no kind of financial exchange or anything like that. They just hold the repository of the open source dev tools.“&lt;/p&gt;
    &lt;p&gt;OSLabs is just a nonprofit that manages student repos. That’s it.&lt;/p&gt;
    &lt;p&gt;Maybe you think there are problems with Codesmith’s guidance. Real quick, why don’t we check some of Michael’s own students from his bootcamp company, Formation?&lt;/p&gt;
    &lt;p&gt;There’s tons of students on his site. Some of them get featured heavily, like here:&lt;/p&gt;
    &lt;p&gt;Here’s Carlitos’ LinkedIn:&lt;/p&gt;
    &lt;p&gt;Carlitos is listing his time at Michael’s bootcamp/training company in his experience section. The exact same thing that Michael RAILS on Codesmith for.&lt;/p&gt;
    &lt;p&gt;To be clear: I don’t think there’s anything wrong with this. Carlitos, all the Codesmith students, or ANY student from ANY bootcamp have done nothing wrong by putting stuff like this on their resumes. If Formation students list their time like this, I don’t see anything wrong with that either.&lt;/p&gt;
    &lt;p&gt;In the thousands of resumes I’ve reviewed, I’ve never had any trouble differentiating between full-time roles, part-time, freelance, training, or any other random item on a resume. Any hiring manager will sniff this stuff from a mile away. No one is fooling anyone.&lt;/p&gt;
    &lt;p&gt;But Michael has now turned this into a story of fraud facilitated by Codesmith. And here I am writing hundreds of words about this nonsense. There’s nothing here beyond what Michael has conjured into the ether of Reddit.&lt;/p&gt;
    &lt;p&gt;You might disagree with some of the guidance that Codesmith gives its graduates but no one is doing anything wrong. It sure as fuck aint fraud.&lt;/p&gt;
    &lt;p&gt;And that’s what Michael is so damn good at, turning innocuous little things into grand conspiracies that take hours of research to untangle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Michael Novati’s Nonstop Attack Campaign Against Codesmith&lt;/head&gt;
    &lt;p&gt;Think I’m cherry picking examples of Michael’s posts?&lt;/p&gt;
    &lt;p&gt;Let’s zoom out.&lt;/p&gt;
    &lt;p&gt;My team and I looked at every Reddit post and comment made by Michael Novati over the past 3 months.&lt;/p&gt;
    &lt;p&gt;We then catalogued the posts that mentioned Codesmith or were referencing Codesmith based on context. Next we categorized those posts based on if the sentiment was positive, neutral, or negative towards Codesmith.&lt;/p&gt;
    &lt;p&gt;Here’s all the days that had negative comments along with their frequency over the past few months:&lt;/p&gt;
    &lt;p&gt;Fucking relentless.&lt;/p&gt;
    &lt;p&gt;It’s easier to highlight days where Michael DOESN’T talk about Codesmith. Dude, get a fucking hobby.&lt;/p&gt;
    &lt;p&gt;What if the past few months don’t adequately reflect Michael’s posting history? What if recent events have just been a negative phase of the bootcamp space? Can we really make any claims using just the last few months?&lt;/p&gt;
    &lt;p&gt;Fine, let’s go further.&lt;/p&gt;
    &lt;p&gt;I went to my team and said “fuck it, let’s go back as far as possible.” So we went through all of Michael’s posts since June 2024. That’s when Reddit stops making it easy to pull up comments. Yes, we looked at every post and comment from over a year. No fast and dirty AI analysis here, we did it by hand.&lt;/p&gt;
    &lt;p&gt;Here’s the timeline of negative posts and comments since June 2024:&lt;/p&gt;
    &lt;p&gt;What. The. Fuck.&lt;/p&gt;
    &lt;p&gt;Who has the time for this?&lt;/p&gt;
    &lt;p&gt;In order to adequately show how often Michael posts negative comments about Codesmith, we had to use HABIT TRACKER VISUALIZATIONS.&lt;/p&gt;
    &lt;p&gt;It’s relentless, it’s nonstop, and in my opinion, completely unhinged. At least Michael slows down a tad during the holidays. Otherwise, he’s all gas, no breaks.&lt;/p&gt;
    &lt;p&gt;Well, alright, MAYBE Michael was posting in such absurd volume that the positive mentions outweigh the negative? Maybe I’m selectively pulling data?&lt;/p&gt;
    &lt;p&gt;NOPE.&lt;/p&gt;
    &lt;p&gt;The comments about Codesmith are severely overweighted on the negative side. In aggregate, here’s how the sentiment breaks down:&lt;/p&gt;
    &lt;p&gt;Is it just me or is there a weeeee bit of bias in this posting?&lt;/p&gt;
    &lt;p&gt;Yes, yes, there fucking is.&lt;/p&gt;
    &lt;p&gt;Alright, fine, MAYBE all this negative attention is deserved. Maybe Codesmith is the villain that Michael claims. Let’s find out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other Bootcamp Competitors Hold Codesmith in High Regard&lt;/head&gt;
    &lt;p&gt;I interviewed 10 different folks. Including students, multiple Codesmith employees from the leadership team, and founders of competing coding bootcamps. I dug through countless docs, many of them internal, and probably annoyed some of these folks with my endless requests for screenshots to verify stories.&lt;/p&gt;
    &lt;p&gt;Overall, I found everyone at Codesmith to be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extremely responsive and on top of their shit. They made promises and kept them.&lt;/item&gt;
      &lt;item&gt;Transparent to an absurd degree.&lt;/item&gt;
      &lt;item&gt;Humble, honest, and self-aware.&lt;/item&gt;
      &lt;item&gt;A desire to do the right thing even at their detriment. These folks don’t know how to play offense, they assume good intent from everyone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In short, these are good folks. Faultless? Of course not, everyone makes mistakes.&lt;/p&gt;
    &lt;p&gt;But I do believe that the world is better with Codesmith in it.&lt;/p&gt;
    &lt;p&gt;Which makes this whole situation all the more tragic.&lt;/p&gt;
    &lt;p&gt;Michael isn’t waging some war against another competitor hellbent on profit-seeking. These are folks that are helping people transition their careers. They are quite literally helping people change their lives. And they’re doing it for the right reasons in my opinion.&lt;/p&gt;
    &lt;p&gt;Don’t take my word for it.&lt;/p&gt;
    &lt;p&gt;I spoke with Anthony Hughes, a co-founder of Tech Elevator, another bootcamp. Here’s what he had to say about Codesmith: “The folks at Codesmith were competitors of Tech Elevator. Especially as everyone went online, we competed directly. I never minded losing to Codesmith. Either choice by students was a good one, both our programs vetted students heavily and would get students great outcomes. If we ever lost to Codesmith, we could accept that because we knew the student would get the outcome they wanted. Not like the lower-quality major programs.”&lt;/p&gt;
    &lt;p&gt;I also heard the same from Kush Patel, a co-founder and former CEO of AppAcademy “Codesmith is one of the best players in the space. They run their programs in a way that we benchmarked against. There are plenty of other schools that are questionable and are not getting any scrutiny from Michael.”&lt;/p&gt;
    &lt;p&gt;We have two co-founders of competing bootcamps saying that Codesmith is one of the good ones. Are there shady bootcamps out there? Absolutely. But Codesmith isn’t one of them.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Tactics Michael Uses to Wage His PR Nightmare Campaign&lt;/head&gt;
    &lt;p&gt;After reading hundreds of Michael’s posts, a few patterns emerge on how he attacks Codesmith.&lt;/p&gt;
    &lt;p&gt;To be honest, it’s a masterclass on how to gut your opponent via PR. If every move is intentional by Michael, I gotta hand it to him becasue he’s exceptionally good at this.&lt;/p&gt;
    &lt;p&gt;Here are the most prominent tactics I saw:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Relentless volume. It’s not enough to post a few days. If you want to destroy someone’s reputation, you gotta hammer the shit outta that drum. Over a 487 day period, we counted 425 negative posts on Codesmith. That’s an average of 0.87 negative posts per day. A negative post almost every fucking day.&lt;/item&gt;
      &lt;item&gt;Reference multiple conspiracies at the same time. If you only mention one conspiracy, folks have a straightforward path to getting to the truth. But if you mention 3-4 at the same time? Only the true masochistics like myself will even attempt to pull it apart.&lt;/item&gt;
      &lt;item&gt;Rarely, if ever, state your full accusation. For any single conspiracy, always allude to it. Don’t make a full, comprehensive case that allows someone to see the full picture. Let your audience sit with the uncertainty. You don’t need to win the argument, you only need to sow doubt and fear.&lt;/item&gt;
      &lt;item&gt;Cherry-pick data. As the saying goes: “there are lies, damned lies, and statistics.” For any industry data you can get your hands on, find the slice that skews the narrative in your favor. Don’t worry about anyone fact-checking, they never do.&lt;/item&gt;
      &lt;item&gt;Attack every misstep, no matter how obscure. Sooner or later, your victim will make mistakes. When they do, attack. No matter how small or obscure, blow it up into a major drama. Hype, distort, and magnify.&lt;/item&gt;
      &lt;item&gt;Take down third-party arbitrators. If your industry has some sort of credential, certification, or regulator, attack that body just as much as your competitor. If your competitor has optimized for this transparency, you’ll turn their strength into a weakness. Even better, accuse both parties of conspiring together.&lt;/item&gt;
      &lt;item&gt;Blame the victim. Whenever someone mentions that you’re going too hard, say that your competitor is forcing your hand. “You’d love to move on! You don’t want to spend your time on all this! If ONLY your competitor would do the right thing, then all this would stop!” But we all know it’ll never stop.&lt;/item&gt;
      &lt;item&gt;Delete comments and posts. Delete comments and posts to skew the narrative in your favor. You don’t need to delete everything, just enough to make the conversation messy and difficult to follow. And a few key deletions tilts the scales in your favor.&lt;/item&gt;
      &lt;item&gt;Make your claims squishy. Don’t ever say your competitor is a cult or committing fraud. Say that things look like it, or remind you of it. You’ll have deniability while the association still gets planted in the minds of your audience.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A sign of the times.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Impact on Codesmith&lt;/head&gt;
    &lt;p&gt;Has all this had a material impact on Codesmith?&lt;/p&gt;
    &lt;p&gt;I spoke to both Will Sentence (former CEO and co-founder) and Alina Vasile (the current CEO). When I asked them how much revenue has declined at Codesmith since their peak, they both told me about 80%.&lt;/p&gt;
    &lt;p&gt;Then I asked how much of that was due to everything on Reddit. They said about half of the decline, so a 40% drop came from the negative PR on Reddit. And the other 40% is from a slowdown in the bootcamp market.&lt;/p&gt;
    &lt;p&gt;Will told me that Codesmith reached a high of $23.5M in revenue. A 40% hit from Reddit means a revenue drop of $9.4M.&lt;/p&gt;
    &lt;p&gt;What about the emotional toll?&lt;/p&gt;
    &lt;p&gt;I asked Will Sentance how it all impacted him “It made me doubt how I can start anything after. Michael Novati is still going at it by commenting on my fellowship at Oxford. And Reddit is so visible in Google – at a meeting last week in the UK, the person used ChatGPT to look up Codesmith and it referenced all those Reddit posts. I have genuine caution about launching new stuff.”&lt;/p&gt;
    &lt;p&gt;Even after leaving Codesmith, the Reddit harassment still impacts Will.&lt;/p&gt;
    &lt;p&gt;Students also feel deeply uncomfortable with Michael’s actions.&lt;/p&gt;
    &lt;p&gt;I spoke with a former employee from Codesmith who told me “I was a teacher at Codesmith and the primary reason for moving away from teaching was MIchael Novati. I signed up to help a team of great people help people achieve their goals like I did, I didn’t sign up to be targeted and attacked by a Reddit troll. It took a toll on my mental health and I decided to step back to focus on my day job at Microsoft.”&lt;/p&gt;
    &lt;p&gt;Here’s another instance of a student calling Michael out on LinkedIn:&lt;/p&gt;
    &lt;p&gt;And another student that reached out to Will Sentence:&lt;/p&gt;
    &lt;p&gt;My sources also told me:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An executive was threatened after moving to another company, accusations were going to be raised over IP conflicts. These accusations appear meritless and ridiculous to me.&lt;/item&gt;
      &lt;item&gt;The morale at Codesmith has sharply declined.&lt;/item&gt;
      &lt;item&gt;Some employees have had mental health difficulties, some left Codesmith.&lt;/item&gt;
      &lt;item&gt;One contractor ended their working relationship with Codesmith because of a fear of being dragged into all this.&lt;/item&gt;
      &lt;item&gt;An employee was doxxed after posting in the /r/codingbootcamp subreddit.&lt;/item&gt;
      &lt;item&gt;Prospective students have pulled applications.&lt;/item&gt;
      &lt;item&gt;Students that had amazing outcomes are afraid to post about their experiences.&lt;/item&gt;
      &lt;item&gt;After graduating, some students began questioning their own experience and considered going back to their old careers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As much as I wanted to detail these stories, I had a lot of requests from sources to remain anonymous. Even when folks went through some heinous shit from all this, they specifically requested not to be included.&lt;/p&gt;
    &lt;p&gt;It is difficult to understate the impact that Michael’s actions have had on Codesmith.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is Michael Novati Breaking the Moderator Code of Conduct on Reddit?&lt;/head&gt;
    &lt;p&gt;I think so. But it’s not my call.&lt;/p&gt;
    &lt;p&gt;The Reddit Moderator Code of Conduct says this:&lt;/p&gt;
    &lt;p&gt;So what counts as compensation?&lt;/p&gt;
    &lt;p&gt;Basically, Reddit Moderators can’t receive compensation for doing basic moderator tasks. Good rule, the point is to prevent companies and third-parties from paying Reddit mods off.&lt;/p&gt;
    &lt;p&gt;Now here’s the tricky part.&lt;/p&gt;
    &lt;p&gt;What if that Reddit Moderator is a co-founder of a competitor? What if they spend YEARS shit talking you and destroying your reputation? What if they’re not technically receiving new stock but the value in their startup equity goes up after they destroy you on Reddit? Doesn’t that count as financial compensation and a violation of Reddit’s own rule to “Moderate with Integrity”?&lt;/p&gt;
    &lt;p&gt;I believe it does.&lt;/p&gt;
    &lt;p&gt;And I believe that’s why Michael is doing what he’s doing. He wins when Codesmith loses.&lt;/p&gt;
    &lt;p&gt;Shit like this is why Reddit has become extremely untrustworthy.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Drives the Crazy Student Outcomes at Codesmith&lt;/head&gt;
    &lt;p&gt;If you dig around, you will find stories from Codesmith students and placement data that seem too good to be true. Is Codesmith actually that good?&lt;/p&gt;
    &lt;p&gt;As far as I can tell, it’s all real.&lt;/p&gt;
    &lt;p&gt;So how has Codesmith managed it? I found two forces that have driven these outcomes.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Codesmith Has an Intense Application Process&lt;/head&gt;
    &lt;p&gt;Not just anyone can join Codesmith. They screen folks OUT of their program. If you’re a line cook, kinda think a software engineer job sounds fun, have never spent even 5 minutes figuring out what that looks like, and apply, you won’t get in.&lt;/p&gt;
    &lt;p&gt;Codesmith only accepts folks that they think have a good chance of succeeding.&lt;/p&gt;
    &lt;p&gt;They filter the front of their student pipeline which keeps the outcome rates really high on the backend. That’s how they get their placement numbers as high as they are. That’s the “trick” to juicing the student outcomes.&lt;/p&gt;
    &lt;p&gt;Now, if you have some software engineering experience, you’ll likely find the application to be absurdly easy. That’s the point.&lt;/p&gt;
    &lt;p&gt;To me, Codesmith isn’t a true zero to one program. The students that thrive have already put a lot of self-study hours into their career transition. They’re motivated, have learned the basics, and are ready to dial up the intensity. These are folks that are also likely to succeed during the job application process after they graduate.&lt;/p&gt;
    &lt;p&gt;This is also one of the easiest ways to spot the shitty bootcamps. If they take anyone and everyone, that’s an enormous red flag. Stay away.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. The Hiring Market Was Extremely Strong from 2019-2022&lt;/head&gt;
    &lt;p&gt;I spoke to one student that went through Codesmith in 2020, they reported that every single one of the folks in their cohort landed a software engineering job within about 6 months. That’s nuts.&lt;/p&gt;
    &lt;p&gt;Another graduate from the 2019 era told me that 80-90% of the students from their cohort, the cohort before, and the cohort after, landed software engineering jobs within a few months of graduating.&lt;/p&gt;
    &lt;p&gt;How were the placements that high? I believe the Codesmith program had something to do with it. But that was also a period where the tech market was exceptionally hot. Across all levels and functions, it was much easier to switch jobs, get promotions, and change careers in tech. I saw that myself.&lt;/p&gt;
    &lt;p&gt;I’ve also personally seen things tighten a lot in the past few years.&lt;/p&gt;
    &lt;p&gt;How do Codesmiths placement rates look now?&lt;/p&gt;
    &lt;p&gt;Even if we look at more recent cohorts, the placements are still pretty damn strong for the full-time program:&lt;/p&gt;
    &lt;p&gt;That’s one of the reports issued by CIRR on Codesmith. It’s the full report so I’m not cherry picking.&lt;/p&gt;
    &lt;p&gt;Even during 2023-2024, a pretty shitty period to try to start a career as a software engineer, 70% of folks are still landing in-field jobs within a year of graduating Codesmith. With a median salary of $110,000. Even if you only count full-time employment, it’s still 62%. I personally think that’s remarkable.&lt;/p&gt;
    &lt;p&gt;The part-time program is only slightly worse at 60%.&lt;/p&gt;
    &lt;p&gt;Again, this is during a hiring market when entry-level software engineering jobs are brutally hard to get. And the placement rates are still in the 60-70% range.&lt;/p&gt;
    &lt;p&gt;I’m also hearing that students have to work a lot harder now to land that job. And it can take a solid year. That’s that state of tech hiring at the moment.&lt;/p&gt;
    &lt;p&gt;So if you see old placement data of 80-100%, that was when the market was hot. Now it’s come back down to earth which is to be expected.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future of Codesmith&lt;/head&gt;
    &lt;p&gt;To me the future of Codesmith looks quite bright. Alina (the CEO) is very much in control, the team is excited about the future, and the student outcomes still look impressive to me. I’ve been told that there are no plans at Codesmith to shut down.&lt;/p&gt;
    &lt;p&gt;Especially with all the changes to software development from AI, there’s a lot of training that everyone is going to have to go through in tech. Good time to be in the education space.&lt;/p&gt;
    &lt;p&gt;If Michael could take his boot off Codesmith’s throat for just half a minute, they’d do a lot of good for a lot people.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Recommendations for Anyone Considering Joining a Developer Bootcamp&lt;/head&gt;
    &lt;p&gt;I cannot tell you which bootcamp to attend. Or whether a bootcamp is worth it at all.&lt;/p&gt;
    &lt;p&gt;I’ve hired well over a hundred folks in my career, interviewed countless others. Including engineers and data scientists.&lt;/p&gt;
    &lt;p&gt;At no point did I ever place any value in a bootcamp. I also never considered it a detractor. It’s neutral to me, never really signaling anything. Neither good nor bad.&lt;/p&gt;
    &lt;p&gt;If you think it’ll get you your first job (which is all that really matters), and the price/time makes sense for you, go for it. But there are plenty of other paths. It’s not required by any means. I know plenty of self-taught devs that have worked at prestigious startups and big tech.&lt;/p&gt;
    &lt;p&gt;As for which bootcamp, that’s also up to you.&lt;/p&gt;
    &lt;p&gt;What I can say is this.&lt;/p&gt;
    &lt;p&gt;As I’ve gone through Michael Novati’s Reddit account, I’ve been appalled. I’ve never seen anything like this in my entire career. And I’ve played in some fucked up corners of the internet. That last thing I’d EVER want is to have my name or career associated with anything that Michael Novati has touched. That includes his startup, Formation.&lt;/p&gt;
    &lt;p&gt;You’ll have to make your own choice. But I know what mine would be.&lt;/p&gt;
    &lt;p&gt;At the very least, when doing research on bootcamps, I would consider anything on r/codingbootcamp/ to be completely contaminated. Whatever the real truth is, that subreddit is Michael’s personal fiefdom. He can do and say whatever he wants. And he has one helluva conflict of interest: he’s a co-founder and CTO of a leading bootcamp (even if he tries to claim that he doesn’t run a bootcamp).&lt;/p&gt;
    &lt;p&gt;If I was looking for unbiased reviews, I’d only factor in content from a community that Michael (or any other bootcamp founder) did not control. Make sure you know who your subreddit Mods are.&lt;/p&gt;
    &lt;head rend="h2"&gt;Every Company is Now Vulnerable to These Types of Reputation Attacks&lt;/head&gt;
    &lt;p&gt;For me, the craziest part of this story is that it’s not an edge case.&lt;/p&gt;
    &lt;p&gt;Anyone can use this exact same method to tear down their competitors.&lt;/p&gt;
    &lt;p&gt;You only need to complete one step: become a Reddit mod in one of the main subreddits for your industry. If you do that and the original mods of the subreddit don’t care what you do, you can completely fuck up your whole industry. Sow lies, fear, and misery at will.&lt;/p&gt;
    &lt;p&gt;Which means anyone can do this to you.&lt;/p&gt;
    &lt;p&gt;Reddit has become THE reputation attack vector.&lt;/p&gt;
    &lt;p&gt;And since Reddit is considered the default source of all human knowledge now, Reddit threads pop to the top of Google. Anyone searching for your brand will stumble into this madness. LLMs also prioritize Reddit so that narrative gets corrupted too. Getting control of one channel allows you to corrupt three major channels all at once. Most people don’t check receipts, they don’t dig for truth, they accept what’s put in front of them.&lt;/p&gt;
    &lt;p&gt;Reddit mods now control our search information ecosystem. Unpaid, corruptible Reddit mods.&lt;/p&gt;
    &lt;p&gt;I know there are good Reddit mods out there. But all it takes is one to fuck up your own business.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://larslofgren.com/codesmith-reddit-reputation-attack/"/><published>2025-10-08T23:48:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45522406</id><title>Designing a Low Latency 10G Ethernet Core (2023)</title><updated>2025-10-09T13:03:14.235076+00:00</updated><content>&lt;doc fingerprint="ed38fb1129f6f98f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Designing a Low Latency 10G Ethernet Core - Part 1 (Introduction)&lt;/head&gt;
    &lt;p&gt;Links to the other parts in this series:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Introduction (this post)&lt;/item&gt;
      &lt;item&gt;Design Overview and Verification&lt;/item&gt;
      &lt;item&gt;Low Latency Techniques&lt;/item&gt;
      &lt;item&gt;Performance Measurement and Comparison&lt;/item&gt;
      &lt;item&gt;Potential Improvements&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;This is the first in a series of blog posts describing my experience developing a low latency 10G Ethernet core for FPGA. I decided to do this as a personal project to develop expertise in low latency FPGA design and high-speed Ethernet, as well as to experiment with tools and techniques that I could use full-time. As a small spoiler, the design has less than 60ns loopback latency, which is comparable to commercial offerings.&lt;/p&gt;
    &lt;p&gt;These posts will focus on the things that are likely different to a ‘standard’ design, as I believe this will be more interesting to the reader. Specifically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The use of cocotb and pyuvm for verification&lt;/item&gt;
      &lt;item&gt;The techniques implemented to reduce packet processing latency&lt;/item&gt;
      &lt;item&gt;Analysis of commercially available low latency and ‘ultra’-low latency cores&lt;/item&gt;
      &lt;item&gt;Latency measurement results and comparison&lt;/item&gt;
      &lt;item&gt;Other techniques not implemented&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the reader is unfamiliar with Layer 1/2 Ethernet, I would recommend the following resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10G Ethernet Layer 1 Overview&lt;/item&gt;
      &lt;item&gt;YouTube - The Big MAC Mystery&lt;/item&gt;
      &lt;item&gt;IEEE Standard for Ethernet - Full Ethernet (802.3) spec&lt;/item&gt;
      &lt;item&gt;64B/66B overview - Overview of 10G PCS from the spec above&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Next Post - Design Overview and Verification&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ttchisholm.github.io/ethernet/2023/05/01/designing-10g-eth-1.html"/><published>2025-10-09T01:17:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45523537</id><title>Two things LLM coding agents are still bad at</title><updated>2025-10-09T13:03:13.766255+00:00</updated><content>&lt;doc fingerprint="be40934db2d75e59"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Two things LLM coding agents are still bad at&lt;/head&gt;
    &lt;p&gt;I’ve been trying to slowly ease into using LLMs for coding help again lately (after quitting cold turkey), but something always feels off -- like we’re not quite on the same wavelength. Call it vibe coding or vibe engineering, but I think I’ve finally pinned down two big reasons why their approach to code feels so awkward.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LLMs don’t copy-paste (or cut and paste) code. For instance, when you ask them to refactor a big file into smaller ones, they’ll "remember" a block or slice of code, use a &lt;code&gt;delete&lt;/code&gt;tool on the old file, and then a&lt;code&gt;write&lt;/code&gt;tool to spit out the extracted code from memory. There are no real&lt;code&gt;cut&lt;/code&gt;or&lt;code&gt;paste&lt;/code&gt;tools. Every tweak is just them emitting&lt;code&gt;write&lt;/code&gt;commands from memory. This feels weird because, as humans, we lean on copy-paste all the time. It’s how we know the code we moved is exactly the same as where we copied it from. I've only seen Codex go against the grain here, sometimes I'd see it issue&lt;code&gt;sed&lt;/code&gt;and&lt;code&gt;awk&lt;/code&gt;to try and replicate that copy-paste interaction, but it doesn't always work.&lt;/item&gt;
      &lt;item&gt;And it’s not just how they handle code movement -- their whole approach to problem-solving feels alien too. LLMs are terrible at asking questions. They just make a bunch of assumptions and brute-force something based on those guesses. Good human developers always pause to ask before making big changes or when they’re unsure (hence the mantra of "there are no bad questions"). But LLMs? They keep trying to make it work until they hit a wall -- and then they just keep banging their head against it. Sure, you can overengineer your prompt to try get them to ask more questions (Roo for example, does a decent job at this) -- but it's very likely they still won't. Maybe the companies building these LLMs do their RL based on making writing code "faster".&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These quirks are why I contest the idea that LLMs are replacing human devs -- they’re still more like weird, overconfident interns. I can’t fully vibe with them yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kix.dev/two-things-llm-coding-agents-are-still-bad-at/"/><published>2025-10-09T04:33:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45524293</id><title>The Unknotting Number Is Not Additive</title><updated>2025-10-09T13:03:13.623794+00:00</updated><content>&lt;doc fingerprint="2c790b511bb3d39a"&gt;
  &lt;main&gt;
    &lt;p&gt;On June 30, 2025, Mark Brittenham and Susan Hermiller uploaded a preprint to the arXiv called “Unknotting number is not additive under connected sum” (and an updated version on September 15, 2025). In it, they surprised the mathematical community by giving a counterexample to a long-standing conjecture in knot theory. The story was picked up by publications like Scientific American and Quanta and by math YouTuber Matt Parker.&lt;/p&gt;
    &lt;p&gt;The conjecture is easy to understand, although we need a few definitions first.&lt;/p&gt;
    &lt;p&gt;(Mathematical) knot: We can think of a mathematical knot as a loop of string sitting in three-dimensional space. In other words, if we took a piece of string, tied a knot, and then glued the two ends together, we’d get a mathematical knot.&lt;/p&gt;
    &lt;p&gt;Knot projection: Given any mathematical knot, we draw a two-dimensional version of it in the plane. It is like the shadow of the knot, but with breaks in the knot to indicate which strand is on top and which is on the bottom.&lt;/p&gt;
    &lt;p&gt;Unknotting number: If we have the projection of a knot, we can change some crossings (change which strand is over and which is under) to make it unknotted (called the unknot). To compute the unknotting number of a knot K, u(K), we look at all possible projections and find the fewest number of crossing changes we must make to obtain the unknot.&lt;/p&gt;
    &lt;p&gt;Connected sum: Given two knots, J and K, we can cut each knot at one point and join the cut ends to form a new, larger knot, J#K. This is called the connected sum of the knots.&lt;/p&gt;
    &lt;p&gt;Below, we see a knot called the (2,7) torus knot and its mirror image (left), and their connected sum on the right.&lt;/p&gt;
    &lt;p&gt;Although unknotting numbers are notoriously difficult to compute, we know that the unknotting number of a (p,q) torus knot is (p-1)(q-1)/2. So, the (2,7) torus knots above have unknotting number (2 – 1)(7 – 1)/2 = 3. It is not difficult to check that by changing three crossings of the projections shown above, the (2,7) torus knot becomes the unknot. It turns out that changing two crossings does not suffice in this projection or any projection.&lt;/p&gt;
    &lt;p&gt;Likewise, changing 3 + 3 = 6 crossings of the connected sum will yield the unknot. In fact, it will always be the case that u(J#K) ≤ u(J) + u(K). The question is: are these equal? An “old” conjecture (which was implicit in an article 88 years ago), states:&lt;/p&gt;
    &lt;p&gt;Conjecture: If J and K are knots, then u(J#K) = u(J) + u(K).&lt;/p&gt;
    &lt;p&gt;In their preprint, Brittenham and Hermiller disprove the conjecture by giving a counterexample! Moreover, the counterexample is precisely the one I’ve shown above! The connected sum of the torus knot with its mirror image has unknotting number 5, which is clearly less than 3 + 3.&lt;/p&gt;
    &lt;p&gt;That said, we can’t simply change the five crossings in the above projection to obtain the unknot. We must produce a different projection first. But what is it?&lt;/p&gt;
    &lt;p&gt;I looked for the answer online and found this arXiv preprint by Chao Wang and Yimu Zhang, which gives the details. They provide the projection and the crossings that must be changed. However, the projection has 56 crossings (far more than the original 14!). They assert, but do not show, that the resulting knot—after the five crossings are changed—is the unknot. They end by writing, “We prefer to leave it to the readers as an interesting game.”&lt;/p&gt;
    &lt;p&gt;Never good at resisting a good nerd sniping, I decided to take them up on the challenge. I’m embarrassed to admit how long it took me to confirm their work, but I did it. Here is my redrawn version of the knot projection—the connected sum of the (2,7) torus knot and its mirror image. The circled crossings are the five that must be changed.&lt;/p&gt;
    &lt;p&gt;After having done so, here’s the resulting projection. Wang and Zhang claim that it is the unknot!&lt;/p&gt;
    &lt;p&gt;Without further ado, here are my drawings. This first sequence shows how I get from the usual projection of the connected sum to the projection in which the crossings must be made. The red strands show the part of the projection that has changed from the previous projection.&lt;/p&gt;
    &lt;p&gt;Thus, the final image above is the knot we claim is the unknot. The following sequence of steps shows that it is indeed the unknot.&lt;/p&gt;
    &lt;p&gt;Ta da!!!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://divisbyzero.com/2025/10/08/the-unknotting-number-is-not-additive/"/><published>2025-10-09T06:39:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45524437</id><title>The Forecasting Company (YC S24) Is Hiring a Machine Learning Engineer</title><updated>2025-10-09T13:03:13.129232+00:00</updated><content>&lt;doc fingerprint="cf94bfdaef1ccb2f"&gt;
  &lt;main&gt;
    &lt;p&gt;Foundation models for time series&lt;/p&gt;
    &lt;p&gt;We are on a mission to create the forecasting foundation model to rule them all. Forecasting drives critical decisions worldwide - impacting staffing, supply chain management, finance and more. Our solution provides companies with the models, platform and APIs they need to easily generate the most accurate forecasts possible, helping to significantly reduce waste and enabling smarter, more confident decisions.&lt;/p&gt;
    &lt;p&gt;The forecasting model is at the heart of our technology. As the second founding MLE, you will build, train and deploy large foundation model architectures: implement and combine ideas from the literature, push the state of the art, and ultimately deploy your model for our customers to use in production. Our goal is for our models to be the best for our customers’ use cases - including for capabilities that do not exist yet in academic models.&lt;/p&gt;
    &lt;p&gt;You love your craft, have high standards, stay up-to-date with the latest ideas in ML, and know when to make trade-offs to ship. You live and breathe neural networks, and speak PyTorch or Jax. You are used to diving deep in large amounts of data, and you know what you train your models on. Bonus if you have experience building solid ML infrastructure.&lt;/p&gt;
    &lt;p&gt;You are passionate about your craft, maintain high standards, stay current with the latest tech and know when to make trade-offs to deliver results efficiently. We do not believe great engineers are “jack of all trades”, but rather that they excel at diving deep into complex topics quickly, leveraging a broad range of experiences to solve challenging problems. You are also open to exploring new concepts, technologies, and enjoy quickly throwing prototypes together to kick the tires. You prefer quick feedback loops, rather than aiming for perfection on the first try.&lt;/p&gt;
    &lt;p&gt;Our goal is to provide the most accurate and easy-to-use forecasts to our customers, by leveraging refined information from their own industry. Foundation models for time series are changing this entirely. With the current advances in data processing and model training, we can now pre-train models on diverse temporal data across industries. We provide value to our customers by enabling rapid interaction with our models when provided data and context in natural language, delivering real-time forecasts with accuracy reports. Our customers do not need to be data scientists or have a PhD in Machine Learning to build and ship an accurate forecasting system for their use-cases.&lt;/p&gt;
    &lt;p&gt;Example use cases include demand forecasting for large furniture chains, predicting sales for a restaurant group and revenue forecasting in the gaming industry.&lt;/p&gt;
    &lt;p&gt;The founders Geoff and Joachim are both Machine Learning PhDs who have built forecasting and ML systems from scratch at JP Morgan, Amazon, Google, Bloomberg, and Sonos in the US.&lt;/p&gt;
    &lt;p&gt;We are a global company that happens to be HQed in Paris. Get the best of both worlds — Silicon Valley work ethic and ambition in the center of Paris, right across from the historical Stock Exchange, in the Sentier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/the-forecasting-company/jobs/cXJzAhA-founding-machine-learning-engineer"/><published>2025-10-09T07:01:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45524624</id><title>The React Foundation</title><updated>2025-10-09T13:03:12.990100+00:00</updated><content>&lt;doc fingerprint="ed2bf7695563fd5c"&gt;
  &lt;main&gt;
    &lt;p&gt;Meta open-sourced React over a decade ago to help developers build better user experiences. Since then, React has grown into one of the world’s most popular open source projects, powering over 50 million websites and products built by companies such as Microsoft, Shopify, Bloomberg, Discord, Coinbase, the NFL, and many others. With React Native, React has expanded to support platforms beyond the web, including mobile, tablets, desktops, TVs, gaming consoles, and even mixed reality devices.&lt;/p&gt;
    &lt;p&gt;This incredible growth is thanks to the thousands of educators, companies, and projects that have contributed to the development of React. The community is the heart of React, and we’re proud to play a part in the cycle of open source innovation throughout the ecosystem that benefits everyone. We’re pleased to give a seat at the table to the people and companies that have made React what it is today.&lt;/p&gt;
    &lt;p&gt;Today, we are excited to announce the next step for React. Several projects within the React ecosystem, including React and React Native, as well as supporting projects such as JSX, will transition to the React Foundation. The React Foundation’s mission is to help the React community and its members. The React Foundation will maintain React’s infrastructure, organize React Conf, and create initiatives to support the React ecosystem. The React Foundation will be part of the Linux Foundation, which has long fostered a vendor-neutral environment for open source projects.&lt;/p&gt;
    &lt;head rend="h2"&gt;Formalizing Governance&lt;/head&gt;
    &lt;p&gt;The React Foundation’s governing board will consist of representatives from Amazon, Callstack, Expo, Meta, Microsoft, Software Mansion, and Vercel, with the intention to expand further over time.&lt;/p&gt;
    &lt;p&gt;There will be a clear separation between the business and technical governance of React. Releases, features, and technical direction will be governed by a new structure driven by the maintainers and contributors of React. This new technical governance structure will be independent of the React Foundation. The React team is actively working on this new technical governance structure and will share more details in a future post on the React blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meta and the React Foundation&lt;/head&gt;
    &lt;p&gt;Meta is committing to a five-year partnership with the React Foundation, including over $3 million in funding and dedicated engineering support. This investment will ensure React’s smooth transition to independent governance while maintaining the stability and innovation the community expects. Meta will continue to invest in React and use it as our primary tool for building UI on the web and across many of Meta’s apps. Meta will also continue to have a dedicated team of engineers working full-time on React and React Native.&lt;/p&gt;
    &lt;p&gt;We believe the best of React is yet to come. The React Foundation will unlock new opportunities for collaboration, innovation, and growth that will benefit the entire ecosystem. We’re excited to see what the community will build together under this new model. With strengthened governance, broader industry participation, and continued technical excellence, React is positioned to tackle the next generation of challenges in UI development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://engineering.fb.com/2025/10/07/open-source/introducing-the-react-foundation-the-new-home-for-react-react-native/"/><published>2025-10-09T07:30:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45525303</id><title>QUIC and the End of TCP Sockets</title><updated>2025-10-09T13:03:12.467297+00:00</updated><content>&lt;doc fingerprint="1ee7e5a408d5952d"&gt;
  &lt;main&gt;
    &lt;p&gt;Practice Now Codemia © 2022 Codemia Resources Blog System Design Legal Terms &amp;amp; Conditions Privacy Policy Contact Social Twitter LinkedIn All Rights Reserved.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://codemia.io/blog/path/QUIC-and-the-End-of-TCP-Sockets-How-User-Space-Transport-Rewrites-Flow-Control"/><published>2025-10-09T09:14:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45526042</id><title>Zippers: Making Functional "Updates" Efficient (2010)</title><updated>2025-10-09T13:03:12.061526+00:00</updated><content>&lt;doc fingerprint="b6c3d03768b1d678"&gt;
  &lt;main&gt;
    &lt;p&gt; In the Haskell stuff, I was planning on moving on to some monad-related&lt;lb/&gt; stuff. But I had a reader write in, and ask me to write another&lt;lb/&gt; post on data structures, focusing on a structured called a&lt;lb/&gt; zipper.&lt;/p&gt;
    &lt;p&gt; A zipper is a remarkably clever idea. It’s not really a single data&lt;lb/&gt; structure, but rather a way of building data structures in functional&lt;lb/&gt; languages. The first mention of the structure seems to be a paper&lt;lb/&gt; by Gerard Huet in 1997, but as he says in the paper, it’s likely that this was&lt;lb/&gt; used before his paper in functional code — but no one thought to formalize it&lt;lb/&gt; and write it up. (In the original version of this post, I said the name of the guy who first wrote about zippers was “Carl Huet”. I have absolutely no idea where that came from – I literally had his paper on my lap as I wrote this post, and I still managed to screwed up his name. My apologies!)&lt;/p&gt;
    &lt;p&gt; It also happens that zippers are one of the rare cases of data structures&lt;lb/&gt; where I think it’s not necessarily clearer to show code. The concept of&lt;lb/&gt; a zipper is very simple and elegant – but when you see a zippered tree&lt;lb/&gt; written out as a sequence of type constructors, it’s confusing, rather&lt;lb/&gt; than clarifying.&lt;/p&gt;
    &lt;p&gt; The basic idea of a zipper is to give you a way of efficiently working with data&lt;lb/&gt; structures in a functional language. There are a lot of cases where in an imperative&lt;lb/&gt; language, there’s some basic operation which is cheap and simple in the imperative&lt;lb/&gt; language, because it’s performed by an in-place update. But in a functional language,&lt;lb/&gt; you can’t update a field of a data structure: instead, you have to create a new copy of the structure with the altered&lt;lb/&gt; field. &lt;/p&gt;
    &lt;p&gt; For example, consider the list &lt;code&gt;[a b c d e f g]&lt;/code&gt;. Implemented&lt;lb/&gt; as a cons-list, it’s a list of 7 cons-cells. Suppose you wanted&lt;lb/&gt; to replace “e” with “q”. In an imperative language, that’s no problem: just&lt;lb/&gt; do a set-car! of the 5th cell. In a functional language, you would&lt;lb/&gt; need to create a new list with “q” instead of&lt;lb/&gt; “e”. You could re-use the common tail &lt;code&gt;[f g]&lt;/code&gt;, but you would need&lt;lb/&gt; to re-create the other 5 cells: you’d need to create a new cell to&lt;lb/&gt; attach “q” to &lt;code&gt;[f g]&lt;/code&gt;. Then you’d need to create a new&lt;lb/&gt; cell to connect “d” to &lt;code&gt;[q f g]&lt;/code&gt;. And so on.&lt;/p&gt;
    &lt;p&gt; That makes the functional program much slower than the imperative one.&lt;lb/&gt; If you’ve got a data structure that conceptually changes over time, and you’re going to make lots of changes,&lt;lb/&gt; the cost of doing it functionally can become very high, because of all of the copying&lt;lb/&gt; you do instead of mutating a data structure.&lt;/p&gt;
    &lt;p&gt; In general, it’s very hard to get around that. You can’t update in place&lt;lb/&gt; in a functional language (at least, not without some serious cleverness, either&lt;lb/&gt; in your code (like monads), you language (like linear types), or your compiler).&lt;lb/&gt; But for many applications, there’s some notion&lt;lb/&gt; of a focus point – that is, a particular key point where changes&lt;lb/&gt; happen — and you can build structures where updates around the focus&lt;lb/&gt; can be performed efficiently.&lt;/p&gt;
    &lt;p&gt; For example, if you’re building a text editor, you’ve got the point&lt;lb/&gt; where the cursor is sitting – and the changes all happen around the cursor.&lt;lb/&gt; The user might type some characters, or delete some characters – but it always&lt;lb/&gt; happens around the cursor.&lt;/p&gt;
    &lt;p&gt; What a zipper does is take a data structure, and unfold it around a focal&lt;lb/&gt; point. Then you can make changes at the focal point very quickly – about as&lt;lb/&gt; quickly as an in-place update in an imperative language.&lt;/p&gt;
    &lt;p&gt; The idea of it is a lot like a gap-buffer. Right now, I’m actually working&lt;lb/&gt; on a text-editor. I’m writing it using a gap-buffer. Conceptually, an&lt;lb/&gt; edit-buffer is one continuous sequence of characters. But if you represent it&lt;lb/&gt; as a continuous sequence of characters, every insert is extremely expensive.&lt;lb/&gt; So what you do is split it into two sub-sequences: one consisting of the&lt;lb/&gt; characters before the cursor point, and one consisting of the&lt;lb/&gt; characters after the cursor point. With that representation,&lt;lb/&gt; inserting a character at the cursor point is O(1). Moving by one character is&lt;lb/&gt; also O(1). Moving by N characters is O(N). With various improvements, you can&lt;lb/&gt; do much better than that – but the key bit is that split between before the&lt;lb/&gt; focus point and after it.&lt;/p&gt;
    &lt;p&gt; A zipper is a tree or graph-based version of a similar idea. For this&lt;lb/&gt; discussion, I’ll describe it in terms of trees; the graph version is more complicated,&lt;lb/&gt; but you should be able to get the idea from seeing how it works on trees. The&lt;lb/&gt; idea is that you take the tree structure, and you split it around a focus. You’re focused on some node in the&lt;lb/&gt; tree. You keep track of a set of nodes that come before you, and a&lt;lb/&gt; set of nodes that come after you – those are basically like the&lt;lb/&gt; pre-gap and post-gap regions of a gap buffer. But because you’re working in a&lt;lb/&gt; tree, you need a bit more information: you need to know the path from&lt;lb/&gt; the root of the tree down to the current node. &lt;/p&gt;
    &lt;p&gt; It’s called a zipper because what you do to create this pre-focus, path,&lt;lb/&gt; and post-focus bits of the structure is unzip the tree. For example,&lt;lb/&gt; look at the tree below. It’s a representation of a string of text represented&lt;lb/&gt; by a tree. In this particular tree, all of the data is stored in the leaves.&lt;lb/&gt; The internal nodes contain metadata, which I haven’t shown in the diagram.&lt;/p&gt;
    &lt;p&gt; Now, suppose I want to put the focus on “mno”. To do that, I climb down&lt;lb/&gt; the tree, unzipping as I go. I start at the root, node N1. Then I go&lt;lb/&gt; right. So I put N1 and its left subtree into the left-context of my&lt;lb/&gt; zipper-tree, and add “Right at N1” to the path. That puts the focus at N3. To&lt;lb/&gt; get to “mno” from N3, I need to go left. So I put N3 and its right child into&lt;lb/&gt; the right context, and add “Left at N3” to the path. Now the focus is at N4.&lt;lb/&gt; To get to “mno”, I need to go right: so I put N4 and its left child into the&lt;lb/&gt; left context, and add “Right at N4” to the path. Now I’ve got the focus set&lt;lb/&gt; where I want it at “mno”; and I’ve got right and left contexts.&lt;/p&gt;
    &lt;p&gt; With the zipper, you can make all sorts of changes very easily at the&lt;lb/&gt; focus. Suppose I want to change the focus node, by inserting some text. I can&lt;lb/&gt; do that functionally, without actually changing anything, by creating a new&lt;lb/&gt; zipper tree which is identical to the old one, but which changes the value of&lt;lb/&gt; the focus node – that is, if I were to add “123” right after “mno”, I could do&lt;lb/&gt; it by creating a new focus node “mno123”, with the same path, left, and right&lt;lb/&gt; contexts. It takes minimal extra memory to create the copy, because I can&lt;lb/&gt; re-use the path and the contexts. &lt;/p&gt;
    &lt;p&gt; I could also add new children nodes. Suppose that instead of adding&lt;lb/&gt; “123” to the focus, I want to keep each leaf containing three characters.&lt;lb/&gt; could replace the focus with a new node, N5, which had children “mno”&lt;lb/&gt; and “123”. I could re-use the “mno” node, and the path, left, and right&lt;lb/&gt; contexts.&lt;/p&gt;
    &lt;p&gt; That’s the beauty of the zipper: most operations can be in terms of local&lt;lb/&gt; changes, re-using most of the structure. If we were using a standard tree,&lt;lb/&gt; then to add a new node in the position of “mno”, we would need to create&lt;lb/&gt; copies of N4, N3, and N1; instead, we only need to create the one new&lt;lb/&gt; node.&lt;/p&gt;
    &lt;p&gt; Doing other things isn’t that difficult either. Suppose we wanted to move&lt;lb/&gt; the focus to “pqr”. We’d need to shift the focus from “mno” to N3, then to N3,&lt;lb/&gt; and then to “pqr”. To get from “mno” to N4, we take the last step off of the&lt;lb/&gt; path – which says we went right at N4 – so we set the focus to N4, and&lt;lb/&gt; re-establish “mno” as its right child. So the focus would be N4, with “jkl” as&lt;lb/&gt; its left child, and “mno” as its right child. To get from N4 to N3, we unroll&lt;lb/&gt; another step of the path: since we went left at N3, that means that N3 is the&lt;lb/&gt; new focus, with N4 as its left child. Then we’d go down to the right from N3,&lt;lb/&gt; so we’d add “right at N3” to the path, and “pqr” would be the new focus.&lt;lb/&gt; Moving the focus like that is a tad more difficult than just traversing&lt;lb/&gt; non-zipper tree, but it’s not significantly slower – and it makes the edits&lt;lb/&gt; much, much faster.&lt;/p&gt;
    &lt;p&gt;So why is it harder to code? Because when we’re dealing with trees, we’re pretty much always dealing with balance. And balance isn’t a local property. No matter which kind of tree you use – red/black, 2/3, AVL – you might need to climb up the tree to do the balance maintenance. That mangles the simple zipper.&lt;/p&gt;
    &lt;p&gt; You’ve got two choices. One is to re-balance&lt;lb/&gt; the tree immediately. You can definitely do that. For&lt;lb/&gt; example, if you think of how you do a re-balance in&lt;lb/&gt; a red-black tree, you climb up the tree doing fixes until you’ve got things rebalanced. You can definitely do that – by using the zipper to move around the tree. But a big part of the point of the zipper is to keep operations local, and the re-balancing is not a&lt;lb/&gt; local operation. Much of the time, you can do things&lt;lb/&gt; locally, but sometimes you’ll be stuck re-zipping as you move the focus up the tree fixing the balance; in&lt;lb/&gt; the worst case, you need to re-zip the entire tree, all the way to the root.&lt;/p&gt;
    &lt;p&gt; The alternative is something called scarring. You put marks in the tree called scars that identify places where you made changes that could trigger a rebalance. (Or more generally,&lt;lb/&gt; in places where you made an edit that could have violated some invariant of the data structure.) You don’t do the fix immediately – you just mark it with&lt;lb/&gt; the scar, and then at some point, whenever it makes sense for your application, you go back to the scars, and fix the tree. (Scaring can also have a more general meaning, which involves memorizing certain paths through&lt;lb/&gt; the tree, so that you can make changes at the leave, then a few steps up, then back at the leaf. It’s a similar concept; in both forms of scarring, you’re optimizing to reduce the cost of zipping up and down the tree. )&lt;/p&gt;
    &lt;p&gt; Either way, it gets a bit more complicated – and when you look at the code&lt;lb/&gt; for a zipper, the re-balancing/invariant fixing has a tendency to dominate the complexity&lt;lb/&gt; of the code. The zipper itself is so simple and so elegant that it just disappears under&lt;lb/&gt; the weight of tree-balancing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://www.goodmath.org/blog/2010/01/13/zippers-making-functional-updates-efficient/"/><published>2025-10-09T11:07:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45526543</id><title>Why Are So Many Pedestrians Killed by Cars in the US?</title><updated>2025-10-09T13:03:11.777518+00:00</updated><content>&lt;doc fingerprint="edd8878eb5e2b615"&gt;
  &lt;main&gt;
    &lt;p&gt;It’s unfortunately not uncommon for pedestrians to be killed by cars in the US. More than 7,300 pedestrians were killed in motor vehicle accidents in the US in 2023, around 18% of all motor vehicle deaths that year. Until around 2009, pedestrian deaths in the US had been falling, declining from 7,516 deaths in 1975 to just 4,109 in 2009 (in per capita terms, this decline would be even larger.) But since 2009, pedestrian deaths have surged.&lt;/p&gt;
    &lt;p&gt;Motor vehicle deaths overall are up, but not nearly to the same degree. From 2009 to 2023, non-pedestrian motor vehicle deaths in the US increased by around 13%, compared to a 78% increase in pedestrian deaths. (The low point in non-pedestrian motor vehicle deaths is actually 2014; deaths are up 20% since then.)&lt;/p&gt;
    &lt;p&gt;Other countries haven’t seen this increase in pedestrian deaths: in every other high-income country, rates are flat or declining. Whatever’s causing the problem seems to be limited to the US.&lt;/p&gt;
    &lt;p&gt;There are a variety of theories for what’s causing this increase in pedestrian deaths. Perhaps the most common theory is that as trucks and SUVs have become both more popular and larger (in height and overall size), pedestrian collisions have become more frequent and more deadly. Another theory (one that’s harder to square with the “US only” nature of the phenomenon) is that drivers are increasingly distracted by smartphones, leading to more accidents. And of course, it could be something else entirely, such as drivers becoming more reckless for some reason.&lt;/p&gt;
    &lt;p&gt;Looking at the data, the strongest evidence seems to be for the “big SUV” hypothesis: the fatality rate for pedestrian accidents has increased dramatically across a variety of states, pointing to “pedestrian accidents becoming more deadly” as a major cause of the increase. But the case for it isn’t open and shut, as pedestrian deaths involving sedans and compacts have also increased. And while there isn’t much evidence for the “distracted by phones” hypothesis, it’s also hard to rule it out completely.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pedestrian deaths in the US&lt;/head&gt;
    &lt;p&gt;To investigate US pedestrian fatalities, we can use NHTSA Fatality Analysis Reporting System (FARS) data, which collects information on motor vehicle fatalities going back to 1975. FARS data is very thorough (it includes vehicle identification numbers, or VINs, the time of accident down to the minute, and the exact coordinates of the crash), making it possible to investigate a variety of possible explanations for the fatality increase. To start, let’s get a sense of where these pedestrian deaths are happening. The map below shows every pedestrian death in the continental US in 2023: each dot is a pedestrian death.&lt;/p&gt;
    &lt;p&gt;This is one of those maps that’s also a population map — pedestrian deaths tend to be concentrated in major metro areas, where there’s the most people and the most driving. If we look at pedestrian deaths by state, we see the same pattern, though it’s also clear some states are punching above and below their weight. The three states with the most pedestrian deaths — California, Florida, and Texas — are the three most populous states. But New York, the 4th most populous state, has fewer pedestrian deaths than Georgia, the 8th most populous state.&lt;/p&gt;
    &lt;p&gt;If we look at pedestrian deaths per capita, we see that deaths are much more frequent in the West and the South, and less frequent in the Northeast and the Midwest. This does not appear to be due to larger amounts of driving in the South and the West. There’s little correlation between the number of vehicle miles traveled and the pedestrian death rate.&lt;/p&gt;
    &lt;p&gt;These numbers are all from 2023. If we look at the change in pedestrian deaths over time, we can see that the rise in pedestrian deaths is worse in the South, but besides a handful of states (Minnesota, Rhode Island, New York, West Virginia, New Jersey, and Vermont), every state has seen substantial increases in pedestrian deaths. Whatever’s causing the increase in US pedestrian deaths is happening across the country.&lt;/p&gt;
    &lt;p&gt;We’ve seen that pedestrian deaths are more likely to occur in population centers, but FARS data also lets us look at the type of road where deaths occur:&lt;/p&gt;
    &lt;p&gt;The increase in fatalities is essentially entirely on urban roads — deaths on rural roads are flat. On most categories of urban road, pedestrian fatalities have doubled. Whatever is causing the increase in pedestrian deaths, it’s only happening in urban areas.&lt;/p&gt;
    &lt;p&gt;We can also look at other trends in pedestrian deaths for clues as to why they might be increasing. Here are pedestrian deaths by the hour the accident occurred, from 2007 to 2023:&lt;/p&gt;
    &lt;p&gt;And here’s the same chart in percentages:&lt;/p&gt;
    &lt;p&gt;Most pedestrian deaths occur at night, but the rate hasn’t changed much (the fraction of deaths between 6pm and 6am is up slightly from 69% in 2009 to 75% in 2023). Whatever’s causing the increase in pedestrian deaths doesn’t seem specific to one time of day.&lt;/p&gt;
    &lt;p&gt;Nor is it specific to one time of week. Deaths have become somewhat less likely to occur on Friday and Saturday, and somewhat more likely to occur during the week, but the shift isn’t dramatic.&lt;/p&gt;
    &lt;p&gt;It’s also not specific to one time of year. The graph below shows pedestrian fatality frequency by month. Pedestrian deaths are more likely to occur in the fall and winter (presumably because there are fewer daylight hours and more driving in darkness), but monthly rates haven’t changed at all.&lt;/p&gt;
    &lt;p&gt;What about trends in the age of pedestrians killed? You sometimes see claims that children are increasingly at risk of getting killed by large trucks and SUVs, because they can’t be seen by the drivers. The graph below shows pedestrian deaths broken down into 10-year age buckets.&lt;/p&gt;
    &lt;p&gt;Deaths of children under 10 are actually down significantly (167 deaths in 2009 to 98 deaths in 2023), and deaths for ages 10-19 are down as well. The biggest increase in deaths actually comes from older age brackets: 30-39 year old deaths are up 153%, 60-69 year olds up 167%, and 70-79 year olds up 119%. So the problem isn’t young kids increasingly getting hit by cars that can’t see them.&lt;/p&gt;
    &lt;p&gt;What about the age of drivers?&lt;/p&gt;
    &lt;p&gt;Deaths are up in every driver age bracket, with older brackets (30-39, 60-69 and 70-79) up the most in percentage terms. So the problem isn’t reckless young drivers.&lt;/p&gt;
    &lt;p&gt;What about trends in drug and alcohol use? According to FARS data, deaths that involved drivers drinking are up modestly, but are a very small fraction of overall pedestrian deaths. The increase in driver drug use is up by a much greater fraction (more than doubling since 2009), but it’s still a small percentage of pedestrian deaths.&lt;/p&gt;
    &lt;p&gt;Interestingly, driver drug and alcohol use in pedestrian fatalities is greatly exceeded by pedestrian alcohol and drug use. (Pedestrian drug use in particular has more than tripled since 2009, while alcohol use is only up modestly.) It’s not enough to explain all of the huge increase in pedestrian deaths, but it’s notable.&lt;/p&gt;
    &lt;p&gt;An obvious possible factor in increased deaths is people increasingly distracted by their phones. Unfortunately, FARS data doesn’t give us much to go on here. Since 2010 FARS has tracked whether drivers are distracted (by phones or anything else), but in the vast majority (94%) of cases, drivers are marked “not distracted,” “not reported,” or “unknown.”&lt;/p&gt;
    &lt;p&gt;Relatedly, in the majority of pedestrian deaths, the pedestrian is blamed for the accident. In 66% of cases, pedestrians are described as “failing to yield right of way,” “jaywalking,” or “in roadway improperly.” In 87% of cases, the driver is not charged with anything following the accident. (This doesn’t necessarily mean the pedestrian was at fault — it could simply indicate that in a pedestrian death we only get one side of the story, which makes it hard to charge the driver with a crime.) About 75% of the time, fatal pedestrian accidents occur outside an intersection (a rate which has been steady since 2010), suggesting pedestrians are most often struck outside of a crosswalk.&lt;/p&gt;
    &lt;p&gt;One limitation of the FARS data is that it only tracks pedestrian fatalities: it doesn’t give us any information or trends in non-fatal pedestrian accidents. Many states, however, do track this information. The chart below shows the total number of accidents involving a pedestrian, and the number of pedestrian fatalities, for different years for 20 different states.&lt;/p&gt;
    &lt;p&gt;In nearly every state examined, the fatality rate for pedestrian accidents has risen dramatically in recent years. Oregon went from deaths in 6% of accidents in 2014 to 14% of accidents in 2023. Illinois went from 2.3% of pedestrian accidents being deadly in 2008 to 4.3% in 203. New Mexico went from 8.4% in 2008 to 16.8% in 2023. In many cases, overall pedestrian accidents were flat or even down (Illinois went from 5,877 accidents in 2008 to 4,533 accidents in 2023), even as the number of pedestrian fatalities went up.&lt;/p&gt;
    &lt;p&gt;Pedestrian accidents getting more deadly seems like fairly strong evidence for the theory that the rise in large SUVs is behind the uptick in pedestrian deaths: it’s not that more pedestrians are getting hit by vehicles, it’s that the ones that are getting hit are more likely to die. There’s other evidence that points to this theory. A study by the Insurance Institute for Highway Safety analyzed 17,897 pedestrian accidents across seven states, and found pedestrians were substantially more likely to be killed when struck by tall vehicles and vehicles with blunt front ends.&lt;/p&gt;
    &lt;p&gt;However, there’s also some muddying evidence here. If the increase of size and frequency of trucks and SUVs was behind the increase in pedestrian deaths, we wouldn’t expect to see an increase in the frequency of pedestrians killed by sedans or compact cars. However, if we look at pedestrian deaths by model of car, we see that pedestrian deaths involving popular sedans have increased as well. Pedestrian deaths involving Honda Civics and Accords, Toyota Corollas and Camrys, and Nissan Altimas have all increased substantially.&lt;/p&gt;
    &lt;p&gt;This does not seem to be because these cars suddenly got more popular. Sales volume since the early 2000s is relatively flat or declining:&lt;/p&gt;
    &lt;p&gt;One possible explanation is that pedestrian accidents are more deadly because cars are speeding more frequently, but it’s hard to find evidence of this. The proportion of vehicles speeding in both pedestrian fatalities and fatalities in car accidents overall is flat.&lt;/p&gt;
    &lt;p&gt;On the state level, different states have dramatically different speeding rates (presumably due to classification differences), but overall rates of speeding collisions seem to be mostly flat or declining.&lt;/p&gt;
    &lt;p&gt;We can also use state-level data to look at the “distracted by phones” theory: some states break out whether “distraction” or “driver inattention” is a factor in an accident. The chart below shows the percentage of all accidents where driver inattention (due to phones or anything else) was a listed factor. If being distracted by phones was a major driver of increased pedestrian fatalities, I’d expect it to also be increasingly a factor in car accidents more broadly.&lt;/p&gt;
    &lt;p&gt;There’s no real clear trend. In some states (such as Texas) the frequency of driver inattention as a factor has increased dramatically over time. But in other states it’s either flat (South Carolina, Kentucky), or has decreased substantially (Arizona, California). This doesn’t necessarily disconfirm the phone theory, as drivers may not be reporting that they were distracted (North Carolina’s annual car crash reports, for instance, note that “Driver Distraction is a self-reporting contributing circumstance. Therefore, the data collected may not reflect the severity of this issue.” But it’s some evidence against it.&lt;/p&gt;
    &lt;p&gt;Similarly, if being distracted by phones was a driver of pedestrian deaths, I’d expect there to be more frequent collisions overall. But the state-level data similarly doesn’t show any consistent trend. In some states (Texas, South Carolina) collisions are up substantially, though the rate of increase is less than for pedestrian deaths. But in California and Georgia, among others, they’re down.&lt;/p&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;To recap, here’s what we know about pedestrian deaths.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pedestrian deaths have risen substantially in the US since 2009, by nearly 80%. This increase is seen across the US, though not in every single state. The increase is highest in states in the West and South. There has not been a similar increase in other types of motor vehicle deaths, or in pedestrian deaths in other countries.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The increase has happened almost entirely on urban roads: pedestrian deaths on rural roads have remained roughly constant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There has not been much change in what time of day, day of week, or time of year pedestrian deaths are occurring.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The 30-39 and 60-79 age brackets have seen the largest increase in victims. Pedestrian deaths of children and teenagers are down over this period. In terms of drivers, the increase has also been largest in the 30-39 and 60-79 age brackets.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It’s hard to pin the increase to any specific driver behavior on the road (including mobile phone use). Drivers are rarely charged with a motor vehicle violation following a pedestrian death, and are rarely categorized as “distracted” (by a phone or anything else). State-level data on frequency of driver inattention shows no consistent trends. Rates of driver drinking are only up modestly; rates of driver drug use have risen greatly in percentage terms, but remain a very small fraction of overall pedestrian death accidents. There doesn’t seem to be a clear trend in rates of collisions overall. Drivers don’t seem to be speeding more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In terms of pedestrian behavior, the number of killed pedestrians believed to be using drugs has risen substantially over the period in question, but is still a small fraction of overall pedestrian deaths. Rates of alcohol use by pedestrians have increased modestly. In roughly 2/3rds of pedestrian deaths, the pedestrian is described as “failing to properly yield,” “jaywalking,” or as otherwise improperly in the roadway.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In terms of changes in the nature of vehicles, the fatality rate of pedestrian accidents (the proportion of accidents where a pedestrian is killed) has increased dramatically across many different states, suggesting pedestrian accidents are getting more deadly. The frequency of pedestrian deaths in both trucks/SUVs and sedans/compacts has risen substantially.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, this doesn’t all add up to a clear cause. The strongest evidence seems to be for the “Big SUV hypothesis” — it’s hard to see what else could be causing the increase in deadliness of pedestrian accidents, and not cause a similar increase in other things. The Big SUV hypothesis also seems like something that could be limited to the US. But this on its own isn’t completely satisfying: if its big SUVs, why are pedestrian deaths for sedans increasing too? Why aren’t deaths increasing on rural roads? There are still unanswered questions here.&lt;/p&gt;
    &lt;p&gt;The evidence does not strongly support the “distracted drivers on cell phones” hypothesis, but neither does it totally disconfirm it. The fact that drivers are rarely charged with anything, and are rarely classified as having been driving distracted, could simply be due to the fact that we rarely get more than the driver’s side of the story in pedestrian fatalities. The fact that there’s not a general trend in more auto collisions, or more pedestrian non-fatal collisions, does push against this theory though.&lt;/p&gt;
    &lt;p&gt;Beyond these hypotheses, there’s also some evidence that increased drug use (both in drivers and pedestrians) is a factor in increased deaths, though almost certainly not the main one. We also can’t rule out that increased recklessness or distractedness on the part of pedestrians is playing a role.&lt;/p&gt;
    &lt;p&gt;One avenue to try and better understand this problem is to look at the level of individual cities, and to try to figure out why places like Boston and Seattle have so few pedestrian deaths compared to other cities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.construction-physics.com/p/why-are-so-many-pedestrians-killed"/><published>2025-10-09T12:12:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45526589</id><title>McKinsey wonders how to sell AI apps with no measurable benefits</title><updated>2025-10-09T13:03:11.438839+00:00</updated><content>&lt;doc fingerprint="3f5b03de093ae5ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;McKinsey wonders how to sell AI apps with no measurable benefits&lt;/head&gt;
    &lt;head rend="h2"&gt;Consultant says software vendors risk hiking prices without cutting costs or boosting productivity&lt;/head&gt;
    &lt;p&gt;Software vendors keen to monetize AI should tread cautiously, since they risk inflating costs for their customers without delivering any promised benefits such as reducing employee head count.&lt;/p&gt;
    &lt;p&gt;The latest report from McKinsey &amp;amp; Company mulls what software-as-a-service (SaaS) vendors need to do to navigate the minefield of hype that surrounds AI and successfully fold such capabilities into their offerings.&lt;/p&gt;
    &lt;p&gt;According to the consultancy, there are three main challenges it identifies as holding back broader growth in AI software monetization in the report "Upgrading software business models to thrive in the AI era."&lt;/p&gt;
    &lt;p&gt;One of these is simply the inability to show any savings that can be expected. Many software firms trumpet potential use cases for AI, but only 30 percent have published quantifiable return on investment from real customer deployments.&lt;/p&gt;
    &lt;p&gt;Meanwhile, many customers see AI hiking IT costs without being able to offset these by slashing labor costs. The billions poured into developing AI models mean they don't come cheap, and AI-enabling the entire customer service stack of a typical business could lead to a 60 to 80 percent price increase, McKinsey says, while quoting an HR executive at a Fortune 100 company griping: "All of these copilots are supposed to make work more efficient with fewer people, but my business leaders are also saying they can't reduce head count yet."&lt;/p&gt;
    &lt;p&gt;Another challenge is scaling up adoption after introduction, which the report blames on underinvestment in change management. It says that for every $1 spent on model development, firms should expect to have to spend $3 on change management, which means user training and performance monitoring.&lt;/p&gt;
    &lt;p&gt;The third issue is a lack of predictable pricing, which means that customers find it hard to forecast how their AI costs will scale with usage because the pricing models are often complex and opaque.&lt;/p&gt;
    &lt;p&gt;To address these, McKinsey focuses mainly on how software firms should structure their pricing in the age of AI, rather than the wisdom of infusing AI into everything in the first place.&lt;/p&gt;
    &lt;p&gt;The report considers it unlikely that the traditional per-user monthly subscription model will disappear entirely, but expects that vendors will have to incorporate some form of consumption-based pricing into the mix.&lt;/p&gt;
    &lt;p&gt;Many are starting with hybrid models, where "additional" consumption that goes beyond a capacity cap is treated in different ways, such as metered throughput that limits the number of tokens processed daily, weekly, or monthly for certain models.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI agent hypefest crashing up against cautious leaders, Gartner finds&lt;/item&gt;
      &lt;item&gt;AI in your toaster: Analyst predicts $1.5T global spend in 2025&lt;/item&gt;
      &lt;item&gt;Goldman Sachs warns AI bubble could burst datacenter boom&lt;/item&gt;
      &lt;item&gt;Amazon's $100B DC spend similar to entire Costa Rica GDP&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, firms with hybrid models will need to revisit their choices frequently, it warns, as the rapid pace of AI evolution means that capabilities that are cutting-edge at launch can quickly become table stakes.&lt;/p&gt;
    &lt;p&gt;Vendors also need to choose their pricing unit carefully, whether that is a per-user flat fee with a capacity cap, like Microsoft Copilot, on a per-task basis, or perhaps on an outcome basis, such as per qualified lead for sales tools.&lt;/p&gt;
    &lt;p&gt;However, McKinsey also claims that the cost of inferencing is dropping rapidly, and so vendors need to consider carefully how they balance charges with growing adoption. The cost of large language model (LLM) delivery has declined by more than 80 percent per year over the past two years, it says.&lt;/p&gt;
    &lt;p&gt;Many SaaS companies believe they need to encourage trials to increase adoption, by offering free initial usage allocations for AI capabilities, for example. Once customers adopt and see value, the thinking goes, the firm can then look to upsell to a higher allocation for additional use cases. The problem with that, of course, is that one MIT study found that many enterprise organizations have so far seen zero return from their AI efforts.&lt;/p&gt;
    &lt;p&gt;Buyers are also changing, McKinsey believes. It says purchasing decisions are shifting from the IT department to line-of-business units. These leaders are increasingly making budget trade-offs between head count investment and AI deployment, and expect vendors to engage them on value and outcomes, not just features.&lt;/p&gt;
    &lt;p&gt;That could be a tricky sell, when trials of AI tools such as Microsoft's Copilot by a UK government department reveal no discernible boost in productivity. Still, the AI firms have to recoup all those billions they've already invested somehow, don't they? ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/09/mckinsey_ai_monetization/"/><published>2025-10-09T12:19:20+00:00</published></entry></feed>