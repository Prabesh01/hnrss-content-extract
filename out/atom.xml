<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-22T20:37:11.657376+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45332859</id><title>I'm spoiled by Apple Silicon but still love Framework</title><updated>2025-09-22T20:37:18.437248+00:00</updated><content>&lt;doc fingerprint="9eb0593b034f91dc"&gt;
  &lt;main&gt;
    &lt;p&gt;I mainly use my MacBook M1 Pro with Apple Silicon for work. Recently I’ve had a few weeks off. It’s been sitting in my laptop bag waiting for me to return to work.&lt;/p&gt;
    &lt;p&gt;Since I’m back to work tomorrow I got it out to see if it was charged enough for the train journey in the morning. 90% remaining. After 3 weeks. It wasn’t turned off, just closed.&lt;/p&gt;
    &lt;p&gt;In contrast, my Framework 13 with an AMD Ryzen 7840HS is almost always dead when I go to use it. It’s very frustrating. I’m not using it every day so it often goes 2-3 days without being opened. I haven’t measured it but I read that I should expect it to lose 3-4% in suspend every hour. Is that a joke?&lt;/p&gt;
    &lt;p&gt;I was running Fedora Workstation on the 13 for a long while until I had a short stint with Arch Linux. It wasn’t stable enough so now I’m on Fedora Silverblue which has been delightful.&lt;/p&gt;
    &lt;p&gt;However, the Framework’s battery life continues to be an issue.&lt;/p&gt;
    &lt;p&gt;I’m a true believer of Framework’s mission. The technology is awesome. But why is the battery life of modern laptops (other than Apple Silicon) so bad? I don’t think it’s unique to Framework, but it affects my love for this groundbreaking device.&lt;/p&gt;
    &lt;p&gt;Apple Silicon is built upon ARM64 which is apparently core to such great battery life. Do I need to get an ARM mainboard as soon as Framework offers it as an upgrade? I’m not sure. It appears to be way more complicated than just switching to ARM64.&lt;/p&gt;
    &lt;p&gt;I still love my Framework, despite its flaws. I will just keep it plugged in so that it’s ready to go when I want to use it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonhartcher.com/posts/2025-09-22-why-im-spoiled-by-apple-silicon-but-still-love-framework/"/><published>2025-09-22T13:03:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45332860</id><title>Cloudflare is sponsoring Ladybird and Omarchy</title><updated>2025-09-22T20:37:18.015961+00:00</updated><content>&lt;doc fingerprint="29520b5b1bc621bc"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;At Cloudflare, we believe that helping build a better Internet means encouraging a healthy ecosystem of options for how people can connect safely and quickly to the resources they need. Sometimes that means we tackle immense, Internet-scale problems with established partners. And sometimes that means we support and partner with fantastic open teams taking big bets on the next generation of tools.&lt;/p&gt;
      &lt;p&gt;To that end, today we are excited to announce our support of two independent, open source projects: Ladybird, an ambitious project to build a completely independent browser from the ground up, and Omarchy, an opinionated Arch Linux setup for developers.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Two open source projects strengthening the open InternetÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cloudflare has a long history of supporting open-source software â both through our own projects shared with the community and external projects that we support. We see our sponsorship of Ladybird and Omarchy as a natural extension of these efforts in a moment where energy for a diverse ecosystem is needed more than ever.Â Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Ladybird, a new and independent browserÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Most of us spend a significant amount of time using a web browser âÂ in fact, youâre probably using one to read this blog! The beauty of browsers is that they help users experience the open Internet, giving you access to everything from the largest news publications in the world to a tiny website hosted on a Raspberry Pi.Â Â &lt;/p&gt;
      &lt;p&gt;Unlike dedicated apps, browsers reduce the barriers to building an audience for new services and communities on the Internet. If you are launching something new, you can offer it through a browser in a world where most people have absolutely zero desire to install an app just to try something out. Browsers help encourage competition and new ideas on the open web.&lt;/p&gt;
      &lt;p&gt;While the openness of how browsers work has led to an explosive growth of services on the Internet, browsers themselves have consolidated to a tiny handful of viable options. Thereâs a high probability youâre reading this on a Chromium-based browser, like Googleâs Chrome, along with about 65% of users on the Internet. However, that consolidation has also scared off new entrants in the space. If all browsers ship on the same operating systems, powered by the same underlying technology, we lose out on potential privacy, security and performance innovations that could benefit developers and everyday Internet users.Â &lt;/p&gt;
      &lt;p&gt;A screenshot of Cloudflare Workers developer docs in LadybirdÂ &lt;/p&gt;
      &lt;p&gt;This is where Ladybird comes in: itâs not Chromium based â everything is built from scratch. The Ladybird project has two main components: LibWeb, a brand-new rendering engine, and LibJS, a brand-new JavaScript engine with its own parser, interpreter, and bytecode execution engine.Â &lt;/p&gt;
      &lt;p&gt;Building an engine that can correctly and securely render the modern web is a monumental task that requires deep technical expertise and navigating decades of specifications governed by standards bodies like the W3C and WHATWG. And because Ladybird implements these standards directly, it also stress-tests them in practice. Along the way, the project has found, reported, and sometimes fixed countless issues in the specifications themselves, contributions that strengthen the entire web platform for developers, browser vendors, and anyone who may attempt to build a browser in the future.&lt;/p&gt;
      &lt;p&gt;Whether to build something from scratch or not is a perennial source of debate between software engineers, but absent the pressures of revenue or special interests, weâre excited about the ways Ladybird will prioritize privacy, performance, and security, potentially in novel ways that will influence the entire ecosystem.&lt;/p&gt;
      &lt;p&gt;A screenshot of the Omarchy development environment&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Omarchy, an independent development environmentÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Developers deserve choice, too. Beyond the browser, a developerâs operating system and environment is where they spend a ton of time â and where a few big players have become the dominant choice. Omarchy challenges this by providing a complete, opinionated Arch Linux distribution that transforms a bare installation into a modern development workstation that developers are excited about.&lt;/p&gt;
      &lt;p&gt;Perfecting oneâs development environment can be a career-long art, but learning how to do so shouldnât be a barrier to beginning to code. The beauty of Omarchy is that it makes Linux approachable to more developers by doing most of the setup for them, making it look good, and then making it configurable. Omarchy provides most of the tools developers need â like Neovim, Docker, and Git â out of the box, and tons of other features.&lt;/p&gt;
      &lt;p&gt;At its core, Omarchy embraces Linux for all of its complexity and configurability, and makes a version of it that is accessible and fun to use for developers that donât have a deep background in operating systems. Projects like this ensure that a powerful, independent Linux desktop remains a compelling choice for people building the next generation of applications and Internet infrastructure.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Our support comes with no strings attachedÂ Â &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We want to be very clear here: we are supporting these projects because we believe the Internet can be better if these projects, and more like them, succeed. No requirement to use our technology stack or any arrangement like that. We are happy to partner with great teams like Ladybird and Omarchy simply because we believe that our missions have real overlap.&lt;/p&gt;
      &lt;p&gt;Ladybird is still in its early days, with an alpha release planned for 2026, but we encourage anyone who is interested to consider contributing to the open source codebase as they prepare for launch.&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;"Cloudflare knows what it means to build critical web infrastructure on the server side. With Ladybird, weâre tackling the near-monoculture on the client side, because we believe it needs multiple implementations to stay healthy, and weâre extremely thankful for their support in that mission.â&lt;/p&gt;
        &lt;p&gt;â Andreas Kling, Founder, LadybirdÂ Â &lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;Omarchy 3.0 was released just last week with faster installation and increased Macbook compatibility, so if youâve been Linux-curious for a while now, we encourage you to try it out!&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;"Cloudflare's support of Omarchy has ensured we have the fastest ISO and package delivery from wherever you are in the world. Without a need to manually configure mirrors or deal with torrents. The combo of a super CDN, great R2 storage, and the best DDoS shield in the business has been a huge help for the project."&lt;/p&gt;
        &lt;p&gt;â David Heinemeier Hansson, Creator of Omarchy and Ruby on Rails&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;A better Internet is one where people have more choice in how they browse and develop new software. Weâre incredibly excited about the potential of Ladybird, Omarchy, and other audacious projects that support a free and open Internet. &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/supporting-the-future-of-the-open-web/"/><published>2025-09-22T13:03:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45332883</id><title>Cap'n Web: a new RPC system for browsers and web servers</title><updated>2025-09-22T20:37:17.640142+00:00</updated><content>&lt;doc fingerprint="e228a11afa2902c6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Allow us to introduce Cap'n Web, an RPC protocol and implementation in pure TypeScript.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is a spiritual sibling to Cap'n Proto, an RPC protocol I (Kenton) created a decade ago, but designed to play nice in the web stack. That means:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Like Cap'n Proto, it is an object-capability protocol. ("Cap'n" is short for "capabilities and".) We'll get into this more below, but it's incredibly powerful.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Unlike Cap'n Proto, Cap'n Web has no schemas. In fact, it has almost no boilerplate whatsoever. This means it works more like the JavaScript-native RPC system in Cloudflare Workers.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;That said, it integrates nicely with TypeScript.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Also unlike Cap'n Proto, Cap'n Web's underlying serialization is human-readable. In fact, it's just JSON, with a little pre-/post-processing.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works over HTTP, WebSocket, and postMessage() out-of-the-box, with the ability to extend it to other transports easily.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works in all major browsers, Cloudflare Workers, Node.js, and other modern JavaScript runtimes.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The whole thing compresses (minify+gzip) to under 10Â kB with no dependencies.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It's open source under the MIT license.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Cap'n Web is more expressive than almost every other RPC system, because it implements an object-capability RPC model. That means it:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Supports bidirectional calling. The client can call the server, and the server can also call the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports passing functions by reference: If you pass a function over RPC, the recipient receives a "stub". When they call the stub, they actually make an RPC back to you, invoking the function where it was created. This is how bidirectional calling happens: the client passes a callback to the server, and then the server can call it later.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Similarly, supports passing objects by reference: If a class extends the special marker type &lt;code&gt;RpcTarget&lt;/code&gt;, then instances of that class are passed by reference, with method calls calling back to the location where the object was created.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports promise pipelining. When you start an RPC, you get back a promise. Instead of awaiting it, you can immediately use the promise in dependent RPCs, thus performing a chain of calls in a single network round trip.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports capability-based security patterns.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In short, Cap'n Web lets you design RPC interfaces the way you'd design regular JavaScript APIs â while still acknowledging and compensating for network latency.&lt;/p&gt;
      &lt;p&gt;The best part is, Cap'n Web is absolutely trivial to set up.&lt;/p&gt;
      &lt;p&gt;A client looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newWebSocketRpcSession } from "capnweb";

// One-line setup.
let api = newWebSocketRpcSession("wss://example.com/api");

// Call a method on the server!
let result = await api.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And here's a complete Cloudflare Worker implementing an RPC server:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { RpcTarget, newWorkersRpcResponse } from "capnweb";

// This is the server implementation.
class MyApiServer extends RpcTarget {
  hello(name) {
    return `Hello, ${name}!`
  }
}

// Standard Workers HTTP handler.
export default {
  fetch(request, env, ctx) {
    // Parse URL for routing.
    let url = new URL(request.url);

    // Serve API at `/api`.
    if (url.pathname === "/api") {
      return newWorkersRpcResponse(request, new MyApiServer());
    }

    // You could serve other endpoints here...
    return new Response("Not found", {status: 404});
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;That's it. That's the app.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;You can add more methods to &lt;code&gt;MyApiServer&lt;/code&gt;, and call them from the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can have the client pass a callback function to the server, and then the server can just call it.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can define a TypeScript interface for your API, and easily apply it to the client and server.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;It just works.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Why RPC? (And what is RPC anyway?)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Remote Procedure Calls (RPC) are a way of expressing communications between two programs over a network. Without RPC, you might communicate using a protocol like HTTP. With HTTP, though, you must format and parse your communications as an HTTP request and response, perhaps designed in REST style. RPC systems try to make communications look like a regular function call instead, as if you were calling a library rather than a remote service. The RPC system provides a "stub" object on the client side which stands in for the real server-side object. When a method is called on the stub, the RPC system figures out how to serialize and transmit the parameters to the server, invoke the method on the server, and then transmit the return value back.&lt;/p&gt;
      &lt;p&gt;The merits of RPC have been subject to a great deal of debate. RPC is often accused of committing many of the fallacies of distributed computing.&lt;/p&gt;
      &lt;p&gt;But this reputation is outdated. When RPC was first invented some 40 years ago, async programming barely existed. We did not have Promises, much less async and await. Early RPC was synchronous: calls would block the calling thread waiting for a reply. At best, latency made the program slow. At worst, network failures would hang or crash the program. No wonder it was deemed "broken".&lt;/p&gt;
      &lt;p&gt;Things are different today. We have Promise and async and await, and we can throw exceptions on network failures. We even understand how RPCs can be pipelined so that a chain of calls takes only one network round trip. Many large distributed systems you likely use every day are built on RPC. It works.&lt;/p&gt;
      &lt;p&gt;The fact is, RPC fits the programming model we're used to. Every programmer is trained to think in terms of APIs composed of function calls, not in terms of byte stream protocols nor even REST. Using RPC frees you from the need to constantly translate between mental models, allowing you to move faster.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;When should you use Cap'n Web?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web is useful anywhere where you have two JavaScript applications speaking to each other over a network, including client-to-server and microservice-to-microservice scenarios. However, it is particularly well-suited to interactive web applications with real-time collaborative features, as well as modeling interactions over complex security boundaries.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is still new and experimental, so for now, a willingness to live on the cutting edge may also be required!&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Features, features, featuresâ¦&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's some more things you can do with Cap'n Web.&lt;/p&gt;
      &lt;p&gt;Sometimes a WebSocket connection is a bit too heavyweight. What if you just want to make a quick one-time batch of calls, but don't need an ongoing connection?&lt;/p&gt;
      &lt;p&gt;For that, Cap'n Web supports HTTP batch mode:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newHttpBatchRpcSession } from "capnweb";

let batch = newHttpBatchRpcSession("https://example.com/api");

let result = await batch.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;(The server is exactly the same as before.)&lt;/p&gt;
      &lt;p&gt;Note that once you've awaited an RPC in the batch, the batch is done, and all the remote references received through it become broken. To make more calls, you need to start over with a new batch. However, you can make multiple calls in a single batch:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// We can call make multiple calls, as long as we await them all at once.
let promise1 = batch.hello("Alice");
let promise2 = batch.hello("Bob");

let [result1, result2] = await Promise.all([promise1, promise2]);

console.log(result1);
console.log(result2);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And that brings us to another featureâ¦&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Chained calls (Promise Pipelining)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's where things get magical.&lt;/p&gt;
      &lt;p&gt;In both batch mode and WebSocket mode, you can make a call that depends on the result of another call, without waiting for the first call to finish. In batch mode, that means you can, in a single batch, call a method, then use its result in another call. The entire batch still requires only one network round trip.&lt;/p&gt;
      &lt;p&gt;For example, say your API is:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  getMyName() {
    return "Alice";
  }

  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = batch.getMyName();
let result = await batch.hello(namePromise);

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Notice the initial call to &lt;code&gt;getMyName()&lt;/code&gt; returned a promise, but we used the promise itself as the input to &lt;code&gt;hello()&lt;/code&gt;, without awaiting it first. With Cap'n Web, this just works: The client sends a message to the server saying: "Please insert the result of the first call into the parameters of the second."&lt;/p&gt;
      &lt;p&gt;Or perhaps the first call returns an object with methods. You can call the methods immediately, without awaiting the first promise, like:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// Authencitate the API key, returning a Session object.
let sessionPromise = batch.authenticate(apiKey);

// Get the user's name.
let name = await sessionPromise.whoami();

console.log(name);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This works because the promise returned by a Cap'n Web call is not a regular promise. Instead, it's a JavaScript Proxy object. Any methods you call on it are interpreted as speculative method calls on the eventual result. These calls are sent to the server immediately, telling the server: "When you finish the call I sent earlier, call this method on what it returns."&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Did you spot the security?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;This last example shows an important security pattern enabled by Cap'n Web's object-capability model.&lt;/p&gt;
      &lt;p&gt;When we call the authenticate() method, after it has verified the provided API key, it returns an authenticated session object. The client can then make further RPCs on the session object to perform operations that require authorization as that user. The server code might look like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  authenticate(apiKey) {
    let username = await checkApiKey(apiKey);
    return new AuthenticatedSession(username);
  }
}

class AuthenticatedSession extends RpcTarget {
  constructor(username) {
    super();
    this.username = username;
  }

  whoami() {
    return this.username;
  }

  // ...other methods requiring auth...
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Here's what makes this work: It is impossible for the client to "forge" a session object. The only way to get one is to call authenticate(), and have it return successfully.&lt;/p&gt;
      &lt;p&gt;In most RPC systems, it is not possible for one RPC to return a stub pointing at a new RPC object in this way. Instead, all functions are top-level, and can be called by anyone. In such a traditional RPC system, it would be necessary to pass the API key again to every function call, and check it again on the server each time. Or, you'd need to do authorization outside the RPC system entirely.&lt;/p&gt;
      &lt;p&gt;This is a common pain point for WebSockets in particular. Due to the design of the web APIs for WebSocket, you generally cannot use headers nor cookies to authorize them. Instead, authorization must happen in-band, by sending a message over the WebSocket itself. But this can be annoying for RPC protocols, as it means the authentication message is "special" and changes the state of the connection itself, affecting later calls. This breaks the abstraction.&lt;/p&gt;
      &lt;p&gt;The authenticate() pattern shown above neatly makes authentication fit naturally into the RPC abstraction. It's even type-safe: you can't possibly forget to authenticate before calling a method requiring auth, because you wouldn't have an object on which to make the call. Speaking of type-safetyâ¦&lt;/p&gt;
      &lt;p&gt;If you use TypeScript, Cap'n Web plays nicely with it. You can declare your RPC API once as a TypeScript interface, implement in on the server, and call it on the client:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Shared interface declaration:
interface MyApi {
  hello(name: string): Promise&amp;lt;string&amp;gt;;
}

// On the client:
let api: RpcStub&amp;lt;MyApi&amp;gt; = newWebSocketRpcSession("wss://example.com/api");

// On the server:
class MyApiServer extends RpcTarget implements MyApi {
  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now you get end-to-end type checking, auto-completed method names, and so on.&lt;/p&gt;
      &lt;p&gt;Note that, as always with TypeScript, no type checks occur at runtime. The RPC system itself does not prevent a malicious client from calling an RPC with parameters of the wrong type. This is, of course, not a problem unique to Cap'n Web â JSON-based APIs have always had this problem. You may wish to use a runtime type-checking system like Zod to solve this. (Meanwhile, we hope to add type checking based directly on TypeScript types in the future.)&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;An alternative to GraphQL?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;If youâve used GraphQL before, you might notice some similarities. One benefit of GraphQL was to solve the âwaterfallâ problem of traditional REST APIs by allowing clients to ask for multiple pieces of data in one query. For example, instead of making three sequential HTTP calls:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;GET /user
GET /user/friends
GET /user/friends/photos&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;â¦you can write one GraphQL query to fetch it all at once.&lt;/p&gt;
      &lt;p&gt;Thatâs a big improvement over REST, but GraphQL comes with its own tradeoffs:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;New language and tooling. You have to adopt GraphQLâs schema language, servers, and client libraries. If your team is all-in on JavaScript, thatâs a lot of extra machinery.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Limited composability. GraphQL queries are declarative, which makes them great for fetching data, but awkward for chaining operations or mutations. For example, you canât easily say: âcreate a user, then immediately use that new user object to make a friend request, all-in-one round trip.â&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Different abstraction model. GraphQL doesnât look or feel like the JavaScript APIs you already know. Youâre learning a new mental model rather than extending the one you use every day.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;How Cap'n Web goes further&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web solves the waterfall problem without introducing a new language or ecosystem. Itâs just JavaScript. Because Cap'n Web supports promise pipelining and object references, you can write code that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.createUser({ name: "Alice" });
let friendRequest = await user.sendFriendRequest("Bob");&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;What happens under the hood? Both calls are pipelined into a single network round trip:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Create the user.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Take the result of that call (a new User object).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Immediately invoke sendFriendRequest() on that object.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of this is expressed naturally in JavaScript, with no schemas, query languages, or special tooling required. You just call methods and pass objects around, like you would in any other JavaScript code.&lt;/p&gt;
      &lt;p&gt;In other words, GraphQL gave us a way to flatten RESTâs waterfalls. Cap'n Web lets us go even further: it gives you the power to model complex interactions exactly the way you would in a normal program, with no impedance mismatch.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;But how do we solve arrays?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;With everything we've presented so far, there's a critical missing piece to seriously consider Cap'n Web as an alternative to GraphQL: handling lists. Often, GraphQL is used to say: "Perform this query, and then, for every result, perform this other query." For example: "List the user's friends, and then for each one, fetch their profile photo."&lt;/p&gt;
      &lt;p&gt;In short, we need an &lt;code&gt;array.map()&lt;/code&gt; operation that can be performed without adding a round trip.&lt;/p&gt;
      &lt;p&gt;Cap'n Proto, historically, has never supported such a thing.&lt;/p&gt;
      &lt;p&gt;But with Cap'n Web, we've solved it. You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.authenticate(token);

// Get the user's list of friends (an array).
let friendsPromise = user.listFriends();

// Do a .map() to annotate each friend record with their photo.
// This operates on the *promise* for the friends list, so does not
// add a round trip.
// (wait WHAT!?!?)
let friendsWithPhotos = friendsPromise.map(friend =&amp;gt; {
  return {friend, photo: api.getUserPhoto(friend.id))};
}

// Await the friends list with attached photos -- one round trip!
let results = await friendsWithPhotos;
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;&lt;code&gt;.map()&lt;/code&gt; takes a callback function, which needs to be applied to each element in the array. As we described earlier, normally when you pass a function to an RPC, the function is passed "by reference", meaning that the remote side receives a stub, where calling that stub makes an RPC back to the client where the function was created.&lt;/p&gt;
      &lt;p&gt;But that is NOT what is happening here. That would defeat the purpose: we don't want the server to have to round-trip to the client to process every member of the array. We want the server to just apply the transformation server-side.&lt;/p&gt;
      &lt;p&gt;To that end, &lt;code&gt;.map() &lt;/code&gt;is special. It does not send JavaScript code to the server, but it does send something like "code", restricted to a domain-specific, non-Turing-complete language. The "code" is a list of instructions that the server should carry out for each member of the array. In this case, the instructions are:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Invoke &lt;code&gt;api.getUserPhoto(friend.id)&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Return an object &lt;code&gt;{friend, photo}&lt;/code&gt;, where friend is the original array element and photo is the result of step 1.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;But the application code just specified a JavaScript method. How on Earth could we convert this into the narrow DSL?&lt;/p&gt;
      &lt;p&gt;The answer is record-replay: On the client side, we execute the callback once, passing in a special placeholder value. The parameter behaves like an RPC promise. However, the callback is required to be synchronous, so it cannot actually await this promise. The only thing it can do is use promise pipelining to make pipelined calls. These calls are intercepted by the implementation and recorded as instructions, which can then be sent to the server, where they can be replayed as needed.&lt;/p&gt;
      &lt;p&gt;And because the recording is based on promise pipelining, which is what the RPC protocol itself is designed to represent, it turns out that the "DSL" used to represent "instructions" for the map function is just the RPC protocol itself. ð¤¯&lt;/p&gt;
      &lt;p&gt;Cap'n Web's underlying protocol is based on JSON â but with a preprocessing step to handle special types. Arrays are treated as "escape sequences" that let us encode other values. For example, JSON does not have an encoding for &lt;code&gt;Date&lt;/code&gt; objects, but Cap'n Web does. You might see a message that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  event: "Birthday Week",
  timestamp: ["date", 1758499200000]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;To encode a literal array, we simply double-wrap it in &lt;code&gt;[]&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  names: [["Alice", "Bob", "Carol"]]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In other words, an array with just one element which is itself an array, evaluates to the inner array literally. An array whose first element is a type name, evaluates to an instance of that type, where the remaining elements are parameters to the type.&lt;/p&gt;
      &lt;p&gt;Note that only a fixed set of types are supported: essentially, "structured clonable" types, and RPC stub types.&lt;/p&gt;
      &lt;p&gt;On top of this basic encoding, we define an RPC protocol inspired by Cap'n Proto â but greatly simplified.&lt;/p&gt;
      &lt;p&gt;Since Cap'n Web is a symmetric protocol, there is no well-defined "client" or "server" at the protocol level. There are just two parties exchanging messages across a connection. Every kind of interaction can happen in either direction.&lt;/p&gt;
      &lt;p&gt;In order to make it easier to describe these interactions, I will refer to the two parties as "Alice" and "Bob".&lt;/p&gt;
      &lt;p&gt;Alice and Bob start the connection by establishing some sort of bidirectional message stream. This may be a WebSocket, but Cap'n Web also allows applications to define their own transports. Each message in the stream is JSON-encoded, as described earlier.&lt;/p&gt;
      &lt;p&gt;Alice and Bob each maintain some state about the connection. In particular, each maintains an "export table", describing all the pass-by-reference objects they have exposed to the other side, and an "import table", describing the references they have received. Alice's exports correspond to Bob's imports, and vice versa. Each entry in the export table has a signed integer ID, which is used to reference it. You can think of these IDs like file descriptors in a POSIX system. Unlike file descriptors, though, IDs can be negative, and an ID is never reused over the lifetime of a connection.&lt;/p&gt;
      &lt;p&gt;At the start of the connection, Alice and Bob each populate their export tables with a single entry, numbered zero, representing their "main" interfaces. Typically, when one side is acting as the "server", they will export their main public RPC interface as ID zero, whereas the "client" will export an empty interface. However, this is up to the application: either side can export whatever they want.&lt;/p&gt;
      &lt;p&gt;From there, new exports are added in two ways:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;When Alice sends a message to Bob that contains within it an object or function reference, Alice adds the target object to her export table. IDs assigned in this case are always negative, starting from -1 and counting downwards.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Alice can send a "push" message to Bob to request that Bob add a value to his export table. The "push" message contains an expression which Bob evaluates, exporting the result. Usually, the expression describes a method call on one of Bob's existing exports â this is how an RPC is made. Each "push" is assigned a positive ID on the export table, starting from 1 and counting upwards. Since positive IDs are only assigned as a result of pushes, Alice can predict the ID of each push she makes, and can immediately use that ID in subsequent messages. This is how promise pipelining is achieved.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;After sending a push message, Alice can subsequently send a "pull" message, which tells Bob that once he is done evaluating the "push", he should proactively serialize the result and send it back to Alice, as a "resolve" (or "reject") message. However, this is optional: Alice may not actually care to receive the return value of an RPC, if Alice only wants to use it in promise pipelining. In fact, the Cap'n Web implementation will only send a "pull" message if the application has actually awaited the returned promise.&lt;/p&gt;
      &lt;p&gt;Putting it together, a code sequence like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = api.getMyName();
let result = await api.hello(namePromise);

console.log(result);&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Might produce a message exchange like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Call api.getByName(). `api` is the server's main export, so has export ID 0.
-&amp;gt; ["push", ["pipeline", 0, "getMyName", []]
// Call api.hello(namePromise). `namePromise` refers to the result of the first push,
// so has ID 1.
-&amp;gt; ["push", ["pipeline", 0, "hello", [["pipeline", 1]]]]
// Ask that the result of the second push be proactively serialized and returned.
-&amp;gt; ["pull", 2]
// Server responds.
&amp;lt;- ["resolve", 2, "Hello, Alice!"]&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;For more details about the protocol, check out the docs.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is new and still highly experimental. There may be bugs to shake out. But, we're already using it today. Cap'n Web is the basis of the recently-launched "remote bindings" feature in Wrangler, allowing a local test instance of workerd to speak RPC to services in production. We've also begun to experiment with it in various frontend applications â expect more blog posts on this in the future.&lt;/p&gt;
      &lt;p&gt;In any case, Cap'n Web is open source, and you can start using it in your own projects now.&lt;/p&gt;
      &lt;p&gt;Check it out on GitHub.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/capnweb-javascript-rpc-library/"/><published>2025-09-22T13:05:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45333021</id><title>Why haven't local-first apps become popular?</title><updated>2025-09-22T20:37:17.581911+00:00</updated><content/><link href="https://marcobambini.substack.com/p/why-local-first-apps-havent-become"/><published>2025-09-22T13:17:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45333978</id><title>What is algebraic about algebraic effects?</title><updated>2025-09-22T20:37:17.034713+00:00</updated><content>&lt;doc fingerprint="2c481533a51a53b2"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;What is Algebraic about Algebraic Effects?&lt;/head&gt;&lt;quote&gt;what does the word "algebraic" mean when used in the context of programming langs?&lt;lb/&gt;- a random tweet&lt;/quote&gt;&lt;p&gt;I'd wondered the same thing about "Algebraic Effects", and was excited to find a talk on YouTube titled What's Algebraic About Algebraic Effects and Handlers? Unfortunately, I'm not the target audience. As an engineer that doesn't shy away from math, it was still out of my depth.&lt;/p&gt;&lt;p&gt;I found some time this past spring looking into Algebraic Effects, and I think I have a decent answer to the question.&lt;/p&gt;&lt;head rend="h3"&gt;Algebra in the context of programming&lt;/head&gt;&lt;p&gt;My view of "Algebra" in the context of programming is a particular kind of compositionality, where there's a structure.&lt;/p&gt;&lt;p&gt;In contrast, mainstream developers often talk about compositionality as just two obj/function that can interoperate due to the same interface, but not much more can be inferred about properties of the interop between the two obj/functions.&lt;/p&gt;&lt;p&gt;So often times, we get some collection of objects/functions that go together in an arbitrary way according to the taste of the developer that wrote it. If they're any good, it feels intuitive. But more often than not, it feels arbitrary. The effect is magnified if you look into the codebase. To a newcomer, it feels like a mess, in the same way that a house built by piling stones high feels like a mess: there's no apparent or easily recognizable structure.&lt;/p&gt;&lt;head rend="h3"&gt;A tangential detour into abstract algebra&lt;/head&gt;&lt;p&gt;In abstract algebra, structure is often where you take some math object 𝛂 (like an int, or matrix), and you pair it with an operation, (like + or *), and you say: integers can be composed with op `+`, but we can ALSO infer properties in these combos--or laws.&lt;/p&gt;&lt;p&gt;So a common one we know is: integer (ℤ) with addition (+) has implied properties that always hold. And the elements (ℤ), the op (+), and the properties together constrain outcomes, and this is what gives us structure. A house with structure feels like it's built with arches, rather than a pile of rocks. What are the properties of (ℤ) and (+)? Due to how ℤ and + are defined, we get these properties:&lt;/p&gt;&lt;p&gt;1. Closure: ℤ + ℤ always gives you another ℤ.&lt;/p&gt;&lt;p&gt;Sometimes devs write code that doesn't give you back the same thing.&lt;/p&gt;&lt;p&gt;2. Associativity: (a + b) + c = a + (b + c) where a, b, c are in ℤ.&lt;/p&gt;&lt;p&gt;This familiar, as they were drilled in grade school. But often devs don't write code that fulfill this property.&lt;/p&gt;&lt;p&gt;The last two are:&lt;/p&gt;&lt;p&gt;3. identity: ℤ has an element that doesn't change when we use +. &lt;lb/&gt;Here, it's zero: a + 0 = a &lt;/p&gt;&lt;p&gt;4. inverse: every ℤ has a matching ℤ that give us the identity when we use + on it: a + (-a) = 0, where a and -a are in ℤ.&lt;/p&gt;&lt;p&gt;Taken together, math peeps gave this kind of structure a name: Groups. So if someone says [a struct] and [an op] together form a group, I can automatically can assume those properties. It's a shorthand.&lt;/p&gt;&lt;p&gt;If you add even more constraints/properties to how ℤ and + behave together, you get another algebraic structure. There's a whole host and families of these. So if we add another constraint, we get an Abelian Group:&lt;/p&gt;&lt;p&gt;5. Commutativity: a+b = b+a, where a, b are in ℤ&lt;/p&gt;&lt;head rend="h3"&gt;Surmounting the network with algebra&lt;/head&gt;&lt;p&gt;Why write constraining data structure and op pairings? It's quite useful if you want to guarantee specific properties of your system. For example, it's well known that syncing is hard, because of the Eight Fallacies of Distributed Systems.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The network is reliable;&lt;/item&gt;&lt;item&gt;Latency is zero;&lt;/item&gt;&lt;item&gt;Bandwidth is infinite;&lt;/item&gt;&lt;item&gt;The network is secure;&lt;/item&gt;&lt;item&gt;Topology doesn't change;&lt;/item&gt;&lt;item&gt;There is one administrator;&lt;/item&gt;&lt;item&gt;Transport cost is zero;&lt;/item&gt;&lt;item&gt;The network is homogeneous.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That means your data, when sent over the network will likely arrive out of order. Worse, clocks can be out of sync, so it can look like data arrived from the future. How can we tame the underlying unreliable system? By constraining our data and operations to have properties.&lt;/p&gt;&lt;p&gt;CRDTs are nowadays used to enforce eventually consistent syncs. It achieves this by pairing a data structure with a merge operation, which together form an algebraic structure called a semi-lattice. The properties of a semi-lattice are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Closure: For all a, b in the set S, the result of a ∘ b is also in S.&lt;/item&gt;&lt;item&gt;Associativity: a ∘ (b ∘ c)=(a ∘ b) ∘ c for all a, b, c ∈ S.&lt;/item&gt;&lt;item&gt;Commutativity: a ∘ b = b ∘ a for all a, b ∈ S.&lt;/item&gt;&lt;item&gt;Idempotence: a ∘ a = a for all a ∈ S.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Together, this is enough to counteract the network mixing up your data when sending it over the network. I wrote about that here:&lt;/p&gt;&lt;p&gt;So by constraining the power of what our code can do, we can ensure the system has specific desirable properties that achieve the goal of syncing data over an unreliable network. It's where we say: "If we compose this kind of data structure in this constrained way with this kind of merge function, then we can guarantee these properties always hold. And with this structure, our data can survive sync over an unreliable network with other syncers."&lt;/p&gt;&lt;head rend="h3"&gt;From Monads to Algebraic Effects&lt;/head&gt;&lt;p&gt;This is why people also like Monads. Monads are about how to compose code, but with specific properties (Monadic laws) so we can achieve some goal in how they compose. I won't go into it here, as this is already long, but that's the core idea.&lt;/p&gt;&lt;p&gt;However, not all types of Monads compose well together. Here's where I'm out of my depth, but I've read and I'm told that this is why there are Monad Transformers, so you can fit different domain Monads together.&lt;/p&gt;&lt;p&gt;Hence, some people have started looking at Algebraic Effects, as a way to achieve the same compositional powers of monads, but in a different way. Most descriptions of Algebraic Effects actually ignore the `algebraic` part, because describing `effects` is already a big leap.&lt;/p&gt;&lt;p&gt;The effects part, is often explained as "resumable exceptions". I wrote a short description of what algebraic effects are from that perspective, so I won't expound on that here.&lt;/p&gt;&lt;p&gt;But the algebraic part of algebraic effects is that the effects that you raise as a "resumable exception" can be composed together! Not just in any way: design them so when composed, they have *guaranteed properties* just like the stuff you saw above!&lt;/p&gt;&lt;p&gt;For example, if we had a key/value store that we interface with using &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;put&lt;/code&gt;, we could express what we expect to happen through some algebraic properties.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Idempotence of consecutive reads (get-get): get k; get k ≡ get x&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This says, two consecutive &lt;code&gt;gets&lt;/code&gt; is functionally equivalent to a single &lt;code&gt;get&lt;/code&gt;. This guarantees that &lt;code&gt;get&lt;/code&gt; is a pure observation: it doesn't consume or advance anything. If this law didn't hold, reading could "drain" or "advance" some hidden cursor. By making it a law, we make it an explicit behavior for our users, so they're not surprised by bugs down the line when their assumptions veer from this property.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Last write wins (put-put): put k v1; put k v2 ≡ put k v2&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Easy. The two &lt;code&gt;puts&lt;/code&gt; together is the functional equivalent of only executing the last one. Hence, the last &lt;code&gt;put&lt;/code&gt; is the value that's currently sitting in key &lt;code&gt;k&lt;/code&gt;. This encodes overwriting semantics, and without it, &lt;code&gt;put&lt;/code&gt; might append, merge, or accumulate. It wouldn't be what users would expect.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Read after write (put-get): put k v; get k ≡ put k v; return v&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Executing a &lt;code&gt;put&lt;/code&gt; and then an immediate &lt;code&gt;get&lt;/code&gt; is the functional equivalent of just executing the put, but then just returning the value &lt;code&gt;v&lt;/code&gt; you already have in hand, instead of executing &lt;code&gt;get&lt;/code&gt;. This is important to guarantee the consistency of reads right after writes. Without this, you could write &lt;code&gt;v&lt;/code&gt; and then not see &lt;code&gt;v&lt;/code&gt; immediately, which would break the intuitive model of state in a key/value store.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Write back same value (get-put): get k &amp;gt;&amp;gt;= (λv. put k v) ≡ return ()&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If you read the value of a key and then immediately write it back unchanged, that's functionally equivalent of doing nothing (returning unit).&lt;/p&gt;&lt;code&gt;&amp;gt;&amp;gt;=&lt;/code&gt; as saying "and then...". So rule 4 in javascript pseudocode might look like:&lt;p&gt;get(store, key).andThen((val) =&amp;gt; put(store, key, val))&lt;/p&gt;&lt;code&gt;return ()&lt;/code&gt;, &lt;code&gt;()&lt;/code&gt; is called &lt;code&gt;unit&lt;/code&gt;, which is the way functional programmers denote "no meaningful value", which is effectively what C programmers use &lt;code&gt;void&lt;/code&gt; for. They're technically different, but in practice, they're used for similar purposes.&lt;list rend="ol"&gt;&lt;item&gt;Independence across keys For &lt;code&gt;k1 ≠ k2&lt;/code&gt;:&lt;/item&gt;&lt;/list&gt;&lt;code&gt;put k1 v1; put k2 v2  ≡  put k2 v2; put k1 v1
get k1; get k2        ≡  get k2; get k1
put k1 v; get k2      ≡  get k2; put k1 v
&lt;/code&gt;&lt;p&gt;Operations on different keys commute, and the store treats each key as an independent cell. This is what makes it a key/value store, rather than some entangled data structure.&lt;/p&gt;&lt;p&gt;Hence, just because you are writing effects, doesn't automatically mean they're algebraic. You have to consciously design them to be so, in order to give properties or guarantees that you want your users to have. Most current programming languages have no way of enforcing these equational axioms, so even esoteric languages that feature algebraic effects don't even try to enforce them.&lt;/p&gt;&lt;p&gt;Languages which feature dependent types, such as Coq, Agda, Idris 2, and Lean are the only languages that can encode these equational axioms explicitly and be able to prove their veracity. Typically, these languages are used by mathematicians to do proofs in math. But interestingly, Lean has been getting a lot of momentum, and it can compile to C. It can be a practical in-road to using these in practice.&lt;/p&gt;&lt;p&gt;And, in my own words, that's what's algebraic about algebraic effects.&lt;/p&gt;&lt;head rend="h3"&gt;Epilogue&lt;/head&gt;&lt;p&gt;Alan Kay was known to lament that 1 million lines in a code base is unconscionable. It's no more a skyscraper than a pile of rocks. That's because there's often no structure. Eventually we figured out arches: they're structure that give strength with less material.&lt;/p&gt;&lt;p&gt;Hence, we can build higher without using more material. By analogy, we're starting to discover what this structure looks like in software. And it looks like math. There's a lot of resistance to this, and will be for a long time.&lt;/p&gt;&lt;p&gt;And maybe with LLMs, it might not matter for a wide swath of applications. But still, there's ever progress moving forward in this direction, where these pure functional programming or math-y ideas filter down to more mainstream languages.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://interjectedfuture.com/what-is-algebraic-about-algebraic-effects/"/><published>2025-09-22T14:30:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45334250</id><title>A simple way to measure knots has come unraveled</title><updated>2025-09-22T20:37:16.636972+00:00</updated><content>&lt;doc fingerprint="a77c8316c9202f75"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Simple Way To Measure Knots Has Come Unraveled&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In 1876, Peter Guthrie Tait set out to measure what he called the “beknottedness” of knots.&lt;/p&gt;
    &lt;p&gt;The Scottish mathematician, whose research laid the foundation for modern knot theory, was trying to find a way to tell knots apart — a notoriously difficult task. In math, a knot is a tangled piece of string with its ends glued together. Two knots are the same if you can twist and stretch one into the other without cutting the string. But it’s hard to tell if this is possible based solely on what the knots look like. A knot that seems really complicated and tangled, for instance, might actually be equivalent to a simple loop.&lt;/p&gt;
    &lt;p&gt;Tait had an idea for how to determine if two knots are different. First, lay a knot flat on a table and find a spot where the string crosses over itself. Cut the string, swap the positions of the strands, and glue everything back together. This is called a crossing change. If you do this enough times, you’ll be left with an unknotted circle. Tait’s beknottedness is the minimum number of crossing changes that this process requires. Today, it’s known as a knot’s “unknotting number.”&lt;/p&gt;
    &lt;p&gt;If two knots have different unknotting numbers, then they must be different. But Tait found that his unknotting numbers generated more questions than they answered.&lt;/p&gt;
    &lt;p&gt;“I have got so thoroughly on one groove,” he wrote in a letter to a friend, the scientist James Clerk Maxwell, “that I fear I may be missing or unduly exalting something which will appear excessively simple to anyone but myself.”&lt;/p&gt;
    &lt;p&gt;If Tait missed something, so did every mathematician who followed him. Over the past 150 years, many knot theorists have been baffled by the unknotting number. They know it can provide a powerful description of a knot. “It’s the most fundamental [measure] of all, arguably,” said Susan Hermiller of the University of Nebraska. But it’s often impossibly hard to compute a knot’s unknotting number, and it’s not always clear how that number corresponds to the knot’s complexity.&lt;/p&gt;
    &lt;p&gt;To untangle this mystery, mathematicians in the early 20th century devised a straightforward conjecture about how the unknotting number changes when you combine knots. If they could prove it, they would have a way to compute the unknotting number for any knot — giving mathematicians a simple, concrete way to measure knot complexity.&lt;/p&gt;
    &lt;p&gt;Researchers searched for nearly a century, finding little evidence either for or against the conjecture.&lt;/p&gt;
    &lt;p&gt;Then, in a paper posted in June, Hermiller and her longtime collaborator Mark Brittenham uncovered a pair of knots that, when combined, form a knot that is easier to untie than the conjecture predicts. In doing so, they disproved the conjecture — and used their counterexample to find infinitely many other pairs of knots that also disprove it.&lt;/p&gt;
    &lt;p&gt;“When the paper was posted, I gasped out loud,” said Allison Moore of Virginia Commonwealth University.&lt;/p&gt;
    &lt;p&gt;The result demonstrates that “the unknotting number is chaotic and unpredictable and really exciting to study,” she added. The paper is “like waving a flag that says, we don’t understand this.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Unknotting and the Great Unknown&lt;/head&gt;
    &lt;p&gt;The conjecture dates back to at least 1937, when the German mathematician Hilmar Wendt set out to understand what happens when you add knots together — that is, when you tie both of them with the same string before gluing the ends together. (Mathematicians call this combined knot the “connect sum.”) Wendt thought that the unknotting number of the resulting knot should always be the sum of the unknotting numbers of the two original knots.&lt;/p&gt;
    &lt;p&gt;His prediction, now known as the additivity conjecture, makes sense. Say you add the two knots above, whose unknotting numbers are known to be 2 and 3. That means that there’s a sequence of two crossing changes that unknots the lefthand side of the connect sum, and a sequence of three crossing changes that unknots the righthand side. If you use these sequences, you can unknot the whole thing in 2 + 3, or 5, crossing changes.&lt;/p&gt;
    &lt;p&gt;But this only tells you that the connect sum’s unknotting number is no bigger than 5. You might be able to find a sequence of crossing changes that’s more efficient than untying each side individually. That is, there might be a knot that really is less than the sum of its parts.&lt;/p&gt;
    &lt;p&gt;To settle the additivity conjecture, mathematicians had to either find a connect sum with a shorter unknotting sequence or prove that no such example exists. In either case, they didn’t have a clue where to begin.&lt;/p&gt;
    &lt;p&gt;Part of the problem was that the way you lay out your knot — what mathematicians call a “diagram” — determines where and how the knot crosses over itself. There are lots of diagrams that can represent the same knot. To find the shortest sequence of crossing changes, you might have to choose just the right diagram. Often, it’s not the one you’d normally associate with the knot.&lt;/p&gt;
    &lt;p&gt;“There are unimaginably large numbers of ways to try and imagine changing your diagram before you decide to introduce the crossing change,” Brittenham said. “We don’t, at least at the start, have any control over how complicated the picture has to look.”&lt;/p&gt;
    &lt;p&gt;In 1985, the mathematician Martin Scharlemann finally made some headway when he proved that for any two knots whose unknotting number is 1, the connect sum will always have an unknotting number of 2. “That made [the whole conjecture] seem much more likely,” said Charles Livingston of Indiana University.&lt;/p&gt;
    &lt;p&gt;The result offered tantalizing evidence that the universe of knots could be neatly organized. That’s because all knots can be built out of a smaller class of “prime” knots. The additivity conjecture implied that once you knew the unknotting numbers of those prime knots, you would know them for all knots. Any information you might want about a given knot would fall naturally out of that much simpler set.&lt;/p&gt;
    &lt;p&gt;Mathematicians wanted the conjecture to be true, said Arunima Ray of the University of Melbourne, “because that would be like, there’s order in the world.”&lt;/p&gt;
    &lt;p&gt;Scharlemann’s result was later extended to other classes of knots. But it wasn’t clear that it would apply to all knots.&lt;/p&gt;
    &lt;p&gt;Then Brittenham and Hermiller convened a cluster of computers to help.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sneakernet&lt;/head&gt;
    &lt;p&gt;The pair began their project a decade ago with a broader aim: to use computers to learn whatever they could about the unknotting number.&lt;/p&gt;
    &lt;p&gt;They turned to software known as SnapPy, which uses sophisticated geometric techniques to test whether two pictures depict the same knot. Just a few years earlier, SnapPy had vastly expanded its database, enabling it to identify nearly 60,000 unique knots.&lt;/p&gt;
    &lt;p&gt;It was perfectly suited for what Brittenham and Hermiller had in mind. They started with a single complicated knot and applied every imaginable crossing change to it, producing scores of new knots. They then used SnapPy to identify those knots — and repeated the process.&lt;/p&gt;
    &lt;p&gt;They did this for millions of knot diagrams that corresponded to hundreds of thousands of knots. Ultimately, they assembled an enormous library of information about unknotting sequences and calculated upper bounds on the unknotting numbers of thousands of knots. The work required a lot of computing power: The pair signed up for supercomputing time at the University of Nebraska’s computing center, while also running their program on old laptops they’d bought at an auction. All told, they were managing dozens of computers. “We had a bit of a sneakernet,” Brittenham said, “where you transfer information from computer to computer by walking between them.”&lt;/p&gt;
    &lt;p&gt;The duo kept their program running in the background for over a decade. During that time, a couple of computers from their ragtag collection succumbed to overheating and even flames. “There was one that actually sent out sparks,” Brittenham said. “That was kind of fun.” (Those machines, he added, were “honorably retired.”)&lt;/p&gt;
    &lt;p&gt;Then, in the fall of 2024, a paper about a failed attempt to use machine learning to disprove the additivity conjecture caught Brittenham and Hermiller’s attention. Perhaps, they thought, machine learning wasn’t the best approach for this particular problem: If a counterexample to the additivity conjecture was out there, it would be “a needle in a haystack,” Hermiller said. “That’s not quite what things like machine learning are about. They’re about trying to find patterns in things.”&lt;/p&gt;
    &lt;p&gt;But it reinforced a suspicion the pair already had — that maybe their more carefully honed sneakernet could find the needle.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Tie That Binds&lt;/head&gt;
    &lt;p&gt;Brittenham and Hermiller realized they could make use of the unknotting sequences they’d uncovered to look for potential counterexamples to the additivity conjecture.&lt;/p&gt;
    &lt;p&gt;Imagine again that you have two knots whose unknotting numbers are 2 and 3, and you’re trying to unknot their connect sum. After one crossing change, you get a new knot. If the additivity conjecture is to be believed, then the original knot’s unknotting number should be 5, and this new knot’s should be 4.&lt;/p&gt;
    &lt;p&gt;But what if this new knot’s unknotting number is already known to be 3? That implies that the original knot can be untied in just four steps, breaking the conjecture.&lt;/p&gt;
    &lt;p&gt;“We get these middle knots,” Brittenham said. “What can we learn from them?”&lt;/p&gt;
    &lt;p&gt;He and Hermiller already had the perfect tool for the occasion humming away on their suite of laptops: the database they’d spent the previous decade developing, with its upper bounds on the unknotting numbers of thousands of knots.&lt;/p&gt;
    &lt;p&gt;The mathematicians started to add pairs of knots and work through the unknotting sequences of their connect sums. They focused on connect sums whose unknotting numbers had only been approximated in the loosest sense, with a big gap between their highest and lowest possible values. But that still left them with a massive list of knots to work through — “definitely in the tens of millions, and probably in the hundreds of millions,” Brittenham said.&lt;/p&gt;
    &lt;p&gt;For months, their computer program applied crossing changes to these knots and compared the resulting knots to those in their database. One day in late spring, Brittenham checked the program’s output files, as he did most days, to see if anything interesting had turned up. To his great surprise, there was a line of text: “CONNECT SUM BROKEN.” It was a message he and Hermiller had coded into the program — but they’d never expected to actually see it.&lt;/p&gt;
    &lt;p&gt;Initially, they were doubtful of the result. “The very first thing that went through our heads was there was something wrong with our programming,” Brittenham said.&lt;/p&gt;
    &lt;p&gt;“We just dropped absolutely everything else,” Hermiller recalled. “All of life just went away. Eating, sleeping got annoying.”&lt;/p&gt;
    &lt;p&gt;But their program checked out. They even tied the knot it had identified in a rope, then worked through the unknotting procedure by hand, just to make sure.&lt;/p&gt;
    &lt;p&gt;Their counterexample was real.&lt;/p&gt;
    &lt;head rend="h2"&gt;Twisted Mysteries&lt;/head&gt;
    &lt;p&gt;The counterexample Brittenham and Hermiller found is built out of two copies of a knot called the (2, 7) torus knot. This knot is made by winding two strings around each other three and a half times and then gluing their opposing ends together. Its mirror image is made by winding three and a half times in the other direction.&lt;/p&gt;
    &lt;p&gt;The unknotting number of both the (2, 7) torus knot and its mirror image is 3. But Brittenham and Hermiller’s program found that if you add these knots, you can unknot the result in just five steps — not six, as the additivity conjecture predicted.&lt;/p&gt;
    &lt;p&gt;“It’s a shockingly simple counterexample,” Moore said. “It goes back to that unpredictability of the crossing change.”&lt;/p&gt;
    &lt;p&gt;The result led Brittenham and Hermiller to an infinite list of other counterexamples, including almost any knot that’s built by winding two strings and gluing.&lt;/p&gt;
    &lt;p&gt;Now, with the additivity conjecture decisively struck down, the knot theory community has a wide world to explore.&lt;/p&gt;
    &lt;p&gt;For some mathematicians, the new result brings disappointment. It reveals that there’s less structure in the world of knots than they had hoped for. The unknotting number is “not as well behaved as we would like,” Ray said. “That’s a bit sad.”&lt;/p&gt;
    &lt;p&gt;But from another perspective, that only makes the unknotting number more intriguing. “There’s just much more complexity and unknowns about knot theory than we knew there were a few months ago,” Livingston said.&lt;/p&gt;
    &lt;p&gt;The nature of that additional complexity isn’t clear yet. During their furious examination of their counterexample, Brittenham and Hermiller weren’t able to develop an intuition for why it broke the additivity conjecture when other knots didn’t. Understanding this could help mathematicians get a better handle on what makes some knots complex and others less so.&lt;/p&gt;
    &lt;p&gt;“I’m still stymied by this most basic question” about the unknotting number, Moore said. “That just lights the fire under you.”&lt;/p&gt;
    &lt;p&gt;Editor’s Note: Brittenham and Hermiller’s research was funded in part by the Simons Foundation, which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/a-simple-way-to-measure-knots-has-come-unraveled-20250922/"/><published>2025-09-22T14:49:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45334545</id><title>PlanetScale for Postgres is now GA</title><updated>2025-09-22T20:37:16.474773+00:00</updated><content>&lt;doc fingerprint="af51e4d16e9161c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PlanetScale for Postgres is now GA&lt;/head&gt;
    &lt;p&gt;By Sam Lambert |&lt;/p&gt;
    &lt;p&gt;PlanetScale for Postgres is now generally available and out of private preview. To create a Postgres database, sign up or log in to your PlanetScale account, create a new database, and select Postgres. If you are looking to migrate from another Postgres provider to PlanetScale, you can use our migration guides to get started. Finally, if you have a large or complex migration, we can help you via our sales team at postgres@planetscale.com.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is PlanetScale for Postgres?&lt;/head&gt;
    &lt;p&gt;Our mission is simple: bring you the fastest and most reliable databases with the best developer experience. We have done this for 5 years now with our managed Vitess product, allowing companies like Cursor, Intercom, and Block to scale beyond previous limits.&lt;/p&gt;
    &lt;p&gt;We are so excited to bring this to Postgres. Our proprietary operator allows us to bring the maturity of PlanetScale and the performance of Metal to an even wider audience. We bring you the best of Postgres and the best of PlanetScale in one product.&lt;/p&gt;
    &lt;head rend="h2"&gt;Customers on PlanetScale for Postgres&lt;/head&gt;
    &lt;p&gt;Hundreds of companies already trust PlanetScale for Postgres to power their production workloads. We say this every time we launch something, but we prefer you hear about real-world usage straight from our customers. Read through some of their stories about their migration to PlanetScale for Postgres below.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Convex: Powered by PlanetScale&lt;/item&gt;
      &lt;item&gt;Supermemory just got faster on PlanetScale&lt;/item&gt;
      &lt;item&gt;Scaling RealâTime Discovery: Inside Layersâ PlanetScale Migration&lt;/item&gt;
      &lt;item&gt;Why We Migrated from Neon to PlanetScale&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Vitess for Postgres&lt;/head&gt;
    &lt;p&gt;Neki is our Postgres sharding solution. Built by the team behind Vitess combining the best of Vitess and Postgres. Neki is not a fork of Vitess. Vitessâ achievements are enabled by leveraging MySQLâs strengths and engineering around its weaknesses. To achieve Vitessâ power for Postgres we are architecting from first principles and building alongside design partners at scale. When we are ready we will release Neki as an open source project suitable for running the most demanding Postgres workloads. To sign up for the Neki waitlist visit neki.dev.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://planetscale.com/blog/planetscale-for-postgres-is-generally-available"/><published>2025-09-22T15:10:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335129</id><title>Human-Oriented Markup Language</title><updated>2025-09-22T20:37:16.403966+00:00</updated><content>&lt;doc fingerprint="57f920aaa3d8d650"&gt;
  &lt;main&gt;
    &lt;code&gt;# A sample HUML document.
website::
  hostname: "huml.io"
  ports:: 80, 443 # Inline list.
  enabled: true
  factor: 3.14
  props:: mime_type: "text/html", encoding: "gzip" # Inline dict.
  tags:: # Multi-line list.
    - "markup"
    - "webpage"
    - "schema"

haikus::
  one: """
    A quiet language
    Lines fall into their places
    Nothing out of place
  """
&lt;/code&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HUML was primarily born out of the numerous frustrations with YAML, where one easy-to-miss, accidental indentation change can dangerously alter the semantics of a document.&lt;/item&gt;
      &lt;item&gt;Other popular markup languages such as TOML and HCL are configuration-oriented. NestedText is an interesting approach, but is too primitive to be suitable for wider use cases. JSON is universal, but lacks comments, does not have a strict form for consistent readability across contexts, and has bracket-matching and formatting woes which make human editing difficult.&lt;/item&gt;
      &lt;item&gt;Of these, YAML is the one that comes closest to indicating structure and hierarchy visually.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ultimately, a new markup language is a subjective endeavor (it was even in 2001, as evidenced by YAML's original name, Yet Another ...). HUML looks like YAML, but borrows characteristics from many existing languages with the primary focus on enforcing human readability and consistency across contexts.&lt;/p&gt;
    &lt;p&gt;Still, why YET another markup language? Why not?&lt;/p&gt;
    &lt;head rend="h2"&gt;Goals&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ensure human readability and editability above all else.&lt;/item&gt;
      &lt;item&gt;Enable visual comprehension of data structures and hierarchies.&lt;/item&gt;
      &lt;item&gt;Avoid footguns and ambiguities in syntax and data types.&lt;/item&gt;
      &lt;item&gt;Provide as few ways as possible—preferably one—of representing something.&lt;/item&gt;
      &lt;item&gt;Maintain strict formatting for uniformity, eliminating the need for formatters.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://huml.io/"/><published>2025-09-22T15:48:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335452</id><title>SWE-Bench Pro</title><updated>2025-09-22T20:37:16.263560+00:00</updated><content>&lt;doc fingerprint="fe13d194cc7994e"&gt;
  &lt;main&gt;
    &lt;p&gt;Code and data for the following works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;SWE-bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HuggingFace: https://huggingface.co/datasets/ScaleAI/SWE-bench_Pro&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Public Leaderboard: https://scale.com/leaderboard/swe_bench_pro_public&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Commercial (Private) Leaderboard: https://scale.com/leaderboard/swe_bench_pro_commercial&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SWE-Bench Pro is a challenging benchmark evaluating LLMs/Agents on long-horizon software engineering tasks. Given a codebase and an issue, a language model is tasked with generating a patch that resolves the described problem.&lt;/p&gt;
    &lt;p&gt;The dataset is inspired from SWE-Bench: https://github.com/SWE-bench/SWE-bench&lt;/p&gt;
    &lt;p&gt;To access SWE-bench Pro, copy and run the following code:&lt;/p&gt;
    &lt;code&gt;from datasets import load_dataset
swebench = load_dataset('ScaleAI/SWE-bench_Pro', split='test')&lt;/code&gt;
    &lt;p&gt;SWE-bench Pro uses Docker for reproducible evaluations. In addition, the evaluation script requires Modal to scale the evaluation set.&lt;/p&gt;
    &lt;p&gt;Follow the instructions in the Docker setup guide to install Docker on your machine. If you're setting up on Linux, we recommend seeing the post-installation steps as well.&lt;/p&gt;
    &lt;p&gt;Run the following commands to store modal credentials:&lt;/p&gt;
    &lt;code&gt;pip install modal
modalv setup # and follow the prompts to generate your token and secret
&lt;/code&gt;
    &lt;p&gt;After running these steps, you should be able to see a token ID and secret in &lt;code&gt;~/.modal.toml&lt;/code&gt;:
EG:&lt;/p&gt;
    &lt;code&gt;token_id = &amp;lt;token id&amp;gt;
token_secret = &amp;lt;token secret&amp;gt;
active = true
&lt;/code&gt;
    &lt;p&gt;We store prebuilt Docker images for each instance. They can be found in this directory:&lt;/p&gt;
    &lt;p&gt;https://hub.docker.com/r/jefzda/sweap-images&lt;/p&gt;
    &lt;p&gt;The format of the images is as follows.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;jefzda/sweap-images:{repo_base}.{repo_name}-{repo_base}__{repo_name}-{hash}&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;jefzda/sweap-images:gravitational.teleport-gravitational__teleport-82185f232ae8974258397e121b3bc2ed0c3729ed-v626ec2a48416b10a88641359a169d99e935ff03&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;First generate patch predictions using your harness of choice. Evaluate patch predictions on SWE-bench Pro with the following command:&lt;/p&gt;
    &lt;code&gt;python sweap_pro_eval_modal.py \
    --raw_sample_path=external_hf_v2.csv \
    --patch_path={OUTPUT}/gold_patches.json \
    --output_dir={OUTPUT}/ \
    --scripts_dir=run_scripts \
    --num_workers=100 \
    --dockerhub_username=your-username&lt;/code&gt;
    &lt;p&gt;Replace gold_patches with your patch json, and point raw_sample_path to the SWE-Bench Pro CSV.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/scaleapi/SWE-bench_Pro-os"/><published>2025-09-22T16:08:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335474</id><title>OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems</title><updated>2025-09-22T20:37:15.642961+00:00</updated><content>&lt;doc fingerprint="a9561c5e408ccdb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenAI and NVIDIA announce strategic partnership to deploy 10 gigawatts of NVIDIA systems&lt;/head&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strategic partnership enables OpenAI to build and deploy at least 10 gigawatts of AI datacenters with NVIDIA systems representing millions of GPUs for OpenAI’s next-generation AI infrastructure.&lt;/item&gt;
      &lt;item&gt;To support the partnership, NVIDIA intends to invest up to $100 billion in OpenAI progressively as each gigawatt is deployed.&lt;/item&gt;
      &lt;item&gt;The first gigawatt of NVIDIA systems will be deployed in the second half of 2026 on NVIDIA’s Vera Rubin platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;San Francisco and Santa Clara—September 22, 2025—NVIDIA and OpenAI today announced a letter of intent for a landmark strategic partnership to deploy at least 10 gigawatts of NVIDIA systems for OpenAI’s next-generation AI infrastructure to train and run its next generation of models on the path to deploying superintelligence. To support this deployment including datacenter and power capacity, NVIDIA intends to invest up to $100 billion in OpenAI as the new NVIDIA systems are deployed. The first phase is targeted to come online in the second half of 2026 using NVIDIA’s Vera Rubin platform.&lt;/p&gt;
    &lt;p&gt;“NVIDIA and OpenAI have pushed each other for a decade, from the first DGX supercomputer to the breakthrough of ChatGPT,” said Jensen Huang, founder and CEO of NVIDIA. “This investment and infrastructure partnership mark the next leap forward—deploying 10 gigawatts to power the next era of intelligence.”&lt;/p&gt;
    &lt;p&gt;“Everything starts with compute,” said Sam Altman, co-founder and CEO of OpenAI. “Compute infrastructure will be the basis for the economy of the future, and we will utilize what we’re building with NVIDIA to both create new AI breakthroughs and empower people and businesses with them at scale.”&lt;/p&gt;
    &lt;p&gt;“We’ve been working closely with NVIDIA since the early days of OpenAI,” said Greg Brockman, co-founder and President of OpenAI. “We’ve utilized their platform to create AI systems that hundreds of millions of people use every day. We’re excited to deploy 10 gigawatts of compute with NVIDIA to push back the frontier of intelligence and scale the benefits of this technology to everyone.”&lt;/p&gt;
    &lt;p&gt;OpenAI will work with NVIDIA as a preferred strategic compute and networking partner for its AI factory growth plans. OpenAI and NVIDIA will work together to co-optimize their roadmaps for OpenAI's model and infrastructure software and NVIDIA’s hardware and software.&lt;/p&gt;
    &lt;p&gt;This partnership complements the deep work OpenAI and NVIDIA are already doing with a broad network of collaborators, including Microsoft, Oracle, SoftBank, and Stargate partners, focused on building the world’s most advanced AI infrastructure.&lt;/p&gt;
    &lt;p&gt;OpenAI has grown to over 700 million weekly active users and strong adoption across global enterprises, small businesses, and developers. This partnership will help OpenAI advance its mission to build artificial general intelligence that benefits all of humanity.&lt;lb/&gt;NVIDIA and OpenAI look forward to finalizing the details of this new phase of strategic partnership in the coming weeks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/openai-nvidia-systems-partnership/"/><published>2025-09-22T16:10:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335635</id><title>Testing is better than data structures and algorithms</title><updated>2025-09-22T20:37:15.494028+00:00</updated><content>&lt;doc fingerprint="bf6210976d050c95"&gt;
  &lt;main&gt;
    &lt;p&gt;People should spend less time learning DSA, more time learning testing.&lt;/p&gt;
    &lt;p&gt;I see new learners asking about “DSA” a lot. Data Structures and Algorithms are of course important: considered broadly, they are the two ingredients that make up all programs. But in my opinion, “DSA” as an abstract field of study is over-emphasized.&lt;/p&gt;
    &lt;p&gt;I understand why people focus on DSA: it’s a concrete thing to learn about, there are web sites devoted to testing you on it, and most importantly, because job interviews often involve DSA coding questions.&lt;/p&gt;
    &lt;p&gt;Before I get to other opinions, let me make clear that anything you can do to help you get a job is a good thing to do. If grinding leetcode will land you a position, then do it.&lt;/p&gt;
    &lt;p&gt;But I hope companies hiring entry-level engineers aren’t asking them to reverse linked lists or balance trees. Asking about techniques that can be memorized ahead of time won’t tell them anything about how well you can work. The stated purpose of those interviews is to see how well you can figure out solutions, in which case memorization will defeat the point.&lt;/p&gt;
    &lt;p&gt;The thing new learners don’t understand about DSA is that actual software engineering almost never involves implementing the kinds of algorithms that “DSA” teaches you. Sure, it can be helpful to work through some of these puzzles and see how they are solved, but writing real code just doesn’t involve writing that kind of code.&lt;/p&gt;
    &lt;p&gt;Here is what I think in-the-trenches software engineers should know about data structures and algorithms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data structures are ways to organize data. Learn some of the basics: linked list, array, hash table, tree. By “learn” I mean understand what it does and why you might want to use one.&lt;/item&gt;
      &lt;item&gt;Different data structures can be used to organize the same data in different ways. Learn some of the trade-offs between structures that are similar.&lt;/item&gt;
      &lt;item&gt;Algorithms are ways of manipulating data. I don’t mean named algorithms like Quicksort, but algorithms as any chunk of code that works on data and does something with it.&lt;/item&gt;
      &lt;item&gt;How you organize data affects what algorithms you can use to work with the data. Some data structures will be slow for some operations where another structure will be fast.&lt;/item&gt;
      &lt;item&gt;Algorithms have a “time complexity” (Big O): how the code slows as the data grows. Get a sense of what this means.&lt;/item&gt;
      &lt;item&gt;Python has a number of built-in data structures. Learn how they work, and the time complexity of their operations.&lt;/item&gt;
      &lt;item&gt;Learn how to think about your code to understand its time complexity.&lt;/item&gt;
      &lt;item&gt;Read a little about more esoteric things like Bloom filters, so you can find them later in the unlikely case you need them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some things you don’t need to learn:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The details of a dozen different sorting algorithms. Look at two to see different ways of approaching the same problem, then move on.&lt;/item&gt;
      &lt;item&gt;The names of “important” algorithms. Those have all been implemented for you.&lt;/item&gt;
      &lt;item&gt;The answers to all N problems on some quiz web site. You won’t be asked these exact questions, and they won’t come up in your real work. Again: try a few to get a feel for how some algorithms work. The exact answers are not what you need.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course some engineers need to implement hash tables, or sorting algorithms or whatever. We love those engineers: they write libraries we can use off the shelf so we don’t have to implement them ourselves.&lt;/p&gt;
    &lt;p&gt;There have been times when I implemented something that felt like An Algorithm (for example, Finding fuzzy floats), but it was more about considering another perspective on my data, looking at the time complexity, and moving operations around to avoid quadratic behavior. It wasn’t opening a textbook to find the famous algorithm that would solve my problem.&lt;/p&gt;
    &lt;p&gt;Again: if it will help you get a job, deep-study DSA. But don’t be disappointed when you don’t use it on the job.&lt;/p&gt;
    &lt;p&gt;If you want to prepare yourself for a career, and also stand out in job interviews, learn how to write tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This will be a skill you use constantly. Real-world software means writing tests much more than school teaches you to.&lt;/item&gt;
      &lt;item&gt;In a job search, testing experience will stand out more than DSA depth. It shows you’ve thought about what it takes to write high-quality software instead of just academic exercises.&lt;/item&gt;
      &lt;item&gt;It’s not obvious how to test code well. It’s a puzzle and a problem to solve. If you like figuring out solutions to tricky questions, focus on how to write code so that it can be tested, and how to test it.&lt;/item&gt;
      &lt;item&gt;Testing not only gives you more confidence in your code, it helps you write better code in the first place.&lt;/item&gt;
      &lt;item&gt;Testing applies everywhere, from tiny bits of code to entire architectures, assisting you in design and implementation at all scales.&lt;/item&gt;
      &lt;item&gt;If pursued diligently, testing is an engineering discipline in its own right, with a fascinating array of tools and techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less DSA, more testing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nedbatchelder.com/blog/202509/testing_is_better_than_dsa.html"/><published>2025-09-22T16:21:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45335774</id><title>California issues fine over lawyer's ChatGPT fabrications</title><updated>2025-09-22T20:37:15.258542+00:00</updated><content>&lt;doc fingerprint="d0399a088fc1a31c"&gt;
  &lt;main&gt;
    &lt;p&gt;In summary&lt;/p&gt;
    &lt;p&gt;The court of appeals said 21 of 23 quotes in an opening brief were fake. State authorities are scrambling to grapple with widespread use of artificial intelligence.&lt;/p&gt;
    &lt;p&gt;A California attorney must pay a $10,000 fine for filing a state court appeal full of fake quotations generated by the artificial intelligence tool ChatGPT.&lt;/p&gt;
    &lt;p&gt;The fine appears to be the largest issued over AI fabrications by a California court and came with a blistering opinion stating that 21 of 23 quotes from cases cited in the attorney’s opening brief were made up. It also noted that numerous out-of-state and federal courts have confronted attorneys for citing fake legal authority.&lt;/p&gt;
    &lt;p&gt;“We therefore publish this opinion as a warning,” it continued. “Simply stated, no brief, pleading, motion, or any other paper filed in any court should contain any citations— whether provided by generative AI or any other source—that the attorney responsible for submitting the pleading has not personally read and verified.”&lt;/p&gt;
    &lt;p&gt;The opinion, issued 10 days ago in California’s 2nd District Court of Appeal, is a clear example of why the state’s legal authorities are scrambling to regulate the use of AI in the judiciary. The state’s Judicial Council two weeks ago issued guidelines requiring judges and court staff to either ban generative AI or adopt a generative AI use policy by Dec. 15. Meanwhile, the California Bar Association is considering whether to strengthen its code of conduct to account for various forms of AI following a request by the California Supreme Court last month.&lt;/p&gt;
    &lt;p&gt;The Los Angeles-area attorney fined last week, Amir Mostafavi, told the court that he did not read text generated by the AI model before submitting the appeal in July 2023, months after OpenAI marketed ChatGPT as capable of passing the bar exam. A three-judge panel fined him for filing a frivolous appeal, violating court rules, citing fake cases, and wasting the court’s time and the taxpayers money, according to the opinion.&lt;/p&gt;
    &lt;p&gt;Mostafavi told CalMatters he wrote the appeal and then used ChatGPT to try and improve it. He said that he didn’t know it would add case citations or make things up.&lt;/p&gt;
    &lt;p&gt;He thinks it is unrealistic to expect lawyers to stop using AI. It’s become an important tool just as online databases largely replaced law libraries and, until AI systems stop hallucinating fake information, he suggests lawyers who use AI to proceed with caution.&lt;/p&gt;
    &lt;p&gt;“In the meantime we’re going to have some victims, we’re going to have some damages, we’re going to have some wreckages,” he said. “I hope this example will help others not fall into the hole. I’m paying the price.”&lt;/p&gt;
    &lt;p&gt;The fine issued to Mostafavi is the most costly penalty issued to an attorney by a California state court and one of the highest fines ever issued over attorney use of AI, according to Damien Charlotin, who teaches a class on AI and the law at a business school in Paris. He tracks instances of attorneys citing fake cases, primarily in Australia, Canada, the United States, and the United Kingdom.&lt;/p&gt;
    &lt;p&gt;In a widely-publicized case in May, a U.S. district court judge in California ordered two law firms to pay $31,100 in fees to defense counsel and the court for costs associated with using “bogus AI-generated research.” In that ruling, the judge described feeling misled, said they almost cited fake material in a judicial order and said “Strong deterrence is needed to make sure that attorneys don’t succumb to this easy shortcut.”&lt;/p&gt;
    &lt;p&gt;Charlotin thinks courts and the public should expect to see an exponential rise in these cases in the future. When he started tracking court filings involving AI and fake cases earlier this year, he encountered a few cases a month. Now he sees a few cases a day. Large language models confidently state falsehoods as facts, particularly when there are no supporting facts.&lt;/p&gt;
    &lt;p&gt;“The harder your legal argument is to make, the more the model will tend to hallucinate, because they will try to please you,” he said. “That’s where the confirmation bias kicks in.”&lt;/p&gt;
    &lt;p&gt;A May 2024 analysis by Stanford University’s RegLab found that although three out of four lawyers plan to use generative AI in their practice, some forms of AI generate hallucinations in one out of three queries. Detecting fake material cited in legal filings could get harder as models grow in size.&lt;/p&gt;
    &lt;p&gt;Another tracker of cases where lawyers cite nonexistent legal authority due to use of AI identifies 52 such cases in California and more than 600 nationwide. That amount is expected to increase in the near future because AI innovation is outpacing the education of attorneys, said Nicholas Sanctis, a law student at Capital University Law School in Ohio.&lt;/p&gt;
    &lt;p&gt;Jenny Wondracek, who leads the tracker project, said she expects this trend to get worse because she still regularly encounters lawyers who don’t know that AI makes things up or believe that legal tech tools can eliminate all fake or false material generated by language models.&lt;/p&gt;
    &lt;p&gt;“I think we’d see a reduction if (lawyers) just understood the basics of the technology,” she said.&lt;/p&gt;
    &lt;p&gt;Like Charlotin, she suspects there are more instances of made up cases generated by AI in state court filings than in federal courts, but a lack of standard filing methods makes it difficult to verify that. She said she encounters fake cases most often among overburdened attorneys or people who choose to represent themselves in family court.&lt;/p&gt;
    &lt;p&gt;She suspects the number of arguments filed by attorneys that use AI and cite fake cases will continue to go up, but added that not just attorneys engage in the practice. In recent weeks, she’s documented three instances of judges citing fake legal authority in their decisions.&lt;/p&gt;
    &lt;p&gt;As California considers how to treat generative AI and fake case citations, Wondracek said they can consider approaches taken by other states, such as temporary suspensions, requiring attorneys who get caught to take courses to better understand how to ethically use AI, or requiring them to teach law students how they can avoid making the same mistake.&lt;/p&gt;
    &lt;p&gt;Mark McKenna, codirector of the UCLA Institute of Technology, Law &amp;amp; Policy praised fines like the one against Mostafavi as punishing lawyers for “an abdication of your responsibility as a party representing someone.” He thinks the problem “will get worse before it gets better,” because there’s been a rush among law schools and private firms to adopt AI without thinking through the appropriate way to use them.&lt;/p&gt;
    &lt;p&gt;UCLA School of Law professor Andrew Selbst agrees, pointing out that clerks that work for judges are recent law school graduates, and students are getting bombarded with the message that they must use AI or get left behind. Educators and other professionals report feeling similar pressures.&lt;/p&gt;
    &lt;p&gt;“This is getting shoved down all our throats,” he said. “It’s being pushed in firms and schools and a lot of places and we have not yet grappled with the consequences of that.”&lt;/p&gt;
    &lt;p&gt;For the record: The fine issued to Mostafavi was for $10,000. Due to an editing error, an earlier version of this article had an incorrect figure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/"/><published>2025-09-22T16:30:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45336282</id><title>Mentra (YC W25) Is Hiring to build smart glasses</title><updated>2025-09-22T20:37:14.872358+00:00</updated><content>&lt;doc fingerprint="c1fe11d2919f8da2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;We’re building the OS for smart glasses because we believe glasses are the next personal computer.&lt;/p&gt;
      &lt;p&gt;This year we launched MentraOS, graduated Y Combinator, and raised an $8M seed round to bring smart glasses software and hardware to market.&lt;/p&gt;
      &lt;p&gt;We're a small team (~11 people) shipping thousands of our first pair of smart glasses in December.&lt;/p&gt;
      &lt;p&gt;We need help in engineering (build smart glasses), design (design glasses interfaces), and growth (make glasses go viral).&lt;/p&gt;
      &lt;p&gt;Apply on the job board or if you don't see a fitting role, email me cayden@mentra.glass&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45336282"/><published>2025-09-22T17:01:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45336989</id><title>Qwen3-Omni: Native Omni AI model for text, image and video</title><updated>2025-09-22T20:37:14.540942+00:00</updated><content>&lt;doc fingerprint="a44ecd1b2dbb5a97"&gt;
  &lt;main&gt;
    &lt;p&gt; 💜 Qwen Chat | 🤗 Hugging Face | 🤖 ModelScope | 📑 Blog | 📚 Cookbooks | 📑 Paper &lt;lb/&gt; 🖥️ Hugging Face Demo | 🖥️ ModelScope Demo | 💬 WeChat (微信) | 🫨 Discord | 📑 API &lt;/p&gt;
    &lt;p&gt;We release Qwen3-Omni, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information 😃&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025.09.22: 🎉🎉🎉 We have released Qwen3-Omni. For more details, please check our blog!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;State-of-the-art across modalities: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multilingual: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Speech Input: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.&lt;/item&gt;
          &lt;item&gt;Speech Output: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Novel Architecture: MoE-based Thinker–Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-time Audio/Video Interaction: Low-latency streaming with natural turn-taking and immediate text or speech responses.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flexible Control: Customize behavior via system prompts for fine-grained control and easy adaptation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Detailed Audio Captioner: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the QuickStart guide to download the model and install the necessary inference environment dependencies, then run and experiment locally—try modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Cookbook&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Open&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
        &lt;cell&gt;Speech Recognition&lt;/cell&gt;
        &lt;cell&gt;Speech recognition, supporting multiple languages and long audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Speech Translation&lt;/cell&gt;
        &lt;cell&gt;Speech-to-Text / Speech-to-Speech translation.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Music Analysis&lt;/cell&gt;
        &lt;cell&gt;Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sound Analysis&lt;/cell&gt;
        &lt;cell&gt;Description and analysis of various sound effects and audio signals.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Caption&lt;/cell&gt;
        &lt;cell&gt;Audio captioning, detailed description of any audio input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mixed Audio Analysis&lt;/cell&gt;
        &lt;cell&gt;Analysis of mixed audio content, such as speech, music, and environmental sounds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Visual&lt;/cell&gt;
        &lt;cell&gt;OCR&lt;/cell&gt;
        &lt;cell&gt;OCR for complex images.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Object Grounding&lt;/cell&gt;
        &lt;cell&gt;Target detection and grounding.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions about any image.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Math&lt;/cell&gt;
        &lt;cell&gt;Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Description&lt;/cell&gt;
        &lt;cell&gt;Detailed description of video content.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Navigation&lt;/cell&gt;
        &lt;cell&gt;Generating navigation commands from first-person motion videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Scene Transition&lt;/cell&gt;
        &lt;cell&gt;Analysis of scene transitions in videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio-Visual&lt;/cell&gt;
        &lt;cell&gt;Audio Visual Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Interaction&lt;/cell&gt;
        &lt;cell&gt;Interactive communication with the model using audio-visual inputs, including task specification via audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Dialogue&lt;/cell&gt;
        &lt;cell&gt;Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;Audio Function Call&lt;/cell&gt;
        &lt;cell&gt;Using audio input to perform function calls, enabling agent-like behaviors.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Downstream Task Fine-tuning&lt;/cell&gt;
        &lt;cell&gt;Omni Captioner&lt;/cell&gt;
        &lt;cell&gt;Introduction and capability demonstration of Qwen3-Omni-30B-A3B-Captioner, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here, we provide several methods to quickly get started with Qwen3-Omni. If you want complete experience of Qwen3-Omni, you can use Hugging Face Transformers. However, since Qwen3-Omni employs an MoE architecture, inference speed with Hugging Face Transformers on MoE models can be very slow. For large-scale invocation or low-latency requirements, we highly recommend using vLLM or performing inference via the DashScope API. We also strongly suggest using our provided Docker image, which includes a complete runtime environment for both Hugging Face Transformers and vLLM. In addition, our cookbooks offer some use cases to show Qwen3-Omni's capabilities. Welcome to learn more!&lt;/p&gt;
    &lt;p&gt;Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Captioner&lt;/cell&gt;
        &lt;cell&gt;A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's cookbook or Hugging Face Demo and ModelScope Demo.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;During loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:&lt;/p&gt;
    &lt;code&gt;# Download through ModelScope (recommended for users in Mainland China)
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

# Download through Hugging Face
pip install -U "huggingface_hub[cli]"
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner&lt;/code&gt;
    &lt;p&gt;The Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you create a new Python environment or use our Docker to avoid environment runtime issues.&lt;/p&gt;
    &lt;code&gt;# If you already have transformers installed, please uninstall it first, or create a new Python environment
# pip uninstall transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate&lt;/code&gt;
    &lt;p&gt;We offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed:&lt;/p&gt;
    &lt;code&gt;pip install qwen-omni-utils -U&lt;/code&gt;
    &lt;p&gt;Additionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using vLLM for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.&lt;/p&gt;
    &lt;code&gt;pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the FlashAttention repository. FlashAttention 2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here is a code snippet to show you how to use Qwen3-Omni with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;qwen_omni_utils&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import soundfile as sf

from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one short sentence."}
        ],
    },
]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, 
                                 speaker="Ethan", 
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)
if audio is not None:
    sf.write(
        "output.wav",
        audio.reshape(-1).detach().cpu().numpy(),
        samplerate=24000,
    )&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;The model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when &lt;code&gt;return_audio=False&lt;/code&gt; is set. Here is an example.&lt;/p&gt;
    &lt;code&gt;from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

# Conversation with image only
conversation1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you hear in this audio?"},
        ]
    }
]

# Conversation with pure text and system prompt
conversation3 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen-Omni."}
        ],
    },
    {
        "role": "user",
        "content": "Who are you?"
    }
]

# Conversation with mixed media
conversation4 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch inference does not support returning audio
text_ids, audio = model.generate(**inputs,
                                 return_audio=False,
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)&lt;/code&gt;
    &lt;head&gt;Use audio output or not&lt;/head&gt;
    &lt;p&gt;The model supports both text and audio outputs. If users do not need audio outputs, they can call &lt;code&gt;model.disable_talker()&lt;/code&gt; after initializing the model. This option will save about &lt;code&gt;10GB&lt;/code&gt; of GPU memory, but the &lt;code&gt;return_audio&lt;/code&gt; option for the &lt;code&gt;generate&lt;/code&gt; function will only allow &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()&lt;/code&gt;
    &lt;p&gt;For a more flexible experience, we recommend that users decide whether to return audio when the &lt;code&gt;generate&lt;/code&gt; function is called. If &lt;code&gt;return_audio&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the model will only return text outputs, resulting in faster text responses.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
...
text_ids, _ = model.generate(..., return_audio=False)```

&amp;lt;/details&amp;gt;

&amp;lt;details&amp;gt;
&amp;lt;summary&amp;gt;Change voice type of output audio&amp;lt;/summary&amp;gt;

Qwen3-Omni supports changing the voice of the output audio. The `"Qwen/Qwen3-Omni-30B-A3B-Instruct"` checkpoint supports three voice types as follows:

| Voice Type | Gender | Description |
|------------|--------|-------------|
| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |
| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |
| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |

Users can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.

```python
text_ids, audio = model.generate(..., speaker="Ethan")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Chelsie")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Aiden")&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;We strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and audio output inference support for the Instruct model will be released in the near future, you can follow the commands below to install vLLM from source. Please note that we recommend you create a new Python environment or use our provided Docker to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the vLLM official documentation.&lt;/p&gt;
    &lt;code&gt;git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
cd vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation
# If you meet an "Undefined symbol" error while using VLLM_USE_PRECOMPILED=1, please use "pip install -e . -v" to build from source.
# Install the Transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;You can use the following code for vLLM inference. The &lt;code&gt;limit_mm_per_prompt&lt;/code&gt; parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting &lt;code&gt;tensor_parallel_size&lt;/code&gt; greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, &lt;code&gt;max_num_seqs&lt;/code&gt; indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the vLLM official documentation. Below is a simple example of how to run Qwen3-Omni with vLLM:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"}
            ], 
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": True,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios

    outputs = llm.generate([inputs], sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;Using vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

def build_input(processor, messages, use_audio_in_video):
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": use_audio_in_video,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios
    
    return inputs

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    # Conversation with image only
    conversation1 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
            ]
        }
    ]

    # Conversation with audio only
    conversation2 = [
        {
            "role": "user",
            "content": [
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
                {"type": "text", "text": "What can you hear in this audio?"},
            ]
        }
    ]

    # Conversation with pure text and system prompt
    conversation3 = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are Qwen-Omni."}
            ],
        },
        {
            "role": "user",
            "content": "Who are you? Answer in one sentence."
        }
    ]

    # Conversation with mixed media
    conversation4 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav"},
                {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
            ],
        }
    ]
    
    USE_AUDIO_IN_VIDEO = True

    # Combine messages for batch processing
    conversations = [conversation1, conversation2, conversation3, conversation4]
    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]

    outputs = llm.generate(inputs, sampling_params=sampling_params)

    result = [outputs[i].outputs[0].text for i in range(len(outputs))]
    print(result)&lt;/code&gt;
    &lt;head&gt;vLLM Serve Usage&lt;/head&gt;
    &lt;p&gt;vLLM serve for Qwen3-Omni currently only supports the thinker model. The &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:&lt;/p&gt;
    &lt;code&gt;# Qwen3-Omni-30B-A3B-Instruct for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4&lt;/code&gt;
    &lt;p&gt;Then you can use the chat API as below (via curl, for example):&lt;/p&gt;
    &lt;code&gt;curl http://localhost:8901/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},
        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},
        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
    ]}
    ]
    }'&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;To further explore Qwen3-Omni, we encourage you to try our DashScope API for a faster and more efficient experience. For detailed API information and documentation, please refer to the following:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;API Description&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (Mainland China)&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (International)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Offline API for Qwen3-Omni-Flash, including Instruct and Thinking models&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen-omni&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/qwen-omni&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Real-time API for Qwen3-Omni-Flash, supporting end-to-end real-time interaction&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/realtime&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/realtime&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;API for Qwen3-Omni-30B-A3B-Captioner model&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Precision&lt;/cell&gt;
        &lt;cell role="head"&gt;15s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;30s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;60s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;120s Video&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;78.85 GB&lt;/cell&gt;
        &lt;cell&gt;88.52 GB&lt;/cell&gt;
        &lt;cell&gt;107.74 GB&lt;/cell&gt;
        &lt;cell&gt;144.81 GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;68.74 GB&lt;/cell&gt;
        &lt;cell&gt;77.79 GB&lt;/cell&gt;
        &lt;cell&gt;95.76 GB&lt;/cell&gt;
        &lt;cell&gt;131.65 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: The table above presents the theoretical minimum memory requirements for inference with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;BF16&lt;/code&gt; precision, tested with &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt;. The Instruct model includes both the thinker and talker components, whereas the Thinking model includes only the thinker part.&lt;/p&gt;
    &lt;p&gt;When using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the following system prompt. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the &lt;code&gt;user_system_prompt&lt;/code&gt; field in the system prompt to include character settings or other role-specific descriptions as needed.&lt;/p&gt;
    &lt;code&gt;user_system_prompt = "You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen."
message = {
    "role": "system",
    "content": [
          {"type": "text", "text": f"{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, “I/me/my/we/our” refer to the user and “you/your” refer to the assistant. In your replies, address the user as “you/your” and yourself as “I/me/my”; never mirror the user’s pronouns—always shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face."}
    ]
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/code&gt; model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:&lt;/p&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Analyze this audio, image, and video together."},
        ], 
    }
]&lt;/code&gt;
    &lt;p&gt;In multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.&lt;/p&gt;
    &lt;code&gt;# In data preprocessing
audios, images, videos = process_mm_info(messages, use_audio_in_video=True)&lt;/code&gt;
    &lt;code&gt;# For Transformers
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", 
                   padding=True, use_audio_in_video=True)
text_ids, audio = model.generate(..., use_audio_in_video=True)

# For vLLM
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = {
    'prompt': text,
    'multi_modal_data': {},
    "mm_processor_kwargs": {
        "use_audio_in_video": True,
    },
}&lt;/code&gt;
    &lt;p&gt;It is worth noting that during a multi-round conversation, the &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter must be set consistently across these steps; otherwise, unexpected results may occur.&lt;/p&gt;
    &lt;p&gt;Without local deployment, you can experience an online web demo directly by visiting our Hugging Face Spaces and ModelScope Studio. This includes quick hands-on experiences for Qwen3-Omni-Realtime, Qwen3-Omni (Instruct and Thinking), and Qwen3-Omni-30B-A3B-Captioner.&lt;/p&gt;
    &lt;p&gt;Real-time streaming interaction with Qwen3-Omni is available now. Please visit Qwen Chat and select the voice/video call option in the chat box to experience it.&lt;/p&gt;
    &lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with the model through a web browser. Follow the steps below to get start :)&lt;/p&gt;
    &lt;p&gt;Before you begin, we strongly recommend that you refer to the Installation section in vLLM Usage to set up your environment, which will allow you to seamlessly use both the vLLM and Transformers backends. However, if you only intend to use the Transformers backend (note that this will result in significantly slower inference), please follow the installation instructions in Transformers Usage. That said, we still highly recommend using our Docker image to avoid potential environment-related issues. Additionally, if you are running locally, make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed and you install the following dependencies:&lt;/p&gt;
    &lt;code&gt;pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1&lt;/code&gt;
    &lt;p&gt;Once the required packages are installed, you can launch the web demo using the following commands. These commands will start a web server and provide you with a link to access the UI in your web browser. You can run &lt;code&gt;python web_demo.py --help&lt;/code&gt; and &lt;code&gt;python web_demo_captioner.py --help&lt;/code&gt; to learn about more options.&lt;/p&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Instruct with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Thinking with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Captioner with vLLM backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2&lt;/code&gt;
    &lt;p&gt;After running the command, you’ll see a link generated in the terminal similar to this:&lt;/p&gt;
    &lt;code&gt;Running on local: http://127.0.0.1:8901/
&lt;/code&gt;
    &lt;p&gt;If you are running locally, copy this link and paste it into your browser to access the web UI. If you are running on a server or in a &lt;code&gt;docker&lt;/code&gt; container, please configure the address according to the server's actual IP, or set up port forwarding where necessary. For instructions on how to configure port forwarding from the official &lt;code&gt;docker&lt;/code&gt; container to the host machine, please refer to here.&lt;/p&gt;
    &lt;p&gt;To simplify the deployment process, we provide Docker images with pre-built environments: qwenllm/qwen3-omni. You only need to install the driver and download model files to launch the demos. Please refer to the guide to install the NVIDIA Container Toolkit, ensuring that your Docker can access the GPU. For users in mainland China who may have difficulty accessing Docker Hub, you can use mirror acceleration services to pull the images. First, run the following command to pull and initialize the container:&lt;/p&gt;
    &lt;code&gt;LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p $HOST_PORT:$CONTAINER_PORT \
    --mount type=bind,source=$LOCAL_WORKDIR,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124&lt;/code&gt;
    &lt;p&gt;After executing the command, you will enter the bash shell of the container. Your local model and data directory (please replace &lt;code&gt;/path/to/your/workspace&lt;/code&gt; with the actual path) will be mounted to the container's internal path &lt;code&gt;/data/shared/Qwen3-Omni&lt;/code&gt;. The host's port &lt;code&gt;8901&lt;/code&gt; is mapped to port &lt;code&gt;80&lt;/code&gt; in the container, meaning you can access the service inside the container by visiting port &lt;code&gt;8901&lt;/code&gt; on the host machine.&lt;/p&gt;
    &lt;p&gt;Please note that services inside the container must be started with the IP &lt;code&gt;0.0.0.0&lt;/code&gt; to ensure proper port forwarding. For example:&lt;/p&gt;
    &lt;code&gt;# Run this command inside the Docker container
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0&lt;/code&gt;
    &lt;p&gt;For more ways to launch the web demo, please refer to Launch Local Web UI Demo. If you exit the container, you can re-enter it using the following command:&lt;/p&gt;
    &lt;code&gt;docker start qwen3-omni
docker exec -it qwen3-omni bash&lt;/code&gt;
    &lt;p&gt;Or if you want to completely remove the container, please run:&lt;/p&gt;
    &lt;code&gt;docker rm -f qwen3-omni&lt;/code&gt;
    &lt;p&gt;Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.&lt;/p&gt;
    &lt;head&gt;Text -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;GPT-4o-0327&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Non Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Instruct-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;91.3&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
        &lt;cell&gt;86.6&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;66.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;69.6&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;26.7&lt;/cell&gt;
        &lt;cell&gt;24.7&lt;/cell&gt;
        &lt;cell&gt;61.3&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
        &lt;cell&gt;65.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ZebraLogic&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;37.7&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;76.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;79.3&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;81.4&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;84.7&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;84.9&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;86.0&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;82.6&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;66.5&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;65.1&lt;/cell&gt;
        &lt;cell&gt;64.4&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;64.0&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;25.5&lt;/cell&gt;
        &lt;cell&gt;27.0&lt;/cell&gt;
        &lt;cell&gt;43.1&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
        &lt;cell&gt;39.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;92.7&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;82.8&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;72.0&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;73.7&lt;/cell&gt;
        &lt;cell&gt;74.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LiveBench 20241125&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;77.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;70.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;79.9&lt;/cell&gt;
        &lt;cell&gt;81.3&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;85.1&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Arena-Hard v2&lt;/cell&gt;
        &lt;cell&gt;56.7&lt;/cell&gt;
        &lt;cell&gt;61.5&lt;/cell&gt;
        &lt;cell&gt;56.0&lt;/cell&gt;
        &lt;cell&gt;55.1&lt;/cell&gt;
        &lt;cell&gt;57.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;82.5&lt;/cell&gt;
        &lt;cell&gt;83.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;85.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;74.4&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;76.4&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;49.8&lt;/cell&gt;
        &lt;cell&gt;54.7&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;48.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Audio -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Seed-ASR&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Mini&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Small&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Transcribe&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;EN &amp;amp; ZH ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Wenetspeech&lt;p&gt;net | meeting&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;4.66 | 5.69&lt;/cell&gt;
        &lt;cell&gt;24.30 | 31.53&lt;/cell&gt;
        &lt;cell&gt;20.33 | 26.08&lt;/cell&gt;
        &lt;cell&gt;15.30 | 32.27&lt;/cell&gt;
        &lt;cell&gt;14.43 | 13.47&lt;/cell&gt;
        &lt;cell&gt;5.91 | 7.65&lt;/cell&gt;
        &lt;cell&gt;4.69 | 5.89&lt;/cell&gt;
        &lt;cell&gt;4.62 | 5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Librispeech&lt;p&gt;clean | other&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.58 | 2.84&lt;/cell&gt;
        &lt;cell&gt;1.88 | 4.12&lt;/cell&gt;
        &lt;cell&gt;1.56 | 3.30&lt;/cell&gt;
        &lt;cell&gt;1.39 | 3.75&lt;/cell&gt;
        &lt;cell&gt;2.89 | 3.56&lt;/cell&gt;
        &lt;cell&gt;1.74 | 3.45&lt;/cell&gt;
        &lt;cell&gt;1.22 | 2.48&lt;/cell&gt;
        &lt;cell&gt;1.27 | 2.44&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;9.47&lt;/cell&gt;
        &lt;cell&gt;7.79&lt;/cell&gt;
        &lt;cell&gt;10.01&lt;/cell&gt;
        &lt;cell&gt;9.89&lt;/cell&gt;
        &lt;cell&gt;7.61&lt;/cell&gt;
        &lt;cell&gt;6.05&lt;/cell&gt;
        &lt;cell&gt;5.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;24.67&lt;/cell&gt;
        &lt;cell&gt;19.30&lt;/cell&gt;
        &lt;cell&gt;9.84&lt;/cell&gt;
        &lt;cell&gt;8.00&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;4.31&lt;/cell&gt;
        &lt;cell&gt;4.28&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en&lt;/cell&gt;
        &lt;cell&gt;3.40&lt;/cell&gt;
        &lt;cell&gt;3.96&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;2.94&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;2.72&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh&lt;/cell&gt;
        &lt;cell&gt;2.69&lt;/cell&gt;
        &lt;cell&gt;12.22&lt;/cell&gt;
        &lt;cell&gt;7.98&lt;/cell&gt;
        &lt;cell&gt;2.44&lt;/cell&gt;
        &lt;cell&gt;2.71&lt;/cell&gt;
        &lt;cell&gt;2.54&lt;/cell&gt;
        &lt;cell&gt;2.20&lt;/cell&gt;
        &lt;cell&gt;2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Multilingual ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-avg&lt;p&gt;(19 lang)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;15.67&lt;/cell&gt;
        &lt;cell&gt;8.09&lt;/cell&gt;
        &lt;cell&gt;4.48&lt;/cell&gt;
        &lt;cell&gt;5.55&lt;/cell&gt;
        &lt;cell&gt;14.04&lt;/cell&gt;
        &lt;cell&gt;5.33&lt;/cell&gt;
        &lt;cell&gt;5.31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lyric ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MIR-1K (vocal-only)&lt;/cell&gt;
        &lt;cell&gt;6.45&lt;/cell&gt;
        &lt;cell&gt;23.33&lt;/cell&gt;
        &lt;cell&gt;18.73&lt;/cell&gt;
        &lt;cell&gt;11.87&lt;/cell&gt;
        &lt;cell&gt;9.85&lt;/cell&gt;
        &lt;cell&gt;8.15&lt;/cell&gt;
        &lt;cell&gt;5.90&lt;/cell&gt;
        &lt;cell&gt;5.85&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Opencpop-test&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;31.01&lt;/cell&gt;
        &lt;cell&gt;16.06&lt;/cell&gt;
        &lt;cell&gt;7.93&lt;/cell&gt;
        &lt;cell&gt;6.49&lt;/cell&gt;
        &lt;cell&gt;2.84&lt;/cell&gt;
        &lt;cell&gt;1.54&lt;/cell&gt;
        &lt;cell&gt;2.02&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;S2TT (BLEU)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;30.35&lt;/cell&gt;
        &lt;cell&gt;37.85&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;39.25&lt;/cell&gt;
        &lt;cell&gt;29.22&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;36.22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-xx2en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;27.54&lt;/cell&gt;
        &lt;cell&gt;32.81&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;35.41&lt;/cell&gt;
        &lt;cell&gt;28.61&lt;/cell&gt;
        &lt;cell&gt;31.08&lt;/cell&gt;
        &lt;cell&gt;30.71&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;17.03&lt;/cell&gt;
        &lt;cell&gt;22.05&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;26.63&lt;/cell&gt;
        &lt;cell&gt;17.97&lt;/cell&gt;
        &lt;cell&gt;25.17&lt;/cell&gt;
        &lt;cell&gt;25.10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Fleurs-xx2zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;28.75&lt;/cell&gt;
        &lt;cell&gt;34.82&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;27.68&lt;/cell&gt;
        &lt;cell&gt;33.13&lt;/cell&gt;
        &lt;cell&gt;31.19&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;VoiceBench&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AlpacaEval&lt;/cell&gt;
        &lt;cell&gt;95.6&lt;/cell&gt;
        &lt;cell&gt;96.1&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;89.9&lt;/cell&gt;
        &lt;cell&gt;94.8&lt;/cell&gt;
        &lt;cell&gt;96.4&lt;/cell&gt;
        &lt;cell&gt;95.4&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CommonEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;88.3&lt;/cell&gt;
        &lt;cell&gt;88.4&lt;/cell&gt;
        &lt;cell&gt;76.7&lt;/cell&gt;
        &lt;cell&gt;90.8&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;91.0&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;WildVoice&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;93.4&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SD-QA&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;90.1&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;78.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;78.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;66.1&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;61.7&lt;/cell&gt;
        &lt;cell&gt;68.1&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
        &lt;cell&gt;68.4&lt;/cell&gt;
        &lt;cell&gt;84.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;OpenBookQA&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;56.9&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;80.9&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;95.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BBH&lt;/cell&gt;
        &lt;cell&gt;84.1&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;92.6&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;53.5&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;80.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AdvBench&lt;/cell&gt;
        &lt;cell&gt;98.7&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
        &lt;cell&gt;98.1&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.3&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
        &lt;cell&gt;73.6&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;85.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Audio Reasoning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMAU-v05.15.25&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
        &lt;cell&gt;65.5&lt;/cell&gt;
        &lt;cell&gt;77.5&lt;/cell&gt;
        &lt;cell&gt;75.4&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;62.6&lt;/cell&gt;
        &lt;cell&gt;69.0&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Best Specialist&lt;p&gt;Models&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;RUL-MuchoMusic&lt;/cell&gt;
        &lt;cell&gt;47.6 (Audio Flamingo 3)&lt;/cell&gt;
        &lt;cell&gt;36.1&lt;/cell&gt;
        &lt;cell&gt;49.4&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;52.0&lt;/cell&gt;
        &lt;cell&gt;52.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GTZAN&lt;p&gt;Acc.&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;87.9 (CLaMP 3)&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
        &lt;cell&gt;93.0&lt;/cell&gt;
        &lt;cell&gt;93.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Genre&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;35.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.3&lt;/cell&gt;
        &lt;cell&gt;32.6&lt;/cell&gt;
        &lt;cell&gt;32.5&lt;/cell&gt;
        &lt;cell&gt;39.0&lt;/cell&gt;
        &lt;cell&gt;39.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Mood/Theme&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;10.9 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;11.3&lt;/cell&gt;
        &lt;cell&gt;14.1&lt;/cell&gt;
        &lt;cell&gt;8.9&lt;/cell&gt;
        &lt;cell&gt;21.0&lt;/cell&gt;
        &lt;cell&gt;21.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Instrument&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;39.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;34.2&lt;/cell&gt;
        &lt;cell&gt;33.0&lt;/cell&gt;
        &lt;cell&gt;22.6&lt;/cell&gt;
        &lt;cell&gt;40.5&lt;/cell&gt;
        &lt;cell&gt;40.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Top50&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;33.2 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.0&lt;/cell&gt;
        &lt;cell&gt;26.1&lt;/cell&gt;
        &lt;cell&gt;21.6&lt;/cell&gt;
        &lt;cell&gt;36.7&lt;/cell&gt;
        &lt;cell&gt;36.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MagnaTagATune&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;41.6 (MuQ)&lt;/cell&gt;
        &lt;cell&gt;29.2&lt;/cell&gt;
        &lt;cell&gt;28.1&lt;/cell&gt;
        &lt;cell&gt;30.1&lt;/cell&gt;
        &lt;cell&gt;44.3&lt;/cell&gt;
        &lt;cell&gt;46.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Vision -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT4-o&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.0-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-VL&lt;p&gt;72B&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;69.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;55.0&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;55.2&lt;/cell&gt;
        &lt;cell&gt;59.7&lt;/cell&gt;
        &lt;cell&gt;58.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.7&lt;/cell&gt;
        &lt;cell&gt;6.7&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
        &lt;cell&gt;7.4&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;51.9&lt;/cell&gt;
        &lt;cell&gt;56.1&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
        &lt;cell&gt;57.0&lt;/cell&gt;
        &lt;cell&gt;57.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;74.8&lt;/cell&gt;
        &lt;cell&gt;75.9&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;48.6&lt;/cell&gt;
        &lt;cell&gt;38.1&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;58.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;AI2D&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;88.7&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
        &lt;cell&gt;86.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;87.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;87.9&lt;/cell&gt;
        &lt;cell&gt;91.2&lt;/cell&gt;
        &lt;cell&gt;93.6&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;70.5&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;30.8&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;50.2&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;71.0&lt;/cell&gt;
        &lt;cell&gt;74.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-flash-thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;InternVL-3.5-241B-A28B&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.9&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;61.1&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;62.8&lt;/cell&gt;
        &lt;cell&gt;63.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.8&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;75.6&lt;/cell&gt;
        &lt;cell&gt;75.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;60.5&lt;/cell&gt;
        &lt;cell&gt;60.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;80.0&lt;/cell&gt;
        &lt;cell&gt;81.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;62.3&lt;/cell&gt;
        &lt;cell&gt;63.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;AI2D_test&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;87.3&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;88.0&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;92.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;79.6&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;49.0&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;82.1&lt;/cell&gt;
        &lt;cell&gt;78.2&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;AudioVisual -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WorldSense&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;50.9&lt;/cell&gt;
        &lt;cell&gt;45.4&lt;/cell&gt;
        &lt;cell&gt;54.0&lt;/cell&gt;
        &lt;cell&gt;54.1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;DailyOmni&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
        &lt;cell&gt;72.7&lt;/cell&gt;
        &lt;cell&gt;75.8&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;VideoHolmes&lt;/cell&gt;
        &lt;cell&gt;55.6&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Zero-shot Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Content Consistency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SEED&lt;p&gt;test-zh | test-en&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Seed-TTSICL&lt;/cell&gt;
        &lt;cell&gt;1.11 | 2.24&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Seed-TTSRL&lt;/cell&gt;
        &lt;cell&gt;1.00 | 1.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MaskGCT&lt;/cell&gt;
        &lt;cell&gt;2.27 | 2.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;E2 TTS&lt;/cell&gt;
        &lt;cell&gt;1.97 | 2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;F5-TTS&lt;/cell&gt;
        &lt;cell&gt;1.56 | 1.83&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Spark TTS&lt;/cell&gt;
        &lt;cell&gt;1.20 | 1.98&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 2&lt;/cell&gt;
        &lt;cell&gt;1.45 | 2.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 3&lt;/cell&gt;
        &lt;cell&gt;0.71 | 1.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2.5-Omni-7B&lt;/cell&gt;
        &lt;cell&gt;1.42 | 2.33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;1.07 | 1.39&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Multilingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Content Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Speaker Similarity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Chinese&lt;/cell&gt;
        &lt;cell&gt;0.716&lt;/cell&gt;
        &lt;cell&gt;2.252&lt;/cell&gt;
        &lt;cell&gt;16.026&lt;/cell&gt;
        &lt;cell&gt;0.772&lt;/cell&gt;
        &lt;cell&gt;0.780&lt;/cell&gt;
        &lt;cell&gt;0.677&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;English&lt;/cell&gt;
        &lt;cell&gt;1.069&lt;/cell&gt;
        &lt;cell&gt;2.164&lt;/cell&gt;
        &lt;cell&gt;2.339&lt;/cell&gt;
        &lt;cell&gt;0.773&lt;/cell&gt;
        &lt;cell&gt;0.756&lt;/cell&gt;
        &lt;cell&gt;0.613&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;German&lt;/cell&gt;
        &lt;cell&gt;0.777&lt;/cell&gt;
        &lt;cell&gt;1.906&lt;/cell&gt;
        &lt;cell&gt;0.572&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
        &lt;cell&gt;0.733&lt;/cell&gt;
        &lt;cell&gt;0.614&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Italian&lt;/cell&gt;
        &lt;cell&gt;1.067&lt;/cell&gt;
        &lt;cell&gt;1.543&lt;/cell&gt;
        &lt;cell&gt;1.743&lt;/cell&gt;
        &lt;cell&gt;0.742&lt;/cell&gt;
        &lt;cell&gt;0.699&lt;/cell&gt;
        &lt;cell&gt;0.579&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Portuguese&lt;/cell&gt;
        &lt;cell&gt;1.872&lt;/cell&gt;
        &lt;cell&gt;1.877&lt;/cell&gt;
        &lt;cell&gt;1.331&lt;/cell&gt;
        &lt;cell&gt;0.770&lt;/cell&gt;
        &lt;cell&gt;0.805&lt;/cell&gt;
        &lt;cell&gt;0.711&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Spanish&lt;/cell&gt;
        &lt;cell&gt;1.765&lt;/cell&gt;
        &lt;cell&gt;1.029&lt;/cell&gt;
        &lt;cell&gt;1.084&lt;/cell&gt;
        &lt;cell&gt;0.744&lt;/cell&gt;
        &lt;cell&gt;0.762&lt;/cell&gt;
        &lt;cell&gt;0.615&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Japanese&lt;/cell&gt;
        &lt;cell&gt;3.631&lt;/cell&gt;
        &lt;cell&gt;3.519&lt;/cell&gt;
        &lt;cell&gt;10.646&lt;/cell&gt;
        &lt;cell&gt;0.763&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Korean&lt;/cell&gt;
        &lt;cell&gt;1.670&lt;/cell&gt;
        &lt;cell&gt;1.747&lt;/cell&gt;
        &lt;cell&gt;1.865&lt;/cell&gt;
        &lt;cell&gt;0.778&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;French&lt;/cell&gt;
        &lt;cell&gt;2.505&lt;/cell&gt;
        &lt;cell&gt;4.099&lt;/cell&gt;
        &lt;cell&gt;5.216&lt;/cell&gt;
        &lt;cell&gt;0.689&lt;/cell&gt;
        &lt;cell&gt;0.628&lt;/cell&gt;
        &lt;cell&gt;0.535&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Russian&lt;/cell&gt;
        &lt;cell&gt;3.986&lt;/cell&gt;
        &lt;cell&gt;4.281&lt;/cell&gt;
        &lt;cell&gt;3.878&lt;/cell&gt;
        &lt;cell&gt;0.759&lt;/cell&gt;
        &lt;cell&gt;0.761&lt;/cell&gt;
        &lt;cell&gt;0.676&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Cross-Lingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice3&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-zh&lt;/cell&gt;
        &lt;cell&gt;5.37&lt;/cell&gt;
        &lt;cell&gt;5.09&lt;/cell&gt;
        &lt;cell&gt;13.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-zh&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;3.05&lt;/cell&gt;
        &lt;cell&gt;48.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-zh&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;1.06&lt;/cell&gt;
        &lt;cell&gt;7.70&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-en&lt;/cell&gt;
        &lt;cell&gt;2.76&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;6.47&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-en&lt;/cell&gt;
        &lt;cell&gt;3.31&lt;/cell&gt;
        &lt;cell&gt;4.20&lt;/cell&gt;
        &lt;cell&gt;17.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-en&lt;/cell&gt;
        &lt;cell&gt;3.34&lt;/cell&gt;
        &lt;cell&gt;4.19&lt;/cell&gt;
        &lt;cell&gt;11.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ja&lt;/cell&gt;
        &lt;cell&gt;8.29&lt;/cell&gt;
        &lt;cell&gt;7.08&lt;/cell&gt;
        &lt;cell&gt;13.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ja&lt;/cell&gt;
        &lt;cell&gt;7.53&lt;/cell&gt;
        &lt;cell&gt;6.80&lt;/cell&gt;
        &lt;cell&gt;14.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-ja&lt;/cell&gt;
        &lt;cell&gt;4.24&lt;/cell&gt;
        &lt;cell&gt;3.93&lt;/cell&gt;
        &lt;cell&gt;5.86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ko&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;14.4&lt;/cell&gt;
        &lt;cell&gt;24.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ko&lt;/cell&gt;
        &lt;cell&gt;4.96&lt;/cell&gt;
        &lt;cell&gt;5.87&lt;/cell&gt;
        &lt;cell&gt;21.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ja-to-ko&lt;/cell&gt;
        &lt;cell&gt;6.23&lt;/cell&gt;
        &lt;cell&gt;7.92&lt;/cell&gt;
        &lt;cell&gt;21.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding Strategy: For the Qwen3-Omni series across all evaluation benchmarks, &lt;code&gt;Instruct&lt;/code&gt;models use greedy decoding during generation without sampling. For&lt;code&gt;Thinking&lt;/code&gt;models, the decoding parameters should be taken from the&lt;code&gt;generation_config.json&lt;/code&gt;file in the checkpoint.&lt;/item&gt;
      &lt;item&gt;Benchmark-Specific Formatting: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to &lt;code&gt;fps=2&lt;/code&gt;during evaluation.&lt;/item&gt;
      &lt;item&gt;Default Prompts: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Task Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Prompt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Chinese&lt;/cell&gt;
        &lt;cell&gt;请将这段中文语音转换为纯文本。&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Other languages&lt;/cell&gt;
        &lt;cell&gt;Transcribe the audio into text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Speech-to-Text Translation (S2TT)&lt;/cell&gt;
        &lt;cell&gt;Listen to the provided &amp;lt;source_language&amp;gt; speech and produce a translation in &amp;lt;target_language&amp;gt; text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Song Lyrics Recognition&lt;/cell&gt;
        &lt;cell&gt;Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System Prompt: No &lt;code&gt;system prompt&lt;/code&gt;should be set for any evaluation benchmark.&lt;/item&gt;
      &lt;item&gt;Input Sequence: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come after multimodal data in the sequence. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Describe the audio, image and video."},
        ],
    },
]&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/QwenLM/Qwen3-Omni"/><published>2025-09-22T17:50:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45337253</id><title>AI-Generated "Workslop" Is Destroying Productivity</title><updated>2025-09-22T20:37:14.196121+00:00</updated><content>&lt;doc fingerprint="3d7583fdb9511a02"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI-Generated “Workslop” Is Destroying Productivity&lt;/head&gt;
    &lt;p&gt;September 22, 2025, Updated September 22, 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary.&lt;/head&gt;
    &lt;p&gt;A confusing contradiction is unfolding in companies embracing generative AI tools: while workers are largely following mandates to embrace the technology, few are seeing it create real value. Consider, for instance, that the number of companies with fully AI-led processes nearly doubled last year, while AI use has likewise doubled at work since 2023. Yet a recent report from the MIT Media Lab found that 95% of organizations see no measurable return on their investment in these technologies. So much activity, so much enthusiasm, so little return. Why?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity"/><published>2025-09-22T18:07:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45337433</id><title>Diffusion Beats Autoregressive in Data-Constrained Settings</title><updated>2025-09-22T20:37:13.326607+00:00</updated><content>&lt;doc fingerprint="b4173bd8031d52cb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;TLDR:&lt;/p&gt;
      &lt;p&gt;If you are compute-constrained, use autoregressive models; if you are data-constrained, use diffusion models.&lt;/p&gt;
      &lt;p&gt;Motivation&lt;/p&gt;
      &lt;p&gt;Progress in AI over the past decade has largely been driven by scaling compute and data. The recipe from GPT-1 to GPT-5 has appeared straightforward: train a larger model on more data, and the result is a more capable system. &lt;/p&gt;
      &lt;p&gt;Yet a central question remains: will this recipe continue to hold from GPT-6 to GPT-N?&lt;/p&gt;
      &lt;p&gt;Many analysts and researchers believe the answer is no. For instance, Ilya Sutskever, in his NeurIPS 2024 Test-of-Time Award talk, remarked: “Compute is growing—better algorithms, better hardware, bigger clusters—but data is not growing. We have just one internet, the fossil fuel of AI.” &lt;/p&gt;
      &lt;p&gt;This concern is echoed by AI forecasters, who have analyzed compute and data growth more systematically and concluded that compute is outpacing data at an accelerating rate.&lt;/p&gt;
      &lt;p&gt;The above Figure, illustrates this tension by overlaying projections from EpochAI’s analysis. Their study extrapolates historical trends in compute, dataset usage, and internet-scale data availability. The forecast suggests that by around 2028, we will enter a data-constrained regime: far more compute will be available than there are training tokens to consume.&lt;/p&gt;
      &lt;p&gt;This paper addresses the challenge by asking: how can we trade off more compute for less data? Our central idea is to revisit the foundations of modern generative modeling and compare the two dominant paradigms for scaling AI.&lt;/p&gt;
      &lt;p&gt;Broadly, there have been two families of algorithms that shaped recent progress in AI:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Autoregressive models, popularized in 2019 in the text domain with the GPT-2 paper.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Diffusion models, popularized in 2020 in the vision domain with the DDPM paper.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Both aim to maximize the joint likelihood, but they differ fundamentally in how they factorize this joint distribution. &lt;/p&gt;
      &lt;p&gt;The success of diffusion in vision and autoregression in language has sparked both excitement and confusion—especially as each community has begun experimenting with the other’s paradigm.&lt;/p&gt;
      &lt;p&gt;For example, the language community has explored diffusion on text: &lt;/p&gt;
      &lt;p&gt;D3PM introduced discrete diffusion via random masking, while Diffusion-LM applied continuous diffusion by projecting tokens to embeddings before adding Gaussian noise. Since then, numerous works have extended this line of research.&lt;/p&gt;
      &lt;p&gt;Conversely, the vision community has experimented with doing autoregressive modeling on images. Models such as PARTI and DALLE exemplify this approach with strong results.&lt;/p&gt;
      &lt;p&gt;This cross-pollination has led to even greater uncertainty in robotics, where both diffusion-based and autoregressive approaches are widely adopted. To illustrate this, OpenAI Deep Research has compiled a list of robotics works across both paradigms, highlighting the lack of consensus in the field.&lt;/p&gt;
      &lt;p&gt;This ambiguity raises a fundamental question: should we be training diffusion models or autoregressive models?&lt;/p&gt;
      &lt;p&gt;Quick Background:&lt;/p&gt;
      &lt;p&gt;Autoregressive language models:&lt;/p&gt;
      &lt;p&gt;They model data distribution in a left-to-right manner&lt;/p&gt;
      &lt;p&gt;Diffusion language models:&lt;/p&gt;
      &lt;p&gt;For a more detailed understanding, with cool animations, please refer to this video from Jia-Bin Huang – https://www.youtube.com/watch?v=8BTOoc0yDVA&lt;/p&gt;
      &lt;p&gt;Prior results with Diffusion Language models&lt;/p&gt;
      &lt;p&gt;Since 2021, diffusion language models have sparked significant interest, with many works focusing on improving their design and performance.&lt;/p&gt;
      &lt;p&gt;In the table above, we highlight representative results from a popular work.&lt;lb/&gt;The takeaways are as follows:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Discrete diffusion performs better than continuous diffusion on text.&lt;/item&gt;
        &lt;item&gt;Autoregressive models still achieve the strongest results overall.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Several works have also explored the scaling behavior of diffusion-based language models.&lt;/p&gt;
      &lt;p&gt;Nie et al report that discrete diffusion LLMs require roughly 16× more compute than autoregressive LLMs to match the same negative log-likelihood. Similar results have been observed in multimodal domains—for instance, UniDisc finds that discrete diffusion needs about 12× more compute than autoregression for comparable likelihoods.&lt;/p&gt;
      &lt;p&gt;However, these results conflate data and compute because they are measured in a single-epoch training regime. This raises an important ambiguity: do diffusion models truly require 16× more compute, or do they in fact require 16× more data?&lt;/p&gt;
      &lt;p&gt;In this work, we explicitly disentangle data and compute. Our goal is to study diffusion and autoregressive models specifically in data-constrained settings.&lt;/p&gt;
      &lt;p&gt;Our Motivation&lt;/p&gt;
      &lt;p&gt;To understand why diffusion may behave differently, let’s revisit its training objective.&lt;/p&gt;
      &lt;p&gt;In diffusion training, tokens are randomly masked and the model learns to recover them. Importantly, left-to-right masking is a special case within this framework.&lt;/p&gt;
      &lt;p&gt;Viewed this way, diffusion can be interpreted as a form of implicit data augmentation for autoregressive training. Instead of only learning from left-to-right sequences, the model also benefits from many alternative masking strategies.&lt;/p&gt;
      &lt;p&gt;And if diffusion is essentially data augmentation, then its benefits should be most pronounced when training is data-bottlenecked.&lt;/p&gt;
      &lt;p&gt;This perspective explains why prior works have reported weaker results for diffusion: they primarily evaluated in single-epoch settings, where data is abundant. In contrast, our study focuses on scenarios where data is limited and compute can be traded off more effectively.&lt;/p&gt;
      &lt;p&gt;Our Experiments&lt;/p&gt;
      &lt;p&gt;In this work, we train hundreds of models spanning multiple orders of magnitude in model size, data quantity, and number of training epochs to fit scaling laws for diffusion models in the data-constrained setting. We summarize some of our key findings below.&lt;/p&gt;
      &lt;p&gt;Finding #1:&lt;/p&gt;
      &lt;p&gt;Diffusion models outperform autoregressive models when trained with sufficient compute (i.e., more epochs &amp;amp; parameters). Across different unique data scales, we observe:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;At low compute, Autoregressive models win.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;After a certain amount of compute, performance matches—we call this the critical compute point.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Beyond this, diffusion keeps improving, while Autoregressive plateaus or overfits. &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Each point in the figure shows a model trained to convergence. The x-axis shows the total training FLOPs of that point, and the y-axis shows the best validation loss achieved by that model family under that training compute budget.&lt;/p&gt;
      &lt;p&gt;Finding #2:&lt;/p&gt;
      &lt;p&gt;Autoregressive models begin to overfit much quickly, while diffusion shows no signs of overfitting even after 10x the number of epochs. In the above figure, we showed that increasing compute eventually favors diffusion. But compute can be scaled in two ways: (i) Increasing model size (ii) Increasing the number of epochs In the following plot, we separate these axes.&lt;/p&gt;
      &lt;p&gt;The colored star marks the 1-epoch point, where Autoregressive outperforms diffusion. The star (★) denotes the best loss achieved by each model.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Autoregressive hits its best around the middle, then overfits.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Diffusion keeps improving and reaches its best loss at the far right. &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Not only does diffusion benefit from more training—it also achieves a better final loss than Autoregressive (3.51 vs. 3.71).&lt;/p&gt;
      &lt;p&gt;Finding #3:&lt;/p&gt;
      &lt;p&gt;Diffusion models are significantly more robust to data repetition than autoregressive (AR) models. &lt;/p&gt;
      &lt;p&gt;We show training curves of models trained with the same total compute, but different trade-offs between unique data and number of epochs. &lt;/p&gt;
      &lt;p&gt;An “epoch” here means reusing a smaller subset of data more times(e.g., 4 Ep is 4 epochs while using 25% unique data, 2 Ep is 2 epochs with 50% and so on).&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;AR models begin to overfit as repetition increases—their validation loss worsens and significantly diverges at higher epoch counts.&lt;/item&gt;
      &lt;/list&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Diffusion models remain stable across all repetition levels, showing no signs of overfitting or diverging—even at 100 epochs.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Finding #4:&lt;/p&gt;
      &lt;p&gt;Diffusion models exhibit a much higher half-life of data reuse (R_D*) —i.e., the number of epochs after which returns from repeating data begins to significantly diminish. &lt;/p&gt;
      &lt;p&gt;We adopt the data-constrained scaling framework introduced by Muennighoff et al. in their excellent NeurIPS paper to fit scaling laws for diffusion models. While Muennighoff et al. found R_D* ~ 15 for autoregressive models, we find a significantly higher value of R_D* ~ 500 for diffusion models—highlighting their ability to benefit from far more data repetition.&lt;/p&gt;
      &lt;p&gt;The above Figure studies the Decay rate of data value under repetition: left shows diffusion, middle AR, and right the average decay rate for both. &lt;/p&gt;
      &lt;p&gt;Points are empirical results (darker color = higher FLOPs, lighter color =&lt;lb/&gt;lower FLOPs; each line = fixed compute), we find that fitted curves (represented as lines) closely match the empirical points, indicating our scaling laws are representative. The decay rate of value for repeated data is lower for diffusion, reflecting its greater robustness to repeating. In this experiment 100% data fraction means training 1 epoch with 100% unique data, while 50% means 2 epoch epoch with only using 50% unique data and so on.&lt;/p&gt;
      &lt;p&gt;Finding #5:&lt;/p&gt;
      &lt;p&gt;Muennighoff et al. showed that repeating the dataset up to 4 epochs is nearly as effective as using fresh data for autoregressive models.&lt;/p&gt;
      &lt;p&gt; In contrast, we find that diffusion models can be trained on repeated data for up to 100 epochs, while having repeated data almost as effective as fresh data.&lt;/p&gt;
      &lt;p&gt;Finding #6:&lt;/p&gt;
      &lt;p&gt;The compute required for diffusion to outperform AR follows a predictable power law. Above we defined the critical compute threshold as the amount of FLOPs where diffusion matches AR performance for a given unique dataset size. &lt;/p&gt;
      &lt;p&gt;We find that we can derive a simple closed-form analytical expression for this threshold, this allows us to predict when diffusion will surpass AR given any unique data size. In the figure we show both the fitted curve and empirical critical threshold points, which align closely.&lt;/p&gt;
      &lt;p&gt;Finding #7:&lt;/p&gt;
      &lt;p&gt;The data efficiency of diffusion models translates to better downstream performance.&lt;/p&gt;
      &lt;p&gt; Lastly we evaluate the best-performing diffusion and AR models (trained under the same data budget) on a range of language understanding tasks. &lt;/p&gt;
      &lt;p&gt;Across most benchmarks, diffusion models outperform AR models, confirming that diffusion’s lower validation loss translates to better downstream performance.&lt;/p&gt;
      &lt;p&gt;Finding #8:&lt;/p&gt;
      &lt;p&gt;Exposure to different token orderings helps explain diffusion’s data efficiency. By adding explicit data augmentations to AR training, we find that diffusion model’s advantage arises from their exposure to a diverse set of token orderings. &lt;/p&gt;
      &lt;p&gt;As seen in the above Figure, increasing N consistently lowered validation loss and delayed overfitting. At N = 16, the 100-epoch validation loss of AR models approached that of diffusion, suggesting that diverse orderings are indeed a key driver of diffusion’s data efficiency. These results support our interpretation that diffusion models outperform AR models in low-data regimes because they are implicitly trained on a richer distribution of conditional prediction tasks. &lt;/p&gt;
      &lt;p&gt;Finally, this analysis suggests a natural continuum between the two paradigms: by controlling task diversity through masking or reordering—we could design hybrid models that interpolate between compute efficiency (AR-like) and data efficiency (diffusion-like).&lt;/p&gt;
      &lt;p&gt;For more experiments and details please refer to original paper –https://arxiv.org/abs/2507.15857&lt;/p&gt;
      &lt;p&gt;Conclusion&lt;/p&gt;
      &lt;p&gt;As the availability of high-quality data plateaus, improving data efficiency becomes essential for scaling deep learning. In this work, we show that masked diffusion models consistently outperform autoregressive (AR) models in data-constrained regimes — when training involves repeated passes over a limited dataset. We establish new scaling laws for diffusion models, revealing their ability to extract value from repeated data far beyond what AR models can achieve.&lt;/p&gt;
      &lt;p&gt; These results challenge the conventional belief that AR models are universally superior and highlight diffusion models as a compelling alternative when data—not compute—is the primary bottleneck. Looking ahead, efficient use of finite data may define the next frontier in scaling deep learning models. Although the studies have been performed in the context of language models, we believe these findings should apply across any kind of sequence modeling data, such as in robotics or healthcare. For practitioners, our takeaway is simple: if you are compute-constrained, use autoregressive models; if you are data-constrained, use diffusion models.&lt;/p&gt;
      &lt;p&gt;Bibtex:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;@article{prabhudesai2025diffusion,&lt;lb/&gt;title={Diffusion Beats Autoregressive in Data-Constrained Settings},&lt;lb/&gt;author={Prabhudesai, Mihir and Wu, Mengning and Zadeh, Amir and Fragkiadaki, Katerina and Pathak, Deepak},&lt;lb/&gt;journal={arXiv preprint arXiv:2507.15857},&lt;lb/&gt;year={2025}&lt;lb/&gt;}&lt;/code&gt;
      &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.ml.cmu.edu/2025/09/22/diffusion-beats-autoregressive-in-data-constrained-settings/"/><published>2025-09-22T18:21:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45337450</id><title>Choose Your Own Adventure</title><updated>2025-09-22T20:37:12.973859+00:00</updated><content>&lt;doc fingerprint="277adbe5aba5ab1d"&gt;
  &lt;main&gt;
    &lt;p/&gt;
    &lt;quote&gt;
      &lt;p&gt;These books were the gateway drugs of interactive entertainment.&lt;/p&gt;
      &lt;p&gt;— Choose Your Own Adventure historian Christian Swineheart&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;My first experience with interactive media wasn’t mediated by any sort of digital technology. Instead it came courtesy of a “technology” that was already more than half a millennium old at the time: the printed book.&lt;/p&gt;
    &lt;p&gt;In the fall of 1980, I was eight years old, and doing my childish best to adjust to life in a suburb of Dallas, Texas, where my family had moved the previous summer from the vicinity of Youngstown, Ohio. I was a skinny, frail kid who wasn’t very good at throwing balls or throwing punches, which did nothing to ease the transition. Even when I wasn’t being actively picked on, I was bewildered at my new classmates’ turns of phrase (“I reckon,” “y’all,” “I’m fixin’ to”) that I had previously heard only in the John Wayne movies I watched on my dad’s knee. In their eyes, my birthplace north of the Mason Dixon Line meant that I could be dismissed as just another clueless, borderline useless “Yankee,” a heathen in the eyes of those who adhered to my new state’s twin religions of Baptist Christianity and Friday-night football.&lt;/p&gt;
    &lt;p&gt;I found my refuge in my imagination. I was interested in just about everything — a trait I’ve never lost, both to my benefit and my detriment in life — and I could sit for long periods of time in my room, spinning out fantasies in my head about school lessons, about books I’d read, about television shows I’d seen, even about songs I’d heard on the radio. I actually framed this as a distinct activity in my mind: “I’m going to go imagine now.” If nothing else, it was good training for becoming a writer. As they say, the child is the father of the man.&lt;/p&gt;
    &lt;p&gt;One Friday afternoon, I discovered a slim, well-thumbed volume in my elementary school’s scanty library. Above the title The Cave of Time was the now-iconic Choose Your Own Adventure masthead, proclaiming it to be the first book in a series. Curious as always, I opened it to the first page. I was precocious enough to know what was meant by a first-person and third-person narrator of written fiction, but this was something else: this book was written in the second person.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You’ve hiked through Snake Canyon once before while visiting your Uncle Howard at Red Creek Ranch, but you never noticed any cave entrance. It looks as though a recent rock slide has uncovered it.&lt;/p&gt;
      &lt;p&gt;Though the late afternoon sun is striking the surface of the cave, the interior remains in total darkness. You step inside a few feet, trying to get an idea of how big it is. As your eyes become used to the dark, you see what looks like a tunnel ahead, dimly lit by some kind of phosphorescent material on its walls. The tunnel walls are smooth, as if they were shaped by running water. After twenty feet or so, the tunnel curves. You wonder where it leads. You venture in a bit further, but you feel nervous being alone in such a strange place. You turn and hurry out.&lt;/p&gt;
      &lt;p&gt;A thunderstorm may be coming, judging by how dark it looks outside. Suddenly you realize the sun has long since set, and the landscape is lit only by the pale light of the full moon. You must have fallen asleep and woken up hours later. But then you remember something even more strange. Just last evening, the moon was only a slim crescent in the sky.&lt;/p&gt;
      &lt;p&gt;You wonder how long you’ve been in the cave. You are not hungry. You don’t feel you have been sleeping. You wonder whether to try to walk back home by moonlight or whether to wait for dawn, rather than risk your footing on the steep and rocky trail.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;All of this was intriguing enough already for a kid like me, but now came the kicker. The book asked me — asked me!! — whether I wanted to “start back home” (“turn to page 4”) or to “wait” (“turn to page 5”). This was the book I had never known I needed, a vehicle for the imagination like no other.&lt;/p&gt;
    &lt;p&gt;I took The Cave of Time home and devoured it that weekend. Through the simple expedient of flipping through its pages, I time-traveled to the age of dinosaurs, to the Battle of Gettysburg, to London during the Blitz, to the building of the Great Wall of China, to the Titanic and the Ice Age and the Middle Ages. Much of this history was entirely new to me, igniting whole new avenues of interest. Today, it’s all too easy to see all of the limitations and infelicities of The Cave of Time and its successors: a book of 115 pages that had, as it proudly trumpeted on the cover, 40 possible endings meant that the sum total of any given adventure wasn’t likely to span more than about three choices if you were lucky. But to a lonely, hyper-imaginative eight-year-old, none of that mattered. I was well and truly smitten, not so much by what the book was as by what I wished it to be, by what I was able to turn it into in my mind by the sheer intensity of that wish.&lt;/p&gt;
    &lt;p&gt;I remained a devoted Choose Your Own Adventure reader for the next couple of years. Back in those days, each book could be had for just $1.25, well within reach of a young boy’s allowance even at a time when a dollar was worth a lot more than it is today. Each volume had some archetypal-feeling adventurous theme that made it catnip for a kid who was also discovering Jules Verne and beginning to flirt with golden-age science fiction (the golden age being, of course, age twelve): deep-sea diving, a journey by hot-air balloon, the Wild West, a cross-country auto race, the Egyptian pyramids, a hunt for the Abominable Snowman. What they evoked in me was as important as what was actually printed on the page; each was a springboard for another weekend of fantasizing about exotic undertakings where nobody mocked you because you had two left feet in gym class and spoke with a stubbornly persistent Northern accent. And each was a springboard for learning as well; this process usually started with pestering my parents, and then, if I didn’t get everything I needed from that source, ended with me turning to the family set of Encyclopedia Britannica in the study. (I remember how when reading Journey Under the Sea I was confused by frequent references to “the bends.” I asked my mom what that meant, and, bless her heart, she said she thought the bends were diarrhea. Needless to say, this put a whole new spin on my underwater exploits until I finally did a bit of my own research about diving.)&lt;/p&gt;
    &lt;p&gt;Inevitably, I did begin to see the limitations of the format in time — right about the time that some of my nerdier classmates, whom I had by now managed to connect with, started to show me a tabletop game called Dungeons &amp;amp; Dragons. Choose Your Own Adventure had primed me to understand and respond to it right away; it would be no exaggeration to say that I saw this game that would remake so much of the entertainment landscape in its image as simply a better, less constrained take on the same core concept. Ditto the computer games that I began to notice in a corner of the bookstore I haunted circa 1984. When Infocom promised me that playing one of their games meant “waking up inside a story,” I knew exactly what they must mean: Choose Your Own Adventure done right. For the Christmas of 1984, I convinced my parents to buy me a disk drive for the Commodore 64 they had bought me the year before. And so the die was cast. If Choose Your Own Adventure hadn’t come along, I don’t think that I would be the Digital Antiquarian today.&lt;/p&gt;
    &lt;p&gt;But since I am the Digital Antiquarian, I have my usual array of questions to ask. Where did Choose Your Own Adventure, that gateway drug for the first generation to be raised on interactive media, come from? Who was responsible for it? The most obvious answer is the authors Edward Packard and R.A. Montgomery, one or the other of whose name could be seen on most of the early books in the series. But two authors alone do not a cultural phenomenon make.&lt;/p&gt;
    &lt;p/&gt;
    &lt;quote&gt;
      &lt;p&gt;“Will you read me a story?”&lt;/p&gt;
      &lt;p&gt;“Read you a story? What fun would that be? I’ve got a better idea: let’s tell a story together.”&lt;/p&gt;
      &lt;p&gt;— Adam Cadre, Photopia&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;During the twentieth century, when print still ruled the roost, the hidden hands behind the American cultural zeitgeist were the agents, editors, and marketers in and around the big Manhattan publishing houses, who decided which books were worth publishing and promoting, who decided what they would look like and even to a large extent how they would read. No one outside of the insular world of print publishing knew these people’s names, but the power they had to shape hearts and minds was enormous — arguably more so than that of any of the writers they served. After all, even the most prolific author of fiction or non-fiction usually couldn’t turn out more than one book per year, whereas an agent or editor could quietly, anonymously leave her fingerprints on dozens. Amy Berkower, a name I’m pretty sure you’ve never heard of, is a fine case in point.&lt;/p&gt;
    &lt;p&gt;Berkower joined Writers House, one of the most prestigious of the New York literary agencies, during the mid-1970s as a “secretarial girl.” Having shown herself to be an enthusiastic go-getter by working long hours and sitting in on countless meetings, she was promoted to the role of agent in 1977, but assigned to “juvenile publishing,” largely because nobody else in the organization wanted to work with such non-prestigious books. Yet the assignment suited Berkower just fine. “As a kid, I read and loved Nancy Drew before I went on to Camus,” she says. “I was in the right place at the right time. I didn’t have the bias that juvenile series wouldn’t lead to Camus.”&lt;/p&gt;
    &lt;p&gt;Thus when a fellow named Ray Montgomery came to her with a unique concept he called Adventures of You, he found a receptive audience. Montgomery was the co-owner of a small press called Vermont Crossroads, far removed from the glitz and glamor of Manhattan. Crossroads’s typical fare was esoteric volumes like Hemingway in Michigan and The Male Nude in Photography that generally weren’t expected to break four digits in total unit sales. A few years earlier, however, Montgomery had himself been approached by Edward Packard, a lawyer by trade who had already pitched a multiple-choice children’s book called Sugarcane Island to what felt like every other publisher in the country without success.&lt;/p&gt;
    &lt;p&gt;As he would find himself relating again and again to curious journalists in the decades to come, Packard had come up with his idea for an interactive book by making a virtue of necessity. During the 1960s, he was an up-and-coming attorney who worked long days in Manhattan, to which he commuted by train from his and his wife’s home in Greenwich, Connecticut. He often arrived home in the evening just in time to put his two daughters to bed. They liked to be told a bedtime story, but Packard was usually so exhausted that he had trouble coming up with one. So, he slyly enlisted his daughters’ help with the creative process. He would feed them a little bit of a story in which they were the stars, then ask them what they wanted to do next. Their answers would jog his tired imagination, and he would be off and running once again.&lt;/p&gt;
    &lt;p&gt;Sometimes, though, the girls would each want to do something different. “What would happen if you wrote both endings?” Packard mused to himself. A long-time frustrated writer as well as a self-described “lawyer who was never comfortable with the law,” Packard began to wonder whether he could turn his interactive bedtime stories into a new kind of book. By as early as 1969, he had invented the classic Choose Your Own Adventure format — turn to this page to do this, turn to that page to do that — and produced his first finished work in the style: the aforementioned Sugarcane Island, about a youngster who gets swept off the deck of a scientific research vessel by a sudden tidal wave and washed ashore on a mysterious Pacific island that has monsters, pirates, sharks, headhunters, and many another staple of more traditional children’s adventure fiction to contend with.&lt;/p&gt;
    &lt;p&gt;He was sure that it was “such a wonderful idea, I’d immediately find a big publisher.” He signed on with an agent, who “said he would be surprised if there were no takers,” recalls Packard. “Then he proceeded to be surprised.” One rejection letter stated that “it’s hard enough to get children to read, and you’re just making it harder with all these choices.” Letters like that came over and over again, over a period of years.&lt;/p&gt;
    &lt;p&gt;By 1975, Edward Packard was divorced from both his agent and his wife. With his daughters no longer of an age to beg for bedtime stories, he had just about resigned himself to being a lawyer forever. Then, whilst flipping through an issue of Vermont Life during a stay at a ski lodge, he happened upon a small advertisement from Crossroads Press. “Authors Wanted,” it read. Crossroads wasn’t the bright-lights, big-city publisher Packard had once dreamed of, but on a lark he sent a copy of Sugarcane Island to the address in the magazine.&lt;/p&gt;
    &lt;p&gt;It arrived on the desk of Ray Montgomery, who was instantly intrigued. “I Xeroxed 50 copies of Ed’s manuscript and took it to a reading teacher in Stowe,” Montgomery told The New York Times in 1981. “His kids — third grade through junior high — couldn’t get enough of it.” Satisfied by that proof of concept, Montgomery agreed to publish the book. Crossroads Press sold 8000 copies of Sugarcane Island over the next couple of years, a figure that was “unbelievable” by their modest standards. Montgomery was inspired to pen a book of his own in the same style, which he called Journey Under the Sea. The budding series was given the name Adventures of You — a proof that, whatever else they may have had going for them, branding was not really Crossroads Press’s strength.&lt;/p&gt;
    &lt;p&gt;Indeed, Montgomery himself was well able to see that he had stumbled over a concept that was too big for his little press. He sent the two extant books to Amy Berkower at Writers House and asked her what she thought. Having grown up on Nancy Drew, she was inclined to judge them less on their individual merits than on their prospects as a franchise in the making. A concept this new, she judged, had to have a strong brand of its own in order for children to get used to it. It would take her some time to find a publisher who agreed with her.&lt;/p&gt;
    &lt;p&gt;In the meantime, Edward Packard, heartened by the relative success of Sugarcane Island, was writing more interactive books. Although their names were destined to be indelibly linked in the annals of pop-culture history, Packard and Montgomery would never really be friends; they would always have a somewhat prickly, contentious relationship with one another. In an early signal of this, Packard chose not to publish more books through Crossroads. Instead he convinced the mid-list Philadelphia-based publisher J.B. Lippincott to take on Deadwood City, a Western, and Third Planet from Altair, a sci-fi tale. These served ironically to confirm Amy Berkower’s belief that there needed to be a concerted push behind the concept as a branded series; released with no fanfare whatsoever, neither sold all that well. Yet Lippincott did do Packard one brilliant service. Above the titles on the covers of the books, it placed the words “Choose your own adventures in the Wild West!” and “Choose your own adventures in outer space!” There was a brand in the offing in those phrases, even if Lippincott didn’t realize it.&lt;/p&gt;
    &lt;p&gt;For her part, Berkower was now more convinced than ever that this book-by-book approach was the wrong one. There needed to be a lot of these books, quickly, in order for them to take off properly. She made the rounds of the big publishing houses one more time. She finally found the ally she was looking for in Joëlle Delbourgo at Bantam Books. Delbourgo recalls getting “really excited” by the concept: “I said, ‘Amy, this is revolutionary.’ This is pre-computer, remember. The idea of interactive fiction, choosing an ending, was fresh and novel. It tapped into something very fundamental. I remember how I felt when I read the books, and how excited I got, the clarity I had about them.”&lt;/p&gt;
    &lt;p&gt;Seeing eye to eye on what needed to be done to cement the concept in the minds of the nation’s children, the two women drew up a contract under whose terms Bantam would publish an initial order of no fewer than six books in two slates of three. They would appear under a distinctive series trade dress, with each volume numbered to feed young readers’ collecting instinct. Barbara Marcus, Bantam’s marketing director for children’s books, needed only slightly modify the phrases deployed by J.B. Lippincott to create the perfect, pithy, and as-yet un-trademarked name for the series: Choose Your Own Adventure.&lt;/p&gt;
    &lt;p&gt;Berkower was acting as the agent of Montgomery alone up to this point. There are conflicting reports as to how and why Packard was brought into the fold. The widow of Ray Montgomery, who died in 2014, told The New Yorker in 2022 that her husband’s innate sense of fair play, plus the need to provide a lot of books quickly, prompted him to voluntarily bring Packard on as an equal partner. Edward Packard told the same magazine that it was Bantam who insisted that he be included, possibly in order to head off potential legal problems in the future.&lt;/p&gt;
    &lt;p&gt;At any rate, the first three Choose Your Own Adventure paperbacks arrived in bookstores in July of 1979. They were The Cave of Time, a new effort by Packard, written with some assistance from his daughter Andrea, she for whom he had first begun to tell his interactive stories; Montgomery’s journeyman Journey Under the Sea; and By Balloon to the Sahara, which Packard and Montgomery had subcontracted out to Douglas Terman, normally an author of adult military thrillers. Faced with an advertising budget that was almost nonexistent, Barbara Marcus devised an unusual grass-roots marketing strategy: “We did absolutely nothing except give the books away. We gave thousands of books to our salesmen and told them to give five to each bookseller and tell him to give them to the first five kids into his shop.”&lt;/p&gt;
    &lt;p&gt;The series sold itself, just as Marcus had believed it would. As The New York Times would soon write with a mixture of bemusement and condescension, it proved “contagious as chickenpox.” By September of 1980, around the time that I first discovered The Cave of Time, Publishers Weekly could report that Choose Your Own Adventure had become a “bonanza” for Bantam, which had sold more than 1 million copies of the first six volumes, with Packard and Montgomery now contracted to provide many more. A year later, eleven books in all had come out and the total sold was 4 million, with the series accounting for eight of the 25 bestselling children’s books at B. Dalton’s, the nation’s largest bookstore chain. A year after that, 10 million copies had been sold. By decade’s end, the total domestic sales of Choose Your Own Adventure would reach 34 million copies, with possibly that many or more again having been sold internationally after being translated into dozens of languages. The series was approaching its hundredth numbered volume by that point. It was a few years past its commercial peak already, but would continue on for another decade, until 184 volumes in all had come out.&lt;/p&gt;
    &lt;p&gt;Edward Packard, who turned 50 in 1981, could finally call himself an author rather than a lawyer by trade — and an astonishingly successful author at that, if not one who was likely to be given any awards by the literary elite. He and Ray Montgomery alone wrote about half of the 184 Choose Your Own Adventure installments. Packard’s prose was consistently solid and evocative without ever feeling like he was writing down to his audience, as the extract from The Cave of Time near the beginning of this article will attest; not all authors of children’s books, then or now, would dare to use a word like “phosphorescent.” If Montgomery was generally a less skilled wordsmith than Packard, and one who displayed less interest in producing internally consistent story spaces — weaknesses that I could see even as a young boy — he does deserve a full measure of credit for the pains he took to get the series off the ground in the first place. Looking back on the long struggle to get his brainstorm into print, Packard liked to quote the philosopher Arthur Schopenhauer: “Every original idea is first ridiculed, then vigorously attacked, and finally taken for granted.”&lt;/p&gt;
    &lt;p&gt;Although Packard at least was always careful to make his protagonists androgynous, it was no secret that Choose Your Own Adventure appealed primarily to boys — which was no bad thing on the whole, given that it was also no secret that reading in general was a harder sell with little boys than it was with little girls. Some educators and child psychologists kvetched about the violence that was undoubtedly one of the sources of the series’s appeal for boys — in just about all of the books, it was disarmingly easy to get yourself flamboyantly and creatively killed — but Packard was quick to counter that the mayhem was all very stylized, “exaggerated and melodramatic” rather than “harsh or nasty.” “Stupid” choices were presented to you all the time, he noted, but never “cruel” ones: “You as [the] reader never hurt anyone.”&lt;/p&gt;
    &lt;p&gt;One had to be a publishing insider to know that this “boys series” owed its enormous success as much to the packaging and promotional skills of three women — Amy Berkower, Joëlle Delbourgo, and Barbara Marcus — as it did to the literary talents of Packard and Montgomery. Berkower in particular became a superstar within the publishing world in the wake of Choose Your Own Adventure. Incredibly, the latter became only her second most successful children’s franchise, after the girl-focused Sweet Valley High, which could boast of 54 million copies sold domestically by the end of the 1980s; meanwhile The Baby-Sitters Club was coming up fast behind Choose Your Own Adventure, with 27 million copies sold. In short, her books were reaching millions upon millions of children every single month. Small wonder that she was made a full partner at Writers House in 1988; she was moving far more books each month than anyone else there.&lt;/p&gt;
    &lt;p&gt;Of course, any hit on the scale of Choose Your Own Adventure is bound to be copied. And this hit most certainly was, prolifically and unashamedly. During the middle years of the 1980s, when the format was at its peak, interactive books had whole aisles dedicated to them in bookstores. Which Way?, Decide Your Own Adventure, Pick-a-Path, Twisted Tales… branders did what they could when the best brand was already taken. While Choose Your Own Adventure remained archetypal in its themes and settings, other lines were unabashedly idiosyncratic: anyone up for a Do-It-Yourself Jewish Adventure? Publishers were quick to leverage other properties for which they owned the rights, from Doctor Who to The Lord of the Rings. TSR, the maker of that other school-cafeteria sensation Dungeons &amp;amp; Dragons, introduced an interactive-book line drawn from the game; even this website’s old friend Infocom came out with Zork books, written by the star computer-game implementor Steve Meretzky. Many of these books were content with the Choose Your Own Adventure approach of nothing but chunks of text tied to arbitrarily branching choices, but others grafted rules systems onto the format to effectively become solo role-playing games packaged as paperback books, with character creation and advancement, a dice-driven combat system, etc. The most successful of these lines was Fighting Fantasy, a name that is today almost as well-remembered as Choose Your Own Adventure itself in some quarters.&lt;/p&gt;
    &lt;p&gt;The gamebook boom was big and real, but relatively short-lived. By 1987, the decline had begun, for both Choose Your Own Adventure and all of the copycats and expansions upon its formula that it had spawned. Although a few of the most lucrative series, like Fighting Fantasy, would join the ur-property of the genre in surviving well into the 1990s, the majority were already starting to shrivel and fall away like apples in November. Demian Katz, the Internet’s foremost archivist of gamebooks, notes that this pattern has tended to hold true “in every country” where they make an appearance: “A few come out, they become explosively popular, a flood of knock-offs are released, they reach critical mass and then drop off into nothing.” It isn’t hard to spot the reason why in the context of 1980s North America. Computers were becoming steadily more commonplace — computers that were capable of bringing vastly more flexible forms of interactive storytelling to American children, via games that didn’t require one to read the same passages of text over and over again or to toss dice and keep track of a list of statistics on paper. The same pattern would be repeated elsewhere, such as in the former Soviet countries, most of which experienced their own gamebook boom and bust during the 1990s. It seems that the arrival of the commercial mass-market publishing infrastructure that makes gamebooks go is generally followed in short order by the arrival of affordable digital technology for the home, which stops them cold.&lt;/p&gt;
    &lt;p&gt;In the United States, Bantam Books tried throughout the 1990s to make Choose Your Own Adventure feel relevant to the children of that decade, introducing a more photo-realistic art style to accompany edgier, more traditionally novelistic plots. None of it worked. In 1999, after a good twelve years of slowly but steadily declining sales, Bantam finally pulled the plug on the series. Choose Your Own Adventure became just another nostalgic relic of the day-glo decade, to be placed on the shelf next to Michael Jackson’s Thriller, a Jane Fonda workout video, and that old Dungeons &amp;amp; Dragons Basic Set.&lt;/p&gt;
    &lt;p&gt;As of this writing, Choose Your Own Adventure is still around in a way, but the only real raison d’être it has left is nostalgia. In 2003, Ray Montgomery saw that Bantam Books had let the trademark for the series lapse, and formed his own company called Chooseco to try to revive it, mostly by republishing the old books that he had written himself. He met with mixed results at best. Since Montgomery’s death in 2014, Chooseco has continued to be operated by his family, who have used it increasingly as an instrument of litigation. In 2020, for example, Netflix agreed to settle for an undisclosed sum a lawsuit over “Bandersnatch,” a bold interactive episode of the critically lauded streaming series Black Mirror whose script unwisely mentioned the book series from which it drew inspiration.&lt;/p&gt;
    &lt;p&gt;A worthier successor on the whole is Choice Of Games, a name whose similarity to Choose Your Own Adventure can hardly be coincidental. Born out of a revival of the old menu-driven computer game Alter Ego, Choice Of has released dozens of digital branching stories over the past fifteen years. In being more adventurous than literary and basing themselves around broad, archetypal ideas — Choice of the Dragon, Choice of Broadsides, Choice of the Vampire — these games, which can run on just about any digital device capable of putting words on a screen, have done a fine job of carrying the spirit of Choose Your Own Adventure forward into this century. That said, there is one noteworthy difference: they are aimed at post-pubescent teens and adults — perhaps ones with fond memories of Choose Your Own Adventure — instead of children. “Play as male, female, or nonbinary; cis or trans; gay, straight, or bisexual; asexual and/or aromantic; allosexual and/or alloromantic; monogamous or polyamorous!” (Boring middle-aged married guy that I am, I must confess that I have no idea what three of those words even mean.)&lt;/p&gt;
    &lt;p&gt;Edward Packard, the father of it all, is still with us at age 94, still blogging from time to time, still a little bemused at how he became one of the most successful working authors in the United States during the 1980s. In a plot twist almost as improbable as some of his stranger Choose Your Own Adventure endings, his grandson is David Corenswet, the latest actor to play Superman on the silver screen. Never a computer gamer, Packard would doubtless be baffled by most of what is featured on this website. And yet I owe him an immense debt of gratitude, for giving me my first glimpse of the potential of interactive storytelling, thus igniting a lifelong obsession. I suspect that more than one of you out there might be able to say the same.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Sources: Publishers Weekly of February 29 1980, September 26 1980, October 8 1982, July 25 1986, August 12 1988, December 1 1989, July 6 1990, February 23 1998; New York Times of August 25 1981; Beaver County Times of March 30 1986; New Yorker of September 19 2022; Journal of American Studies of May 2021.&lt;/p&gt;
    &lt;p&gt;Online sources include “A Brief History of Choose Your Own Adventure“ by Jake Rossen at Mental Floss, “Choose Your Own Adventure: How The Cave of Time Taught Us to Love Interactive Entertainment” by Grady Hendrix at Slate, “The Surprising Long History of Choose Your Own Adventure Stories” by Jackie Mansky at the Smithsonian’s website, and “Meet the 91-Year-Old Mastermind Behind Choose Your Own Adventure“ by Seth Abramovitch at The Hollywood Reporter. Plus Edward Packard’s personal site. And Damian Katz’s exhaustive gamebook site is essential to anyone interested in these subjects; all of the book covers shown in this article were taken from his site.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;↑1&lt;/cell&gt;
        &lt;cell&gt;A truly incredible figure of 250 million copies sold is frequently cited for the original Choose Your Own Adventure series today, apparently on the basis of a statement released in January of 2007 by Choosco, a company which has repeatedly attempted to reboot the series in the post-millennial era. Based upon the running tally of sales which appeared in Publishers Weekly during the books’ 1980s heyday, I struggle to see how this figure can be correct. That journal of record reported 34 million Choose Your Own Adventure books sold in North America as of December 1, 1989. By that time, the series’s best years as a commercial proposition were already behind it. Even when factoring in international sales, which were definitely considerable, it is difficult to see how the total figure could have exceeded 100 million books sold at the outside. Having said that, however, the fact remains that the series sold an awful lot of books by any standard.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.filfre.net/2025/09/choose-your-own-adventure/"/><published>2025-09-22T18:22:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45338561</id><title>Fine-grained HTTP filtering for Claude Code</title><updated>2025-09-22T20:37:12.679296+00:00</updated><content>&lt;doc fingerprint="fcf5f913cb30106f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fine-grained HTTP filtering for Claude Code&lt;/head&gt;
    &lt;p&gt;Coding agents are becoming more powerful every day without commensurate security and governance tooling. The result is a world where solo developers happily run &lt;code&gt;claude --dangerously-skip-permissions&lt;/code&gt; for hours unmoderated while many of the world's most important organizations have barely tried agentic developmentLearned from our experience at Coder . I've been working on a tool called &lt;code&gt;httpjail&lt;/code&gt; in an effort to make agents available everywhere.&lt;/p&gt;
    &lt;p&gt;The tool is focused on mitigating these classes of risks:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Agents performing destructive actions&lt;/cell&gt;
        &lt;cell&gt;Deleting your database&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Agents leaking sensitive information&lt;/cell&gt;
        &lt;cell&gt;Exposing API keys or credentials&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Agents operating with more authority than desired&lt;/cell&gt;
        &lt;cell&gt;Pushing straight to main instead of opening a PR&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Agents may transgress accidentally (user misinterpretation) or intentionally (prompt injection).&lt;/p&gt;
    &lt;p&gt;There is a class of risks at the file-system interface too, but, I believe existing tooling (containers) is sufficient here. Existing network isolation tools rely on IP-based rules. In our case, they're imprecisecentralized, anycast load balancers power much of the internet and require constant maintenanceIPs change randomly and they're not a part of a service's implicit "contract".&lt;/p&gt;
    &lt;head rend="h2"&gt;httpjail&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; implements an HTTP(S) interceptor alongside process-level network isolation. Under default configuration, all DNS (udp:53) is permitted and
all other non-HTTP(S) traffic is blocked.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; rules are either JavaScript expressions or custom programs. This approach makes
them far more flexible than traditional rule-oriented firewalls and avoids the learning curve of a DSL.&lt;/p&gt;
    &lt;p&gt;Block all HTTP requests other than the LLM API traffic itself:&lt;/p&gt;
    &lt;code&gt;$ httpjail --js "r.host === 'api.anthropic.com'" -- claude "build something great"
&lt;/code&gt;
    &lt;p&gt;Allow only &lt;code&gt;GET&lt;/code&gt; requests i.e. make the internet read-only:&lt;/p&gt;
    &lt;code&gt;$ httpjail --js "r.method === 'GET'" -- claude "build something great"
&lt;/code&gt;
    &lt;p&gt;Only allow hosts in a &lt;code&gt;whitelist.txt&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;$ httpjail --sh "grep -qx \"$HTTPJAIL_HOST\" whitelist.txt" -- claude "research these APIs"
&lt;/code&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;In a nutshell:&lt;/p&gt;
    &lt;p&gt;Strong| NS[Create namespace&lt;/p&gt;
    &lt;p&gt;+ nftables redirect&lt;/p&gt;
    &lt;p&gt;+ setuid $SUDO_USER] Start --&amp;gt;|macOS/--weak&lt;/p&gt;
    &lt;p&gt;Weak| Env[Set $HTTP_PROXY&lt;/p&gt;
    &lt;p&gt;env vars] end subgraph "2. Target Process" direction TB Target[Target Process&lt;/p&gt;
    &lt;p&gt;e.g., claude] Target --&amp;gt; Request[HTTP/HTTPS&lt;/p&gt;
    &lt;p&gt;Request] Request --&amp;gt; Route{Route&lt;/p&gt;
    &lt;p&gt;Request} end subgraph "3. Interception" direction TB Proxy[Proxy :8080/:8443] Proxy --&amp;gt; Rules{Evaluate&lt;/p&gt;
    &lt;p&gt;JS/Script Rules} end subgraph "4. Result" direction TB Internet[✓ Internet Access] Blocked[✗ 403 Blocked] Bypass[⚠️ BYPASSED!&lt;/p&gt;
    &lt;p&gt;weak mode only] end NS --&amp;gt; Target Env --&amp;gt; Target Route --&amp;gt;|Strong: forced&lt;/p&gt;
    &lt;p&gt;via nftables| Proxy Route --&amp;gt;|Weak: respects&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY| Proxy Route --&amp;gt;|Weak: ignores&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY| Bypass Rules --&amp;gt;|Allow| Internet Rules --&amp;gt;|Deny| Blocked style NS fill:#00d4ff,color:#0a0a0a style Env fill:#00d4ff,color:#0a0a0a style Proxy fill:#404040 style Blocked fill:#8b2635 style Internet fill:#2a7f62 style Bypass fill:#8b2635&lt;/p&gt;
    &lt;head rend="h3"&gt;macOS (Weak Mode)&lt;/head&gt;
    &lt;p&gt;macOS uses &lt;code&gt;--weak/-w&lt;/code&gt; mode by default (see #7).&lt;/p&gt;
    &lt;p&gt;In weak mode, we rely on process cooperation via the standard &lt;code&gt;HTTP_PROXY&lt;/code&gt;/&lt;code&gt;HTTPS_PROXY&lt;/code&gt; environment variables. This mode is less of a jail and more of a suggestion that the majority of
well-meaning applications happen to comply with.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLS Interception&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; implements full TLS interception to inspect and filter HTTPS traffic. Without interception, rules would only have access to the hostname via SNI, and most of the power of this tool would be lost.&lt;/p&gt;
    &lt;head rend="h4"&gt;How TLS Interception Works&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Certificate Authority Generation: On first run,&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;generates a self-signed Certificate Authority (CA) that's stored in&lt;code&gt;~/.config/httpjail/&lt;/code&gt;. This CA is used to sign certificates for intercepted connections.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Dynamic Certificate Generation: When a client connects,&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;Extracts the Server Name Indication (SNI) from the TLS ClientHello&lt;/item&gt;&lt;item&gt;Generates a certificate on-the-fly for that specific hostname &lt;list rend="ul"&gt;&lt;item&gt;Uses a shared ECDSA P-256 key pair for all server certificates for O(1) keygen overhead&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Signs it with the CA certificate&lt;/item&gt;&lt;item&gt;Caches certificate in memory for performance&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dual Mode Operation:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Transparent Proxy Mode: Directly accepts TLS connections by detecting the TLS ClientHello packet (starts with &lt;code&gt;0x16&lt;/code&gt;)&lt;/item&gt;
          &lt;item&gt;Explicit Proxy Mode: Handles HTTP CONNECT tunnels, responds with "200 Connection Established", then transitions to Transparent Proxy Mode&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Transparent Proxy Mode: Directly accepts TLS connections by detecting the TLS ClientHello packet (starts with &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Trust Injection: The CA certificate must be trusted by client applications.&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;automatically sets environment variables for common tools:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;SSL_CERT_FILE&lt;/code&gt;/&lt;code&gt;SSL_CERT_DIR&lt;/code&gt;for OpenSSL-based tools&lt;/item&gt;&lt;item&gt;&lt;code&gt;CURL_CA_BUNDLE&lt;/code&gt;for curl&lt;/item&gt;&lt;item&gt;&lt;code&gt;NODE_EXTRA_CA_CERTS&lt;/code&gt;for Node.js&lt;/item&gt;&lt;item&gt;&lt;code&gt;REQUESTS_CA_BUNDLE&lt;/code&gt;for Python requests&lt;/item&gt;&lt;item&gt;&lt;code&gt;GIT_SSL_CAINFO&lt;/code&gt;for Git&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Jail Escapes&lt;/head&gt;
    &lt;p&gt;In the weak jail, it's trivial for the agent to escape the jail by funnelling requests through a shim program that disregards the &lt;code&gt;HTTP_PROXY&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;p&gt;Even the strong jail is not perfect. There are potential escape hatches in the filesystem. For example, the agent could create a container via a Docker socket which would spawn outside the network namespace.&lt;/p&gt;
    &lt;p&gt;To combine filesystem and network isolation into one, &lt;code&gt;httpjail&lt;/code&gt; has a &lt;code&gt;--docker-run&lt;/code&gt; flag that works like this:&lt;/p&gt;
    &lt;code&gt;httpjail --js "r.host === 'api.github.com'" --docker-run -- \
    --rm alpine:latest wget -qO- https://api.github.com
&lt;/code&gt;
    &lt;p&gt;I believe there's still much value in this approach even with imperfect isolation. In my experience, models seldom try to escape restrictions intentionally placed by the user. And, if the jail is rendered ineffective by prompt injection, it wasn't doing its job in the first place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Server Mode&lt;/head&gt;
    &lt;p&gt;For the strongest level of isolation, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run &lt;code&gt;httpjail --server&lt;/code&gt;on a standalone server.&lt;/item&gt;
      &lt;item&gt;Configure the network firewall to only permit 80/443 traffic to the proxy server. &lt;list rend="ul"&gt;&lt;item&gt;Optionally, you may redirect all traffic to the proxy server, otherwise you will need to take care in ensuring HTTP_PROXY is set in your environments and all of your web-faring applications respect it.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Run processes as usual in the development environment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Request] Check{Respects&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY?} end subgraph "Network Layer" FW[Network Firewall] Config{Config&lt;/p&gt;
    &lt;p&gt;Mode?} FW --&amp;gt; Config Config --&amp;gt;|Redirect All&lt;/p&gt;
    &lt;p&gt;80/443 Traffic| Force[Forced to Proxy] Config --&amp;gt;|Allow Only&lt;/p&gt;
    &lt;p&gt;Proxy IP| Check Check --&amp;gt;|Yes| Voluntary[To Proxy] Check --&amp;gt;|No| Drop[✗ Dropped] end subgraph "httpjail --server" Decision{Evaluate&lt;/p&gt;
    &lt;p&gt;Rules} end subgraph Result direction TB API[✓ Allowed APIs] Blocked[✗ Blocked] end Request --&amp;gt; FW Force --&amp;gt; Decision Voluntary --&amp;gt; Decision Decision --&amp;gt;|Allow| API Decision --&amp;gt;|Deny| Blocked style Request fill:#404040 style Decision fill:#00d4ff,color:#0a0a0a style FW fill:#404040 style Blocked fill:#8b2635 style Drop fill:#8b2635 style API fill:#2a7f62 style Force fill:#2a7f62 style Voluntary fill:#4a7c59&lt;/p&gt;
    &lt;head rend="h2"&gt;Try it out&lt;/head&gt;
    &lt;code&gt;cargo install httpjail
&lt;/code&gt;
    &lt;p&gt;And, check out the GitHub repository for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ammar.io/blog/httpjail"/><published>2025-09-22T19:49:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45338625</id><title>Unweaving warp specialization on modern tensor core GPUs</title><updated>2025-09-22T20:37:12.519843+00:00</updated><content>&lt;doc fingerprint="e24a939688344faa"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently, I have been thinking deeply about warp specialization in the context of high performance kernels for modern Tensor Core GPUs like NVIDIA’s H100 and B200. My understanding of what warp specialization achieves has deepened and led me to the interesting question of: do we actually need warp specialization (and the complexity that it entails)? My conclusion is that the answer is indeed yes, but it might not be as mandatory as it seems. In this post, I’ll discuss when warp specialization is actually necessary, and describe the underlying trade-off space that I believe warp specialization resides within. While I will give some context on GPUs as necessary for discussing the topics at hand, this won’t be a tutorial – some experience with GPUs and parallel programming will be assumed.&lt;/p&gt;
    &lt;head rend="h1"&gt;Background&lt;/head&gt;
    &lt;p&gt;A GPU is a collection of processors called streaming multiprocessors (SM’s). For this discussion, we will focus on programming an individual SM. An SM is programmed with a hierarchy of threads, called a thread block. Threads in a thread block are further grouped into warps, which are groups of 32 threads. Each warp executes in a single-instruction-multiple-threads (SIMT) model. Each thread in a warp has its own instruction stream, and the warp issues one instruction on behalf of its threads in each issue slot. Performance is maximized (as discussed later) when all threads in a warp want to issue the same instruction at the same time. A Hopper SM (pictured below) has four execution contexts that can host an active warp, shown by the 4 quadrants.&lt;/p&gt;
    &lt;p&gt;At any cycle, at most 4 warps may issue instructions into the SM to execute. When a thread block contains more than 4 warps worth of threads (128), a hardware component called the warp scheduler selects 4 available warps to execute instructions.&lt;/p&gt;
    &lt;p&gt;A way to view an SM is as a collection of functional units (arithmetic units, load/store units, a Tensor Core) that are issued instructions at each clock cycle from the 4 execution contexts. These functional units have varying properties. Arithmetic units (ALU’s) perform individual math operations with short and fixed cycle latencies, Tensor Cores perform thousands of FLOPs in a single instruction with long cycle latencies, and load/store units (LSU’s) have long and unpredictable latencies due to interacting with the memory system. High performance GPU programs efficiently utilize the available functional units; compute-bound programs should use the Tensor Core and ALU’s at every clock cycle, while bandwidth-bound programs should keep the LSU’s busy to maximize bandwidth. To achieve high utilization, there must be work present for the functional units to perform (i.e. the floating point operations in a compute bound application should not be stalled waiting for loads to complete), and this available work must be issued into the functional units whenever they are available. This second aspect is where warp specialization becomes useful.&lt;/p&gt;
    &lt;head rend="h1"&gt;Warp Specialization&lt;/head&gt;
    &lt;p&gt;Warp specialization is a technique that became popularized through work on CUDA-DMA and the Singe Compiler, and is now a table-stakes technique for achieving high Tensor Core performance on Hopper and Blackwell GPUs. Warp specialization exploits the hierarchical grouping of threads within a thread block. When threads within the same warp diverge (i.e. branch on control flow in different ways), the SIMT nature of each warp results in performance degradation. Suppose that a warp reaches a branch where half the threads take the branch and the other half do not. The warp will now execute instructions from either side of the branch; when the warp selects an instruction from one side of the branch, the threads executing the other side do not progress. As a result, execution may take twice as long than if all threads in the warp took the same path through the branch. In the worst case, if all 32 threads in a warp take different control flow paths, the code could execute 32-times slower than the ideal! Unlike different threads within a warp, different warps within a thread block execute independently on separate execution contexts, which means that there is no cost when divergence occurs between warps. Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each warp, while a warp specialized program uses different warps to execute different components of the overall program. Let’s take a look at some of these warp specialization strategies in the aforementioned contexts.&lt;/p&gt;
    &lt;p&gt;The CUDA-DMA project proposed separating the loading of data from the GPU’s global (slow) memory to shared (fast) memory from the computation on data in the shared memory itself. CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the loaded data is available.&lt;/p&gt;
    &lt;p&gt;The Singe compiler targeted the generation of efficient combustion chemistry kernels. For the purposes of this post, these kernels essentially looked like large data-parallel computations (i.e. apply some function \(f\) to each element of an array) with a catch: computing \(f\) requires a large amount of intermediate state (numerous temporary variables in the chemical formulae). A straightforward implementation of these kernels requires too many registers to store the intermediate state and spills values to the stack, which lowers performance significantly. The annoying bit here is that the SM’s register file has enough space to store all the temporaries. However, the architecture provides each thread with a fixed number of accessible registers (for example, 255 per thread on Hopper). Singe used warp specialization to bypass the register-per-thread limit by partitioning the computation of \(f\) onto different warps. Concretely, suppose \(f(x) = 1 + x + 2 \cdot x + x^2 + 8 \cdot x^3\). Assuming a small register-per-thread budget, a warp specialized implementation of \(f\) might place the computation of \(1 + x + 2\cdot x\) onto warp one, and place \(x^2 + 8\cdot x^3\) onto warp two; the two warps would then communicate to sum the intermediate values.&lt;/p&gt;
    &lt;p&gt;Finally, warp specialization is used in high performance Tensor Core kernels targeting Hopper and Blackwell to interact with the accelerators appearing within the SM. On these GPUs, the SM contains accelerators that perform matrix multiplication (Tensor Core) and data movement to/from global memory (Tensor Memory Accelerator, or TMA). These accelerators offer instructions to multiply tiles of data or copy tiles of data to and from global memory. These accelerators are also asynchronous, where work on the accelerator is launched by a single instruction and then a blocking “wait” operation must be issued before using the results of the instruction. Specialized warps are used on Hopper and Blackwell to issue either TMA copies or Tensor Core matrix-multiplies. The TMA warp issue copies and notifies the Tensor Core warps when data is ready to be multiplied, and the Tensor Core warps notify the TMA warp when data has been consumed and the memory is free to use for more copies. This code looks something like:&lt;/p&gt;
    &lt;code&gt;if warpid() == LOAD:
  for i, tile in enumerate(tiles):
    if i &amp;gt; 0:
      wait_for_tile_release()
    async_tma_load(tile)
    wait_for_tma_load()
    signal_tile_loaded()
else:
  for tile in enumerate(tiles):
    wait_for_tile_loaded()
    tile_data = get_loaded_tile(tile)
    async_mma(tile_data)
    wait_for_async_mma()
    signal_tile_released()
&lt;/code&gt;
    &lt;p&gt;Significantly more complex warp specialization strategies can be found in Tensor Core kernels that do more than just matrix-multiplication. For example, a high performance Flash Attention implementation on Blackwell uses at least 5 different kinds of specialized warps! In this Flash Attention implementation, there are warps for loading data, issuing matrix multiplication, computing softmax, scaling intermediate results, and storing data. As a result, the code is complex; the strategy itself is carefully constructed to yield high performance, and there is abundant cross-warp data movement and synchronization. Imagine the code above with 5 different warp cases and each cases signaling the others to proceed at different times!&lt;/p&gt;
    &lt;head rend="h1"&gt;Why is Warp Specialization Good?&lt;/head&gt;
    &lt;p&gt;The complexity of this Flash Attention implementation inspired me to take a step back and investigate the role of warp specialization in achieving high performance with the Tensor Cores. I had taken this need for warp specialization in Tensor Core kernels as a given; people who knew more than I told me that it was required, and I didn’t question (which is embarrassing for me, as an academic). In addition, other explanations of warp specialization out there often say vague things like “the architecture mandates it” or “it is needed for creating producer-consumer pipelines”.&lt;/p&gt;
    &lt;p&gt;Let’s derive from first principles when warp specialization is useful. An SM has some fixed number of compute resources available (i.e. ALU’s, LSU’s, a Tensor Core) and issue slots per clock cycle, regardless of how many warps a thread block uses. Therefore, a kernel has the same theoretical peak compute throughput and peak instructions issued per cycle on an H100 SM whether it uses 4 or 64 warps. So where are the benefits coming from? Consider two versions of a target program: one that is warp specialized and one that is not. The warp specialized kernel uses more than 4 warps to issue potentially different instruction streams into the SM, and the warps themselves are dynamically interleaved by the instruction scheduler. The standard program uses a single instruction stream issued from 4 identical warps. Clearly, warp specialization can only impact performance when the dynamically interleaved stream of instructions from more than 4 warps differs from the statically-specified instruction stream issued from the 4 warps in the standard program. The conditions that cause these proposed instruction streams to differ are the conditions where warp specialization can deliver increased performance. I believe there are three cases where this occurs.&lt;/p&gt;
    &lt;p&gt;The first case is simple to identify, and it is targeted by Singe: there does not exist a non-specialized version due to resource limitations! If the non-specialized version would use too many registers, predicates, or other warp-constrained SM resources, then warp specialization version allows for those resource constraints to be satisfied. Warp specialization due to resource constraints is commonplace in Hopper Tensor Core kernels, where accumulator tiles are split across multiple groups of warps to stay below the register-per-thread limit and spilling to the stack1.&lt;/p&gt;
    &lt;p&gt;The second case is a little trickier, since a non-specialized version of the target program must exist. This case involves discussing instruction scheduling. The SM contains several independent functional units, like FP units for floating point arithmetic and INT units for integer arithmetic, that may be executing operations at the same time. Consider a program with 2 floating point operations followed by 2 independent integer operations; a good instruction schedule would order the instructions to issue one floating point operation followed by one integer operation to utilize the FP units at the same time as the INT units. When a compiler (like NVCC) has accurate information about the number of cycles that each instruction takes, it can produce high quality static schedules that exploit instruction-level parallelism (ILP) to overlap independent instructions on different functional units. However, instruction scheduling becomes difficult for the compiler when the cycle counts of instructions are imprecise. Statically constructing a tight schedule when an instruction may take between 10-100 cycles instead of always 25 cycles is significantly harder. This is precisely the second case where warp specialization is useful: dynamic scheduling with the warp scheduler can gracefully handle variable instruction latency, which is common for memory-related operations. In this case, the statically-scheduled, non-specialized program must guess how long each variable-latency operation takes and construct a schedule that interleaves the variable-latency operation with other fixed-latency operations. When variable latency operations execute faster than expected, functional units end up under-utilized, and slower-than-expected operations result in stalls. A warp specialized implementation avoids the guesswork and interleaves instructions at runtime with the warp scheduler.&lt;/p&gt;
    &lt;p&gt;The third (and final) reason that I am aware of is related to the difficulties with variable-latency instructions, but gets further into details about GPU architecture. To set the stage, we’ll contrast GPU architecture with that of general purpose CPU’s. Modern CPU’s are out-of-order (OOO) issue processors; while a CPU processes a sequence of instructions, it finds ways to dynamically reorder those instructions to exploit ILP while maintaining an illusion of sequential execution. Concretely, if a CPU executes the instructions &lt;code&gt;addf r1 r2; addf r3 r4; addi r5 r6; addi r7 r8&lt;/code&gt; (the scheduling example from earlier)
it may automatically pull the independent &lt;code&gt;addi&lt;/code&gt; instructions earlier and execute them while the &lt;code&gt;addf&lt;/code&gt; instructions execute.
The compiler can help this scheduling hardware by partially reordering instructions, but the OOO capabilities of the processor do some heavy lifting.
The main downside of OOO is that the hardware required to implement it is costly — instead, GPUs
save chip area and energy by being in-order issue. When the GPU instruction stream contains
&lt;code&gt;addf r1 r2; addf r3 r4; addi r5 r6; addi r7 r8&lt;/code&gt;, the instructions are issued to the SM in that order;
if the compiler didn’t perform any reordering, the SM will have under-utilized functional units. This problem is exacerbated when we introduce
the synchronization instructions that are used to interact with asynchronous accelerators (Tensor Core, TMA). These synchronization
instructions are essentially semaphores that put warps to sleep until the invoked accelerator
has completed. If these synchronization instructions are placed sub-optimally in the instruction stream, the warp may be blocked
from executing independent instructions until the operation being synchronized against completes. While compilers are pretty good
at scheduling, there are two difficulties that these synchronization operations introduce: 1) sychronization operations can often
act as code-motion barriers, as proving correctness of reordering operations (especially those that touch memory) around synchronization can be difficult, and 2) it can be difficult
for the compiler to track exactly what operation will resolve a synchronization point (i.e. which thread will release a lock).
Warp specialization allows the programmer to ignore the effects of this synchronization: by breaking the computation into separate warps, other warps
can immediately execute as soon as one warp blocks waiting on synchronization.&lt;/p&gt;
    &lt;p&gt;Summarizing the last few paragraphs, warp specialization can provide benefits when:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;An application’s resource requirements overflow the resources available to a single thread or warp.&lt;/item&gt;
      &lt;item&gt;An application contains variable latency operations that need to be interleaved to maximize utilization.&lt;/item&gt;
      &lt;item&gt;An application contains blocking synchronization that must be placed intelligently to avoid stalling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Conditions 2 and 3 are inherently intertwined, as the source of both conditions is complications that make producing optimal static instruction schedules for in-order processors difficult. A view of warp specialization that gets at why it helps in this circumstance is that warp specialization essentially turns the SM from an in-order processor into a quasi-out-of-order processor, specifically at the specialization boundaries at each warp. The specialization points chosen by the warp specialization strategy indicate where ILP may be found (but is difficult to realize statically) while instructions within a specialized warp can be reasonably scheduled statically.&lt;/p&gt;
    &lt;head rend="h1"&gt;Testing the Hypothesis&lt;/head&gt;
    &lt;p&gt;The previous section concluded that warp specialization is useful when 1) resource constraints mandate specialization, 2) variable latency instructions are difficult to statically schedule, and 3) blocking synchronization interferes with instruction issue. Let’s examine how (and if) these cases appear when developing a high performance H100 GEMM implementation for the problem size 8192x8192x8192. To do these experiments, I hand-modified code generated by the Cypress compiler; the generated code is too ugly to present, so I’ll discuss it at the level of vaguely-Pythonic psuedocode. An FP16 H100 GEMM roughly has the structure shown below.&lt;/p&gt;
    &lt;p&gt;An efficient GEMM orchestrates a software pipeline where at most &lt;code&gt;PIPE&lt;/code&gt; outstanding
TMA loads of tiles from global memory to shared memory are pending, while GEMM operations execute
in parallel on the Tensor Core. In existing implementations, these two pipeline components run on
separate specialized warps. More detailed code than before describing a multiplication &lt;code&gt;C = A @ B&lt;/code&gt;
is shown below.&lt;/p&gt;
    &lt;code&gt;# Initialize circular buffers holding PIPE tiles of A and B.
Abuf = [tile()] * PIPE
Bbuf = [tile()] * PIPE
if warpid() == LOAD:
  for k in K / KTILE:
    if k &amp;gt; PIPE:
      wait_for_compute_iter(k - PIPE)
    # The circular buffer index used by iteration k is k % PIPE.
    async_tma_load(tile(A, k), Abuf[k % PIPE])
    async_tma_load(tile(B, k), Bbuf[k % PIPE])
    signal_when_load_complete_iter(k)
  wait_all_mmas_done()
  copy_C_shared_memory_to_global()
else:
  C = init_accumulator()
  for k in K / KTILE:
    wait_for_load_complete(k)
    async_mma(C, Abuf[k % PIPE], Bbuf[k % PIPE])
    wait_for_mma()
    signal_compute_iter_done(k)
  store_C_into_shared_memory()
  notify_all_mmas_done()
&lt;/code&gt;
    &lt;p&gt;Let’s examine this code and understand which warp specialization conditions apply. Tuning yields that the best performing tile size for the accumulator &lt;code&gt;C&lt;/code&gt; is 256x256.
The accumulator &lt;code&gt;C&lt;/code&gt; for H100 must be placed in the registers. Simple math
shows (256 * 256 FP16 elements / 128 threads (4 warps, 1 for each execution context) / 2 FP16 elements per 32-bit register = 256 registers) the accumulator
breaks the register-per-thread limit, satisfying condition 1:  at least two groups of 4 warps are needed
to store &lt;code&gt;C&lt;/code&gt;. What about the other two conditions? It’s not completely
clear that the synchronization or instruction scheduling is so hard that we need to move the
data loading to a separate warp. If we use a deep enough pipeline and synchronize only when
we already have pending work issued, we might be able to get away without warp specialization.
A non-specialized version of the GEMM above is shown below; some additional reordering and loop
peeling is required to start the pipeline, but the structure is similar.&lt;/p&gt;
    &lt;code&gt;Abuf = [tile()] * PIPE
Bbuf = [tile()] * PIPE
# Split the accumulator into 2 pieces for each
# group of warps to access.
C = init_accumulator(warpid())
# Issue the first PIPE outstanding loads.
for k in PIPE:
  async_tma_load(tile(A, k), Abuf[k])
  async_tma_load(tile(B, k), Bbuf[k])
# Main loop executes MMAs and loads into the future.
for k in [PIPE, K / KTILE):
  # Logically, this MMA is for iteration k - PIPE.
  wait_for_load_complete(k - PIPE)
  async_mma(C, Abuf[k % PIPE], Bbuf[k % PIPE]) 
  # This MMA wait will also include waiting on
  # the paired cluster thread block.
  wait_for_mma()
  # Start the next load.
  async_tma_load(tile(A, k), Abuf[k % PIPE])
  async_tma_load(tile(B, k), Bbuf[k % PIPE])
# Perform the trailing PIPE MMA operations.
for k in [K / KTILE - PIPE, K / KTILE):
  wait_for_load_complete(k)
  async_mma(C, Abuf[k % PIPE], Bbuf[k % PIPE]) 
  wait_for_mma()
# Copy out the accumulator to global memory (elided).
&lt;/code&gt;
    &lt;p&gt;Unfortunately, it doesn’t perform as well as I hoped:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./gemm_nows_v1 --m 8192 --n 8192 --k 8192
MY_GEMM:         [675868.8]GFlop/s  (1.6268)ms
CUBLAS_GEMM:     [805409.4]GFlop/s  (1.3652)ms
&lt;/code&gt;
    &lt;p&gt;This indicates that one of conditions 2 or 3 is applicable, causing under-utilization of either the TMA or Tensor Core. However, all hope is not lost — what stuck out to me about this program was the &lt;code&gt;wait_for_mma()&lt;/code&gt; operation inside the main loop,
which blocks the warp from issuing more loads until the pending MMA completes, which in turn
could lead to stalling the issue of future MMA’s. The solution here is to pipeline the loop again, where we also have some pending MMA’s
issued before the synchronization, hoping that the synchronization is now covered by already issued work. The code now looks something like this:&lt;/p&gt;
    &lt;code&gt;Abuf = [tile()] * PIPE
Bbuf = [tile()] * PIPE
# Split the accumulator into 2 pieces for each
# group of warps to access.
C = init_accumulator(warpid())
# Issue the first PIPE outstanding loads.
for k in PIPE:
  async_tma_load(tile(A, k), Abuf[k])
  async_tma_load(tile(B, k), Bbuf[k])
# Start the k=0 MMA, but don't wait for it.
wait_for_load_complete(0)
async_mma(C, Abuf[0], BBuf[0])
for k in [PIPE, K / KTILE):
  # Logically, this MMA is for iteration k - PIPE + 1.
  wait_for_load_complete(k - PIPE + 1)
  async_mma(C, Abuf[(k - PIPE + 1) % PIPE], Bbuf[(k - PIPE + 1) + 1]) 
  # Wait for the MMA from iteration k - PIPE, rather
  # than the MMA we just launched (which is k - PIPE + 1).
  wait_for_mma(k - PIPE + 1)
  # Start the next load.
  async_tma_load(tile(A, k), Abuf[k % PIPE])
  async_tma_load(tile(B, k), Bbuf[k % PIPE])
# Perform the trailing PIPE MMA operations.
for k in [K / KTILE - PIPE + 1, K / KTILE):
  wait_for_load_complete(k)
  async_mma(C, Abuf[k % PIPE], Bbuf[k % PIPE]) 
  wait_for_mma()
&lt;/code&gt;
    &lt;p&gt;Behold!&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./gemm_nows_v2 --m 8192 --n 8192 --k 8192
MY_GEMM:         [815881.7]GFlop/s  (1.3476)ms
CUBLAS_GEMM:     [807708.0]GFlop/s  (1.3613)ms
&lt;/code&gt;
    &lt;p&gt;We can achieve performance competitive with CuBLAS in this specific case without separating the TMA loads into a separate warp. For this problem instance, it was possible to mess with the loops by hand to avoid the effects of conditions 2 and 3. I’ll even argue here that while condition 1 was applicable, the resulting program isn’t even really warp specialized! The accumulator was split across multiple warps to fit within the register constraints, but the warps themselves are executing the same program. This exercise showed that for at least one problem size on H100, warp specialization is not required to hit peak performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Warp Specialization is a Trade-Off&lt;/head&gt;
    &lt;p&gt;In the previous sections, I laid out some principles for warp specialization, and then showed at least one useful case where we (or at least I) thought we needed warp specialization but achieved high performance without it. However, I don’t think the takeaway from this exercise should be that we should stop writing warp specialized programs. Instead, I think we should be viewing warp specialization as a point in a trade-off space over implementation choices for our kernels that have extreme performance demands (such as Tensor Core kernels). I argue that the implementations of high performance Tensor Core kernels navigate a trade-off space between the effort required to write a kernel and the effort required to develop compiler analyses that realize the intentions of the kernel writer. The cost-benefit analysis is about which allocation of effort allows us as GPU programmers to save more time overall.&lt;/p&gt;
    &lt;p&gt;If a human is not willing to put in a large amount of effort, there are several kinds of analyses a compiler could perform to achieve high performance. The first is high quality instruction scheduling by a low-level compiler like NVCC, which has the problems discussed earlier. Another direction is warp specialization performed by the compiler on a higher level language, something performed by the Triton or Cypress compilers. Compiler support for warp specialization seems promising, but compilers are not yet as good as humans at coming up with good warp specialization strategies; Triton even added a low-level language called Gluon so that expert humans could bypass the compiler’s warp specialization and do it themselves!&lt;/p&gt;
    &lt;p&gt;When a human is willing to put in a large amount of effort to achieve high performance, they can hand-write warp specialized programs and deal with the difficulties of scheduling and synchronization entailed by warp specialization. This direction naturally requires more kernel-writer effort, but makes the resulting program less dependent on the compiler’s instruction scheduling or warp specialization algorithms. A human may also consider writing a non-warp-specialized implementation, such as the GEMM kernel above that achieved high performance without warp specialization. This implementation also required some (but a very different kind of) kernel-writer effort to reorder loops to launch and synchronize operations in the right order. This reordering essentially “told” the compiler the right answer. When compared to a human-written, warp specialized implementation, the non-warp-specialized implementation may be more brittle; a different problem size or tile size may ruin the carefully crafted static schedule, while a dynamically scheduled, warp specialized implementation easily adapts.&lt;/p&gt;
    &lt;p&gt;The costs that define this trade-off space are changing with the capabilities of the architecture and the applications being developed. For example, a GEMM implementation on the Ampere architecture has similar complications as H100 around asynchronous and variable latency load instructions, but NVIDIA engineers found that high performance was achievable with acceptable complexity without warp specialization. A different example is an implementation of Flash Attention for the H100 GPU. In this application, the SM must perform large amounts of non-accelerator work (floating point reductions and exponential computations) while issuing and blocking on asynchronous accelerator work. Finding a static schedule that performs this interleaving perfectly is very difficult for a human or a compiler, hence why most H100 and later Flash Attention implementations are warp specialized. It’s not clear to me how far we can continue to push these directions for warp specialization without making the human effort too large to be worth the cost — as I mentioned before, the warp specialization strategies for efficient Blackwell kernels are really complicated. Both coming up with and realizing these strategies in correct code seems to be getting extremely difficult. Looking into the future, I can see a few (non-exhaustive) potential paths for where we might go with warp specialization: 1) GPU hardware may become easier to program and need warp specialization in fewer cases (unlikely), 2) compiler algorithms for warp specialization will continue to improve and achieve parity with humans, or 3) systems software will be developed that removes many of the footguns of writing warp specialized code.&lt;/p&gt;
    &lt;head rend="h1"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;This post was put together after conversations with several colleagues and mentors: Rupanshu Soi, Fredrik Kjolstad, Alex Aiken, Michael Garland, Michael Bauer and Duane Merrill. Michael Garland, Michael Bauer and Rupanshu Soi gave helpful comments to improve this article. Thanks to Manya Bansal for giving me a push to condense these thoughts and release them into the wild.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Spilling any registers to the stack in high performance linear algebra kernels on GPUs can result in significant performance degradation. Serious care is taken to optimize code and tune parameters to fit within register limits. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rohany.github.io/blog/warp-specialization/"/><published>2025-09-22T19:53:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45338861</id><title>ABC ends Jimmy Kimmel's suspension and his show will return Tuesday</title><updated>2025-09-22T20:37:12.286085+00:00</updated><content>&lt;doc fingerprint="a534b57d71005433"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ABC ends Jimmy Kimmel’s suspension and his show will return Tuesday&lt;/head&gt;
    &lt;p&gt;“We have spent the last days having thoughtful conversations with Jimmy, and after those conversations, we reached the decision to return the show on Tuesday,” said a statement from the network.&lt;/p&gt;
    &lt;p&gt;NEW YORK — ABC will reinstate Jimmy Kimmel’s late night show in the wake of criticism over his comments about the assassination of conservative activist Charlie Kirk, officials with the network said Monday.&lt;/p&gt;
    &lt;p&gt;“We have spent the last days having thoughtful conversations with Jimmy, and after those conversations, we reached the decision to return the show on Tuesday,” said a statement from the network.&lt;/p&gt;
    &lt;p&gt;ABC suspended Kimmel indefinitely after comments he made about Kirk, who was killed Sept. 10, in a monologue. Kimmel said “many in MAGA land are working very hard to capitalize on the murder of Charlie Kirk” and that “the MAGA gang” was “desperately trying to characterize this kid who murdered Charlie Kirk as anything other than one of them.”&lt;/p&gt;
    &lt;p&gt;Kimmel has hosted “Jimmy Kimmel Live!” on ABC since 2003 and has been a fixture in television and comedy for even longer. He is also well known as a presenter, having hosted the Academy Awards four times.&lt;/p&gt;
    &lt;p&gt;Backlash to Kimmel’s comments about Kirk was swift. Nexstar and Sinclair, two of ABC’s largest affiliate owners, said they would be pulling “Jimmy Kimmel Live!” from their stations. Others, including several fellow comedians, came to his defense.&lt;/p&gt;
    &lt;p&gt;President Donald Trump, one of Kimmel’s frequent targets, posted on social media that Kimmel’s suspension was “great news for America.” He also called for other late night hosts to be fired.&lt;/p&gt;
    &lt;p&gt;Kimmel was asked in an interview with Variety this past summer if he was worried that the administration would come after comedians. He expressed concern that a crackdown could be on the way.&lt;/p&gt;
    &lt;p&gt;“Well, you’d have to be naive not to worry a little bit,” he said. “But that can’t change what you’re doing.”&lt;/p&gt;
    &lt;p&gt;Kimmel’s suspension arrived in a time when Trump and his administration have pursued threats, lawsuits and federal government pressure to try to exert more control over the media industry. Trump has reached settlements with ABC and CBS over their coverage.&lt;/p&gt;
    &lt;p&gt;Trump has also filed defamation lawsuits against The Wall Street Journal and The New York Times. Republicans in Congress stripped federal funding from NPR and PBS.&lt;/p&gt;
    &lt;p&gt;Brendan Carr, the head of the Federal Communications Commission, issued a warning prior to Kimmel’s suspension that criticized Kimmel’s remarks about the Kirk assassination.&lt;/p&gt;
    &lt;p&gt;“We can do this the easy way or the hard way,” Carr said. “These companies can find ways to change conduct, to take action, frankly, on Kimmel, or there is going to be additional work for the FCC ahead.”&lt;/p&gt;
    &lt;p&gt;The suspension also happened at a time when the late night landscape is shifting. CBS announced the cancellation of Stephen Colbert’s show over the summer&lt;/p&gt;
    &lt;p&gt;Kimmel’s contract with The Walt Disney Co.-owned network had been set to expire in May 2026.&lt;/p&gt;
    &lt;p&gt;Word of the reinstatement came as hundreds of Hollywood and Broadway stars — including Robert De Niro, Ben Affleck, Jennifer Aniston, Selena Gomez, Lin-Manuel Miranda, Tom Hanks and Meryl Streep — urged Americans “fight to defend and preserve our constitutionally protected rights” in the wake of Jimmy Kimmel’s suspension.&lt;/p&gt;
    &lt;p&gt;More than 430 movie, TV and stage stars as well as comedians, directors and writers added their names to an open letter Monday from the American Civil Liberties Union that argues it is “a dark moment for freedom of speech in our nation.”&lt;/p&gt;
    &lt;p&gt;Also Monday, ABC’s “The View” weighed in on the controversy after not raising it for two episodes after Kimmel was suspended. Co-host Whoopi Goldberg opened the show saying: “No one silences us” and she and her fellow hosts condemned Disney’s decision.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.inquirer.com/entertainment/tv/abc-ends-jimmy-kimmel-suspension-20250922.html"/><published>2025-09-22T20:13:31+00:00</published></entry></feed>