<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-02-03T17:43:34.644455+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46868479</id><title>Show HN: Safe-now.live ‚Äì Ultra-light emergency info site (&lt;10KB)</title><updated>2026-02-03T17:43:43.572975+00:00</updated><content>&lt;doc fingerprint="ca634b944efc74a1"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;‚ö† Active Disasters &amp;amp; Declarations (FEMA | EC)&lt;/head&gt;üá∫üá∏ USA: ‚ùÑ SEVERE WINTER STORM (WV) | ‚ùÑ SEVERE WINTER STORM (NC) | ‚ùÑ SEVERE WINTER STORM (IN) | ‚ùÑ SEVERE WINTER STORM (AR) | ‚ùÑ SEVERE WINTER STORM (TN) | ‚ùÑ SEVERE WINTER STORM (KY)&lt;head rend="h2"&gt;üìç Find Local Info&lt;/head&gt;&lt;p&gt;United States: AL AK AZ AR CA CO CT DE DC FL GA HI ID IL IN IA KS KY LA ME MD MA MI MN MS MO MT NE NV NH NJ NM NY NC ND OH OK OR PA PR RI SC SD TN TX UT VT VA WA WV WI WY | VI AS GU MP&lt;/p&gt;&lt;p&gt;Canada: AB BC MB NB NL NS NT NU ON PE QC SK YT&lt;/p&gt;&lt;p&gt;International: UK/EU 112 | AU 000 | NZ 111 | JP 110/119 | MX 911 | BR 190&lt;/p&gt;&lt;head rend="h2"&gt;üÜò Quick Reference&lt;/head&gt;Earthquake: Drop, Cover, Hold On&lt;p&gt;Tornado: Lowest floor, interior room&lt;/p&gt;&lt;p&gt;Flood: Higher ground - Turn around, don't drown&lt;/p&gt;&lt;p&gt;Fire: Get out, stay out, call 911&lt;/p&gt;&lt;p&gt;Gas leak: Leave immediately, don't use electronics&lt;/p&gt;&lt;p&gt;Chemical: Move upwind, shelter in place&lt;/p&gt;&lt;p&gt;Active threat: Run, Hide, Fight&lt;/p&gt;&lt;p&gt;CO alarm: Get outside, call 911, don't re-enter&lt;/p&gt;&lt;p&gt;Flood water: Turn off electricity at breaker before contact&lt;/p&gt;&lt;p&gt;Lightning: Get inside; avoid trees, water, metal; crouch if stuck outside&lt;/p&gt;&lt;p&gt;Tsunami: Move inland and uphill immediately; don't wait&lt;/p&gt;&lt;p&gt;Power out: Fridge safe 4hrs, freezer 48hrs if closed&lt;/p&gt;&lt;p&gt;Heat stroke: Cool rapidly with water/ice; call 911; confusion is danger sign&lt;/p&gt;&lt;p&gt;Hypothermia: Remove wet clothes; warm gradually; hot drinks if conscious&lt;/p&gt;&lt;p&gt;Civil unrest: Avoid crowds; get indoors; don't engage; document if safe&lt;/p&gt;&lt;head rend="h2"&gt;üéí Emergency Kit&lt;/head&gt;- Water/Food: Water: 1 gal/4L per person per day; Water purification tablets; Non-perishable food&lt;p&gt;- Medical: First aid kit; Prescription medications; Infant/special needs items; N95 masks&lt;/p&gt;&lt;p&gt;- Tools/Comms: Flashlight + batteries; Battery/crank radio; Whistle; Phone + charger; Wrench (utilities)&lt;/p&gt;&lt;p&gt;- Docs/Money: Cash in small bills; Copies: IDs, insurance, bank; Emergency contacts; Local maps&lt;/p&gt;&lt;p&gt;- Shelter/Clothing: Emergency blanket; Change of clothes; Sturdy shoes; Rain gear&lt;/p&gt;&lt;p&gt;- Sanitation: Moist towelettes; Garbage bags + ties; Bleach; Personal hygiene items&lt;/p&gt;&lt;p&gt;Source: Ready.gov / GetPrepared.gc.ca&lt;/p&gt;&lt;head rend="h2"&gt;üè† Home Prep&lt;/head&gt;- Know: 2 evacuation routes; Family communication plan; Utility shutoffs (gas, water, electric); Nearest shelters; How to receive alerts&lt;p&gt;- Safety: Test smoke/CO detectors monthly; Secure water heater + heavy furniture; Fire extinguisher accessible; Know shelter-in-place location&lt;/p&gt;&lt;p&gt;- Power Outage: Generator: OUTSIDE ONLY; Keep 20ft/6m from openings (CO risk)&lt;/p&gt;&lt;p&gt;- Pets: ID tags + microchip; Carrier/leash; Food/water; Medications; Vaccination records&lt;/p&gt;&lt;p&gt;- Children: Comfort item; ID bracelet; Emergency contact card; Age-appropriate instructions&lt;/p&gt;&lt;head rend="h2"&gt;üí∞ Financial Help&lt;/head&gt;üá∫üá∏ USA: FEMA (housing, repairs) | SBA Loans | Benefits.gov | SNAP/Food&lt;p&gt;üá®üá¶ Canada: EI Benefits | CRA Disaster Relief | Red Cross Aid&lt;/p&gt;&lt;p&gt;Both: 211 connects to local aid&lt;/p&gt;&lt;p&gt;Tip: Document damage with photos BEFORE cleanup; keep all receipts&lt;/p&gt;&lt;head rend="h2"&gt;üîÑ Recovery&lt;/head&gt;Before entering: Wait for official all-clear; check for gas smell, structural damage&lt;p&gt;Inside: No electricity if flooding; photograph everything; wear N95 + gloves&lt;/p&gt;&lt;p&gt;Food safety: Discard if power out 4+ hrs (fridge) or thawed (freezer); when in doubt, throw it out&lt;/p&gt;&lt;p&gt;Mold: Appears within 24-48 hrs; dry everything; clean with water/detergent; consider N95&lt;/p&gt;&lt;p&gt;Avoid: Contractor scams (get multiple bids, check licenses); price gouging (report to AG)&lt;/p&gt;&lt;head rend="h2"&gt;üìö Resources&lt;/head&gt;Guides: Emergency Guide | First Aid | Shelter &amp;amp; Safety&lt;p&gt;üá∫üá∏ USA: FEMA 1-800-621-3362 (no immigration check) | Shelters 1-800-733-2767 | Power Outages&lt;/p&gt;&lt;p&gt;üá®üá¶ Canada: GetPrepared.gc.ca | Red Cross 1-800-418-1111 | Weather Alerts | Public Safety&lt;/p&gt;&lt;p&gt;Crisis (Both): 988 Suicide/Crisis | 211 Local aid | DV: 1-800-799-7233 (US) / 1-866-363-1681 (CA)&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://safe-now.live"/><published>2026-02-03T09:06:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46868759</id><title>What's up with all those equals signs anyway?</title><updated>2026-02-03T17:43:43.170676+00:00</updated><content>&lt;doc fingerprint="6ff2d15d65fe3faf"&gt;
  &lt;main&gt;
    &lt;p&gt;For some reason or other, people have been posting a lot of excerpts from old emails on Twitter over the last few days. The most vital question everybody‚Äôs asking themselves is: What‚Äôs up with all those equals signs?!&lt;/p&gt;
    &lt;p&gt;And that‚Äôs something I‚Äôm somewhat of an expert on. I mean, having written mail readers and stuff; not because I‚Äôve been to Caribbean islands.&lt;/p&gt;
    &lt;p&gt;I‚Äôve seen people confidently claim that it‚Äôs a code, or that it‚Äôs an artefact of scanning and then using OCR, but it‚Äôs neither ‚Äî it‚Äôs just that whoever converted these emails to a readable format were morons.&lt;/p&gt;
    &lt;p&gt;What‚Äôs that you say? ‚ÄúConverted?! Surely emails are just text!!‚Äù Well, if you lived in the stone age (i.e., the 80s), they mostly were, but then people invented things like ‚Äúlong lines‚Äù and ‚Äúrock d√∂ts‚Äù, and computers had to ‚Äúencode‚Äù the mail before sending.&lt;/p&gt;
    &lt;p&gt;The artefact we see here is from something called ‚Äúquoted printable‚Äù, or as we used to call it when it was introduced: ‚ÄúQuoted unreadable‚Äù.&lt;/p&gt;
    &lt;p&gt;To take the first line. Whoever wrote this, typed in the following in their mail reader:&lt;/p&gt;
    &lt;quote&gt;we talked about designing a pig with different non- cloven hoofs in order to make kosher bacon&lt;/quote&gt;
    &lt;p&gt;We see that that‚Äôs quite a long line. Mail servers don‚Äôt like that, so mail software will break it into two lines, like so:&lt;/p&gt;
    &lt;quote&gt;we talked about designing a pig with different non- = cloven hoofs in order to make kosher bacon&lt;/quote&gt;
    &lt;p&gt;See? There‚Äôs that equals sign! Yes, the equals sign is used to say ‚Äúthis should really be one single line, but I‚Äôve broken it in two so that the mail server doesn‚Äôt get mad at me‚Äù.&lt;/p&gt;
    &lt;p&gt;The formal definition here is important, though, so I have to be a bit technical here: To say ‚Äúthis is a continuation line‚Äù, you insert an equals sign, then a carriage return, and then a line feed.&lt;/p&gt;
    &lt;p&gt;Or,&lt;/p&gt;
    &lt;quote&gt;=CRLF&lt;/quote&gt;
    &lt;p&gt;Three characters in total, i.e., :&lt;/p&gt;
    &lt;quote&gt;... non- =CRLF cloven hoofs...&lt;/quote&gt;
    &lt;p&gt;When displaying this, we remove all these three characters, and end up&lt;lb/&gt; with:&lt;/p&gt;
    &lt;quote&gt;... non- cloven hoofs...&lt;/quote&gt;
    &lt;p&gt;So what‚Äôs happened here? Well, whoever collected these emails first converted from CRLF (also known as the ‚ÄúWindows‚Äù line ending coding, but it‚Äôs the standard line ending in the SMTP standard) to ‚ÄúNL‚Äù (i.e., ‚ÄúUnix‚Äù line ending coding). This is pretty normal if you want to deal with email. But you then have one byte fewer:&lt;/p&gt;
    &lt;quote&gt;... non- =NL cloven hoofs...&lt;/quote&gt;
    &lt;p&gt;If your algorithm to decode this is, stupidly, ‚Äúfind equals signs at the end of the line, and then delete two characters, and then finally the equals sign‚Äù, you should end up with:&lt;/p&gt;
    &lt;quote&gt;... non- loven hoofs...&lt;/quote&gt;
    &lt;p&gt;I.e., you lose the ‚Äúc‚Äù. That‚Äôs almost what happened here, but not quite: Why does the equals sign still remain?&lt;/p&gt;
    &lt;p&gt;This StackOverflow post from 14 years ago explains the phenomenon, sort of:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Obviously the client notices that = is not followed by a proper CR LF sequence, so it assumes that it is not a soft line break, but a character encoded in two hex digits, therefore it reads the next two bytes. It should notice that the next two bytes are not valid hex digits, so its behavior is wrong too, but we have to admit that at that point it does not have a chance to display something useful. They opted for the garbage in, garbage out approach.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That is, equals signs are also used for something else besides wrapping long lines, and that‚Äôs what we see later in the post:&lt;/p&gt;
    &lt;quote&gt;=C2 please note&lt;/quote&gt;
    &lt;p&gt;If the equals sign is not at the end of a line, it‚Äôs used to encode ‚Äúfunny characters‚Äù, like what you use with ‚Äúrock d√∂ts‚Äù. =C2 is 194, which is a first character in a UTF-8 sequence, and the following char is most likely a =A0: =C2=A0 is ‚Äúnon-breakable space‚Äù, which is something people often use to indent text (and the ‚Äúplease note‚Äù is indented) and you see =A0 in many other places in these emails.&lt;/p&gt;
    &lt;p&gt;My guess is that whoever did this part just did a search-replace for =C2 and/or =A0 instead of using a proper decoder, but other explanations are certainly possible. Any ideas?&lt;/p&gt;
    &lt;p&gt;Anyway, that‚Äôs what‚Äôs up with those equals signs: 1) ‚Äúit‚Äôs technical‚Äù, and 2) ‚Äúit‚Äôs a combination of buggy continuation line decoding and buggy non-ASCII decoding‚Äù, and 3) ‚Äúwhoever processed these mails are incompetent‚Äù. I don‚Äôt think 2) should be very surprising at this point, do you?&lt;/p&gt;
    &lt;p&gt;(Edit a bit later: To nitpick a bit here: When the standard was written, people mostly envisioned that the quoted-printable content transport encoding would be unwound upon reception (note ‚Äútransport‚Äù), and that you‚Äôd end up with ‚Äúclean text‚Äù on disk after reception. This didn‚Äôt really happen, so all ‚Äúreal‚Äù implementations do the right thing with single-character (i.e., ‚Äúunencoded‚Äù) newlines. For instance:&lt;/p&gt;
    &lt;quote&gt;(quoted-printable-decode-string "he=\nllo") =&amp;gt; "hello"&lt;/quote&gt;
    &lt;p&gt;Which leads me to assume that they reused an algo that was usually run in an SMTP server context to do the line unfolding ‚Äî in that context, you can safely assume that the line ending is a CRLF. And by chance, this algo also works fine if you‚Äôre working with a Windows-based file, but fails for a Unix-based file.)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lars.ingebrigtsen.no/2026/02/02/whats-up-with-all-those-equals-signs-anyway/"/><published>2026-02-03T09:37:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46869901</id><title>Emerge Career (YC S22) is hiring a product designer</title><updated>2026-02-03T17:43:42.709746+00:00</updated><content>&lt;doc fingerprint="3d88781899c444b5"&gt;
  &lt;main&gt;
    &lt;p&gt;All-in-one re-entry &amp;amp; workforce development training platform&lt;/p&gt;
    &lt;p&gt;Emerge Career‚Äôs mission is to break the cycle of poverty and incarceration. We‚Äôre not just building software; we‚Äôre creating pathways to real second chances. Through an all-in-one platform deeply embedded within the criminal justice system, we recruit, train, and place justice-impacted individuals into life-changing careers.&lt;/p&gt;
    &lt;p&gt;Our vision is to become the country‚Äôs unified workforce development system, replacing disconnected brick-and-mortar job centers with one integrated, tech-powered solution that meets low-income individuals exactly where they are. Today, the federal government spends billions annually on education and training programs, yet only about 70% of participants graduate, just 38.6% secure training-related employment, and average first-year earnings hover around $34,708.&lt;/p&gt;
    &lt;p&gt;By contrast, our seven-person team has already outperformed the job centers in two entire states (Vermont and South Dakota) in just the past year. With an 89% graduation rate and 92% of graduates securing training-related employment, our alumni aren‚Äôt just getting jobs‚Äîthey‚Äôre launching new lives with average first-year earnings of $77,352. The results speak for themselves, and we‚Äôre just getting started.&lt;/p&gt;
    &lt;p&gt;Before Emerge, our founders Zo and Gabe co-founded Ameelio, an award-winning tech nonprofit that is dismantling the prison communication duopoly. Backed by tech luminaries like Reid Hoffman, Vinod Khosla, and Jack Dorsey, and by major criminal-justice philanthropies such as Arnold Ventures and the Mellon Foundation, Ameelio became a recognized leader in the space. Because of this experience both Zo and Gabe understood what it took to create change from within the system. After serving over 1M people impacted by incarceration, they witnessed firsthand the gap in second-chance opportunities and the chronic unemployment plaguing those impacted by the justice system. Emerge Career is committed to solving this issue.&lt;/p&gt;
    &lt;p&gt;Our students are at the heart of our work. Their journeys have captured national attention on CBS, NBC, and in The Boston Globe, and our programs now serve entire states and cities. And we‚Äôre not doing it alone: our vision has attracted support from Alexis Ohanian (776), Michael Seibel, Y Combinator, the Opportunity Fund, and public figures like Diana Taurasi, Deandre Ayton, and Marshawn Lynch. All of us believe that, with the right mix of technology and hands-on practice, we can redefine workforce development and deliver true second chances at scale.&lt;/p&gt;
    &lt;p&gt;Emerge Career was designed to tackle two systemic issues: recidivism, fueled by post-incarceration unemployment and poverty, and labor shortages in key industries. Over 60% of formerly incarcerated people remain unemployed a year after incarceration, seeking work but not finding it. The reality is shocking, workforce development programs are severely limited inside prison, with only one-third of incarcerated people ever participating. To worsen, the available prison jobs offer meager wages, often less than $1 per hour, and often do not equip individuals with the skills for long-term stable employment.&lt;/p&gt;
    &lt;p&gt;We call this a Founding Design Engineer role, even three years in and with multiple contracts under our belt, for two reasons. First, you'll be our very first engineer, joining our co-founder, who's built the entire platform solo to date. Second, our growth is now outpacing our systems, and we can't keep up on maintenance alone. We're at a critical juncture: we can either hire someone to simply care for what exists, or we can bring on a talent who believes that, with the right blend of technology and hands-on practice, we can unify the workforce-development system and deliver second chances at true scale. We hope that can be you.&lt;/p&gt;
    &lt;p&gt;This is not a traditional engineering job. You'll build features in React and TypeScript, but your real job is helping students finish. That means understanding the human problem first: why do people disengage? What makes someone choose to keep going when the payoff is months away? You'll answer those questions through direct conversations, usability research, and watching how people actually use what you build. Then you'll prototype fast, ship real software, and measure whether it worked. Some days that looks like code. Other days it looks like a phone call, a support ticket, or a whiteboard session figuring out how to turn a one-off fix into a system that scales.&lt;/p&gt;
    &lt;p&gt;This role blends engineering, product, design, and program operations. We're looking for someone who believes good design can inspire a person to invest in their own future, and who wants to prove it, week after week, by shipping work that measurably helps students succeed. If you want to be close to users, own outcomes end to end, and build something that actually matters, you'll thrive here.&lt;/p&gt;
    &lt;p&gt;You design by building. You don't hand off mockups and wait. You open Cursor, Claude Code, or whatever gets you closest to a real, testable thing fastest. You might already be shipping code in production ‚Äî or you're itching to. You believe the fastest path to a great design is putting something real in front of a real user and watching what happens.&lt;/p&gt;
    &lt;p&gt;You are relentlessly scrappy. You prototype in hours, not weeks. You'd rather test an ugly thing that teaches you something than polish a beautiful thing nobody's used yet. You know that at this stage, speed of learning is the only thing that matters. Fidelity comes later. Signal comes first.&lt;/p&gt;
    &lt;p&gt;You refuse to be blocked. When engineering bandwidth isn't there, you don't sit around. You figure it out ‚Äî a Figma prototype, a coded prototype, a quick hack in the codebase. You treat "waiting for a developer" as a personal failure. You find a way or you make one.&lt;/p&gt;
    &lt;p&gt;You think in outcomes, not outputs. You don't measure your work in screens delivered. You measure it in whether students finished, whether they came back, whether the thing you shipped actually moved a number that matters. You're obsessed with the gap between what you designed and what actually happened.&lt;/p&gt;
    &lt;p&gt;You talk to users constantly. Not in scheduled quarterly research sprints ‚Äî in real conversations, every week. You build relationships with students. You know their names, their blockers, their moments of doubt. Your best design ideas come from a 10-minute phone call, not a brainstorm.&lt;/p&gt;
    &lt;p&gt;You have strong taste but low ego. You have opinions about what good looks like and you'll fight for them. But when the data says you're wrong, you move on fast. You don't fall in love with your work. You fall in love with the problem.&lt;/p&gt;
    &lt;p&gt;You believe everyone deserves a second chance. You treat everyone with dignity. You know how to meet people exactly where they are ‚Äî with empathy and compassion ‚Äî helping create a space where everyone feels seen and valued, regardless of their background.&lt;/p&gt;
    &lt;p&gt;You work hard. You show up early, stay late, and do what needs to get done ‚Äî no ego, no excuses. This isn't a 9-to-5. The team puts in 10+ hour days because we care about the mission and each other. If that sounds miserable, this isn't for you. If it sounds exciting, you'll fit right in.&lt;/p&gt;
    &lt;p&gt;Talking to students ‚Äî a lot. Your week starts and ends with users. You'll build real relationships with students, not just run usability sessions. You'll understand why someone almost quit, what message made them log back in, what screen confused them at 11pm. These conversations are your primary design tool.&lt;/p&gt;
    &lt;p&gt;Prototyping at the speed of conversation. You hear a problem on a call Tuesday. By Wednesday you have something testable ‚Äî a coded prototype, a functional hack, a Figma flow wired to real data. By Thursday a student is using it. By Friday you know if it worked. That's the cycle. Repeat.&lt;/p&gt;
    &lt;p&gt;Shipping real product, not just designs. You'll work in our React and TypeScript codebase ‚Äî or use AI tools like Cursor and Claude Code to get there. The goal isn't to become a full-time engineer. The goal is to never let "it hasn't been built yet" slow down learning. Some of what you build will go straight to production. Some will be throwaway prototypes. You'll know the difference.&lt;/p&gt;
    &lt;p&gt;Designing the moments that keep students going. The hardest design problem here isn't layout or typography. It's commitment. Students are betting months of effort on a future they have to imagine. You'll study where they disengage, what triggers doubt, and what reignites momentum. Then you'll design the moments ‚Äî an interface, a message, a milestone ‚Äî that help someone choose to keep going. How do you make a better life in three months feel worth the sacrifice today? You'll own that problem.&lt;/p&gt;
    &lt;p&gt;Measuring what matters. Polished decks don't matter here. You'll define success metrics for what you ship, track whether completion rates moved, whether more students hit the next milestone, whether the intervention you designed actually intervened. You'll close the loop between design and outcome every time.&lt;/p&gt;
    &lt;p&gt;Working across the entire stack of the student experience. Some days that looks like interface design. Other days it looks like rethinking a Customer.io campaign, redesigning an onboarding flow, or sitting with the ops team to understand why students in one facility disengage faster than another. You go where the problem is.&lt;/p&gt;
    &lt;p&gt;Documenting your work clearly. Our work spans months and involves multiple teams. You'll create visibility when a change impacts operations and help others understand how features affect training and service delivery. Precision matters.&lt;/p&gt;
    &lt;p&gt;Start Date: ASAP&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/emerge-career/jobs/omqT34S-founding-product-designer"/><published>2026-02-03T12:00:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46870015</id><title>Bunny Database</title><updated>2026-02-03T17:43:42.123710+00:00</updated><content>&lt;doc fingerprint="a71f183a5d20d688"&gt;
  &lt;main&gt;
    &lt;p&gt;Don√¢t want to babysit your app database on a VM but not willing to pay the DBaaS tax either? We're building a third way.&lt;/p&gt;
    &lt;p&gt;Today, we√¢re launching Bunny Database as a public preview: a SQLite-compatible managed service that spins down when idle, keeps latency low wherever your users are, and doesn√¢t cost a fortune.&lt;/p&gt;
    &lt;head rend="h2"&gt;So what√¢s the deal with database services in 2026?&lt;/head&gt;
    &lt;p&gt;It√¢s become clear by now that the DBaaS platforms that garnered the love of so many devs are all going upmarket. Removing or dumbing down free tiers, charging for unused capacity, charging extra for small features, or bundling them in higher tiers √¢ you already know the drill.&lt;/p&gt;
    &lt;p&gt;Hard to blame anyone for growing their business, but it doesn√¢t feel right when these services stop making sense for the very people who helped popularize them in the first place.&lt;/p&gt;
    &lt;p&gt;So where does that leave you?&lt;/p&gt;
    &lt;head rend="h2"&gt;Like SQLite, but for the web&lt;/head&gt;
    &lt;p&gt;Not every project needs Postgres, and that√¢s okay. Sometimes you just want a simple, reliable database that you can spin up quickly and build on, without worrying it√¢ll hit your wallet like an EC2.&lt;/p&gt;
    &lt;p&gt;That√¢s what we built Bunny Database for.&lt;/p&gt;
    &lt;p&gt;What you get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One-click deployment: just name your database and go, no config needed&lt;/item&gt;
      &lt;item&gt;Language-specific tooling: SDKs for TS/JS, Go, Rust, and .NET help you handle the boring bits&lt;/item&gt;
      &lt;item&gt;Low latency anywhere: replication regions let you serve reads close to your users&lt;/item&gt;
      &lt;item&gt;41 regions worldwide: choose between automatic, single-region, and multi-region deployment&lt;/item&gt;
      &lt;item&gt;Works over HTTP: wire up anything you√¢d like&lt;/item&gt;
      &lt;item&gt;Database editor: insert data or run queries on the spot&lt;/item&gt;
      &lt;item&gt;Metrics: instant visibility into reads, writes, storage, and latency&lt;/item&gt;
      &lt;item&gt;Affordable, pay-as-you-go pricing: only pay for what you use, but without the serverless tax&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Get the full tour including how to connect Bunny Database to your app in this quick demo from our DX Engineer, Jamie Barton:&lt;/p&gt;
    &lt;head rend="h2"&gt;Why care about database latency anyway?&lt;/head&gt;
    &lt;p&gt;You probably optimize the heck out of your frontend, APIs, and caching layers, all for the sake of delivering an experience that feels instant to your users. But when your database is far away from them, round-trip time starts to add noticeable latency.&lt;/p&gt;
    &lt;p&gt;The usual fix is to introduce more caching layers, denormalized reads, or other workarounds. That√¢s obviously no fun.&lt;/p&gt;
    &lt;p&gt;And when you think about it, devs end up doing this because the popular DBaaS platforms are usually either limited, complex, or too costly when it comes to multi-region deployments. So what looks like a caching problem is actually a data locality issue.&lt;/p&gt;
    &lt;p&gt;OK, but how bad can it really be?&lt;/p&gt;
    &lt;p&gt;To find out, we ran a read latency benchmark and measured p95 latency in Bunny Database.&lt;/p&gt;
    &lt;p&gt;We picked a number of regions across the world and compared round-trip time for client locations ever farther away from the database in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a single-region setup,&lt;/item&gt;
      &lt;item&gt;with replication regions enabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Turns out serving reads close to clients reduced latency by up to 99%.&lt;/p&gt;
    &lt;p&gt;Check out the full write-up on the benchmark setup and results here.&lt;/p&gt;
    &lt;p&gt;While this definitely matters most to apps with global users, data locality does apply to everyone. With Bunny Database, you don√¢t have to stick to major data center locations and compensate with caching workarounds any more. Instead, you get a lot of flexibility to set up regions in an intuitive interface and it√¢s easy to switch things up as your requirements change.&lt;/p&gt;
    &lt;p&gt;Choose between 3 deployment types when creating a database:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic region selection gives you one-click deployment with minimal latency. Bunny Database will select regions for you based on your IP address (you can check and tweak the selection in settings later).&lt;/item&gt;
      &lt;item&gt;Single-region deployment lets you pick one of 41 regions available worldwide (check the full list here).&lt;/item&gt;
      &lt;item&gt;Manual region selection gives you custom multi-region setup, where you can freely pick regions that make the most sense for your audience.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of this lets you start wherever you√¢d like and add regions as needed, without re-architecting your app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Usage-based pricing, but without the serverless tax&lt;/head&gt;
    &lt;p&gt;In the database world, capacity-based pricing gives you some predictability. But no one likes to pay for unused capacity, right?&lt;/p&gt;
    &lt;p&gt;Serverless, on the other hand, is supposed to be cost-efficient, yet can rack up bills quickly, especially when the DBaaS charges significant markups on top of already pricey compute.&lt;/p&gt;
    &lt;p&gt;We don√¢t do hyperscalers, though, so we can charge a fair price for Bunny Database in a usage-based model.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reads: $0.30 per billion rows&lt;/item&gt;
      &lt;item&gt;Writes: $0.30 per million rows&lt;/item&gt;
      &lt;item&gt;Storage: $0.10 per GB per active region (monthly)&lt;/item&gt;
      &lt;item&gt;When not getting requests, Bunny Database only incurs storage costs. One primary region is charged continuously, while read replicas only add storage costs when serving traffic (metered by the hour)&lt;/item&gt;
      &lt;item&gt;Your usage is charged continuously (pay-as-you-go) and invoiced monthly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;During the public preview phase, Bunny Database is free.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wait, what does √¢SQLite-compatible√¢ actually mean?&lt;/head&gt;
    &lt;p&gt;Bunny Database wouldn√¢t be possible without libSQL, the open-source, open-contribution fork of SQLite created by Turso.&lt;/p&gt;
    &lt;p&gt;We run Bunny Database on our own fork of libSQL, which gives us the freedom to integrate it tightly with the bunny.net platform and handle the infrastructure and orchestration needed to run it as a managed, multi-region service.&lt;/p&gt;
    &lt;p&gt;What does this mean for Bunny Database√¢s upstream feature parity with libSQL and SQLite, respectively?&lt;/p&gt;
    &lt;p&gt;The short answer is that we don√¢t currently promise automatic or complete feature parity with either upstream libSQL or the latest SQLite releases.&lt;/p&gt;
    &lt;p&gt;While libSQL aims to stay compatible with SQLite√¢s API and file format, it doesn√¢t move in lockstep with upstream SQLite. We wouldn√¢t expect otherwise, especially as Turso has shifted focus from libSQL toward a long-term rewrite of SQLite in Rust.&lt;/p&gt;
    &lt;p&gt;For Bunny Database, this means that compatibility today is defined by the libSQL version we√¢re built on, rather than by chasing every upstream SQLite or libSQL change as it lands. We haven√¢t pulled in any upstream changes yet, and we don√¢t currently treat upstream parity as an automatic goal.&lt;/p&gt;
    &lt;p&gt;That√¢s intentional. Our focus so far has been on making Bunny Database reliable and easy to operate as a service. We think bringing in upstream changes only makes sense when they clearly improve real-world use cases, not just to tick a parity checkbox.&lt;/p&gt;
    &lt;p&gt;If there are specific libSQL features you√¢d like to see exposed in Bunny Database, or recent SQLite features you√¢d want us to pull in, we√¢d love to hear about it. Join our Discord to discuss your use cases and help shape the roadmap!&lt;/p&gt;
    &lt;head rend="h2"&gt;What√¢s ahead for Bunny Database&lt;/head&gt;
    &lt;p&gt;Speaking of the roadmap, we don√¢t stop cooking. Here√¢s what√¢s coming up next:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic backups&lt;/item&gt;
      &lt;item&gt;Database file import/export&lt;/item&gt;
      &lt;item&gt;Auto-generated, schema-aware API with type-safe SDKs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There√¢s even more to come, but it√¢s too soon to spill the beans yet, especially while we√¢re in public preview. We√¢d love to hear your feedback, so we can shape what ships next together.&lt;/p&gt;
    &lt;head rend="h2"&gt;More tools to build with&lt;/head&gt;
    &lt;p&gt;Bunny Database works standalone and fits right into your stack via the SDKs (or you can hook up anything using the HTTP API). But it also plays nicely with Bunny Edge Scripting and Bunny Magic Containers.&lt;/p&gt;
    &lt;p&gt;To connect your database to an Edge Script or a Magic Containers app, simply go to the Access tab of the chosen database and click Generate Tokens to create new access credentials for it.&lt;/p&gt;
    &lt;p&gt;Once they√¢re generated, you√¢ll get two paths to choose from:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Click Add Secrets to an Edge Script and select the one you√¢d like to connect from the list. You√¢ll also need to import the libSQL TypeScript client and use the provided code snippet to connect it to your database.&lt;/item&gt;
      &lt;item&gt;Click Add Secrets to Magic Container App and select the one you√¢d like to connect from the list. You√¢ll also need to connect to the database from your app using one of the client libraries or the HTTP API.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After you complete the setup, the database URL and access token will be available as environment variables in your script or app. Use them to connect to your database:&lt;/p&gt;
    &lt;code&gt;import&lt;/code&gt;
    &lt;code&gt;{&lt;/code&gt;
    &lt;code&gt;createClient&lt;/code&gt;
    &lt;code&gt;}&lt;/code&gt;
    &lt;code&gt;from&lt;/code&gt;
    &lt;code&gt;"@libsql/client/web"&lt;/code&gt;
    &lt;code&gt;;&lt;/code&gt;
    &lt;code&gt;const&lt;/code&gt;
    &lt;code&gt;client&lt;/code&gt;
    &lt;code&gt;=&lt;/code&gt;
    &lt;code&gt;createClient&lt;/code&gt;
    &lt;code&gt;({&lt;/code&gt;
    &lt;code&gt;url&lt;/code&gt;
    &lt;code&gt;:&lt;/code&gt;
    &lt;code&gt;process.env.&lt;/code&gt;
    &lt;code&gt;DB_URL&lt;/code&gt;
    &lt;code&gt;,&lt;/code&gt;
    &lt;code&gt;authToken&lt;/code&gt;
    &lt;code&gt;:&lt;/code&gt;
    &lt;code&gt;process.env.&lt;/code&gt;
    &lt;code&gt;DB_TOKEN&lt;/code&gt;
    &lt;code&gt;});&lt;/code&gt;
    &lt;code&gt;const&lt;/code&gt;
    &lt;code&gt;result&lt;/code&gt;
    &lt;code&gt;=&lt;/code&gt;
    &lt;code&gt;client&lt;/code&gt;
    &lt;code&gt;.&lt;/code&gt;
    &lt;code&gt;execute&lt;/code&gt;
    &lt;code&gt;(&lt;/code&gt;
    &lt;code&gt;"SELECT * FROM users"&lt;/code&gt;
    &lt;code&gt;);&lt;/code&gt;
    &lt;p&gt;You can find more detailed, step-by-step integration instructions in the docs:&lt;/p&gt;
    &lt;head rend="h2"&gt;Hop on board&lt;/head&gt;
    &lt;p&gt;We can√¢t wait to see what you√¢ll build with Bunny Database and what you think of it. During the public preview phase, you get 50 databases per user account, each capped at 1 GB, but we hope this should be more than enough for lots of fun projects.&lt;/p&gt;
    &lt;p&gt;Just sign in to the bunny.net dashboard to get started. Happy building!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bunny.net/blog/meet-bunny-database-the-sql-service-that-just-works/"/><published>2026-02-03T12:13:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46870917</id><title>Show HN: difi ‚Äì A Git diff TUI with Neovim integration (written in Go)</title><updated>2026-02-03T17:43:41.362374+00:00</updated><content>&lt;doc fingerprint="77950df911a2f782"&gt;
  &lt;main&gt;
    &lt;p&gt;Review and refine Git diffs before you push&lt;/p&gt;
    &lt;p&gt;git diff shows changes. difi helps you review them.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ö°Ô∏è Instant ‚Äî Built in Go. Launches immediately with no daemon or indexing.&lt;/item&gt;
      &lt;item&gt;üé® Structured ‚Äî A clean file tree and focused diffs for fast mental parsing.&lt;/item&gt;
      &lt;item&gt;üß† Editor-Aware ‚Äî Jump straight to the exact line in &lt;code&gt;nvim&lt;/code&gt;/&lt;code&gt;vim&lt;/code&gt;to fix issues.&lt;/item&gt;
      &lt;item&gt;‚å®Ô∏è Keyboard-First ‚Äî Navigate everything with &lt;code&gt;h j k l&lt;/code&gt;. No mouse required.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;brew tap oug-t/difi
brew install difi&lt;/code&gt;
    &lt;code&gt;go install github.com/oug-t/difi/cmd/difi@latest&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download the binary from Releases and add it to your &lt;code&gt;$PATH&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run difi in any Git repository.&lt;/item&gt;
      &lt;item&gt;By default, it compares your current branch against main.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd my-project
difi&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Tab&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Toggle focus between File Tree and Diff View&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;j / k&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Move cursor down / up&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;h / l&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Focus Left (Tree) / Focus Right (Diff)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;e&lt;/code&gt; / &lt;code&gt;Enter&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Edit file (opens editor at selected line)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Toggle help drawer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;q&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Quit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Get the ultimate review experience with difi.nvim.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Auto-Open: Instantly jumps to the file and line when you press &lt;code&gt;e&lt;/code&gt;in the CLI.&lt;/item&gt;
      &lt;item&gt;Visual Diff: Renders diffs inline with familiar green/red highlights‚Äîjust like reviewing a PR on GitHub.&lt;/item&gt;
      &lt;item&gt;Interactive Review: Restore a "deleted" line by simply removing the &lt;code&gt;-&lt;/code&gt;marker. Discard an added line by deleting it entirely.&lt;/item&gt;
      &lt;item&gt;Context Aware: Automatically syncs with your &lt;code&gt;difi&lt;/code&gt;session target.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To use &lt;code&gt;difi&lt;/code&gt; as a native git command (e.g., &lt;code&gt;git difi&lt;/code&gt;), add it as an alias in your global git config:&lt;/p&gt;
    &lt;code&gt;git config --global alias.difi '!difi'&lt;/code&gt;
    &lt;p&gt;Now you can run it directly from git:&lt;/p&gt;
    &lt;code&gt;git difi&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/oug-t/difi
cd difi
go run cmd/difi/main.go&lt;/code&gt;
    &lt;p&gt;Contributions are especially welcome in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;diff.nvim rendering edge cases&lt;/item&gt;
      &lt;item&gt;UI polish and accessibility&lt;/item&gt;
      &lt;item&gt;Windows support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Made with ‚ù§Ô∏è by oug-t&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/oug-t/difi"/><published>2026-02-03T13:47:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46871173</id><title>Agent Skills</title><updated>2026-02-03T17:43:41.183254+00:00</updated><content>&lt;doc fingerprint="8f77afff0b4fb448"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Why Agent Skills?&lt;/head&gt;Agents are increasingly capable, but often don‚Äôt have the context they need to do real work reliably. Skills solve this by giving agents access to procedural knowledge and company-, team-, and user-specific context they can load on demand. Agents with access to a set of skills can extend their capabilities based on the task they‚Äôre working on. For skill authors: Build capabilities once and deploy them across multiple agent products. For compatible agents: Support for skills lets end users give agents new capabilities out of the box. For teams and enterprises: Capture organizational knowledge in portable, version-controlled packages.&lt;head rend="h2"&gt;What can Agent Skills enable?&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Domain expertise: Package specialized knowledge into reusable instructions, from legal review processes to data analysis pipelines.&lt;/item&gt;&lt;item&gt;New capabilities: Give agents new capabilities (e.g. creating presentations, building MCP servers, analyzing datasets).&lt;/item&gt;&lt;item&gt;Repeatable workflows: Turn multi-step tasks into consistent and auditable workflows.&lt;/item&gt;&lt;item&gt;Interoperability: Reuse the same skill across different skills-compatible agent products.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://agentskills.io/home"/><published>2026-02-03T14:09:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46871251</id><title>Show HN: Inverting Agent Model (App as Clients, Chat as Server and Reflection)</title><updated>2026-02-03T17:43:40.542009+00:00</updated><content>&lt;doc fingerprint="d929afdb8c6ea8b5"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;One line of code to make any application AI-controllable&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;RAIL is a universal bridge that connects any application (C#, C++, Python, Node.js) to any LLM (GPT, Claude, Gemini). Instead of rewriting your application, you add one line of code and the AI can call your methods directly.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Project&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RailOrchestrator&lt;/cell&gt;
        &lt;cell&gt;Main AI application (UI + LLM routing)&lt;/cell&gt;
        &lt;cell&gt;C# / WPF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RailBridge.Native&lt;/cell&gt;
        &lt;cell&gt;Native DLL for cross-language IPC&lt;/cell&gt;
        &lt;cell&gt;C# (Native AOT)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RailSDK.Universal&lt;/cell&gt;
        &lt;cell&gt;Client SDK for .NET apps&lt;/cell&gt;
        &lt;cell&gt;C# (.NET Standard)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RailSDK&lt;/cell&gt;
        &lt;cell&gt;Analysis &amp;amp; manifest generation tools&lt;/cell&gt;
        &lt;cell&gt;C#&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RailSDK-Cpp&lt;/cell&gt;
        &lt;cell&gt;Client SDK for C++ apps&lt;/cell&gt;
        &lt;cell&gt;C++&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RailSDK-Python&lt;/cell&gt;
        &lt;cell&gt;Client SDK for Python apps&lt;/cell&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RailSDK-Node&lt;/cell&gt;
        &lt;cell&gt;Client SDK for Node.js apps&lt;/cell&gt;
        &lt;cell&gt;TypeScript&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RailStudio&lt;/cell&gt;
        &lt;cell&gt;Visual tool for scanning/analyzing apps&lt;/cell&gt;
        &lt;cell&gt;C# / WPF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ConvertedProjectExample&lt;/cell&gt;
        &lt;cell&gt;Example applications&lt;/cell&gt;
        &lt;cell&gt;C#&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The Brain - Main application that users interact with.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WPF desktop application (.NET 9)&lt;/item&gt;
      &lt;item&gt;Connects to LLMs (Gemini, OpenAI, Anthropic, Claude)&lt;/item&gt;
      &lt;item&gt;ReAct agent loop for multi-step reasoning&lt;/item&gt;
      &lt;item&gt;Hosts the Named Pipe server for client connections&lt;/item&gt;
      &lt;item&gt;Manages assets (Chips) and tool routing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Services/Host/HostService.cs&lt;/code&gt;- Named Pipe server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Services/LLMService.cs&lt;/code&gt;- LLM API integration&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Services/ReAct/ReActOrchestrator.cs&lt;/code&gt;- Agent loop&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Bridge - Native DLL that enables cross-language communication.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compiled with Native AOT for C-ABI compatibility&lt;/item&gt;
      &lt;item&gt;Exposes functions callable from any language: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;RAIL_Ignite()&lt;/code&gt;- Connect to host&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Uses Named Pipes for IPC&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Target: Python, C++, Node.js, Rust, Go&lt;/p&gt;
    &lt;p&gt;The .NET SDK - Client library for C# applications.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;.NET Standard 2.0 (compatible with .NET Framework 4.6.1+)&lt;/item&gt;
      &lt;item&gt;Simple API: &lt;code&gt;RailEngine.Ignite(this)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Auto-discovers methods via reflection&lt;/item&gt;
      &lt;item&gt;Loads &lt;code&gt;RailBridge.dll&lt;/code&gt;for communication&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;code&gt;// In App.xaml.cs
RailEngine.Ignite(this);&lt;/code&gt;
    &lt;p&gt;The Toolkit - Assembly scanning and manifest generation.&lt;/p&gt;
    &lt;p&gt;Contains:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;RuntimeRegistry&lt;/code&gt;- Detect .NET / Native binaries&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AssemblyScanner&lt;/code&gt;- Extract methods from DLLs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CompositeManifest&lt;/code&gt;- Multi-module manifest format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DependencyAnalyzer&lt;/code&gt;- Analyze project dependencies&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SolutionScanner&lt;/code&gt;- Scan entire .sln solutions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Used by: RailStudio, RailOrchestrator&lt;/p&gt;
    &lt;p&gt;The C++ SDK - Enable C++ applications to connect.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CMake-based build system&lt;/item&gt;
      &lt;item&gt;Loads &lt;code&gt;RailBridge.dll&lt;/code&gt;via&lt;code&gt;LoadLibrary&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Callback-based command execution&lt;/item&gt;
      &lt;item&gt;Supports both x64 and x86 builds&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build:&lt;/p&gt;
    &lt;code&gt;build_x64.bat   # 64-bit
build_x86.bat   # 32-bit (legacy apps)&lt;/code&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;code&gt;Rail::ignite("MyApp", manifestJson, onCommand);&lt;/code&gt;
    &lt;p&gt;The Python SDK - Enable Python scripts to connect.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses &lt;code&gt;ctypes&lt;/code&gt;to load&lt;code&gt;RailBridge.dll&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Decorator-based method registration&lt;/item&gt;
      &lt;item&gt;Simple API matching other SDKs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;code&gt;from rail import RailEngine

engine = RailEngine()
engine.ignite([MyService()])&lt;/code&gt;
    &lt;p&gt;The Node.js SDK - Enable TypeScript/JavaScript apps to connect.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses &lt;code&gt;ffi-napi&lt;/code&gt;for native bridge access&lt;/item&gt;
      &lt;item&gt;TypeScript types included&lt;/item&gt;
      &lt;item&gt;Promise-based API&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;code&gt;import { RailEngine } from 'rail-sdk';
engine.ignite([new MyService()]);&lt;/code&gt;
    &lt;p&gt;The Visual Tool - Scan and analyze applications.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;EXE/DLL to analyzer&lt;/item&gt;
      &lt;item&gt;Auto-generates &lt;code&gt;rail.manifest.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Visualizes dependencies&lt;/item&gt;
      &lt;item&gt;Solution-wide scanning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use case: Preparing legacy apps for Rail integration&lt;/p&gt;
    &lt;p&gt;Example applications showing SDK integration.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;AgentTest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Simple WPF app with customer database&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;WorkflowDemo&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Workflow automation example&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Manifest Here you can find Manifest folder with all the "rail.manifest.json" already created for this example application&lt;/p&gt;
    &lt;p&gt;When converting your application to be AI-controllable, here's exactly what you need:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;What to Add&lt;/cell&gt;
        &lt;cell role="head"&gt;Automatically Included&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;C# (.NET)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;RailSDK.Universal.dll&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;RailBridge.dll&lt;/code&gt; (auto-copied)&lt;/cell&gt;
        &lt;cell&gt;One reference, everything included&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;C++ (Modern)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;rail_sdk.dll&lt;/code&gt; + &lt;code&gt;RailBridge.dll&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;cell&gt;RTTR reflection, auto method discovery&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;C++ (Legacy)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;rail_sdk.dll&lt;/code&gt; + &lt;code&gt;RailBridge.dll&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;cell&gt;Custom dispatcher, manual routing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;RailBridge.dll&lt;/code&gt; + &lt;code&gt;rail&lt;/code&gt; package&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;cell&gt;Load via &lt;code&gt;ctypes&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;RailBridge.dll&lt;/code&gt; + &lt;code&gt;rail-sdk&lt;/code&gt; npm&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;cell&gt;Load via &lt;code&gt;ffi-napi&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When you add &lt;code&gt;RailSDK.Universal&lt;/code&gt;, the native bridge is automatically copied to your output:&lt;/p&gt;
    &lt;code&gt;üìÅ bin/Debug/net8.0/
‚îú‚îÄ‚îÄ YourApp.exe
‚îú‚îÄ‚îÄ RailSDK.Universal.dll   ‚Üê You add this reference
‚îú‚îÄ‚îÄ RailBridge.dll          ‚Üê Copied automatically!
‚îî‚îÄ‚îÄ rail.manifest.json      ‚Üê You create this
&lt;/code&gt;
    &lt;p&gt;How to add:&lt;/p&gt;
    &lt;code&gt;&amp;lt;PackageReference Include="RailSDK.Universal" Version="2.0.0" /&amp;gt;&lt;/code&gt;
    &lt;p&gt;C++ has two integration modes depending on your codebase:&lt;/p&gt;
    &lt;p&gt;For new applications or codebases that support C++17:&lt;/p&gt;
    &lt;code&gt;// Register your classes with RTTR macros
RTTR_REGISTRATION {
    rttr::registration::class_&amp;lt;OrderManager&amp;gt;("OrderManager")
        .method("CreateOrder", &amp;amp;OrderManager::CreateOrder);
}

// SDK auto-discovers methods
rail::RegisterInstance("OrderManager", &amp;amp;myManager);
rail::Ignite("MyApp");&lt;/code&gt;
    &lt;p&gt;Files needed:&lt;/p&gt;
    &lt;code&gt;üìÅ YourApp/
‚îú‚îÄ‚îÄ YourApp.exe
‚îú‚îÄ‚îÄ rail_sdk.dll            ‚Üê C++ wrapper (includes RTTR)
‚îú‚îÄ‚îÄ RailBridge.dll          ‚Üê Native IPC bridge
‚îî‚îÄ‚îÄ rail.manifest.json      ‚Üê Auto-generated
&lt;/code&gt;
    &lt;p&gt;For legacy applications that can't use C++17 or RTTR (e.g., games, old codebases):&lt;/p&gt;
    &lt;code&gt;#define RAIL_NO_RTTR  // Disable RTTR

// Define your own command router
std::string MyDispatcher(const std::string&amp;amp; json) {
    if (json.find("MovePlayer") != std::string::npos) {
        MovePlayer();
        return "{\"result\": \"success\"}";
    }
    return "{\"error\": \"unknown\"}";
}

// Register and connect
rail::SetCustomDispatcher(MyDispatcher);
rail::Ignite("MyLegacyApp", "1.0", customManifest);&lt;/code&gt;
    &lt;p&gt;Files needed:&lt;/p&gt;
    &lt;code&gt;üìÅ YourApp/
‚îú‚îÄ‚îÄ YourApp.exe
‚îú‚îÄ‚îÄ rail_sdk.dll            ‚Üê C++ wrapper (no RTTR)
‚îú‚îÄ‚îÄ RailBridge.dll          ‚Üê Native IPC bridge
‚îî‚îÄ‚îÄ rail.manifest.json      ‚Üê You write this manually
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Real Examples: Notepad++ and Doom were integrated using Option B (Custom Dispatcher) because their codebases couldn't support RTTR.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;üìÅ YourProject/
‚îú‚îÄ‚îÄ main.py (or index.ts)
‚îú‚îÄ‚îÄ RailBridge.dll          ‚Üê Copy manually
‚îî‚îÄ‚îÄ rail.manifest.json      ‚Üê You create this
&lt;/code&gt;
    &lt;p&gt;Install the wrapper package that handles ctypes/ffi calls for you.&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    RailOrchestrator                       ‚îÇ
‚îÇ                    (Main AI Application)                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  LLM APIs   ‚îÇ    ‚îÇ HostService ‚îÇ    ‚îÇ AssetService  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  (Gemini,   ‚îÇ    ‚îÇ (Named Pipe ‚îÇ    ‚îÇ (Chip/Manifest‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   OpenAI)   ‚îÇ    ‚îÇ   Server)   ‚îÇ    ‚îÇ   Discovery)  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ
          ‚îÇ         Named Pipe: "RailHost"
          ‚îÇ                  ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ               RailBridge.Native            ‚îÇ
    ‚îÇ            (C-ABI Native DLL)                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ               ‚îÇ               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RailSDK   ‚îÇ ‚îÇ RailSDK   ‚îÇ ‚îÇ RailSDK   ‚îÇ
‚îÇ .Universal  ‚îÇ ‚îÇ -Cpp        ‚îÇ ‚îÇ -Python     ‚îÇ
‚îÇ (C# Apps)   ‚îÇ ‚îÇ (C++ Apps)  ‚îÇ ‚îÇ (Python)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ               ‚îÇ               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Your C#   ‚îÇ ‚îÇ  Your C++  ‚îÇ ‚îÇ Your Python ‚îÇ
‚îÇ    App     ‚îÇ ‚îÇ    App     ‚îÇ ‚îÇ   Script    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;code&gt;RailOrchestrator
    ‚îî‚îÄ‚îÄ uses ‚Üí RailSDK (RailFactory.Core) for manifest parsing
    
RailSDK.Universal
    ‚îî‚îÄ‚îÄ loads ‚Üí RailBridge.Native (DLL)
    
RailSDK-Cpp / RailSDK-Python / RailSDK-Node
    ‚îî‚îÄ‚îÄ load ‚Üí RailBridge.Native (DLL)

RailStudio
    ‚îî‚îÄ‚îÄ uses ‚Üí RailSDK (RailFactory.Core) for scanning
&lt;/code&gt;
    &lt;code&gt;// 1. Create your service
public class CustomerService
{
    public Customer GetCustomer(int id) =&amp;gt; Database.Find(id);
    public void CreateCustomer(string name, string email) { ... }
}

// 2. Add one line in App.xaml.cs
protected override void OnStartup(StartupEventArgs e)
{
    base.OnStartup(e);
    RailEngine.Ignite(this);
}

// 3. Create rail.manifest.json (or use RailStudio)&lt;/code&gt;
    &lt;code&gt;// Define callback
const char* OnCommand(const char* json) {
    auto cmd = ParseJson(json);
    if (cmd.method == "MoveMachine") {
        Machine::Move(cmd.args["x"], cmd.args["y"]);
        return R"({"result": "OK"})";
    }
    return R"({"error": "Unknown"})";
}

// Connect
rail::ignite("CNCController", manifest, OnCommand);&lt;/code&gt;
    &lt;code&gt;class DataProcessor:
    def analyze_data(self, file_path: str) -&amp;gt; dict:
        return {"rows": 1000, "status": "processed"}

engine = RailEngine()
engine.ignite([DataProcessor()])
engine.wait()&lt;/code&gt;
    &lt;p&gt;With apps connected, ask in natural language:&lt;/p&gt;
    &lt;code&gt;"Create a customer named John Smith with email john@example.com"
‚Üí AI calls CustomerService.CreateCustomer("John Smith", "john@example.com")

"Move the machine to position X=100, Y=200"
‚Üí AI calls CNCController.MoveMachine(100, 200)
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run RailOrchestrator - The main AI interface&lt;/item&gt;
      &lt;item&gt;Connect your app - Add SDK and call &lt;code&gt;Ignite()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Ask AI - Natural language commands execute your code&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RAIL Protocol - Bridging Legacy Applications and AI&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/RAIL-Suite/RAIL"/><published>2026-02-03T14:16:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46871387</id><title>Show HN: Sandboxing untrusted code using WebAssembly</title><updated>2026-02-03T17:43:39.924522+00:00</updated><content>&lt;doc fingerprint="c8903e58cf8302c2"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;Capsule&lt;/code&gt; is a runtime for coordinating AI agent tasks in isolated environments. It is designed to handle, long-running workflows, large-scale processing, autonomous decision-making securely, or even multi-agent systems.&lt;/p&gt;
    &lt;p&gt;Each task runs inside its own WebAssembly sandbox, providing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Isolated execution: Each task runs isolated from your host system&lt;/item&gt;
      &lt;item&gt;Resource limits: Set CPU, memory, and timeout limits per task&lt;/item&gt;
      &lt;item&gt;Automatic retries: Handle failures without manual intervention&lt;/item&gt;
      &lt;item&gt;Lifecycle tracking: Monitor which tasks are running, completed, or failed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This enables safe task-level execution of untrusted code within AI agent systems.&lt;/p&gt;
    &lt;p&gt;Simply annotate your Python functions with the &lt;code&gt;@task&lt;/code&gt; decorator:&lt;/p&gt;
    &lt;code&gt;from capsule import task

@task(name="analyze_data", compute="MEDIUM", ram="512MB", timeout="30s", max_retries=1)
def analyze_data(dataset: list) -&amp;gt; dict:
    """Process data in an isolated, resource-controlled environment."""
    # Your code runs safely in a Wasm sandbox
    return {"processed": len(dataset), "status": "complete"}&lt;/code&gt;
    &lt;p&gt;For TypeScript and JavaScript, use the &lt;code&gt;task()&lt;/code&gt; wrapper function with full access to the npm ecosystem:&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";

export const analyzeData = task({
  name: "analyze_data",
  compute: "MEDIUM",
  ram: "512MB",
  timeout: "30s",
  maxRetries: 1
}, (dataset: number[]): object =&amp;gt; {
  // Your code runs safely in a Wasm sandbox
  return { processed: dataset.length, status: "complete" };
});

// The "main" task is required as the entrypoint
export const main = task({
    name: "main",
    compute: "HIGH"
}, () =&amp;gt; {
  return analyzeData([1, 2, 3, 4, 5]);
});&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The runtime requires a task named &lt;code&gt;"main"&lt;/code&gt; as the entry point. Python can define the main task itself, but it's recommended to set it manually.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;capsule run main.py&lt;/code&gt; (or &lt;code&gt;main.ts&lt;/code&gt;), your code is compiled into a WebAssembly module and executed in a dedicated sandbox to isolate tasks.&lt;/p&gt;
    &lt;p&gt;Each task operates within its own sandbox with configurable resource limits, ensuring that failures are contained and don't cascade to other parts of your workflow. The host system controls every aspect of execution, from CPU allocation via Wasm fuel metering to memory constraints and timeout enforcement.&lt;/p&gt;
    &lt;p&gt;Every task returns a structured JSON envelope containing both the result and execution metadata:&lt;/p&gt;
    &lt;code&gt;{
  "success": true,
  "result": "Hello from Capsule!",
  "error": null,
  "execution": {
    "task_name": "data_processor",
    "duration_ms": 1523,
    "retries": 0,
    "fuel_consumed": 45000
  }
}&lt;/code&gt;
    &lt;p&gt;Response fields:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;success&lt;/code&gt;‚Äî Boolean indicating whether the task completed successfully&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;result&lt;/code&gt;‚Äî The actual return value from your task (json, string, null on failure etc..)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;error&lt;/code&gt;‚Äî Error details if the task failed (&lt;code&gt;{ error_type: string, message: string }&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;execution&lt;/code&gt;‚Äî Performance metrics:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;task_name&lt;/code&gt;‚Äî Name of the executed task&lt;/item&gt;&lt;item&gt;&lt;code&gt;duration_ms&lt;/code&gt;‚Äî Execution time in milliseconds&lt;/item&gt;&lt;item&gt;&lt;code&gt;retries&lt;/code&gt;‚Äî Number of retry attempts that occurred&lt;/item&gt;&lt;item&gt;&lt;code&gt;fuel_consumed&lt;/code&gt;‚Äî CPU resources used (see Compute Levels)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install capsule-run&lt;/code&gt;
    &lt;p&gt;Create &lt;code&gt;hello.py&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;from capsule import task

@task(name="main", compute="LOW", ram="64MB")
def main() -&amp;gt; str:
    return "Hello from Capsule!"&lt;/code&gt;
    &lt;p&gt;Run it:&lt;/p&gt;
    &lt;code&gt;capsule run hello.py&lt;/code&gt;
    &lt;code&gt;npm install -g @capsule-run/cli
npm install @capsule-run/sdk&lt;/code&gt;
    &lt;p&gt;Create &lt;code&gt;hello.ts&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";

export const main = task({
  name: "main",
  compute: "LOW",
  ram: "64MB"
}, (): string =&amp;gt; {
  return "Hello from Capsule!";
});&lt;/code&gt;
    &lt;p&gt;Run it:&lt;/p&gt;
    &lt;code&gt;capsule run hello.ts&lt;/code&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;--verbose&lt;/code&gt; to display real-time task execution details.&lt;/p&gt;
    &lt;p&gt;Configure your tasks with these parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;name&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Task identifier&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;function name (Python) / required (TS)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"process_data"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;compute&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;CPU allocation level: &lt;code&gt;"LOW"&lt;/code&gt;, &lt;code&gt;"MEDIUM"&lt;/code&gt;, or &lt;code&gt;"HIGH"&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"MEDIUM"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"HIGH"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;ram&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Memory limit for the task&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;unlimited&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;"512MB"&lt;/code&gt;, &lt;code&gt;"2GB"&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;timeout&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maximum execution time&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;unlimited&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;"30s"&lt;/code&gt;, &lt;code&gt;"5m"&lt;/code&gt;, &lt;code&gt;"1h"&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;&lt;code&gt;max_retries&lt;/code&gt; / &lt;code&gt;maxRetries&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Number of retry attempts on failure&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;&lt;code&gt;allowed_files&lt;/code&gt; / &lt;code&gt;allowedFiles&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Folders accessible in the sandbox&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;list&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["./data", "./output"]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;env_variables&lt;/code&gt; / &lt;code&gt;envVariables&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Environment variables accessible in the sandbox&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;list&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["API_KEY"]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Capsule controls CPU usage through WebAssembly's fuel mechanism, which meters instruction execution. The compute level determines how much fuel your task receives.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LOW provides minimal allocation for lightweight tasks&lt;/item&gt;
      &lt;item&gt;MEDIUM offers balanced resources for typical workloads&lt;/item&gt;
      &lt;item&gt;HIGH grants maximum fuel for compute-intensive operations&lt;/item&gt;
      &lt;item&gt;CUSTOM to specify an exact fuel value (e.g., &lt;code&gt;compute="1000000"&lt;/code&gt;) for precise control over execution limits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can create a &lt;code&gt;capsule.toml&lt;/code&gt; file in your project root to set default options for all tasks and define workflow metadata:&lt;/p&gt;
    &lt;code&gt;# capsule.toml

[workflow]
name = "My AI Workflow"
version = "1.0.0"
entrypoint = "src/main.py"  # Default file when running `capsule run`

[tasks]
default_compute = "MEDIUM"
default_ram = "256MB"
default_timeout = "30s"
default_max_retries = 2&lt;/code&gt;
    &lt;p&gt;With an entrypoint defined, you can simply run:&lt;/p&gt;
    &lt;code&gt;capsule run&lt;/code&gt;
    &lt;p&gt;Task-level options always override these defaults when specified.&lt;/p&gt;
    &lt;p&gt;The standard Python &lt;code&gt;requests&lt;/code&gt; library and socket-based networking aren't natively compatible with WebAssembly's sandboxed I/O model. Capsule provides its own HTTP client that works within the Wasm environment:&lt;/p&gt;
    &lt;code&gt;from capsule import task
from capsule.http import get, post, put, delete

@task(name="http_example", compute="MEDIUM", timeout="30s")
def main() -&amp;gt; dict:
    """Example demonstrating HTTP client usage within a task."""

    # GET request
    response = get("https://api.example.com/data")

    # POST with JSON body
    response = post("https://api.example.com/submit", json={"key": "value"})

    # Response methods
    is_ok = response.ok()           # Returns True if status code is 2xx
    status = response.status_code    # Get the HTTP status code
    data = response.json()           # Parse response as JSON
    text = response.text()           # Get response as text

    return {"status": status, "success": is_ok}&lt;/code&gt;
    &lt;p&gt;Standard libraries like &lt;code&gt;fetch&lt;/code&gt; are already compatible, so no custom HTTP client is needed for TypeScript/JavaScript.&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";

export const main = task({
    name: "main",
    compute: "MEDIUM"
}, async () =&amp;gt; {
    const response = await fetch("https://api.example.com/data");
    return response.json();
});&lt;/code&gt;
    &lt;p&gt;Tasks can read and write files within directories specified in &lt;code&gt;allowed_files&lt;/code&gt;. Any attempt to access files outside these directories is not possible.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Currently, &lt;code&gt;allowed_files&lt;/code&gt; supports directory paths, not individual files.&lt;/p&gt;
    &lt;p&gt;Python's standard file operations work normally. Use &lt;code&gt;open()&lt;/code&gt;, &lt;code&gt;os&lt;/code&gt;, &lt;code&gt;pathlib&lt;/code&gt;, or any file manipulation library.&lt;/p&gt;
    &lt;code&gt;from capsule import task

@task(name="restricted_writer", allowed_files=["./output"])
def restricted_writer() -&amp;gt; None:
    with open("./output/result.txt", "w") as f:
        f.write("result")

@task(name="main")
def main() -&amp;gt; str:
    restricted_writer()&lt;/code&gt;
    &lt;p&gt;Common Node.js built-ins are available. Use the standard &lt;code&gt;fs&lt;/code&gt; module:&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";
import fs from "fs/promises";

export const restrictedWriter = task({
    name: "restricted_writer",
    allowedFiles: ["./output"]
}, async () =&amp;gt; {
    await fs.writeFile("./output/result.txt", "result");
});

export const main = task({ name: "main", allowedFiles: ["./data"] }, async () =&amp;gt; {
    await restrictedWriter();
    return await fs.readFile("./data/input.txt", "utf8");
});&lt;/code&gt;
    &lt;p&gt;Tasks can access environment variables to read configuration, API keys, or other runtime settings.&lt;/p&gt;
    &lt;p&gt;Use Python's standard &lt;code&gt;os.environ&lt;/code&gt; to access environment variables:&lt;/p&gt;
    &lt;code&gt;from capsule import task
import os

@task(name="main", env_variables=["API_KEY"])
def main() -&amp;gt; dict:
    api_key = os.environ.get("API_KEY")
    return {"api_key": api_key}&lt;/code&gt;
    &lt;p&gt;Use the standard &lt;code&gt;process.env&lt;/code&gt; to access environment variables:&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";

export const main = task({
    name: "main",
    envVariables: ["API_KEY"]
}, () =&amp;gt; {
    const apiKey = process.env.API_KEY;
    return { apiKeySet: apiKey !== undefined };
});&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;TypeScript/JavaScript has broader compatibility than Python since it doesn't rely on native bindings.&lt;/p&gt;
    &lt;p&gt;Python: Pure Python packages and standard library modules work. Packages with C extensions (&lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;) are not yet supported.&lt;/p&gt;
    &lt;p&gt;TypeScript/JavaScript: npm packages and ES modules work. Common Node.js built-ins are available. If you have any trouble with a built-in do not hesitate to open an issue.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome!&lt;/p&gt;
    &lt;p&gt;Prerequisites: Rust (latest stable), Python 3.13+, Node.js 22+&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/mavdol/capsule.git
cd capsule

# Build and install CLI
cargo install --path crates/capsule-cli

# Python SDK (editable install)
pip install -e crates/capsule-sdk/python

# TypeScript SDK (link for local dev)
cd crates/capsule-sdk/javascript
npm install &amp;amp;&amp;amp; npm run build &amp;amp;&amp;amp; npm link

# Then in your project: npm link @capsule-run/sdk&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create a feature branch: &lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Run tests: &lt;code&gt;cargo test&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Need help? Open an issue&lt;/p&gt;
    &lt;p&gt;Capsule builds on these open source projects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;componentize-py ‚Äì Python to WebAssembly Component compilation&lt;/item&gt;
      &lt;item&gt;jco ‚Äì JavaScript toolchain for WebAssembly Components&lt;/item&gt;
      &lt;item&gt;wasmtime ‚Äì WebAssembly runtime&lt;/item&gt;
      &lt;item&gt;WASI ‚Äì WebAssembly System Interface&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0 - see the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mavdol/capsule"/><published>2026-02-03T14:28:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46871473</id><title>GitHub Browser Plugin for AI Contribution Blame in Pull Requests</title><updated>2026-02-03T17:43:39.774315+00:00</updated><content>&lt;doc fingerprint="3741df528d35da3a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Quickstart, TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Learn about and add git-ai to your tooling, (git cli and editor extensions)&lt;/item&gt;
      &lt;item&gt;Build, install and authenticate refined-github-ai-pr&lt;/item&gt;
      &lt;item&gt;Push and pull request some Ai generated code via git to Github&lt;/item&gt;
      &lt;item&gt; Navigate to your PR in Github &lt;code&gt;https://github.com/&amp;lt;owner&amp;gt;/&amp;lt;repo&amp;gt;/pull/&amp;lt;PR ID&amp;gt;/changes&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Bask in the glory of ai annotations (scroll to the end ‚Üì for example screenshots)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Join my email list for updates&lt;/p&gt;
    &lt;head rend="h2"&gt;Identifying AI Contributions&lt;/head&gt;
    &lt;head rend="h3"&gt;The Rise of Low‚ÄëFriction AI Contributions&lt;/head&gt;
    &lt;p&gt;With the proliferation of effortless code‚Äëgenerating tools like Claude Code, Codex, and Cursor, slop‚Äëslung contributions are being doled out as outright spam in hopes of getting a name tacked onto popular open‚Äësource projects. Most are well‚Äëintentioned ‚Äî it‚Äôs just that this workflow is entirely new, and the tools and norms haven‚Äôt been established yet. Some open‚Äësource projects have publicly banned them (see: zig, tldr, ghostty), going so far as to vet contributors into a select trusted group.&lt;/p&gt;
    &lt;head rend="h3"&gt;When AI‚ÄëGenerated Code Can Be Appropriate&lt;/head&gt;
    &lt;p&gt;Oftentimes, depending on the preference of the team and project, less consequential and isolated code could warrant a 100% AI contribution. Non‚Äëuser‚Äëfacing tooling, a private beta feature, or a proof‚Äëof‚Äëconcept immediately come to mind. The ability to retroactively see which parts of the codebase were AI contributions, especially in these use cases, could be very valuable. What was tabbed in by Cursor at 3am six months ago could be a part of today‚Äôs refactor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Percentages, Policies, and Maintainer Trust&lt;/head&gt;
    &lt;p&gt;Projects like Zig may never allow ai contributions, and I am not here to argue that they should change this stance. But in other cases, where the reaction is a heavy‚Äëhanded outright refusal, maintainers and developers could have a change of heart if they could codify an allowable percentage done by AI in each pull request. Even without a hard‚Äëand‚Äëfast rule, a percentage could serve as a sort of gut check ‚Äî an overall score as part of a bigger picture of quality in a PR.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enter the Git-Ai Project&lt;/head&gt;
    &lt;head rend="h3"&gt;What Git‚ÄëAI Does&lt;/head&gt;
    &lt;p&gt;The git-ai project allows you to automatically track agentic ai code contributions across your team and codebase, zeroing in line-by-line, preserving code-generating prompts, all while working within common git workflows. Git-ai works by extending and enhancing your current tooling without slowdown (thanks to Rust) while ‚Äòstaying out of the way‚Äô - so you can work as if it‚Äôs not even there.&lt;/p&gt;
    &lt;head rend="h3"&gt;What Data Git‚ÄëAI Captures&lt;/head&gt;
    &lt;p&gt;git-ai stores things like per-line ai contributions, the model and prompt given for the code generated.&lt;/p&gt;
    &lt;p&gt;git-ai works by storing this ai contribution data in git notes. Git notes are simply blobs attached to commit refs. It‚Äôs eloquent in that the meta data stays with the commit, git-ai also contains additional instrumentation to ‚Äúsurvive a &lt;code&gt;merge --squash&lt;/code&gt;, &lt;code&gt;rebase&lt;/code&gt;, &lt;code&gt;reset&lt;/code&gt;, &lt;code&gt;cherry-pick&lt;/code&gt; etc.‚Äù&lt;/p&gt;
    &lt;head rend="h3"&gt;Project Goals (From the README)&lt;/head&gt;
    &lt;p&gt;From the README.md:&lt;/p&gt;
    &lt;quote&gt;&lt;head&gt;Goals of&lt;/head&gt;&lt;code&gt;git-ai&lt;/code&gt;project&lt;p&gt;ü§ñ Track AI code in a Multi-Agent world. Because developers get to choose their tools, engineering teams need a vendor agnostic way to track AI impact in their repos.&lt;/p&gt;&lt;p&gt;üéØ Accurate attribution from Laptop ‚Üí Pull Request ‚Üí Merged. Claude Code, Cursor and Copilot cannot track code after generation‚ÄîGit AI follows it through the entire workflow.&lt;/p&gt;&lt;p&gt;üîÑ Support real-world git workflows by making sure AI-Authorship annotations survive a&lt;/p&gt;&lt;code&gt;merge --squash&lt;/code&gt;,&lt;code&gt;rebase&lt;/code&gt;,&lt;code&gt;reset&lt;/code&gt;,&lt;code&gt;cherry-pick&lt;/code&gt;etc.&lt;p&gt;üîó Maintain link between prompts and code - there is valuable context and requirements in team prompts‚Äîpreserve them alongside code.&lt;/p&gt;&lt;p&gt;üöÄ Git-native + Fast -&lt;/p&gt;&lt;code&gt;git-ai&lt;/code&gt;is built on git plumbing commands. Negligible impact even in large repos (&amp;lt;100ms). Tested in Chromium.&lt;/quote&gt;
    &lt;p&gt;NOTE: I have no affiliation with git-ai, but happily applaud their efforts, go check em‚Äô out! github.com/git-ai-project/git-ai&lt;/p&gt;
    &lt;head rend="h2"&gt;Github PR interface Support&lt;/head&gt;
    &lt;head rend="h3"&gt;Why Focus on Pull Requests&lt;/head&gt;
    &lt;p&gt;To experimentally work towards a developer friendly solution, I wanted to try dropping this tooling into a common point of convergence within collaborative version control workflows; Github Pull Requests&lt;/p&gt;
    &lt;head rend="h3"&gt;Existing Git‚ÄëAI Integrations with VSCode&lt;/head&gt;
    &lt;p&gt;git-ai comes with many integrations, and even has an RFC v3.0, so other tooling providers may implement it themselves. The VSCode integration works very well. AI contributed code is given a gutter highlight, and upon line selectshows the model responsible for said ai generated code, long-hovering provides even more context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extending the GitHub PR Experience&lt;/head&gt;
    &lt;p&gt;To recreate this editor/code-view highlighting, as well as provide human-vs-ai percentage metering in the Github PR experience, I forked an existing github extended plugin github-refined into refined-github-ai-pr This plugin has all the features of the prior, even allowing you to toggle this ai contribution blaming feature on and off in the options (Be sure to check out the screenshots below)&lt;/p&gt;
    &lt;head rend="h3"&gt;More on Git-Ai Tooling‚Ä¶&lt;/head&gt;
    &lt;p&gt;Although there is currently no official support from git-ai (as of Jan 2026) for extending the Github PR interface with Git-ai annotations. There is an early access feature: Stat Bot - to ‚ÄúAggregate git-ai data at the PR, developer, repository and organization levels‚Äù It may be worth it for you to check out and could serve as an excellent way to support the creators of git-ai&lt;/p&gt;
    &lt;head rend="h3"&gt;Caveats&lt;/head&gt;
    &lt;p&gt;One Major Caveat with &lt;code&gt;refined-github-with-ai-pr&lt;/code&gt;, is that it relies on augmenting Github‚Äôs HTML via classes, which could very well change without notice, breaking this plugin.&lt;/p&gt;
    &lt;p&gt;This plugin serves as a beta and prototype, to fuel the conversation of what working with these new tools might look like; and I encourage community members to join the conversation. Maybe github will work towards adding this themselves in the future. Please comment on this post in hackernews or open an issue for refined-github-ai-pr I‚Äôd love to hear what you‚Äôre thinking!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.rbby.dev/posts/github-ai-contribution-blame-for-pull-requests/"/><published>2026-02-03T14:35:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46871504</id><title>A WhatsApp bug lets malicious media files spread through group chats</title><updated>2026-02-03T17:43:39.596062+00:00</updated><content>&lt;doc fingerprint="9f00cd378b391549"&gt;
  &lt;main&gt;
    &lt;p&gt;WhatsApp is going through a rough patch. Some users would argue it has been ever since Meta acquired the once widely trusted messaging platform. User sentiment has shifted from ‚Äútrusted default messenger‚Äù to a grudgingly necessary Meta product.&lt;/p&gt;
    &lt;p&gt;Privacy-aware users still see WhatsApp as one of the more secure mass-market messaging platforms if you lock down its settings. Even then, many remain uneasy about Meta‚Äôs broader ecosystem, and wish all their contacts would switch to a more secure platform.&lt;/p&gt;
    &lt;p&gt;Back to current affairs, which will only reinforce that sentiment.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs Project Zero has just disclosed a WhatsApp vulnerability where a malicious media file, sent into a newly created group chat, can be automatically downloaded and used as an attack vector.&lt;/p&gt;
    &lt;p&gt;The bug affects WhatsApp on Android and involves zero‚Äëclick media downloads in group chats. You can be attacked simply by being added to a group and having a malicious file sent to you.&lt;/p&gt;
    &lt;p&gt;According to Project Zero, the attack is most likely to be used in targeted campaigns, since the attacker needs to know or guess at least one contact. While focused, it is relatively easy to repeat once an attacker has a likely target list.&lt;/p&gt;
    &lt;p&gt;And to put a cherry on top for WhatsApp‚Äôs competitors, a potentially even more serious concern for the popular messaging platform, an international group of plaintiffs sued Meta Platforms, alleging the WhatsApp owner can store, analyze, and access virtually all of users‚Äô private communications, despite WhatsApp‚Äôs end-to-end encryption claims.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to secure WhatsApp&lt;/head&gt;
    &lt;p&gt;Reportedly, Meta pushed a server change on November 11, 2025, but Google says that only partially resolved the issue. So, Meta is working on a comprehensive fix.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs advice is to disable Automatic Download or enable WhatsApp‚Äôs Advanced Privacy Mode so that media is not automatically downloaded to your phone.&lt;/p&gt;
    &lt;p&gt;And you‚Äôll need to keep WhatsApp updated to get the latest patches, which is true for any app and for Android itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;Turn off auto-download of media&lt;/head&gt;
    &lt;p&gt;Goal: ensure that no photos, videos, audio, or documents are pulled to the device without an explicit decision.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Open WhatsApp on your Android device.&lt;/item&gt;
      &lt;item&gt;Tap the three‚Äëdot menu in the top‚Äëright corner, then tap Settings.&lt;/item&gt;
      &lt;item&gt;Go to Storage and data (sometimes labeled Data and storage usage).&lt;/item&gt;
      &lt;item&gt;Under Media auto-download, you will see When using mobile data, when connected on Wi‚ÄëFi. and when roaming.&lt;/item&gt;
      &lt;item&gt;For each of these three entries, tap it and uncheck all media types: Photos, Audio, Videos, Documents. Then tap OK.&lt;/item&gt;
      &lt;item&gt;Confirm that each category now shows something like ‚ÄúNo media‚Äù under it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Doing this directly implements Project Zero‚Äôs guidance to ‚Äúdisable Automatic Download‚Äù so that malicious media can‚Äôt silently land on your storage as soon as you are dropped into a hostile group.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stop WhatsApp from saving media to your Android gallery&lt;/head&gt;
    &lt;p&gt;Even if WhatsApp still downloads some content, you can stop it from leaking into shared storage where other apps and system components see it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Settings, go to Chats.&lt;/item&gt;
      &lt;item&gt;Turn off Media visibility (or similar option such as Show media in gallery). For particularly sensitive chats, open the chat, tap the contact or group name, find Media visibility, and set it to No for that thread.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;WhatsApp is a sandbox, and should contain the threat. Which means, keeping media inside WhatsApp makes it harder for a malicious file to be processed by other, possibly more vulnerable components.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lock down who can add you to groups&lt;/head&gt;
    &lt;p&gt;The attack chain requires the attacker to add you and one of your contacts to a new group. Reducing who can do that lowers risk.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Settings, tap Privacy.&lt;/item&gt;
      &lt;item&gt;Tap Groups.&lt;/item&gt;
      &lt;item&gt;Change from Everyone to My contacts or ideally My contacts except‚Ä¶ and exclude any numbers you do not fully trust.&lt;/item&gt;
      &lt;item&gt;If you use WhatsApp for work, consider keeping group membership strictly to known contacts and approved admins.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Set up two-step verification on your WhatsApp account&lt;/head&gt;
    &lt;p&gt;Read this guide for Android and iOS to learn how to do that.&lt;/p&gt;
    &lt;p&gt;We don‚Äôt just report on phone security‚Äîwe provide it&lt;/p&gt;
    &lt;p&gt;Cybersecurity risks should never spread beyond a headline. Keep threats off your mobile devices by downloading Malwarebytes for iOS, and Malwarebytes for Android today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.malwarebytes.com/blog/news/2026/01/a-whatsapp-bug-lets-malicious-media-files-spread-through-group-chats"/><published>2026-02-03T14:38:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872190</id><title>Data Brokers Can Fuel Violence Against Public Servants</title><updated>2026-02-03T17:43:39.353899+00:00</updated><content>&lt;doc fingerprint="e29891d9dc839717"&gt;
  &lt;main&gt;
    &lt;p&gt;A new report published Tuesday finds that while violent threats to public servants across the US have been increasing, ‚Äúcomprehensive‚Äù state-level consumer privacy laws do not provide adequate protections for those civil servants, creating a ‚Äúdata-to-violence pipeline.‚Äù&lt;/p&gt;
    &lt;p&gt;The report was published by researcher Justin Sherman of the Security Project at the Public Service Alliance, a platform that provides free and discounted security services to current and former public servants. While Trump officials have referred to documenting federal immigration agents‚Äô behavior on the job as ‚Äúviolence‚Äù and ‚Äúdoxing,‚Äù Sherman says the report focuses on the more traditional, widely accepted definition‚Äîthe publication of someone‚Äôs personal, private information, such as their home address, with the specific intent of harming them.&lt;/p&gt;
    &lt;p&gt;Sherman analyzed 19 different consumer privacy laws and found that while they all give consumers the right to stop data brokers from selling personal information obtained from private sources, none give ‚Äúpublic servants the right to legally compel state agencies to redact their personal data from public records,‚Äù and none prevent data brokers from selling data, including people‚Äôs home addresses, when they are obtained through public sources such as property records or court filings. Further, none include what is called a ‚Äúprivate right of action,‚Äù which would allow individuals to sue over violations of their respective state‚Äôs privacy law.&lt;/p&gt;
    &lt;p&gt;Together, this means that information about public employees is uniquely available and that they have uniquely few ways to prevent its dissemination.&lt;/p&gt;
    &lt;p&gt;Violent threats against public servants have been increasing, according to a separate analysis by PSA and the Impact Project of over 1,600 individual threats made against public servants between 2015 and 2025. That analysis found that violent threats against local public servants, including school board members and election workers, represented nearly a third of the reports reviewed. It also found that threatening statements occurred at nearly nine times the rate of physical attacks, and that one form of threat can escalate into another.&lt;/p&gt;
    &lt;p&gt;A 2024 report from the Brennan Center for Justice found that larger shares of women and Democrats reported increases in the severity of abuse since first taking public office, compared with men and Republicans.&lt;/p&gt;
    &lt;p&gt;Last year, a 57-year-old man was charged with assassinating Melissa Hortman, a Democratic state representative, along with her husband at their home in Minnesota. According to court records, the alleged shooter had handwritten lists of dozens of Minnesota state and federal public officials, including Hortman‚Äôs name and her home address, along with 11 ‚Äúpeople search engines‚Äù that allow anyone to find personal information about a person, including their home addresses, phone numbers, and names of relatives, often for a nominal fee.&lt;/p&gt;
    &lt;p&gt;The report advocates for legislation that would specifically address privacy concerns for all public servants, including public school educators and local elected officials, who are not necessarily covered by existing federal or state privacy laws. It suggests that lawmakers could try to balance First Amendment and privacy concerns by regulating the digitization of public records and how easy they are to access remotely, instead of limiting them completely.&lt;/p&gt;
    &lt;p&gt;Sherman, the author of the new report, said that while many public records can be useful to journalists and accountability watchdogs, repackaged public records sold by data brokers can make it too easy for abusive individuals to stalk and harass victims even when they move to a different state. In the past, people seeking out public records would already have to have an idea of where that public record was, and physically go to that location.&lt;/p&gt;
    &lt;p&gt;While residents of states with consumer privacy laws can request limits on data collected from private sources, it‚Äôs not always easy to do so. Only one state, California, offers a way for residents to limit what information data brokers collect and sell about them en masse and for free via its Delete Request and Opt-out Platform. People in other states, including public servants, must file deletion requests manually or pay for an opt-out service that promises to do so on their behalf.&lt;/p&gt;
    &lt;p&gt;Last year, dozens of data brokers were caught hiding data removal instructions from Google, making it difficult for consumers to find them. Even when consumers pay to use services that specialize in filing data deletion requests, the results aren‚Äôt perfect. In 2024, Consumer Reports studied the effectiveness of seven different data removal services, which ranged from $19.99 to $249 a year in cost, and found that at best they were only successful about two-thirds of the time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wired.com/story/how-data-brokers-can-fuel-violence-against-public-servants/"/><published>2026-02-03T15:28:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872562</id><title>AI Didn't Break Copyright Law, It Just Exposed How Broken It Was</title><updated>2026-02-03T17:43:39.045441+00:00</updated><content>&lt;doc fingerprint="ad50135a7bee2f97"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Didn't Break Copyright Law, It Just Exposed How Broken It Already Was&lt;/head&gt;
    &lt;p&gt;If you paint a picture of Sonic the Hedgehog in your living room, you are technically creating an unauthorized derivative work‚Äîbut in practice, no one cares. Private, noncommercial creation has always lived in a space where copyright law exists on paper but is rarely enforced.&lt;/p&gt;
    &lt;p&gt;Gift it to a friend? Still functionally tolerated‚Äîa technical act of distribution that copyright law mostly ignores at human scale. Take a photo and post it on Instagram? Now you‚Äôve crossed into public distribution of a derivative work without permission. Under the letter of the law, that‚Äôs infringement, although a fair-use defense might apply and Sega almost certainly won‚Äôt care. It‚Äôs fan engagement, free marketing, and good PR.&lt;/p&gt;
    &lt;p&gt;Sell that painting, though, and the tolerance disappears. You‚Äôre no longer a fan, you‚Äôre a competitor.&lt;/p&gt;
    &lt;p&gt;This gap between the written law and its practical application isn‚Äôt an AI problem. It‚Äôs something that has been quietly tolerated for decades. The system works not because the rules are clear, but because enforcement has been selectively targeted for impact and the scale has been human. With generative AI, that ambiguity suddenly represents billions of dollars in litigation and potential damages, transforming what was once informal tolerance into an existential legal battle that forces long-implicit assumptions into the open.&lt;/p&gt;
    &lt;head rend="h2"&gt;Copyright Law Was Built for Human Scale&lt;/head&gt;
    &lt;p&gt;Copyright law has always relied on a set of quiet assumptions that were never written down but broadly understood: creation and transformation are slow, distribution is costly, and enforcement can be discretionary. Most infringement has been tolerated not because it was legal, but because it was rare, small, and culturally useful.&lt;/p&gt;
    &lt;p&gt;Fan art lives in this space. So do fan films. Star Wars is a canonical example; not because Lucasfilm formally granted rights, but because informal norms emerged: no monetization, no confusion with official releases, and the understanding that rights holders could assert control at any moment. This worked because these projects were expensive to make, clearly unofficial, and very few in number.&lt;/p&gt;
    &lt;p&gt;AI didn‚Äôt cherry-pick gray areas in copyright law; it removed the human-scale constraints that made those gray areas manageable. When AI can generate feature-length content at ultra-high volume, rights holders will inevitably argue that mere creation‚Äînot just distribution‚Äîmust be prohibited.&lt;/p&gt;
    &lt;p&gt;But is there an easy solution?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Training Layer: Why You Can‚Äôt Solve It There&lt;/head&gt;
    &lt;p&gt;The most common proposal is simple: ban training on copyrighted content. Keep AI ‚Äúclean‚Äù from the start.&lt;/p&gt;
    &lt;p&gt;This sounds reasonable until you examine what ‚Äúclean‚Äù actually means.&lt;/p&gt;
    &lt;p&gt;Even if you scrupulously avoid pirated material, the internet is saturated with legally posted content about copyrighted characters. Fair-use images of Sonic appear in reviews, commentary, parody, and news articles. There are photographs of licensed Sonic merchandise. There are text descriptions in product listings, forum discussions, and blog posts.&lt;/p&gt;
    &lt;p&gt;An AI trained only on legally accessible data will still learn what Sonic looks like. It will learn the character design, the color palette, the aesthetic. You cannot train on the open internet without absorbing information about major cultural properties. That knowledge emerges from millions of non-infringing references.&lt;/p&gt;
    &lt;p&gt;It gets worse. An entire article‚Äôs worth of information can be reconstructed from fair-use snippets scattered across the web‚Äîa quote here, a summary there, facts recombined from public sources. The model may not copy any single source; it synthesizes patterns from a sea of legitimate data.&lt;/p&gt;
    &lt;p&gt;Copyright law does recognize intermediate copies as potentially infringing absent fair-use defenses, and courts have historically treated unauthorized reproductions, however temporary, as relevant. But applying that doctrine to modern model training collapses under scale. When training datasets contain billions of documents, establishing precisely what went in, which copies matter, and what causal role any individual work played becomes functionally impossible.&lt;/p&gt;
    &lt;p&gt;Even outside AI, the boundaries around ‚Äútransformation‚Äù have always been murky. Artisans de Gen√®ve, for example, is an independent workshop that was sued by Rolex for modifying its watches. The eventual compromise was not a clean rule about transformation, but a narrow procedural boundary imposed by settlement: Artisans de Gen√®ve could only modify watches brought in directly by customers, not purchase them themselves. The law avoided defining where transformation ends and infringement begins; it simply constrained scale and distribution.&lt;/p&gt;
    &lt;p&gt;Model training removes those constraints entirely.&lt;/p&gt;
    &lt;p&gt;And even when pirated content is used in training, proving it is extraordinarily difficult‚Äîand removing it without retraining is nearly impossible. The New York Times attempted to demonstrate this by prompt-engineering models to reproduce exact articles through next-token prediction (a resource-intensive effort that doesn‚Äôt scale, which OpenAI characterizes as a form of adversarial prompting). Declaring an entire model ‚Äútainted‚Äù and subject to destruction because some fraction of its inputs were improperly sourced pushes copyright into uncharted territory. Unlike trade secret law, copyright traditionally remedies harm through damages, not destruction of downstream systems.&lt;/p&gt;
    &lt;p&gt;The training layer simply cannot act as a reliable enforcement point.&lt;/p&gt;
    &lt;p&gt;But rather than disappearing entirely, that pressure shifts downward, toward generation and distribution.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Generation Layer: Creation Without Friction&lt;/head&gt;
    &lt;p&gt;At the generation layer, AI systems turn abstract knowledge into concrete outputs. This is where large model providers implement partial copyright enforcement today, implicitly framing LLMs as something more than neutral tools. Adobe Illustrator doesn‚Äôt impose IP restrictions on what users can draw; LLMs increasingly do.&lt;/p&gt;
    &lt;p&gt;Effective enforcement at this layer requires treating intent, experimentation, parody, reference, and coincidence identically. This is not a subtle shift. Copyright law was never designed to evaluate private acts of creation at massive scale.&lt;/p&gt;
    &lt;p&gt;Two problems immediately emerge:&lt;/p&gt;
    &lt;p&gt;First, intent becomes unreadable. A model generating something ‚Äúsimilar‚Äù to Sonic isn‚Äôt forming an artistic goal. It‚Äôs sampling from a probability space shaped by its training. Inferring copyright-relevant intent here is conceptually incoherent and operationally impossible. Enforcement degrades into a cat-and-mouse game of banned phrases and descriptions that users can always circumvent (purposefully, or otherwise). There is no discrete boundary where output becomes infringing, nor any realistic way for a model to recognize all forms of infringement across global IP.&lt;/p&gt;
    &lt;p&gt;Second, statutory damages become absurd. Copyright penalties were calibrated for rare, explicit, human-scale violations, often with willfulness in mind. When generation is cheap and ubiquitous, applying the same framework produces penalties so disproportionate that enforcement loses legitimacy. A single automated script generating infringing content at scale could theoretically expose model providers or users to catastrophic liability entirely disconnected from actual harm.&lt;/p&gt;
    &lt;p&gt;Crucially, generation alone does not imply harm. Most generated content is never shared. Context matters.&lt;/p&gt;
    &lt;p&gt;Which brings us to the only layer where copyright law has ever really functioned.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Distribution Layer: Where Harm Actually Manifests&lt;/head&gt;
    &lt;p&gt;Copyright law polices distribution, not thought. Harm occurs when works are shared, substituted, or monetized.&lt;/p&gt;
    &lt;p&gt;Uploading an AI-generated Sonic animation to a private folder is meaningfully different from uploading it to YouTube. Flooding a platform with AI-generated Star Wars shorts is meaningfully different from sketching them for yourself. Market substitution, audience diversion, and brand dilution occur only once content is distributed.&lt;/p&gt;
    &lt;p&gt;This is why enforcement mechanisms like takedowns, DMCA safe harbors, content-ID systems, and platform moderation, however imperfect, operate here. They‚Äôre blunt, but they align with where harm occurs.&lt;/p&gt;
    &lt;p&gt;Trying to push enforcement downward into generation destabilizes the system. You end up policing private creation before any material harm occurs because you cannot control public scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Liability Maze: Everyone Loses&lt;/head&gt;
    &lt;p&gt;At this point, the question becomes not just where enforcement happens, but who absorbs the liability when it does. Different enforcement points pull different actors into the blast radius, and none of them produce a clean outcome. We‚Äôve already covered why training enforcement is simply unenforceable. But how about generation or distribution?&lt;/p&gt;
    &lt;head rend="h3"&gt;Enforcement at Generation&lt;/head&gt;
    &lt;p&gt;If liability attaches at the moment of generation, responsibility has to fall on either model providers or users.&lt;/p&gt;
    &lt;p&gt;Model Providers Require safeguards, filters, surveillance of API usage, and comprehensive IP databases.&lt;/p&gt;
    &lt;p&gt;This overwhelmingly favors incumbents. Google, OpenAI, and Meta can afford legal teams, bespoke filtering infrastructure, and licensing deals with major rights holders. A startup can‚Äôt. An open-source project certainly can‚Äôt. Compliance becomes an existential cost, entrenching monopolies and freezing competition at exactly the wrong moment.&lt;/p&gt;
    &lt;p&gt;Users Make generating copyrighted content illegal.&lt;/p&gt;
    &lt;p&gt;This quickly collapses into absurdity. Is typing ‚ÄúMario‚Äù now a crime? What about ‚ÄúItalian plumber in a red hat‚Äù? What if a vague prompt unintentionally produces something infringing? Is discovering gaps in the model safeguards akin to hacking or infringement? Enforcing this would require monitoring generation at scale; a surveillance regime that‚Äôs both impractical and deeply unsettling to users who share deeply personal information with AI models (a concern already surfacing in NYT v. OpenAI‚Äìadjacent arguments that would require access to large volumes of OpenAI customer query data).&lt;/p&gt;
    &lt;p&gt;Generation-level enforcement either centralizes power or becomes extremely overzealous in its application. Sometimes both.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enforcement at Distribution&lt;/head&gt;
    &lt;p&gt;Alternatively, liability can attach when content is shared, published, or monetized.&lt;/p&gt;
    &lt;p&gt;Platforms and intermediaries. Shift responsibility to hosting and distribution layers.&lt;/p&gt;
    &lt;p&gt;This mirrors how copyright already functions online. Individual users are technically liable for infringement, but platforms are shielded by safe-harbor regimes so long as they remove content when notified. In practice, this turns legal enforcement into platform policy.&lt;/p&gt;
    &lt;p&gt;Faced with massive volumes of AI-generated content, platforms would respond defensively. The safest move is over-enforcement: aggressive takedowns, broad bans on derivative works, and stricter moderation of fan art and remix culture. The result is likely to be quiet erosion of some of the internet‚Äôs most creative ecosystems.&lt;/p&gt;
    &lt;p&gt;Users still bear liability in theory, but experience it indirectly through rules that increasingly err on the side of deletion.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Clean Cut&lt;/head&gt;
    &lt;p&gt;Every enforcement point shifts responsibility and damage elsewhere:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generation favors incumbents or mandates surveillance.&lt;/item&gt;
      &lt;item&gt;Distribution incentivizes over-policing and cultural flattening.&lt;/item&gt;
      &lt;item&gt;User liability is impractical at scale.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;None of these approaches resolve the tension so much as relocate it, trading one set of pathologies for another.&lt;/p&gt;
    &lt;head rend="h2"&gt;Foreign Models: Why US Regulation Might Not Matter&lt;/head&gt;
    &lt;p&gt;Even if the U.S. solved every problem above perfectly, it might not matter.&lt;/p&gt;
    &lt;p&gt;AI is global. If U.S. regulation becomes too restrictive, development elsewhere will outpace it. A foreign company can train models with minimal copyright constraints, host them abroad, and make them accessible to Americans. Using foreign AI services is not inherently illegal today.&lt;/p&gt;
    &lt;p&gt;This leaves bleak options regarding foreign models:&lt;/p&gt;
    &lt;p&gt;Restrict access to foreign models: Block access via ISPs, DNS providers, and hosting platforms‚Äîan approach that has not meaningfully stopped piracy&lt;/p&gt;
    &lt;p&gt;Criminalize use: Nearly impossible to enforce without mass internet surveillance.&lt;/p&gt;
    &lt;p&gt;Restrict commercial outputs: Accept access as inevitable, but prohibit monetization without compliance. This is more enforceable, but requires traceability through watermarking or permitting systems for commercial goods. It is also predicated on the assumption that non-commercial content creation won‚Äôt damage the rights holder‚Äôs ability to monetize their work.&lt;/p&gt;
    &lt;p&gt;The likely outcome is already emerging: a two-tier system. ‚ÄúSafe‚Äù U.S. AI for commercial use, increasingly tightly integrated with major rights holders, while accepting unavoidable access to foreign or open-source models for everyone else.&lt;/p&gt;
    &lt;p&gt;Once the ecosystem splits this way, compliance stops being a universal requirement and becomes a choice. Large companies that need legal certainty will opt into the regulated tier. Everyone else will not. And once model weights are public, compliance becomes voluntary in a much more literal sense.&lt;/p&gt;
    &lt;p&gt;You can sue Meta. You can‚Äôt stop someone in another jurisdiction from hosting an unrestricted fork. Strict domestic regulation risks handicapping U.S. companies while failing to protect rights holders. It also places a disproportionate burden on smaller companies that can‚Äôt afford the legal and compliance overhead required to navigate these gray areas.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Pressure Cooker: Irreconcilable Forces&lt;/head&gt;
    &lt;p&gt;All of this is playing out against a backdrop of enormous, contradictory pressures.&lt;/p&gt;
    &lt;p&gt;Publishers are desperate. Each technological wave‚Äîblogs, social media, ad blockers, paywalls‚Äîeroded their leverage. AI feels like another existential blow, spanning text, images, audio, and video.&lt;/p&gt;
    &lt;p&gt;Governments are trapped. Historically aligned with rightsholders, they now face the political cost of crippling domestic AI while global competitors surge ahead. Meanwhile, AI companies now make the GDP-growth argument once owned by the media industry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Searching for Alternatives&lt;/head&gt;
    &lt;p&gt;Given those pressures, it‚Äôs tempting to look for alternative enforcement frameworks that can satisfy all parties. The problem is that every proposal fails somewhere fundamental.&lt;/p&gt;
    &lt;p&gt;Training-based enforcement is untraceable. Generation-based enforcement is incoherent. Distribution-based enforcement is reactive, incomplete, and historically failed to curtail piracy. Jurisdictions diverge, and global access routes around national rules.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not just enforcement breaking down, but attribution itself. Influence can‚Äôt be cleanly measured, harm can‚Äôt be localized, and responsibility can‚Äôt be assigned without collapsing into surveillance or overreach.&lt;/p&gt;
    &lt;p&gt;This naturally leads to proposals that sidestep enforcement entirely, embedding compensation directly into the AI industry itself.&lt;/p&gt;
    &lt;p&gt;Government-mediated compensation: Tax AI companies and redistribute funds to rights holders, similar to blank media levies. But how do you determine fair distribution? Should a photographer whose work appeared once in training receive the same as an artist whose style was heavily learned? At internet scale, everyone with an online presence becomes a rights holder whose work was ingested indirectly. The administrative complexity is staggering. The likely outcome is either a blunt, unfair allocation or a system that primarily benefits the largest players who can navigate it best.&lt;/p&gt;
    &lt;p&gt;Mandatory licensing: Treat AI training like music covers, with compulsory licenses at government-set rates. But where does licensing apply? At training time, when data ingestion is diffuse and provenance unclear? At generation time, forcing cumbersome per-output fees? Or only at commercial use, which just pushes enforcement back onto platforms? Licensing works when the unit being licensed is discrete and identifiable. In AI systems, the unit of influence is neither.&lt;/p&gt;
    &lt;p&gt;What these approaches share is an assumption that influence can be measured, attributed, and priced with reasonable precision. That assumption no longer holds.&lt;/p&gt;
    &lt;p&gt;If both traditional enforcement and alternative frameworks collapse under their own complexity, we‚Äôre left with an uncomfortable question: should AI companies bear responsibility at all for diffuse, non-attributable learning? And do existing IP norms‚Äîbuilt around discrete works and identifiable acts of copying‚Äîstill make sense in a world where content creation is increasingly statistical, generative, and cheap?&lt;/p&gt;
    &lt;head rend="h3"&gt;No Good Answers: The Fundamental Incompatibility&lt;/head&gt;
    &lt;p&gt;The uncomfortable truth is that copyright law works best under conditions of scarcity: creation and transformation are costly, distribution is constrained, enforcement can happen at a limited number of chokepoints, and the volume of disputes remains manageable.&lt;/p&gt;
    &lt;p&gt;While we argue about how AI interacts with these rules, there is a deeper problem: the nature of content itself is changing. We are moving from static, general-purpose works to dynamic, personalized, on-demand experiences.&lt;/p&gt;
    &lt;p&gt;Imagine the New York Times not publishing fixed articles, but operating an AI service that generates reporting tailored to your preferred length, tone, and depth. Journalists still source information, investigate facts, and shape narratives, but the ‚Äúarticle‚Äù is assembled dynamically for each reader, potentially never existing twice in the same form.&lt;/p&gt;
    &lt;p&gt;This pattern is already familiar. Many people now prefer learning about complex topics through conversation with an LLM rather than reading a static Wikipedia page. Over time, this decoupling of interface from underlying information will extend beyond text. Fixed films may give way to interactive narratives that adapt to viewer choices and generate scenes on the fly. With heavy personalization, companies may not even retain a complete record of what content was generated for whom.&lt;/p&gt;
    &lt;p&gt;In that world, what does copyright mean? If an article is generated fresh for each reader, synthesized from thousands of sources, where do rights attach? Is the output the copyrighted work? The underlying system? Each individualized experience? If a story adapts in real time, is every interaction a new work?&lt;/p&gt;
    &lt;p&gt;These aren‚Äôt distant hypotheticals (or won‚Äôt be for long). We are at the beginning of a shift that existing IP frameworks‚Äîdesigned for physical books, fixed recordings, and discrete artworks‚Äîwere never meant to handle. The problem isn‚Äôt that these frameworks are poorly written; it‚Äôs that they assume stable works, identifiable acts of copying, and enforceable boundaries.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why every attempted fix breaks down. We try to regulate training, but learning is diffuse. We try to regulate generation, but creation is private and cheap. We try to regulate distribution, but content is fluid, personalized, and global.&lt;/p&gt;
    &lt;p&gt;The irony is that by the time we resolve today‚Äôs AI copyright debates, if we ever do, the thing we‚Äôre fighting over may already be obsolete. We‚Äôre litigating static articles while the concept of ‚Äúan article‚Äù dissolves into something conversational and adaptive. We‚Äôre arguing about copies while the boundary between creation and consumption blurs into a single interactive process.&lt;/p&gt;
    &lt;p&gt;The problems we‚Äôre grappling with today are real and difficult. But they may also be the wrong problems. The AI copyright debate isn‚Äôt just exposing how strained existing law has become; it‚Äôs revealing that we‚Äôre trying to regulate a world that no longer exists while the actual future races ahead, raising questions we haven‚Äôt yet learned how to ask.&lt;/p&gt;
    &lt;p&gt;The ambiguity was always there. AI just made it impossible to keep pretending we had good answers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jasonwillems.com/technology/2026/02/02/AI-Copyright/"/><published>2026-02-03T15:52:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872706</id><title>Qwen3-Coder-Next</title><updated>2026-02-03T17:43:38.388343+00:00</updated><link href="https://qwen.ai/blog?id=qwen3-coder-next"/><published>2026-02-03T16:01:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872733</id><title>Launch HN: Modelence (YC S25) ‚Äì App Builder with TypeScript / MongoDB Framework</title><updated>2026-02-03T17:43:38.073446+00:00</updated><content>&lt;doc fingerprint="bb22d77b01df145e"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi all, Aram and Eduard here - co-founders of Modelence (&lt;/p&gt;https://modelence.com&lt;p&gt;). After spending years on scaling our previous startup‚Äôs platform, we built an open-source full-stack TypeScript + MongoDB framework to stop solving the same auth / database / API / cron job implementations every time we created an app, and we didn‚Äôt like the idea of using multiple managed platforms for each of these to run our apps either.&lt;/p&gt;&lt;p&gt;(Here‚Äôs our prior Show HN post for reference: https://news.ycombinator.com/item?id=44902227)&lt;/p&gt;&lt;p&gt;At the same time, we were excited by the whole AI app builder boom and realized that the real challenge there is the platform rather than the tool itself. Now we‚Äôre making Modelence the first full-stack framework that‚Äôs built for coding agents and humans alike:&lt;/p&gt;&lt;p&gt;- TypeScript is already great for AI coding because it provides guardrails and catches many errors at build time, so agents can auto-correct&lt;/p&gt;&lt;p&gt;- MongoDB eliminates the schema management problem for agents, which is where they fail the most often otherwise (+ works great with TS/Node.js)&lt;/p&gt;&lt;p&gt;- Built-in auth, database, cron jobs and else that just works together out of the box means agents only focus on your product logic and don‚Äôt fail at trying to set these things up (+ less tokens spent on boilerplate).&lt;/p&gt;&lt;p&gt;You can now try the Modelence app builder (based on Claude Agent SDK) by just typing a prompt on our landing page ( https://modelence.com ) - watch a demo video here: https://youtu.be/BPsYvj_nGuE&lt;/p&gt;&lt;p&gt;Then you can check it out locally and continue working in your own IDE, while still using Modelence Cloud as your backend, with a dev cloud environment, and later deploy and run on Modelence Cloud with built-in observability around every operation running in your app.&lt;/p&gt;&lt;p&gt;We‚Äôre also going to add a built-in DevOps agent that lives in the same cloud, knows the framework end-to-end, and will use all this observability data to act on errors, alerts, and incidents - closing the loop, because running in production is much harder than just building.&lt;/p&gt;&lt;p&gt;We launched the app builder as a quick start for developers, to demonstrate the framework and Modelence Cloud without having to manually read docs and follow the steps to set up a new app. Our main focus is still the platform itself, since we believe the real challenge in AI coding is the framework and the platform rather than the builder tool itself.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46872733"/><published>2026-02-03T16:03:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872818</id><title>Show HN: C discrete event SIM w stackful coroutines runs 45x faster than SimPy</title><updated>2026-02-03T17:43:37.318307+00:00</updated><content>&lt;doc fingerprint="6a388adbeae31e0c"&gt;
  &lt;main&gt;
    &lt;p&gt;A fast discrete event simulation library written in C and assembly with POSIX pthreads. Simulated processes are implemented as stackful coroutines ("fibers") inside the pthreads.&lt;/p&gt;
    &lt;p&gt;Implementation status:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;x86-64: Stable, both for Linux and Windows&lt;/item&gt;
      &lt;item&gt;Apple Silicon: Planned&lt;/item&gt;
      &lt;item&gt;ARM: Planned&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cimba models run 40-50 times faster than SimPy equivalents. The chart below shows the number of simulated events processed per second of wall clock time on a simple M/M/1 queue implemented in SimPy and Cimba. Cimba runs this scenario 45 times faster than SimPy with all CPU cores in use. Cimba runs 25 % faster (20M events/sec) on a single core than SimPy using all 64 cores (16M events/sec).&lt;/p&gt;
    &lt;p&gt;It is fast, powerful, reliable, and free.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Fast: The speed from multithreaded parallel execution translates to high resolution in your simulation modeling. You can run hundreds of replications and parameter variations in just a few seconds, generating tight confidence intervals in your experiments and a high density of data points along parameter variations.&lt;/p&gt;
        &lt;p&gt;In the benchmark shown above, Cimba reduces the run time by 97.8 % compared to the same model in SimPy using all CPU cores. This translates into doing your simulation experiments in seconds instead of minutes, or in minutes instead of hours.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Powerful: Cimba provides a comprehensive toolkit for discrete event simulation:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Processes implemented as asymmetric stackful coroutines. A simulated process can yield and resume control from any level of a function call stack, allowing well-structured coding of arbitrarily large simulation models. As a first-order object, a simulated process can be passed as an argument to other functions, returned from functions, and stored in data structures, allowing rich and complex interactions between processes.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Pre-packaged process interaction mechanisms like resources, resource pools, buffers, object queues, priority queues, and timeouts. Cimba also provides condition variables where your simulated processes can wait for arbitrarily complex conditions to become true ‚Äì anything you can express as a function returning a binary true or false result.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;A wide range of fast, high-quality random number generators, both of academically important and more empirically oriented types. Important distributions like normal and exponential are implemented by state-of-the-art ziggurat rejection sampling for speed and accuracy.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Integrated logging and data collection features that make it easy to get a model running and understand what is happening inside it, including custom asserts to pinpoint sources of errors.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;As a C library, Cimba allows easy integration with other libraries and programs. You could call CUDA routines to enhance your simulation models with GPU-powered agentic behavior or drive a fancy graphics interface like a 3D visualization of a manufacturing plant. You could even call the Cimba simulation engine from other programming languages, since the C calling convention is standard and well-documented.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Reliable: Cimba is well-engineered open source. There is no mystery to the results you get. The code is written with liberal use of assertions to enforce preconditions, invariants, and postconditions in each function. The assertions act as self-enforcing documentation on expected inputs to and outputs from the Cimba functions. About 13 % of all code lines in the Cimba library are asssertions, a very high density. There are unit tests for each module. Running the unit test battery in debug mode (all assertions active) verifies the correct operation in great detail. You can do that by the one-liner&lt;/p&gt;&lt;code&gt;meson test -C build&lt;/code&gt;from the terminal command line.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Free: Cimba should fit well into the budget of most research groups.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is a general-purpose discrete event simulation library, in the spirit of a 21st century Simula67 descendant. You can use it to model, e.g.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;computer networks,&lt;/item&gt;
      &lt;item&gt;transportation networks,&lt;/item&gt;
      &lt;item&gt;operating system task scheduling,&lt;/item&gt;
      &lt;item&gt;manufacturing systems and job shops,&lt;/item&gt;
      &lt;item&gt;military command and control systems,&lt;/item&gt;
      &lt;item&gt;hospital and emergency room patient flows,&lt;/item&gt;
      &lt;item&gt;queuing systems like bank tellers and store checkouts,&lt;/item&gt;
      &lt;item&gt;urban systems like public transport and garbage collection,&lt;/item&gt;
      &lt;item&gt;and quite a few more application domains of similar kinds, where overall system complexity arises from interactions between relatively simple components.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you look under the hood, you will also find additional reusable internal components. Cimba contains stackful coroutines doing their own thing on thread-safe cactus stacks. There are fast memory pool allocators for generic small objects and hash-heaps combining a binary heap and an open addressing hash map using Fibonacci hashing. Although not part of the public Cimba API, these components can also be used in your model if needed.&lt;/p&gt;
    &lt;p&gt;It is C11/C17. As an illustration, this is the entire code for our multithreaded M/M/1 benchmark mentioned above:&lt;/p&gt;
    &lt;code&gt;    #include &amp;lt;inttypes.h&amp;gt;
    #include &amp;lt;stdio.h&amp;gt;
    #include &amp;lt;stdint.h&amp;gt;
    
    #include &amp;lt;cimba.h&amp;gt;
    
    #define NUM_OBJECTS 1000000u
    #define ARRIVAL_RATE 0.9
    #define SERVICE_RATE 1.0
    #define NUM_TRIALS 100
    
    CMB_THREAD_LOCAL struct cmi_mempool objectpool = CMI_MEMPOOL_STATIC_INIT(8u, 512u);
    
    struct simulation {
        struct cmb_process *arrival;
        struct cmb_process *service;
        struct cmb_objectqueue *queue;
    };
    
    struct trial {
        double arr_mean;
        double srv_mean;
        uint64_t obj_cnt;
        double sum_wait;
        double avg_wait;
    };
    
    struct context {
        struct simulation *sim;
        struct trial *trl;
    };
    
    void *arrivalfunc(struct cmb_process *me, void *vctx)
    {
        cmb_unused(me);
        const struct context *ctx = vctx;
        struct cmb_objectqueue *qp = ctx-&amp;gt;sim-&amp;gt;queue;
        const double mean_hld = ctx-&amp;gt;trl-&amp;gt;arr_mean;
        for (uint64_t ui = 0; ui &amp;lt; NUM_OBJECTS; ui++) {
            const double t_hld = cmb_random_exponential(mean_hld);
            cmb_process_hold(t_hld);
            void *object = cmi_mempool_alloc(&amp;amp;objectpool);
            double *dblp = object;
            *dblp = cmb_time();
            cmb_objectqueue_put(qp, object);
        }
    
        return NULL;
    }
    
    void *servicefunc(struct cmb_process *me, void *vctx)
    {
        cmb_unused(me);
        const struct context *ctx = vctx;
        struct cmb_objectqueue *qp = ctx-&amp;gt;sim-&amp;gt;queue;
        const double mean_srv = ctx-&amp;gt;trl-&amp;gt;srv_mean;
        uint64_t *cnt = &amp;amp;(ctx-&amp;gt;trl-&amp;gt;obj_cnt);
        double *sum = &amp;amp;(ctx-&amp;gt;trl-&amp;gt;sum_wait);
        while (true) {
            void *object = NULL;
            cmb_objectqueue_get(qp, &amp;amp;object);
            const double *dblp = object;
            const double t_srv = cmb_random_exponential(mean_srv);
            cmb_process_hold(t_srv);
            const double t_sys = cmb_time() - *dblp;
            *sum += t_sys;
            *cnt += 1u;
            cmi_mempool_free(&amp;amp;objectpool, object);
        }
    }
    
    void run_trial(void *vtrl)
    {
        struct trial *trl = vtrl;
    
        cmb_logger_flags_off(CMB_LOGGER_INFO);
        cmb_random_initialize(cmb_random_hwseed());
        cmb_event_queue_initialize(0.0);
        struct context *ctx = malloc(sizeof(*ctx));
        ctx-&amp;gt;trl = trl;
        struct simulation *sim = malloc(sizeof(*sim));
        ctx-&amp;gt;sim = sim;
    
        sim-&amp;gt;queue = cmb_objectqueue_create();
        cmb_objectqueue_initialize(sim-&amp;gt;queue, "Queue", CMB_UNLIMITED);
    
        sim-&amp;gt;arrival = cmb_process_create();
        cmb_process_initialize(sim-&amp;gt;arrival, "Arrival", arrivalfunc, ctx, 0);
        cmb_process_start(sim-&amp;gt;arrival);
        sim-&amp;gt;service = cmb_process_create();
        cmb_process_initialize(sim-&amp;gt;service, "Service", servicefunc, ctx, 0);
        cmb_process_start(sim-&amp;gt;service);
    
        cmb_event_queue_execute();
    
        cmb_process_stop(sim-&amp;gt;service, NULL);
        cmb_process_terminate(sim-&amp;gt;arrival);
        cmb_process_terminate(sim-&amp;gt;service);
        cmb_process_destroy(sim-&amp;gt;arrival);
        cmb_process_destroy(sim-&amp;gt;service);
    
        cmb_objectqueue_destroy(sim-&amp;gt;queue);
        cmb_event_queue_terminate();
        free(sim);
        free(ctx);
    }
    
    int main(void)
    {
        struct trial *experiment = calloc(NUM_TRIALS, sizeof(*experiment));
        for (unsigned ui = 0; ui &amp;lt; NUM_TRIALS; ui++) {
            struct trial *trl = &amp;amp;experiment[ui];
            trl-&amp;gt;arr_mean = 1.0 / ARRIVAL_RATE;
            trl-&amp;gt;srv_mean = 1.0 / SERVICE_RATE;
            trl-&amp;gt;obj_cnt = 0u;
            trl-&amp;gt;sum_wait = 0.0;
        }
    
        cimba_run_experiment(experiment,
                             NUM_TRIALS,
                             sizeof(*experiment),
                             run_trial);
    
        struct cmb_datasummary summary;
        cmb_datasummary_initialize(&amp;amp;summary);
        for (unsigned ui = 0; ui &amp;lt; NUM_TRIALS; ui++) {
            const double avg_tsys = experiment[ui].sum_wait / (double)(experiment[ui].obj_cnt);
            cmb_datasummary_add(&amp;amp;summary, avg_tsys);
        }
    
        const unsigned un = cmb_datasummary_count(&amp;amp;summary);
        if (un &amp;gt; 1) {
            const double mean_tsys = cmb_datasummary_mean(&amp;amp;summary);
            const double sdev_tsys = cmb_datasummary_stddev(&amp;amp;summary);
            const double serr_tsys = sdev_tsys / sqrt((double)un);
            const double ci_w = 1.96 * serr_tsys;
            const double ci_l = mean_tsys - ci_w;
            const double ci_u = mean_tsys + ci_w;
    
            printf("Average system time %f (n %u, conf.int. %f - %f, expected %f)\n",
                   mean_tsys, un, ci_l, ci_u, 1.0 / (SERVICE_RATE - ARRIVAL_RATE));
    
            return 0;
        }
    }

&lt;/code&gt;
    &lt;p&gt;See our tutorial at ReadTheDocs for more usage examples.&lt;/p&gt;
    &lt;p&gt;As shown above, it is some 45 times faster than SimPy in a relevant benchmark. It means getting your results almost immediately rather than after a "go brew a pot of coffee" delay breaking your line of thought.&lt;/p&gt;
    &lt;p&gt;For another illustration of how to benefit from the sheer speed, the experiment in test_cimba.c simulates an M/G/1 queue at four different levels of service process variability. For each variability level, it tries five system utilization levels. There are ten replications for each parameter combination, in total 4 * 5 * 10 = 200 trials. Each trial lasts for one million time units, where the average service time always is 1.0 time units.&lt;/p&gt;
    &lt;p&gt;This entire simulation runs in about 1.5 seconds on an AMD Threadripper 3970X with Arch Linux and produces the chart below.&lt;/p&gt;
    &lt;p&gt;Discrete event simulation fits well with an object-oriented paradigm. That is why object-oriented programming was invented in the first place for Simula67. Since OOP is not directly enforced in plain C, we provide the object-oriented characteristics (such as encapsulation, inheritance, polymorphism, and abstraction) in the Cimba software design instead. (See the ReadTheDocs explanation for more details.)&lt;/p&gt;
    &lt;p&gt;The simulated processes are stackful coroutines on their own call stacks, allowing the processes to store their state at arbitrary points and resume execution from there later with minimal overhead. The context-switching code is hand-coded in assembly for each platform. (You can find more details here.)&lt;/p&gt;
    &lt;p&gt;The C code is liberally sprinkled with &lt;code&gt;assert&lt;/code&gt; statements testing for preconditions,
invariants, and postconditions wherever possible, applying
Design by Contract
principles for high reliability. The Cimba library contains 958 asserts in 7132 lines of
C code, for a very high assert density of 13.4 %. These are custom-written
assert macros that will report
what trial, what process, the simulated time, the function and line number, and even the
random number seed used, if anything should go wrong. All time-consuming invariants and
postconditions are debug asserts, while the release asserts mostly check preconditions
like function argument validity. Turning off the debug asserts doubles the speed of your
model when you are ready for it, while turning off the release asserts as well gives
a small incremental improvement. (Again,
more explanation here.)&lt;/p&gt;
    &lt;p&gt;This is then combined with extensive unit testing of each module, ensuring that all lower level functionality works as expected before moving on to higher levels. You will find the test files corresponding to each code module in the test directory.&lt;/p&gt;
    &lt;p&gt;But do read the LICENSE. We are not giving any warranties here.&lt;/p&gt;
    &lt;p&gt;Long story made short: C++ exception handling is not very friendly to the stackful coroutines we need in Cimba. The stackless coroutines in C++ are entirely different from what we need.&lt;/p&gt;
    &lt;p&gt;C++ has also become a large and feature-rich language, where it will be hard to ensure compatibility with every possible combination of features.&lt;/p&gt;
    &lt;p&gt;Hence (like the Linux kernel), we chose the simpler platform for speed, clarity, and reliability. If you need to call Cimba from some other language, the C calling convention is well-known and well-documented.&lt;/p&gt;
    &lt;p&gt;Because it was not made public before. What retrospectively can be called Cimba 1.0 was implemented in K&amp;amp;R C at MIT in the early 1990's, followed by a parallelized version 2.0 in ANSI C and Perl around 1995‚Äì96. The present version written in C17 with POSIX pthreads is the third major rebuild, and the first public version.&lt;/p&gt;
    &lt;p&gt;It is right here. You clone the repository, build, and install it. You will need a C compiler and the Meson build manager. On Linux, you can use GCC or Clang, while the recommended approach on Windows is MinGW with its GCC compiler. For convenience, we recommend the CLion integrated development environment with GCC, Meson, and Ninja built-in support on both Linux and Windows.&lt;/p&gt;
    &lt;p&gt;You will find the installation guide here: https://cimba.readthedocs.io/en/latest/installation.html&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ambonvik/cimba"/><published>2026-02-03T16:09:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873138</id><title>Prek: A better, faster, drop-in pre-commit replacement, engineered in Rust</title><updated>2026-02-03T17:43:36.501350+00:00</updated><content>&lt;doc fingerprint="28df57ddde9b6d0"&gt;
  &lt;main&gt;
    &lt;p&gt;pre-commit is a framework to run hooks written in many languages, and it manages the language toolchain and dependencies for running the hooks.&lt;/p&gt;
    &lt;p&gt;prek is a reimagined version of pre-commit, built in Rust. It is designed to be a faster, dependency-free and drop-in alternative for it, while also providing some additional long-requested features.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Although prek is pretty new, it‚Äôs already powering real‚Äëworld projects like CPython, Apache Airflow, FastAPI, and more projects are picking it up‚Äîsee Who is using prek?. If you‚Äôre looking for an alternative to &lt;code&gt;pre-commit&lt;/code&gt;, please give it a try‚Äîwe‚Äôd love your feedback!&lt;/p&gt;
    &lt;p&gt;Please note that some languages are not yet supported for full drop‚Äëin parity with &lt;code&gt;pre-commit&lt;/code&gt;. See Language Support for current status.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üöÄ A single binary with no dependencies, does not require Python or any other runtime.&lt;/item&gt;
      &lt;item&gt;‚ö° Faster than &lt;code&gt;pre-commit&lt;/code&gt;and more efficient in disk space usage.&lt;/item&gt;
      &lt;item&gt;üîÑ Fully compatible with the original pre-commit configurations and hooks.&lt;/item&gt;
      &lt;item&gt;üèóÔ∏è Built-in support for monorepos (i.e. workspace mode).&lt;/item&gt;
      &lt;item&gt;üêç Integration with &lt;code&gt;uv&lt;/code&gt;for managing Python virtual environments and dependencies.&lt;/item&gt;
      &lt;item&gt;üõ†Ô∏è Improved toolchain installations for Python, Node.js, Bun, Go, Rust and Ruby, shared between hooks.&lt;/item&gt;
      &lt;item&gt;üì¶ Built-in Rust-native implementation of some common hooks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Standalone installer&lt;/head&gt;
    &lt;p&gt;prek provides a standalone installer script to download and install the tool,&lt;/p&gt;
    &lt;p&gt;On Linux and macOS:&lt;/p&gt;
    &lt;code&gt;curl --proto '=https' --tlsv1.2 -LsSf https://github.com/j178/prek/releases/download/v0.3.1/prek-installer.sh | sh&lt;/code&gt;
    &lt;p&gt;On Windows:&lt;/p&gt;
    &lt;code&gt;powershell -ExecutionPolicy ByPass -c "irm https://github.com/j178/prek/releases/download/v0.3.1/prek-installer.ps1 | iex"&lt;/code&gt;
    &lt;head&gt;PyPI&lt;/head&gt;
    &lt;p&gt;prek is published as Python binary wheel to PyPI, you can install it using &lt;code&gt;pip&lt;/code&gt;, &lt;code&gt;uv&lt;/code&gt; (recommended), or &lt;code&gt;pipx&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Using uv (recommended)
uv tool install prek

# Using uvx (install and run in one command)
uvx prek

# Adding prek to the project dev-dependencies
uv add --dev prek

# Using pip
pip install prek

# Using pipx
pipx install prek&lt;/code&gt;
    &lt;head&gt;Homebrew&lt;/head&gt;
    &lt;code&gt;brew install prek&lt;/code&gt;
    &lt;head&gt;Cargo&lt;/head&gt;
    &lt;p&gt;Build from source using Cargo (Rust 1.89+ is required):&lt;/p&gt;
    &lt;code&gt;cargo install --locked prek&lt;/code&gt;
    &lt;head&gt;npmjs&lt;/head&gt;
    &lt;p&gt;prek is published as a Node.js package and can be installed with any npm-compatible package manager:&lt;/p&gt;
    &lt;code&gt;# As a dev dependency
npm add -D @j178/prek
pnpm add -D @j178/prek
bun add -D @j178/prek

# Or install globally
npm install -g @j178/prek
pnpm add -g @j178/prek
bun install -g @j178/prek

# Or run directly without installing
npx @j178/prek --version
bunx @j178/prek --version&lt;/code&gt;
    &lt;head&gt;Nix&lt;/head&gt;
    &lt;p&gt;prek is available via Nixpkgs.&lt;/p&gt;
    &lt;code&gt;# Choose what's appropriate for your use case.
# One-off in a shell:
nix-shell -p prek

# NixOS or non-NixOS without flakes:
nix-env -iA nixos.prek

# Non-NixOS with flakes:
nix profile install nixpkgs#prek&lt;/code&gt;
    &lt;head&gt;GitHub Releases&lt;/head&gt;
    &lt;p&gt;Pre-built binaries are available for download from the GitHub releases page.&lt;/p&gt;
    &lt;head&gt;GitHub Actions&lt;/head&gt;
    &lt;p&gt;prek can be used in GitHub Actions via the j178/prek-action repository.&lt;/p&gt;
    &lt;p&gt;Example workflow:&lt;/p&gt;
    &lt;code&gt;name: Prek checks
on: [push, pull_request]

jobs:
  prek:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6
      - uses: j178/prek-action@v1&lt;/code&gt;
    &lt;p&gt;This action installs prek and runs &lt;code&gt;prek run --all-files&lt;/code&gt; on your repository.&lt;/p&gt;
    &lt;p&gt;prek is also available via &lt;code&gt;taiki-e/install-action&lt;/code&gt; for installing various tools.&lt;/p&gt;
    &lt;p&gt;If installed via the standalone installer, prek can update itself to the latest version:&lt;/p&gt;
    &lt;code&gt;prek self update&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I already use pre-commit: follow the short migration checklist in the quickstart guide to swap in &lt;code&gt;prek&lt;/code&gt;safely.&lt;/item&gt;
      &lt;item&gt;I'm new to pre-commit-style tools: learn the basics‚Äîcreating a config, running hooks, and installing git hooks‚Äîin the beginner quickstart walkthrough.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is multiple times faster than &lt;code&gt;pre-commit&lt;/code&gt;and takes up half the disk space.&lt;/item&gt;
      &lt;item&gt;It redesigned how hook environments and toolchains are managed, they are all shared between hooks, which reduces the disk space usage and speeds up the installation process.&lt;/item&gt;
      &lt;item&gt;Repositories are cloned in parallel, and hooks are installed in parallel if their dependencies are disjoint.&lt;/item&gt;
      &lt;item&gt;Hooks can run in parallel by priority (hooks with the same &lt;code&gt;priority&lt;/code&gt;may run concurrently), reducing end-to-end runtime.&lt;/item&gt;
      &lt;item&gt;It uses &lt;code&gt;uv&lt;/code&gt;for creating Python virtualenvs and installing dependencies, which is known for its speed and efficiency.&lt;/item&gt;
      &lt;item&gt;It implements some common hooks in Rust, built in prek, which are faster than their Python counterparts.&lt;/item&gt;
      &lt;item&gt;It supports &lt;code&gt;repo: builtin&lt;/code&gt;for offline, zero-setup hooks, which is not available in&lt;code&gt;pre-commit&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No need to install Python or any other runtime, just download a single binary.&lt;/item&gt;
      &lt;item&gt;No hassle with your Python version or virtual environments, prek automatically installs the required Python version and creates a virtual environment for you.&lt;/item&gt;
      &lt;item&gt;Built-in support for workspaces (or monorepos), each subproject can have its own &lt;code&gt;.pre-commit-config.yaml&lt;/code&gt;file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prek run&lt;/code&gt;has some nifty improvements over&lt;code&gt;pre-commit run&lt;/code&gt;, such as:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;prek run --directory &amp;lt;dir&amp;gt;&lt;/code&gt;runs hooks for files in the specified directory, no need to use&lt;code&gt;git ls-files -- &amp;lt;dir&amp;gt; | xargs pre-commit run --files&lt;/code&gt;anymore.&lt;/item&gt;&lt;item&gt;&lt;code&gt;prek run --last-commit&lt;/code&gt;runs hooks for files changed in the last commit.&lt;/item&gt;&lt;item&gt;&lt;code&gt;prek run [HOOK] [HOOK]&lt;/code&gt;selects and runs multiple hooks.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prek list&lt;/code&gt;command lists all available hooks, their ids, and descriptions, providing a better overview of the configured hooks.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prek auto-update&lt;/code&gt;supports&lt;code&gt;--cooldown-days&lt;/code&gt;to mitigate open source supply chain attacks.&lt;/item&gt;
      &lt;item&gt;prek provides shell completions for &lt;code&gt;prek run &amp;lt;hook_id&amp;gt;&lt;/code&gt;command, making it easier to run specific hooks without remembering their ids.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more detailed improvements prek offers, take a look at Difference from pre-commit.&lt;/p&gt;
    &lt;p&gt;prek is pretty new, but it is already being used or recommend by some projects and organizations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;apache/airflow&lt;/item&gt;
      &lt;item&gt;python/cpython&lt;/item&gt;
      &lt;item&gt;pdm-project/pdm&lt;/item&gt;
      &lt;item&gt;fastapi/fastapi&lt;/item&gt;
      &lt;item&gt;fastapi/typer&lt;/item&gt;
      &lt;item&gt;fastapi/asyncer&lt;/item&gt;
      &lt;item&gt;astral-sh/ruff&lt;/item&gt;
      &lt;item&gt;astral-sh/ty&lt;/item&gt;
      &lt;item&gt;openclaw/openclaw&lt;/item&gt;
      &lt;item&gt;home-assistant/core&lt;/item&gt;
      &lt;item&gt;DetachHead/basedpyright&lt;/item&gt;
      &lt;item&gt;OpenLineage/OpenLineage&lt;/item&gt;
      &lt;item&gt;authlib/authlib&lt;/item&gt;
      &lt;item&gt;django/djangoproject.com&lt;/item&gt;
      &lt;item&gt;Future-House/paper-qa&lt;/item&gt;
      &lt;item&gt;requests-cache/requests-cache&lt;/item&gt;
      &lt;item&gt;Goldziher/kreuzberg&lt;/item&gt;
      &lt;item&gt;python-attrs/attrs&lt;/item&gt;
      &lt;item&gt;jlowin/fastmcp&lt;/item&gt;
      &lt;item&gt;apache/iceberg-python&lt;/item&gt;
      &lt;item&gt;apache/lucene&lt;/item&gt;
      &lt;item&gt;jcrist/msgspec&lt;/item&gt;
      &lt;item&gt;python-humanize/humanize&lt;/item&gt;
      &lt;item&gt;MoonshotAI/kimi-cli&lt;/item&gt;
      &lt;item&gt;simple-icons/simple-icons&lt;/item&gt;
      &lt;item&gt;ast-grep/ast-grep&lt;/item&gt;
      &lt;item&gt;commitizen-tools/commitizen&lt;/item&gt;
      &lt;item&gt;cocoindex-io/cocoindex&lt;/item&gt;
      &lt;item&gt;cachix/devenv&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is heavily inspired by the original pre-commit tool, and it wouldn't be possible without the hard work of the maintainers and contributors of that project.&lt;/p&gt;
    &lt;p&gt;And a special thanks to the Astral team for their remarkable projects, particularly uv, from which I've learned a lot on how to write efficient and idiomatic Rust code.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/j178/prek"/><published>2026-02-03T16:29:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873294</id><title>France dumps Zoom and Teams as Europe seeks digital autonomy from the US</title><updated>2026-02-03T17:43:36.154826+00:00</updated><content>&lt;doc fingerprint="af50b110b70b4885"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;France dumps Zoom and Teams as Europe seeks digital autonomy from the US&lt;/head&gt;
    &lt;head rend="h2"&gt;France dumps Zoom and Teams as Europe seeks digital autonomy from the US&lt;/head&gt;
    &lt;p&gt;LONDON (AP) ‚Äî In France, civil servants will ditch Zoom and Teams for a homegrown video conference system. Soldiers in Austria are using open source office software to write reports after the military dropped Microsoft Office. Bureaucrats in a German state have also turned to free software for their administrative work.&lt;/p&gt;
    &lt;p&gt;Around Europe, governments and institutions are seeking to reduce their use of digital services from U.S. Big Tech companies and turning to domestic or free alternatives. The push for ‚Äúdigital sovereignty‚Äù is gaining attention as the Trump administration strikes an increasingly belligerent posture toward the continent, highlighted by recent tensions over Greenland that intensified fears that Silicon Valley giants could be compelled to cut off access.&lt;/p&gt;
    &lt;p&gt;Concerns about data privacy and worries that Europe is not doing enough to keep up with the United States and Chinese tech leadership are also fueling the drive.&lt;/p&gt;
    &lt;p&gt;The French government referenced some of these concerns when it announced last week that 2.5 million civil servants would stop using video conference tools from U.S. providers ‚Äî including Zoom, Microsoft Teams, Webex and GoTo Meeting ‚Äî by 2027 and switch to Visio, a homegrown service.&lt;/p&gt;
    &lt;p&gt;The objective is ‚Äúto put an end to the use of non-European solutions, to guarantee the security and confidentiality of public electronic communications by relying on a powerful and sovereign tool,‚Äù the announcement said.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe cannot risk having our scientific exchanges, our sensitive data, and our strategic innovations exposed to non-European actors,‚Äù David Amiel, a civil service minister, said in a press release.&lt;/p&gt;
    &lt;p&gt;Microsoft said it continues to ‚Äúpartner closely with the government in France and respect the importance of security, privacy, and digital trust for public institutions.‚Äù&lt;/p&gt;
    &lt;p&gt;The company said it is ‚Äúfocused on providing customers with greater choice, stronger data protection, and resilient cloud services ‚Äî ensuring data stays in Europe, under European law, with robust security and privacy protections.‚Äù&lt;/p&gt;
    &lt;p&gt;Zoom, Webex and GoTo Meeting did not respond to requests for comment.&lt;/p&gt;
    &lt;p&gt;French President Emmanuel Macron has been pushing digital sovereignty for years. But there‚Äôs now a lot more ‚Äúpolitical momentum behind this idea now that we need to de-risk from U.S. tech,‚Äù Nick Reiners, senior geotechnology analyst at the Eurasia Group.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt feels kind of like there‚Äôs a real zeitgeist shift,‚Äù Reiners said&lt;/p&gt;
    &lt;p&gt;It was a hot topic at the World Economic Forum‚Äôs annual meeting of global political and business elites last month in Davos, Switzerland. The European Commission‚Äôs official for tech sovereignty, Henna Virkkunen, told an audience that Europe‚Äôs reliance on others ‚Äúcan be weaponized against us.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThat‚Äôs why it‚Äôs so important that we are not dependent on one country or one company when it comes to very critical fields of our economy or society,‚Äù she said, without naming countries or companies.&lt;/p&gt;
    &lt;p&gt;A decisive moment came last year when the Trump administration sanctioned the International Criminal Court‚Äôs top prosecutor after the tribunal, based in The Hague, Netherlands, issued an arrest warrant for Israeli Prime Minister Benjamin Netanyahu, an ally of President Donald Trump.&lt;/p&gt;
    &lt;p&gt;The sanctions led Microsoft to cancel Khan‚Äôs ICC email, a move that was first reported by The Associated Press and sparked fears of a ‚Äúkill switch‚Äù that Big Tech companies can use to turn off service at will.&lt;/p&gt;
    &lt;p&gt;Microsoft maintains it kept in touch with the ICC ‚Äúthroughout the process that resulted in the disconnection of its sanctioned official from Microsoft services. At no point did Microsoft cease or suspend its services to the ICC.‚Äù&lt;/p&gt;
    &lt;p&gt;Microsoft President Brad Smith has repeatedly sought to strengthen trans-Atlantic ties, the company‚Äôs press office said, and pointed to an interview he did last month with CNN in Davos in which he said that jobs, trade and investment. as well as security, would be affected by a rift over Greenland.&lt;/p&gt;
    &lt;p&gt;‚ÄúEurope is the American tech sector‚Äôs biggest market after the United States itself. It all depends on trust. Trust requires dialogue,‚Äù Smith said.&lt;/p&gt;
    &lt;p&gt;Other incidents have added to the movement. There‚Äôs a growing sense that repeated EU efforts to rein in tech giants such as Google with blockbuster antitrust fines and sweeping digital rule books haven‚Äôt done much to curb their dominance.&lt;/p&gt;
    &lt;p&gt;Billionaire Elon Musk is also a factor. Officials worry about relying on his Starlink satellite internet system for communications in Ukraine.&lt;/p&gt;
    &lt;p&gt;Washington and Brussels wrangled for years over data transfer agreements, triggered by former National Security Agency contractor Edward Snowden‚Äôs revelations of U.S. cyber-snooping.&lt;/p&gt;
    &lt;p&gt;With online services now mainly hosted in the cloud through data centers, Europeans fear that their data is vulnerable.&lt;/p&gt;
    &lt;p&gt;U.S. cloud providers have responded by setting up so-called ‚Äúsovereign cloud‚Äù operations, with data centers located in European countries, owned by European entities and with physical and remote access only for staff who are European Union residents.&lt;/p&gt;
    &lt;p&gt;The idea is that ‚Äúonly Europeans can take decisions so that they can‚Äôt be coerced by the U.S.,‚Äù Reiners said.&lt;/p&gt;
    &lt;p&gt;The German state of Schleswig-Holstein last year migrated 44,000 employee inboxes from Microsoft to an open source email program. It also switched from Microsoft‚Äôs SharePoint file sharing system to Nextcloud, an open source platform, and is even considering replacing Windows with Linux and telephones and videoconferencing with open source systems.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe want to become independent of large tech companies and ensure digital sovereignty,‚Äù Digitalization Minister Dirk Schr√∂dter said in an October announcement.&lt;/p&gt;
    &lt;p&gt;The French city of Lyon said last year that it‚Äôs deploying free office software to replace Microsoft. Denmark‚Äôs government and the cities of Copenhagen and Aarhus have also been trying out open-source software.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe must never make ourselves so dependent on so few that we can no longer act freely,‚Äù Digital Minister Caroline Stage Olsen wrote on LinkedIn last year. ‚ÄúToo much public digital infrastructure is currently tied up with very few foreign suppliers.‚Äù&lt;/p&gt;
    &lt;p&gt;The Austrian military said it has also switched to LibreOffice, a software package with word processor, spreadsheet and presentation programs that mirrors Microsoft 365‚Äôs Word, Excel and PowerPoint.&lt;/p&gt;
    &lt;p&gt;The Document Foundation, a nonprofit based in Germany that‚Äôs behind LibreOffice, said the military‚Äôs switch ‚Äúreflects a growing demand for independence from single vendors.‚Äù Reports also said the military was concerned that Microsoft was moving file storage online to the cloud ‚Äî the standard version of LibreOffice is not cloud-based.&lt;/p&gt;
    &lt;p&gt;Some Italian cities and regions adopted the software years ago, said Italo Vignoli, a spokesman for The Document Foundation. Back then, the appeal was not needing to pay for software licenses. Now, it‚Äôs the main reason is to avoid being locked into a proprietary system.&lt;/p&gt;
    &lt;p&gt;‚ÄúAt first, it was: we will save money and by the way, we will get freedom,‚Äù Vignoli said. ‚ÄúToday it is: we will be free and by the way, we will also save some money.‚Äù&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;Associated Press writer Molly Quell in The Hague, Netherlands contributed to this report.&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;This version corrects the contribution line to Molly Quell instead of Molly Hague.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apnews.com/article/europe-digital-sovereignty-big-tech-9f5388b68a0648514cebc8d92f682060"/><published>2026-02-03T16:39:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873539</id><title>Kilobyte is precisely 1000 bytes</title><updated>2026-02-03T17:43:35.868104+00:00</updated><content>&lt;doc fingerprint="3b2134c2f49f1add"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;1 kilobyte is precisely 1000 bytes&lt;/head&gt;
    &lt;p&gt; Published on &lt;lb/&gt;Updated on &lt;/p&gt;
    &lt;p&gt;When it comes to computer memory, we usually learn that a kilobyte is 1024 bytes, a megabyte is 1024 kilobytes, and so on. But what if I told you that it's not necessarily true and 1 kilobyte can be 1000 bytes? And what's more, this makes even more sense.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why do we often say 1 kilobyte = 1024 bytes?&lt;/head&gt;
    &lt;p&gt;Since computers work in a binary system (base 2), the memory is also addressed in binary. This means it's quite impractical to use memory addresses or produce RAM sticks with memory amounts that are not multiples of powers of 2. From the powers of 2 we chose 1024 (210) as the base order of magnitude, since it's very close to 1000 (2.4% difference) and it's not insanely large. So, in practice we often consider kilo as 1024 (210), mega as 1048576 (220), giga as 1073741824 (230), etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does 1000 still make more sense?&lt;/head&gt;
    &lt;p&gt;While binary kilo, mega and giga units are close to their decimal counterparts, some might already notice that the larger are the units, the more is the proportional inaccuracy. In order to illustrate, let's increase the units:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Unit&lt;/cell&gt;
        &lt;cell role="head"&gt;Decimal value&lt;/cell&gt;
        &lt;cell role="head"&gt;Binary value&lt;/cell&gt;
        &lt;cell role="head"&gt;Relative difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Kilo&lt;/cell&gt;
        &lt;cell&gt;1000&lt;/cell&gt;
        &lt;cell&gt;1024&lt;/cell&gt;
        &lt;cell&gt;2.4%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mega&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;1048576&lt;/cell&gt;
        &lt;cell&gt;‚âà 4.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Giga&lt;/cell&gt;
        &lt;cell&gt;1000000000&lt;/cell&gt;
        &lt;cell&gt;1073741824&lt;/cell&gt;
        &lt;cell&gt;‚âà 7.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tera&lt;/cell&gt;
        &lt;cell&gt;1000000000000&lt;/cell&gt;
        &lt;cell&gt;1099511627776&lt;/cell&gt;
        &lt;cell&gt;‚âà 10%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Peta&lt;/cell&gt;
        &lt;cell&gt;1015&lt;/cell&gt;
        &lt;cell&gt;~ 1.126 * 1015&lt;/cell&gt;
        &lt;cell&gt;‚âà 12.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Exa&lt;/cell&gt;
        &lt;cell&gt;1018&lt;/cell&gt;
        &lt;cell&gt;~ 1.153 * 1018&lt;/cell&gt;
        &lt;cell&gt;‚âà 15.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Zetta&lt;/cell&gt;
        &lt;cell&gt;1021&lt;/cell&gt;
        &lt;cell&gt;~ 1.181 * 1021&lt;/cell&gt;
        &lt;cell&gt;‚âà 18.1%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Yotta&lt;/cell&gt;
        &lt;cell&gt;1024&lt;/cell&gt;
        &lt;cell&gt;~ 1.209 * 1024&lt;/cell&gt;
        &lt;cell&gt;‚âà 20.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Ronna&lt;/cell&gt;
        &lt;cell&gt;1027&lt;/cell&gt;
        &lt;cell&gt;~ 1.238 * 1027&lt;/cell&gt;
        &lt;cell&gt;‚âà 23.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Quetta&lt;/cell&gt;
        &lt;cell&gt;1030&lt;/cell&gt;
        &lt;cell&gt;~ 1.268 * 1030&lt;/cell&gt;
        &lt;cell&gt;‚âà 26.8%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For 1 quettabyte the inaccuracy is already larger than a quarter. Even for 1 terabyte the difference is noticeable, around 10%. This problem often happens when hardware manufacturers (such as HDD or SSD) advertise the memory capacity in decimal units while the operating system might show in binary units.&lt;/p&gt;
    &lt;p&gt;For smaller amounts of memory the binary representation is pretty close to the decimal one, but diverges for huge amounts of memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;What are the standard units?&lt;/head&gt;
    &lt;p&gt;This "kilobyte = 1024 bytes" rule is actually an old (often confusing) convention. In the tech industry there is still huge inertia, this old convention is still used by RAM manufacturers (JEDEC), tons of software and some operating systems (such as Windows). Interestingly, storage vendors often prefer the decimal convention, which creates even more confusion (mentioned above).&lt;/p&gt;
    &lt;p&gt;In order to solve this confusion, International Electrotechnical Commission introduced binary prefixes for binary units:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Binary units (International Electrotechnical Commission)&lt;/cell&gt;
        &lt;cell role="head"&gt;Decimal units (International System of Units)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Unit&lt;/cell&gt;
        &lt;cell&gt;Value&lt;/cell&gt;
        &lt;cell&gt;Unit&lt;/cell&gt;
        &lt;cell&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;KiB (kibibyte)&lt;/cell&gt;
        &lt;cell&gt;10241&lt;/cell&gt;
        &lt;cell&gt;kB (kilobyte)&lt;/cell&gt;
        &lt;cell&gt;10001&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MiB (mebibyte)&lt;/cell&gt;
        &lt;cell&gt;10242&lt;/cell&gt;
        &lt;cell&gt;MB (megabyte)&lt;/cell&gt;
        &lt;cell&gt;10002&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GiB (gibibyte)&lt;/cell&gt;
        &lt;cell&gt;10243&lt;/cell&gt;
        &lt;cell&gt;GB (gigabyte)&lt;/cell&gt;
        &lt;cell&gt;10003&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;TiB (tebibyte)&lt;/cell&gt;
        &lt;cell&gt;10244&lt;/cell&gt;
        &lt;cell&gt;TB (terabyte)&lt;/cell&gt;
        &lt;cell&gt;10004&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;PiB (pebibyte)&lt;/cell&gt;
        &lt;cell&gt;10245&lt;/cell&gt;
        &lt;cell&gt;PB (petabyte)&lt;/cell&gt;
        &lt;cell&gt;10005&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;EiB (exbibyte)&lt;/cell&gt;
        &lt;cell&gt;10246&lt;/cell&gt;
        &lt;cell&gt;EB (exabyte)&lt;/cell&gt;
        &lt;cell&gt;10006&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ZiB (zebibyte)&lt;/cell&gt;
        &lt;cell&gt;10247&lt;/cell&gt;
        &lt;cell&gt;ZB (zettabyte)&lt;/cell&gt;
        &lt;cell&gt;10007&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;YiB (yobibyte)&lt;/cell&gt;
        &lt;cell&gt;10248&lt;/cell&gt;
        &lt;cell&gt;YB (yottabyte)&lt;/cell&gt;
        &lt;cell&gt;10008&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;RiB (robibyte)&lt;/cell&gt;
        &lt;cell&gt;10249&lt;/cell&gt;
        &lt;cell&gt;RB (ronnabyte)&lt;/cell&gt;
        &lt;cell&gt;10009&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;QiB (quebibyte)&lt;/cell&gt;
        &lt;cell&gt;102410&lt;/cell&gt;
        &lt;cell&gt;QB (quettabyte)&lt;/cell&gt;
        &lt;cell&gt;100010&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The guidance is: SI prefixes are powers of 10 only, and if you mean powers of 2 you should use IEC binary prefixes (Ki, Mi, Gi, ‚Ä¶).&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;There is still a lot of inertia to equate 1 kilobyte to 1024 bytes. While this is usually acceptable depending on the context, it can sometimes cause confusion, especially for non-technical people.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://waspdev.com/articles/2026-01-11/kilobyte-is-1000-bytes"/><published>2026-02-03T16:53:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873782</id><title>Show HN: Octosphere, a tool to decentralise scientific publishing</title><updated>2026-02-03T17:43:35.267849+00:00</updated><content>&lt;doc fingerprint="f9dc5bdd087c8698"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Octosphere&lt;/head&gt;
    &lt;p&gt;Connecting open science with the social web&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Octosphere?&lt;/head&gt;
    &lt;p&gt;Octosphere bridges the gap between academic publishing and the social web. It automatically syncs your research publications from Octopus to the AT Protocol (the atmosphere) ‚Äî an open, decentralized network for social apps like Bluesky.&lt;/p&gt;
    &lt;p&gt;By sharing your work on the atmosphere, you can reach broader audiences, engage with the public, and increase the visibility of your research beyond traditional academic channels.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sign in with ORCID ‚Äî Authenticate using your researcher identifier.&lt;/item&gt;
      &lt;item&gt;Connect to the atmosphere ‚Äî Sign in with your Bluesky account (or any AT Protocol app).&lt;/item&gt;
      &lt;item&gt;Link your Octopus profile ‚Äî Connect your Octopus author page.&lt;/item&gt;
      &lt;item&gt;Sync your publications ‚Äî Choose one-time sync or enable automatic syncing of future publications.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://octosphere.social/"/><published>2026-02-03T17:11:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873790</id><title>Defining Safe Hardware Design [pdf]</title><updated>2026-02-03T17:43:34.884345+00:00</updated><content/><link href="https://people.csail.mit.edu/rachit/files/pubs/safe-hdls.pdf"/><published>2026-02-03T17:12:04+00:00</published></entry></feed>